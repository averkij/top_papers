{
    "date": {
        "ru": "1 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
        "en": "August 1",
        "zh": "8æœˆ1æ—¥"
    },
    "time_utc": "2025-08-01 03:49",
    "weekday": 4,
    "issue_id": 5124,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2507.23726",
            "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving",
            "url": "https://huggingface.co/papers/2507.23726",
            "abstract": "Seed-Prover, a lemma-style reasoning model using Lean, achieves high performance in formal theorem proving and automated mathematical reasoning through iterative refinement and specialized geometry support.  \t\t\t\t\tAI-generated summary \t\t\t\t LLMs have demonstrated strong mathematical reasoning abilities by leveraging reinforcement learning with long chain-of-thought, yet they continue to struggle with theorem proving due to the lack of clear supervision signals when solely using natural language. Dedicated domain-specific languages like Lean provide clear supervision via formal verification of proofs, enabling effective training through reinforcement learning. In this work, we propose Seed-Prover, a lemma-style whole-proof reasoning model. Seed-Prover can iteratively refine its proof based on Lean feedback, proved lemmas, and self-summarization. To solve IMO-level contest problems, we design three test-time inference strategies that enable both deep and broad reasoning. Seed-Prover proves 78.1% of formalized past IMO problems, saturates MiniF2F, and achieves over 50\\% on PutnamBench, outperforming the previous state-of-the-art by a large margin. To address the lack of geometry support in Lean, we introduce a geometry reasoning engine Seed-Geometry, which outperforms previous formal geometry engines. We use these two systems to participate in IMO 2025 and fully prove 5 out of 6 problems. This work represents a significant advancement in automated mathematical reasoning, demonstrating the effectiveness of formal verification with long chain-of-thought reasoning.",
            "score": 34,
            "issue_id": 5124,
            "pub_date": "2025-07-31",
            "pub_date_card": {
                "ru": "31 Ğ¸ÑĞ»Ñ",
                "en": "July 31",
                "zh": "7æœˆ31æ—¥"
            },
            "hash": "ab5bfbdad68eb6bf",
            "authors": [
                "Luoxin Chen",
                "Jinming Gu",
                "Liankai Huang",
                "Wenhao Huang",
                "Zhicheng Jiang",
                "Allan Jie",
                "Xiaoran Jin",
                "Xing Jin",
                "Chenggang Li",
                "Kaijing Ma",
                "Cheng Ren",
                "Jiawei Shen",
                "Wenlei Shi",
                "Tong Sun",
                "He Sun",
                "Jiahui Wang",
                "Siran Wang",
                "Zhihong Wang",
                "Chenrui Wei",
                "Shufa Wei",
                "Yonghui Wu",
                "Yuchen Wu",
                "Yihang Xia",
                "Huajian Xin",
                "Fan Yang",
                "Huaiyuan Ying",
                "Hongyi Yuan",
                "Zheng Yuan",
                "Tianyang Zhan",
                "Chi Zhang",
                "Yue Zhang",
                "Ge Zhang",
                "Tianyun Zhao",
                "Jianqiu Zhao",
                "Yichi Zhou",
                "Thomas Hanwen Zhu"
            ],
            "affiliations": [
                "ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.23726.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#math",
                    "#reasoning",
                    "#rl"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğµ Ñ‚ĞµĞ¾Ñ€ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "Seed-Prover - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ñ‚ĞµĞ¾Ñ€ĞµĞ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ ÑĞ·Ñ‹Ğº Lean. ĞĞ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ² Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºÑƒ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğµ Ñ‚ĞµĞ¾Ñ€ĞµĞ¼ Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ…. Seed-Prover Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ĞœĞµĞ¶Ğ´ÑƒĞ½Ğ°Ñ€Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ğ»Ğ¸Ğ¼Ğ¿Ğ¸Ğ°Ğ´Ñ‹."
                },
                "en": {
                    "title": "Seed-Prover: Revolutionizing Theorem Proving with Iterative Refinement",
                    "desc": "The paper introduces Seed-Prover, a model designed for formal theorem proving and automated mathematical reasoning using the Lean programming language. It leverages iterative refinement and specialized geometry support to enhance its proof capabilities. By employing reinforcement learning and clear supervision from formal verification, Seed-Prover achieves impressive results on challenging mathematical problems. The model outperforms previous systems, proving a high percentage of formalized IMO problems and demonstrating significant advancements in automated reasoning."
                },
                "zh": {
                    "title": "Seed-Proverï¼šè‡ªåŠ¨åŒ–æ•°å­¦æ¨ç†çš„æ–°çªç ´",
                    "desc": "Seed-Proveræ˜¯ä¸€ç§åŸºäºLeançš„å¼•ç†é£æ ¼æ¨ç†æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨å½¢å¼å®šç†è¯æ˜å’Œè‡ªåŠ¨æ•°å­¦æ¨ç†ä¸­å®ç°é«˜æ€§èƒ½ã€‚è¯¥æ¨¡å‹é€šè¿‡è¿­ä»£ä¼˜åŒ–å’Œä¸“é—¨çš„å‡ ä½•æ”¯æŒï¼Œå…‹æœäº†ä¼ ç»Ÿè‡ªç„¶è¯­è¨€æ¨ç†çš„å±€é™æ€§ã€‚Seed-Proveråˆ©ç”¨Leançš„åé¦ˆå’Œè‡ªæˆ‘æ€»ç»“æ¥ä¸æ–­æ”¹è¿›å…¶è¯æ˜è¿‡ç¨‹ï¼Œå¹¶è®¾è®¡äº†ä¸‰ç§æ¨ç†ç­–ç•¥ä»¥åº”å¯¹å›½é™…æ•°å­¦å¥¥æ—åŒ¹å…‹ï¼ˆIMOï¼‰çº§åˆ«çš„é—®é¢˜ã€‚é€šè¿‡å¼•å…¥Seed-Geometryå‡ ä½•æ¨ç†å¼•æ“ï¼ŒSeed-Proveråœ¨å‡ ä½•é—®é¢˜ä¸Šä¹Ÿå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå±•ç¤ºäº†å½¢å¼éªŒè¯ä¸é•¿é“¾æ¨ç†çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.22879",
            "title": "RecGPT Technical Report",
            "url": "https://huggingface.co/papers/2507.22879",
            "abstract": "RecGPT integrates large language models into recommender systems to focus on user intent, improving content diversity and satisfaction while enhancing merchant and platform performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Recommender systems are among the most impactful applications of artificial intelligence, serving as critical infrastructure connecting users, merchants, and platforms. However, most current industrial systems remain heavily reliant on historical co-occurrence patterns and log-fitting objectives, i.e., optimizing for past user interactions without explicitly modeling user intent. This log-fitting approach often leads to overfitting to narrow historical preferences, failing to capture users' evolving and latent interests. As a result, it reinforces filter bubbles and long-tail phenomena, ultimately harming user experience and threatening the sustainability of the whole recommendation ecosystem.   To address these challenges, we rethink the overall design paradigm of recommender systems and propose RecGPT, a next-generation framework that places user intent at the center of the recommendation pipeline. By integrating large language models (LLMs) into key stages of user interest mining, item retrieval, and explanation generation, RecGPT transforms log-fitting recommendation into an intent-centric process. To effectively align general-purpose LLMs to the above domain-specific recommendation tasks at scale, RecGPT incorporates a multi-stage training paradigm, which integrates reasoning-enhanced pre-alignment and self-training evolution, guided by a Human-LLM cooperative judge system. Currently, RecGPT has been fully deployed on the Taobao App. Online experiments demonstrate that RecGPT achieves consistent performance gains across stakeholders: users benefit from increased content diversity and satisfaction, merchants and the platform gain greater exposure and conversions. These comprehensive improvement results across all stakeholders validates that LLM-driven, intent-centric design can foster a more sustainable and mutually beneficial recommendation ecosystem.",
            "score": 9,
            "issue_id": 5124,
            "pub_date": "2025-07-30",
            "pub_date_card": {
                "ru": "30 Ğ¸ÑĞ»Ñ",
                "en": "July 30",
                "zh": "7æœˆ30æ—¥"
            },
            "hash": "2bd5536810f1694b",
            "authors": [
                "Chao Yi",
                "Dian Chen",
                "Gaoyang Guo",
                "Jiakai Tang",
                "Jian Wu",
                "Jing Yu",
                "Mao Zhang",
                "Sunhao Dai",
                "Wen Chen",
                "Wenjun Yang",
                "Yuning Jiang",
                "Zhujin Gao",
                "Bo Zheng",
                "Chi Li",
                "Dimin Wang",
                "Dixuan Wang",
                "Fan Li",
                "Fan Zhang",
                "Haibin Chen",
                "Haozhuang Liu",
                "Jialin Zhu",
                "Jiamang Wang",
                "Jiawei Wu",
                "Jin Cui",
                "Ju Huang",
                "Kai Zhang",
                "Kan Liu",
                "Lang Tian",
                "Liang Rao",
                "Longbin Li",
                "Lulu Zhao",
                "Na He",
                "Peiyang Wang",
                "Qiqi Huang",
                "Tao Luo",
                "Wenbo Su",
                "Xiaoxiao He",
                "Xin Tong",
                "Xu Chen",
                "Xunke Xi",
                "Yang Li",
                "Yaxuan Wu",
                "Yeqiu Yang",
                "Yi Hu",
                "Yinnan Song",
                "Yuchen Li",
                "Yujie Luo",
                "Yujin Yuan",
                "Yuliang Yan",
                "Zhengyang Wang",
                "Zhibo Xiao",
                "Zhixin Ma",
                "Zile Zhou",
                "Ziqi Zhang"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2507.22879.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#reasoning",
                    "#alignment",
                    "#multimodal"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "RecGPT: Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹",
                    "desc": "RecGPT - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑÑ‰Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ´Ğ»Ñ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ½Ğ° Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸ÑÑ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. ĞĞ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸ ÑƒĞ´Ğ¾Ğ²Ğ»ĞµÑ‚Ğ²Ğ¾Ñ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ²Ñ†Ğ¾Ğ² Ğ¸ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñ‹. RecGPT Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ÑƒÑ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑƒĞ¶Ğµ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ğ½ÑƒÑ‚Ğ° Ğ² Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¸ Taobao Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ¾ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ğ·Ğ°Ğ¸Ğ½Ñ‚ĞµÑ€ĞµÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½."
                },
                "en": {
                    "title": "Empowering Recommendations with User Intent",
                    "desc": "RecGPT is a new framework that enhances recommender systems by focusing on user intent rather than just historical data. It integrates large language models (LLMs) to better understand and predict user interests, which helps in retrieving more relevant items and generating clearer explanations. This approach reduces the risk of overfitting to past preferences, thereby improving content diversity and user satisfaction. By deploying RecGPT on the Taobao App, the system has shown significant performance improvements for users, merchants, and the platform itself, creating a more sustainable recommendation ecosystem."
                },
                "zh": {
                    "title": "ä»¥ç”¨æˆ·æ„å›¾ä¸ºä¸­å¿ƒçš„æ¨èç³»ç»Ÿæ–°èŒƒå¼",
                    "desc": "RecGPT æ˜¯ä¸€ç§å°†å¤§å‹è¯­è¨€æ¨¡å‹æ•´åˆåˆ°æ¨èç³»ç»Ÿä¸­çš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨å…³æ³¨ç”¨æˆ·æ„å›¾ã€‚é€šè¿‡é‡æ–°è®¾è®¡æ¨èæµç¨‹ï¼ŒRecGPT ä½¿æ¨èè¿‡ç¨‹ä»å•çº¯ä¾èµ–å†å²æ•°æ®è½¬å˜ä¸ºä»¥ç”¨æˆ·æ„å›¾ä¸ºä¸­å¿ƒã€‚è¯¥ç³»ç»Ÿé€šè¿‡å¤šé˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼Œç»“åˆæ¨ç†å¢å¼ºçš„é¢„å¯¹é½å’Œè‡ªæˆ‘è®­ç»ƒï¼Œæå‡äº†æ¨èçš„å‡†ç¡®æ€§å’Œå¤šæ ·æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRecGPT åœ¨ç”¨æˆ·æ»¡æ„åº¦ã€å•†å®¶æ›å…‰ç‡å’Œå¹³å°è½¬åŒ–ç‡ç­‰æ–¹é¢å‡å–å¾—äº†æ˜¾è‘—æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.23682",
            "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action\n  Models",
            "url": "https://huggingface.co/papers/2507.23682",
            "abstract": "The ViLLA framework enhances VLA models by incorporating latent actions, improving performance in both simulated and real-world robot manipulation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual-Language-Action (VLA) models have emerged as a popular paradigm for learning robot manipulation policies that can follow language instructions and generalize to novel scenarios. Recent work has begun to explore the incorporation of latent actions, an abstract representation of visual change between two frames, into VLA pre-training. In this paper, we introduce villa-X, a novel Visual-Language-Latent-Action (ViLLA) framework that advances latent action modeling for learning generalizable robot manipulation policies. Our approach improves both how latent actions are learned and how they are incorporated into VLA pre-training. Together, these contributions enable villa-X to achieve superior performance across simulated environments including SIMPLER and LIBERO, as well as on two real-world robot setups including gripper and dexterous hand manipulation. We believe the ViLLA paradigm holds significant promise, and that our villa-X provides a strong foundation for future research.",
            "score": 7,
            "issue_id": 5124,
            "pub_date": "2025-07-31",
            "pub_date_card": {
                "ru": "31 Ğ¸ÑĞ»Ñ",
                "en": "July 31",
                "zh": "7æœˆ31æ—¥"
            },
            "hash": "baa73e4730b01a97",
            "authors": [
                "Xiaoyu Chen",
                "Hangxing Wei",
                "Pushi Zhang",
                "Chuheng Zhang",
                "Kaixin Wang",
                "Yanjiang Guo",
                "Rushuai Yang",
                "Yucen Wang",
                "Xinquan Xiao",
                "Li Zhao",
                "Jianyu Chen",
                "Jiang Bian"
            ],
            "affiliations": [
                "Hong Kong University of Science and Technology",
                "Microsoft Research",
                "Nanjing University",
                "Tsinghua University",
                "Wuhan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.23682.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#agents",
                    "#agi",
                    "#games",
                    "#robotics",
                    "#architecture"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ViLLA: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹",
                    "desc": "Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ViLLA ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ (VLA) Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ²ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ĞºĞ°Ğº Ğ² ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ villa-X ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²ÑƒĞµÑ‚ ĞºĞ°Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼, Ñ‚Ğ°Ğº Ğ¸ Ğ¸Ñ… Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ² Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ VLA. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ°Ñ… SIMPLER Ğ¸ LIBERO, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ñ… Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ğ»Ğ¾Ğ²ĞºĞ¸Ğ¼Ğ¸ Ñ€ÑƒĞºĞ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Robot Manipulation with Latent Actions in ViLLA Framework",
                    "desc": "The ViLLA framework enhances Visual-Language-Action (VLA) models by integrating latent actions, which represent abstract visual changes between frames. This integration improves the learning of robot manipulation policies that can effectively follow language instructions and adapt to new situations. The proposed villa-X model advances the way latent actions are learned and utilized during VLA pre-training, leading to better performance in both simulated and real-world tasks. Overall, the ViLLA paradigm shows great potential for future advancements in robot manipulation research."
                },
                "zh": {
                    "title": "ViLLAæ¡†æ¶ï¼šæå‡æœºå™¨äººæ“ä½œçš„æ½œåŠ›",
                    "desc": "ViLLAæ¡†æ¶é€šè¿‡å¼•å…¥æ½œåœ¨åŠ¨ä½œæ¥å¢å¼ºè§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹ï¼Œä»è€Œæé«˜æœºå™¨äººæ“ä½œä»»åŠ¡çš„æ€§èƒ½ã€‚æ½œåœ¨åŠ¨ä½œæ˜¯ä¸€ç§æŠ½è±¡è¡¨ç¤ºï¼Œèƒ½å¤Ÿæ•æ‰ä¸¤ä¸ªå¸§ä¹‹é—´çš„è§†è§‰å˜åŒ–ã€‚æœ¬æ–‡ä»‹ç»çš„villa-Xæ˜¯ä¸€ä¸ªæ–°é¢–çš„è§†è§‰-è¯­è¨€-æ½œåœ¨åŠ¨ä½œæ¡†æ¶ï¼Œæ—¨åœ¨æ”¹è¿›æ½œåœ¨åŠ¨ä½œå»ºæ¨¡ï¼Œä»¥å­¦ä¹ å¯æ¨å¹¿çš„æœºå™¨äººæ“ä½œç­–ç•¥ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œvilla-Xåœ¨æ¨¡æ‹Ÿç¯å¢ƒå’ŒçœŸå®æœºå™¨äººè®¾ç½®ä¸­å‡è¡¨ç°å‡ºè‰²ï¼Œå±•ç¤ºäº†ViLLAèŒƒå¼çš„å·¨å¤§æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.22968",
            "title": "C3: A Bilingual Benchmark for Spoken Dialogue Models Exploring\n  Challenges in Complex Conversations",
            "url": "https://huggingface.co/papers/2507.22968",
            "abstract": "A benchmark dataset for Spoken Dialogue Models (SDMs) in English and Chinese is presented to evaluate their performance in understanding and emulating human spoken conversations, addressing challenges like ambiguity and context-dependency.  \t\t\t\t\tAI-generated summary \t\t\t\t Spoken Dialogue Models (SDMs) have recently attracted significant attention for their ability to generate voice responses directly to users' spoken queries. Despite their increasing popularity, there exists a gap in research focused on comprehensively understanding their practical effectiveness in comprehending and emulating human conversations. This is especially true compared to text-based Large Language Models (LLMs), which benefit from extensive benchmarking. Human voice interactions are inherently more complex than text due to characteristics unique to spoken dialogue. Ambiguity poses one challenge, stemming from semantic factors like polysemy, as well as phonological aspects such as heterograph, heteronyms, and stress patterns. Additionally, context-dependency, like omission, coreference, and multi-turn interaction, adds further complexity to human conversational dynamics. To illuminate the current state of SDM development and to address these challenges, we present a benchmark dataset in this paper, which comprises 1,079 instances in English and Chinese. Accompanied by an LLM-based evaluation method that closely aligns with human judgment, this dataset facilitates a comprehensive exploration of the performance of SDMs in tackling these practical challenges.",
            "score": 5,
            "issue_id": 5124,
            "pub_date": "2025-07-30",
            "pub_date_card": {
                "ru": "30 Ğ¸ÑĞ»Ñ",
                "en": "July 30",
                "zh": "7æœˆ30æ—¥"
            },
            "hash": "3a2f5273d610d5d6",
            "authors": [
                "Chengqian Ma",
                "Wei Tao",
                "Yiwen Guo"
            ],
            "affiliations": [
                "Independent Researcher",
                "LIGHTSPEED",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.22968.jpg",
            "data": {
                "categories": [
                    "#survey",
                    "#long_context",
                    "#benchmark",
                    "#dataset",
                    "#multimodal"
                ],
                "emoji": "ğŸ—£ï¸",
                "ru": {
                    "title": "Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼ Ğ¸ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞ°Ñ…. Ğ­Ñ‚Ğ¾Ñ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ñ†ĞµĞ½Ğ¸Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¸ Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ñ‹, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ Ñ‚Ğ°ĞºĞ¸Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğº Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ°Ñ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ. ĞĞ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 1079 Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸ ÑĞ¾Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ¶Ğ´Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ĞºĞ¾Ñ€Ñ€ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¾ Ğ½Ğ° ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğµ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡."
                },
                "en": {
                    "title": "Benchmarking Spoken Dialogue Models for Real-World Conversations",
                    "desc": "This paper introduces a benchmark dataset designed for evaluating Spoken Dialogue Models (SDMs) in both English and Chinese. The dataset aims to address the complexities of human spoken conversations, such as ambiguity and context-dependency, which are more pronounced in voice interactions compared to text. It includes 1,079 instances that reflect real-world dialogue scenarios, allowing for a thorough assessment of SDM performance. Additionally, the paper presents an evaluation method based on Large Language Models (LLMs) that aligns closely with human judgment, enhancing the understanding of SDMs' effectiveness."
                },
                "zh": {
                    "title": "æå‡å£è¯­å¯¹è¯æ¨¡å‹çš„è¯„ä¼°æ ‡å‡†",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç”¨äºè¯„ä¼°å£è¯­å¯¹è¯æ¨¡å‹ï¼ˆSDMsï¼‰æ€§èƒ½çš„åŸºå‡†æ•°æ®é›†ï¼Œæ¶µç›–è‹±è¯­å’Œä¸­æ–‡ï¼Œæ—¨åœ¨ç†è§£å’Œæ¨¡æ‹Ÿäººç±»å£è¯­å¯¹è¯ã€‚å£è¯­å¯¹è¯çš„å¤æ‚æ€§ä½“ç°åœ¨æ­§ä¹‰æ€§å’Œä¸Šä¸‹æ–‡ä¾èµ–æ€§ç­‰æŒ‘æˆ˜ä¸Šï¼Œè¿™äº›å› ç´ ä½¿å¾—ä¸æ–‡æœ¬åŸºç¡€çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç›¸æ¯”ï¼ŒSDMsçš„ç ”ç©¶ç›¸å¯¹è¾ƒå°‘ã€‚æ•°æ®é›†ä¸­åŒ…å«1079ä¸ªå®ä¾‹ï¼Œå¹¶é…å¤‡äº†ä¸€ç§åŸºäºLLMçš„è¯„ä¼°æ–¹æ³•ï¼Œä»¥æ›´å¥½åœ°ä¸äººç±»åˆ¤æ–­ç›¸ä¸€è‡´ã€‚é€šè¿‡è¿™ä¸ªæ•°æ®é›†ï¼Œç ”ç©¶è€…å¯ä»¥å…¨é¢æ¢è®¨SDMsåœ¨åº”å¯¹å®é™…å¯¹è¯æŒ‘æˆ˜ä¸­çš„è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.23632",
            "title": "On the Expressiveness of Softmax Attention: A Recurrent Neural Network\n  Perspective",
            "url": "https://huggingface.co/papers/2507.23632",
            "abstract": "Softmax attention is more expressive than linear attention due to its recurrent form, which can be analyzed using RNN components.  \t\t\t\t\tAI-generated summary \t\t\t\t Since its introduction, softmax attention has become the backbone of modern transformer architectures due to its expressiveness and scalability across a wide range of tasks. However, the main drawback of softmax attention is the quadratic memory requirement and computational complexity with respect to the sequence length. By replacing the softmax nonlinearity, linear attention and similar methods have been introduced to avoid the quadratic bottleneck of softmax attention. Despite these linear forms of attention being derived from the original softmax formulation, they typically lag in terms of downstream accuracy. While strong intuition of the softmax nonlinearity on the query and key inner product suggests that it has desirable properties compared to other nonlinearities, the question of why this discrepancy exists still remains unanswered. This work demonstrates that linear attention is an approximation of softmax attention by deriving the recurrent form of softmax attention. Using this form, each part of softmax attention can be described in the language of recurrent neural networks (RNNs). Describing softmax attention as an RNN allows for the ablation of the components of softmax attention to understand the importance of each part and how they interact. In this way, our work helps explain why softmax attention is more expressive than its counterparts.",
            "score": 0,
            "issue_id": 5124,
            "pub_date": "2025-07-31",
            "pub_date_card": {
                "ru": "31 Ğ¸ÑĞ»Ñ",
                "en": "July 31",
                "zh": "7æœˆ31æ—¥"
            },
            "hash": "b07ddf6cb6b8bee8",
            "authors": [
                "Gabriel Mongaras",
                "Eric C. Larson"
            ],
            "affiliations": [
                "Lyle School of Engineering Southern Methodist University Dallas, TX 75205"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.23632.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#interpretability"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°Ñ ÑĞ¸Ğ»Ñƒ ÑĞ¾Ñ„Ñ‚Ğ¼Ğ°ĞºÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ğ·Ğ¼Ñƒ RNN",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¾Ñ„Ñ‚Ğ¼Ğ°ĞºÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ñ„Ñ‚Ğ¼Ğ°ĞºÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ² Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğµ, Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¼ ÑĞµÑ‚ÑĞ¼ (RNN). Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²ĞµÑÑ‚Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ² ÑĞ¾Ñ„Ñ‚Ğ¼Ğ°ĞºÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±ÑŠÑÑĞ½Ğ¸Ñ‚ÑŒ ĞµĞ³Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ°Ğ¼Ğ¸. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ½ÑÑ‚ÑŒ, Ğ¿Ğ¾Ñ‡ĞµĞ¼Ñƒ ÑĞ¾Ñ„Ñ‚Ğ¼Ğ°ĞºÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¾ÑÑ‚Ğ°ĞµÑ‚ÑÑ Ğ¾ÑĞ½Ğ¾Ğ²Ğ¾Ğ¹ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€, Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½ÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "Unlocking the Power of Softmax Attention",
                    "desc": "This paper explores the differences between softmax attention and linear attention in machine learning models, particularly in transformers. It shows that softmax attention, which is more expressive, can be understood through the lens of recurrent neural networks (RNNs). By analyzing softmax attention as an RNN, the authors can break down its components to see how they contribute to its performance. The findings clarify why softmax attention outperforms linear attention in terms of accuracy despite the latter's computational efficiency."
                },
                "zh": {
                    "title": "è½¯maxæ³¨æ„åŠ›çš„ä¼˜åŠ¿è§£æ",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†softmaxæ³¨æ„åŠ›ä¸çº¿æ€§æ³¨æ„åŠ›çš„åŒºåˆ«ã€‚softmaxæ³¨æ„åŠ›å› å…¶è¡¨è¾¾èƒ½åŠ›å¼ºè€Œæˆä¸ºç°ä»£å˜æ¢å™¨æ¶æ„çš„åŸºç¡€ï¼Œä½†å…¶åœ¨åºåˆ—é•¿åº¦ä¸Šçš„è®¡ç®—å¤æ‚åº¦å’Œå†…å­˜éœ€æ±‚æ˜¯ä¸€ä¸ªä¸»è¦ç¼ºç‚¹ã€‚é€šè¿‡å°†softmaxéçº¿æ€§æ›¿æ¢ä¸ºçº¿æ€§æ³¨æ„åŠ›ï¼Œç ”ç©¶è€…ä»¬è¯•å›¾è§£å†³è¿™ä¸€ç“¶é¢ˆã€‚æœ¬æ–‡è¡¨æ˜ï¼Œçº¿æ€§æ³¨æ„åŠ›å®é™…ä¸Šæ˜¯softmaxæ³¨æ„åŠ›çš„ä¸€ç§è¿‘ä¼¼ï¼Œå¹¶é€šè¿‡é€’å½’ç¥ç»ç½‘ç»œçš„è¯­è¨€æ¥æè¿°softmaxæ³¨æ„åŠ›çš„å„ä¸ªéƒ¨åˆ†ï¼Œä»è€Œæ­ç¤ºå…¶æ›´å¼ºè¡¨è¾¾èƒ½åŠ›çš„åŸå› ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-07-31.html",
    "link_next": "2025-08-04.html",
    "link_month": "2025-08.html",
    "short_date_prev": {
        "ru": "31.07",
        "en": "07/31",
        "zh": "7æœˆ31æ—¥"
    },
    "short_date_next": {
        "ru": "04.08",
        "en": "08/04",
        "zh": "8æœˆ4æ—¥"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 1,
        "#agents": 1,
        "#cv": 0,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 2,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 1,
        "#agi": 1,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 4,
        "#survey": 1,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}