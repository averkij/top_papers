{
    "date": {
        "ru": "24 апреля",
        "en": "April 24",
        "zh": "4月24日"
    },
    "time_utc": "2025-04-24 19:08",
    "weekday": 3,
    "issue_id": 3420,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.15279",
            "title": "VisuLogic: A Benchmark for Evaluating Visual Reasoning in Multi-modal\n  Large Language Models",
            "url": "https://huggingface.co/papers/2504.15279",
            "abstract": "Visual reasoning is a core component of human intelligence and a critical capability for advanced multimodal models. Yet current reasoning evaluations of multimodal large language models (MLLMs) often rely on text descriptions and allow language-based reasoning shortcuts, failing to measure genuine vision-centric reasoning. To address this, we introduce VisuLogic: a benchmark of 1,000 human-verified problems across six categories (e.g., quantitative shifts, spatial relations, attribute comparisons). These various types of questions can be evaluated to assess the visual reasoning capabilities of MLLMs from multiple perspectives. We evaluate leading MLLMs on this benchmark and analyze their results to identify common failure modes. Most models score below 30% accuracy-only slightly above the 25% random baseline and far below the 51.4% achieved by humans-revealing significant gaps in visual reasoning. Furthermore, we provide a supplementary training dataset and a reinforcement-learning baseline to support further progress.",
            "score": 54,
            "issue_id": 3404,
            "pub_date": "2025-04-21",
            "pub_date_card": {
                "ru": "21 апреля",
                "en": "April 21",
                "zh": "4月21日"
            },
            "hash": "5ec39c2a399dba2b",
            "authors": [
                "Weiye Xu",
                "Jiahao Wang",
                "Weiyun Wang",
                "Zhe Chen",
                "Wengang Zhou",
                "Aijun Yang",
                "Lewei Lu",
                "Houqiang Li",
                "Xiaohua Wang",
                "Xizhou Zhu",
                "Wenhai Wang",
                "Jifeng Dai",
                "Jinguo Zhu"
            ],
            "affiliations": [
                "SenseTime Research",
                "Shanghai Artificial Intelligence Laboratory",
                "Tsinghua University",
                "University of Science and Technology of China",
                "Xian Jiaotong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.15279.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#rl",
                    "#multimodal",
                    "#dataset"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "VisuLogic: преодолевая разрыв в визуальном мышлении ИИ",
                    "desc": "Статья представляет VisuLogic - новый бенчмарк для оценки визуального мышления мультимодальных больших языковых моделей (MLLM). Бенчмарк состоит из 1000 проверенных человеком задач в шести категориях, включая количественные изменения и пространственные отношения. Тестирование ведущих MLLM на этом бенчмарке показало, что большинство моделей набирают менее 30% точности, что значительно ниже человеческого результата в 51.4%. Авторы также предоставляют дополнительный набор данных для обучения и базовую модель с обучением с подкреплением для дальнейших исследований."
                },
                "en": {
                    "title": "Enhancing Visual Reasoning in Multimodal Models with VisuLogic",
                    "desc": "This paper introduces VisuLogic, a new benchmark designed to evaluate the visual reasoning abilities of multimodal large language models (MLLMs). Unlike previous evaluations that relied on text descriptions, VisuLogic presents 1,000 human-verified problems across six categories, focusing on genuine vision-centric reasoning. The results show that leading MLLMs struggle with these tasks, scoring below 30% accuracy, which is only slightly better than random guessing and significantly lower than human performance. To aid in improving these models, the authors also provide a supplementary training dataset and a reinforcement-learning baseline."
                },
                "zh": {
                    "title": "提升视觉推理能力的基准评估",
                    "desc": "视觉推理是人类智能的核心组成部分，也是高级多模态模型的重要能力。然而，目前对多模态大型语言模型（MLLMs）的推理评估往往依赖文本描述，允许语言基础的推理捷径，未能真实测量视觉中心的推理能力。为了解决这个问题，我们引入了VisuLogic：一个包含1000个经过人工验证的问题的基准，涵盖六个类别（例如，定量变化、空间关系、属性比较）。我们对领先的MLLMs在这个基准上的表现进行了评估，结果显示大多数模型的准确率低于30%，远低于人类的51.4%，揭示了视觉推理方面的显著差距。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.14509",
            "title": "DreamID: High-Fidelity and Fast diffusion-based Face Swapping via\n  Triplet ID Group Learning",
            "url": "https://huggingface.co/papers/2504.14509",
            "abstract": "In this paper, we introduce DreamID, a diffusion-based face swapping model that achieves high levels of ID similarity, attribute preservation, image fidelity, and fast inference speed. Unlike the typical face swapping training process, which often relies on implicit supervision and struggles to achieve satisfactory results. DreamID establishes explicit supervision for face swapping by constructing Triplet ID Group data, significantly enhancing identity similarity and attribute preservation. The iterative nature of diffusion models poses challenges for utilizing efficient image-space loss functions, as performing time-consuming multi-step sampling to obtain the generated image during training is impractical. To address this issue, we leverage the accelerated diffusion model SD Turbo, reducing the inference steps to a single iteration, enabling efficient pixel-level end-to-end training with explicit Triplet ID Group supervision. Additionally, we propose an improved diffusion-based model architecture comprising SwapNet, FaceNet, and ID Adapter. This robust architecture fully unlocks the power of the Triplet ID Group explicit supervision. Finally, to further extend our method, we explicitly modify the Triplet ID Group data during training to fine-tune and preserve specific attributes, such as glasses and face shape. Extensive experiments demonstrate that DreamID outperforms state-of-the-art methods in terms of identity similarity, pose and expression preservation, and image fidelity. Overall, DreamID achieves high-quality face swapping results at 512*512 resolution in just 0.6 seconds and performs exceptionally well in challenging scenarios such as complex lighting, large angles, and occlusions.",
            "score": 34,
            "issue_id": 3408,
            "pub_date": "2025-04-20",
            "pub_date_card": {
                "ru": "20 апреля",
                "en": "April 20",
                "zh": "4月20日"
            },
            "hash": "ed89c8434e937bfd",
            "authors": [
                "Fulong Ye",
                "Miao Hua",
                "Pengze Zhang",
                "Xinghui Li",
                "Qichao Sun",
                "Songtao Zhao",
                "Qian He",
                "Xinglong Wu"
            ],
            "affiliations": [
                "Intelligent Creation Team, ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.14509.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#diffusion",
                    "#cv",
                    "#architecture"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "DreamID: Быстрый и качественный обмен лиц с помощью диффузионных моделей",
                    "desc": "DreamID - это модель обмена лиц на основе диффузии, которая достигает высокого уровня сходства ID, сохранения атрибутов и качества изображения. Она использует явное обучение с помощью данных Triplet ID Group и ускоренную модель диффузии SD Turbo для эффективного обучения. Архитектура DreamID включает SwapNet, FaceNet и ID Adapter, что позволяет полностью раскрыть потенциал явного обучения. Эксперименты показывают, что DreamID превосходит современные методы по сходству личности, сохранению позы и выражения, а также качеству изображения."
                },
                "en": {
                    "title": "DreamID: Fast and Accurate Face Swapping with Explicit Supervision",
                    "desc": "DreamID is a novel face swapping model that utilizes diffusion techniques to enhance identity similarity and attribute preservation while maintaining high image quality and fast processing times. It introduces explicit supervision through Triplet ID Group data, which significantly improves the results compared to traditional methods that rely on implicit supervision. The model employs an accelerated diffusion architecture, allowing for efficient training and inference by reducing the number of sampling steps required. Extensive testing shows that DreamID excels in maintaining facial features and quality even in difficult conditions, achieving impressive results in just 0.6 seconds."
                },
                "zh": {
                    "title": "DreamID：高效的人脸交换新方法",
                    "desc": "本文介绍了一种名为DreamID的基于扩散模型的人脸交换技术，该模型在身份相似性、属性保留、图像保真度和推理速度方面表现出色。与传统的人脸交换训练过程不同，DreamID通过构建三元组ID组数据，建立了显式监督，从而显著提高了身份相似性和属性保留。为了克服扩散模型在训练中多步采样的效率问题，我们采用了加速扩散模型SD Turbo，将推理步骤减少到单次迭代，实现了高效的像素级端到端训练。实验结果表明，DreamID在身份相似性、姿态和表情保留以及图像保真度方面均优于现有的最先进方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.15431",
            "title": "Trillion 7B Technical Report",
            "url": "https://huggingface.co/papers/2504.15431",
            "abstract": "We introduce Trillion-7B, the most token-efficient Korean-centric multilingual LLM available. Our novel Cross-lingual Document Attention (XLDA) mechanism enables highly efficient and effective knowledge transfer from English to target languages like Korean and Japanese. Combined with optimized data mixtures, language-specific filtering, and tailored tokenizer construction, Trillion-7B achieves competitive performance while dedicating only 10\\% of its 2T training tokens to multilingual data and requiring just 59.4K H100 GPU hours (\\$148K) for full training. Comprehensive evaluations across 27 benchmarks in four languages demonstrate Trillion-7B's robust multilingual performance and exceptional cross-lingual consistency.",
            "score": 25,
            "issue_id": 3404,
            "pub_date": "2025-04-21",
            "pub_date_card": {
                "ru": "21 апреля",
                "en": "April 21",
                "zh": "4月21日"
            },
            "hash": "0d2a201b09695822",
            "authors": [
                "Sungjun Han",
                "Juyoung Suk",
                "Suyeong An",
                "Hyungguk Kim",
                "Kyuseok Kim",
                "Wonsuk Yang",
                "Seungtaek Choi",
                "Jamin Shin"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2504.15431.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multilingual",
                    "#low_resource",
                    "#training",
                    "#transfer_learning",
                    "#architecture",
                    "#data"
                ],
                "emoji": "🌏",
                "ru": {
                    "title": "Эффективный перенос знаний между языками в компактной мультиязычной модели",
                    "desc": "Trillion-7B - это мультиязычная языковая модель с акцентом на корейский язык, отличающаяся высокой эффективностью использования токенов. Модель использует новый механизм межъязыкового внимания к документам (XLDA) для эффективного переноса знаний с английского на целевые языки. Благодаря оптимизированным наборам данных и специальной фильтрации, Trillion-7B достигает конкурентоспособной производительности, используя лишь 10% мультиязычных данных при обучении. Модель демонстрирует надежную многоязычную производительность и исключительную межъязыковую согласованность по результатам тестирования на 27 бенчмарках на четырех языках."
                },
                "en": {
                    "title": "Efficient Multilingual Mastery with Trillion-7B",
                    "desc": "Trillion-7B is a multilingual language model designed specifically for Korean and other languages. It utilizes a new technique called Cross-lingual Document Attention (XLDA) to efficiently transfer knowledge from English to languages like Korean and Japanese. The model is trained with a small portion of multilingual data, only 10% of its total training tokens, and is optimized for performance with specific data mixtures and tokenizer adjustments. Evaluations show that Trillion-7B performs well across multiple languages and maintains strong consistency in cross-lingual tasks."
                },
                "zh": {
                    "title": "高效的多语言大模型 Trillion-7B",
                    "desc": "Trillion-7B 是一种高效的多语言大模型，专注于韩语等语言的处理。它采用了新颖的跨语言文档注意力机制（XLDA），能够有效地将知识从英语转移到目标语言，如韩语和日语。通过优化数据混合、语言特定过滤和定制的分词器构建，Trillion-7B 在仅使用 10% 的多语言数据的情况下，仍能实现竞争力的性能。经过对 27 个基准测试的全面评估，Trillion-7B 展现了强大的多语言性能和卓越的跨语言一致性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.16929",
            "title": "I-Con: A Unifying Framework for Representation Learning",
            "url": "https://huggingface.co/papers/2504.16929",
            "abstract": "As the field of representation learning grows, there has been a proliferation of different loss functions to solve different classes of problems. We introduce a single information-theoretic equation that generalizes a large collection of modern loss functions in machine learning. In particular, we introduce a framework that shows that several broad classes of machine learning methods are precisely minimizing an integrated KL divergence between two conditional distributions: the supervisory and learned representations. This viewpoint exposes a hidden information geometry underlying clustering, spectral methods, dimensionality reduction, contrastive learning, and supervised learning. This framework enables the development of new loss functions by combining successful techniques from across the literature. We not only present a wide array of proofs, connecting over 23 different approaches, but we also leverage these theoretical results to create state-of-the-art unsupervised image classifiers that achieve a +8% improvement over the prior state-of-the-art on unsupervised classification on ImageNet-1K. We also demonstrate that I-Con can be used to derive principled debiasing methods which improve contrastive representation learners.",
            "score": 21,
            "issue_id": 3406,
            "pub_date": "2025-04-23",
            "pub_date_card": {
                "ru": "23 апреля",
                "en": "April 23",
                "zh": "4月23日"
            },
            "hash": "0896fcd8d4d48d98",
            "authors": [
                "Shaden Alshammari",
                "John Hershey",
                "Axel Feldmann",
                "William T. Freeman",
                "Mark Hamilton"
            ],
            "affiliations": [
                "Google",
                "MIT",
                "Microsoft"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.16929.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#math",
                    "#interpretability",
                    "#cv",
                    "#optimization"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Единая теория функций потерь в машинном обучении",
                    "desc": "Статья представляет единое информационно-теоретическое уравнение, обобщающее множество современных функций потерь в машинном обучении. Авторы показывают, что многие методы машинного обучения минимизируют интегрированную KL-дивергенцию между условными распределениями. Этот подход раскрывает скрытую информационную геометрию, лежащую в основе кластеризации, спектральных методов, снижения размерности, контрастного и supervised обучения. Применение этой теории позволило создать улучшенные алгоритмы несупервизорной классификации изображений, превосходящие предыдущие state-of-the-art результаты на ImageNet-1K."
                },
                "en": {
                    "title": "Unifying Loss Functions through Information Geometry",
                    "desc": "This paper presents a unified information-theoretic framework that generalizes various loss functions used in machine learning. It shows that many methods minimize an integrated KL divergence between supervisory and learned representations, revealing a deeper information geometry in techniques like clustering and dimensionality reduction. The authors provide proofs connecting over 23 different approaches and use these insights to develop advanced unsupervised image classifiers, achieving significant performance improvements. Additionally, the framework aids in creating effective debiasing methods for contrastive representation learning."
                },
                "zh": {
                    "title": "统一损失函数，提升表示学习的力量",
                    "desc": "随着表示学习领域的发展，出现了许多不同的损失函数来解决各种问题。我们提出了一个信息论方程，它可以概括现代机器学习中大量的损失函数。特别地，我们展示了几种广泛的机器学习方法实际上是在最小化两个条件分布之间的集成KL散度：监督分布和学习到的表示。这个框架不仅连接了23种不同的方法，还帮助我们开发了新的无监督图像分类器，显著提高了分类性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.15843",
            "title": "Pre-DPO: Improving Data Utilization in Direct Preference Optimization\n  Using a Guiding Reference Model",
            "url": "https://huggingface.co/papers/2504.15843",
            "abstract": "Direct Preference Optimization (DPO) simplifies reinforcement learning from human feedback (RLHF) for large language models (LLMs) by directly optimizing human preferences without an explicit reward model. We find that during DPO training, the reference model plays the role of a data weight adjuster. However, the common practice of initializing the policy and reference models identically in DPO can lead to inefficient data utilization and impose a performance ceiling. Meanwhile, the lack of a reference model in Simple Preference Optimization (SimPO) reduces training robustness and necessitates stricter conditions to prevent catastrophic forgetting. In this work, we propose Pre-DPO, a simple yet effective DPO-based training paradigm that enhances preference optimization performance by leveraging a guiding reference model. This reference model provides foresight into the optimal policy state achievable through the training preference data, serving as a guiding mechanism that adaptively assigns higher weights to samples more suitable for the model and lower weights to those less suitable. Extensive experiments on AlpacaEval 2.0 and Arena-Hard v0.1 benchmarks demonstrate that Pre-DPO consistently improves the performance of both DPO and SimPO, without relying on external models or additional data.",
            "score": 16,
            "issue_id": 3403,
            "pub_date": "2025-04-22",
            "pub_date_card": {
                "ru": "22 апреля",
                "en": "April 22",
                "zh": "4月22日"
            },
            "hash": "e8cb4456a20efe2a",
            "authors": [
                "Junshu Pan",
                "Wei Shen",
                "Shulin Huang",
                "Qiji Zhou",
                "Yue Zhang"
            ],
            "affiliations": [
                "Independent Researcher",
                "School of Engineering, Westlake University",
                "Shanghai Innovation Institute",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.15843.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#rlhf",
                    "#alignment",
                    "#benchmark"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Pre-DPO: Эффективная оптимизация языковых моделей с учетом предпочтений",
                    "desc": "Эта статья представляет Pre-DPO - новый подход к обучению языковых моделей на основе предпочтений человека. Pre-DPO улучшает существующие методы DPO и SimPO, используя ориентирующую эталонную модель для оптимизации весов данных. Этот метод позволяет более эффективно использовать обучающие данные и достигать лучших результатов. Эксперименты на бенчмарках AlpacaEval 2.0 и Arena-Hard v0.1 показывают преимущества Pre-DPO без необходимости во внешних моделях или дополнительных данных."
                },
                "en": {
                    "title": "Enhancing Preference Learning with Pre-DPO",
                    "desc": "This paper introduces Pre-DPO, a new training method for large language models that enhances Direct Preference Optimization (DPO) by using a guiding reference model. The reference model helps adjust the importance of training data, allowing the model to focus on more relevant samples and improve learning efficiency. The authors highlight that traditional methods can lead to poor performance due to identical initialization of models and lack of robustness in simpler approaches. Through experiments, they show that Pre-DPO outperforms existing methods without needing extra data or external models."
                },
                "zh": {
                    "title": "提升偏好优化性能的Pre-DPO方法",
                    "desc": "直接偏好优化（DPO）通过直接优化人类偏好，简化了大型语言模型（LLM）的强化学习过程。研究发现，在DPO训练中，参考模型充当数据权重调整器，但常见的将策略模型和参考模型初始化为相同的做法可能导致数据利用效率低下。我们提出的Pre-DPO训练范式，通过利用指导性参考模型，增强了偏好优化的性能，能够自适应地为更适合模型的样本分配更高的权重。实验结果表明，Pre-DPO在多个基准测试中持续提升了DPO和SimPO的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.15777",
            "title": "Tina: Tiny Reasoning Models via LoRA",
            "url": "https://huggingface.co/papers/2504.15777",
            "abstract": "How cost-effectively can strong reasoning abilities be achieved in language models? Driven by this fundamental question, we present Tina, a family of tiny reasoning models achieved with high cost-efficiency. Notably, Tina demonstrates that substantial reasoning performance can be developed using only minimal resources, by applying parameter-efficient updates during reinforcement learning (RL), using low-rank adaptation (LoRA), to an already tiny 1.5B parameter base model. This minimalist approach produces models that achieve reasoning performance which is competitive with, and sometimes surpasses, SOTA RL reasoning models built upon the same base model. Crucially, this is achieved at a tiny fraction of the computational post-training cost employed by existing SOTA models. In fact, the best Tina model achieves a >20\\% reasoning performance increase and 43.33\\% Pass@1 accuracy on AIME24, at only \\$9 USD post-training and evaluation cost (i.e., an estimated 260x cost reduction). Our work reveals the surprising effectiveness of efficient RL reasoning via LoRA. We validate this across multiple open-source reasoning datasets and various ablation settings starting with a single, fixed set of hyperparameters. Furthermore, we hypothesize that this effectiveness and efficiency stem from LoRA rapidly adapting the model to the structural format of reasoning rewarded by RL, while largely preserving the base model's underlying knowledge. In service of accessibility and open research, we fully open-source all code, training logs, and model weights \\& checkpoints.",
            "score": 15,
            "issue_id": 3410,
            "pub_date": "2025-04-22",
            "pub_date_card": {
                "ru": "22 апреля",
                "en": "April 22",
                "zh": "4月22日"
            },
            "hash": "684e9f5f755cf0c6",
            "authors": [
                "Shangshang Wang",
                "Julian Asilis",
                "Ömer Faruk Akgül",
                "Enes Burak Bilgin",
                "Ollie Liu",
                "Willie Neiswanger"
            ],
            "affiliations": [
                "University of Southern California"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.15777.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#training",
                    "#optimization",
                    "#rl",
                    "#open_source",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Эффективное обучение рассуждениям с минимальными ресурсами",
                    "desc": "Статья представляет семейство моделей Tina, демонстрирующих сильные способности к рассуждениям при минимальных вычислительных затратах. Используя параметрически-эффективные обновления во время обучения с подкреплением (RL) и низкоранговую адаптацию (LoRA), авторы достигают производительности, конкурирующей с современными моделями рассуждений. Лучшая модель Tina показывает более чем 20% увеличение производительности рассуждений при затратах всего $9 на дообучение и оценку. Исследователи предполагают, что эффективность связана с быстрой адаптацией модели к структурному формату рассуждений, вознаграждаемому RL, при сохранении базовых знаний модели."
                },
                "en": {
                    "title": "Tiny Models, Big Reasoning: Cost-Effective AI with Tina",
                    "desc": "This paper introduces Tina, a series of compact reasoning models that achieve strong reasoning capabilities with minimal resources. By utilizing low-rank adaptation (LoRA) during reinforcement learning (RL), Tina enhances a small 1.5B parameter base model, demonstrating that effective reasoning can be achieved cost-effectively. The models not only compete with but sometimes outperform state-of-the-art (SOTA) RL reasoning models, all while significantly reducing post-training costs. The findings suggest that LoRA effectively tailors the model for reasoning tasks, maintaining the base model's knowledge while improving performance."
                },
                "zh": {
                    "title": "高效推理，低成本实现",
                    "desc": "本文介绍了一种名为Tina的小型推理模型系列，旨在以高性价比实现强大的推理能力。Tina通过在一个仅有1.5亿参数的基础模型上，采用低秩适应（LoRA）和强化学习（RL）进行参数高效更新，展示了在资源有限的情况下仍能获得显著的推理性能。与现有的最先进（SOTA）模型相比，Tina在推理性能上具有竞争力，甚至在某些情况下超越了这些模型，同时其后期训练和评估成本仅为9美元，节省了约260倍的成本。我们的研究表明，LoRA在快速适应推理结构的同时，能够有效保留基础模型的知识，从而实现高效的推理能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.16801",
            "title": "Decoupled Global-Local Alignment for Improving Compositional\n  Understanding",
            "url": "https://huggingface.co/papers/2504.16801",
            "abstract": "Contrastive Language-Image Pre-training (CLIP) has achieved success on multiple downstream tasks by aligning image and text modalities. However, the nature of global contrastive learning limits CLIP's ability to comprehend compositional concepts, such as relations and attributes. Although recent studies employ global hard negative samples to improve compositional understanding, these methods significantly compromise the model's inherent general capabilities by forcibly distancing textual negative samples from images in the embedding space. To overcome this limitation, we introduce a Decoupled Global-Local Alignment (DeGLA) framework that improves compositional understanding while substantially mitigating losses in general capabilities. To optimize the retention of the model's inherent capabilities, we incorporate a self-distillation mechanism within the global alignment process, aligning the learnable image-text encoder with a frozen teacher model derived from an exponential moving average. Under the constraint of self-distillation, it effectively mitigates the catastrophic forgetting of pretrained knowledge during fine-tuning. To improve compositional understanding, we first leverage the in-context learning capability of Large Language Models (LLMs) to construct about 2M high-quality negative captions across five types. Subsequently, we propose the Image-Grounded Contrast (IGC) loss and Text-Grounded Contrast (TGC) loss to enhance vision-language compositionally. Extensive experimental results demonstrate the effectiveness of the DeGLA framework. Compared to previous state-of-the-art methods, DeGLA achieves an average enhancement of 3.5% across the VALSE, SugarCrepe, and ARO benchmarks. Concurrently, it obtains an average performance improvement of 13.0% on zero-shot classification tasks across eleven datasets. Our code will be released at https://github.com/xiaoxing2001/DeGLA",
            "score": 14,
            "issue_id": 3406,
            "pub_date": "2025-04-23",
            "pub_date_card": {
                "ru": "23 апреля",
                "en": "April 23",
                "zh": "4月23日"
            },
            "hash": "c50b5cc723595a69",
            "authors": [
                "Xiaoxing Hu",
                "Kaicheng Yang",
                "Jun Wang",
                "Haoran Xu",
                "Ziyong Feng",
                "Yupei Wang"
            ],
            "affiliations": [
                "Beijing Institute of Technology, Beijing, China",
                "DeepGlint, Beijing, China",
                "Zhejiang University, Zhejiang Province, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.16801.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#training",
                    "#alignment",
                    "#multimodal",
                    "#optimization"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "DeGLA: Улучшение композиционного понимания без потери общих возможностей",
                    "desc": "Данная статья представляет новый подход под названием DeGLA (Decoupled Global-Local Alignment) для улучшения понимания композиционных концепций в моделях машинного обучения типа CLIP. Метод использует самодистилляцию для сохранения общих возможностей модели и вводит новые функции потерь IGC и TGC для улучшения композиционного понимания. Авторы применяют обучение в контексте больших языковых моделей для создания высококачественных негативных примеров. Эксперименты показывают значительное улучшение производительности на нескольких бенчмарках по сравнению с предыдущими методами."
                },
                "en": {
                    "title": "Enhancing Compositional Understanding in Vision-Language Models with DeGLA",
                    "desc": "The paper introduces a new framework called Decoupled Global-Local Alignment (DeGLA) to enhance the compositional understanding of images and text in machine learning models. It addresses the limitations of existing methods that use global contrastive learning, which can hinder a model's general capabilities. By incorporating a self-distillation mechanism, DeGLA retains the model's pretrained knowledge while improving its ability to understand complex relationships and attributes in data. The framework also utilizes advanced techniques to generate high-quality negative samples, leading to significant performance improvements on various benchmarks."
                },
                "zh": {
                    "title": "去耦合全局-局部对齐提升组合理解能力",
                    "desc": "对比语言-图像预训练（CLIP）在多个下游任务中取得了成功，但其全局对比学习的特性限制了其理解组合概念的能力。为了解决这个问题，我们提出了去耦合的全局-局部对齐（DeGLA）框架，旨在提高组合理解能力，同时减少对模型一般能力的损失。我们在全局对齐过程中引入自蒸馏机制，以优化模型固有能力的保留，并有效减轻在微调过程中预训练知识的灾难性遗忘。通过构建高质量的负样本和引入新的损失函数，DeGLA在多个基准测试中表现出色，显著提升了模型的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.16915",
            "title": "DreamO: A Unified Framework for Image Customization",
            "url": "https://huggingface.co/papers/2504.16915",
            "abstract": "Recently, extensive research on image customization (e.g., identity, subject, style, background, etc.) demonstrates strong customization capabilities in large-scale generative models. However, most approaches are designed for specific tasks, restricting their generalizability to combine different types of condition. Developing a unified framework for image customization remains an open challenge. In this paper, we present DreamO, an image customization framework designed to support a wide range of tasks while facilitating seamless integration of multiple conditions. Specifically, DreamO utilizes a diffusion transformer (DiT) framework to uniformly process input of different types. During training, we construct a large-scale training dataset that includes various customization tasks, and we introduce a feature routing constraint to facilitate the precise querying of relevant information from reference images. Additionally, we design a placeholder strategy that associates specific placeholders with conditions at particular positions, enabling control over the placement of conditions in the generated results. Moreover, we employ a progressive training strategy consisting of three stages: an initial stage focused on simple tasks with limited data to establish baseline consistency, a full-scale training stage to comprehensively enhance the customization capabilities, and a final quality alignment stage to correct quality biases introduced by low-quality data. Extensive experiments demonstrate that the proposed DreamO can effectively perform various image customization tasks with high quality and flexibly integrate different types of control conditions.",
            "score": 13,
            "issue_id": 3405,
            "pub_date": "2025-04-23",
            "pub_date_card": {
                "ru": "23 апреля",
                "en": "April 23",
                "zh": "4月23日"
            },
            "hash": "86de2482f329ee9f",
            "authors": [
                "Chong Mou",
                "Yanze Wu",
                "Wenxu Wu",
                "Zinan Guo",
                "Pengze Zhang",
                "Yufeng Cheng",
                "Yiming Luo",
                "Fei Ding",
                "Shiwen Zhang",
                "Xinghui Li",
                "Mengtian Li",
                "Songtao Zhao",
                "Jian Zhang",
                "Qian He",
                "Xinglong Wu"
            ],
            "affiliations": [
                "Intelligent Creation Team, ByteDance",
                "School of Electronic and Computer Engineering, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.16915.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#cv",
                    "#dataset",
                    "#optimization",
                    "#training"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "DreamO: универсальная кастомизация изображений с помощью диффузионных трансформеров",
                    "desc": "DreamO - это унифицированная система для кастомизации изображений, использующая архитектуру диффузионного трансформера. Она позволяет комбинировать различные типы условий и контролировать их размещение в генерируемых результатах. Обучение проводится в три этапа на большом наборе данных с разнообразными задачами кастомизации. Эксперименты показывают, что DreamO эффективно выполняет различные задачи по настройке изображений с высоким качеством."
                },
                "en": {
                    "title": "DreamO: Unifying Image Customization with Flexibility and Precision",
                    "desc": "This paper introduces DreamO, a novel framework for image customization that allows for the integration of multiple conditions such as identity, style, and background. Unlike previous models that are limited to specific tasks, DreamO employs a diffusion transformer (DiT) to uniformly handle diverse input types. The framework is trained on a large dataset with a feature routing constraint to ensure accurate information retrieval from reference images. Additionally, a progressive training strategy is implemented to enhance customization capabilities and improve output quality, demonstrating DreamO's effectiveness across various image customization tasks."
                },
                "zh": {
                    "title": "DreamO：多任务图像定制的统一框架",
                    "desc": "最近的研究表明，大规模生成模型在图像定制方面具有强大的能力，但大多数方法仅针对特定任务，限制了其通用性。本文提出了DreamO，一个支持多种任务的图像定制框架，能够无缝整合多种条件。DreamO采用扩散变换器（DiT）框架，统一处理不同类型的输入，并通过特征路由约束精确查询参考图像中的相关信息。实验表明，DreamO能够高质量地执行各种图像定制任务，并灵活整合不同类型的控制条件。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.16074",
            "title": "PHYBench: Holistic Evaluation of Physical Perception and Reasoning in\n  Large Language Models",
            "url": "https://huggingface.co/papers/2504.16074",
            "abstract": "We introduce PHYBench, a novel, high-quality benchmark designed for evaluating reasoning capabilities of large language models (LLMs) in physical contexts. PHYBench consists of 500 meticulously curated physics problems based on real-world physical scenarios, designed to assess the ability of models to understand and reason about realistic physical processes. Covering mechanics, electromagnetism, thermodynamics, optics, modern physics, and advanced physics, the benchmark spans difficulty levels from high school exercises to undergraduate problems and Physics Olympiad challenges. Additionally, we propose the Expression Edit Distance (EED) Score, a novel evaluation metric based on the edit distance between mathematical expressions, which effectively captures differences in model reasoning processes and results beyond traditional binary scoring methods. We evaluate various LLMs on PHYBench and compare their performance with human experts. Our results reveal that even state-of-the-art reasoning models significantly lag behind human experts, highlighting their limitations and the need for improvement in complex physical reasoning scenarios. Our benchmark results and dataset are publicly available at https://phybench-official.github.io/phybench-demo/.",
            "score": 13,
            "issue_id": 3417,
            "pub_date": "2025-04-22",
            "pub_date_card": {
                "ru": "22 апреля",
                "en": "April 22",
                "zh": "4月22日"
            },
            "hash": "5357bfeea528614a",
            "authors": [
                "Shi Qiu",
                "Shaoyang Guo",
                "Zhuo-Yang Song",
                "Yunbo Sun",
                "Zeyu Cai",
                "Jiashen Wei",
                "Tianyu Luo",
                "Yixuan Yin",
                "Haoxu Zhang",
                "Yi Hu",
                "Chenyang Wang",
                "Chencheng Tang",
                "Haoling Chang",
                "Qi Liu",
                "Ziheng Zhou",
                "Tianyu Zhang",
                "Jingtian Zhang",
                "Zhangyi Liu",
                "Minghao Li",
                "Yuku Zhang",
                "Boxuan Jing",
                "Xianqi Yin",
                "Yutong Ren",
                "Zizhuo Fu",
                "Weike Wang",
                "Xudong Tian",
                "Anqi Lv",
                "Laifu Man",
                "Jianxiang Li",
                "Feiyu Tao",
                "Qihua Sun",
                "Zhou Liang",
                "Yushu Mu",
                "Zhongxuan Li",
                "Jing-Jun Zhang",
                "Shutao Zhang",
                "Xiaotian Li",
                "Xingqi Xia",
                "Jiawei Lin",
                "Zheyu Shen",
                "Jiahang Chen",
                "Qiuhao Xiong",
                "Binran Wang",
                "Fengyuan Wang",
                "Ziyang Ni",
                "Bohan Zhang",
                "Fan Cui",
                "Changkun Shao",
                "Qing-Hong Cao",
                "Ming-xing Luo",
                "Muhan Zhang",
                "Hua Xing Zhu"
            ],
            "affiliations": [
                "Beijing Computational Science Research Center",
                "Institute for Artificial Intelligence, Peking University",
                "School of Integrated Circuits, Peking University",
                "School of Physics, Peking University",
                "Yuanpei College, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.16074.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#survey",
                    "#math",
                    "#dataset"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "PHYBench: Проверка физического мышления искусственного интеллекта",
                    "desc": "PHYBench - это новый высококачественный бенчмарк для оценки способностей больших языковых моделей (LLM) к рассуждениям в физическом контексте. Он содержит 500 тщательно отобранных задач по физике, охватывающих различные области от механики до современной физики. Авторы также предлагают новую метрику оценки - Expression Edit Distance (EED) Score, основанную на редакционном расстоянии между математическими выражениями. Результаты тестирования показывают, что даже современные модели значительно отстают от экспертов-людей в сложных физических рассуждениях."
                },
                "en": {
                    "title": "Evaluating LLMs in Physics: Bridging the Reasoning Gap",
                    "desc": "PHYBench is a new benchmark created to test how well large language models (LLMs) can reason about physics. It includes 500 carefully selected physics problems that reflect real-world situations, covering various topics like mechanics and thermodynamics. The benchmark is designed for different skill levels, from high school to advanced physics challenges. To evaluate the models, a new metric called Expression Edit Distance (EED) Score is introduced, which measures the differences in reasoning by comparing mathematical expressions, showing that current models still fall short compared to human experts in complex reasoning tasks."
                },
                "zh": {
                    "title": "评估物理推理能力的新基准",
                    "desc": "我们介绍了PHYBench，这是一个新颖的高质量基准，用于评估大型语言模型（LLMs）在物理情境下的推理能力。PHYBench包含500个精心策划的物理问题，基于真实的物理场景，旨在评估模型理解和推理现实物理过程的能力。基准涵盖了从高中练习到本科问题和物理奥林匹克挑战的多个难度级别，涉及力学、电磁学、热力学、光学、现代物理和高级物理。此外，我们提出了表达编辑距离（EED）评分，这是一种新颖的评估指标，基于数学表达式之间的编辑距离，有效捕捉模型推理过程和结果的差异。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.15585",
            "title": "A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training\n  and Deployment",
            "url": "https://huggingface.co/papers/2504.15585",
            "abstract": "The remarkable success of Large Language Models (LLMs) has illuminated a promising pathway toward achieving Artificial General Intelligence for both academic and industrial communities, owing to their unprecedented performance across various applications. As LLMs continue to gain prominence in both research and commercial domains, their security and safety implications have become a growing concern, not only for researchers and corporations but also for every nation. Currently, existing surveys on LLM safety primarily focus on specific stages of the LLM lifecycle, e.g., deployment phase or fine-tuning phase, lacking a comprehensive understanding of the entire \"lifechain\" of LLMs. To address this gap, this paper introduces, for the first time, the concept of \"full-stack\" safety to systematically consider safety issues throughout the entire process of LLM training, deployment, and eventual commercialization. Compared to the off-the-shelf LLM safety surveys, our work demonstrates several distinctive advantages: (I) Comprehensive Perspective. We define the complete LLM lifecycle as encompassing data preparation, pre-training, post-training, deployment and final commercialization. To our knowledge, this represents the first safety survey to encompass the entire lifecycle of LLMs. (II) Extensive Literature Support. Our research is grounded in an exhaustive review of over 800+ papers, ensuring comprehensive coverage and systematic organization of security issues within a more holistic understanding. (III) Unique Insights. Through systematic literature analysis, we have developed reliable roadmaps and perspectives for each chapter. Our work identifies promising research directions, including safety in data generation, alignment techniques, model editing, and LLM-based agent systems. These insights provide valuable guidance for researchers pursuing future work in this field.",
            "score": 9,
            "issue_id": 3406,
            "pub_date": "2025-04-22",
            "pub_date_card": {
                "ru": "22 апреля",
                "en": "April 22",
                "zh": "4月22日"
            },
            "hash": "3eb03bb75acef757",
            "authors": [
                "Kun Wang",
                "Guibin Zhang",
                "Zhenhong Zhou",
                "Jiahao Wu",
                "Miao Yu",
                "Shiqian Zhao",
                "Chenlong Yin",
                "Jinhu Fu",
                "Yibo Yan",
                "Hanjun Luo",
                "Liang Lin",
                "Zhihao Xu",
                "Haolang Lu",
                "Xinye Cao",
                "Xinyun Zhou",
                "Weifei Jin",
                "Fanci Meng",
                "Junyuan Mao",
                "Hao Wu",
                "Minghe Wang",
                "Fan Zhang",
                "Junfeng Fang",
                "Chengwei Liu",
                "Yifan Zhang",
                "Qiankun Li",
                "Chongye Guo",
                "Yalan Qin",
                "Yi Ding",
                "Donghai Hong",
                "Jiaming Ji",
                "Xinfeng Li",
                "Yifan Jiang",
                "Dongxia Wang",
                "Yihao Huang",
                "Yufei Guo",
                "Jen-tse Huang",
                "Yanwei Yue",
                "Wenke Huang",
                "Guancheng Wan",
                "Tianlin Li",
                "Lei Bai",
                "Jie Zhang",
                "Qing Guo",
                "Jingyi Wang",
                "Tianlong Chen",
                "Joey Tianyi Zhou",
                "Xiaojun Jia",
                "Weisong Sun",
                "Cong Wu",
                "Jing Chen",
                "Xuming Hu",
                "Yiming Li",
                "Xiao Wang",
                "Ningyu Zhang",
                "Luu Anh Tuan",
                "Guowen Xu",
                "Tianwei Zhang",
                "Xingjun Ma",
                "Xiang Wang",
                "Bo An",
                "Jun Sun",
                "Mohit Bansal",
                "Shirui Pan",
                "Yuval Elovici",
                "Bhavya Kailkhura",
                "Bo Li",
                "Yaodong Yang",
                "Hongwei Li",
                "Wenyuan Xu",
                "Yizhou Sun",
                "Wei Wang",
                "Qing Li",
                "Ke Tang",
                "Yu-Gang Jiang",
                "Felix Juefei-Xu",
                "Hui Xiong",
                "Xiaofeng Wang",
                "Shuicheng Yan",
                "Dacheng Tao",
                "Philip S. Yu",
                "Qingsong Wen",
                "Yang Liu"
            ],
            "affiliations": [
                "A*STAR",
                "Georgia Institute of Technology",
                "Hong Kong University of Science and Technology",
                "Hong Kong University of Science and Technology (Guangzhou)",
                "Institute of Automation, Chinese Academy of Sciences",
                "Institute of Information Engineering, Chinese Academy of Sciences",
                "Nanyang Technological University",
                "National University of Singapore",
                "Peking University",
                "Renmin University of China",
                "Shanghai AI Laboratory",
                "Shanghai University",
                "Southern University of Science and Technology",
                "Squirrel AI Learning",
                "TeleAI",
                "Tencent",
                "The Hong Kong Polytechnic University",
                "The Pennsylvania State University",
                "University of Science and Technology of China",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.15585.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#survey",
                    "#alignment",
                    "#agi",
                    "#agents",
                    "#security",
                    "#data"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "Полностековая безопасность LLM: от данных до применения",
                    "desc": "Эта статья представляет концепцию 'полностековой' безопасности для больших языковых моделей (LLM), охватывая весь жизненный цикл от подготовки данных до коммерциализации. Авторы провели обширный обзор более 800 научных работ, систематизируя проблемы безопасности в рамках целостного понимания. Исследование выделяет перспективные направления, включая безопасность при генерации данных, техники выравнивания, редактирование моделей и системы агентов на основе LLM. Работа предоставляет ценные рекомендации для будущих исследований в этой области."
                },
                "en": {
                    "title": "Ensuring Safety Across the Entire Lifecycle of Large Language Models",
                    "desc": "This paper discusses the safety and security concerns related to Large Language Models (LLMs) as they become more widely used in various applications. It introduces the concept of 'full-stack' safety, which looks at the entire lifecycle of LLMs, from data preparation to commercialization, rather than focusing on just one phase. The authors conducted a thorough review of over 800 research papers to provide a comprehensive understanding of safety issues throughout this lifecycle. They also offer unique insights and research directions for improving safety in areas like data generation and model alignment."
                },
                "zh": {
                    "title": "全面关注大型语言模型的安全性",
                    "desc": "大型语言模型（LLMs）的成功为实现人工通用智能提供了新的可能性，但其安全性问题日益受到关注。现有的研究主要集中在LLM生命周期的特定阶段，缺乏对整个生命周期的全面理解。本文首次提出了“全栈安全”的概念，系统地考虑LLM训练、部署和商业化过程中的安全问题。通过对800多篇文献的深入分析，我们为未来的研究方向提供了可靠的路线图和见解。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.11919",
            "title": "Rethinking the Generation of High-Quality CoT Data from the Perspective\n  of LLM-Adaptive Question Difficulty Grading",
            "url": "https://huggingface.co/papers/2504.11919",
            "abstract": "Recently, DeepSeek-R1 (671B) (DeepSeek-AIet al., 2025) has demonstrated its excellent reasoning ability in complex tasks and has publiclyshared its methodology. This provides potentially high-quality chain-of-thought (CoT) data for stimulating the reasoning abilities of small-sized large language models (LLMs). To generate high-quality CoT data for different LLMs, we seek an efficient method for generating high-quality CoT data with LLM-Adaptive questiondifficulty levels. First, we grade the difficulty of the questions according to the reasoning ability of the LLMs themselves and construct a LLM-Adaptive question database. Second, we sample the problem database based on a distribution of difficulty levels of the questions and then use DeepSeek-R1 (671B) (DeepSeek-AI et al., 2025) to generate the corresponding high-quality CoT data with correct answers. Thanks to the construction of CoT data with LLM-Adaptive difficulty levels, we have significantly reduced the cost of data generation and enhanced the efficiency of model supervised fine-tuning (SFT). Finally, we have validated the effectiveness and generalizability of the proposed method in the fields of complex mathematical competitions and code generation tasks. Notably, with only 2k high-quality mathematical CoT data, our ZMath-32B surpasses DeepSeek-Distill-32B in math reasoning task. Similarly, with only 2k high-quality code CoT data, our ZCode-32B surpasses DeepSeek-Distill-32B in code reasoning tasks.",
            "score": 9,
            "issue_id": 3411,
            "pub_date": "2025-04-16",
            "pub_date_card": {
                "ru": "16 апреля",
                "en": "April 16",
                "zh": "4月16日"
            },
            "hash": "902a4c2acec9b9d4",
            "authors": [
                "Qianjin Yu",
                "Keyu Wu",
                "Zihan Chen",
                "Chushu Zhang",
                "Manlin Mei",
                "Lingjun Huang",
                "Fang Tan",
                "Yongsheng Du",
                "Kunlin Liu",
                "Yurui Zhu"
            ],
            "affiliations": [
                "Intelligent System Department, Zhongxing Telecom Equipment(ZTE), Changsha, Hunan, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.11919.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#dataset",
                    "#math",
                    "#optimization",
                    "#plp",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Эффективное обучение рассуждениям малых языковых моделей с помощью адаптивных данных",
                    "desc": "Статья описывает метод создания высококачественных данных для обучения рассуждениям (chain-of-thought) малых языковых моделей. Авторы предлагают использовать большую модель DeepSeek-R1 для генерации примеров рассуждений, адаптированных под способности конкретных малых моделей. Они создают базу вопросов различной сложности и используют их для генерации обучающих данных. Эффективность метода подтверждена экспериментами в области математических и программистских задач."
                },
                "en": {
                    "title": "Adaptive CoT Data Generation for Enhanced LLM Reasoning",
                    "desc": "The paper introduces a method for generating high-quality chain-of-thought (CoT) data tailored to the reasoning abilities of different large language models (LLMs). It involves creating a database of questions graded by difficulty, which allows for adaptive sampling based on the LLM's capabilities. By using the DeepSeek-R1 model to generate CoT data from this adaptive question database, the authors significantly reduce the cost and improve the efficiency of supervised fine-tuning (SFT) for smaller models. The results show that their approach leads to superior performance in complex tasks like mathematical reasoning and code generation with minimal data requirements."
                },
                "zh": {
                    "title": "提升推理能力的自适应链式思维数据生成",
                    "desc": "本文介绍了一种生成高质量链式思维（CoT）数据的方法，旨在提高小型大型语言模型（LLMs）的推理能力。我们首先根据LLMs的推理能力对问题进行难度分级，并构建了一个自适应问题数据库。然后，我们根据问题的难度分布从数据库中抽样，并使用DeepSeek-R1生成相应的高质量CoT数据。通过这种方法，我们显著降低了数据生成成本，并提高了模型的监督微调效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.15707",
            "title": "RePOPE: Impact of Annotation Errors on the POPE Benchmark",
            "url": "https://huggingface.co/papers/2504.15707",
            "abstract": "Since data annotation is costly, benchmark datasets often incorporate labels from established image datasets. In this work, we assess the impact of label errors in MSCOCO on the frequently used object hallucination benchmark POPE. We re-annotate the benchmark images and identify an imbalance in annotation errors across different subsets. Evaluating multiple models on the revised labels, which we denote as RePOPE, we observe notable shifts in model rankings, highlighting the impact of label quality. Code and data are available at https://github.com/YanNeu/RePOPE .",
            "score": 8,
            "issue_id": 3410,
            "pub_date": "2025-04-22",
            "pub_date_card": {
                "ru": "22 апреля",
                "en": "April 22",
                "zh": "4月22日"
            },
            "hash": "413cf886c426d830",
            "authors": [
                "Yannic Neuhaus",
                "Matthias Hein"
            ],
            "affiliations": [
                "Tubingen AI Center, University of Tubingen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.15707.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#benchmark",
                    "#open_source",
                    "#dataset",
                    "#data"
                ],
                "emoji": "🏷️",
                "ru": {
                    "title": "Качество разметки определяет точность оценки моделей",
                    "desc": "Исследование анализирует влияние ошибок в разметке набора данных MSCOCO на бенчмарк объектных галлюцинаций POPE. Авторы провели повторную аннотацию изображений и обнаружили несбалансированность ошибок разметки в разных подмножествах. При оценке нескольких моделей на исправленных метках (названных RePOPE) были выявлены значительные изменения в рейтинге моделей. Это подчеркивает важность качества разметки для корректной оценки эффективности моделей машинного обучения."
                },
                "en": {
                    "title": "Revising Labels, Revising Rankings: The Impact of Annotation Quality",
                    "desc": "This paper investigates the effects of label errors in the MSCOCO dataset on the object hallucination benchmark known as POPE. The authors re-annotate the images in the benchmark to identify inconsistencies in the annotation quality across various subsets. By evaluating several models using the newly revised labels, referred to as RePOPE, they find significant changes in model performance rankings. This study emphasizes the critical importance of label accuracy in machine learning benchmarks and its influence on model evaluation outcomes."
                },
                "zh": {
                    "title": "标签质量影响模型表现",
                    "desc": "在这项研究中，我们评估了MSCOCO数据集中标签错误对常用的物体幻觉基准POPE的影响。我们重新标注了基准图像，并发现不同子集之间的标注错误存在不平衡。通过在修订后的标签上评估多个模型，我们观察到模型排名发生了显著变化，突显了标签质量的重要性。此研究的代码和数据可在https://github.com/YanNeu/RePOPE获取。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.16891",
            "title": "AIMO-2 Winning Solution: Building State-of-the-Art Mathematical\n  Reasoning Models with OpenMathReasoning dataset",
            "url": "https://huggingface.co/papers/2504.16891",
            "abstract": "This paper presents our winning submission to the AI Mathematical Olympiad - Progress Prize 2 (AIMO-2) competition. Our recipe for building state-of-the-art mathematical reasoning models relies on three key pillars. First, we create a large-scale dataset comprising 540K unique high-quality math problems, including olympiad-level problems, and their 3.2M long-reasoning solutions. Second, we develop a novel method to integrate code execution with long reasoning models through iterative training, generation, and quality filtering, resulting in 1.7M high-quality Tool-Integrated Reasoning solutions. Third, we create a pipeline to train models to select the most promising solution from many candidates. We show that such generative solution selection (GenSelect) can significantly improve upon majority voting baseline. Combining these ideas, we train a series of models that achieve state-of-the-art results on mathematical reasoning benchmarks. To facilitate further research, we release our code, models, and the complete OpenMathReasoning dataset under a commercially permissive license.",
            "score": 5,
            "issue_id": 3417,
            "pub_date": "2025-04-23",
            "pub_date_card": {
                "ru": "23 апреля",
                "en": "April 23",
                "zh": "4月23日"
            },
            "hash": "9b9614fe42f410e8",
            "authors": [
                "Ivan Moshkov",
                "Darragh Hanley",
                "Ivan Sorokin",
                "Shubham Toshniwal",
                "Christof Henkel",
                "Benedikt Schifferer",
                "Wei Du",
                "Igor Gitman"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2504.16891.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#math",
                    "#training",
                    "#dataset",
                    "#data",
                    "#open_source"
                ],
                "emoji": "🧮",
                "ru": {
                    "title": "Прорыв в математическом мышлении ИИ: от олимпиадных задач к передовым моделям",
                    "desc": "Статья представляет победившее решение в конкурсе AI Mathematical Olympiad - Progress Prize 2. Авторы создали масштабный датасет из 540 тысяч уникальных математических задач и 3,2 миллиона решений к ним. Они разработали новый метод интеграции выполнения кода с моделями длинных рассуждений через итеративное обучение, генерацию и фильтрацию качества. Исследователи также создали конвейер для обучения моделей выбору наиболее перспективного решения из множества кандидатов, что значительно улучшило базовый метод мажоритарного голосования."
                },
                "en": {
                    "title": "Revolutionizing Mathematical Reasoning with Advanced Models",
                    "desc": "This paper describes a successful approach to building advanced mathematical reasoning models for the AI Mathematical Olympiad competition. The authors created a large dataset of 540,000 unique math problems and their detailed solutions, which serves as the foundation for training their models. They introduced a novel method that combines code execution with long reasoning processes, enhancing the quality of generated solutions. Additionally, they developed a generative solution selection technique that improves model performance by effectively choosing the best solutions from multiple candidates, leading to state-of-the-art results in mathematical reasoning tasks."
                },
                "zh": {
                    "title": "数学推理模型的创新之路",
                    "desc": "本文介绍了我们在AI数学奥林匹克-进步奖2（AIMO-2）竞赛中的获胜方案。我们构建先进数学推理模型的关键在于三个支柱：首先，我们创建了一个包含54万个独特高质量数学问题的大规模数据集，以及320万个长推理解决方案。其次，我们开发了一种新方法，通过迭代训练、生成和质量过滤，将代码执行与长推理模型相结合，最终得到170万个高质量的工具集成推理解决方案。最后，我们建立了一个管道，训练模型从多个候选方案中选择最有前景的解决方案，显著提高了生成解决方案选择的效果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10419",
            "title": "Unchecked and Overlooked: Addressing the Checkbox Blind Spot in Large\n  Language Models with CheckboxQA",
            "url": "https://huggingface.co/papers/2504.10419",
            "abstract": "Checkboxes are critical in real-world document processing where the presence or absence of ticks directly informs data extraction and decision-making processes. Yet, despite the strong performance of Large Vision and Language Models across a wide range of tasks, they struggle with interpreting checkable content. This challenge becomes particularly pressing in industries where a single overlooked checkbox may lead to costly regulatory or contractual oversights. To address this gap, we introduce the CheckboxQA dataset, a targeted resource designed to evaluate and improve model performance on checkbox-related tasks. It reveals the limitations of current models and serves as a valuable tool for advancing document comprehension systems, with significant implications for applications in sectors such as legal tech and finance.   The dataset is publicly available at: https://github.com/Snowflake-Labs/CheckboxQA",
            "score": 4,
            "issue_id": 3407,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 апреля",
                "en": "April 14",
                "zh": "4月14日"
            },
            "hash": "38ab7a70097348c6",
            "authors": [
                "Michał Turski",
                "Mateusz Chiliński",
                "Łukasz Borchmann"
            ],
            "affiliations": [
                "Snowflake AI Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10419.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#interpretability",
                    "#dataset",
                    "#science"
                ],
                "emoji": "☑️",
                "ru": {
                    "title": "CheckboxQA: новый датасет для улучшения распознавания флажков в документах",
                    "desc": "Статья представляет набор данных CheckboxQA, созданный для оценки и улучшения работы моделей машинного обучения с задачами, связанными с флажками в документах. Авторы отмечают, что несмотря на высокую эффективность больших моделей компьютерного зрения и обработки естественного языка в различных задачах, они испытывают трудности с интерпретацией отмеченных флажков. Этот набор данных раскрывает ограничения существующих моделей и служит инструментом для совершенствования систем понимания документов. Особое значение это имеет для таких отраслей, как юридические технологии и финансы, где пропуск одного флажка может привести к дорогостоящим ошибкам."
                },
                "en": {
                    "title": "Enhancing Checkbox Interpretation in Document Processing",
                    "desc": "This paper addresses the challenges faced by Large Vision and Language Models in interpreting checkboxes in documents, which are crucial for data extraction and decision-making. The authors introduce the CheckboxQA dataset, specifically designed to evaluate and enhance model performance on checkbox-related tasks. The dataset highlights the limitations of existing models in this area and aims to improve document comprehension systems. This work has important implications for industries like legal tech and finance, where missing a checkbox can lead to significant errors."
                },
                "zh": {
                    "title": "提升复选框理解能力的关键数据集",
                    "desc": "在现实世界的文档处理过程中，复选框的存在与否直接影响数据提取和决策过程。尽管大型视觉和语言模型在多种任务中表现出色，但它们在解读复选内容方面仍然存在困难。这个问题在某些行业尤为重要，因为一个被忽视的复选框可能导致昂贵的合规或合同失误。为了解决这个问题，我们推出了CheckboxQA数据集，旨在评估和提升模型在复选框相关任务上的表现。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.15254",
            "title": "CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation",
            "url": "https://huggingface.co/papers/2504.15254",
            "abstract": "C-to-Rust transpilation is essential for modernizing legacy C code while enhancing safety and interoperability with modern Rust ecosystems. However, no dataset currently exists for evaluating whether a system can transpile C into safe Rust that passes a set of test cases. We introduce CRUST-Bench, a dataset of 100 C repositories, each paired with manually-written interfaces in safe Rust as well as test cases that can be used to validate correctness of the transpilation. By considering entire repositories rather than isolated functions, CRUST-Bench captures the challenges of translating complex projects with dependencies across multiple files. The provided Rust interfaces provide explicit specifications that ensure adherence to idiomatic, memory-safe Rust patterns, while the accompanying test cases enforce functional correctness. We evaluate state-of-the-art large language models (LLMs) on this task and find that safe and idiomatic Rust generation is still a challenging problem for various state-of-the-art methods and techniques. We also provide insights into the errors LLMs usually make in transpiling code from C to safe Rust. The best performing model, OpenAI o1, is able to solve only 15 tasks in a single-shot setting. Improvements on CRUST-Bench would lead to improved transpilation systems that can reason about complex scenarios and help in migrating legacy codebases from C into languages like Rust that ensure memory safety. You can find the dataset and code at https://github.com/anirudhkhatry/CRUST-bench.",
            "score": 3,
            "issue_id": 3408,
            "pub_date": "2025-04-21",
            "pub_date_card": {
                "ru": "21 апреля",
                "en": "April 21",
                "zh": "4月21日"
            },
            "hash": "d60d8c4dbf521ab4",
            "authors": [
                "Anirudh Khatry",
                "Robert Zhang",
                "Jia Pan",
                "Ziteng Wang",
                "Qiaochu Chen",
                "Greg Durrett",
                "Isil Dillig"
            ],
            "affiliations": [
                "New York University",
                "The University of Texas at Austin"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.15254.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#benchmark",
                    "#optimization",
                    "#dataset",
                    "#reasoning"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "CRUST-Bench: новый бенчмарк для оценки транспиляции C в безопасный Rust",
                    "desc": "CRUST-Bench - это новый набор данных для оценки транспиляции кода с C на безопасный Rust. Он включает 100 репозиториев C с вручную написанными интерфейсами на Rust и тестами для проверки корректности. Набор данных охватывает сложности перевода целых проектов, а не отдельных функций. Эксперименты показали, что даже современные языковые модели испытывают трудности с этой задачей - лучшая модель решила только 15 задач."
                },
                "en": {
                    "title": "Transforming C Code to Safe Rust: Introducing CRUST-Bench",
                    "desc": "This paper introduces CRUST-Bench, a new dataset designed to evaluate the transpilation of C code into safe Rust. It consists of 100 C repositories, each with corresponding safe Rust interfaces and test cases to ensure correctness. By focusing on entire repositories, CRUST-Bench addresses the complexities of translating projects with multiple dependencies. The study also assesses the performance of large language models (LLMs) in this task, revealing that generating safe and idiomatic Rust remains a significant challenge."
                },
                "zh": {
                    "title": "CRUST-Bench：提升C到Rust转译的安全性与准确性",
                    "desc": "C到Rust的转译对于现代化遗留C代码至关重要，同时增强了与现代Rust生态系统的安全性和互操作性。我们提出了CRUST-Bench，这是一个包含100个C代码库的数据集，每个库都配有手动编写的安全Rust接口和测试用例，以验证转译的正确性。该数据集考虑了整个代码库，而不是孤立的函数，从而捕捉到跨多个文件的复杂项目转译的挑战。我们评估了最新的大型语言模型，发现生成安全和符合Rust习惯的代码仍然是一个具有挑战性的问题。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.16145",
            "title": "Progressive Language-guided Visual Learning for Multi-Task Visual\n  Grounding",
            "url": "https://huggingface.co/papers/2504.16145",
            "abstract": "Multi-task visual grounding (MTVG) includes two sub-tasks, i.e., Referring Expression Comprehension (REC) and Referring Expression Segmentation (RES). The existing representative approaches generally follow the research pipeline which mainly consists of three core procedures, including independent feature extraction for visual and linguistic modalities, respectively, cross-modal interaction module, and independent prediction heads for different sub-tasks. Albeit achieving remarkable performance, this research line has two limitations: 1) The linguistic content has not been fully injected into the entire visual backbone for boosting more effective visual feature extraction and it needs an extra cross-modal interaction module; 2) The relationship between REC and RES tasks is not effectively exploited to help the collaborative prediction for more accurate output. To deal with these problems, in this paper, we propose a Progressive Language-guided Visual Learning framework for multi-task visual grounding, called PLVL, which not only finely mine the inherent feature expression of the visual modality itself but also progressively inject the language information to help learn linguistic-related visual features. In this manner, our PLVL does not need additional cross-modal fusion module while fully introducing the language guidance. Furthermore, we analyze that the localization center for REC would help identify the to-be-segmented object region for RES to some extent. Inspired by this investigation, we design a multi-task head to accomplish collaborative predictions for these two sub-tasks. Extensive experiments conducted on several benchmark datasets comprehensively substantiate that our PLVL obviously outperforms the representative methods in both REC and RES tasks. https://github.com/jcwang0602/PLVL",
            "score": 1,
            "issue_id": 3413,
            "pub_date": "2025-04-22",
            "pub_date_card": {
                "ru": "22 апреля",
                "en": "April 22",
                "zh": "4月22日"
            },
            "hash": "0f428629b7acb19c",
            "authors": [
                "Jingchao Wang",
                "Hong Wang",
                "Wenlong Zhang",
                "Kunhua Ji",
                "Dingjiang Huang",
                "Yefeng Zheng"
            ],
            "affiliations": [
                "East China Normal University, Shanghai, China",
                "Westlake University, Hangzhou, Zhejiang, China",
                "Xian Jiaotong University, Xian, Shaanxi, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.16145.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#cv"
                ],
                "emoji": "👁️",
                "ru": {
                    "title": "Прогрессивное языковое обучение для улучшения визуального заземления",
                    "desc": "Эта статья представляет новый подход к многозадачному визуальному заземлению, называемый PLVL. Авторы предлагают прогрессивное обучение визуальным признакам под руководством языковой информации, что устраняет необходимость в отдельном модуле кросс-модального взаимодействия. PLVL также использует многозадачную архитектуру для совместного предсказания в задачах понимания и сегментации реферирующих выражений. Эксперименты показывают, что PLVL превосходит существующие методы на нескольких эталонных наборах данных."
                },
                "en": {
                    "title": "Enhancing Visual Grounding with Language Integration",
                    "desc": "This paper introduces a new framework called Progressive Language-guided Visual Learning (PLVL) for multi-task visual grounding, which includes tasks like Referring Expression Comprehension (REC) and Referring Expression Segmentation (RES). The authors identify limitations in existing methods, such as the lack of effective integration of linguistic information into visual feature extraction and the insufficient collaboration between the REC and RES tasks. PLVL addresses these issues by progressively incorporating language guidance into the visual learning process, eliminating the need for a separate cross-modal interaction module. The proposed approach enhances the accuracy of predictions by leveraging the relationship between the two tasks, as demonstrated by extensive experiments on benchmark datasets that show significant performance improvements."
                },
                "zh": {
                    "title": "渐进式语言引导视觉学习：提升多任务视觉定位的效果",
                    "desc": "多任务视觉定位（MTVG）包括两个子任务：指代表达理解（REC）和指代表达分割（RES）。现有的方法通常采用独立特征提取、跨模态交互模块和独立预测头的流程，尽管取得了良好效果，但存在语言信息未充分融入视觉特征提取和子任务间关系未有效利用的问题。为了解决这些问题，本文提出了一种渐进式语言引导视觉学习框架（PLVL），该框架通过逐步注入语言信息来提升视觉特征学习，同时避免了额外的跨模态融合模块。实验结果表明，PLVL在REC和RES任务上均显著优于现有方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.13263",
            "title": "Causal-Copilot: An Autonomous Causal Analysis Agent",
            "url": "https://huggingface.co/papers/2504.13263",
            "abstract": "Causal analysis plays a foundational role in scientific discovery and reliable decision-making, yet it remains largely inaccessible to domain experts due to its conceptual and algorithmic complexity. This disconnect between causal methodology and practical usability presents a dual challenge: domain experts are unable to leverage recent advances in causal learning, while causal researchers lack broad, real-world deployment to test and refine their methods. To address this, we introduce Causal-Copilot, an autonomous agent that operationalizes expert-level causal analysis within a large language model framework. Causal-Copilot automates the full pipeline of causal analysis for both tabular and time-series data -- including causal discovery, causal inference, algorithm selection, hyperparameter optimization, result interpretation, and generation of actionable insights. It supports interactive refinement through natural language, lowering the barrier for non-specialists while preserving methodological rigor. By integrating over 20 state-of-the-art causal analysis techniques, our system fosters a virtuous cycle -- expanding access to advanced causal methods for domain experts while generating rich, real-world applications that inform and advance causal theory. Empirical evaluations demonstrate that Causal-Copilot achieves superior performance compared to existing baselines, offering a reliable, scalable, and extensible solution that bridges the gap between theoretical sophistication and real-world applicability in causal analysis. A live interactive demo of Causal-Copilot is available at https://causalcopilot.com/.",
            "score": 0,
            "issue_id": 3418,
            "pub_date": "2025-04-17",
            "pub_date_card": {
                "ru": "17 апреля",
                "en": "April 17",
                "zh": "4月17日"
            },
            "hash": "53da6426fe219919",
            "authors": [
                "Xinyue Wang",
                "Kun Zhou",
                "Wenyi Wu",
                "Har Simrat Singh",
                "Fang Nan",
                "Songyao Jin",
                "Aryan Philip",
                "Saloni Patnaik",
                "Hou Zhu",
                "Shivam Singh",
                "Parjanya Prashant",
                "Qian Shen",
                "Biwei Huang"
            ],
            "affiliations": [
                "University of California San Diego, CA 92093, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.13263.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#science",
                    "#data",
                    "#inference",
                    "#agents"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Демократизация причинно-следственного анализа с помощью ИИ",
                    "desc": "Статья представляет Causal-Copilot - автономного агента, который автоматизирует полный цикл причинно-следственного анализа в рамках большой языковой модели. Система поддерживает более 20 современных методов каузального анализа для табличных и временных рядов данных, включая каузальное обнаружение, вывод и оптимизацию алгоритмов. Causal-Copilot обеспечивает интерактивное уточнение через естественный язык, делая сложные методы доступными для неспециалистов. Эмпирические оценки показывают превосходную производительность системы по сравнению с существующими базовыми решениями."
                },
                "en": {
                    "title": "Bridging the Gap in Causal Analysis with Causal-Copilot",
                    "desc": "Causal-Copilot is an innovative tool that simplifies causal analysis for domain experts by using a large language model. It automates the entire process of causal analysis, including discovery, inference, and interpretation, making it accessible to non-specialists. By integrating over 20 advanced causal techniques, it allows users to refine their analyses interactively through natural language. Empirical tests show that Causal-Copilot outperforms existing methods, making it a practical solution for applying causal analysis in real-world scenarios."
                },
                "zh": {
                    "title": "因果分析的智能助手",
                    "desc": "因果分析在科学发现和可靠决策中起着基础性作用，但由于其概念和算法的复杂性，领域专家往往难以接触。为了解决这一问题，我们提出了Causal-Copilot，这是一种在大型语言模型框架内实现专家级因果分析的自主代理。Causal-Copilot自动化了因果分析的完整流程，包括因果发现、因果推断、算法选择、超参数优化、结果解释和可操作见解的生成。通过自然语言支持交互式优化，Causal-Copilot降低了非专业人士的使用门槛，同时保持了方法论的严谨性。"
                }
            }
        }
    ],
    "link_prev": "2025-04-23.html",
    "link_next": "2025-04-25.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "23.04",
        "en": "04/23",
        "zh": "4月23日"
    },
    "short_date_next": {
        "ru": "25.04",
        "en": "04/25",
        "zh": "4月25日"
    },
    "categories": {
        "#dataset": 9,
        "#data": 8,
        "#benchmark": 9,
        "#agents": 2,
        "#cv": 4,
        "#rl": 2,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 1,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 3,
        "#math": 4,
        "#multilingual": 1,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 10,
        "#robotics": 0,
        "#agi": 1,
        "#games": 0,
        "#interpretability": 2,
        "#reasoning": 6,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 8,
        "#survey": 2,
        "#diffusion": 2,
        "#alignment": 3,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 1,
        "#science": 2,
        "#low_resource": 1
    },
    "zh": {
        "text": "视觉推理是人类智能的核心组成部分，也是先进多模态模型的关键能力。然而，当前对多模态大语言模型（MLLMs）的推理评估往往依赖文本描述，允许基于语言的推理捷径，无法衡量真正的视觉中心推理。为解决这一问题，我们引入了VisuLogic：一个包含1,000个人类验证问题的基准，涵盖六个类别（如量化转换、空间关系、属性比较）。这些不同类型的问题可以从多个角度评估MLLMs的视觉推理能力。我们对该基准评估了领先的MLLMs，并分析其结果，以识别常见的失败模式。大多数模型的准确率低于30%，仅略高于25%的随机基线，远低于人类的51.4%，显示出视觉推理方面的显著差距。此外，我们提供了一个补充训练数据集和一个强化学习基线，以支持进一步的进展。",
        "title": "VisuLogic: A Benchmark for Evaluating Visual Reasoning in Multi-modal\n  Large Language Models",
        "pinyin": "视觉推理是人类智能的核心组成部分，也是先进多模态模型的关键能力。然而，当前对多模态大语言模型（MLLMs）的推理评估往往依赖文本描述，允许基于语言的推理捷径，无法衡量真正的视觉中心推理。为解决这一问题，我们引入了VisuLogic：一个包含1,000个人类验证问题的基准，涵盖六个类别（如量化转换、空间关系、属性比较）。这些不同类型的问题可以从多个角度评估MLLMs的视觉推理能力。我们对该基准评估了领先的MLLMs，并分析其结果，以识别常见的失败模式。大多数模型的准确率低于30%，仅略高于25%的随机基线，远低于人类的51.4%，显示出视觉推理方面的显著差距。此外，我们提供了一个补充训练数据集和一个强化学习基线，以支持进一步的进展。\n\nShìjué tuīlǐ shì rénlèi zhìnéng de héxīn zǔchéng bùfēn, yě shì xiānjìn duō móshì duō móshì móxíng de guǎnjiàn nénglì. Rán'ér, dāngqián duì duō móshì dà yǔyán móxíng (MLLMs) de tuīlǐ píngjià wǎngwǎng yīlài wénběn miáoshù, yǔn xǔ jīyú yǔyán de tuīlǐ jiéjìng, wúfǎ héngliáng zhēnzhèng de shìjué zhōngxīn tuīlǐ. Wèi jiějué zhè yī wèntí, wǒmen yǐn rùle VisuLogic: yīgè bāohán 1,000 gè rénlèi yànzhèng wèntí de jīzhǔn, hánjiē liù gè lèibié (rú liàng huà zhuǎnhuàn, kōngjiān guānxì, shǔxìng bǐjiào). Zhèxiē bùtóng lèixíng de wèntí kěyǐ cóng duō gè jiǎodù píngjià MLLMs de shìjué tuīlǐ nénglì. Wǒmen duì gāi jīzhǔn píngjià le lǐngxiān de MLLMs, bìng fēnxi qí jiéguǒ, yǐ shíbié chángjiàn de shībài móshì. Dàduōshù móxíng de zhǔnquèlǜ dīyú 30%, jǐn lüè gāoyú 25% de suíjī jīzhǔn, yuǎn dīyú rénlèi de 51.4%, xiǎnshì chū shìjué tuīlǐ fāngmiàn de xiǎnzhù chājù. Cǐwài, wǒmen tígōngle yīgè bǔchōng xùnliàn shùjùjí hé yīgè qiángzhù xuéxí jīzhǔn, yǐ zhīchí jìnfā de jìnbù.",
        "vocab": "[\n    {\"word\": \"视觉\", \"pinyin\": \"shìjué\", \"trans\": \"vision\"},\n    {\"word\": \"推理\", \"pinyin\": \"tuīlǐ\", \"trans\": \"reasoning\"},\n    {\"word\": \"核心\", \"pinyin\": \"héxīn\", \"trans\": \"core\"},\n    {\"word\": \"组成部分\", \"pinyin\": \"zǔchéng bùfēn\", \"trans\": \"component\"},\n    {\"word\": \"先进\", \"pinyin\": \"xiānjìn\", \"trans\": \"advanced\"},\n    {\"word\": \"多模态\", \"pinyin\": \"duō móshì\", \"trans\": \"multimodal\"},\n    {\"word\": \"模型\", \"pinyin\": \"móxíng\", \"trans\": \"model\"},\n    {\"word\": \"关键\", \"pinyin\": \"guǎnjiàn\", \"trans\": \"key\"},\n    {\"word\": \"能力\", \"pinyin\": \"nénglì\", \"trans\": \"ability\"},\n    {\"word\": \"依赖\", \"pinyin\": \"yīlài\", \"trans\": \"rely on\"},\n    {\"word\": \"描述\", \"pinyin\": \"miáoshù\", \"trans\": \"description\"},\n    {\"word\": \"允许\", \"pinyin\": \"yǔnxǔ\", \"trans\": \"allow\"},\n    {\"word\": \"基于\", \"pinyin\": \"jīyú\", \"trans\": \"based on\"},\n    {\"word\": \"捷径\", \"pinyin\": \"jiéjìng\", \"trans\": \"shortcut\"},\n    {\"word\": \"衡量\", \"pinyin\": \"héngliáng\", \"trans\": \"measure\"},\n    {\"word\": \"真正\", \"pinyin\": \"zhēnzhèng\", \"trans\": \"genuine\"},\n    {\"word\": \"引入\", \"pinyin\": \"yǐnrù\", \"trans\": \"introduce\"},\n    {\"word\": \"基准\", \"pinyin\": \"jīzhǔn\", \"trans\": \"benchmark\"},\n    {\"word\": \"验证\", \"pinyin\": \"yànzhèng\", \"trans\": \"verification\"},\n    {\"word\": \"问题\", \"pinyin\": \"wèntí\", \"trans\": \"question\"},\n    {\"word\": \"涵盖\", \"pinyin\": \"hángài\", \"trans\": \"cover\"},\n    {\"word\": \"类别\", \"pinyin\": \"lèibié\", \"trans\": \"category\"},\n    {\"word\": \"量化\", \"pinyin\": \"liànghuà\", \"trans\": \"quantification\"},\n    {\"word\": \"转换\", \"pinyin\": \"zhuǎnhuàn\", \"trans\": \"conversion\"},\n    {\"word\": \"空间\", \"pinyin\": \"kōngjiān\", \"trans\": \"space\"},\n    {\"word\": \"关系\", \"pinyin\": \"guānxì\", \"trans\": \"relationship\"},\n    {\"word\": \"属性\", \"pinyin\": \"shǔxìng\", \"trans\": \"attribute\"},\n    {\"word\": \"比较\", \"pinyin\": \"bǐjiào\", \"trans\": \"comparison\"},\n    {\"word\": \"角度\", \"pinyin\": \"jiǎodù\", \"trans\": \"angle\"},\n    {\"word\": \"评估\", \"pinyin\": \"pínggū\", \"trans\": \"evaluate\"},\n    {\"word\": \"领先\", \"pinyin\": \"lǐngxiān\", \"trans\": \"leading\"},\n    {\"word\": \"分析\", \"pinyin\": \"fēnxī\", \"trans\": \"analyze\"},\n    {\"word\": \"结果\", \"pinyin\": \"jiéguǒ\", \"trans\": \"result\"},\n    {\"word\": \"识别\", \"pinyin\": \"shíbié\", \"trans\": \"identify\"},\n    {\"word\": \"模式\", \"pinyin\": \"móshì\", \"trans\": \"pattern\"},\n    {\"word\": \"准确率\", \"pinyin\": \"zhǔnquèlǜ\", \"trans\": \"accuracy\"},\n    {\"word\": \"随机\", \"pinyin\": \"suíjī\", \"trans\": \"random\"},\n    {\"word\": \"基线\", \"pinyin\": \"jīxiàn\", \"trans\": \"baseline\"},\n    {\"word\": \"显著\", \"pinyin\": \"xiǎnzhù\", \"trans\": \"significant\"},\n    {\"word\": \"差距\", \"pinyin\": \"chājù\", \"trans\": \"gap\"},\n    {\"word\": \"补充\", \"pinyin\": \"bǔchōng\", \"trans\": \"supplement\"},\n    {\"word\": \"数据集\", \"pinyin\": \"shùjùjí\", \"trans\": \"dataset\"},\n    {\"word\": \"强化\", \"pinyin\": \"qiánghuà\", \"trans\": \"reinforce\"},\n    {\"word\": \"学习\", \"pinyin\": \"xuéxí\", \"trans\": \"learning\"},\n    {\"word\": \"支持\", \"pinyin\": \"zhīchí\", \"trans\": \"support\"},\n    {\"word\": \"进展\", \"pinyin\": \"jìnzhǎn\", \"trans\": \"progress\"}\n]",
        "trans": "Visual reasoning is a core component of human intelligence and a key capability of advanced multimodal models. However, current evaluations of reasoning in multimodal large language models (MLLMs) often rely on textual descriptions, allowing for language-based reasoning shortcuts that fail to measure true vision-centric reasoning. To address this issue, we introduce VisuLogic: a benchmark containing 1,000 human-verified questions covering six categories (such as quantitative transformations, spatial relationships, and attribute comparisons). These different types of questions can evaluate the visual reasoning capabilities of MLLMs from multiple angles. We evaluated leading MLLMs on this benchmark and analyzed the results to identify common failure modes. Most models achieved an accuracy of less than 30%, only slightly above the 25% random baseline and significantly lower than the human accuracy of 51.4%, indicating a notable gap in visual reasoning. Additionally, we provide a supplementary training dataset and a reinforcement learning baseline to support further advancements.",
        "update_ts": "2025-04-24 09:12"
    }
}