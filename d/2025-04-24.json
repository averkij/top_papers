{
    "date": {
        "ru": "24 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
        "en": "April 24",
        "zh": "4æœˆ24æ—¥"
    },
    "time_utc": "2025-04-24 19:08",
    "weekday": 3,
    "issue_id": 3420,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.15279",
            "title": "VisuLogic: A Benchmark for Evaluating Visual Reasoning in Multi-modal\n  Large Language Models",
            "url": "https://huggingface.co/papers/2504.15279",
            "abstract": "Visual reasoning is a core component of human intelligence and a critical capability for advanced multimodal models. Yet current reasoning evaluations of multimodal large language models (MLLMs) often rely on text descriptions and allow language-based reasoning shortcuts, failing to measure genuine vision-centric reasoning. To address this, we introduce VisuLogic: a benchmark of 1,000 human-verified problems across six categories (e.g., quantitative shifts, spatial relations, attribute comparisons). These various types of questions can be evaluated to assess the visual reasoning capabilities of MLLMs from multiple perspectives. We evaluate leading MLLMs on this benchmark and analyze their results to identify common failure modes. Most models score below 30% accuracy-only slightly above the 25% random baseline and far below the 51.4% achieved by humans-revealing significant gaps in visual reasoning. Furthermore, we provide a supplementary training dataset and a reinforcement-learning baseline to support further progress.",
            "score": 54,
            "issue_id": 3404,
            "pub_date": "2025-04-21",
            "pub_date_card": {
                "ru": "21 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 21",
                "zh": "4æœˆ21æ—¥"
            },
            "hash": "5ec39c2a399dba2b",
            "authors": [
                "Weiye Xu",
                "Jiahao Wang",
                "Weiyun Wang",
                "Zhe Chen",
                "Wengang Zhou",
                "Aijun Yang",
                "Lewei Lu",
                "Houqiang Li",
                "Xiaohua Wang",
                "Xizhou Zhu",
                "Wenhai Wang",
                "Jifeng Dai",
                "Jinguo Zhu"
            ],
            "affiliations": [
                "SenseTime Research",
                "Shanghai Artificial Intelligence Laboratory",
                "Tsinghua University",
                "University of Science and Technology of China",
                "Xian Jiaotong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.15279.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#rl",
                    "#multimodal",
                    "#dataset"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "VisuLogic: Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°Ñ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğ¸ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ VisuLogic - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· 1000 Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ñ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² ÑˆĞµÑÑ‚Ğ¸ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑÑ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… MLLM Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ°Ğ±Ğ¸Ñ€Ğ°ÑÑ‚ Ğ¼ĞµĞ½ĞµĞµ 30% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½Ğ¸Ğ¶Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ° Ğ² 51.4%. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Enhancing Visual Reasoning in Multimodal Models with VisuLogic",
                    "desc": "This paper introduces VisuLogic, a new benchmark designed to evaluate the visual reasoning abilities of multimodal large language models (MLLMs). Unlike previous evaluations that relied on text descriptions, VisuLogic presents 1,000 human-verified problems across six categories, focusing on genuine vision-centric reasoning. The results show that leading MLLMs struggle with these tasks, scoring below 30% accuracy, which is only slightly better than random guessing and significantly lower than human performance. To aid in improving these models, the authors also provide a supplementary training dataset and a reinforcement-learning baseline."
                },
                "zh": {
                    "title": "æå‡è§†è§‰æ¨ç†èƒ½åŠ›çš„åŸºå‡†è¯„ä¼°",
                    "desc": "è§†è§‰æ¨ç†æ˜¯äººç±»æ™ºèƒ½çš„æ ¸å¿ƒç»„æˆéƒ¨åˆ†ï¼Œä¹Ÿæ˜¯é«˜çº§å¤šæ¨¡æ€æ¨¡å‹çš„é‡è¦èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç›®å‰å¯¹å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†è¯„ä¼°å¾€å¾€ä¾èµ–æ–‡æœ¬æè¿°ï¼Œå…è®¸è¯­è¨€åŸºç¡€çš„æ¨ç†æ·å¾„ï¼Œæœªèƒ½çœŸå®æµ‹é‡è§†è§‰ä¸­å¿ƒçš„æ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†VisuLogicï¼šä¸€ä¸ªåŒ…å«1000ä¸ªç»è¿‡äººå·¥éªŒè¯çš„é—®é¢˜çš„åŸºå‡†ï¼Œæ¶µç›–å…­ä¸ªç±»åˆ«ï¼ˆä¾‹å¦‚ï¼Œå®šé‡å˜åŒ–ã€ç©ºé—´å…³ç³»ã€å±æ€§æ¯”è¾ƒï¼‰ã€‚æˆ‘ä»¬å¯¹é¢†å…ˆçš„MLLMsåœ¨è¿™ä¸ªåŸºå‡†ä¸Šçš„è¡¨ç°è¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºå¤§å¤šæ•°æ¨¡å‹çš„å‡†ç¡®ç‡ä½äº30%ï¼Œè¿œä½äºäººç±»çš„51.4%ï¼Œæ­ç¤ºäº†è§†è§‰æ¨ç†æ–¹é¢çš„æ˜¾è‘—å·®è·ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.14509",
            "title": "DreamID: High-Fidelity and Fast diffusion-based Face Swapping via\n  Triplet ID Group Learning",
            "url": "https://huggingface.co/papers/2504.14509",
            "abstract": "In this paper, we introduce DreamID, a diffusion-based face swapping model that achieves high levels of ID similarity, attribute preservation, image fidelity, and fast inference speed. Unlike the typical face swapping training process, which often relies on implicit supervision and struggles to achieve satisfactory results. DreamID establishes explicit supervision for face swapping by constructing Triplet ID Group data, significantly enhancing identity similarity and attribute preservation. The iterative nature of diffusion models poses challenges for utilizing efficient image-space loss functions, as performing time-consuming multi-step sampling to obtain the generated image during training is impractical. To address this issue, we leverage the accelerated diffusion model SD Turbo, reducing the inference steps to a single iteration, enabling efficient pixel-level end-to-end training with explicit Triplet ID Group supervision. Additionally, we propose an improved diffusion-based model architecture comprising SwapNet, FaceNet, and ID Adapter. This robust architecture fully unlocks the power of the Triplet ID Group explicit supervision. Finally, to further extend our method, we explicitly modify the Triplet ID Group data during training to fine-tune and preserve specific attributes, such as glasses and face shape. Extensive experiments demonstrate that DreamID outperforms state-of-the-art methods in terms of identity similarity, pose and expression preservation, and image fidelity. Overall, DreamID achieves high-quality face swapping results at 512*512 resolution in just 0.6 seconds and performs exceptionally well in challenging scenarios such as complex lighting, large angles, and occlusions.",
            "score": 34,
            "issue_id": 3408,
            "pub_date": "2025-04-20",
            "pub_date_card": {
                "ru": "20 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 20",
                "zh": "4æœˆ20æ—¥"
            },
            "hash": "ed89c8434e937bfd",
            "authors": [
                "Fulong Ye",
                "Miao Hua",
                "Pengze Zhang",
                "Xinghui Li",
                "Qichao Sun",
                "Songtao Zhao",
                "Qian He",
                "Xinglong Wu"
            ],
            "affiliations": [
                "Intelligent Creation Team, ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.14509.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#diffusion",
                    "#cv",
                    "#architecture"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "DreamID: Ğ‘Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¾Ğ±Ğ¼ĞµĞ½ Ğ»Ğ¸Ñ† Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "DreamID - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±Ğ¼ĞµĞ½Ğ° Ğ»Ğ¸Ñ† Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ° ID, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ² Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Triplet ID Group Ğ¸ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ SD Turbo Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° DreamID Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ SwapNet, FaceNet Ğ¸ ID Adapter, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ñ€Ğ°ÑĞºÑ€Ñ‹Ñ‚ÑŒ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» ÑĞ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ DreamID Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ñƒ Ğ»Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ·Ñ‹ Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "DreamID: Fast and Accurate Face Swapping with Explicit Supervision",
                    "desc": "DreamID is a novel face swapping model that utilizes diffusion techniques to enhance identity similarity and attribute preservation while maintaining high image quality and fast processing times. It introduces explicit supervision through Triplet ID Group data, which significantly improves the results compared to traditional methods that rely on implicit supervision. The model employs an accelerated diffusion architecture, allowing for efficient training and inference by reducing the number of sampling steps required. Extensive testing shows that DreamID excels in maintaining facial features and quality even in difficult conditions, achieving impressive results in just 0.6 seconds."
                },
                "zh": {
                    "title": "DreamIDï¼šé«˜æ•ˆçš„äººè„¸äº¤æ¢æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºDreamIDçš„åŸºäºæ‰©æ•£æ¨¡å‹çš„äººè„¸äº¤æ¢æŠ€æœ¯ï¼Œè¯¥æ¨¡å‹åœ¨èº«ä»½ç›¸ä¼¼æ€§ã€å±æ€§ä¿ç•™ã€å›¾åƒä¿çœŸåº¦å’Œæ¨ç†é€Ÿåº¦æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ä¸ä¼ ç»Ÿçš„äººè„¸äº¤æ¢è®­ç»ƒè¿‡ç¨‹ä¸åŒï¼ŒDreamIDé€šè¿‡æ„å»ºä¸‰å…ƒç»„IDç»„æ•°æ®ï¼Œå»ºç«‹äº†æ˜¾å¼ç›‘ç£ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†èº«ä»½ç›¸ä¼¼æ€§å’Œå±æ€§ä¿ç•™ã€‚ä¸ºäº†å…‹æœæ‰©æ•£æ¨¡å‹åœ¨è®­ç»ƒä¸­å¤šæ­¥é‡‡æ ·çš„æ•ˆç‡é—®é¢˜ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†åŠ é€Ÿæ‰©æ•£æ¨¡å‹SD Turboï¼Œå°†æ¨ç†æ­¥éª¤å‡å°‘åˆ°å•æ¬¡è¿­ä»£ï¼Œå®ç°äº†é«˜æ•ˆçš„åƒç´ çº§ç«¯åˆ°ç«¯è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDreamIDåœ¨èº«ä»½ç›¸ä¼¼æ€§ã€å§¿æ€å’Œè¡¨æƒ…ä¿ç•™ä»¥åŠå›¾åƒä¿çœŸåº¦æ–¹é¢å‡ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.15431",
            "title": "Trillion 7B Technical Report",
            "url": "https://huggingface.co/papers/2504.15431",
            "abstract": "We introduce Trillion-7B, the most token-efficient Korean-centric multilingual LLM available. Our novel Cross-lingual Document Attention (XLDA) mechanism enables highly efficient and effective knowledge transfer from English to target languages like Korean and Japanese. Combined with optimized data mixtures, language-specific filtering, and tailored tokenizer construction, Trillion-7B achieves competitive performance while dedicating only 10\\% of its 2T training tokens to multilingual data and requiring just 59.4K H100 GPU hours (\\$148K) for full training. Comprehensive evaluations across 27 benchmarks in four languages demonstrate Trillion-7B's robust multilingual performance and exceptional cross-lingual consistency.",
            "score": 25,
            "issue_id": 3404,
            "pub_date": "2025-04-21",
            "pub_date_card": {
                "ru": "21 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 21",
                "zh": "4æœˆ21æ—¥"
            },
            "hash": "0d2a201b09695822",
            "authors": [
                "Sungjun Han",
                "Juyoung Suk",
                "Suyeong An",
                "Hyungguk Kim",
                "Kyuseok Kim",
                "Wonsuk Yang",
                "Seungtaek Choi",
                "Jamin Shin"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2504.15431.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multilingual",
                    "#low_resource",
                    "#training",
                    "#transfer_learning",
                    "#architecture",
                    "#data"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµĞ½Ğ¾Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ·Ñ‹ĞºĞ°Ğ¼Ğ¸ Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
                    "desc": "Trillion-7B - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ÑĞ·Ñ‹Ñ‡Ğ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ°ĞºÑ†ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ½Ğ° ĞºĞ¾Ñ€ĞµĞ¹ÑĞºĞ¸Ğ¹ ÑĞ·Ñ‹Ğº, Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ÑÑ‰Ğ°ÑÑÑ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¼ĞµĞ¶ÑŠÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğº Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼ (XLDA) Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ° Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ñ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ³Ğ¾ Ğ½Ğ° Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¸. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸, Trillion-7B Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ»Ğ¸ÑˆÑŒ 10% Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½ÑƒÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¼ĞµĞ¶ÑŠÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° 27 Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… ÑĞ·Ñ‹ĞºĞ°Ñ…."
                },
                "en": {
                    "title": "Efficient Multilingual Mastery with Trillion-7B",
                    "desc": "Trillion-7B is a multilingual language model designed specifically for Korean and other languages. It utilizes a new technique called Cross-lingual Document Attention (XLDA) to efficiently transfer knowledge from English to languages like Korean and Japanese. The model is trained with a small portion of multilingual data, only 10% of its total training tokens, and is optimized for performance with specific data mixtures and tokenizer adjustments. Evaluations show that Trillion-7B performs well across multiple languages and maintains strong consistency in cross-lingual tasks."
                },
                "zh": {
                    "title": "é«˜æ•ˆçš„å¤šè¯­è¨€å¤§æ¨¡å‹ Trillion-7B",
                    "desc": "Trillion-7B æ˜¯ä¸€ç§é«˜æ•ˆçš„å¤šè¯­è¨€å¤§æ¨¡å‹ï¼Œä¸“æ³¨äºéŸ©è¯­ç­‰è¯­è¨€çš„å¤„ç†ã€‚å®ƒé‡‡ç”¨äº†æ–°é¢–çš„è·¨è¯­è¨€æ–‡æ¡£æ³¨æ„åŠ›æœºåˆ¶ï¼ˆXLDAï¼‰ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å°†çŸ¥è¯†ä»è‹±è¯­è½¬ç§»åˆ°ç›®æ ‡è¯­è¨€ï¼Œå¦‚éŸ©è¯­å’Œæ—¥è¯­ã€‚é€šè¿‡ä¼˜åŒ–æ•°æ®æ··åˆã€è¯­è¨€ç‰¹å®šè¿‡æ»¤å’Œå®šåˆ¶çš„åˆ†è¯å™¨æ„å»ºï¼ŒTrillion-7B åœ¨ä»…ä½¿ç”¨ 10% çš„å¤šè¯­è¨€æ•°æ®çš„æƒ…å†µä¸‹ï¼Œä»èƒ½å®ç°ç«äº‰åŠ›çš„æ€§èƒ½ã€‚ç»è¿‡å¯¹ 27 ä¸ªåŸºå‡†æµ‹è¯•çš„å…¨é¢è¯„ä¼°ï¼ŒTrillion-7B å±•ç°äº†å¼ºå¤§çš„å¤šè¯­è¨€æ€§èƒ½å’Œå“è¶Šçš„è·¨è¯­è¨€ä¸€è‡´æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.16929",
            "title": "I-Con: A Unifying Framework for Representation Learning",
            "url": "https://huggingface.co/papers/2504.16929",
            "abstract": "As the field of representation learning grows, there has been a proliferation of different loss functions to solve different classes of problems. We introduce a single information-theoretic equation that generalizes a large collection of modern loss functions in machine learning. In particular, we introduce a framework that shows that several broad classes of machine learning methods are precisely minimizing an integrated KL divergence between two conditional distributions: the supervisory and learned representations. This viewpoint exposes a hidden information geometry underlying clustering, spectral methods, dimensionality reduction, contrastive learning, and supervised learning. This framework enables the development of new loss functions by combining successful techniques from across the literature. We not only present a wide array of proofs, connecting over 23 different approaches, but we also leverage these theoretical results to create state-of-the-art unsupervised image classifiers that achieve a +8% improvement over the prior state-of-the-art on unsupervised classification on ImageNet-1K. We also demonstrate that I-Con can be used to derive principled debiasing methods which improve contrastive representation learners.",
            "score": 21,
            "issue_id": 3406,
            "pub_date": "2025-04-23",
            "pub_date_card": {
                "ru": "23 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 23",
                "zh": "4æœˆ23æ—¥"
            },
            "hash": "0896fcd8d4d48d98",
            "authors": [
                "Shaden Alshammari",
                "John Hershey",
                "Axel Feldmann",
                "William T. Freeman",
                "Mark Hamilton"
            ],
            "affiliations": [
                "Google",
                "MIT",
                "Microsoft"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.16929.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#math",
                    "#interpretability",
                    "#cv",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ñ‚ĞµĞ¾Ñ€Ğ¸Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞµĞ´Ğ¸Ğ½Ğ¾Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾-Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ, Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞµ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ KL-Ğ´Ğ¸Ğ²ĞµÑ€Ğ³ĞµĞ½Ñ†Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞºÑ€Ñ‹Ñ‚ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ, Ğ»ĞµĞ¶Ğ°Ñ‰ÑƒÑ Ğ² Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ supervised Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ¾Ğ¹ Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ Ğ½ĞµÑÑƒĞ¿ĞµÑ€Ğ²Ğ¸Ğ·Ğ¾Ñ€Ğ½Ğ¾Ğ¹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ImageNet-1K."
                },
                "en": {
                    "title": "Unifying Loss Functions through Information Geometry",
                    "desc": "This paper presents a unified information-theoretic framework that generalizes various loss functions used in machine learning. It shows that many methods minimize an integrated KL divergence between supervisory and learned representations, revealing a deeper information geometry in techniques like clustering and dimensionality reduction. The authors provide proofs connecting over 23 different approaches and use these insights to develop advanced unsupervised image classifiers, achieving significant performance improvements. Additionally, the framework aids in creating effective debiasing methods for contrastive representation learning."
                },
                "zh": {
                    "title": "ç»Ÿä¸€æŸå¤±å‡½æ•°ï¼Œæå‡è¡¨ç¤ºå­¦ä¹ çš„åŠ›é‡",
                    "desc": "éšç€è¡¨ç¤ºå­¦ä¹ é¢†åŸŸçš„å‘å±•ï¼Œå‡ºç°äº†è®¸å¤šä¸åŒçš„æŸå¤±å‡½æ•°æ¥è§£å†³å„ç§é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä¿¡æ¯è®ºæ–¹ç¨‹ï¼Œå®ƒå¯ä»¥æ¦‚æ‹¬ç°ä»£æœºå™¨å­¦ä¹ ä¸­å¤§é‡çš„æŸå¤±å‡½æ•°ã€‚ç‰¹åˆ«åœ°ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å‡ ç§å¹¿æ³›çš„æœºå™¨å­¦ä¹ æ–¹æ³•å®é™…ä¸Šæ˜¯åœ¨æœ€å°åŒ–ä¸¤ä¸ªæ¡ä»¶åˆ†å¸ƒä¹‹é—´çš„é›†æˆKLæ•£åº¦ï¼šç›‘ç£åˆ†å¸ƒå’Œå­¦ä¹ åˆ°çš„è¡¨ç¤ºã€‚è¿™ä¸ªæ¡†æ¶ä¸ä»…è¿æ¥äº†23ç§ä¸åŒçš„æ–¹æ³•ï¼Œè¿˜å¸®åŠ©æˆ‘ä»¬å¼€å‘äº†æ–°çš„æ— ç›‘ç£å›¾åƒåˆ†ç±»å™¨ï¼Œæ˜¾è‘—æé«˜äº†åˆ†ç±»æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.15843",
            "title": "Pre-DPO: Improving Data Utilization in Direct Preference Optimization\n  Using a Guiding Reference Model",
            "url": "https://huggingface.co/papers/2504.15843",
            "abstract": "Direct Preference Optimization (DPO) simplifies reinforcement learning from human feedback (RLHF) for large language models (LLMs) by directly optimizing human preferences without an explicit reward model. We find that during DPO training, the reference model plays the role of a data weight adjuster. However, the common practice of initializing the policy and reference models identically in DPO can lead to inefficient data utilization and impose a performance ceiling. Meanwhile, the lack of a reference model in Simple Preference Optimization (SimPO) reduces training robustness and necessitates stricter conditions to prevent catastrophic forgetting. In this work, we propose Pre-DPO, a simple yet effective DPO-based training paradigm that enhances preference optimization performance by leveraging a guiding reference model. This reference model provides foresight into the optimal policy state achievable through the training preference data, serving as a guiding mechanism that adaptively assigns higher weights to samples more suitable for the model and lower weights to those less suitable. Extensive experiments on AlpacaEval 2.0 and Arena-Hard v0.1 benchmarks demonstrate that Pre-DPO consistently improves the performance of both DPO and SimPO, without relying on external models or additional data.",
            "score": 16,
            "issue_id": 3403,
            "pub_date": "2025-04-22",
            "pub_date_card": {
                "ru": "22 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 22",
                "zh": "4æœˆ22æ—¥"
            },
            "hash": "e8cb4456a20efe2a",
            "authors": [
                "Junshu Pan",
                "Wei Shen",
                "Shulin Huang",
                "Qiji Zhou",
                "Yue Zhang"
            ],
            "affiliations": [
                "Independent Researcher",
                "School of Engineering, Westlake University",
                "Shanghai Innovation Institute",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.15843.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#rlhf",
                    "#alignment",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Pre-DPO: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Pre-DPO - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. Pre-DPO ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ DPO Ğ¸ SimPO, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€ÑƒÑÑ‰ÑƒÑ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞµÑĞ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ‚ÑŒ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… AlpacaEval 2.0 Ğ¸ Arena-Hard v0.1 Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Pre-DPO Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¸Ğ»Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Enhancing Preference Learning with Pre-DPO",
                    "desc": "This paper introduces Pre-DPO, a new training method for large language models that enhances Direct Preference Optimization (DPO) by using a guiding reference model. The reference model helps adjust the importance of training data, allowing the model to focus on more relevant samples and improve learning efficiency. The authors highlight that traditional methods can lead to poor performance due to identical initialization of models and lack of robustness in simpler approaches. Through experiments, they show that Pre-DPO outperforms existing methods without needing extra data or external models."
                },
                "zh": {
                    "title": "æå‡åå¥½ä¼˜åŒ–æ€§èƒ½çš„Pre-DPOæ–¹æ³•",
                    "desc": "ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰é€šè¿‡ç›´æ¥ä¼˜åŒ–äººç±»åå¥½ï¼Œç®€åŒ–äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨DPOè®­ç»ƒä¸­ï¼Œå‚è€ƒæ¨¡å‹å……å½“æ•°æ®æƒé‡è°ƒæ•´å™¨ï¼Œä½†å¸¸è§çš„å°†ç­–ç•¥æ¨¡å‹å’Œå‚è€ƒæ¨¡å‹åˆå§‹åŒ–ä¸ºç›¸åŒçš„åšæ³•å¯èƒ½å¯¼è‡´æ•°æ®åˆ©ç”¨æ•ˆç‡ä½ä¸‹ã€‚æˆ‘ä»¬æå‡ºçš„Pre-DPOè®­ç»ƒèŒƒå¼ï¼Œé€šè¿‡åˆ©ç”¨æŒ‡å¯¼æ€§å‚è€ƒæ¨¡å‹ï¼Œå¢å¼ºäº†åå¥½ä¼˜åŒ–çš„æ€§èƒ½ï¼Œèƒ½å¤Ÿè‡ªé€‚åº”åœ°ä¸ºæ›´é€‚åˆæ¨¡å‹çš„æ ·æœ¬åˆ†é…æ›´é«˜çš„æƒé‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPre-DPOåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æŒç»­æå‡äº†DPOå’ŒSimPOçš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.15777",
            "title": "Tina: Tiny Reasoning Models via LoRA",
            "url": "https://huggingface.co/papers/2504.15777",
            "abstract": "How cost-effectively can strong reasoning abilities be achieved in language models? Driven by this fundamental question, we present Tina, a family of tiny reasoning models achieved with high cost-efficiency. Notably, Tina demonstrates that substantial reasoning performance can be developed using only minimal resources, by applying parameter-efficient updates during reinforcement learning (RL), using low-rank adaptation (LoRA), to an already tiny 1.5B parameter base model. This minimalist approach produces models that achieve reasoning performance which is competitive with, and sometimes surpasses, SOTA RL reasoning models built upon the same base model. Crucially, this is achieved at a tiny fraction of the computational post-training cost employed by existing SOTA models. In fact, the best Tina model achieves a >20\\% reasoning performance increase and 43.33\\% Pass@1 accuracy on AIME24, at only \\$9 USD post-training and evaluation cost (i.e., an estimated 260x cost reduction). Our work reveals the surprising effectiveness of efficient RL reasoning via LoRA. We validate this across multiple open-source reasoning datasets and various ablation settings starting with a single, fixed set of hyperparameters. Furthermore, we hypothesize that this effectiveness and efficiency stem from LoRA rapidly adapting the model to the structural format of reasoning rewarded by RL, while largely preserving the base model's underlying knowledge. In service of accessibility and open research, we fully open-source all code, training logs, and model weights \\& checkpoints.",
            "score": 15,
            "issue_id": 3410,
            "pub_date": "2025-04-22",
            "pub_date_card": {
                "ru": "22 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 22",
                "zh": "4æœˆ22æ—¥"
            },
            "hash": "684e9f5f755cf0c6",
            "authors": [
                "Shangshang Wang",
                "Julian Asilis",
                "Ã–mer Faruk AkgÃ¼l",
                "Enes Burak Bilgin",
                "Ollie Liu",
                "Willie Neiswanger"
            ],
            "affiliations": [
                "University of Southern California"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.15777.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#training",
                    "#optimization",
                    "#rl",
                    "#open_source",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Tina, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ…. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸-ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ²ÑƒÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ (LoRA), Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€ÑƒÑÑ‰ĞµĞ¹ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ›ÑƒÑ‡ÑˆĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Tina Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 20% ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ… Ğ²ÑĞµĞ³Ğ¾ $9 Ğ½Ğ° Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºÑƒ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ²ÑĞ·Ğ°Ğ½Ğ° Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¼Ñƒ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´Ğ°ĞµĞ¼Ğ¾Ğ¼Ñƒ RL, Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Tiny Models, Big Reasoning: Cost-Effective AI with Tina",
                    "desc": "This paper introduces Tina, a series of compact reasoning models that achieve strong reasoning capabilities with minimal resources. By utilizing low-rank adaptation (LoRA) during reinforcement learning (RL), Tina enhances a small 1.5B parameter base model, demonstrating that effective reasoning can be achieved cost-effectively. The models not only compete with but sometimes outperform state-of-the-art (SOTA) RL reasoning models, all while significantly reducing post-training costs. The findings suggest that LoRA effectively tailors the model for reasoning tasks, maintaining the base model's knowledge while improving performance."
                },
                "zh": {
                    "title": "é«˜æ•ˆæ¨ç†ï¼Œä½æˆæœ¬å®ç°",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºTinaçš„å°å‹æ¨ç†æ¨¡å‹ç³»åˆ—ï¼Œæ—¨åœ¨ä»¥é«˜æ€§ä»·æ¯”å®ç°å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚Tinaé€šè¿‡åœ¨ä¸€ä¸ªä»…æœ‰1.5äº¿å‚æ•°çš„åŸºç¡€æ¨¡å‹ä¸Šï¼Œé‡‡ç”¨ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œå‚æ•°é«˜æ•ˆæ›´æ–°ï¼Œå±•ç¤ºäº†åœ¨èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ä»èƒ½è·å¾—æ˜¾è‘—çš„æ¨ç†æ€§èƒ½ã€‚ä¸ç°æœ‰çš„æœ€å…ˆè¿›ï¼ˆSOTAï¼‰æ¨¡å‹ç›¸æ¯”ï¼ŒTinaåœ¨æ¨ç†æ€§èƒ½ä¸Šå…·æœ‰ç«äº‰åŠ›ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹è¶…è¶Šäº†è¿™äº›æ¨¡å‹ï¼ŒåŒæ—¶å…¶åæœŸè®­ç»ƒå’Œè¯„ä¼°æˆæœ¬ä»…ä¸º9ç¾å…ƒï¼ŒèŠ‚çœäº†çº¦260å€çš„æˆæœ¬ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼ŒLoRAåœ¨å¿«é€Ÿé€‚åº”æ¨ç†ç»“æ„çš„åŒæ—¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆä¿ç•™åŸºç¡€æ¨¡å‹çš„çŸ¥è¯†ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„æ¨ç†èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.16801",
            "title": "Decoupled Global-Local Alignment for Improving Compositional\n  Understanding",
            "url": "https://huggingface.co/papers/2504.16801",
            "abstract": "Contrastive Language-Image Pre-training (CLIP) has achieved success on multiple downstream tasks by aligning image and text modalities. However, the nature of global contrastive learning limits CLIP's ability to comprehend compositional concepts, such as relations and attributes. Although recent studies employ global hard negative samples to improve compositional understanding, these methods significantly compromise the model's inherent general capabilities by forcibly distancing textual negative samples from images in the embedding space. To overcome this limitation, we introduce a Decoupled Global-Local Alignment (DeGLA) framework that improves compositional understanding while substantially mitigating losses in general capabilities. To optimize the retention of the model's inherent capabilities, we incorporate a self-distillation mechanism within the global alignment process, aligning the learnable image-text encoder with a frozen teacher model derived from an exponential moving average. Under the constraint of self-distillation, it effectively mitigates the catastrophic forgetting of pretrained knowledge during fine-tuning. To improve compositional understanding, we first leverage the in-context learning capability of Large Language Models (LLMs) to construct about 2M high-quality negative captions across five types. Subsequently, we propose the Image-Grounded Contrast (IGC) loss and Text-Grounded Contrast (TGC) loss to enhance vision-language compositionally. Extensive experimental results demonstrate the effectiveness of the DeGLA framework. Compared to previous state-of-the-art methods, DeGLA achieves an average enhancement of 3.5% across the VALSE, SugarCrepe, and ARO benchmarks. Concurrently, it obtains an average performance improvement of 13.0% on zero-shot classification tasks across eleven datasets. Our code will be released at https://github.com/xiaoxing2001/DeGLA",
            "score": 14,
            "issue_id": 3406,
            "pub_date": "2025-04-23",
            "pub_date_card": {
                "ru": "23 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 23",
                "zh": "4æœˆ23æ—¥"
            },
            "hash": "c50b5cc723595a69",
            "authors": [
                "Xiaoxing Hu",
                "Kaicheng Yang",
                "Jun Wang",
                "Haoran Xu",
                "Ziyong Feng",
                "Yupei Wang"
            ],
            "affiliations": [
                "Beijing Institute of Technology, Beijing, China",
                "DeepGlint, Beijing, China",
                "Zhejiang University, Zhejiang Province, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.16801.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#training",
                    "#alignment",
                    "#multimodal",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "DeGLA: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹",
                    "desc": "Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ DeGLA (Decoupled Global-Local Alignment) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‚Ğ¸Ğ¿Ğ° CLIP. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ°Ğ¼Ğ¾Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ IGC Ğ¸ TGC Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Compositional Understanding in Vision-Language Models with DeGLA",
                    "desc": "The paper introduces a new framework called Decoupled Global-Local Alignment (DeGLA) to enhance the compositional understanding of images and text in machine learning models. It addresses the limitations of existing methods that use global contrastive learning, which can hinder a model's general capabilities. By incorporating a self-distillation mechanism, DeGLA retains the model's pretrained knowledge while improving its ability to understand complex relationships and attributes in data. The framework also utilizes advanced techniques to generate high-quality negative samples, leading to significant performance improvements on various benchmarks."
                },
                "zh": {
                    "title": "å»è€¦åˆå…¨å±€-å±€éƒ¨å¯¹é½æå‡ç»„åˆç†è§£èƒ½åŠ›",
                    "desc": "å¯¹æ¯”è¯­è¨€-å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­å–å¾—äº†æˆåŠŸï¼Œä½†å…¶å…¨å±€å¯¹æ¯”å­¦ä¹ çš„ç‰¹æ€§é™åˆ¶äº†å…¶ç†è§£ç»„åˆæ¦‚å¿µçš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å»è€¦åˆçš„å…¨å±€-å±€éƒ¨å¯¹é½ï¼ˆDeGLAï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜ç»„åˆç†è§£èƒ½åŠ›ï¼ŒåŒæ—¶å‡å°‘å¯¹æ¨¡å‹ä¸€èˆ¬èƒ½åŠ›çš„æŸå¤±ã€‚æˆ‘ä»¬åœ¨å…¨å±€å¯¹é½è¿‡ç¨‹ä¸­å¼•å…¥è‡ªè’¸é¦æœºåˆ¶ï¼Œä»¥ä¼˜åŒ–æ¨¡å‹å›ºæœ‰èƒ½åŠ›çš„ä¿ç•™ï¼Œå¹¶æœ‰æ•ˆå‡è½»åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­é¢„è®­ç»ƒçŸ¥è¯†çš„ç¾éš¾æ€§é—å¿˜ã€‚é€šè¿‡æ„å»ºé«˜è´¨é‡çš„è´Ÿæ ·æœ¬å’Œå¼•å…¥æ–°çš„æŸå¤±å‡½æ•°ï¼ŒDeGLAåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.16915",
            "title": "DreamO: A Unified Framework for Image Customization",
            "url": "https://huggingface.co/papers/2504.16915",
            "abstract": "Recently, extensive research on image customization (e.g., identity, subject, style, background, etc.) demonstrates strong customization capabilities in large-scale generative models. However, most approaches are designed for specific tasks, restricting their generalizability to combine different types of condition. Developing a unified framework for image customization remains an open challenge. In this paper, we present DreamO, an image customization framework designed to support a wide range of tasks while facilitating seamless integration of multiple conditions. Specifically, DreamO utilizes a diffusion transformer (DiT) framework to uniformly process input of different types. During training, we construct a large-scale training dataset that includes various customization tasks, and we introduce a feature routing constraint to facilitate the precise querying of relevant information from reference images. Additionally, we design a placeholder strategy that associates specific placeholders with conditions at particular positions, enabling control over the placement of conditions in the generated results. Moreover, we employ a progressive training strategy consisting of three stages: an initial stage focused on simple tasks with limited data to establish baseline consistency, a full-scale training stage to comprehensively enhance the customization capabilities, and a final quality alignment stage to correct quality biases introduced by low-quality data. Extensive experiments demonstrate that the proposed DreamO can effectively perform various image customization tasks with high quality and flexibly integrate different types of control conditions.",
            "score": 13,
            "issue_id": 3405,
            "pub_date": "2025-04-23",
            "pub_date_card": {
                "ru": "23 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 23",
                "zh": "4æœˆ23æ—¥"
            },
            "hash": "86de2482f329ee9f",
            "authors": [
                "Chong Mou",
                "Yanze Wu",
                "Wenxu Wu",
                "Zinan Guo",
                "Pengze Zhang",
                "Yufeng Cheng",
                "Yiming Luo",
                "Fei Ding",
                "Shiwen Zhang",
                "Xinghui Li",
                "Mengtian Li",
                "Songtao Zhao",
                "Jian Zhang",
                "Qian He",
                "Xinglong Wu"
            ],
            "affiliations": [
                "Intelligent Creation Team, ByteDance",
                "School of Electronic and Computer Engineering, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.16915.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#cv",
                    "#dataset",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "DreamO: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ ĞºĞ°ÑÑ‚Ğ¾Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²",
                    "desc": "DreamO - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ĞºĞ°ÑÑ‚Ğ¾Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°. ĞĞ½Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ñ…. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ² Ñ‚Ñ€Ğ¸ ÑÑ‚Ğ°Ğ¿Ğ° Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ ĞºĞ°ÑÑ‚Ğ¾Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ DreamO ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ¾ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼."
                },
                "en": {
                    "title": "DreamO: Unifying Image Customization with Flexibility and Precision",
                    "desc": "This paper introduces DreamO, a novel framework for image customization that allows for the integration of multiple conditions such as identity, style, and background. Unlike previous models that are limited to specific tasks, DreamO employs a diffusion transformer (DiT) to uniformly handle diverse input types. The framework is trained on a large dataset with a feature routing constraint to ensure accurate information retrieval from reference images. Additionally, a progressive training strategy is implemented to enhance customization capabilities and improve output quality, demonstrating DreamO's effectiveness across various image customization tasks."
                },
                "zh": {
                    "title": "DreamOï¼šå¤šä»»åŠ¡å›¾åƒå®šåˆ¶çš„ç»Ÿä¸€æ¡†æ¶",
                    "desc": "æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå¤§è§„æ¨¡ç”Ÿæˆæ¨¡å‹åœ¨å›¾åƒå®šåˆ¶æ–¹é¢å…·æœ‰å¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†å¤§å¤šæ•°æ–¹æ³•ä»…é’ˆå¯¹ç‰¹å®šä»»åŠ¡ï¼Œé™åˆ¶äº†å…¶é€šç”¨æ€§ã€‚æœ¬æ–‡æå‡ºäº†DreamOï¼Œä¸€ä¸ªæ”¯æŒå¤šç§ä»»åŠ¡çš„å›¾åƒå®šåˆ¶æ¡†æ¶ï¼Œèƒ½å¤Ÿæ— ç¼æ•´åˆå¤šç§æ¡ä»¶ã€‚DreamOé‡‡ç”¨æ‰©æ•£å˜æ¢å™¨ï¼ˆDiTï¼‰æ¡†æ¶ï¼Œç»Ÿä¸€å¤„ç†ä¸åŒç±»å‹çš„è¾“å…¥ï¼Œå¹¶é€šè¿‡ç‰¹å¾è·¯ç”±çº¦æŸç²¾ç¡®æŸ¥è¯¢å‚è€ƒå›¾åƒä¸­çš„ç›¸å…³ä¿¡æ¯ã€‚å®éªŒè¡¨æ˜ï¼ŒDreamOèƒ½å¤Ÿé«˜è´¨é‡åœ°æ‰§è¡Œå„ç§å›¾åƒå®šåˆ¶ä»»åŠ¡ï¼Œå¹¶çµæ´»æ•´åˆä¸åŒç±»å‹çš„æ§åˆ¶æ¡ä»¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.16074",
            "title": "PHYBench: Holistic Evaluation of Physical Perception and Reasoning in\n  Large Language Models",
            "url": "https://huggingface.co/papers/2504.16074",
            "abstract": "We introduce PHYBench, a novel, high-quality benchmark designed for evaluating reasoning capabilities of large language models (LLMs) in physical contexts. PHYBench consists of 500 meticulously curated physics problems based on real-world physical scenarios, designed to assess the ability of models to understand and reason about realistic physical processes. Covering mechanics, electromagnetism, thermodynamics, optics, modern physics, and advanced physics, the benchmark spans difficulty levels from high school exercises to undergraduate problems and Physics Olympiad challenges. Additionally, we propose the Expression Edit Distance (EED) Score, a novel evaluation metric based on the edit distance between mathematical expressions, which effectively captures differences in model reasoning processes and results beyond traditional binary scoring methods. We evaluate various LLMs on PHYBench and compare their performance with human experts. Our results reveal that even state-of-the-art reasoning models significantly lag behind human experts, highlighting their limitations and the need for improvement in complex physical reasoning scenarios. Our benchmark results and dataset are publicly available at https://phybench-official.github.io/phybench-demo/.",
            "score": 13,
            "issue_id": 3417,
            "pub_date": "2025-04-22",
            "pub_date_card": {
                "ru": "22 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 22",
                "zh": "4æœˆ22æ—¥"
            },
            "hash": "5357bfeea528614a",
            "authors": [
                "Shi Qiu",
                "Shaoyang Guo",
                "Zhuo-Yang Song",
                "Yunbo Sun",
                "Zeyu Cai",
                "Jiashen Wei",
                "Tianyu Luo",
                "Yixuan Yin",
                "Haoxu Zhang",
                "Yi Hu",
                "Chenyang Wang",
                "Chencheng Tang",
                "Haoling Chang",
                "Qi Liu",
                "Ziheng Zhou",
                "Tianyu Zhang",
                "Jingtian Zhang",
                "Zhangyi Liu",
                "Minghao Li",
                "Yuku Zhang",
                "Boxuan Jing",
                "Xianqi Yin",
                "Yutong Ren",
                "Zizhuo Fu",
                "Weike Wang",
                "Xudong Tian",
                "Anqi Lv",
                "Laifu Man",
                "Jianxiang Li",
                "Feiyu Tao",
                "Qihua Sun",
                "Zhou Liang",
                "Yushu Mu",
                "Zhongxuan Li",
                "Jing-Jun Zhang",
                "Shutao Zhang",
                "Xiaotian Li",
                "Xingqi Xia",
                "Jiawei Lin",
                "Zheyu Shen",
                "Jiahang Chen",
                "Qiuhao Xiong",
                "Binran Wang",
                "Fengyuan Wang",
                "Ziyang Ni",
                "Bohan Zhang",
                "Fan Cui",
                "Changkun Shao",
                "Qing-Hong Cao",
                "Ming-xing Luo",
                "Muhan Zhang",
                "Hua Xing Zhu"
            ],
            "affiliations": [
                "Beijing Computational Science Research Center",
                "Institute for Artificial Intelligence, Peking University",
                "School of Integrated Circuits, Peking University",
                "School of Physics, Peking University",
                "Yuanpei College, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.16074.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#survey",
                    "#math",
                    "#dataset"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "PHYBench: ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°",
                    "desc": "PHYBench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ² Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ. ĞĞ½ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 500 Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ Ñ„Ğ¸Ğ·Ğ¸ĞºĞµ, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸ĞºĞ¸ Ğ´Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ - Expression Edit Distance (EED) Score, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ñ€ĞµĞ´Ğ°ĞºÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ñ€Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚ÑÑ‚Ğ°ÑÑ‚ Ğ¾Ñ‚ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ²-Ğ»ÑĞ´ĞµĞ¹ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Evaluating LLMs in Physics: Bridging the Reasoning Gap",
                    "desc": "PHYBench is a new benchmark created to test how well large language models (LLMs) can reason about physics. It includes 500 carefully selected physics problems that reflect real-world situations, covering various topics like mechanics and thermodynamics. The benchmark is designed for different skill levels, from high school to advanced physics challenges. To evaluate the models, a new metric called Expression Edit Distance (EED) Score is introduced, which measures the differences in reasoning by comparing mathematical expressions, showing that current models still fall short compared to human experts in complex reasoning tasks."
                },
                "zh": {
                    "title": "è¯„ä¼°ç‰©ç†æ¨ç†èƒ½åŠ›çš„æ–°åŸºå‡†",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†PHYBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„é«˜è´¨é‡åŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç‰©ç†æƒ…å¢ƒä¸‹çš„æ¨ç†èƒ½åŠ›ã€‚PHYBenchåŒ…å«500ä¸ªç²¾å¿ƒç­–åˆ’çš„ç‰©ç†é—®é¢˜ï¼ŒåŸºäºçœŸå®çš„ç‰©ç†åœºæ™¯ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹ç†è§£å’Œæ¨ç†ç°å®ç‰©ç†è¿‡ç¨‹çš„èƒ½åŠ›ã€‚åŸºå‡†æ¶µç›–äº†ä»é«˜ä¸­ç»ƒä¹ åˆ°æœ¬ç§‘é—®é¢˜å’Œç‰©ç†å¥¥æ—åŒ¹å…‹æŒ‘æˆ˜çš„å¤šä¸ªéš¾åº¦çº§åˆ«ï¼Œæ¶‰åŠåŠ›å­¦ã€ç”µç£å­¦ã€çƒ­åŠ›å­¦ã€å…‰å­¦ã€ç°ä»£ç‰©ç†å’Œé«˜çº§ç‰©ç†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†è¡¨è¾¾ç¼–è¾‘è·ç¦»ï¼ˆEEDï¼‰è¯„åˆ†ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„è¯„ä¼°æŒ‡æ ‡ï¼ŒåŸºäºæ•°å­¦è¡¨è¾¾å¼ä¹‹é—´çš„ç¼–è¾‘è·ç¦»ï¼Œæœ‰æ•ˆæ•æ‰æ¨¡å‹æ¨ç†è¿‡ç¨‹å’Œç»“æœçš„å·®å¼‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.15585",
            "title": "A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training\n  and Deployment",
            "url": "https://huggingface.co/papers/2504.15585",
            "abstract": "The remarkable success of Large Language Models (LLMs) has illuminated a promising pathway toward achieving Artificial General Intelligence for both academic and industrial communities, owing to their unprecedented performance across various applications. As LLMs continue to gain prominence in both research and commercial domains, their security and safety implications have become a growing concern, not only for researchers and corporations but also for every nation. Currently, existing surveys on LLM safety primarily focus on specific stages of the LLM lifecycle, e.g., deployment phase or fine-tuning phase, lacking a comprehensive understanding of the entire \"lifechain\" of LLMs. To address this gap, this paper introduces, for the first time, the concept of \"full-stack\" safety to systematically consider safety issues throughout the entire process of LLM training, deployment, and eventual commercialization. Compared to the off-the-shelf LLM safety surveys, our work demonstrates several distinctive advantages: (I) Comprehensive Perspective. We define the complete LLM lifecycle as encompassing data preparation, pre-training, post-training, deployment and final commercialization. To our knowledge, this represents the first safety survey to encompass the entire lifecycle of LLMs. (II) Extensive Literature Support. Our research is grounded in an exhaustive review of over 800+ papers, ensuring comprehensive coverage and systematic organization of security issues within a more holistic understanding. (III) Unique Insights. Through systematic literature analysis, we have developed reliable roadmaps and perspectives for each chapter. Our work identifies promising research directions, including safety in data generation, alignment techniques, model editing, and LLM-based agent systems. These insights provide valuable guidance for researchers pursuing future work in this field.",
            "score": 9,
            "issue_id": 3406,
            "pub_date": "2025-04-22",
            "pub_date_card": {
                "ru": "22 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 22",
                "zh": "4æœˆ22æ—¥"
            },
            "hash": "3eb03bb75acef757",
            "authors": [
                "Kun Wang",
                "Guibin Zhang",
                "Zhenhong Zhou",
                "Jiahao Wu",
                "Miao Yu",
                "Shiqian Zhao",
                "Chenlong Yin",
                "Jinhu Fu",
                "Yibo Yan",
                "Hanjun Luo",
                "Liang Lin",
                "Zhihao Xu",
                "Haolang Lu",
                "Xinye Cao",
                "Xinyun Zhou",
                "Weifei Jin",
                "Fanci Meng",
                "Junyuan Mao",
                "Hao Wu",
                "Minghe Wang",
                "Fan Zhang",
                "Junfeng Fang",
                "Chengwei Liu",
                "Yifan Zhang",
                "Qiankun Li",
                "Chongye Guo",
                "Yalan Qin",
                "Yi Ding",
                "Donghai Hong",
                "Jiaming Ji",
                "Xinfeng Li",
                "Yifan Jiang",
                "Dongxia Wang",
                "Yihao Huang",
                "Yufei Guo",
                "Jen-tse Huang",
                "Yanwei Yue",
                "Wenke Huang",
                "Guancheng Wan",
                "Tianlin Li",
                "Lei Bai",
                "Jie Zhang",
                "Qing Guo",
                "Jingyi Wang",
                "Tianlong Chen",
                "Joey Tianyi Zhou",
                "Xiaojun Jia",
                "Weisong Sun",
                "Cong Wu",
                "Jing Chen",
                "Xuming Hu",
                "Yiming Li",
                "Xiao Wang",
                "Ningyu Zhang",
                "Luu Anh Tuan",
                "Guowen Xu",
                "Tianwei Zhang",
                "Xingjun Ma",
                "Xiang Wang",
                "Bo An",
                "Jun Sun",
                "Mohit Bansal",
                "Shirui Pan",
                "Yuval Elovici",
                "Bhavya Kailkhura",
                "Bo Li",
                "Yaodong Yang",
                "Hongwei Li",
                "Wenyuan Xu",
                "Yizhou Sun",
                "Wei Wang",
                "Qing Li",
                "Ke Tang",
                "Yu-Gang Jiang",
                "Felix Juefei-Xu",
                "Hui Xiong",
                "Xiaofeng Wang",
                "Shuicheng Yan",
                "Dacheng Tao",
                "Philip S. Yu",
                "Qingsong Wen",
                "Yang Liu"
            ],
            "affiliations": [
                "A*STAR",
                "Georgia Institute of Technology",
                "Hong Kong University of Science and Technology",
                "Hong Kong University of Science and Technology (Guangzhou)",
                "Institute of Automation, Chinese Academy of Sciences",
                "Institute of Information Engineering, Chinese Academy of Sciences",
                "Nanyang Technological University",
                "National University of Singapore",
                "Peking University",
                "Renmin University of China",
                "Shanghai AI Laboratory",
                "Shanghai University",
                "Southern University of Science and Technology",
                "Squirrel AI Learning",
                "TeleAI",
                "Tencent",
                "The Hong Kong Polytechnic University",
                "The Pennsylvania State University",
                "University of Science and Technology of China",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.15585.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#survey",
                    "#alignment",
                    "#agi",
                    "#agents",
                    "#security",
                    "#data"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "ĞŸĞ¾Ğ»Ğ½Ğ¾ÑÑ‚ĞµĞºĞ¾Ğ²Ğ°Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ LLM: Ğ¾Ñ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ 'Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ĞµĞºĞ¾Ğ²Ğ¾Ğ¹' Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ Ğ²ĞµÑÑŒ Ğ¶Ğ¸Ğ·Ğ½ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ†Ğ¸ĞºĞ» Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¾ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ±Ğ¾Ğ»ĞµĞµ 800 Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚, ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ†ĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Ensuring Safety Across the Entire Lifecycle of Large Language Models",
                    "desc": "This paper discusses the safety and security concerns related to Large Language Models (LLMs) as they become more widely used in various applications. It introduces the concept of 'full-stack' safety, which looks at the entire lifecycle of LLMs, from data preparation to commercialization, rather than focusing on just one phase. The authors conducted a thorough review of over 800 research papers to provide a comprehensive understanding of safety issues throughout this lifecycle. They also offer unique insights and research directions for improving safety in areas like data generation and model alignment."
                },
                "zh": {
                    "title": "å…¨é¢å…³æ³¨å¤§å‹è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æˆåŠŸä¸ºå®ç°äººå·¥é€šç”¨æ™ºèƒ½æä¾›äº†æ–°çš„å¯èƒ½æ€§ï¼Œä½†å…¶å®‰å…¨æ€§é—®é¢˜æ—¥ç›Šå—åˆ°å…³æ³¨ã€‚ç°æœ‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨LLMç”Ÿå‘½å‘¨æœŸçš„ç‰¹å®šé˜¶æ®µï¼Œç¼ºä¹å¯¹æ•´ä¸ªç”Ÿå‘½å‘¨æœŸçš„å…¨é¢ç†è§£ã€‚æœ¬æ–‡é¦–æ¬¡æå‡ºäº†â€œå…¨æ ˆå®‰å…¨â€çš„æ¦‚å¿µï¼Œç³»ç»Ÿåœ°è€ƒè™‘LLMè®­ç»ƒã€éƒ¨ç½²å’Œå•†ä¸šåŒ–è¿‡ç¨‹ä¸­çš„å®‰å…¨é—®é¢˜ã€‚é€šè¿‡å¯¹800å¤šç¯‡æ–‡çŒ®çš„æ·±å…¥åˆ†æï¼Œæˆ‘ä»¬ä¸ºæœªæ¥çš„ç ”ç©¶æ–¹å‘æä¾›äº†å¯é çš„è·¯çº¿å›¾å’Œè§è§£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.11919",
            "title": "Rethinking the Generation of High-Quality CoT Data from the Perspective\n  of LLM-Adaptive Question Difficulty Grading",
            "url": "https://huggingface.co/papers/2504.11919",
            "abstract": "Recently, DeepSeek-R1 (671B) (DeepSeek-AIet al., 2025) has demonstrated its excellent reasoning ability in complex tasks and has publiclyshared its methodology. This provides potentially high-quality chain-of-thought (CoT) data for stimulating the reasoning abilities of small-sized large language models (LLMs). To generate high-quality CoT data for different LLMs, we seek an efficient method for generating high-quality CoT data with LLM-Adaptive questiondifficulty levels. First, we grade the difficulty of the questions according to the reasoning ability of the LLMs themselves and construct a LLM-Adaptive question database. Second, we sample the problem database based on a distribution of difficulty levels of the questions and then use DeepSeek-R1 (671B) (DeepSeek-AI et al., 2025) to generate the corresponding high-quality CoT data with correct answers. Thanks to the construction of CoT data with LLM-Adaptive difficulty levels, we have significantly reduced the cost of data generation and enhanced the efficiency of model supervised fine-tuning (SFT). Finally, we have validated the effectiveness and generalizability of the proposed method in the fields of complex mathematical competitions and code generation tasks. Notably, with only 2k high-quality mathematical CoT data, our ZMath-32B surpasses DeepSeek-Distill-32B in math reasoning task. Similarly, with only 2k high-quality code CoT data, our ZCode-32B surpasses DeepSeek-Distill-32B in code reasoning tasks.",
            "score": 9,
            "issue_id": 3411,
            "pub_date": "2025-04-16",
            "pub_date_card": {
                "ru": "16 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 16",
                "zh": "4æœˆ16æ—¥"
            },
            "hash": "902a4c2acec9b9d4",
            "authors": [
                "Qianjin Yu",
                "Keyu Wu",
                "Zihan Chen",
                "Chushu Zhang",
                "Manlin Mei",
                "Lingjun Huang",
                "Fang Tan",
                "Yongsheng Du",
                "Kunlin Liu",
                "Yurui Zhu"
            ],
            "affiliations": [
                "Intelligent System Department, Zhongxing Telecom Equipment(ZTE), Changsha, Hunan, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.11919.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#dataset",
                    "#math",
                    "#optimization",
                    "#plp",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ¼Ğ°Ğ»Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ (chain-of-thought) Ğ¼Ğ°Ğ»Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ DeepSeek-R1 Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¼Ğ°Ğ»Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ Ğ±Ğ°Ğ·Ñƒ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¸Ñ… Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ° ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸ÑÑ‚ÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡."
                },
                "en": {
                    "title": "Adaptive CoT Data Generation for Enhanced LLM Reasoning",
                    "desc": "The paper introduces a method for generating high-quality chain-of-thought (CoT) data tailored to the reasoning abilities of different large language models (LLMs). It involves creating a database of questions graded by difficulty, which allows for adaptive sampling based on the LLM's capabilities. By using the DeepSeek-R1 model to generate CoT data from this adaptive question database, the authors significantly reduce the cost and improve the efficiency of supervised fine-tuning (SFT) for smaller models. The results show that their approach leads to superior performance in complex tasks like mathematical reasoning and code generation with minimal data requirements."
                },
                "zh": {
                    "title": "æå‡æ¨ç†èƒ½åŠ›çš„è‡ªé€‚åº”é“¾å¼æ€ç»´æ•°æ®ç”Ÿæˆ",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç”Ÿæˆé«˜è´¨é‡é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ•°æ®çš„æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜å°å‹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬é¦–å…ˆæ ¹æ®LLMsçš„æ¨ç†èƒ½åŠ›å¯¹é—®é¢˜è¿›è¡Œéš¾åº¦åˆ†çº§ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªè‡ªé€‚åº”é—®é¢˜æ•°æ®åº“ã€‚ç„¶åï¼Œæˆ‘ä»¬æ ¹æ®é—®é¢˜çš„éš¾åº¦åˆ†å¸ƒä»æ•°æ®åº“ä¸­æŠ½æ ·ï¼Œå¹¶ä½¿ç”¨DeepSeek-R1ç”Ÿæˆç›¸åº”çš„é«˜è´¨é‡CoTæ•°æ®ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬æ˜¾è‘—é™ä½äº†æ•°æ®ç”Ÿæˆæˆæœ¬ï¼Œå¹¶æé«˜äº†æ¨¡å‹çš„ç›‘ç£å¾®è°ƒæ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.15707",
            "title": "RePOPE: Impact of Annotation Errors on the POPE Benchmark",
            "url": "https://huggingface.co/papers/2504.15707",
            "abstract": "Since data annotation is costly, benchmark datasets often incorporate labels from established image datasets. In this work, we assess the impact of label errors in MSCOCO on the frequently used object hallucination benchmark POPE. We re-annotate the benchmark images and identify an imbalance in annotation errors across different subsets. Evaluating multiple models on the revised labels, which we denote as RePOPE, we observe notable shifts in model rankings, highlighting the impact of label quality. Code and data are available at https://github.com/YanNeu/RePOPE .",
            "score": 8,
            "issue_id": 3410,
            "pub_date": "2025-04-22",
            "pub_date_card": {
                "ru": "22 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 22",
                "zh": "4æœˆ22æ—¥"
            },
            "hash": "413cf886c426d830",
            "authors": [
                "Yannic Neuhaus",
                "Matthias Hein"
            ],
            "affiliations": [
                "Tubingen AI Center, University of Tubingen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.15707.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#benchmark",
                    "#open_source",
                    "#dataset",
                    "#data"
                ],
                "emoji": "ğŸ·ï¸",
                "ru": {
                    "title": "ĞšĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… MSCOCO Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ½Ñ‹Ñ… Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ POPE. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½ÑƒÑ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ Ğ½ĞµÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ°Ñ…. ĞŸÑ€Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚ĞºĞ°Ñ… (Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… RePOPE) Ğ±Ñ‹Ğ»Ğ¸ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ñ‹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ² Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ´Ğ»Ñ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Revising Labels, Revising Rankings: The Impact of Annotation Quality",
                    "desc": "This paper investigates the effects of label errors in the MSCOCO dataset on the object hallucination benchmark known as POPE. The authors re-annotate the images in the benchmark to identify inconsistencies in the annotation quality across various subsets. By evaluating several models using the newly revised labels, referred to as RePOPE, they find significant changes in model performance rankings. This study emphasizes the critical importance of label accuracy in machine learning benchmarks and its influence on model evaluation outcomes."
                },
                "zh": {
                    "title": "æ ‡ç­¾è´¨é‡å½±å“æ¨¡å‹è¡¨ç°",
                    "desc": "åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬è¯„ä¼°äº†MSCOCOæ•°æ®é›†ä¸­æ ‡ç­¾é”™è¯¯å¯¹å¸¸ç”¨çš„ç‰©ä½“å¹»è§‰åŸºå‡†POPEçš„å½±å“ã€‚æˆ‘ä»¬é‡æ–°æ ‡æ³¨äº†åŸºå‡†å›¾åƒï¼Œå¹¶å‘ç°ä¸åŒå­é›†ä¹‹é—´çš„æ ‡æ³¨é”™è¯¯å­˜åœ¨ä¸å¹³è¡¡ã€‚é€šè¿‡åœ¨ä¿®è®¢åçš„æ ‡ç­¾ä¸Šè¯„ä¼°å¤šä¸ªæ¨¡å‹ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°æ¨¡å‹æ’åå‘ç”Ÿäº†æ˜¾è‘—å˜åŒ–ï¼Œçªæ˜¾äº†æ ‡ç­¾è´¨é‡çš„é‡è¦æ€§ã€‚æ­¤ç ”ç©¶çš„ä»£ç å’Œæ•°æ®å¯åœ¨https://github.com/YanNeu/RePOPEè·å–ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.16891",
            "title": "AIMO-2 Winning Solution: Building State-of-the-Art Mathematical\n  Reasoning Models with OpenMathReasoning dataset",
            "url": "https://huggingface.co/papers/2504.16891",
            "abstract": "This paper presents our winning submission to the AI Mathematical Olympiad - Progress Prize 2 (AIMO-2) competition. Our recipe for building state-of-the-art mathematical reasoning models relies on three key pillars. First, we create a large-scale dataset comprising 540K unique high-quality math problems, including olympiad-level problems, and their 3.2M long-reasoning solutions. Second, we develop a novel method to integrate code execution with long reasoning models through iterative training, generation, and quality filtering, resulting in 1.7M high-quality Tool-Integrated Reasoning solutions. Third, we create a pipeline to train models to select the most promising solution from many candidates. We show that such generative solution selection (GenSelect) can significantly improve upon majority voting baseline. Combining these ideas, we train a series of models that achieve state-of-the-art results on mathematical reasoning benchmarks. To facilitate further research, we release our code, models, and the complete OpenMathReasoning dataset under a commercially permissive license.",
            "score": 5,
            "issue_id": 3417,
            "pub_date": "2025-04-23",
            "pub_date_card": {
                "ru": "23 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 23",
                "zh": "4æœˆ23æ—¥"
            },
            "hash": "9b9614fe42f410e8",
            "authors": [
                "Ivan Moshkov",
                "Darragh Hanley",
                "Ivan Sorokin",
                "Shubham Toshniwal",
                "Christof Henkel",
                "Benedikt Schifferer",
                "Wei Du",
                "Igor Gitman"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2504.16891.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#math",
                    "#training",
                    "#dataset",
                    "#data",
                    "#open_source"
                ],
                "emoji": "ğŸ§®",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğ¸ Ğ˜Ğ˜: Ğ¾Ñ‚ Ğ¾Ğ»Ğ¸Ğ¼Ğ¿Ğ¸Ğ°Ğ´Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğº Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ±ĞµĞ´Ğ¸Ğ²ÑˆĞµĞµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ² ĞºĞ¾Ğ½ĞºÑƒÑ€ÑĞµ AI Mathematical Olympiad - Progress Prize 2. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 540 Ñ‚Ñ‹ÑÑÑ‡ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ 3,2 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğº Ğ½Ğ¸Ğ¼. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ñƒ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ· Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ»Ğ¾ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ°Ğ¶Ğ¾Ñ€Ğ¸Ñ‚Ğ°Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Revolutionizing Mathematical Reasoning with Advanced Models",
                    "desc": "This paper describes a successful approach to building advanced mathematical reasoning models for the AI Mathematical Olympiad competition. The authors created a large dataset of 540,000 unique math problems and their detailed solutions, which serves as the foundation for training their models. They introduced a novel method that combines code execution with long reasoning processes, enhancing the quality of generated solutions. Additionally, they developed a generative solution selection technique that improves model performance by effectively choosing the best solutions from multiple candidates, leading to state-of-the-art results in mathematical reasoning tasks."
                },
                "zh": {
                    "title": "æ•°å­¦æ¨ç†æ¨¡å‹çš„åˆ›æ–°ä¹‹è·¯",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†æˆ‘ä»¬åœ¨AIæ•°å­¦å¥¥æ—åŒ¹å…‹-è¿›æ­¥å¥–2ï¼ˆAIMO-2ï¼‰ç«èµ›ä¸­çš„è·èƒœæ–¹æ¡ˆã€‚æˆ‘ä»¬æ„å»ºå…ˆè¿›æ•°å­¦æ¨ç†æ¨¡å‹çš„å…³é”®åœ¨äºä¸‰ä¸ªæ”¯æŸ±ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªåŒ…å«54ä¸‡ä¸ªç‹¬ç‰¹é«˜è´¨é‡æ•°å­¦é—®é¢˜çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œä»¥åŠ320ä¸‡ä¸ªé•¿æ¨ç†è§£å†³æ–¹æ¡ˆã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡è¿­ä»£è®­ç»ƒã€ç”Ÿæˆå’Œè´¨é‡è¿‡æ»¤ï¼Œå°†ä»£ç æ‰§è¡Œä¸é•¿æ¨ç†æ¨¡å‹ç›¸ç»“åˆï¼Œæœ€ç»ˆå¾—åˆ°170ä¸‡ä¸ªé«˜è´¨é‡çš„å·¥å…·é›†æˆæ¨ç†è§£å†³æ–¹æ¡ˆã€‚æœ€åï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªç®¡é“ï¼Œè®­ç»ƒæ¨¡å‹ä»å¤šä¸ªå€™é€‰æ–¹æ¡ˆä¸­é€‰æ‹©æœ€æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œæ˜¾è‘—æé«˜äº†ç”Ÿæˆè§£å†³æ–¹æ¡ˆé€‰æ‹©çš„æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10419",
            "title": "Unchecked and Overlooked: Addressing the Checkbox Blind Spot in Large\n  Language Models with CheckboxQA",
            "url": "https://huggingface.co/papers/2504.10419",
            "abstract": "Checkboxes are critical in real-world document processing where the presence or absence of ticks directly informs data extraction and decision-making processes. Yet, despite the strong performance of Large Vision and Language Models across a wide range of tasks, they struggle with interpreting checkable content. This challenge becomes particularly pressing in industries where a single overlooked checkbox may lead to costly regulatory or contractual oversights. To address this gap, we introduce the CheckboxQA dataset, a targeted resource designed to evaluate and improve model performance on checkbox-related tasks. It reveals the limitations of current models and serves as a valuable tool for advancing document comprehension systems, with significant implications for applications in sectors such as legal tech and finance.   The dataset is publicly available at: https://github.com/Snowflake-Labs/CheckboxQA",
            "score": 4,
            "issue_id": 3407,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 14",
                "zh": "4æœˆ14æ—¥"
            },
            "hash": "38ab7a70097348c6",
            "authors": [
                "MichaÅ‚ Turski",
                "Mateusz ChiliÅ„ski",
                "Åukasz Borchmann"
            ],
            "affiliations": [
                "Snowflake AI Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10419.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#interpretability",
                    "#dataset",
                    "#science"
                ],
                "emoji": "â˜‘ï¸",
                "ru": {
                    "title": "CheckboxQA: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ„Ğ»Ğ°Ğ¶ĞºĞ¾Ğ² Ğ² Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… CheckboxQA, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ Ñ„Ğ»Ğ°Ğ¶ĞºĞ°Ğ¼Ğ¸ Ğ² Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ‚Ğ¼ĞµÑ‡Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ¾Ğ½Ğ¸ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾Ñ‚Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ„Ğ»Ğ°Ğ¶ĞºĞ¾Ğ². Ğ­Ñ‚Ğ¾Ñ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞÑĞ¾Ğ±Ğ¾Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ¾ Ğ¸Ğ¼ĞµĞµÑ‚ Ğ´Ğ»Ñ Ñ‚Ğ°ĞºĞ¸Ñ… Ğ¾Ñ‚Ñ€Ğ°ÑĞ»ĞµĞ¹, ĞºĞ°Ğº ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ğ¸ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑÑ‹, Ğ³Ğ´Ğµ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞº Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ñ„Ğ»Ğ°Ğ¶ĞºĞ° Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ²ĞµÑÑ‚Ğ¸ Ğº Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ğ¼ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ğ¼."
                },
                "en": {
                    "title": "Enhancing Checkbox Interpretation in Document Processing",
                    "desc": "This paper addresses the challenges faced by Large Vision and Language Models in interpreting checkboxes in documents, which are crucial for data extraction and decision-making. The authors introduce the CheckboxQA dataset, specifically designed to evaluate and enhance model performance on checkbox-related tasks. The dataset highlights the limitations of existing models in this area and aims to improve document comprehension systems. This work has important implications for industries like legal tech and finance, where missing a checkbox can lead to significant errors."
                },
                "zh": {
                    "title": "æå‡å¤é€‰æ¡†ç†è§£èƒ½åŠ›çš„å…³é”®æ•°æ®é›†",
                    "desc": "åœ¨ç°å®ä¸–ç•Œçš„æ–‡æ¡£å¤„ç†è¿‡ç¨‹ä¸­ï¼Œå¤é€‰æ¡†çš„å­˜åœ¨ä¸å¦ç›´æ¥å½±å“æ•°æ®æå–å’Œå†³ç­–è¿‡ç¨‹ã€‚å°½ç®¡å¤§å‹è§†è§‰å’Œè¯­è¨€æ¨¡å‹åœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬åœ¨è§£è¯»å¤é€‰å†…å®¹æ–¹é¢ä»ç„¶å­˜åœ¨å›°éš¾ã€‚è¿™ä¸ªé—®é¢˜åœ¨æŸäº›è¡Œä¸šå°¤ä¸ºé‡è¦ï¼Œå› ä¸ºä¸€ä¸ªè¢«å¿½è§†çš„å¤é€‰æ¡†å¯èƒ½å¯¼è‡´æ˜‚è´µçš„åˆè§„æˆ–åˆåŒå¤±è¯¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†CheckboxQAæ•°æ®é›†ï¼Œæ—¨åœ¨è¯„ä¼°å’Œæå‡æ¨¡å‹åœ¨å¤é€‰æ¡†ç›¸å…³ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.15254",
            "title": "CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation",
            "url": "https://huggingface.co/papers/2504.15254",
            "abstract": "C-to-Rust transpilation is essential for modernizing legacy C code while enhancing safety and interoperability with modern Rust ecosystems. However, no dataset currently exists for evaluating whether a system can transpile C into safe Rust that passes a set of test cases. We introduce CRUST-Bench, a dataset of 100 C repositories, each paired with manually-written interfaces in safe Rust as well as test cases that can be used to validate correctness of the transpilation. By considering entire repositories rather than isolated functions, CRUST-Bench captures the challenges of translating complex projects with dependencies across multiple files. The provided Rust interfaces provide explicit specifications that ensure adherence to idiomatic, memory-safe Rust patterns, while the accompanying test cases enforce functional correctness. We evaluate state-of-the-art large language models (LLMs) on this task and find that safe and idiomatic Rust generation is still a challenging problem for various state-of-the-art methods and techniques. We also provide insights into the errors LLMs usually make in transpiling code from C to safe Rust. The best performing model, OpenAI o1, is able to solve only 15 tasks in a single-shot setting. Improvements on CRUST-Bench would lead to improved transpilation systems that can reason about complex scenarios and help in migrating legacy codebases from C into languages like Rust that ensure memory safety. You can find the dataset and code at https://github.com/anirudhkhatry/CRUST-bench.",
            "score": 3,
            "issue_id": 3408,
            "pub_date": "2025-04-21",
            "pub_date_card": {
                "ru": "21 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 21",
                "zh": "4æœˆ21æ—¥"
            },
            "hash": "d60d8c4dbf521ab4",
            "authors": [
                "Anirudh Khatry",
                "Robert Zhang",
                "Jia Pan",
                "Ziteng Wang",
                "Qiaochu Chen",
                "Greg Durrett",
                "Isil Dillig"
            ],
            "affiliations": [
                "New York University",
                "The University of Texas at Austin"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.15254.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#benchmark",
                    "#optimization",
                    "#dataset",
                    "#reasoning"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "CRUST-Bench: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¸Ğ»ÑÑ†Ğ¸Ğ¸ C Ğ² Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğ¹ Rust",
                    "desc": "CRUST-Bench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¸Ğ»ÑÑ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ñ C Ğ½Ğ° Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğ¹ Rust. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 100 Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸ĞµĞ² C Ñ Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°Ğ¼Ğ¸ Ğ½Ğ° Rust Ğ¸ Ñ‚ĞµÑÑ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ñ†ĞµĞ»Ñ‹Ñ… Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¾Ğ², Ğ° Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡ĞµĞ¹ - Ğ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€ĞµÑˆĞ¸Ğ»Ğ° Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 15 Ğ·Ğ°Ğ´Ğ°Ñ‡."
                },
                "en": {
                    "title": "Transforming C Code to Safe Rust: Introducing CRUST-Bench",
                    "desc": "This paper introduces CRUST-Bench, a new dataset designed to evaluate the transpilation of C code into safe Rust. It consists of 100 C repositories, each with corresponding safe Rust interfaces and test cases to ensure correctness. By focusing on entire repositories, CRUST-Bench addresses the complexities of translating projects with multiple dependencies. The study also assesses the performance of large language models (LLMs) in this task, revealing that generating safe and idiomatic Rust remains a significant challenge."
                },
                "zh": {
                    "title": "CRUST-Benchï¼šæå‡Cåˆ°Rustè½¬è¯‘çš„å®‰å…¨æ€§ä¸å‡†ç¡®æ€§",
                    "desc": "Cåˆ°Rustçš„è½¬è¯‘å¯¹äºç°ä»£åŒ–é—ç•™Cä»£ç è‡³å…³é‡è¦ï¼ŒåŒæ—¶å¢å¼ºäº†ä¸ç°ä»£Rustç”Ÿæ€ç³»ç»Ÿçš„å®‰å…¨æ€§å’Œäº’æ“ä½œæ€§ã€‚æˆ‘ä»¬æå‡ºäº†CRUST-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«100ä¸ªCä»£ç åº“çš„æ•°æ®é›†ï¼Œæ¯ä¸ªåº“éƒ½é…æœ‰æ‰‹åŠ¨ç¼–å†™çš„å®‰å…¨Rustæ¥å£å’Œæµ‹è¯•ç”¨ä¾‹ï¼Œä»¥éªŒè¯è½¬è¯‘çš„æ­£ç¡®æ€§ã€‚è¯¥æ•°æ®é›†è€ƒè™‘äº†æ•´ä¸ªä»£ç åº“ï¼Œè€Œä¸æ˜¯å­¤ç«‹çš„å‡½æ•°ï¼Œä»è€Œæ•æ‰åˆ°è·¨å¤šä¸ªæ–‡ä»¶çš„å¤æ‚é¡¹ç›®è½¬è¯‘çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬è¯„ä¼°äº†æœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå‘ç°ç”Ÿæˆå®‰å…¨å’Œç¬¦åˆRustä¹ æƒ¯çš„ä»£ç ä»ç„¶æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.16145",
            "title": "Progressive Language-guided Visual Learning for Multi-Task Visual\n  Grounding",
            "url": "https://huggingface.co/papers/2504.16145",
            "abstract": "Multi-task visual grounding (MTVG) includes two sub-tasks, i.e., Referring Expression Comprehension (REC) and Referring Expression Segmentation (RES). The existing representative approaches generally follow the research pipeline which mainly consists of three core procedures, including independent feature extraction for visual and linguistic modalities, respectively, cross-modal interaction module, and independent prediction heads for different sub-tasks. Albeit achieving remarkable performance, this research line has two limitations: 1) The linguistic content has not been fully injected into the entire visual backbone for boosting more effective visual feature extraction and it needs an extra cross-modal interaction module; 2) The relationship between REC and RES tasks is not effectively exploited to help the collaborative prediction for more accurate output. To deal with these problems, in this paper, we propose a Progressive Language-guided Visual Learning framework for multi-task visual grounding, called PLVL, which not only finely mine the inherent feature expression of the visual modality itself but also progressively inject the language information to help learn linguistic-related visual features. In this manner, our PLVL does not need additional cross-modal fusion module while fully introducing the language guidance. Furthermore, we analyze that the localization center for REC would help identify the to-be-segmented object region for RES to some extent. Inspired by this investigation, we design a multi-task head to accomplish collaborative predictions for these two sub-tasks. Extensive experiments conducted on several benchmark datasets comprehensively substantiate that our PLVL obviously outperforms the representative methods in both REC and RES tasks. https://github.com/jcwang0602/PLVL",
            "score": 1,
            "issue_id": 3413,
            "pub_date": "2025-04-22",
            "pub_date_card": {
                "ru": "22 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 22",
                "zh": "4æœˆ22æ—¥"
            },
            "hash": "0f428629b7acb19c",
            "authors": [
                "Jingchao Wang",
                "Hong Wang",
                "Wenlong Zhang",
                "Kunhua Ji",
                "Dingjiang Huang",
                "Yefeng Zheng"
            ],
            "affiliations": [
                "East China Normal University, Shanghai, China",
                "Westlake University, Hangzhou, Zhejiang, China",
                "Xian Jiaotong University, Xian, Shaanxi, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.16145.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#cv"
                ],
                "emoji": "ğŸ‘ï¸",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ·ĞµĞ¼Ğ»ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ¾Ğ¼Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ·Ğ°Ğ·ĞµĞ¼Ğ»ĞµĞ½Ğ¸Ñ, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ PLVL. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼ Ğ¿Ğ¾Ğ´ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ÑƒĞ»Ğµ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. PLVL Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµÑ„ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ PLVL Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Enhancing Visual Grounding with Language Integration",
                    "desc": "This paper introduces a new framework called Progressive Language-guided Visual Learning (PLVL) for multi-task visual grounding, which includes tasks like Referring Expression Comprehension (REC) and Referring Expression Segmentation (RES). The authors identify limitations in existing methods, such as the lack of effective integration of linguistic information into visual feature extraction and the insufficient collaboration between the REC and RES tasks. PLVL addresses these issues by progressively incorporating language guidance into the visual learning process, eliminating the need for a separate cross-modal interaction module. The proposed approach enhances the accuracy of predictions by leveraging the relationship between the two tasks, as demonstrated by extensive experiments on benchmark datasets that show significant performance improvements."
                },
                "zh": {
                    "title": "æ¸è¿›å¼è¯­è¨€å¼•å¯¼è§†è§‰å­¦ä¹ ï¼šæå‡å¤šä»»åŠ¡è§†è§‰å®šä½çš„æ•ˆæœ",
                    "desc": "å¤šä»»åŠ¡è§†è§‰å®šä½ï¼ˆMTVGï¼‰åŒ…æ‹¬ä¸¤ä¸ªå­ä»»åŠ¡ï¼šæŒ‡ä»£è¡¨è¾¾ç†è§£ï¼ˆRECï¼‰å’ŒæŒ‡ä»£è¡¨è¾¾åˆ†å‰²ï¼ˆRESï¼‰ã€‚ç°æœ‰çš„æ–¹æ³•é€šå¸¸é‡‡ç”¨ç‹¬ç«‹ç‰¹å¾æå–ã€è·¨æ¨¡æ€äº¤äº’æ¨¡å—å’Œç‹¬ç«‹é¢„æµ‹å¤´çš„æµç¨‹ï¼Œå°½ç®¡å–å¾—äº†è‰¯å¥½æ•ˆæœï¼Œä½†å­˜åœ¨è¯­è¨€ä¿¡æ¯æœªå……åˆ†èå…¥è§†è§‰ç‰¹å¾æå–å’Œå­ä»»åŠ¡é—´å…³ç³»æœªæœ‰æ•ˆåˆ©ç”¨çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ¸è¿›å¼è¯­è¨€å¼•å¯¼è§†è§‰å­¦ä¹ æ¡†æ¶ï¼ˆPLVLï¼‰ï¼Œè¯¥æ¡†æ¶é€šè¿‡é€æ­¥æ³¨å…¥è¯­è¨€ä¿¡æ¯æ¥æå‡è§†è§‰ç‰¹å¾å­¦ä¹ ï¼ŒåŒæ—¶é¿å…äº†é¢å¤–çš„è·¨æ¨¡æ€èåˆæ¨¡å—ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPLVLåœ¨RECå’ŒRESä»»åŠ¡ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.13263",
            "title": "Causal-Copilot: An Autonomous Causal Analysis Agent",
            "url": "https://huggingface.co/papers/2504.13263",
            "abstract": "Causal analysis plays a foundational role in scientific discovery and reliable decision-making, yet it remains largely inaccessible to domain experts due to its conceptual and algorithmic complexity. This disconnect between causal methodology and practical usability presents a dual challenge: domain experts are unable to leverage recent advances in causal learning, while causal researchers lack broad, real-world deployment to test and refine their methods. To address this, we introduce Causal-Copilot, an autonomous agent that operationalizes expert-level causal analysis within a large language model framework. Causal-Copilot automates the full pipeline of causal analysis for both tabular and time-series data -- including causal discovery, causal inference, algorithm selection, hyperparameter optimization, result interpretation, and generation of actionable insights. It supports interactive refinement through natural language, lowering the barrier for non-specialists while preserving methodological rigor. By integrating over 20 state-of-the-art causal analysis techniques, our system fosters a virtuous cycle -- expanding access to advanced causal methods for domain experts while generating rich, real-world applications that inform and advance causal theory. Empirical evaluations demonstrate that Causal-Copilot achieves superior performance compared to existing baselines, offering a reliable, scalable, and extensible solution that bridges the gap between theoretical sophistication and real-world applicability in causal analysis. A live interactive demo of Causal-Copilot is available at https://causalcopilot.com/.",
            "score": 0,
            "issue_id": 3418,
            "pub_date": "2025-04-17",
            "pub_date_card": {
                "ru": "17 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 17",
                "zh": "4æœˆ17æ—¥"
            },
            "hash": "53da6426fe219919",
            "authors": [
                "Xinyue Wang",
                "Kun Zhou",
                "Wenyi Wu",
                "Har Simrat Singh",
                "Fang Nan",
                "Songyao Jin",
                "Aryan Philip",
                "Saloni Patnaik",
                "Hou Zhu",
                "Shivam Singh",
                "Parjanya Prashant",
                "Qian Shen",
                "Biwei Huang"
            ],
            "affiliations": [
                "University of California San Diego, CA 92093, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.13263.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#science",
                    "#data",
                    "#inference",
                    "#agents"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ”ĞµĞ¼Ğ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Causal-Copilot - Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ Ñ†Ğ¸ĞºĞ» Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 20 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ĞºĞ°ÑƒĞ·Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ»Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ĞºĞ°ÑƒĞ·Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ, Ğ²Ñ‹Ğ²Ğ¾Ğ´ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ². Causal-Copilot Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº, Ğ´ĞµĞ»Ğ°Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ½ĞµÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸ÑÑ‚Ğ¾Ğ². Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Bridging the Gap in Causal Analysis with Causal-Copilot",
                    "desc": "Causal-Copilot is an innovative tool that simplifies causal analysis for domain experts by using a large language model. It automates the entire process of causal analysis, including discovery, inference, and interpretation, making it accessible to non-specialists. By integrating over 20 advanced causal techniques, it allows users to refine their analyses interactively through natural language. Empirical tests show that Causal-Copilot outperforms existing methods, making it a practical solution for applying causal analysis in real-world scenarios."
                },
                "zh": {
                    "title": "å› æœåˆ†æçš„æ™ºèƒ½åŠ©æ‰‹",
                    "desc": "å› æœåˆ†æåœ¨ç§‘å­¦å‘ç°å’Œå¯é å†³ç­–ä¸­èµ·ç€åŸºç¡€æ€§ä½œç”¨ï¼Œä½†ç”±äºå…¶æ¦‚å¿µå’Œç®—æ³•çš„å¤æ‚æ€§ï¼Œé¢†åŸŸä¸“å®¶å¾€å¾€éš¾ä»¥æ¥è§¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Causal-Copilotï¼Œè¿™æ˜¯ä¸€ç§åœ¨å¤§å‹è¯­è¨€æ¨¡å‹æ¡†æ¶å†…å®ç°ä¸“å®¶çº§å› æœåˆ†æçš„è‡ªä¸»ä»£ç†ã€‚Causal-Copilotè‡ªåŠ¨åŒ–äº†å› æœåˆ†æçš„å®Œæ•´æµç¨‹ï¼ŒåŒ…æ‹¬å› æœå‘ç°ã€å› æœæ¨æ–­ã€ç®—æ³•é€‰æ‹©ã€è¶…å‚æ•°ä¼˜åŒ–ã€ç»“æœè§£é‡Šå’Œå¯æ“ä½œè§è§£çš„ç”Ÿæˆã€‚é€šè¿‡è‡ªç„¶è¯­è¨€æ”¯æŒäº¤äº’å¼ä¼˜åŒ–ï¼ŒCausal-Copiloté™ä½äº†éä¸“ä¸šäººå£«çš„ä½¿ç”¨é—¨æ§›ï¼ŒåŒæ—¶ä¿æŒäº†æ–¹æ³•è®ºçš„ä¸¥è°¨æ€§ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-04-23.html",
    "link_next": "2025-04-25.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "23.04",
        "en": "04/23",
        "zh": "4æœˆ23æ—¥"
    },
    "short_date_next": {
        "ru": "25.04",
        "en": "04/25",
        "zh": "4æœˆ25æ—¥"
    },
    "categories": {
        "#dataset": 9,
        "#data": 8,
        "#benchmark": 9,
        "#agents": 2,
        "#cv": 4,
        "#rl": 2,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 1,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 3,
        "#math": 4,
        "#multilingual": 1,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 10,
        "#robotics": 0,
        "#agi": 1,
        "#games": 0,
        "#interpretability": 2,
        "#reasoning": 6,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 8,
        "#survey": 2,
        "#diffusion": 2,
        "#alignment": 3,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 1,
        "#science": 2,
        "#low_resource": 1
    },
    "zh": {
        "text": "è§†è§‰æ¨ç†æ˜¯äººç±»æ™ºèƒ½çš„æ ¸å¿ƒç»„æˆéƒ¨åˆ†ï¼Œä¹Ÿæ˜¯å…ˆè¿›å¤šæ¨¡æ€æ¨¡å‹çš„å…³é”®èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå½“å‰å¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†è¯„ä¼°å¾€å¾€ä¾èµ–æ–‡æœ¬æè¿°ï¼Œå…è®¸åŸºäºè¯­è¨€çš„æ¨ç†æ·å¾„ï¼Œæ— æ³•è¡¡é‡çœŸæ­£çš„è§†è§‰ä¸­å¿ƒæ¨ç†ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†VisuLogicï¼šä¸€ä¸ªåŒ…å«1,000ä¸ªäººç±»éªŒè¯é—®é¢˜çš„åŸºå‡†ï¼Œæ¶µç›–å…­ä¸ªç±»åˆ«ï¼ˆå¦‚é‡åŒ–è½¬æ¢ã€ç©ºé—´å…³ç³»ã€å±æ€§æ¯”è¾ƒï¼‰ã€‚è¿™äº›ä¸åŒç±»å‹çš„é—®é¢˜å¯ä»¥ä»å¤šä¸ªè§’åº¦è¯„ä¼°MLLMsçš„è§†è§‰æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬å¯¹è¯¥åŸºå‡†è¯„ä¼°äº†é¢†å…ˆçš„MLLMsï¼Œå¹¶åˆ†æå…¶ç»“æœï¼Œä»¥è¯†åˆ«å¸¸è§çš„å¤±è´¥æ¨¡å¼ã€‚å¤§å¤šæ•°æ¨¡å‹çš„å‡†ç¡®ç‡ä½äº30%ï¼Œä»…ç•¥é«˜äº25%çš„éšæœºåŸºçº¿ï¼Œè¿œä½äºäººç±»çš„51.4%ï¼Œæ˜¾ç¤ºå‡ºè§†è§‰æ¨ç†æ–¹é¢çš„æ˜¾è‘—å·®è·ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªè¡¥å……è®­ç»ƒæ•°æ®é›†å’Œä¸€ä¸ªå¼ºåŒ–å­¦ä¹ åŸºçº¿ï¼Œä»¥æ”¯æŒè¿›ä¸€æ­¥çš„è¿›å±•ã€‚",
        "title": "VisuLogic: A Benchmark for Evaluating Visual Reasoning in Multi-modal\n  Large Language Models",
        "pinyin": "è§†è§‰æ¨ç†æ˜¯äººç±»æ™ºèƒ½çš„æ ¸å¿ƒç»„æˆéƒ¨åˆ†ï¼Œä¹Ÿæ˜¯å…ˆè¿›å¤šæ¨¡æ€æ¨¡å‹çš„å…³é”®èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå½“å‰å¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†è¯„ä¼°å¾€å¾€ä¾èµ–æ–‡æœ¬æè¿°ï¼Œå…è®¸åŸºäºè¯­è¨€çš„æ¨ç†æ·å¾„ï¼Œæ— æ³•è¡¡é‡çœŸæ­£çš„è§†è§‰ä¸­å¿ƒæ¨ç†ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†VisuLogicï¼šä¸€ä¸ªåŒ…å«1,000ä¸ªäººç±»éªŒè¯é—®é¢˜çš„åŸºå‡†ï¼Œæ¶µç›–å…­ä¸ªç±»åˆ«ï¼ˆå¦‚é‡åŒ–è½¬æ¢ã€ç©ºé—´å…³ç³»ã€å±æ€§æ¯”è¾ƒï¼‰ã€‚è¿™äº›ä¸åŒç±»å‹çš„é—®é¢˜å¯ä»¥ä»å¤šä¸ªè§’åº¦è¯„ä¼°MLLMsçš„è§†è§‰æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬å¯¹è¯¥åŸºå‡†è¯„ä¼°äº†é¢†å…ˆçš„MLLMsï¼Œå¹¶åˆ†æå…¶ç»“æœï¼Œä»¥è¯†åˆ«å¸¸è§çš„å¤±è´¥æ¨¡å¼ã€‚å¤§å¤šæ•°æ¨¡å‹çš„å‡†ç¡®ç‡ä½äº30%ï¼Œä»…ç•¥é«˜äº25%çš„éšæœºåŸºçº¿ï¼Œè¿œä½äºäººç±»çš„51.4%ï¼Œæ˜¾ç¤ºå‡ºè§†è§‰æ¨ç†æ–¹é¢çš„æ˜¾è‘—å·®è·ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªè¡¥å……è®­ç»ƒæ•°æ®é›†å’Œä¸€ä¸ªå¼ºåŒ–å­¦ä¹ åŸºçº¿ï¼Œä»¥æ”¯æŒè¿›ä¸€æ­¥çš„è¿›å±•ã€‚\n\nShÃ¬juÃ© tuÄ«lÇ shÃ¬ rÃ©nlÃ¨i zhÃ¬nÃ©ng de hÃ©xÄ«n zÇ”chÃ©ng bÃ¹fÄ“n, yÄ› shÃ¬ xiÄnjÃ¬n duÅ mÃ³shÃ¬ duÅ mÃ³shÃ¬ mÃ³xÃ­ng de guÇnjiÃ n nÃ©nglÃ¬. RÃ¡n'Ã©r, dÄngqiÃ¡n duÃ¬ duÅ mÃ³shÃ¬ dÃ  yÇ”yÃ¡n mÃ³xÃ­ng (MLLMs) de tuÄ«lÇ pÃ­ngjiÃ  wÇngwÇng yÄ«lÃ i wÃ©nbÄ›n miÃ¡oshÃ¹, yÇ”n xÇ” jÄ«yÃº yÇ”yÃ¡n de tuÄ«lÇ jiÃ©jÃ¬ng, wÃºfÇ hÃ©ngliÃ¡ng zhÄ“nzhÃ¨ng de shÃ¬juÃ© zhÅngxÄ«n tuÄ«lÇ. WÃ¨i jiÄ›juÃ© zhÃ¨ yÄ« wÃ¨ntÃ­, wÇ’men yÇn rÃ¹le VisuLogic: yÄ«gÃ¨ bÄohÃ¡n 1,000 gÃ¨ rÃ©nlÃ¨i yÃ nzhÃ¨ng wÃ¨ntÃ­ de jÄ«zhÇ”n, hÃ¡njiÄ“ liÃ¹ gÃ¨ lÃ¨ibiÃ© (rÃº liÃ ng huÃ  zhuÇnhuÃ n, kÅngjiÄn guÄnxÃ¬, shÇ”xÃ¬ng bÇjiÃ o). ZhÃ¨xiÄ“ bÃ¹tÃ³ng lÃ¨ixÃ­ng de wÃ¨ntÃ­ kÄ›yÇ cÃ³ng duÅ gÃ¨ jiÇodÃ¹ pÃ­ngjiÃ  MLLMs de shÃ¬juÃ© tuÄ«lÇ nÃ©nglÃ¬. WÇ’men duÃ¬ gÄi jÄ«zhÇ”n pÃ­ngjiÃ  le lÇngxiÄn de MLLMs, bÃ¬ng fÄ“nxi qÃ­ jiÃ©guÇ’, yÇ shÃ­biÃ© chÃ¡ngjiÃ n de shÄ«bÃ i mÃ³shÃ¬. DÃ duÅshÃ¹ mÃ³xÃ­ng de zhÇ”nquÃ¨lÇœ dÄ«yÃº 30%, jÇn lÃ¼Ã¨ gÄoyÃº 25% de suÃ­jÄ« jÄ«zhÇ”n, yuÇn dÄ«yÃº rÃ©nlÃ¨i de 51.4%, xiÇnshÃ¬ chÅ« shÃ¬juÃ© tuÄ«lÇ fÄngmiÃ n de xiÇnzhÃ¹ chÄjÃ¹. CÇwÃ i, wÇ’men tÃ­gÅngle yÄ«gÃ¨ bÇ”chÅng xÃ¹nliÃ n shÃ¹jÃ¹jÃ­ hÃ© yÄ«gÃ¨ qiÃ¡ngzhÃ¹ xuÃ©xÃ­ jÄ«zhÇ”n, yÇ zhÄ«chÃ­ jÃ¬nfÄ de jÃ¬nbÃ¹.",
        "vocab": "[\n    {\"word\": \"è§†è§‰\", \"pinyin\": \"shÃ¬juÃ©\", \"trans\": \"vision\"},\n    {\"word\": \"æ¨ç†\", \"pinyin\": \"tuÄ«lÇ\", \"trans\": \"reasoning\"},\n    {\"word\": \"æ ¸å¿ƒ\", \"pinyin\": \"hÃ©xÄ«n\", \"trans\": \"core\"},\n    {\"word\": \"ç»„æˆéƒ¨åˆ†\", \"pinyin\": \"zÇ”chÃ©ng bÃ¹fÄ“n\", \"trans\": \"component\"},\n    {\"word\": \"å…ˆè¿›\", \"pinyin\": \"xiÄnjÃ¬n\", \"trans\": \"advanced\"},\n    {\"word\": \"å¤šæ¨¡æ€\", \"pinyin\": \"duÅ mÃ³shÃ¬\", \"trans\": \"multimodal\"},\n    {\"word\": \"æ¨¡å‹\", \"pinyin\": \"mÃ³xÃ­ng\", \"trans\": \"model\"},\n    {\"word\": \"å…³é”®\", \"pinyin\": \"guÇnjiÃ n\", \"trans\": \"key\"},\n    {\"word\": \"èƒ½åŠ›\", \"pinyin\": \"nÃ©nglÃ¬\", \"trans\": \"ability\"},\n    {\"word\": \"ä¾èµ–\", \"pinyin\": \"yÄ«lÃ i\", \"trans\": \"rely on\"},\n    {\"word\": \"æè¿°\", \"pinyin\": \"miÃ¡oshÃ¹\", \"trans\": \"description\"},\n    {\"word\": \"å…è®¸\", \"pinyin\": \"yÇ”nxÇ”\", \"trans\": \"allow\"},\n    {\"word\": \"åŸºäº\", \"pinyin\": \"jÄ«yÃº\", \"trans\": \"based on\"},\n    {\"word\": \"æ·å¾„\", \"pinyin\": \"jiÃ©jÃ¬ng\", \"trans\": \"shortcut\"},\n    {\"word\": \"è¡¡é‡\", \"pinyin\": \"hÃ©ngliÃ¡ng\", \"trans\": \"measure\"},\n    {\"word\": \"çœŸæ­£\", \"pinyin\": \"zhÄ“nzhÃ¨ng\", \"trans\": \"genuine\"},\n    {\"word\": \"å¼•å…¥\", \"pinyin\": \"yÇnrÃ¹\", \"trans\": \"introduce\"},\n    {\"word\": \"åŸºå‡†\", \"pinyin\": \"jÄ«zhÇ”n\", \"trans\": \"benchmark\"},\n    {\"word\": \"éªŒè¯\", \"pinyin\": \"yÃ nzhÃ¨ng\", \"trans\": \"verification\"},\n    {\"word\": \"é—®é¢˜\", \"pinyin\": \"wÃ¨ntÃ­\", \"trans\": \"question\"},\n    {\"word\": \"æ¶µç›–\", \"pinyin\": \"hÃ¡ngÃ i\", \"trans\": \"cover\"},\n    {\"word\": \"ç±»åˆ«\", \"pinyin\": \"lÃ¨ibiÃ©\", \"trans\": \"category\"},\n    {\"word\": \"é‡åŒ–\", \"pinyin\": \"liÃ nghuÃ \", \"trans\": \"quantification\"},\n    {\"word\": \"è½¬æ¢\", \"pinyin\": \"zhuÇnhuÃ n\", \"trans\": \"conversion\"},\n    {\"word\": \"ç©ºé—´\", \"pinyin\": \"kÅngjiÄn\", \"trans\": \"space\"},\n    {\"word\": \"å…³ç³»\", \"pinyin\": \"guÄnxÃ¬\", \"trans\": \"relationship\"},\n    {\"word\": \"å±æ€§\", \"pinyin\": \"shÇ”xÃ¬ng\", \"trans\": \"attribute\"},\n    {\"word\": \"æ¯”è¾ƒ\", \"pinyin\": \"bÇjiÃ o\", \"trans\": \"comparison\"},\n    {\"word\": \"è§’åº¦\", \"pinyin\": \"jiÇodÃ¹\", \"trans\": \"angle\"},\n    {\"word\": \"è¯„ä¼°\", \"pinyin\": \"pÃ­nggÅ«\", \"trans\": \"evaluate\"},\n    {\"word\": \"é¢†å…ˆ\", \"pinyin\": \"lÇngxiÄn\", \"trans\": \"leading\"},\n    {\"word\": \"åˆ†æ\", \"pinyin\": \"fÄ“nxÄ«\", \"trans\": \"analyze\"},\n    {\"word\": \"ç»“æœ\", \"pinyin\": \"jiÃ©guÇ’\", \"trans\": \"result\"},\n    {\"word\": \"è¯†åˆ«\", \"pinyin\": \"shÃ­biÃ©\", \"trans\": \"identify\"},\n    {\"word\": \"æ¨¡å¼\", \"pinyin\": \"mÃ³shÃ¬\", \"trans\": \"pattern\"},\n    {\"word\": \"å‡†ç¡®ç‡\", \"pinyin\": \"zhÇ”nquÃ¨lÇœ\", \"trans\": \"accuracy\"},\n    {\"word\": \"éšæœº\", \"pinyin\": \"suÃ­jÄ«\", \"trans\": \"random\"},\n    {\"word\": \"åŸºçº¿\", \"pinyin\": \"jÄ«xiÃ n\", \"trans\": \"baseline\"},\n    {\"word\": \"æ˜¾è‘—\", \"pinyin\": \"xiÇnzhÃ¹\", \"trans\": \"significant\"},\n    {\"word\": \"å·®è·\", \"pinyin\": \"chÄjÃ¹\", \"trans\": \"gap\"},\n    {\"word\": \"è¡¥å……\", \"pinyin\": \"bÇ”chÅng\", \"trans\": \"supplement\"},\n    {\"word\": \"æ•°æ®é›†\", \"pinyin\": \"shÃ¹jÃ¹jÃ­\", \"trans\": \"dataset\"},\n    {\"word\": \"å¼ºåŒ–\", \"pinyin\": \"qiÃ¡nghuÃ \", \"trans\": \"reinforce\"},\n    {\"word\": \"å­¦ä¹ \", \"pinyin\": \"xuÃ©xÃ­\", \"trans\": \"learning\"},\n    {\"word\": \"æ”¯æŒ\", \"pinyin\": \"zhÄ«chÃ­\", \"trans\": \"support\"},\n    {\"word\": \"è¿›å±•\", \"pinyin\": \"jÃ¬nzhÇn\", \"trans\": \"progress\"}\n]",
        "trans": "Visual reasoning is a core component of human intelligence and a key capability of advanced multimodal models. However, current evaluations of reasoning in multimodal large language models (MLLMs) often rely on textual descriptions, allowing for language-based reasoning shortcuts that fail to measure true vision-centric reasoning. To address this issue, we introduce VisuLogic: a benchmark containing 1,000 human-verified questions covering six categories (such as quantitative transformations, spatial relationships, and attribute comparisons). These different types of questions can evaluate the visual reasoning capabilities of MLLMs from multiple angles. We evaluated leading MLLMs on this benchmark and analyzed the results to identify common failure modes. Most models achieved an accuracy of less than 30%, only slightly above the 25% random baseline and significantly lower than the human accuracy of 51.4%, indicating a notable gap in visual reasoning. Additionally, we provide a supplementary training dataset and a reinforcement learning baseline to support further advancements.",
        "update_ts": "2025-04-24 09:12"
    }
}