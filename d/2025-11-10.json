{
    "date": {
        "ru": "10 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
        "en": "November 10",
        "zh": "11æœˆ10æ—¥"
    },
    "time_utc": "2025-11-10 09:00",
    "weekday": 0,
    "issue_id": 2,
    "home_page_url": "https://huggingface.co/papers?date=2025-11-10",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2511.04962",
            "title": "Too Good to be Bad: On the Failure of LLMs to Role-Play Villains",
            "url": "https://huggingface.co/papers/2511.04962",
            "abstract": "LLMs struggle to authentically portray morally ambiguous or villainous characters due to safety alignment, as evidenced by the Moral RolePlay benchmark.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) are increasingly tasked with creative generation, including the simulation of fictional characters. However, their ability to portray non-prosocial, antagonistic personas remains largely unexamined. We hypothesize that the safety alignment of modern LLMs creates a fundamental conflict with the task of authentically role-playing morally ambiguous or villainous characters. To investigate this, we introduce the Moral RolePlay benchmark, a new dataset featuring a four-level moral alignment scale and a balanced test set for rigorous evaluation. We task state-of-the-art LLMs with role-playing characters from moral paragons to pure villains. Our large-scale evaluation reveals a consistent, monotonic decline in role-playing fidelity as character morality decreases. We find that models struggle most with traits directly antithetical to safety principles, such as ``Deceitful'' and ``Manipulative'', often substituting nuanced malevolence with superficial aggression. Furthermore, we demonstrate that general chatbot proficiency is a poor predictor of villain role-playing ability, with highly safety-aligned models performing particularly poorly. Our work provides the first systematic evidence of this critical limitation, highlighting a key tension between model safety and creative fidelity. Our benchmark and findings pave the way for developing more nuanced, context-aware alignment methods.",
            "score": 52,
            "issue_id": 1,
            "pub_date": "2025-11-07",
            "pub_date_card": {
                "ru": "7 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 7",
                "zh": "11æœˆ7æ—¥"
            },
            "hash": "fc87d40dbfc83633",
            "authors": [
                "Zihao Yi",
                "Qingxuan Jiang",
                "Ruotian Ma",
                "Xingyu Chen",
                "Qu Yang",
                "Mengru Wang",
                "Fanghua Ye",
                "Ying Shen",
                "Zhaopeng Tu",
                "Xiaolong Li",
                "Linus"
            ],
            "affiliations": [
                "Sun Yat-Sen University",
                "Tencent Multimodal Department"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.04962.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#interpretability",
                    "#story_generation",
                    "#alignment",
                    "#dataset"
                ],
                "emoji": "ğŸ˜ˆ",
                "ru": {
                    "title": "ĞšĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ñ‚Ğ²Ğ¾Ñ€Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼: Ğ¿Ğ¾Ñ‡ĞµĞ¼Ñƒ LLM Ğ½Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¸Ğ³Ñ€Ğ°Ñ‚ÑŒ Ğ·Ğ»Ğ¾Ğ´ĞµĞµĞ²",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°, ĞºĞ¾Ğ³Ğ´Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ½Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ°ÑƒÑ‚ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ñ‹Ñ… Ğ¸ Ğ·Ğ»Ğ¾Ğ´ĞµĞ¹ÑĞºĞ¸Ñ… Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ¸Ğ·-Ğ·Ğ° Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸, Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ñ… Ğ² Ğ¸Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Moral RolePlay Ñ Ñ‡ĞµÑ‚Ñ‹Ñ€Ñ‘Ñ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğ¹ ÑˆĞºĞ°Ğ»Ğ¾Ğ¹ Ğ¼Ğ¾Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ‚Ğ¾Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ° Ñ‡Ñ‘Ñ‚ĞºĞ°Ñ Ğ¼Ğ¾Ğ½Ğ¾Ñ‚Ğ¾Ğ½Ğ½Ğ°Ñ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€Ğ¾Ğ»ĞµĞ²Ğ¾Ğ¹ Ğ¸Ğ³Ñ€Ñ‹ Ğ¿Ğ¾ Ğ¼ĞµÑ€Ğµ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ñ‚Ğ²Ğ¾Ñ€Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¿Ğ¾Ñ€Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ°."
                },
                "en": {
                    "title": "Navigating the Tension: Safety vs. Creative Fidelity in LLMs",
                    "desc": "This paper explores the limitations of Large Language Models (LLMs) in portraying morally ambiguous or villainous characters due to their safety alignment. The authors introduce the Moral RolePlay benchmark, which evaluates LLMs on a scale of moral alignment, revealing that as character morality decreases, the models' ability to role-play authentically declines. The study finds that LLMs often replace complex villainous traits with simpler forms of aggression, indicating a struggle with safety principles. This research highlights the tension between ensuring model safety and achieving creative fidelity in character portrayal, suggesting a need for improved alignment methods."
                },
                "zh": {
                    "title": "å®‰å…¨æ€§ä¸åˆ›é€ æ€§ä¹‹é—´çš„çŸ›ç›¾",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨¡æ‹Ÿé“å¾·æ¨¡ç³Šæˆ–åæ´¾è§’è‰²æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºå…¶å®‰å…¨å¯¹é½çš„é™åˆ¶ã€‚æˆ‘ä»¬æå‡ºäº†é“å¾·è§’è‰²æ‰®æ¼”åŸºå‡†ï¼ˆMoral RolePlay benchmarkï¼‰ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹åœ¨ä¸åŒé“å¾·æ°´å¹³ä¸‹çš„è§’è‰²æ‰®æ¼”èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œéšç€è§’è‰²é“å¾·æ°´å¹³çš„é™ä½ï¼Œæ¨¡å‹çš„è§’è‰²æ‰®æ¼”å‡†ç¡®æ€§æ˜¾è‘—ä¸‹é™ï¼Œå°¤å…¶æ˜¯åœ¨è¡¨ç°å‡ºæ¬ºéª—å’Œæ“æ§ç­‰ç‰¹å¾æ—¶ã€‚æˆ‘ä»¬çš„å·¥ä½œæ­ç¤ºäº†æ¨¡å‹å®‰å…¨æ€§ä¸åˆ›é€ æ€§è¡¨ç°ä¹‹é—´çš„å…³é”®çŸ›ç›¾ï¼Œä¸ºæœªæ¥å¼€å‘æ›´ç»†è‡´çš„å¯¹é½æ–¹æ³•æä¾›äº†åŸºç¡€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.05491",
            "title": "Visual Spatial Tuning",
            "url": "https://huggingface.co/papers/2511.05491",
            "abstract": "A framework called Visual Spatial Tuning (VST) enhances the spatial abilities of Vision-Language Models (VLMs) through progressive training with specialized datasets, achieving state-of-the-art results on spatial benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Capturing spatial relationships from visual inputs is a cornerstone of human-like general intelligence. Several previous studies have tried to enhance the spatial awareness of Vision-Language Models (VLMs) by adding extra expert encoders, which brings extra overhead and usually harms general capabilities. To enhance the spatial ability in general architectures, we introduce Visual Spatial Tuning (VST), a comprehensive framework to cultivate VLMs with human-like visuospatial abilities, from spatial perception to reasoning. We first attempt to enhance spatial perception in VLMs by constructing a large-scale dataset termed VST-P, which comprises 4.1 million samples spanning 19 skills across single views, multiple images, and videos. Then, we present VST-R, a curated dataset with 135K samples that instruct models to reason in space. In particular, we adopt a progressive training pipeline: supervised fine-tuning to build foundational spatial knowledge, followed by reinforcement learning to further improve spatial reasoning abilities. Without the side-effect to general capabilities, the proposed VST consistently achieves state-of-the-art results on several spatial benchmarks, including 34.8% on MMSI-Bench and 61.2% on VSIBench. It turns out that the Vision-Language-Action models can be significantly enhanced with the proposed spatial tuning paradigm, paving the way for more physically grounded AI.",
            "score": 49,
            "issue_id": 1,
            "pub_date": "2025-11-07",
            "pub_date_card": {
                "ru": "7 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 7",
                "zh": "11æœˆ7æ—¥"
            },
            "hash": "47d747b35f57a189",
            "authors": [
                "Rui Yang",
                "Ziyu Zhu",
                "Yanwei Li",
                "Jingjia Huang",
                "Shen Yan",
                "Siyuan Zhou",
                "Zhe Liu",
                "Xiangtai Li",
                "Shuangye Li",
                "Wenqian Wang",
                "Yi Lin",
                "Hengshuang Zhao"
            ],
            "affiliations": [
                "ByteDance Seed",
                "The University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.05491.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#cv",
                    "#multimodal",
                    "#training",
                    "#dataset",
                    "#rl"
                ],
                "emoji": "ğŸ§­",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½ÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºĞ° Visual Spatial Tuning (VST), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ° Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ²Ğ° ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°: VST-P Ñ 4.1 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ VST-R Ñ 135K Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° supervised fine-tuning Ğ´Ğ»Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ·Ğ°Ñ‚ĞµĞ¼ reinforcement learning Ğ´Ğ»Ñ ÑƒĞ³Ğ»ÑƒĞ±Ğ»ĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ±ĞµĞ· ÑƒÑ…ÑƒĞ´ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ñ‰Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Enhancing Vision-Language Models with Visual Spatial Tuning",
                    "desc": "The paper introduces Visual Spatial Tuning (VST), a framework designed to improve the spatial abilities of Vision-Language Models (VLMs). It does this by using specialized datasets, including VST-P for spatial perception and VST-R for spatial reasoning, which help the models learn from a wide range of visual inputs. The training process involves a combination of supervised fine-tuning and reinforcement learning, allowing the models to develop strong spatial reasoning skills without compromising their general capabilities. As a result, VST achieves state-of-the-art performance on various spatial benchmarks, demonstrating its effectiveness in enhancing AI's understanding of spatial relationships."
                },
                "zh": {
                    "title": "è§†è§‰ç©ºé—´è°ƒä¼˜ï¼šæå‡æ¨¡å‹ç©ºé—´èƒ½åŠ›çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºè§†è§‰ç©ºé—´è°ƒä¼˜ï¼ˆVSTï¼‰çš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡é€æ­¥è®­ç»ƒå’Œä¸“é—¨æ•°æ®é›†æ¥å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„ç©ºé—´èƒ½åŠ›ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¤§å‹æ•°æ®é›†VST-Pï¼ŒåŒ…å«410ä¸‡æ ·æœ¬ï¼Œæ¶µç›–19ç§æŠ€èƒ½ï¼Œå¸®åŠ©æ¨¡å‹æé«˜ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›ã€‚æ¥ç€ï¼Œæˆ‘ä»¬å¼•å…¥äº†VST-Ræ•°æ®é›†ï¼ŒåŒ…å«135Kæ ·æœ¬ï¼ŒæŒ‡å¯¼æ¨¡å‹è¿›è¡Œç©ºé—´æ¨ç†ã€‚é€šè¿‡ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ çš„é€æ­¥è®­ç»ƒï¼ŒVSTåœ¨å¤šä¸ªç©ºé—´åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œè¯æ˜äº†å…¶åœ¨æå‡è§†è§‰è¯­è¨€è¡ŒåŠ¨æ¨¡å‹æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.05271",
            "title": "DeepEyesV2: Toward Agentic Multimodal Model",
            "url": "https://huggingface.co/papers/2511.05271",
            "abstract": "DeepEyesV2, an agentic multimodal model, uses a two-stage training pipeline to effectively integrate tool use, demonstrating robust performance across real-world reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Agentic multimodal models should not only comprehend text and images, but also actively invoke external tools, such as code execution environments and web search, and integrate these operations into reasoning. In this work, we introduce DeepEyesV2 and explore how to build an agentic multimodal model from the perspectives of data construction, training methods, and model evaluation. We observe that direct reinforcement learning alone fails to induce robust tool-use behavior. This phenomenon motivates a two-stage training pipeline: a cold-start stage to establish tool-use patterns, and reinforcement learning stage to further refine tool invocation. We curate a diverse, moderately challenging training dataset, specifically including examples where tool use is beneficial. We further introduce RealX-Bench, a comprehensive benchmark designed to evaluate real-world multimodal reasoning, which inherently requires the integration of multiple capabilities, including perception, search, and reasoning. We evaluate DeepEyesV2 on RealX-Bench and other representative benchmarks, demonstrating its effectiveness across real-world understanding, mathematical reasoning, and search-intensive tasks. Moreover, DeepEyesV2 exhibits task-adaptive tool invocation, tending to use image operations for perception tasks and numerical computations for reasoning tasks. Reinforcement learning further enables complex tool combinations and allows model to selectively invoke tools based on context. We hope our study can provide guidance for community in developing agentic multimodal models.",
            "score": 42,
            "issue_id": 1,
            "pub_date": "2025-11-07",
            "pub_date_card": {
                "ru": "7 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 7",
                "zh": "11æœˆ7æ—¥"
            },
            "hash": "d54572b9ee618c41",
            "authors": [
                "Jack Hong",
                "Chenxiao Zhao",
                "ChengLin Zhu",
                "Weiheng Lu",
                "Guohai Xu",
                "Xing Yu"
            ],
            "affiliations": [
                "Xiaohongshu Inc."
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.05271.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#optimization",
                    "#agents",
                    "#multimodal",
                    "#training",
                    "#dataset",
                    "#rl",
                    "#reasoning"
                ],
                "emoji": "ğŸ”§",
                "ru": {
                    "title": "Ğ”Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "DeepEyesV2 Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ½Ğ¾ Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ´Ğ° Ğ¸ Ğ²ĞµĞ±-Ğ¿Ğ¾Ğ¸ÑĞº. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: Ğ½Ğ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ° Ğ½Ğ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ reinforcement learning Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ² Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸, Ğ³Ğ´Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾, Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº RealX-Bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ DeepEyesV2 ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ, Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°."
                },
                "en": {
                    "title": "Empowering Multimodal Reasoning with Adaptive Tool Use",
                    "desc": "DeepEyesV2 is a multimodal model designed to integrate tool use effectively for real-world reasoning tasks. It employs a two-stage training pipeline that first establishes tool-use patterns and then refines them through reinforcement learning. The model is evaluated using RealX-Bench, a benchmark that tests its ability to combine perception, search, and reasoning capabilities. DeepEyesV2 demonstrates adaptive tool invocation, using different tools based on the task context, which enhances its performance in various reasoning scenarios."
                },
                "zh": {
                    "title": "DeepEyesV2ï¼šæ™ºèƒ½å¤šæ¨¡æ€æ¨¡å‹çš„å·¥å…·æ•´åˆä¸æ¨ç†èƒ½åŠ›",
                    "desc": "DeepEyesV2æ˜¯ä¸€ç§å…·æœ‰ä»£ç†èƒ½åŠ›çš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæµç¨‹æ¥æœ‰æ•ˆæ•´åˆå·¥å…·ä½¿ç”¨ã€‚è¯¥æ¨¡å‹ä¸ä»…èƒ½å¤Ÿç†è§£æ–‡æœ¬å’Œå›¾åƒï¼Œè¿˜èƒ½ä¸»åŠ¨è°ƒç”¨å¤–éƒ¨å·¥å…·ï¼Œå¦‚ä»£ç æ‰§è¡Œç¯å¢ƒå’Œç½‘ç»œæœç´¢ï¼Œå¹¶å°†è¿™äº›æ“ä½œèå…¥æ¨ç†ä¸­ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå•é ç›´æ¥çš„å¼ºåŒ–å­¦ä¹ æ— æ³•æœ‰æ•ˆä¿ƒä½¿å·¥å…·ä½¿ç”¨è¡Œä¸ºï¼Œå› æ­¤æå‡ºäº†å†·å¯åŠ¨é˜¶æ®µå’Œå¼ºåŒ–å­¦ä¹ é˜¶æ®µçš„ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•ã€‚DeepEyesV2åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå±•ç¤ºäº†å…¶åœ¨ç°å®ä¸–ç•Œç†è§£ã€æ•°å­¦æ¨ç†å’Œæœç´¢å¯†é›†å‹ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.04662",
            "title": "VeriCoT: Neuro-symbolic Chain-of-Thought Validation via Logical\n  Consistency Checks",
            "url": "https://huggingface.co/papers/2511.04662",
            "abstract": "VeriCoT, a neuro-symbolic method, formalizes and verifies logical arguments in Chain-of-Thought reasoning to improve the reliability and accuracy of LLMs.  \t\t\t\t\tAI-generated summary \t\t\t\t LLMs can perform multi-step reasoning through Chain-of-Thought (CoT), but they cannot reliably verify their own logic. Even when they reach correct answers, the underlying reasoning may be flawed, undermining trust in high-stakes scenarios. To mitigate this issue, we introduce VeriCoT, a neuro-symbolic method that extracts and verifies formal logical arguments from CoT reasoning. VeriCoT formalizes each CoT reasoning step into first-order logic and identifies premises that ground the argument in source context, commonsense knowledge, or prior reasoning steps. The symbolic representation enables automated solvers to verify logical validity while the NL premises allow humans and systems to identify ungrounded or fallacious reasoning steps. Experiments on the ProofWriter, LegalBench, and BioASQ datasets show VeriCoT effectively identifies flawed reasoning, and serves as a strong predictor of final answer correctness. We also leverage VeriCoT's verification signal for (1) inference-time self-reflection, (2) supervised fine-tuning (SFT) on VeriCoT-distilled datasets and (3) preference fine-tuning (PFT) with direct preference optimization (DPO) using verification-based pairwise rewards, further improving reasoning validity and accuracy.",
            "score": 34,
            "issue_id": 1,
            "pub_date": "2025-11-06",
            "pub_date_card": {
                "ru": "6 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 6",
                "zh": "11æœˆ6æ—¥"
            },
            "hash": "378f11bdb7af3595",
            "authors": [
                "Yu Feng",
                "Nathaniel Weir",
                "Kaj Bostrom",
                "Sam Bayless",
                "Darion Cassel",
                "Sapana Chaudhary",
                "Benjamin Kiesl-Reiter",
                "Huzefa Rangwala"
            ],
            "affiliations": [
                "Amazon Web Services",
                "University of Pennsylvania"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.04662.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#rlhf",
                    "#training"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ›Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "VeriCoT â€” ÑÑ‚Ğ¾ Ğ½ĞµĞ¹Ñ€Ğ¾-ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Chain-of-Thought Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ»Ğ¾Ğ³Ğ¸ĞºÑƒ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ¸Ñ… ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾ÑÑ‹Ğ»ĞºĞ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ, Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸ÑÑ… Ğ¸ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ñ€ĞµÑˆĞ°Ñ‚ĞµĞ»ÑĞ¼ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… ProofWriter, LegalBench Ğ¸ BioASQ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ VeriCoT ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°. Ğ’ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ´Ğ»Ñ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ¸, supervised fine-tuning Ğ¸ preference fine-tuning Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼ DPO, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ LLM."
                },
                "en": {
                    "title": "VeriCoT: Enhancing LLM Reliability through Logical Verification",
                    "desc": "VeriCoT is a neuro-symbolic approach designed to enhance the reliability of large language models (LLMs) by formalizing and verifying their logical arguments during Chain-of-Thought (CoT) reasoning. It translates each reasoning step into first-order logic, allowing for the identification of premises that are grounded in context or prior knowledge. This method enables automated verification of logical validity, helping to pinpoint flawed reasoning that could lead to incorrect conclusions. Experiments demonstrate that VeriCoT not only identifies errors in reasoning but also improves the overall accuracy of LLMs through various fine-tuning techniques."
                },
                "zh": {
                    "title": "VeriCoTï¼šæå‡æ¨ç†å¯é æ€§çš„ç¥ç»ç¬¦å·æ–¹æ³•",
                    "desc": "VeriCoTæ˜¯ä¸€ç§ç¥ç»ç¬¦å·æ–¹æ³•ï¼Œæ—¨åœ¨å½¢å¼åŒ–å’ŒéªŒè¯é“¾å¼æ€ç»´æ¨ç†ä¸­çš„é€»è¾‘è®ºè¯ï¼Œä»è€Œæé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¯é æ€§å’Œå‡†ç¡®æ€§ã€‚å°½ç®¡LLMsèƒ½å¤Ÿè¿›è¡Œå¤šæ­¥æ¨ç†ï¼Œä½†å®ƒä»¬æ— æ³•å¯é åœ°éªŒè¯è‡ªå·±çš„é€»è¾‘ï¼Œå¯¼è‡´åœ¨é«˜é£é™©åœºæ™¯ä¸­ä¿¡ä»»åº¦ä¸‹é™ã€‚VeriCoTé€šè¿‡å°†æ¯ä¸ªæ¨ç†æ­¥éª¤å½¢å¼åŒ–ä¸ºä¸€é˜¶é€»è¾‘ï¼Œå¹¶è¯†åˆ«æ”¯æŒè®ºè¯çš„å‰æï¼Œæ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVeriCoTèƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«æ¨ç†ä¸­çš„ç¼ºé™·ï¼Œå¹¶ä½œä¸ºæœ€ç»ˆç­”æ¡ˆæ­£ç¡®æ€§çš„å¼ºé¢„æµ‹æŒ‡æ ‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.04898",
            "title": "Real-Time Reasoning Agents in Evolving Environments",
            "url": "https://huggingface.co/papers/2511.04898",
            "abstract": "Real-Time Reasoning Gym demonstrates the challenges of deploying language models in dynamic environments, introducing AgileThinker to balance reasoning depth and response latency.  \t\t\t\t\tAI-generated summary \t\t\t\t Agents in the real world must make not only logical but also timely judgments. This requires continuous awareness of the dynamic environment: hazards emerge, opportunities arise, and other agents act, while the agent's reasoning is still unfolding. Despite advances in language model reasoning, existing approaches fail to account for this dynamic nature. We introduce real-time reasoning as a new problem formulation for agents in evolving environments and build Real-Time Reasoning Gym to demonstrate it. We study two paradigms for deploying language models in agents: (1) reactive agents, which employ language models with bounded reasoning computation for rapid responses, and (2) planning agents, which allow extended reasoning computation for complex problems. Our experiments show that even state-of-the-art models struggle with making logical and timely judgments in either paradigm. To address this limitation, we propose AgileThinker, which simultaneously engages both reasoning paradigms. AgileThinker consistently outperforms agents engaging only one reasoning paradigm as the task difficulty and time pressure rise, effectively balancing reasoning depth and response latency. Our work establishes real-time reasoning as a critical testbed for developing practical agents and provides a foundation for research in temporally constrained AI systems, highlighting a path toward real-time capable agents.",
            "score": 11,
            "issue_id": 1,
            "pub_date": "2025-11-07",
            "pub_date_card": {
                "ru": "7 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 7",
                "zh": "11æœˆ7æ—¥"
            },
            "hash": "33419e29d68f1224",
            "authors": [
                "Yule Wen",
                "Yixin Ye",
                "Yanzhe Zhang",
                "Diyi Yang",
                "Hao Zhu"
            ],
            "affiliations": [
                "Georgia Institute of Technology",
                "Shanghai Jiao Tong University",
                "Stanford University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.04898.jpg",
            "data": {
                "categories": [],
                "emoji": "âš¡",
                "ru": {
                    "title": "ĞĞ³ĞµĞ½Ñ‚Ñ‹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸: Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¾Ğ¹ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒÑ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Real-Time Reasoning Gym â€” Ñ‚ĞµÑÑ‚Ğ¾Ğ²ÑƒÑ ÑÑ€ĞµĞ´Ñƒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ÑÑ…, Ğ³Ğ´Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ñ‹ Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²ĞµÑ€Ğ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ Ğ´Ğ²Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ LLM: Ñ€ĞµĞ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ AgileThinker Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±Ğ° Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°, ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒÑ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¼ Ğ¾Ñ‚ĞºĞ»ĞµÑ€Ğ¾Ğ¼ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡Ñ‘Ñ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸."
                },
                "en": {
                    "title": "AgileThinker: Balancing Depth and Speed in Real-Time Reasoning",
                    "desc": "The paper introduces the concept of real-time reasoning for agents operating in dynamic environments, where timely decision-making is crucial. It presents the Real-Time Reasoning Gym, a framework to evaluate how well language models can adapt to changing conditions while reasoning. The authors propose two types of agents: reactive agents for quick responses and planning agents for deeper reasoning, but find that both struggle under pressure. To overcome these challenges, they introduce AgileThinker, which effectively combines both reasoning approaches, leading to better performance in complex and time-sensitive tasks."
                },
                "zh": {
                    "title": "å®æ—¶æ¨ç†ï¼šå¹³è¡¡æ·±åº¦ä¸é€Ÿåº¦çš„æ™ºèƒ½ä½“æ–°ç­–ç•¥",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†åœ¨åŠ¨æ€ç¯å¢ƒä¸­éƒ¨ç½²è¯­è¨€æ¨¡å‹çš„æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†AgileThinkerä»¥å¹³è¡¡æ¨ç†æ·±åº¦å’Œå“åº”å»¶è¿Ÿã€‚ç°å®ä¸–ç•Œä¸­çš„æ™ºèƒ½ä½“ä¸ä»…éœ€è¦é€»è¾‘åˆ¤æ–­ï¼Œè¿˜éœ€è¦åŠæ—¶åšå‡ºå†³ç­–ï¼Œè¿™è¦æ±‚å®ƒä»¬å¯¹ç¯å¢ƒçš„å˜åŒ–ä¿æŒæŒç»­çš„å…³æ³¨ã€‚æˆ‘ä»¬æå‡ºäº†å®æ—¶æ¨ç†ä½œä¸ºä¸€ç§æ–°é—®é¢˜çš„è¡¨è¿°ï¼Œå¹¶æ„å»ºäº†å®æ—¶æ¨ç†è®­ç»ƒåœºæ¥å±•ç¤ºè¿™ä¸€ç‚¹ã€‚å®éªŒè¡¨æ˜ï¼Œå°½ç®¡ç°æœ‰çš„è¯­è¨€æ¨¡å‹å·²ç»å¾ˆå…ˆè¿›ï¼Œä½†åœ¨å¤æ‚é—®é¢˜å’Œæ—¶é—´å‹åŠ›ä¸‹ï¼Œå®ƒä»¬ä»ç„¶éš¾ä»¥åšå‡ºé€»è¾‘å’ŒåŠæ—¶çš„åˆ¤æ–­ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.05369",
            "title": "Dense Motion Captioning",
            "url": "https://huggingface.co/papers/2511.05369",
            "abstract": "A new task, Dense Motion Captioning, is introduced with a large-scale dataset, CompMo, and a model, DEMO, that integrates a language model with a motion adapter to generate detailed, temporally grounded captions for 3D human motion sequences.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in 3D human motion and language integration have primarily focused on text-to-motion generation, leaving the task of motion understanding relatively unexplored. We introduce Dense Motion Captioning, a novel task that aims to temporally localize and caption actions within 3D human motion sequences. Current datasets fall short in providing detailed temporal annotations and predominantly consist of short sequences featuring few actions. To overcome these limitations, we present the Complex Motion Dataset (CompMo), the first large-scale dataset featuring richly annotated, complex motion sequences with precise temporal boundaries. Built through a carefully designed data generation pipeline, CompMo includes 60,000 motion sequences, each composed of multiple actions ranging from at least two to ten, accurately annotated with their temporal extents. We further present DEMO, a model that integrates a large language model with a simple motion adapter, trained to generate dense, temporally grounded captions. Our experiments show that DEMO substantially outperforms existing methods on CompMo as well as on adapted benchmarks, establishing a robust baseline for future research in 3D motion understanding and captioning.",
            "score": 9,
            "issue_id": 1,
            "pub_date": "2025-11-07",
            "pub_date_card": {
                "ru": "7 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 7",
                "zh": "11æœˆ7æ—¥"
            },
            "hash": "dd532f9caca9df70",
            "authors": [
                "Shiyao Xu",
                "Benedetta Liberatori",
                "GÃ¼l Varol",
                "Paolo Rota"
            ],
            "affiliations": [
                "LIGM, Ecole des Ponts, IP Paris, Univ Gustave Eiffel, CNRS",
                "University of Trento"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.05369.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#open_source",
                    "#3d",
                    "#multimodal",
                    "#dataset",
                    "#synthetic"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° Dense Motion Captioning Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸Ñ… Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ² Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ CompMo Ñ 60 000 ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ñ… Ğ¾Ñ‚ 2 Ğ´Ğ¾ 10 Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹. Ğ‘Ñ‹Ğ»Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ DEMO, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğµ, Ğ¿Ñ€Ğ¸Ğ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ ĞºĞ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Revolutionizing 3D Motion Understanding with Dense Captions",
                    "desc": "This paper introduces Dense Motion Captioning, a new task focused on understanding and describing 3D human motion sequences. It presents the Complex Motion Dataset (CompMo), which is a large-scale dataset containing 60,000 richly annotated motion sequences with precise temporal boundaries. The authors propose a model called DEMO that combines a language model with a motion adapter to generate detailed captions that are temporally grounded. The results demonstrate that DEMO significantly outperforms existing methods, providing a strong foundation for future research in the field of 3D motion understanding and captioning."
                },
                "zh": {
                    "title": "å¯†é›†è¿åŠ¨å­—å¹•ç”Ÿæˆï¼š3DåŠ¨ä½œç†è§£çš„æ–°çªç ´",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€é¡¹æ–°ä»»åŠ¡ï¼Œç§°ä¸ºå¯†é›†è¿åŠ¨å­—å¹•ç”Ÿæˆï¼ˆDense Motion Captioningï¼‰ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªå¤§å‹æ•°æ®é›†CompMoå’Œæ¨¡å‹DEMOã€‚è¯¥ä»»åŠ¡æ—¨åœ¨å¯¹3Däººç±»è¿åŠ¨åºåˆ—ä¸­çš„åŠ¨ä½œè¿›è¡Œæ—¶é—´å®šä½å’Œå­—å¹•ç”Ÿæˆã€‚CompMoæ•°æ®é›†åŒ…å«60,000ä¸ªå¤æ‚è¿åŠ¨åºåˆ—ï¼Œæä¾›äº†è¯¦ç»†çš„æ—¶é—´æ³¨é‡Šï¼Œå…‹æœäº†ç°æœ‰æ•°æ®é›†çš„ä¸è¶³ã€‚DEMOæ¨¡å‹ç»“åˆäº†è¯­è¨€æ¨¡å‹å’Œè¿åŠ¨é€‚é…å™¨ï¼Œèƒ½å¤Ÿç”Ÿæˆå¯†é›†ä¸”æ—¶é—´å‡†ç¡®çš„å­—å¹•ï¼Œå®éªŒç»“æœè¡¨æ˜å…¶åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.05017",
            "title": "Towards Mitigating Hallucinations in Large Vision-Language Models by\n  Refining Textual Embeddings",
            "url": "https://huggingface.co/papers/2511.05017",
            "abstract": "Refining textual embeddings with average-pooled visual features improves visual grounding and reduces hallucinations in LVLM architectures.  \t\t\t\t\tAI-generated summary \t\t\t\t In this work, we identify an inherent bias in prevailing LVLM architectures toward the language modality, largely resulting from the common practice of simply appending visual embeddings to the input text sequence. To address this, we propose a simple yet effective method that refines textual embeddings by integrating average-pooled visual features. Our approach demonstrably improves visual grounding and significantly reduces hallucinations on established benchmarks. While average pooling offers a straightforward, robust, and efficient means of incorporating visual information, we believe that more sophisticated fusion methods could further enhance visual grounding and cross-modal alignment. Given that the primary focus of this work is to highlight the modality imbalance and its impact on hallucinations -- and to show that refining textual embeddings with visual information mitigates this issue -- we leave exploration of advanced fusion strategies for future work.",
            "score": 7,
            "issue_id": 1,
            "pub_date": "2025-11-07",
            "pub_date_card": {
                "ru": "7 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 7",
                "zh": "11æœˆ7æ—¥"
            },
            "hash": "1e2a84ff5448137b",
            "authors": [
                "Aakriti Agrawal",
                "Gouthaman KV",
                "Rohith Aralikatti",
                "Gauri Jagatap",
                "Jiaxin Yuan",
                "Vijay Kamarshi",
                "Andrea Fanelli",
                "Furong Huang"
            ],
            "affiliations": [
                "Capital One",
                "Dolby Laboratories",
                "Hilabs",
                "University of Maryland"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.05017.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#architecture",
                    "#multimodal"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ£Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹: ÑƒÑÑ€ĞµĞ´Ğ½Ñ‘Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚ÑŒ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñƒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ĞµÑ‚ Ğ¸Ğ·-Ğ·Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ² Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²ÑƒÑ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ ÑƒÑÑ€ĞµĞ´Ğ½Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ·Ğ°Ğ·ĞµĞ¼Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ñ…Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğµ ÑƒÑÑ€ĞµĞ´Ğ½Ñ‘Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ (average pooling) ĞºĞ°Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ± ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°Ñ„Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Enhancing Textual Embeddings with Visual Features to Reduce Hallucinations",
                    "desc": "This paper addresses a bias in large vision-language models (LVLMs) that favors language over visual information. The authors propose a method to enhance textual embeddings by incorporating average-pooled visual features, which helps improve the model's ability to ground visual content accurately. By refining the textual embeddings in this way, the approach reduces the occurrence of hallucinations, where the model generates incorrect or nonsensical outputs. The study suggests that while their method is effective, more complex fusion techniques could further improve the integration of visual and textual modalities in future research."
                },
                "zh": {
                    "title": "ä¼˜åŒ–æ–‡æœ¬åµŒå…¥ï¼Œæå‡è§†è§‰å®šä½ä¸å‡å°‘å¹»è§‰",
                    "desc": "æœ¬ç ”ç©¶å‘ç°å½“å‰çš„LVLMæ¶æ„åœ¨è¯­è¨€æ¨¡æ€ä¸Šå­˜åœ¨å›ºæœ‰åå·®ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºå°†è§†è§‰åµŒå…¥ç®€å•åœ°é™„åŠ åˆ°è¾“å…¥æ–‡æœ¬åºåˆ—çš„å¸¸è§åšæ³•æ‰€è‡´ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œé€šè¿‡æ•´åˆå¹³å‡æ± åŒ–çš„è§†è§‰ç‰¹å¾æ¥ä¼˜åŒ–æ–‡æœ¬åµŒå…¥ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•æ˜¾è‘—æ”¹å–„äº†è§†è§‰å®šä½ï¼Œå¹¶åœ¨å·²å»ºç«‹çš„åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—å‡å°‘äº†å¹»è§‰ç°è±¡ã€‚è™½ç„¶å¹³å‡æ± åŒ–æ˜¯ä¸€ç§ç›´æ¥ã€ç¨³å¥ä¸”é«˜æ•ˆçš„è§†è§‰ä¿¡æ¯æ•´åˆæ–¹å¼ï¼Œä½†æˆ‘ä»¬è®¤ä¸ºæ›´å¤æ‚çš„èåˆæ–¹æ³•å¯èƒ½ä¼šè¿›ä¸€æ­¥å¢å¼ºè§†è§‰å®šä½å’Œè·¨æ¨¡æ€å¯¹é½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.04707",
            "title": "Jailbreaking in the Haystack",
            "url": "https://huggingface.co/papers/2511.04707",
            "abstract": "NINJA, a jailbreak attack method, appends benign content to harmful goals in long-context language models, increasing attack success rates and revealing vulnerabilities in these models.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in long-context language models (LMs) have enabled million-token inputs, expanding their capabilities across complex tasks like computer-use agents. Yet, the safety implications of these extended contexts remain unclear. To bridge this gap, we introduce NINJA (short for Needle-in-haystack jailbreak attack), a method that jailbreaks aligned LMs by appending benign, model-generated content to harmful user goals. Critical to our method is the observation that the position of harmful goals play an important role in safety. Experiments on standard safety benchmark, HarmBench, show that NINJA significantly increases attack success rates across state-of-the-art open and proprietary models, including LLaMA, Qwen, Mistral, and Gemini. Unlike prior jailbreaking methods, our approach is low-resource, transferable, and less detectable. Moreover, we show that NINJA is compute-optimal -- under a fixed compute budget, increasing context length can outperform increasing the number of trials in best-of-N jailbreak. These findings reveal that even benign long contexts -- when crafted with careful goal positioning -- introduce fundamental vulnerabilities in modern LMs.",
            "score": 3,
            "issue_id": 1,
            "pub_date": "2025-11-05",
            "pub_date_card": {
                "ru": "5 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 5",
                "zh": "11æœˆ5æ—¥"
            },
            "hash": "24320d1ba080516b",
            "authors": [
                "Rishi Rajesh Shah",
                "Chen Henry Wu",
                "Shashwat Saxena",
                "Ziqian Zhong",
                "Alexander Robey",
                "Aditi Raghunathan"
            ],
            "affiliations": [
                "Carnegie Mellon University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.04707.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#alignment",
                    "#agents",
                    "#long_context",
                    "#security"
                ],
                "emoji": "ğŸª¡",
                "ru": {
                    "title": "Ğ˜Ğ³Ğ¾Ğ»ĞºĞ° Ğ² ÑÑ‚Ğ¾Ğ³Ğµ ÑĞµĞ½Ğ°: ĞºĞ°Ğº Ğ±ĞµĞ·Ğ¾Ğ±Ğ¸Ğ´Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ñ€Ğ°Ğ·Ñ€ÑƒÑˆĞ°ĞµÑ‚ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ NINJA Ğ´Ğ»Ñ Ğ²Ğ·Ğ»Ğ¾Ğ¼Ğ° Ğ·Ğ°Ñ‰Ğ¸Ñ‰Ñ‘Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ±ĞµĞ·Ğ¾Ğ±Ğ¸Ğ´Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ, Ğº Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ğ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ğ¾Ğ¹ Ñ†ĞµĞ»Ğ¸ Ğ² Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¸Ğ³Ñ€Ğ°ĞµÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ€Ğ¾Ğ»ÑŒ Ğ² Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ HarmBench Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ñ‚Ğ°Ğº Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ LLaMA, Qwen, Mistral Ğ¸ Gemini. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¸Ğ·ĞºĞ¸Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸, Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "NINJA: Uncovering Vulnerabilities in Long-Context Language Models",
                    "desc": "The paper introduces NINJA, a novel jailbreak attack method that exploits long-context language models by appending harmless content to harmful objectives. This technique enhances the success rates of attacks by strategically positioning harmful goals within the input context. Experiments demonstrate that NINJA effectively compromises various state-of-the-art models, revealing significant safety vulnerabilities. The findings suggest that even seemingly benign content can pose risks when used inappropriately, highlighting the need for improved safety measures in long-context LMs."
                },
                "zh": {
                    "title": "NINJAï¼šæ­ç¤ºè¯­è¨€æ¨¡å‹çš„è„†å¼±æ€§",
                    "desc": "NINJAæ˜¯ä¸€ç§è¶Šç‹±æ”»å‡»æ–¹æ³•ï¼Œé€šè¿‡åœ¨é•¿ä¸Šä¸‹æ–‡è¯­è¨€æ¨¡å‹ä¸­é™„åŠ è‰¯æ€§å†…å®¹æ¥å®ç°å¯¹æœ‰å®³ç›®æ ‡çš„æ”»å‡»ï¼Œä»è€Œæé«˜æ”»å‡»æˆåŠŸç‡å¹¶æ­ç¤ºæ¨¡å‹çš„è„†å¼±æ€§ã€‚è¯¥æ–¹æ³•çš„å…³é”®åœ¨äºæœ‰å®³ç›®æ ‡çš„ä½ç½®å¯¹å®‰å…¨æ€§çš„é‡è¦å½±å“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNINJAåœ¨å¤šä¸ªå…ˆè¿›çš„å¼€æ”¾å’Œä¸“æœ‰æ¨¡å‹ä¸Šæ˜¾è‘—æé«˜äº†æ”»å‡»æˆåŠŸç‡ï¼ŒåŒ…æ‹¬LLaMAã€Qwenã€Mistralå’ŒGeminiã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œå³ä½¿æ˜¯ç»è¿‡ç²¾å¿ƒè®¾è®¡çš„è‰¯æ€§é•¿ä¸Šä¸‹æ–‡ï¼Œä¹Ÿå¯èƒ½åœ¨ç°ä»£è¯­è¨€æ¨¡å‹ä¸­å¼•å…¥æ ¹æœ¬æ€§çš„è„†å¼±æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.01047",
            "title": "HAFixAgent: History-Aware Automated Program Repair Agent",
            "url": "https://huggingface.co/papers/2511.01047",
            "abstract": "HAFixAgent, a history-aware bug-fixing agent, improves automated program repair by incorporating repository history, enhancing effectiveness and efficiency for complex multi-hunk bugs.  \t\t\t\t\tAI-generated summary \t\t\t\t Automated program repair (APR) has recently shifted toward large language models and agent-based systems, yet most systems rely on local snapshot context, overlooking repository history. Prior work shows that repository history helps repair single-line bugs, since the last commit touching the buggy line is often the bug-introducing one. In this paper, we investigate whether repository history can also improve agentic APR systems at scale, especially for complex multi-hunk bugs. We present HAFixAgent, a History-Aware Bug-Fixing Agent that injects blame-derived repository heuristics into its repair loop. A preliminary study of all 854 real-world bugs from Defects4J motivates our design, showing that bug-relevant history is both widely available and highly concentrated. Empirical comparison of HAFixAgent with two state-of-the-art baselines shows: (1) Effectiveness: HAFixAgent significantly improves over the agent-based baseline (by 212.3%) and the multi-hunk baseline (by 29.9%). (2) Efficiency: history does not significantly increase agent steps and keeps token costs comparable, with notably lower median costs for complex multi-file-multi-hunk bugs. (3) Practicality: combining different historical heuristics repairs more bugs, offering a clear cost-benefit trade-off. HAFixAgent offers a practical recipe for history-aware agentic APR: ground the agent in version control history, prioritize diff-based historical context, and integrate complementary heuristics when needed.",
            "score": 3,
            "issue_id": 1,
            "pub_date": "2025-11-02",
            "pub_date_card": {
                "ru": "2 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 2",
                "zh": "11æœˆ2æ—¥"
            },
            "hash": "91f8e98e37d5b148",
            "authors": [
                "Yu Shi",
                "Hao Li",
                "Bram Adams",
                "Ahmed E. Hassan"
            ],
            "affiliations": [
                "Queens University, Canada"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.01047.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ”§",
                "ru": {
                    "title": "Ğ˜ÑÑ‚Ğ¾Ñ€Ğ¸Ñ ĞºĞ¾Ğ´Ğ° ĞºĞ°Ğº ĞºĞ»ÑÑ‡ Ğº ÑƒĞ¼Ğ½Ğ¾Ğ¼Ñƒ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº",
                    "desc": "HAFixAgent â€” ÑÑ‚Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰Ğ° ĞºĞ¾Ğ´Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ğ¾Ñ‡Ğ¸Ğ½ĞºĞ¸ Ğ±Ğ°Ğ³Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ blame-Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ² Ñ†Ğ¸ĞºĞ» Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ±Ğ»Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ: Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ½Ğ° 212% Ğ´Ğ»Ñ Ğ¾Ğ´Ğ½Ğ¾Ñ…ÑƒĞ½ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ ÑĞ»ÑƒÑ‡Ğ°Ñ Ğ¸ Ğ½Ğ° 30% Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…ÑƒĞ½ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ²ĞµÑ€ÑĞ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ· ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚."
                },
                "en": {
                    "title": "Harnessing History for Smarter Bug Fixing",
                    "desc": "HAFixAgent is a novel approach to automated program repair (APR) that leverages repository history to enhance the repair of complex multi-hunk bugs. By incorporating blame-derived heuristics from version control history, it significantly improves the effectiveness of bug fixing, outperforming existing agent-based systems. The empirical results demonstrate that HAFixAgent repairs more bugs while maintaining efficiency, as it does not substantially increase the number of agent steps or costs. This paper highlights the importance of historical context in APR, providing a practical framework for integrating repository insights into bug-fixing agents."
                },
                "zh": {
                    "title": "å†å²æ„ŸçŸ¥ï¼Œæ™ºèƒ½ä¿®å¤ï¼",
                    "desc": "HAFixAgentæ˜¯ä¸€ç§å†å²æ„ŸçŸ¥çš„è‡ªåŠ¨ä¿®å¤ä»£ç†ï¼Œæ—¨åœ¨é€šè¿‡å¼•å…¥ä»£ç åº“å†å²æ¥æé«˜å¤æ‚å¤šå—é”™è¯¯çš„ä¿®å¤æ•ˆæœå’Œæ•ˆç‡ã€‚ä»¥å¾€çš„ç ”ç©¶è¡¨æ˜ï¼Œä»£ç åº“å†å²å¯¹äºä¿®å¤å•è¡Œé”™è¯¯éå¸¸æœ‰æ•ˆï¼Œå› ä¸ºæœ€åä¸€æ¬¡ä¿®æ”¹é”™è¯¯è¡Œçš„æäº¤é€šå¸¸æ˜¯å¼•å…¥é”™è¯¯çš„æäº¤ã€‚æœ¬æ–‡æ¢è®¨äº†å†å²ä¿¡æ¯æ˜¯å¦ä¹Ÿèƒ½åœ¨å¤§è§„æ¨¡çš„ä»£ç†è‡ªåŠ¨ä¿®å¤ç³»ç»Ÿä¸­æ”¹å–„ä¿®å¤æ•ˆæœï¼Œå°¤å…¶æ˜¯é’ˆå¯¹å¤æ‚çš„å¤šå—é”™è¯¯ã€‚HAFixAgenté€šè¿‡å°†åŸºäºè´£ä»»çš„å†å²å¯å‘å¼æ–¹æ³•æ³¨å…¥ä¿®å¤å¾ªç¯ï¼Œæ˜¾è‘—æé«˜äº†ä¿®å¤æ•ˆç‡å’Œæ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.24505",
            "title": "CritiCal: Can Critique Help LLM Uncertainty or Confidence Calibration?",
            "url": "https://huggingface.co/papers/2510.24505",
            "abstract": "Natural language critiques improve confidence calibration in LLMs, with CritiCal training method outperforming other approaches and enhancing reliability.  \t\t\t\t\tAI-generated summary \t\t\t\t Accurate confidence calibration in Large Language Models (LLMs) is critical for safe use in high-stakes domains, where clear verbalized confidence enhances user trust. Traditional methods that mimic reference confidence expressions often fail to capture the reasoning needed for accurate confidence assessment. We propose natural language critiques as a solution, ideally suited for confidence calibration, as precise gold confidence labels are hard to obtain and often require multiple generations. This paper studies how natural language critiques can enhance verbalized confidence, addressing: (1) What to critique: uncertainty (question-focused) or confidence (answer-specific)? Analysis shows confidence suits multiple-choice tasks, while uncertainty excels in open-ended scenarios. (2) How to critique: self-critique or critique calibration training? We propose Self-Critique, enabling LLMs to critique and optimize their confidence beyond mere accuracy, and CritiCal, a novel Critique Calibration training method that leverages natural language critiques to improve confidence calibration, moving beyond direct numerical optimization. Experiments show that CritiCal significantly outperforms Self-Critique and other competitive baselines, even surpassing its teacher model, GPT-4o, in complex reasoning tasks. CritiCal also shows robust generalization in out-of-distribution settings, advancing LLM's reliability.",
            "score": 3,
            "issue_id": 1,
            "pub_date": "2025-10-28",
            "pub_date_card": {
                "ru": "28 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 28",
                "zh": "10æœˆ28æ—¥"
            },
            "hash": "89e565a8c78ad0bd",
            "authors": [
                "Qing Zong",
                "Jiayu Liu",
                "Tianshi Zheng",
                "Chunyang Li",
                "Baixuan Xu",
                "Haochen Shi",
                "Weiqi Wang",
                "Zhaowei Wang",
                "Chunkit Chan",
                "Yangqiu Song"
            ],
            "affiliations": [
                "Department of Computer Science and Engineering, HKUST"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2510.24505.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#training"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "ĞšÑ€Ğ¸Ñ‚Ğ¸ĞºĞ° Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ ĞºĞ°Ğº Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾Ğ¹ ÑĞ°Ğ¼Ğ¾Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¸ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… ĞºÑ€Ğ¸Ñ‚Ğ¸Ğº. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¾Ğ²Ğ°Ñ‚ÑŒ (Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ»Ğ¸ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ) Ğ¸ ĞºĞ°Ğº ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¾Ğ²Ğ°Ñ‚ÑŒ (ÑĞ°Ğ¼Ğ¾ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ° Ğ¸Ğ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¾Ğ¹), Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ, Ñ‡Ñ‚Ğ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ° ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğ¼. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ CritiCal, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞµ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¹ Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ CritiCal Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ Ğ´Ğ°Ğ¶Ğµ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ GPT-4o Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ñ‹Ñ… ÑĞ´Ğ²Ğ¸Ğ³Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Enhancing LLM Reliability with Natural Language Critiques",
                    "desc": "This paper introduces a method called CritiCal that uses natural language critiques to improve the confidence calibration of Large Language Models (LLMs). Accurate confidence calibration is essential for ensuring that LLMs can be trusted in important situations, and traditional methods often fall short. The study explores how to effectively critique LLM outputs, determining that confidence critiques work best for multiple-choice tasks while uncertainty critiques are better for open-ended questions. The results show that CritiCal outperforms other methods, including Self-Critique, and enhances the reliability of LLMs in complex reasoning tasks."
                },
                "zh": {
                    "title": "è‡ªç„¶è¯­è¨€æ‰¹è¯„æå‡LLMsä¿¡å¿ƒæ ¡å‡†",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†è‡ªç„¶è¯­è¨€æ‰¹è¯„å¦‚ä½•æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ä¿¡å¿ƒæ ¡å‡†ã€‚ä¼ ç»Ÿæ–¹æ³•å¾€å¾€æ— æ³•å‡†ç¡®è¯„ä¼°ä¿¡å¿ƒï¼Œè€Œè‡ªç„¶è¯­è¨€æ‰¹è¯„æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬æå‡ºäº†è‡ªæˆ‘æ‰¹è¯„å’Œæ‰¹è¯„æ ¡å‡†è®­ç»ƒï¼ˆCritiCalï¼‰ä¸¤ç§æ–¹æ³•ï¼Œå‰è€…å¸®åŠ©æ¨¡å‹ä¼˜åŒ–ä¿¡å¿ƒï¼Œåè€…åˆ™åˆ©ç”¨è‡ªç„¶è¯­è¨€æ‰¹è¯„æ¥æå‡ä¿¡å¿ƒæ ¡å‡†çš„æ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCritiCalæ–¹æ³•åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå¢å¼ºäº†æ¨¡å‹çš„å¯é æ€§ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-11-07.html",
    "link_next": "2025-11-11.html",
    "link_month": "2025-11.html",
    "short_date_prev": {
        "ru": "07.11",
        "en": "11/07",
        "zh": "11æœˆ7æ—¥"
    },
    "short_date_next": {
        "ru": "11.11",
        "en": "11/11",
        "zh": "11æœˆ11æ—¥"
    },
    "categories": {
        "#dataset": 4,
        "#data": 0,
        "#benchmark": 6,
        "#agents": 2,
        "#cv": 1,
        "#rl": 2,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 4,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 2,
        "#story_generation": 1,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}