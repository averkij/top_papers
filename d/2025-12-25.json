{
    "date": {
        "ru": "25 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
        "en": "December 25",
        "zh": "12æœˆ25æ—¥"
    },
    "time_utc": "2025-12-25 10:24",
    "weekday": 3,
    "issue_id": 242,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2512.20557",
            "title": "Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models",
            "url": "https://huggingface.co/papers/2512.20557",
            "abstract": "DSR Suite enhances vision-language models with dynamic spatial reasoning through automated data generation and a geometry selection module that integrates geometric priors.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-language models (VLM) excel at general understanding yet remain weak at dynamic spatial reasoning (DSR), i.e., reasoning about the evolvement of object geometry and relationship in 3D space over time, largely due to the scarcity of scalable 4D-aware training resources. To bridge this gap across aspects of dataset, benchmark and model, we introduce DSR Suite. First, we propose an automated pipeline that generates multiple-choice question-answer pairs from in-the-wild videos for DSR. By leveraging modern vision foundation models, the pipeline extracts rich geometric and motion information, including camera poses, local point clouds, object masks, orientations, and 3D trajectories. These geometric cues enable the construction of DSR-Train for learning and further human-refined DSR-Bench for evaluation. Compared with previous works, our data emphasize (i) in-the-wild video sources, (ii) object- and scene-level 3D requirements, (iii) viewpoint transformations, (iv) multi-object interactions, and (v) fine-grained, procedural answers. Beyond data, we propose a lightweight Geometry Selection Module (GSM) to seamlessly integrate geometric priors into VLMs, which condenses question semantics and extracts question-relevant knowledge from pretrained 4D reconstruction priors into a compact set of geometry tokens. This targeted extraction avoids overwhelming the model with irrelevant knowledge. Experiments show that integrating DSR-Train and GSM into Qwen2.5-VL-7B significantly enhances its dynamic spatial reasoning capability, while maintaining accuracy on general video understanding benchmarks.",
            "score": 37,
            "issue_id": 235,
            "pub_date": "2025-12-23",
            "pub_date_card": {
                "ru": "23 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 23",
                "zh": "12æœˆ23æ—¥"
            },
            "hash": "03a0333b07003ea9",
            "authors": [
                "Shengchao Zhou",
                "Yuxin Chen",
                "Yuying Ge",
                "Wei Huang",
                "Jiehong Lin",
                "Ying Shan",
                "Xiaojuan Qi"
            ],
            "affiliations": [
                "ARC Lab, Tencent PCG",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.20557.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#architecture",
                    "#video",
                    "#multimodal",
                    "#benchmark",
                    "#3d"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ“ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ Ğ² Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· 4D Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° DSR Suite Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ¾ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¸ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² 3D Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼Ğ¸, Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· foundation Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ñ‹, Ğ¾Ğ±Ğ»Ğ°ĞºĞ° Ñ‚Ğ¾Ñ‡ĞµĞº, Ğ¼Ğ°ÑĞºĞ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ 3D Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Geometry Selection Module (GSM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ñ‹ Ğ² vision-language Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ñ‚Ğ±Ğ¸Ñ€Ğ°Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ GSM Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Vision-Language Models with Dynamic Spatial Reasoning",
                    "desc": "The DSR Suite improves vision-language models by focusing on dynamic spatial reasoning (DSR), which involves understanding how objects change and relate in 3D space over time. It introduces an automated data generation pipeline that creates question-answer pairs from real-world videos, capturing essential geometric and motion details. Additionally, a Geometry Selection Module (GSM) is implemented to incorporate geometric knowledge into the models without overwhelming them with unnecessary information. Experiments demonstrate that this approach significantly boosts the DSR capabilities of the Qwen2.5-VL-7B model while preserving its performance on general video understanding tasks."
                },
                "zh": {
                    "title": "åŠ¨æ€ç©ºé—´æ¨ç†çš„å¢å¼ºå·¥å…·",
                    "desc": "DSR Suite æ˜¯ä¸€ç§å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹çš„å·¥å…·ï¼Œä¸“æ³¨äºåŠ¨æ€ç©ºé—´æ¨ç†ï¼ˆDSRï¼‰ã€‚å®ƒé€šè¿‡è‡ªåŠ¨åŒ–æ•°æ®ç”Ÿæˆå’Œå‡ ä½•é€‰æ‹©æ¨¡å—ï¼Œç»“åˆå‡ ä½•å…ˆéªŒï¼Œæ¥è§£å†³æ¨¡å‹åœ¨ä¸‰ç»´ç©ºé—´ä¸­å¯¹è±¡å‡ ä½•å’Œå…³ç³»æ¼”å˜çš„æ¨ç†èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚è¯¥å·¥å…·ç”Ÿæˆå¤šé€‰é¢˜é—®ç­”å¯¹ï¼Œå¹¶æå–ä¸°å¯Œçš„å‡ ä½•å’Œè¿åŠ¨ä¿¡æ¯ï¼Œä»¥æ„å»ºç”¨äºå­¦ä¹ çš„ DSR-Train å’Œç”¨äºè¯„ä¼°çš„ DSR-Benchã€‚å®éªŒè¡¨æ˜ï¼Œå°† DSR-Train å’Œå‡ ä½•é€‰æ‹©æ¨¡å—æ•´åˆåˆ°æ¨¡å‹ä¸­ï¼Œæ˜¾è‘—æå‡äº†åŠ¨æ€ç©ºé—´æ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒäº†å¯¹ä¸€èˆ¬è§†é¢‘ç†è§£åŸºå‡†çš„å‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.16093",
            "title": "TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times",
            "url": "https://huggingface.co/papers/2512.16093",
            "abstract": "TurboDiffusion accelerates video generation by 100-200x using attention acceleration, step distillation, and quantization, while maintaining video quality.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce TurboDiffusion, a video generation acceleration framework that can speed up end-to-end diffusion generation by 100-200x while maintaining video quality. TurboDiffusion mainly relies on several components for acceleration: (1) Attention acceleration: TurboDiffusion uses low-bit SageAttention and trainable Sparse-Linear Attention (SLA) to speed up attention computation. (2) Step distillation: TurboDiffusion adopts rCM for efficient step distillation. (3) W8A8 quantization: TurboDiffusion quantizes model parameters and activations to 8 bits to accelerate linear layers and compress the model. In addition, TurboDiffusion incorporates several other engineering optimizations.   We conduct experiments on the Wan2.2-I2V-14B-720P, Wan2.1-T2V-1.3B-480P, Wan2.1-T2V-14B-720P, and Wan2.1-T2V-14B-480P models. Experimental results show that TurboDiffusion achieves 100-200x speedup for video generation even on a single RTX 5090 GPU, while maintaining comparable video quality. The GitHub repository, which includes model checkpoints and easy-to-use code, is available at https://github.com/thu-ml/TurboDiffusion.",
            "score": 34,
            "issue_id": 235,
            "pub_date": "2025-12-18",
            "pub_date_card": {
                "ru": "18 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 18",
                "zh": "12æœˆ18æ—¥"
            },
            "hash": "cc018bddf4ff209e",
            "authors": [
                "Jintao Zhang",
                "Kaiwen Zheng",
                "Kai Jiang",
                "Haoxu Wang",
                "Ion Stoica",
                "Joseph E. Gonzalez",
                "Jianfei Chen",
                "Jun Zhu"
            ],
            "affiliations": [
                "Shengshu Technology",
                "Tsinghua University",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.16093.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#video",
                    "#open_source",
                    "#diffusion",
                    "#inference",
                    "#optimization"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "ĞœĞ¾Ğ»Ğ½Ğ¸ĞµĞ½Ğ¾ÑĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾: 100-200x ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°",
                    "desc": "TurboDiffusion â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‰Ğ¸Ğ¹ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ² 100-200 Ñ€Ğ°Ğ· Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ attention-Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ½Ğ¸Ğ·ĞºĞ¾Ğ±Ğ¸Ñ‚Ğ¾Ğ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½ÑƒÑ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½ÑƒÑ attention, Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ ÑˆĞ°Ğ³Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ñ‡Ğ¸ÑĞ»Ğ° Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ´Ğ¾ 8 Ğ±Ğ¸Ñ‚. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ°Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ¶Ğµ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ GPU. ĞŸÑ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ° Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼ Ñ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ñ‡ĞµĞºĞ¿Ğ¾Ğ¹Ğ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "TurboDiffusion: Speeding Up Video Generation by 100-200x!",
                    "desc": "TurboDiffusion is a framework designed to significantly speed up video generation processes by 100-200 times while preserving high video quality. It achieves this acceleration through three main techniques: attention acceleration using low-bit SageAttention and Sparse-Linear Attention, step distillation with a method called rCM, and W8A8 quantization that reduces model parameters to 8 bits. These innovations allow for faster computation in attention mechanisms and efficient model compression. Experimental results demonstrate that TurboDiffusion can effectively enhance video generation speed even on a single high-performance GPU, making it a powerful tool for AI video generation."
                },
                "zh": {
                    "title": "TurboDiffusionï¼šè§†é¢‘ç”Ÿæˆé€Ÿåº¦æå‡100-200å€",
                    "desc": "TurboDiffusion æ˜¯ä¸€ä¸ªåŠ é€Ÿè§†é¢‘ç”Ÿæˆçš„æ¡†æ¶ï¼Œå¯ä»¥å°†è§†é¢‘ç”Ÿæˆé€Ÿåº¦æé«˜ 100-200 å€ï¼ŒåŒæ—¶ä¿æŒè§†é¢‘è´¨é‡ã€‚å®ƒä¸»è¦ä¾èµ–å‡ ä¸ªç»„ä»¶æ¥å®ç°åŠ é€Ÿï¼šé¦–å…ˆï¼Œä½¿ç”¨ä½ä½æ•°çš„ SageAttention å’Œå¯è®­ç»ƒçš„ç¨€ç–çº¿æ€§æ³¨æ„åŠ›ï¼ˆSLAï¼‰æ¥åŠ é€Ÿæ³¨æ„åŠ›è®¡ç®—ï¼›å…¶æ¬¡ï¼Œé‡‡ç”¨ rCM è¿›è¡Œé«˜æ•ˆçš„æ­¥éª¤è’¸é¦ï¼›æœ€åï¼Œé€šè¿‡å°†æ¨¡å‹å‚æ•°å’Œæ¿€æ´»é‡åŒ–ä¸º 8 ä½æ¥åŠ é€Ÿçº¿æ€§å±‚å¹¶å‹ç¼©æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿åœ¨å•ä¸ª RTX 5090 GPU ä¸Šï¼ŒTurboDiffusion ä¹Ÿèƒ½å®ç° 100-200 å€çš„è§†é¢‘ç”ŸæˆåŠ é€Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.21094",
            "title": "T2AV-Compass: Towards Unified Evaluation for Text-to-Audio-Video Generation",
            "url": "https://huggingface.co/papers/2512.21094",
            "abstract": "Text-to-Audio-Video (T2AV) generation aims to synthesize temporally coherent video and semantically synchronized audio from natural language, yet its evaluation remains fragmented, often relying on unimodal metrics or narrowly scoped benchmarks that fail to capture cross-modal alignment, instruction following, and perceptual realism under complex prompts. To address this limitation, we present T2AV-Compass, a unified benchmark for comprehensive evaluation of T2AV systems, consisting of 500 diverse and complex prompts constructed via a taxonomy-driven pipeline to ensure semantic richness and physical plausibility. Besides, T2AV-Compass introduces a dual-level evaluation framework that integrates objective signal-level metrics for video quality, audio quality, and cross-modal alignment with a subjective MLLM-as-a-Judge protocol for instruction following and realism assessment. Extensive evaluation of 11 representative T2AVsystems reveals that even the strongest models fall substantially short of human-level realism and cross-modal consistency, with persistent failures in audio realism, fine-grained synchronization, instruction following, etc. These results indicate significant improvement room for future models and highlight the value of T2AV-Compass as a challenging and diagnostic testbed for advancing text-to-audio-video generation.",
            "score": 22,
            "issue_id": 235,
            "pub_date": "2025-12-24",
            "pub_date_card": {
                "ru": "24 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 24",
                "zh": "12æœˆ24æ—¥"
            },
            "hash": "2d1ce69f802bf812",
            "authors": [
                "Zhe Cao",
                "Tao Wang",
                "Jiaming Wang",
                "Yanghai Wang",
                "Yuanxing Zhang",
                "Jialu Chen",
                "Miao Deng",
                "Jiahao Wang",
                "Yubin Guo",
                "Chenxi Liao",
                "Yize Zhang",
                "Zhaoxiang Zhang",
                "Jiaheng Liu"
            ],
            "affiliations": [
                "Institute of Automation, Chinese Academy of Sciences",
                "Kuaishou Technology",
                "Nanjing University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.21094.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#video",
                    "#multimodal",
                    "#benchmark",
                    "#audio",
                    "#survey"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ·Ğ²ÑƒĞºĞ° Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ T2AV-Compass â€” ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 500 Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²ÑƒÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¾Ğ¹ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ LLM Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑÑƒĞ´ÑŒĞ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑÑ‚ÑƒĞ¿Ğ°ÑÑ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¼Ñƒ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¼ĞµĞ¶Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¸Ğ¼ĞµÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¾Ğ¼ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹."
                },
                "en": {
                    "title": "T2AV-Compass: Elevating Text-to-Audio-Video Evaluation",
                    "desc": "This paper introduces T2AV-Compass, a new benchmark designed to evaluate Text-to-Audio-Video (T2AV) generation systems more effectively. It addresses the current limitations in evaluation methods by providing a comprehensive set of 500 complex prompts that ensure both semantic richness and physical plausibility. The benchmark features a dual-level evaluation framework that combines objective metrics for video and audio quality with subjective assessments of instruction following and realism. The findings reveal that existing T2AV models struggle to achieve human-level performance, particularly in audio realism and synchronization, indicating a need for further advancements in the field."
                },
                "zh": {
                    "title": "T2AV-Compassï¼šæ¨åŠ¨æ–‡æœ¬åˆ°éŸ³é¢‘è§†é¢‘ç”Ÿæˆçš„è¯„ä¼°æ–°æ ‡å‡†",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„åŸºå‡†æµ‹è¯•å·¥å…·T2AV-Compassï¼Œç”¨äºè¯„ä¼°æ–‡æœ¬åˆ°éŸ³é¢‘è§†é¢‘ç”Ÿæˆï¼ˆT2AVï¼‰ç³»ç»Ÿçš„æ€§èƒ½ã€‚è¯¥åŸºå‡†åŒ…å«500ä¸ªå¤šæ ·åŒ–å’Œå¤æ‚çš„æç¤ºï¼Œç¡®ä¿è¯­ä¹‰ä¸°å¯Œæ€§å’Œç‰©ç†åˆç†æ€§ã€‚T2AV-Compassé‡‡ç”¨åŒå±‚è¯„ä¼°æ¡†æ¶ï¼Œç»“åˆäº†å®¢è§‚çš„ä¿¡å·çº§æŒ‡æ ‡å’Œä¸»è§‚çš„è¯„ä¼°åè®®ï¼Œä»¥å…¨é¢è¯„ä¼°ç”Ÿæˆçš„éŸ³é¢‘å’Œè§†é¢‘è´¨é‡ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç°æœ‰æ¨¡å‹åœ¨éŸ³é¢‘çœŸå®æ„Ÿå’Œè·¨æ¨¡æ€ä¸€è‡´æ€§æ–¹é¢ä»æœ‰å¾ˆå¤§æ”¹è¿›ç©ºé—´ï¼Œå¼ºè°ƒäº†T2AV-Compassåœ¨æ¨åŠ¨T2AVç”ŸæˆæŠ€æœ¯å‘å±•ä¸­çš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.21252",
            "title": "DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation",
            "url": "https://huggingface.co/papers/2512.21252",
            "abstract": "The \"one-shot\" technique represents a distinct and sophisticated aesthetic in filmmaking. However, its practical realization is often hindered by prohibitive costs and complex real-world constraints. Although emerging video generation models offer a virtual alternative, existing approaches typically rely on naive clip concatenation, which frequently fails to maintain visual smoothness and temporal coherence. In this paper, we introduce DreaMontage, a comprehensive framework designed for arbitrary frame-guided generation, capable of synthesizing seamless, expressive, and long-duration one-shot videos from diverse user-provided inputs. To achieve this, we address the challenge through three primary dimensions. (i) We integrate a lightweight intermediate-conditioning mechanism into the DiT architecture. By employing an Adaptive Tuning strategy that effectively leverages base training data, we unlock robust arbitrary-frame control capabilities. (ii) To enhance visual fidelity and cinematic expressiveness, we curate a high-quality dataset and implement a Visual Expression SFT stage. In addressing critical issues such as subject motion rationality and transition smoothness, we apply a Tailored DPO scheme, which significantly improves the success rate and usability of the generated content. (iii) To facilitate the production of extended sequences, we design a Segment-wise Auto-Regressive (SAR) inference strategy that operates in a memory-efficient manner. Extensive experiments demonstrate that our approach achieves visually striking and seamlessly coherent one-shot effects while maintaining computational efficiency, empowering users to transform fragmented visual materials into vivid, cohesive one-shot cinematic experiences.",
            "score": 20,
            "issue_id": 238,
            "pub_date": "2025-12-24",
            "pub_date_card": {
                "ru": "24 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 24",
                "zh": "12æœˆ24æ—¥"
            },
            "hash": "a6f7b00490f90d69",
            "authors": [
                "Jiawei Liu",
                "Junqiao Li",
                "Jiangfan Deng",
                "Gen Li",
                "Siyu Zhou",
                "Zetao Fang",
                "Shanshan Lao",
                "Zengde Deng",
                "Jianing Zhu",
                "Tingting Ma",
                "Jiayi Li",
                "Yunqiu Wang",
                "Qian He",
                "Xinglong Wu"
            ],
            "affiliations": [
                "Intelligence Creation Team, ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.21252.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#dataset",
                    "#training",
                    "#video",
                    "#architecture"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² ĞºĞ¸Ğ½ĞµĞ¼Ğ°Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ°æ¡†æ¶ DreaMontage Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² ÑÑ‚Ğ¸Ğ»Ğµ ĞºĞ¸Ğ½ĞµĞ¼Ğ°Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ°-ÑÑ†ĞµĞ½Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ²-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ DiT Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¾Ğ¹ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ½Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Visual Expression SFT Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ ÑÑ…ĞµĞ¼Ğ¾Ğ¹ DPO Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ»Ğ°Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¾Ğ² Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ”Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Segment-wise Auto-Regressive inference, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ°Ñ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ."
                },
                "en": {
                    "title": "Seamless One-Shot Video Generation with DreaMontage",
                    "desc": "This paper presents DreaMontage, a novel framework for generating one-shot videos that are visually smooth and coherent. It addresses the limitations of existing video generation methods by integrating a lightweight conditioning mechanism and an Adaptive Tuning strategy for better frame control. The authors enhance visual quality through a curated dataset and a Visual Expression SFT stage, while also improving motion rationality and transition smoothness with a Tailored DPO scheme. Finally, they introduce a Segment-wise Auto-Regressive inference strategy to efficiently produce long video sequences, resulting in impressive cinematic outputs from user inputs."
                },
                "zh": {
                    "title": "æ— ç¼ç”Ÿæˆä¸€é•œåˆ°åº•è§†é¢‘çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºDreaMontageçš„æ¡†æ¶ï¼Œæ—¨åœ¨ç”Ÿæˆæ— ç¼ä¸”å¯Œæœ‰è¡¨ç°åŠ›çš„é•¿æ—¶æ®µä¸€é•œåˆ°åº•è§†é¢‘ã€‚æˆ‘ä»¬é€šè¿‡é›†æˆè½»é‡çº§çš„ä¸­é—´æ¡ä»¶æœºåˆ¶å’Œè‡ªé€‚åº”è°ƒä¼˜ç­–ç•¥ï¼Œå¢å¼ºäº†å¯¹ä»»æ„å¸§çš„æ§åˆ¶èƒ½åŠ›ã€‚ä¸ºäº†æé«˜è§†è§‰è´¨é‡å’Œç”µå½±è¡¨ç°åŠ›ï¼Œæˆ‘ä»¬æ„å»ºäº†é«˜è´¨é‡çš„æ•°æ®é›†ï¼Œå¹¶å®æ–½äº†è§†è§‰è¡¨è¾¾çš„å¾®è°ƒé˜¶æ®µã€‚æœ€åï¼Œæˆ‘ä»¬è®¾è®¡äº†æ®µå¼è‡ªå›å½’æ¨ç†ç­–ç•¥ï¼Œä»¥é«˜æ•ˆçš„æ–¹å¼ç”Ÿæˆæ‰©å±•åºåˆ—ï¼Œç¡®ä¿ç”Ÿæˆå†…å®¹çš„è¿è´¯æ€§å’Œè§†è§‰å†²å‡»åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.21337",
            "title": "Beyond Memorization: A Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models",
            "url": "https://huggingface.co/papers/2512.21337",
            "abstract": "We expose a significant popularity bias in state-of-the-art vision-language models (VLMs), which achieve up to 34% higher accuracy on famous buildings compared to ordinary ones, indicating a reliance on memorization over generalizable understanding. To systematically investigate this, we introduce the largest open benchmark for this task: the YearGuessr dataset, a collection of 55,546 building images with multi-modal attributes from 157 countries, annotated with continuous ordinal labels of their construction year (1001-2024), GPS data, and page-view counts as a proxy for popularity. Using this dataset, we frame the construction year prediction task as ordinal regression and introduce popularity-aware interval accuracy metrics to quantify this bias. Our resulting benchmark of 30+ models, including our YearCLIP model, confirms that VLMs excel on popular, memorized items but struggle significantly with unrecognized subjects, exposing a critical flaw in their reasoning capabilities. Project page: https://sytwu.github.io/BeyondMemo/",
            "score": 14,
            "issue_id": 238,
            "pub_date": "2025-12-24",
            "pub_date_card": {
                "ru": "24 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 24",
                "zh": "12æœˆ24æ—¥"
            },
            "hash": "843be0302a54b421",
            "authors": [
                "Li-Zhong Szu-Tu",
                "Ting-Lin Wu",
                "Chia-Jui Chang",
                "He Syu",
                "Yu-Lun Liu"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2512.21337.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#dataset",
                    "#cv",
                    "#benchmark"
                ],
                "emoji": "ğŸ›ï¸",
                "ru": {
                    "title": "ĞšĞ¾Ğ³Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°ÑÑ‚ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ğ¾Ğ³Ğ¾, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ: Ñ€Ğ°Ğ·Ğ¾Ğ±Ğ»Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ğ¾ÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ ÑĞµÑ€ÑŒÑ‘Ğ·Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ² ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ° (VLM): Ğ¾Ğ½Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° 34% Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ñ… Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. Ğ”Ğ»Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ YearGuessr â€” ÑĞ°Ğ¼Ñ‹Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ 55,546 Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ· 157 ÑÑ‚Ñ€Ğ°Ğ½ Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ¾ Ğ³Ğ¾Ğ´Ğµ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸, Ğ³ĞµĞ¾Ğ»Ğ¾ĞºĞ°Ñ†Ğ¸ĞµĞ¹ Ğ¸ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ğ¾ÑÑ‚ÑŒÑ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ³Ğ¾Ğ´Ğ° Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ ĞºĞ°Ğº Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½ÑƒÑ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ñ Ğ¸ Ğ²Ğ²ĞµĞ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 30+ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ½Ğ¾Ğ²ÑƒÑ YearCLIP, Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ´Ğ¸Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ VLM Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ…, Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ½ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ñ… Ğ¸ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ½ĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ğº Ğ² Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Unveiling Popularity Bias in Vision-Language Models",
                    "desc": "This paper reveals a notable popularity bias in vision-language models (VLMs), showing they perform significantly better on well-known buildings than on lesser-known ones. The authors present the YearGuessr dataset, which includes over 55,000 images of buildings from around the world, annotated with their construction years and popularity metrics. They reformulate the task of predicting construction years as ordinal regression and introduce new metrics to measure the impact of popularity on model performance. The findings indicate that while VLMs can memorize popular items effectively, they struggle with less recognized subjects, highlighting a major limitation in their generalization abilities."
                },
                "zh": {
                    "title": "æ­ç¤ºè§†è§‰-è¯­è¨€æ¨¡å‹çš„æµè¡Œæ€§åå·®",
                    "desc": "æœ¬ç ”ç©¶æ­ç¤ºäº†å½“å‰æœ€å…ˆè¿›çš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å­˜åœ¨æ˜¾è‘—çš„æµè¡Œæ€§åå·®ï¼Œè¿™äº›æ¨¡å‹åœ¨è‘—åå»ºç­‘ä¸Šçš„å‡†ç¡®ç‡æ¯”æ™®é€šå»ºç­‘é«˜å‡º34%ã€‚ä¸ºäº†ç³»ç»Ÿåœ°ç ”ç©¶è¿™ä¸€ç°è±¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†YearGuessræ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«55,546å¼ å»ºç­‘å›¾åƒçš„å¼€æ”¾åŸºå‡†ï¼Œæ¶µç›–157ä¸ªå›½å®¶ï¼Œå¹¶é™„æœ‰å»ºç­‘å¹´ä»½ã€GPSæ•°æ®å’Œé¡µé¢æµè§ˆé‡ç­‰å¤šæ¨¡æ€å±æ€§ã€‚æˆ‘ä»¬å°†å»ºç­‘å¹´ä»½é¢„æµ‹ä»»åŠ¡æ¡†å®šä¸ºåºæ•°å›å½’ï¼Œå¹¶å¼•å…¥äº†è€ƒè™‘æµè¡Œæ€§çš„åŒºé—´å‡†ç¡®ç‡æŒ‡æ ‡æ¥é‡åŒ–è¿™ç§åå·®ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•ç»“æœæ˜¾ç¤ºï¼ŒVLMsåœ¨æµè¡Œçš„ã€è¢«è®°å¿†çš„é¡¹ç›®ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨ä¸è¢«è¯†åˆ«çš„å¯¹è±¡ä¸Šåˆ™æ˜¾è‘—æŒ£æ‰ï¼Œæš´éœ²äº†å®ƒä»¬æ¨ç†èƒ½åŠ›çš„å…³é”®ç¼ºé™·ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.20848",
            "title": "Nemotron 3 Nano: Open, Efficient Mixture-of-Experts Hybrid Mamba-Transformer Model for Agentic Reasoning",
            "url": "https://huggingface.co/papers/2512.20848",
            "abstract": "We present Nemotron 3 Nano 30B-A3B, a Mixture-of-Experts hybrid Mamba-Transformer language model. Nemotron 3 Nano was pretrained on 25 trillion text tokens, including more than 3 trillion new unique tokens over Nemotron 2, followed by supervised fine tuning and large-scale RL on diverse environments. Nemotron 3 Nano achieves better accuracy than our previous generation Nemotron 2 Nano while activating less than half of the parameters per forward pass. It achieves up to 3.3x higher inference throughput than similarly-sized open models like GPT-OSS-20B and Qwen3-30B-A3B-Thinking-2507, while also being more accurate on popular benchmarks. Nemotron 3 Nano demonstrates enhanced agentic, reasoning, and chat abilities and supports context lengths up to 1M tokens. We release both our pretrained Nemotron 3 Nano 30B-A3B Base and post-trained Nemotron 3 Nano 30B-A3B checkpoints on Hugging Face.",
            "score": 10,
            "issue_id": 235,
            "pub_date": "2025-12-23",
            "pub_date_card": {
                "ru": "23 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 23",
                "zh": "12æœˆ23æ—¥"
            },
            "hash": "b8f18d5988a4af2c",
            "authors": [
                "NVIDIA",
                ":",
                "Aaron Blakeman",
                "Aaron Grattafiori",
                "Aarti Basant",
                "Abhibha Gupta",
                "Abhinav Khattar",
                "Adi Renduchintala",
                "Aditya Vavre",
                "Akanksha Shukla",
                "Akhiad Bercovich",
                "Aleksander Ficek",
                "Aleksandr Shaposhnikov",
                "Alex Kondratenko",
                "Alexander Bukharin",
                "Alexandre Milesi",
                "Ali Taghibakhshi",
                "Alisa Liu",
                "Amelia Barton",
                "Ameya Sunil Mahabaleshwarkar",
                "Amir Klein",
                "Amit Zuker",
                "Amnon Geifman",
                "Amy Shen",
                "Anahita Bhiwandiwalla",
                "Andrew Tao",
                "Ann Guan",
                "Anubhav Mandarwal",
                "Arham Mehta",
                "Ashwath Aithal",
                "Ashwin Poojary",
                "Asif Ahamed",
                "Asma Kuriparambil Thekkumpate",
                "Ayush Dattagupta",
                "Banghua Zhu",
                "Bardiya Sadeghi",
                "Barnaby Simkin",
                "Ben Lanir",
                "Benedikt Schifferer",
                "Besmira Nushi",
                "Bilal Kartal",
                "Bita Darvish Rouhani",
                "Boris Ginsburg",
                "Brandon Norick",
                "Brandon Soubasis",
                "Branislav Kisacanin",
                "Brian Yu",
                "Bryan Catanzaro",
                "Carlo del Mundo",
                "Chantal Hwang",
                "Charles Wang",
                "Cheng-Ping Hsieh",
                "Chenghao Zhang",
                "Chenhan Yu",
                "Chetan Mungekar",
                "Chintan Patel",
                "Chris Alexiuk",
                "Christopher Parisien",
                "Collin Neale",
                "Damon Mosk-Aoyama",
                "Dan Su",
                "Dane Corneil",
                "Daniel Afrimi",
                "Daniel Rohrer",
                "Daniel Serebrenik",
                "Daria Gitman",
                "Daria Levy",
                "Darko Stosic",
                "David Mosallanezhad",
                "Deepak Narayanan",
                "Dhruv Nathawani",
                "Dima Rekesh",
                "Dina Yared",
                "Divyanshu Kakwani",
                "Dong Ahn",
                "Duncan Riach",
                "Dusan Stosic",
                "Edgar Minasyan",
                "Edward Lin",
                "Eileen Long",
                "Eileen Peters Long",
                "Elena Lantz",
                "Ellie Evans",
                "Elliott Ning",
                "Eric Chung",
                "Eric Harper",
                "Eric Tramel",
                "Erick Galinkin",
                "Erik Pounds",
                "Evan Briones",
                "Evelina Bakhturina",
                "Faisal Ladhak",
                "Fay Wang",
                "Fei Jia",
                "Felipe Soares",
                "Feng Chen",
                "Ferenc Galko",
                "Frankie Siino",
                "Gal Hubara Agam",
                "Ganesh Ajjanagadde",
                "Gantavya Bhatt",
                "Gargi Prasad",
                "George Armstrong",
                "Gerald Shen",
                "Gorkem Batmaz",
                "Grigor Nalbandyan",
                "Haifeng Qian",
                "Harsh Sharma",
                "Hayley Ross",
                "Helen Ngo",
                "Herman Sahota",
                "Hexin Wang",
                "Himanshu Soni",
                "Hiren Upadhyay",
                "Huizi Mao",
                "Huy C Nguyen",
                "Huy Q Nguyen",
                "Iain Cunningham",
                "Ido Shahaf",
                "Igor Gitman",
                "Ilya Loshchilov",
                "Ivan Moshkov",
                "Izzy Putterman",
                "Jan Kautz",
                "Jane Polak Scowcroft",
                "Jared Casper",
                "Jatin Mitra",
                "Jeffrey Glick",
                "Jenny Chen",
                "Jesse Oliver",
                "Jian Zhang",
                "Jiaqi Zeng",
                "Jie Lou",
                "Jimmy Zhang",
                "Jining Huang",
                "Joey Conway",
                "Joey Guman",
                "John Kamalu",
                "Johnny Greco",
                "Jonathan Cohen",
                "Joseph Jennings",
                "Joyjit Daw",
                "Julien Veron Vialard",
                "Junkeun Yi",
                "Jupinder Parmar",
                "Kai Xu",
                "Kan Zhu",
                "Kari Briski",
                "Katherine Cheung",
                "Katherine Luna",
                "Keshav Santhanam",
                "Kevin Shih",
                "Kezhi Kong",
                "Khushi Bhardwaj",
                "Krishna C. Puvvada",
                "Krzysztof Pawelec",
                "Kumar Anik",
                "Lawrence McAfee",
                "Laya Sleiman",
                "Leon Derczynski",
                "Li Ding",
                "Lucas Liebenwein",
                "Luis Vega",
                "Maanu Grover",
                "Maarten Van Segbroeck",
                "Maer Rodrigues de Melo",
                "Makesh Narsimhan Sreedhar",
                "Manoj Kilaru",
                "Maor Ashkenazi",
                "Marc Romeijn",
                "Mark Cai",
                "Markus Kliegl",
                "Maryam Moosaei",
                "Matvei Novikov",
                "Mehrzad Samadi",
                "Melissa Corpuz",
                "Mengru Wang",
                "Meredith Price",
                "Michael Boone",
                "Michael Evans",
                "Miguel Martinez",
                "Mike Chrzanowski",
                "Mohammad Shoeybi",
                "Mostofa Patwary",
                "Nabin Mulepati",
                "Natalie Hereth",
                "Nave Assaf",
                "Negar Habibi",
                "Neta Zmora",
                "Netanel Haber",
                "Nicola Sessions",
                "Nidhi Bhatia",
                "Nikhil Jukar",
                "Nikki Pope",
                "Nikolai Ludwig",
                "Nima Tajbakhsh",
                "Nirmal Juluru",
                "Oleksii Hrinchuk",
                "Oleksii Kuchaiev",
                "Olivier Delalleau",
                "Oluwatobi Olabiyi",
                "Omer Ullman Argov",
                "Ouye Xie",
                "Parth Chadha",
                "Pasha Shamis",
                "Pavlo Molchanov",
                "Pawel Morkisz",
                "Peter Dykas",
                "Peter Jin",
                "Pinky Xu",
                "Piotr Januszewski",
                "Pranav Prashant Thombre",
                "Prasoon Varshney",
                "Pritam Gundecha",
                "Qing Miao",
                "Rabeeh Karimi Mahabadi",
                "Ran El-Yaniv",
                "Ran Zilberstein",
                "Rasoul Shafipour",
                "Rich Harang",
                "Rick Izzo",
                "Rima Shahbazyan",
                "Rishabh Garg",
                "Ritika Borkar",
                "Ritu Gala",
                "Riyad Islam",
                "Roger Waleffe",
                "Rohit Watve",
                "Roi Koren",
                "Ruoxi Zhang",
                "Russell J. Hewett",
                "Ryan Prenger",
                "Ryan Timbrook",
                "Sadegh Mahdavi",
                "Sahil Modi",
                "Samuel Kriman",
                "Sanjay Kariyappa",
                "Sanjeev Satheesh",
                "Saori Kaji",
                "Satish Pasumarthi",
                "Sean Narentharen",
                "Sean Narenthiran",
                "Seonmyeong Bak",
                "Sergey Kashirsky",
                "Seth Poulos",
                "Shahar Mor",
                "Shanmugam Ramasamy",
                "Shantanu Acharya",
                "Shaona Ghosh",
                "Sharath Turuvekere Sreenivas",
                "Shelby Thomas",
                "Shiqing Fan",
                "Shreya Gopal",
                "Shrimai Prabhumoye",
                "Shubham Pachori",
                "Shubham Toshniwal",
                "Shuoyang Ding",
                "Siddharth Singh",
                "Simeng Sun",
                "Smita Ithape",
                "Somshubra Majumdar",
                "Soumye Singhal",
                "Stefania Alborghetti",
                "Stephen Ge",
                "Sugam Dipak Devare",
                "Sumeet Kumar Barua",
                "Suseella Panguluri",
                "Suyog Gupta",
                "Sweta Priyadarshi",
                "Syeda Nahida Akter",
                "Tan Bui",
                "Teodor-Dumitru Ene",
                "Terry Kong",
                "Thanh Do",
                "Tijmen Blankevoort",
                "Tom Balough",
                "Tomer Asida",
                "Tomer Bar Natan",
                "Tugrul Konuk",
                "Twinkle Vashishth",
                "Udi Karpas",
                "Ushnish De",
                "Vahid Noorozi",
                "Vahid Noroozi",
                "Venkat Srinivasan",
                "Venmugil Elango",
                "Vijay Korthikanti",
                "Vitaly Kurin",
                "Vitaly Lavrukhin",
                "Wanli Jiang",
                "Wasi Uddin Ahmad",
                "Wei Du",
                "Wei Ping",
                "Wenfei Zhou",
                "Will Jennings",
                "William Zhang",
                "Wojciech Prazuch",
                "Xiaowei Ren",
                "Yashaswi Karnati",
                "Yejin Choi",
                "Yev Meyer",
                "Yi-Fu Wu",
                "Yian Zhang",
                "Ying Lin",
                "Yonatan Geifman",
                "Yonggan Fu",
                "Yoshi Subara",
                "Yoshi Suhara",
                "Yubo Gao",
                "Zach Moshe",
                "Zhen Dong",
                "Zihan Liu",
                "Zijia Chen",
                "Zijie Yan"
            ],
            "affiliations": [
                "NVIDIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.20848.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#agents",
                    "#benchmark",
                    "#open_source",
                    "#reasoning",
                    "#inference",
                    "#long_context",
                    "#training",
                    "#optimization"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ­ĞºĞ¾Ğ½Ğ¾Ğ¼Ğ½Ğ°Ñ Ğ¼Ğ¾Ñ‰ÑŒ: Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ¿Ğ¾Ğ»Ğ¾Ğ²Ğ¸Ğ½Ğ½Ğ¾Ğ¹ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Nemotron 3 Nano, Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Mamba Ğ¸ Transformer Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ¼ĞµÑĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° 25 Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ‚ĞµĞºÑÑ‚Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Nemotron 3 Nano Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‡ĞµĞ¼ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰ĞµĞµ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ğµ, Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€ÑƒÑ Ğ¼ĞµĞ½ĞµĞµ Ğ¿Ğ¾Ğ»Ğ¾Ğ²Ğ¸Ğ½Ñ‹ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¿Ñ€Ğ¸ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´Ğµ Ğ²Ğ¿ĞµÑ€ĞµĞ´, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ² Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğ¸, Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸ Ğ¸ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğµ Ğ¾ĞºĞ½Ğ° Ğ´Ğ¾ 1 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²."
                },
                "en": {
                    "title": "Unlocking Efficiency and Accuracy in Language Modeling with Nemotron 3 Nano",
                    "desc": "The paper introduces Nemotron 3 Nano 30B-A3B, a new language model that uses a Mixture-of-Experts approach combined with a Mamba-Transformer architecture. It was pretrained on an extensive dataset of 25 trillion text tokens, significantly increasing the diversity of unique tokens compared to its predecessor, Nemotron 2. This model not only improves accuracy but also reduces the number of parameters activated during inference, leading to faster processing speeds. Additionally, Nemotron 3 Nano showcases advanced capabilities in reasoning and conversation, supporting very long context lengths of up to 1 million tokens."
                },
                "zh": {
                    "title": "Nemotron 3 Nanoï¼šé«˜æ•ˆçš„è¯­è¨€æ¨¡å‹æ–°çºªå…ƒ",
                    "desc": "Nemotron 3 Nano 30B-A3B æ˜¯ä¸€ç§æ··åˆä¸“å®¶çš„ Mamba-Transformer è¯­è¨€æ¨¡å‹ï¼Œç»è¿‡ 25 ä¸‡äº¿æ–‡æœ¬æ ‡è®°çš„é¢„è®­ç»ƒã€‚ä¸ä¹‹å‰çš„ Nemotron 2 Nano ç›¸æ¯”ï¼Œå®ƒåœ¨æ¿€æ´»å‚æ•°æ–¹é¢å‡å°‘äº†ä¸€åŠï¼ŒåŒæ—¶åœ¨æ¨ç†é€Ÿåº¦ä¸Šæé«˜äº† 3.3 å€ã€‚è¯¥æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæ›´é«˜çš„å‡†ç¡®æ€§ï¼Œå¹¶ä¸”å…·å¤‡æ›´å¼ºçš„æ¨ç†å’Œå¯¹è¯èƒ½åŠ›ï¼Œæ”¯æŒé•¿è¾¾ 100 ä¸‡ä¸ªæ ‡è®°çš„ä¸Šä¸‹æ–‡é•¿åº¦ã€‚æˆ‘ä»¬åœ¨ Hugging Face ä¸Šå‘å¸ƒäº†é¢„è®­ç»ƒå’Œåè®­ç»ƒçš„ Nemotron 3 Nano 30B-A3B æ£€æŸ¥ç‚¹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.21338",
            "title": "HiStream: Efficient High-Resolution Video Generation via Redundancy-Eliminated Streaming",
            "url": "https://huggingface.co/papers/2512.21338",
            "abstract": "High-resolution video generation, while crucial for digital media and film, is computationally bottlenecked by the quadratic complexity of diffusion models, making practical inference infeasible. To address this, we introduce HiStream, an efficient autoregressive framework that systematically reduces redundancy across three axes: i) Spatial Compression: denoising at low resolution before refining at high resolution with cached features; ii) Temporal Compression: a chunk-by-chunk strategy with a fixed-size anchor cache, ensuring stable inference speed; and iii) Timestep Compression: applying fewer denoising steps to subsequent, cache-conditioned chunks. On 1080p benchmarks, our primary HiStream model (i+ii) achieves state-of-the-art visual quality while demonstrating up to 76.2x faster denoising compared to the Wan2.1 baseline and negligible quality loss. Our faster variant, HiStream+, applies all three optimizations (i+ii+iii), achieving a 107.5x acceleration over the baseline, offering a compelling trade-off between speed and quality, thereby making high-resolution video generation both practical and scalable.",
            "score": 9,
            "issue_id": 235,
            "pub_date": "2025-12-24",
            "pub_date_card": {
                "ru": "24 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 24",
                "zh": "12æœˆ24æ—¥"
            },
            "hash": "e0de146fbb9ff7d7",
            "authors": [
                "Haonan Qiu",
                "Shikun Liu",
                "Zijian Zhou",
                "Zhaochong An",
                "Weiming Ren",
                "Zhiheng Liu",
                "Jonas Schult",
                "Sen He",
                "Shoufa Chen",
                "Yuren Cong",
                "Tao Xiang",
                "Ziwei Liu",
                "Juan-Manuel Perez-Rua"
            ],
            "affiliations": [
                "Meta AI",
                "Nanyang Technological University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.21338.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#video",
                    "#optimization",
                    "#inference"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¸Ñ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ HiStream â€” ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞ°ÑÑ‰Ğ¸Ñ…ÑÑ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ Ñ‚Ñ€Ñ‘Ğ¼ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼: Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ°Ñ ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ğ½Ğ³ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ² Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¼ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ¹ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ¾Ğ¹ Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¼, Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞºĞ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ ĞºÑÑˆĞ° Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°, Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ´ĞµĞ½Ğ¾Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹. ĞĞ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼ 1080p Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ HiStream Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ 76.2x ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğ¸ Ğ´ĞµĞ½Ğ¾Ğ¸Ğ·Ğ¸Ğ½Ğ³Ğ°, Ğ° ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚ HiStream+ Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ²ÑĞµÑ… Ñ‚Ñ€Ñ‘Ñ… Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¹ Ğ´Ğ°Ñ‘Ñ‚ 107.5x ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ¢Ğ°ĞºĞ¸Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ¼, Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ğ¼Ğ¾Ğ¹ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹."
                },
                "en": {
                    "title": "HiStream: Fast and Efficient High-Resolution Video Generation",
                    "desc": "This paper presents HiStream, a new framework for generating high-resolution videos more efficiently. It reduces computational complexity by compressing spatial, temporal, and timestep dimensions, allowing for faster denoising processes. HiStream achieves impressive visual quality while being significantly faster than previous models, with up to 107.5 times acceleration. This makes high-resolution video generation more practical and scalable for digital media applications."
                },
                "zh": {
                    "title": "é«˜æ•ˆè§†é¢‘ç”Ÿæˆï¼šé€Ÿåº¦ä¸è´¨é‡çš„å®Œç¾å¹³è¡¡",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºHiStreamçš„é«˜æ•ˆè‡ªå›å½’æ¡†æ¶ï¼Œç”¨äºé«˜åˆ†è¾¨ç‡è§†é¢‘ç”Ÿæˆã€‚è¯¥æ¡†æ¶é€šè¿‡ç©ºé—´å‹ç¼©ã€æ—¶é—´å‹ç¼©å’Œæ—¶é—´æ­¥å‹ç¼©ä¸‰ç§æ–¹å¼ç³»ç»Ÿæ€§åœ°å‡å°‘å†—ä½™ï¼Œä»è€Œæé«˜è®¡ç®—æ•ˆç‡ã€‚HiStreamåœ¨1080påŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„è§†è§‰è´¨é‡ï¼ŒåŒæ—¶åœ¨å»å™ªé€Ÿåº¦ä¸Šæ¯”Wan2.1åŸºçº¿å¿«76.2å€ï¼Œå‡ ä¹æ²¡æœ‰è´¨é‡æŸå¤±ã€‚å…¶æ›´å¿«çš„å˜ä½“HiStream+ç»“åˆäº†æ‰€æœ‰ä¸‰ç§ä¼˜åŒ–ï¼Œå®ç°äº†107.5å€çš„åŠ é€Ÿï¼Œæä¾›äº†é€Ÿåº¦ä¸è´¨é‡ä¹‹é—´çš„è‰¯å¥½å¹³è¡¡ï¼Œä½¿é«˜åˆ†è¾¨ç‡è§†é¢‘ç”Ÿæˆå˜å¾—å®ç”¨ä¸”å¯æ‰©å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.20757",
            "title": "TokSuite: Measuring the Impact of Tokenizer Choice on Language Model Behavior",
            "url": "https://huggingface.co/papers/2512.20757",
            "abstract": "Tokenizers provide the fundamental basis through which text is represented and processed by language models (LMs). Despite the importance of tokenization, its role in LM performance and behavior is poorly understood due to the challenge of measuring the impact of tokenization in isolation. To address this need, we present TokSuite, a collection of models and a benchmark that supports research into tokenization's influence on LMs. Specifically, we train fourteen models that use different tokenizers but are otherwise identical using the same architecture, dataset, training budget, and initialization. Additionally, we curate and release a new benchmark that specifically measures model performance subject to real-world perturbations that are likely to influence tokenization. Together, TokSuite allows robust decoupling of the influence of a model's tokenizer, supporting a series of novel findings that elucidate the respective benefits and shortcomings of a wide range of popular tokenizers.",
            "score": 8,
            "issue_id": 238,
            "pub_date": "2025-12-23",
            "pub_date_card": {
                "ru": "23 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 23",
                "zh": "12æœˆ23æ—¥"
            },
            "hash": "172308efe5877279",
            "authors": [
                "GÃ¼l Sena AltÄ±ntaÅŸ",
                "Malikeh Ehghaghi",
                "Brian Lester",
                "Fengyuan Liu",
                "Wanru Zhao",
                "Marco Ciccone",
                "Colin Raffel"
            ],
            "affiliations": [
                "Google DeepMind",
                "Hugging Face",
                "McGill University",
                "Mila - Quebec AI Institute",
                "University of Cambridge",
                "University of Toronto",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.20757.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "ğŸ”¤",
                "ru": {
                    "title": "Ğ Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°Ñ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğµ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ½Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° TokSuite â€” Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ½Ğ° Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğ½Ğ°Ğ´Ñ†Ğ°Ñ‚ÑŒ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ¸Ğ·Ğ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ, Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ‘Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸ÑÑ…, Ğ²Ğ»Ğ¸ÑÑÑ‰Ğ¸Ñ… Ğ½Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ Ğ¸Ğ·Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ½Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Unpacking Tokenization: Insights from TokSuite",
                    "desc": "This paper introduces TokSuite, a comprehensive framework designed to study the impact of tokenization on language models (LMs). By training fourteen identical models with different tokenizers, the authors isolate the effects of tokenization on model performance. They also present a new benchmark that evaluates how real-world variations affect tokenization outcomes. The findings from TokSuite reveal important insights into the strengths and weaknesses of various popular tokenizers, enhancing our understanding of their role in LM behavior."
                },
                "zh": {
                    "title": "åˆ†è¯å¯¹è¯­è¨€æ¨¡å‹æ€§èƒ½çš„æ·±è¿œå½±å“",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†TokSuiteï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºç ”ç©¶åˆ†è¯å¯¹è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰å½±å“çš„æ¨¡å‹é›†åˆå’ŒåŸºå‡†æµ‹è¯•ã€‚å°½ç®¡åˆ†è¯åœ¨æ–‡æœ¬è¡¨ç¤ºå’Œå¤„ç†ä¸­çš„é‡è¦æ€§ä¸è¨€è€Œå–»ï¼Œä½†å…¶å¯¹LMæ€§èƒ½çš„å…·ä½“å½±å“å°šä¸æ˜ç¡®ã€‚æˆ‘ä»¬è®­ç»ƒäº†åå››ä¸ªä½¿ç”¨ä¸åŒåˆ†è¯å™¨çš„æ¨¡å‹ï¼Œç¡®ä¿å®ƒä»¬åœ¨æ¶æ„ã€æ•°æ®é›†ã€è®­ç»ƒé¢„ç®—å’Œåˆå§‹åŒ–ä¸Šå®Œå…¨ç›¸åŒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åˆ›å»ºå¹¶å‘å¸ƒäº†ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œä¸“é—¨æµ‹é‡åœ¨ç°å®ä¸–ç•Œæ‰°åŠ¨ä¸‹æ¨¡å‹çš„æ€§èƒ½ï¼Œä»è€Œæ›´å¥½åœ°ç†è§£åˆ†è¯çš„å½±å“ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.20856",
            "title": "NVIDIA Nemotron 3: Efficient and Open Intelligence",
            "url": "https://huggingface.co/papers/2512.20856",
            "abstract": "We introduce the Nemotron 3 family of models - Nano, Super, and Ultra. These models deliver strong agentic, reasoning, and conversational capabilities. The Nemotron 3 family uses a Mixture-of-Experts hybrid Mamba-Transformer architecture to provide best-in-class throughput and context lengths of up to 1M tokens. Super and Ultra models are trained with NVFP4 and incorporate LatentMoE, a novel approach that improves model quality. The two larger models also include MTP layers for faster text generation. All Nemotron 3 models are post-trained using multi-environment reinforcement learning enabling reasoning, multi-step tool use, and support granular reasoning budget control. Nano, the smallest model, outperforms comparable models in accuracy while remaining extremely cost-efficient for inference. Super is optimized for collaborative agents and high-volume workloads such as IT ticket automation. Ultra, the largest model, provides state-of-the-art accuracy and reasoning performance. Nano is released together with its technical report and this white paper, while Super and Ultra will follow in the coming months. We will openly release the model weights, pre- and post-training software, recipes, and all data for which we hold redistribution rights.",
            "score": 7,
            "issue_id": 235,
            "pub_date": "2025-12-24",
            "pub_date_card": {
                "ru": "24 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 24",
                "zh": "12æœˆ24æ—¥"
            },
            "hash": "982311269fcc6451",
            "authors": [
                "NVIDIA",
                ":",
                "Aaron Blakeman",
                "Aaron Grattafiori",
                "Aarti Basant",
                "Abhibha Gupta",
                "Abhinav Khattar",
                "Adi Renduchintala",
                "Aditya Vavre",
                "Akanksha Shukla",
                "Akhiad Bercovich",
                "Aleksander Ficek",
                "Aleksandr Shaposhnikov",
                "Alex Kondratenko",
                "Alexander Bukharin",
                "Alexandre Milesi",
                "Ali Taghibakhshi",
                "Alisa Liu",
                "Amelia Barton",
                "Ameya Sunil Mahabaleshwarkar",
                "Amir Klein",
                "Amit Zuker",
                "Amnon Geifman",
                "Amy Shen",
                "Anahita Bhiwandiwalla",
                "Andrew Tao",
                "Anjulie Agrusa",
                "Ankur Verma",
                "Ann Guan",
                "Anubhav Mandarwal",
                "Arham Mehta",
                "Ashwath Aithal",
                "Ashwin Poojary",
                "Asif Ahamed",
                "Asit Mishra",
                "Asma Kuriparambil Thekkumpate",
                "Ayush Dattagupta",
                "Banghua Zhu",
                "Bardiya Sadeghi",
                "Barnaby Simkin",
                "Ben Lanir",
                "Benedikt Schifferer",
                "Besmira Nushi",
                "Bilal Kartal",
                "Bita Darvish Rouhani",
                "Boris Ginsburg",
                "Brandon Norick",
                "Brandon Soubasis",
                "Branislav Kisacanin",
                "Brian Yu",
                "Bryan Catanzaro",
                "Carlo del Mundo",
                "Chantal Hwang",
                "Charles Wang",
                "Cheng-Ping Hsieh",
                "Chenghao Zhang",
                "Chenhan Yu",
                "Chetan Mungekar",
                "Chintan Patel",
                "Chris Alexiuk",
                "Christopher Parisien",
                "Collin Neale",
                "Cyril Meurillon",
                "Damon Mosk-Aoyama",
                "Dan Su",
                "Dane Corneil",
                "Daniel Afrimi",
                "Daniel Lo",
                "Daniel Rohrer",
                "Daniel Serebrenik",
                "Daria Gitman",
                "Daria Levy",
                "Darko Stosic",
                "David Mosallanezhad",
                "Deepak Narayanan",
                "Dhruv Nathawani",
                "Dima Rekesh",
                "Dina Yared",
                "Divyanshu Kakwani",
                "Dong Ahn",
                "Duncan Riach",
                "Dusan Stosic",
                "Edgar Minasyan",
                "Edward Lin",
                "Eileen Long",
                "Eileen Peters Long",
                "Elad Segal",
                "Elena Lantz",
                "Ellie Evans",
                "Elliott Ning",
                "Eric Chung",
                "Eric Harper",
                "Eric Tramel",
                "Erick Galinkin",
                "Erik Pounds",
                "Evan Briones",
                "Evelina Bakhturina",
                "Evgeny Tsykunov",
                "Faisal Ladhak",
                "Fay Wang",
                "Fei Jia",
                "Felipe Soares",
                "Feng Chen",
                "Ferenc Galko",
                "Frank Sun",
                "Frankie Siino",
                "Gal Hubara Agam",
                "Ganesh Ajjanagadde",
                "Gantavya Bhatt",
                "Gargi Prasad",
                "George Armstrong",
                "Gerald Shen",
                "Gorkem Batmaz",
                "Grigor Nalbandyan",
                "Haifeng Qian",
                "Harsh Sharma",
                "Hayley Ross",
                "Helen Ngo",
                "Herbert Hum",
                "Herman Sahota",
                "Hexin Wang",
                "Himanshu Soni",
                "Hiren Upadhyay",
                "Huizi Mao",
                "Huy C Nguyen",
                "Huy Q Nguyen",
                "Iain Cunningham",
                "Ido Galil",
                "Ido Shahaf",
                "Igor Gitman",
                "Ilya Loshchilov",
                "Itamar Schen",
                "Itay Levy",
                "Ivan Moshkov",
                "Izik Golan",
                "Izzy Putterman",
                "Jan Kautz",
                "Jane Polak Scowcroft",
                "Jared Casper",
                "Jatin Mitra",
                "Jeffrey Glick",
                "Jenny Chen",
                "Jesse Oliver",
                "Jian Zhang",
                "Jiaqi Zeng",
                "Jie Lou",
                "Jimmy Zhang",
                "Jinhang Choi",
                "Jining Huang",
                "Joey Conway",
                "Joey Guman",
                "John Kamalu",
                "Johnny Greco",
                "Jonathan Cohen",
                "Joseph Jennings",
                "Joyjit Daw",
                "Julien Veron Vialard",
                "Junkeun Yi",
                "Jupinder Parmar",
                "Kai Xu",
                "Kan Zhu",
                "Kari Briski",
                "Katherine Cheung",
                "Katherine Luna",
                "Keith Wyss",
                "Keshav Santhanam",
                "Kevin Shih",
                "Kezhi Kong",
                "Khushi Bhardwaj",
                "Kirthi Shankar",
                "Krishna C. Puvvada",
                "Krzysztof Pawelec",
                "Kumar Anik",
                "Lawrence McAfee",
                "Laya Sleiman",
                "Leon Derczynski",
                "Li Ding",
                "Lizzie Wei",
                "Lucas Liebenwein",
                "Luis Vega",
                "Maanu Grover",
                "Maarten Van Segbroeck",
                "Maer Rodrigues de Melo",
                "Mahdi Nazemi",
                "Makesh Narsimhan Sreedhar",
                "Manoj Kilaru",
                "Maor Ashkenazi",
                "Marc Romeijn",
                "Marcin Chochowski",
                "Mark Cai",
                "Markus Kliegl",
                "Maryam Moosaei",
                "Matt Kulka",
                "Matvei Novikov",
                "Mehrzad Samadi",
                "Melissa Corpuz",
                "Mengru Wang",
                "Meredith Price",
                "Michael Andersch",
                "Michael Boone",
                "Michael Evans",
                "Miguel Martinez",
                "Mikail Khona",
                "Mike Chrzanowski",
                "Minseok Lee",
                "Mohammad Dabbah",
                "Mohammad Shoeybi",
                "Mostofa Patwary",
                "Nabin Mulepati",
                "Najeeb Nabwani",
                "Natalie Hereth",
                "Nave Assaf",
                "Negar Habibi",
                "Neta Zmora",
                "Netanel Haber",
                "Nicola Sessions",
                "Nidhi Bhatia",
                "Nikhil Jukar",
                "Nikki Pope",
                "Nikolai Ludwig",
                "Nima Tajbakhsh",
                "Nir Ailon",
                "Nirmal Juluru",
                "Nishant Sharma",
                "Oleksii Hrinchuk",
                "Oleksii Kuchaiev",
                "Olivier Delalleau",
                "Oluwatobi Olabiyi",
                "Omer Ullman Argov",
                "Omri Puny",
                "Oren Tropp",
                "Ouye Xie",
                "Parth Chadha",
                "Pasha Shamis",
                "Paul Gibbons",
                "Pavlo Molchanov",
                "Pawel Morkisz",
                "Peter Dykas",
                "Peter Jin",
                "Pinky Xu",
                "Piotr Januszewski",
                "Pranav Prashant Thombre",
                "Prasoon Varshney",
                "Pritam Gundecha",
                "Przemek Tredak",
                "Qing Miao",
                "Qiyu Wan",
                "Rabeeh Karimi Mahabadi",
                "Rachit Garg",
                "Ran El-Yaniv",
                "Ran Zilberstein",
                "Rasoul Shafipour",
                "Rich Harang",
                "Rick Izzo",
                "Rima Shahbazyan",
                "Rishabh Garg",
                "Ritika Borkar",
                "Ritu Gala",
                "Riyad Islam",
                "Robert Hesse",
                "Roger Waleffe",
                "Rohit Watve",
                "Roi Koren",
                "Ruoxi Zhang",
                "Russell Hewett",
                "Russell J. Hewett",
                "Ryan Prenger",
                "Ryan Timbrook",
                "Sadegh Mahdavi",
                "Sahil Modi",
                "Samuel Kriman",
                "Sangkug Lim",
                "Sanjay Kariyappa",
                "Sanjeev Satheesh",
                "Saori Kaji",
                "Satish Pasumarthi",
                "Saurav Muralidharan",
                "Sean Narentharen",
                "Sean Narenthiran",
                "Seonmyeong Bak",
                "Sergey Kashirsky",
                "Seth Poulos",
                "Shahar Mor",
                "Shanmugam Ramasamy",
                "Shantanu Acharya",
                "Shaona Ghosh",
                "Sharath Turuvekere Sreenivas",
                "Shelby Thomas",
                "Shiqing Fan",
                "Shreya Gopal",
                "Shrimai Prabhumoye",
                "Shubham Pachori",
                "Shubham Toshniwal",
                "Shuoyang Ding",
                "Siddharth Singh",
                "Simeng Sun",
                "Smita Ithape",
                "Somshubra Majumdar",
                "Soumye Singhal",
                "Stas Sergienko",
                "Stefania Alborghetti",
                "Stephen Ge",
                "Sugam Dipak Devare",
                "Sumeet Kumar Barua",
                "Suseella Panguluri",
                "Suyog Gupta",
                "Sweta Priyadarshi",
                "Syeda Nahida Akter",
                "Tan Bui",
                "Teodor-Dumitru Ene",
                "Terry Kong",
                "Thanh Do",
                "Tijmen Blankevoort",
                "Tim Moon",
                "Tom Balough",
                "Tomer Asida",
                "Tomer Bar Natan",
                "Tomer Ronen",
                "Tugrul Konuk",
                "Twinkle Vashishth",
                "Udi Karpas",
                "Ushnish De",
                "Vahid Noorozi",
                "Vahid Noroozi",
                "Venkat Srinivasan",
                "Venmugil Elango",
                "Victor Cui",
                "Vijay Korthikanti",
                "Vinay Rao",
                "Vitaly Kurin",
                "Vitaly Lavrukhin",
                "Vladimir Anisimov",
                "Wanli Jiang",
                "Wasi Uddin Ahmad",
                "Wei Du",
                "Wei Ping",
                "Wenfei Zhou",
                "Will Jennings",
                "William Zhang",
                "Wojciech Prazuch",
                "Xiaowei Ren",
                "Yashaswi Karnati",
                "Yejin Choi",
                "Yev Meyer",
                "Yi-Fu Wu",
                "Yian Zhang",
                "Yigong Qin",
                "Ying Lin",
                "Yonatan Geifman",
                "Yonggan Fu",
                "Yoshi Subara",
                "Yoshi Suhara",
                "Yubo Gao",
                "Zach Moshe",
                "Zhen Dong",
                "Zhongbo Zhu",
                "Zihan Liu",
                "Zijia Chen",
                "Zijie Yan"
            ],
            "affiliations": [
                "NVIDIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.20856.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#agents",
                    "#small_models",
                    "#open_source",
                    "#reasoning",
                    "#inference",
                    "#long_context",
                    "#training",
                    "#rl",
                    "#optimization"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµĞ¼ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Nemotron 3 - Nano, Super Ğ¸ Ultra, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ÑÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğº Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ, Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ñƒ Ğ¸ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ñƒ. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Mixture-of-Experts Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Mamba-Transformer, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‰ÑƒÑ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğµ Ğ¾ĞºĞ½Ğ° Ğ´Ğ¾ 1Ğœ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ñ‹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ NVFP4 Ğ¸ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° LatentMoE Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ MTP-ÑĞ»Ğ¾Ğ¸ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ’ÑĞµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ° Nemotron 3 Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ¼ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±ÑĞ´Ğ¶ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Unlocking Advanced Reasoning with Nemotron 3 Models",
                    "desc": "The Nemotron 3 family introduces three models: Nano, Super, and Ultra, each designed for advanced reasoning and conversational tasks. Utilizing a Mixture-of-Experts hybrid Mamba-Transformer architecture, these models achieve high throughput and can handle context lengths of up to 1 million tokens. The Super and Ultra models leverage NVFP4 training and a new LatentMoE technique to enhance performance, while MTP layers accelerate text generation. All models are fine-tuned with multi-environment reinforcement learning, enabling complex reasoning and efficient tool use, with Nano being particularly cost-effective and accurate for smaller applications."
                },
                "zh": {
                    "title": "Nemotron 3ï¼šæ™ºèƒ½æ¨ç†ä¸é«˜æ•ˆå¯¹è¯çš„æœªæ¥",
                    "desc": "Nemotron 3ç³»åˆ—æ¨¡å‹åŒ…æ‹¬Nanoã€Superå’ŒUltraï¼Œå…·å¤‡å¼ºå¤§çš„æ™ºèƒ½ã€æ¨ç†å’Œå¯¹è¯èƒ½åŠ›ã€‚è¿™äº›æ¨¡å‹é‡‡ç”¨æ··åˆä¸“å®¶çš„Mamba-Transformeræ¶æ„ï¼Œèƒ½å¤Ÿå¤„ç†é«˜è¾¾100ä¸‡æ ‡è®°çš„ä¸Šä¸‹æ–‡ï¼Œæä¾›å“è¶Šçš„ååé‡ã€‚Superå’ŒUltraæ¨¡å‹ä½¿ç”¨NVFP4è®­ç»ƒï¼Œå¹¶å¼•å…¥äº†LatentMoEæ–°æ–¹æ³•ï¼Œæå‡äº†æ¨¡å‹è´¨é‡ï¼ŒåŒæ—¶è¿˜åŒ…å«MTPå±‚ä»¥åŠ å¿«æ–‡æœ¬ç”Ÿæˆé€Ÿåº¦ã€‚æ‰€æœ‰Nemotron 3æ¨¡å‹éƒ½ç»è¿‡å¤šç¯å¢ƒå¼ºåŒ–å­¦ä¹ çš„åè®­ç»ƒï¼Œæ”¯æŒæ¨ç†ã€å¤šæ­¥éª¤å·¥å…·ä½¿ç”¨å’Œç»†ç²’åº¦æ¨ç†é¢„ç®—æ§åˆ¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.21004",
            "title": "Learning from Next-Frame Prediction: Autoregressive Video Modeling Encodes Effective Representations",
            "url": "https://huggingface.co/papers/2512.21004",
            "abstract": "Recent advances in pretraining general foundation models have significantly improved performance across diverse downstream tasks. While autoregressive (AR) generative models like GPT have revolutionized NLP, most visual generative pretraining methods still rely on BERT-style masked modeling, which often disregards the temporal information essential for video analysis. The few existing autoregressive visual pretraining methods suffer from issues such as inaccurate semantic localization and poor generation quality, leading to poor semantics. In this work, we propose NExT-Vid, a novel autoregressive visual generative pretraining framework that utilizes masked next-frame prediction to jointly model images and videos. NExT-Vid introduces a context-isolated autoregressive predictor to decouple semantic representation from target decoding, and a conditioned flow-matching decoder to enhance generation quality and diversity. Through context-isolated flow-matching pretraining, our approach achieves strong representations. Extensive experiments on large-scale pretrained models demonstrate that our proposed method consistently outperforms previous generative pretraining methods for visual representation learning via attentive probing in downstream classification.",
            "score": 4,
            "issue_id": 238,
            "pub_date": "2025-12-24",
            "pub_date_card": {
                "ru": "24 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 24",
                "zh": "12æœˆ24æ—¥"
            },
            "hash": "498c4c8176dbe0af",
            "authors": [
                "Jinghan Li",
                "Yang Jin",
                "Hao Jiang",
                "Yadong Mu",
                "Yang Song",
                "Kun Xu"
            ],
            "affiliations": [
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.21004.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#video",
                    "#multimodal"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ ĞºĞ°Ğ´Ñ€Ğ°",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ NExT-Vid â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‚Ğ¸Ğ¿Ğ° BERT, Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¹ ĞºĞ°Ğ´Ñ€, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğµ. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾-Ğ¸Ğ·Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¸ĞºÑ‚Ğ¾Ñ€Ğ° Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ² Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ flow-matching Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "NExT-Vid: Revolutionizing Visual Pretraining with Autoregressive Modeling",
                    "desc": "This paper introduces NExT-Vid, a new framework for autoregressive visual generative pretraining that focuses on improving video analysis. Unlike traditional methods that use BERT-style masked modeling, NExT-Vid employs masked next-frame prediction to better capture the temporal dynamics of both images and videos. The framework features a context-isolated autoregressive predictor to separate semantic representation from the decoding process, along with a conditioned flow-matching decoder to improve the quality and diversity of generated outputs. Experimental results show that NExT-Vid outperforms existing visual pretraining methods, leading to better performance in downstream classification tasks."
                },
                "zh": {
                    "title": "NExT-Vidï¼šæå‡è§†è§‰ç”Ÿæˆé¢„è®­ç»ƒçš„æ–°æ–¹æ³•",
                    "desc": "æœ€è¿‘ï¼Œé€šç”¨åŸºç¡€æ¨¡å‹çš„é¢„è®­ç»ƒè¿›å±•æ˜¾è‘—æå‡äº†å¤šç§ä¸‹æ¸¸ä»»åŠ¡çš„è¡¨ç°ã€‚å°½ç®¡è‡ªå›å½’ç”Ÿæˆæ¨¡å‹å¦‚GPTåœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå–å¾—äº†é©å‘½æ€§è¿›å±•ï¼Œä½†å¤§å¤šæ•°è§†è§‰ç”Ÿæˆé¢„è®­ç»ƒæ–¹æ³•ä»ä¾èµ–äºBERTé£æ ¼çš„æ©ç å»ºæ¨¡ï¼Œå¸¸å¸¸å¿½è§†è§†é¢‘åˆ†æä¸­é‡è¦çš„æ—¶é—´ä¿¡æ¯ã€‚ç°æœ‰çš„è‡ªå›å½’è§†è§‰é¢„è®­ç»ƒæ–¹æ³•å­˜åœ¨è¯­ä¹‰å®šä½ä¸å‡†ç¡®å’Œç”Ÿæˆè´¨é‡å·®ç­‰é—®é¢˜ï¼Œå¯¼è‡´è¯­ä¹‰æ•ˆæœä¸ä½³ã€‚æˆ‘ä»¬æå‡ºäº†NExT-Vidï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„è‡ªå›å½’è§†è§‰ç”Ÿæˆé¢„è®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡æ©ç ä¸‹ä¸€å¸§é¢„æµ‹æ¥è”åˆå»ºæ¨¡å›¾åƒå’Œè§†é¢‘ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆè´¨é‡å’Œå¤šæ ·æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.21334",
            "title": "Streaming Video Instruction Tuning",
            "url": "https://huggingface.co/papers/2512.21334",
            "abstract": "We present Streamo, a real-time streaming video LLM that serves as a general-purpose interactive assistant. Unlike existing online video models that focus narrowly on question answering or captioning, Streamo performs a broad spectrum of streaming video tasks, including real-time narration, action understanding, event captioning, temporal event grounding, and time-sensitive question answering. To develop such versatility, we construct Streamo-Instruct-465K, a large-scale instruction-following dataset tailored for streaming video understanding. The dataset covers diverse temporal contexts and multi-task supervision, enabling unified training across heterogeneous streaming tasks. After training end-to-end on the instruction-following dataset through a streamlined pipeline, Streamo exhibits strong temporal reasoning, responsive interaction, and broad generalization across a variety of streaming benchmarks. Extensive experiments show that Streamo bridges the gap between offline video perception models and real-time multimodal assistants, making a step toward unified, intelligent video understanding in continuous video streams.",
            "score": 3,
            "issue_id": 235,
            "pub_date": "2025-12-24",
            "pub_date_card": {
                "ru": "24 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 24",
                "zh": "12æœˆ24æ—¥"
            },
            "hash": "c1ff79b122635cb2",
            "authors": [
                "Jiaer Xia",
                "Peixian Chen",
                "Mengdan Zhang",
                "Xing Sun",
                "Kaiyang Zhou"
            ],
            "affiliations": [
                "Hong Kong Baptist University",
                "Tencent Youtu Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.21334.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#video",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "ğŸ“¹",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Streamo â€” Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ ĞºĞ°Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸Ğº. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Streamo Ñ€ĞµÑˆĞ°ĞµÑ‚ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¹ ÑĞ¿ĞµĞºÑ‚Ñ€ Ğ·Ğ°Ğ´Ğ°Ñ‡: Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²ĞµÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹, Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ° ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹, Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Streamo-Instruct-465K Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñ‹ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Streamo ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ°Ñ…."
                },
                "en": {
                    "title": "Streamo: Your Real-Time Video Assistant!",
                    "desc": "Streamo is a real-time streaming video large language model (LLM) designed to function as an interactive assistant across various video tasks. It goes beyond traditional models by integrating capabilities such as real-time narration, action understanding, and temporal event grounding, allowing it to handle a wide range of streaming video applications. The model is trained on a specially created dataset, Streamo-Instruct-465K, which includes diverse instruction sets for effective multi-task learning in streaming contexts. As a result, Streamo demonstrates advanced temporal reasoning and adaptability, bridging the gap between offline video models and real-time interactive systems."
                },
                "zh": {
                    "title": "Streamoï¼šå®æ—¶æµåª’ä½“è§†é¢‘çš„æ™ºèƒ½åŠ©æ‰‹",
                    "desc": "Streamoæ˜¯ä¸€ç§å®æ—¶æµåª’ä½“è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼Œä½œä¸ºé€šç”¨çš„äº’åŠ¨åŠ©æ‰‹ã€‚ä¸ç°æœ‰çš„åœ¨çº¿è§†é¢‘æ¨¡å‹ä¸åŒï¼ŒStreamoèƒ½å¤Ÿæ‰§è¡Œå¤šç§æµåª’ä½“è§†é¢‘ä»»åŠ¡ï¼ŒåŒ…æ‹¬å®æ—¶å™è¿°ã€åŠ¨ä½œç†è§£ã€äº‹ä»¶å­—å¹•ã€æ—¶é—´äº‹ä»¶å®šä½å’Œæ—¶é—´æ•æ„Ÿçš„é—®é¢˜å›ç­”ã€‚ä¸ºäº†å®ç°è¿™ç§å¤šåŠŸèƒ½æ€§ï¼Œæˆ‘ä»¬æ„å»ºäº†Streamo-Instruct-465Kï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹æµåª’ä½“è§†é¢‘ç†è§£çš„å¤§è§„æ¨¡æŒ‡ä»¤è·Ÿéšæ•°æ®é›†ï¼Œæ¶µç›–äº†å¤šæ ·çš„æ—¶é—´ä¸Šä¸‹æ–‡å’Œå¤šä»»åŠ¡ç›‘ç£ã€‚ç»è¿‡ç«¯åˆ°ç«¯çš„è®­ç»ƒï¼ŒStreamoåœ¨æ—¶é—´æ¨ç†ã€å“åº”äº’åŠ¨å’Œå¤šç§æµåª’ä½“åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ¨åŠ¨äº†å®æ—¶å¤šæ¨¡æ€åŠ©æ‰‹ä¸ç¦»çº¿è§†é¢‘æ„ŸçŸ¥æ¨¡å‹ä¹‹é—´çš„èåˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.18470",
            "title": "SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios",
            "url": "https://huggingface.co/papers/2512.18470",
            "abstract": "SWE-EVO benchmark evaluates AI coding agents on complex, multi-step software evolution tasks across multiple files, highlighting a significant gap in current models' capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing benchmarks for AI coding agents focus on isolated, single-issue tasks such as fixing a bug or implementing a small feature. However, real-world software engineering is fundamentally a long-horizon endeavor: developers must interpret high-level requirements, plan coordinated changes across many files, and evolve codebases over multiple iterations while preserving existing functionality. We introduce SWE-EVO, a benchmark that evaluates agents on this long-horizon software evolution challenge. Constructed from release notes and version histories of seven mature open-source Python projects, Tool comprises 48 evolution tasks that require agents to implement multi-step modifications spanning an average of 21 files, validated against comprehensive test suites averaging 874 tests per instance. Experiments with state-of-the-art models reveal a striking capability gap: even GPT-5 with OpenHands achieves only a 21 percent resolution rate on Tool, compared to 65 percent on the single-issue SWE-Bench Verified. This demonstrates that current agents struggle with sustained, multi-file reasoning. We also propose Fix Rate, a fine-grained metric that captures partial progress toward solving these complex, long-horizon tasks.",
            "score": 3,
            "issue_id": 241,
            "pub_date": "2025-12-20",
            "pub_date_card": {
                "ru": "20 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 20",
                "zh": "12æœˆ20æ—¥"
            },
            "hash": "5542a2f184e9dabd",
            "authors": [
                "Minh V. T. Thai",
                "Tue Le",
                "Dung Nguyen Manh",
                "Huy Phan Nhat",
                "Nghi D. Q. Bui"
            ],
            "affiliations": [
                "FPT Software AI Center",
                "School of Computing and Information Systems - University of Melbourne"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.18470.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#reasoning",
                    "#agents",
                    "#plp",
                    "#open_source"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "ĞÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ¿Ñ€Ğ°Ğ²Ğ¾Ğº Ğº Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°: Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¼Ñ‹ÑĞ»ÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾-Ğ½Ğ°ÑÑ‚Ğ¾ÑÑ‰ĞµĞ¼Ñƒ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº SWE-EVO Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ€ĞµÑˆĞ°ÑÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğµ Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ², ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ¾Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, SWE-EVO Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€ĞµĞ°Ğ»ÑŒĞ½ÑƒÑ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸ÑĞ¼Ğ¸ ĞºĞ¾Ğ´Ğ¾Ğ²Ğ¾Ğ¹ Ğ±Ğ°Ğ·Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ GPT-4, Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ² Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑÑ…: Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 21% ÑƒÑĞ¿ĞµÑ…Ğ° Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² 65% Ğ½Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Fine-Grained, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµÑ‚ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ Ğ¿Ñ€Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡."
                },
                "en": {
                    "title": "Bridging the Gap in AI Coding: Tackling Complex Software Evolution",
                    "desc": "The SWE-EVO benchmark assesses AI coding agents on complex software evolution tasks that involve multiple files and steps, reflecting real-world software development challenges. Unlike existing benchmarks that focus on simple, isolated tasks, SWE-EVO requires agents to interpret high-level requirements and make coordinated changes across various files while maintaining functionality. The benchmark consists of 48 evolution tasks derived from actual open-source Python projects, demanding agents to perform multi-step modifications validated by extensive test suites. Results show that even advanced models like GPT-5 struggle significantly with these tasks, achieving only a 21% resolution rate, highlighting a critical gap in the capabilities of current AI coding agents."
                },
                "zh": {
                    "title": "è¯„ä¼°AIç¼–ç ä»£ç†çš„é•¿æœŸè½¯ä»¶æ¼”åŒ–èƒ½åŠ›",
                    "desc": "SWE-EVOåŸºå‡†æµ‹è¯•è¯„ä¼°äººå·¥æ™ºèƒ½ç¼–ç ä»£ç†åœ¨å¤æ‚çš„å¤šæ­¥éª¤è½¯ä»¶æ¼”åŒ–ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚è¿™äº›ä»»åŠ¡æ¶‰åŠå¤šä¸ªæ–‡ä»¶ï¼Œæ˜¾ç¤ºäº†å½“å‰æ¨¡å‹èƒ½åŠ›çš„æ˜¾è‘—å·®è·ã€‚ä¸ç°æœ‰çš„å•ä¸€é—®é¢˜ä»»åŠ¡åŸºå‡†ä¸åŒï¼ŒSWE-EVOå…³æ³¨çš„æ˜¯è½¯ä»¶å·¥ç¨‹çš„é•¿æœŸæŒ‘æˆ˜ï¼Œè¦æ±‚ä»£ç†åœ¨å¤šä¸ªè¿­ä»£ä¸­è¿›è¡Œåè°ƒçš„ä»£ç ä¿®æ”¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨å¤„ç†è¿™äº›å¤šæ–‡ä»¶æ¨ç†æ—¶ä¹Ÿé¢ä¸´å›°éš¾ï¼ŒFix RateæŒ‡æ ‡åˆ™æä¾›äº†å¯¹å¤æ‚ä»»åŠ¡éƒ¨åˆ†è¿›å±•çš„ç»†è‡´è¯„ä¼°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.20144",
            "title": "Multi-hop Reasoning via Early Knowledge Alignment",
            "url": "https://huggingface.co/papers/2512.20144",
            "abstract": "Early Knowledge Alignment improves retrieval and reasoning in iterative RAG systems by aligning LLMs with relevant knowledge before planning, enhancing performance and efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for Large Language Models (LLMs) to address knowledge-intensive queries requiring domain-specific or up-to-date information. To handle complex multi-hop questions that are challenging for single-step retrieval, iterative RAG approaches incorporating reinforcement learning have been proposed. However, existing iterative RAG systems typically plan to decompose questions without leveraging information about the available retrieval corpus, leading to inefficient retrieval and reasoning chains that cascade into suboptimal performance. In this paper, we introduce Early Knowledge Alignment (EKA), a simple but effective module that aligns LLMs with retrieval set before planning in iterative RAG systems with contextually relevant retrieved knowledge. Extensive experiments on six standard RAG datasets demonstrate that by establishing a stronger reasoning foundation, EKA significantly improves retrieval precision, reduces cascading errors, and enhances both performance and efficiency. Our analysis from an entropy perspective demonstrate that incorporating early knowledge reduces unnecessary exploration during the reasoning process, enabling the model to focus more effectively on relevant information subsets. Moreover, EKA proves effective as a versatile, training-free inference strategy that scales seamlessly to large models. Generalization tests across diverse datasets and retrieval corpora confirm the robustness of our approach. Overall, EKA advances the state-of-the-art in iterative RAG systems while illuminating the critical interplay between structured reasoning and efficient exploration in reinforcement learning-augmented frameworks. The code is released at https://github.com/yxzwang/EarlyKnowledgeAlignment{Github}.",
            "score": 2,
            "issue_id": 240,
            "pub_date": "2025-12-23",
            "pub_date_card": {
                "ru": "23 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 23",
                "zh": "12æœˆ23æ—¥"
            },
            "hash": "fb7770738dfcb47e",
            "authors": [
                "Yuxin Wang",
                "Shicheng Fang",
                "Bo Wang",
                "Qi Luo",
                "Xuanjing Huang",
                "Yining Zheng",
                "Xipeng Qiu"
            ],
            "affiliations": [
                "Computer Science, Fudan University",
                "Institute of Modern Languages and Linguistics, Fudan University",
                "Shanghai SII"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.20144.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#rag",
                    "#benchmark",
                    "#open_source",
                    "#rl",
                    "#reasoning"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ’Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ½Ğ°Ñ‡Ğ°Ğ»Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞº Ğ² Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ RAG",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Early Knowledge Alignment (EKA) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ retrieval-augmented generation. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ¸Ğ· ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ° Ğ´Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ ĞºĞ°ÑĞºĞ°Ğ´Ğ½Ñ‹Ñ… Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ÑˆĞµÑÑ‚Ğ¸ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ĞºĞ¾Ñ€Ğ¿ÑƒÑÑ‹ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Aligning Knowledge Early for Smarter Retrieval and Reasoning",
                    "desc": "This paper presents Early Knowledge Alignment (EKA), a method that enhances Retrieval-Augmented Generation (RAG) systems by aligning Large Language Models (LLMs) with relevant knowledge before they plan their responses. By doing this, EKA improves the efficiency and accuracy of retrieving information for complex multi-hop questions. The authors demonstrate that EKA reduces errors in reasoning and retrieval, leading to better overall performance in iterative RAG systems. Their experiments show that EKA is a scalable and effective strategy that helps models focus on the most relevant information, improving the reasoning process without requiring additional training."
                },
                "zh": {
                    "title": "æ—©æœŸçŸ¥è¯†å¯¹é½æå‡RAGç³»ç»Ÿçš„æ€§èƒ½ä¸æ•ˆç‡",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºæ—©æœŸçŸ¥è¯†å¯¹é½ï¼ˆEKAï¼‰çš„æ–°æ¨¡å—ï¼Œæ—¨åœ¨æé«˜è¿­ä»£æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿä¸­çš„æ£€ç´¢å’Œæ¨ç†èƒ½åŠ›ã€‚EKAé€šè¿‡åœ¨è§„åˆ’ä¹‹å‰å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸ç›¸å…³çŸ¥è¯†å¯¹é½ï¼Œä»è€Œå¢å¼ºäº†æ¨¡å‹çš„æ¨ç†åŸºç¡€ï¼Œæ˜¾è‘—æé«˜äº†æ£€ç´¢ç²¾åº¦å’Œæ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEKAèƒ½å¤Ÿå‡å°‘çº§è”é”™è¯¯ï¼Œå¹¶æœ‰æ•ˆåœ°èšç„¦äºç›¸å…³ä¿¡æ¯å­é›†ï¼Œé™ä½ä¸å¿…è¦çš„æ¢ç´¢ã€‚è¯¥æ–¹æ³•æ— éœ€è®­ç»ƒï¼Œé€‚ç”¨äºå¤§è§„æ¨¡æ¨¡å‹ï¼Œå¹¶åœ¨å¤šç§æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.21010",
            "title": "LLM Swiss Round: Aggregating Multi-Benchmark Performance via Competitive Swiss-System Dynamics",
            "url": "https://huggingface.co/papers/2512.21010",
            "abstract": "The rapid proliferation of Large Language Models (LLMs) and diverse specialized benchmarks necessitates a shift from fragmented, task-specific metrics to a holistic, competitive ranking system that effectively aggregates performance across multiple ability dimensions. Primarily using static scoring, current evaluation methods are fundamentally limited. They struggle to determine the proper mix ratio across diverse benchmarks, and critically, they fail to capture a model's dynamic competitive fitness or its vulnerability when confronted with sequential, high-stakes tasks. To address this, we introduce the novel Competitive Swiss-System Dynamics (CSD) framework. CSD simulates a multi-round, sequential contest where models are dynamically paired across a curated sequence of benchmarks based on their accumulated win-loss record. And Monte Carlo Simulation (N=100,000 iterations) is used to approximate the statistically robust Expected Win Score (E[S_m]), which eliminates the noise of random pairing and early-round luck. Furthermore, we implement a Failure Sensitivity Analysis by parameterizing the per-round elimination quantity (T_k), which allows us to profile models based on their risk appetite--distinguishing between robust generalists and aggressive specialists. We demonstrate that CSD provides a more nuanced and context-aware ranking than traditional aggregate scoring and static pairwise models, representing a vital step towards risk-informed, next-generation LLM evaluation.",
            "score": 0,
            "issue_id": 235,
            "pub_date": "2025-12-24",
            "pub_date_card": {
                "ru": "24 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 24",
                "zh": "12æœˆ24æ—¥"
            },
            "hash": "e52c7e88e97ec24e",
            "authors": [
                "Jiashuo Liu",
                "Jiayun Wu",
                "Chunjie Wu",
                "Jingkai Liu",
                "Zaiyuan Wang",
                "Huan Zhou",
                "Wenhao Huang",
                "Hongseok Namkoong"
            ],
            "affiliations": [
                "ByteDance Seed",
                "Carnegie Mellon University",
                "Columbia University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.21010.jpg",
            "data": {
                "categories": [
                    "#benchmark"
                ],
                "emoji": "ğŸ†",
                "ru": {
                    "title": "Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ñ‚ÑƒÑ€Ğ½Ğ¸Ñ€Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Competitive Swiss-System Dynamics (CSD), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ·Ğ°Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ¾Ğ¼, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ÑƒĞ½Ğ´Ğ½Ñ‹Ñ… Ñ‚ÑƒÑ€Ğ½Ğ¸Ñ€Ğ½Ñ‹Ñ… ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸ÑÑ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ĞœĞ¾Ğ½Ñ‚Ğµ-ĞšĞ°Ñ€Ğ»Ğ¾ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑ‡ĞµÑ‚Ğ° Ğ¾Ğ¶Ğ¸Ğ´Ğ°ĞµĞ¼Ğ¾Ğ³Ğ¾ ÑÑ‡Ñ‘Ñ‚Ğ° Ğ¿Ğ¾Ğ±ĞµĞ´Ñ‹ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ ĞµÑ‘ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¸ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚. CSD Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ñ‹ÑĞ²Ğ¸Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ Ğ°Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğº ÑĞ±Ğ¾ÑĞ¼. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½ÑƒÑ Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Dynamic Evaluation for Next-Gen Language Models",
                    "desc": "This paper introduces the Competitive Swiss-System Dynamics (CSD) framework for evaluating Large Language Models (LLMs) across multiple benchmarks. Unlike traditional methods that use static scoring, CSD dynamically pairs models in a multi-round contest based on their performance history. It employs Monte Carlo Simulation to calculate a robust Expected Win Score, reducing the impact of randomness in early rounds. Additionally, the framework includes a Failure Sensitivity Analysis to assess models' risk profiles, distinguishing between generalists and specialists, thus offering a more comprehensive evaluation of LLMs."
                },
                "zh": {
                    "title": "åŠ¨æ€è¯„ä¼°ï¼šå¤§å‹è¯­è¨€æ¨¡å‹çš„æ–°æ ‡å‡†",
                    "desc": "éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤šæ ·åŒ–çš„ä¸“ä¸šåŸºå‡†çš„å¿«é€Ÿå‘å±•ï¼Œç°æœ‰çš„ä»»åŠ¡ç‰¹å®šè¯„ä¼°æŒ‡æ ‡æ˜¾å¾—ä¸å¤Ÿå…¨é¢ã€‚ä¼ ç»Ÿçš„é™æ€è¯„åˆ†æ–¹æ³•æ— æ³•æœ‰æ•ˆè¯„ä¼°æ¨¡å‹åœ¨ä¸åŒåŸºå‡†ä¸Šçš„è¡¨ç°æ··åˆæ¯”ä¾‹ï¼Œä¹Ÿæ— æ³•æ•æ‰æ¨¡å‹åœ¨è¿ç»­é«˜é£é™©ä»»åŠ¡ä¸­çš„åŠ¨æ€ç«äº‰èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç«äº‰ç‘å£«ç³»ç»ŸåŠ¨æ€ï¼ˆCSDï¼‰æ¡†æ¶ï¼Œé€šè¿‡å¤šè½®æ¯”èµ›åŠ¨æ€é…å¯¹æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿæ¥è¿‘ä¼¼ç»Ÿè®¡ç¨³å¥çš„æœŸæœ›èƒœåˆ†ï¼ˆE[S_m]ï¼‰ã€‚CSDæä¾›äº†æ¯”ä¼ ç»Ÿè¯„åˆ†æ–¹æ³•æ›´ç»†è‡´å’Œä¸Šä¸‹æ–‡æ•æ„Ÿçš„æ’åï¼Œæ ‡å¿—ç€å‘é£é™©å¯¼å‘çš„ä¸‹ä¸€ä»£LLMè¯„ä¼°è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-12-24.html",
    "link_next": "2025-12-26.html",
    "link_month": "2025-12.html",
    "short_date_prev": {
        "ru": "24.12",
        "en": "12/24",
        "zh": "12æœˆ24æ—¥"
    },
    "short_date_next": {
        "ru": "26.12",
        "en": "12/26",
        "zh": "12æœˆ26æ—¥"
    },
    "categories": {
        "#dataset": 7,
        "#data": 0,
        "#benchmark": 9,
        "#agents": 3,
        "#cv": 1,
        "#rl": 2,
        "#rlhf": 1,
        "#rag": 1,
        "#plp": 1,
        "#inference": 4,
        "#3d": 1,
        "#audio": 1,
        "#video": 7,
        "#multimodal": 5,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 6,
        "#healthcare": 0,
        "#training": 5,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 4,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 4,
        "#survey": 1,
        "#diffusion": 2,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 2,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 5,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    }
}