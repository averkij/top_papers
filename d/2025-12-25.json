{
    "date": {
        "ru": "25 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
        "en": "December 25",
        "zh": "12æœˆ25æ—¥"
    },
    "time_utc": "2025-12-25 05:25",
    "weekday": 3,
    "issue_id": 237,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2512.20557",
            "title": "Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models",
            "url": "https://huggingface.co/papers/2512.20557",
            "abstract": "DSR Suite enhances vision-language models with dynamic spatial reasoning through automated data generation and a geometry selection module that integrates geometric priors.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-language models (VLM) excel at general understanding yet remain weak at dynamic spatial reasoning (DSR), i.e., reasoning about the evolvement of object geometry and relationship in 3D space over time, largely due to the scarcity of scalable 4D-aware training resources. To bridge this gap across aspects of dataset, benchmark and model, we introduce DSR Suite. First, we propose an automated pipeline that generates multiple-choice question-answer pairs from in-the-wild videos for DSR. By leveraging modern vision foundation models, the pipeline extracts rich geometric and motion information, including camera poses, local point clouds, object masks, orientations, and 3D trajectories. These geometric cues enable the construction of DSR-Train for learning and further human-refined DSR-Bench for evaluation. Compared with previous works, our data emphasize (i) in-the-wild video sources, (ii) object- and scene-level 3D requirements, (iii) viewpoint transformations, (iv) multi-object interactions, and (v) fine-grained, procedural answers. Beyond data, we propose a lightweight Geometry Selection Module (GSM) to seamlessly integrate geometric priors into VLMs, which condenses question semantics and extracts question-relevant knowledge from pretrained 4D reconstruction priors into a compact set of geometry tokens. This targeted extraction avoids overwhelming the model with irrelevant knowledge. Experiments show that integrating DSR-Train and GSM into Qwen2.5-VL-7B significantly enhances its dynamic spatial reasoning capability, while maintaining accuracy on general video understanding benchmarks.",
            "score": 26,
            "issue_id": 235,
            "pub_date": "2025-12-23",
            "pub_date_card": {
                "ru": "23 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 23",
                "zh": "12æœˆ23æ—¥"
            },
            "hash": "03a0333b07003ea9",
            "authors": [
                "Shengchao Zhou",
                "Yuxin Chen",
                "Yuying Ge",
                "Wei Huang",
                "Jiehong Lin",
                "Ying Shan",
                "Xiaojuan Qi"
            ],
            "affiliations": [
                "ARC Lab, Tencent PCG",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.20557.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#architecture",
                    "#video",
                    "#multimodal",
                    "#benchmark",
                    "#3d"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ“ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ Ğ² Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· 4D Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° DSR Suite Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ¾ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¸ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² 3D Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼Ğ¸, Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· foundation Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ñ‹, Ğ¾Ğ±Ğ»Ğ°ĞºĞ° Ñ‚Ğ¾Ñ‡ĞµĞº, Ğ¼Ğ°ÑĞºĞ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ 3D Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Geometry Selection Module (GSM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ñ‹ Ğ² vision-language Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ñ‚Ğ±Ğ¸Ñ€Ğ°Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ GSM Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Vision-Language Models with Dynamic Spatial Reasoning",
                    "desc": "The DSR Suite improves vision-language models by focusing on dynamic spatial reasoning (DSR), which involves understanding how objects change and relate in 3D space over time. It introduces an automated data generation pipeline that creates question-answer pairs from real-world videos, capturing essential geometric and motion details. Additionally, a Geometry Selection Module (GSM) is implemented to incorporate geometric knowledge into the models without overwhelming them with unnecessary information. Experiments demonstrate that this approach significantly boosts the DSR capabilities of the Qwen2.5-VL-7B model while preserving its performance on general video understanding tasks."
                },
                "zh": {
                    "title": "åŠ¨æ€ç©ºé—´æ¨ç†çš„å¢å¼ºå·¥å…·",
                    "desc": "DSR Suite æ˜¯ä¸€ç§å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹çš„å·¥å…·ï¼Œä¸“æ³¨äºåŠ¨æ€ç©ºé—´æ¨ç†ï¼ˆDSRï¼‰ã€‚å®ƒé€šè¿‡è‡ªåŠ¨åŒ–æ•°æ®ç”Ÿæˆå’Œå‡ ä½•é€‰æ‹©æ¨¡å—ï¼Œç»“åˆå‡ ä½•å…ˆéªŒï¼Œæ¥è§£å†³æ¨¡å‹åœ¨ä¸‰ç»´ç©ºé—´ä¸­å¯¹è±¡å‡ ä½•å’Œå…³ç³»æ¼”å˜çš„æ¨ç†èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚è¯¥å·¥å…·ç”Ÿæˆå¤šé€‰é¢˜é—®ç­”å¯¹ï¼Œå¹¶æå–ä¸°å¯Œçš„å‡ ä½•å’Œè¿åŠ¨ä¿¡æ¯ï¼Œä»¥æ„å»ºç”¨äºå­¦ä¹ çš„ DSR-Train å’Œç”¨äºè¯„ä¼°çš„ DSR-Benchã€‚å®éªŒè¡¨æ˜ï¼Œå°† DSR-Train å’Œå‡ ä½•é€‰æ‹©æ¨¡å—æ•´åˆåˆ°æ¨¡å‹ä¸­ï¼Œæ˜¾è‘—æå‡äº†åŠ¨æ€ç©ºé—´æ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒäº†å¯¹ä¸€èˆ¬è§†é¢‘ç†è§£åŸºå‡†çš„å‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.16093",
            "title": "TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times",
            "url": "https://huggingface.co/papers/2512.16093",
            "abstract": "TurboDiffusion accelerates video generation by 100-200x using attention acceleration, step distillation, and quantization, while maintaining video quality.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce TurboDiffusion, a video generation acceleration framework that can speed up end-to-end diffusion generation by 100-200x while maintaining video quality. TurboDiffusion mainly relies on several components for acceleration: (1) Attention acceleration: TurboDiffusion uses low-bit SageAttention and trainable Sparse-Linear Attention (SLA) to speed up attention computation. (2) Step distillation: TurboDiffusion adopts rCM for efficient step distillation. (3) W8A8 quantization: TurboDiffusion quantizes model parameters and activations to 8 bits to accelerate linear layers and compress the model. In addition, TurboDiffusion incorporates several other engineering optimizations.   We conduct experiments on the Wan2.2-I2V-14B-720P, Wan2.1-T2V-1.3B-480P, Wan2.1-T2V-14B-720P, and Wan2.1-T2V-14B-480P models. Experimental results show that TurboDiffusion achieves 100-200x speedup for video generation even on a single RTX 5090 GPU, while maintaining comparable video quality. The GitHub repository, which includes model checkpoints and easy-to-use code, is available at https://github.com/thu-ml/TurboDiffusion.",
            "score": 22,
            "issue_id": 235,
            "pub_date": "2025-12-18",
            "pub_date_card": {
                "ru": "18 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 18",
                "zh": "12æœˆ18æ—¥"
            },
            "hash": "cc018bddf4ff209e",
            "authors": [
                "Jintao Zhang",
                "Kaiwen Zheng",
                "Kai Jiang",
                "Haoxu Wang",
                "Ion Stoica",
                "Joseph E. Gonzalez",
                "Jianfei Chen",
                "Jun Zhu"
            ],
            "affiliations": [
                "Shengshu Technology",
                "Tsinghua University",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.16093.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#video",
                    "#open_source",
                    "#diffusion",
                    "#inference",
                    "#optimization"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "ĞœĞ¾Ğ»Ğ½Ğ¸ĞµĞ½Ğ¾ÑĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾: 100-200x ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°",
                    "desc": "TurboDiffusion â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‰Ğ¸Ğ¹ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ² 100-200 Ñ€Ğ°Ğ· Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ attention-Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ½Ğ¸Ğ·ĞºĞ¾Ğ±Ğ¸Ñ‚Ğ¾Ğ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½ÑƒÑ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½ÑƒÑ attention, Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ ÑˆĞ°Ğ³Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ñ‡Ğ¸ÑĞ»Ğ° Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ´Ğ¾ 8 Ğ±Ğ¸Ñ‚. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ°Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ¶Ğµ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ GPU. ĞŸÑ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ° Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼ Ñ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ñ‡ĞµĞºĞ¿Ğ¾Ğ¹Ğ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "TurboDiffusion: Speeding Up Video Generation by 100-200x!",
                    "desc": "TurboDiffusion is a framework designed to significantly speed up video generation processes by 100-200 times while preserving high video quality. It achieves this acceleration through three main techniques: attention acceleration using low-bit SageAttention and Sparse-Linear Attention, step distillation with a method called rCM, and W8A8 quantization that reduces model parameters to 8 bits. These innovations allow for faster computation in attention mechanisms and efficient model compression. Experimental results demonstrate that TurboDiffusion can effectively enhance video generation speed even on a single high-performance GPU, making it a powerful tool for AI video generation."
                },
                "zh": {
                    "title": "TurboDiffusionï¼šè§†é¢‘ç”Ÿæˆé€Ÿåº¦æå‡100-200å€",
                    "desc": "TurboDiffusion æ˜¯ä¸€ä¸ªåŠ é€Ÿè§†é¢‘ç”Ÿæˆçš„æ¡†æ¶ï¼Œå¯ä»¥å°†è§†é¢‘ç”Ÿæˆé€Ÿåº¦æé«˜ 100-200 å€ï¼ŒåŒæ—¶ä¿æŒè§†é¢‘è´¨é‡ã€‚å®ƒä¸»è¦ä¾èµ–å‡ ä¸ªç»„ä»¶æ¥å®ç°åŠ é€Ÿï¼šé¦–å…ˆï¼Œä½¿ç”¨ä½ä½æ•°çš„ SageAttention å’Œå¯è®­ç»ƒçš„ç¨€ç–çº¿æ€§æ³¨æ„åŠ›ï¼ˆSLAï¼‰æ¥åŠ é€Ÿæ³¨æ„åŠ›è®¡ç®—ï¼›å…¶æ¬¡ï¼Œé‡‡ç”¨ rCM è¿›è¡Œé«˜æ•ˆçš„æ­¥éª¤è’¸é¦ï¼›æœ€åï¼Œé€šè¿‡å°†æ¨¡å‹å‚æ•°å’Œæ¿€æ´»é‡åŒ–ä¸º 8 ä½æ¥åŠ é€Ÿçº¿æ€§å±‚å¹¶å‹ç¼©æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿åœ¨å•ä¸ª RTX 5090 GPU ä¸Šï¼ŒTurboDiffusion ä¹Ÿèƒ½å®ç° 100-200 å€çš„è§†é¢‘ç”ŸæˆåŠ é€Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.21094",
            "title": "T2AV-Compass: Towards Unified Evaluation for Text-to-Audio-Video Generation",
            "url": "https://huggingface.co/papers/2512.21094",
            "abstract": "Text-to-Audio-Video (T2AV) generation aims to synthesize temporally coherent video and semantically synchronized audio from natural language, yet its evaluation remains fragmented, often relying on unimodal metrics or narrowly scoped benchmarks that fail to capture cross-modal alignment, instruction following, and perceptual realism under complex prompts. To address this limitation, we present T2AV-Compass, a unified benchmark for comprehensive evaluation of T2AV systems, consisting of 500 diverse and complex prompts constructed via a taxonomy-driven pipeline to ensure semantic richness and physical plausibility. Besides, T2AV-Compass introduces a dual-level evaluation framework that integrates objective signal-level metrics for video quality, audio quality, and cross-modal alignment with a subjective MLLM-as-a-Judge protocol for instruction following and realism assessment. Extensive evaluation of 11 representative T2AVsystems reveals that even the strongest models fall substantially short of human-level realism and cross-modal consistency, with persistent failures in audio realism, fine-grained synchronization, instruction following, etc. These results indicate significant improvement room for future models and highlight the value of T2AV-Compass as a challenging and diagnostic testbed for advancing text-to-audio-video generation.",
            "score": 19,
            "issue_id": 235,
            "pub_date": "2025-12-24",
            "pub_date_card": {
                "ru": "24 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 24",
                "zh": "12æœˆ24æ—¥"
            },
            "hash": "2d1ce69f802bf812",
            "authors": [
                "Zhe Cao",
                "Tao Wang",
                "Jiaming Wang",
                "Yanghai Wang",
                "Yuanxing Zhang",
                "Jialu Chen",
                "Miao Deng",
                "Jiahao Wang",
                "Yubin Guo",
                "Chenxi Liao",
                "Yize Zhang",
                "Zhaoxiang Zhang",
                "Jiaheng Liu"
            ],
            "affiliations": [
                "Institute of Automation, Chinese Academy of Sciences",
                "Kuaishou Technology",
                "Nanjing University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.21094.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#video",
                    "#multimodal",
                    "#benchmark",
                    "#audio",
                    "#survey"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ·Ğ²ÑƒĞºĞ° Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ T2AV-Compass â€” ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 500 Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²ÑƒÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¾Ğ¹ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ LLM Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑÑƒĞ´ÑŒĞ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑÑ‚ÑƒĞ¿Ğ°ÑÑ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¼Ñƒ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¼ĞµĞ¶Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¸Ğ¼ĞµÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¾Ğ¼ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹."
                },
                "en": {
                    "title": "T2AV-Compass: Elevating Text-to-Audio-Video Evaluation",
                    "desc": "This paper introduces T2AV-Compass, a new benchmark designed to evaluate Text-to-Audio-Video (T2AV) generation systems more effectively. It addresses the current limitations in evaluation methods by providing a comprehensive set of 500 complex prompts that ensure both semantic richness and physical plausibility. The benchmark features a dual-level evaluation framework that combines objective metrics for video and audio quality with subjective assessments of instruction following and realism. The findings reveal that existing T2AV models struggle to achieve human-level performance, particularly in audio realism and synchronization, indicating a need for further advancements in the field."
                },
                "zh": {
                    "title": "T2AV-Compassï¼šæ¨åŠ¨æ–‡æœ¬åˆ°éŸ³é¢‘è§†é¢‘ç”Ÿæˆçš„è¯„ä¼°æ–°æ ‡å‡†",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„åŸºå‡†æµ‹è¯•å·¥å…·T2AV-Compassï¼Œç”¨äºè¯„ä¼°æ–‡æœ¬åˆ°éŸ³é¢‘è§†é¢‘ç”Ÿæˆï¼ˆT2AVï¼‰ç³»ç»Ÿçš„æ€§èƒ½ã€‚è¯¥åŸºå‡†åŒ…å«500ä¸ªå¤šæ ·åŒ–å’Œå¤æ‚çš„æç¤ºï¼Œç¡®ä¿è¯­ä¹‰ä¸°å¯Œæ€§å’Œç‰©ç†åˆç†æ€§ã€‚T2AV-Compassé‡‡ç”¨åŒå±‚è¯„ä¼°æ¡†æ¶ï¼Œç»“åˆäº†å®¢è§‚çš„ä¿¡å·çº§æŒ‡æ ‡å’Œä¸»è§‚çš„è¯„ä¼°åè®®ï¼Œä»¥å…¨é¢è¯„ä¼°ç”Ÿæˆçš„éŸ³é¢‘å’Œè§†é¢‘è´¨é‡ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç°æœ‰æ¨¡å‹åœ¨éŸ³é¢‘çœŸå®æ„Ÿå’Œè·¨æ¨¡æ€ä¸€è‡´æ€§æ–¹é¢ä»æœ‰å¾ˆå¤§æ”¹è¿›ç©ºé—´ï¼Œå¼ºè°ƒäº†T2AV-Compassåœ¨æ¨åŠ¨T2AVç”ŸæˆæŠ€æœ¯å‘å±•ä¸­çš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.20856",
            "title": "NVIDIA Nemotron 3: Efficient and Open Intelligence",
            "url": "https://huggingface.co/papers/2512.20856",
            "abstract": "We introduce the Nemotron 3 family of models - Nano, Super, and Ultra. These models deliver strong agentic, reasoning, and conversational capabilities. The Nemotron 3 family uses a Mixture-of-Experts hybrid Mamba-Transformer architecture to provide best-in-class throughput and context lengths of up to 1M tokens. Super and Ultra models are trained with NVFP4 and incorporate LatentMoE, a novel approach that improves model quality. The two larger models also include MTP layers for faster text generation. All Nemotron 3 models are post-trained using multi-environment reinforcement learning enabling reasoning, multi-step tool use, and support granular reasoning budget control. Nano, the smallest model, outperforms comparable models in accuracy while remaining extremely cost-efficient for inference. Super is optimized for collaborative agents and high-volume workloads such as IT ticket automation. Ultra, the largest model, provides state-of-the-art accuracy and reasoning performance. Nano is released together with its technical report and this white paper, while Super and Ultra will follow in the coming months. We will openly release the model weights, pre- and post-training software, recipes, and all data for which we hold redistribution rights.",
            "score": 4,
            "issue_id": 235,
            "pub_date": "2025-12-24",
            "pub_date_card": {
                "ru": "24 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 24",
                "zh": "12æœˆ24æ—¥"
            },
            "hash": "982311269fcc6451",
            "authors": [
                "NVIDIA",
                ":",
                "Aaron Blakeman",
                "Aaron Grattafiori",
                "Aarti Basant",
                "Abhibha Gupta",
                "Abhinav Khattar",
                "Adi Renduchintala",
                "Aditya Vavre",
                "Akanksha Shukla",
                "Akhiad Bercovich",
                "Aleksander Ficek",
                "Aleksandr Shaposhnikov",
                "Alex Kondratenko",
                "Alexander Bukharin",
                "Alexandre Milesi",
                "Ali Taghibakhshi",
                "Alisa Liu",
                "Amelia Barton",
                "Ameya Sunil Mahabaleshwarkar",
                "Amir Klein",
                "Amit Zuker",
                "Amnon Geifman",
                "Amy Shen",
                "Anahita Bhiwandiwalla",
                "Andrew Tao",
                "Anjulie Agrusa",
                "Ankur Verma",
                "Ann Guan",
                "Anubhav Mandarwal",
                "Arham Mehta",
                "Ashwath Aithal",
                "Ashwin Poojary",
                "Asif Ahamed",
                "Asit Mishra",
                "Asma Kuriparambil Thekkumpate",
                "Ayush Dattagupta",
                "Banghua Zhu",
                "Bardiya Sadeghi",
                "Barnaby Simkin",
                "Ben Lanir",
                "Benedikt Schifferer",
                "Besmira Nushi",
                "Bilal Kartal",
                "Bita Darvish Rouhani",
                "Boris Ginsburg",
                "Brandon Norick",
                "Brandon Soubasis",
                "Branislav Kisacanin",
                "Brian Yu",
                "Bryan Catanzaro",
                "Carlo del Mundo",
                "Chantal Hwang",
                "Charles Wang",
                "Cheng-Ping Hsieh",
                "Chenghao Zhang",
                "Chenhan Yu",
                "Chetan Mungekar",
                "Chintan Patel",
                "Chris Alexiuk",
                "Christopher Parisien",
                "Collin Neale",
                "Cyril Meurillon",
                "Damon Mosk-Aoyama",
                "Dan Su",
                "Dane Corneil",
                "Daniel Afrimi",
                "Daniel Lo",
                "Daniel Rohrer",
                "Daniel Serebrenik",
                "Daria Gitman",
                "Daria Levy",
                "Darko Stosic",
                "David Mosallanezhad",
                "Deepak Narayanan",
                "Dhruv Nathawani",
                "Dima Rekesh",
                "Dina Yared",
                "Divyanshu Kakwani",
                "Dong Ahn",
                "Duncan Riach",
                "Dusan Stosic",
                "Edgar Minasyan",
                "Edward Lin",
                "Eileen Long",
                "Eileen Peters Long",
                "Elad Segal",
                "Elena Lantz",
                "Ellie Evans",
                "Elliott Ning",
                "Eric Chung",
                "Eric Harper",
                "Eric Tramel",
                "Erick Galinkin",
                "Erik Pounds",
                "Evan Briones",
                "Evelina Bakhturina",
                "Evgeny Tsykunov",
                "Faisal Ladhak",
                "Fay Wang",
                "Fei Jia",
                "Felipe Soares",
                "Feng Chen",
                "Ferenc Galko",
                "Frank Sun",
                "Frankie Siino",
                "Gal Hubara Agam",
                "Ganesh Ajjanagadde",
                "Gantavya Bhatt",
                "Gargi Prasad",
                "George Armstrong",
                "Gerald Shen",
                "Gorkem Batmaz",
                "Grigor Nalbandyan",
                "Haifeng Qian",
                "Harsh Sharma",
                "Hayley Ross",
                "Helen Ngo",
                "Herbert Hum",
                "Herman Sahota",
                "Hexin Wang",
                "Himanshu Soni",
                "Hiren Upadhyay",
                "Huizi Mao",
                "Huy C Nguyen",
                "Huy Q Nguyen",
                "Iain Cunningham",
                "Ido Galil",
                "Ido Shahaf",
                "Igor Gitman",
                "Ilya Loshchilov",
                "Itamar Schen",
                "Itay Levy",
                "Ivan Moshkov",
                "Izik Golan",
                "Izzy Putterman",
                "Jan Kautz",
                "Jane Polak Scowcroft",
                "Jared Casper",
                "Jatin Mitra",
                "Jeffrey Glick",
                "Jenny Chen",
                "Jesse Oliver",
                "Jian Zhang",
                "Jiaqi Zeng",
                "Jie Lou",
                "Jimmy Zhang",
                "Jinhang Choi",
                "Jining Huang",
                "Joey Conway",
                "Joey Guman",
                "John Kamalu",
                "Johnny Greco",
                "Jonathan Cohen",
                "Joseph Jennings",
                "Joyjit Daw",
                "Julien Veron Vialard",
                "Junkeun Yi",
                "Jupinder Parmar",
                "Kai Xu",
                "Kan Zhu",
                "Kari Briski",
                "Katherine Cheung",
                "Katherine Luna",
                "Keith Wyss",
                "Keshav Santhanam",
                "Kevin Shih",
                "Kezhi Kong",
                "Khushi Bhardwaj",
                "Kirthi Shankar",
                "Krishna C. Puvvada",
                "Krzysztof Pawelec",
                "Kumar Anik",
                "Lawrence McAfee",
                "Laya Sleiman",
                "Leon Derczynski",
                "Li Ding",
                "Lizzie Wei",
                "Lucas Liebenwein",
                "Luis Vega",
                "Maanu Grover",
                "Maarten Van Segbroeck",
                "Maer Rodrigues de Melo",
                "Mahdi Nazemi",
                "Makesh Narsimhan Sreedhar",
                "Manoj Kilaru",
                "Maor Ashkenazi",
                "Marc Romeijn",
                "Marcin Chochowski",
                "Mark Cai",
                "Markus Kliegl",
                "Maryam Moosaei",
                "Matt Kulka",
                "Matvei Novikov",
                "Mehrzad Samadi",
                "Melissa Corpuz",
                "Mengru Wang",
                "Meredith Price",
                "Michael Andersch",
                "Michael Boone",
                "Michael Evans",
                "Miguel Martinez",
                "Mikail Khona",
                "Mike Chrzanowski",
                "Minseok Lee",
                "Mohammad Dabbah",
                "Mohammad Shoeybi",
                "Mostofa Patwary",
                "Nabin Mulepati",
                "Najeeb Nabwani",
                "Natalie Hereth",
                "Nave Assaf",
                "Negar Habibi",
                "Neta Zmora",
                "Netanel Haber",
                "Nicola Sessions",
                "Nidhi Bhatia",
                "Nikhil Jukar",
                "Nikki Pope",
                "Nikolai Ludwig",
                "Nima Tajbakhsh",
                "Nir Ailon",
                "Nirmal Juluru",
                "Nishant Sharma",
                "Oleksii Hrinchuk",
                "Oleksii Kuchaiev",
                "Olivier Delalleau",
                "Oluwatobi Olabiyi",
                "Omer Ullman Argov",
                "Omri Puny",
                "Oren Tropp",
                "Ouye Xie",
                "Parth Chadha",
                "Pasha Shamis",
                "Paul Gibbons",
                "Pavlo Molchanov",
                "Pawel Morkisz",
                "Peter Dykas",
                "Peter Jin",
                "Pinky Xu",
                "Piotr Januszewski",
                "Pranav Prashant Thombre",
                "Prasoon Varshney",
                "Pritam Gundecha",
                "Przemek Tredak",
                "Qing Miao",
                "Qiyu Wan",
                "Rabeeh Karimi Mahabadi",
                "Rachit Garg",
                "Ran El-Yaniv",
                "Ran Zilberstein",
                "Rasoul Shafipour",
                "Rich Harang",
                "Rick Izzo",
                "Rima Shahbazyan",
                "Rishabh Garg",
                "Ritika Borkar",
                "Ritu Gala",
                "Riyad Islam",
                "Robert Hesse",
                "Roger Waleffe",
                "Rohit Watve",
                "Roi Koren",
                "Ruoxi Zhang",
                "Russell Hewett",
                "Russell J. Hewett",
                "Ryan Prenger",
                "Ryan Timbrook",
                "Sadegh Mahdavi",
                "Sahil Modi",
                "Samuel Kriman",
                "Sangkug Lim",
                "Sanjay Kariyappa",
                "Sanjeev Satheesh",
                "Saori Kaji",
                "Satish Pasumarthi",
                "Saurav Muralidharan",
                "Sean Narentharen",
                "Sean Narenthiran",
                "Seonmyeong Bak",
                "Sergey Kashirsky",
                "Seth Poulos",
                "Shahar Mor",
                "Shanmugam Ramasamy",
                "Shantanu Acharya",
                "Shaona Ghosh",
                "Sharath Turuvekere Sreenivas",
                "Shelby Thomas",
                "Shiqing Fan",
                "Shreya Gopal",
                "Shrimai Prabhumoye",
                "Shubham Pachori",
                "Shubham Toshniwal",
                "Shuoyang Ding",
                "Siddharth Singh",
                "Simeng Sun",
                "Smita Ithape",
                "Somshubra Majumdar",
                "Soumye Singhal",
                "Stas Sergienko",
                "Stefania Alborghetti",
                "Stephen Ge",
                "Sugam Dipak Devare",
                "Sumeet Kumar Barua",
                "Suseella Panguluri",
                "Suyog Gupta",
                "Sweta Priyadarshi",
                "Syeda Nahida Akter",
                "Tan Bui",
                "Teodor-Dumitru Ene",
                "Terry Kong",
                "Thanh Do",
                "Tijmen Blankevoort",
                "Tim Moon",
                "Tom Balough",
                "Tomer Asida",
                "Tomer Bar Natan",
                "Tomer Ronen",
                "Tugrul Konuk",
                "Twinkle Vashishth",
                "Udi Karpas",
                "Ushnish De",
                "Vahid Noorozi",
                "Vahid Noroozi",
                "Venkat Srinivasan",
                "Venmugil Elango",
                "Victor Cui",
                "Vijay Korthikanti",
                "Vinay Rao",
                "Vitaly Kurin",
                "Vitaly Lavrukhin",
                "Vladimir Anisimov",
                "Wanli Jiang",
                "Wasi Uddin Ahmad",
                "Wei Du",
                "Wei Ping",
                "Wenfei Zhou",
                "Will Jennings",
                "William Zhang",
                "Wojciech Prazuch",
                "Xiaowei Ren",
                "Yashaswi Karnati",
                "Yejin Choi",
                "Yev Meyer",
                "Yi-Fu Wu",
                "Yian Zhang",
                "Yigong Qin",
                "Ying Lin",
                "Yonatan Geifman",
                "Yonggan Fu",
                "Yoshi Subara",
                "Yoshi Suhara",
                "Yubo Gao",
                "Zach Moshe",
                "Zhen Dong",
                "Zhongbo Zhu",
                "Zihan Liu",
                "Zijia Chen",
                "Zijie Yan"
            ],
            "affiliations": [
                "NVIDIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.20856.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#agents",
                    "#small_models",
                    "#open_source",
                    "#reasoning",
                    "#inference",
                    "#long_context",
                    "#training",
                    "#rl",
                    "#optimization"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµĞ¼ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Nemotron 3 - Nano, Super Ğ¸ Ultra, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ÑÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğº Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ, Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ñƒ Ğ¸ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ñƒ. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Mixture-of-Experts Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Mamba-Transformer, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‰ÑƒÑ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğµ Ğ¾ĞºĞ½Ğ° Ğ´Ğ¾ 1Ğœ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ñ‹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ NVFP4 Ğ¸ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° LatentMoE Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ MTP-ÑĞ»Ğ¾Ğ¸ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ’ÑĞµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ° Nemotron 3 Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ¼ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±ÑĞ´Ğ¶ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Unlocking Advanced Reasoning with Nemotron 3 Models",
                    "desc": "The Nemotron 3 family introduces three models: Nano, Super, and Ultra, each designed for advanced reasoning and conversational tasks. Utilizing a Mixture-of-Experts hybrid Mamba-Transformer architecture, these models achieve high throughput and can handle context lengths of up to 1 million tokens. The Super and Ultra models leverage NVFP4 training and a new LatentMoE technique to enhance performance, while MTP layers accelerate text generation. All models are fine-tuned with multi-environment reinforcement learning, enabling complex reasoning and efficient tool use, with Nano being particularly cost-effective and accurate for smaller applications."
                },
                "zh": {
                    "title": "Nemotron 3ï¼šæ™ºèƒ½æ¨ç†ä¸é«˜æ•ˆå¯¹è¯çš„æœªæ¥",
                    "desc": "Nemotron 3ç³»åˆ—æ¨¡å‹åŒ…æ‹¬Nanoã€Superå’ŒUltraï¼Œå…·å¤‡å¼ºå¤§çš„æ™ºèƒ½ã€æ¨ç†å’Œå¯¹è¯èƒ½åŠ›ã€‚è¿™äº›æ¨¡å‹é‡‡ç”¨æ··åˆä¸“å®¶çš„Mamba-Transformeræ¶æ„ï¼Œèƒ½å¤Ÿå¤„ç†é«˜è¾¾100ä¸‡æ ‡è®°çš„ä¸Šä¸‹æ–‡ï¼Œæä¾›å“è¶Šçš„ååé‡ã€‚Superå’ŒUltraæ¨¡å‹ä½¿ç”¨NVFP4è®­ç»ƒï¼Œå¹¶å¼•å…¥äº†LatentMoEæ–°æ–¹æ³•ï¼Œæå‡äº†æ¨¡å‹è´¨é‡ï¼ŒåŒæ—¶è¿˜åŒ…å«MTPå±‚ä»¥åŠ å¿«æ–‡æœ¬ç”Ÿæˆé€Ÿåº¦ã€‚æ‰€æœ‰Nemotron 3æ¨¡å‹éƒ½ç»è¿‡å¤šç¯å¢ƒå¼ºåŒ–å­¦ä¹ çš„åè®­ç»ƒï¼Œæ”¯æŒæ¨ç†ã€å¤šæ­¥éª¤å·¥å…·ä½¿ç”¨å’Œç»†ç²’åº¦æ¨ç†é¢„ç®—æ§åˆ¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.20848",
            "title": "Nemotron 3 Nano: Open, Efficient Mixture-of-Experts Hybrid Mamba-Transformer Model for Agentic Reasoning",
            "url": "https://huggingface.co/papers/2512.20848",
            "abstract": "We present Nemotron 3 Nano 30B-A3B, a Mixture-of-Experts hybrid Mamba-Transformer language model. Nemotron 3 Nano was pretrained on 25 trillion text tokens, including more than 3 trillion new unique tokens over Nemotron 2, followed by supervised fine tuning and large-scale RL on diverse environments. Nemotron 3 Nano achieves better accuracy than our previous generation Nemotron 2 Nano while activating less than half of the parameters per forward pass. It achieves up to 3.3x higher inference throughput than similarly-sized open models like GPT-OSS-20B and Qwen3-30B-A3B-Thinking-2507, while also being more accurate on popular benchmarks. Nemotron 3 Nano demonstrates enhanced agentic, reasoning, and chat abilities and supports context lengths up to 1M tokens. We release both our pretrained Nemotron 3 Nano 30B-A3B Base and post-trained Nemotron 3 Nano 30B-A3B checkpoints on Hugging Face.",
            "score": 2,
            "issue_id": 235,
            "pub_date": "2025-12-23",
            "pub_date_card": {
                "ru": "23 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 23",
                "zh": "12æœˆ23æ—¥"
            },
            "hash": "b8f18d5988a4af2c",
            "authors": [
                "NVIDIA",
                ":",
                "Aaron Blakeman",
                "Aaron Grattafiori",
                "Aarti Basant",
                "Abhibha Gupta",
                "Abhinav Khattar",
                "Adi Renduchintala",
                "Aditya Vavre",
                "Akanksha Shukla",
                "Akhiad Bercovich",
                "Aleksander Ficek",
                "Aleksandr Shaposhnikov",
                "Alex Kondratenko",
                "Alexander Bukharin",
                "Alexandre Milesi",
                "Ali Taghibakhshi",
                "Alisa Liu",
                "Amelia Barton",
                "Ameya Sunil Mahabaleshwarkar",
                "Amir Klein",
                "Amit Zuker",
                "Amnon Geifman",
                "Amy Shen",
                "Anahita Bhiwandiwalla",
                "Andrew Tao",
                "Ann Guan",
                "Anubhav Mandarwal",
                "Arham Mehta",
                "Ashwath Aithal",
                "Ashwin Poojary",
                "Asif Ahamed",
                "Asma Kuriparambil Thekkumpate",
                "Ayush Dattagupta",
                "Banghua Zhu",
                "Bardiya Sadeghi",
                "Barnaby Simkin",
                "Ben Lanir",
                "Benedikt Schifferer",
                "Besmira Nushi",
                "Bilal Kartal",
                "Bita Darvish Rouhani",
                "Boris Ginsburg",
                "Brandon Norick",
                "Brandon Soubasis",
                "Branislav Kisacanin",
                "Brian Yu",
                "Bryan Catanzaro",
                "Carlo del Mundo",
                "Chantal Hwang",
                "Charles Wang",
                "Cheng-Ping Hsieh",
                "Chenghao Zhang",
                "Chenhan Yu",
                "Chetan Mungekar",
                "Chintan Patel",
                "Chris Alexiuk",
                "Christopher Parisien",
                "Collin Neale",
                "Damon Mosk-Aoyama",
                "Dan Su",
                "Dane Corneil",
                "Daniel Afrimi",
                "Daniel Rohrer",
                "Daniel Serebrenik",
                "Daria Gitman",
                "Daria Levy",
                "Darko Stosic",
                "David Mosallanezhad",
                "Deepak Narayanan",
                "Dhruv Nathawani",
                "Dima Rekesh",
                "Dina Yared",
                "Divyanshu Kakwani",
                "Dong Ahn",
                "Duncan Riach",
                "Dusan Stosic",
                "Edgar Minasyan",
                "Edward Lin",
                "Eileen Long",
                "Eileen Peters Long",
                "Elena Lantz",
                "Ellie Evans",
                "Elliott Ning",
                "Eric Chung",
                "Eric Harper",
                "Eric Tramel",
                "Erick Galinkin",
                "Erik Pounds",
                "Evan Briones",
                "Evelina Bakhturina",
                "Faisal Ladhak",
                "Fay Wang",
                "Fei Jia",
                "Felipe Soares",
                "Feng Chen",
                "Ferenc Galko",
                "Frankie Siino",
                "Gal Hubara Agam",
                "Ganesh Ajjanagadde",
                "Gantavya Bhatt",
                "Gargi Prasad",
                "George Armstrong",
                "Gerald Shen",
                "Gorkem Batmaz",
                "Grigor Nalbandyan",
                "Haifeng Qian",
                "Harsh Sharma",
                "Hayley Ross",
                "Helen Ngo",
                "Herman Sahota",
                "Hexin Wang",
                "Himanshu Soni",
                "Hiren Upadhyay",
                "Huizi Mao",
                "Huy C Nguyen",
                "Huy Q Nguyen",
                "Iain Cunningham",
                "Ido Shahaf",
                "Igor Gitman",
                "Ilya Loshchilov",
                "Ivan Moshkov",
                "Izzy Putterman",
                "Jan Kautz",
                "Jane Polak Scowcroft",
                "Jared Casper",
                "Jatin Mitra",
                "Jeffrey Glick",
                "Jenny Chen",
                "Jesse Oliver",
                "Jian Zhang",
                "Jiaqi Zeng",
                "Jie Lou",
                "Jimmy Zhang",
                "Jining Huang",
                "Joey Conway",
                "Joey Guman",
                "John Kamalu",
                "Johnny Greco",
                "Jonathan Cohen",
                "Joseph Jennings",
                "Joyjit Daw",
                "Julien Veron Vialard",
                "Junkeun Yi",
                "Jupinder Parmar",
                "Kai Xu",
                "Kan Zhu",
                "Kari Briski",
                "Katherine Cheung",
                "Katherine Luna",
                "Keshav Santhanam",
                "Kevin Shih",
                "Kezhi Kong",
                "Khushi Bhardwaj",
                "Krishna C. Puvvada",
                "Krzysztof Pawelec",
                "Kumar Anik",
                "Lawrence McAfee",
                "Laya Sleiman",
                "Leon Derczynski",
                "Li Ding",
                "Lucas Liebenwein",
                "Luis Vega",
                "Maanu Grover",
                "Maarten Van Segbroeck",
                "Maer Rodrigues de Melo",
                "Makesh Narsimhan Sreedhar",
                "Manoj Kilaru",
                "Maor Ashkenazi",
                "Marc Romeijn",
                "Mark Cai",
                "Markus Kliegl",
                "Maryam Moosaei",
                "Matvei Novikov",
                "Mehrzad Samadi",
                "Melissa Corpuz",
                "Mengru Wang",
                "Meredith Price",
                "Michael Boone",
                "Michael Evans",
                "Miguel Martinez",
                "Mike Chrzanowski",
                "Mohammad Shoeybi",
                "Mostofa Patwary",
                "Nabin Mulepati",
                "Natalie Hereth",
                "Nave Assaf",
                "Negar Habibi",
                "Neta Zmora",
                "Netanel Haber",
                "Nicola Sessions",
                "Nidhi Bhatia",
                "Nikhil Jukar",
                "Nikki Pope",
                "Nikolai Ludwig",
                "Nima Tajbakhsh",
                "Nirmal Juluru",
                "Oleksii Hrinchuk",
                "Oleksii Kuchaiev",
                "Olivier Delalleau",
                "Oluwatobi Olabiyi",
                "Omer Ullman Argov",
                "Ouye Xie",
                "Parth Chadha",
                "Pasha Shamis",
                "Pavlo Molchanov",
                "Pawel Morkisz",
                "Peter Dykas",
                "Peter Jin",
                "Pinky Xu",
                "Piotr Januszewski",
                "Pranav Prashant Thombre",
                "Prasoon Varshney",
                "Pritam Gundecha",
                "Qing Miao",
                "Rabeeh Karimi Mahabadi",
                "Ran El-Yaniv",
                "Ran Zilberstein",
                "Rasoul Shafipour",
                "Rich Harang",
                "Rick Izzo",
                "Rima Shahbazyan",
                "Rishabh Garg",
                "Ritika Borkar",
                "Ritu Gala",
                "Riyad Islam",
                "Roger Waleffe",
                "Rohit Watve",
                "Roi Koren",
                "Ruoxi Zhang",
                "Russell J. Hewett",
                "Ryan Prenger",
                "Ryan Timbrook",
                "Sadegh Mahdavi",
                "Sahil Modi",
                "Samuel Kriman",
                "Sanjay Kariyappa",
                "Sanjeev Satheesh",
                "Saori Kaji",
                "Satish Pasumarthi",
                "Sean Narentharen",
                "Sean Narenthiran",
                "Seonmyeong Bak",
                "Sergey Kashirsky",
                "Seth Poulos",
                "Shahar Mor",
                "Shanmugam Ramasamy",
                "Shantanu Acharya",
                "Shaona Ghosh",
                "Sharath Turuvekere Sreenivas",
                "Shelby Thomas",
                "Shiqing Fan",
                "Shreya Gopal",
                "Shrimai Prabhumoye",
                "Shubham Pachori",
                "Shubham Toshniwal",
                "Shuoyang Ding",
                "Siddharth Singh",
                "Simeng Sun",
                "Smita Ithape",
                "Somshubra Majumdar",
                "Soumye Singhal",
                "Stefania Alborghetti",
                "Stephen Ge",
                "Sugam Dipak Devare",
                "Sumeet Kumar Barua",
                "Suseella Panguluri",
                "Suyog Gupta",
                "Sweta Priyadarshi",
                "Syeda Nahida Akter",
                "Tan Bui",
                "Teodor-Dumitru Ene",
                "Terry Kong",
                "Thanh Do",
                "Tijmen Blankevoort",
                "Tom Balough",
                "Tomer Asida",
                "Tomer Bar Natan",
                "Tugrul Konuk",
                "Twinkle Vashishth",
                "Udi Karpas",
                "Ushnish De",
                "Vahid Noorozi",
                "Vahid Noroozi",
                "Venkat Srinivasan",
                "Venmugil Elango",
                "Vijay Korthikanti",
                "Vitaly Kurin",
                "Vitaly Lavrukhin",
                "Wanli Jiang",
                "Wasi Uddin Ahmad",
                "Wei Du",
                "Wei Ping",
                "Wenfei Zhou",
                "Will Jennings",
                "William Zhang",
                "Wojciech Prazuch",
                "Xiaowei Ren",
                "Yashaswi Karnati",
                "Yejin Choi",
                "Yev Meyer",
                "Yi-Fu Wu",
                "Yian Zhang",
                "Ying Lin",
                "Yonatan Geifman",
                "Yonggan Fu",
                "Yoshi Subara",
                "Yoshi Suhara",
                "Yubo Gao",
                "Zach Moshe",
                "Zhen Dong",
                "Zihan Liu",
                "Zijia Chen",
                "Zijie Yan"
            ],
            "affiliations": [
                "NVIDIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.20848.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#agents",
                    "#benchmark",
                    "#open_source",
                    "#reasoning",
                    "#inference",
                    "#long_context",
                    "#training",
                    "#optimization"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ­ĞºĞ¾Ğ½Ğ¾Ğ¼Ğ½Ğ°Ñ Ğ¼Ğ¾Ñ‰ÑŒ: Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ¿Ğ¾Ğ»Ğ¾Ğ²Ğ¸Ğ½Ğ½Ğ¾Ğ¹ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Nemotron 3 Nano, Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Mamba Ğ¸ Transformer Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ¼ĞµÑĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° 25 Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ‚ĞµĞºÑÑ‚Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Nemotron 3 Nano Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‡ĞµĞ¼ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰ĞµĞµ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ğµ, Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€ÑƒÑ Ğ¼ĞµĞ½ĞµĞµ Ğ¿Ğ¾Ğ»Ğ¾Ğ²Ğ¸Ğ½Ñ‹ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¿Ñ€Ğ¸ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´Ğµ Ğ²Ğ¿ĞµÑ€ĞµĞ´, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ² Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğ¸, Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸ Ğ¸ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğµ Ğ¾ĞºĞ½Ğ° Ğ´Ğ¾ 1 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²."
                },
                "en": {
                    "title": "Unlocking Efficiency and Accuracy in Language Modeling with Nemotron 3 Nano",
                    "desc": "The paper introduces Nemotron 3 Nano 30B-A3B, a new language model that uses a Mixture-of-Experts approach combined with a Mamba-Transformer architecture. It was pretrained on an extensive dataset of 25 trillion text tokens, significantly increasing the diversity of unique tokens compared to its predecessor, Nemotron 2. This model not only improves accuracy but also reduces the number of parameters activated during inference, leading to faster processing speeds. Additionally, Nemotron 3 Nano showcases advanced capabilities in reasoning and conversation, supporting very long context lengths of up to 1 million tokens."
                },
                "zh": {
                    "title": "Nemotron 3 Nanoï¼šé«˜æ•ˆçš„è¯­è¨€æ¨¡å‹æ–°çºªå…ƒ",
                    "desc": "Nemotron 3 Nano 30B-A3B æ˜¯ä¸€ç§æ··åˆä¸“å®¶çš„ Mamba-Transformer è¯­è¨€æ¨¡å‹ï¼Œç»è¿‡ 25 ä¸‡äº¿æ–‡æœ¬æ ‡è®°çš„é¢„è®­ç»ƒã€‚ä¸ä¹‹å‰çš„ Nemotron 2 Nano ç›¸æ¯”ï¼Œå®ƒåœ¨æ¿€æ´»å‚æ•°æ–¹é¢å‡å°‘äº†ä¸€åŠï¼ŒåŒæ—¶åœ¨æ¨ç†é€Ÿåº¦ä¸Šæé«˜äº† 3.3 å€ã€‚è¯¥æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæ›´é«˜çš„å‡†ç¡®æ€§ï¼Œå¹¶ä¸”å…·å¤‡æ›´å¼ºçš„æ¨ç†å’Œå¯¹è¯èƒ½åŠ›ï¼Œæ”¯æŒé•¿è¾¾ 100 ä¸‡ä¸ªæ ‡è®°çš„ä¸Šä¸‹æ–‡é•¿åº¦ã€‚æˆ‘ä»¬åœ¨ Hugging Face ä¸Šå‘å¸ƒäº†é¢„è®­ç»ƒå’Œåè®­ç»ƒçš„ Nemotron 3 Nano 30B-A3B æ£€æŸ¥ç‚¹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.21338",
            "title": "HiStream: Efficient High-Resolution Video Generation via Redundancy-Eliminated Streaming",
            "url": "https://huggingface.co/papers/2512.21338",
            "abstract": "High-resolution video generation, while crucial for digital media and film, is computationally bottlenecked by the quadratic complexity of diffusion models, making practical inference infeasible. To address this, we introduce HiStream, an efficient autoregressive framework that systematically reduces redundancy across three axes: i) Spatial Compression: denoising at low resolution before refining at high resolution with cached features; ii) Temporal Compression: a chunk-by-chunk strategy with a fixed-size anchor cache, ensuring stable inference speed; and iii) Timestep Compression: applying fewer denoising steps to subsequent, cache-conditioned chunks. On 1080p benchmarks, our primary HiStream model (i+ii) achieves state-of-the-art visual quality while demonstrating up to 76.2x faster denoising compared to the Wan2.1 baseline and negligible quality loss. Our faster variant, HiStream+, applies all three optimizations (i+ii+iii), achieving a 107.5x acceleration over the baseline, offering a compelling trade-off between speed and quality, thereby making high-resolution video generation both practical and scalable.",
            "score": 1,
            "issue_id": 235,
            "pub_date": "2025-12-24",
            "pub_date_card": {
                "ru": "24 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 24",
                "zh": "12æœˆ24æ—¥"
            },
            "hash": "e0de146fbb9ff7d7",
            "authors": [
                "Haonan Qiu",
                "Shikun Liu",
                "Zijian Zhou",
                "Zhaochong An",
                "Weiming Ren",
                "Zhiheng Liu",
                "Jonas Schult",
                "Sen He",
                "Shoufa Chen",
                "Yuren Cong",
                "Tao Xiang",
                "Ziwei Liu",
                "Juan-Manuel Perez-Rua"
            ],
            "affiliations": [
                "Meta AI",
                "Nanyang Technological University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.21338.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#video",
                    "#optimization",
                    "#inference"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¸Ñ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ HiStream â€” ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞ°ÑÑ‰Ğ¸Ñ…ÑÑ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ Ñ‚Ñ€Ñ‘Ğ¼ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼: Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ°Ñ ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ğ½Ğ³ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ² Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¼ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ¹ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ¾Ğ¹ Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¼, Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞºĞ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ ĞºÑÑˆĞ° Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°, Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ´ĞµĞ½Ğ¾Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹. ĞĞ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼ 1080p Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ HiStream Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ 76.2x ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğ¸ Ğ´ĞµĞ½Ğ¾Ğ¸Ğ·Ğ¸Ğ½Ğ³Ğ°, Ğ° ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚ HiStream+ Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ²ÑĞµÑ… Ñ‚Ñ€Ñ‘Ñ… Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¹ Ğ´Ğ°Ñ‘Ñ‚ 107.5x ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ¢Ğ°ĞºĞ¸Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ¼, Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ğ¼Ğ¾Ğ¹ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹."
                },
                "en": {
                    "title": "HiStream: Fast and Efficient High-Resolution Video Generation",
                    "desc": "This paper presents HiStream, a new framework for generating high-resolution videos more efficiently. It reduces computational complexity by compressing spatial, temporal, and timestep dimensions, allowing for faster denoising processes. HiStream achieves impressive visual quality while being significantly faster than previous models, with up to 107.5 times acceleration. This makes high-resolution video generation more practical and scalable for digital media applications."
                },
                "zh": {
                    "title": "é«˜æ•ˆè§†é¢‘ç”Ÿæˆï¼šé€Ÿåº¦ä¸è´¨é‡çš„å®Œç¾å¹³è¡¡",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºHiStreamçš„é«˜æ•ˆè‡ªå›å½’æ¡†æ¶ï¼Œç”¨äºé«˜åˆ†è¾¨ç‡è§†é¢‘ç”Ÿæˆã€‚è¯¥æ¡†æ¶é€šè¿‡ç©ºé—´å‹ç¼©ã€æ—¶é—´å‹ç¼©å’Œæ—¶é—´æ­¥å‹ç¼©ä¸‰ç§æ–¹å¼ç³»ç»Ÿæ€§åœ°å‡å°‘å†—ä½™ï¼Œä»è€Œæé«˜è®¡ç®—æ•ˆç‡ã€‚HiStreamåœ¨1080påŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„è§†è§‰è´¨é‡ï¼ŒåŒæ—¶åœ¨å»å™ªé€Ÿåº¦ä¸Šæ¯”Wan2.1åŸºçº¿å¿«76.2å€ï¼Œå‡ ä¹æ²¡æœ‰è´¨é‡æŸå¤±ã€‚å…¶æ›´å¿«çš„å˜ä½“HiStream+ç»“åˆäº†æ‰€æœ‰ä¸‰ç§ä¼˜åŒ–ï¼Œå®ç°äº†107.5å€çš„åŠ é€Ÿï¼Œæä¾›äº†é€Ÿåº¦ä¸è´¨é‡ä¹‹é—´çš„è‰¯å¥½å¹³è¡¡ï¼Œä½¿é«˜åˆ†è¾¨ç‡è§†é¢‘ç”Ÿæˆå˜å¾—å®ç”¨ä¸”å¯æ‰©å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.21334",
            "title": "Streaming Video Instruction Tuning",
            "url": "https://huggingface.co/papers/2512.21334",
            "abstract": "We present Streamo, a real-time streaming video LLM that serves as a general-purpose interactive assistant. Unlike existing online video models that focus narrowly on question answering or captioning, Streamo performs a broad spectrum of streaming video tasks, including real-time narration, action understanding, event captioning, temporal event grounding, and time-sensitive question answering. To develop such versatility, we construct Streamo-Instruct-465K, a large-scale instruction-following dataset tailored for streaming video understanding. The dataset covers diverse temporal contexts and multi-task supervision, enabling unified training across heterogeneous streaming tasks. After training end-to-end on the instruction-following dataset through a streamlined pipeline, Streamo exhibits strong temporal reasoning, responsive interaction, and broad generalization across a variety of streaming benchmarks. Extensive experiments show that Streamo bridges the gap between offline video perception models and real-time multimodal assistants, making a step toward unified, intelligent video understanding in continuous video streams.",
            "score": 0,
            "issue_id": 235,
            "pub_date": "2025-12-24",
            "pub_date_card": {
                "ru": "24 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 24",
                "zh": "12æœˆ24æ—¥"
            },
            "hash": "c1ff79b122635cb2",
            "authors": [
                "Jiaer Xia",
                "Peixian Chen",
                "Mengdan Zhang",
                "Xing Sun",
                "Kaiyang Zhou"
            ],
            "affiliations": [
                "Hong Kong Baptist University",
                "Tencent Youtu Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.21334.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#video",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "ğŸ“¹",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Streamo â€” Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ ĞºĞ°Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸Ğº. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Streamo Ñ€ĞµÑˆĞ°ĞµÑ‚ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¹ ÑĞ¿ĞµĞºÑ‚Ñ€ Ğ·Ğ°Ğ´Ğ°Ñ‡: Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²ĞµÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹, Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ° ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹, Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Streamo-Instruct-465K Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñ‹ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Streamo ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ°Ñ…."
                },
                "en": {
                    "title": "Streamo: Your Real-Time Video Assistant!",
                    "desc": "Streamo is a real-time streaming video large language model (LLM) designed to function as an interactive assistant across various video tasks. It goes beyond traditional models by integrating capabilities such as real-time narration, action understanding, and temporal event grounding, allowing it to handle a wide range of streaming video applications. The model is trained on a specially created dataset, Streamo-Instruct-465K, which includes diverse instruction sets for effective multi-task learning in streaming contexts. As a result, Streamo demonstrates advanced temporal reasoning and adaptability, bridging the gap between offline video models and real-time interactive systems."
                },
                "zh": {
                    "title": "Streamoï¼šå®æ—¶æµåª’ä½“è§†é¢‘çš„æ™ºèƒ½åŠ©æ‰‹",
                    "desc": "Streamoæ˜¯ä¸€ç§å®æ—¶æµåª’ä½“è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼Œä½œä¸ºé€šç”¨çš„äº’åŠ¨åŠ©æ‰‹ã€‚ä¸ç°æœ‰çš„åœ¨çº¿è§†é¢‘æ¨¡å‹ä¸åŒï¼ŒStreamoèƒ½å¤Ÿæ‰§è¡Œå¤šç§æµåª’ä½“è§†é¢‘ä»»åŠ¡ï¼ŒåŒ…æ‹¬å®æ—¶å™è¿°ã€åŠ¨ä½œç†è§£ã€äº‹ä»¶å­—å¹•ã€æ—¶é—´äº‹ä»¶å®šä½å’Œæ—¶é—´æ•æ„Ÿçš„é—®é¢˜å›ç­”ã€‚ä¸ºäº†å®ç°è¿™ç§å¤šåŠŸèƒ½æ€§ï¼Œæˆ‘ä»¬æ„å»ºäº†Streamo-Instruct-465Kï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹æµåª’ä½“è§†é¢‘ç†è§£çš„å¤§è§„æ¨¡æŒ‡ä»¤è·Ÿéšæ•°æ®é›†ï¼Œæ¶µç›–äº†å¤šæ ·çš„æ—¶é—´ä¸Šä¸‹æ–‡å’Œå¤šä»»åŠ¡ç›‘ç£ã€‚ç»è¿‡ç«¯åˆ°ç«¯çš„è®­ç»ƒï¼ŒStreamoåœ¨æ—¶é—´æ¨ç†ã€å“åº”äº’åŠ¨å’Œå¤šç§æµåª’ä½“åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ¨åŠ¨äº†å®æ—¶å¤šæ¨¡æ€åŠ©æ‰‹ä¸ç¦»çº¿è§†é¢‘æ„ŸçŸ¥æ¨¡å‹ä¹‹é—´çš„èåˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.21010",
            "title": "LLM Swiss Round: Aggregating Multi-Benchmark Performance via Competitive Swiss-System Dynamics",
            "url": "https://huggingface.co/papers/2512.21010",
            "abstract": "The rapid proliferation of Large Language Models (LLMs) and diverse specialized benchmarks necessitates a shift from fragmented, task-specific metrics to a holistic, competitive ranking system that effectively aggregates performance across multiple ability dimensions. Primarily using static scoring, current evaluation methods are fundamentally limited. They struggle to determine the proper mix ratio across diverse benchmarks, and critically, they fail to capture a model's dynamic competitive fitness or its vulnerability when confronted with sequential, high-stakes tasks. To address this, we introduce the novel Competitive Swiss-System Dynamics (CSD) framework. CSD simulates a multi-round, sequential contest where models are dynamically paired across a curated sequence of benchmarks based on their accumulated win-loss record. And Monte Carlo Simulation (N=100,000 iterations) is used to approximate the statistically robust Expected Win Score (E[S_m]), which eliminates the noise of random pairing and early-round luck. Furthermore, we implement a Failure Sensitivity Analysis by parameterizing the per-round elimination quantity (T_k), which allows us to profile models based on their risk appetite--distinguishing between robust generalists and aggressive specialists. We demonstrate that CSD provides a more nuanced and context-aware ranking than traditional aggregate scoring and static pairwise models, representing a vital step towards risk-informed, next-generation LLM evaluation.",
            "score": 0,
            "issue_id": 235,
            "pub_date": "2025-12-24",
            "pub_date_card": {
                "ru": "24 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 24",
                "zh": "12æœˆ24æ—¥"
            },
            "hash": "e52c7e88e97ec24e",
            "authors": [
                "Jiashuo Liu",
                "Jiayun Wu",
                "Chunjie Wu",
                "Jingkai Liu",
                "Zaiyuan Wang",
                "Huan Zhou",
                "Wenhao Huang",
                "Hongseok Namkoong"
            ],
            "affiliations": [
                "ByteDance Seed",
                "Carnegie Mellon University",
                "Columbia University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.21010.jpg",
            "data": {
                "categories": [
                    "#benchmark"
                ],
                "emoji": "ğŸ†",
                "ru": {
                    "title": "Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ñ‚ÑƒÑ€Ğ½Ğ¸Ñ€Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Competitive Swiss-System Dynamics (CSD), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ·Ğ°Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ¾Ğ¼, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ÑƒĞ½Ğ´Ğ½Ñ‹Ñ… Ñ‚ÑƒÑ€Ğ½Ğ¸Ñ€Ğ½Ñ‹Ñ… ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸ÑÑ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ĞœĞ¾Ğ½Ñ‚Ğµ-ĞšĞ°Ñ€Ğ»Ğ¾ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑ‡ĞµÑ‚Ğ° Ğ¾Ğ¶Ğ¸Ğ´Ğ°ĞµĞ¼Ğ¾Ğ³Ğ¾ ÑÑ‡Ñ‘Ñ‚Ğ° Ğ¿Ğ¾Ğ±ĞµĞ´Ñ‹ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ ĞµÑ‘ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¸ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚. CSD Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ñ‹ÑĞ²Ğ¸Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ Ğ°Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğº ÑĞ±Ğ¾ÑĞ¼. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½ÑƒÑ Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Dynamic Evaluation for Next-Gen Language Models",
                    "desc": "This paper introduces the Competitive Swiss-System Dynamics (CSD) framework for evaluating Large Language Models (LLMs) across multiple benchmarks. Unlike traditional methods that use static scoring, CSD dynamically pairs models in a multi-round contest based on their performance history. It employs Monte Carlo Simulation to calculate a robust Expected Win Score, reducing the impact of randomness in early rounds. Additionally, the framework includes a Failure Sensitivity Analysis to assess models' risk profiles, distinguishing between generalists and specialists, thus offering a more comprehensive evaluation of LLMs."
                },
                "zh": {
                    "title": "åŠ¨æ€è¯„ä¼°ï¼šå¤§å‹è¯­è¨€æ¨¡å‹çš„æ–°æ ‡å‡†",
                    "desc": "éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤šæ ·åŒ–çš„ä¸“ä¸šåŸºå‡†çš„å¿«é€Ÿå‘å±•ï¼Œç°æœ‰çš„ä»»åŠ¡ç‰¹å®šè¯„ä¼°æŒ‡æ ‡æ˜¾å¾—ä¸å¤Ÿå…¨é¢ã€‚ä¼ ç»Ÿçš„é™æ€è¯„åˆ†æ–¹æ³•æ— æ³•æœ‰æ•ˆè¯„ä¼°æ¨¡å‹åœ¨ä¸åŒåŸºå‡†ä¸Šçš„è¡¨ç°æ··åˆæ¯”ä¾‹ï¼Œä¹Ÿæ— æ³•æ•æ‰æ¨¡å‹åœ¨è¿ç»­é«˜é£é™©ä»»åŠ¡ä¸­çš„åŠ¨æ€ç«äº‰èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç«äº‰ç‘å£«ç³»ç»ŸåŠ¨æ€ï¼ˆCSDï¼‰æ¡†æ¶ï¼Œé€šè¿‡å¤šè½®æ¯”èµ›åŠ¨æ€é…å¯¹æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿæ¥è¿‘ä¼¼ç»Ÿè®¡ç¨³å¥çš„æœŸæœ›èƒœåˆ†ï¼ˆE[S_m]ï¼‰ã€‚CSDæä¾›äº†æ¯”ä¼ ç»Ÿè¯„åˆ†æ–¹æ³•æ›´ç»†è‡´å’Œä¸Šä¸‹æ–‡æ•æ„Ÿçš„æ’åï¼Œæ ‡å¿—ç€å‘é£é™©å¯¼å‘çš„ä¸‹ä¸€ä»£LLMè¯„ä¼°è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-12-24.html",
    "link_next": "2025-12-26.html",
    "link_month": "2025-12.html",
    "short_date_prev": {
        "ru": "24.12",
        "en": "12/24",
        "zh": "12æœˆ24æ—¥"
    },
    "short_date_next": {
        "ru": "26.12",
        "en": "12/26",
        "zh": "12æœˆ26æ—¥"
    },
    "categories": {
        "#dataset": 3,
        "#data": 0,
        "#benchmark": 5,
        "#agents": 2,
        "#cv": 0,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 4,
        "#3d": 1,
        "#audio": 1,
        "#video": 5,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 4,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 4,
        "#survey": 1,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 2,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    }
}