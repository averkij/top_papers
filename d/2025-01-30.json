{
    "date": {
        "ru": "30 ÑĞ½Ğ²Ğ°Ñ€Ñ",
        "en": "January 30",
        "zh": "1æœˆ30æ—¥"
    },
    "time_utc": "2025-01-30 12:18",
    "weekday": 3,
    "issue_id": 1949,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.17703",
            "title": "Critique Fine-Tuning: Learning to Critique is More Effective than Learning to Imitate",
            "url": "https://huggingface.co/papers/2501.17703",
            "abstract": "Supervised Fine-Tuning (SFT) is commonly used to train language models to imitate annotated responses for given instructions. In this paper, we challenge this paradigm and propose Critique Fine-Tuning (CFT), a strategy where models learn to critique noisy responses rather than simply imitate correct ones. Inspired by human learning processes that emphasize critical thinking, CFT encourages deeper analysis and nuanced understanding-traits often overlooked by standard SFT. To validate the effectiveness of CFT, we construct a 50K-sample dataset from WebInstruct, using GPT-4o as the teacher to generate critiques in the form of (input=[query; noisy response], output=critique). CFT on this dataset yields a consistent 4-10% improvement over SFT on six math benchmarks with different base models like Qwen2.5, Qwen2.5-Math and DeepSeek-Math. We further expand to MetaMath and NuminaMath datasets and observe similar gains over SFT. Notably, our Qwen2.5-Math-CFT model-trained on just 50K samples-matches or outperforms competitive models such as AceMath and Qwen2.5-Math-Instruct on most benchmarks, both of which use over 2M samples. Ablation studies show that CFT is robust to the source of noisy response and teacher critique model. Through these findings, we argue that critique-based training offers a more effective alternative to advance the reasoning of language models.",
            "score": 13,
            "issue_id": 1940,
            "pub_date": "2025-01-29",
            "pub_date_card": {
                "ru": "29 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 29",
                "zh": "1æœˆ29æ—¥"
            },
            "hash": "18b6407976346581",
            "authors": [
                "Yubo Wang",
                "Xiang Yue",
                "Wenhu Chen"
            ],
            "affiliations": [
                "Carnegie Mellon University, Pittsburgh",
                "Department of Computer Science, University of Waterloo",
                "Vector Institute, Toronto"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.17703.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#optimization",
                    "#dataset",
                    "#math"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞšÑ€Ğ¸Ñ‚Ğ¸ĞºĞ° Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¸: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ - Critique Fine-Tuning (CFT), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒÑ‡Ğ¸Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹, Ğ° Ğ½Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ. CFT Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 4-10% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Supervised Fine-Tuning (SFT) Ğ½Ğ° ÑˆĞµÑÑ‚Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Qwen2.5-Math-CFT, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ²ÑĞµĞ³Ğ¾ Ğ½Ğ° 50 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¸Ğ»Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ñ… Ğ±Ğ¾Ğ»ĞµĞµ 2 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Critique Fine-Tuning: Enhancing Language Model Reasoning through Critical Analysis",
                    "desc": "This paper introduces Critique Fine-Tuning (CFT), a novel approach to training language models that focuses on teaching them to critique noisy responses instead of merely imitating correct ones. By mimicking human critical thinking, CFT fosters a deeper understanding and analysis of language, which is often neglected in traditional Supervised Fine-Tuning (SFT). The authors validate CFT's effectiveness using a dataset of 50,000 samples generated by GPT-4o, demonstrating consistent performance improvements of 4-10% over SFT across various math benchmarks. The results suggest that critique-based training can significantly enhance the reasoning capabilities of language models, outperforming models trained on much larger datasets."
                },
                "zh": {
                    "title": "æ‰¹è¯„å¾®è°ƒï¼šæå‡è¯­è¨€æ¨¡å‹æ¨ç†çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„æ–¹æ³•ï¼Œç§°ä¸ºæ‰¹è¯„å¾®è°ƒï¼ˆCFTï¼‰ï¼Œä¸ä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä¸åŒï¼ŒCFTè®©æ¨¡å‹å­¦ä¹ å¦‚ä½•æ‰¹è¯„å™ªå£°å“åº”ï¼Œè€Œä¸æ˜¯ä»…ä»…æ¨¡ä»¿æ­£ç¡®çš„ç­”æ¡ˆã€‚è¿™ç§æ–¹æ³•å—åˆ°äººç±»å­¦ä¹ è¿‡ç¨‹çš„å¯å‘ï¼Œå¼ºè°ƒæ‰¹åˆ¤æ€§æ€ç»´ï¼Œä¿ƒè¿›æ›´æ·±å±‚æ¬¡çš„åˆ†æå’Œç»†è‡´çš„ç†è§£ã€‚é€šè¿‡æ„å»ºä¸€ä¸ªåŒ…å«5ä¸‡æ ·æœ¬çš„æ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨GPT-4oç”Ÿæˆæ‰¹è¯„ï¼ŒCFTåœ¨å¤šä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸­ç›¸è¾ƒäºSFTå–å¾—äº†4-10%çš„ä¸€è‡´æ€§æå‡ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒåŸºäºæ‰¹è¯„çš„è®­ç»ƒæ–¹æ³•ä¸ºæå‡è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æä¾›äº†æ›´æœ‰æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.14334",
            "title": "Exploring the sustainable scaling of AI dilemma: A projective study of corporations' AI environmental impacts",
            "url": "https://huggingface.co/papers/2501.14334",
            "abstract": "The rapid growth of artificial intelligence (AI), particularly Large Language Models (LLMs), has raised concerns regarding its global environmental impact that extends beyond greenhouse gas emissions to include consideration of hardware fabrication and end-of-life processes. The opacity from major providers hinders companies' abilities to evaluate their AI-related environmental impacts and achieve net-zero targets.   In this paper, we propose a methodology to estimate the environmental impact of a company's AI portfolio, providing actionable insights without necessitating extensive AI and Life-Cycle Assessment (LCA) expertise. Results confirm that large generative AI models consume up to 4600x more energy than traditional models. Our modelling approach, which accounts for increased AI usage, hardware computing efficiency, and changes in electricity mix in line with IPCC scenarios, forecasts AI electricity use up to 2030. Under a high adoption scenario, driven by widespread Generative AI and agents adoption associated to increasingly complex models and frameworks, AI electricity use is projected to rise by a factor of 24.4.   Mitigating the environmental impact of Generative AI by 2030 requires coordinated efforts across the AI value chain. Isolated measures in hardware efficiency, model efficiency, or grid improvements alone are insufficient. We advocate for standardized environmental assessment frameworks, greater transparency from the all actors of the value chain and the introduction of a \"Return on Environment\" metric to align AI development with net-zero goals.",
            "score": 11,
            "issue_id": 1945,
            "pub_date": "2025-01-24",
            "pub_date_card": {
                "ru": "24 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 24",
                "zh": "1æœˆ24æ—¥"
            },
            "hash": "af6e1a0fd9d77530",
            "authors": [
                "ClÃ©ment Desroches",
                "Martin Chauvin",
                "Louis Ladan",
                "Caroline Vateau",
                "Simon Gosset",
                "Philippe Cordier"
            ],
            "affiliations": [
                "Capgemini Invent 145 quai du PrÃ©sident Roosevelt, 92130 Issy Les Moulineaux, France"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.14334.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#data",
                    "#benchmark",
                    "#ethics"
                ],
                "emoji": "ğŸŒ±",
                "ru": {
                    "title": "Ğ—ĞµĞ»ĞµĞ½Ñ‹Ğ¹ Ğ˜Ğ˜: Ğ¿ÑƒÑ‚ÑŒ Ğº ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾Ğ¼Ñƒ Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ¼Ñƒ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞºĞ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ²Ğ¾Ğ·Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞºĞ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ»ĞµĞ´Ğ° AI-Ğ¿Ğ¾Ñ€Ñ‚Ñ„ĞµĞ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¸, Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰ÑƒÑ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ˜Ğ˜ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¶Ğ¸Ğ·Ğ½ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ†Ğ¸ĞºĞ»Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ˜Ğ˜ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ÑÑÑ‚ Ğ´Ğ¾ 4600 Ñ€Ğ°Ğ· Ğ±Ğ¾Ğ»ÑŒÑˆĞµ ÑĞ½ĞµÑ€Ğ³Ğ¸Ğ¸, Ñ‡ĞµĞ¼ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ, Ğ° Ğº 2030 Ğ³Ğ¾Ğ´Ñƒ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ»ĞµĞºÑ‚Ñ€Ğ¾ÑĞ½ĞµÑ€Ğ³Ğ¸Ğ¸ Ğ˜Ğ˜ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ²Ñ‹Ñ€Ğ°ÑÑ‚Ğ¸ Ğ² 24,4 Ñ€Ğ°Ğ·Ğ°. Ğ”Ğ»Ñ ÑĞ¼ÑĞ³Ñ‡ĞµĞ½Ğ¸Ñ ÑĞºĞ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ·Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğº ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ 'Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‚Ğ° Ğ½Ğ° Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ÑƒÑ ÑÑ€ĞµĞ´Ñƒ'."
                },
                "en": {
                    "title": "Assessing AI's Environmental Footprint for a Sustainable Future",
                    "desc": "This paper addresses the environmental impact of artificial intelligence, especially focusing on Large Language Models (LLMs). It highlights that these models can consume significantly more energy than traditional models, with estimates showing up to 4600 times higher energy use. The authors propose a methodology for companies to assess their AI portfolio's environmental impact, making it easier to achieve net-zero targets without needing deep expertise in AI or Life-Cycle Assessment. They emphasize the need for coordinated efforts across the AI value chain and advocate for standardized frameworks and transparency to mitigate the environmental effects of Generative AI by 2030."
                },
                "zh": {
                    "title": "æ¨åŠ¨AIå¯æŒç»­å‘å±•ï¼Œä¿æŠ¤ç¯å¢ƒæœªæ¥",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†äººå·¥æ™ºèƒ½ï¼Œç‰¹åˆ«æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹ç¯å¢ƒçš„å½±å“ï¼ŒåŒ…æ‹¬ç¡¬ä»¶åˆ¶é€ å’Œç”Ÿå‘½å‘¨æœŸç»“æŸçš„è¿‡ç¨‹ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå¤§å‹ç”Ÿæˆæ€§AIæ¨¡å‹çš„èƒ½è€—æ˜¯ä¼ ç»Ÿæ¨¡å‹çš„4600å€ã€‚ä¸ºäº†è¯„ä¼°å…¬å¸çš„AIæŠ•èµ„ç»„åˆçš„ç¯å¢ƒå½±å“ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§æ–¹æ³•è®ºï¼Œèƒ½å¤Ÿåœ¨ä¸éœ€è¦æ·±å…¥çš„AIå’Œç”Ÿå‘½å‘¨æœŸè¯„ä¼°ï¼ˆLCAï¼‰ä¸“ä¸šçŸ¥è¯†çš„æƒ…å†µä¸‹æä¾›å¯è¡Œçš„è§è§£ã€‚ä¸ºäº†åœ¨2030å¹´å‰å‡è½»ç”Ÿæˆæ€§AIçš„ç¯å¢ƒå½±å“ï¼Œéœ€è¦åœ¨AIä»·å€¼é“¾çš„å„ä¸ªç¯èŠ‚è¿›è¡Œåè°ƒåŠªåŠ›ï¼Œè€Œä¸ä»…ä»…ä¾é ç¡¬ä»¶æ•ˆç‡æˆ–æ¨¡å‹æ•ˆç‡çš„å•ä¸€æªæ–½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.17195",
            "title": "Atla Selene Mini: A General Purpose Evaluation Model",
            "url": "https://huggingface.co/papers/2501.17195",
            "abstract": "We introduce Atla Selene Mini, a state-of-the-art small language model-as-a-judge (SLMJ). Selene Mini is a general-purpose evaluator that outperforms the best SLMJs and GPT-4o-mini on overall performance across 11 out-of-distribution benchmarks, spanning absolute scoring, classification, and pairwise preference tasks. It is the highest-scoring 8B generative model on RewardBench, surpassing strong baselines like GPT-4o and specialized judges. To achieve this, we develop a principled data curation strategy that augments public datasets with synthetically generated critiques and ensures high quality through filtering and dataset ablations. We train our model on a combined direct preference optimization (DPO) and supervised fine-tuning (SFT) loss, and produce a highly promptable evaluator that excels in real-world scenarios. Selene Mini shows dramatically improved zero-shot agreement with human expert evaluations on financial and medical industry datasets. It is also robust to variations in prompt format. Preliminary results indicate that Selene Mini is the top-ranking evaluator in a live, community-driven Judge Arena. We release the model weights on HuggingFace (https://hf.co/AtlaAI/Selene-1-Mini-Llama-3.1-8B) and Ollama to encourage widespread community adoption.",
            "score": 9,
            "issue_id": 1940,
            "pub_date": "2025-01-27",
            "pub_date_card": {
                "ru": "27 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 27",
                "zh": "1æœˆ27æ—¥"
            },
            "hash": "fb718aaf1278709b",
            "authors": [
                "Andrei Alexandru",
                "Antonia Calvi",
                "Henry Broomfield",
                "Jackson Golden",
                "Kyle Dai",
                "Mathias Leys",
                "Maurice Burger",
                "Max Bartolo",
                "Roman Engeler",
                "Sashank Pisupati",
                "Toby Drane",
                "Young Sun Park"
            ],
            "affiliations": [
                "Cohere",
                "University College London",
                "atla"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.17195.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#open_source",
                    "#agi",
                    "#synthetic",
                    "#data",
                    "#training",
                    "#optimization",
                    "#dataset",
                    "#rlhf"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "Atla Selene Mini: ĞœĞ°Ğ»Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-ÑÑƒĞ´ÑŒÑ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Atla Selene Mini - Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²ÑƒÑ Ğ¼Ğ°Ğ»ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-ÑÑƒĞ´ÑŒÑ (SLMJ). ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ SLMJ Ğ¸ GPT-4o-mini Ğ¿Ğ¾ Ğ¾Ğ±Ñ‰ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² 11 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ…. Selene Mini Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ ĞºÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ĞµĞ¹ Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¸Ğµ Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ²-Ğ»ÑĞ´ĞµĞ¹ Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ±ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸."
                },
                "en": {
                    "title": "Selene Mini: The Next Level in Language Model Evaluation!",
                    "desc": "Atla Selene Mini is a cutting-edge small language model designed to evaluate various tasks effectively. It surpasses existing models, including GPT-4o-mini, by achieving superior performance on 11 benchmarks related to scoring, classification, and preference tasks. The model benefits from a unique data curation strategy that enhances public datasets with synthetic critiques, ensuring high-quality training data. With a combination of direct preference optimization and supervised fine-tuning, Selene Mini demonstrates strong alignment with human evaluations, particularly in specialized fields like finance and medicine."
                },
                "zh": {
                    "title": "Atla Selene Miniï¼šè¶…è¶Šä¼ ç»Ÿè¯„ä¼°çš„è¯­è¨€æ¨¡å‹",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†Atla Selene Miniï¼Œè¿™æ˜¯ä¸€ç§å…ˆè¿›çš„å°å‹è¯­è¨€æ¨¡å‹è¯„ä¼°å™¨ï¼ˆSLMJï¼‰ã€‚Selene Miniåœ¨11ä¸ªä¸åŒçš„åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ï¼ŒåŒ…æ‹¬GPT-4o-miniï¼Œå°¤å…¶åœ¨ç»å¯¹è¯„åˆ†ã€åˆ†ç±»å’Œæˆå¯¹åå¥½ä»»åŠ¡ä¸Šã€‚é€šè¿‡ç²¾å¿ƒçš„æ•°æ®ç­–åˆ’ç­–ç•¥ï¼Œæˆ‘ä»¬å¢å¼ºäº†å…¬å…±æ•°æ®é›†ï¼Œå¹¶ç¡®ä¿æ•°æ®è´¨é‡ï¼Œä»è€Œè®­ç»ƒå‡ºä¸€ä¸ªåœ¨çœŸå®åœºæ™¯ä¸­è¡¨ç°å‡ºè‰²çš„è¯„ä¼°å™¨ã€‚Selene Miniåœ¨é‡‘èå’ŒåŒ»ç–—è¡Œä¸šæ•°æ®é›†ä¸Šä¸äººç±»ä¸“å®¶è¯„ä¼°çš„é›¶-shotä¸€è‡´æ€§æ˜¾è‘—æé«˜ï¼Œä¸”å¯¹æç¤ºæ ¼å¼çš„å˜åŒ–å…·æœ‰è‰¯å¥½çš„é²æ£’æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.17749",
            "title": "Early External Safety Testing of OpenAI's o3-mini: Insights from the Pre-Deployment Evaluation",
            "url": "https://huggingface.co/papers/2501.17749",
            "abstract": "Large Language Models (LLMs) have become an integral part of our daily lives. However, they impose certain risks, including those that can harm individuals' privacy, perpetuate biases and spread misinformation. These risks highlight the need for robust safety mechanisms, ethical guidelines, and thorough testing to ensure their responsible deployment. Safety of LLMs is a key property that needs to be thoroughly tested prior the model to be deployed and accessible to the general users. This paper reports the external safety testing experience conducted by researchers from Mondragon University and University of Seville on OpenAI's new o3-mini LLM as part of OpenAI's early access for safety testing program. In particular, we apply our tool, ASTRAL, to automatically and systematically generate up to date unsafe test inputs (i.e., prompts) that helps us test and assess different safety categories of LLMs. We automatically generate and execute a total of 10,080 unsafe test input on a early o3-mini beta version. After manually verifying the test cases classified as unsafe by ASTRAL, we identify a total of 87 actual instances of unsafe LLM behavior. We highlight key insights and findings uncovered during the pre-deployment external testing phase of OpenAI's latest LLM.",
            "score": 7,
            "issue_id": 1940,
            "pub_date": "2025-01-29",
            "pub_date_card": {
                "ru": "29 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 29",
                "zh": "1æœˆ29æ—¥"
            },
            "hash": "325df13c4995c5e9",
            "authors": [
                "Aitor Arrieta",
                "Miriam Ugarte",
                "Pablo Valle",
                "JosÃ© Antonio Parejo",
                "Sergio Segura"
            ],
            "affiliations": [
                "Mondragon University, Mondragon, Spain",
                "University of Seville, Seville, Spain"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.17749.jpg",
            "data": {
                "categories": [
                    "#ethics",
                    "#data",
                    "#inference",
                    "#training",
                    "#security"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: ĞºĞ»ÑÑ‡ Ğº Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ˜Ğ˜",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ¿Ñ‹Ñ‚ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ¾Ğ²Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ OpenAI o3-mini. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ ASTRAL Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½ĞµĞ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ‘Ñ‹Ğ»Ğ¾ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾ Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¾ 10 080 Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¸Ğ· ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… 87 ÑĞ»ÑƒÑ‡Ğ°ĞµĞ² Ğ±Ñ‹Ğ»Ğ¸ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹ ĞºĞ°Ğº Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ½ĞµĞ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿ĞµÑ€ĞµĞ´ Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼."
                },
                "en": {
                    "title": "Ensuring Safety in Large Language Models: A Testing Approach",
                    "desc": "This paper discusses the safety testing of Large Language Models (LLMs), focusing on the o3-mini model from OpenAI. Researchers from Mondragon University and University of Seville used a tool called ASTRAL to automatically create unsafe test inputs to evaluate the model's safety. They generated and executed 10,080 test prompts, identifying 87 instances of unsafe behavior after manual verification. The findings emphasize the importance of rigorous safety assessments before deploying LLMs to mitigate risks like privacy violations and misinformation."
                },
                "zh": {
                    "title": "ç¡®ä¿å¤§å‹è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§ä¸è´£ä»»ä½¿ç”¨",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æˆ‘ä»¬çš„æ—¥å¸¸ç”Ÿæ´»ä¸­å˜å¾—ä¸å¯æˆ–ç¼ºï¼Œä½†å®ƒä»¬ä¹Ÿå¸¦æ¥äº†éšç§é£é™©ã€åè§å’Œé”™è¯¯ä¿¡æ¯ä¼ æ’­ç­‰é—®é¢˜ã€‚è¿™äº›é£é™©è¡¨æ˜éœ€è¦å»ºç«‹å¼ºæœ‰åŠ›çš„å®‰å…¨æœºåˆ¶å’Œä¼¦ç†æŒ‡å¯¼ï¼Œä»¥ç¡®ä¿æ¨¡å‹çš„è´Ÿè´£ä»»ä½¿ç”¨ã€‚æœ¬æ–‡æŠ¥å‘Šäº†è’™å¾·æ‹‰è´¡å¤§å­¦å’Œå¡ç»´åˆ©äºšå¤§å­¦çš„ç ”ç©¶äººå‘˜å¯¹OpenAIçš„æ–°o3-mini LLMè¿›è¡Œçš„å¤–éƒ¨å®‰å…¨æµ‹è¯•ç»éªŒã€‚æˆ‘ä»¬ä½¿ç”¨ASTRALå·¥å…·è‡ªåŠ¨ç”Ÿæˆä¸å®‰å…¨çš„æµ‹è¯•è¾“å…¥ï¼Œä»¥è¯„ä¼°LLMsçš„ä¸åŒå®‰å…¨ç±»åˆ«ï¼Œå¹¶å‘ç°äº†87ä¸ªå®é™…çš„ä¸å®‰å…¨è¡Œä¸ºå®ä¾‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.17433",
            "title": "Virus: Harmful Fine-tuning Attack for Large Language Models Bypassing Guardrail Moderation",
            "url": "https://huggingface.co/papers/2501.17433",
            "abstract": "Recent research shows that Large Language Models (LLMs) are vulnerable to harmful fine-tuning attacks -- models lose their safety alignment ability after fine-tuning on a few harmful samples. For risk mitigation, a guardrail is typically used to filter out harmful samples before fine-tuning. By designing a new red-teaming method, we in this paper show that purely relying on the moderation guardrail for data filtration is not reliable. Our proposed attack method, dubbed Virus, easily bypasses the guardrail moderation by slightly modifying the harmful data. Experimental results show that the harmful data optimized by Virus is not detectable by the guardrail with up to 100\\% leakage ratio, and can simultaneously achieve superior attack performance. Finally, the key message we want to convey through this paper is that: it is reckless to consider guardrail moderation as a clutch at straws towards harmful fine-tuning attack, as it cannot solve the inherent safety issue of the pre-trained LLMs. Our code is available at https://github.com/git-disl/Virus",
            "score": 3,
            "issue_id": 1944,
            "pub_date": "2025-01-29",
            "pub_date_card": {
                "ru": "29 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 29",
                "zh": "1æœˆ29æ—¥"
            },
            "hash": "34a1ea7c32567968",
            "authors": [
                "Tiansheng Huang",
                "Sihao Hu",
                "Fatih Ilhan",
                "Selim Furkan Tekin",
                "Ling Liu"
            ],
            "affiliations": [
                "Georgia Institute of Technology, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.17433.jpg",
            "data": {
                "categories": [
                    "#ethics",
                    "#security",
                    "#training",
                    "#data"
                ],
                "emoji": "ğŸ¦ ",
                "ru": {
                    "title": "Ğ’Ğ¸Ñ€ÑƒÑĞ½Ğ°Ñ Ğ°Ñ‚Ğ°ĞºĞ°: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ğ¾Ğ¹ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ¿Ğ¾Ñ‚ĞµÑ€Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¾Ğ±Ğ»ÑĞ´Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ° Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ñ‚Ğ°ĞºĞ¸ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Virus, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ»ĞµĞ³ĞºĞ¾ Ğ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ½Ñ‹Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹, ÑĞ»ĞµĞ³ĞºĞ° Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒÑ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ñ‚Ğ°ĞºĞ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°Ñ‚ÑŒÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… LLM."
                },
                "en": {
                    "title": "Guardrails Can't Save LLMs from Harmful Fine-Tuning Attacks!",
                    "desc": "This paper discusses the vulnerabilities of Large Language Models (LLMs) to harmful fine-tuning attacks, where models can lose their safety features after being trained on malicious data. The authors introduce a new red-teaming method called Virus, which demonstrates that relying solely on guardrails for filtering harmful samples is ineffective. Their experiments reveal that the Virus method can modify harmful data in a way that evades detection by the guardrail, achieving a 100% leakage ratio while maintaining high attack performance. The study emphasizes that guardrail moderation is not a reliable solution for addressing the fundamental safety issues present in pre-trained LLMs."
                },
                "zh": {
                    "title": "é˜²æŠ¤æªæ–½å¹¶éä¸‡æ— ä¸€å¤±ï¼Œéœ€è­¦æƒ•å¾®è°ƒæ”»å‡»",
                    "desc": "æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æœ‰å®³å¾®è°ƒæ”»å‡»ä¸‹éå¸¸è„†å¼±ï¼Œç»è¿‡å°‘é‡æœ‰å®³æ ·æœ¬çš„å¾®è°ƒåï¼Œæ¨¡å‹çš„å®‰å…¨æ€§ä¼šä¸‹é™ã€‚ä¸ºäº†é™ä½é£é™©ï¼Œé€šå¸¸ä¼šä½¿ç”¨é˜²æŠ¤æªæ–½æ¥è¿‡æ»¤æœ‰å®³æ ·æœ¬ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œä»…ä»…ä¾é è¿™ç§é˜²æŠ¤æªæ–½è¿›è¡Œæ•°æ®è¿‡æ»¤å¹¶ä¸å¯é ã€‚æˆ‘ä»¬æå‡ºçš„æ”»å‡»æ–¹æ³•â€œVirusâ€èƒ½å¤Ÿè½»æ¾ç»•è¿‡é˜²æŠ¤æªæ–½ï¼Œé€šè¿‡è½»å¾®ä¿®æ”¹æœ‰å®³æ•°æ®ï¼Œä½¿å…¶åœ¨é«˜è¾¾100%çš„æ³„æ¼ç‡ä¸‹ä»ç„¶æ— æ³•è¢«æ£€æµ‹åˆ°ï¼ŒåŒæ—¶å®ç°äº†ä¼˜è¶Šçš„æ”»å‡»æ€§èƒ½ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-01-29.html",
    "link_next": "2025-01-31.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "29.01",
        "en": "01/29",
        "zh": "1æœˆ29æ—¥"
    },
    "short_date_next": {
        "ru": "31.01",
        "en": "01/31",
        "zh": "1æœˆ31æ—¥"
    },
    "categories": {
        "#dataset": 2,
        "#data": 4,
        "#benchmark": 1,
        "#agents": 1,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 0,
        "#agi": 1,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 3,
        "#security": 2,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« æŒ‘æˆ˜äº†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„èŒƒå¼ï¼Œæå‡ºäº†æ‰¹è¯„å¾®è°ƒï¼ˆCFTï¼‰ç­–ç•¥ã€‚CFTè®©æ¨¡å‹å­¦ä¹ æ‰¹è¯„æœ‰å™ªéŸ³çš„å“åº”ï¼Œè€Œä¸æ˜¯ç®€å•åœ°æ¨¡ä»¿æ­£ç¡®çš„å“åº”ã€‚å—å¼ºè°ƒæ‰¹åˆ¤æ€§æ€ç»´çš„äººç±»å­¦ä¹ è¿‡ç¨‹å¯å‘ï¼ŒCFTé¼“åŠ±æ›´æ·±å…¥çš„åˆ†æå’Œç»†è‡´çš„ç†è§£ã€‚ä½œè€…é€šè¿‡å®éªŒéªŒè¯äº†CFTçš„æœ‰æ•ˆæ€§ï¼Œç»“æœæ˜¾ç¤ºCFTåœ¨å¤šä¸ªæ•°å­¦åŸºå‡†ä¸Šä¼˜äºSFTã€‚",
        "title": "Critique Fine-Tuning: Learning to Critique is More Effective than Learning to Imitate",
        "pinyin": "è¿™ç¯‡æ–‡ç« æŒ‘æˆ˜äº†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„èŒƒå¼ï¼Œæå‡ºäº†æ‰¹è¯„å¾®è°ƒï¼ˆCFTï¼‰ç­–ç•¥ã€‚CFTè®©æ¨¡å‹å­¦ä¹ æ‰¹è¯„æœ‰å™ªéŸ³çš„å“åº”ï¼Œè€Œä¸æ˜¯ç®€å•åœ°æ¨¡ä»¿æ­£ç¡®çš„å“åº”ã€‚å—å¼ºè°ƒæ‰¹åˆ¤æ€§æ€ç»´çš„äººç±»å­¦ä¹ è¿‡ç¨‹å¯å‘ï¼ŒCFTé¼“åŠ±æ›´æ·±å…¥çš„åˆ†æå’Œç»†è‡´çš„ç†è§£ã€‚ä½œè€…é€šè¿‡å®éªŒéªŒè¯äº†CFTçš„æœ‰æ•ˆæ€§ï¼Œç»“æœæ˜¾ç¤ºCFTåœ¨å¤šä¸ªæ•°å­¦åŸºå‡†ä¸Šä¼˜äºSFTã€‚\n\nZhÃ¨ piÄn wÃ©nzhÄng tiÇozhÃ n le jiÃ ndÅ« wÄ“itiÃ¡o (SFT) de fÃ nshÃ¬, tÃ­chÅ« le pÄ«pÃ­ng wÄ“itiÃ¡o (CFT) cÃ¨lÃ¼Ã¨. CFT rÃ ng mÃ³xÃ­ng xuÃ©xÃ­ pÄ«pÃ­ng yÇ’u zÃ oyÄ«n de xiÇngyÃ¬ng, Ã©r bÃ¹shÃ¬ jiÇndÄn de mÃ³fÇng zhÃ¨ngquÃ¨ de xiÇngyÃ¬ng. ShÃ²u qiÃ¡ngdiÃ o pÄ«pÃ n xÃ¬ng sÄ«wÃ©i de rÃ©nlÃ¨i xuÃ©xÃ­ guÃ²chÃ©ng qÇfÄ, CFT gÇ”lÃ¬ gÃ¨ng shÄ“nrÃ¹ de fÄ“nxi hÃ© xÃ¬zhÃ¬ de lÇjiÄ›. ZuÃ²zhÄ› tÅngguÃ² shÃ¬yÃ n yÃ nzhÃ¨ng le CFT de yÇ’uxiÃ oxÃ¬ng, jiÃ©guÇ’ xiÇnshÃ¬ CFT zÃ i duÅgÃ¨ shÃ¹xuÃ© jÄ«zhÇ”n shÃ ng yÅuyÃº SFT.",
        "vocab": "[{'word': 'æŒ‘æˆ˜', 'pinyin': 'tiÇo zhÃ n', 'trans': 'challenge'},\n{'word': 'ç›‘ç£', 'pinyin': 'jiÃ n dÅ«', 'trans': 'supervise'},\n{'word': 'å¾®è°ƒ', 'pinyin': 'wÄ“i tiÃ¡o', 'trans': 'fine-tune'},\n{'word': 'èŒƒå¼', 'pinyin': 'fÃ n shÃ¬', 'trans': 'paradigm'},\n{'word': 'æå‡º', 'pinyin': 'tÃ­ chÅ«', 'trans': 'propose'},\n{'word': 'æ‰¹è¯„', 'pinyin': 'pÄ« pÃ­ng', 'trans': 'criticize'},\n{'word': 'ç­–ç•¥', 'pinyin': 'cÃ¨ lÃ¼Ã¨', 'trans': 'strategy'},\n{'word': 'å™ªéŸ³', 'pinyin': 'zÃ o yÄ«n', 'trans': 'noise'},\n{'word': 'å“åº”', 'pinyin': 'xiÇng yÃ¬ng', 'trans': 'response'},\n{'word': 'æ¨¡ä»¿', 'pinyin': 'mÃ³ fÇng', 'trans': 'imitate'},\n{'word': 'å¯å‘', 'pinyin': 'qÇ fÄ', 'trans': 'inspire'},\n{'word': 'æ‰¹åˆ¤æ€§', 'pinyin': 'pÄ« pÃ n xÃ¬ng', 'trans': 'critical'},\n{'word': 'æ€ç»´', 'pinyin': 'sÄ« wÃ©i', 'trans': 'thinking'},\n{'word': 'é¼“åŠ±', 'pinyin': 'gÇ” lÃ¬', 'trans': 'encourage'},\n{'word': 'æ·±å…¥', 'pinyin': 'shÄ“n rÃ¹', 'trans': 'in-depth'},\n{'word': 'ç»†è‡´', 'pinyin': 'xÃ¬ zhÃ¬', 'trans': 'detailed'},\n{'word': 'éªŒè¯', 'pinyin': 'yÃ n zhÃ¨ng', 'trans': 'verify'},\n{'word': 'æœ‰æ•ˆæ€§', 'pinyin': 'yÇ’u xiÃ o xÃ¬ng', 'trans': 'effectiveness'},\n{'word': 'åŸºå‡†', 'pinyin': 'jÄ« zhÇ”n', 'trans': 'benchmark'},\n{'word': 'ä¼˜äº', 'pinyin': 'yÅu yÃº', 'trans': 'superior to'}]",
        "trans": "This article challenges the paradigm of Supervised Fine-Tuning (SFT) by proposing the Critical Fine-Tuning (CFT) strategy. CFT allows the model to learn to critique noisy responses rather than simply mimicking correct responses. Inspired by the human learning process that emphasizes critical thinking, CFT encourages deeper analysis and detailed understanding. The authors validated the effectiveness of CFT through experiments, with results showing that CFT outperforms SFT on multiple mathematical benchmarks.",
        "update_ts": "2025-01-30 09:10"
    }
}