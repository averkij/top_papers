{
    "date": {
        "ru": "30 января",
        "en": "January 30",
        "zh": "1月30日"
    },
    "time_utc": "2025-01-30 12:18",
    "weekday": 3,
    "issue_id": 1949,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.17703",
            "title": "Critique Fine-Tuning: Learning to Critique is More Effective than Learning to Imitate",
            "url": "https://huggingface.co/papers/2501.17703",
            "abstract": "Supervised Fine-Tuning (SFT) is commonly used to train language models to imitate annotated responses for given instructions. In this paper, we challenge this paradigm and propose Critique Fine-Tuning (CFT), a strategy where models learn to critique noisy responses rather than simply imitate correct ones. Inspired by human learning processes that emphasize critical thinking, CFT encourages deeper analysis and nuanced understanding-traits often overlooked by standard SFT. To validate the effectiveness of CFT, we construct a 50K-sample dataset from WebInstruct, using GPT-4o as the teacher to generate critiques in the form of (input=[query; noisy response], output=critique). CFT on this dataset yields a consistent 4-10% improvement over SFT on six math benchmarks with different base models like Qwen2.5, Qwen2.5-Math and DeepSeek-Math. We further expand to MetaMath and NuminaMath datasets and observe similar gains over SFT. Notably, our Qwen2.5-Math-CFT model-trained on just 50K samples-matches or outperforms competitive models such as AceMath and Qwen2.5-Math-Instruct on most benchmarks, both of which use over 2M samples. Ablation studies show that CFT is robust to the source of noisy response and teacher critique model. Through these findings, we argue that critique-based training offers a more effective alternative to advance the reasoning of language models.",
            "score": 13,
            "issue_id": 1940,
            "pub_date": "2025-01-29",
            "pub_date_card": {
                "ru": "29 января",
                "en": "January 29",
                "zh": "1月29日"
            },
            "hash": "18b6407976346581",
            "authors": [
                "Yubo Wang",
                "Xiang Yue",
                "Wenhu Chen"
            ],
            "affiliations": [
                "Carnegie Mellon University, Pittsburgh",
                "Department of Computer Science, University of Waterloo",
                "Vector Institute, Toronto"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.17703.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#optimization",
                    "#dataset",
                    "#math"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Критика вместо имитации: новый подход к обучению языковых моделей",
                    "desc": "В статье предлагается новый подход к обучению языковых моделей - Critique Fine-Tuning (CFT), который учит модели критиковать неточные ответы, а не просто имитировать правильные. CFT показывает улучшение на 4-10% по сравнению с традиционным Supervised Fine-Tuning (SFT) на шести математических тестах с различными базовыми моделями. Модель Qwen2.5-Math-CFT, обученная всего на 50 тысячах примеров, показывает результаты на уровне или лучше конкурентных моделей, использующих более 2 миллионов примеров. Авторы утверждают, что обучение на основе критики более эффективно для развития рассуждений языковых моделей."
                },
                "en": {
                    "title": "Critique Fine-Tuning: Enhancing Language Model Reasoning through Critical Analysis",
                    "desc": "This paper introduces Critique Fine-Tuning (CFT), a novel approach to training language models that focuses on teaching them to critique noisy responses instead of merely imitating correct ones. By mimicking human critical thinking, CFT fosters a deeper understanding and analysis of language, which is often neglected in traditional Supervised Fine-Tuning (SFT). The authors validate CFT's effectiveness using a dataset of 50,000 samples generated by GPT-4o, demonstrating consistent performance improvements of 4-10% over SFT across various math benchmarks. The results suggest that critique-based training can significantly enhance the reasoning capabilities of language models, outperforming models trained on much larger datasets."
                },
                "zh": {
                    "title": "批评微调：提升语言模型推理的新方法",
                    "desc": "本文提出了一种新的训练语言模型的方法，称为批评微调（CFT），与传统的监督微调（SFT）不同，CFT让模型学习如何批评噪声响应，而不是仅仅模仿正确的答案。这种方法受到人类学习过程的启发，强调批判性思维，促进更深层次的分析和细致的理解。通过构建一个包含5万样本的数据集，并使用GPT-4o生成批评，CFT在多个数学基准测试中相较于SFT取得了4-10%的一致性提升。研究结果表明，基于批评的训练方法为提升语言模型的推理能力提供了更有效的替代方案。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.14334",
            "title": "Exploring the sustainable scaling of AI dilemma: A projective study of corporations' AI environmental impacts",
            "url": "https://huggingface.co/papers/2501.14334",
            "abstract": "The rapid growth of artificial intelligence (AI), particularly Large Language Models (LLMs), has raised concerns regarding its global environmental impact that extends beyond greenhouse gas emissions to include consideration of hardware fabrication and end-of-life processes. The opacity from major providers hinders companies' abilities to evaluate their AI-related environmental impacts and achieve net-zero targets.   In this paper, we propose a methodology to estimate the environmental impact of a company's AI portfolio, providing actionable insights without necessitating extensive AI and Life-Cycle Assessment (LCA) expertise. Results confirm that large generative AI models consume up to 4600x more energy than traditional models. Our modelling approach, which accounts for increased AI usage, hardware computing efficiency, and changes in electricity mix in line with IPCC scenarios, forecasts AI electricity use up to 2030. Under a high adoption scenario, driven by widespread Generative AI and agents adoption associated to increasingly complex models and frameworks, AI electricity use is projected to rise by a factor of 24.4.   Mitigating the environmental impact of Generative AI by 2030 requires coordinated efforts across the AI value chain. Isolated measures in hardware efficiency, model efficiency, or grid improvements alone are insufficient. We advocate for standardized environmental assessment frameworks, greater transparency from the all actors of the value chain and the introduction of a \"Return on Environment\" metric to align AI development with net-zero goals.",
            "score": 11,
            "issue_id": 1945,
            "pub_date": "2025-01-24",
            "pub_date_card": {
                "ru": "24 января",
                "en": "January 24",
                "zh": "1月24日"
            },
            "hash": "af6e1a0fd9d77530",
            "authors": [
                "Clément Desroches",
                "Martin Chauvin",
                "Louis Ladan",
                "Caroline Vateau",
                "Simon Gosset",
                "Philippe Cordier"
            ],
            "affiliations": [
                "Capgemini Invent 145 quai du Président Roosevelt, 92130 Issy Les Moulineaux, France"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.14334.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#data",
                    "#benchmark",
                    "#ethics"
                ],
                "emoji": "🌱",
                "ru": {
                    "title": "Зеленый ИИ: путь к устойчивому будущему технологий",
                    "desc": "Статья рассматривает экологическое воздействие искусственного интеллекта, особенно больших языковых моделей (LLM). Авторы предлагают методологию для оценки экологического следа AI-портфеля компании, не требующую глубоких знаний в области ИИ и анализа жизненного цикла. Результаты показывают, что генеративные модели ИИ потребляют до 4600 раз больше энергии, чем традиционные, а к 2030 году при высоком уровне внедрения использование электроэнергии ИИ может вырасти в 24,4 раза. Для смягчения экологического воздействия генеративного ИИ авторы призывают к стандартизации оценки, большей прозрачности и введению метрики 'возврата на окружающую среду'."
                },
                "en": {
                    "title": "Assessing AI's Environmental Footprint for a Sustainable Future",
                    "desc": "This paper addresses the environmental impact of artificial intelligence, especially focusing on Large Language Models (LLMs). It highlights that these models can consume significantly more energy than traditional models, with estimates showing up to 4600 times higher energy use. The authors propose a methodology for companies to assess their AI portfolio's environmental impact, making it easier to achieve net-zero targets without needing deep expertise in AI or Life-Cycle Assessment. They emphasize the need for coordinated efforts across the AI value chain and advocate for standardized frameworks and transparency to mitigate the environmental effects of Generative AI by 2030."
                },
                "zh": {
                    "title": "推动AI可持续发展，保护环境未来",
                    "desc": "这篇论文探讨了人工智能，特别是大型语言模型（LLMs）对环境的影响，包括硬件制造和生命周期结束的过程。研究表明，大型生成性AI模型的能耗是传统模型的4600倍。为了评估公司的AI投资组合的环境影响，论文提出了一种方法论，能够在不需要深入的AI和生命周期评估（LCA）专业知识的情况下提供可行的见解。为了在2030年前减轻生成性AI的环境影响，需要在AI价值链的各个环节进行协调努力，而不仅仅依靠硬件效率或模型效率的单一措施。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.17195",
            "title": "Atla Selene Mini: A General Purpose Evaluation Model",
            "url": "https://huggingface.co/papers/2501.17195",
            "abstract": "We introduce Atla Selene Mini, a state-of-the-art small language model-as-a-judge (SLMJ). Selene Mini is a general-purpose evaluator that outperforms the best SLMJs and GPT-4o-mini on overall performance across 11 out-of-distribution benchmarks, spanning absolute scoring, classification, and pairwise preference tasks. It is the highest-scoring 8B generative model on RewardBench, surpassing strong baselines like GPT-4o and specialized judges. To achieve this, we develop a principled data curation strategy that augments public datasets with synthetically generated critiques and ensures high quality through filtering and dataset ablations. We train our model on a combined direct preference optimization (DPO) and supervised fine-tuning (SFT) loss, and produce a highly promptable evaluator that excels in real-world scenarios. Selene Mini shows dramatically improved zero-shot agreement with human expert evaluations on financial and medical industry datasets. It is also robust to variations in prompt format. Preliminary results indicate that Selene Mini is the top-ranking evaluator in a live, community-driven Judge Arena. We release the model weights on HuggingFace (https://hf.co/AtlaAI/Selene-1-Mini-Llama-3.1-8B) and Ollama to encourage widespread community adoption.",
            "score": 9,
            "issue_id": 1940,
            "pub_date": "2025-01-27",
            "pub_date_card": {
                "ru": "27 января",
                "en": "January 27",
                "zh": "1月27日"
            },
            "hash": "fb718aaf1278709b",
            "authors": [
                "Andrei Alexandru",
                "Antonia Calvi",
                "Henry Broomfield",
                "Jackson Golden",
                "Kyle Dai",
                "Mathias Leys",
                "Maurice Burger",
                "Max Bartolo",
                "Roman Engeler",
                "Sashank Pisupati",
                "Toby Drane",
                "Young Sun Park"
            ],
            "affiliations": [
                "Cohere",
                "University College London",
                "atla"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.17195.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#open_source",
                    "#agi",
                    "#synthetic",
                    "#data",
                    "#training",
                    "#optimization",
                    "#dataset",
                    "#rlhf"
                ],
                "emoji": "⚖️",
                "ru": {
                    "title": "Atla Selene Mini: Малая модель-судья с большими возможностями",
                    "desc": "Статья представляет Atla Selene Mini - передовую малую языковую модель-судью (SLMJ). Модель превосходит существующие SLMJ и GPT-4o-mini по общей производительности в 11 различных тестовых наборах. Selene Mini обучена с использованием стратегии курирования данных, сочетающей публичные наборы данных с синтетически сгенерированными критическими оценками. Модель демонстрирует улучшенное согласие с оценками экспертов-людей в финансовых и медицинских задачах без предварительной настройки."
                },
                "en": {
                    "title": "Selene Mini: The Next Level in Language Model Evaluation!",
                    "desc": "Atla Selene Mini is a cutting-edge small language model designed to evaluate various tasks effectively. It surpasses existing models, including GPT-4o-mini, by achieving superior performance on 11 benchmarks related to scoring, classification, and preference tasks. The model benefits from a unique data curation strategy that enhances public datasets with synthetic critiques, ensuring high-quality training data. With a combination of direct preference optimization and supervised fine-tuning, Selene Mini demonstrates strong alignment with human evaluations, particularly in specialized fields like finance and medicine."
                },
                "zh": {
                    "title": "Atla Selene Mini：超越传统评估的语言模型",
                    "desc": "我们介绍了Atla Selene Mini，这是一种先进的小型语言模型评估器（SLMJ）。Selene Mini在11个不同的基准测试中表现优于其他模型，包括GPT-4o-mini，尤其在绝对评分、分类和成对偏好任务上。通过精心的数据策划策略，我们增强了公共数据集，并确保数据质量，从而训练出一个在真实场景中表现出色的评估器。Selene Mini在金融和医疗行业数据集上与人类专家评估的零-shot一致性显著提高，且对提示格式的变化具有良好的鲁棒性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.17749",
            "title": "Early External Safety Testing of OpenAI's o3-mini: Insights from the Pre-Deployment Evaluation",
            "url": "https://huggingface.co/papers/2501.17749",
            "abstract": "Large Language Models (LLMs) have become an integral part of our daily lives. However, they impose certain risks, including those that can harm individuals' privacy, perpetuate biases and spread misinformation. These risks highlight the need for robust safety mechanisms, ethical guidelines, and thorough testing to ensure their responsible deployment. Safety of LLMs is a key property that needs to be thoroughly tested prior the model to be deployed and accessible to the general users. This paper reports the external safety testing experience conducted by researchers from Mondragon University and University of Seville on OpenAI's new o3-mini LLM as part of OpenAI's early access for safety testing program. In particular, we apply our tool, ASTRAL, to automatically and systematically generate up to date unsafe test inputs (i.e., prompts) that helps us test and assess different safety categories of LLMs. We automatically generate and execute a total of 10,080 unsafe test input on a early o3-mini beta version. After manually verifying the test cases classified as unsafe by ASTRAL, we identify a total of 87 actual instances of unsafe LLM behavior. We highlight key insights and findings uncovered during the pre-deployment external testing phase of OpenAI's latest LLM.",
            "score": 7,
            "issue_id": 1940,
            "pub_date": "2025-01-29",
            "pub_date_card": {
                "ru": "29 января",
                "en": "January 29",
                "zh": "1月29日"
            },
            "hash": "325df13c4995c5e9",
            "authors": [
                "Aitor Arrieta",
                "Miriam Ugarte",
                "Pablo Valle",
                "José Antonio Parejo",
                "Sergio Segura"
            ],
            "affiliations": [
                "Mondragon University, Mondragon, Spain",
                "University of Seville, Seville, Spain"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.17749.jpg",
            "data": {
                "categories": [
                    "#ethics",
                    "#data",
                    "#inference",
                    "#training",
                    "#security"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "Автоматизированное тестирование безопасности языковых моделей: ключ к ответственному ИИ",
                    "desc": "Эта статья описывает опыт внешнего тестирования безопасности новой языковой модели OpenAI o3-mini. Исследователи использовали инструмент ASTRAL для автоматической генерации небезопасных промптов, чтобы оценить различные аспекты безопасности модели. Было сгенерировано и выполнено 10 080 тестовых входных данных, из которых 87 случаев были идентифицированы как фактически небезопасное поведение модели. Статья подчеркивает важность тщательного тестирования безопасности больших языковых моделей перед их развертыванием."
                },
                "en": {
                    "title": "Ensuring Safety in Large Language Models: A Testing Approach",
                    "desc": "This paper discusses the safety testing of Large Language Models (LLMs), focusing on the o3-mini model from OpenAI. Researchers from Mondragon University and University of Seville used a tool called ASTRAL to automatically create unsafe test inputs to evaluate the model's safety. They generated and executed 10,080 test prompts, identifying 87 instances of unsafe behavior after manual verification. The findings emphasize the importance of rigorous safety assessments before deploying LLMs to mitigate risks like privacy violations and misinformation."
                },
                "zh": {
                    "title": "确保大型语言模型的安全性与责任使用",
                    "desc": "大型语言模型（LLMs）在我们的日常生活中变得不可或缺，但它们也带来了隐私风险、偏见和错误信息传播等问题。这些风险表明需要建立强有力的安全机制和伦理指导，以确保模型的负责任使用。本文报告了蒙德拉贡大学和塞维利亚大学的研究人员对OpenAI的新o3-mini LLM进行的外部安全测试经验。我们使用ASTRAL工具自动生成不安全的测试输入，以评估LLMs的不同安全类别，并发现了87个实际的不安全行为实例。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.17433",
            "title": "Virus: Harmful Fine-tuning Attack for Large Language Models Bypassing Guardrail Moderation",
            "url": "https://huggingface.co/papers/2501.17433",
            "abstract": "Recent research shows that Large Language Models (LLMs) are vulnerable to harmful fine-tuning attacks -- models lose their safety alignment ability after fine-tuning on a few harmful samples. For risk mitigation, a guardrail is typically used to filter out harmful samples before fine-tuning. By designing a new red-teaming method, we in this paper show that purely relying on the moderation guardrail for data filtration is not reliable. Our proposed attack method, dubbed Virus, easily bypasses the guardrail moderation by slightly modifying the harmful data. Experimental results show that the harmful data optimized by Virus is not detectable by the guardrail with up to 100\\% leakage ratio, and can simultaneously achieve superior attack performance. Finally, the key message we want to convey through this paper is that: it is reckless to consider guardrail moderation as a clutch at straws towards harmful fine-tuning attack, as it cannot solve the inherent safety issue of the pre-trained LLMs. Our code is available at https://github.com/git-disl/Virus",
            "score": 3,
            "issue_id": 1944,
            "pub_date": "2025-01-29",
            "pub_date_card": {
                "ru": "29 января",
                "en": "January 29",
                "zh": "1月29日"
            },
            "hash": "34a1ea7c32567968",
            "authors": [
                "Tiansheng Huang",
                "Sihao Hu",
                "Fatih Ilhan",
                "Selim Furkan Tekin",
                "Ling Liu"
            ],
            "affiliations": [
                "Georgia Institute of Technology, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.17433.jpg",
            "data": {
                "categories": [
                    "#ethics",
                    "#security",
                    "#training",
                    "#data"
                ],
                "emoji": "🦠",
                "ru": {
                    "title": "Вирусная атака: новый вызов безопасности языковых моделей",
                    "desc": "Исследование показывает уязвимость больших языковых моделей (LLM) к вредоносной дообучению, что приводит к потере способности соблюдать правила безопасности. Авторы разработали новый метод атаки под названием Virus, который легко обходит защитные механизмы, слегка модифицируя вредоносные данные. Эксперименты показали, что оптимизированные вредоносные данные не обнаруживаются защитными механизмами и при этом достигают высокой эффективности атаки. Исследователи подчеркивают, что полагаться только на фильтрацию данных недостаточно для решения проблем безопасности предобученных LLM."
                },
                "en": {
                    "title": "Guardrails Can't Save LLMs from Harmful Fine-Tuning Attacks!",
                    "desc": "This paper discusses the vulnerabilities of Large Language Models (LLMs) to harmful fine-tuning attacks, where models can lose their safety features after being trained on malicious data. The authors introduce a new red-teaming method called Virus, which demonstrates that relying solely on guardrails for filtering harmful samples is ineffective. Their experiments reveal that the Virus method can modify harmful data in a way that evades detection by the guardrail, achieving a 100% leakage ratio while maintaining high attack performance. The study emphasizes that guardrail moderation is not a reliable solution for addressing the fundamental safety issues present in pre-trained LLMs."
                },
                "zh": {
                    "title": "防护措施并非万无一失，需警惕微调攻击",
                    "desc": "最近的研究表明，大型语言模型（LLMs）在有害微调攻击下非常脆弱，经过少量有害样本的微调后，模型的安全性会下降。为了降低风险，通常会使用防护措施来过滤有害样本。然而，我们的研究表明，仅仅依靠这种防护措施进行数据过滤并不可靠。我们提出的攻击方法“Virus”能够轻松绕过防护措施，通过轻微修改有害数据，使其在高达100%的泄漏率下仍然无法被检测到，同时实现了优越的攻击性能。"
                }
            }
        }
    ],
    "link_prev": "2025-01-29.html",
    "link_next": "2025-01-31.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "29.01",
        "en": "01/29",
        "zh": "1月29日"
    },
    "short_date_next": {
        "ru": "31.01",
        "en": "01/31",
        "zh": "1月31日"
    },
    "categories": {
        "#dataset": 2,
        "#data": 4,
        "#benchmark": 1,
        "#agents": 1,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 0,
        "#agi": 1,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 3,
        "#security": 2,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章挑战了监督微调（SFT）的范式，提出了批评微调（CFT）策略。CFT让模型学习批评有噪音的响应，而不是简单地模仿正确的响应。受强调批判性思维的人类学习过程启发，CFT鼓励更深入的分析和细致的理解。作者通过实验验证了CFT的有效性，结果显示CFT在多个数学基准上优于SFT。",
        "title": "Critique Fine-Tuning: Learning to Critique is More Effective than Learning to Imitate",
        "pinyin": "这篇文章挑战了监督微调（SFT）的范式，提出了批评微调（CFT）策略。CFT让模型学习批评有噪音的响应，而不是简单地模仿正确的响应。受强调批判性思维的人类学习过程启发，CFT鼓励更深入的分析和细致的理解。作者通过实验验证了CFT的有效性，结果显示CFT在多个数学基准上优于SFT。\n\nZhè piān wénzhāng tiǎozhàn le jiàndū wēitiáo (SFT) de fànshì, tíchū le pīpíng wēitiáo (CFT) cèlüè. CFT ràng móxíng xuéxí pīpíng yǒu zàoyīn de xiǎngyìng, ér bùshì jiǎndān de mófǎng zhèngquè de xiǎngyìng. Shòu qiángdiào pīpàn xìng sīwéi de rénlèi xuéxí guòchéng qǐfā, CFT gǔlì gèng shēnrù de fēnxi hé xìzhì de lǐjiě. Zuòzhě tōngguò shìyàn yànzhèng le CFT de yǒuxiàoxìng, jiéguǒ xiǎnshì CFT zài duōgè shùxué jīzhǔn shàng yōuyú SFT.",
        "vocab": "[{'word': '挑战', 'pinyin': 'tiǎo zhàn', 'trans': 'challenge'},\n{'word': '监督', 'pinyin': 'jiàn dū', 'trans': 'supervise'},\n{'word': '微调', 'pinyin': 'wēi tiáo', 'trans': 'fine-tune'},\n{'word': '范式', 'pinyin': 'fàn shì', 'trans': 'paradigm'},\n{'word': '提出', 'pinyin': 'tí chū', 'trans': 'propose'},\n{'word': '批评', 'pinyin': 'pī píng', 'trans': 'criticize'},\n{'word': '策略', 'pinyin': 'cè lüè', 'trans': 'strategy'},\n{'word': '噪音', 'pinyin': 'zào yīn', 'trans': 'noise'},\n{'word': '响应', 'pinyin': 'xiǎng yìng', 'trans': 'response'},\n{'word': '模仿', 'pinyin': 'mó fǎng', 'trans': 'imitate'},\n{'word': '启发', 'pinyin': 'qǐ fā', 'trans': 'inspire'},\n{'word': '批判性', 'pinyin': 'pī pàn xìng', 'trans': 'critical'},\n{'word': '思维', 'pinyin': 'sī wéi', 'trans': 'thinking'},\n{'word': '鼓励', 'pinyin': 'gǔ lì', 'trans': 'encourage'},\n{'word': '深入', 'pinyin': 'shēn rù', 'trans': 'in-depth'},\n{'word': '细致', 'pinyin': 'xì zhì', 'trans': 'detailed'},\n{'word': '验证', 'pinyin': 'yàn zhèng', 'trans': 'verify'},\n{'word': '有效性', 'pinyin': 'yǒu xiào xìng', 'trans': 'effectiveness'},\n{'word': '基准', 'pinyin': 'jī zhǔn', 'trans': 'benchmark'},\n{'word': '优于', 'pinyin': 'yōu yú', 'trans': 'superior to'}]",
        "trans": "This article challenges the paradigm of Supervised Fine-Tuning (SFT) by proposing the Critical Fine-Tuning (CFT) strategy. CFT allows the model to learn to critique noisy responses rather than simply mimicking correct responses. Inspired by the human learning process that emphasizes critical thinking, CFT encourages deeper analysis and detailed understanding. The authors validated the effectiveness of CFT through experiments, with results showing that CFT outperforms SFT on multiple mathematical benchmarks.",
        "update_ts": "2025-01-30 09:10"
    }
}