{
    "date": {
        "ru": "7 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
        "en": "February 7",
        "zh": "2æœˆ7æ—¥"
    },
    "time_utc": "2025-02-07 02:11",
    "weekday": 4,
    "issue_id": 2085,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2502.02737",
            "title": "SmolLM2: When Smol Goes Big -- Data-Centric Training of a Small Language Model",
            "url": "https://huggingface.co/papers/2502.02737",
            "abstract": "While large language models have facilitated breakthroughs in many applications of artificial intelligence, their inherent largeness makes them computationally expensive and challenging to deploy in resource-constrained settings. In this paper, we document the development of SmolLM2, a state-of-the-art \"small\" (1.7 billion parameter) language model (LM). To attain strong performance, we overtrain SmolLM2 on ~11 trillion tokens of data using a multi-stage training process that mixes web text with specialized math, code, and instruction-following data. We additionally introduce new specialized datasets (FineMath, Stack-Edu, and SmolTalk) at stages where we found existing datasets to be problematically small or low-quality. To inform our design decisions, we perform both small-scale ablations as well as a manual refinement process that updates the dataset mixing rates at each stage based on the performance at the previous stage. Ultimately, we demonstrate that SmolLM2 outperforms other recent small LMs including Qwen2.5-1.5B and Llama3.2-1B. To facilitate future research on LM development as well as applications of small LMs, we release both SmolLM2 as well as all of the datasets we prepared in the course of this project.",
            "score": 87,
            "issue_id": 2066,
            "pub_date": "2025-02-04",
            "pub_date_card": {
                "ru": "4 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 4",
                "zh": "2æœˆ4æ—¥"
            },
            "hash": "c78fe4c39300443d",
            "authors": [
                "Loubna Ben Allal",
                "Anton Lozhkov",
                "Elie Bakouch",
                "Gabriel MartÃ­n BlÃ¡zquez",
                "Guilherme Penedo",
                "Lewis Tunstall",
                "AndrÃ©s Marafioti",
                "Hynek KydlÃ­Äek",
                "AgustÃ­n Piqueres LajarÃ­n",
                "Vaibhav Srivastav",
                "Joshua Lochner",
                "Caleb Fahlgren",
                "Xuan-Son Nguyen",
                "ClÃ©mentine Fourrier",
                "Ben Burtenshaw",
                "Hugo Larcher",
                "Haojun Zhao",
                "Cyril Zakka",
                "Mathieu Morlon",
                "Colin Raffel",
                "Leandro von Werra",
                "Thomas Wolf"
            ],
            "affiliations": [
                "HuggingFaceTB"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.02737.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#dataset",
                    "#low_resource",
                    "#training",
                    "#small_models"
                ],
                "emoji": "ğŸ¤",
                "ru": {
                    "title": "Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼Ğ°Ğ»ĞµĞ½ÑŒĞºĞ¾Ğ¼ Ğ¿Ğ°ĞºĞµÑ‚Ğµ: SmolLM2 - ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ SmolLM2 - ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ 'Ğ¼Ğ°Ğ»ĞµĞ½ÑŒĞºĞ¾Ğ¹' ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ 1,7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°Ğ»Ğ°ÑÑŒ Ğ½Ğ° ~11 Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ğ²ĞµĞ±-Ñ‚ĞµĞºÑÑ‚Ñ‹ ÑĞ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞµ, ĞºĞ¾Ğ´Ñƒ Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğµ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ SmolLM2 Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞ»Ğ° Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ°Ğ»Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Qwen2.5-1.5B Ğ¸ Llama3.2-1B."
                },
                "en": {
                    "title": "SmolLM2: Efficient Language Modeling for Resource-Constrained Environments",
                    "desc": "This paper presents SmolLM2, a compact language model with 1.7 billion parameters designed to be efficient for deployment in resource-limited environments. The model is trained on an extensive dataset of approximately 11 trillion tokens, utilizing a multi-stage training approach that incorporates diverse data sources, including web text and specialized datasets for math and coding. The authors introduce new datasets to enhance training quality and perform systematic evaluations to optimize dataset mixing based on performance feedback. SmolLM2 demonstrates superior performance compared to other recent small language models, and the authors provide access to the model and datasets for further research."
                },
                "zh": {
                    "title": "å°å‹è¯­è¨€æ¨¡å‹çš„å¼ºå¤§çªç ´",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†SmolLM2çš„å¼€å‘ï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰17äº¿å‚æ•°çš„å°å‹è¯­è¨€æ¨¡å‹ã€‚ä¸ºäº†å®ç°å¼ºå¤§çš„æ€§èƒ½ï¼Œæˆ‘ä»¬åœ¨çº¦11ä¸‡äº¿ä¸ªæ•°æ®ä¸Šè¿›è¡Œäº†è¿‡åº¦è®­ç»ƒï¼Œé‡‡ç”¨äº†å¤šé˜¶æ®µè®­ç»ƒè¿‡ç¨‹ï¼Œç»“åˆäº†ç½‘ç»œæ–‡æœ¬ã€æ•°å­¦ã€ä»£ç å’ŒæŒ‡ä»¤è·Ÿéšæ•°æ®ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†æ–°çš„ä¸“ç”¨æ•°æ®é›†ï¼Œä»¥è§£å†³ç°æœ‰æ•°æ®é›†è§„æ¨¡å°æˆ–è´¨é‡ä½çš„é—®é¢˜ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬è¯æ˜SmolLM2åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†å…¶ä»–è¿‘æœŸçš„å°å‹è¯­è¨€æ¨¡å‹ï¼Œå¦‚Qwen2.5-1.5Bå’ŒLlama3.2-1Bã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.01506",
            "title": "TwinMarket: A Scalable Behavioral and Social Simulation for Financial Markets",
            "url": "https://huggingface.co/papers/2502.01506",
            "abstract": "The study of social emergence has long been a central focus in social science. Traditional modeling approaches, such as rule-based Agent-Based Models (ABMs), struggle to capture the diversity and complexity of human behavior, particularly the irrational factors emphasized in behavioral economics. Recently, large language model (LLM) agents have gained traction as simulation tools for modeling human behavior in social science and role-playing applications. Studies suggest that LLMs can account for cognitive biases, emotional fluctuations, and other non-rational influences, enabling more realistic simulations of socio-economic dynamics. In this work, we introduce TwinMarket, a novel multi-agent framework that leverages LLMs to simulate socio-economic systems. Specifically, we examine how individual behaviors, through interactions and feedback mechanisms, give rise to collective dynamics and emergent phenomena. Through experiments in a simulated stock market environment, we demonstrate how individual actions can trigger group behaviors, leading to emergent outcomes such as financial bubbles and recessions. Our approach provides valuable insights into the complex interplay between individual decision-making and collective socio-economic patterns.",
            "score": 26,
            "issue_id": 2063,
            "pub_date": "2025-02-03",
            "pub_date_card": {
                "ru": "3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 3",
                "zh": "2æœˆ3æ—¥"
            },
            "hash": "f5ec0450054af574",
            "authors": [
                "Yuzhe Yang",
                "Yifei Zhang",
                "Minghao Wu",
                "Kaidi Zhang",
                "Yunmiao Zhang",
                "Honghai Yu",
                "Yan Hu",
                "Benyou Wang"
            ],
            "affiliations": [
                "Nanjing University",
                "The Chinese University of Hong Kong, Shenzhen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.01506.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#agents"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ğ°Ğ¹Ğ½Ñ‹ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾-ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº TwinMarket Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾-ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ñ‹. Ğ’ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ñ… Ğ½Ğ° ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ñ„Ğ¾Ğ½Ğ´Ğ¾Ğ²Ğ¾Ğ¼ Ñ€Ñ‹Ğ½ĞºĞµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ, ĞºĞ°Ğº Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´ÑÑ‚ Ğº Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¼ĞµÑ€Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼ ÑĞ²Ğ»ĞµĞ½Ğ¸ÑĞ¼. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¿Ğ¾Ğ½ÑÑ‚ÑŒ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾ÑĞ²ÑĞ·ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸ĞµĞ¼ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ ĞºĞ¾Ğ»Ğ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾-ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Harnessing LLMs for Realistic Socio-Economic Simulations",
                    "desc": "This paper presents TwinMarket, a new framework that uses large language models (LLMs) to simulate socio-economic systems. Unlike traditional Agent-Based Models, TwinMarket captures the complexity of human behavior, including cognitive biases and emotional influences. The framework allows for the exploration of how individual actions can lead to collective dynamics, such as financial bubbles and recessions, in a simulated stock market. By leveraging LLMs, the study provides deeper insights into the interactions between individual decision-making and broader socio-economic patterns."
                },
                "zh": {
                    "title": "åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ¨¡æ‹Ÿç¤¾ä¼šç»æµç³»ç»Ÿçš„æ¶Œç°ç°è±¡",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†ç¤¾ä¼šæ¶Œç°ç°è±¡ï¼Œä¼ ç»Ÿçš„åŸºäºè§„åˆ™çš„ä»£ç†æ¨¡å‹ï¼ˆABMï¼‰éš¾ä»¥æ•æ‰äººç±»è¡Œä¸ºçš„å¤šæ ·æ€§å’Œå¤æ‚æ€§ï¼Œå°¤å…¶æ˜¯è¡Œä¸ºç»æµå­¦ä¸­å¼ºè°ƒçš„éç†æ€§å› ç´ ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å¤šä»£ç†æ¡†æ¶TwinMarketï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥æ¨¡æ‹Ÿç¤¾ä¼šç»æµç³»ç»Ÿã€‚é€šè¿‡æ¨¡æ‹Ÿè‚¡ç¥¨å¸‚åœºç¯å¢ƒçš„å®éªŒï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä¸ªä½“è¡Œä¸ºå¦‚ä½•é€šè¿‡äº’åŠ¨å’Œåé¦ˆæœºåˆ¶å¼•å‘é›†ä½“åŠ¨æ€ï¼Œå¯¼è‡´é‡‘èæ³¡æ²«å’Œç»æµè¡°é€€ç­‰æ¶Œç°ç°è±¡ã€‚è¯¥æ–¹æ³•ä¸ºä¸ªä½“å†³ç­–ä¸é›†ä½“ç¤¾ä¼šç»æµæ¨¡å¼ä¹‹é—´çš„å¤æ‚å…³ç³»æä¾›äº†å®è´µçš„è§è§£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.03373",
            "title": "Demystifying Long Chain-of-Thought Reasoning in LLMs",
            "url": "https://huggingface.co/papers/2502.03373",
            "abstract": "Scaling inference compute enhances reasoning in large language models (LLMs), with long chains-of-thought (CoTs) enabling strategies like backtracking and error correction. Reinforcement learning (RL) has emerged as a crucial method for developing these capabilities, yet the conditions under which long CoTs emerge remain unclear, and RL training requires careful design choices. In this study, we systematically investigate the mechanics of long CoT reasoning, identifying the key factors that enable models to generate long CoT trajectories. Through extensive supervised fine-tuning (SFT) and RL experiments, we present four main findings: (1) While SFT is not strictly necessary, it simplifies training and improves efficiency; (2) Reasoning capabilities tend to emerge with increased training compute, but their development is not guaranteed, making reward shaping crucial for stabilizing CoT length growth; (3) Scaling verifiable reward signals is critical for RL. We find that leveraging noisy, web-extracted solutions with filtering mechanisms shows strong potential, particularly for out-of-distribution (OOD) tasks such as STEM reasoning; and (4) Core abilities like error correction are inherently present in base models, but incentivizing these skills effectively for complex tasks via RL demands significant compute, and measuring their emergence requires a nuanced approach. These insights provide practical guidance for optimizing training strategies to enhance long CoT reasoning in LLMs. Our code is available at: https://github.com/eddycmu/demystify-long-cot.",
            "score": 25,
            "issue_id": 2064,
            "pub_date": "2025-02-05",
            "pub_date_card": {
                "ru": "5 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 5",
                "zh": "2æœˆ5æ—¥"
            },
            "hash": "a1d00a6c8452131a",
            "authors": [
                "Edward Yeo",
                "Yuxuan Tong",
                "Morry Niu",
                "Graham Neubig",
                "Xiang Yue"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "IN.AI",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.03373.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#rl",
                    "#training",
                    "#long_context"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°Ñ ÑĞµĞºÑ€ĞµÑ‚Ñ‹ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (CoT) Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ñ‹, Ğ²Ğ»Ğ¸ÑÑÑ‰Ğ¸Ğµ Ğ½Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ CoT Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ¸ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¾Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ², Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²ĞµĞ±-Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… CoT Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² LLM."
                },
                "en": {
                    "title": "Unlocking Reasoning Power in Large Language Models",
                    "desc": "This paper explores how to improve reasoning in large language models (LLMs) by enhancing their inference capabilities through long chains-of-thought (CoTs). It highlights the importance of reinforcement learning (RL) in developing these reasoning skills, while also addressing the unclear conditions for the emergence of long CoTs. The study presents four key findings, including the role of supervised fine-tuning (SFT) in simplifying training, the necessity of reward shaping for stabilizing CoT growth, and the significance of scaling reward signals for effective RL. Overall, the research provides valuable insights for optimizing training strategies to boost long CoT reasoning in LLMs."
                },
                "zh": {
                    "title": "ä¼˜åŒ–è®­ç»ƒç­–ç•¥ï¼Œæå‡é•¿æ¨ç†é“¾èƒ½åŠ›",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­é•¿æ¨ç†é“¾ï¼ˆCoTsï¼‰çš„ç”Ÿæˆæœºåˆ¶ï¼Œæ­ç¤ºäº†å½±å“æ¨¡å‹ç”Ÿæˆé•¿æ¨ç†é“¾çš„å…³é”®å› ç´ ã€‚æˆ‘ä»¬å‘ç°ï¼Œè™½ç„¶ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä¸æ˜¯ç»å¯¹å¿…è¦çš„ï¼Œä½†å®ƒå¯ä»¥ç®€åŒ–è®­ç»ƒè¿‡ç¨‹å¹¶æé«˜æ•ˆç‡ã€‚éšç€è®­ç»ƒè®¡ç®—èƒ½åŠ›çš„å¢åŠ ï¼Œæ¨ç†èƒ½åŠ›æœ‰å¯èƒ½å‡ºç°ï¼Œä½†å…¶å‘å±•å¹¶ä¸æ€»æ˜¯ä¿è¯ï¼Œå› æ­¤å¥–åŠ±è®¾è®¡å¯¹äºç¨³å®šæ¨ç†é“¾çš„é•¿åº¦å¢é•¿è‡³å…³é‡è¦ã€‚æœ€åï¼Œæˆ‘ä»¬çš„ç ”ç©¶ä¸ºä¼˜åŒ–è®­ç»ƒç­–ç•¥ä»¥å¢å¼ºLLMsä¸­çš„é•¿æ¨ç†é“¾æä¾›äº†å®ç”¨æŒ‡å¯¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.03387",
            "title": "LIMO: Less is More for Reasoning",
            "url": "https://huggingface.co/papers/2502.03387",
            "abstract": "We present a fundamental discovery that challenges our understanding of how complex reasoning emerges in large language models. While conventional wisdom suggests that sophisticated reasoning tasks demand extensive training data (>100,000 examples), we demonstrate that complex mathematical reasoning abilities can be effectively elicited with surprisingly few examples. Through comprehensive experiments, our proposed model LIMO demonstrates unprecedented performance in mathematical reasoning. With merely 817 curated training samples, LIMO achieves 57.1% accuracy on AIME and 94.8% on MATH, improving from previous SFT-based models' 6.5% and 59.2% respectively, while only using 1% of the training data required by previous approaches. LIMO demonstrates exceptional out-of-distribution generalization, achieving 40.5% absolute improvement across 10 diverse benchmarks, outperforming models trained on 100x more data, challenging the notion that SFT leads to memorization rather than generalization. Based on these results, we propose the Less-Is-More Reasoning Hypothesis (LIMO Hypothesis): In foundation models where domain knowledge has been comprehensively encoded during pre-training, sophisticated reasoning capabilities can emerge through minimal but precisely orchestrated demonstrations of cognitive processes. This hypothesis posits that the elicitation threshold for complex reasoning is determined by two key factors: (1) the completeness of the model's encoded knowledge foundation during pre-training, and (2) the effectiveness of post-training examples as \"cognitive templates\" that show the model how to utilize its knowledge base to solve complex reasoning tasks. To facilitate reproducibility and future research in data-efficient reasoning, we release LIMO as a comprehensive open-source suite at https://github.com/GAIR-NLP/LIMO.",
            "score": 21,
            "issue_id": 2066,
            "pub_date": "2025-02-05",
            "pub_date_card": {
                "ru": "5 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 5",
                "zh": "2æœˆ5æ—¥"
            },
            "hash": "ad1fa98bc3904527",
            "authors": [
                "Yixin Ye",
                "Zhen Huang",
                "Yang Xiao",
                "Ethan Chern",
                "Shijie Xia",
                "Pengfei Liu"
            ],
            "affiliations": [
                "SJTU, SII, GAIR"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.03387.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#dataset",
                    "#reasoning",
                    "#training",
                    "#math"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞœĞµĞ½ÑŒÑˆĞµ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ²Ñ‹Ğ·Ğ²Ğ°Ñ‚ÑŒ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑƒĞ´Ğ¸Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼Ğ°Ğ»Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². Ğ˜Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ LIMO Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰Ğ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ 817 Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞµ, Ñ‡ĞµĞ¼ Ñƒ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ². LIMO Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ° Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ²Ğ½Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ğ´Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ³Ğ¾Ñ€Ğ°Ğ·Ğ´Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞ¼ Ğ¾Ğ±ÑŠĞµĞ¼Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·Ñƒ LIMO, ÑĞ¾Ğ³Ğ»Ğ°ÑĞ½Ğ¾ ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°Ñ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ, Ğ½Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ğ² Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…."
                },
                "en": {
                    "title": "Less Data, More Reasoning: The LIMO Hypothesis",
                    "desc": "This paper reveals a surprising finding about how large language models can perform complex reasoning tasks. It shows that instead of needing a lot of training data, a model called LIMO can achieve high accuracy in mathematical reasoning with only a small number of examples. LIMO outperforms previous models that used much more data, indicating that less can be more when it comes to training for reasoning tasks. The authors introduce the Less-Is-More Reasoning Hypothesis, suggesting that a well-prepared model can effectively learn complex reasoning from minimal, well-structured examples."
                },
                "zh": {
                    "title": "å°‘å³æ˜¯å¤šï¼Œæ¨ç†èƒ½åŠ›çš„æ–°å‘ç°",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€é¡¹é‡è¦å‘ç°ï¼ŒæŒ‘æˆ˜äº†æˆ‘ä»¬å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ä¸­å¤æ‚æ¨ç†èƒ½åŠ›äº§ç”Ÿæœºåˆ¶çš„ç†è§£ã€‚ä¼ ç»Ÿè§‚ç‚¹è®¤ä¸ºï¼Œå¤æ‚æ¨ç†ä»»åŠ¡éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®ï¼Œä½†æˆ‘ä»¬è¯æ˜åªéœ€å°‘é‡ç¤ºä¾‹å³å¯æœ‰æ•ˆå¼•å‘å¤æ‚çš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ¨¡å‹LIMOåœ¨æ•°å­¦æ¨ç†æ–¹é¢è¡¨ç°å‡ºå‰æ‰€æœªæœ‰çš„æ€§èƒ½ï¼Œä½¿ç”¨ä»…817ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œåˆ†åˆ«åœ¨AIMEå’ŒMATHä¸Šè¾¾åˆ°äº†57.1%å’Œ94.8%çš„å‡†ç¡®ç‡ã€‚æˆ‘ä»¬æå‡ºçš„â€œå°‘å³æ˜¯å¤šæ¨ç†å‡è®¾â€è¡¨æ˜ï¼Œåœ¨åŸºç¡€æ¨¡å‹ä¸­ï¼Œç»è¿‡å……åˆ†ç¼–ç çš„é¢†åŸŸçŸ¥è¯†å¯ä»¥é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„å°‘é‡ç¤ºä¾‹æ¥æ¿€å‘å¤æ‚æ¨ç†èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.02339",
            "title": "Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking",
            "url": "https://huggingface.co/papers/2502.02339",
            "abstract": "Multimodal large language models (MLLMs) exhibit impressive capabilities but still face challenges in complex visual reasoning. While recent efforts attempt to enhance MLLMs' reasoning by incorporating OpenAI o1-like structured thinking through explicit search structures or teacher-guided distillation, they often struggle to balance performance and efficiency. A critical limitation is their heavy reliance on extensive data and search spaces, resulting in low-efficiency implicit insight extraction and data utilization. To address this, we propose AStar, an Automated Structured thinking paradigm for multimodal reasoning via Monte Carlo Tree Search (MCTS). AStar automatically derives high-level cognitive reasoning patterns from limited data using MCTS-powered hierarchical structures. Building on these explicit patterns, we design a unified reasoning framework that seamlessly integrates models' internal reasoning capabilities and external reasoning guidelines, enabling efficient inference with minimal tree iterations. This novel paradigm strikes a compelling balance between performance and efficiency. Extensive experiments demonstrate AStar's effectiveness, achieving superior accuracy (54.0%) on the MathVerse benchmark with a 7B backbone, surpassing GPT-4o (50.2%) while maintaining substantial data and computational efficiency.",
            "score": 9,
            "issue_id": 2063,
            "pub_date": "2025-02-04",
            "pub_date_card": {
                "ru": "4 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 4",
                "zh": "2æœˆ4æ—¥"
            },
            "hash": "3f3413717efb32f6",
            "authors": [
                "Jinyang Wu",
                "Mingkuan Feng",
                "Shuai Zhang",
                "Ruihan Jin",
                "Feihu Che",
                "Zengqi Wen",
                "Jianhua Tao"
            ],
            "affiliations": [
                "Beijing",
                "Department of Automation, Tsinghua University, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.02339.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#benchmark",
                    "#multimodal",
                    "#architecture",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "AStar: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ AStar, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ¸ÑĞºĞ° ĞœĞ¾Ğ½Ñ‚Ğµ-ĞšĞ°Ñ€Ğ»Ğ¾ Ğ¿Ğ¾ Ğ´ĞµÑ€ĞµĞ²Ñƒ (MCTS). AStar Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¼Ğ¸ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ AStar Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ 54.0% Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ MathVerse, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ GPT-4o Ğ¿Ñ€Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "AStar: Enhancing Multimodal Reasoning with Efficient Structured Thinking",
                    "desc": "This paper introduces AStar, a new approach to improve the reasoning abilities of multimodal large language models (MLLMs) using Monte Carlo Tree Search (MCTS). AStar focuses on deriving high-level cognitive reasoning patterns from limited data, which helps in enhancing the models' performance without requiring extensive data sets. The framework integrates internal reasoning capabilities of the models with external guidelines, allowing for efficient inference with fewer iterations. Experimental results show that AStar outperforms existing models like GPT-4o in accuracy while being more data and computationally efficient."
                },
                "zh": {
                    "title": "AStarï¼šé«˜æ•ˆçš„å¤šæ¨¡æ€æ¨ç†æ–°èŒƒå¼",
                    "desc": "å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤æ‚è§†è§‰æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ä»é¢ä¸´æŒ‘æˆ˜ã€‚å°½ç®¡æœ€è¿‘çš„ç ”ç©¶å°è¯•é€šè¿‡å¼•å…¥ç»“æ„åŒ–æ€ç»´å’Œæ•™å¸ˆæŒ‡å¯¼æ¥å¢å¼ºæ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨æ€§èƒ½å’Œæ•ˆç‡ä¹‹é—´çš„å¹³è¡¡ä»ç„¶å›°éš¾ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºAStarçš„è‡ªåŠ¨åŒ–ç»“æ„åŒ–æ€ç»´èŒƒå¼ï¼Œåˆ©ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ä»æœ‰é™æ•°æ®ä¸­è‡ªåŠ¨æ¨å¯¼é«˜å±‚æ¬¡çš„è®¤çŸ¥æ¨ç†æ¨¡å¼ã€‚AStaré€šè¿‡ç»Ÿä¸€çš„æ¨ç†æ¡†æ¶ï¼Œç»“åˆæ¨¡å‹çš„å†…éƒ¨æ¨ç†èƒ½åŠ›å’Œå¤–éƒ¨æ¨ç†æŒ‡å¯¼ï¼Œå®ç°é«˜æ•ˆæ¨ç†ï¼Œæ˜¾è‘—æé«˜äº†å‡†ç¡®æ€§å’Œæ•°æ®åˆ©ç”¨æ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.01105",
            "title": "LayerTracer: Cognitive-Aligned Layered SVG Synthesis via Diffusion Transformer",
            "url": "https://huggingface.co/papers/2502.01105",
            "abstract": "Generating cognitive-aligned layered SVGs remains challenging due to existing methods' tendencies toward either oversimplified single-layer outputs or optimization-induced shape redundancies. We propose LayerTracer, a diffusion transformer based framework that bridges this gap by learning designers' layered SVG creation processes from a novel dataset of sequential design operations. Our approach operates in two phases: First, a text-conditioned DiT generates multi-phase rasterized construction blueprints that simulate human design workflows. Second, layer-wise vectorization with path deduplication produces clean, editable SVGs. For image vectorization, we introduce a conditional diffusion mechanism that encodes reference images into latent tokens, guiding hierarchical reconstruction while preserving structural integrity. Extensive experiments demonstrate LayerTracer's superior performance against optimization-based and neural baselines in both generation quality and editability, effectively aligning AI-generated vectors with professional design cognition.",
            "score": 7,
            "issue_id": 2067,
            "pub_date": "2025-02-03",
            "pub_date_card": {
                "ru": "3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 3",
                "zh": "2æœˆ3æ—¥"
            },
            "hash": "b4eb829c549c6a2e",
            "authors": [
                "Yiren Song",
                "Danze Chen",
                "Mike Zheng Shou"
            ],
            "affiliations": [
                "Show Lab, National University of Singapore, Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.01105.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#multimodal",
                    "#diffusion",
                    "#cv",
                    "#dataset"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "LayerTracer: Ğ˜Ğ˜-Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½ĞµÑ€ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸",
                    "desc": "LayerTracer - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ñ‹Ñ… SVG Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğµ. ĞĞ½ Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½ĞµÑ€Ğ°, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ñ€Ğ°ÑÑ‚Ñ€Ğ¾Ğ²Ñ‹Ğµ Ñ‡ĞµÑ€Ñ‚ĞµĞ¶Ğ¸, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ·ÑƒÑ Ğ¸Ñ… Ğ¿Ğ¾ ÑĞ»Ğ¾ÑĞ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ´Ğ»Ñ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ LayerTracer Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸."
                },
                "en": {
                    "title": "LayerTracer: Bridging the Gap in Layered SVG Generation",
                    "desc": "This paper introduces LayerTracer, a new framework that improves the generation of layered SVGs by learning from how designers create them. It uses a two-phase process: first, it generates rasterized blueprints that mimic human design steps, and then it converts these into clean, editable SVGs while removing duplicate paths. The framework employs a conditional diffusion mechanism to ensure that the generated images maintain their structure and quality. Experiments show that LayerTracer outperforms existing methods in both the quality of the generated designs and their ease of editing, aligning better with professional design practices."
                },
                "zh": {
                    "title": "LayerTracerï¼šæ™ºèƒ½ç”Ÿæˆå¯ç¼–è¾‘çš„åˆ†å±‚SVGå›¾å½¢",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºLayerTracerçš„æ¡†æ¶ï¼Œæ—¨åœ¨ç”Ÿæˆè®¤çŸ¥å¯¹é½çš„åˆ†å±‚SVGå›¾å½¢ã€‚è¯¥æ–¹æ³•é€šè¿‡å­¦ä¹ è®¾è®¡å¸ˆçš„åˆ†å±‚SVGåˆ›å»ºè¿‡ç¨‹ï¼Œåˆ©ç”¨ä¸€ä¸ªæ–°é¢–çš„é¡ºåºè®¾è®¡æ“ä½œæ•°æ®é›†ã€‚LayerTraceråˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆï¼ŒåŸºäºæ–‡æœ¬çš„æ‰©æ•£å˜æ¢å™¨ç”Ÿæˆå¤šé˜¶æ®µçš„å…‰æ …åŒ–æ„å»ºè“å›¾ï¼›å…¶æ¬¡ï¼Œé€šè¿‡è·¯å¾„å»é‡å®ç°åˆ†å±‚çŸ¢é‡åŒ–ï¼Œç”Ÿæˆå¹²å‡€ä¸”å¯ç¼–è¾‘çš„SVGæ–‡ä»¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLayerTraceråœ¨ç”Ÿæˆè´¨é‡å’Œå¯ç¼–è¾‘æ€§æ–¹é¢ä¼˜äºåŸºäºä¼˜åŒ–å’Œç¥ç»ç½‘ç»œçš„åŸºçº¿æ–¹æ³•ï¼Œæœ‰æ•ˆåœ°å°†AIç”Ÿæˆçš„çŸ¢é‡å›¾ä¸ä¸“ä¸šè®¾è®¡è®¤çŸ¥å¯¹é½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.02671",
            "title": "On Teacher Hacking in Language Model Distillation",
            "url": "https://huggingface.co/papers/2502.02671",
            "abstract": "Post-training of language models (LMs) increasingly relies on the following two stages: (i) knowledge distillation, where the LM is trained to imitate a larger teacher LM, and (ii) reinforcement learning from human feedback (RLHF), where the LM is aligned by optimizing a reward model. In the second RLHF stage, a well-known challenge is reward hacking, where the LM over-optimizes the reward model. Such phenomenon is in line with Goodhart's law and can lead to degraded performance on the true objective. In this paper, we investigate whether a similar phenomenon, that we call teacher hacking, can occur during knowledge distillation. This could arise because the teacher LM is itself an imperfect approximation of the true distribution. To study this, we propose a controlled experimental setup involving: (i) an oracle LM representing the ground-truth distribution, (ii) a teacher LM distilled from the oracle, and (iii) a student LM distilled from the teacher. Our experiments reveal the following insights. When using a fixed offline dataset for distillation, teacher hacking occurs; moreover, we can detect it by observing when the optimization process deviates from polynomial convergence laws. In contrast, employing online data generation techniques effectively mitigates teacher hacking. More precisely, we identify data diversity as the key factor in preventing hacking. Overall, our findings provide a deeper understanding of the benefits and limitations of distillation for building robust and efficient LMs.",
            "score": 6,
            "issue_id": 2072,
            "pub_date": "2025-02-04",
            "pub_date_card": {
                "ru": "4 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 4",
                "zh": "2æœˆ4æ—¥"
            },
            "hash": "defca87e9bf06d0b",
            "authors": [
                "Daniil Tiapkin",
                "Daniele Calandriello",
                "Johan Ferret",
                "Sarah Perrin",
                "Nino Vieillard",
                "Alexandre RamÃ©",
                "Mathieu Blondel"
            ],
            "affiliations": [
                "Ecole 1CMAP, France; Polytechnique",
                "Google DeepMind"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.02671.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#optimization",
                    "#rlhf",
                    "#training",
                    "#data"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ‘Ğ¾Ñ€ÑŒĞ±Ğ° Ñ 'teacher hacking': ĞºĞ»ÑÑ‡ Ğº robust ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ñ„ĞµĞ½Ğ¾Ğ¼ĞµĞ½ 'teacher hacking' Ğ¿Ñ€Ğ¸ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºÑƒ Ñ Ğ¾Ñ€Ğ°ĞºÑƒĞ»Ğ¾Ğ¼, ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ ÑƒÑ‡ĞµĞ½Ğ¸ĞºĞ¾Ğ¼ Ğ´Ğ»Ñ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ 'teacher hacking' Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½-Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°, Ğ½Ğ¾ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ ÑĞ¼ÑĞ³Ñ‡ĞµĞ½ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ 'hacking' Ğ¸ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Preventing Teacher Hacking: The Key Role of Data Diversity in Distillation",
                    "desc": "This paper explores the concept of 'teacher hacking' in the context of knowledge distillation for language models (LMs). Teacher hacking occurs when a student LM overly optimizes based on an imperfect teacher LM, leading to poor performance on the actual task. The authors conducted experiments using an oracle LM as the ground truth, a teacher LM distilled from it, and a student LM distilled from the teacher. They found that using diverse online data can prevent teacher hacking, highlighting the importance of data diversity in the distillation process."
                },
                "zh": {
                    "title": "é˜²æ­¢æ•™å¸ˆé»‘å®¢ï¼Œæå‡è¯­è¨€æ¨¡å‹çš„è’¸é¦æ•ˆæœ",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰åœ¨çŸ¥è¯†è’¸é¦é˜¶æ®µå¯èƒ½å‡ºç°çš„â€œæ•™å¸ˆé»‘å®¢â€ç°è±¡ã€‚æ•™å¸ˆé»‘å®¢æ˜¯æŒ‡å­¦ç”Ÿæ¨¡å‹åœ¨æ¨¡ä»¿æ•™å¸ˆæ¨¡å‹æ—¶ï¼Œè¿‡åº¦ä¼˜åŒ–å¯¼è‡´æ€§èƒ½ä¸‹é™çš„æƒ…å†µã€‚è¿™ç§ç°è±¡ä¸å¤å¾·å“ˆç‰¹æ³•åˆ™ç›¸ç¬¦ï¼Œå¯èƒ½æºäºæ•™å¸ˆæ¨¡å‹å¯¹çœŸå®åˆ†å¸ƒçš„ä¸å®Œç¾è¿‘ä¼¼ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨å›ºå®šçš„ç¦»çº¿æ•°æ®é›†è¿›è¡Œè’¸é¦æ—¶ï¼Œæ•™å¸ˆé»‘å®¢ç°è±¡ä¼šå‘ç”Ÿï¼Œè€Œé‡‡ç”¨åœ¨çº¿æ•°æ®ç”ŸæˆæŠ€æœ¯åˆ™èƒ½æœ‰æ•ˆç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œæ•°æ®å¤šæ ·æ€§æ˜¯é˜²æ­¢æ•™å¸ˆé»‘å®¢çš„å…³é”®å› ç´ ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.02928",
            "title": "Large Language Model Guided Self-Debugging Code Generation",
            "url": "https://huggingface.co/papers/2502.02928",
            "abstract": "Automated code generation is gaining significant importance in intelligent computer programming and system deployment. However, current approaches often face challenges in computational efficiency and lack robust mechanisms for code parsing and error correction. In this work, we propose a novel framework, PyCapsule, with a simple yet effective two-agent pipeline and efficient self-debugging modules for Python code generation. PyCapsule features sophisticated prompt inference, iterative error handling, and case testing, ensuring high generation stability, safety, and correctness. Empirically, PyCapsule achieves up to 5.7% improvement of success rate on HumanEval, 10.3% on HumanEval-ET, and 24.4% on BigCodeBench compared to the state-of-art methods. We also observe a decrease in normalized success rate given more self-debugging attempts, potentially affected by limited and noisy error feedback in retention. PyCapsule demonstrates broader impacts on advancing lightweight and efficient code generation for artificial intelligence systems.",
            "score": 5,
            "issue_id": 2075,
            "pub_date": "2025-02-05",
            "pub_date_card": {
                "ru": "5 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 5",
                "zh": "2æœˆ5æ—¥"
            },
            "hash": "dc7aaadeeee7e1e7",
            "authors": [
                "Muntasir Adnan",
                "Zhiwei Xu",
                "Carlos C. N. Kuhn"
            ],
            "affiliations": [
                "Open Source Institute, Faculty of Science and Technology, University of Canberra, Australia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.02928.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#plp",
                    "#training"
                ],
                "emoji": "ğŸ",
                "ru": {
                    "title": "PyCapsule: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Python-ĞºĞ¾Ğ´Ğ° Ñ ÑĞ°Ğ¼Ğ¾Ğ¾Ñ‚Ğ»Ğ°Ğ´ĞºĞ¾Ğ¹",
                    "desc": "PyCapsule - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ½Ğ° Python, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ´Ğ²ÑƒÑ…Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸ ÑĞ°Ğ¼Ğ¾Ğ¾Ñ‚Ğ»Ğ°Ğ´ĞºĞ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ²Ğ¾Ğ´ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ², Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. PyCapsule Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. ĞĞ´Ğ½Ğ°ĞºĞ¾ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµÑ‚ÑÑ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğº ÑĞ°Ğ¼Ğ¾Ğ¾Ñ‚Ğ»Ğ°Ğ´ĞºĞ¸, Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾, Ğ¸Ğ·-Ğ·Ğ° Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸ Ğ·Ğ°ÑˆÑƒĞ¼Ğ»ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¾Ğ± Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ñ…."
                },
                "en": {
                    "title": "Revolutionizing Python Code Generation with PyCapsule",
                    "desc": "This paper introduces PyCapsule, a new framework designed to enhance automated code generation, particularly for Python. It employs a two-agent pipeline that focuses on efficient self-debugging and robust error handling, addressing common issues in existing methods. The framework utilizes advanced prompt inference and iterative testing to improve the stability and correctness of generated code. Empirical results show that PyCapsule outperforms current state-of-the-art techniques in various benchmarks, highlighting its potential for more efficient AI-driven programming solutions."
                },
                "zh": {
                    "title": "PyCapsuleï¼šé«˜æ•ˆçš„è‡ªåŠ¨åŒ–ä»£ç ç”Ÿæˆæ¡†æ¶",
                    "desc": "è‡ªåŠ¨åŒ–ä»£ç ç”Ÿæˆåœ¨æ™ºèƒ½è®¡ç®—æœºç¼–ç¨‹å’Œç³»ç»Ÿéƒ¨ç½²ä¸­å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•åœ¨è®¡ç®—æ•ˆç‡ä¸Šå¸¸å¸¸é¢ä¸´æŒ‘æˆ˜ï¼Œå¹¶ä¸”ç¼ºä¹å¼ºå¤§çš„ä»£ç è§£æå’Œé”™è¯¯ä¿®æ­£æœºåˆ¶ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶PyCapsuleï¼Œé‡‡ç”¨ç®€å•è€Œæœ‰æ•ˆçš„åŒä»£ç†ç®¡é“å’Œé«˜æ•ˆçš„è‡ªæˆ‘è°ƒè¯•æ¨¡å—æ¥ç”ŸæˆPythonä»£ç ã€‚PyCapsuleé€šè¿‡å¤æ‚çš„æç¤ºæ¨ç†ã€è¿­ä»£é”™è¯¯å¤„ç†å’Œæ¡ˆä¾‹æµ‹è¯•ï¼Œç¡®ä¿äº†é«˜ç”Ÿæˆç¨³å®šæ€§ã€å®‰å…¨æ€§å’Œæ­£ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.01618",
            "title": "A Probabilistic Inference Approach to Inference-Time Scaling of LLMs using Particle-Based Monte Carlo Methods",
            "url": "https://huggingface.co/papers/2502.01618",
            "abstract": "Large language models (LLMs) have achieved significant performance gains via scaling up model sizes and/or data. However, recent evidence suggests diminishing returns from such approaches, motivating scaling the computation spent at inference time. Existing inference-time scaling methods, usually with reward models, cast the task as a search problem, which tends to be vulnerable to reward hacking as a consequence of approximation errors in reward models. In this paper, we instead cast inference-time scaling as a probabilistic inference task and leverage sampling-based techniques to explore the typical set of the state distribution of a state-space model with an approximate likelihood, rather than optimize for its mode directly. We propose a novel inference-time scaling approach by adapting particle-based Monte Carlo methods to this task. Our empirical evaluation demonstrates that our methods have a 4-16x better scaling rate over our deterministic search counterparts on various challenging mathematical reasoning tasks. Using our approach, we show that Qwen2.5-Math-1.5B-Instruct can surpass GPT-4o accuracy in only 4 rollouts, while Qwen2.5-Math-7B-Instruct scales to o1 level accuracy in only 32 rollouts. Our work not only presents an effective method to inference-time scaling, but also connects the rich literature in probabilistic inference with inference-time scaling of LLMs to develop more robust algorithms in future work. Code and further information is available at https://probabilistic-inference-scaling.github.io.",
            "score": 5,
            "issue_id": 2065,
            "pub_date": "2025-02-03",
            "pub_date_card": {
                "ru": "3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 3",
                "zh": "2æœˆ3æ—¥"
            },
            "hash": "c9971916eb027101",
            "authors": [
                "Isha Puri",
                "Shivchander Sudalairaj",
                "Guangxuan Xu",
                "Kai Xu",
                "Akash Srivastava"
            ],
            "affiliations": [
                "MIT CSAIL",
                "Red Hat AI Innovation"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.01618.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#optimization",
                    "#math",
                    "#inference"
                ],
                "emoji": "ğŸ²",
                "ru": {
                    "title": "Ğ’ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° LLM",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ĞºĞ°Ğº Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ²Ğ¾Ğ´, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ĞœĞ¾Ğ½Ñ‚Ğµ-ĞšĞ°Ñ€Ğ»Ğ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ†. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ¼ĞµĞµÑ‚ Ğ² 4-16 Ñ€Ğ°Ğ· Ğ»ÑƒÑ‡ÑˆÑƒÑ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ´ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ°Ğ¼Ğ¸ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, ĞºĞ°Ğº Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¿Ñ€Ğ¾Ğ³Ğ¾Ğ½Ğ¾Ğ²."
                },
                "en": {
                    "title": "Revolutionizing Inference: Probabilistic Scaling for LLMs",
                    "desc": "This paper addresses the limitations of scaling large language models (LLMs) by focusing on improving inference time rather than just increasing model size or data. The authors propose a new approach that treats inference-time scaling as a probabilistic inference task, using sampling techniques to better explore the state distribution. By applying particle-based Monte Carlo methods, their method shows significant improvements in scaling rates compared to traditional deterministic search methods. The results indicate that their approach can achieve higher accuracy with fewer rollouts, demonstrating a promising direction for enhancing LLM performance."
                },
                "zh": {
                    "title": "æ¨ç†æ—¶é—´æ‰©å±•çš„æ–°æ–¹æ³•ï¼šæ¦‚ç‡æ¨ç†ä¸ç²’å­é‡‡æ ·ç»“åˆ",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡å¢åŠ æ¨¡å‹è§„æ¨¡å’Œæ•°æ®é‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•çš„æ”¶ç›Šé€’å‡ï¼Œä¿ƒä½¿æˆ‘ä»¬è€ƒè™‘åœ¨æ¨ç†æ—¶å¢åŠ è®¡ç®—é‡ã€‚ç°æœ‰çš„æ¨ç†æ—¶é—´æ‰©å±•æ–¹æ³•é€šå¸¸å°†ä»»åŠ¡è§†ä¸ºæœç´¢é—®é¢˜ï¼Œå®¹æ˜“å—åˆ°å¥–åŠ±æ¨¡å‹çš„è¿‘ä¼¼è¯¯å·®å½±å“è€Œå¯¼è‡´å¥–åŠ±æ“æ§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¨ç†æ—¶é—´æ‰©å±•æ–¹æ³•ï¼Œé€šè¿‡é€‚åº”åŸºäºç²’å­çš„è’™ç‰¹å¡æ´›æ–¹æ³•ï¼Œå°†æ¨ç†æ—¶é—´æ‰©å±•è§†ä¸ºæ¦‚ç‡æ¨ç†ä»»åŠ¡ï¼Œä»è€Œåœ¨å„ç§æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­å®ç°äº†æ›´å¥½çš„æ‰©å±•ç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.01154",
            "title": "Jailbreaking with Universal Multi-Prompts",
            "url": "https://huggingface.co/papers/2502.01154",
            "abstract": "Large language models (LLMs) have seen rapid development in recent years, revolutionizing various applications and significantly enhancing convenience and productivity. However, alongside their impressive capabilities, ethical concerns and new types of attacks, such as jailbreaking, have emerged. While most prompting techniques focus on optimizing adversarial inputs for individual cases, resulting in higher computational costs when dealing with large datasets. Less research has addressed the more general setting of training a universal attacker that can transfer to unseen tasks. In this paper, we introduce JUMP, a prompt-based method designed to jailbreak LLMs using universal multi-prompts. We also adapt our approach for defense, which we term DUMP. Experimental results demonstrate that our method for optimizing universal multi-prompts outperforms existing techniques.",
            "score": 4,
            "issue_id": 2068,
            "pub_date": "2025-02-03",
            "pub_date_card": {
                "ru": "3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 3",
                "zh": "2æœˆ3æ—¥"
            },
            "hash": "aa9860c81d83ac21",
            "authors": [
                "Yu-Ling Hsu",
                "Hsuan Su",
                "Shang-Tse Chen"
            ],
            "affiliations": [
                "National Taiwan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.01154.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#rl",
                    "#data",
                    "#optimization",
                    "#transfer_learning",
                    "#training",
                    "#ethics"
                ],
                "emoji": "ğŸ”“",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ²Ğ·Ğ»Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ JUMP",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ JUMP Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ»Ğ¾Ğ¼Ğ° (jailbreak) Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸-Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ²ĞµÑ€ÑĞ¸Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼ÑƒÑ DUMP. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ¿Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸-Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ğ³Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½ÑƒÑ Ñ‚ĞµĞ¼Ñƒ ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ°Ñ‚Ğ°Ğº Ğ½Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "JUMP: Universal Multi-Prompts for Jailbreaking LLMs",
                    "desc": "This paper presents JUMP, a novel method for jailbreaking large language models (LLMs) using universal multi-prompts. Unlike traditional prompting techniques that focus on specific adversarial inputs, JUMP aims to create a universal attacker that can adapt to various unseen tasks, reducing computational costs. Additionally, the authors propose a defense mechanism called DUMP, which leverages the same principles to protect against such attacks. Experimental results indicate that JUMP significantly outperforms existing methods in optimizing these universal multi-prompts."
                },
                "zh": {
                    "title": "é€šç”¨å¤šæç¤ºï¼šç ´è§£ä¸é˜²å¾¡çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿‘å¹´æ¥è¿…é€Ÿå‘å±•ï¼Œæ”¹å˜äº†è®¸å¤šåº”ç”¨ï¼Œæ˜¾è‘—æé«˜äº†ä¾¿åˆ©æ€§å’Œç”Ÿäº§åŠ›ã€‚ç„¶è€Œï¼Œéšç€å…¶å¼ºå¤§èƒ½åŠ›çš„æå‡ï¼Œä¼¦ç†é—®é¢˜å’Œæ–°å‹æ”»å‡»ï¼ˆå¦‚è¶Šç‹±æ”»å‡»ï¼‰ä¹Ÿéšä¹‹å‡ºç°ã€‚å¤§å¤šæ•°æç¤ºæŠ€æœ¯ä¸“æ³¨äºä¼˜åŒ–å•ä¸ªæ¡ˆä¾‹çš„å¯¹æŠ—è¾“å…¥ï¼Œè¿™åœ¨å¤„ç†å¤§æ•°æ®é›†æ—¶ä¼šå¯¼è‡´æ›´é«˜çš„è®¡ç®—æˆæœ¬ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºJUMPçš„æ–¹æ³•ï¼Œæ—¨åœ¨ä½¿ç”¨é€šç”¨å¤šæç¤ºå¯¹LLMsè¿›è¡Œè¶Šç‹±ï¼ŒåŒæ—¶æˆ‘ä»¬è¿˜æå‡ºäº†é˜²å¾¡æ–¹æ³•DUMPï¼Œå®éªŒç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¼˜åŒ–é€šç”¨å¤šæç¤ºæ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.03275",
            "title": "Token Assorted: Mixing Latent and Text Tokens for Improved Language Model Reasoning",
            "url": "https://huggingface.co/papers/2502.03275",
            "abstract": "Large Language Models (LLMs) excel at reasoning and planning when trained on chainof-thought (CoT) data, where the step-by-step thought process is explicitly outlined by text tokens. However, this results in lengthy inputs where many words support textual coherence rather than core reasoning information, and processing these inputs consumes substantial computation resources. In this work, we propose a hybrid representation of the reasoning process, where we partially abstract away the initial reasoning steps using latent discrete tokens generated by VQ-VAE, significantly reducing the length of reasoning traces. We explore the use of latent trace abstractions in two scenarios: 1) training the model from scratch for the Keys-Finding Maze problem, 2) fine-tuning LLMs on this hybrid data with an extended vocabulary including unseen latent tokens, for both logical and mathematical reasoning problems. To facilitate effective learning, we introduce a simple training procedure that randomly mixes latent and text tokens, which enables fast adaptation to new latent tokens. Our approach consistently outperforms the baselines methods in various benchmarks.",
            "score": 3,
            "issue_id": 2066,
            "pub_date": "2025-02-05",
            "pub_date_card": {
                "ru": "5 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 5",
                "zh": "2æœˆ5æ—¥"
            },
            "hash": "f94d674e0f57dcf9",
            "authors": [
                "DiJia Su",
                "Hanlin Zhu",
                "Yingchen Xu",
                "Jiantao Jiao",
                "Yuandong Tian",
                "Qinqing Zheng"
            ],
            "affiliations": [
                "Meta AI",
                "UC Berkeley",
                "UCL"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.03275.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#reasoning",
                    "#optimization",
                    "#training",
                    "#benchmark",
                    "#math"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ñ",
                    "desc": "Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ VQ-VAE, Ğ´Ğ»Ñ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ğ¸ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ´Ğ»Ğ¸Ğ½Ñƒ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ ĞºĞ°Ğº Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ½ÑƒĞ»Ñ, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ñ€Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… LLM Ğ½Ğ° Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğ¼ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ĞµĞ¼. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ½Ğ° Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Streamlining Reasoning with Hybrid Token Representations",
                    "desc": "This paper presents a new method to improve reasoning in Large Language Models (LLMs) by using a hybrid representation of reasoning processes. Instead of relying solely on lengthy text inputs, the authors introduce latent discrete tokens generated by VQ-VAE to simplify the reasoning steps. This approach reduces the input length and computational resources needed while maintaining effective reasoning capabilities. The proposed method shows superior performance in training and fine-tuning scenarios for logical and mathematical reasoning tasks compared to traditional methods."
                },
                "zh": {
                    "title": "ä¼˜åŒ–æ¨ç†è¿‡ç¨‹ï¼Œæå‡æ¨¡å‹æ•ˆç‡",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†å’Œè§„åˆ’ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ•°æ®è®­ç»ƒæ—¶çš„è¡¨ç°ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ··åˆè¡¨ç¤ºæ³•ï¼Œé€šè¿‡ä½¿ç”¨VQ-VAEç”Ÿæˆçš„æ½œåœ¨ç¦»æ•£æ ‡è®°ï¼Œéƒ¨åˆ†æŠ½è±¡åŒ–åˆå§‹æ¨ç†æ­¥éª¤ï¼Œä»è€Œæ˜¾è‘—å‡å°‘æ¨ç†è¿‡ç¨‹çš„é•¿åº¦ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªåœºæ™¯ä¸­æ¢ç´¢äº†æ½œåœ¨è¿½è¸ªæŠ½è±¡çš„ä½¿ç”¨ï¼šä¸€æ˜¯ä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹è§£å†³é’¥åŒ™å¯»æ‰¾è¿·å®«é—®é¢˜ï¼ŒäºŒæ˜¯å¯¹LLMsè¿›è¡Œå¾®è°ƒä»¥å¤„ç†é€»è¾‘å’Œæ•°å­¦æ¨ç†é—®é¢˜ã€‚æˆ‘ä»¬çš„è®­ç»ƒæ–¹æ³•é€šè¿‡éšæœºæ··åˆæ½œåœ¨æ ‡è®°å’Œæ–‡æœ¬æ ‡è®°ï¼Œä¿ƒè¿›äº†å¯¹æ–°æ½œåœ¨æ ‡è®°çš„å¿«é€Ÿé€‚åº”ï¼Œä¸”åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.00306",
            "title": "Riddle Me This! Stealthy Membership Inference for Retrieval-Augmented Generation",
            "url": "https://huggingface.co/papers/2502.00306",
            "abstract": "Retrieval-Augmented Generation (RAG) enables Large Language Models (LLMs) to generate grounded responses by leveraging external knowledge databases without altering model parameters. Although the absence of weight tuning prevents leakage via model parameters, it introduces the risk of inference adversaries exploiting retrieved documents in the model's context. Existing methods for membership inference and data extraction often rely on jailbreaking or carefully crafted unnatural queries, which can be easily detected or thwarted with query rewriting techniques common in RAG systems. In this work, we present Interrogation Attack (IA), a membership inference technique targeting documents in the RAG datastore. By crafting natural-text queries that are answerable only with the target document's presence, our approach demonstrates successful inference with just 30 queries while remaining stealthy; straightforward detectors identify adversarial prompts from existing methods up to ~76x more frequently than those generated by our attack. We observe a 2x improvement in TPR@1%FPR over prior inference attacks across diverse RAG configurations, all while costing less than $0.02 per document inference.",
            "score": 2,
            "issue_id": 2076,
            "pub_date": "2025-02-01",
            "pub_date_card": {
                "ru": "1 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 1",
                "zh": "2æœˆ1æ—¥"
            },
            "hash": "4987f380f5ddb7af",
            "authors": [
                "Ali Naseh",
                "Yuefeng Peng",
                "Anshuman Suri",
                "Harsh Chaudhari",
                "Alina Oprea",
                "Amir Houmansadr"
            ],
            "affiliations": [
                "Northeastern University",
                "University of Massachusetts Amherst"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.00306.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#rag",
                    "#leakage",
                    "#security"
                ],
                "emoji": "ğŸ•µï¸",
                "ru": {
                    "title": "ĞĞµĞ·Ğ°Ğ¼ĞµÑ‚Ğ½Ğ°Ñ Ğ°Ñ‚Ğ°ĞºĞ° Ğ½Ğ° RAG-ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹: ĞºĞ°Ğº Ğ²Ñ‹ÑĞ²Ğ¸Ñ‚ÑŒ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ² Ğ±Ğ°Ğ·Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ñ‚Ğ°ĞºĞ¸ Ğ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ (RAG). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 'Interrogation Attack', ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑŒ, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ÑÑ Ğ»Ğ¸ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¹ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚ Ğ² Ğ±Ğ°Ğ·Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ RAG-ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ğ½Ğ° ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¸Ñ‚ÑŒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ñ€Ğ¸ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ°Ñ‚Ğ°ĞºĞ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¸ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ¼Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Stealthy Inference: Unveiling Membership in RAG Systems",
                    "desc": "This paper introduces a new method called Interrogation Attack (IA) for membership inference in Retrieval-Augmented Generation (RAG) systems. RAG allows Large Language Models (LLMs) to generate responses using external knowledge without changing their internal parameters, but this can be exploited by adversaries. The IA technique uses natural-text queries that can only be answered if a specific document is present, making it harder to detect than previous methods. The authors demonstrate that their approach is more effective and stealthy, achieving better performance with fewer queries and lower costs compared to existing techniques."
                },
                "zh": {
                    "title": "éšè”½çš„ä¼šå‘˜æ¨æ–­æ”»å‡»ï¼šRAGç³»ç»Ÿçš„æ–°æŒ‘æˆ˜",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„ä¼šå‘˜æ¨æ–­æŠ€æœ¯ï¼Œç§°ä¸ºå®¡é—®æ”»å‡»ï¼ˆInterrogation Attack, IAï¼‰ï¼Œæ—¨åœ¨é’ˆå¯¹RAGæ•°æ®å­˜å‚¨ä¸­çš„æ–‡æ¡£è¿›è¡Œæ”»å‡»ã€‚è¯¥æ–¹æ³•é€šè¿‡æ„é€ è‡ªç„¶è¯­è¨€æŸ¥è¯¢ï¼Œä»…åœ¨ç›®æ ‡æ–‡æ¡£å­˜åœ¨æ—¶æ‰èƒ½å¾—åˆ°ç­”æ¡ˆï¼Œä»è€Œå®ç°æœ‰æ•ˆçš„æ¨æ–­ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ”»å‡»åœ¨ä»…ä½¿ç”¨30ä¸ªæŸ¥è¯¢çš„æƒ…å†µä¸‹ï¼ŒæˆåŠŸç‡æé«˜äº†2å€ï¼ŒåŒæ—¶ä¿æŒéšè”½æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼ŒIAåœ¨å¤šç§RAGé…ç½®ä¸‹çš„è¡¨ç°ä¼˜äºä»¥å¾€çš„æ¨æ–­æ”»å‡»ï¼Œä¸”æ¯ä¸ªæ–‡æ¡£çš„æ¨æ–­æˆæœ¬ä½äº0.02ç¾å…ƒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.02421",
            "title": "Activation-Informed Merging of Large Language Models",
            "url": "https://huggingface.co/papers/2502.02421",
            "abstract": "Model merging, a method that combines the parameters and embeddings of multiple fine-tuned large language models (LLMs), offers a promising approach to enhance model performance across various tasks while maintaining computational efficiency. This paper introduces Activation-Informed Merging (AIM), a technique that integrates the information from the activation space of LLMs into the merging process to improve performance and robustness. AIM is designed as a flexible, complementary solution that is applicable to any existing merging method. It aims to preserve critical weights from the base model, drawing on principles from continual learning~(CL) and model compression. Utilizing a task-agnostic calibration set, AIM selectively prioritizes essential weights during merging. We empirically demonstrate that AIM significantly enhances the performance of merged models across multiple benchmarks. Our findings suggest that considering the activation-space information can provide substantial advancements in the model merging strategies for LLMs with up to 40\\% increase in benchmark performance.",
            "score": 1,
            "issue_id": 2079,
            "pub_date": "2025-02-04",
            "pub_date_card": {
                "ru": "4 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 4",
                "zh": "2æœˆ4æ—¥"
            },
            "hash": "90e80efaaef789ec",
            "authors": [
                "Amin Heyrani Nobari",
                "Kaveh Alimohammadi",
                "Ali ArjomandBigdeli",
                "Akash Srivastava",
                "Faez Ahmed",
                "Navid Azizan"
            ],
            "affiliations": [
                "Massachusetts Institute of Technology",
                "RedHat AI Innovation & MIT-IBM Watson AI Lab",
                "Stony Brook University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.02421.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#architecture",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "AIM: Ğ£Ğ¼Ğ½Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Activation-Informed Merging (AIM). AIM Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼ Ğº Ğ»ÑĞ±Ğ¾Ğ¼Ñƒ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰ĞµĞ¼Ñƒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ñƒ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ¾ 40%."
                },
                "en": {
                    "title": "Boosting Model Performance with Activation-Informed Merging",
                    "desc": "This paper presents Activation-Informed Merging (AIM), a novel technique for merging large language models (LLMs) that leverages activation space information to improve model performance. AIM enhances the merging process by selectively prioritizing essential weights from the base models, which helps maintain robustness and efficiency. The method is flexible and can be integrated with existing merging techniques, making it widely applicable. Empirical results show that AIM can lead to significant performance improvements, achieving up to a 40% increase in benchmark scores for merged models."
                },
                "zh": {
                    "title": "æ¿€æ´»ä¿¡æ¯åˆå¹¶ï¼šæå‡æ¨¡å‹åˆå¹¶æ€§èƒ½çš„æ–°æ–¹æ³•",
                    "desc": "æ¨¡å‹åˆå¹¶æ˜¯ä¸€ç§å°†å¤šä¸ªå¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‚æ•°å’ŒåµŒå…¥ç»“åˆèµ·æ¥çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒè®¡ç®—æ•ˆç‡çš„åŒæ—¶æå‡æ¨¡å‹åœ¨å„ç§ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºæ¿€æ´»ä¿¡æ¯åˆå¹¶ï¼ˆAIMï¼‰çš„æŠ€æœ¯ï¼Œå®ƒå°†LLMsçš„æ¿€æ´»ç©ºé—´ä¿¡æ¯æ•´åˆåˆ°åˆå¹¶è¿‡ç¨‹ä¸­ï¼Œä»¥æé«˜æ€§èƒ½å’Œé²æ£’æ€§ã€‚AIMæ—¨åœ¨ä½œä¸ºä¸€ç§çµæ´»çš„è¡¥å……è§£å†³æ–¹æ¡ˆï¼Œé€‚ç”¨äºä»»ä½•ç°æœ‰çš„åˆå¹¶æ–¹æ³•ï¼Œå¹¶é€šè¿‡æŒç»­å­¦ä¹ å’Œæ¨¡å‹å‹ç¼©çš„åŸåˆ™æ¥ä¿ç•™åŸºç¡€æ¨¡å‹ä¸­çš„å…³é”®æƒé‡ã€‚é€šè¿‡ä½¿ç”¨ä¸ä»»åŠ¡æ— å…³çš„æ ¡å‡†é›†ï¼ŒAIMåœ¨åˆå¹¶è¿‡ç¨‹ä¸­ä¼˜å…ˆè€ƒè™‘é‡è¦æƒé‡ï¼Œå®éªŒè¯æ˜AIMåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†åˆå¹¶æ¨¡å‹çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.00226",
            "title": "HackerRank-ASTRA: Evaluating Correctness & Consistency of Large Language Models on cross-domain multi-file project problems",
            "url": "https://huggingface.co/papers/2502.00226",
            "abstract": "Evaluating the real-world applicability of large language models (LLMs) provides valuable insights for their development and use in software development tasks. Existing benchmarks often focus on standalone coding problems or specific libraries, overlooking multi-file, project-based scenarios and lacking a rigorous evaluation of consistency. The HackerRank-ASTRA Benchmark introduces project-based coding problems that mirror real-world scenarios. It evaluates model consistency through 32 runs (k = 32) and median standard deviation while incorporating taxonomy-level analysis to assess sub-skill capabilities. Initial evaluations on 65 problems show that the top three models -- o1, o1-preview, and Claude-3.5-Sonnet-1022 -- achieved comparable average scores of 75%, with no statistically significant differences in performance. Notably, Claude-3.5-Sonnet-1022 demonstrated the highest consistency across problems, with low variability (SD = 0.0497), which was statistically significant compared to other models, highlighting its reliability for real-world software development tasks.",
            "score": 0,
            "issue_id": 2079,
            "pub_date": "2025-01-31",
            "pub_date_card": {
                "ru": "31 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 31",
                "zh": "1æœˆ31æ—¥"
            },
            "hash": "c9615b5d00a42037",
            "authors": [
                "Jun Xing",
                "Mayur Bhatia",
                "Sahil Phulwani",
                "Darshan Suresh",
                "Rafik Matta"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2502.00226.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#science",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» LLM Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ ĞŸĞ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ. HackerRank-ASTRA Benchmark Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸, Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡ĞµÑ€ĞµĞ· 32 Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ñ‚Ñ€Ğ¸ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ½Ğ¸Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ² 75%. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Claude-3.5-Sonnet-1022 Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ° Ğ½Ğ°Ğ¸Ğ²Ñ‹ÑÑˆÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ½Ğ¸Ğ·ĞºÑƒÑ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Benchmarking LLMs for Real-World Coding Consistency",
                    "desc": "This paper evaluates the effectiveness of large language models (LLMs) in real-world software development tasks using the HackerRank-ASTRA Benchmark. Unlike previous benchmarks that focus on isolated coding problems, this benchmark introduces project-based scenarios that require multi-file handling and assesses model consistency through extensive testing. The study analyzes the performance of top models, revealing that while they achieved similar average scores, one model, Claude-3.5-Sonnet-1022, stood out for its high consistency and low variability in results. This research emphasizes the importance of rigorous evaluation methods to ensure LLMs are reliable for practical applications in coding."
                },
                "zh": {
                    "title": "è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è½¯ä»¶å¼€å‘ä¸­çš„çœŸå®åº”ç”¨æ€§",
                    "desc": "è¿™ç¯‡è®ºæ–‡è¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å®é™…è½¯ä»¶å¼€å‘ä»»åŠ¡ä¸­çš„é€‚ç”¨æ€§ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•é€šå¸¸åªå…³æ³¨å•ä¸€çš„ç¼–ç é—®é¢˜æˆ–ç‰¹å®šåº“ï¼Œå¿½è§†äº†å¤šæ–‡ä»¶ã€åŸºäºé¡¹ç›®çš„åœºæ™¯ï¼Œå¹¶ç¼ºä¹å¯¹ä¸€è‡´æ€§çš„ä¸¥æ ¼è¯„ä¼°ã€‚HackerRank-ASTRAåŸºå‡†å¼•å…¥äº†æ¨¡æ‹ŸçœŸå®åœºæ™¯çš„é¡¹ç›®åŸºç¡€ç¼–ç é—®é¢˜ï¼Œå¹¶é€šè¿‡32æ¬¡è¿è¡Œè¯„ä¼°æ¨¡å‹çš„ä¸€è‡´æ€§ã€‚åˆæ­¥è¯„ä¼°æ˜¾ç¤ºï¼ŒClaude-3.5-Sonnet-1022åœ¨é—®é¢˜ä¸€è‡´æ€§æ–¹é¢è¡¨ç°æœ€ä½³ï¼Œå…·æœ‰è¾ƒä½çš„å˜å¼‚æ€§ï¼Œçªæ˜¾äº†å…¶åœ¨å®é™…è½¯ä»¶å¼€å‘ä»»åŠ¡ä¸­çš„å¯é æ€§ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-02-06.html",
    "link_next": "2025-02-10.html",
    "link_month": "2025-02.html",
    "short_date_prev": {
        "ru": "06.02",
        "en": "02/06",
        "zh": "2æœˆ6æ—¥"
    },
    "short_date_next": {
        "ru": "10.02",
        "en": "02/10",
        "zh": "2æœˆ10æ—¥"
    },
    "categories": {
        "#dataset": 4,
        "#data": 2,
        "#benchmark": 4,
        "#agents": 2,
        "#cv": 1,
        "#rl": 2,
        "#rlhf": 1,
        "#rag": 1,
        "#plp": 1,
        "#inference": 2,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 3,
        "#math": 3,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 10,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 5,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 2,
        "#optimization": 8,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 1,
        "#open_source": 2,
        "#small_models": 1,
        "#science": 1,
        "#low_resource": 1
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ç ”ç©¶ç¤¾ä¼šè¡Œä¸ºçš„äº§ç”Ÿã€‚ä¼ ç»Ÿæ¨¡å‹éš¾ä»¥æ•æ‰äººç±»è¡Œä¸ºçš„å¤æ‚æ€§ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†å¯æ¨¡æ‹Ÿéç†æ€§å› ç´ ã€‚æˆ‘ä»¬ä»‹ç»äº†TwinMarketï¼Œä½¿ç”¨LLMæ¨¡æ‹Ÿç¤¾ä¼šç»æµç³»ç»Ÿã€‚å®éªŒå±•ç¤ºäº†ä¸ªä½“è¡Œä¸ºå¦‚ä½•å¼•å‘ç¾¤ä½“è¡Œä¸ºå’Œçªç°ç°è±¡ã€‚",
        "title": "TwinMarket: A Scalable Behavioral and Social Simulation for Financial Markets",
        "pinyin": "è¿™ç¯‡æ–‡ç« ç ”ç©¶ç¤¾ä¼šè¡Œä¸ºçš„äº§ç”Ÿã€‚\nZhÃ¨ piÄn wÃ©nzhÄng yÃ¡njiÅ« shÃ¨huÃ¬ xÃ­ngwÃ©i de chÇnshÄ“ng.\n\nä¼ ç»Ÿæ¨¡å‹éš¾ä»¥æ•æ‰äººç±»è¡Œä¸ºçš„å¤æ‚æ€§ã€‚\nChuÃ¡ntÇ’ng mÃ³xÃ­ng nÃ¡nyÇ bÇ”zhuÅ rÃ©nlÃ¨i xÃ­ngwÃ©i de fÃ¹zÃ¡xÃ¬ng.\n\nå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†å¯æ¨¡æ‹Ÿéç†æ€§å› ç´ ã€‚\nDÃ xÃ­ng yÇ”yÃ¡n mÃ³xÃ­ng (LLM) dÃ ilÇ kÄ› mÃ³nÇ fÄ“i lÇxÃ¬ng yÄ«nsÃ¹.\n\næˆ‘ä»¬ä»‹ç»äº†TwinMarketï¼Œä½¿ç”¨LLMæ¨¡æ‹Ÿç¤¾ä¼šç»æµç³»ç»Ÿã€‚\nWÇ’men jiÃ¨shÃ o le TwinMarket, shÇyÃ²ng LLM mÃ³nÇ shÃ¨huÃ¬ jÄ«ngjÃ¬ xÃ¬tÇ’ng.\n\nå®éªŒå±•ç¤ºäº†ä¸ªä½“è¡Œä¸ºå¦‚ä½•å¼•å‘ç¾¤ä½“è¡Œä¸ºå’Œçªç°ç°è±¡ã€‚\nShÃ­yÃ n zhÇnshÃ¬ le gÃ¨tÇ xÃ­ngwÃ©i rÃºhÃ© yÇnfÄ qÃºntÇ xÃ­ngwÃ©i hÃ© tÅ«xiÃ n xiÃ nxiÃ ng.",
        "vocab": "[\n    {\"word\": \"ç ”ç©¶\", \"pinyin\": \"yÃ¡n jiÅ«\", \"trans\": \"research\"},\n    {\"word\": \"ç¤¾ä¼šè¡Œä¸º\", \"pinyin\": \"shÃ¨ huÃ¬ xÃ­ng wÃ©i\", \"trans\": \"social behavior\"},\n    {\"word\": \"äº§ç”Ÿ\", \"pinyin\": \"chÇn shÄ“ng\", \"trans\": \"generate\"},\n    {\"word\": \"ä¼ ç»Ÿ\", \"pinyin\": \"chuÃ¡n tÇ’ng\", \"trans\": \"traditional\"},\n    {\"word\": \"æ¨¡å‹\", \"pinyin\": \"mÃ³ xÃ­ng\", \"trans\": \"model\"},\n    {\"word\": \"éš¾ä»¥\", \"pinyin\": \"nÃ¡n yÇ\", \"trans\": \"difficult to\"},\n    {\"word\": \"æ•æ‰\", \"pinyin\": \"bÇ” zhuÅ\", \"trans\": \"capture\"},\n    {\"word\": \"å¤æ‚æ€§\", \"pinyin\": \"fÃ¹ zÃ¡ xÃ¬ng\", \"trans\": \"complexity\"},\n    {\"word\": \"å¤§å‹\", \"pinyin\": \"dÃ  xÃ­ng\", \"trans\": \"large-scale\"},\n    {\"word\": \"è¯­è¨€æ¨¡å‹\", \"pinyin\": \"yÇ” yÃ¡n mÃ³ xÃ­ng\", \"trans\": \"language model\"},\n    {\"word\": \"ä»£ç†\", \"pinyin\": \"dÃ i lÇ\", \"trans\": \"agent\"},\n    {\"word\": \"æ¨¡æ‹Ÿ\", \"pinyin\": \"mÃ³ nÇ\", \"trans\": \"simulate\"},\n    {\"word\": \"éç†æ€§\", \"pinyin\": \"fÄ“i lÇ xÃ¬ng\", \"trans\": \"irrational\"},\n    {\"word\": \"å› ç´ \", \"pinyin\": \"yÄ«n sÃ¹\", \"trans\": \"factor\"},\n    {\"word\": \"ä»‹ç»\", \"pinyin\": \"jiÃ¨ shÃ o\", \"trans\": \"introduce\"},\n    {\"word\": \"TwinMarket\", \"pinyin\": \"TwinMarket\", \"trans\": \"TwinMarket\"},\n    {\"word\": \"ä½¿ç”¨\", \"pinyin\": \"shÇ yÃ²ng\", \"trans\": \"use\"},\n    {\"word\": \"ç¤¾ä¼šç»æµç³»ç»Ÿ\", \"pinyin\": \"shÃ¨ huÃ¬ jÄ«ng jÃ¬ xÃ¬ tÇ’ng\", \"trans\": \"socio-economic system\"},\n    {\"word\": \"å®éªŒ\", \"pinyin\": \"shÃ­ yÃ n\", \"trans\": \"experiment\"},\n    {\"word\": \"å±•ç¤º\", \"pinyin\": \"zhÇn shÃ¬\", \"trans\": \"demonstrate\"},\n    {\"word\": \"ä¸ªä½“è¡Œä¸º\", \"pinyin\": \"gÃ¨ tÇ xÃ­ng wÃ©i\", \"trans\": \"individual behavior\"},\n    {\"word\": \"å¼•å‘\", \"pinyin\": \"yÇn fÄ\", \"trans\": \"trigger\"},\n    {\"word\": \"ç¾¤ä½“è¡Œä¸º\", \"pinyin\": \"qÃºn tÇ xÃ­ng wÃ©i\", \"trans\": \"group behavior\"},\n    {\"word\": \"çªç°ç°è±¡\", \"pinyin\": \"tÅ« xiÃ n xiÃ n xiÃ ng\", \"trans\": \"emergent phenomenon\"}\n]",
        "trans": "This article investigates the generation of social behaviors. Traditional models struggle to capture the complexity of human behavior. Large Language Model (LLM) agents can simulate non-rational factors. We introduce TwinMarket, which uses LLM to simulate socio-economic systems. Experiments demonstrate how individual behaviors trigger group behaviors and emergent phenomena.",
        "update_ts": "2025-02-06 09:11"
    }
}