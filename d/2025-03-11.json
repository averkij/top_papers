{
    "date": {
        "ru": "11 –º–∞—Ä—Ç–∞",
        "en": "March 11",
        "zh": "3Êúà11Êó•"
    },
    "time_utc": "2025-03-11 02:17",
    "weekday": 1,
    "issue_id": 2630,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2503.06680",
            "title": "FEA-Bench: A Benchmark for Evaluating Repository-Level Code Generation\n  for Feature Implementation",
            "url": "https://huggingface.co/papers/2503.06680",
            "abstract": "Implementing new features in repository-level codebases is a crucial application of code generation models. However, current benchmarks lack a dedicated evaluation framework for this capability. To fill this gap, we introduce FEA-Bench, a benchmark designed to assess the ability of large language models (LLMs) to perform incremental development within code repositories. We collect pull requests from 83 GitHub repositories and use rule-based and intent-based filtering to construct task instances focused on new feature development. Each task instance containing code changes is paired with relevant unit test files to ensure that the solution can be verified. The feature implementation requires LLMs to simultaneously possess code completion capabilities for new components and code editing abilities for other relevant parts in the code repository, providing a more comprehensive evaluation method of LLMs' automated software engineering capabilities. Experimental results show that LLMs perform significantly worse in the FEA-Bench, highlighting considerable challenges in such repository-level incremental code development.",
            "score": 4,
            "issue_id": 2630,
            "pub_date": "2025-03-09",
            "pub_date_card": {
                "ru": "9 –º–∞—Ä—Ç–∞",
                "en": "March 9",
                "zh": "3Êúà9Êó•"
            },
            "hash": "4394ce17a18696a3",
            "authors": [
                "Wei Li",
                "Xin Zhang",
                "Zhongxin Guo",
                "Shaoguang Mao",
                "Wen Luo",
                "Guangyue Peng",
                "Yangyu Huang",
                "Houfeng Wang",
                "Scarlett Li"
            ],
            "affiliations": [
                "Microsoft Research Asia",
                "State Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.06680.jpg",
            "data": {
                "categories": [
                    "#plp",
                    "#dataset",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "üß™",
                "ru": {
                    "title": "FEA-Bench: –Ω–æ–≤—ã–π –≤—ã–∑–æ–≤ –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –ü–û",
                    "desc": "FEA-Bench - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤—ã–ø–æ–ª–Ω—è—Ç—å –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—É—é —Ä–∞–∑—Ä–∞–±–æ—Ç–∫—É –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è—Ö –∫–æ–¥–∞. –û–Ω –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –ø—É–ª-—Ä–µ–∫–≤–µ—Å—Ç–∞—Ö –∏–∑ 83 —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–µ–≤ GitHub –∏ –≤–∫–ª—é—á–∞–µ—Ç –∑–∞–¥–∞—á–∏ –ø–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –Ω–æ–≤—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π. –ö–∞–∂–¥–∞—è –∑–∞–¥–∞—á–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è –∫–æ–¥–∞ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –º–æ–¥—É–ª—å–Ω—ã–µ —Ç–µ—Å—Ç—ã –¥–ª—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ LLM –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Ö—É–∂–µ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å –∑–∞–¥–∞—á–∞–º–∏ FEA-Bench, —á—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ —Å–µ—Ä—å–µ–∑–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –≤ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–µ–≤."
                },
                "en": {
                    "title": "FEA-Bench: Evaluating Code Generation in Repositories",
                    "desc": "This paper introduces FEA-Bench, a new benchmark for evaluating large language models (LLMs) in the context of code generation for new features in software repositories. It addresses the lack of dedicated evaluation frameworks by collecting pull requests from 83 GitHub repositories and creating task instances that focus on incremental development. Each task is designed to test the LLM's ability to generate code changes while ensuring that these changes can be verified through associated unit tests. The findings reveal that LLMs struggle with this type of repository-level code development, indicating significant challenges in their automated software engineering capabilities."
                },
                "zh": {
                    "title": "ËØÑ‰º∞‰ª£Á†ÅÁîüÊàêÊ®°ÂûãÁöÑÊñ∞Âü∫ÂáÜÔºöFEA-Bench",
                    "desc": "Âú®‰ª£Á†ÅÁîüÊàêÊ®°Âûã‰∏≠ÔºåÂÆûÁé∞Êñ∞ÁâπÊÄßÊòØ‰∏Ä‰∏™ÈáçË¶ÅÁöÑÂ∫îÁî®„ÄÇÂΩìÂâçÁöÑÂü∫ÂáÜÊµãËØïÁº∫‰πè‰∏ìÈó®ËØÑ‰º∞Ëøô‰∏ÄËÉΩÂäõÁöÑÊ°ÜÊû∂„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜFEA-BenchÔºåËøôÊòØ‰∏Ä‰∏™Êó®Âú®ËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®‰ª£Á†ÅÂ∫ì‰∏≠ËøõË°åÂ¢ûÈáèÂºÄÂèëËÉΩÂäõÁöÑÂü∫ÂáÜ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåLLMsÂú®FEA-Bench‰∏≠ÁöÑË°®Áé∞ÊòæËëóËæÉÂ∑ÆÔºåÁ™ÅÊòæ‰∫ÜÂú®‰ª£Á†ÅÂ∫ìÁ∫ßÂà´Â¢ûÈáèÂºÄÂèë‰∏≠Èù¢‰∏¥ÁöÑÈáçÂ§ßÊåëÊàò„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.07027",
            "title": "EasyControl: Adding Efficient and Flexible Control for Diffusion\n  Transformer",
            "url": "https://huggingface.co/papers/2503.07027",
            "abstract": "Recent advancements in Unet-based diffusion models, such as ControlNet and IP-Adapter, have introduced effective spatial and subject control mechanisms. However, the DiT (Diffusion Transformer) architecture still struggles with efficient and flexible control. To tackle this issue, we propose EasyControl, a novel framework designed to unify condition-guided diffusion transformers with high efficiency and flexibility. Our framework is built on three key innovations. First, we introduce a lightweight <PRE_TAG>Condition Injection LoRA Module</POST_TAG>. This module processes conditional signals in isolation, acting as a plug-and-play solution. It avoids modifying the base model weights, ensuring compatibility with customized models and enabling the flexible injection of diverse conditions. Notably, this module also supports harmonious and robust zero-shot multi-condition generalization, even when trained only on single-condition data. Second, we propose a <PRE_TAG>Position-Aware Training Paradigm</POST_TAG>. This approach standardizes input conditions to fixed resolutions, allowing the generation of images with arbitrary aspect ratios and flexible resolutions. At the same time, it optimizes computational efficiency, making the framework more practical for real-world applications. Third, we develop a Causal Attention Mechanism combined with the KV Cache technique, adapted for conditional generation tasks. This innovation significantly reduces the latency of image synthesis, improving the overall efficiency of the framework. Through extensive experiments, we demonstrate that EasyControl achieves exceptional performance across various application scenarios. These innovations collectively make our framework highly efficient, flexible, and suitable for a wide range of tasks.",
            "score": 3,
            "issue_id": 2630,
            "pub_date": "2025-03-10",
            "pub_date_card": {
                "ru": "10 –º–∞—Ä—Ç–∞",
                "en": "March 10",
                "zh": "3Êúà10Êó•"
            },
            "hash": "1b1589f9873720c7",
            "authors": [
                "Yuxuan Zhang",
                "Yirui Yuan",
                "Yiren Song",
                "Haofan Wang",
                "Jiaming Liu"
            ],
            "affiliations": [
                "Liblib AI",
                "National University of Singapore",
                "ShanghaiTech University",
                "Tiamat AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.07027.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#architecture",
                    "#training",
                    "#optimization",
                    "#diffusion"
                ],
                "emoji": "üé®",
                "ru": {
                    "title": "EasyControl: –≥–∏–±–∫–æ–µ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–º–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞–º–∏",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç EasyControl - –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–æ–Ω—Ç—Ä–æ–ª—è –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö. –û—Å–Ω–æ–≤–Ω—ã–µ –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏ –≤–∫–ª—é—á–∞—é—Ç –º–æ–¥—É–ª—å –≤–Ω–µ–¥—Ä–µ–Ω–∏—è —É—Å–ª–æ–≤–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ LoRA, –ø–∞—Ä–∞–¥–∏–≥–º—É –æ–±—É—á–µ–Ω–∏—è —Å —É—á–µ—Ç–æ–º –ø–æ–∑–∏—Ü–∏–∏ –∏ –º–µ—Ö–∞–Ω–∏–∑–º –ø—Ä–∏—á–∏–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è —Å KV-–∫—ç—à–µ–º. EasyControl –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –∏ –≥–∏–±–∫–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–º–∏ –ø—Ä–æ–ø–æ—Ä—Ü–∏—è–º–∏ –∏ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ–º. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–∏—Å—Ç–µ–º—ã –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è."
                },
                "en": {
                    "title": "EasyControl: Unifying Efficiency and Flexibility in Diffusion Transformers",
                    "desc": "This paper presents EasyControl, a new framework that enhances the efficiency and flexibility of condition-guided diffusion transformers, particularly addressing the limitations of the DiT architecture. It introduces a lightweight Condition Injection LoRA Module that allows for the independent processing of conditional signals without altering the base model, enabling easy integration of various conditions. Additionally, the Position-Aware Training Paradigm standardizes input conditions, facilitating the generation of images with different aspect ratios while optimizing computational resources. Lastly, the Causal Attention Mechanism with KV Cache significantly reduces image synthesis latency, making EasyControl a robust solution for diverse applications in machine learning."
                },
                "zh": {
                    "title": "È´òÊïàÁÅµÊ¥ªÁöÑÊù°‰ª∂ÁîüÊàêÊ°ÜÊû∂EasyControl",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫EasyControlÁöÑÊñ∞Ê°ÜÊû∂ÔºåÊó®Âú®ÊèêÈ´òÂü∫‰∫éÊâ©Êï£ÂèòÊç¢Âô®ÁöÑÊù°‰ª∂ÁîüÊàêÊ®°ÂûãÁöÑÊïàÁéáÂíåÁÅµÊ¥ªÊÄß„ÄÇËØ•Ê°ÜÊû∂ÂåÖÂê´‰∏â‰∏™ÂÖ≥ÈîÆÂàõÊñ∞ÔºöÈ¶ñÂÖàÔºåËΩªÈáèÁ∫ßÁöÑÊù°‰ª∂Ê≥®ÂÖ•LoRAÊ®°ÂùóÂèØ‰ª•Áã¨Á´ãÂ§ÑÁêÜÊù°‰ª∂‰ø°Âè∑ÔºåÈÅøÂÖç‰øÆÊîπÂü∫Á°ÄÊ®°ÂûãÊùÉÈáçÔºå‰ªéËÄåÂÆûÁé∞‰∏éÂÆöÂà∂Ê®°ÂûãÁöÑÂÖºÂÆπÊÄß„ÄÇÂÖ∂Ê¨°Ôºå‰ΩçÁΩÆÊÑüÁü•ËÆ≠ÁªÉËåÉÂºèÊ†áÂáÜÂåñËæìÂÖ•Êù°‰ª∂Ôºå‰ΩøÂæóÁîüÊàê‰ªªÊÑèÈïøÂÆΩÊØîÂíåÁÅµÊ¥ªÂàÜËæ®ÁéáÁöÑÂõæÂÉèÊàê‰∏∫ÂèØËÉΩÔºåÂêåÊó∂‰ºòÂåñËÆ°ÁÆóÊïàÁéá„ÄÇÊúÄÂêéÔºåÁªìÂêàKVÁºìÂ≠òÊäÄÊúØÁöÑÂõ†ÊûúÊ≥®ÊÑèÊú∫Âà∂ÊòæËëóÈôç‰Ωé‰∫ÜÂõæÂÉèÂêàÊàêÁöÑÂª∂ËøüÔºåÊèêÂçá‰∫ÜÊï¥‰ΩìÊïàÁéá„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.07365",
            "title": "MM-Eureka: Exploring Visual Aha Moment with Rule-based Large-scale\n  Reinforcement Learning",
            "url": "https://huggingface.co/papers/2503.07365",
            "abstract": "We present MM-Eureka, a multimodal reasoning model that successfully extends large-scale rule-based reinforcement learning (RL) to multimodal reasoning. While rule-based RL has shown remarkable success in improving LLMs' reasoning abilities in text domains, its application to multimodal settings has remained challenging. Our work reproduces key characteristics of text-based RL systems like DeepSeek-R1 in the multimodal space, including steady increases in accuracy reward and response length, and the emergence of reflection behaviors. We demonstrate that both instruction-tuned and pre-trained models can develop strong multimodal reasoning capabilities through rule-based RL without supervised fine-tuning, showing superior data efficiency compared to alternative approaches. We open-source our complete pipeline to foster further research in this area. We release all our codes, models, data, etc. at https://github.com/ModalMinds/MM-EUREKA",
            "score": 2,
            "issue_id": 2630,
            "pub_date": "2025-03-10",
            "pub_date_card": {
                "ru": "10 –º–∞—Ä—Ç–∞",
                "en": "March 10",
                "zh": "3Êúà10Êó•"
            },
            "hash": "765d475b38d9f289",
            "authors": [
                "Fanqing Meng",
                "Lingxiao Du",
                "Zongkai Liu",
                "Zhixiang Zhou",
                "Quanfeng Lu",
                "Daocheng Fu",
                "Botian Shi",
                "Wenhai Wang",
                "Junjun He",
                "Kaipeng Zhang",
                "Ping Luo",
                "Yu Qiao",
                "Qiaosheng Zhang",
                "Wenqi Shao"
            ],
            "affiliations": [
                "Shanghai AI Laboratory",
                "Shanghai Innovation Institute",
                "Shanghai Jiao Tong University",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.07365.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#multimodal",
                    "#reasoning",
                    "#rag",
                    "#open_source"
                ],
                "emoji": "ü§ñ",
                "ru": {
                    "title": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º",
                    "desc": "MM-Eureka ‚Äì —ç—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä–∞—è —É—Å–ø–µ—à–Ω–æ —Ä–∞—Å—à–∏—Ä—è–µ—Ç –º–∞—Å—à—Ç–∞–±–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∞–≤–∏–ª –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á. –ú–æ–¥–µ–ª—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Å–∏—Å—Ç–µ–º –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –≤–∫–ª—é—á–∞—è —É—Å—Ç–æ–π—á–∏–≤–æ–µ –ø–æ–≤—ã—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ –¥–ª–∏–Ω—ã –æ—Ç–≤–µ—Ç–æ–≤, –∞ —Ç–∞–∫–∂–µ –ø–æ—è–≤–ª–µ–Ω–∏–µ —Ä–µ—Ñ–ª–µ–∫—Å–∏–≤–Ω–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –∫–∞–∫ –º–æ–¥–µ–ª–∏, –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –Ω–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏, —Ç–∞–∫ –∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç —Ä–∞–∑–≤–∏–≤–∞—Ç—å —Å–∏–ª—å–Ω—ã–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –±–µ–∑ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–π —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏. –ê–≤—Ç–æ—Ä—ã –æ—Ç–∫—Ä—ã–ª–∏ –∏—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ –≤—Å–µ–≥–æ –∫–æ–Ω–≤–µ–π–µ—Ä–∞ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏."
                },
                "en": {
                    "title": "Unlocking Multimodal Reasoning with MM-Eureka!",
                    "desc": "MM-Eureka is a new model that enhances reasoning across different types of data, like text and images, using a method called rule-based reinforcement learning (RL). This approach builds on successful techniques from text-based RL, allowing the model to improve its accuracy and response quality in multimodal contexts. The model shows that it can learn effectively without needing extra labeled data, making it more efficient than other methods. By sharing our tools and findings, we aim to encourage more research in multimodal reasoning."
                },
                "zh": {
                    "title": "MM-EurekaÔºöÂ§öÊ®°ÊÄÅÊé®ÁêÜÁöÑÊñ∞Á™ÅÁ†¥",
                    "desc": "Êàë‰ª¨ÊèêÂá∫‰∫ÜMM-EurekaÔºåËøôÊòØ‰∏Ä‰∏™Â§öÊ®°ÊÄÅÊé®ÁêÜÊ®°ÂûãÔºåÊàêÂäüÂú∞Â∞ÜÂ§ßËßÑÊ®°Âü∫‰∫éËßÑÂàôÁöÑÂº∫ÂåñÂ≠¶‰π†Êâ©Â±ïÂà∞Â§öÊ®°ÊÄÅÊé®ÁêÜÈ¢ÜÂüü„ÄÇËôΩÁÑ∂Âü∫‰∫éËßÑÂàôÁöÑÂº∫ÂåñÂ≠¶‰π†Âú®ÊñáÊú¨È¢ÜÂüüÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõÊñπÈù¢ÂèñÂæó‰∫ÜÊòæËëóÊàêÂäüÔºå‰ΩÜÂú®Â§öÊ®°ÊÄÅÁéØÂ¢É‰∏≠ÁöÑÂ∫îÁî®‰ªçÁÑ∂ÂÖ∑ÊúâÊåëÊàòÊÄß„ÄÇÊàë‰ª¨ÁöÑÂ∑•‰ΩúÂú®Â§öÊ®°ÊÄÅÁ©∫Èó¥‰∏≠ÈáçÁé∞‰∫ÜÊñáÊú¨Âü∫Á°ÄÂº∫ÂåñÂ≠¶‰π†Á≥ªÁªüÁöÑÂÖ≥ÈîÆÁâπÂæÅÔºåÂåÖÊã¨ÂáÜÁ°ÆÊÄßÂ•ñÂä±ÂíåÂìçÂ∫îÈïøÂ∫¶ÁöÑÁ®≥ÂÆöÂ¢ûÂä†Ôºå‰ª•ÂèäÂèçÊÄùË°å‰∏∫ÁöÑÂá∫Áé∞„ÄÇÊàë‰ª¨Â±ïÁ§∫‰∫ÜÊó†ÁõëÁù£ÂæÆË∞ÉÁöÑÊÉÖÂÜµ‰∏ãÔºåÊåá‰ª§Ë∞É‰ºòÂíåÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÈÉΩËÉΩÈÄöËøáÂü∫‰∫éËßÑÂàôÁöÑÂº∫ÂåñÂ≠¶‰π†ÂèëÂ±ïÂá∫Âº∫Â§ßÁöÑÂ§öÊ®°ÊÄÅÊé®ÁêÜËÉΩÂäõÔºå‰∏îÊï∞ÊçÆÊïàÁéá‰ºò‰∫éÂÖ∂‰ªñÊñπÊ≥ï„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.06520",
            "title": "Seg-Zero: Reasoning-Chain Guided Segmentation via Cognitive\n  Reinforcement",
            "url": "https://huggingface.co/papers/2503.06520",
            "abstract": "Traditional methods for reasoning segmentation rely on supervised fine-tuning with categorical labels and simple descriptions, limiting its out-of-domain generalization and lacking explicit reasoning processes. To address these limitations, we propose Seg-Zero, a novel framework that demonstrates remarkable generalizability and derives explicit chain-of-thought reasoning through cognitive reinforcement. Seg-Zero introduces a decoupled architecture consisting of a reasoning model and a segmentation model. The reasoning model interprets user intentions, generates explicit reasoning chains, and produces positional prompts, which are subsequently used by the segmentation model to generate precious pixel-level masks. We design a sophisticated reward mechanism that integrates both format and accuracy rewards to effectively guide optimization directions. Trained exclusively via reinforcement learning with GRPO and without explicit reasoning data, Seg-Zero achieves robust zero-shot generalization and exhibits emergent test-time reasoning capabilities. Experiments show that Seg-Zero-7B achieves a zero-shot performance of 57.5 on the ReasonSeg benchmark, surpassing the prior LISA-7B by 18\\%. This significant improvement highlights Seg-Zero's ability to generalize across domains while presenting an explicit reasoning process. Code is available at https://github.com/dvlab-research/Seg-Zero.",
            "score": 2,
            "issue_id": 2630,
            "pub_date": "2025-03-09",
            "pub_date_card": {
                "ru": "9 –º–∞—Ä—Ç–∞",
                "en": "March 9",
                "zh": "3Êúà9Êó•"
            },
            "hash": "b21eb23a448d282e",
            "authors": [
                "Yuqi Liu",
                "Bohao Peng",
                "Zhisheng Zhong",
                "Zihao Yue",
                "Fanbin Lu",
                "Bei Yu",
                "Jiaya Jia"
            ],
            "affiliations": [
                "CUHK",
                "HKUST",
                "RUC"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.06520.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#benchmark",
                    "#architecture",
                    "#reasoning",
                    "#rlhf",
                    "#optimization"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è —Å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ–º: –ò–ò —É—á–∏—Ç—Å—è –æ–±—ä—è—Å–Ω—è—Ç—å —Å–≤–æ–∏ —Ä–µ—à–µ–Ω–∏—è",
                    "desc": "Seg-Zero - —ç—Ç–æ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–æ–µ –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±–æ–±—â–∞—é—â–µ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∏ —è–≤–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –º–æ–¥–µ–ª–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –≥–µ–Ω–µ—Ä–∏—Ä—É—é—â–µ–π —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ –ø–æ–¥—Å–∫–∞–∑–∫–∏, –∏ –º–æ–¥–µ–ª–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏, —Å–æ–∑–¥–∞—é—â–µ–π –º–∞—Å–∫–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø–∏–∫—Å–µ–ª–µ–π. –û–±—É—á–µ–Ω–∏–µ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–≥–æ –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π. Seg-Zero-7B –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ 18% –≤ –∑–∞–¥–∞—á–µ zero-shot —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ ReasonSeg."
                },
                "en": {
                    "title": "Seg-Zero: Revolutionizing Reasoning Segmentation with Zero-Shot Generalization",
                    "desc": "The paper introduces Seg-Zero, a new framework for reasoning segmentation that overcomes the limitations of traditional supervised methods. It features a decoupled architecture with a reasoning model that interprets user intentions and generates reasoning chains, which are then used by a segmentation model to create detailed pixel-level masks. The framework employs a unique reward mechanism that balances format and accuracy to optimize performance through reinforcement learning. Seg-Zero demonstrates impressive zero-shot generalization capabilities, achieving a significant performance boost on the ReasonSeg benchmark compared to previous models."
                },
                "zh": {
                    "title": "Seg-ZeroÔºöÁ™ÅÁ†¥ÊÄßÊé®ÁêÜ‰∏éÂàÜÂâ≤ÁöÑÁªìÂêà",
                    "desc": "‰º†ÁªüÁöÑÂàÜÂâ≤Êé®ÁêÜÊñπÊ≥ï‰æùËµñ‰∫éÂ∏¶ÊúâÁ±ªÂà´Ê†áÁ≠æÁöÑÁõëÁù£ÂæÆË∞ÉÔºåËøôÈôêÂà∂‰∫ÜÂÖ∂Âú®‰∏çÂêåÈ¢ÜÂüüÁöÑÊ≥õÂåñËÉΩÂäõÔºåÂπ∂Áº∫‰πèÊòéÁ°ÆÁöÑÊé®ÁêÜËøáÁ®ã„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜSeg-ZeroÔºåËøôÊòØ‰∏ÄÁßçÊñ∞È¢ñÁöÑÊ°ÜÊû∂ÔºåÂ±ïÁ§∫‰∫ÜÊòæËëóÁöÑÊ≥õÂåñËÉΩÂäõÔºåÂπ∂ÈÄöËøáËÆ§Áü•Âº∫ÂåñÊé®ÂØºÂá∫ÊòéÁ°ÆÁöÑÊé®ÁêÜÈìæ„ÄÇSeg-ZeroÂºïÂÖ•‰∫Ü‰∏Ä‰∏™Ëß£ËÄ¶Êû∂ÊûÑÔºåÂåÖÊã¨Êé®ÁêÜÊ®°ÂûãÂíåÂàÜÂâ≤Ê®°ÂûãÔºåÊé®ÁêÜÊ®°ÂûãËß£ÈáäÁî®Êà∑ÊÑèÂõæÔºåÁîüÊàêÊòéÁ°ÆÁöÑÊé®ÁêÜÈìæÔºåÂπ∂‰∫ßÁîü‰ΩçÁΩÆÊèêÁ§∫ÔºåÈöèÂêéÁî±ÂàÜÂâ≤Ê®°ÂûãÁîüÊàêÁ≤æÁ°ÆÁöÑÂÉèÁ¥†Á∫ßÊé©Á†Å„ÄÇÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉÔºåSeg-ZeroÂú®Ê≤°ÊúâÊòéÁ°ÆÊé®ÁêÜÊï∞ÊçÆÁöÑÊÉÖÂÜµ‰∏ãÔºåÂÆûÁé∞‰∫ÜÂº∫Â§ßÁöÑÈõ∂Ê†∑Êú¨Ê≥õÂåñËÉΩÂäõÔºåÂπ∂Â±ïÁé∞Âá∫Á™ÅÂá∫ÁöÑÊµãËØïÊó∂Êé®ÁêÜËÉΩÂäõ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.03499",
            "title": "State-offset Tuning: State-based Parameter-Efficient Fine-Tuning for\n  State Space Models",
            "url": "https://huggingface.co/papers/2503.03499",
            "abstract": "State Space Models (SSMs) have emerged as efficient alternatives to Transformers, mitigating their quadratic computational cost. However, the application of Parameter-Efficient Fine-Tuning (PEFT) methods to SSMs remains largely unexplored. In particular, prompt-based methods like Prompt Tuning and Prefix-Tuning, which are widely used in Transformers, do not perform well on SSMs. To address this, we propose state-based methods as a superior alternative to prompt-based methods. This new family of methods naturally stems from the architectural characteristics of SSMs. State-based methods adjust state-related features directly instead of depending on external prompts. Furthermore, we introduce a novel state-based PEFT method: State-offset Tuning. At every timestep, our method directly affects the state at the current step, leading to more effective adaptation. Through extensive experiments across diverse datasets, we demonstrate the effectiveness of our method. Code is available at https://github.com/furiosa-ai/ssm-state-tuning.",
            "score": 2,
            "issue_id": 2630,
            "pub_date": "2025-03-05",
            "pub_date_card": {
                "ru": "5 –º–∞—Ä—Ç–∞",
                "en": "March 5",
                "zh": "3Êúà5Êó•"
            },
            "hash": "80ab6abd822f2976",
            "authors": [
                "Wonjun Kang",
                "Kevin Galim",
                "Yuchen Zeng",
                "Minjae Lee",
                "Hyung Il Koo",
                "Nam Ik Cho"
            ],
            "affiliations": [
                "UW-Madison"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.03499.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#optimization"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Ç–æ–Ω–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–µ–π –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ —Å–æ—Å—Ç–æ—è–Ω–∏–π –±–µ–∑ –ø—Ä–æ–º–ø—Ç–æ–≤",
                    "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–µ –º–æ–¥–µ–ª–µ–π –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ —Å–æ—Å—Ç–æ—è–Ω–∏–π (SSM) –≤ –º–∞—à–∏–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥—ã, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è—Ö, –∫–∞–∫ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—É –º–µ—Ç–æ–¥–∞–º, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–º –Ω–∞ –ø—Ä–æ–º–ø—Ç–∞—Ö, –∫–æ—Ç–æ—Ä—ã–µ —Ö–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞—é—Ç –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤, –Ω–æ –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã –¥–ª—è SSM. –û–Ω–∏ –≤–≤–æ–¥—è—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º State-offset Tuning, –∫–æ—Ç–æ—Ä—ã–π –Ω–∞–ø—Ä—è–º—É—é –≤–ª–∏—è–µ—Ç –Ω–∞ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –∫–∞–∂–¥–æ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–º —à–∞–≥–µ. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞."
                },
                "en": {
                    "title": "Revolutionizing Fine-Tuning with State-Based Methods for SSMs",
                    "desc": "This paper discusses the limitations of using traditional prompt-based fine-tuning methods on State Space Models (SSMs), which are more efficient than Transformers. The authors propose a new approach called state-based methods that leverage the unique structure of SSMs to improve performance. Specifically, they introduce State-offset Tuning, a novel parameter-efficient fine-tuning technique that modifies the state at each timestep directly. Experimental results show that this method outperforms existing prompt-based techniques, highlighting its effectiveness in adapting SSMs for various tasks."
                },
                "zh": {
                    "title": "Âü∫‰∫éÁä∂ÊÄÅÁöÑÂæÆË∞ÉÔºöË∂ÖË∂äÊèêÁ§∫ÁöÑÊñπÊ≥ï",
                    "desc": "Áä∂ÊÄÅÁ©∫Èó¥Ê®°ÂûãÔºàSSMsÔºâ‰Ωú‰∏∫ÂèòÊç¢Âô®ÁöÑÈ´òÊïàÊõø‰ª£ÊñπÊ°àÔºåËÉΩÂ§üÂáèËΩªÂÖ∂‰∫åÊ¨°ËÆ°ÁÆóÊàêÊú¨„ÄÇÁÑ∂ËÄåÔºåÂèÇÊï∞È´òÊïàÂæÆË∞ÉÔºàPEFTÔºâÊñπÊ≥ïÂú®SSMs‰∏äÁöÑÂ∫îÁî®‰ªçÁÑ∂Êú™Ë¢´ÂÖÖÂàÜÊé¢Á¥¢„ÄÇÊàë‰ª¨ÊèêÂá∫Âü∫‰∫éÁä∂ÊÄÅÁöÑÊñπÊ≥ïÔºå‰Ωú‰∏∫‰ºò‰∫éÂü∫‰∫éÊèêÁ§∫ÁöÑÊñπÊ≥ïÁöÑÊñ∞ÈÄâÊã©ÔºåËøô‰∫õÊñπÊ≥ïÁõ¥Êé•Ë∞ÉÊï¥‰∏éÁä∂ÊÄÅÁõ∏ÂÖ≥ÁöÑÁâπÂæÅÔºåËÄå‰∏çÊòØ‰æùËµñÂ§ñÈÉ®ÊèêÁ§∫„ÄÇÊàë‰ª¨ËøòÂºïÂÖ•‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂü∫‰∫éÁä∂ÊÄÅÁöÑPEFTÊñπÊ≥ïÔºöÁä∂ÊÄÅÂÅèÁßªÂæÆË∞ÉÔºåËÉΩÂ§üÂú®ÊØè‰∏™Êó∂Èó¥Ê≠•Áõ¥Êé•ÂΩ±ÂìçÂΩìÂâçÁä∂ÊÄÅÔºå‰ªéËÄåÂÆûÁé∞Êõ¥ÊúâÊïàÁöÑÈÄÇÂ∫î„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.03511",
            "title": "NeuGrasp: Generalizable Neural Surface Reconstruction with Background\n  Priors for Material-Agnostic Object Grasp Detection",
            "url": "https://huggingface.co/papers/2503.03511",
            "abstract": "Robotic grasping in scenes with transparent and specular objects presents great challenges for methods relying on accurate depth information. In this paper, we introduce NeuGrasp, a neural surface reconstruction method that leverages background priors for material-agnostic grasp detection. NeuGrasp integrates transformers and global prior volumes to aggregate multi-view features with spatial encoding, enabling robust surface reconstruction in narrow and sparse viewing conditions. By focusing on foreground objects through residual feature enhancement and refining spatial perception with an occupancy-prior volume, NeuGrasp excels in handling objects with transparent and specular surfaces. Extensive experiments in both simulated and real-world scenarios show that NeuGrasp outperforms state-of-the-art methods in grasping while maintaining comparable reconstruction quality. More details are available at https://neugrasp.github.io/.",
            "score": 0,
            "issue_id": 2630,
            "pub_date": "2025-03-05",
            "pub_date_card": {
                "ru": "5 –º–∞—Ä—Ç–∞",
                "en": "March 5",
                "zh": "3Êúà5Êó•"
            },
            "hash": "ba660b9e0f676df1",
            "authors": [
                "Qingyu Fan",
                "Yinghao Cai",
                "Chao Li",
                "Wenzhe He",
                "Xudong Zheng",
                "Tao Lu",
                "Bin Liang",
                "Shuo Wang"
            ],
            "affiliations": [
                "Qiyuan Lab",
                "School of Artificial Intelligence, University of Chinese Academy of Sciences",
                "State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.03511.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#agents",
                    "#architecture",
                    "#cv"
                ],
                "emoji": "ü§ñ",
                "ru": {
                    "title": "NeuGrasp: –Ω–µ–π—Ä–æ–Ω–Ω—ã–π –∑–∞—Ö–≤–∞—Ç –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–µ–π",
                    "desc": "NeuGrasp - —ç—Ç–æ –Ω–µ–π—Ä–æ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–∏ –¥–ª—è –∑–∞—Ö–≤–∞—Ç–∞ –æ–±—ä–µ–∫—Ç–æ–≤ —Å –ø—Ä–æ–∑—Ä–∞—á–Ω—ã–º–∏ –∏ –∑–µ—Ä–∫–∞–ª—å–Ω—ã–º–∏ –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç—è–º–∏. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –∏ –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –ø—Ä–∏–æ—Ä—ã –¥–ª—è –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –º—É–ª—å—Ç–∏—Ä–∞–∫—É—Ä—Å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–º –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ–º. NeuGrasp —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –æ–±—ä–µ–∫—Ç–∞—Ö –ø–µ—Ä–µ–¥–Ω–µ–≥–æ –ø–ª–∞–Ω–∞ –∏ —É—Ç–æ—á–Ω—è–µ—Ç –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ —Å –ø–æ–º–æ—â—å—é –æ–±—ä–µ–º–∞ –ø—Ä–∏–æ—Ä–æ–≤ –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω–æ—Å—Ç–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ NeuGrasp –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã –≤ –∑–∞–¥–∞—á–µ –∑–∞—Ö–≤–∞—Ç–∞ –æ–±—ä–µ–∫—Ç–æ–≤."
                },
                "en": {
                    "title": "NeuGrasp: Mastering Grasping with Transparent and Specular Objects",
                    "desc": "This paper presents NeuGrasp, a novel approach for robotic grasping that effectively deals with transparent and specular objects, which are challenging for traditional depth-based methods. NeuGrasp utilizes a neural surface reconstruction technique that incorporates background priors to enhance grasp detection without being limited by material properties. By combining transformers and global prior volumes, it aggregates multi-view features with spatial encoding, improving surface reconstruction even in difficult viewing conditions. The method demonstrates superior performance in grasping tasks compared to existing techniques, while also achieving high-quality surface reconstruction in both simulated and real-world environments."
                },
                "zh": {
                    "title": "NeuGraspÔºöÈÄèÊòéÁâ©‰ΩìÊäìÂèñÁöÑÊñ∞Á™ÅÁ†¥",
                    "desc": "Êú¨ËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫NeuGraspÁöÑÁ•ûÁªèË°®Èù¢ÈáçÂª∫ÊñπÊ≥ïÔºåÊó®Âú®Ëß£ÂÜ≥ÈÄèÊòéÂíåÈïúÈù¢Áâ©‰ΩìÊäìÂèñ‰∏≠ÁöÑÊåëÊàò„ÄÇËØ•ÊñπÊ≥ïÂà©Áî®ËÉåÊôØÂÖàÈ™åËøõË°åÊùêÊñôÊó†ÂÖ≥ÁöÑÊäìÂèñÊ£ÄÊµãÔºåÁªìÂêà‰∫ÜÂèòÊç¢Âô®ÂíåÂÖ®Â±ÄÂÖàÈ™å‰ΩìÁßØÔºå‰ª•ËÅöÂêàÂ§öËßÜËßíÁâπÂæÅÂπ∂ËøõË°åÁ©∫Èó¥ÁºñÁ†Å„ÄÇNeuGraspÈÄöËøáÊÆãÂ∑ÆÁâπÂæÅÂ¢ûÂº∫ËÅöÁÑ¶‰∫éÂâçÊôØÁâ©‰ΩìÔºåÂπ∂Âà©Áî®Âç†Áî®ÂÖàÈ™å‰ΩìÁßØÊù•ÊîπÂñÑÁ©∫Èó¥ÊÑüÁü•ÔºåËÉΩÂ§üÊúâÊïàÂ§ÑÁêÜÈÄèÊòéÂíåÈïúÈù¢Ë°®Èù¢ÁöÑÁâ©‰Ωì„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåNeuGraspÂú®ÊäìÂèñÊÄßËÉΩ‰∏ä‰ºò‰∫éÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊñπÊ≥ïÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜÁõ∏‰ººÁöÑÈáçÂª∫Ë¥®Èáè„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.02199",
            "title": "Words or Vision: Do Vision-Language Models Have Blind Faith in Text?",
            "url": "https://huggingface.co/papers/2503.02199",
            "abstract": "Vision-Language Models (VLMs) excel in integrating visual and textual information for vision-centric tasks, but their handling of inconsistencies between modalities is underexplored. We investigate VLMs' modality preferences when faced with visual data and varied textual inputs in vision-centered settings. By introducing textual variations to four vision-centric tasks and evaluating ten Vision-Language Models (VLMs), we discover a ``blind faith in text'' phenomenon: VLMs disproportionately trust textual data over visual data when inconsistencies arise, leading to significant performance drops under corrupted text and raising safety concerns. We analyze factors influencing this text bias, including instruction prompts, language model size, text relevance, token order, and the interplay between visual and textual certainty. While certain factors, such as scaling up the language model size, slightly mitigate text bias, others like token order can exacerbate it due to positional biases inherited from language models. To address this issue, we explore supervised fine-tuning with text augmentation and demonstrate its effectiveness in reducing text bias. Additionally, we provide a theoretical analysis suggesting that the blind faith in text phenomenon may stem from an imbalance of pure text and multi-modal data during training. Our findings highlight the need for balanced training and careful consideration of modality interactions in VLMs to enhance their robustness and reliability in handling multi-modal data inconsistencies.",
            "score": 0,
            "issue_id": 2630,
            "pub_date": "2025-03-04",
            "pub_date_card": {
                "ru": "4 –º–∞—Ä—Ç–∞",
                "en": "March 4",
                "zh": "3Êúà4Êó•"
            },
            "hash": "a354f8de058f0f84",
            "authors": [
                "Ailin Deng",
                "Tri Cao",
                "Zhirui Chen",
                "Bryan Hooi"
            ],
            "affiliations": [
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.02199.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#alignment",
                    "#training",
                    "#multimodal"
                ],
                "emoji": "üîç",
                "ru": {
                    "title": "–û–ø–∞—Å–Ω–æ—Å—Ç—å —Å–ª–µ–ø–æ–π –≤–µ—Ä—ã –≤ —Ç–µ–∫—Å—Ç: –ø—Ä–æ–±–ª–µ–º–∞ –∏ —Ä–µ—à–µ–Ω–∏—è –¥–ª—è VLM –º–æ–¥–µ–ª–µ–π",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –º–æ–¥–µ–ª–∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ (VLM) —Å–∫–ª–æ–Ω–Ω—ã —á—Ä–µ–∑–º–µ—Ä–Ω–æ –¥–æ–≤–µ—Ä—è—Ç—å —Ç–µ–∫—Å—Ç–æ–≤—ã–º –¥–∞–Ω–Ω—ã–º –ø—Ä–∏ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ –º–µ–∂–¥—É –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∏ —Ç–µ–∫—Å—Ç–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π. –≠—Ç–æ —è–≤–ª–µ–Ω–∏–µ, –Ω–∞–∑–≤–∞–Ω–Ω–æ–µ '—Å–ª–µ–ø–æ–π –≤–µ—Ä–æ–π –≤ —Ç–µ–∫—Å—Ç', –º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–º—É —Å–Ω–∏–∂–µ–Ω–∏—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –∏ –≤—ã–∑—ã–≤–∞–µ—Ç –æ–ø–∞—Å–µ–Ω–∏—è –ø–æ –ø–æ–≤–æ–¥—É –∏—Ö –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏. –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç —Ñ–∞–∫—Ç–æ—Ä—ã, –≤–ª–∏—è—é—â–∏–µ –Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ —Å–º–µ—â–µ–Ω–∏–µ, –∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥—ã –µ–≥–æ —É–º–µ–Ω—å—à–µ–Ω–∏—è, –≤–∫–ª—é—á–∞—è –¥–æ–æ–±—É—á–µ–Ω–∏–µ —Å –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π —Ç–µ–∫—Å—Ç–∞. –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç, —á—Ç–æ –ø—Ä–æ–±–ª–µ–º–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å —Å–≤—è–∑–∞–Ω–∞ —Å –¥–∏—Å–±–∞–ª–∞–Ω—Å–æ–º —á–∏—Å—Ç–æ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π."
                },
                "en": {
                    "title": "Balancing Vision and Text: Overcoming Bias in Vision-Language Models",
                    "desc": "This paper investigates how Vision-Language Models (VLMs) manage inconsistencies between visual and textual information in tasks that rely heavily on vision. The authors identify a phenomenon called 'blind faith in text,' where VLMs tend to rely more on textual data than visual data when faced with conflicting inputs, which can lead to performance issues. They analyze various factors that contribute to this text bias, such as the size of the language model and the order of tokens in the text. To mitigate this bias, the paper proposes supervised fine-tuning with text augmentation and emphasizes the importance of balanced training for improving the reliability of VLMs in multi-modal contexts."
                },
                "zh": {
                    "title": "Âπ≥Ë°°ËÆ≠ÁªÉÔºåÊèêÂçáËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑÂèØÈù†ÊÄß",
                    "desc": "ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÂú®Â§ÑÁêÜËßÜËßâÂíåÊñáÊú¨‰ø°ÊÅØÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂÆÉ‰ª¨Âú®Èù¢ÂØπÊ®°ÊÄÅ‰∏ç‰∏ÄËá¥Êó∂ÁöÑË°®Áé∞Â∞öÊú™ÂæóÂà∞ÂÖÖÂàÜÁ†îÁ©∂„ÄÇÊàë‰ª¨Êé¢ËÆ®‰∫ÜVLMsÂú®ËßÜËßâÊï∞ÊçÆÂíå‰∏çÂêåÊñáÊú¨ËæìÂÖ•‰∏ãÁöÑÊ®°ÊÄÅÂÅèÂ•ΩÔºåÂèëÁé∞‰∫Ü‚ÄúÂØπÊñáÊú¨ÁöÑÁõ≤ÁõÆ‰ø°‰ªª‚ÄùÁé∞Ë±°ÔºöÂΩìÂá∫Áé∞‰∏ç‰∏ÄËá¥Êó∂ÔºåVLMsËøáÂ∫¶‰æùËµñÊñáÊú¨Êï∞ÊçÆÔºåÂØºËá¥ÊÄßËÉΩÊòæËëó‰∏ãÈôç„ÄÇÊàë‰ª¨ÂàÜÊûê‰∫ÜÂΩ±ÂìçËøôÁßçÊñáÊú¨ÂÅèËßÅÁöÑÂõ†Á¥†ÔºåÂåÖÊã¨Êåá‰ª§ÊèêÁ§∫„ÄÅËØ≠Ë®ÄÊ®°ÂûãÂ§ßÂ∞è„ÄÅÊñáÊú¨Áõ∏ÂÖ≥ÊÄß„ÄÅÊ†áËÆ∞È°∫Â∫è‰ª•ÂèäËßÜËßâÂíåÊñáÊú¨Á°ÆÂÆöÊÄß‰πãÈó¥ÁöÑÁõ∏‰∫í‰ΩúÁî®„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨Êé¢Á¥¢‰∫ÜÂ∏¶ÊúâÊñáÊú¨Â¢ûÂº∫ÁöÑÁõëÁù£ÂæÆË∞ÉÔºåÂπ∂ËØÅÊòéÂÖ∂Âú®ÂáèÂ∞ëÊñáÊú¨ÂÅèËßÅÊñπÈù¢ÁöÑÊúâÊïàÊÄß„ÄÇ"
                }
            }
        }
    ],
    "link_prev": "2025-03-10.html",
    "link_next": "2025-03-12.html",
    "link_month": "2025-03.html",
    "short_date_prev": {
        "ru": "10.03",
        "en": "03/10",
        "zh": "3Êúà10Êó•"
    },
    "short_date_next": {
        "ru": "12.03",
        "en": "03/12",
        "zh": "3Êúà12Êó•"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 1,
        "#cv": 2,
        "#rl": 2,
        "#rlhf": 1,
        "#rag": 1,
        "#plp": 1,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 4,
        "#healthcare": 0,
        "#training": 3,
        "#robotics": 1,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 4,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "ÊúÄËøë‰∫∫Á±ªÂÅèÂ•ΩÂØπÈΩêÁöÑËøõÂ±ïÊòæËëóÊèêÂçá‰∫ÜÂ§öÊ®°ÊÄÅÁîüÊàêÂíåÁêÜËß£„ÄÇÂÖ≥ÈîÆÊñπÊ≥ïÊòØËÆ≠ÁªÉÂ•ñÂä±Ê®°ÂûãÊåáÂØºÂÅèÂ•Ω‰ºòÂåñ„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÊ®°ÂûãÈÄöÂ∏∏ÊòØ‰ªªÂä°ÁâπÂÆöÁöÑÔºåÈôêÂà∂‰∫ÜÂÆÉ‰ª¨Âú®‰∏çÂêåËßÜËßâÂ∫îÁî®‰∏≠ÁöÑÈÄÇÂ∫îÊÄß„ÄÇÊàë‰ª¨ËÆ§‰∏∫ÔºåËÅîÂêàÂ≠¶‰π†ËØÑ‰º∞Â§ö‰ªªÂä°ÂèØËÉΩ‰∫ßÁîüÂçèÂêåÊïàÂ∫îÔºåÊîπÂñÑÂõæÂÉèÁêÜËß£ÂíåÁîüÊàêËØÑ‰º∞ÔºåÂπ∂ÈÄöËøáÊõ¥Â•ΩÁöÑÂ∏ßÂàÜÊûêÊèêÂçáËßÜÈ¢ëËØÑ‰º∞„ÄÇÂõ†Ê≠§ÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜUnifiedRewardÔºåÁ¨¨‰∏Ä‰∏™Áî®‰∫éÂ§öÊ®°ÊÄÅÁêÜËß£ÂíåÁîüÊàêËØÑ‰º∞ÁöÑÁªü‰∏ÄÂ•ñÂä±Ê®°Âûã„ÄÇ",
        "title": "Unified Reward Model for Multimodal Understanding and Generation",
        "pinyin": "ÊúÄËøë‰∫∫Á±ªÂÅèÂ•ΩÂØπÈΩêÁöÑËøõÂ±ïÊòæËëóÊèêÂçá‰∫ÜÂ§öÊ®°ÊÄÅÁîüÊàêÂíåÁêÜËß£„ÄÇÂÖ≥ÈîÆÊñπÊ≥ïÊòØËÆ≠ÁªÉÂ•ñÂä±Ê®°ÂûãÊåáÂØºÂÅèÂ•Ω‰ºòÂåñ„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÊ®°ÂûãÈÄöÂ∏∏ÊòØ‰ªªÂä°ÁâπÂÆöÁöÑÔºåÈôêÂà∂‰∫ÜÂÆÉ‰ª¨Âú®‰∏çÂêåËßÜËßâÂ∫îÁî®‰∏≠ÁöÑÈÄÇÂ∫îÊÄß„ÄÇÊàë‰ª¨ËÆ§‰∏∫ÔºåËÅîÂêàÂ≠¶‰π†ËØÑ‰º∞Â§ö‰ªªÂä°ÂèØËÉΩ‰∫ßÁîüÂçèÂêåÊïàÂ∫îÔºåÊîπÂñÑÂõæÂÉèÁêÜËß£ÂíåÁîüÊàêËØÑ‰º∞ÔºåÂπ∂ÈÄöËøáÊõ¥Â•ΩÁöÑÂ∏ßÂàÜÊûêÊèêÂçáËßÜÈ¢ëËØÑ‰º∞„ÄÇÂõ†Ê≠§ÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜUnifiedRewardÔºåÁ¨¨‰∏Ä‰∏™Áî®‰∫éÂ§öÊ®°ÊÄÅÁêÜËß£ÂíåÁîüÊàêËØÑ‰º∞ÁöÑÁªü‰∏ÄÂ•ñÂä±Ê®°Âûã„ÄÇ\n\nZu√¨j√¨n r√©nl√®i piƒÅnh√†o du√¨q√≠ de j√¨nzh«én xi«énzh√π t√≠shƒìng le du≈ç m√≥sh√π shƒìngch√©ng h√© l«êjiƒõ. Gu«énji√†n fƒÅngf«é sh√¨ x√πnli√†n ji«éngl√¨ m√≥x√≠ng zh«êd«éo piƒÅnh√†o y≈çuhu√†. R√°n'√©r, xi√†ny«íu m√≥x√≠ng t≈çngch√°ng sh√¨ r√®nw√π t√®d√¨ng de, xi√†nzh√¨ le tƒÅmen z√†i b√πt√≥ng sh√¨juƒì y√¨ngy√≤ng zh≈çng de sh√¨y√¨ngx√¨ng. W«ímen r√®nw√©i, li√°nh√© xu√©x√≠ p√≠ngji√† du≈ç r√®nw√π kƒõn√©ng ch«énshƒìng xi√©t√≥ng xi√†oy√¨ng, g«éish√†n t√∫xi√†ng l«êjiƒõ h√© shƒìngch√©ng p√≠ngji√†, b√¨ng t≈çnggu√≤ g√®ng h«éo de zhƒìn fƒìnxi t√≠shƒìng sh√¨p√≠n p√≠ngji√†. Yƒ´nc«ê, bƒõnw√©n t√≠ch≈´ le UnifiedReward, d√¨-yƒ´g√® y√≤ngy√∫ du≈ç m√≥sh√π l«êjiƒõ h√© shƒìngch√©ng p√≠ngji√† de t«íngyƒ´ ji«éngl√¨ m√≥x√≠ng.",
        "vocab": "[\n    {\"word\": \"ÂÅèÂ•Ω\", \"pinyin\": \"piƒÅn h√†o\", \"trans\": \"preference\"},\n    {\"word\": \"ÂØπÈΩê\", \"pinyin\": \"du√¨ q√≠\", \"trans\": \"alignment\"},\n    {\"word\": \"ËøõÂ±ï\", \"pinyin\": \"j√¨n zh«én\", \"trans\": \"progress\"},\n    {\"word\": \"ÊòæËëó\", \"pinyin\": \"xi«én zh√π\", \"trans\": \"significant\"},\n    {\"word\": \"ÊèêÂçá\", \"pinyin\": \"t√≠ shƒìng\", \"trans\": \"improve\"},\n    {\"word\": \"Â§öÊ®°ÊÄÅ\", \"pinyin\": \"du≈ç m√≥ t√†i\", \"trans\": \"multimodal\"},\n    {\"word\": \"ÁîüÊàê\", \"pinyin\": \"shƒìng ch√©ng\", \"trans\": \"generation\"},\n    {\"word\": \"ÁêÜËß£\", \"pinyin\": \"l«ê jiƒõ\", \"trans\": \"understanding\"},\n    {\"word\": \"ÂÖ≥ÈîÆ\", \"pinyin\": \"gu«én ji√†n\", \"trans\": \"key\"},\n    {\"word\": \"ÊñπÊ≥ï\", \"pinyin\": \"fƒÅng f«é\", \"trans\": \"method\"},\n    {\"word\": \"ËÆ≠ÁªÉ\", \"pinyin\": \"x√πn li√†n\", \"trans\": \"training\"},\n    {\"word\": \"Â•ñÂä±\", \"pinyin\": \"ji«éng l√¨\", \"trans\": \"reward\"},\n    {\"word\": \"Ê®°Âûã\", \"pinyin\": \"m√≥ x√≠ng\", \"trans\": \"model\"},\n    {\"word\": \"ÊåáÂØº\", \"pinyin\": \"zh«ê d«éo\", \"trans\": \"guide\"},\n    {\"word\": \"‰ºòÂåñ\", \"pinyin\": \"y≈çu hu√†\", \"trans\": \"optimization\"},\n    {\"word\": \"‰ªªÂä°\", \"pinyin\": \"r√®n wu\", \"trans\": \"task\"},\n    {\"word\": \"ÁâπÂÆö\", \"pinyin\": \"t√® d√¨ng\", \"trans\": \"specific\"},\n    {\"word\": \"ÈôêÂà∂\", \"pinyin\": \"xi√†n zh√¨\", \"trans\": \"limit\"},\n    {\"word\": \"ÈÄÇÂ∫îÊÄß\", \"pinyin\": \"sh√¨ y√¨ng x√¨ng\", \"trans\": \"adaptability\"},\n    {\"word\": \"ËßÜËßâ\", \"pinyin\": \"sh√¨ ju√©\", \"trans\": \"visual\"},\n    {\"word\": \"Â∫îÁî®\", \"pinyin\": \"y√¨ng y√≤ng\", \"trans\": \"application\"},\n    {\"word\": \"ËÅîÂêà\", \"pinyin\": \"li√°n h√©\", \"trans\": \"joint\"},\n    {\"word\": \"Â≠¶‰π†\", \"pinyin\": \"xu√© x√≠\", \"trans\": \"learning\"},\n    {\"word\": \"ËØÑ‰º∞\", \"pinyin\": \"p√≠ng g≈´\", \"trans\": \"evaluation\"},\n    {\"word\": \"ÂçèÂêå\", \"pinyin\": \"xi√© t√≥ng\", \"trans\": \"synergy\"},\n    {\"word\": \"ÊîπÂñÑ\", \"pinyin\": \"g«éi sh√†n\", \"trans\": \"improve\"},\n    {\"word\": \"ÂõæÂÉè\", \"pinyin\": \"t√∫ xi√†ng\", \"trans\": \"image\"},\n    {\"word\": \"Â∏ß\", \"pinyin\": \"zhƒìn\", \"trans\": \"frame\"},\n    {\"word\": \"ÂàÜÊûê\", \"pinyin\": \"fƒìn xƒ´\", \"trans\": \"analysis\"},\n    {\"word\": \"ËßÜÈ¢ë\", \"pinyin\": \"sh√¨ p√≠n\", \"trans\": \"video\"},\n    {\"word\": \"ÊèêÂá∫\", \"pinyin\": \"t√≠ ch≈´\", \"trans\": \"propose\"},\n    {\"word\": \"Áªü‰∏Ä\", \"pinyin\": \"t«íng yƒ´\", \"trans\": \"unified\"},\n    {\"word\": \"Á¨¨‰∏Ä‰∏™\", \"pinyin\": \"d√¨ yƒ´ g√®\", \"trans\": \"first\"}\n]",
        "trans": "Recent advancements in human preference alignment have significantly enhanced multimodal generation and understanding. The key method involves training reward models to guide preference optimization. However, existing models are typically task-specific, limiting their adaptability across different visual applications. We believe that jointly learning to evaluate multiple tasks may produce synergistic effects, improving image understanding and generation evaluation, and enhancing video evaluation through better frame analysis. Therefore, this paper introduces UnifiedReward, the first unified reward model for multimodal understanding and generation evaluation.",
        "update_ts": "2025-03-10 09:10"
    }
}