
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 23 papers. October 7.</title>
    <link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #0989eacf;
            --secondary-color: #fff;
            --background-color: #f5f5f5;
            --text-color: #333333;
            --header-color: #0989eacf;
            --body-color: #f5f5f5;
            --menu-color: #002370;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 2em;
            padding: 10px 0 20px 0;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.tags {
            color: #555;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);
            transition: background-color 0.2s ease;
            display: flex;
            flex-direction: column;
            position: relative;
        }
        .article-content {
            padding: 1.5em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
        }
        body.dark-theme>div>main>article {
            background-color: #444;
        }
        body.light-theme>div>main>article {
            background-color: #fff;
        }
        body.dark-theme>div>main>article:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
        }
        .pub-date {
            font-size: 0.9em;
            margin-bottom: 0.8em;
            font-weight: 300;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 1em;
            position: absolute;
            bottom: 10px;
            font-weight: 300;
            font-family: 'Roboto Slab';
        }
        .background-digit {
            position: absolute;
            bottom: -20px;
            right: -10px;
            font-size: 12em;
            font-weight: bold;
            color: rgba(0, 0, 0, 0.03);
            z-index: 0;
            line-height: 1;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 80px;
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        a:hover {
            color: #e73838;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            padding: 1em 0;
            margin-top: 2em;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: fixed;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
        }
        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
        }
        .update-info-container {
            flex: 1;
        }
        .sort-container {
            flex: 2;
        }
        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
                display: block;
                margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .category-toggle {
            display: inline-block;
            margin-bottom: 10px;
            margin-top: 15px;
            cursor: pointer;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 0px;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }
        
        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        body.light-theme>div>main>article.xfae9004e0f12e4fc { background: url("https://hfday.ru/img/20241007/fae9004e0f12e4fc.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.xfae9004e0f12e4fc:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.xfae9004e0f12e4fc { background: url("https://hfday.ru/img/20241007/fae9004e0f12e4fc.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.xfae9004e0f12e4fc:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.xd3da276d1028f171 { background: url("https://hfday.ru/img/20241003/d3da276d1028f171.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.xd3da276d1028f171:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.xd3da276d1028f171 { background: url("https://hfday.ru/img/20241003/d3da276d1028f171.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.xd3da276d1028f171:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.xfd005deb7206d4fe { background: url("https://hfday.ru/img/20241006/fd005deb7206d4fe.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.xfd005deb7206d4fe:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.xfd005deb7206d4fe { background: url("https://hfday.ru/img/20241006/fd005deb7206d4fe.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.xfd005deb7206d4fe:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x9138501c89f38809 { background: url("https://hfday.ru/img/20241003/9138501c89f38809.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x9138501c89f38809:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x9138501c89f38809 { background: url("https://hfday.ru/img/20241003/9138501c89f38809.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x9138501c89f38809:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x13152fb25949544e { background: url("https://hfday.ru/img/20241007/13152fb25949544e.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x13152fb25949544e:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x13152fb25949544e { background: url("https://hfday.ru/img/20241007/13152fb25949544e.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x13152fb25949544e:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.xe1f883a68716278f { background: url("https://hfday.ru/img/20241007/e1f883a68716278f.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.xe1f883a68716278f:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.xe1f883a68716278f { background: url("https://hfday.ru/img/20241007/e1f883a68716278f.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.xe1f883a68716278f:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.xf332523f23a7857e { background: url("https://hfday.ru/img/20241007/f332523f23a7857e.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.xf332523f23a7857e:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.xf332523f23a7857e { background: url("https://hfday.ru/img/20241007/f332523f23a7857e.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.xf332523f23a7857e:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.xc91b020a4bdfc21e { background: url("https://hfday.ru/img/20241007/c91b020a4bdfc21e.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.xc91b020a4bdfc21e:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.xc91b020a4bdfc21e { background: url("https://hfday.ru/img/20241007/c91b020a4bdfc21e.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.xc91b020a4bdfc21e:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x4bfc87467c16c280 { background: url("https://hfday.ru/img/20241006/4bfc87467c16c280.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x4bfc87467c16c280:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x4bfc87467c16c280 { background: url("https://hfday.ru/img/20241006/4bfc87467c16c280.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x4bfc87467c16c280:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x8c4b74044ea31d8d { background: url("https://hfday.ru/img/20241007/8c4b74044ea31d8d.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x8c4b74044ea31d8d:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x8c4b74044ea31d8d { background: url("https://hfday.ru/img/20241007/8c4b74044ea31d8d.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x8c4b74044ea31d8d:hover { background-color: rgba(60,60,60,0.92) !important;}

        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .sort-container {
                margin-top: 0px;
                text-align: left;
                width: 100%;
            .sort-dropdown {
                float: right;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">7 октября</span> | <span id="title-articles-count">23 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item" id="nav-prev"><a href="/d/2024-10-04.html">⬅️ <span id="prev-date">04.10</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-10-08.html">➡️ <span id="next-date">08.10</span></a></span>
            <!--<span class="nav-item" id="nav-weekly">Топ за неделю</span>
            <span class="nav-item" id="nav-weekly">Топ за месяц</span>-->
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="category-toggle">
            <div class="svg-container">
                <span id="category-toggle">🏷️ Фильтр</span>
                <svg height="3" width="200">
                    <line x1="0" y1="0" x2="200" y2="0" 
                        stroke="black" 
                        stroke-width="2" 
                        stroke-dasharray="3, 3" />
                </svg>
            </div>
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'};
        let feedDateNext = {'ru': '08.10', 'en': '10/08', 'zh': '10月8日'};
        let feedDatePrev = {'ru': '04.10', 'en': '10/04', 'zh': '10月4日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'Статья от ', 'en': 'Published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2410.05258', 'title': 'Differential Transformer', 'url': 'https://huggingface.co/papers/2410.05258', 'abstract': 'Transformer tends to overallocate attention to irrelevant context. In this work, we introduce Diff Transformer, which amplifies attention to the relevant context while canceling noise. Specifically, the differential attention mechanism calculates attention scores as the difference between two separate softmax attention maps. The subtraction cancels noise, promoting the emergence of sparse attention patterns. Experimental results on language modeling show that Diff Transformer outperforms Transformer in various settings of scaling up model size and training tokens. More intriguingly, it offers notable advantages in practical applications, such as long-context modeling, key information retrieval, hallucination mitigation, in-context learning, and reduction of activation outliers. By being less distracted by irrelevant context, Diff Transformer can mitigate hallucination in question answering and text summarization. For in-context learning, Diff Transformer not only enhances accuracy but is also more robust to order permutation, which was considered as a chronic robustness issue. The results position Diff Transformer as a highly effective and promising architecture to advance large language models.', 'score': 49, 'issue_id': 14, 'pub_date': '2024-10-07', 'pub_date_ru': '7 октября', 'data': {'categories': ['#architecture', '#hallucinations', '#inference', '#long_context'], 'emoji': '🔍', 'ru': {'title': 'Diff Transformer: точнее фокусируемся на важном, отсекая шум', 'desc': 'Статья представляет новую архитектуру Diff Transformer, которая улучшает внимание к релевантному контексту и подавляет шум. Механизм дифференциального внимания вычисляет оценки внимания как разницу между двумя отдельными картами внимания softmax. Эксперименты показывают, что Diff Transformer превосходит обычный Transformer в различных задачах обработки естественного языка. Модель демонстрирует преимущества в моделировании длинного контекста, извлечении ключевой информации, снижении галлюцинаций и обучении в контексте.'}, 'en': {'title': 'Diff Transformer: Sharpening Focus, Reducing Noise in Language Models', 'desc': 'The paper introduces Diff Transformer, a new model that improves attention mechanisms by focusing more on relevant information and reducing noise. It uses a differential attention mechanism that calculates attention scores by subtracting two softmax attention maps, leading to clearer and more focused attention patterns. This approach enhances performance in tasks like long-context modeling and information retrieval, and it reduces issues like hallucination in text generation. Diff Transformer also shows improved robustness in in-context learning, making it a promising advancement for large language models.'}, 'zh': {'title': 'Diff Transformer：更专注的注意力机制', 'desc': 'Transformer模型有时会过多关注不相关的上下文。Diff Transformer通过差分注意力机制，增强对相关上下文的关注，同时消除噪声。实验表明，Diff Transformer在语言建模中表现优于传统Transformer，尤其在长上下文建模和信息检索等应用中效果显著。它还能减少幻觉现象，提高上下文学习的准确性和鲁棒性。'}}, 'hash': 'fae9004e0f12e4fc', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}}, {'id': 'https://huggingface.co/papers/2410.02707', 'title': 'LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations', 'url': 'https://huggingface.co/papers/2410.02707', 'abstract': 'Large language models (LLMs) often produce errors, including factual inaccuracies, biases, and reasoning failures, collectively referred to as "hallucinations". Recent studies have demonstrated that LLMs\' internal states encode information regarding the truthfulness of their outputs, and that this information can be utilized to detect errors. In this work, we show that the internal representations of LLMs encode much more information about truthfulness than previously recognized. We first discover that the truthfulness information is concentrated in specific tokens, and leveraging this property significantly enhances error detection performance. Yet, we show that such error detectors fail to generalize across datasets, implying that -- contrary to prior claims -- truthfulness encoding is not universal but rather multifaceted. Next, we show that internal representations can also be used for predicting the types of errors the model is likely to make, facilitating the development of tailored mitigation strategies. Lastly, we reveal a discrepancy between LLMs\' internal encoding and external behavior: they may encode the correct answer, yet consistently generate an incorrect one. Taken together, these insights deepen our understanding of LLM errors from the model\'s internal perspective, which can guide future research on enhancing error analysis and mitigation.', 'score': 25, 'issue_id': 15, 'pub_date': '2024-10-03', 'pub_date_ru': '3 октября', 'data': {'categories': ['#hallucinations', '#interpretability'], 'emoji': '🔍', 'ru': {'title': 'Раскрытие тайн внутренних представлений LLM для улучшения обнаружения ошибок', 'desc': 'Исследование показывает, что внутренние представления больших языковых моделей (LLM) содержат больше информации о достоверности генерируемых данных, чем считалось ранее. Авторы обнаружили, что эта информация сконцентрирована в определенных токенах, что позволяет значительно улучшить обнаружение ошибок. Однако детекторы ошибок не обобщаются между датасетами, что указывает на отсутствие универсального кодирования достоверности. Исследование также выявило, что LLM могут внутренне кодировать правильный ответ, но при этом генерировать неверный.'}, 'en': {'title': 'Unlocking the Hidden Truth: Enhancing Error Detection in LLMs', 'desc': "This paper explores how large language models (LLMs) encode information about the truthfulness of their outputs, which can be used to detect errors like hallucinations. The study finds that truthfulness information is concentrated in specific tokens, improving error detection, but these detectors don't generalize well across different datasets. It also shows that internal representations can predict the types of errors a model might make, helping to create specific strategies to reduce these errors. Additionally, the research highlights a gap between what LLMs internally know and what they output, as they might encode the correct answer but still produce an incorrect one."}, 'zh': {'title': '揭示大型语言模型内部的真实性编码', 'desc': '大型语言模型（LLMs）常常会产生错误，包括事实不准确、偏见和推理失败，这些统称为“幻觉”。研究表明，LLMs的内部状态包含关于其输出真实性的信息，这些信息可以用来检测错误。我们发现，真实性信息集中在特定的标记上，利用这一特性可以显著提高错误检测性能。然而，这种错误检测器在不同数据集上无法泛化，说明真实性编码不是普遍的，而是多方面的。'}}, 'hash': 'd3da276d1028f171', 'pub_date_card': {'ru': '3 октября', 'en': 'October 3', 'zh': '10月3日'}}, {'id': 'https://huggingface.co/papers/2410.04364', 'title': "VideoGuide: Improving Video Diffusion Models without Training Through a Teacher's Guide", 'url': 'https://huggingface.co/papers/2410.04364', 'abstract': "Text-to-image (T2I) diffusion models have revolutionized visual content creation, but extending these capabilities to text-to-video (T2V) generation remains a challenge, particularly in preserving temporal consistency. Existing methods that aim to improve consistency often cause trade-offs such as reduced imaging quality and impractical computational time. To address these issues we introduce VideoGuide, a novel framework that enhances the temporal consistency of pretrained T2V models without the need for additional training or fine-tuning. Instead, VideoGuide leverages any pretrained video diffusion model (VDM) or itself as a guide during the early stages of inference, improving temporal quality by interpolating the guiding model's denoised samples into the sampling model's denoising process. The proposed method brings about significant improvement in temporal consistency and image fidelity, providing a cost-effective and practical solution that synergizes the strengths of various video diffusion models. Furthermore, we demonstrate prior distillation, revealing that base models can achieve enhanced text coherence by utilizing the superior data prior of the guiding model through the proposed method. Project Page: http://videoguide2025.github.io/", 'score': 21, 'issue_id': 13, 'pub_date': '2024-10-06', 'pub_date_ru': '6 октября', 'data': {'categories': ['#diffusion', '#video'], 'emoji': '🎬', 'ru': {'title': 'VideoGuide: повышение качества генерации видео без переобучения', 'desc': 'VideoGuide - это новая система, которая улучшает временную согласованность предобученных моделей генерации видео по тексту без дополнительного обучения. Она использует предобученную модель диффузии видео в качестве гида на ранних этапах вывода, интерполируя ее образцы в процесс шумоподавления основной модели. Это значительно повышает временную согласованность и качество изображения, объединяя сильные стороны различных моделей генерации видео. Метод также демонстрирует дистилляцию приора, позволяя базовым моделям улучшить текстовую согласованность.'}, 'en': {'title': 'VideoGuide: Elevating T2V Consistency Without Compromise', 'desc': 'The paper introduces VideoGuide, a framework designed to improve the temporal consistency of text-to-video (T2V) generation models without additional training. VideoGuide uses a guiding model to enhance the denoising process of a sampling model, leading to better temporal quality and image fidelity. This approach effectively balances the trade-offs between consistency and computational efficiency. Additionally, the method allows base models to enhance text coherence by leveraging the data prior of the guiding model.'}, 'zh': {'title': 'VideoGuide：提升文本到视频生成的一致性与质量', 'desc': '这篇论文介绍了一种名为VideoGuide的新框架，用于提高文本到视频生成模型的时间一致性。现有方法在提高一致性时常常导致图像质量下降或计算时间过长，而VideoGuide无需额外训练或微调即可解决这些问题。它通过在推理的早期阶段利用预训练的视频扩散模型作为指导，改善时间质量。该方法显著提高了时间一致性和图像保真度，是一种经济实用的解决方案。'}}, 'hash': 'fd005deb7206d4fe', 'pub_date_card': {'ru': '6 октября', 'en': 'October 6', 'zh': '10月6日'}}, {'id': 'https://huggingface.co/papers/2410.02675', 'title': 'FAN: Fourier Analysis Networks', 'url': 'https://huggingface.co/papers/2410.02675', 'abstract': 'Despite the remarkable success achieved by neural networks, particularly those represented by MLP and Transformer, we reveal that they exhibit potential flaws in the modeling and reasoning of periodicity, i.e., they tend to memorize the periodic data rather than genuinely understanding the underlying principles of periodicity. However, periodicity is a crucial trait in various forms of reasoning and generalization, underpinning predictability across natural and engineered systems through recurring patterns in observations. In this paper, we propose FAN, a novel network architecture based on Fourier Analysis, which empowers the ability to efficiently model and reason about periodic phenomena. By introducing Fourier Series, the periodicity is naturally integrated into the structure and computational processes of the neural network, thus achieving a more accurate expression and prediction of periodic patterns. As a promising substitute to multi-layer perceptron (MLP), FAN can seamlessly replace MLP in various models with fewer parameters and FLOPs. Through extensive experiments, we demonstrate the effectiveness of FAN in modeling and reasoning about periodic functions, and the superiority and generalizability of FAN across a range of real-world tasks, including symbolic formula representation, time series forecasting, and language modeling.', 'score': 21, 'issue_id': 12, 'pub_date': '2024-10-03', 'pub_date_ru': '3 октября', 'data': {'categories': ['#architecture', '#math'], 'emoji': '🔄', 'ru': {'title': 'FAN: Революция в моделировании периодичности нейронными сетями', 'desc': 'Статья представляет новую архитектуру нейронной сети под названием FAN, основанную на анализе Фурье. FAN эффективно моделирует и обрабатывает периодические явления, преодолевая ограничения традиционных нейронных сетей в этой области. Авторы демонстрируют превосходство FAN над многослойным перцептроном (MLP) в различных задачах, включая представление символьных формул и прогнозирование временных рядов. FAN интегрирует периодичность в структуру сети, обеспечивая более точное выражение и предсказание периодических паттернов.'}, 'en': {'title': 'FAN: Revolutionizing Periodic Pattern Recognition with Fourier Analysis', 'desc': 'The paper identifies a limitation in traditional neural networks like MLPs and Transformers, which tend to memorize rather than understand periodic data. To address this, the authors introduce FAN, a new network architecture that uses Fourier Analysis to better model periodic phenomena. FAN integrates Fourier Series into its structure, allowing it to predict periodic patterns more accurately with fewer parameters. Experiments show that FAN outperforms traditional models in tasks like time series forecasting and language modeling.'}, 'zh': {'title': 'FAN：用傅里叶分析重塑周期性建模', 'desc': '这篇论文指出，尽管神经网络如MLP和Transformer取得了显著成功，但在处理周期性数据时存在潜在缺陷，倾向于记忆而非理解周期性原理。周期性在推理和泛化中至关重要，影响自然和工程系统的可预测性。为此，作者提出了一种基于傅里叶分析的新型网络架构FAN，能够更有效地建模和推理周期现象。实验表明，FAN在多种实际任务中表现出色，具有更少的参数和计算量。'}}, 'hash': '9138501c89f38809', 'pub_date_card': {'ru': '3 октября', 'en': 'October 3', 'zh': '10月3日'}}, {'id': 'https://huggingface.co/papers/2410.05046', 'title': 'Named Clinical Entity Recognition Benchmark', 'url': 'https://huggingface.co/papers/2410.05046', 'abstract': 'This technical report introduces a Named Clinical Entity Recognition Benchmark for evaluating language models in healthcare, addressing the crucial natural language processing (NLP) task of extracting structured information from clinical narratives to support applications like automated coding, clinical trial cohort identification, and clinical decision support.   The leaderboard provides a standardized platform for assessing diverse language models, including encoder and decoder architectures, on their ability to identify and classify clinical entities across multiple medical domains. A curated collection of openly available clinical datasets is utilized, encompassing entities such as diseases, symptoms, medications, procedures, and laboratory measurements. Importantly, these entities are standardized according to the Observational Medical Outcomes Partnership (OMOP) Common Data Model, ensuring consistency and interoperability across different healthcare systems and datasets, and a comprehensive evaluation of model performance. Performance of models is primarily assessed using the F1-score, and it is complemented by various assessment modes to provide comprehensive insights into model performance. The report also includes a brief analysis of models evaluated to date, highlighting observed trends and limitations.   By establishing this benchmarking framework, the leaderboard aims to promote transparency, facilitate comparative analyses, and drive innovation in clinical entity recognition tasks, addressing the need for robust evaluation methods in healthcare NLP.', 'score': 14, 'issue_id': 17, 'pub_date': '2024-10-07', 'pub_date_ru': '7 октября', 'data': {'categories': ['#benchmark', '#dataset', '#medicine'], 'emoji': '🏥', 'ru': {'title': 'Новый стандарт оценки языковых моделей в медицинском NLP', 'desc': 'Этот технический отчет представляет новый бенчмарк для оценки языковых моделей в распознавании именованных клинических сущностей. Бенчмарк использует стандартизированные наборы данных для оценки способности моделей идентифицировать и классифицировать клинические сущности в различных медицинских областях. Основной метрикой оценки является F1-мера, дополненная другими режимами оценки для комплексного анализа производительности моделей. Целью бенчмарка является продвижение прозрачности, сравнительного анализа и инноваций в задачах распознавания клинических сущностей.'}, 'en': {'title': 'Benchmarking Healthcare NLP: Setting the Standard for Clinical Entity Recognition', 'desc': 'This paper introduces a benchmark for evaluating language models in healthcare, focusing on the task of extracting structured information from clinical narratives. It provides a standardized platform to assess various language models, including encoder and decoder architectures, on their ability to identify and classify clinical entities like diseases and medications. The benchmark uses datasets standardized by the OMOP Common Data Model to ensure consistency across healthcare systems. Model performance is primarily measured using the F1-score, with additional assessment modes to offer comprehensive insights.'}, 'zh': {'title': '推动医疗领域语言模型的创新与透明', 'desc': '这篇技术报告介绍了一种用于评估语言模型在医疗领域表现的命名临床实体识别基准。该基准通过标准化平台评估不同语言模型在识别和分类多种医学领域临床实体的能力。使用的临床数据集包括疾病、症状、药物、程序和实验室测量等实体，并根据OMOP通用数据模型进行标准化。通过这种基准框架，排行榜旨在促进透明度、便于比较分析，并推动临床实体识别任务中的创新。'}}, 'hash': '13152fb25949544e', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}}, {'id': 'https://huggingface.co/papers/2410.05080', 'title': 'ScienceAgentBench: Toward Rigorous Assessment of Language Agents for Data-Driven Scientific Discovery', 'url': 'https://huggingface.co/papers/2410.05080', 'abstract': 'The advancements of language language models (LLMs) have piqued growing interest in developing LLM-based language agents to automate scientific discovery end-to-end, which has sparked both excitement and skepticism about the true capabilities of such agents. In this work, we argue that for an agent to fully automate scientific discovery, it must be able to complete all essential tasks in the workflow. Thus, we call for rigorous assessment of agents on individual tasks in a scientific workflow before making bold claims on end-to-end automation. To this end, we present ScienceAgentBench, a new benchmark for evaluating language agents for data-driven scientific discovery. To ensure the scientific authenticity and real-world relevance of our benchmark, we extract 102 tasks from 44 peer-reviewed publications in four disciplines and engage nine subject matter experts to validate them. We unify the target output for every task to a self-contained Python program file and employ an array of evaluation metrics to examine the generated programs, execution results, and costs. Each task goes through multiple rounds of manual validation by annotators and subject matter experts to ensure its annotation quality and scientific plausibility. We also propose two effective strategies to mitigate data contamination concerns. Using our benchmark, we evaluate five open-weight and proprietary LLMs, each with three frameworks: direct prompting, OpenHands, and self-debug. Given three attempts for each task, the best-performing agent can only solve 32.4% of the tasks independently and 34.3% with expert-provided knowledge. These results underscore the limited capacities of current language agents in generating code for data-driven discovery, let alone end-to-end automation for scientific research.', 'score': 12, 'issue_id': 19, 'pub_date': '2024-10-07', 'pub_date_ru': '7 октября', 'data': {'categories': ['#agents', '#benchmark'], 'emoji': '🧪', 'ru': {'title': 'Реальная оценка возможностей ИИ-агентов в науке', 'desc': 'В статье представлен новый бенчмарк ScienceAgentBench для оценки языковых агентов в научных исследованиях. Авторы извлекли 102 задачи из 44 рецензируемых публикаций в четырех дисциплинах и привлекли экспертов для их валидации. Бенчмарк оценивает способность агентов генерировать Python-код для решения научных задач. Результаты показывают, что даже лучший агент решает только 32.4% задач самостоятельно, что указывает на ограниченные возможности современных языковых моделей в научной сфере.'}, 'en': {'title': 'Benchmarking the Future: Evaluating Language Agents in Scientific Discovery', 'desc': "The paper discusses the development of language model-based agents aimed at automating scientific discovery, highlighting the need for these agents to handle all tasks in a scientific workflow. To evaluate these agents, the authors introduce ScienceAgentBench, a benchmark derived from 102 tasks across four scientific disciplines, validated by experts. The benchmark assesses the agents' ability to generate Python programs for these tasks, using various evaluation metrics to ensure scientific accuracy and relevance. Results show that even the best-performing language agents can only solve a limited percentage of tasks, indicating their current limitations in fully automating scientific research."}, 'zh': {'title': '科学发现自动化：语言代理的挑战与机遇', 'desc': '这篇论文讨论了基于大型语言模型（LLM）的语言代理在科学发现中的应用。作者认为，要实现科学发现的全自动化，代理必须能够完成工作流程中的所有关键任务。为此，他们提出了一个名为ScienceAgentBench的新基准，用于评估语言代理在数据驱动的科学发现中的表现。研究结果表明，目前的语言代理在生成代码方面能力有限，尚无法实现科学研究的端到端自动化。'}}, 'hash': 'e1f883a68716278f', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}}, {'id': 'https://huggingface.co/papers/2410.05167', 'title': 'Presto! Distilling Steps and Layers for Accelerating Music Generation', 'url': 'https://huggingface.co/papers/2410.05167', 'abstract': 'Despite advances in diffusion-based text-to-music (TTM) methods, efficient, high-quality generation remains a challenge. We introduce Presto!, an approach to inference acceleration for score-based diffusion transformers via reducing both sampling steps and cost per step. To reduce steps, we develop a new score-based distribution matching distillation (DMD) method for the EDM-family of diffusion models, the first GAN-based distillation method for TTM. To reduce the cost per step, we develop a simple, but powerful improvement to a recent layer distillation method that improves learning via better preserving hidden state variance. Finally, we combine our step and layer distillation methods together for a dual-faceted approach. We evaluate our step and layer distillation methods independently and show each yield best-in-class performance. Our combined distillation method can generate high-quality outputs with improved diversity, accelerating our base model by 10-18x (230/435ms latency for 32 second mono/stereo 44.1kHz, 15x faster than comparable SOTA) -- the fastest high-quality TTM to our knowledge. Sound examples can be found at https://presto-music.github.io/web/.', 'score': 11, 'issue_id': 13, 'pub_date': '2024-10-07', 'pub_date_ru': '7 октября', 'data': {'categories': ['#audio', '#diffusion', '#inference'], 'emoji': '🎵', 'ru': {'title': 'Ускорение генерации музыки по тексту с помощью дистилляции диффузионных моделей', 'desc': 'Статья представляет Presto! - новый подход к ускорению генерации музыки на основе текста с использованием диффузионных трансформеров. Авторы разработали метод дистилляции на основе согласования распределений (DMD) для уменьшения количества шагов сэмплирования. Они также улучшили метод послойной дистилляции для снижения вычислительных затрат на каждом шаге. Комбинация этих методов позволяет генерировать высококачественную музыку в 10-18 раз быстрее базовой модели.'}, 'en': {'title': 'Presto! - Fast-Track Your Text-to-Music Creations', 'desc': 'The paper introduces Presto!, a method to speed up text-to-music generation using diffusion transformers by reducing both the number of sampling steps and the cost per step. It employs a novel score-based distribution matching distillation (DMD) method, which is the first GAN-based distillation approach for text-to-music models. Additionally, it enhances a recent layer distillation method to better preserve hidden state variance, improving learning efficiency. The combined approach significantly accelerates the model, achieving high-quality outputs with greater diversity and up to 18 times faster than previous state-of-the-art methods.'}, 'zh': {'title': 'Presto!：快速高质量的文本到音乐生成', 'desc': '这篇论文介绍了一种名为Presto!的方法，用于加速基于扩散的文本到音乐生成。通过减少采样步骤和每步的计算成本，Presto!提高了生成效率。研究者开发了一种新的基于得分的分布匹配蒸馏方法和改进的层蒸馏方法，以更好地保持隐藏状态的方差。最终，结合这两种蒸馏方法，Presto!实现了高质量输出的快速生成，比现有技术快15倍。'}}, 'hash': 'f332523f23a7857e', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}}, {'id': 'https://huggingface.co/papers/2410.05243', 'title': 'Navigating the Digital World as Humans Do: Universal Visual Grounding for GUI Agents', 'url': 'https://huggingface.co/papers/2410.05243', 'abstract': 'Multimodal large language models (MLLMs) are transforming the capabilities of graphical user interface (GUI) agents, facilitating their transition from controlled simulations to complex, real-world applications across various platforms. However, the effectiveness of these agents hinges on the robustness of their grounding capability. Current GUI agents predominantly utilize text-based representations such as HTML or accessibility trees, which, despite their utility, often introduce noise, incompleteness, and increased computational overhead. In this paper, we advocate a human-like embodiment for GUI agents that perceive the environment entirely visually and directly take pixel-level operations on the GUI. The key is visual grounding models that can accurately map diverse referring expressions of GUI elements to their coordinates on the GUI across different platforms. We show that a simple recipe, which includes web-based synthetic data and slight adaptation of the LLaVA architecture, is surprisingly effective for training such visual grounding models. We collect the largest dataset for GUI visual grounding so far, containing 10M GUI elements and their referring expressions over 1.3M screenshots, and use it to train UGround, a strong universal visual grounding model for GUI agents. Empirical results on six benchmarks spanning three categories (grounding, offline agent, and online agent) show that 1) UGround substantially outperforms existing visual grounding models for GUI agents, by up to 20% absolute, and 2) agents with UGround outperform state-of-the-art agents, despite the fact that existing agents use additional text-based input while ours only uses visual perception. These results provide strong support for the feasibility and promises of GUI agents that navigate the digital world as humans do.', 'score': 10, 'issue_id': 19, 'pub_date': '2024-10-07', 'pub_date_ru': '7 октября', 'data': {'categories': ['#agents', '#cv', '#dataset', '#multimodal', '#synthetic', '#training'], 'emoji': '👁️', 'ru': {'title': 'Визуальное восприятие - ключ к эффективным GUI-агентам', 'desc': 'Статья представляет новый подход к созданию агентов графического пользовательского интерфейса (GUI) с использованием мультимодальных больших языковых моделей. Авторы предлагают модель UGround, которая обучается на визуальном восприятии интерфейса без использования текстовых представлений. Эксперименты показывают, что UGround превосходит существующие модели визуальной привязки для GUI-агентов на 20%. Результаты демонстрируют перспективность создания агентов, навигирующих в цифровом мире подобно людям.'}, 'en': {'title': 'Seeing is Believing: Revolutionizing GUI Agents with Visual Grounding', 'desc': "This paper explores the development of GUI agents that operate using visual perception, similar to how humans interact with digital interfaces. By focusing on visual grounding models, the authors aim to improve the accuracy of mapping GUI elements to their coordinates, enhancing the agents' effectiveness. They introduce UGround, a model trained on a large dataset of GUI elements, which significantly outperforms existing models by relying solely on visual data. The study demonstrates that GUI agents can achieve superior performance without the need for text-based inputs, suggesting a promising future for visually-grounded digital navigation."}, 'zh': {'title': '像人类一样导航数字世界的GUI代理', 'desc': '这篇论文介绍了一种新的图形用户界面（GUI）代理方法，利用多模态大语言模型（MLLMs）来增强其在真实世界应用中的能力。传统的GUI代理主要依赖于文本表示，但这种方法常常带来噪声和不完整性。作者提出了一种类似人类的GUI代理方法，完全通过视觉感知环境，并在像素级别进行操作。通过使用合成数据和LLaVA架构的轻微调整，他们训练出了一种名为UGround的视觉定位模型，显著提升了GUI代理的性能。'}}, 'hash': 'c91b020a4bdfc21e', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}}, {'id': 'https://huggingface.co/papers/2410.04534', 'title': 'UniMuMo: Unified Text, Music and Motion Generation', 'url': 'https://huggingface.co/papers/2410.04534', 'abstract': 'We introduce UniMuMo, a unified multimodal model capable of taking arbitrary text, music, and motion data as input conditions to generate outputs across all three modalities. To address the lack of time-synchronized data, we align unpaired music and motion data based on rhythmic patterns to leverage existing large-scale music-only and motion-only datasets. By converting music, motion, and text into token-based representation, our model bridges these modalities through a unified encoder-decoder transformer architecture. To support multiple generation tasks within a single framework, we introduce several architectural improvements. We propose encoding motion with a music codebook, mapping motion into the same feature space as music. We introduce a music-motion parallel generation scheme that unifies all music and motion generation tasks into a single transformer decoder architecture with a single training task of music-motion joint generation. Moreover, the model is designed by fine-tuning existing pre-trained single-modality models, significantly reducing computational demands. Extensive experiments demonstrate that UniMuMo achieves competitive results on all unidirectional generation benchmarks across music, motion, and text modalities. Quantitative results are available in the https://hanyangclarence.github.io/unimumo_demo/{project page}.', 'score': 10, 'issue_id': 14, 'pub_date': '2024-10-06', 'pub_date_ru': '6 октября', 'data': {'categories': ['#architecture', '#multimodal'], 'emoji': '🎭', 'ru': {'title': 'UniMuMo: единая модель для генерации текста, музыки и движений', 'desc': 'UniMuMo - это унифицированная мультимодальная модель, способная генерировать текст, музыку и движения на основе входных данных в этих модальностях. Модель использует выравнивание неспаренных музыкальных и двигательных данных на основе ритмических паттернов. Архитектура UniMuMo основана на трансформере типа энкодер-декодер с токенизированным представлением всех модальностей. Модель достигает конкурентоспособных результатов на различных бенчмарках по однонаправленной генерации для музыки, движений и текста.'}, 'en': {'title': 'UniMuMo: Harmonizing Text, Music, and Motion in One Model', 'desc': 'The paper introduces UniMuMo, a model that can generate text, music, and motion from any of these inputs. It solves the problem of unpaired data by aligning music and motion based on rhythm, using large datasets. The model uses a unified transformer architecture to convert these inputs into tokens, allowing them to be processed together. By fine-tuning existing models, it efficiently handles multiple tasks, showing strong performance across different generation benchmarks.'}, 'zh': {'title': 'UniMuMo：跨越文本、音乐与动作的多模态生成', 'desc': '这篇论文介绍了一个名为UniMuMo的统一多模态模型，可以根据任意的文本、音乐和动作数据生成相应的输出。为了克服时间同步数据的缺乏，研究人员通过节奏模式对未配对的音乐和动作数据进行对齐。模型通过将音乐、动作和文本转换为基于token的表示，使用统一的编码器-解码器Transformer架构连接这些模态。通过对现有的单模态预训练模型进行微调，显著降低了计算需求，并在多种生成任务中取得了竞争性结果。'}}, 'hash': '4bfc87467c16c280', 'pub_date_card': {'ru': '6 октября', 'en': 'October 6', 'zh': '10月6日'}}, {'id': 'https://huggingface.co/papers/2410.04734', 'title': 'TLDR: Token-Level Detective Reward Model for Large Vision Language Models', 'url': 'https://huggingface.co/papers/2410.04734', 'abstract': 'Although reward models have been successful in improving multimodal large language models, the reward models themselves remain brutal and contain minimal information. Notably, existing reward models only mimic human annotations by assigning only one binary feedback to any text, no matter how long the text is. In the realm of multimodal language models, where models are required to process both images and texts, a naive reward model may learn implicit biases toward texts and become less grounded in images. In this paper, we propose a Token-Level Detective Reward Model (TLDR) to provide fine-grained annotations to each text token. We first introduce a perturbation-based method to generate synthetic hard negatives and their token-level labels to train TLDR models. Then we show the rich usefulness of TLDR models both in assisting off-the-shelf models to self-correct their generations, and in serving as a hallucination evaluation tool. Finally, we show that TLDR models can significantly speed up human annotation by 3 times to acquire a broader range of high-quality vision language data.', 'score': 10, 'issue_id': 13, 'pub_date': '2024-10-07', 'pub_date_ru': '7 октября', 'data': {'categories': ['#hallucinations', '#interpretability', '#multimodal', '#rag', '#synthetic', '#training'], 'emoji': '🔍', 'ru': {'title': 'Точность на уровне токенов: новый подход к мультимодальным языковым моделям', 'desc': "В статье представлена модель вознаграждения на уровне токенов (TLDR) для мультимодальных языковых моделей. TLDR обеспечивает детальную аннотацию каждого текстового токена, что позволяет лучше учитывать как текстовую, так и визуальную информацию. Метод использует синтетические 'сложные отрицательные примеры' для обучения модели. TLDR может применяться для самокоррекции генераций моделей и оценки галлюцинаций, а также ускоряет процесс аннотации данных в 3 раза."}, 'en': {'title': '"Token-Level Precision: Elevating Multimodal Models with TLDR"', 'desc': "The paper introduces a Token-Level Detective Reward Model (TLDR) to enhance the feedback mechanism in multimodal large language models. Unlike traditional reward models that provide binary feedback, TLDR offers detailed annotations for each text token, improving the model's understanding of both text and images. The authors use a perturbation-based method to create challenging examples and train the TLDR model, which helps models self-correct and evaluate hallucinations. Additionally, TLDR models can accelerate human annotation processes, making it three times faster to gather high-quality data."}, 'zh': {'title': '细粒度奖励模型：提升多模态语言模型的关键', 'desc': '这篇论文提出了一种新的奖励模型，称为Token-Level Detective Reward Model（TLDR），用于改进多模态大语言模型。传统的奖励模型只给文本整体一个二元反馈，而TLDR模型则对每个文本标记提供细粒度的注释。通过引入基于扰动的方法，生成合成的困难负样本及其标记，来训练TLDR模型。实验表明，TLDR模型不仅能帮助现有模型自我纠正生成内容，还能作为幻觉评估工具，并能将人工注释速度提高三倍。'}}, 'hash': '8c4b74044ea31d8d', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}}, {'id': 'https://huggingface.co/papers/2410.05229', 'title': 'GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models', 'url': 'https://huggingface.co/papers/2410.05229', 'abstract': "Recent advancements in Large Language Models (LLMs) have sparked interest in their formal reasoning capabilities, particularly in mathematics. The GSM8K benchmark is widely used to assess the mathematical reasoning of models on grade-school-level questions. While the performance of LLMs on GSM8K has significantly improved in recent years, it remains unclear whether their mathematical reasoning capabilities have genuinely advanced, raising questions about the reliability of the reported metrics. To address these concerns, we conduct a large-scale study on several SOTA open and closed models. To overcome the limitations of existing evaluations, we introduce GSM-Symbolic, an improved benchmark created from symbolic templates that allow for the generation of a diverse set of questions. GSM-Symbolic enables more controllable evaluations, providing key insights and more reliable metrics for measuring the reasoning capabilities of models.Our findings reveal that LLMs exhibit noticeable variance when responding to different instantiations of the same question. Specifically, the performance of all models declines when only the numerical values in the question are altered in the GSM-Symbolic benchmark. Furthermore, we investigate the fragility of mathematical reasoning in these models and show that their performance significantly deteriorates as the number of clauses in a question increases. We hypothesize that this decline is because current LLMs cannot perform genuine logical reasoning; they replicate reasoning steps from their training data. Adding a single clause that seems relevant to the question causes significant performance drops (up to 65%) across all state-of-the-art models, even though the clause doesn't contribute to the reasoning chain needed for the final answer. Overall, our work offers a more nuanced understanding of LLMs' capabilities and limitations in mathematical reasoning.", 'score': 8, 'issue_id': 18, 'pub_date': '2024-10-07', 'pub_date_ru': '7 октября', 'data': {'categories': ['#benchmark', '#interpretability', '#math', '#reasoning'], 'emoji': '🧮', 'ru': {'title': 'Разоблачение иллюзии математического мышления у языковых моделей', 'desc': 'Статья посвящена оценке способностей больших языковых моделей (LLM) к математическим рассуждениям. Авторы представляют новый бенчмарк GSM-Symbolic, созданный на основе символических шаблонов для более контролируемой оценки. Исследование показывает, что производительность LLM значительно снижается при изменении числовых значений в вопросах и увеличении количества условий. Авторы предполагают, что современные LLM не способны к подлинным логическим рассуждениям, а лишь воспроизводят шаги рассуждений из обучающих данных.'}, 'en': {'title': 'Unveiling the Limits of LLMs in Math Reasoning', 'desc': "This paper explores the mathematical reasoning abilities of Large Language Models (LLMs) using a new benchmark called GSM-Symbolic. The study reveals that LLMs struggle with genuine logical reasoning, as their performance drops significantly when questions are slightly altered. The research highlights that LLMs often rely on patterns from their training data rather than true reasoning, especially when questions become more complex. The findings suggest that current metrics may overestimate LLMs' reasoning capabilities, emphasizing the need for more reliable evaluation methods."}, 'zh': {'title': '揭示大型语言模型数学推理的局限性', 'desc': '这篇论文研究了大型语言模型在数学推理方面的能力，特别是使用GSM8K基准进行评估。作者提出了一个新的评估基准GSM-Symbolic，通过符号模板生成多样化的问题集，以更好地评估模型的推理能力。研究发现，模型在面对同一问题的不同实例时表现出显著的差异，尤其是在问题中仅改变数值时，性能会下降。论文指出，当前的模型可能无法进行真正的逻辑推理，而是依赖于训练数据中的推理步骤。'}}, 'hash': '96f3985414b99bf9', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}}, {'id': 'https://huggingface.co/papers/2410.04698', 'title': 'MathHay: An Automated Benchmark for Long-Context Mathematical Reasoning in LLMs', 'url': 'https://huggingface.co/papers/2410.04698', 'abstract': "Recent large language models (LLMs) have demonstrated versatile capabilities in long-context scenarios. Although some recent benchmarks have been developed to evaluate the long-context capabilities of LLMs, there is a lack of benchmarks evaluating the mathematical reasoning abilities of LLMs over long contexts, which is crucial for LLMs' application in real-world scenarios. In this paper, we introduce MathHay, an automated benchmark designed to assess the long-context mathematical reasoning capabilities of LLMs. Unlike previous benchmarks like Needle in a Haystack, which focus primarily on information retrieval within long texts, MathHay demands models with both information-seeking and complex mathematical reasoning abilities. We conduct extensive experiments on MathHay to assess the long-context mathematical reasoning abilities of eight top-performing LLMs. Even the best-performing model, Gemini-1.5-Pro-002, still struggles with mathematical reasoning over long contexts, achieving only 51.26% accuracy at 128K tokens. This highlights the significant room for improvement on the MathHay benchmark.", 'score': 8, 'issue_id': 15, 'pub_date': '2024-10-07', 'pub_date_ru': '7 октября', 'data': {'categories': ['#benchmark', '#long_context', '#math'], 'emoji': '🧮', 'ru': {'title': 'MathHay: Новый вызов для математических способностей языковых моделей', 'desc': 'Статья представляет новый бенчмарк MathHay для оценки способностей больших языковых моделей (LLM) к математическим рассуждениям в контексте длинных текстов. В отличие от существующих бенчмарков, MathHay требует от моделей не только поиска информации, но и сложных математических вычислений. Эксперименты с восемью ведущими LLM показали, что даже лучшая модель Gemini-1.5-Pro-002 достигает точности всего 51.26% на текстах длиной 128 тысяч токенов. Результаты указывают на значительный потенциал для улучшения способностей LLM в области длинных математических рассуждений.'}, 'en': {'title': 'Pushing the Limits: Math Reasoning in Long Contexts', 'desc': 'The paper introduces MathHay, a new benchmark specifically designed to evaluate the mathematical reasoning abilities of large language models (LLMs) over long contexts. Unlike previous benchmarks, MathHay requires models to not only retrieve information but also perform complex mathematical reasoning. Experiments conducted on eight leading LLMs reveal that even the best model, Gemini-1.5-Pro-002, achieves only 51.26% accuracy at 128K tokens, indicating challenges in this area. This study highlights the need for further advancements in LLMs to improve their mathematical reasoning capabilities in long-context scenarios.'}, 'zh': {'title': 'MathHay：挑战大型语言模型的数学推理极限', 'desc': '这篇论文介绍了一种名为MathHay的自动化基准，用于评估大型语言模型在长文本中的数学推理能力。与之前的基准不同，MathHay不仅要求模型具备信息检索能力，还需要复杂的数学推理能力。研究表明，即使是表现最好的模型在长文本数学推理中也仅能达到51.26%的准确率。这表明在MathHay基准上还有很大的改进空间。'}}, 'hash': '1e15804dd57fc363', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}}, {'id': 'https://huggingface.co/papers/2410.04932', 'title': 'OmniBooth: Learning Latent Control for Image Synthesis with Multi-modal Instruction', 'url': 'https://huggingface.co/papers/2410.04932', 'abstract': 'We present OmniBooth, an image generation framework that enables spatial control with instance-level multi-modal customization. For all instances, the multimodal instruction can be described through text prompts or image references. Given a set of user-defined masks and associated text or image guidance, our objective is to generate an image, where multiple objects are positioned at specified coordinates and their attributes are precisely aligned with the corresponding guidance. This approach significantly expands the scope of text-to-image generation, and elevates it to a more versatile and practical dimension in controllability. In this paper, our core contribution lies in the proposed latent control signals, a high-dimensional spatial feature that provides a unified representation to integrate the spatial, textual, and image conditions seamlessly. The text condition extends ControlNet to provide instance-level open-vocabulary generation. The image condition further enables fine-grained control with personalized identity. In practice, our method empowers users with more flexibility in controllable generation, as users can choose multi-modal conditions from text or images as needed. Furthermore, thorough experiments demonstrate our enhanced performance in image synthesis fidelity and alignment across different tasks and datasets. Project page: https://len-li.github.io/omnibooth-web/', 'score': 8, 'issue_id': 14, 'pub_date': '2024-10-07', 'pub_date_ru': '7 октября', 'data': {'categories': ['#cv', '#multimodal'], 'emoji': '🎨', 'ru': {'title': 'Управляемое творчество: новый уровень генерации изображений', 'desc': 'OmniBooth - это фреймворк для генерации изображений с пространственным контролем и мультимодальной кастомизацией на уровне отдельных объектов. Система позволяет задавать расположение и атрибуты объектов с помощью текстовых запросов или изображений-образцов. Ключевым элементом является предложенный латентный сигнал управления - многомерный пространственный признак, объединяющий пространственные, текстовые и визуальные условия. OmniBooth расширяет возможности текст-в-изображение генерации, предоставляя пользователям гибкий инструмент для контролируемого создания изображений.'}, 'en': {'title': 'OmniBooth: Mastering Image Generation with Precision and Flexibility', 'desc': 'OmniBooth is a framework for generating images where users can control the placement and attributes of objects using text prompts or image references. It introduces latent control signals, which are high-dimensional features that integrate spatial, textual, and image conditions for seamless image generation. This method enhances the flexibility of text-to-image generation by allowing instance-level customization and open-vocabulary generation. Experiments show that OmniBooth improves image synthesis quality and alignment across various tasks and datasets.'}, 'zh': {'title': 'OmniBooth：实现图像生成的多模态空间控制', 'desc': 'OmniBooth 是一个图像生成框架，允许用户通过文本提示或图像参考来进行空间控制和实例级多模态定制。用户可以定义遮罩和相关的文本或图像指导，以生成在指定坐标上放置多个对象的图像，并精确对齐其属性。该方法通过提出潜在控制信号，将空间、文本和图像条件无缝整合，扩展了文本到图像生成的范围。实验表明，OmniBooth 在图像合成的保真度和对齐性上表现出色。'}}, 'hash': '20ada6e6ba364412', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}}, {'id': 'https://huggingface.co/papers/2410.03825', 'title': 'MonST3R: A Simple Approach for Estimating Geometry in the Presence of Motion', 'url': 'https://huggingface.co/papers/2410.03825', 'abstract': "Estimating geometry from dynamic scenes, where objects move and deform over time, remains a core challenge in computer vision. Current approaches often rely on multi-stage pipelines or global optimizations that decompose the problem into subtasks, like depth and flow, leading to complex systems prone to errors. In this paper, we present Motion DUSt3R (MonST3R), a novel geometry-first approach that directly estimates per-timestep geometry from dynamic scenes. Our key insight is that by simply estimating a pointmap for each timestep, we can effectively adapt DUST3R's representation, previously only used for static scenes, to dynamic scenes. However, this approach presents a significant challenge: the scarcity of suitable training data, namely dynamic, posed videos with depth labels. Despite this, we show that by posing the problem as a fine-tuning task, identifying several suitable datasets, and strategically training the model on this limited data, we can surprisingly enable the model to handle dynamics, even without an explicit motion representation. Based on this, we introduce new optimizations for several downstream video-specific tasks and demonstrate strong performance on video depth and camera pose estimation, outperforming prior work in terms of robustness and efficiency. Moreover, MonST3R shows promising results for primarily feed-forward 4D reconstruction.", 'score': 8, 'issue_id': 14, 'pub_date': '2024-10-04', 'pub_date_ru': '4 октября', 'data': {'categories': ['#3d', '#cv', '#video'], 'emoji': '🎥', 'ru': {'title': 'MonST3R: Революция в оценке геометрии динамических сцен', 'desc': 'MonST3R - это новый подход к оценке геометрии динамических сцен в компьютерном зрении. Вместо сложных многоступенчатых систем, он напрямую оценивает геометрию для каждого кадра, адаптируя представление DUST3R для динамических сцен. Несмотря на проблему нехватки данных для обучения, авторы смогли обучить модель, используя дообучение и стратегический подбор датасетов. MonST3R показывает высокую производительность в задачах оценки глубины видео и позы камеры, а также обещающие результаты в 4D-реконструкции.'}, 'en': {'title': 'Simplifying Dynamic Scene Geometry with MonST3R', 'desc': 'The paper introduces Motion DUSt3R (MonST3R), a new method for estimating geometry in dynamic scenes, where objects move and change shape over time. Unlike traditional methods that break the problem into smaller tasks, MonST3R directly estimates geometry for each moment, simplifying the process and reducing errors. The authors tackle the challenge of limited training data by fine-tuning the model on selected datasets, allowing it to handle dynamic scenes effectively. MonST3R not only improves video depth and camera pose estimation but also shows potential for efficient 4D reconstruction.'}, 'zh': {'title': '动态场景几何估计的新突破：MonST3R', 'desc': '这篇论文提出了一种新的方法，称为Motion DUSt3R（MonST3R），用于从动态场景中直接估计每个时间步的几何结构。传统方法通常将问题分解为多个子任务，导致系统复杂且容易出错，而MonST3R通过简单估计每个时间步的点图来解决这一问题。尽管缺乏合适的训练数据是一个挑战，但通过将问题视为微调任务并选择合适的数据集进行训练，模型能够在没有明确运动表示的情况下处理动态场景。实验结果表明，MonST3R在视频深度和相机姿态估计等任务中表现出色，优于以往的方法。'}}, 'hash': '0327c1225649bc76', 'pub_date_card': {'ru': '4 октября', 'en': 'October 4', 'zh': '10月4日'}}, {'id': 'https://huggingface.co/papers/2410.02884', 'title': 'LLaMA-Berry: Pairwise Optimization for O1-like Olympiad-Level Mathematical Reasoning', 'url': 'https://huggingface.co/papers/2410.02884', 'abstract': 'This paper presents an advanced mathematical problem-solving framework, LLaMA-Berry, for enhancing the mathematical reasoning ability of Large Language Models (LLMs). The framework combines Monte Carlo Tree Search (MCTS) with iterative Self-Refine to optimize the reasoning path and utilizes a pairwise reward model to evaluate different paths globally. By leveraging the self-critic and rewriting capabilities of LLMs, Self-Refine applied to MCTS (SR-MCTS) overcomes the inefficiencies and limitations of conventional step-wise and greedy search algorithms by fostering a more efficient exploration of solution spaces. Pairwise Preference Reward Model~(PPRM), inspired by Reinforcement Learning from Human Feedback (RLHF), is then used to model pairwise preferences between solutions, utilizing an Enhanced Borda Count (EBC) method to synthesize these preferences into a global ranking score to find better answers. This approach addresses the challenges of scoring variability and non-independent distributions in mathematical reasoning tasks. The framework has been tested on general and advanced benchmarks, showing superior performance in terms of search efficiency and problem-solving capability compared to existing methods like ToT and rStar, particularly in complex Olympiad-level benchmarks, including GPQA, AIME24 and AMC23.', 'score': 7, 'issue_id': 16, 'pub_date': '2024-10-03', 'pub_date_ru': '3 октября', 'data': {'categories': ['#math', '#rlhf'], 'emoji': '🧮', 'ru': {'title': 'LLaMA-Berry: Прорыв в математическом мышлении искусственного интеллекта', 'desc': 'LLaMA-Berry - это передовая система для решения математических задач, улучшающая способности больших языковых моделей к математическим рассуждениям. Она объединяет метод Монте-Карло для поиска по дереву (MCTS) с итеративным самоулучшением для оптимизации пути рассуждений. Система использует модель парных вознаграждений для глобальной оценки различных путей решения. LLaMA-Berry демонстрирует превосходную производительность по сравнению с существующими методами, особенно на сложных олимпиадных задачах.'}, 'en': {'title': 'LLaMA-Berry: Revolutionizing Math Reasoning in AI', 'desc': 'The paper introduces LLaMA-Berry, a framework designed to improve the mathematical reasoning skills of Large Language Models by integrating Monte Carlo Tree Search with a method called Self-Refine. This combination allows for a more efficient exploration of possible solutions by overcoming the limitations of traditional search algorithms. The framework also uses a Pairwise Preference Reward Model to evaluate and rank different solution paths, inspired by Reinforcement Learning from Human Feedback. Tested on various benchmarks, LLaMA-Berry demonstrates superior performance in solving complex mathematical problems compared to existing methods.'}, 'zh': {'title': 'LLaMA-Berry：提升大型语言模型数学推理的新框架', 'desc': '这篇论文介绍了一种名为LLaMA-Berry的高级数学问题解决框架，用于增强大型语言模型的数学推理能力。该框架结合了蒙特卡洛树搜索（MCTS）和迭代自我优化，以优化推理路径，并利用成对奖励模型对不同路径进行全局评估。通过利用大型语言模型的自我批评和重写能力，SR-MCTS克服了传统逐步和贪婪搜索算法的低效和局限性。成对偏好奖励模型（PPRM）则通过增强的博尔达计数方法，将解决方案的成对偏好合成为全局排名分数，以找到更好的答案。'}}, 'hash': 'cc4c09dfca59a8d4', 'pub_date_card': {'ru': '3 октября', 'en': 'October 3', 'zh': '10月3日'}}, {'id': 'https://huggingface.co/papers/2410.05262', 'title': 'TurtleBench: Evaluating Top Language Models via Real-World Yes/No Puzzles', 'url': 'https://huggingface.co/papers/2410.05262', 'abstract': 'As the application of Large Language Models (LLMs) expands, the demand for reliable evaluations increases. Existing LLM evaluation benchmarks primarily rely on static datasets, making it challenging to assess model performance in dynamic interactions with users. Moreover, these benchmarks often depend on specific background knowledge, complicating the measurement of a model\'s logical reasoning capabilities. Other dynamic evaluation methods based on strong models or manual efforts may introduce biases and incur high costs and time demands, hindering large-scale application. To address these issues, we propose TurtleBench. TurtleBench collects real user guesses from our online Turtle Soup Puzzle platform that we developed. This approach allows for the relatively dynamic generation of evaluation datasets, mitigating the risk of model cheating while aligning assessments more closely with genuine user needs for reasoning capabilities, thus enhancing the reliability of evaluations. TurtleBench includes 1,532 user guesses along with the correctness of guesses after annotation. Using this dataset, we thoroughly evaluated nine of the most advanced LLMs available today. Notably, the OpenAI o1 series models did not achieve leading results in these evaluations. We propose several hypotheses for further research, such as "the latent reasoning of o1 utilizes trivial Chain-of-Thought (CoT) techniques" and "increasing CoT length not only provides reasoning benefits but also incurs noise costs."', 'score': 7, 'issue_id': 13, 'pub_date': '2024-10-07', 'pub_date_ru': '7 октября', 'data': {'categories': ['#benchmark', '#dataset', '#reasoning'], 'emoji': '🐢', 'ru': {'title': 'TurtleBench: Динамическая оценка LLM на основе реальных пользовательских догадок', 'desc': 'TurtleBench - новый метод оценки больших языковых моделей (LLM), основанный на реальных догадках пользователей в игре Turtle Soup Puzzle. Этот подход позволяет динамически генерировать наборы данных для оценки, снижая риск обмана со стороны моделей и более точно отражая потребности пользователей в способностях рассуждения. В исследовании оценивались девять передовых LLM, причем модели серии OpenAI o1 не показали лидирующих результатов. Авторы выдвигают гипотезы для дальнейших исследований, включая использование тривиальных методов цепочки рассуждений (Chain-of-Thought) в латентных рассуждениях o1.'}, 'en': {'title': 'TurtleBench: Revolutionizing LLM Evaluation with Real User Interactions', 'desc': "The paper introduces TurtleBench, a novel evaluation framework for Large Language Models (LLMs) that uses real user interactions from an online puzzle platform to create dynamic datasets. This method addresses the limitations of static benchmarks and reduces biases and costs associated with manual evaluations. TurtleBench's dataset includes over 1,500 user guesses, providing a more authentic measure of a model's reasoning capabilities. The study reveals that some advanced models, like OpenAI's o1 series, may not perform as well as expected, suggesting areas for further research into reasoning techniques and their trade-offs."}, 'zh': {'title': 'TurtleBench：动态评估大型语言模型的新方法', 'desc': '随着大型语言模型（LLMs）的应用扩大，对可靠评估的需求也在增加。现有的评估基准主要依赖于静态数据集，难以评估模型在与用户动态交互中的表现。TurtleBench通过收集真实用户在Turtle Soup Puzzle平台上的猜测，动态生成评估数据集，降低了模型作弊的风险。通过这种方法，我们对九个先进的LLMs进行了评估，发现OpenAI o1系列模型未能取得领先结果。'}}, 'hash': '6d3f3633d606dec9', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}}, {'id': 'https://huggingface.co/papers/2410.03187', 'title': 'Autonomous Character-Scene Interaction Synthesis from Text Instruction', 'url': 'https://huggingface.co/papers/2410.03187', 'abstract': 'Synthesizing human motions in 3D environments, particularly those with complex activities such as locomotion, hand-reaching, and human-object interaction, presents substantial demands for user-defined waypoints and stage transitions. These requirements pose challenges for current models, leading to a notable gap in automating the animation of characters from simple human inputs. This paper addresses this challenge by introducing a comprehensive framework for synthesizing multi-stage scene-aware interaction motions directly from a single text instruction and goal location. Our approach employs an auto-regressive diffusion model to synthesize the next motion segment, along with an autonomous scheduler predicting the transition for each action stage. To ensure that the synthesized motions are seamlessly integrated within the environment, we propose a scene representation that considers the local perception both at the start and the goal location. We further enhance the coherence of the generated motion by integrating frame embeddings with language input. Additionally, to support model training, we present a comprehensive motion-captured dataset comprising 16 hours of motion sequences in 120 indoor scenes covering 40 types of motions, each annotated with precise language descriptions. Experimental results demonstrate the efficacy of our method in generating high-quality, multi-stage motions closely aligned with environmental and textual conditions.', 'score': 6, 'issue_id': 15, 'pub_date': '2024-10-04', 'pub_date_ru': '4 октября', 'data': {'categories': ['#3d', '#dataset', '#diffusion'], 'emoji': '🤖', 'ru': {'title': 'ИИ оживляет персонажей в 3D по текстовым командам', 'desc': 'Статья представляет новую систему для синтеза сложных движений человека в 3D-средах на основе текстовых инструкций и целевого положения. Авторы используют авторегрессионную диффузионную модель для генерации последовательных сегментов движения и автономный планировщик для предсказания переходов между этапами действий. Для интеграции движений с окружением предложено специальное представление сцены, учитывающее локальное восприятие. Также создан обширный датасет из 16 часов захваченных движений в 120 indoor-сценах для обучения модели.'}, 'en': {'title': 'Animating Life: From Text to Motion in 3D Worlds', 'desc': "This paper introduces a new framework for creating 3D human motions in complex environments using a single text instruction and goal location. It uses an auto-regressive diffusion model to generate motion segments and an autonomous scheduler to manage transitions between action stages. The approach includes a scene representation that accounts for local perception at both the start and goal locations, enhancing motion integration. A large motion-captured dataset with language annotations supports the model's training, showing effective results in producing realistic, multi-stage motions."}, 'zh': {'title': '从文本到动作：自动化多阶段场景感知运动合成', 'desc': '这篇论文提出了一种新的框架，可以从单一的文本指令和目标位置合成多阶段的场景感知交互动作。该方法使用自回归扩散模型来合成下一个动作片段，并通过自主调度器预测每个动作阶段的过渡。为了确保合成的动作与环境无缝集成，论文提出了一种场景表示方法，考虑了起始和目标位置的局部感知。实验结果表明，该方法能够生成高质量的多阶段动作，与环境和文本条件高度一致。'}}, 'hash': '233eb6997f48b10d', 'pub_date_card': {'ru': '4 октября', 'en': 'October 4', 'zh': '10月4日'}}, {'id': 'https://huggingface.co/papers/2410.05057', 'title': 'SELECT: A Large-Scale Benchmark of Data Curation Strategies for Image Classification', 'url': 'https://huggingface.co/papers/2410.05057', 'abstract': 'Data curation is the problem of how to collect and organize samples into a dataset that supports efficient learning. Despite the centrality of the task, little work has been devoted towards a large-scale, systematic comparison of various curation methods. In this work, we take steps towards a formal evaluation of data curation strategies and introduce SELECT, the first large-scale benchmark of curation strategies for image classification.   In order to generate baseline methods for the SELECT benchmark, we create a new dataset, ImageNet++, which constitutes the largest superset of ImageNet-1K to date. Our dataset extends ImageNet with 5 new training-data shifts, each approximately the size of ImageNet-1K itself, and each assembled using a distinct curation strategy. We evaluate our data curation baselines in two ways: (i) using each training-data shift to train identical image classification models from scratch (ii) using the data itself to fit a pretrained self-supervised representation.   Our findings show interesting trends, particularly pertaining to recent methods for data curation such as synthetic data generation and lookup based on CLIP embeddings. We show that although these strategies are highly competitive for certain tasks, the curation strategy used to assemble the original ImageNet-1K dataset remains the gold standard. We anticipate that our benchmark can illuminate the path for new methods to further reduce the gap. We release our checkpoints, code, documentation, and a link to our dataset at https://github.com/jimmyxu123/SELECT.', 'score': 4, 'issue_id': 18, 'pub_date': '2024-10-07', 'pub_date_ru': '7 октября', 'data': {'categories': ['#benchmark', '#cv', '#dataset', '#synthetic', '#training'], 'emoji': '🔍', 'ru': {'title': 'Систематическая оценка стратегий курирования данных для компьютерного зрения', 'desc': 'Статья представляет новый эталонный тест SELECT для оценки стратегий курирования данных в задачах классификации изображений. Авторы создали расширенный набор данных ImageNet++, включающий 5 новых сдвигов обучающих данных. Проведена оценка различных стратегий курирования, включая синтетическую генерацию данных и поиск на основе CLIP-эмбеддингов. Результаты показывают, что оригинальная стратегия курирования ImageNet-1K остается золотым стандартом.'}, 'en': {'title': 'SELECT: Benchmarking the Future of Data Curation', 'desc': 'The paper addresses the challenge of data curation, which involves collecting and organizing samples into datasets that enhance machine learning efficiency. It introduces SELECT, a benchmark for evaluating different data curation strategies, using a new dataset called ImageNet++. This dataset expands on ImageNet-1K with five new data shifts, each created using a unique curation method. The study finds that while new curation methods like synthetic data generation show promise, the original ImageNet-1K curation strategy remains the most effective.'}, 'zh': {'title': '数据整理新基准：SELECT的诞生', 'desc': '这篇论文探讨了如何收集和组织样本以支持高效学习的数据整理问题。作者引入了SELECT，这是第一个用于图像分类的大规模数据整理策略基准。通过创建新的数据集ImageNet++，他们评估了不同的数据整理策略。研究发现，尽管新方法在某些任务中表现出色，但原始ImageNet-1K的数据整理策略仍然是标准。'}}, 'hash': '830db899ae55d1d4', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}}, {'id': 'https://huggingface.co/papers/2410.03617', 'title': 'What Matters for Model Merging at Scale?', 'url': 'https://huggingface.co/papers/2410.03617', 'abstract': "Model merging aims to combine multiple expert models into a more capable single model, offering benefits such as reduced storage and serving costs, improved generalization, and support for decentralized model development. Despite its promise, previous studies have primarily focused on merging a few small models. This leaves many unanswered questions about the effect of scaling model size and how it interplays with other key factors -- like the base model quality and number of expert models -- , to affect the merged model's performance. This work systematically evaluates the utility of model merging at scale, examining the impact of these different factors. We experiment with merging fully fine-tuned models using 4 popular merging methods -- Averaging, Task~Arithmetic, Dare, and TIES -- across model sizes ranging from 1B-64B parameters and merging up to 8 different expert models. We evaluate the merged models on both held-in tasks, i.e., the expert's training tasks, and zero-shot generalization to unseen held-out tasks. Our experiments provide several new insights about model merging at scale and the interplay between different factors. First, we find that merging is more effective when experts are created from strong base models, i.e., models with good zero-shot performance. Second, larger models facilitate easier merging. Third merging consistently improves generalization capabilities. Notably, when merging 8 large expert models, the merged models often generalize better compared to the multitask trained models. Fourth, we can better merge more expert models when working with larger models. Fifth, different merging methods behave very similarly at larger scales. Overall, our findings shed light on some interesting properties of model merging while also highlighting some limitations. We hope that this study will serve as a reference point on large-scale merging for upcoming research.", 'score': 3, 'issue_id': 18, 'pub_date': '2024-10-04', 'pub_date_ru': '4 октября', 'data': {'categories': ['#optimization'], 'emoji': '🔀', 'ru': {'title': 'Объединение моделей в масштабе: больше, лучше, эффективнее', 'desc': 'Статья исследует масштабируемое объединение моделей машинного обучения, анализируя влияние размера модели, качества базовой модели и количества экспертных моделей на производительность объединенной модели. Авторы проводят эксперименты с объединением полностью дообученных моделей размером от 1 до 64 миллиардов параметров, используя четыре популярных метода объединения. Результаты показывают, что объединение более эффективно при использовании сильных базовых моделей и более крупных моделей, а также улучшает способности к обобщению. Исследование предоставляет ценные выводы о свойствах и ограничениях масштабного объединения моделей.'}, 'en': {'title': 'Unlocking the Power of Model Merging at Scale', 'desc': 'This paper explores the concept of model merging, which combines multiple expert models into a single, more capable model, focusing on large-scale applications. The study evaluates the effectiveness of merging models of varying sizes, from 1 billion to 64 billion parameters, using four different methods. Key findings include that merging is more successful with strong base models, larger models facilitate easier merging, and merging improves generalization capabilities. The research provides insights into the benefits and limitations of model merging, aiming to guide future studies in this area.'}, 'zh': {'title': '大规模模型合并：提升泛化能力的新途径', 'desc': '这篇论文研究了如何将多个专家模型合并成一个更强大的单一模型。研究发现，合并效果在专家模型来自强大的基础模型时更好，尤其是这些基础模型在零样本任务中表现良好。大规模模型更容易进行合并，并且合并后模型的泛化能力通常优于多任务训练的模型。不同的合并方法在大规模下表现相似，这为未来的大规模模型合并研究提供了参考。'}}, 'hash': '1890d06170bcbcb8', 'pub_date_card': {'ru': '4 октября', 'en': 'October 4', 'zh': '10月4日'}}, {'id': 'https://huggingface.co/papers/2410.03959', 'title': 'Grounding Language in Multi-Perspective Referential Communication', 'url': 'https://huggingface.co/papers/2410.03959', 'abstract': "We introduce a task and dataset for referring expression generation and comprehension in multi-agent embodied environments. In this task, two agents in a shared scene must take into account one another's visual perspective, which may be different from their own, to both produce and understand references to objects in a scene and the spatial relations between them. We collect a dataset of 2,970 human-written referring expressions, each paired with human comprehension judgments, and evaluate the performance of automated models as speakers and listeners paired with human partners, finding that model performance in both reference generation and comprehension lags behind that of pairs of human agents. Finally, we experiment training an open-weight speaker model with evidence of communicative success when paired with a listener, resulting in an improvement from 58.9 to 69.3% in communicative success and even outperforming the strongest proprietary model.", 'score': 3, 'issue_id': 12, 'pub_date': '2024-10-04', 'pub_date_ru': '4 октября', 'data': {'categories': ['#agents', '#dataset', '#multimodal'], 'emoji': '👥', 'ru': {'title': 'Улучшение коммуникации ИИ-агентов через учет перспективы собеседника', 'desc': 'Статья представляет новую задачу и набор данных для генерации и понимания референциальных выражений в многоагентных средах. Два агента в общей сцене должны учитывать визуальную перспективу друг друга для создания и понимания ссылок на объекты и их пространственные отношения. Авторы собрали датасет из 2970 человеческих референциальных выражений с оценками понимания. Эксперименты показали, что производительность моделей в генерации и понимании ссылок отстает от человеческих пар, но обучение с обратной связью об успешности коммуникации улучшило результаты.'}, 'en': {'title': 'Bridging Perspectives: Enhancing AI Communication in Shared Spaces', 'desc': "This paper introduces a new task and dataset for generating and understanding referring expressions in environments where two agents must consider each other's visual perspectives. The dataset includes 2,970 human-written expressions and human comprehension judgments, used to evaluate automated models in both generating and understanding references. The study finds that current models perform worse than human pairs in these tasks. However, training an open-weight speaker model with feedback on communicative success improves its performance significantly, even surpassing the best proprietary model."}, 'zh': {'title': '多智能体环境中的指代表达生成与理解', 'desc': '这篇论文介绍了一项任务和数据集，用于在多智能体环境中生成和理解指代表达。在这个任务中，两个智能体需要考虑彼此不同的视觉视角，以生成和理解场景中物体的指代和空间关系。研究收集了2970个人类编写的指代表达，并评估了自动化模型在与人类搭档时作为说话者和听者的表现，发现模型在生成和理解指代方面的表现落后于人类。最后，通过训练一个开放权重的说话者模型，结合听者的沟通成功证据，模型的沟通成功率从58.9%提高到69.3%，甚至超过了最强的专有模型。'}}, 'hash': '7cc5107d8cc84062', 'pub_date_card': {'ru': '4 октября', 'en': 'October 4', 'zh': '10月4日'}}, {'id': 'https://huggingface.co/papers/2410.05255', 'title': 'SePPO: Semi-Policy Preference Optimization for Diffusion Alignment', 'url': 'https://huggingface.co/papers/2410.05255', 'abstract': 'Reinforcement learning from human feedback (RLHF) methods are emerging as a way to fine-tune diffusion models (DMs) for visual generation. However, commonly used on-policy strategies are limited by the generalization capability of the reward model, while off-policy approaches require large amounts of difficult-to-obtain paired human-annotated data, particularly in visual generation tasks. To address the limitations of both on- and off-policy RLHF, we propose a preference optimization method that aligns DMs with preferences without relying on reward models or paired human-annotated data. Specifically, we introduce a Semi-Policy Preference Optimization (SePPO) method. SePPO leverages previous checkpoints as reference models while using them to generate on-policy reference samples, which replace "losing images" in preference pairs. This approach allows us to optimize using only off-policy "winning images." Furthermore, we design a strategy for reference model selection that expands the exploration in the policy space. Notably, we do not simply treat reference samples as negative examples for learning. Instead, we design an anchor-based criterion to assess whether the reference samples are likely to be winning or losing images, allowing the model to selectively learn from the generated reference samples. This approach mitigates performance degradation caused by the uncertainty in reference sample quality. We validate SePPO across both text-to-image and text-to-video benchmarks. SePPO surpasses all previous approaches on the text-to-image benchmarks and also demonstrates outstanding performance on the text-to-video benchmarks. Code will be released in https://github.com/DwanZhang-AI/SePPO.', 'score': 2, 'issue_id': 19, 'pub_date': '2024-10-07', 'pub_date_ru': '7 октября', 'data': {'categories': ['#cv', '#diffusion', '#rlhf', '#video'], 'emoji': '🖼️', 'ru': {'title': 'Оптимизация предпочтений без размеченных данных для генеративных моделей', 'desc': "Статья представляет новый метод оптимизации предпочтений для диффузионных моделей в задачах генерации изображений и видео. Метод SePPO (Semi-Policy Preference Optimization) использует предыдущие чекпоинты модели в качестве референсных и генерирует референсные примеры для замены 'проигрышных' изображений в парах предпочтений. Авторы разработали стратегию выбора референсных моделей и критерий на основе якорей для оценки качества референсных примеров. SePPO превзошел существующие подходы на бенчмарках text-to-image и text-to-video."}, 'en': {'title': 'Revolutionizing Visual Generation with SePPO: Smarter Learning from Human Preferences', 'desc': 'The paper introduces a new method called Semi-Policy Preference Optimization (SePPO) to improve diffusion models for visual generation using reinforcement learning from human feedback. Unlike traditional methods that rely heavily on reward models or large datasets of human-annotated images, SePPO uses previous model checkpoints to generate reference samples, optimizing only with "winning images." This approach allows the model to learn more effectively by using an anchor-based criterion to evaluate the quality of reference samples, avoiding performance issues from uncertain data. The method shows superior results in both text-to-image and text-to-video generation tasks, outperforming existing techniques.'}, 'zh': {'title': 'SePPO：无需奖励模型的视觉生成优化', 'desc': '这篇论文介绍了一种新的方法来优化视觉生成模型，称为半策略偏好优化（SePPO）。传统的强化学习方法需要大量的人类标注数据，而SePPO通过使用之前的模型检查点作为参考，减少了对这些数据的依赖。SePPO通过选择性地学习生成的参考样本，避免了因样本质量不确定性导致的性能下降。实验结果表明，SePPO在文本到图像和文本到视频的基准测试中表现优异。'}}, 'hash': '1202d13e6d3d4583', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}}, {'id': 'https://huggingface.co/papers/2410.03160', 'title': 'Redefining Temporal Modeling in Video Diffusion: The Vectorized Timestep Approach', 'url': 'https://huggingface.co/papers/2410.03160', 'abstract': "Diffusion models have revolutionized image generation, and their extension to video generation has shown promise. However, current video diffusion models~(VDMs) rely on a scalar timestep variable applied at the clip level, which limits their ability to model complex temporal dependencies needed for various tasks like image-to-video generation. To address this limitation, we propose a frame-aware video diffusion model~(FVDM), which introduces a novel vectorized timestep variable~(VTV). Unlike conventional VDMs, our approach allows each frame to follow an independent noise schedule, enhancing the model's capacity to capture fine-grained temporal dependencies. FVDM's flexibility is demonstrated across multiple tasks, including standard video generation, image-to-video generation, video interpolation, and long video synthesis. Through a diverse set of VTV configurations, we achieve superior quality in generated videos, overcoming challenges such as catastrophic forgetting during fine-tuning and limited generalizability in zero-shot methods.Our empirical evaluations show that FVDM outperforms state-of-the-art methods in video generation quality, while also excelling in extended tasks. By addressing fundamental shortcomings in existing VDMs, FVDM sets a new paradigm in video synthesis, offering a robust framework with significant implications for generative modeling and multimedia applications.", 'score': 1, 'issue_id': 22, 'pub_date': '2024-10-04', 'pub_date_ru': '4 октября', 'data': {'categories': ['#diffusion', '#video'], 'emoji': '🎬', 'ru': {'title': 'Революция в генерации видео: покадровая диффузионная модель', 'desc': 'Статья представляет новый подход к генерации видео с использованием диффузионных моделей. Авторы предлагают модель FVDM (Frame-aware Video Diffusion Model), которая вводит векторизованную переменную временного шага (VTV) для каждого кадра. Это позволяет модели лучше улавливать сложные временные зависимости в видео. FVDM демонстрирует превосходные результаты в различных задачах, включая генерацию видео из изображений и интерполяцию видео.'}, 'en': {'title': 'Frame-Aware Diffusion: Revolutionizing Video Generation', 'desc': 'The paper introduces a new approach to video generation using diffusion models, called the frame-aware video diffusion model (FVDM). Unlike traditional models that use a single timestep for an entire video clip, FVDM uses a vectorized timestep variable, allowing each frame to have its own noise schedule. This innovation enables the model to better capture complex temporal dependencies, improving the quality of generated videos across various tasks. The results show that FVDM outperforms existing methods, offering a more flexible and robust framework for video synthesis.'}, 'zh': {'title': '帧感知扩散模型：视频生成的新范式', 'desc': '扩散模型在图像生成中取得了革命性进展，并在视频生成中展现了潜力。然而，现有的视频扩散模型在处理复杂的时间依赖性时存在局限性。为了解决这个问题，我们提出了一种帧感知视频扩散模型，通过引入矢量化时间步变量来增强模型的时间依赖性捕捉能力。实验结果表明，该模型在视频生成质量上优于现有方法，并在多项任务中表现出色。'}}, 'hash': '9b55e7feeb00d83b', 'pub_date_card': {'ru': '4 октября', 'en': 'October 4', 'zh': '10月4日'}}, {'id': 'https://huggingface.co/papers/2410.03960', 'title': 'SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving Model Transformation', 'url': 'https://huggingface.co/papers/2410.03960', 'abstract': "LLM inference for popular enterprise use cases, such as summarization, RAG, and code-generation, typically observes orders of magnitude longer prompt lengths than generation lengths. This characteristic leads to high cost of prefill and increased response latency. In this paper, we present SwiftKV, a novel model transformation and distillation procedure specifically designed to reduce the time and cost of processing prompt tokens while preserving high quality of generated tokens. SwiftKV combines three key mechanisms: i) SingleInputKV, which prefills later layers' KV cache using a much earlier layer's output, allowing prompt tokens to skip much of the model computation, ii) AcrossKV, which merges the KV caches of neighboring layers to reduce the memory footprint and support larger batch size for higher throughput, and iii) a knowledge-preserving distillation procedure that can adapt existing LLMs for SwiftKV with minimal accuracy impact and low compute and data requirement. For Llama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50% and the memory requirement of the KV cache by 62.5% while incurring minimum quality degradation across a wide range of tasks. In the end-to-end inference serving using an optimized vLLM implementation, SwiftKV realizes up to 2x higher aggregate throughput and 60% lower time per output token. It can achieve a staggering 560 TFlops/GPU of normalized inference throughput, which translates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100 GPUs.", 'score': 0, 'issue_id': 21, 'pub_date': '2024-10-04', 'pub_date_ru': '4 октября', 'data': {'categories': ['#architecture', '#inference', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'SwiftKV: Революция в ускорении инференса LLM', 'desc': 'SwiftKV - это новый метод трансформации и дистилляции моделей, направленный на ускорение обработки токенов промпта в LLM. Он включает в себя три ключевых механизма: SingleInputKV, AcrossKV и процедуру дистилляции с сохранением знаний. SwiftKV снижает вычислительные требования предварительного заполнения на 50% и требования к памяти KV-кэша на 62,5% для моделей Llama-3.1. В результате достигается двукратное увеличение общей пропускной способности и снижение времени на выходной токен на 60%.'}, 'en': {'title': 'SwiftKV: Speeding Up LLMs Without Sacrificing Quality', 'desc': 'The paper introduces SwiftKV, a method to make large language models (LLMs) more efficient by reducing the time and cost of processing prompt tokens. It uses three main techniques: SingleInputKV, which allows skipping some model computations; AcrossKV, which reduces memory use by merging caches; and a distillation process that adapts existing models with minimal accuracy loss. SwiftKV significantly cuts down on computational and memory needs while maintaining output quality, achieving faster processing speeds and higher throughput. This approach is particularly effective for models like Llama-3.1, enhancing their performance in enterprise applications.'}, 'zh': {'title': 'SwiftKV：高效处理提示令牌的创新方案', 'desc': 'SwiftKV 是一种新型模型转换和蒸馏方法，旨在降低处理提示令牌的时间和成本，同时保持生成令牌的高质量。它通过三个关键机制实现：SingleInputKV 允许提示令牌跳过大部分模型计算，AcrossKV 合并相邻层的 KV 缓存以减少内存占用，并通过知识保留蒸馏程序适应现有 LLM。SwiftKV 在 Llama-3.1-8B 和 70B 上减少了 50% 的预填充计算需求和 62.5% 的 KV 缓存内存需求。最终，SwiftKV 在优化的 vLLM 实现中实现了高达 2 倍的总吞吐量和 60% 的每个输出令牌时间降低。'}}, 'hash': 'f79582d6875d464c', 'pub_date_card': {'ru': '4 октября', 'en': 'October 4', 'zh': '10月4日'}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            const themeToggle = document.getElementById('theme-toggle');
            let settingSortBy = localStorage.getItem('sort_by');
            const sortDropdown = document.getElementById('sort-dropdown');
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }
            
            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (3)', '#agi', '#alignment', '#architecture (4)', '#audio (1)', '#benchmark (6)', '#cv (5)', '#data', '#dataset (6)', '#diffusion (5)', '#edge_computing', '#ethics', '#games', '#graphs', '#hallucinations (3)', '#inference (3)', '#interpretability (3)', '#long_context (2)', '#math (4)', '#medicine (1)', '#multilingual', '#multimodal (5)', '#optimization (2)', '#plp', '#rag (1)', '#reasoning (2)', '#rl', '#rlhf (2)', '#robotics', '#security', '#story_generation', '#survey', '#synthetic (3)', '#training (3)', '#transfer_learning', '#translation', '#video (4)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles = selectedCategories.length === 0
                ? articlesData
                : articlesData.filter(article => 
                    article.data && article.data.categories && 
                    article.data.categories.some(cat => selectedCategories.includes(cat))
                );

            console.log('filteredArticles', filteredArticles)

            //if (filteredArticles.length === 0) {
            //    selectedArticles = articlesData;
            //    selectedCategories = [];
            //    cleanCategorySelection();
            //} else {
            //    selectedArticles = filteredArticles;
            //}

            selectedArticles = filteredArticles;

            console.log('selectedArticles', selectedArticles)

            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].join(" ");
                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="background-digit">${index + 1}</div>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <h2>${item['data']['emoji']} ${item['title']}</h2>
                            <p class="meta"><svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path transform="translate(0, 2)" fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> ${item['score']}. ${title}</p>
                            <p class="pub-date">📝 ${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>
                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>
                            <p class="tags">${cats}</p>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            }
            if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
        });

        clearCategoriesButton.addEventListener('click', clearAllCategories);
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2024-10-07 20:26',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];       
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink() {
            if (isToday('2024-10-07 20:26')) {
                const element = document.getElementById('nav-next');
                if (element) {    
                    element.style.display = 'none';
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink(); 
        initializeLanguageFlags();
        updateLocalization();
    </script>
</body>
</html>
    