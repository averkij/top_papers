
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF (23 статьи)</title>
    <link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #0989eacf;
            --secondary-color: #fff;
            --background-color: #f5f5f5;
            --text-color: #333333;
            --header-color: #0989eacf;
            --body-color: #f5f5f5;
            --menu-color: #002370;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        header {
            padding: 3em 0;
            text-align: center;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 2em;
            padding: 10px 0 20px 0;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.tags {
            color: #555;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);
            transition: background-color 0.2s ease;
            display: flex;
            flex-direction: column;
            position: relative;
        }
        .article-content {
            padding: 1.5em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
        }
        body.dark-theme>div>main>article {
            background-color: #444;
        }
        body.light-theme>div>main>article {
            background-color: #fff;
        }
        body.dark-theme>div>main>article:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
        }
        .pub-date {
            font-size: 0.9em;
            margin-bottom: 0.8em;
            font-weight: 300;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 1em;
            position: absolute;
            bottom: 10px;
            font-weight: 300;
            font-family: 'Roboto Slab';
        }
        .background-digit {
            position: absolute;
            bottom: -20px;
            right: -10px;
            font-size: 12em;
            font-weight: bold;
            color: rgba(0, 0, 0, 0.03);
            z-index: 0;
            line-height: 1;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 80px;
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        a:hover {
            color: var(--secondary-color);
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            padding: 1em 0;
            margin-top: 2em;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: fixed;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
        }
        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
        }
        .update-info-container {
            flex: 1;
        }
        .sort-container {
            flex: 2;
        }
        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .category-toggle {
            display: none;
            margin-bottom: 10px;
            margin-top: 15px;
            cursor: pointer;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }
        
        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        body.light-theme>div>main>article.xfae9004e0f12e4fc { background: url("https://hfday.ru/img/20241007/fae9004e0f12e4fc.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.xfae9004e0f12e4fc:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.xfae9004e0f12e4fc { background: url("https://hfday.ru/img/20241007/fae9004e0f12e4fc.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.xfae9004e0f12e4fc:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.xd3da276d1028f171 { background: url("https://hfday.ru/img/20241003/d3da276d1028f171.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.xd3da276d1028f171:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.xd3da276d1028f171 { background: url("https://hfday.ru/img/20241003/d3da276d1028f171.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.xd3da276d1028f171:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.xfd005deb7206d4fe { background: url("https://hfday.ru/img/20241006/fd005deb7206d4fe.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.xfd005deb7206d4fe:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.xfd005deb7206d4fe { background: url("https://hfday.ru/img/20241006/fd005deb7206d4fe.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.xfd005deb7206d4fe:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x9138501c89f38809 { background: url("https://hfday.ru/img/20241003/9138501c89f38809.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x9138501c89f38809:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x9138501c89f38809 { background: url("https://hfday.ru/img/20241003/9138501c89f38809.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x9138501c89f38809:hover { background-color: rgba(60,60,60,0.92) !important;}

        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .sort-container {
                margin-top: 0px;
                text-align: left;
                width: 100%;
            .sort-dropdown {
                float: right;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiffRu(dateString) {
        const timeUnits = {
            minute: ["минуту", "минуты", "минут"],
            hour: ["час", "часа", "часов"],
            day: ["день", "дня", "дней"]
        };

        function getRussianPlural(number, words) {
            if (number % 10 === 1 && number % 100 !== 11) {
                return words[0];
            } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                return words[1];
            } else {
                return words[2];
            }
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);

        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes == 0) {
            return 'только что';
        }
        else if (minutes < 60) {
            return `${minutes} ${getRussianPlural(minutes, timeUnits.minute)} назад`;
        } else if (hours < 24) {
            return `${hours} ${getRussianPlural(hours, timeUnits.hour)} назад`;
        } else {
            return `${days} ${getRussianPlural(days, timeUnits.day)} назад`;
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function formatArticlesTitle(number) {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;

        let word;

        if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
            word = "статей";
        } else if (lastDigit === 1) {
            word = "статья";
        } else if (lastDigit >= 2 && lastDigit <= 4) {
            word = "статьи";
        } else {
            word = "статей";
        }

        return `${number} ${word}`;
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p>7 октября | 23 статьи</p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item" id="nav-prev"><a href="/d/2024-10-04.html">⬅️ 04.10</a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-10-08.html">➡️ 08.10</a></span>
            <!--<span class="nav-item" id="nav-weekly">Топ за неделю</span>
            <span class="nav-item" id="nav-weekly">Топ за месяц</span>-->
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 Сортировка по</label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="category-toggle">
            <div class="svg-container">
                <span id="category-toggle">🔍 Фильтр</span>
                <svg height="3" width="200">
                    <line x1="0" y1="0" x2="200" y2="0" 
                        stroke="black" 
                        stroke-width="2" 
                        stroke-dasharray="3, 3" />
                </svg>
            </div>
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">градиент обреченный</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>    
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2410.05258', 'title': 'Differential Transformer', 'url': 'https://huggingface.co/papers/2410.05258', 'abstract': 'Transformer tends to overallocate attention to irrelevant context. In this work, we introduce Diff Transformer, which amplifies attention to the relevant context while canceling noise. Specifically, the differential attention mechanism calculates attention scores as the difference between two separate softmax attention maps. The subtraction cancels noise, promoting the emergence of sparse attention patterns. Experimental results on language modeling show that Diff Transformer outperforms Transformer in various settings of scaling up model size and training tokens. More intriguingly, it offers notable advantages in practical applications, such as long-context modeling, key information retrieval, hallucination mitigation, in-context learning, and reduction of activation outliers. By being less distracted by irrelevant context, Diff Transformer can mitigate hallucination in question answering and text summarization. For in-context learning, Diff Transformer not only enhances accuracy but is also more robust to order permutation, which was considered as a chronic robustness issue. The results position Diff Transformer as a highly effective and promising architecture to advance large language models.', 'score': 49, 'issue_id': 14, 'pub_date': '2024-10-07', 'pub_date_ru': '7 октября', 'data': {'desc': 'Статья представляет новую архитектуру Diff Transformer, которая улучшает внимание к релевантному контексту и подавляет шум. Механизм дифференциального внимания вычисляет оценки внимания как разницу между двумя отдельными картами внимания softmax. Эксперименты показывают, что Diff Transformer превосходит обычный Transformer в различных задачах обработки естественного языка. Модель демонстрирует преимущества в моделировании длинного контекста, извлечении ключевой информации, снижении галлюцинаций и обучении в контексте.', 'tags': ['#DiffTransformer', '#DifferentialAttention', '#LongContextModeling'], 'emoji': '🔍', 'title': 'Diff Transformer: точнее фокусируемся на важном, отсекая шум', 'categories': ['#architecture', '#hallucination', '#inference']}, 'hash': 'fae9004e0f12e4fc'}, {'id': 'https://huggingface.co/papers/2410.02707', 'title': 'LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations', 'url': 'https://huggingface.co/papers/2410.02707', 'abstract': 'Large language models (LLMs) often produce errors, including factual inaccuracies, biases, and reasoning failures, collectively referred to as "hallucinations". Recent studies have demonstrated that LLMs\' internal states encode information regarding the truthfulness of their outputs, and that this information can be utilized to detect errors. In this work, we show that the internal representations of LLMs encode much more information about truthfulness than previously recognized. We first discover that the truthfulness information is concentrated in specific tokens, and leveraging this property significantly enhances error detection performance. Yet, we show that such error detectors fail to generalize across datasets, implying that -- contrary to prior claims -- truthfulness encoding is not universal but rather multifaceted. Next, we show that internal representations can also be used for predicting the types of errors the model is likely to make, facilitating the development of tailored mitigation strategies. Lastly, we reveal a discrepancy between LLMs\' internal encoding and external behavior: they may encode the correct answer, yet consistently generate an incorrect one. Taken together, these insights deepen our understanding of LLM errors from the model\'s internal perspective, which can guide future research on enhancing error analysis and mitigation.', 'score': 25, 'issue_id': 15, 'pub_date': '2024-10-03', 'pub_date_ru': '3 октября', 'data': {'desc': 'Исследование показывает, что внутренние представления больших языковых моделей (LLM) содержат больше информации о достоверности генерируемых данных, чем считалось ранее. Авторы обнаружили, что эта информация сконцентрирована в определенных токенах, что позволяет значительно улучшить обнаружение ошибок. Однако детекторы ошибок не обобщаются между датасетами, что указывает на отсутствие универсального кодирования достоверности. Исследование также выявило, что LLM могут внутренне кодировать правильный ответ, но при этом генерировать неверный.', 'tags': ['#hallucinations', '#errorDetection', '#internalRepresentations'], 'emoji': '🔍', 'title': 'Раскрытие тайн внутренних представлений LLM для улучшения обнаружения ошибок', 'categories': ['#hallucination', '#interpretability']}, 'hash': 'd3da276d1028f171'}, {'id': 'https://huggingface.co/papers/2410.04364', 'title': "VideoGuide: Improving Video Diffusion Models without Training Through a Teacher's Guide", 'url': 'https://huggingface.co/papers/2410.04364', 'abstract': "Text-to-image (T2I) diffusion models have revolutionized visual content creation, but extending these capabilities to text-to-video (T2V) generation remains a challenge, particularly in preserving temporal consistency. Existing methods that aim to improve consistency often cause trade-offs such as reduced imaging quality and impractical computational time. To address these issues we introduce VideoGuide, a novel framework that enhances the temporal consistency of pretrained T2V models without the need for additional training or fine-tuning. Instead, VideoGuide leverages any pretrained video diffusion model (VDM) or itself as a guide during the early stages of inference, improving temporal quality by interpolating the guiding model's denoised samples into the sampling model's denoising process. The proposed method brings about significant improvement in temporal consistency and image fidelity, providing a cost-effective and practical solution that synergizes the strengths of various video diffusion models. Furthermore, we demonstrate prior distillation, revealing that base models can achieve enhanced text coherence by utilizing the superior data prior of the guiding model through the proposed method. Project Page: http://videoguide2025.github.io/", 'score': 21, 'issue_id': 13, 'pub_date': '2024-10-06', 'pub_date_ru': '6 октября', 'data': {'desc': 'VideoGuide - это новая система, которая улучшает временную согласованность предобученных моделей генерации видео по тексту без дополнительного обучения. Она использует предобученную модель диффузии видео в качестве гида на ранних этапах вывода, интерполируя ее образцы в процесс шумоподавления основной модели. Это значительно повышает временную согласованность и качество изображения, объединяя сильные стороны различных моделей генерации видео. Метод также демонстрирует дистилляцию приора, позволяя базовым моделям улучшить текстовую согласованность.', 'tags': ['#text2video', '#temporalConsistency', '#diffusionModels'], 'emoji': '🎬', 'title': 'VideoGuide: повышение качества генерации видео без переобучения', 'categories': ['#video', '#diffusion']}, 'hash': 'fd005deb7206d4fe'}, {'id': 'https://huggingface.co/papers/2410.02675', 'title': 'FAN: Fourier Analysis Networks', 'url': 'https://huggingface.co/papers/2410.02675', 'abstract': 'Despite the remarkable success achieved by neural networks, particularly those represented by MLP and Transformer, we reveal that they exhibit potential flaws in the modeling and reasoning of periodicity, i.e., they tend to memorize the periodic data rather than genuinely understanding the underlying principles of periodicity. However, periodicity is a crucial trait in various forms of reasoning and generalization, underpinning predictability across natural and engineered systems through recurring patterns in observations. In this paper, we propose FAN, a novel network architecture based on Fourier Analysis, which empowers the ability to efficiently model and reason about periodic phenomena. By introducing Fourier Series, the periodicity is naturally integrated into the structure and computational processes of the neural network, thus achieving a more accurate expression and prediction of periodic patterns. As a promising substitute to multi-layer perceptron (MLP), FAN can seamlessly replace MLP in various models with fewer parameters and FLOPs. Through extensive experiments, we demonstrate the effectiveness of FAN in modeling and reasoning about periodic functions, and the superiority and generalizability of FAN across a range of real-world tasks, including symbolic formula representation, time series forecasting, and language modeling.', 'score': 21, 'issue_id': 12, 'pub_date': '2024-10-03', 'pub_date_ru': '3 октября', 'data': {'desc': 'Статья представляет новую архитектуру нейронной сети под названием FAN, основанную на анализе Фурье. FAN эффективно моделирует и обрабатывает периодические явления, преодолевая ограничения традиционных нейронных сетей в этой области. Авторы демонстрируют превосходство FAN над многослойным перцептроном (MLP) в различных задачах, включая представление символьных формул и прогнозирование временных рядов. FAN интегрирует периодичность в структуру сети, обеспечивая более точное выражение и предсказание периодических паттернов.', 'tags': ['#FourierAnalysisNetwork', '#ПериодическоеМоделирование', '#ЗаменаMLP'], 'emoji': '🔄', 'title': 'FAN: Революция в моделировании периодичности нейронными сетями', 'categories': ['#architecture', '#math', '#training']}, 'hash': '9138501c89f38809'}, {'id': 'https://huggingface.co/papers/2410.05046', 'title': 'Named Clinical Entity Recognition Benchmark', 'url': 'https://huggingface.co/papers/2410.05046', 'abstract': 'This technical report introduces a Named Clinical Entity Recognition Benchmark for evaluating language models in healthcare, addressing the crucial natural language processing (NLP) task of extracting structured information from clinical narratives to support applications like automated coding, clinical trial cohort identification, and clinical decision support.   The leaderboard provides a standardized platform for assessing diverse language models, including encoder and decoder architectures, on their ability to identify and classify clinical entities across multiple medical domains. A curated collection of openly available clinical datasets is utilized, encompassing entities such as diseases, symptoms, medications, procedures, and laboratory measurements. Importantly, these entities are standardized according to the Observational Medical Outcomes Partnership (OMOP) Common Data Model, ensuring consistency and interoperability across different healthcare systems and datasets, and a comprehensive evaluation of model performance. Performance of models is primarily assessed using the F1-score, and it is complemented by various assessment modes to provide comprehensive insights into model performance. The report also includes a brief analysis of models evaluated to date, highlighting observed trends and limitations.   By establishing this benchmarking framework, the leaderboard aims to promote transparency, facilitate comparative analyses, and drive innovation in clinical entity recognition tasks, addressing the need for robust evaluation methods in healthcare NLP.', 'score': 14, 'issue_id': 17, 'pub_date': '2024-10-07', 'pub_date_ru': '7 октября', 'data': {'desc': 'Этот технический отчет представляет новый бенчмарк для оценки языковых моделей в распознавании именованных клинических сущностей. Бенчмарк использует стандартизированные наборы данных для оценки способности моделей идентифицировать и классифицировать клинические сущности в различных медицинских областях. Основной метрикой оценки является F1-мера, дополненная другими режимами оценки для комплексного анализа производительности моделей. Целью бенчмарка является продвижение прозрачности, сравнительного анализа и инноваций в задачах распознавания клинических сущностей.', 'tags': ['#клиническоеНЛП', '#именованныеСущности', '#медицинскиеДанные'], 'emoji': '🏥', 'title': 'Новый стандарт оценки языковых моделей в медицинском NLP', 'categories': ['#benchmark', '#medicine', '#dataset']}, 'hash': '13152fb25949544e'}, {'id': 'https://huggingface.co/papers/2410.05080', 'title': 'ScienceAgentBench: Toward Rigorous Assessment of Language Agents for Data-Driven Scientific Discovery', 'url': 'https://huggingface.co/papers/2410.05080', 'abstract': 'The advancements of language language models (LLMs) have piqued growing interest in developing LLM-based language agents to automate scientific discovery end-to-end, which has sparked both excitement and skepticism about the true capabilities of such agents. In this work, we argue that for an agent to fully automate scientific discovery, it must be able to complete all essential tasks in the workflow. Thus, we call for rigorous assessment of agents on individual tasks in a scientific workflow before making bold claims on end-to-end automation. To this end, we present ScienceAgentBench, a new benchmark for evaluating language agents for data-driven scientific discovery. To ensure the scientific authenticity and real-world relevance of our benchmark, we extract 102 tasks from 44 peer-reviewed publications in four disciplines and engage nine subject matter experts to validate them. We unify the target output for every task to a self-contained Python program file and employ an array of evaluation metrics to examine the generated programs, execution results, and costs. Each task goes through multiple rounds of manual validation by annotators and subject matter experts to ensure its annotation quality and scientific plausibility. We also propose two effective strategies to mitigate data contamination concerns. Using our benchmark, we evaluate five open-weight and proprietary LLMs, each with three frameworks: direct prompting, OpenHands, and self-debug. Given three attempts for each task, the best-performing agent can only solve 32.4% of the tasks independently and 34.3% with expert-provided knowledge. These results underscore the limited capacities of current language agents in generating code for data-driven discovery, let alone end-to-end automation for scientific research.', 'score': 12, 'issue_id': 19, 'pub_date': '2024-10-07', 'pub_date_ru': '7 октября', 'data': {'desc': 'В статье представлен новый бенчмарк ScienceAgentBench для оценки языковых агентов в научных исследованиях. Авторы извлекли 102 задачи из 44 рецензируемых публикаций в четырех дисциплинах и привлекли экспертов для их валидации. Бенчмарк оценивает способность агентов генерировать Python-код для решения научных задач. Результаты показывают, что даже лучший агент решает только 32.4% задач самостоятельно, что указывает на ограниченные возможности современных языковых моделей в научной сфере.', 'tags': ['#languageAgents', '#scientificDiscovery', '#codeGeneration'], 'emoji': '🧪', 'title': 'Реальная оценка возможностей ИИ-агентов в науке', 'categories': ['#benchmark', '#agents']}, 'hash': 'e1f883a68716278f'}, {'id': 'https://huggingface.co/papers/2410.05167', 'title': 'Presto! Distilling Steps and Layers for Accelerating Music Generation', 'url': 'https://huggingface.co/papers/2410.05167', 'abstract': 'Despite advances in diffusion-based text-to-music (TTM) methods, efficient, high-quality generation remains a challenge. We introduce Presto!, an approach to inference acceleration for score-based diffusion transformers via reducing both sampling steps and cost per step. To reduce steps, we develop a new score-based distribution matching distillation (DMD) method for the EDM-family of diffusion models, the first GAN-based distillation method for TTM. To reduce the cost per step, we develop a simple, but powerful improvement to a recent layer distillation method that improves learning via better preserving hidden state variance. Finally, we combine our step and layer distillation methods together for a dual-faceted approach. We evaluate our step and layer distillation methods independently and show each yield best-in-class performance. Our combined distillation method can generate high-quality outputs with improved diversity, accelerating our base model by 10-18x (230/435ms latency for 32 second mono/stereo 44.1kHz, 15x faster than comparable SOTA) -- the fastest high-quality TTM to our knowledge. Sound examples can be found at https://presto-music.github.io/web/.', 'score': 11, 'issue_id': 13, 'pub_date': '2024-10-07', 'pub_date_ru': '7 октября', 'data': {'desc': 'Статья представляет Presto! - новый подход к ускорению генерации музыки на основе текста с использованием диффузионных трансформеров. Авторы разработали метод дистилляции на основе согласования распределений (DMD) для уменьшения количества шагов сэмплирования. Они также улучшили метод послойной дистилляции для снижения вычислительных затрат на каждом шаге. Комбинация этих методов позволяет генерировать высококачественную музыку в 10-18 раз быстрее базовой модели.', 'tags': ['#text-to-music', '#diffusion-models', '#model-distillation'], 'emoji': '🎵', 'title': 'Ускорение генерации музыки по тексту с помощью дистилляции диффузионных моделей', 'categories': ['#audio', '#diffusion', '#inference']}, 'hash': 'f332523f23a7857e'}, {'id': 'https://huggingface.co/papers/2410.05243', 'title': 'Navigating the Digital World as Humans Do: Universal Visual Grounding for GUI Agents', 'url': 'https://huggingface.co/papers/2410.05243', 'abstract': 'Multimodal large language models (MLLMs) are transforming the capabilities of graphical user interface (GUI) agents, facilitating their transition from controlled simulations to complex, real-world applications across various platforms. However, the effectiveness of these agents hinges on the robustness of their grounding capability. Current GUI agents predominantly utilize text-based representations such as HTML or accessibility trees, which, despite their utility, often introduce noise, incompleteness, and increased computational overhead. In this paper, we advocate a human-like embodiment for GUI agents that perceive the environment entirely visually and directly take pixel-level operations on the GUI. The key is visual grounding models that can accurately map diverse referring expressions of GUI elements to their coordinates on the GUI across different platforms. We show that a simple recipe, which includes web-based synthetic data and slight adaptation of the LLaVA architecture, is surprisingly effective for training such visual grounding models. We collect the largest dataset for GUI visual grounding so far, containing 10M GUI elements and their referring expressions over 1.3M screenshots, and use it to train UGround, a strong universal visual grounding model for GUI agents. Empirical results on six benchmarks spanning three categories (grounding, offline agent, and online agent) show that 1) UGround substantially outperforms existing visual grounding models for GUI agents, by up to 20% absolute, and 2) agents with UGround outperform state-of-the-art agents, despite the fact that existing agents use additional text-based input while ours only uses visual perception. These results provide strong support for the feasibility and promises of GUI agents that navigate the digital world as humans do.', 'score': 10, 'issue_id': 19, 'pub_date': '2024-10-07', 'pub_date_ru': '7 октября', 'data': {'desc': 'Статья представляет новый подход к созданию агентов графического пользовательского интерфейса (GUI) с использованием мультимодальных больших языковых моделей. Авторы предлагают модель UGround, которая обучается на визуальном восприятии интерфейса без использования текстовых представлений. Эксперименты показывают, что UGround превосходит существующие модели визуальной привязки для GUI-агентов на 20%. Результаты демонстрируют перспективность создания агентов, навигирующих в цифровом мире подобно людям.', 'tags': ['#визуальная_привязка', '#GUI_агенты', '#UGround'], 'emoji': '👁️', 'title': 'Визуальное восприятие - ключ к эффективным GUI-агентам', 'categories': ['#multimodal', '#agents', '#cv', '#dataset']}, 'hash': 'c91b020a4bdfc21e'}, {'id': 'https://huggingface.co/papers/2410.04534', 'title': 'UniMuMo: Unified Text, Music and Motion Generation', 'url': 'https://huggingface.co/papers/2410.04534', 'abstract': 'We introduce UniMuMo, a unified multimodal model capable of taking arbitrary text, music, and motion data as input conditions to generate outputs across all three modalities. To address the lack of time-synchronized data, we align unpaired music and motion data based on rhythmic patterns to leverage existing large-scale music-only and motion-only datasets. By converting music, motion, and text into token-based representation, our model bridges these modalities through a unified encoder-decoder transformer architecture. To support multiple generation tasks within a single framework, we introduce several architectural improvements. We propose encoding motion with a music codebook, mapping motion into the same feature space as music. We introduce a music-motion parallel generation scheme that unifies all music and motion generation tasks into a single transformer decoder architecture with a single training task of music-motion joint generation. Moreover, the model is designed by fine-tuning existing pre-trained single-modality models, significantly reducing computational demands. Extensive experiments demonstrate that UniMuMo achieves competitive results on all unidirectional generation benchmarks across music, motion, and text modalities. Quantitative results are available in the https://hanyangclarence.github.io/unimumo_demo/{project page}.', 'score': 10, 'issue_id': 14, 'pub_date': '2024-10-06', 'pub_date_ru': '6 октября', 'data': {'desc': 'UniMuMo - это унифицированная мультимодальная модель, способная генерировать текст, музыку и движения на основе входных данных в этих модальностях. Модель использует выравнивание неспаренных музыкальных и двигательных данных на основе ритмических паттернов. Архитектура UniMuMo основана на трансформере типа энкодер-декодер с токенизированным представлением всех модальностей. Модель достигает конкурентоспособных результатов на различных бенчмарках по однонаправленной генерации для музыки, движений и текста.', 'tags': ['#мультимодальнаяГенерация', '#выравниваниеМодальностей', '#трансформерныеМодели'], 'emoji': '🎭', 'title': 'UniMuMo: единая модель для генерации текста, музыки и движений', 'categories': ['#multimodal', '#architecture']}, 'hash': '4bfc87467c16c280'}, {'id': 'https://huggingface.co/papers/2410.04734', 'title': 'TLDR: Token-Level Detective Reward Model for Large Vision Language Models', 'url': 'https://huggingface.co/papers/2410.04734', 'abstract': 'Although reward models have been successful in improving multimodal large language models, the reward models themselves remain brutal and contain minimal information. Notably, existing reward models only mimic human annotations by assigning only one binary feedback to any text, no matter how long the text is. In the realm of multimodal language models, where models are required to process both images and texts, a naive reward model may learn implicit biases toward texts and become less grounded in images. In this paper, we propose a Token-Level Detective Reward Model (TLDR) to provide fine-grained annotations to each text token. We first introduce a perturbation-based method to generate synthetic hard negatives and their token-level labels to train TLDR models. Then we show the rich usefulness of TLDR models both in assisting off-the-shelf models to self-correct their generations, and in serving as a hallucination evaluation tool. Finally, we show that TLDR models can significantly speed up human annotation by 3 times to acquire a broader range of high-quality vision language data.', 'score': 10, 'issue_id': 13, 'pub_date': '2024-10-07', 'pub_date_ru': '7 октября', 'data': {'desc': "В статье представлена модель вознаграждения на уровне токенов (TLDR) для мультимодальных языковых моделей. TLDR обеспечивает детальную аннотацию каждого текстового токена, что позволяет лучше учитывать как текстовую, так и визуальную информацию. Метод использует синтетические 'сложные отрицательные примеры' для обучения модели. TLDR может применяться для самокоррекции генераций моделей и оценки галлюцинаций, а также ускоряет процесс аннотации данных в 3 раза.", 'tags': ['#TokenLevelRewardModel', '#MultimodalLLM', '#HallucinationDetection'], 'emoji': '🔍', 'title': 'Точность на уровне токенов: новый подход к мультимодальным языковым моделям', 'categories': ['#rag', '#multimodal', '#hallucination', '#interpretability']}, 'hash': '8c4b74044ea31d8d'}, {'id': 'https://huggingface.co/papers/2410.05229', 'title': 'GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models', 'url': 'https://huggingface.co/papers/2410.05229', 'abstract': "Recent advancements in Large Language Models (LLMs) have sparked interest in their formal reasoning capabilities, particularly in mathematics. The GSM8K benchmark is widely used to assess the mathematical reasoning of models on grade-school-level questions. While the performance of LLMs on GSM8K has significantly improved in recent years, it remains unclear whether their mathematical reasoning capabilities have genuinely advanced, raising questions about the reliability of the reported metrics. To address these concerns, we conduct a large-scale study on several SOTA open and closed models. To overcome the limitations of existing evaluations, we introduce GSM-Symbolic, an improved benchmark created from symbolic templates that allow for the generation of a diverse set of questions. GSM-Symbolic enables more controllable evaluations, providing key insights and more reliable metrics for measuring the reasoning capabilities of models.Our findings reveal that LLMs exhibit noticeable variance when responding to different instantiations of the same question. Specifically, the performance of all models declines when only the numerical values in the question are altered in the GSM-Symbolic benchmark. Furthermore, we investigate the fragility of mathematical reasoning in these models and show that their performance significantly deteriorates as the number of clauses in a question increases. We hypothesize that this decline is because current LLMs cannot perform genuine logical reasoning; they replicate reasoning steps from their training data. Adding a single clause that seems relevant to the question causes significant performance drops (up to 65%) across all state-of-the-art models, even though the clause doesn't contribute to the reasoning chain needed for the final answer. Overall, our work offers a more nuanced understanding of LLMs' capabilities and limitations in mathematical reasoning.", 'score': 8, 'issue_id': 18, 'pub_date': '2024-10-07', 'pub_date_ru': '7 октября', 'data': {'desc': 'Статья посвящена оценке способностей больших языковых моделей (LLM) к математическим рассуждениям. Авторы представляют новый бенчмарк GSM-Symbolic, созданный на основе символических шаблонов для более контролируемой оценки. Исследование показывает, что производительность LLM значительно снижается при изменении числовых значений в вопросах и увеличении количества условий. Авторы предполагают, что современные LLM не способны к подлинным логическим рассуждениям, а лишь воспроизводят шаги рассуждений из обучающих данных.', 'tags': ['#математические_рассуждения', '#оценка_LLM', '#GSM-Symbolic'], 'emoji': '🧮', 'title': 'Разоблачение иллюзии математического мышления у языковых моделей', 'categories': ['#benchmark', '#math', '#interpretability', '#reasoning']}, 'hash': '96f3985414b99bf9'}, {'id': 'https://huggingface.co/papers/2410.04698', 'title': 'MathHay: An Automated Benchmark for Long-Context Mathematical Reasoning in LLMs', 'url': 'https://huggingface.co/papers/2410.04698', 'abstract': "Recent large language models (LLMs) have demonstrated versatile capabilities in long-context scenarios. Although some recent benchmarks have been developed to evaluate the long-context capabilities of LLMs, there is a lack of benchmarks evaluating the mathematical reasoning abilities of LLMs over long contexts, which is crucial for LLMs' application in real-world scenarios. In this paper, we introduce MathHay, an automated benchmark designed to assess the long-context mathematical reasoning capabilities of LLMs. Unlike previous benchmarks like Needle in a Haystack, which focus primarily on information retrieval within long texts, MathHay demands models with both information-seeking and complex mathematical reasoning abilities. We conduct extensive experiments on MathHay to assess the long-context mathematical reasoning abilities of eight top-performing LLMs. Even the best-performing model, Gemini-1.5-Pro-002, still struggles with mathematical reasoning over long contexts, achieving only 51.26% accuracy at 128K tokens. This highlights the significant room for improvement on the MathHay benchmark.", 'score': 8, 'issue_id': 15, 'pub_date': '2024-10-07', 'pub_date_ru': '7 октября', 'data': {'desc': 'Статья представляет новый бенчмарк MathHay для оценки способностей больших языковых моделей (LLM) к математическим рассуждениям в контексте длинных текстов. В отличие от существующих бенчмарков, MathHay требует от моделей не только поиска информации, но и сложных математических вычислений. Эксперименты с восемью ведущими LLM показали, что даже лучшая модель Gemini-1.5-Pro-002 достигает точности всего 51.26% на текстах длиной 128 тысяч токенов. Результаты указывают на значительный потенциал для улучшения способностей LLM в области длинных математических рассуждений.', 'tags': ['#LongContextMathReasoning', '#MathHayBenchmark', '#LLMEvaluation'], 'emoji': '🧮', 'title': 'MathHay: Новый вызов для математических способностей языковых моделей', 'categories': ['#benchmark', '#math']}, 'hash': '1e15804dd57fc363'}, {'id': 'https://huggingface.co/papers/2410.04932', 'title': 'OmniBooth: Learning Latent Control for Image Synthesis with Multi-modal Instruction', 'url': 'https://huggingface.co/papers/2410.04932', 'abstract': 'We present OmniBooth, an image generation framework that enables spatial control with instance-level multi-modal customization. For all instances, the multimodal instruction can be described through text prompts or image references. Given a set of user-defined masks and associated text or image guidance, our objective is to generate an image, where multiple objects are positioned at specified coordinates and their attributes are precisely aligned with the corresponding guidance. This approach significantly expands the scope of text-to-image generation, and elevates it to a more versatile and practical dimension in controllability. In this paper, our core contribution lies in the proposed latent control signals, a high-dimensional spatial feature that provides a unified representation to integrate the spatial, textual, and image conditions seamlessly. The text condition extends ControlNet to provide instance-level open-vocabulary generation. The image condition further enables fine-grained control with personalized identity. In practice, our method empowers users with more flexibility in controllable generation, as users can choose multi-modal conditions from text or images as needed. Furthermore, thorough experiments demonstrate our enhanced performance in image synthesis fidelity and alignment across different tasks and datasets. Project page: https://len-li.github.io/omnibooth-web/', 'score': 8, 'issue_id': 14, 'pub_date': '2024-10-07', 'pub_date_ru': '7 октября', 'data': {'desc': 'OmniBooth - это фреймворк для генерации изображений с пространственным контролем и мультимодальной кастомизацией на уровне отдельных объектов. Система позволяет задавать расположение и атрибуты объектов с помощью текстовых запросов или изображений-образцов. Ключевым элементом является предложенный латентный сигнал управления - многомерный пространственный признак, объединяющий пространственные, текстовые и визуальные условия. OmniBooth расширяет возможности текст-в-изображение генерации, предоставляя пользователям гибкий инструмент для контролируемого создания изображений.', 'tags': ['#контролируемая_генерация', '#мультимодальный_синтез', '#пространственное_управление'], 'emoji': '🎨', 'title': 'Управляемое творчество: новый уровень генерации изображений', 'categories': ['#cv', '#multimodal']}, 'hash': '20ada6e6ba364412'}, {'id': 'https://huggingface.co/papers/2410.03825', 'title': 'MonST3R: A Simple Approach for Estimating Geometry in the Presence of Motion', 'url': 'https://huggingface.co/papers/2410.03825', 'abstract': "Estimating geometry from dynamic scenes, where objects move and deform over time, remains a core challenge in computer vision. Current approaches often rely on multi-stage pipelines or global optimizations that decompose the problem into subtasks, like depth and flow, leading to complex systems prone to errors. In this paper, we present Motion DUSt3R (MonST3R), a novel geometry-first approach that directly estimates per-timestep geometry from dynamic scenes. Our key insight is that by simply estimating a pointmap for each timestep, we can effectively adapt DUST3R's representation, previously only used for static scenes, to dynamic scenes. However, this approach presents a significant challenge: the scarcity of suitable training data, namely dynamic, posed videos with depth labels. Despite this, we show that by posing the problem as a fine-tuning task, identifying several suitable datasets, and strategically training the model on this limited data, we can surprisingly enable the model to handle dynamics, even without an explicit motion representation. Based on this, we introduce new optimizations for several downstream video-specific tasks and demonstrate strong performance on video depth and camera pose estimation, outperforming prior work in terms of robustness and efficiency. Moreover, MonST3R shows promising results for primarily feed-forward 4D reconstruction.", 'score': 8, 'issue_id': 14, 'pub_date': '2024-10-04', 'pub_date_ru': '4 октября', 'data': {'desc': 'MonST3R - это новый подход к оценке геометрии динамических сцен в компьютерном зрении. Вместо сложных многоступенчатых систем, он напрямую оценивает геометрию для каждого кадра, адаптируя представление DUST3R для динамических сцен. Несмотря на проблему нехватки данных для обучения, авторы смогли обучить модель, используя дообучение и стратегический подбор датасетов. MonST3R показывает высокую производительность в задачах оценки глубины видео и позы камеры, а также обещающие результаты в 4D-реконструкции.', 'tags': ['#динамическая_геометрия', '#3D_реконструкция', '#монокулярное_видео'], 'emoji': '🎥', 'title': 'MonST3R: Революция в оценке геометрии динамических сцен', 'categories': ['#cv', '#3d', '#video', '#training']}, 'hash': '0327c1225649bc76'}, {'id': 'https://huggingface.co/papers/2410.02884', 'title': 'LLaMA-Berry: Pairwise Optimization for O1-like Olympiad-Level Mathematical Reasoning', 'url': 'https://huggingface.co/papers/2410.02884', 'abstract': 'This paper presents an advanced mathematical problem-solving framework, LLaMA-Berry, for enhancing the mathematical reasoning ability of Large Language Models (LLMs). The framework combines Monte Carlo Tree Search (MCTS) with iterative Self-Refine to optimize the reasoning path and utilizes a pairwise reward model to evaluate different paths globally. By leveraging the self-critic and rewriting capabilities of LLMs, Self-Refine applied to MCTS (SR-MCTS) overcomes the inefficiencies and limitations of conventional step-wise and greedy search algorithms by fostering a more efficient exploration of solution spaces. Pairwise Preference Reward Model~(PPRM), inspired by Reinforcement Learning from Human Feedback (RLHF), is then used to model pairwise preferences between solutions, utilizing an Enhanced Borda Count (EBC) method to synthesize these preferences into a global ranking score to find better answers. This approach addresses the challenges of scoring variability and non-independent distributions in mathematical reasoning tasks. The framework has been tested on general and advanced benchmarks, showing superior performance in terms of search efficiency and problem-solving capability compared to existing methods like ToT and rStar, particularly in complex Olympiad-level benchmarks, including GPQA, AIME24 and AMC23.', 'score': 7, 'issue_id': 16, 'pub_date': '2024-10-03', 'pub_date_ru': '3 октября', 'data': {'desc': 'LLaMA-Berry - это передовая система для решения математических задач, улучшающая способности больших языковых моделей к математическим рассуждениям. Она объединяет метод Монте-Карло для поиска по дереву (MCTS) с итеративным самоулучшением для оптимизации пути рассуждений. Система использует модель парных вознаграждений для глобальной оценки различных путей решения. LLaMA-Berry демонстрирует превосходную производительность по сравнению с существующими методами, особенно на сложных олимпиадных задачах.', 'tags': ['#математическое-рассуждение', '#MCTS', '#самоулучшение-LLM'], 'emoji': '🧮', 'title': 'LLaMA-Berry: Прорыв в математическом мышлении искусственного интеллекта', 'categories': ['#math', '#rlhf', '#training']}, 'hash': 'cc4c09dfca59a8d4'}, {'id': 'https://huggingface.co/papers/2410.05262', 'title': 'TurtleBench: Evaluating Top Language Models via Real-World Yes/No Puzzles', 'url': 'https://huggingface.co/papers/2410.05262', 'abstract': 'As the application of Large Language Models (LLMs) expands, the demand for reliable evaluations increases. Existing LLM evaluation benchmarks primarily rely on static datasets, making it challenging to assess model performance in dynamic interactions with users. Moreover, these benchmarks often depend on specific background knowledge, complicating the measurement of a model\'s logical reasoning capabilities. Other dynamic evaluation methods based on strong models or manual efforts may introduce biases and incur high costs and time demands, hindering large-scale application. To address these issues, we propose TurtleBench. TurtleBench collects real user guesses from our online Turtle Soup Puzzle platform that we developed. This approach allows for the relatively dynamic generation of evaluation datasets, mitigating the risk of model cheating while aligning assessments more closely with genuine user needs for reasoning capabilities, thus enhancing the reliability of evaluations. TurtleBench includes 1,532 user guesses along with the correctness of guesses after annotation. Using this dataset, we thoroughly evaluated nine of the most advanced LLMs available today. Notably, the OpenAI o1 series models did not achieve leading results in these evaluations. We propose several hypotheses for further research, such as "the latent reasoning of o1 utilizes trivial Chain-of-Thought (CoT) techniques" and "increasing CoT length not only provides reasoning benefits but also incurs noise costs."', 'score': 7, 'issue_id': 13, 'pub_date': '2024-10-07', 'pub_date_ru': '7 октября', 'data': {'desc': 'TurtleBench - новый метод оценки больших языковых моделей (LLM), основанный на реальных догадках пользователей в игре Turtle Soup Puzzle. Этот подход позволяет динамически генерировать наборы данных для оценки, снижая риск обмана со стороны моделей и более точно отражая потребности пользователей в способностях рассуждения. В исследовании оценивались девять передовых LLM, причем модели серии OpenAI o1 не показали лидирующих результатов. Авторы выдвигают гипотезы для дальнейших исследований, включая использование тривиальных методов цепочки рассуждений (Chain-of-Thought) в латентных рассуждениях o1.', 'tags': ['#TurtleBench', '#ОценкаLLM', '#ЦепочкаРассуждений'], 'emoji': '🐢', 'title': 'TurtleBench: Динамическая оценка LLM на основе реальных пользовательских догадок', 'categories': ['#benchmark', '#dataset', '#reasoning']}, 'hash': '6d3f3633d606dec9'}, {'id': 'https://huggingface.co/papers/2410.03187', 'title': 'Autonomous Character-Scene Interaction Synthesis from Text Instruction', 'url': 'https://huggingface.co/papers/2410.03187', 'abstract': 'Synthesizing human motions in 3D environments, particularly those with complex activities such as locomotion, hand-reaching, and human-object interaction, presents substantial demands for user-defined waypoints and stage transitions. These requirements pose challenges for current models, leading to a notable gap in automating the animation of characters from simple human inputs. This paper addresses this challenge by introducing a comprehensive framework for synthesizing multi-stage scene-aware interaction motions directly from a single text instruction and goal location. Our approach employs an auto-regressive diffusion model to synthesize the next motion segment, along with an autonomous scheduler predicting the transition for each action stage. To ensure that the synthesized motions are seamlessly integrated within the environment, we propose a scene representation that considers the local perception both at the start and the goal location. We further enhance the coherence of the generated motion by integrating frame embeddings with language input. Additionally, to support model training, we present a comprehensive motion-captured dataset comprising 16 hours of motion sequences in 120 indoor scenes covering 40 types of motions, each annotated with precise language descriptions. Experimental results demonstrate the efficacy of our method in generating high-quality, multi-stage motions closely aligned with environmental and textual conditions.', 'score': 6, 'issue_id': 15, 'pub_date': '2024-10-04', 'pub_date_ru': '4 октября', 'data': {'desc': 'Статья представляет новую систему для синтеза сложных движений человека в 3D-средах на основе текстовых инструкций и целевого положения. Авторы используют авторегрессионную диффузионную модель для генерации последовательных сегментов движения и автономный планировщик для предсказания переходов между этапами действий. Для интеграции движений с окружением предложено специальное представление сцены, учитывающее локальное восприятие. Также создан обширный датасет из 16 часов захваченных движений в 120 indoor-сценах для обучения модели.', 'tags': ['#синтез_движений', '#диффузионные_модели', '#3D_анимация'], 'emoji': '🤖', 'title': 'ИИ оживляет персонажей в 3D по текстовым командам', 'categories': ['#3d', '#diffusion', '#dataset']}, 'hash': '233eb6997f48b10d'}, {'id': 'https://huggingface.co/papers/2410.05057', 'title': 'SELECT: A Large-Scale Benchmark of Data Curation Strategies for Image Classification', 'url': 'https://huggingface.co/papers/2410.05057', 'abstract': 'Data curation is the problem of how to collect and organize samples into a dataset that supports efficient learning. Despite the centrality of the task, little work has been devoted towards a large-scale, systematic comparison of various curation methods. In this work, we take steps towards a formal evaluation of data curation strategies and introduce SELECT, the first large-scale benchmark of curation strategies for image classification.   In order to generate baseline methods for the SELECT benchmark, we create a new dataset, ImageNet++, which constitutes the largest superset of ImageNet-1K to date. Our dataset extends ImageNet with 5 new training-data shifts, each approximately the size of ImageNet-1K itself, and each assembled using a distinct curation strategy. We evaluate our data curation baselines in two ways: (i) using each training-data shift to train identical image classification models from scratch (ii) using the data itself to fit a pretrained self-supervised representation.   Our findings show interesting trends, particularly pertaining to recent methods for data curation such as synthetic data generation and lookup based on CLIP embeddings. We show that although these strategies are highly competitive for certain tasks, the curation strategy used to assemble the original ImageNet-1K dataset remains the gold standard. We anticipate that our benchmark can illuminate the path for new methods to further reduce the gap. We release our checkpoints, code, documentation, and a link to our dataset at https://github.com/jimmyxu123/SELECT.', 'score': 4, 'issue_id': 18, 'pub_date': '2024-10-07', 'pub_date_ru': '7 октября', 'data': {'desc': 'Статья представляет новый эталонный тест SELECT для оценки стратегий курирования данных в задачах классификации изображений. Авторы создали расширенный набор данных ImageNet++, включающий 5 новых сдвигов обучающих данных. Проведена оценка различных стратегий курирования, включая синтетическую генерацию данных и поиск на основе CLIP-эмбеддингов. Результаты показывают, что оригинальная стратегия курирования ImageNet-1K остается золотым стандартом.', 'tags': ['#datasetCuration', '#ImageNetPlusPlus', '#benchmarkEvaluation'], 'emoji': '🔍', 'title': 'Систематическая оценка стратегий курирования данных для компьютерного зрения', 'categories': ['#dataset', '#benchmark', '#cv']}, 'hash': '830db899ae55d1d4'}, {'id': 'https://huggingface.co/papers/2410.03617', 'title': 'What Matters for Model Merging at Scale?', 'url': 'https://huggingface.co/papers/2410.03617', 'abstract': "Model merging aims to combine multiple expert models into a more capable single model, offering benefits such as reduced storage and serving costs, improved generalization, and support for decentralized model development. Despite its promise, previous studies have primarily focused on merging a few small models. This leaves many unanswered questions about the effect of scaling model size and how it interplays with other key factors -- like the base model quality and number of expert models -- , to affect the merged model's performance. This work systematically evaluates the utility of model merging at scale, examining the impact of these different factors. We experiment with merging fully fine-tuned models using 4 popular merging methods -- Averaging, Task~Arithmetic, Dare, and TIES -- across model sizes ranging from 1B-64B parameters and merging up to 8 different expert models. We evaluate the merged models on both held-in tasks, i.e., the expert's training tasks, and zero-shot generalization to unseen held-out tasks. Our experiments provide several new insights about model merging at scale and the interplay between different factors. First, we find that merging is more effective when experts are created from strong base models, i.e., models with good zero-shot performance. Second, larger models facilitate easier merging. Third merging consistently improves generalization capabilities. Notably, when merging 8 large expert models, the merged models often generalize better compared to the multitask trained models. Fourth, we can better merge more expert models when working with larger models. Fifth, different merging methods behave very similarly at larger scales. Overall, our findings shed light on some interesting properties of model merging while also highlighting some limitations. We hope that this study will serve as a reference point on large-scale merging for upcoming research.", 'score': 3, 'issue_id': 18, 'pub_date': '2024-10-04', 'pub_date_ru': '4 октября', 'data': {'desc': 'Статья исследует масштабируемое объединение моделей машинного обучения, анализируя влияние размера модели, качества базовой модели и количества экспертных моделей на производительность объединенной модели. Авторы проводят эксперименты с объединением полностью дообученных моделей размером от 1 до 64 миллиардов параметров, используя четыре популярных метода объединения. Результаты показывают, что объединение более эффективно при использовании сильных базовых моделей и более крупных моделей, а также улучшает способности к обобщению. Исследование предоставляет ценные выводы о свойствах и ограничениях масштабного объединения моделей.', 'tags': ['#model_merging', '#scaling_study', '#zero_shot_generalization'], 'emoji': '🔀', 'title': 'Объединение моделей в масштабе: больше, лучше, эффективнее', 'categories': ['#training', '#optimization']}, 'hash': '1890d06170bcbcb8'}, {'id': 'https://huggingface.co/papers/2410.03959', 'title': 'Grounding Language in Multi-Perspective Referential Communication', 'url': 'https://huggingface.co/papers/2410.03959', 'abstract': "We introduce a task and dataset for referring expression generation and comprehension in multi-agent embodied environments. In this task, two agents in a shared scene must take into account one another's visual perspective, which may be different from their own, to both produce and understand references to objects in a scene and the spatial relations between them. We collect a dataset of 2,970 human-written referring expressions, each paired with human comprehension judgments, and evaluate the performance of automated models as speakers and listeners paired with human partners, finding that model performance in both reference generation and comprehension lags behind that of pairs of human agents. Finally, we experiment training an open-weight speaker model with evidence of communicative success when paired with a listener, resulting in an improvement from 58.9 to 69.3% in communicative success and even outperforming the strongest proprietary model.", 'score': 3, 'issue_id': 12, 'pub_date': '2024-10-04', 'pub_date_ru': '4 октября', 'data': {'desc': 'Статья представляет новую задачу и набор данных для генерации и понимания референциальных выражений в многоагентных средах. Два агента в общей сцене должны учитывать визуальную перспективу друг друга для создания и понимания ссылок на объекты и их пространственные отношения. Авторы собрали датасет из 2970 человеческих референциальных выражений с оценками понимания. Эксперименты показали, что производительность моделей в генерации и понимании ссылок отстает от человеческих пар, но обучение с обратной связью об успешности коммуникации улучшило результаты.', 'tags': ['#референциальные_выражения', '#многоагентные_системы', '#коммуникативное_обучение'], 'emoji': '👥', 'title': 'Улучшение коммуникации ИИ-агентов через учет перспективы собеседника', 'categories': ['#dataset', '#agents', '#multimodal', '#training']}, 'hash': '7cc5107d8cc84062'}, {'id': 'https://huggingface.co/papers/2410.05255', 'title': 'SePPO: Semi-Policy Preference Optimization for Diffusion Alignment', 'url': 'https://huggingface.co/papers/2410.05255', 'abstract': 'Reinforcement learning from human feedback (RLHF) methods are emerging as a way to fine-tune diffusion models (DMs) for visual generation. However, commonly used on-policy strategies are limited by the generalization capability of the reward model, while off-policy approaches require large amounts of difficult-to-obtain paired human-annotated data, particularly in visual generation tasks. To address the limitations of both on- and off-policy RLHF, we propose a preference optimization method that aligns DMs with preferences without relying on reward models or paired human-annotated data. Specifically, we introduce a Semi-Policy Preference Optimization (SePPO) method. SePPO leverages previous checkpoints as reference models while using them to generate on-policy reference samples, which replace "losing images" in preference pairs. This approach allows us to optimize using only off-policy "winning images." Furthermore, we design a strategy for reference model selection that expands the exploration in the policy space. Notably, we do not simply treat reference samples as negative examples for learning. Instead, we design an anchor-based criterion to assess whether the reference samples are likely to be winning or losing images, allowing the model to selectively learn from the generated reference samples. This approach mitigates performance degradation caused by the uncertainty in reference sample quality. We validate SePPO across both text-to-image and text-to-video benchmarks. SePPO surpasses all previous approaches on the text-to-image benchmarks and also demonstrates outstanding performance on the text-to-video benchmarks. Code will be released in https://github.com/DwanZhang-AI/SePPO.', 'score': 2, 'issue_id': 19, 'pub_date': '2024-10-07', 'pub_date_ru': '7 октября', 'data': {'desc': "Статья представляет новый метод оптимизации предпочтений для диффузионных моделей в задачах генерации изображений и видео. Метод SePPO (Semi-Policy Preference Optimization) использует предыдущие чекпоинты модели в качестве референсных и генерирует референсные примеры для замены 'проигрышных' изображений в парах предпочтений. Авторы разработали стратегию выбора референсных моделей и критерий на основе якорей для оценки качества референсных примеров. SePPO превзошел существующие подходы на бенчмарках text-to-image и text-to-video.", 'tags': ['#diffusion_models', '#preference_optimization', '#text_to_image_generation'], 'emoji': '🖼️', 'title': 'Оптимизация предпочтений без размеченных данных для генеративных моделей', 'categories': ['#rlhf', '#diffusion', '#video', '#cv']}, 'hash': '1202d13e6d3d4583'}, {'id': 'https://huggingface.co/papers/2410.03160', 'title': 'Redefining Temporal Modeling in Video Diffusion: The Vectorized Timestep Approach', 'url': 'https://huggingface.co/papers/2410.03160', 'abstract': "Diffusion models have revolutionized image generation, and their extension to video generation has shown promise. However, current video diffusion models~(VDMs) rely on a scalar timestep variable applied at the clip level, which limits their ability to model complex temporal dependencies needed for various tasks like image-to-video generation. To address this limitation, we propose a frame-aware video diffusion model~(FVDM), which introduces a novel vectorized timestep variable~(VTV). Unlike conventional VDMs, our approach allows each frame to follow an independent noise schedule, enhancing the model's capacity to capture fine-grained temporal dependencies. FVDM's flexibility is demonstrated across multiple tasks, including standard video generation, image-to-video generation, video interpolation, and long video synthesis. Through a diverse set of VTV configurations, we achieve superior quality in generated videos, overcoming challenges such as catastrophic forgetting during fine-tuning and limited generalizability in zero-shot methods.Our empirical evaluations show that FVDM outperforms state-of-the-art methods in video generation quality, while also excelling in extended tasks. By addressing fundamental shortcomings in existing VDMs, FVDM sets a new paradigm in video synthesis, offering a robust framework with significant implications for generative modeling and multimedia applications.", 'score': 1, 'issue_id': 22, 'pub_date': '2024-10-04', 'pub_date_ru': '4 октября', 'data': {'desc': 'Статья представляет новый подход к генерации видео с использованием диффузионных моделей. Авторы предлагают модель FVDM (Frame-aware Video Diffusion Model), которая вводит векторизованную переменную временного шага (VTV) для каждого кадра. Это позволяет модели лучше улавливать сложные временные зависимости в видео. FVDM демонстрирует превосходные результаты в различных задачах, включая генерацию видео из изображений и интерполяцию видео.', 'tags': ['#диффузионные_модели', '#генерация_видео', '#временные_зависимости'], 'emoji': '🎬', 'title': 'Революция в генерации видео: покадровая диффузионная модель', 'categories': ['#video', '#diffusion']}, 'hash': '9b55e7feeb00d83b'}, {'id': 'https://huggingface.co/papers/2410.03960', 'title': 'SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving Model Transformation', 'url': 'https://huggingface.co/papers/2410.03960', 'abstract': "LLM inference for popular enterprise use cases, such as summarization, RAG, and code-generation, typically observes orders of magnitude longer prompt lengths than generation lengths. This characteristic leads to high cost of prefill and increased response latency. In this paper, we present SwiftKV, a novel model transformation and distillation procedure specifically designed to reduce the time and cost of processing prompt tokens while preserving high quality of generated tokens. SwiftKV combines three key mechanisms: i) SingleInputKV, which prefills later layers' KV cache using a much earlier layer's output, allowing prompt tokens to skip much of the model computation, ii) AcrossKV, which merges the KV caches of neighboring layers to reduce the memory footprint and support larger batch size for higher throughput, and iii) a knowledge-preserving distillation procedure that can adapt existing LLMs for SwiftKV with minimal accuracy impact and low compute and data requirement. For Llama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50% and the memory requirement of the KV cache by 62.5% while incurring minimum quality degradation across a wide range of tasks. In the end-to-end inference serving using an optimized vLLM implementation, SwiftKV realizes up to 2x higher aggregate throughput and 60% lower time per output token. It can achieve a staggering 560 TFlops/GPU of normalized inference throughput, which translates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100 GPUs.", 'score': 0, 'issue_id': 21, 'pub_date': '2024-10-04', 'pub_date_ru': '4 октября', 'data': {'desc': 'SwiftKV - это новый метод трансформации и дистилляции моделей, направленный на ускорение обработки токенов промпта в LLM. Он включает в себя три ключевых механизма: SingleInputKV, AcrossKV и процедуру дистилляции с сохранением знаний. SwiftKV снижает вычислительные требования предварительного заполнения на 50% и требования к памяти KV-кэша на 62,5% для моделей Llama-3.1. В результате достигается двукратное увеличение общей пропускной способности и снижение времени на выходной токен на 60%.', 'tags': ['#SwiftKV', '#LLMOptimization', '#InferenceAcceleration'], 'emoji': '🚀', 'title': 'SwiftKV: Революция в ускорении инференса LLM', 'categories': ['#inference', '#optimization', '#architecture']}, 'hash': 'f79582d6875d464c'}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            const themeToggle = document.getElementById('theme-toggle');
            let settingSortBy = localStorage.getItem('sort_by');
            const sortDropdown = document.getElementById('sort-dropdown');
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }
            
            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }
        
        function createCategoryButtons() {
            const categories = getUniqueCategories(articlesData);
            categories.forEach(category => {
                const button = document.createElement('span');
                button.textContent = category;
                button.className = 'category-button';
                button.onclick = () => toggleCategory(category, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if (selectedArticles.length === articlesData.length) {
                categoryToggle.textContent = '🏷️ Фильтр';
            } else {
                categoryToggle.textContent = `🏷️ Фильтр (${formatArticlesTitle(selectedArticles.length)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles = selectedCategories.length === 0
                ? articlesData
                : articlesData.filter(article => 
                    article.data && article.data.categories && 
                    article.data.categories.some(cat => selectedCategories.includes(cat))
                );

            console.log('filteredArticles', filteredArticles)

            if (filteredArticles.length === 0) {
                selectedArticles = articlesData;
                selectedCategories = [];
                cleanCategorySelection();
            } else {
                selectedArticles = filteredArticles;
            }

            console.log('selectedArticles', selectedArticles)

            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                const explanation = item["data"]["desc"];
                const cats = item["data"]["categories"].join(" ");
                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="background-digit">${index + 1}</div>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <h2>${item['data']['emoji']} ${item['title']}</h2>
                            <p class="meta"><svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path transform="translate(0, 2)" fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> ${item['score']}. ${item['data']['title']}</p>
                            <p class="pub-date">📅 Статья от ${item['pub_date_ru']}</p>
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>
                            <div class="links">
                                <a href="${item['url']}" target="_blank">Статья</a>
                            </div>
                            <p class="tags">${cats}</p>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            }
            if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
        });

        clearCategoriesButton.addEventListener('click', clearAllCategories);
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiffRu('2024-10-07 20:26');
        } 
        function hideNextLink() {
            if (isToday('2024-10-07 20:26')) {
                const element = document.getElementById('nav-next');
                if (element) {    
                    element.style.display = 'none';
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink(); 
    </script>
</body>
</html>
    