{
    "date": {
        "ru": "3 декабря",
        "en": "December 3",
        "zh": "12月3日"
    },
    "time_utc": "2024-12-03 11:09",
    "weekday": 1,
    "issue_id": 917,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.01824",
            "title": "X-Prompt: Towards Universal In-Context Image Generation in Auto-Regressive Vision Language Foundation Models",
            "url": "https://huggingface.co/papers/2412.01824",
            "abstract": "In-context generation is a key component of large language models' (LLMs) open-task generalization capability. By leveraging a few examples as context, LLMs can perform both in-domain and out-of-domain tasks. Recent advancements in auto-regressive vision-language models (VLMs) built upon LLMs have showcased impressive performance in text-to-image generation. However, the potential of in-context learning for general image generation tasks remains largely unexplored. To address this, we introduce X-Prompt, a purely auto-regressive large-vision language model designed to deliver competitive performance across a wide range of both seen and unseen image generation tasks, all within a unified in-context learning framework. X-Prompt incorporates a specialized design that efficiently compresses valuable features from in-context examples, supporting longer in-context token sequences and improving its ability to generalize to unseen tasks. A unified training task for both text and image prediction enables X-Prompt to handle general image generation with enhanced task awareness from in-context examples. Extensive experiments validate the model's performance across diverse seen image generation tasks and its capacity to generalize to previously unseen tasks.",
            "score": 21,
            "issue_id": 911,
            "pub_date": "2024-12-02",
            "pub_date_card": {
                "ru": "2 декабря",
                "en": "December 2",
                "zh": "12月2日"
            },
            "hash": "6b7c2d6555d8df8d",
            "authors": [
                "Zeyi Sun",
                "Ziyang Chu",
                "Pan Zhang",
                "Tong Wu",
                "Xiaoyi Dong",
                "Yuhang Zang",
                "Yuanjun Xiong",
                "Dahua Lin",
                "Jiaqi Wang"
            ],
            "affiliations": [
                "CPII under InnoHK",
                "MThreads AI",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.01824.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#cv",
                    "#games",
                    "#optimization",
                    "#training",
                    "#multimodal"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "X-Prompt: универсальная модель для генерации изображений с контекстным обучением",
                    "desc": "Статья представляет X-Prompt - авторегрессивную мультимодальную модель для генерации изображений. Модель использует контекстное обучение, что позволяет ей решать как знакомые, так и новые задачи генерации изображений. X-Prompt эффективно сжимает признаки из контекстных примеров и поддерживает длинные последовательности токенов контекста. Эксперименты подтверждают способность модели решать разнообразные задачи генерации изображений, включая ранее не встречавшиеся."
                },
                "en": {
                    "title": "X-Prompt: Unlocking Image Generation with In-Context Learning",
                    "desc": "This paper presents X-Prompt, an advanced auto-regressive vision-language model that enhances image generation through in-context learning. By utilizing a few examples as context, X-Prompt can effectively generate images for both familiar and novel tasks. The model is designed to compress important features from these examples, allowing it to manage longer sequences of context and improve generalization. Extensive testing shows that X-Prompt performs well on a variety of image generation tasks, demonstrating its ability to adapt to new challenges."
                },
                "zh": {
                    "title": "X-Prompt：提升图像生成的上下文学习能力",
                    "desc": "本文介绍了一种名为X-Prompt的自回归大规模视觉语言模型，旨在提升图像生成任务的表现。X-Prompt通过利用上下文中的示例，能够在已知和未知的图像生成任务中实现竞争力的性能。该模型采用了专门的设计，能够有效压缩上下文示例中的重要特征，从而支持更长的上下文序列并提高对未知任务的泛化能力。通过统一的训练任务，X-Prompt在文本和图像预测方面表现出色，验证了其在多样化图像生成任务中的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.18499",
            "title": "GATE OpenING: A Comprehensive Benchmark for Judging Open-ended Interleaved Image-Text Generation",
            "url": "https://huggingface.co/papers/2411.18499",
            "abstract": "Multimodal Large Language Models (MLLMs) have made significant strides in visual understanding and generation tasks. However, generating interleaved image-text content remains a challenge, which requires integrated multimodal understanding and generation abilities. While the progress in unified models offers new solutions, existing benchmarks are insufficient for evaluating these methods due to data size and diversity limitations. To bridge this gap, we introduce GATE OpenING (OpenING), a comprehensive benchmark comprising 5,400 high-quality human-annotated instances across 56 real-world tasks. OpenING covers diverse daily scenarios such as travel guide, design, and brainstorming, offering a robust platform for challenging interleaved generation methods. In addition, we present IntJudge, a judge model for evaluating open-ended multimodal generation methods. Trained with a novel data pipeline, our IntJudge achieves an agreement rate of 82. 42% with human judgments, outperforming GPT-based evaluators by 11.34%. Extensive experiments on OpenING reveal that current interleaved generation methods still have substantial room for improvement. Key findings on interleaved image-text generation are further presented to guide the development of next-generation models. The OpenING is open-sourced at https://opening.github.io.",
            "score": 16,
            "issue_id": 914,
            "pub_date": "2024-11-27",
            "pub_date_card": {
                "ru": "27 ноября",
                "en": "November 27",
                "zh": "11月27日"
            },
            "hash": "bec1bf0079934886",
            "authors": [
                "Pengfei Zhou",
                "Xiaopeng Peng",
                "Jiajun Song",
                "Chuanhao Li",
                "Zhaopan Xu",
                "Yue Yang",
                "Ziyao Guo",
                "Hao Zhang",
                "Yuqi Lin",
                "Yefei He",
                "Lirui Zhao",
                "Shuo Liu",
                "Tianhua Li",
                "Yuxuan Xie",
                "Xiaojun Chang",
                "Yu Qiao",
                "Wenqi Shao",
                "Kaipeng Zhang"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2411.18499.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#open_source",
                    "#dataset",
                    "#games"
                ],
                "emoji": "🔀",
                "ru": {
                    "title": "Новый бенчмарк для оценки мультимодальной генерации",
                    "desc": "Статья представляет новый набор данных GATE OpenING для оценки мультимодальных языковых моделей в задачах генерации чередующегося текстово-визуального контента. OpenING включает 5400 аннотированных примеров из 56 реальных задач. Авторы также предлагают модель IntJudge для оценки генерации открытого типа, которая превосходит GPT-based оценщиков на 11.34%. Эксперименты показывают, что существующие методы генерации чередующегося контента имеют значительный потенциал для улучшения."
                },
                "en": {
                    "title": "Bridging the Gap in Multimodal Generation with OpenING!",
                    "desc": "This paper discusses the advancements in Multimodal Large Language Models (MLLMs) for tasks that involve both images and text. It highlights the challenges of generating content that combines these two modalities effectively, which requires a deep understanding of both. To address the limitations of existing benchmarks, the authors introduce GATE OpenING, a new dataset with 5,400 annotated examples across 56 tasks that reflect real-world scenarios. Additionally, they present IntJudge, a model designed to evaluate multimodal generation methods, which shows improved agreement with human judgments compared to previous evaluators."
                },
                "zh": {
                    "title": "推动多模态生成的基准与评估",
                    "desc": "多模态大型语言模型（MLLMs）在视觉理解和生成任务上取得了显著进展。然而，生成交错的图像-文本内容仍然是一个挑战，这需要综合的多模态理解和生成能力。为了解决这一问题，我们引入了GATE OpenING（OpenING），这是一个包含5400个高质量人类标注实例的综合基准，涵盖56个真实世界任务。我们的研究还提出了IntJudge，一个用于评估开放式多模态生成方法的评估模型，显示出与人类判断的高一致性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.18671",
            "title": "TAPTRv3: Spatial and Temporal Context Foster Robust Tracking of Any Point in Long Video",
            "url": "https://huggingface.co/papers/2411.18671",
            "abstract": "In this paper, we present TAPTRv3, which is built upon TAPTRv2 to improve its point tracking robustness in long videos. TAPTRv2 is a simple DETR-like framework that can accurately track any point in real-world videos without requiring cost-volume. TAPTRv3 improves TAPTRv2 by addressing its shortage in querying high quality features from long videos, where the target tracking points normally undergo increasing variation over time. In TAPTRv3, we propose to utilize both spatial and temporal context to bring better feature querying along the spatial and temporal dimensions for more robust tracking in long videos. For better spatial feature querying, we present Context-aware Cross-Attention (CCA), which leverages surrounding spatial context to enhance the quality of attention scores when querying image features. For better temporal feature querying, we introduce Visibility-aware Long-Temporal Attention (VLTA) to conduct temporal attention to all past frames while considering their corresponding visibilities, which effectively addresses the feature drifting problem in TAPTRv2 brought by its RNN-like long-temporal modeling. TAPTRv3 surpasses TAPTRv2 by a large margin on most of the challenging datasets and obtains state-of-the-art performance. Even when compared with methods trained with large-scale extra internal data, TAPTRv3 is still competitive.",
            "score": 11,
            "issue_id": 909,
            "pub_date": "2024-11-27",
            "pub_date_card": {
                "ru": "27 ноября",
                "en": "November 27",
                "zh": "11月27日"
            },
            "hash": "f99e1015e9222dc6",
            "authors": [
                "Jinyuan Qu",
                "Hongyang Li",
                "Shilong Liu",
                "Tianhe Ren",
                "Zhaoyang Zeng",
                "Lei Zhang"
            ],
            "affiliations": [
                "International Digital Economy Academy (IDEA)",
                "South China University of Technology",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.18671.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#benchmark",
                    "#video",
                    "#cv",
                    "#optimization"
                ],
                "emoji": "👁️",
                "ru": {
                    "title": "Контекстное внимание для надежного отслеживания точек в видео",
                    "desc": "TAPTRv3 - это улучшенная версия TAPTRv2 для более надежного отслеживания точек в длинных видео. Система использует пространственный и временной контекст для повышения качества запроса признаков. Введены два новых механизма: Context-aware Cross-Attention (CCA) для улучшения пространственного запроса и Visibility-aware Long-Temporal Attention (VLTA) для временного запроса. TAPTRv3 значительно превосходит предыдущую версию и достигает наилучших результатов на сложных наборах данных."
                },
                "en": {
                    "title": "Enhancing Video Point Tracking with TAPTRv3",
                    "desc": "TAPTRv3 is an advanced framework designed to enhance point tracking in lengthy videos, building on the previous version, TAPTRv2. It improves the ability to query high-quality features by incorporating both spatial and temporal contexts, which helps in maintaining robust tracking despite variations over time. The paper introduces Context-aware Cross-Attention (CCA) for better spatial feature querying and Visibility-aware Long-Temporal Attention (VLTA) for improved temporal feature querying, effectively addressing issues like feature drifting. TAPTRv3 demonstrates significant performance improvements over TAPTRv2 and competes well against other state-of-the-art methods, even those trained on larger datasets."
                },
                "zh": {
                    "title": "TAPTRv3：长视频点跟踪的新突破",
                    "desc": "本文介绍了TAPTRv3，这是在TAPTRv2基础上开发的，旨在提高长视频中的点跟踪鲁棒性。TAPTRv2是一个简单的类似DETR的框架，可以准确跟踪现实视频中的任意点，而无需成本体积。TAPTRv3通过利用空间和时间上下文来改善特征查询，从而在长视频中实现更稳健的跟踪。我们提出了上下文感知交叉注意力（CCA）和可见性感知长时间注意力（VLTA），显著提升了特征查询的质量，超越了TAPTRv2，达到了最先进的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.00131",
            "title": "Open-Sora Plan: Open-Source Large Video Generation Model",
            "url": "https://huggingface.co/papers/2412.00131",
            "abstract": "We introduce Open-Sora Plan, an open-source project that aims to contribute a large generation model for generating desired high-resolution videos with long durations based on various user inputs. Our project comprises multiple components for the entire video generation process, including a Wavelet-Flow Variational Autoencoder, a Joint Image-Video Skiparse Denoiser, and various condition controllers. Moreover, many assistant strategies for efficient training and inference are designed, and a multi-dimensional data curation pipeline is proposed for obtaining desired high-quality data. Benefiting from efficient thoughts, our Open-Sora Plan achieves impressive video generation results in both qualitative and quantitative evaluations. We hope our careful design and practical experience can inspire the video generation research community. All our codes and model weights are publicly available at https://github.com/PKU-YuanGroup/Open-Sora-Plan.",
            "score": 9,
            "issue_id": 911,
            "pub_date": "2024-11-28",
            "pub_date_card": {
                "ru": "28 ноября",
                "en": "November 28",
                "zh": "11月28日"
            },
            "hash": "fa9b0009de797ca2",
            "authors": [
                "Bin Lin",
                "Yunyang Ge",
                "Xinhua Cheng",
                "Zongjian Li",
                "Bin Zhu",
                "Shaodong Wang",
                "Xianyi He",
                "Yang Ye",
                "Shenghai Yuan",
                "Liuhan Chen",
                "Tanghui Jia",
                "Junwu Zhang",
                "Zhenyu Tang",
                "Yatian Pang",
                "Bin She",
                "Cen Yan",
                "Zhiheng Hu",
                "Xiaoyi Dong",
                "Lin Chen",
                "Zhang Pan",
                "Xing Zhou",
                "Shaoling Dong",
                "Yonghong Tian",
                "Li Yuan"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2412.00131.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#data",
                    "#inference",
                    "#training",
                    "#video"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Открытая модель для генерации высококачественного видео",
                    "desc": "Проект Open-Sora Plan представляет собой открытую модель генерации видео высокого разрешения на основе различных пользовательских входных данных. Модель включает в себя вейвлет-поточный вариационный автоэнкодер, совместный денойзер изображений и видео, а также различные контроллеры условий. Разработаны стратегии для эффективного обучения и вывода, а также предложен многомерный конвейер курирования данных. Проект достиг впечатляющих результатов в генерации видео как в качественных, так и в количественных оценках."
                },
                "en": {
                    "title": "Empowering High-Resolution Video Generation with Open-Sora Plan",
                    "desc": "The Open-Sora Plan is an open-source initiative focused on creating a large-scale generative model for producing high-resolution videos that can last for extended periods, tailored to user specifications. It integrates several advanced components, including a Wavelet-Flow Variational Autoencoder and a Joint Image-Video Skiparse Denoiser, to enhance the video generation process. The project also introduces various condition controllers and efficient training strategies, along with a multi-dimensional data curation pipeline to ensure high-quality output. Overall, the Open-Sora Plan demonstrates significant advancements in video generation, aiming to inspire further research in the field."
                },
                "zh": {
                    "title": "开放源代码，生成高质量视频的未来",
                    "desc": "Open-Sora计划是一个开源项目，旨在基于用户输入生成高分辨率的长时视频。该项目包含多个组件，如小波流变分自编码器和联合图像-视频去噪器，支持整个视频生成过程。我们还设计了多种高效的训练和推理策略，并提出了多维数据策划管道，以获取高质量数据。通过这些高效的设计，Open-Sora计划在视频生成的定性和定量评估中取得了令人印象深刻的结果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.01819",
            "title": "Switti: Designing Scale-Wise Transformers for Text-to-Image Synthesis",
            "url": "https://huggingface.co/papers/2412.01819",
            "abstract": "This work presents Switti, a scale-wise transformer for text-to-image generation. Starting from existing next-scale prediction AR models, we first explore them for T2I generation and propose architectural modifications to improve their convergence and overall performance. We then observe that self-attention maps of our pretrained scale-wise AR model exhibit weak dependence on preceding scales. Based on this insight, we propose a non-AR counterpart facilitating {sim}11% faster sampling and lower memory usage while also achieving slightly better generation quality.Furthermore, we reveal that classifier-free guidance at high-resolution scales is often unnecessary and can even degrade performance. %may be not only unnecessary but potentially detrimental. By disabling guidance at these scales, we achieve an additional sampling acceleration of {sim}20% and improve the generation of fine-grained details. Extensive human preference studies and automated evaluations show that Switti outperforms existing T2I AR models and competes with state-of-the-art T2I diffusion models while being up to 7{times} faster.",
            "score": 7,
            "issue_id": 914,
            "pub_date": "2024-12-02",
            "pub_date_card": {
                "ru": "2 декабря",
                "en": "December 2",
                "zh": "12月2日"
            },
            "hash": "25cf4512f592aea4",
            "authors": [
                "Anton Voronov",
                "Denis Kuznedelev",
                "Mikhail Khoroshikh",
                "Valentin Khrulkov",
                "Dmitry Baranchuk"
            ],
            "affiliations": [
                "HSE University",
                "MIPT",
                "Skoltech",
                "Yandex Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.01819.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#architecture",
                    "#video",
                    "#cv",
                    "#optimization"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Switti: быстрый и эффективный генератор изображений по тексту",
                    "desc": "Статья представляет Switti - трансформер для генерации изображений по текстовому описанию. Авторы предлагают архитектурные модификации для улучшения сходимости и производительности авторегрессионных моделей. На основе наблюдения о слабой зависимости карт внимания от предыдущих масштабов, разработана неавторегрессионная версия, обеспечивающая ускорение и снижение потребления памяти. Исследование также показывает, что отключение guidance на высоких разрешениях дополнительно ускоряет генерацию и улучшает детализацию."
                },
                "en": {
                    "title": "Switti: Accelerating Text-to-Image Generation with Scale-Wise Transformers",
                    "desc": "This paper introduces Switti, a new transformer model designed for generating images from text. It builds on existing autoregressive (AR) models by making architectural changes that enhance performance and speed. The authors find that the self-attention mechanisms in their model do not rely heavily on previous scales, allowing for a non-autoregressive approach that speeds up sampling and reduces memory usage. Additionally, they demonstrate that removing classifier-free guidance at higher resolutions can improve detail generation and further accelerate the process, leading to a model that is significantly faster than current state-of-the-art methods."
                },
                "zh": {
                    "title": "Switti：加速文本到图像生成的变换器",
                    "desc": "本文介绍了Switti，一种用于文本到图像生成的尺度变换器。我们从现有的下一尺度预测自回归模型出发，探索其在T2I生成中的应用，并提出架构修改以提高收敛性和整体性能。研究发现，我们的预训练尺度自回归模型的自注意力图对前一尺度的依赖性较弱，因此我们提出了一种非自回归的替代方案，能够实现约11%的采样加速和更低的内存使用，同时生成质量略有提升。此外，我们发现高分辨率尺度下的无分类器引导通常是不必要的，甚至可能会降低性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.00154",
            "title": "o1-Coder: an o1 Replication for Coding",
            "url": "https://huggingface.co/papers/2412.00154",
            "abstract": "The technical report introduces O1-CODER, an attempt to replicate OpenAI's o1 model with a focus on coding tasks. It integrates reinforcement learning (RL) and Monte Carlo Tree Search (MCTS) to enhance the model's System-2 thinking capabilities. The framework includes training a Test Case Generator (TCG) for standardized code testing, using MCTS to generate code data with reasoning processes, and iteratively fine-tuning the policy model to initially produce pseudocode, followed by the generation of the full code. The report also addresses the opportunities and challenges in deploying o1-like models in real-world applications, suggesting transitioning to the System-2 paradigm and highlighting the imperative for environment state updates. Updated model progress and experimental results will be reported in subsequent versions. All source code, curated datasets, as well as the derived models will be disclosed at https://github.com/ADaM-BJTU/O1-CODER .",
            "score": 7,
            "issue_id": 912,
            "pub_date": "2024-11-29",
            "pub_date_card": {
                "ru": "29 ноября",
                "en": "November 29",
                "zh": "11月29日"
            },
            "hash": "4a9b72e0b9a6ba64",
            "authors": [
                "Yuxiang Zhang",
                "Shangxi Wu",
                "Yuqi Yang",
                "Jiangming Shu",
                "Jinlin Xiao",
                "Chao Kong",
                "Jitao Sang"
            ],
            "affiliations": [
                "School of Computer Science and Technology, Beijing Jiaotong University, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.00154.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#open_source",
                    "#optimization",
                    "#reasoning",
                    "#rl"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "O1-CODER: Усиление ИИ для программирования через Систему-2 мышления",
                    "desc": "Технический отчет представляет O1-CODER - попытку воспроизвести модель o1 от OpenAI для задач программирования. Модель интегрирует обучение с подкреплением и метод Монте-Карло для улучшения способностей мышления Системы-2. Фреймворк включает обучение генератора тестовых случаев, использование MCTS для генерации кода с процессами рассуждения, и итеративную доводку модели политики. Отчет также рассматривает возможности и проблемы развертывания подобных o1 моделей в реальных приложениях."
                },
                "en": {
                    "title": "Enhancing Coding with O1-CODER: A New Approach to System-2 Thinking",
                    "desc": "The paper presents O1-CODER, a model designed to replicate OpenAI's o1 specifically for coding tasks. It combines reinforcement learning (RL) with Monte Carlo Tree Search (MCTS) to improve the model's ability to think critically and reason through problems. The framework includes a Test Case Generator (TCG) for consistent code testing and uses MCTS to create code data that incorporates reasoning steps. The report discusses the potential and challenges of applying o1-like models in practical scenarios, emphasizing the need for updates in the environment state during the learning process."
                },
                "zh": {
                    "title": "O1-CODER：提升编码任务的智能模型",
                    "desc": "本文介绍了O1-CODER，这是一个旨在复制OpenAI的o1模型，专注于编码任务的技术报告。该模型结合了强化学习（RL）和蒙特卡洛树搜索（MCTS），以增强其系统2思维能力。框架中包括训练测试用例生成器（TCG）以进行标准化代码测试，利用MCTS生成带有推理过程的代码数据，并迭代微调策略模型，初步生成伪代码，随后生成完整代码。报告还讨论了在实际应用中部署类似o1模型的机遇和挑战，建议转向系统2范式，并强调环境状态更新的重要性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.01199",
            "title": "TinyFusion: Diffusion Transformers Learned Shallow",
            "url": "https://huggingface.co/papers/2412.01199",
            "abstract": "Diffusion Transformers have demonstrated remarkable capabilities in image generation but often come with excessive parameterization, resulting in considerable inference overhead in real-world applications. In this work, we present TinyFusion, a depth pruning method designed to remove redundant layers from diffusion transformers via end-to-end learning. The core principle of our approach is to create a pruned model with high recoverability, allowing it to regain strong performance after fine-tuning. To accomplish this, we introduce a differentiable sampling technique to make pruning learnable, paired with a co-optimized parameter to simulate future fine-tuning. While prior works focus on minimizing loss or error after pruning, our method explicitly models and optimizes the post-fine-tuning performance of pruned models. Experimental results indicate that this learnable paradigm offers substantial benefits for layer pruning of diffusion transformers, surpassing existing importance-based and error-based methods. Additionally, TinyFusion exhibits strong generalization across diverse architectures, such as DiTs, MARs, and SiTs. Experiments with DiT-XL show that TinyFusion can craft a shallow diffusion transformer at less than 7% of the pre-training cost, achieving a 2times speedup with an FID score of 2.86, outperforming competitors with comparable efficiency. Code is available at https://github.com/VainF/TinyFusion.",
            "score": 7,
            "issue_id": 912,
            "pub_date": "2024-12-02",
            "pub_date_card": {
                "ru": "2 декабря",
                "en": "December 2",
                "zh": "12月2日"
            },
            "hash": "7d1de7010c3fabd7",
            "authors": [
                "Gongfan Fang",
                "Kunjun Li",
                "Xinyin Ma",
                "Xinchao Wang"
            ],
            "affiliations": [
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.01199.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#diffusion",
                    "#training",
                    "#optimization",
                    "#architecture"
                ],
                "emoji": "✂️",
                "ru": {
                    "title": "TinyFusion: Эффективная обрезка диффузионных трансформеров без потери качества",
                    "desc": "TinyFusion - это метод обрезки глубины для уменьшения количества параметров в диффузионных трансформерах. Он использует дифференцируемую технику сэмплирования для обучаемой обрезки и оптимизирует производительность модели после дообучения. TinyFusion превосходит существующие методы обрезки и хорошо обобщается на различные архитектуры. Эксперименты показывают, что метод позволяет создать компактный диффузионный трансформер с двукратным ускорением при сохранении высокого качества генерации изображений."
                },
                "en": {
                    "title": "TinyFusion: Efficient Layer Pruning for Fast and Effective Diffusion Transformers",
                    "desc": "This paper introduces TinyFusion, a method for optimizing diffusion transformers by removing unnecessary layers through a process called depth pruning. The approach focuses on maintaining high performance after the model is pruned, using a learnable technique that allows the model to adapt during fine-tuning. Unlike previous methods that only aim to reduce loss or error, TinyFusion specifically targets the performance of the pruned model post-fine-tuning. Experimental results show that TinyFusion significantly improves layer pruning efficiency and generalization across various architectures, achieving faster inference times and better performance metrics."
                },
                "zh": {
                    "title": "TinyFusion：高效剪枝，提升扩散变换器性能",
                    "desc": "本论文提出了一种名为TinyFusion的深度剪枝方法，旨在减少扩散变换器中的冗余层，从而降低推理开销。我们的方法通过端到端学习实现剪枝，并确保剪枝后的模型在微调后能够恢复强大的性能。TinyFusion引入了一种可微分采样技术，使得剪枝过程可学习，并与共同优化的参数结合，以模拟未来的微调效果。实验结果表明，TinyFusion在扩散变换器的层剪枝方面优于现有的方法，展现出强大的泛化能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.00174",
            "title": "SOLAMI: Social Vision-Language-Action Modeling for Immersive Interaction with 3D Autonomous Characters",
            "url": "https://huggingface.co/papers/2412.00174",
            "abstract": "Human beings are social animals. How to equip 3D autonomous characters with similar social intelligence that can perceive, understand and interact with humans remains an open yet foundamental problem. In this paper, we introduce SOLAMI, the first end-to-end Social vision-Language-Action (VLA) Modeling framework for Immersive interaction with 3D autonomous characters. Specifically, SOLAMI builds 3D autonomous characters from three aspects: (1) Social VLA Architecture: We propose a unified social VLA framework to generate multimodal response (speech and motion) based on the user's multimodal input to drive the character for social interaction. (2) Interactive Multimodal Data: We present SynMSI, a synthetic multimodal social interaction dataset generated by an automatic pipeline using only existing motion datasets to address the issue of data scarcity. (3) Immersive VR Interface: We develop a VR interface that enables users to immersively interact with these characters driven by various architectures. Extensive quantitative experiments and user studies demonstrate that our framework leads to more precise and natural character responses (in both speech and motion) that align with user expectations with lower latency.",
            "score": 5,
            "issue_id": 913,
            "pub_date": "2024-11-29",
            "pub_date_card": {
                "ru": "29 ноября",
                "en": "November 29",
                "zh": "11月29日"
            },
            "hash": "5a30d9c535229ab0",
            "authors": [
                "Jianping Jiang",
                "Weiye Xiao",
                "Zhengyu Lin",
                "Huaizhong Zhang",
                "Tianxiang Ren",
                "Yang Gao",
                "Zhiqian Lin",
                "Zhongang Cai",
                "Lei Yang",
                "Ziwei Liu"
            ],
            "affiliations": [
                "S-Lab, Nanyang Technological University",
                "SenseTime Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.00174.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#games",
                    "#dataset",
                    "#agents",
                    "#3d",
                    "#multimodal"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Создание социально умных 3D персонажей для виртуальной реальности",
                    "desc": "В статье представлен SOLAMI - первый сквозной фреймворк для социального моделирования зрения-языка-действия (VLA) для иммерсивного взаимодействия с 3D автономными персонажами. Фреймворк включает в себя унифицированную архитектуру социального VLA для генерации мультимодальных ответов на основе пользовательского ввода. Для решения проблемы нехватки данных авторы создали синтетический набор данных SynMSI, используя существующие наборы данных о движении. Также разработан VR-интерфейс для иммерсивного взаимодействия пользователей с персонажами."
                },
                "en": {
                    "title": "Empowering 3D Characters with Social Intelligence through SOLAMI",
                    "desc": "This paper presents SOLAMI, a novel framework designed to enhance the social intelligence of 3D autonomous characters. It integrates a Social Vision-Language-Action (VLA) architecture that allows characters to respond to user inputs with both speech and motion, facilitating more natural interactions. To support this, the authors introduce SynMSI, a synthetic dataset that helps overcome the challenge of limited training data for social interactions. Additionally, a virtual reality interface is developed to provide users with an immersive experience when interacting with these characters, resulting in improved response accuracy and reduced latency."
                },
                "zh": {
                    "title": "赋予3D角色社交智能的创新框架",
                    "desc": "本文介绍了SOLAMI，这是第一个端到端的社会视觉-语言-动作（VLA）建模框架，旨在与3D自主角色进行沉浸式互动。SOLAMI从三个方面构建3D自主角色：首先，提出了统一的社会VLA架构，根据用户的多模态输入生成多模态响应（语音和动作），以驱动角色进行社交互动。其次，介绍了SynMSI，这是一个合成的多模态社交互动数据集，通过自动化流程生成，解决了数据稀缺的问题。最后，开发了一个虚拟现实接口，使用户能够与这些角色进行沉浸式互动，实验结果表明，该框架能够提供更精确和自然的角色响应。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.17459",
            "title": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent Video Diffusion Model",
            "url": "https://huggingface.co/papers/2411.17459",
            "abstract": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional latent space, becoming a key component of most Latent Video Diffusion Models (LVDMs) to reduce model training costs. However, as the resolution and duration of generated videos increase, the encoding cost of Video VAEs becomes a limiting bottleneck in training LVDMs. Moreover, the block-wise inference method adopted by most LVDMs can lead to discontinuities of latent space when processing long-duration videos. The key to addressing the computational bottleneck lies in decomposing videos into distinct components and efficiently encoding the critical information. Wavelet transform can decompose videos into multiple frequency-domain components and improve the efficiency significantly, we thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages multi-level wavelet transform to facilitate low-frequency energy flow into latent representation. Furthermore, we introduce a method called Causal Cache, which maintains the integrity of latent space during block-wise inference. Compared to state-of-the-art video VAEs, WF-VAE demonstrates superior performance in both PSNR and LPIPS metrics, achieving 2x higher throughput and 4x lower memory consumption while maintaining competitive reconstruction quality. Our code and models are available at https://github.com/PKU-YuanGroup/WF-VAE.",
            "score": 5,
            "issue_id": 909,
            "pub_date": "2024-11-26",
            "pub_date_card": {
                "ru": "26 ноября",
                "en": "November 26",
                "zh": "11月26日"
            },
            "hash": "9b68162718865b81",
            "authors": [
                "Zongjian Li",
                "Bin Lin",
                "Yang Ye",
                "Liuhan Chen",
                "Xinhua Cheng",
                "Shenghai Yuan",
                "Li Yuan"
            ],
            "affiliations": [
                "Peking University",
                "Peng Cheng Laboratory",
                "Rabbitpre Intelligence"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.17459.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#data",
                    "#video",
                    "#architecture",
                    "#open_source",
                    "#optimization",
                    "#training"
                ],
                "emoji": "🌊",
                "ru": {
                    "title": "Эффективное кодирование видео с помощью вейвлетов",
                    "desc": "Статья представляет новый метод кодирования видео под названием Wavelet Flow VAE (WF-VAE). Этот автоэнкодер использует многоуровневое вейвлет-преобразование для эффективного кодирования низкочастотной информации в видео. WF-VAE решает проблему вычислительных ограничений при обработке видео высокого разрешения и большой длительности в латентных видео-диффузионных моделях (LVDM). Метод также включает технику Causal Cache для сохранения целостности латентного пространства при поблочной обработке."
                },
                "en": {
                    "title": "Efficient Video Encoding with Wavelet Flow VAE",
                    "desc": "The paper introduces Wavelet Flow VAE (WF-VAE), a novel approach to encoding videos into a low-dimensional latent space using wavelet transforms. This method addresses the computational bottleneck of traditional Video VAEs, especially when generating high-resolution and long-duration videos. By decomposing videos into frequency-domain components, WF-VAE enhances the efficiency of encoding critical information. Additionally, the proposed Causal Cache method ensures continuity in the latent space during block-wise inference, resulting in improved performance metrics compared to existing video VAEs."
                },
                "zh": {
                    "title": "小波流VAE：高效视频编码的新方法",
                    "desc": "视频变分自编码器（VAE）将视频编码为低维潜在空间，是大多数潜在视频扩散模型（LVDMs）的关键组成部分，能够降低模型训练成本。然而，随着生成视频的分辨率和时长增加，视频VAE的编码成本成为训练LVDMs的瓶颈。此外，大多数LVDMs采用的块状推理方法在处理长时长视频时可能导致潜在空间的不连续性。为了解决计算瓶颈，我们提出了小波流VAE（WF-VAE），通过多级小波变换有效编码视频的关键信息，并引入因果缓存方法以保持潜在空间的完整性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.01822",
            "title": "VLsI: Verbalized Layers-to-Interactions from Large to Small Vision Language Models",
            "url": "https://huggingface.co/papers/2412.01822",
            "abstract": "The recent surge in high-quality visual instruction tuning samples from closed-source vision-language models (VLMs) such as GPT-4V has accelerated the release of open-source VLMs across various model sizes. However, scaling VLMs to improve performance using larger models brings significant computational challenges, especially for deployment on resource-constrained devices like mobile platforms and robots. To address this, we propose VLsI: Verbalized Layers-to-Interactions, a new VLM family in 2B and 7B model sizes, which prioritizes efficiency without compromising accuracy. VLsI leverages a unique, layer-wise distillation process, introducing intermediate \"verbalizers\" that map features from each layer to natural language space, allowing smaller VLMs to flexibly align with the reasoning processes of larger VLMs. This approach mitigates the training instability often encountered in output imitation and goes beyond typical final-layer tuning by aligning the small VLMs' layer-wise progression with that of the large ones. We validate VLsI across ten challenging vision-language benchmarks, achieving notable performance gains (11.0% for 2B and 17.4% for 7B) over GPT-4V without the need for model scaling, merging, or architectural changes.",
            "score": 4,
            "issue_id": 916,
            "pub_date": "2024-12-02",
            "pub_date_card": {
                "ru": "2 декабря",
                "en": "December 2",
                "zh": "12月2日"
            },
            "hash": "0c3ac9e6ce3f4cd2",
            "authors": [
                "Byung-Kwan Lee",
                "Ryo Hachiuma",
                "Yu-Chiang Frank Wang",
                "Yong Man Ro",
                "Yueh-Hua Wu"
            ],
            "affiliations": [
                "KAIST",
                "NVIDIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.01822.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#small_models",
                    "#architecture",
                    "#transfer_learning",
                    "#optimization",
                    "#training",
                    "#benchmark",
                    "#open_source"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "Эффективное визуально-языковое обучение через послойную вербализацию",
                    "desc": "Исследователи представили новое семейство моделей визуально-языкового обучения под названием VLsI, которое фокусируется на эффективности без ущерба для точности. VLsI использует уникальный процесс послойной дистилляции, вводя промежуточные 'вербализаторы', которые отображают признаки из каждого слоя в пространство естественного языка. Этот подход позволяет меньшим моделям гибко согласовываться с процессами рассуждений более крупных моделей. Авторы продемонстрировали значительное улучшение производительности VLsI по сравнению с GPT-4V на десяти сложных визуально-языковых тестах без необходимости увеличения размера модели или изменения архитектуры."
                },
                "en": {
                    "title": "Efficient Vision-Language Models with Layer-wise Distillation",
                    "desc": "This paper introduces VLsI, a new family of vision-language models designed to enhance efficiency while maintaining accuracy. It employs a layer-wise distillation technique that uses intermediate 'verbalizers' to translate features from each layer into natural language, enabling smaller models to better mimic the reasoning of larger models. This method addresses the common issue of training instability seen in traditional output imitation approaches. The results show significant performance improvements on various benchmarks, demonstrating that smaller models can achieve competitive results without needing to increase their size or alter their architecture."
                },
                "zh": {
                    "title": "高效视觉语言模型的创新之路",
                    "desc": "本文提出了一种新的视觉语言模型（VLM）家族，称为VLsI，旨在提高模型的效率而不牺牲准确性。VLsI采用了一种独特的层级蒸馏过程，通过引入中间的“语言化器”，将每一层的特征映射到自然语言空间，使得较小的VLM能够灵活地与较大VLM的推理过程对齐。该方法有效缓解了输出模仿中常见的训练不稳定性，并通过对齐小型VLM的层级进展与大型VLM的层级进展，超越了典型的最终层调优。我们在十个具有挑战性的视觉语言基准上验证了VLsI，取得了显著的性能提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.19939",
            "title": "VLSBench: Unveiling Visual Leakage in Multimodal Safety",
            "url": "https://huggingface.co/papers/2411.19939",
            "abstract": "Safety concerns of Multimodal large language models (MLLMs) have gradually become an important problem in various applications. Surprisingly, previous works indicate a counter-intuitive phenomenon that using textual unlearning to align MLLMs achieves comparable safety performances with MLLMs trained with image-text pairs. To explain such a counter-intuitive phenomenon, we discover a visual safety information leakage (VSIL) problem in existing multimodal safety benchmarks, i.e., the potentially risky and sensitive content in the image has been revealed in the textual query. In this way, MLLMs can easily refuse these sensitive text-image queries according to textual queries. However, image-text pairs without VSIL are common in real-world scenarios and are overlooked by existing multimodal safety benchmarks. To this end, we construct multimodal visual leakless safety benchmark (VLSBench) preventing visual safety leakage from image to textual query with 2.4k image-text pairs. Experimental results indicate that VLSBench poses a significant challenge to both open-source and close-source MLLMs, including LLaVA, Qwen2-VL, Llama3.2-Vision, and GPT-4o. This study demonstrates that textual alignment is enough for multimodal safety scenarios with VSIL, while multimodal alignment is a more promising solution for multimodal safety scenarios without VSIL. Please see our code and data at: http://hxhcreate.github.io/VLSBench",
            "score": 3,
            "issue_id": 914,
            "pub_date": "2024-11-29",
            "pub_date_card": {
                "ru": "29 ноября",
                "en": "November 29",
                "zh": "11月29日"
            },
            "hash": "03d8c636cf8c9486",
            "authors": [
                "Xuhao Hu",
                "Dongrui Liu",
                "Hao Li",
                "Xuanjing Huang",
                "Jing Shao"
            ],
            "affiliations": [
                "Beihang University",
                "Fudan University",
                "Shanghai Artificial Intelligence Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.19939.jpg",
            "data": {
                "categories": [
                    "#ethics",
                    "#multimodal",
                    "#benchmark",
                    "#leakage",
                    "#open_source",
                    "#dataset"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Новый взгляд на безопасность мультимодальных ИИ-моделей",
                    "desc": "Статья рассматривает проблему безопасности мультимодальных больших языковых моделей (MLLM). Авторы обнаружили феномен утечки визуальной информации о безопасности (VSIL) в существующих мультимодальных тестах безопасности. Для решения этой проблемы они создали новый набор данных VLSBench, содержащий 2400 пар изображение-текст без VSIL. Эксперименты показали, что VLSBench представляет значительную сложность для современных MLLM и демонстрирует необходимость мультимодальной настройки для сценариев без VSIL."
                },
                "en": {
                    "title": "Unveiling Safety in Multimodal Models: The VSIL Challenge",
                    "desc": "This paper addresses safety issues in Multimodal Large Language Models (MLLMs) by highlighting a problem called Visual Safety Information Leakage (VSIL). It shows that using textual unlearning can achieve safety performance similar to training with image-text pairs, which is surprising. The authors create a new benchmark, VLSBench, to test MLLMs without the influence of VSIL, using 2.4k image-text pairs. The findings suggest that while textual alignment can ensure safety in scenarios with VSIL, multimodal alignment is necessary for scenarios without it."
                },
                "zh": {
                    "title": "多模态安全性的新挑战与解决方案",
                    "desc": "这篇论文探讨了多模态大型语言模型（MLLMs）在安全性方面的挑战。研究发现，文本去学习可以与使用图像-文本对训练的模型在安全性表现上相当。作者指出，现有的多模态安全基准存在视觉安全信息泄漏（VSIL）问题，这使得模型能够轻易拒绝敏感的文本-图像查询。为了解决这个问题，研究者构建了一个新的多模态视觉无泄漏安全基准（VLSBench），以更好地评估模型在没有VSIL情况下的安全性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.18933",
            "title": "Efficient Track Anything",
            "url": "https://huggingface.co/papers/2411.18933",
            "abstract": "Segment Anything Model 2 (SAM 2) has emerged as a powerful tool for video object segmentation and tracking anything. Key components of SAM 2 that drive the impressive video object segmentation performance include a large multistage image encoder for frame feature extraction and a memory mechanism that stores memory contexts from past frames to help current frame segmentation. The high computation complexity of multistage image encoder and memory module has limited its applications in real-world tasks, e.g., video object segmentation on mobile devices. To address this limitation, we propose EfficientTAMs, lightweight track anything models that produce high-quality results with low latency and model size. Our idea is based on revisiting the plain, nonhierarchical Vision Transformer (ViT) as an image encoder for video object segmentation, and introducing an efficient memory module, which reduces the complexity for both frame feature extraction and memory computation for current frame segmentation. We take vanilla lightweight ViTs and efficient memory module to build EfficientTAMs, and train the models on SA-1B and SA-V datasets for video object segmentation and track anything tasks. We evaluate on multiple video segmentation benchmarks including semi-supervised VOS and promptable video segmentation, and find that our proposed EfficientTAM with vanilla ViT perform comparably to SAM 2 model (HieraB+SAM 2) with ~2x speedup on A100 and ~2.4x parameter reduction. On segment anything image tasks, our EfficientTAMs also perform favorably over original SAM with ~20x speedup on A100 and ~20x parameter reduction. On mobile devices such as iPhone 15 Pro Max, our EfficientTAMs can run at ~10 FPS for performing video object segmentation with reasonable quality, highlighting the capability of small models for on-device video object segmentation applications.",
            "score": 3,
            "issue_id": 912,
            "pub_date": "2024-11-28",
            "pub_date_card": {
                "ru": "28 ноября",
                "en": "November 28",
                "zh": "11月28日"
            },
            "hash": "86ff7f63f69fa104",
            "authors": [
                "Yunyang Xiong",
                "Chong Zhou",
                "Xiaoyu Xiang",
                "Lemeng Wu",
                "Chenchen Zhu",
                "Zechun Liu",
                "Saksham Suri",
                "Balakrishnan Varadarajan",
                "Ramya Akula",
                "Forrest Iandola",
                "Raghuraman Krishnamoorthi",
                "Bilge Soran",
                "Vikas Chandra"
            ],
            "affiliations": [
                "Meta AI",
                "Nanyang Technological University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.18933.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#small_models",
                    "#benchmark",
                    "#optimization",
                    "#architecture"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Эффективная сегментация видео на мобильных устройствах",
                    "desc": "EfficientTAMs - это облегченные модели для сегментации и отслеживания объектов в видео. Они основаны на использовании простого Vision Transformer (ViT) в качестве энкодера изображений и эффективного модуля памяти. EfficientTAMs показывают результаты, сравнимые с SAM 2, но работают в 2 раза быстрее и имеют в 2,4 раза меньше параметров. На мобильных устройствах, таких как iPhone 15 Pro Max, EfficientTAMs могут выполнять сегментацию объектов в видео со скоростью около 10 кадров в секунду."
                },
                "en": {
                    "title": "EfficientTAMs: Lightweight Video Segmentation for Mobile Devices",
                    "desc": "The Segment Anything Model 2 (SAM 2) is a sophisticated tool for video object segmentation, but its complexity limits its use on mobile devices. To overcome this, the authors introduce EfficientTAMs, which are lightweight models designed to maintain high-quality segmentation while reducing latency and model size. They utilize a simple Vision Transformer (ViT) for feature extraction and an efficient memory module to streamline the segmentation process. The results show that EfficientTAMs can achieve comparable performance to SAM 2 with significant improvements in speed and resource efficiency, making them suitable for real-time applications on mobile platforms."
                },
                "zh": {
                    "title": "高效视频物体分割，轻量化模型新选择",
                    "desc": "Segment Anything Model 2（SAM 2）是一种强大的视频物体分割和跟踪工具。为了提高性能，SAM 2使用了多阶段图像编码器和记忆机制，但其计算复杂性限制了在移动设备上的应用。为了解决这个问题，我们提出了高效的跟踪模型EfficientTAMs，它使用轻量级的视觉变换器（ViT）作为图像编码器，并引入高效的记忆模块，从而降低了计算复杂性。我们的EfficientTAMs在多个视频分割基准测试中表现出色，能够在移动设备上以合理的质量进行视频物体分割。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.00947",
            "title": "VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information",
            "url": "https://huggingface.co/papers/2412.00947",
            "abstract": "Errors in understanding visual information in images (i.e., visual perception errors) remain a major source of mistakes in Large Vision Language Models (LVLMs). While further analysis is essential, there is a deficiency in datasets for evaluating the visual perception of LVLMs. In this work, we introduce VisOnlyQA, a new dataset designed to directly evaluate the visual perception capabilities of LVLMs on questions about geometric and numerical information in scientific figures. Our dataset enables us to analyze the visual perception of LVLMs for fine-grained visual information, independent of other capabilities such as reasoning. The evaluation set of VisOnlyQA includes 1,200 multiple-choice questions in 12 tasks on four categories of figures. We also provide synthetic training data consisting of 70k instances. Our experiments on VisOnlyQA highlight the following findings: (i) 20 LVLMs we evaluate, including GPT-4o and Gemini 1.5 Pro, work poorly on the visual perception tasks in VisOnlyQA, while human performance is nearly perfect. (ii) Fine-tuning on synthetic training data demonstrates the potential for enhancing the visual perception of LVLMs, but observed improvements are limited to certain tasks and specific models. (iii) Stronger language models improve the visual perception of LVLMs. In summary, our experiments suggest that both training data and model architectures should be improved to enhance the visual perception capabilities of LVLMs. The datasets, code, and model responses are provided at https://github.com/psunlpgroup/VisOnlyQA.",
            "score": 3,
            "issue_id": 911,
            "pub_date": "2024-12-01",
            "pub_date_card": {
                "ru": "1 декабря",
                "en": "December 1",
                "zh": "12月1日"
            },
            "hash": "7135f8936b0d3ee8",
            "authors": [
                "Ryo Kamoi",
                "Yusen Zhang",
                "Sarkar Snigdha Sarathi Das",
                "Ranran Haoran Zhang",
                "Rui Zhang"
            ],
            "affiliations": [
                "Penn State University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.00947.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#interpretability",
                    "#cv",
                    "#synthetic",
                    "#training"
                ],
                "emoji": "👁️",
                "ru": {
                    "title": "VisOnlyQA: новый путь к улучшению визуального восприятия AI",
                    "desc": "Исследователи представили новый датасет VisOnlyQA для оценки визуального восприятия больших визуально-языковых моделей (LVLM). Датасет фокусируется на геометрической и числовой информации в научных изображениях, позволяя анализировать способности моделей к тонкому визуальному восприятию. Эксперименты показали, что даже передовые LVLM, такие как GPT-4o и Gemini 1.5 Pro, плохо справляются с задачами VisOnlyQA, в то время как люди демонстрируют почти идеальные результаты. Исследование подчеркивает необходимость улучшения как обучающих данных, так и архитектур моделей для повышения качества визуального восприятия LVLM."
                },
                "en": {
                    "title": "Enhancing Visual Perception in LVLMs with VisOnlyQA",
                    "desc": "This paper addresses the issue of visual perception errors in Large Vision Language Models (LVLMs), which can lead to mistakes when interpreting images. The authors introduce a new dataset called VisOnlyQA, specifically designed to evaluate LVLMs on geometric and numerical questions related to scientific figures. The dataset includes 1,200 multiple-choice questions across various tasks and provides synthetic training data to improve model performance. The findings reveal that while LVLMs struggle with visual perception tasks, fine-tuning with synthetic data can enhance their capabilities, particularly when using stronger language models."
                },
                "zh": {
                    "title": "提升视觉感知，助力大型视觉语言模型",
                    "desc": "本研究介绍了一个新的数据集VisOnlyQA，旨在直接评估大型视觉语言模型（LVLMs）在科学图形中几何和数值信息问题上的视觉感知能力。该数据集包含1200个多项选择题，涵盖四类图形的12个任务，帮助分析LVLMs对细粒度视觉信息的理解。实验结果显示，评估的20个LVLMs在视觉感知任务上的表现较差，而人类的表现几乎完美。通过合成训练数据进行微调可以提升LVLMs的视觉感知能力，但改进效果有限，且更强的语言模型能够提高视觉感知能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.01316",
            "title": "Long Video Diffusion Generation with Segmented Cross-Attention and Content-Rich Video Data Curation",
            "url": "https://huggingface.co/papers/2412.01316",
            "abstract": "We introduce Presto, a novel video diffusion model designed to generate 15-second videos with long-range coherence and rich content. Extending video generation methods to maintain scenario diversity over long durations presents significant challenges. To address this, we propose a Segmented Cross-Attention (SCA) strategy, which splits hidden states into segments along the temporal dimension, allowing each segment to cross-attend to a corresponding sub-caption. SCA requires no additional parameters, enabling seamless incorporation into current DiT-based architectures. To facilitate high-quality long video generation, we build the LongTake-HD dataset, consisting of 261k content-rich videos with scenario coherence, annotated with an overall video caption and five progressive sub-captions. Experiments show that our Presto achieves 78.5% on the VBench Semantic Score and 100% on the Dynamic Degree, outperforming existing state-of-the-art video generation methods. This demonstrates that our proposed Presto significantly enhances content richness, maintains long-range coherence, and captures intricate textual details. More details are displayed on our project page: https://presto-video.github.io/.",
            "score": 3,
            "issue_id": 910,
            "pub_date": "2024-12-02",
            "pub_date_card": {
                "ru": "2 декабря",
                "en": "December 2",
                "zh": "12月2日"
            },
            "hash": "5bca597a08ec0a14",
            "authors": [
                "Xin Yan",
                "Yuxuan Cai",
                "Qiuyue Wang",
                "Yuan Zhou",
                "Wenhao Huang",
                "Huan Yang"
            ],
            "affiliations": [
                "01.AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.01316.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#dataset",
                    "#architecture",
                    "#diffusion",
                    "#long_context"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Presto: Революция в генерации длинных видео с помощью ИИ",
                    "desc": "Presto - это новая модель диффузии видео, разработанная для генерации 15-секундных видеороликов с долгосрочной согласованностью и богатым содержанием. Модель использует стратегию Segmented Cross-Attention (SCA), которая разделяет скрытые состояния на сегменты вдоль временного измерения, позволяя каждому сегменту перекрестно обращаться к соответствующей подписи. Для обучения модели был создан датасет LongTake-HD, содержащий 261 тысячу видео с богатым содержанием и согласованностью сценариев. Эксперименты показывают, что Presto превосходит существующие методы генерации видео по показателям семантической оценки и степени динамичности."
                },
                "en": {
                    "title": "Presto: Revolutionizing Video Generation with Long-Range Coherence",
                    "desc": "Presto is a new video diffusion model that generates 15-second videos while ensuring long-range coherence and rich content. It introduces a Segmented Cross-Attention (SCA) strategy that divides hidden states into segments, allowing each segment to focus on specific sub-captions without needing extra parameters. To support this model, the LongTake-HD dataset was created, containing 261,000 videos with coherent scenarios and detailed captions. Experiments show that Presto outperforms existing methods, achieving high scores in semantic understanding and dynamic content generation."
                },
                "zh": {
                    "title": "Presto：生成长时间一致性视频的新方法",
                    "desc": "我们介绍了一种新的视频扩散模型Presto，旨在生成具有长时间一致性和丰富内容的15秒视频。为了解决在长时间内保持场景多样性的挑战，我们提出了一种分段交叉注意力(SCA)策略，该策略将隐藏状态沿时间维度分段，使每个段能够与相应的子标题进行交叉关注。SCA不需要额外的参数，可以无缝集成到现有的基于DiT的架构中。我们的实验表明，Presto在视频生成方面显著优于现有的最先进方法，提升了内容丰富性和长距离一致性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.01250",
            "title": "Collaborative Instance Navigation: Leveraging Agent Self-Dialogue to Minimize User Input",
            "url": "https://huggingface.co/papers/2412.01250",
            "abstract": "Existing embodied instance goal navigation tasks, driven by natural language, assume human users to provide complete and nuanced instance descriptions prior to the navigation, which can be impractical in the real world as human instructions might be brief and ambiguous. To bridge this gap, we propose a new task, Collaborative Instance Navigation (CoIN), with dynamic agent-human interaction during navigation to actively resolve uncertainties about the target instance in natural, template-free, open-ended dialogues. To address CoIN, we propose a novel method, Agent-user Interaction with UncerTainty Awareness (AIUTA), leveraging the perception capability of Vision Language Models (VLMs) and the capability of Large Language Models (LLMs). First, upon object detection, a Self-Questioner model initiates a self-dialogue to obtain a complete and accurate observation description, while a novel uncertainty estimation technique mitigates inaccurate VLM perception. Then, an Interaction Trigger module determines whether to ask a question to the user, continue or halt navigation, minimizing user input. For evaluation, we introduce CoIN-Bench, a benchmark supporting both real and simulated humans. AIUTA achieves competitive performance in instance navigation against state-of-the-art methods, demonstrating great flexibility in handling user inputs.",
            "score": 2,
            "issue_id": 915,
            "pub_date": "2024-12-02",
            "pub_date_card": {
                "ru": "2 декабря",
                "en": "December 2",
                "zh": "12月2日"
            },
            "hash": "8b768de35e4c5114",
            "authors": [
                "Francesco Taioli",
                "Edoardo Zorzi",
                "Gianni Franchi",
                "Alberto Castellini",
                "Alessandro Farinelli",
                "Marco Cristani",
                "Yiming Wang"
            ],
            "affiliations": [
                "Fondazione Bruno Kessler",
                "Polytechnic of Turin",
                "U2IS, ENSTA Paris, Institut Polytechnique de Paris",
                "University of Verona"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.01250.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#interpretability",
                    "#multimodal",
                    "#reasoning",
                    "#agents"
                ],
                "emoji": "🗺️",
                "ru": {
                    "title": "Интерактивная навигация робота с уточнением цели у человека",
                    "desc": "Статья представляет новую задачу навигации по инстансам объектов - Collaborative Instance Navigation (CoIN), где агент взаимодействует с человеком во время навигации для уточнения целевого объекта. Предложен метод AIUTA, использующий Vision Language Models и Large Language Models для обработки визуальной информации и генерации вопросов. AIUTA включает модули Self-Questioner для самоанализа наблюдений и Interaction Trigger для определения необходимости задать вопрос пользователю. Авторы также представили бенчмарк CoIN-Bench для оценки подобных систем."
                },
                "en": {
                    "title": "Navigating Together: Enhancing Instance Navigation with Human-AI Collaboration",
                    "desc": "This paper introduces a new task called Collaborative Instance Navigation (CoIN), which allows agents to interact with humans during navigation to clarify ambiguous instructions. The proposed method, Agent-user Interaction with UncerTainty Awareness (AIUTA), uses Vision Language Models (VLMs) and Large Language Models (LLMs) to enhance the navigation process. It includes a Self-Questioner model that helps the agent gather detailed information about the target instance and an Interaction Trigger module that decides when to engage the user for input. The authors also present CoIN-Bench, a benchmark for evaluating the performance of navigation systems in both real and simulated environments, showing that AIUTA performs well compared to existing methods."
                },
                "zh": {
                    "title": "协作实例导航：让机器更懂人类指令",
                    "desc": "现有的实例目标导航任务通常需要用户提供详细的描述，但在现实中，这种要求往往不切实际。为了解决这个问题，我们提出了一种新的任务，称为协作实例导航（CoIN），通过动态的代理-用户互动来解决导航中的不确定性。我们的方法，代理-用户互动与不确定性意识（AIUTA），结合了视觉语言模型和大型语言模型的能力，能够在导航过程中主动与用户对话。通过引入CoIN-Bench基准，我们的AIUTA方法在实例导航中表现出色，展示了处理用户输入的灵活性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.19799",
            "title": "INCLUDE: Evaluating Multilingual Language Understanding with Regional Knowledge",
            "url": "https://huggingface.co/papers/2411.19799",
            "abstract": "The performance differential of large language models (LLM) between languages hinders their effective deployment in many regions, inhibiting the potential economic and societal value of generative AI tools in many communities. However, the development of functional LLMs in many languages (\\ie, multilingual LLMs) is bottlenecked by the lack of high-quality evaluation resources in languages other than English. Moreover, current practices in multilingual benchmark construction often translate English resources, ignoring the regional and cultural knowledge of the environments in which multilingual systems would be used. In this work, we construct an evaluation suite of 197,243 QA pairs from local exam sources to measure the capabilities of multilingual LLMs in a variety of regional contexts. Our novel resource, INCLUDE, is a comprehensive knowledge- and reasoning-centric benchmark across 44 written languages that evaluates multilingual LLMs for performance in the actual language environments where they would be deployed.",
            "score": 2,
            "issue_id": 914,
            "pub_date": "2024-11-29",
            "pub_date_card": {
                "ru": "29 ноября",
                "en": "November 29",
                "zh": "11月29日"
            },
            "hash": "7b0cbdd28adecf2c",
            "authors": [
                "Angelika Romanou",
                "Negar Foroutan",
                "Anna Sotnikova",
                "Zeming Chen",
                "Sree Harsha Nelaturu",
                "Shivalika Singh",
                "Rishabh Maheshwary",
                "Micol Altomare",
                "Mohamed A. Haggag",
                "Snegha A",
                "Alfonso Amayuelas",
                "Azril Hafizi Amirudin",
                "Viraat Aryabumi",
                "Danylo Boiko",
                "Michael Chang",
                "Jenny Chim",
                "Gal Cohen",
                "Aditya Kumar Dalmia",
                "Abraham Diress",
                "Sharad Duwal",
                "Daniil Dzenhaliou",
                "Daniel Fernando Erazo Florez",
                "Fabian Farestam",
                "Joseph Marvin Imperial",
                "Shayekh Bin Islam",
                "Perttu Isotalo",
                "Maral Jabbarishiviari",
                "Börje F. Karlsson",
                "Eldar Khalilov",
                "Christopher Klamm",
                "Fajri Koto",
                "Dominik Krzemiński",
                "Gabriel Adriano de Melo",
                "Syrielle Montariol",
                "Yiyang Nan",
                "Joel Niklaus",
                "Jekaterina Novikova",
                "Johan Samir Obando Ceron",
                "Debjit Paul",
                "Esther Ploeger",
                "Jebish Purbey",
                "Swati Rajwal",
                "Selvan Sunitha Ravi",
                "Sara Rydell",
                "Roshan Santhosh",
                "Drishti Sharma",
                "Marjana Prifti Skenduli",
                "Arshia Soltani Moakhar",
                "Bardia Soltani Moakhar",
                "Ran Tamir",
                "Ayush Kumar Tarun",
                "Azmine Toushik Wasi",
                "Thenuka Ovin Weerasinghe",
                "Serhan Yilmaz",
                "Mike Zhang",
                "Imanol Schlag",
                "Marzieh Fadaee",
                "Sara Hooker",
                "Antoine Bosselut"
            ],
            "affiliations": [
                "Cohere For AI",
                "Cohere For AI Community",
                "EPFL",
                "ETH Zurich",
                "Swiss AI Initiative"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.19799.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#low_resource",
                    "#benchmark",
                    "#reasoning"
                ],
                "emoji": "🌍",
                "ru": {
                    "title": "Глобальная оценка многоязычных ИИ-моделей в локальных контекстах",
                    "desc": "Статья посвящена проблеме неравномерной эффективности больших языковых моделей (LLM) для разных языков. Авторы создали набор данных INCLUDE из 197,243 пар вопросов и ответов на 44 языках для оценки многоязычных LLM. Этот ресурс основан на местных экзаменационных материалах и охватывает различные региональные контексты. INCLUDE позволяет оценивать знания и способности к рассуждению многоязычных LLM в реальных языковых средах их применения."
                },
                "en": {
                    "title": "Bridging Language Gaps with INCLUDE: A Multilingual Benchmark for LLMs",
                    "desc": "This paper addresses the performance gap of large language models (LLMs) across different languages, which limits their use in various regions. It highlights the challenge of developing multilingual LLMs due to the scarcity of high-quality evaluation resources in non-English languages. The authors propose a new evaluation suite called INCLUDE, consisting of 197,243 question-and-answer pairs sourced from local exams. This benchmark is designed to assess the capabilities of multilingual LLMs in real-world contexts, taking into account regional and cultural knowledge."
                },
                "zh": {
                    "title": "提升多语言模型的实际应用能力",
                    "desc": "这篇论文讨论了大型语言模型（LLM）在不同语言之间的性能差异，这影响了它们在许多地区的有效应用。为了克服多语言LLM开发中的瓶颈，作者构建了一个包含197,243个问答对的评估套件，来源于当地考试材料。这个新资源INCLUDE是一个全面的知识和推理中心基准，涵盖44种书面语言，旨在评估多语言LLM在实际语言环境中的表现。通过这种方式，研究希望提升生成性人工智能工具在不同社区的经济和社会价值。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.00927",
            "title": "VISTA: Enhancing Long-Duration and High-Resolution Video Understanding by Video Spatiotemporal Augmentation",
            "url": "https://huggingface.co/papers/2412.00927",
            "abstract": "Current large multimodal models (LMMs) face significant challenges in processing and comprehending long-duration or high-resolution videos, which is mainly due to the lack of high-quality datasets. To address this issue from a data-centric perspective, we propose VISTA, a simple yet effective Video Spatiotemporal Augmentation framework that synthesizes long-duration and high-resolution video instruction-following pairs from existing video-caption datasets. VISTA spatially and temporally combines videos to create new synthetic videos with extended durations and enhanced resolutions, and subsequently produces question-answer pairs pertaining to these newly synthesized videos. Based on this paradigm, we develop seven video augmentation methods and curate VISTA-400K, a video instruction-following dataset aimed at enhancing long-duration and high-resolution video understanding. Finetuning various video LMMs on our data resulted in an average improvement of 3.3% across four challenging benchmarks for long-video understanding. Furthermore, we introduce the first comprehensive high-resolution video understanding benchmark HRVideoBench, on which our finetuned models achieve a 6.5% performance gain. These results highlight the effectiveness of our framework.",
            "score": 2,
            "issue_id": 912,
            "pub_date": "2024-12-01",
            "pub_date_card": {
                "ru": "1 декабря",
                "en": "December 1",
                "zh": "12月1日"
            },
            "hash": "b4065e6e554dea31",
            "authors": [
                "Weiming Ren",
                "Huan Yang",
                "Jie Min",
                "Cong Wei",
                "Wenhu Chen"
            ],
            "affiliations": [
                "University of Waterloo",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.00927.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#video",
                    "#dataset",
                    "#data",
                    "#long_context",
                    "#benchmark",
                    "#synthetic"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Синтез данных для обучения ИИ пониманию сложных видео",
                    "desc": "VISTA - это система для улучшения понимания длинных и высокого разрешения видео большими мультимодальными моделями. Она синтезирует новые видео, комбинируя существующие пространственно и временно, и создает вопросно-ответные пары к ним. На основе VISTA создан датасет VISTA-400K, который позволил улучшить результаты моделей на 3.3% в задачах понимания длинных видео. Также авторы представили первый бенчмарк HRVideoBench для оценки понимания видео высокого разрешения."
                },
                "en": {
                    "title": "Enhancing Video Understanding with VISTA: A Data-Centric Approach",
                    "desc": "This paper introduces VISTA, a framework designed to improve the understanding of long-duration and high-resolution videos by creating synthetic video instruction-following pairs. VISTA utilizes existing video-caption datasets to spatially and temporally augment videos, generating new high-quality video data. The authors developed seven augmentation methods and created the VISTA-400K dataset, which significantly enhances the training of large multimodal models (LMMs). The results show that finetuning LMMs on this dataset leads to notable performance improvements on various benchmarks, demonstrating the framework's effectiveness in video comprehension tasks."
                },
                "zh": {
                    "title": "VISTA：提升视频理解的新方法",
                    "desc": "当前的大型多模态模型在处理长时长或高分辨率视频时面临重大挑战，主要是由于缺乏高质量的数据集。为了解决这个问题，我们提出了VISTA，一个简单而有效的视频时空增强框架，能够从现有的视频-字幕数据集中合成长时长和高分辨率的视频指令对。VISTA通过空间和时间的组合，创建新的合成视频，并生成与这些新合成视频相关的问题-答案对。通过在我们的数据上微调各种视频多模态模型，平均提高了3.3%的性能，进一步验证了我们框架的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.00100",
            "title": "Steering Rectified Flow Models in the Vector Field for Controlled Image Generation",
            "url": "https://huggingface.co/papers/2412.00100",
            "abstract": "Diffusion models (DMs) excel in photorealism, image editing, and solving inverse problems, aided by classifier-free guidance and image inversion techniques. However, rectified flow models (RFMs) remain underexplored for these tasks. Existing DM-based methods often require additional training, lack generalization to pretrained latent models, underperform, and demand significant computational resources due to extensive backpropagation through ODE solvers and inversion processes. In this work, we first develop a theoretical and empirical understanding of the vector field dynamics of RFMs in efficiently guiding the denoising trajectory. Our findings reveal that we can navigate the vector field in a deterministic and gradient-free manner. Utilizing this property, we propose FlowChef, which leverages the vector field to steer the denoising trajectory for controlled image generation tasks, facilitated by gradient skipping. FlowChef is a unified framework for controlled image generation that, for the first time, simultaneously addresses classifier guidance, linear inverse problems, and image editing without the need for extra training, inversion, or intensive backpropagation. Finally, we perform extensive evaluations and show that FlowChef significantly outperforms baselines in terms of performance, memory, and time requirements, achieving new state-of-the-art results. Project Page: https://flowchef.github.io.",
            "score": 2,
            "issue_id": 911,
            "pub_date": "2024-11-27",
            "pub_date_card": {
                "ru": "27 ноября",
                "en": "November 27",
                "zh": "11月27日"
            },
            "hash": "5fceae277a0e3d85",
            "authors": [
                "Maitreya Patel",
                "Song Wen",
                "Dimitris N. Metaxas",
                "Yezhou Yang"
            ],
            "affiliations": [
                "Arizona State University",
                "Rutgers University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.00100.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#cv",
                    "#optimization",
                    "#diffusion",
                    "#benchmark"
                ],
                "emoji": "🌊",
                "ru": {
                    "title": "FlowChef: Управление векторным полем для эффективной генерации изображений",
                    "desc": "Статья представляет новый подход к управляемой генерации изображений под названием FlowChef. Он основан на использовании динамики векторного поля в ректифицированных потоковых моделях (RFM) для эффективного управления траекторией шумоподавления. FlowChef позволяет решать задачи управляемой генерации изображений, линейных обратных задач и редактирования изображений без дополнительного обучения или инверсии. Результаты показывают, что FlowChef значительно превосходит существующие методы по производительности, требованиям к памяти и времени вычислений."
                },
                "en": {
                    "title": "FlowChef: Revolutionizing Controlled Image Generation with Efficiency",
                    "desc": "This paper introduces FlowChef, a new framework that enhances controlled image generation using rectified flow models (RFMs). Unlike traditional diffusion models, FlowChef efficiently guides the denoising process without requiring additional training or extensive computational resources. It utilizes a unique property of RFMs to navigate the vector field in a deterministic way, allowing for effective classifier guidance and image editing. The results demonstrate that FlowChef outperforms existing methods in performance, memory usage, and processing time, setting new benchmarks in the field."
                },
                "zh": {
                    "title": "FlowChef：高效的受控图像生成新方法",
                    "desc": "扩散模型在生成真实感图像、图像编辑和解决逆问题方面表现出色，但修正流模型在这些任务中仍未得到充分探索。现有的基于扩散模型的方法通常需要额外的训练，缺乏对预训练潜在模型的泛化能力，并且在性能上表现不佳，计算资源消耗大。本文提出FlowChef，通过有效引导去噪轨迹，利用向量场的特性，实现了受控图像生成任务，且无需额外训练或复杂的反向传播。实验结果表明，FlowChef在性能、内存和时间需求上显著优于基线方法，达到了新的最先进结果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.01800",
            "title": "PhysGame: Uncovering Physical Commonsense Violations in Gameplay Videos",
            "url": "https://huggingface.co/papers/2412.01800",
            "abstract": "Recent advancements in video-based large language models (Video LLMs) have witnessed the emergence of diverse capabilities to reason and interpret dynamic visual content. Among them, gameplay videos stand out as a distinctive data source, often containing glitches that defy physics commonsense. This characteristic renders them an effective benchmark for assessing the under-explored capability of physical commonsense understanding in video LLMs. In this paper, we propose PhysGame as a pioneering benchmark to evaluate physical commonsense violations in gameplay videos. PhysGame comprises 880 videos associated with glitches spanning four fundamental domains (i.e., mechanics, kinematics, optics, and material properties) and across 12 distinct physical commonsense. Through extensively evaluating various state-ofthe-art video LLMs, our findings reveal that the performance of current open-source video LLMs significantly lags behind that of proprietary counterparts. To bridge this gap, we curate an instruction tuning dataset PhysInstruct with 140,057 question-answering pairs to facilitate physical commonsense learning. In addition, we also propose a preference optimization dataset PhysDPO with 34,358 training pairs, where the dis-preferred responses are generated conditioned on misleading titles (i.e., meta information hacking), fewer frames (i.e., temporal hacking) and lower spatial resolutions (i.e., spatial hacking). Based on the suite of datasets, we propose PhysVLM as a physical knowledge-enhanced video LLM. Extensive experiments on both physical-oriented benchmark PhysGame and general video understanding benchmarks demonstrate the state-ofthe-art performance of PhysVLM.",
            "score": 1,
            "issue_id": 917,
            "pub_date": "2024-12-02",
            "pub_date_card": {
                "ru": "2 декабря",
                "en": "December 2",
                "zh": "12月2日"
            },
            "hash": "43cc035146493599",
            "authors": [
                "Meng Cao",
                "Haoran Tang",
                "Haoze Zhao",
                "Hangyu Guo",
                "Jiaheng Liu",
                "Ge Zhang",
                "Ruyang Liu",
                "Qiang Sun",
                "Ian Reid",
                "Xiaodan Liang"
            ],
            "affiliations": [
                "Alibaba Group",
                "Mohamed bin Zayed University of Artificial Intelligence",
                "Peking University",
                "Sun Yat-sen University",
                "University of Toronto",
                "University of Waterloo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.01800.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#video",
                    "#open_source",
                    "#optimization",
                    "#benchmark",
                    "#reasoning",
                    "#multimodal",
                    "#interpretability"
                ],
                "emoji": "🎮",
                "ru": {
                    "title": "PhysVLM: Обучение видеоязыковых моделей физическому здравому смыслу через игровые глитчи",
                    "desc": "Исследователи представили новый бенчмарк PhysGame для оценки понимания физических закономерностей в видеоязыковых моделях (Video LLM) на основе геймплейных видео с глитчами. Они создали набор данных PhysInstruct для обучения моделей физическому здравому смыслу и набор PhysDPO для оптимизации предпочтений. На базе этих наборов данных была разработана модель PhysVLM, показавшая лучшие результаты как на специализированном бенчмарке PhysGame, так и на общих тестах понимания видео."
                },
                "en": {
                    "title": "Enhancing Video LLMs with Physical Commonsense Understanding",
                    "desc": "This paper introduces PhysGame, a new benchmark designed to evaluate how well video-based large language models (Video LLMs) understand physical commonsense by analyzing glitches in gameplay videos. The benchmark includes 880 videos that highlight violations of physical laws across four domains: mechanics, kinematics, optics, and material properties. To improve the performance of Video LLMs, the authors created two additional datasets: PhysInstruct for instruction tuning and PhysDPO for preference optimization, which help models learn from common misconceptions and misleading information. The proposed PhysVLM model, enhanced with physical knowledge, shows superior performance on both the PhysGame benchmark and general video understanding tasks compared to existing models."
                },
                "zh": {
                    "title": "提升视频模型的物理常识理解能力",
                    "desc": "本文介绍了一种新颖的基准测试PhysGame，用于评估视频大语言模型在游戏视频中对物理常识的理解能力。游戏视频中常常出现违反物理常识的故障，这使得它们成为评估模型能力的有效数据源。我们还创建了PhysInstruct和PhysDPO两个数据集，以帮助模型学习物理常识并优化其偏好。通过这些数据集，我们提出了PhysVLM，一个增强物理知识的视频大语言模型，并在多个基准测试中展示了其优越的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.01064",
            "title": "FLOAT: Generative Motion Latent Flow Matching for Audio-driven Talking Portrait",
            "url": "https://huggingface.co/papers/2412.01064",
            "abstract": "With the rapid advancement of diffusion-based generative models, portrait image animation has achieved remarkable results. However, it still faces challenges in temporally consistent video generation and fast sampling due to its iterative sampling nature. This paper presents FLOAT, an audio-driven talking portrait video generation method based on flow matching generative model. We shift the generative modeling from the pixel-based latent space to a learned motion latent space, enabling efficient design of temporally consistent motion. To achieve this, we introduce a transformer-based vector field predictor with a simple yet effective frame-wise conditioning mechanism. Additionally, our method supports speech-driven emotion enhancement, enabling a natural incorporation of expressive motions. Extensive experiments demonstrate that our method outperforms state-of-the-art audio-driven talking portrait methods in terms of visual quality, motion fidelity, and efficiency.",
            "score": 1,
            "issue_id": 912,
            "pub_date": "2024-12-02",
            "pub_date_card": {
                "ru": "2 декабря",
                "en": "December 2",
                "zh": "12月2日"
            },
            "hash": "1978e3ecf42cac0c",
            "authors": [
                "Taekyung Ki",
                "Dongchan Min",
                "Gyoungsu Chae"
            ],
            "affiliations": [
                "DeepBrain AI Inc.",
                "Graduate School of AI, KAIST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.01064.jpg",
            "data": {
                "categories": [
                    "#audio",
                    "#diffusion",
                    "#multimodal",
                    "#video",
                    "#architecture"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "Генерация реалистичных видео с говорящим портретом на основе аудио и потоковых моделей",
                    "desc": "Статья представляет FLOAT - метод генерации видео с говорящим портретом на основе аудио, используя генеративную модель сопоставления потоков. Авторы переносят генеративное моделирование из пиксельного латентного пространства в пространство движения, что позволяет эффективно создавать согласованные во времени движения. Метод включает предиктор векторного поля на основе трансформера с покадровым механизмом обусловливания. FLOAT также поддерживает усиление эмоций на основе речи, позволяя естественно добавлять выразительные движения."
                },
                "en": {
                    "title": "FLOAT: Revolutionizing Audio-Driven Talking Portraits with Motion Consistency",
                    "desc": "This paper introduces FLOAT, a novel method for generating talking portrait videos driven by audio. It addresses the challenges of creating temporally consistent animations and speeding up the sampling process by utilizing a flow matching generative model. By transitioning from pixel-based latent spaces to learned motion latent spaces, FLOAT enhances the design of motion consistency. The method also incorporates a transformer-based vector field predictor that allows for effective frame-wise conditioning and supports emotion enhancement based on speech, leading to improved visual quality and motion fidelity."
                },
                "zh": {
                    "title": "FLOAT：高效音频驱动的人像视频生成",
                    "desc": "本论文提出了一种名为FLOAT的音频驱动人像视频生成方法，基于流匹配生成模型。我们将生成建模从基于像素的潜在空间转移到学习的运动潜在空间，从而实现时间一致的运动设计。该方法引入了基于变换器的向量场预测器，并采用简单有效的逐帧条件机制。实验结果表明，我们的方法在视觉质量、运动保真度和效率方面优于现有的音频驱动人像生成方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.19477",
            "title": "A Simple and Provable Scaling Law for the Test-Time Compute of Large Language Models",
            "url": "https://huggingface.co/papers/2411.19477",
            "abstract": "We propose a general two-stage algorithm that enjoys a provable scaling law for the test-time compute of large language models (LLMs). Given an input problem, the proposed algorithm first generates N candidate solutions, and then chooses the best one via a multiple-round knockout tournament where each pair of candidates are compared for K times and only the winners move on to the next round. In a minimalistic implementation, both stages can be executed with a black-box LLM alone and nothing else (e.g., no external verifier or reward model), and a total of N times (K + 1) highly parallelizable LLM calls are needed for solving an input problem. Assuming that a generated candidate solution is correct with probability p_{gen} > 0 and a comparison between a pair of correct and incorrect solutions identifies the right winner with probability p_{comp} > 0.5 (i.e., better than a random guess), we prove theoretically that the failure probability of the proposed algorithm decays to zero exponentially with respect to N and K: $P(final output is incorrect) le (1 - p_{gen})^N + lceil log_2 N rceil e^{-2 K (p_{comp} - 0.5)^2}.$ Our empirical results with the challenging MMLU-Pro benchmark validate the technical assumptions, as well as the efficacy of the proposed algorithm and the gains from scaling up its test-time compute.",
            "score": 1,
            "issue_id": 912,
            "pub_date": "2024-11-29",
            "pub_date_card": {
                "ru": "29 ноября",
                "en": "November 29",
                "zh": "11月29日"
            },
            "hash": "6057902d5fcbe3f4",
            "authors": [
                "Yanxi Chen",
                "Xuchen Pan",
                "Yaliang Li",
                "Bolin Ding",
                "Jingren Zhou"
            ],
            "affiliations": [
                "Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.19477.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#benchmark",
                    "#architecture",
                    "#optimization"
                ],
                "emoji": "🏆",
                "ru": {
                    "title": "Масштабируемый алгоритм улучшения точности больших языковых моделей",
                    "desc": "Авторы предлагают двухэтапный алгоритм, который демонстрирует масштабируемый закон для вычислений больших языковых моделей (LLM) во время тестирования. Алгоритм сначала генерирует N кандидатов решений, а затем выбирает лучшее с помощью многораундового турнира на выбывание. Теоретически доказано, что вероятность ошибки алгоритма экспоненциально уменьшается с увеличением N и K. Эмпирические результаты на сложном бенчмарке MMLU-Pro подтверждают эффективность предложенного метода."
                },
                "en": {
                    "title": "Efficient Solution Selection for Large Language Models",
                    "desc": "This paper introduces a two-stage algorithm designed to improve the efficiency of large language models (LLMs) during test time. The first stage generates multiple candidate solutions for a given problem, while the second stage employs a knockout tournament to select the best solution through repeated comparisons. The algorithm can operate solely with a black-box LLM, requiring a manageable number of parallel calls to generate and evaluate candidates. The authors provide theoretical proof that the likelihood of incorrect outputs decreases exponentially as the number of candidates and comparisons increases, supported by empirical results from the MMLU-Pro benchmark."
                },
                "zh": {
                    "title": "高效选择：两阶段算法优化大语言模型计算",
                    "desc": "我们提出了一种通用的两阶段算法，能够在大语言模型（LLMs）的测试时间计算中实现可证明的扩展规律。该算法首先生成N个候选解，然后通过多轮淘汰赛选择最佳解，每对候选解比较K次，只有胜者进入下一轮。该算法的最简实现仅需使用黑箱LLM，无需外部验证器或奖励模型，总共需要N次(K + 1)高度可并行的LLM调用来解决输入问题。理论证明表明，假设生成的候选解正确的概率为p_{gen} > 0，且正确与错误解的比较能以概率p_{comp} > 0.5识别出正确的胜者，则该算法的失败概率随着N和K的增加呈指数级下降。"
                }
            }
        }
    ],
    "link_prev": "2024-12-02.html",
    "link_next": "2024-12-04.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "02.12",
        "en": "12/02",
        "zh": "12月2日"
    },
    "short_date_next": {
        "ru": "04.12",
        "en": "12/04",
        "zh": "12月4日"
    },
    "categories": {
        "#dataset": 10,
        "#data": 3,
        "#benchmark": 11,
        "#agents": 2,
        "#cv": 5,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 1,
        "#audio": 1,
        "#video": 9,
        "#multimodal": 8,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 8,
        "#healthcare": 0,
        "#training": 8,
        "#robotics": 0,
        "#agi": 1,
        "#games": 3,
        "#interpretability": 3,
        "#reasoning": 4,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 11,
        "#survey": 0,
        "#diffusion": 6,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 3,
        "#synthetic": 3,
        "#machine_translation": 0,
        "#leakage": 1,
        "#open_source": 7,
        "#small_models": 2,
        "#science": 0,
        "#low_resource": 1
    },
    "zh": {
        "text": "这篇文章介绍了多模态大语言模型（MLLMs）在视觉理解和生成任务中的进展。然而，生成交错的图像-文本内容仍然是一个挑战。现有的基准测试由于数据量和多样性的限制，无法充分评估这些方法。为了解决这个问题，作者提出了GATE OpenING（OpenING），一个包含5,400个高质量人工标注实例的全面基准，涵盖了56个真实世界任务。OpenING涵盖了各种日常场景，如旅行指南、设计和头脑风暴，为挑战性的交错生成方法提供了一个强大的平台。此外，作者还介绍了IntJudge，一个用于评估开放式多模态生成方法的评估模型。",
        "title": "GATE OpenING: A Comprehensive Benchmark for Judging Open-ended Interleaved Image-Text Generation",
        "pinyin": "这篇文章介绍了多模态大语言模型（MLLMs）在视觉理解和生成任务中的进展。然而，生成交错的图像-文本内容仍然是一个挑战。现有的基准测试由于数据量和多样性的限制，无法充分评估这些方法。为了解决这个问题，作者提出了GATE OpenING（OpenING），一个包含5,400个高质量人工标注实例的全面基准，涵盖了56个真实世界任务。OpenING涵盖了各种日常场景，如旅行指南、设计和头脑风暴，为挑战性的交错生成方法提供了一个强大的平台。此外，作者还介绍了IntJudge，一个用于评估开放式多模态生成方法的评估模型。\n\nzhè piān wén zhāng jiè shào le duō mó tài dà yǔ yán mó xíng (MLLMs) zài shì jué lǐ jiě hé shēng chéng rèn wù zhōng de jìn zhàn. rán ér, shēng chéng jiāo cuò de tú xiàng-wén běn nèi róng réng shì yī gè tiǎo zhàn. xiàn yǒu de jī zhǔn cè shì yóu yú shù jù liàng hé duō yàng xìng de xiàn zhì, wú fǎ chōng fēn píng gū zhè xiē fāng fǎ. wèi le jiě jué zhè gè wèn tí, zuò zhě tí chū le GATE OpenING (OpenING), yī gè bāo hán 5,400 gè gāo zhì liàng rén gōng biāo zhù shí lì de quán miàn jī zhǔn, hán gǎi le 56 gè zhēn shí shì jiè rèn wù. OpenING hán gǎi le gè zhǒng rì cháng chǎng jīng, rú lǚ xíng zhǐ nán, shè jì hé tóu nǎo fēng bào, wèi tiǎo zhàn xìng de jiāo cuò shēng chéng fāng fǎ tí gōng le yī gè qiáng dà de píng tái. cǐ wài, zuò zhě hái jiè shào le IntJudge, yī gè yòng yú píng gū kāi fàng shì duō mó tài shēng chéng fāng fǎ de píng gū mó xíng.",
        "vocab": "[\n    {\"word\": \"多模态\", \"pinyin\": \"duō mó tài\", \"trans\": \"multimodal\"},\n    {\"word\": \"大语言模型\", \"pinyin\": \"dà yǔ yán mó xíng\", \"trans\": \"large language model\"},\n    {\"word\": \"视觉理解\", \"pinyin\": \"shì jué lǐ jiě\", \"trans\": \"visual understanding\"},\n    {\"word\": \"生成\", \"pinyin\": \"shēng chéng\", \"trans\": \"generation\"},\n    {\"word\": \"交错\", \"pinyin\": \"jiāo cuò\", \"trans\": \"interleaved\"},\n    {\"word\": \"图像-文本\", \"pinyin\": \"tú xiàng wén běn\", \"trans\": \"image-text\"},\n    {\"word\": \"挑战\", \"pinyin\": \"tiǎo zhàn\", \"trans\": \"challenge\"},\n    {\"word\": \"基准测试\", \"pinyin\": \"jī zhǔn cè shì\", \"trans\": \"benchmark test\"},\n    {\"word\": \"数据量\", \"pinyin\": \"shù jù liàng\", \"trans\": \"data volume\"},\n    {\"word\": \"多样性\", \"pinyin\": \"duō yàng xìng\", \"trans\": \"diversity\"},\n    {\"word\": \"评估\", \"pinyin\": \"píng gū\", \"trans\": \"evaluation\"},\n    {\"word\": \"方法\", \"pinyin\": \"fāng fǎ\", \"trans\": \"method\"},\n    {\"word\": \"提出\", \"pinyin\": \"tí chū\", \"trans\": \"propose\"},\n    {\"word\": \"高质量\", \"pinyin\": \"gāo zhì liàng\", \"trans\": \"high quality\"},\n    {\"word\": \"人工标注\", \"pinyin\": \"réngōng biāo zhù\", \"trans\": \"manual annotation\"},\n    {\"word\": \"实例\", \"pinyin\": \"shí lì\", \"trans\": \"instance\"},\n    {\"word\": \"真实世界\", \"pinyin\": \"zhēn shí shì jiè\", \"trans\": \"real world\"},\n    {\"word\": \"任务\", \"pinyin\": \"rèn wù\", \"trans\": \"task\"},\n    {\"word\": \"日常场景\", \"pinyin\": \"rì cháng chǎng jǐng\", \"trans\": \"daily scenarios\"},\n    {\"word\": \"旅行指南\", \"pinyin\": \"lǚ xíng zhǐ nán\", \"trans\": \"travel guide\"},\n    {\"word\": \"设计\", \"pinyin\": \"shè jì\", \"trans\": \"design\"},\n    {\"word\": \"头脑风暴\", \"pinyin\": \"tóu nǎo fēng bào\", \"trans\": \"brainstorming\"},\n    {\"word\": \"平台\", \"pinyin\": \"píng tài\", \"trans\": \"platform\"},\n    {\"word\": \"评估模型\", \"pinyin\": \"píng gū mó xíng\", \"trans\": \"evaluation model\"},\n    {\"word\": \"开放式\", \"pinyin\": \"kāi fàng shì\", \"trans\": \"open-ended\"}\n]",
        "trans": "This article discusses the advancements of Multimodal Large Language Models (MLLMs) in visual understanding and generation tasks. However, generating interleaved image-text content remains a challenge. Existing benchmarks, due to limitations in data volume and diversity, are insufficient for evaluating these methods. To address this issue, the authors propose GATE OpenING (OpenING), a comprehensive benchmark containing 5,400 high-quality, manually annotated instances covering 56 real-world tasks. OpenING encompasses a variety of everyday scenarios, such as travel guides, design, and brainstorming, providing a robust platform for challenging interleaved generation methods. Additionally, the authors introduce IntJudge, a model for evaluating open-ended multimodal generation methods.",
        "update_ts": "2024-12-03 09:11"
    }
}