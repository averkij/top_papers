
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 16 papers. June 10.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">10 Ğ¸ÑĞ½Ñ</span> | <span id="title-articles-count">16 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-06-09.html">â¬…ï¸ <span id="prev-date">09.06</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-06-11.html">â¡ï¸ <span id="next-date">11.06</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-06.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '10 Ğ¸ÑĞ½Ñ', 'en': 'June 10', 'zh': '6æœˆ10æ—¥'};
        let feedDateNext = {'ru': '11.06', 'en': '06/11', 'zh': '6æœˆ11æ—¥'};
        let feedDatePrev = {'ru': '09.06', 'en': '06/09', 'zh': '6æœˆ9æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2506.08007', 'title': 'Reinforcement Pre-Training', 'url': 'https://huggingface.co/papers/2506.08007', 'abstract': 'Reinforcement Pre-Training (RPT) enhances language model pre-training by framing next-token prediction as a reinforcement learning task, improving accuracy and providing a strong foundation for further fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t In this work, we introduce Reinforcement Pre-Training (RPT) as a new scaling paradigm for large language models and reinforcement learning (RL). Specifically, we reframe next-token prediction as a reasoning task trained using RL, where it receives verifiable rewards for correctly predicting the next token for a given context. RPT offers a scalable method to leverage vast amounts of text data for general-purpose RL, rather than relying on domain-specific annotated answers. By incentivizing the capability of next-token reasoning, RPT significantly improves the language modeling accuracy of predicting the next tokens. Moreover, RPT provides a strong pre-trained foundation for further reinforcement fine-tuning. The scaling curves show that increased training compute consistently improves the next-token prediction accuracy. The results position RPT as an effective and promising scaling paradigm to advance language model pre-training.', 'score': 58, 'issue_id': 4208, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 Ğ¸ÑĞ½Ñ', 'en': 'June 9', 'zh': '6æœˆ9æ—¥'}, 'hash': 'a1ef74d27d1d9d50', 'authors': ['Qingxiu Dong', 'Li Dong', 'Yao Tang', 'Tianzhu Ye', 'Yutao Sun', 'Zhifang Sui', 'Furu Wei'], 'affiliations': ['Microsoft Research', 'Peking University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2506.08007.jpg', 'data': {'categories': ['#optimization', '#training', '#rlhf', '#reasoning', '#rl'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ñ‹ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Reinforcement Pre-Training (RPT). RPT Ğ¿ĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° ĞºĞ°Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ³Ğ´Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ·Ğ° Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ³Ñ€Ğ¾Ğ¼Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞ¼Ñ‹ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ RPT Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ‡Ğ½ÑƒÑ Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞ¹ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸.'}, 'en': {'title': 'Reinforcement Learning Boosts Language Model Training', 'desc': 'Reinforcement Pre-Training (RPT) is a novel approach that enhances the pre-training of language models by treating next-token prediction as a reinforcement learning (RL) task. This method allows the model to receive rewards for accurately predicting the next token based on the context, which improves its reasoning capabilities. RPT utilizes large amounts of unannotated text data, making it scalable and effective for general-purpose RL applications. The results demonstrate that RPT not only boosts prediction accuracy but also lays a solid groundwork for subsequent fine-tuning in reinforcement learning tasks.'}, 'zh': {'title': 'å¼ºåŒ–é¢„è®­ç»ƒï¼šæå‡è¯­è¨€æ¨¡å‹çš„ä¸‹ä¸€æ­¥é¢„æµ‹èƒ½åŠ›', 'desc': 'å¼ºåŒ–é¢„è®­ç»ƒï¼ˆRPTï¼‰é€šè¿‡å°†ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹è§†ä¸ºå¼ºåŒ–å­¦ä¹ ä»»åŠ¡ï¼Œå¢å¼ºäº†è¯­è¨€æ¨¡å‹çš„é¢„è®­ç»ƒã€‚è¿™ç§æ–¹æ³•é€šè¿‡ä¸ºæ­£ç¡®é¢„æµ‹ç»™å®šä¸Šä¸‹æ–‡çš„ä¸‹ä¸€ä¸ªæ ‡è®°æä¾›å¯éªŒè¯çš„å¥–åŠ±ï¼Œä»è€Œæé«˜äº†å‡†ç¡®æ€§ã€‚RPTåˆ©ç”¨å¤§é‡æ–‡æœ¬æ•°æ®è¿›è¡Œé€šç”¨å¼ºåŒ–å­¦ä¹ ï¼Œè€Œä¸ä¾èµ–äºç‰¹å®šé¢†åŸŸçš„æ ‡æ³¨ç­”æ¡ˆã€‚é€šè¿‡æ¿€åŠ±ä¸‹ä¸€ä¸ªæ ‡è®°æ¨ç†çš„èƒ½åŠ›ï¼ŒRPTæ˜¾è‘—æé«˜äº†è¯­è¨€å»ºæ¨¡çš„å‡†ç¡®æ€§ï¼Œå¹¶ä¸ºè¿›ä¸€æ­¥çš„å¼ºåŒ–å¾®è°ƒæä¾›äº†åšå®çš„åŸºç¡€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.07977', 'title': 'OneIG-Bench: Omni-dimensional Nuanced Evaluation for Image Generation', 'url': 'https://huggingface.co/papers/2506.07977', 'abstract': 'OneIG-Bench is a benchmark framework that comprehensively evaluates text-to-image models across prompt-image alignment, text rendering, reasoning, stylization, and diversity.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-image (T2I) models have garnered significant attention for generating high-quality images aligned with text prompts. However, rapid T2I model advancements reveal limitations in early benchmarks, lacking comprehensive evaluations, for example, the evaluation on reasoning, text rendering and style. Notably, recent state-of-the-art models, with their rich knowledge modeling capabilities, show promising results on the image generation problems requiring strong reasoning ability, yet existing evaluation systems have not adequately addressed this frontier. To systematically address these gaps, we introduce OneIG-Bench, a meticulously designed comprehensive benchmark framework for fine-grained evaluation of T2I models across multiple dimensions, including prompt-image alignment, text rendering precision, reasoning-generated content, stylization, and diversity. By structuring the evaluation, this benchmark enables in-depth analysis of model performance, helping researchers and practitioners pinpoint strengths and bottlenecks in the full pipeline of image generation. Specifically, OneIG-Bench enables flexible evaluation by allowing users to focus on a particular evaluation subset. Instead of generating images for the entire set of prompts, users can generate images only for the prompts associated with the selected dimension and complete the corresponding evaluation accordingly. Our codebase and dataset are now publicly available to facilitate reproducible evaluation studies and cross-model comparisons within the T2I research community.', 'score': 26, 'issue_id': 4208, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 Ğ¸ÑĞ½Ñ', 'en': 'June 9', 'zh': '6æœˆ9æ—¥'}, 'hash': 'f87355dfc9390cc7', 'authors': ['Jingjing Chang', 'Yixiao Fang', 'Peng Xing', 'Shuhan Wu', 'Wei Cheng', 'Rui Wang', 'Xianfang Zeng', 'Gang Yu', 'Hai-Bao Chen'], 'affiliations': ['SJTU', 'StepFun'], 'pdf_title_img': 'assets/pdf/title_img/2506.07977.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#cv', '#open_source', '#survey'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ’ÑĞµÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½ÑÑ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‚ĞµĞºÑÑ‚-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ', 'desc': 'OneIG-Bench - ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ. ĞĞ½Ğ° Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ°Ğ¼, Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ°, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, ÑÑ‚Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ²Ñ‹ÑĞ²Ğ»ÑÑ Ğ¸Ñ… ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ ÑĞ»Ğ°Ğ±Ñ‹Ğµ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñ‹. OneIG-Bench Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ³Ğ¸Ğ±ĞºÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ¾Ñ‡Ğ¸Ñ‚ÑŒÑÑ Ğ½Ğ° ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'OneIG-Bench: A Comprehensive Evaluation for Text-to-Image Models', 'desc': 'OneIG-Bench is a new framework designed to evaluate text-to-image (T2I) models in a detailed way. It focuses on several important aspects like how well the images match the text prompts, the quality of text rendering, reasoning capabilities, artistic style, and diversity of generated images. This benchmark addresses the shortcomings of previous evaluation methods by providing a structured approach that allows researchers to analyze specific areas of model performance. By making the code and dataset publicly available, OneIG-Bench promotes reproducibility and encourages comparisons among different T2I models.'}, 'zh': {'title': 'å…¨é¢è¯„ä¼°æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„åŸºå‡†æ¡†æ¶', 'desc': 'OneIG-Benchæ˜¯ä¸€ä¸ªåŸºå‡†æ¡†æ¶ï¼Œæ—¨åœ¨å…¨é¢è¯„ä¼°æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹çš„æ€§èƒ½ã€‚å®ƒæ¶µç›–äº†å¤šä¸ªç»´åº¦ï¼ŒåŒ…æ‹¬æç¤ºä¸å›¾åƒçš„å¯¹é½ã€æ–‡æœ¬æ¸²æŸ“ã€æ¨ç†èƒ½åŠ›ã€é£æ ¼åŒ–å’Œå¤šæ ·æ€§ã€‚é€šè¿‡ç³»ç»ŸåŒ–çš„è¯„ä¼°ï¼ŒOneIG-Benchå¸®åŠ©ç ”ç©¶äººå‘˜æ·±å…¥åˆ†ææ¨¡å‹çš„ä¼˜ç¼ºç‚¹ï¼Œè¯†åˆ«å›¾åƒç”Ÿæˆè¿‡ç¨‹ä¸­çš„ç“¶é¢ˆã€‚è¯¥æ¡†æ¶çš„ä»£ç åº“å’Œæ•°æ®é›†å·²å…¬å¼€ï¼Œä¾¿äºåœ¨T2Iç ”ç©¶ç¤¾åŒºä¸­è¿›è¡Œå¯é‡å¤çš„è¯„ä¼°ç ”ç©¶å’Œè·¨æ¨¡å‹æ¯”è¾ƒã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.07900', 'title': 'MiniCPM4: Ultra-Efficient LLMs on End Devices', 'url': 'https://huggingface.co/papers/2506.07900', 'abstract': 'MiniCPM4, a large language model optimized for end-side devices, achieves efficiency and effectiveness through innovations in model architecture, training data, algorithms, and inference systems.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper introduces MiniCPM4, a highly efficient large language model (LLM) designed explicitly for end-side devices. We achieve this efficiency through systematic innovation in four key dimensions: model architecture, training data, training algorithms, and inference systems. Specifically, in terms of model architecture, we propose InfLLM v2, a trainable sparse attention mechanism that accelerates both prefilling and decoding phases for long-context processing. Regarding training data, we propose UltraClean, an efficient and accurate pre-training data filtering and generation strategy, and UltraChat v2, a comprehensive supervised fine-tuning dataset. These datasets enable satisfactory model performance to be achieved using just 8 trillion training tokens. Regarding training algorithms, we propose ModelTunnel v2 for efficient pre-training strategy search, and improve existing post-training methods by introducing chunk-wise rollout for load-balanced reinforcement learning and data-efficient tenary LLM, BitCPM. Regarding inference systems, we propose CPM.cu that integrates sparse attention, model quantization, and speculative sampling to achieve efficient prefilling and decoding. To meet diverse on-device requirements, MiniCPM4 is available in two versions, with 0.5B and 8B parameters, respectively. Sufficient evaluation results show that MiniCPM4 outperforms open-source models of similar size across multiple benchmarks, highlighting both its efficiency and effectiveness. Notably, MiniCPM4-8B demonstrates significant speed improvements over Qwen3-8B when processing long sequences. Through further adaptation, MiniCPM4 successfully powers diverse applications, including trustworthy survey generation and tool use with model context protocol, clearly showcasing its broad usability.', 'score': 21, 'issue_id': 4208, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 Ğ¸ÑĞ½Ñ', 'en': 'June 9', 'zh': '6æœˆ9æ—¥'}, 'hash': 'edbb02209bdb677b', 'authors': ['MiniCPM Team', 'Chaojun Xiao', 'Yuxuan Li', 'Xu Han', 'Yuzhuo Bai', 'Jie Cai', 'Haotian Chen', 'Wentong Chen', 'Xin Cong', 'Ganqu Cui', 'Ning Ding', 'Shengdan Fan', 'Yewei Fang', 'Zixuan Fu', 'Wenyu Guan', 'Yitong Guan', 'Junshao Guo', 'Yufeng Han', 'Bingxiang He', 'Yuxiang Huang', 'Cunliang Kong', 'Qiuzuo Li', 'Siyuan Li', 'Wenhao Li', 'Yanghao Li', 'Yishan Li', 'Zhen Li', 'Dan Liu', 'Biyuan Lin', 'Yankai Lin', 'Xiang Long', 'Quanyu Lu', 'Yaxi Lu', 'Peiyan Luo', 'Hongya Lyu', 'Litu Ou', 'Yinxu Pan', 'Zekai Qu', 'Qundong Shi', 'Zijun Song', 'Jiayuan Su', 'Zhou Su', 'Ao Sun', 'Xianghui Sun', 'Peijun Tang', 'Fangzheng Wang', 'Feng Wang', 'Shuo Wang', 'Yudong Wang', 'Yesai Wu', 'Zhenyu Xiao', 'Jie Xie', 'Zihao Xie', 'Yukun Yan', 'Jiarui Yuan', 'Kaihuo Zhang', 'Lei Zhang', 'Linyue Zhang', 'Xueren Zhang', 'Yudi Zhang', 'Hengyu Zhao', 'Weilin Zhao', 'Weilun Zhao', 'Yuanqian Zhao', 'Zhi Zheng', 'Ge Zhou', 'Jie Zhou', 'Wei Zhou', 'Zihan Zhou', 'Zixuan Zhou', 'Zhiyuan Liu', 'Guoyang Zeng', 'Chao Jia', 'Dahai Li', 'Maosong Sun'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2506.07900.jpg', 'data': {'categories': ['#optimization', '#dataset', '#long_context', '#training', '#architecture', '#inference', '#open_source', '#data', '#games'], 'emoji': 'ğŸš€', 'ru': {'title': 'MiniCPM4: ĞœĞ¾Ñ‰ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸', 'desc': 'MiniCPM4 - ÑÑ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (LLM), Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸ÑĞ¼ Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ, Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ°Ñ… Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ InfLLM v2, ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… UltraClean Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… UltraChat v2 Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. MiniCPM4 Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¿Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ğ¼ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹.'}, 'en': {'title': 'MiniCPM4: Efficiency Meets Performance for On-Device AI', 'desc': 'MiniCPM4 is a large language model specifically designed for efficient use on end-side devices. It incorporates innovations in model architecture, such as a new sparse attention mechanism, and utilizes advanced training data strategies to enhance performance. The model employs unique training algorithms for effective pre-training and post-training, ensuring it can handle diverse tasks with minimal resources. Evaluation results indicate that MiniCPM4 outperforms similar-sized models, demonstrating its speed and effectiveness in real-world applications.'}, 'zh': {'title': 'MiniCPM4ï¼šç»ˆç«¯è®¾å¤‡çš„é«˜æ•ˆè¯­è¨€æ¨¡å‹', 'desc': 'æœ¬æ–‡ä»‹ç»äº†MiniCPM4ï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸ºç»ˆç«¯è®¾å¤‡ä¼˜åŒ–çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚é€šè¿‡åœ¨æ¨¡å‹æ¶æ„ã€è®­ç»ƒæ•°æ®ã€è®­ç»ƒç®—æ³•å’Œæ¨ç†ç³»ç»Ÿå››ä¸ªå…³é”®æ–¹é¢çš„åˆ›æ–°ï¼ŒMiniCPM4å®ç°äº†é«˜æ•ˆæ€§å’Œæœ‰æ•ˆæ€§ã€‚ç‰¹åˆ«æ˜¯ï¼Œé‡‡ç”¨äº†å¯è®­ç»ƒçš„ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶å’Œé«˜æ•ˆçš„æ•°æ®è¿‡æ»¤ç­–ç•¥ï¼Œä½¿å¾—æ¨¡å‹åœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡æ—¶è¡¨ç°å‡ºè‰²ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒMiniCPM4åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†åŒç±»å¼€æºæ¨¡å‹ï¼Œå±•ç°äº†å…¶å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.07044', 'title': 'Lingshu: A Generalist Foundation Model for Unified Multimodal Medical\n  Understanding and Reasoning', 'url': 'https://huggingface.co/papers/2506.07044', 'abstract': "Lingshu, a medical-specialized MLLM, enhances performance in medical tasks through comprehensive data curation, multi-stage training, and reinforcement learning with verifiable rewards, outperforming existing open-source models.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities in understanding common visual elements, largely due to their large-scale datasets and advanced training strategies. However, their effectiveness in medical applications remains limited due to the inherent discrepancies between data and tasks in medical scenarios and those in the general domain. Concretely, existing medical MLLMs face the following critical limitations: (1) limited coverage of medical knowledge beyond imaging, (2) heightened susceptibility to hallucinations due to suboptimal data curation processes, (3) lack of reasoning capabilities tailored for complex medical scenarios. To address these challenges, we first propose a comprehensive data curation procedure that (1) efficiently acquires rich medical knowledge data not only from medical imaging but also from extensive medical texts and general-domain data; and (2) synthesizes accurate medical captions, visual question answering (VQA), and reasoning samples. As a result, we build a multimodal dataset enriched with extensive medical knowledge. Building on the curated data, we introduce our medical-specialized MLLM: Lingshu. Lingshu undergoes multi-stage training to embed medical expertise and enhance its task-solving capabilities progressively. Besides, we preliminarily explore the potential of applying reinforcement learning with verifiable rewards paradigm to enhance Lingshu's medical reasoning ability. Additionally, we develop MedEvalKit, a unified evaluation framework that consolidates leading multimodal and textual medical benchmarks for standardized, fair, and efficient model assessment. We evaluate the performance of Lingshu on three fundamental medical tasks, multimodal QA, text-based QA, and medical report generation. The results show that Lingshu consistently outperforms the existing open-source multimodal models on most tasks ...", 'score': 14, 'issue_id': 4208, 'pub_date': '2025-06-08', 'pub_date_card': {'ru': '8 Ğ¸ÑĞ½Ñ', 'en': 'June 8', 'zh': '6æœˆ8æ—¥'}, 'hash': '475506f0d0cabb39', 'authors': ['LASA Team', 'Weiwen Xu', 'Hou Pong Chan', 'Long Li', 'Mahani Aljunied', 'Ruifeng Yuan', 'Jianyu Wang', 'Chenghao Xiao', 'Guizhen Chen', 'Chaoqun Liu', 'Zhaodonghui Li', 'Yu Sun', 'Junao Shen', 'Chaojun Wang', 'Jie Tan', 'Deli Zhao', 'Tingyang Xu', 'Hao Zhang', 'Yu Rong'], 'affiliations': ['DAMO Academy, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2506.07044.jpg', 'data': {'categories': ['#optimization', '#dataset', '#training', '#hallucinations', '#benchmark', '#medical', '#rl', '#reasoning', '#data', '#healthcare', '#multimodal'], 'emoji': 'ğŸ©º', 'ru': {'title': 'Lingshu: Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½Ñ‹', 'desc': 'Lingshu - ÑÑ‚Ğ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½Ñ‹, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ĞºÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ñ‚ĞµĞºÑÑ‚Ñ‹ Ğ¸ Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. Lingshu Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Lingshu: Revolutionizing Medical AI with Enhanced Learning and Reasoning', 'desc': 'Lingshu is a specialized Multimodal Large Language Model (MLLM) designed to improve medical task performance through enhanced data curation and training techniques. It addresses key limitations of existing medical models, such as insufficient medical knowledge coverage and a tendency to produce hallucinations. By utilizing a comprehensive dataset that includes both medical texts and images, Lingshu is trained in multiple stages to develop strong reasoning capabilities for complex medical scenarios. The model is evaluated using the MedEvalKit framework, demonstrating superior performance in tasks like multimodal question answering and medical report generation compared to other open-source models.'}, 'zh': {'title': 'Lingshuï¼šåŒ»ç–—ä»»åŠ¡çš„æ™ºèƒ½åŠ©æ‰‹', 'desc': 'Lingshuæ˜¯ä¸€ç§ä¸“é—¨é’ˆå¯¹åŒ»ç–—ä»»åŠ¡çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œé€šè¿‡å…¨é¢çš„æ•°æ®æ•´ç†ã€å¤šé˜¶æ®µè®­ç»ƒå’Œå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ æ¥æå‡å…¶æ€§èƒ½ã€‚è¯¥æ¨¡å‹è§£å†³äº†ç°æœ‰åŒ»ç–—MLLMåœ¨çŸ¥è¯†è¦†ç›–ã€æ•°æ®æ•´ç†å’Œæ¨ç†èƒ½åŠ›æ–¹é¢çš„ä¸è¶³ã€‚é€šè¿‡é«˜æ•ˆè·å–ä¸°å¯Œçš„åŒ»ç–—çŸ¥è¯†æ•°æ®ï¼ŒLingshuèƒ½å¤Ÿåœ¨å¤šç§åŒ»ç–—ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚æœ€ç»ˆï¼ŒLingshuåœ¨å¤šæ¨¡æ€é—®ç­”ã€åŸºäºæ–‡æœ¬çš„é—®ç­”å’ŒåŒ»ç–—æŠ¥å‘Šç”Ÿæˆç­‰ä»»åŠ¡ä¸­è¶…è¶Šäº†ç°æœ‰çš„å¼€æºæ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.07553', 'title': 'GTR-CoT: Graph Traversal as Visual Chain of Thought for Molecular\n  Structure Recognition', 'url': 'https://huggingface.co/papers/2506.07553', 'abstract': "GTR-Mol-VLM, a vision-language model with graph traversal and data-centric principles, achieves superior accuracy in converting molecular images to machine-readable formats, particularly in handling complex structures and functional group abbreviations.  \t\t\t\t\tAI-generated summary \t\t\t\t Optical Chemical Structure Recognition (OCSR) is crucial for digitizing chemical knowledge by converting molecular images into machine-readable formats. While recent vision-language models (VLMs) have shown potential in this task, their image-captioning approach often struggles with complex molecular structures and inconsistent annotations. To overcome these challenges, we introduce GTR-Mol-VLM, a novel framework featuring two key innovations: (1) the Graph Traversal as Visual Chain of Thought mechanism that emulates human reasoning by incrementally parsing molecular graphs through sequential atom-bond predictions, and (2) the data-centric principle of Faithfully Recognize What You've Seen, which addresses the mismatch between abbreviated structures in images and their expanded annotations. To support model development, we constructed GTR-CoT-1.3M, a large-scale instruction-tuning dataset with meticulously corrected annotations, and introduced MolRec-Bench, the first benchmark designed for a fine-grained evaluation of graph-parsing accuracy in OCSR. Comprehensive experiments demonstrate that GTR-Mol-VLM achieves superior results compared to specialist models, chemistry-domain VLMs, and commercial general-purpose VLMs. Notably, in scenarios involving molecular images with functional group abbreviations, GTR-Mol-VLM outperforms the second-best baseline by approximately 14 percentage points, both in SMILES-based and graph-based metrics. We hope that this work will drive OCSR technology to more effectively meet real-world needs, thereby advancing the fields of cheminformatics and AI for Science. We will release GTR-CoT at https://github.com/opendatalab/GTR-CoT.", 'score': 11, 'issue_id': 4208, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 Ğ¸ÑĞ½Ñ', 'en': 'June 9', 'zh': '6æœˆ9æ—¥'}, 'hash': '379cf5e0efdacd27', 'authors': ['Jingchao Wang', 'Haote Yang', 'Jiang Wu', 'Yifan He', 'Xingjian Wei', 'Yinfan Wang', 'Chengjin Liu', 'Lingli Ge', 'Lijun Wu', 'Bin Wang', 'Dahua Lin', 'Conghui He'], 'affiliations': ['Chinese University of Hong Kong', 'East China Normal University', 'Fudan University', 'Northwestern Polytechnical University', 'Peking University', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai University'], 'pdf_title_img': 'assets/pdf/title_img/2506.07553.jpg', 'data': {'categories': ['#dataset', '#graphs', '#benchmark', '#reasoning', '#cv', '#science', '#games'], 'emoji': 'ğŸ§ª', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ…Ğ¸Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±Ñ…Ğ¾Ğ´Ğ° Ğ³Ñ€Ğ°Ñ„Ğ° Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'GTR-Mol-VLM - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ´Ğ»Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ…Ğ¸Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€ Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¾Ğ±Ñ…Ğ¾Ğ´Ğ° Ğ³Ñ€Ğ°Ñ„Ğ° Ğ¸ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ ÑƒĞ²Ğ¸Ğ´ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ ÑĞ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸ Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ³Ñ€ÑƒĞ¿Ğ¿. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ñ‹Ğ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ñ‹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… GTR-CoT-1.3M Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº MolRec-Bench.'}, 'en': {'title': 'Transforming Molecular Images into Accurate Data with GTR-Mol-VLM', 'desc': 'GTR-Mol-VLM is a new vision-language model designed to convert molecular images into machine-readable formats with high accuracy. It uses a unique Graph Traversal mechanism that mimics human reasoning by analyzing molecular graphs step-by-step. Additionally, it employs a data-centric approach to ensure that the model accurately recognizes abbreviated structures in images. The model has been tested against various benchmarks and has shown significant improvements, especially in handling complex molecular structures and functional group abbreviations.'}, 'zh': {'title': 'GTR-Mol-VLMï¼šæå‡åˆ†å­å›¾åƒè¯†åˆ«çš„æ™ºèƒ½æ–°æ–¹æ³•', 'desc': 'GTR-Mol-VLMæ˜¯ä¸€ç§ç»“åˆå›¾éå†å’Œæ•°æ®ä¸­å¿ƒåŸåˆ™çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿå°†åˆ†å­å›¾åƒå‡†ç¡®è½¬æ¢ä¸ºæœºå™¨å¯è¯»æ ¼å¼ã€‚è¯¥æ¨¡å‹é€šè¿‡å›¾éå†æœºåˆ¶æ¨¡æ‹Ÿäººç±»æ¨ç†ï¼Œé€æ­¥è§£æåˆ†å­å›¾ï¼Œè§£å†³äº†å¤æ‚ç»“æ„å’ŒåŠŸèƒ½åŸºå›¢ç¼©å†™çš„é—®é¢˜ã€‚ä¸ºäº†æ”¯æŒæ¨¡å‹å¼€å‘ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„æŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†GTR-CoT-1.3Mï¼Œå¹¶æ¨å‡ºäº†MolRec-BenchåŸºå‡†æµ‹è¯•ï¼Œä»¥è¯„ä¼°å›¾è§£æçš„å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGTR-Mol-VLMåœ¨å¤„ç†åˆ†å­å›¾åƒæ—¶çš„è¡¨ç°ä¼˜äºå…¶ä»–ä¸“ä¸šæ¨¡å‹å’Œé€šç”¨æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨åŠŸèƒ½åŸºå›¢ç¼©å†™çš„åœºæ™¯ä¸­ï¼Œå‡†ç¡®ç‡æé«˜äº†çº¦14ä¸ªç™¾åˆ†ç‚¹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.06444', 'title': 'Saffron-1: Towards an Inference Scaling Paradigm for LLM Safety\n  Assurance', 'url': 'https://huggingface.co/papers/2506.06444', 'abstract': "SAFFRON introduces multifurcation reward models to improve safety assurance in large language models by addressing the exploration-efficiency dilemma in inference scaling.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing safety assurance research has primarily focused on training-phase alignment to instill safe behaviors into LLMs. However, recent studies have exposed these methods' susceptibility to diverse jailbreak attacks. Concurrently, inference scaling has significantly advanced LLM reasoning capabilities but remains unexplored in the context of safety assurance. Addressing this gap, our work pioneers inference scaling for robust and effective LLM safety against emerging threats. We reveal that conventional inference scaling techniques, despite their success in reasoning tasks, perform poorly in safety contexts, even falling short of basic approaches like Best-of-N Sampling. We attribute this inefficiency to a newly identified challenge, the exploration--efficiency dilemma, arising from the high computational overhead associated with frequent process reward model (PRM) evaluations. To overcome this dilemma, we propose SAFFRON, a novel inference scaling paradigm tailored explicitly for safety assurance. Central to our approach is the introduction of a multifurcation reward model (MRM) that significantly reduces the required number of reward model evaluations. To operationalize this paradigm, we further propose: (i) a partial supervision training objective for MRM, (ii) a conservative exploration constraint to prevent out-of-distribution explorations, and (iii) a Trie-based key--value caching strategy that facilitates cache sharing across sequences during tree search. Extensive experiments validate the effectiveness of our method. Additionally, we publicly release our trained multifurcation reward model (Saffron-1) and the accompanying token-level safety reward dataset (Safety4M) to accelerate future research in LLM safety. Our code, model, and data are publicly available at https://github.com/q-rz/saffron , and our project homepage is at https://q-rz.github.io/p/saffron .", 'score': 7, 'issue_id': 4209, 'pub_date': '2025-06-06', 'pub_date_card': {'ru': '6 Ğ¸ÑĞ½Ñ', 'en': 'June 6', 'zh': '6æœˆ6æ—¥'}, 'hash': '86bb27c97ab5d915', 'authors': ['Ruizhong Qiu', 'Gaotang Li', 'Tianxin Wei', 'Jingrui He', 'Hanghang Tong'], 'affiliations': ['University of Illinois Urbana-Champaign, IL, USA'], 'pdf_title_img': 'assets/pdf/title_img/2506.06444.jpg', 'data': {'categories': ['#open_source', '#security', '#dataset', '#alignment', '#rl', '#inference'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'SAFFRON: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SAFFRON - Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ (MRM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµĞ¼Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ñ†ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ MRM, Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½ÑĞµÑ€Ğ²Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµÑ„Ğ¸ĞºÑĞ½Ğ¾Ğ³Ğ¾ Ğ´ĞµÑ€ĞµĞ²Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ² Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ LLM Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‰Ğ¸Ñ… ÑƒĞ³Ñ€Ğ¾Ğ·.'}, 'en': {'title': 'SAFFRON: Enhancing LLM Safety with Multifurcation Reward Models', 'desc': 'SAFFRON presents a new approach to enhance safety in large language models (LLMs) by introducing multifurcation reward models that tackle the exploration-efficiency dilemma during inference scaling. Traditional safety methods have struggled against jailbreak attacks and have not effectively integrated with the advancements in LLM reasoning capabilities. The paper identifies that existing inference scaling techniques are inadequate for safety assurance, often performing worse than simpler methods. To address this, SAFFRON proposes a novel paradigm that reduces the number of necessary reward model evaluations, ensuring safer and more efficient LLM operations.'}, 'zh': {'title': 'SAFFRONï¼šæå‡å¤§å‹è¯­è¨€æ¨¡å‹å®‰å…¨æ€§çš„åˆ›æ–°æ–¹æ¡ˆ', 'desc': 'SAFFRONæå‡ºäº†ä¸€ç§å¤šåˆ†å‰å¥–åŠ±æ¨¡å‹ï¼Œä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§ä¿éšœï¼Œè§£å†³æ¨ç†æ‰©å±•ä¸­çš„æ¢ç´¢æ•ˆç‡å›°å¢ƒã€‚ç°æœ‰çš„å®‰å…¨ä¿éšœç ”ç©¶ä¸»è¦é›†ä¸­åœ¨è®­ç»ƒé˜¶æ®µçš„å¯¹é½ï¼Œä»¥åŸ¹å…»LLMçš„å®‰å…¨è¡Œä¸ºï¼Œä½†è¿™äº›æ–¹æ³•åœ¨é¢å¯¹å„ç§æ”»å‡»æ—¶è¡¨ç°å‡ºè„†å¼±æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶é¦–æ¬¡æ¢è®¨äº†æ¨ç†æ‰©å±•åœ¨å®‰å…¨ä¿éšœä¸­çš„åº”ç”¨ï¼Œå‘ç°ä¼ ç»Ÿçš„æ¨ç†æ‰©å±•æŠ€æœ¯åœ¨å®‰å…¨ä¸Šä¸‹æ–‡ä¸­è¡¨ç°ä¸ä½³ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¨ç†æ‰©å±•èŒƒå¼ï¼Œç»“åˆå¤šåˆ†å‰å¥–åŠ±æ¨¡å‹ï¼Œæ˜¾è‘—å‡å°‘äº†å¥–åŠ±æ¨¡å‹è¯„ä¼°çš„æ¬¡æ•°ï¼Œä»è€Œæé«˜äº†å®‰å…¨æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.07986', 'title': 'Rethinking Cross-Modal Interaction in Multimodal Diffusion Transformers', 'url': 'https://huggingface.co/papers/2506.07986', 'abstract': 'Temperature-Adjusted Cross-modal Attention (TACA) improves text-image alignment in diffusion-based multimodal transformers by dynamically balancing multimodal interactions and provides a parameter-efficient solution.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Diffusion Transformers (MM-DiTs) have achieved remarkable progress in text-driven visual generation. However, even state-of-the-art MM-DiT models like FLUX struggle with achieving precise alignment between text prompts and generated content. We identify two key issues in the attention mechanism of MM-DiT, namely 1) the suppression of cross-modal attention due to token imbalance between visual and textual modalities and 2) the lack of timestep-aware attention weighting, which hinder the alignment. To address these issues, we propose Temperature-Adjusted Cross-modal Attention (TACA), a parameter-efficient method that dynamically rebalances multimodal interactions through temperature scaling and timestep-dependent adjustment. When combined with LoRA fine-tuning, TACA significantly enhances text-image alignment on the T2I-CompBench benchmark with minimal computational overhead. We tested TACA on state-of-the-art models like FLUX and SD3.5, demonstrating its ability to improve image-text alignment in terms of object appearance, attribute binding, and spatial relationships. Our findings highlight the importance of balancing cross-modal attention in improving semantic fidelity in text-to-image diffusion models. Our codes are publicly available at https://github.com/Vchitect/TACA', 'score': 6, 'issue_id': 4209, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 Ğ¸ÑĞ½Ñ', 'en': 'June 9', 'zh': '6æœˆ9æ—¥'}, 'hash': '54c2bf2395cf58cc', 'authors': ['Zhengyao Lv', 'Tianlin Pan', 'Chenyang Si', 'Zhaoxi Chen', 'Wangmeng Zuo', 'Ziwei Liu', 'Kwan-Yee K. Wong'], 'affiliations': ['Harbin Institute of Technology', 'Nanjing University', 'Nanyang Technological University', 'The University of Hong Kong', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2506.07986.jpg', 'data': {'categories': ['#multimodal', '#open_source', '#optimization', '#training', '#diffusion', '#benchmark'], 'emoji': 'ğŸ”€', 'ru': {'title': 'Ğ‘Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Temperature-Adjusted Cross-modal Attention (TACA) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ…. TACA Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‚ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸, Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‰ĞµĞ¹ Ğ¾Ñ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑˆĞ°Ğ³Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ñ ÑƒÑ‡ĞµÑ‚Ğ° Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ğ² Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. TACA Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ….'}, 'en': {'title': 'Enhancing Text-Image Alignment with TACA', 'desc': 'The paper introduces Temperature-Adjusted Cross-modal Attention (TACA), a novel approach to enhance text-image alignment in multimodal diffusion transformers. TACA addresses two main challenges: the imbalance in token representation between text and images, and the absence of attention weighting that considers the timing of inputs. By implementing temperature scaling and timestep-dependent adjustments, TACA efficiently rebalances interactions between modalities. The results show that TACA, when used with LoRA fine-tuning, significantly improves alignment in state-of-the-art models, leading to better semantic fidelity in generated images.'}, 'zh': {'title': 'æ¸©åº¦è°ƒæ•´è·¨æ¨¡æ€æ³¨æ„åŠ›ï¼šæå‡æ–‡æœ¬ä¸å›¾åƒå¯¹é½çš„åˆ©å™¨', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºæ¸©åº¦è°ƒæ•´è·¨æ¨¡æ€æ³¨æ„åŠ›ï¼ˆTACAï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨æ”¹å–„åŸºäºæ‰©æ•£çš„å¤šæ¨¡æ€å˜æ¢å™¨ä¸­çš„æ–‡æœ¬ä¸å›¾åƒå¯¹é½ã€‚æˆ‘ä»¬å‘ç°ç°æœ‰æ¨¡å‹åœ¨æ–‡æœ¬æç¤ºä¸ç”Ÿæˆå†…å®¹ä¹‹é—´å­˜åœ¨æ³¨æ„åŠ›æœºåˆ¶çš„ä¸è¶³ï¼Œä¸»è¦ä½“ç°åœ¨è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€ä¹‹é—´çš„ä»¤ç‰Œä¸å¹³è¡¡ä»¥åŠç¼ºä¹æ—¶é—´æ­¥æ„ŸçŸ¥çš„æ³¨æ„åŠ›åŠ æƒã€‚TACAé€šè¿‡æ¸©åº¦ç¼©æ”¾å’Œæ—¶é—´æ­¥ä¾èµ–çš„è°ƒæ•´ï¼ŒåŠ¨æ€å¹³è¡¡å¤šæ¨¡æ€äº¤äº’ï¼Œä»è€Œæé«˜æ–‡æœ¬ä¸å›¾åƒçš„å¯¹é½æ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTACAåœ¨FLUXå’ŒSD3.5ç­‰å…ˆè¿›æ¨¡å‹ä¸Šæ˜¾è‘—æå‡äº†å›¾åƒ-æ–‡æœ¬å¯¹é½çš„å‡†ç¡®æ€§ï¼Œä¸”è®¡ç®—å¼€é”€æå°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.07298', 'title': 'Pre-trained Large Language Models Learn Hidden Markov Models In-context', 'url': 'https://huggingface.co/papers/2506.07298', 'abstract': 'LLMs using in-context learning can accurately predict sequences generated by HMMs, showcasing its potential for uncovering hidden structures in complex data.  \t\t\t\t\tAI-generated summary \t\t\t\t Hidden Markov Models (HMMs) are foundational tools for modeling sequential data with latent Markovian structure, yet fitting them to real-world data remains computationally challenging. In this work, we show that pre-trained large language models (LLMs) can effectively model data generated by HMMs via in-context learning (ICL)x2013their ability to infer patterns from examples within a prompt. On a diverse set of synthetic HMMs, LLMs achieve predictive accuracy approaching the theoretical optimum. We uncover novel scaling trends influenced by HMM properties, and offer theoretical conjectures for these empirical observations. We also provide practical guidelines for scientists on using ICL as a diagnostic tool for complex data. On real-world animal decision-making tasks, ICL achieves competitive performance with models designed by human experts. To our knowledge, this is the first demonstration that ICL can learn and predict HMM-generated sequencesx2013an advance that deepens our understanding of in-context learning in LLMs and establishes its potential as a powerful tool for uncovering hidden structure in complex scientific data.', 'score': 5, 'issue_id': 4208, 'pub_date': '2025-06-08', 'pub_date_card': {'ru': '8 Ğ¸ÑĞ½Ñ', 'en': 'June 8', 'zh': '6æœˆ8æ—¥'}, 'hash': '568113a6bc87dd15', 'authors': ['Yijia Dai', 'Zhaolin Gao', 'Yahya Satter', 'Sarah Dean', 'Jennifer J. Sun'], 'affiliations': ['Cornell University'], 'pdf_title_img': 'assets/pdf/title_img/2506.07298.jpg', 'data': {'categories': ['#rl', '#transfer_learning', '#data', '#science', '#multimodal'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'LLM Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¼Ğ°Ñ€ĞºĞ¾Ğ²ÑĞºĞ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (HMM), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ (ICL). LLM Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ±Ğ»Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğº Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼ÑƒĞ¼Ñƒ, Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… HMM. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ñ‚ĞµĞ½Ğ´ĞµĞ½Ñ†Ğ¸Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‰Ğ¸Ğµ Ğ¾Ñ‚ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² HMM, Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑƒĞ³Ğ»ÑƒĞ±Ğ»ÑÑÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ICL Ğ² LLM Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ĞµĞ³Ğ¾ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» ĞºĞ°Ğº Ğ¼Ğ¾Ñ‰Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Unlocking Hidden Structures with In-Context Learning in LLMs', 'desc': 'This paper demonstrates that large language models (LLMs) can effectively use in-context learning (ICL) to predict sequences generated by Hidden Markov Models (HMMs). The authors show that LLMs can achieve high predictive accuracy on synthetic HMM data, approaching theoretical limits. They also identify scaling trends related to HMM characteristics and provide insights into using ICL as a diagnostic tool for complex datasets. Additionally, the study highlights that ICL performs competitively on real-world tasks compared to expert-designed models, showcasing its potential in revealing hidden structures in data.'}, 'zh': {'title': 'åˆ©ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ æ­ç¤ºå¤æ‚æ•°æ®çš„æ½œåŠ›', 'desc': 'æœ¬ç ”ç©¶å±•ç¤ºäº†é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å¤Ÿé€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æœ‰æ•ˆåœ°é¢„æµ‹ç”±éšé©¬å°”å¯å¤«æ¨¡å‹ï¼ˆHMMsï¼‰ç”Ÿæˆçš„åºåˆ—ã€‚è¿™è¡¨æ˜LLMsåœ¨æ­ç¤ºå¤æ‚æ•°æ®ä¸­çš„éšè—ç»“æ„æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚æˆ‘ä»¬åœ¨å¤šç§åˆæˆHMMsä¸Šæµ‹è¯•äº†LLMsï¼Œå‘ç°å…¶é¢„æµ‹å‡†ç¡®æ€§æ¥è¿‘ç†è®ºæœ€ä¼˜å€¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†ä½¿ç”¨ICLä½œä¸ºå¤æ‚æ•°æ®è¯Šæ–­å·¥å…·çš„å®ç”¨æŒ‡å—ï¼Œå¹¶åœ¨çœŸå®çš„åŠ¨ç‰©å†³ç­–ä»»åŠ¡ä¸­å–å¾—äº†ä¸äººç±»ä¸“å®¶è®¾è®¡æ¨¡å‹ç›¸å½“çš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.07712', 'title': 'Through the Valley: Path to Effective Long CoT Training for Small\n  Language Models', 'url': 'https://huggingface.co/papers/2506.07712', 'abstract': 'Small language models experience significant performance declines when trained on long chain-of-thought data due to error accumulation, impacting downstream reinforcement learning but potentially mitigated by extensive supervised fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Long chain-of-thought (CoT) supervision has become a common strategy to enhance reasoning in language models. While effective for large models, we identify a phenomenon we call Long CoT Degradation, in which small language models (SLMs; <=3B parameters) trained on limited long CoT data experience significant performance deterioration. Through extensive experiments on the Qwen2.5, LLaMA3 and Gemma3 families, we demonstrate that this degradation is widespread across SLMs. In some settings, models trained on only 8k long CoT examples lose up to 75% of their original performance before fine-tuning. Strikingly, we further observe that for some particularly small models, even training on 220k long CoT examples fails to recover or surpass their original performance prior to fine-tuning. Our analysis attributes this effect to error accumulation: while longer responses increase the capacity for multi-step reasoning, they also amplify the risk of compounding mistakes. Furthermore, we find that Long CoT Degradation may negatively impacts downstream reinforcement learning (RL), although this can be alleviated by sufficiently scaled supervised fine-tuning (SFT). Our findings challenge common assumptions about the benefits of long CoT training for SLMs and offer practical guidance for building more effective small-scale reasoning models.', 'score': 4, 'issue_id': 4208, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 Ğ¸ÑĞ½Ñ', 'en': 'June 9', 'zh': '6æœˆ9æ—¥'}, 'hash': '856fa5edc302286a', 'authors': ['Renjie Luo', 'Jiaxi Li', 'Chen Huang', 'Wei Lu'], 'affiliations': ['StatNLP Research Group, Singapore University of Technology and Design'], 'pdf_title_img': 'assets/pdf/title_img/2506.07712.jpg', 'data': {'categories': ['#small_models', '#long_context', '#training', '#reasoning', '#rl'], 'emoji': 'ğŸ“‰', 'ru': {'title': 'ĞœĞ°Ğ»Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑÑ‚Ñ€Ğ°Ğ´Ğ°ÑÑ‚ Ğ¾Ñ‚ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ñ„ĞµĞ½Ğ¾Ğ¼ĞµĞ½ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ°Ğ»Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (CoT). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¼ĞµĞ½ĞµĞµ Ñ‡ĞµĞ¼ 3 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ñ‚ĞµÑ€ÑÑ‚ÑŒ Ğ´Ğ¾ 75% ÑĞ²Ğ¾ĞµĞ¹ Ğ¸Ğ·Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ÑĞ»Ğµ Ñ‚Ğ°ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾ ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑŠÑÑĞ½ÑĞµÑ‚ÑÑ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ñ CoT Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ğ»Ğ¸ÑÑ‚ÑŒ Ğ½Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ñ…Ğ¾Ñ‚Ñ ÑÑ‚Ğ¾Ñ‚ ÑÑ„Ñ„ĞµĞºÑ‚ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ ÑĞ¼ÑĞ³Ñ‡Ğ¸Ñ‚ÑŒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¼ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ´ Ğ¿Ñ€Ğ¸ÑĞ¼Ğ¾Ñ‚Ñ€Ğ¾Ğ¼.'}, 'en': {'title': 'Mitigating Long CoT Degradation in Small Language Models', 'desc': 'This paper discusses a problem called Long CoT Degradation, which affects small language models (SLMs) when they are trained on long chain-of-thought (CoT) data. The authors found that these models can lose a significant amount of performance due to error accumulation, especially when trained on limited long CoT examples. Their experiments show that even with a large number of training examples, some small models still struggle to recover their original performance. The study suggests that while long CoT training can enhance reasoning, it may not be beneficial for smaller models without adequate supervised fine-tuning to mitigate the negative effects.'}, 'zh': {'title': 'å°å‹æ¨¡å‹çš„é•¿é“¾æ€ç»´é€€åŒ–é—®é¢˜', 'desc': 'å°å‹è¯­è¨€æ¨¡å‹åœ¨é•¿é“¾æ€ç»´æ•°æ®ä¸Šè®­ç»ƒæ—¶ï¼Œå› é”™è¯¯ç´¯ç§¯è€Œå¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œè¿™ç§ç°è±¡è¢«ç§°ä¸ºé•¿é“¾æ€ç»´é€€åŒ–ã€‚å°½ç®¡é•¿é“¾æ€ç»´ç›‘ç£å¯¹å¤§å‹æ¨¡å‹æœ‰æ•ˆï¼Œä½†å°å‹æ¨¡å‹åœ¨æœ‰é™çš„é•¿é“¾æ€ç»´æ•°æ®ä¸Šè®­ç»ƒæ—¶ï¼Œæ€§èƒ½å¯èƒ½ä¸‹é™é«˜è¾¾75%ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œé”™è¯¯ç´¯ç§¯æ˜¯å¯¼è‡´è¿™ç§é€€åŒ–çš„ä¸»è¦åŸå› ï¼Œé•¿å“åº”è™½ç„¶å¢åŠ äº†å¤šæ­¥æ¨ç†çš„èƒ½åŠ›ï¼Œä½†ä¹ŸåŠ å¤§äº†é”™è¯¯å åŠ çš„é£é™©ã€‚æ­¤å¤–ï¼Œé•¿é“¾æ€ç»´é€€åŒ–è¿˜å¯èƒ½å¯¹åç»­çš„å¼ºåŒ–å­¦ä¹ äº§ç”Ÿè´Ÿé¢å½±å“ï¼Œä½†é€šè¿‡å……åˆ†è§„æ¨¡çš„ç›‘ç£å¾®è°ƒå¯ä»¥ç¼“è§£è¿™ä¸€é—®é¢˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.07463', 'title': 'CCI4.0: A Bilingual Pretraining Dataset for Enhancing Reasoning in Large\n  Language Models', 'url': 'https://huggingface.co/papers/2506.07463', 'abstract': 'A large-scale bilingual pre-training dataset, CCI4.0, enhances data quality and diverse reasoning patterns for language models, leading to improved performance in downstream tasks like math and code reflection.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce CCI4.0, a large-scale bilingual pre-training dataset engineered for superior data quality and diverse human-like reasoning trajectory. CCI4.0 occupies roughly 35 TB of disk space and comprises two sub-datasets: CCI4.0-M2-Base and CCI4.0-M2-CoT. CCI4.0-M2-Base combines a 5.2 TB carefully curated Chinese web corpus, a 22.5 TB English subset from Nemotron-CC, and diverse sources from math, wiki, arxiv, and code. Although these data are mostly sourced from well-processed datasets, the quality standards of various domains are dynamic and require extensive expert experience and labor to process. So, we propose a novel pipeline justifying data quality mainly based on models through two-stage deduplication, multiclassifier quality scoring, and domain-aware fluency filtering. We extract 4.5 billion pieces of CoT(Chain-of-Thought) templates, named CCI4.0-M2-CoT. Differing from the distillation of CoT from larger models, our proposed staged CoT extraction exemplifies diverse reasoning patterns and significantly decreases the possibility of hallucination. Empirical evaluations demonstrate that LLMs pre-trained in CCI4.0 benefit from cleaner, more reliable training signals, yielding consistent improvements in downstream tasks, especially in math and code reflection tasks. Our results underscore the critical role of rigorous data curation and human thinking templates in advancing LLM performance, shedding some light on automatically processing pretraining corpora.', 'score': 4, 'issue_id': 4209, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 Ğ¸ÑĞ½Ñ', 'en': 'June 9', 'zh': '6æœˆ9æ—¥'}, 'hash': '219a3c7d2fbb1e22', 'authors': ['Guang Liu', 'Liangdong Wang', 'Jijie Li', 'Yang Yu', 'Yao Xu', 'Jiabei Chen', 'Yu Bai', 'Feng Liao', 'Yonghua Lin'], 'affiliations': ['baai.ac.cn'], 'pdf_title_img': 'assets/pdf/title_img/2506.07463.jpg', 'data': {'categories': ['#hallucinations', '#data', '#dataset', '#reasoning', '#transfer_learning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'CCI4.0: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'CCI4.0 - ÑÑ‚Ğ¾ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ñ€Ğ¾Ğ´Ğµ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¸ Ğ¸ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸ ĞºĞ¾Ğ´Ğ°. ĞĞ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… Ğ¿Ğ¾Ğ´Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ²: CCI4.0-M2-Base Ğ¸ CCI4.0-M2-CoT, Ğ¾Ğ±Ñ‰Ğ¸Ğ¼ Ğ¾Ğ±ÑŠĞµĞ¼Ğ¾Ğ¼ Ğ¾ĞºĞ¾Ğ»Ğ¾ 35 Ğ¢Ğ‘. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ğ´ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚Ğ¾Ğ², Ğ¾Ñ†ĞµĞ½ĞºÑƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ° Ğ¸ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ğ¼.'}, 'en': {'title': 'Enhancing Language Models with CCI4.0: Quality Data for Better Reasoning', 'desc': 'The paper presents CCI4.0, a large-scale bilingual pre-training dataset designed to improve the quality of data and enhance reasoning capabilities in language models. It consists of two main sub-datasets, CCI4.0-M2-Base and CCI4.0-M2-CoT, which together provide a diverse range of high-quality data from various domains. The authors introduce a novel data curation pipeline that includes deduplication, quality scoring, and fluency filtering to ensure the reliability of the training data. Empirical results show that language models trained on CCI4.0 exhibit significant performance improvements in tasks such as mathematics and code reflection, highlighting the importance of quality data in machine learning.'}, 'zh': {'title': 'æå‡è¯­è¨€æ¨¡å‹æ€§èƒ½çš„åŒè¯­æ•°æ®é›†CCI4.0', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºCCI4.0çš„å¤§è§„æ¨¡åŒè¯­é¢„è®­ç»ƒæ•°æ®é›†ï¼Œæ—¨åœ¨æé«˜æ•°æ®è´¨é‡å’Œå¤šæ ·åŒ–çš„æ¨ç†æ¨¡å¼ã€‚CCI4.0åŒ…å«çº¦35 TBçš„æ•°æ®ï¼Œåˆ†ä¸ºä¸¤ä¸ªå­æ•°æ®é›†ï¼šCCI4.0-M2-Baseå’ŒCCI4.0-M2-CoTã€‚é€šè¿‡ä¸¤é˜¶æ®µå»é‡ã€å¤šåˆ†ç±»å™¨è´¨é‡è¯„åˆ†å’Œé¢†åŸŸæ„ŸçŸ¥æµç•…æ€§è¿‡æ»¤ç­‰æ–°é¢–æµç¨‹ï¼Œç¡®ä¿äº†æ•°æ®çš„é«˜è´¨é‡ã€‚å®éªŒè¯æ˜ï¼Œä½¿ç”¨CCI4.0é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦å’Œä»£ç åå°„ç­‰ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.08012', 'title': 'GUI-Reflection: Empowering Multimodal GUI Models with Self-Reflection\n  Behavior', 'url': 'https://huggingface.co/papers/2506.08012', 'abstract': 'A novel framework, GUI-Reflection, integrates self-reflection and error correction into multimodal GUI models through specialized training stages, enabling more robust and intelligent automation.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) have shown great potential in revolutionizing Graphical User Interface (GUI) automation. However, existing GUI models mostly rely on learning from nearly error-free offline trajectories, thus lacking reflection and error recovery capabilities. To bridge this gap, we propose GUI-Reflection, a novel framework that explicitly integrates self-reflection and error correction capabilities into end-to-end multimodal GUI models throughout dedicated training stages: GUI-specific pre-training, offline supervised fine-tuning (SFT), and online reflection tuning. GUI-reflection enables self-reflection behavior emergence with fully automated data generation and learning processes without requiring any human annotation. Specifically, 1) we first propose scalable data pipelines to automatically construct reflection and error correction data from existing successful trajectories. While existing GUI models mainly focus on grounding and UI understanding ability, we propose the GUI-Reflection Task Suite to learn and evaluate reflection-oriented abilities explicitly. 2) Furthermore, we built a diverse and efficient environment for online training and data collection of GUI models on mobile devices. 3) We also present an iterative online reflection tuning algorithm leveraging the proposed environment, enabling the model to continuously enhance its reflection and error correction abilities. Our framework equips GUI agents with self-reflection and correction capabilities, paving the way for more robust, adaptable, and intelligent GUI automation, with all data, models, environments, and tools to be released publicly.', 'score': 3, 'issue_id': 4208, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 Ğ¸ÑĞ½Ñ', 'en': 'June 9', 'zh': '6æœˆ9æ—¥'}, 'hash': '8052bb442adf3c53', 'authors': ['Penghao Wu', 'Shengnan Ma', 'Bo Wang', 'Jiaheng Yu', 'Lewei Lu', 'Ziwei Liu'], 'affiliations': ['S-Lab, Nanyang Technological University', 'SenseTime Research'], 'pdf_title_img': 'assets/pdf/title_img/2506.08012.jpg', 'data': {'categories': ['#optimization', '#dataset', '#agents', '#training', '#open_source', '#multimodal'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¸ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² Ğ˜Ğ˜ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ GUI-Reflection, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ°Ğ¼Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¸ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. Ğ­Ñ‚Ğ° ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑÑ‚Ğ°Ğ¿Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ GUI, Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸. GUI-Reflection Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ·Ğ°Ğ´Ğ°Ñ‡ GUI-Reflection Task Suite Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğº Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ÑÑ€ĞµĞ´Ñƒ Ğ´Ğ»Ñ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ….'}, 'en': {'title': 'Empowering GUI Automation with Self-Reflection and Error Correction', 'desc': 'The paper introduces GUI-Reflection, a new framework that enhances multimodal GUI models by incorporating self-reflection and error correction during training. This approach addresses the limitations of existing models that primarily learn from error-free data, enabling them to recover from mistakes and improve over time. The framework includes specialized training stages such as pre-training, supervised fine-tuning, and online reflection tuning, allowing models to learn from their own errors without human intervention. By automating data generation and focusing on reflection-oriented tasks, GUI-Reflection aims to create more robust and intelligent automation for graphical user interfaces.'}, 'zh': {'title': 'è‡ªæˆ‘åæ€ä¸é”™è¯¯çº æ­£ï¼Œæå‡GUIè‡ªåŠ¨åŒ–æ™ºèƒ½', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ¡†æ¶GUI-Reflectionï¼Œå°†è‡ªæˆ‘åæ€å’Œé”™è¯¯çº æ­£èƒ½åŠ›æ•´åˆåˆ°å¤šæ¨¡æ€å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰æ¨¡å‹ä¸­ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸“é—¨çš„è®­ç»ƒé˜¶æ®µï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨è‡ªåŠ¨åŒ–è¿‡ç¨‹ä¸­æ›´å…·é²æ£’æ€§å’Œæ™ºèƒ½æ€§ã€‚æˆ‘ä»¬è®¾è®¡äº†å¯æ‰©å±•çš„æ•°æ®ç®¡é“ï¼Œè‡ªåŠ¨ç”Ÿæˆåæ€å’Œé”™è¯¯çº æ­£çš„æ•°æ®ï¼Œå¹¶æå‡ºäº†GUI-Reflectionä»»åŠ¡å¥—ä»¶æ¥è¯„ä¼°è¿™äº›èƒ½åŠ›ã€‚æœ€ç»ˆï¼Œæ¡†æ¶ä½¿å¾—GUIä»£ç†å…·å¤‡è‡ªæˆ‘åæ€å’Œçº æ­£èƒ½åŠ›ï¼Œä¸ºæ›´æ™ºèƒ½çš„GUIè‡ªåŠ¨åŒ–å¥ å®šåŸºç¡€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.07530', 'title': 'BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation', 'url': 'https://huggingface.co/papers/2506.07530', 'abstract': "BitVLA, a 1-bit VLA model with ternary parameters, achieves comparable performance to OpenVLA-OFT on LIBERO while using 29.8% less memory through distillation-aware training.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language-Action (VLA) models have shown impressive capabilities across a wide range of robotics manipulation tasks. However, their growing model size poses significant challenges for deployment on resource-constrained robotic systems. While 1-bit pretraining has proven effective for enhancing the inference efficiency of large language models with minimal performance loss, its application to VLA models remains underexplored. In this work, we present BitVLA, the first 1-bit VLA model for robotics manipulation, in which every parameter is ternary, i.e., {-1, 0, 1}. To further reduce the memory footprint of the vision encoder, we propose the distillation-aware training strategy that compresses the full-precision encoder to 1.58-bit weights. During this process, a full-precision encoder serves as a teacher model to better align latent representations. Despite the lack of large-scale robotics pretraining, BitVLA achieves performance comparable to the state-of-the-art model OpenVLA-OFT with 4-bit post-training quantization on the LIBERO benchmark, while consuming only 29.8% of the memory. These results highlight BitVLA's promise for deployment on memory-constrained edge devices. We release the code and model weights in https://github.com/ustcwhy/BitVLA.", 'score': 3, 'issue_id': 4208, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 Ğ¸ÑĞ½Ñ', 'en': 'June 9', 'zh': '6æœˆ9æ—¥'}, 'hash': '2ff752565d34986b', 'authors': ['Hongyu Wang', 'Chuyan Xiong', 'Ruiping Wang', 'Xilin Chen'], 'affiliations': ['Key Laboratory of AI Safety, Institute of Computing Technology, Chinese Academy of Sciences', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2506.07530.jpg', 'data': {'categories': ['#optimization', '#small_models', '#robotics', '#inference', '#open_source'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ 1-Ğ±Ğ¸Ñ‚Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ VLA Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸', 'desc': 'BitVLA - ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ°Ñ 1-Ğ±Ğ¸Ñ‚Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ VLA Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹, Ğ³Ğ´Ğµ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ñ‚ĞµÑ€Ğ½Ğ°Ñ€Ğ½Ñ‹Ğ¼ (-1, 0, 1). ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸ĞµĞ¹ Ğ´Ğ»Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° Ğ´Ğ¾ 1,58-Ğ±Ğ¸Ñ‚Ğ½Ñ‹Ñ… Ğ²ĞµÑĞ¾Ğ². BitVLA Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ¹ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ OpenVLA-OFT Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ LIBERO, Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ÑÑ Ğ²ÑĞµĞ³Ğ¾ 29,8% Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. Ğ­Ñ‚Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ BitVLA Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ… Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ.'}, 'en': {'title': 'Efficient Robotics with BitVLA: Less Memory, Same Performance!', 'desc': 'BitVLA is a novel 1-bit Vision-Language-Action (VLA) model designed for robotics manipulation tasks, utilizing ternary parameters to optimize performance. It employs a distillation-aware training strategy that compresses a full-precision vision encoder into 1.58-bit weights, significantly reducing memory usage. Despite not being pretrained on large-scale robotics data, BitVLA achieves performance on par with the state-of-the-art OpenVLA-OFT model while using 29.8% less memory. This advancement makes BitVLA particularly suitable for deployment in resource-limited robotic systems.'}, 'zh': {'title': 'BitVLAï¼šé«˜æ•ˆçš„1ä½è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹', 'desc': 'BitVLAæ˜¯ä¸€ç§1ä½è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹ï¼Œé‡‡ç”¨ä¸‰å…ƒå‚æ•°ï¼Œæ—¨åœ¨è§£å†³æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­çš„å†…å­˜é™åˆ¶é—®é¢˜ã€‚é€šè¿‡ä½¿ç”¨æ³¨æ„è’¸é¦è®­ç»ƒç­–ç•¥ï¼ŒBitVLAåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—å‡å°‘äº†å†…å­˜å ç”¨ï¼Œè¾¾åˆ°ä»…ä½¿ç”¨29.8%çš„å†…å­˜ã€‚å°½ç®¡ç¼ºä¹å¤§è§„æ¨¡çš„æœºå™¨äººé¢„è®­ç»ƒï¼ŒBitVLAåœ¨LIBEROåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºä¸æœ€å…ˆè¿›çš„OpenVLA-OFTæ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚è¯¥ç ”ç©¶å±•ç¤ºäº†BitVLAåœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šçš„åº”ç”¨æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.07491', 'title': 'SpatialLM: Training Large Language Models for Structured Indoor Modeling', 'url': 'https://huggingface.co/papers/2506.07491', 'abstract': 'SpatialLM is a large language model capable of processing 3D point cloud data to produce structured outputs for spatial understanding, outperforming previous methods on layout estimation and object detection tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t SpatialLM is a large language model designed to process 3D point cloud data and generate structured 3D scene understanding outputs. These outputs include architectural elements like walls, doors, windows, and oriented object boxes with their semantic categories. Unlike previous methods which exploit task-specific network designs, our model adheres to the standard multimodal LLM architecture and is fine-tuned directly from open-source LLMs.   To train SpatialLM, we collect a large-scale, high-quality synthetic dataset consisting of the point clouds of 12,328 indoor scenes (54,778 rooms) with ground-truth 3D annotations, and conduct a careful study on various modeling and training decisions. On public benchmarks, our model gives state-of-the-art performance in layout estimation and competitive results in 3D object detection. With that, we show a feasible path for enhancing the spatial understanding capabilities of modern LLMs for applications in augmented reality, embodied robotics, and more.', 'score': 3, 'issue_id': 4208, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 Ğ¸ÑĞ½Ñ', 'en': 'June 9', 'zh': '6æœˆ9æ—¥'}, 'hash': '604b98825a94f8d1', 'authors': ['Yongsen Mao', 'Junhao Zhong', 'Chuan Fang', 'Jia Zheng', 'Rui Tang', 'Hao Zhu', 'Ping Tan', 'Zihan Zhou'], 'affiliations': ['Hong Kong University of Science and Technology', 'Manycore Tech Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2506.07491.jpg', 'data': {'categories': ['#dataset', '#training', '#benchmark', '#3d', '#open_source', '#synthetic', '#multimodal'], 'emoji': 'ğŸ ', 'ru': {'title': 'ĞŸÑ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ 3D-ÑÑ†ĞµĞ½ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'SpatialLM - ÑÑ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ĞºĞ° Ñ‚Ğ¾Ñ‡ĞµĞº Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑÑ†ĞµĞ½. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², SpatialLM Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ LLM Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ¸Ğ· Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… LLM. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¿Ğ¾Ğ¼ĞµÑ‰ĞµĞ½Ğ¸Ğ¹ Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ².'}, 'en': {'title': 'Revolutionizing 3D Spatial Understanding with SpatialLM', 'desc': 'SpatialLM is a large language model that specializes in understanding 3D point cloud data, enabling it to produce structured outputs for spatial comprehension. It identifies and categorizes architectural features such as walls, doors, and windows, as well as oriented object boxes. Unlike earlier approaches that relied on specific network designs for tasks, SpatialLM uses a standard multimodal architecture and is fine-tuned from existing open-source models. The model is trained on a comprehensive dataset of indoor scenes, achieving state-of-the-art results in layout estimation and strong performance in 3D object detection, paving the way for advancements in augmented reality and robotics.'}, 'zh': {'title': 'SpatialLMï¼šæå‡ç©ºé—´ç†è§£çš„è¯­è¨€æ¨¡å‹', 'desc': 'SpatialLMæ˜¯ä¸€ç§å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿå¤„ç†3Dç‚¹äº‘æ•°æ®ï¼Œå¹¶ç”Ÿæˆç»“æ„åŒ–çš„ç©ºé—´ç†è§£è¾“å‡ºã€‚è¿™äº›è¾“å‡ºåŒ…æ‹¬å»ºç­‘å…ƒç´ ï¼Œå¦‚å¢™å£ã€é—¨ã€çª—æˆ·ä»¥åŠå¸¦æœ‰è¯­ä¹‰ç±»åˆ«çš„å®šå‘ç‰©ä½“æ¡†ã€‚ä¸ä¹‹å‰çš„æ–¹æ³•ä¸åŒï¼ŒSpatialLMéµå¾ªæ ‡å‡†çš„å¤šæ¨¡æ€LLMæ¶æ„ï¼Œå¹¶ç›´æ¥ä»å¼€æºLLMè¿›è¡Œå¾®è°ƒã€‚é€šè¿‡æ”¶é›†åŒ…å«12,328ä¸ªå®¤å†…åœºæ™¯çš„é«˜è´¨é‡åˆæˆæ•°æ®é›†ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å¸ƒå±€ä¼°è®¡å’Œ3Dç‰©ä½“æ£€æµ‹ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.07434', 'title': 'Well Begun is Half Done: Low-resource Preference Alignment by\n  Weak-to-Strong Decoding', 'url': 'https://huggingface.co/papers/2506.07434', 'abstract': 'A novel Weak-to-Strong Decoding (WSD) framework enhances the alignment of large language models using a small aligned model to draft responses initially, improving alignment and performance without degrading downstream task performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) require alignment with human preferences to avoid generating offensive, false, or meaningless content. Recently, low-resource methods for LLM alignment have been popular, while still facing challenges in obtaining both high-quality and aligned content. Motivated by the observation that the difficulty of generating aligned responses is concentrated at the beginning of decoding, we propose a novel framework, Weak-to-Strong Decoding (WSD), to enhance the alignment ability of base models by the guidance of a small aligned model. The small model first drafts well-aligned beginnings, followed by the large base model to continue the rest, controlled by a well-designed auto-switch mechanism. We also collect a new dataset, GenerAlign, to fine-tune a small-sized Pilot-3B as the draft model, which effectively enhances different base models under the WSD framework to outperform all baseline methods, while avoiding degradation on downstream tasks, termed as the alignment tax. Extensive experiments are further conducted to examine the impact of different settings and time efficiency, as well as analyses on the intrinsic mechanisms of WSD in depth.', 'score': 3, 'issue_id': 4208, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 Ğ¸ÑĞ½Ñ', 'en': 'June 9', 'zh': '6æœˆ9æ—¥'}, 'hash': '11d72d8081b838d5', 'authors': ['Feifan Song', 'Shaohang Wei', 'Wen Luo', 'Yuxuan Fan', 'Tianyu Liu', 'Guoyin Wang', 'Houfeng Wang'], 'affiliations': ['State Key Laboratory of Multimedia Information Processing, School of Computer Science Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2506.07434.jpg', 'data': {'categories': ['#alignment', '#dataset', '#training', '#rlhf'], 'emoji': 'ğŸ”€', 'ru': {'title': 'Ğ¡Ğ»Ğ°Ğ±Ñ‹Ğ¹ ÑÑ‚Ğ°Ñ€Ñ‚, ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ„Ğ¸Ğ½Ğ¸Ñˆ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. ĞœĞµÑ‚Ğ¾Ğ´ Weak-to-Strong Decoding (WSD) Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ°Ğ»ĞµĞ½ÑŒĞºÑƒÑ Ğ²Ñ‹Ñ€Ğ¾Ğ²Ğ½ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°, Ğ¿Ğ¾ÑĞ»Ğµ Ñ‡ĞµĞ³Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ±Ñ€Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ GenerAlign Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Pilot-3B, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… WSD. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ±ĞµĞ· ÑƒÑ…ÑƒĞ´ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ….'}, 'en': {'title': 'Enhancing LLM Alignment with Weak-to-Strong Decoding', 'desc': 'The paper introduces a new framework called Weak-to-Strong Decoding (WSD) that improves the alignment of large language models (LLMs) with human preferences. It uses a smaller, aligned model to generate the initial part of responses, which helps guide the larger model in producing better outputs. This approach addresses the challenge of generating high-quality and aligned content without negatively impacting the performance on other tasks, known as the alignment tax. The authors also present a new dataset, GenerAlign, which is used to fine-tune the smaller model, leading to superior results compared to existing methods.'}, 'zh': {'title': 'å¼±åˆ°å¼ºè§£ç ï¼šæå‡è¯­è¨€æ¨¡å‹å¯¹é½èƒ½åŠ›çš„åˆ›æ–°æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¼±åˆ°å¼ºè§£ç æ¡†æ¶ï¼ˆWSDï¼‰ï¼Œæ—¨åœ¨é€šè¿‡å°å‹å¯¹é½æ¨¡å‹çš„æŒ‡å¯¼æ¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¯¹é½èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é¦–å…ˆç”±å°æ¨¡å‹è‰æ‹Ÿå‡ºé«˜è´¨é‡çš„å¯¹é½å¼€å¤´ï¼Œç„¶åç”±å¤§å‹åŸºç¡€æ¨¡å‹ç»§ç»­ç”Ÿæˆåç»­å†…å®¹ã€‚é€šè¿‡è®¾è®¡è‰¯å¥½çš„è‡ªåŠ¨åˆ‡æ¢æœºåˆ¶ï¼ŒWSDèƒ½å¤Ÿåœ¨ä¸é™ä½ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—æé«˜æ¨¡å‹çš„å¯¹é½æ•ˆæœã€‚æˆ‘ä»¬è¿˜æ”¶é›†äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†GenerAlignï¼Œä»¥å¾®è°ƒå°å‹Pilot-3Bæ¨¡å‹ï¼Œå®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨å¤šç§åŸºçº¿æ–¹æ³•ä¸­è¡¨ç°ä¼˜è¶Šã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.06941', 'title': 'The Illusion of Thinking: Understanding the Strengths and Limitations of\n  Reasoning Models via the Lens of Problem Complexity', 'url': 'https://huggingface.co/papers/2506.06941', 'abstract': "Large Reasoning Models (LRMs) undergo accuracy collapse at higher complexities and exhibit unique performance scaling behaviors compared to standard LLMs, with failures in exact computation and inconsistent reasoning across scales.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent generations of language models have introduced Large Reasoning Models (LRMs) that generate detailed thinking processes before providing answers. While these models demonstrate improved performance on reasoning benchmarks, their fundamental capabilities, scaling properties, and limitations remain insufficiently understood. Current evaluations primarily focus on established math and coding benchmarks, emphasizing final answer accuracy. However, this evaluation paradigm often suffers from contamination and does not provide insights into the reasoning traces. In this work, we systematically investigate these gaps with the help of controllable puzzle environments that allow precise manipulation of complexity while maintaining consistent logical structures. This setup enables the analysis of not only final answers but also the internal reasoning traces, offering insights into how LRMs think. Through extensive experiments, we show that LRMs face a complete accuracy collapse beyond certain complexities. Moreover, they exhibit a counterintuitive scaling limit: their reasoning effort increases with problem complexity up to a point, then declines despite having remaining token budget. By comparing LRMs with their standard LLM counterparts under same inference compute, we identify three performance regimes: (1) low-complexity tasks where standard models outperform LRMs, (2) medium-complexity tasks where LRMs demonstrates advantage, and (3) high-complexity tasks where both models face complete collapse. We found that LRMs have limitations in exact computation: they fail to use explicit algorithms and reason inconsistently across scales. We also investigate the reasoning traces in more depth, studying the patterns of explored solutions and analyzing the models' computational behavior, shedding light on their strengths, limitations, and raising questions about their reasoning capabilities.", 'score': 2, 'issue_id': 4208, 'pub_date': '2025-06-07', 'pub_date_card': {'ru': '7 Ğ¸ÑĞ½Ñ', 'en': 'June 7', 'zh': '6æœˆ7æ—¥'}, 'hash': '6f3991ea3357f456', 'authors': ['Parshin Shojaee', 'Iman Mirzadeh', 'Keivan Alizadeh', 'Maxwell Horton', 'Samy Bengio', 'Mehrdad Farajtabar'], 'affiliations': ['Apple'], 'pdf_title_img': 'assets/pdf/title_img/2506.06941.jpg', 'data': {'categories': ['#math', '#training', '#benchmark', '#reasoning', '#interpretability'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹: Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (LRM) Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ñ€ĞµĞ·ĞºĞ¾Ğµ Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, LRM Ğ¸Ğ¼ĞµÑÑ‚ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾ÑĞ²Ğ»ÑÑÑ‚ Ğ½ĞµĞ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… Ğ¿Ñ€Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ LRM Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ¸Ñ… ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ ÑĞ»Ğ°Ğ±Ñ‹Ğµ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñ‹, Ğ¿Ğ¾Ğ´Ğ½Ğ¸Ğ¼Ğ°Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¾ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑÑ… ÑÑ‚Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Understanding the Limits of Large Reasoning Models in Complex Tasks', 'desc': 'This paper explores the performance of Large Reasoning Models (LRMs) in relation to their complexity and reasoning capabilities. It reveals that LRMs experience accuracy collapse when faced with high-complexity tasks, which is a significant limitation compared to standard language models (LLMs). The study introduces a controlled environment to analyze both the final answers and the internal reasoning processes of LRMs, highlighting their inconsistent reasoning and failure to perform exact computations. The findings categorize performance into three regimes based on task complexity, providing insights into the strengths and weaknesses of LRMs in reasoning tasks.'}, 'zh': {'title': 'å¤§å‹æ¨ç†æ¨¡å‹çš„å¤æ‚æ€§æŒ‘æˆ˜', 'desc': 'å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨å¤„ç†æ›´å¤æ‚çš„é—®é¢˜æ—¶ä¼šå‡ºç°å‡†ç¡®æ€§å´©æºƒï¼Œå¹¶ä¸”ä¸æ ‡å‡†è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç›¸æ¯”ï¼Œå®ƒä»¬çš„æ€§èƒ½æ‰©å±•è¡Œä¸ºç‹¬ç‰¹ã€‚å°½ç®¡LRMsåœ¨æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬çš„åŸºæœ¬èƒ½åŠ›å’Œå±€é™æ€§ä»ç„¶ä¸å¤Ÿæ¸…æ¥šã€‚é€šè¿‡å¯æ§çš„éš¾é¢˜ç¯å¢ƒï¼Œæˆ‘ä»¬èƒ½å¤Ÿç²¾ç¡®æ“æ§å¤æ‚æ€§ï¼Œå¹¶åˆ†æLRMsçš„å†…éƒ¨æ¨ç†è¿‡ç¨‹ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒLRMsåœ¨é«˜å¤æ‚æ€§ä»»åŠ¡ä¸­ä¼šå®Œå…¨å´©æºƒï¼Œå¹¶ä¸”åœ¨é—®é¢˜å¤æ‚æ€§å¢åŠ æ—¶ï¼Œå®ƒä»¬çš„æ¨ç†åŠªåŠ›ä¼šå…ˆå¢åŠ åå‡å°‘ï¼Œæ˜¾ç¤ºå‡ºåç›´è§‰çš„æ‰©å±•é™åˆ¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.03690', 'title': 'Robust Preference Optimization via Dynamic Target Margins', 'url': 'https://huggingface.co/papers/2506.03690', 'abstract': "The paper introduces Î³-PO, a dynamic target margin preference optimization algorithm that enhances Large Language Models' alignment by adjusting reward margins at the pairwise level, leading to improved performance with minimal impact on training.  \t\t\t\t\tAI-generated summary \t\t\t\t The alignment of Large Language Models (LLMs) is crucial for ensuring their safety and reliability in practical applications. Direct Preference Optimization (DPO) has emerged as an efficient method that directly optimizes models using preference pairs, significantly reducing resource demands. However, the effectiveness of DPO heavily depends on the data quality, which is frequently compromised by noise. In this work, we propose gamma-PO, a dynamic target margin preference optimization algorithm that adjust reward margins at the pairwise level. By introducing instance-specific margin calibration, gamma-PO strategically prioritizes high-confidence pairs (those demonstrating higher reward margins) while suppressing potential noise from ambiguous pairs. Moreover, gamma-PO is a plug-and-play method, compatible with variants of DPO that rely on reward margin between preference pairs. Across benchmarks such as AlpacaEval2 and Arena-Hard, gamma-PO achieves an average 4.4\\% improvement over other baselines, setting new benchmarks for state-of-the-art performance. Additionally, gamma-PO requires minimal code changes and has a negligible impact on training efficiency, making it a robust solution for enhancing LLMs alignment. Our codes are available at https://github.com/sunjie279/gammaPO{https://github.com/sunjie279/gammaPO}.", 'score': 2, 'issue_id': 4208, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 Ğ¸ÑĞ½Ñ', 'en': 'June 4', 'zh': '6æœˆ4æ—¥'}, 'hash': 'a3134e659a450a93', 'authors': ['Jie Sun', 'Junkang Wu', 'Jiancan Wu', 'Zhibo Zhu', 'Xingyu Lu', 'Jun Zhou', 'Lintao Ma', 'Xiang Wang'], 'affiliations': ['Ant Group', 'National University of Singapore', 'Shanghai Key Laboratory of Data Science'], 'pdf_title_img': 'assets/pdf/title_img/2506.03690.jpg', 'data': {'categories': ['#optimization', '#training', '#benchmark', '#rlhf', '#alignment'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Î³-PO: Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Î³-PO - Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ¼Ğ°Ñ€Ğ¶Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). Î³-PO ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ°Ñ€Ğ¶Ñƒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¿Ğ¾Ğ¿Ğ°Ñ€Ğ½Ğ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑˆÑƒĞ¼. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼ Ñ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ°Ğ¼Ğ¸ Direct Preference Optimization (DPO) Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ 4.4% ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. Î³-PO Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ğ² ĞºĞ¾Ğ´Ğµ Ğ¸ Ğ½Ğµ Ğ²Ğ»Ğ¸ÑĞµÑ‚ Ğ½Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞµĞ³Ğ¾ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğ¼ Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ LLM.'}, 'en': {'title': 'Enhancing LLM Alignment with Dynamic Margin Optimization', 'desc': 'The paper presents Î³-PO, a novel algorithm designed to optimize the alignment of Large Language Models (LLMs) by dynamically adjusting reward margins at the pairwise level. This method enhances Direct Preference Optimization (DPO) by focusing on high-confidence preference pairs, which helps to mitigate the effects of noisy data. By implementing instance-specific margin calibration, Î³-PO improves model performance while maintaining training efficiency with minimal code modifications. The results demonstrate an average improvement of 4.4% over existing baselines, establishing new benchmarks in LLM alignment.'}, 'zh': {'title': 'Î³-POï¼šæå‡å¤§å‹è¯­è¨€æ¨¡å‹å¯¹é½æ€§çš„åŠ¨æ€ä¼˜åŒ–ç®—æ³•', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºÎ³-POçš„åŠ¨æ€ç›®æ ‡è¾¹é™…åå¥½ä¼˜åŒ–ç®—æ³•ï¼Œæ—¨åœ¨é€šè¿‡è°ƒæ•´æˆå¯¹çš„å¥–åŠ±è¾¹é™…æ¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¯¹é½æ€§ã€‚è¯¥ç®—æ³•é€šè¿‡å®ä¾‹ç‰¹å®šçš„è¾¹é™…æ ¡å‡†ï¼Œä¼˜å…ˆè€ƒè™‘é«˜ç½®ä¿¡åº¦çš„æˆå¯¹æ•°æ®ï¼ŒåŒæ—¶æŠ‘åˆ¶æ¨¡ç³Šæˆå¯¹æ•°æ®çš„æ½œåœ¨å™ªå£°ã€‚Î³-POä¸ç°æœ‰çš„ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æ–¹æ³•å…¼å®¹ï¼Œèƒ½å¤Ÿåœ¨ä¸æ˜¾è‘—å½±å“è®­ç»ƒæ•ˆç‡çš„æƒ…å†µä¸‹ï¼Œæå‡æ¨¡å‹çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒÎ³-POåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å¹³å‡æé«˜äº†4.4%çš„æ€§èƒ½ï¼Œè®¾å®šäº†æ–°çš„æœ€å…ˆè¿›çš„æ€§èƒ½åŸºå‡†ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents (1)', '#agi', '#alignment (3)', '#architecture (1)', '#audio', '#benchmark (7)', '#cv (2)', '#data (4)', '#dataset (8)', '#diffusion (1)', '#ethics', '#games (2)', '#graphs (1)', '#hallucinations (2)', '#healthcare (1)', '#inference (3)', '#interpretability (1)', '#leakage', '#long_context (2)', '#low_resource', '#machine_translation', '#math (1)', '#multilingual', '#multimodal (5)', '#open_source (7)', '#optimization (7)', '#plp', '#rag', '#reasoning (7)', '#rl (5)', '#rlhf (3)', '#robotics (1)', '#science (2)', '#security (1)', '#small_models (2)', '#story_generation', '#survey (1)', '#synthetic (1)', '#training (10)', '#transfer_learning (2)', '#video'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-06-10 03:43',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-06-10 03:43')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-06-10 03:43')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    