
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 28 papers. June 10.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">10 июня</span> | <span id="title-articles-count">28 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-06-09.html">⬅️ <span id="prev-date">09.06</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-06-11.html">➡️ <span id="next-date">11.06</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-06.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '10 июня', 'en': 'June 10', 'zh': '6月10日'};
        let feedDateNext = {'ru': '11.06', 'en': '06/11', 'zh': '6月11日'};
        let feedDatePrev = {'ru': '09.06', 'en': '06/09', 'zh': '6月9日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2506.08007', 'title': 'Reinforcement Pre-Training', 'url': 'https://huggingface.co/papers/2506.08007', 'abstract': 'Reinforcement Pre-Training (RPT) enhances language model pre-training by framing next-token prediction as a reinforcement learning task, improving accuracy and providing a strong foundation for further fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t In this work, we introduce Reinforcement Pre-Training (RPT) as a new scaling paradigm for large language models and reinforcement learning (RL). Specifically, we reframe next-token prediction as a reasoning task trained using RL, where it receives verifiable rewards for correctly predicting the next token for a given context. RPT offers a scalable method to leverage vast amounts of text data for general-purpose RL, rather than relying on domain-specific annotated answers. By incentivizing the capability of next-token reasoning, RPT significantly improves the language modeling accuracy of predicting the next tokens. Moreover, RPT provides a strong pre-trained foundation for further reinforcement fine-tuning. The scaling curves show that increased training compute consistently improves the next-token prediction accuracy. The results position RPT as an effective and promising scaling paradigm to advance language model pre-training.', 'score': 101, 'issue_id': 4208, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': 'a1ef74d27d1d9d50', 'authors': ['Qingxiu Dong', 'Li Dong', 'Yao Tang', 'Tianzhu Ye', 'Yutao Sun', 'Zhifang Sui', 'Furu Wei'], 'affiliations': ['Microsoft Research', 'Peking University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2506.08007.jpg', 'data': {'categories': ['#optimization', '#training', '#rlhf', '#reasoning', '#rl'], 'emoji': '🧠', 'ru': {'title': 'Обучение с подкреплением открывает новые горизонты для языковых моделей', 'desc': 'Эта статья представляет новый подход к предварительному обучению больших языковых моделей, называемый Reinforcement Pre-Training (RPT). RPT переосмысливает задачу предсказания следующего токена как задачу обучения с подкреплением, где модель получает вознаграждения за правильные предсказания. Этот метод позволяет эффективно использовать огромные объемы текстовых данных для обучения с подкреплением общего назначения. Результаты показывают, что RPT значительно улучшает точность языкового моделирования и создает прочную основу для дальнейшей тонкой настройки.'}, 'en': {'title': 'Reinforcement Learning Boosts Language Model Training', 'desc': 'Reinforcement Pre-Training (RPT) is a novel approach that enhances the pre-training of language models by treating next-token prediction as a reinforcement learning (RL) task. This method allows the model to receive rewards for accurately predicting the next token based on the context, which improves its reasoning capabilities. RPT utilizes large amounts of unannotated text data, making it scalable and effective for general-purpose RL applications. The results demonstrate that RPT not only boosts prediction accuracy but also lays a solid groundwork for subsequent fine-tuning in reinforcement learning tasks.'}, 'zh': {'title': '强化预训练：提升语言模型的下一步预测能力', 'desc': '强化预训练（RPT）通过将下一个标记预测视为强化学习任务，增强了语言模型的预训练。这种方法通过为正确预测给定上下文的下一个标记提供可验证的奖励，从而提高了准确性。RPT利用大量文本数据进行通用强化学习，而不依赖于特定领域的标注答案。通过激励下一个标记推理的能力，RPT显著提高了语言建模的准确性，并为进一步的强化微调提供了坚实的基础。'}}}, {'id': 'https://huggingface.co/papers/2506.07044', 'title': 'Lingshu: A Generalist Foundation Model for Unified Multimodal Medical\n  Understanding and Reasoning', 'url': 'https://huggingface.co/papers/2506.07044', 'abstract': "Lingshu, a medical-specialized MLLM, enhances performance in medical tasks through comprehensive data curation, multi-stage training, and reinforcement learning with verifiable rewards, outperforming existing open-source models.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities in understanding common visual elements, largely due to their large-scale datasets and advanced training strategies. However, their effectiveness in medical applications remains limited due to the inherent discrepancies between data and tasks in medical scenarios and those in the general domain. Concretely, existing medical MLLMs face the following critical limitations: (1) limited coverage of medical knowledge beyond imaging, (2) heightened susceptibility to hallucinations due to suboptimal data curation processes, (3) lack of reasoning capabilities tailored for complex medical scenarios. To address these challenges, we first propose a comprehensive data curation procedure that (1) efficiently acquires rich medical knowledge data not only from medical imaging but also from extensive medical texts and general-domain data; and (2) synthesizes accurate medical captions, visual question answering (VQA), and reasoning samples. As a result, we build a multimodal dataset enriched with extensive medical knowledge. Building on the curated data, we introduce our medical-specialized MLLM: Lingshu. Lingshu undergoes multi-stage training to embed medical expertise and enhance its task-solving capabilities progressively. Besides, we preliminarily explore the potential of applying reinforcement learning with verifiable rewards paradigm to enhance Lingshu's medical reasoning ability. Additionally, we develop MedEvalKit, a unified evaluation framework that consolidates leading multimodal and textual medical benchmarks for standardized, fair, and efficient model assessment. We evaluate the performance of Lingshu on three fundamental medical tasks, multimodal QA, text-based QA, and medical report generation. The results show that Lingshu consistently outperforms the existing open-source multimodal models on most tasks ...", 'score': 48, 'issue_id': 4208, 'pub_date': '2025-06-08', 'pub_date_card': {'ru': '8 июня', 'en': 'June 8', 'zh': '6月8日'}, 'hash': '475506f0d0cabb39', 'authors': ['LASA Team', 'Weiwen Xu', 'Hou Pong Chan', 'Long Li', 'Mahani Aljunied', 'Ruifeng Yuan', 'Jianyu Wang', 'Chenghao Xiao', 'Guizhen Chen', 'Chaoqun Liu', 'Zhaodonghui Li', 'Yu Sun', 'Junao Shen', 'Chaojun Wang', 'Jie Tan', 'Deli Zhao', 'Tingyang Xu', 'Hao Zhang', 'Yu Rong'], 'affiliations': ['DAMO Academy, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2506.07044.jpg', 'data': {'categories': ['#optimization', '#dataset', '#training', '#hallucinations', '#benchmark', '#medical', '#rl', '#reasoning', '#data', '#healthcare', '#multimodal'], 'emoji': '🩺', 'ru': {'title': 'Lingshu: мультимодальная ИИ-модель нового поколения для медицины', 'desc': 'Lingshu - это специализированная мультимодальная языковая модель для медицины, разработанная для улучшения производительности в медицинских задачах. Модель использует комплексный подход к курированию данных, включая медицинские изображения, тексты и общие данные. Lingshu проходит многоэтапное обучение для встраивания медицинских знаний и улучшения способностей решения задач. Модель также применяет обучение с подкреплением с проверяемыми наградами для усиления способностей медицинского рассуждения.'}, 'en': {'title': 'Lingshu: Revolutionizing Medical AI with Enhanced Learning and Reasoning', 'desc': 'Lingshu is a specialized Multimodal Large Language Model (MLLM) designed to improve medical task performance through enhanced data curation and training techniques. It addresses key limitations of existing medical models, such as insufficient medical knowledge coverage and a tendency to produce hallucinations. By utilizing a comprehensive dataset that includes both medical texts and images, Lingshu is trained in multiple stages to develop strong reasoning capabilities for complex medical scenarios. The model is evaluated using the MedEvalKit framework, demonstrating superior performance in tasks like multimodal question answering and medical report generation compared to other open-source models.'}, 'zh': {'title': 'Lingshu：医疗任务的智能助手', 'desc': 'Lingshu是一种专门针对医疗任务的多模态大语言模型（MLLM），通过全面的数据整理、多阶段训练和可验证奖励的强化学习来提升其性能。该模型解决了现有医疗MLLM在知识覆盖、数据整理和推理能力方面的不足。通过高效获取丰富的医疗知识数据，Lingshu能够在多种医疗任务中表现优异。最终，Lingshu在多模态问答、基于文本的问答和医疗报告生成等任务中超越了现有的开源模型。'}}}, {'id': 'https://huggingface.co/papers/2506.07900', 'title': 'MiniCPM4: Ultra-Efficient LLMs on End Devices', 'url': 'https://huggingface.co/papers/2506.07900', 'abstract': 'MiniCPM4, a large language model optimized for end-side devices, achieves efficiency and effectiveness through innovations in model architecture, training data, algorithms, and inference systems.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper introduces MiniCPM4, a highly efficient large language model (LLM) designed explicitly for end-side devices. We achieve this efficiency through systematic innovation in four key dimensions: model architecture, training data, training algorithms, and inference systems. Specifically, in terms of model architecture, we propose InfLLM v2, a trainable sparse attention mechanism that accelerates both prefilling and decoding phases for long-context processing. Regarding training data, we propose UltraClean, an efficient and accurate pre-training data filtering and generation strategy, and UltraChat v2, a comprehensive supervised fine-tuning dataset. These datasets enable satisfactory model performance to be achieved using just 8 trillion training tokens. Regarding training algorithms, we propose ModelTunnel v2 for efficient pre-training strategy search, and improve existing post-training methods by introducing chunk-wise rollout for load-balanced reinforcement learning and data-efficient tenary LLM, BitCPM. Regarding inference systems, we propose CPM.cu that integrates sparse attention, model quantization, and speculative sampling to achieve efficient prefilling and decoding. To meet diverse on-device requirements, MiniCPM4 is available in two versions, with 0.5B and 8B parameters, respectively. Sufficient evaluation results show that MiniCPM4 outperforms open-source models of similar size across multiple benchmarks, highlighting both its efficiency and effectiveness. Notably, MiniCPM4-8B demonstrates significant speed improvements over Qwen3-8B when processing long sequences. Through further adaptation, MiniCPM4 successfully powers diverse applications, including trustworthy survey generation and tool use with model context protocol, clearly showcasing its broad usability.', 'score': 41, 'issue_id': 4208, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': 'edbb02209bdb677b', 'authors': ['MiniCPM Team', 'Chaojun Xiao', 'Yuxuan Li', 'Xu Han', 'Yuzhuo Bai', 'Jie Cai', 'Haotian Chen', 'Wentong Chen', 'Xin Cong', 'Ganqu Cui', 'Ning Ding', 'Shengdan Fan', 'Yewei Fang', 'Zixuan Fu', 'Wenyu Guan', 'Yitong Guan', 'Junshao Guo', 'Yufeng Han', 'Bingxiang He', 'Yuxiang Huang', 'Cunliang Kong', 'Qiuzuo Li', 'Siyuan Li', 'Wenhao Li', 'Yanghao Li', 'Yishan Li', 'Zhen Li', 'Dan Liu', 'Biyuan Lin', 'Yankai Lin', 'Xiang Long', 'Quanyu Lu', 'Yaxi Lu', 'Peiyan Luo', 'Hongya Lyu', 'Litu Ou', 'Yinxu Pan', 'Zekai Qu', 'Qundong Shi', 'Zijun Song', 'Jiayuan Su', 'Zhou Su', 'Ao Sun', 'Xianghui Sun', 'Peijun Tang', 'Fangzheng Wang', 'Feng Wang', 'Shuo Wang', 'Yudong Wang', 'Yesai Wu', 'Zhenyu Xiao', 'Jie Xie', 'Zihao Xie', 'Yukun Yan', 'Jiarui Yuan', 'Kaihuo Zhang', 'Lei Zhang', 'Linyue Zhang', 'Xueren Zhang', 'Yudi Zhang', 'Hengyu Zhao', 'Weilin Zhao', 'Weilun Zhao', 'Yuanqian Zhao', 'Zhi Zheng', 'Ge Zhou', 'Jie Zhou', 'Wei Zhou', 'Zihan Zhou', 'Zixuan Zhou', 'Zhiyuan Liu', 'Guoyang Zeng', 'Chao Jia', 'Dahai Li', 'Maosong Sun'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2506.07900.jpg', 'data': {'categories': ['#optimization', '#dataset', '#long_context', '#training', '#architecture', '#inference', '#open_source', '#data', '#games'], 'emoji': '🚀', 'ru': {'title': 'MiniCPM4: Мощь большой языковой модели в компактном исполнении', 'desc': 'MiniCPM4 - это эффективная большая языковая модель (LLM), оптимизированная для использования на конечных устройствах. Модель достигает высокой производительности благодаря инновациям в архитектуре, обучающих данных, алгоритмах и системах вывода. Ключевые особенности включают механизм разреженного внимания InfLLM v2, стратегию фильтрации данных UltraClean и набор данных UltraChat v2 для обучения. MiniCPM4 превосходит модели аналогичного размера по нескольким бенчмаркам и демонстрирует значительное ускорение при обработке длинных последовательностей.'}, 'en': {'title': 'MiniCPM4: Efficiency Meets Performance for On-Device AI', 'desc': 'MiniCPM4 is a large language model specifically designed for efficient use on end-side devices. It incorporates innovations in model architecture, such as a new sparse attention mechanism, and utilizes advanced training data strategies to enhance performance. The model employs unique training algorithms for effective pre-training and post-training, ensuring it can handle diverse tasks with minimal resources. Evaluation results indicate that MiniCPM4 outperforms similar-sized models, demonstrating its speed and effectiveness in real-world applications.'}, 'zh': {'title': 'MiniCPM4：终端设备的高效语言模型', 'desc': '本文介绍了MiniCPM4，这是一种专为终端设备优化的大型语言模型。通过在模型架构、训练数据、训练算法和推理系统四个关键方面的创新，MiniCPM4实现了高效性和有效性。特别是，采用了可训练的稀疏注意力机制和高效的数据过滤策略，使得模型在处理长上下文时表现出色。评估结果显示，MiniCPM4在多个基准测试中超越了同类开源模型，展现了其广泛的应用潜力。'}}}, {'id': 'https://huggingface.co/papers/2506.06444', 'title': 'Saffron-1: Towards an Inference Scaling Paradigm for LLM Safety\n  Assurance', 'url': 'https://huggingface.co/papers/2506.06444', 'abstract': "SAFFRON introduces multifurcation reward models to improve safety assurance in large language models by addressing the exploration-efficiency dilemma in inference scaling.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing safety assurance research has primarily focused on training-phase alignment to instill safe behaviors into LLMs. However, recent studies have exposed these methods' susceptibility to diverse jailbreak attacks. Concurrently, inference scaling has significantly advanced LLM reasoning capabilities but remains unexplored in the context of safety assurance. Addressing this gap, our work pioneers inference scaling for robust and effective LLM safety against emerging threats. We reveal that conventional inference scaling techniques, despite their success in reasoning tasks, perform poorly in safety contexts, even falling short of basic approaches like Best-of-N Sampling. We attribute this inefficiency to a newly identified challenge, the exploration--efficiency dilemma, arising from the high computational overhead associated with frequent process reward model (PRM) evaluations. To overcome this dilemma, we propose SAFFRON, a novel inference scaling paradigm tailored explicitly for safety assurance. Central to our approach is the introduction of a multifurcation reward model (MRM) that significantly reduces the required number of reward model evaluations. To operationalize this paradigm, we further propose: (i) a partial supervision training objective for MRM, (ii) a conservative exploration constraint to prevent out-of-distribution explorations, and (iii) a Trie-based key--value caching strategy that facilitates cache sharing across sequences during tree search. Extensive experiments validate the effectiveness of our method. Additionally, we publicly release our trained multifurcation reward model (Saffron-1) and the accompanying token-level safety reward dataset (Safety4M) to accelerate future research in LLM safety. Our code, model, and data are publicly available at https://github.com/q-rz/saffron , and our project homepage is at https://q-rz.github.io/p/saffron .", 'score': 40, 'issue_id': 4209, 'pub_date': '2025-06-06', 'pub_date_card': {'ru': '6 июня', 'en': 'June 6', 'zh': '6月6日'}, 'hash': '86bb27c97ab5d915', 'authors': ['Ruizhong Qiu', 'Gaotang Li', 'Tianxin Wei', 'Jingrui He', 'Hanghang Tong'], 'affiliations': ['University of Illinois Urbana-Champaign, IL, USA'], 'pdf_title_img': 'assets/pdf/title_img/2506.06444.jpg', 'data': {'categories': ['#open_source', '#security', '#dataset', '#alignment', '#rl', '#inference'], 'emoji': '🛡️', 'ru': {'title': 'SAFFRON: Революция в обеспечении безопасности языковых моделей', 'desc': 'Статья представляет SAFFRON - новую парадигму масштабирования вывода для обеспечения безопасности больших языковых моделей (LLM). Авторы вводят концепцию многовариантной модели вознаграждения (MRM), которая значительно сокращает количество необходимых оценок модели вознаграждения. Предлагаемый подход включает частично контролируемую цель обучения для MRM, ограничение консервативного исследования и стратегию кэширования на основе префиксного дерева. Эксперименты подтверждают эффективность метода в повышении безопасности LLM против возникающих угроз.'}, 'en': {'title': 'SAFFRON: Enhancing LLM Safety with Multifurcation Reward Models', 'desc': 'SAFFRON presents a new approach to enhance safety in large language models (LLMs) by introducing multifurcation reward models that tackle the exploration-efficiency dilemma during inference scaling. Traditional safety methods have struggled against jailbreak attacks and have not effectively integrated with the advancements in LLM reasoning capabilities. The paper identifies that existing inference scaling techniques are inadequate for safety assurance, often performing worse than simpler methods. To address this, SAFFRON proposes a novel paradigm that reduces the number of necessary reward model evaluations, ensuring safer and more efficient LLM operations.'}, 'zh': {'title': 'SAFFRON：提升大型语言模型安全性的创新方案', 'desc': 'SAFFRON提出了一种多分叉奖励模型，以提高大型语言模型的安全性保障，解决推理扩展中的探索效率困境。现有的安全保障研究主要集中在训练阶段的对齐，以培养LLM的安全行为，但这些方法在面对各种攻击时表现出脆弱性。我们的研究首次探讨了推理扩展在安全保障中的应用，发现传统的推理扩展技术在安全上下文中表现不佳。为此，我们提出了一种新的推理扩展范式，结合多分叉奖励模型，显著减少了奖励模型评估的次数，从而提高了安全性。'}}}, {'id': 'https://huggingface.co/papers/2506.07977', 'title': 'OneIG-Bench: Omni-dimensional Nuanced Evaluation for Image Generation', 'url': 'https://huggingface.co/papers/2506.07977', 'abstract': 'OneIG-Bench is a benchmark framework that comprehensively evaluates text-to-image models across prompt-image alignment, text rendering, reasoning, stylization, and diversity.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-image (T2I) models have garnered significant attention for generating high-quality images aligned with text prompts. However, rapid T2I model advancements reveal limitations in early benchmarks, lacking comprehensive evaluations, for example, the evaluation on reasoning, text rendering and style. Notably, recent state-of-the-art models, with their rich knowledge modeling capabilities, show promising results on the image generation problems requiring strong reasoning ability, yet existing evaluation systems have not adequately addressed this frontier. To systematically address these gaps, we introduce OneIG-Bench, a meticulously designed comprehensive benchmark framework for fine-grained evaluation of T2I models across multiple dimensions, including prompt-image alignment, text rendering precision, reasoning-generated content, stylization, and diversity. By structuring the evaluation, this benchmark enables in-depth analysis of model performance, helping researchers and practitioners pinpoint strengths and bottlenecks in the full pipeline of image generation. Specifically, OneIG-Bench enables flexible evaluation by allowing users to focus on a particular evaluation subset. Instead of generating images for the entire set of prompts, users can generate images only for the prompts associated with the selected dimension and complete the corresponding evaluation accordingly. Our codebase and dataset are now publicly available to facilitate reproducible evaluation studies and cross-model comparisons within the T2I research community.', 'score': 34, 'issue_id': 4208, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': 'f87355dfc9390cc7', 'authors': ['Jingjing Chang', 'Yixiao Fang', 'Peng Xing', 'Shuhan Wu', 'Wei Cheng', 'Rui Wang', 'Xianfang Zeng', 'Gang Yu', 'Hai-Bao Chen'], 'affiliations': ['SJTU', 'StepFun'], 'pdf_title_img': 'assets/pdf/title_img/2506.07977.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#cv', '#open_source', '#survey'], 'emoji': '🖼️', 'ru': {'title': 'Всесторонняя оценка моделей текст-изображение', 'desc': 'OneIG-Bench - это комплексная система оценки моделей преобразования текста в изображение. Она оценивает соответствие изображений текстовым подсказкам, точность отображения текста, способность к рассуждению, стилизацию и разнообразие. Этот бенчмарк позволяет проводить детальный анализ производительности моделей, выявляя их сильные и слабые стороны. OneIG-Bench предоставляет гибкую оценку, позволяя пользователям сосредоточиться на конкретных аспектах генерации изображений.'}, 'en': {'title': 'OneIG-Bench: A Comprehensive Evaluation for Text-to-Image Models', 'desc': 'OneIG-Bench is a new framework designed to evaluate text-to-image (T2I) models in a detailed way. It focuses on several important aspects like how well the images match the text prompts, the quality of text rendering, reasoning capabilities, artistic style, and diversity of generated images. This benchmark addresses the shortcomings of previous evaluation methods by providing a structured approach that allows researchers to analyze specific areas of model performance. By making the code and dataset publicly available, OneIG-Bench promotes reproducibility and encourages comparisons among different T2I models.'}, 'zh': {'title': '全面评估文本到图像模型的基准框架', 'desc': 'OneIG-Bench是一个基准框架，旨在全面评估文本到图像（T2I）模型的性能。它涵盖了多个维度，包括提示与图像的对齐、文本渲染、推理能力、风格化和多样性。通过系统化的评估，OneIG-Bench帮助研究人员深入分析模型的优缺点，识别图像生成过程中的瓶颈。该框架的代码库和数据集已公开，便于在T2I研究社区中进行可重复的评估研究和跨模型比较。'}}}, {'id': 'https://huggingface.co/papers/2506.07491', 'title': 'SpatialLM: Training Large Language Models for Structured Indoor Modeling', 'url': 'https://huggingface.co/papers/2506.07491', 'abstract': 'SpatialLM is a large language model capable of processing 3D point cloud data to produce structured outputs for spatial understanding, outperforming previous methods on layout estimation and object detection tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t SpatialLM is a large language model designed to process 3D point cloud data and generate structured 3D scene understanding outputs. These outputs include architectural elements like walls, doors, windows, and oriented object boxes with their semantic categories. Unlike previous methods which exploit task-specific network designs, our model adheres to the standard multimodal LLM architecture and is fine-tuned directly from open-source LLMs.   To train SpatialLM, we collect a large-scale, high-quality synthetic dataset consisting of the point clouds of 12,328 indoor scenes (54,778 rooms) with ground-truth 3D annotations, and conduct a careful study on various modeling and training decisions. On public benchmarks, our model gives state-of-the-art performance in layout estimation and competitive results in 3D object detection. With that, we show a feasible path for enhancing the spatial understanding capabilities of modern LLMs for applications in augmented reality, embodied robotics, and more.', 'score': 21, 'issue_id': 4208, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': '604b98825a94f8d1', 'authors': ['Yongsen Mao', 'Junhao Zhong', 'Chuan Fang', 'Jia Zheng', 'Rui Tang', 'Hao Zhu', 'Ping Tan', 'Zihan Zhou'], 'affiliations': ['Hong Kong University of Science and Technology', 'Manycore Tech Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2506.07491.jpg', 'data': {'categories': ['#dataset', '#training', '#benchmark', '#3d', '#open_source', '#synthetic', '#multimodal'], 'emoji': '🏠', 'ru': {'title': 'Пространственное понимание 3D-сцен с помощью языковых моделей', 'desc': 'SpatialLM - это большая языковая модель, способная обрабатывать трехмерные облака точек для пространственного понимания сцен. Модель генерирует структурированные выходные данные, включая архитектурные элементы и семантические категории объектов. В отличие от предыдущих методов, SpatialLM использует стандартную архитектуру мультимодальной LLM и дообучается напрямую из открытых LLM. Модель демонстрирует передовые результаты в оценке планировки помещений и конкурентоспособные результаты в трехмерном обнаружении объектов.'}, 'en': {'title': 'Revolutionizing 3D Spatial Understanding with SpatialLM', 'desc': 'SpatialLM is a large language model that specializes in understanding 3D point cloud data, enabling it to produce structured outputs for spatial comprehension. It identifies and categorizes architectural features such as walls, doors, and windows, as well as oriented object boxes. Unlike earlier approaches that relied on specific network designs for tasks, SpatialLM uses a standard multimodal architecture and is fine-tuned from existing open-source models. The model is trained on a comprehensive dataset of indoor scenes, achieving state-of-the-art results in layout estimation and strong performance in 3D object detection, paving the way for advancements in augmented reality and robotics.'}, 'zh': {'title': 'SpatialLM：提升空间理解的语言模型', 'desc': 'SpatialLM是一种大型语言模型，能够处理3D点云数据，并生成结构化的空间理解输出。这些输出包括建筑元素，如墙壁、门、窗户以及带有语义类别的定向物体框。与之前的方法不同，SpatialLM遵循标准的多模态LLM架构，并直接从开源LLM进行微调。通过收集包含12,328个室内场景的高质量合成数据集，我们的模型在布局估计和3D物体检测任务上表现出色。'}}}, {'id': 'https://huggingface.co/papers/2506.07553', 'title': 'GTR-CoT: Graph Traversal as Visual Chain of Thought for Molecular\n  Structure Recognition', 'url': 'https://huggingface.co/papers/2506.07553', 'abstract': "GTR-Mol-VLM, a vision-language model with graph traversal and data-centric principles, achieves superior accuracy in converting molecular images to machine-readable formats, particularly in handling complex structures and functional group abbreviations.  \t\t\t\t\tAI-generated summary \t\t\t\t Optical Chemical Structure Recognition (OCSR) is crucial for digitizing chemical knowledge by converting molecular images into machine-readable formats. While recent vision-language models (VLMs) have shown potential in this task, their image-captioning approach often struggles with complex molecular structures and inconsistent annotations. To overcome these challenges, we introduce GTR-Mol-VLM, a novel framework featuring two key innovations: (1) the Graph Traversal as Visual Chain of Thought mechanism that emulates human reasoning by incrementally parsing molecular graphs through sequential atom-bond predictions, and (2) the data-centric principle of Faithfully Recognize What You've Seen, which addresses the mismatch between abbreviated structures in images and their expanded annotations. To support model development, we constructed GTR-CoT-1.3M, a large-scale instruction-tuning dataset with meticulously corrected annotations, and introduced MolRec-Bench, the first benchmark designed for a fine-grained evaluation of graph-parsing accuracy in OCSR. Comprehensive experiments demonstrate that GTR-Mol-VLM achieves superior results compared to specialist models, chemistry-domain VLMs, and commercial general-purpose VLMs. Notably, in scenarios involving molecular images with functional group abbreviations, GTR-Mol-VLM outperforms the second-best baseline by approximately 14 percentage points, both in SMILES-based and graph-based metrics. We hope that this work will drive OCSR technology to more effectively meet real-world needs, thereby advancing the fields of cheminformatics and AI for Science. We will release GTR-CoT at https://github.com/opendatalab/GTR-CoT.", 'score': 11, 'issue_id': 4208, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': '379cf5e0efdacd27', 'authors': ['Jingchao Wang', 'Haote Yang', 'Jiang Wu', 'Yifan He', 'Xingjian Wei', 'Yinfan Wang', 'Chengjin Liu', 'Lingli Ge', 'Lijun Wu', 'Bin Wang', 'Dahua Lin', 'Conghui He'], 'affiliations': ['Chinese University of Hong Kong', 'East China Normal University', 'Fudan University', 'Northwestern Polytechnical University', 'Peking University', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai University'], 'pdf_title_img': 'assets/pdf/title_img/2506.07553.jpg', 'data': {'categories': ['#dataset', '#graphs', '#benchmark', '#reasoning', '#cv', '#science', '#games'], 'emoji': '🧪', 'ru': {'title': 'Точное распознавание химических структур с помощью обхода графа и машинного обучения', 'desc': 'GTR-Mol-VLM - это новая модель компьютерного зрения и обработки естественного языка для распознавания химических структур на изображениях. Она использует механизм обхода графа и принцип точного распознавания увиденного для повышения точности. Модель превосходит существующие решения, особенно при работе со сложными структурами и сокращениями функциональных групп. Для обучения и оценки модели были созданы большой набор данных GTR-CoT-1.3M и бенчмарк MolRec-Bench.'}, 'en': {'title': 'Transforming Molecular Images into Accurate Data with GTR-Mol-VLM', 'desc': 'GTR-Mol-VLM is a new vision-language model designed to convert molecular images into machine-readable formats with high accuracy. It uses a unique Graph Traversal mechanism that mimics human reasoning by analyzing molecular graphs step-by-step. Additionally, it employs a data-centric approach to ensure that the model accurately recognizes abbreviated structures in images. The model has been tested against various benchmarks and has shown significant improvements, especially in handling complex molecular structures and functional group abbreviations.'}, 'zh': {'title': 'GTR-Mol-VLM：提升分子图像识别的智能新方法', 'desc': 'GTR-Mol-VLM是一种结合图遍历和数据中心原则的视觉语言模型，能够将分子图像准确转换为机器可读格式。该模型通过图遍历机制模拟人类推理，逐步解析分子图，解决了复杂结构和功能基团缩写的问题。为了支持模型开发，我们构建了一个大规模的指令调优数据集GTR-CoT-1.3M，并推出了MolRec-Bench基准测试，以评估图解析的准确性。实验结果表明，GTR-Mol-VLM在处理分子图像时的表现优于其他专业模型和通用模型，特别是在功能基团缩写的场景中，准确率提高了约14个百分点。'}}}, {'id': 'https://huggingface.co/papers/2506.07986', 'title': 'Rethinking Cross-Modal Interaction in Multimodal Diffusion Transformers', 'url': 'https://huggingface.co/papers/2506.07986', 'abstract': 'Temperature-Adjusted Cross-modal Attention (TACA) improves text-image alignment in diffusion-based multimodal transformers by dynamically balancing multimodal interactions and provides a parameter-efficient solution.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Diffusion Transformers (MM-DiTs) have achieved remarkable progress in text-driven visual generation. However, even state-of-the-art MM-DiT models like FLUX struggle with achieving precise alignment between text prompts and generated content. We identify two key issues in the attention mechanism of MM-DiT, namely 1) the suppression of cross-modal attention due to token imbalance between visual and textual modalities and 2) the lack of timestep-aware attention weighting, which hinder the alignment. To address these issues, we propose Temperature-Adjusted Cross-modal Attention (TACA), a parameter-efficient method that dynamically rebalances multimodal interactions through temperature scaling and timestep-dependent adjustment. When combined with LoRA fine-tuning, TACA significantly enhances text-image alignment on the T2I-CompBench benchmark with minimal computational overhead. We tested TACA on state-of-the-art models like FLUX and SD3.5, demonstrating its ability to improve image-text alignment in terms of object appearance, attribute binding, and spatial relationships. Our findings highlight the importance of balancing cross-modal attention in improving semantic fidelity in text-to-image diffusion models. Our codes are publicly available at https://github.com/Vchitect/TACA', 'score': 10, 'issue_id': 4209, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': '54c2bf2395cf58cc', 'authors': ['Zhengyao Lv', 'Tianlin Pan', 'Chenyang Si', 'Zhaoxi Chen', 'Wangmeng Zuo', 'Ziwei Liu', 'Kwan-Yee K. Wong'], 'affiliations': ['Harbin Institute of Technology', 'Nanjing University', 'Nanyang Technological University', 'The University of Hong Kong', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2506.07986.jpg', 'data': {'categories': ['#multimodal', '#open_source', '#optimization', '#training', '#diffusion', '#benchmark'], 'emoji': '🔀', 'ru': {'title': 'Балансировка внимания для точной генерации изображений по тексту', 'desc': 'Статья представляет метод Temperature-Adjusted Cross-modal Attention (TACA) для улучшения согласованности текста и изображений в мультимодальных диффузионных трансформерах. TACA динамически балансирует взаимодействие между модальностями с помощью температурного масштабирования и настройки, зависящей от временного шага. Метод эффективно решает проблемы подавления кросс-модального внимания и отсутствия учета временных шагов в механизме внимания. TACA значительно улучшает соответствие текста и изображений при минимальных вычислительных затратах.'}, 'en': {'title': 'Enhancing Text-Image Alignment with TACA', 'desc': 'The paper introduces Temperature-Adjusted Cross-modal Attention (TACA), a novel approach to enhance text-image alignment in multimodal diffusion transformers. TACA addresses two main challenges: the imbalance in token representation between text and images, and the absence of attention weighting that considers the timing of inputs. By implementing temperature scaling and timestep-dependent adjustments, TACA efficiently rebalances interactions between modalities. The results show that TACA, when used with LoRA fine-tuning, significantly improves alignment in state-of-the-art models, leading to better semantic fidelity in generated images.'}, 'zh': {'title': '温度调整跨模态注意力：提升文本与图像对齐的利器', 'desc': '本文提出了一种名为温度调整跨模态注意力（TACA）的方法，旨在改善基于扩散的多模态变换器中的文本与图像对齐。我们发现现有模型在文本提示与生成内容之间存在注意力机制的不足，主要体现在视觉和文本模态之间的令牌不平衡以及缺乏时间步感知的注意力加权。TACA通过温度缩放和时间步依赖的调整，动态平衡多模态交互，从而提高文本与图像的对齐效果。实验结果表明，TACA在FLUX和SD3.5等先进模型上显著提升了图像-文本对齐的准确性，且计算开销极小。'}}}, {'id': 'https://huggingface.co/papers/2506.07712', 'title': 'Through the Valley: Path to Effective Long CoT Training for Small\n  Language Models', 'url': 'https://huggingface.co/papers/2506.07712', 'abstract': 'Small language models experience significant performance declines when trained on long chain-of-thought data due to error accumulation, impacting downstream reinforcement learning but potentially mitigated by extensive supervised fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Long chain-of-thought (CoT) supervision has become a common strategy to enhance reasoning in language models. While effective for large models, we identify a phenomenon we call Long CoT Degradation, in which small language models (SLMs; <=3B parameters) trained on limited long CoT data experience significant performance deterioration. Through extensive experiments on the Qwen2.5, LLaMA3 and Gemma3 families, we demonstrate that this degradation is widespread across SLMs. In some settings, models trained on only 8k long CoT examples lose up to 75% of their original performance before fine-tuning. Strikingly, we further observe that for some particularly small models, even training on 220k long CoT examples fails to recover or surpass their original performance prior to fine-tuning. Our analysis attributes this effect to error accumulation: while longer responses increase the capacity for multi-step reasoning, they also amplify the risk of compounding mistakes. Furthermore, we find that Long CoT Degradation may negatively impacts downstream reinforcement learning (RL), although this can be alleviated by sufficiently scaled supervised fine-tuning (SFT). Our findings challenge common assumptions about the benefits of long CoT training for SLMs and offer practical guidance for building more effective small-scale reasoning models.', 'score': 10, 'issue_id': 4208, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': '856fa5edc302286a', 'authors': ['Renjie Luo', 'Jiaxi Li', 'Chen Huang', 'Wei Lu'], 'affiliations': ['StatNLP Research Group, Singapore University of Technology and Design'], 'pdf_title_img': 'assets/pdf/title_img/2506.07712.jpg', 'data': {'categories': ['#small_models', '#long_context', '#training', '#reasoning', '#rl'], 'emoji': '📉', 'ru': {'title': 'Малые модели страдают от длинных рассуждений', 'desc': 'Статья исследует феномен деградации производительности малых языковых моделей при обучении на длинных цепочках рассуждений (CoT). Авторы обнаружили, что модели с менее чем 3 миллиардами параметров могут терять до 75% своей изначальной эффективности после такого обучения. Это явление объясняется накоплением ошибок в длинных последовательностях рассуждений. Исследование также показывает, что деградация CoT может негативно влиять на последующее обучение с подкреплением, хотя этот эффект можно смягчить масштабным дообучением под присмотром.'}, 'en': {'title': 'Mitigating Long CoT Degradation in Small Language Models', 'desc': 'This paper discusses a problem called Long CoT Degradation, which affects small language models (SLMs) when they are trained on long chain-of-thought (CoT) data. The authors found that these models can lose a significant amount of performance due to error accumulation, especially when trained on limited long CoT examples. Their experiments show that even with a large number of training examples, some small models still struggle to recover their original performance. The study suggests that while long CoT training can enhance reasoning, it may not be beneficial for smaller models without adequate supervised fine-tuning to mitigate the negative effects.'}, 'zh': {'title': '小型模型的长链思维退化问题', 'desc': '小型语言模型在长链思维数据上训练时，因错误累积而导致性能显著下降，这种现象被称为长链思维退化。尽管长链思维监督对大型模型有效，但小型模型在有限的长链思维数据上训练时，性能可能下降高达75%。我们的研究表明，错误累积是导致这种退化的主要原因，长响应虽然增加了多步推理的能力，但也加大了错误叠加的风险。此外，长链思维退化还可能对后续的强化学习产生负面影响，但通过充分规模的监督微调可以缓解这一问题。'}}}, {'id': 'https://huggingface.co/papers/2506.07530', 'title': 'BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation', 'url': 'https://huggingface.co/papers/2506.07530', 'abstract': "BitVLA, a 1-bit VLA model with ternary parameters, achieves comparable performance to OpenVLA-OFT on LIBERO while using 29.8% less memory through distillation-aware training.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language-Action (VLA) models have shown impressive capabilities across a wide range of robotics manipulation tasks. However, their growing model size poses significant challenges for deployment on resource-constrained robotic systems. While 1-bit pretraining has proven effective for enhancing the inference efficiency of large language models with minimal performance loss, its application to VLA models remains underexplored. In this work, we present BitVLA, the first 1-bit VLA model for robotics manipulation, in which every parameter is ternary, i.e., {-1, 0, 1}. To further reduce the memory footprint of the vision encoder, we propose the distillation-aware training strategy that compresses the full-precision encoder to 1.58-bit weights. During this process, a full-precision encoder serves as a teacher model to better align latent representations. Despite the lack of large-scale robotics pretraining, BitVLA achieves performance comparable to the state-of-the-art model OpenVLA-OFT with 4-bit post-training quantization on the LIBERO benchmark, while consuming only 29.8% of the memory. These results highlight BitVLA's promise for deployment on memory-constrained edge devices. We release the code and model weights in https://github.com/ustcwhy/BitVLA.", 'score': 9, 'issue_id': 4208, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': '2ff752565d34986b', 'authors': ['Hongyu Wang', 'Chuyan Xiong', 'Ruiping Wang', 'Xilin Chen'], 'affiliations': ['Key Laboratory of AI Safety, Institute of Computing Technology, Chinese Academy of Sciences', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2506.07530.jpg', 'data': {'categories': ['#optimization', '#small_models', '#robotics', '#inference', '#open_source'], 'emoji': '🤖', 'ru': {'title': 'Эффективная 1-битная модель VLA для роботов с ограниченными ресурсами', 'desc': 'BitVLA - это первая 1-битная модель VLA для роботизированных манипуляций, где каждый параметр является тернарным (-1, 0, 1). Модель использует стратегию обучения с дистилляцией для сжатия полноточного энкодера до 1,58-битных весов. BitVLA достигает производительности, сопоставимой с современной моделью OpenVLA-OFT на бенчмарке LIBERO, потребляя всего 29,8% памяти. Эти результаты демонстрируют перспективность BitVLA для развертывания на устройствах с ограниченной памятью.'}, 'en': {'title': 'Efficient Robotics with BitVLA: Less Memory, Same Performance!', 'desc': 'BitVLA is a novel 1-bit Vision-Language-Action (VLA) model designed for robotics manipulation tasks, utilizing ternary parameters to optimize performance. It employs a distillation-aware training strategy that compresses a full-precision vision encoder into 1.58-bit weights, significantly reducing memory usage. Despite not being pretrained on large-scale robotics data, BitVLA achieves performance on par with the state-of-the-art OpenVLA-OFT model while using 29.8% less memory. This advancement makes BitVLA particularly suitable for deployment in resource-limited robotic systems.'}, 'zh': {'title': 'BitVLA：高效的1位视觉-语言-动作模型', 'desc': 'BitVLA是一种1位视觉-语言-动作（VLA）模型，采用三元参数，旨在解决机器人操作任务中的内存限制问题。通过使用注意蒸馏训练策略，BitVLA在保持性能的同时，显著减少了内存占用，达到仅使用29.8%的内存。尽管缺乏大规模的机器人预训练，BitVLA在LIBERO基准测试中表现出与最先进的OpenVLA-OFT模型相当的性能。该研究展示了BitVLA在资源受限的边缘设备上的应用潜力。'}}}, {'id': 'https://huggingface.co/papers/2506.07298', 'title': 'Pre-trained Large Language Models Learn Hidden Markov Models In-context', 'url': 'https://huggingface.co/papers/2506.07298', 'abstract': 'LLMs using in-context learning can accurately predict sequences generated by HMMs, showcasing its potential for uncovering hidden structures in complex data.  \t\t\t\t\tAI-generated summary \t\t\t\t Hidden Markov Models (HMMs) are foundational tools for modeling sequential data with latent Markovian structure, yet fitting them to real-world data remains computationally challenging. In this work, we show that pre-trained large language models (LLMs) can effectively model data generated by HMMs via in-context learning (ICL)x2013their ability to infer patterns from examples within a prompt. On a diverse set of synthetic HMMs, LLMs achieve predictive accuracy approaching the theoretical optimum. We uncover novel scaling trends influenced by HMM properties, and offer theoretical conjectures for these empirical observations. We also provide practical guidelines for scientists on using ICL as a diagnostic tool for complex data. On real-world animal decision-making tasks, ICL achieves competitive performance with models designed by human experts. To our knowledge, this is the first demonstration that ICL can learn and predict HMM-generated sequencesx2013an advance that deepens our understanding of in-context learning in LLMs and establishes its potential as a powerful tool for uncovering hidden structure in complex scientific data.', 'score': 8, 'issue_id': 4208, 'pub_date': '2025-06-08', 'pub_date_card': {'ru': '8 июня', 'en': 'June 8', 'zh': '6月8日'}, 'hash': '568113a6bc87dd15', 'authors': ['Yijia Dai', 'Zhaolin Gao', 'Yahya Satter', 'Sarah Dean', 'Jennifer J. Sun'], 'affiliations': ['Cornell University'], 'pdf_title_img': 'assets/pdf/title_img/2506.07298.jpg', 'data': {'categories': ['#rl', '#transfer_learning', '#data', '#science', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'LLM раскрывают скрытые структуры данных через обучение в контексте', 'desc': 'Исследование демонстрирует способность больших языковых моделей (LLM) эффективно моделировать данные, генерируемые скрытыми марковскими моделями (HMM), используя обучение в контексте (ICL). LLM достигают предсказательной точности, близкой к теоретическому оптимуму, на разнообразном наборе синтетических HMM. Авторы выявляют новые тенденции масштабирования, зависящие от свойств HMM, и предлагают теоретические гипотезы для объяснения эмпирических наблюдений. Результаты исследования углубляют понимание ICL в LLM и демонстрируют его потенциал как мощного инструмента для выявления скрытых структур в сложных научных данных.'}, 'en': {'title': 'Unlocking Hidden Structures with In-Context Learning in LLMs', 'desc': 'This paper demonstrates that large language models (LLMs) can effectively use in-context learning (ICL) to predict sequences generated by Hidden Markov Models (HMMs). The authors show that LLMs can achieve high predictive accuracy on synthetic HMM data, approaching theoretical limits. They also identify scaling trends related to HMM characteristics and provide insights into using ICL as a diagnostic tool for complex datasets. Additionally, the study highlights that ICL performs competitively on real-world tasks compared to expert-designed models, showcasing its potential in revealing hidden structures in data.'}, 'zh': {'title': '利用上下文学习揭示复杂数据的潜力', 'desc': '本研究展示了预训练的大型语言模型（LLMs）能够通过上下文学习（ICL）有效地预测由隐马尔可夫模型（HMMs）生成的序列。这表明LLMs在揭示复杂数据中的隐藏结构方面具有潜力。我们在多种合成HMMs上测试了LLMs，发现其预测准确性接近理论最优值。此外，我们还提供了使用ICL作为复杂数据诊断工具的实用指南，并在真实的动物决策任务中取得了与人类专家设计模型相当的表现。'}}}, {'id': 'https://huggingface.co/papers/2506.07463', 'title': 'CCI4.0: A Bilingual Pretraining Dataset for Enhancing Reasoning in Large\n  Language Models', 'url': 'https://huggingface.co/papers/2506.07463', 'abstract': 'A large-scale bilingual pre-training dataset, CCI4.0, enhances data quality and diverse reasoning patterns for language models, leading to improved performance in downstream tasks like math and code reflection.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce CCI4.0, a large-scale bilingual pre-training dataset engineered for superior data quality and diverse human-like reasoning trajectory. CCI4.0 occupies roughly 35 TB of disk space and comprises two sub-datasets: CCI4.0-M2-Base and CCI4.0-M2-CoT. CCI4.0-M2-Base combines a 5.2 TB carefully curated Chinese web corpus, a 22.5 TB English subset from Nemotron-CC, and diverse sources from math, wiki, arxiv, and code. Although these data are mostly sourced from well-processed datasets, the quality standards of various domains are dynamic and require extensive expert experience and labor to process. So, we propose a novel pipeline justifying data quality mainly based on models through two-stage deduplication, multiclassifier quality scoring, and domain-aware fluency filtering. We extract 4.5 billion pieces of CoT(Chain-of-Thought) templates, named CCI4.0-M2-CoT. Differing from the distillation of CoT from larger models, our proposed staged CoT extraction exemplifies diverse reasoning patterns and significantly decreases the possibility of hallucination. Empirical evaluations demonstrate that LLMs pre-trained in CCI4.0 benefit from cleaner, more reliable training signals, yielding consistent improvements in downstream tasks, especially in math and code reflection tasks. Our results underscore the critical role of rigorous data curation and human thinking templates in advancing LLM performance, shedding some light on automatically processing pretraining corpora.', 'score': 7, 'issue_id': 4209, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': '219a3c7d2fbb1e22', 'authors': ['Guang Liu', 'Liangdong Wang', 'Jijie Li', 'Yang Yu', 'Yao Xu', 'Jiabei Chen', 'Yu Bai', 'Feng Liao', 'Yonghua Lin'], 'affiliations': ['baai.ac.cn'], 'pdf_title_img': 'assets/pdf/title_img/2506.07463.jpg', 'data': {'categories': ['#hallucinations', '#data', '#dataset', '#reasoning', '#transfer_learning'], 'emoji': '🧠', 'ru': {'title': 'CCI4.0: Новый уровень качества данных для обучения языковых моделей', 'desc': 'CCI4.0 - это крупномасштабный двуязычный набор данных для предварительного обучения языковых моделей. Он включает в себя высококачественные данные и разнообразные шаблоны рассуждений, что улучшает производительность в задачах вроде математики и рефлексии кода. Набор данных состоит из двух поднаборов: CCI4.0-M2-Base и CCI4.0-M2-CoT, общим объемом около 35 ТБ. Авторы предлагают новый конвейер обработки данных, включающий двухэтапное удаление дубликатов, оценку качества с помощью мультиклассификатора и фильтрацию по доменам.'}, 'en': {'title': 'Enhancing Language Models with CCI4.0: Quality Data for Better Reasoning', 'desc': 'The paper presents CCI4.0, a large-scale bilingual pre-training dataset designed to improve the quality of data and enhance reasoning capabilities in language models. It consists of two main sub-datasets, CCI4.0-M2-Base and CCI4.0-M2-CoT, which together provide a diverse range of high-quality data from various domains. The authors introduce a novel data curation pipeline that includes deduplication, quality scoring, and fluency filtering to ensure the reliability of the training data. Empirical results show that language models trained on CCI4.0 exhibit significant performance improvements in tasks such as mathematics and code reflection, highlighting the importance of quality data in machine learning.'}, 'zh': {'title': '提升语言模型性能的双语数据集CCI4.0', 'desc': '本文介绍了一个名为CCI4.0的大规模双语预训练数据集，旨在提高数据质量和多样化的推理模式。CCI4.0包含约35 TB的数据，分为两个子数据集：CCI4.0-M2-Base和CCI4.0-M2-CoT。通过两阶段去重、多分类器质量评分和领域感知流畅性过滤等新颖流程，确保了数据的高质量。实验证明，使用CCI4.0预训练的语言模型在数学和代码反射等下游任务中表现出显著的性能提升。'}}}, {'id': 'https://huggingface.co/papers/2506.07434', 'title': 'Well Begun is Half Done: Low-resource Preference Alignment by\n  Weak-to-Strong Decoding', 'url': 'https://huggingface.co/papers/2506.07434', 'abstract': 'A novel Weak-to-Strong Decoding (WSD) framework enhances the alignment of large language models using a small aligned model to draft responses initially, improving alignment and performance without degrading downstream task performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) require alignment with human preferences to avoid generating offensive, false, or meaningless content. Recently, low-resource methods for LLM alignment have been popular, while still facing challenges in obtaining both high-quality and aligned content. Motivated by the observation that the difficulty of generating aligned responses is concentrated at the beginning of decoding, we propose a novel framework, Weak-to-Strong Decoding (WSD), to enhance the alignment ability of base models by the guidance of a small aligned model. The small model first drafts well-aligned beginnings, followed by the large base model to continue the rest, controlled by a well-designed auto-switch mechanism. We also collect a new dataset, GenerAlign, to fine-tune a small-sized Pilot-3B as the draft model, which effectively enhances different base models under the WSD framework to outperform all baseline methods, while avoiding degradation on downstream tasks, termed as the alignment tax. Extensive experiments are further conducted to examine the impact of different settings and time efficiency, as well as analyses on the intrinsic mechanisms of WSD in depth.', 'score': 7, 'issue_id': 4208, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': '11d72d8081b838d5', 'authors': ['Feifan Song', 'Shaohang Wei', 'Wen Luo', 'Yuxuan Fan', 'Tianyu Liu', 'Guoyin Wang', 'Houfeng Wang'], 'affiliations': ['State Key Laboratory of Multimedia Information Processing, School of Computer Science Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2506.07434.jpg', 'data': {'categories': ['#alignment', '#dataset', '#training', '#rlhf'], 'emoji': '🔀', 'ru': {'title': 'Слабый старт, сильный финиш: новый метод выравнивания языковых моделей', 'desc': 'Статья представляет новый подход к улучшению выравнивания больших языковых моделей (LLM) с предпочтениями человека. Метод Weak-to-Strong Decoding (WSD) использует маленькую выровненную модель для создания начала ответа, после чего большая базовая модель продолжает генерацию. Авторы собрали датасет GenerAlign для обучения небольшой модели Pilot-3B, которая эффективно улучшает работу различных базовых моделей в рамках WSD. Эксперименты показывают, что этот подход превосходит базовые методы без ухудшения производительности на других задачах.'}, 'en': {'title': 'Enhancing LLM Alignment with Weak-to-Strong Decoding', 'desc': 'The paper introduces a new framework called Weak-to-Strong Decoding (WSD) that improves the alignment of large language models (LLMs) with human preferences. It uses a smaller, aligned model to generate the initial part of responses, which helps guide the larger model in producing better outputs. This approach addresses the challenge of generating high-quality and aligned content without negatively impacting the performance on other tasks, known as the alignment tax. The authors also present a new dataset, GenerAlign, which is used to fine-tune the smaller model, leading to superior results compared to existing methods.'}, 'zh': {'title': '弱到强解码：提升语言模型对齐能力的创新框架', 'desc': '本文提出了一种新的弱到强解码框架（WSD），旨在通过小型对齐模型的指导来增强大型语言模型的对齐能力。该框架首先由小模型草拟出高质量的对齐开头，然后由大型基础模型继续生成后续内容。通过设计良好的自动切换机制，WSD能够在不降低下游任务性能的情况下，显著提高模型的对齐效果。我们还收集了一个新的数据集GenerAlign，以微调小型Pilot-3B模型，实验结果表明该方法在多种基线方法中表现优越。'}}}, {'id': 'https://huggingface.co/papers/2506.06941', 'title': 'The Illusion of Thinking: Understanding the Strengths and Limitations of\n  Reasoning Models via the Lens of Problem Complexity', 'url': 'https://huggingface.co/papers/2506.06941', 'abstract': "Large Reasoning Models (LRMs) undergo accuracy collapse at higher complexities and exhibit unique performance scaling behaviors compared to standard LLMs, with failures in exact computation and inconsistent reasoning across scales.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent generations of language models have introduced Large Reasoning Models (LRMs) that generate detailed thinking processes before providing answers. While these models demonstrate improved performance on reasoning benchmarks, their fundamental capabilities, scaling properties, and limitations remain insufficiently understood. Current evaluations primarily focus on established math and coding benchmarks, emphasizing final answer accuracy. However, this evaluation paradigm often suffers from contamination and does not provide insights into the reasoning traces. In this work, we systematically investigate these gaps with the help of controllable puzzle environments that allow precise manipulation of complexity while maintaining consistent logical structures. This setup enables the analysis of not only final answers but also the internal reasoning traces, offering insights into how LRMs think. Through extensive experiments, we show that LRMs face a complete accuracy collapse beyond certain complexities. Moreover, they exhibit a counterintuitive scaling limit: their reasoning effort increases with problem complexity up to a point, then declines despite having remaining token budget. By comparing LRMs with their standard LLM counterparts under same inference compute, we identify three performance regimes: (1) low-complexity tasks where standard models outperform LRMs, (2) medium-complexity tasks where LRMs demonstrates advantage, and (3) high-complexity tasks where both models face complete collapse. We found that LRMs have limitations in exact computation: they fail to use explicit algorithms and reason inconsistently across scales. We also investigate the reasoning traces in more depth, studying the patterns of explored solutions and analyzing the models' computational behavior, shedding light on their strengths, limitations, and raising questions about their reasoning capabilities.", 'score': 7, 'issue_id': 4208, 'pub_date': '2025-06-07', 'pub_date_card': {'ru': '7 июня', 'en': 'June 7', 'zh': '6月7日'}, 'hash': '6f3991ea3357f456', 'authors': ['Parshin Shojaee', 'Iman Mirzadeh', 'Keivan Alizadeh', 'Maxwell Horton', 'Samy Bengio', 'Mehrdad Farajtabar'], 'affiliations': ['Apple'], 'pdf_title_img': 'assets/pdf/title_img/2506.06941.jpg', 'data': {'categories': ['#math', '#training', '#benchmark', '#reasoning', '#interpretability'], 'emoji': '🧠', 'ru': {'title': 'Ограничения крупных моделей рассуждений: анализ производительности и масштабируемости', 'desc': 'Исследование показывает, что крупные модели рассуждений (LRM) демонстрируют резкое падение точности при повышении сложности задач. В отличие от стандартных языковых моделей, LRM имеют уникальные характеристики масштабирования производительности. Модели испытывают трудности с точными вычислениями и проявляют непоследовательность в рассуждениях при изменении масштаба. Анализ внутренних процессов рассуждений LRM выявляет их сильные и слабые стороны, поднимая вопросы о реальных возможностях этих моделей.'}, 'en': {'title': 'Understanding the Limits of Large Reasoning Models in Complex Tasks', 'desc': 'This paper explores the performance of Large Reasoning Models (LRMs) in relation to their complexity and reasoning capabilities. It reveals that LRMs experience accuracy collapse when faced with high-complexity tasks, which is a significant limitation compared to standard language models (LLMs). The study introduces a controlled environment to analyze both the final answers and the internal reasoning processes of LRMs, highlighting their inconsistent reasoning and failure to perform exact computations. The findings categorize performance into three regimes based on task complexity, providing insights into the strengths and weaknesses of LRMs in reasoning tasks.'}, 'zh': {'title': '大型推理模型的复杂性挑战', 'desc': '大型推理模型（LRMs）在处理更复杂的问题时会出现准确性崩溃，并且与标准语言模型（LLMs）相比，它们的性能扩展行为独特。尽管LRMs在推理基准测试中表现出色，但它们的基本能力和局限性仍然不够清楚。通过可控的难题环境，我们能够精确操控复杂性，并分析LRMs的内部推理过程。研究表明，LRMs在高复杂性任务中会完全崩溃，并且在问题复杂性增加时，它们的推理努力会先增加后减少，显示出反直觉的扩展限制。'}}}, {'id': 'https://huggingface.co/papers/2506.08012', 'title': 'GUI-Reflection: Empowering Multimodal GUI Models with Self-Reflection\n  Behavior', 'url': 'https://huggingface.co/papers/2506.08012', 'abstract': 'A novel framework, GUI-Reflection, integrates self-reflection and error correction into multimodal GUI models through specialized training stages, enabling more robust and intelligent automation.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) have shown great potential in revolutionizing Graphical User Interface (GUI) automation. However, existing GUI models mostly rely on learning from nearly error-free offline trajectories, thus lacking reflection and error recovery capabilities. To bridge this gap, we propose GUI-Reflection, a novel framework that explicitly integrates self-reflection and error correction capabilities into end-to-end multimodal GUI models throughout dedicated training stages: GUI-specific pre-training, offline supervised fine-tuning (SFT), and online reflection tuning. GUI-reflection enables self-reflection behavior emergence with fully automated data generation and learning processes without requiring any human annotation. Specifically, 1) we first propose scalable data pipelines to automatically construct reflection and error correction data from existing successful trajectories. While existing GUI models mainly focus on grounding and UI understanding ability, we propose the GUI-Reflection Task Suite to learn and evaluate reflection-oriented abilities explicitly. 2) Furthermore, we built a diverse and efficient environment for online training and data collection of GUI models on mobile devices. 3) We also present an iterative online reflection tuning algorithm leveraging the proposed environment, enabling the model to continuously enhance its reflection and error correction abilities. Our framework equips GUI agents with self-reflection and correction capabilities, paving the way for more robust, adaptable, and intelligent GUI automation, with all data, models, environments, and tools to be released publicly.', 'score': 6, 'issue_id': 4208, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': '8052bb442adf3c53', 'authors': ['Penghao Wu', 'Shengnan Ma', 'Bo Wang', 'Jiaheng Yu', 'Lewei Lu', 'Ziwei Liu'], 'affiliations': ['S-Lab, Nanyang Technological University', 'SenseTime Research'], 'pdf_title_img': 'assets/pdf/title_img/2506.08012.jpg', 'data': {'categories': ['#optimization', '#dataset', '#agents', '#training', '#open_source', '#multimodal'], 'emoji': '🤖', 'ru': {'title': 'Самоанализ и исправление ошибок в ИИ для автоматизации графических интерфейсов', 'desc': 'Статья представляет новую структуру под названием GUI-Reflection, которая интегрирует самоанализ и исправление ошибок в мультимодальные модели графического интерфейса пользователя. Эта структура использует специализированные этапы обучения, включая предварительное обучение для GUI, офлайн-обучение с учителем и онлайн-настройку рефлексии. GUI-Reflection позволяет автоматически генерировать данные для обучения без необходимости аннотации человеком. Авторы также разработали набор задач GUI-Reflection Task Suite для оценки способностей к рефлексии и создали среду для онлайн-обучения на мобильных устройствах.'}, 'en': {'title': 'Empowering GUI Automation with Self-Reflection and Error Correction', 'desc': 'The paper introduces GUI-Reflection, a new framework that enhances multimodal GUI models by incorporating self-reflection and error correction during training. This approach addresses the limitations of existing models that primarily learn from error-free data, enabling them to recover from mistakes and improve over time. The framework includes specialized training stages such as pre-training, supervised fine-tuning, and online reflection tuning, allowing models to learn from their own errors without human intervention. By automating data generation and focusing on reflection-oriented tasks, GUI-Reflection aims to create more robust and intelligent automation for graphical user interfaces.'}, 'zh': {'title': '自我反思与错误纠正，提升GUI自动化智能', 'desc': '本文提出了一种新框架GUI-Reflection，将自我反思和错误纠正能力整合到多模态图形用户界面（GUI）模型中。该框架通过专门的训练阶段，使得模型能够在自动化过程中更具鲁棒性和智能性。我们设计了可扩展的数据管道，自动生成反思和错误纠正的数据，并提出了GUI-Reflection任务套件来评估这些能力。最终，框架使得GUI代理具备自我反思和纠正能力，为更智能的GUI自动化奠定基础。'}}}, {'id': 'https://huggingface.co/papers/2506.07309', 'title': 'ConfQA: Answer Only If You Are Confident', 'url': 'https://huggingface.co/papers/2506.07309', 'abstract': 'ConfQA fine-tuning strategy reduces factual statement hallucination in LLMs by 80%, using a dampening prompt and factual statements from knowledge graphs to improve confidence calibration and knowledge selection.  \t\t\t\t\tAI-generated summary \t\t\t\t Can we teach Large Language Models (LLMs) to refrain from hallucinating factual statements? In this paper we present a fine-tuning strategy that we call ConfQA, which can reduce hallucination rate from 20-40% to under 5% across multiple factuality benchmarks. The core idea is simple: when the LLM answers a question correctly, it is trained to continue with the answer; otherwise, it is trained to admit "I am unsure". But there are two key factors that make the training highly effective. First, we introduce a dampening prompt "answer only if you are confident" to explicitly guide the behavior, without which hallucination remains high as 15%-25%. Second, we leverage simple factual statements, specifically attribute values from knowledge graphs, to help LLMs calibrate the confidence, resulting in robust generalization across domains and question types. Building on this insight, we propose the Dual Neural Knowledge framework, which seamlessly select between internally parameterized neural knowledge and externally recorded symbolic knowledge based on ConfQA\'s confidence. The framework enables potential accuracy gains to beyond 95%, while reducing unnecessary external retrievals by over 30%.', 'score': 6, 'issue_id': 4211, 'pub_date': '2025-06-08', 'pub_date_card': {'ru': '8 июня', 'en': 'June 8', 'zh': '6月8日'}, 'hash': '8a4581c11bf35360', 'authors': ['Yin Huang', 'Yifan Ethan Xu', 'Kai Sun', 'Vera Yan', 'Alicia Sun', 'Haidar Khan', 'Jimmy Nguyen', 'Mohammad Kachuee', 'Zhaojiang Lin', 'Yue Liu', 'Aaron Colak', 'Anuj Kumar', 'Wen-tau Yih', 'Xin Luna Dong'], 'affiliations': ['FAIR at Meta', 'Meta Reality Labs'], 'pdf_title_img': 'assets/pdf/title_img/2506.07309.jpg', 'data': {'categories': ['#rlhf', '#interpretability', '#hallucinations', '#training'], 'emoji': '🧠', 'ru': {'title': 'ConfQA: обуздание галлюцинаций в LLM через уверенное молчание', 'desc': 'Статья представляет стратегию дообучения под названием ConfQA, которая значительно снижает уровень галлюцинаций в больших языковых моделях (LLM) при ответах на фактические вопросы. Ключевыми элементами стратегии являются использование специального промпта для подавления неуверенных ответов и применение фактических утверждений из графов знаний для улучшения калибровки уверенности модели. Результаты показывают снижение уровня галлюцинаций с 20-40% до менее 5% на различных тестах фактической точности. Авторы также предлагают фреймворк Dual Neural Knowledge для эффективного выбора между внутренними параметризованными знаниями модели и внешними символьными знаниями.'}, 'en': {'title': 'ConfQA: Reducing Hallucinations in LLMs with Confidence Calibration', 'desc': "This paper introduces a fine-tuning strategy called ConfQA that significantly reduces the occurrence of factual statement hallucinations in Large Language Models (LLMs) by up to 80%. The approach involves training the model to confidently provide answers when it knows them and to express uncertainty when it does not. Key components of this strategy include a dampening prompt that encourages the model to answer only when confident, and the use of factual statements from knowledge graphs to enhance the model's confidence calibration. Additionally, the proposed Dual Neural Knowledge framework allows the model to effectively choose between internal neural knowledge and external symbolic knowledge based on its confidence level, leading to improved accuracy and reduced reliance on external data retrieval."}, 'zh': {'title': '减少幻觉，提高准确性！', 'desc': '本文提出了一种名为ConfQA的微调策略，旨在减少大型语言模型（LLMs）中的事实陈述幻觉现象。通过使用减弱提示和知识图谱中的事实陈述，该策略能够将幻觉率从20-40%降低到5%以下。核心思想是，当LLM正确回答问题时，继续给出答案；否则，承认“不确定”。此外，ConfQA框架通过选择内部参数化的神经知识和外部记录的符号知识，进一步提高了模型的准确性和知识选择能力。'}}}, {'id': 'https://huggingface.co/papers/2506.08010', 'title': "Vision Transformers Don't Need Trained Registers", 'url': 'https://huggingface.co/papers/2506.08010', 'abstract': 'A training-free method shifts high-norm activations in Vision Transformers to an untrained token, enhancing attention maps and performance across visual tasks, and improving interpretability in vision-language models.  \t\t\t\t\tAI-generated summary \t\t\t\t We investigate the mechanism underlying a previously identified phenomenon in Vision Transformers -- the emergence of high-norm tokens that lead to noisy attention maps. We observe that in multiple models (e.g., CLIP, DINOv2), a sparse set of neurons is responsible for concentrating high-norm activations on outlier tokens, leading to irregular attention patterns and degrading downstream visual processing. While the existing solution for removing these outliers involves retraining models from scratch with additional learned register tokens, we use our findings to create a training-free approach to mitigate these artifacts. By shifting the high-norm activations from our discovered register neurons into an additional untrained token, we can mimic the effect of register tokens on a model already trained without registers. We demonstrate that our method produces cleaner attention and feature maps, enhances performance over base models across multiple downstream visual tasks, and achieves results comparable to models explicitly trained with register tokens. We then extend test-time registers to off-the-shelf vision-language models to improve their interpretability. Our results suggest that test-time registers effectively take on the role of register tokens at test-time, offering a training-free solution for any pre-trained model released without them.', 'score': 4, 'issue_id': 4212, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': 'd833304e26f5d1ee', 'authors': ['Nick Jiang', 'Amil Dravid', 'Alexei Efros', 'Yossi Gandelsman'], 'affiliations': ['UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2506.08010.jpg', 'data': {'categories': ['#cv', '#training', '#interpretability'], 'emoji': '🔍', 'ru': {'title': 'Улучшение внимания в трансформерах без переобучения', 'desc': 'Исследователи обнаружили, что в моделях компьютерного зрения, основанных на трансформерах, небольшое количество нейронов создает токены с высокой нормой, что приводит к шумным картам внимания. Они разработали метод без дополнительного обучения, который перемещает активации с высокой нормой в дополнительный необученный токен. Этот подход улучшает карты внимания и признаков, повышает производительность на различных задачах компьютерного зрения и улучшает интерпретируемость мультимодальных моделей. Метод сравним по эффективности с моделями, специально обученными с регистровыми токенами, но не требует переобучения.'}, 'en': {'title': 'Enhancing Vision Transformers with Training-Free Token Shifting', 'desc': 'This paper presents a novel training-free method to improve Vision Transformers by addressing the issue of high-norm activations that lead to noisy attention maps. The authors identify that certain neurons concentrate these high-norm activations on outlier tokens, which disrupts attention patterns and affects visual task performance. Instead of retraining models with additional learned tokens, they propose shifting these activations to an untrained token, effectively simulating the benefits of register tokens without the need for retraining. Their approach not only enhances attention and feature maps but also boosts performance across various visual tasks, making it applicable to existing vision-language models for better interpretability.'}, 'zh': {'title': '无训练方法提升视觉模型性能与可解释性', 'desc': '本文提出了一种无训练的方法，通过将高范数激活转移到未训练的标记上，来增强视觉变换器中的注意力图和性能。研究发现，在多个模型中，一小部分神经元负责将高范数激活集中在异常标记上，导致注意力模式不规则，影响视觉处理。我们的方法无需重新训练模型，而是利用发现的注册神经元将高范数激活转移到额外的未训练标记上，从而改善注意力和特征图。实验结果表明，该方法在多个视觉任务中提升了性能，并且与显式训练的注册标记模型的结果相当。'}}}, {'id': 'https://huggingface.co/papers/2506.08006', 'title': 'Dreamland: Controllable World Creation with Simulator and Generative\n  Models', 'url': 'https://huggingface.co/papers/2506.08006', 'abstract': 'Dreamland integrates physics simulators and large-scale pretrained generative models for controllable and photorealistic video world generation, improving image quality and controllability.  \t\t\t\t\tAI-generated summary \t\t\t\t Large-scale video generative models can synthesize diverse and realistic visual content for dynamic world creation, but they often lack element-wise controllability, hindering their use in editing scenes and training embodied AI agents. We propose Dreamland, a hybrid world generation framework combining the granular control of a physics-based simulator and the photorealistic content output of large-scale pretrained generative models. In particular, we design a layered world abstraction that encodes both pixel-level and object-level semantics and geometry as an intermediate representation to bridge the simulator and the generative model. This approach enhances controllability, minimizes adaptation cost through early alignment with real-world distributions, and supports off-the-shelf use of existing and future pretrained generative models. We further construct a D3Sim dataset to facilitate the training and evaluation of hybrid generation pipelines. Experiments demonstrate that Dreamland outperforms existing baselines with 50.8% improved image quality, 17.9% stronger controllability, and has great potential to enhance embodied agent training. Code and data will be made available.', 'score': 4, 'issue_id': 4210, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': 'e9dd998a336df034', 'authors': ['Sicheng Mo', 'Ziyang Leng', 'Leon Liu', 'Weizhen Wang', 'Honglin He', 'Bolei Zhou'], 'affiliations': ['University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2506.08006.jpg', 'data': {'categories': ['#synthetic', '#dataset', '#games', '#video', '#agents'], 'emoji': '🎥', 'ru': {'title': 'Фотореалистичные видеомиры под полным контролем', 'desc': 'Dreamland - это новый подход к генерации видеомиров, объединяющий физические симуляторы и предобученные генеративные модели большого масштаба. Система использует многоуровневую абстракцию мира, кодирующую как пиксельную, так и объектную семантику и геометрию. Это позволяет улучшить контролируемость и фотореалистичность генерируемого контента. Эксперименты показывают, что Dreamland превосходит существующие методы по качеству изображения и степени контроля.'}, 'en': {'title': 'Dreamland: Bridging Physics and Generative Models for Enhanced Video World Creation', 'desc': 'Dreamland is a novel framework that combines physics simulators with large-scale pretrained generative models to create realistic and controllable video worlds. It addresses the limitations of existing video generative models by providing element-wise controllability, which is essential for editing scenes and training AI agents. The framework uses a layered world abstraction that captures both pixel-level and object-level details, allowing for better integration between the simulator and the generative model. Experiments show that Dreamland significantly improves image quality and controllability, making it a promising tool for future AI applications.'}, 'zh': {'title': '梦境：可控的真实视频世界生成', 'desc': 'Dreamland是一个结合物理模拟器和大规模预训练生成模型的混合世界生成框架。它通过设计分层世界抽象，编码像素级和物体级的语义与几何信息，从而提高了图像质量和可控性。该方法能够在生成过程中实现更细粒度的控制，减少适应成本，并支持现有和未来生成模型的使用。实验结果表明，Dreamland在图像质量和可控性方面均显著优于现有基线，具有提升智能体训练的巨大潜力。'}}}, {'id': 'https://huggingface.co/papers/2506.06266', 'title': 'Cartridges: Lightweight and general-purpose long context representations\n  via self-study', 'url': 'https://huggingface.co/papers/2506.06266', 'abstract': "Cartridges trained with self-study replicate in-context learning functionality while reducing memory and increasing throughput, and enable longer context lengths and composability at inference.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models are often used to answer queries grounded in large text corpora (e.g. codebases, legal documents, or chat histories) by placing the entire corpus in the context window and leveraging in-context learning (ICL). Although current models support contexts of 100K-1M tokens, this setup is costly to serve because the memory consumption of the KV cache scales with input length. We explore an alternative: training a smaller KV cache offline on each corpus. At inference time, we load this trained KV cache, which we call a Cartridge, and decode a response. Critically, the cost of training a Cartridge can be amortized across all the queries referencing the same corpus. However, we find that the naive approach of training the Cartridge with next-token prediction on the corpus is not competitive with ICL. Instead, we propose self-study, a training recipe in which we generate synthetic conversations about the corpus and train the Cartridge with a context-distillation objective. We find that Cartridges trained with self-study replicate the functionality of ICL, while being significantly cheaper to serve. On challenging long-context benchmarks, Cartridges trained with self-study match ICL performance while using 38.6x less memory and enabling 26.4x higher throughput. Self-study also extends the model's effective context length (e.g. from 128k to 484k tokens on MTOB) and surprisingly, leads to Cartridges that can be composed at inference time without retraining.", 'score': 4, 'issue_id': 4211, 'pub_date': '2025-06-06', 'pub_date_card': {'ru': '6 июня', 'en': 'June 6', 'zh': '6月6日'}, 'hash': '0d49d6022b9d6786', 'authors': ['Sabri Eyuboglu', 'Ryan Ehrlich', 'Simran Arora', 'Neel Guha', 'Dylan Zinsley', 'Emily Liu', 'Will Tennien', 'Atri Rudra', 'James Zou', 'Azalia Mirhoseini', 'Christopher Re'], 'affiliations': ['Caltech', 'Stanford University', 'University at Buffalo'], 'pdf_title_img': 'assets/pdf/title_img/2506.06266.jpg', 'data': {'categories': ['#long_context', '#data', '#inference', '#synthetic', '#training', '#optimization'], 'emoji': '💾', 'ru': {'title': 'Картриджи: эффективное обучение языковых моделей для работы с длинными контекстами', 'desc': "Статья представляет новый метод обучения языковых моделей, названный 'картриджами'. Картриджи, обученные с помощью техники 'самообучения', способны воспроизводить функциональность обучения в контексте, при этом значительно снижая потребление памяти и увеличивая пропускную способность. Этот подход позволяет работать с более длинными контекстами и обеспечивает возможность композиции картриджей во время вывода. Метод особенно эффективен для задач, требующих анализа больших текстовых корпусов, таких как кодовые базы или юридические документы."}, 'en': {'title': 'Efficient In-Context Learning with Cartridges', 'desc': "This paper introduces a novel approach called Cartridges, which are trained using a method called self-study to enhance in-context learning (ICL) in large language models. By creating a smaller key-value (KV) cache for each text corpus, Cartridges significantly reduce memory usage and increase processing speed during inference. The self-study training method involves generating synthetic conversations to improve the Cartridge's performance, allowing it to match ICL capabilities while being more efficient. The results show that Cartridges can handle longer context lengths and can be composed without the need for retraining, making them a powerful tool for various applications."}, 'zh': {'title': '自我学习：高效的上下文学习解决方案', 'desc': '本文提出了一种名为Cartridge的技术，通过自我学习训练小型KV缓存，以实现上下文学习功能，同时降低内存消耗并提高处理速度。传统的大型语言模型需要将整个文本语料放入上下文窗口，导致高昂的内存成本，而Cartridge通过离线训练缓存来解决这一问题。研究表明，使用自我学习的方法训练Cartridge，可以在保持与上下文学习相似的性能的同时，显著减少内存使用和提高吞吐量。最终，Cartridge不仅扩展了模型的有效上下文长度，还能在推理时进行组合，而无需重新训练。'}}}, {'id': 'https://huggingface.co/papers/2506.07848', 'title': 'PolyVivid: Vivid Multi-Subject Video Generation with Cross-Modal\n  Interaction and Enhancement', 'url': 'https://huggingface.co/papers/2506.07848', 'abstract': 'PolyVivid is a multi-subject video customization framework that uses text-image fusion, 3D-RoPE enhancement, attention-inherited identity injection, and MLLM-based data processing to ensure identity consistency and realistic video generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite recent advances in video generation, existing models still lack fine-grained controllability, especially for multi-subject customization with consistent identity and interaction. In this paper, we propose PolyVivid, a multi-subject video customization framework that enables flexible and identity-consistent generation. To establish accurate correspondences between subject images and textual entities, we design a VLLM-based text-image fusion module that embeds visual identities into the textual space for precise grounding. To further enhance identity preservation and subject interaction, we propose a 3D-RoPE-based enhancement module that enables structured bidirectional fusion between text and image embeddings. Moreover, we develop an attention-inherited identity injection module to effectively inject fused identity features into the video generation process, mitigating identity drift. Finally, we construct an MLLM-based data pipeline that combines MLLM-based grounding, segmentation, and a clique-based subject consolidation strategy to produce high-quality multi-subject data, effectively enhancing subject distinction and reducing ambiguity in downstream video generation. Extensive experiments demonstrate that PolyVivid achieves superior performance in identity fidelity, video realism, and subject alignment, outperforming existing open-source and commercial baselines.', 'score': 2, 'issue_id': 4214, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': 'e91b69e6dac8c694', 'authors': ['Teng Hu', 'Zhentao Yu', 'Zhengguang Zhou', 'Jiangning Zhang', 'Yuan Zhou', 'Qinglin Lu', 'Ran Yi'], 'affiliations': ['Shanghai Jiao Tong University', 'Tencent Hunyuan', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.07848.jpg', 'data': {'categories': ['#3d', '#video', '#open_source', '#multimodal', '#games', '#optimization'], 'emoji': '🎬', 'ru': {'title': 'PolyVivid: Новый уровень кастомизации видео с множеством субъектов', 'desc': 'PolyVivid - это фреймворк для кастомизации видео с несколькими субъектами, использующий слияние текста и изображений, улучшение с помощью 3D-RoPE, внедрение идентичности на основе наследования внимания и обработку данных на основе MLLM. Система обеспечивает согласованность идентичности и реалистичную генерацию видео. PolyVivid позволяет осуществлять гибкую и согласованную по идентичности генерацию, используя модуль слияния текста и изображений на основе VLLM для точной привязки. Фреймворк также включает модуль улучшения на основе 3D-RoPE и модуль внедрения идентичности для сохранения идентичности и взаимодействия субъектов.'}, 'en': {'title': 'PolyVivid: Consistent Multi-Subject Video Generation Made Easy!', 'desc': 'PolyVivid is a framework designed for creating videos with multiple subjects while maintaining their identities and interactions. It uses advanced techniques like text-image fusion to accurately link visual identities with textual descriptions. The framework also incorporates a 3D-RoPE enhancement module to improve how text and image data interact, ensuring that identities remain consistent throughout the video. By employing a multi-level language model (MLLM) for data processing, PolyVivid enhances the quality of video generation, achieving better identity fidelity and realism compared to existing models.'}, 'zh': {'title': 'PolyVivid：多主体视频定制的新突破', 'desc': 'PolyVivid是一个多主体视频定制框架，利用文本-图像融合、3D-RoPE增强、注意力继承身份注入和基于MLLM的数据处理，确保身份一致性和真实的视频生成。该框架解决了现有视频生成模型在多主体定制中缺乏细粒度可控性的问题。通过设计基于VLLM的文本-图像融合模块，PolyVivid能够准确建立主体图像与文本实体之间的对应关系。实验结果表明，PolyVivid在身份保真度、视频真实感和主体对齐方面表现优越，超越了现有的开源和商业基线。'}}}, {'id': 'https://huggingface.co/papers/2506.07527', 'title': "Learning What Reinforcement Learning Can't: Interleaved Online\n  Fine-Tuning for Hardest Questions", 'url': 'https://huggingface.co/papers/2506.07527', 'abstract': "ReLIFT, a method combining reinforcement learning and supervised fine-tuning, enhances large language model reasoning by addressing limitations of RL through interleaved training, improving performance across benchmarks with minimal data.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language model (LLM) reasoning have shown that sophisticated behaviors such as planning and self-reflection can emerge through reinforcement learning (RL). However, despite these successes, RL in its current form remains insufficient to induce capabilities that exceed the limitations of the base model, as it is primarily optimized based on existing knowledge of the model rather than facilitating the acquisition of new information. To address this limitation, we employ supervised fine-tuning (SFT) to learn what RL cannot, which enables the incorporation of new knowledge and reasoning patterns by leveraging high-quality demonstration data. We analyze the training dynamics of RL and SFT for LLM reasoning and find that RL excels at maintaining and improving performance on questions within the model's original capabilities, while SFT is more effective at enabling progress on questions beyond the current scope of the model. Motivated by the complementary strengths of RL and SFT, we introduce a novel training approach, ReLIFT (Reinforcement Learning Interleaved with Online Fine-Tuning). In ReLIFT, the model is primarily trained using RL, but when it encounters challenging questions, high-quality solutions are collected for fine-tuning, and the training process alternates between RL and fine-tuning to enhance the model's reasoning abilities. ReLIFT achieves an average improvement of over +5.2 points across five competition-level benchmarks and one out-of-distribution benchmark compared to other zero-RL models. Furthermore, we demonstrate that ReLIFT outperforms both RL and SFT while using only 13\\% of the detailed demonstration data, highlighting its scalability. These results provide compelling evidence that ReLIFT overcomes the fundamental limitations of RL and underscores the significant potential.", 'score': 2, 'issue_id': 4214, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': '6978ee3d97028c45', 'authors': ['Lu Ma', 'Hao Liang', 'Meiyi Qiang', 'Lexiang Tang', 'Xiaochen Ma', 'Zhen Hao Wong', 'Junbo Niu', 'Chengyu Shen', 'Runming He', 'Bin Cui', 'Wentao Zhang'], 'affiliations': ['Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2506.07527.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#rl', '#training'], 'emoji': '🧠', 'ru': {'title': 'ReLIFT: Улучшение рассуждений языковых моделей через комбинацию RL и SFT', 'desc': 'ReLIFT - это новый метод обучения больших языковых моделей (LLM), сочетающий обучение с подкреплением (RL) и супервизорную тонкую настройку (SFT). Он решает проблему ограниченности RL, чередуя его с SFT для приобретения новых знаний. ReLIFT улучшает способности модели к рассуждению, показывая прирост более 5.2 пунктов на различных бенчмарках. Метод эффективно использует всего 13% детальных обучающих данных, демонстрируя высокую масштабируемость.'}, 'en': {'title': 'ReLIFT: Enhancing LLM Reasoning with Smart Training Mix', 'desc': 'ReLIFT is a novel training method that combines reinforcement learning (RL) and supervised fine-tuning (SFT) to improve the reasoning capabilities of large language models (LLMs). It addresses the limitations of traditional RL by interleaving training sessions, allowing the model to learn from high-quality demonstration data when faced with difficult questions. This approach enables the model to acquire new knowledge and reasoning patterns, enhancing its performance on challenging tasks. The results show that ReLIFT significantly outperforms both standalone RL and SFT methods, achieving better results with less training data.'}, 'zh': {'title': 'ReLIFT：强化学习与微调的完美结合', 'desc': 'ReLIFT是一种结合强化学习和监督微调的方法，旨在提升大型语言模型的推理能力。该方法通过交替训练来解决强化学习的局限性，使模型在面对挑战性问题时能够有效地吸收新知识。研究表明，ReLIFT在五个竞争级基准测试中平均提高了超过5.2分，并且在使用仅13%的示范数据的情况下，超越了传统的强化学习和监督微调方法。此结果表明，ReLIFT能够克服强化学习的基本局限性，展现出巨大的潜力。'}}}, {'id': 'https://huggingface.co/papers/2506.07240', 'title': 'Overclocking LLM Reasoning: Monitoring and Controlling Thinking Path\n  Lengths in LLMs', 'url': 'https://huggingface.co/papers/2506.07240', 'abstract': 'LLMs use progressive encoding and visualization to optimize the length of reasoning processes, enhancing accuracy and reducing inference time.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, techniques such as explicit structured reasoning have demonstrated strong test-time scaling behavior by enforcing a separation between the model\'s internal "thinking" process and the final response. A key factor influencing answer quality in this setting is the length of the thinking stage. When the reasoning is too short, the model may fail to capture the complexity of the task. Conversely, when it is too long, the model may overthink, leading to unnecessary computation and degraded performance. This paper explores and exploits the underlying mechanisms by which LLMs understand and regulate the length of their reasoning during explicit thought processes. First, we show that LLMs encode their progress through the reasoning process and introduce an interactive progress bar visualization, which is then used to reveal insights on the model\'s planning dynamics. Second, we manipulate the internal progress encoding during inference to reduce unnecessary steps and generate a more concise and decisive chain of thoughts. Our empirical results demonstrate that this "overclocking" method mitigates overthinking, improves answer accuracy, and reduces inference latency. Our code is publicly available.', 'score': 2, 'issue_id': 4211, 'pub_date': '2025-06-08', 'pub_date_card': {'ru': '8 июня', 'en': 'June 8', 'zh': '6月8日'}, 'hash': '25ea5847c07c7881', 'authors': ['Roy Eisenstadt', 'Itamar Zimerman', 'Lior Wolf'], 'affiliations': ['IBM Research', 'Tel-Aviv University'], 'pdf_title_img': 'assets/pdf/title_img/2506.07240.jpg', 'data': {'categories': ['#inference', '#reasoning', '#optimization', '#training'], 'emoji': '⏱️', 'ru': {'title': 'Оптимизация рассуждений LLM: быстрее, точнее, эффективнее', 'desc': "Статья исследует механизмы, с помощью которых большие языковые модели (LLM) регулируют длину процесса рассуждений. Авторы демонстрируют, что LLM кодируют прогресс рассуждений и предлагают визуализацию в виде интерактивной полосы прогресса. Они манипулируют внутренним кодированием прогресса во время вывода, чтобы сократить ненужные шаги и генерировать более краткую цепочку мыслей. Эмпирические результаты показывают, что этот метод 'разгона' уменьшает излишние размышления, повышает точность ответов и сокращает задержку вывода."}, 'en': {'title': 'Optimizing Reasoning Length for Enhanced LLM Performance', 'desc': 'This paper discusses how large language models (LLMs) can improve their reasoning processes by optimizing the length of their internal thought stages. It highlights the importance of balancing the duration of reasoning: too short may miss complexities, while too long can lead to overthinking and inefficiency. The authors introduce a method for encoding reasoning progress and a visualization tool to better understand how LLMs plan their responses. By adjusting this internal encoding, they demonstrate that LLMs can enhance accuracy and speed during inference, leading to better performance overall.'}, 'zh': {'title': '优化推理长度，提升模型表现', 'desc': '本文探讨了大型语言模型（LLMs）在推理过程中如何优化思维长度，以提高准确性并减少推理时间。研究表明，推理阶段的长度对答案质量有重要影响，过短可能无法捕捉任务复杂性，过长则可能导致不必要的计算。我们引入了一种交互式进度条可视化，帮助揭示模型的思维动态，并通过操控内部进度编码来减少冗余步骤。实验证明，这种“超频”方法有效减轻了过度思考，提高了答案的准确性，并降低了推理延迟。'}}}, {'id': 'https://huggingface.co/papers/2506.07160', 'title': 'GeometryZero: Improving Geometry Solving for LLM with Group Contrastive\n  Policy Optimization', 'url': 'https://huggingface.co/papers/2506.07160', 'abstract': 'A new reinforcement learning framework, Group Contrastive Policy Optimization (GCPO), enhances geometric reasoning in large language models with judicious auxiliary constructions, outperforming existing methods on benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models (LLMs) have demonstrated remarkable capabilities across diverse domains, particularly in mathematical reasoning, amid which geometry problem solving remains a challenging area where auxiliary construction plays a enssential role. Existing approaches either achieve suboptimal performance or rely on massive LLMs (e.g., GPT-4o), incurring massive computational costs. We posit that reinforcement learning with verifiable reward (e.g., GRPO) offers a promising direction for training smaller models that effectively combine auxiliary construction with robust geometric reasoning. However, directly applying GRPO to geometric reasoning presents fundamental limitations due to its dependence on unconditional rewards, which leads to indiscriminate and counterproductive auxiliary constructions. To address these challenges, we propose Group Contrastive Policy Optimization (GCPO), a novel reinforcement learning framework featuring two key innovations: (1) Group Contrastive Masking, which adaptively provides positive or negative reward signals for auxiliary construction based on contextual utility, and a (2) length reward that promotes longer reasoning chains. Building on GCPO, we develop GeometryZero, a family of affordable-size geometric reasoning models that judiciously determine when to employ auxiliary construction. Our extensive empirical evaluation across popular geometric benchmarks (Geometry3K, MathVista) demonstrates that GeometryZero models consistently outperform baselines (e.g. GRPO), achieving an average improvement of 4.29% across all benchmarks.', 'score': 2, 'issue_id': 4212, 'pub_date': '2025-06-08', 'pub_date_card': {'ru': '8 июня', 'en': 'June 8', 'zh': '6月8日'}, 'hash': '25d25cf0d2a1b0d5', 'authors': ['Yikun Wang', 'Yibin Wang', 'Dianyi Wang', 'Zimian Peng', 'Qipeng Guo', 'Dacheng Tao', 'Jiaqi Wang'], 'affiliations': ['Fudan University', 'Nanyang Technological University', 'Shanghai AI Laboratory', 'Shanghai Innovation Institute', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.07160.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#optimization', '#rl', '#training'], 'emoji': '📐', 'ru': {'title': 'Умные вспомогательные построения для геометрических задач', 'desc': 'Новый фреймворк обучения с подкреплением, Group Contrastive Policy Optimization (GCPO), улучшает геометрические рассуждения в больших языковых моделях с помощью вспомогательных построений. GCPO использует групповое контрастное маскирование для адаптивного предоставления сигналов вознаграждения за вспомогательные построения. На основе GCPO разработано семейство моделей GeometryZero, которые эффективно определяют, когда применять вспомогательные построения. Эмпирическая оценка показывает, что GeometryZero превосходит базовые модели на популярных геометрических тестах.'}, 'en': {'title': 'Enhancing Geometric Reasoning with GCPO in Language Models', 'desc': 'The paper introduces a new reinforcement learning framework called Group Contrastive Policy Optimization (GCPO) that improves geometric reasoning in large language models. It addresses the limitations of existing methods that either underperform or require large models, which are costly to run. GCPO uses innovative techniques like Group Contrastive Masking to provide context-sensitive rewards for auxiliary constructions and a length reward to encourage longer reasoning processes. The results show that models developed using GCPO, named GeometryZero, significantly outperform previous benchmarks in geometric problem-solving tasks.'}, 'zh': {'title': '群体对比策略优化：提升几何推理的新方法', 'desc': '本文提出了一种新的强化学习框架，称为群体对比策略优化（GCPO），旨在增强大型语言模型在几何推理方面的能力。GCPO通过适应性地提供正负奖励信号，优化辅助构造的使用，从而克服了现有方法的局限性。我们还开发了GeometryZero模型系列，这些模型能够有效地结合辅助构造与稳健的几何推理。实验结果表明，GeometryZero在多个几何基准测试中表现优于现有方法，平均提升了4.29%。'}}}, {'id': 'https://huggingface.co/papers/2506.03690', 'title': 'Robust Preference Optimization via Dynamic Target Margins', 'url': 'https://huggingface.co/papers/2506.03690', 'abstract': "The paper introduces γ-PO, a dynamic target margin preference optimization algorithm that enhances Large Language Models' alignment by adjusting reward margins at the pairwise level, leading to improved performance with minimal impact on training.  \t\t\t\t\tAI-generated summary \t\t\t\t The alignment of Large Language Models (LLMs) is crucial for ensuring their safety and reliability in practical applications. Direct Preference Optimization (DPO) has emerged as an efficient method that directly optimizes models using preference pairs, significantly reducing resource demands. However, the effectiveness of DPO heavily depends on the data quality, which is frequently compromised by noise. In this work, we propose gamma-PO, a dynamic target margin preference optimization algorithm that adjust reward margins at the pairwise level. By introducing instance-specific margin calibration, gamma-PO strategically prioritizes high-confidence pairs (those demonstrating higher reward margins) while suppressing potential noise from ambiguous pairs. Moreover, gamma-PO is a plug-and-play method, compatible with variants of DPO that rely on reward margin between preference pairs. Across benchmarks such as AlpacaEval2 and Arena-Hard, gamma-PO achieves an average 4.4\\% improvement over other baselines, setting new benchmarks for state-of-the-art performance. Additionally, gamma-PO requires minimal code changes and has a negligible impact on training efficiency, making it a robust solution for enhancing LLMs alignment. Our codes are available at https://github.com/sunjie279/gammaPO{https://github.com/sunjie279/gammaPO}.", 'score': 2, 'issue_id': 4208, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': 'a3134e659a450a93', 'authors': ['Jie Sun', 'Junkang Wu', 'Jiancan Wu', 'Zhibo Zhu', 'Xingyu Lu', 'Jun Zhou', 'Lintao Ma', 'Xiang Wang'], 'affiliations': ['Ant Group', 'National University of Singapore', 'Shanghai Key Laboratory of Data Science'], 'pdf_title_img': 'assets/pdf/title_img/2506.03690.jpg', 'data': {'categories': ['#optimization', '#training', '#benchmark', '#rlhf', '#alignment'], 'emoji': '🎯', 'ru': {'title': 'γ-PO: Точная настройка больших языковых моделей через динамическую оптимизацию предпочтений', 'desc': 'Статья представляет γ-PO - алгоритм динамической оптимизации целевой маржи предпочтений для улучшения выравнивания больших языковых моделей (LLM). γ-PO корректирует маржу вознаграждения на попарном уровне, что позволяет приоритизировать высококачественные пары данных и подавлять потенциальный шум. Метод совместим с вариантами Direct Preference Optimization (DPO) и достигает в среднем 4.4% улучшения производительности на различных бенчмарках. γ-PO требует минимальных изменений в коде и не влияет на эффективность обучения, что делает его надежным решением для улучшения выравнивания LLM.'}, 'en': {'title': 'Enhancing LLM Alignment with Dynamic Margin Optimization', 'desc': 'The paper presents γ-PO, a novel algorithm designed to optimize the alignment of Large Language Models (LLMs) by dynamically adjusting reward margins at the pairwise level. This method enhances Direct Preference Optimization (DPO) by focusing on high-confidence preference pairs, which helps to mitigate the effects of noisy data. By implementing instance-specific margin calibration, γ-PO improves model performance while maintaining training efficiency with minimal code modifications. The results demonstrate an average improvement of 4.4% over existing baselines, establishing new benchmarks in LLM alignment.'}, 'zh': {'title': 'γ-PO：提升大型语言模型对齐性的动态优化算法', 'desc': '本文介绍了一种名为γ-PO的动态目标边际偏好优化算法，旨在通过调整成对的奖励边际来增强大型语言模型（LLMs）的对齐性。该算法通过实例特定的边际校准，优先考虑高置信度的成对数据，同时抑制模糊成对数据的潜在噪声。γ-PO与现有的直接偏好优化（DPO）方法兼容，能够在不显著影响训练效率的情况下，提升模型的性能。实验结果表明，γ-PO在多个基准测试中平均提高了4.4%的性能，设定了新的最先进的性能基准。'}}}, {'id': 'https://huggingface.co/papers/2506.07803', 'title': 'Image Reconstruction as a Tool for Feature Analysis', 'url': 'https://huggingface.co/papers/2506.07803', 'abstract': 'Image reconstruction reveals that vision encoders retain more image information after image-based tasks and that orthogonal rotations in feature space control color encoding.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision encoders are increasingly used in modern applications, from vision-only models to multimodal systems such as vision-language models. Despite their remarkable success, it remains unclear how these architectures represent features internally. Here, we propose a novel approach for interpreting vision features via image reconstruction. We compare two related model families, SigLIP and SigLIP2, which differ only in their training objective, and show that encoders pre-trained on image-based tasks retain significantly more image information than those trained on non-image tasks such as contrastive learning. We further apply our method to a range of vision encoders, ranking them by the informativeness of their feature representations. Finally, we demonstrate that manipulating the feature space yields predictable changes in reconstructed images, revealing that orthogonal rotations (rather than spatial transformations) control color encoding. Our approach can be applied to any vision encoder, shedding light on the inner structure of its feature space. The code and model weights to reproduce the experiments are available in GitHub.', 'score': 0, 'issue_id': 4213, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': '0bba64237ba10a58', 'authors': ['Eduard Allakhverdov', 'Dmitrii Tarasov', 'Elizaveta Goncharova', 'Andrey Kuznetsov'], 'affiliations': ['AIRI Moscow, Russia', 'MIPT Dolgoprudny, Russia'], 'pdf_title_img': 'assets/pdf/title_img/2506.07803.jpg', 'data': {'categories': ['#open_source', '#interpretability', '#cv', '#multimodal', '#architecture'], 'emoji': '🔍', 'ru': {'title': 'Раскрытие тайн внутренней структуры энкодеров компьютерного зрения', 'desc': 'Исследователи предложили новый метод интерпретации признаков в энкодерах компьютерного зрения через реконструкцию изображений. Они обнаружили, что энкодеры, предобученные на задачах, связанных с изображениями, сохраняют значительно больше информации об изображении, чем обученные на других задачах. Авторы применили свой метод к различным энкодерам и ранжировали их по информативности представлений признаков. Также было показано, что ортогональные вращения в пространстве признаков контролируют кодирование цвета.'}, 'en': {'title': 'Unlocking the Secrets of Vision Encoders Through Image Reconstruction', 'desc': 'This paper explores how vision encoders, which are used in various AI applications, represent image features internally. The authors introduce a method for interpreting these features through image reconstruction, comparing two model families with different training objectives. They find that encoders trained on image-based tasks retain more information than those trained on non-image tasks. Additionally, they show that manipulating the feature space through orthogonal rotations can control how colors are encoded in the reconstructed images.'}, 'zh': {'title': '揭示视觉编码器的特征空间结构', 'desc': '本论文探讨了视觉编码器在图像重建中的表现，发现经过图像任务训练的编码器保留了更多的图像信息。我们比较了两种模型家族，SigLIP和SigLIP2，发现前者在图像任务上的预训练使其特征表示更具信息量。通过对特征空间的操作，我们发现正交旋转可以控制颜色编码，而不是空间变换。该方法适用于任何视觉编码器，有助于理解其特征空间的内部结构。'}}}, {'id': 'https://huggingface.co/papers/2506.05904', 'title': 'Proactive Assistant Dialogue Generation from Streaming Egocentric Videos', 'url': 'https://huggingface.co/papers/2506.05904', 'abstract': 'A framework provides automated data synthesis, evaluation metrics, and an end-to-end model for real-time, proactive conversational AI task guidance using streaming video inputs.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in conversational AI have been substantial, but developing real-time systems for perceptual task guidance remains challenging. These systems must provide interactive, proactive assistance based on streaming visual inputs, yet their development is constrained by the costly and labor-intensive process of data collection and system evaluation. To address these limitations, we present a comprehensive framework with three key contributions. First, we introduce a novel data curation pipeline that synthesizes dialogues from annotated egocentric videos, resulting in \\dataset, a large-scale synthetic dialogue dataset spanning multiple domains. Second, we develop a suite of automatic evaluation metrics, validated through extensive human studies. Third, we propose an end-to-end model that processes streaming video inputs to generate contextually appropriate responses, incorporating novel techniques for handling data imbalance and long-duration videos. This work lays the foundation for developing real-time, proactive AI assistants capable of guiding users through diverse tasks. Project page: https://pro-assist.github.io/', 'score': 0, 'issue_id': 4214, 'pub_date': '2025-06-06', 'pub_date_card': {'ru': '6 июня', 'en': 'June 6', 'zh': '6月6日'}, 'hash': 'a29f2576c90935c3', 'authors': ['Yichi Zhang', 'Xin Luna Dong', 'Zhaojiang Lin', 'Andrea Madotto', 'Anuj Kumar', 'Babak Damavandi', 'Joyce Chai', 'Seungwhan Moon'], 'affiliations': ['Meta', 'University of Michigan'], 'pdf_title_img': 'assets/pdf/title_img/2506.05904.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#long_context', '#data', '#synthetic', '#cv', '#multimodal'], 'emoji': '🤖', 'ru': {'title': 'Прорыв в создании проактивных ИИ-ассистентов для задач реального мира', 'desc': 'Статья представляет комплексную систему для разработки разговорного ИИ, способного в реальном времени давать указания по выполнению задач на основе видеопотока. Авторы предлагают автоматизированный конвейер для синтеза диалогов из аннотированных эгоцентрических видео, создавая крупномасштабный датасет. Разработан набор метрик для автоматической оценки, подтвержденных исследованиями с участием людей. Представлена модель, обрабатывающая потоковое видео для генерации контекстно-релевантных ответов с учетом дисбаланса данных и длительных видео.'}, 'en': {'title': 'Empowering Real-Time AI Guidance with Streaming Video Insights', 'desc': "This paper presents a framework for creating real-time conversational AI that can guide users through tasks using live video feeds. It introduces a new data curation pipeline that generates a large synthetic dialogue dataset from annotated egocentric videos, which helps in training the AI. The authors also propose automatic evaluation metrics that have been validated through human studies to assess the system's performance. Finally, they develop an end-to-end model that effectively processes streaming video inputs to provide timely and relevant responses, addressing challenges like data imbalance and long video durations."}, 'zh': {'title': '实时主动对话AI任务指导的创新框架', 'desc': '本文提出了一个框架，用于自动化数据合成、评估指标和端到端模型，以实现基于流媒体视频输入的实时主动对话AI任务指导。该框架的核心贡献包括：首先，开发了一种新颖的数据策划管道，从标注的自我中心视频中合成对话，生成了一个大规模的合成对话数据集。其次，提出了一套自动评估指标，并通过广泛的人类研究进行了验证。最后，构建了一个处理流媒体视频输入的端到端模型，能够生成上下文适当的响应，解决了数据不平衡和长时视频处理的问题。'}}}, {'id': 'https://huggingface.co/papers/2506.04807', 'title': 'MegaHan97K: A Large-Scale Dataset for Mega-Category Chinese Character\n  Recognition with over 97K Categories', 'url': 'https://huggingface.co/papers/2506.04807', 'abstract': 'MegaHan97K, a large-scale dataset for recognizing over 97,000 Chinese characters, addresses the long-tail distribution problem and reveals new challenges in mega-category OCR.  \t\t\t\t\tAI-generated summary \t\t\t\t Foundational to the Chinese language and culture, Chinese characters encompass extraordinarily extensive and ever-expanding categories, with the latest Chinese GB18030-2022 standard containing 87,887 categories. The accurate recognition of this vast number of characters, termed mega-category recognition, presents a formidable yet crucial challenge for cultural heritage preservation and digital applications. Despite significant advances in Optical Character Recognition (OCR), mega-category recognition remains unexplored due to the absence of comprehensive datasets, with the largest existing dataset containing merely 16,151 categories. To bridge this critical gap, we introduce MegaHan97K, a mega-category, large-scale dataset covering an unprecedented 97,455 categories of Chinese characters. Our work offers three major contributions: (1) MegaHan97K is the first dataset to fully support the latest GB18030-2022 standard, providing at least six times more categories than existing datasets; (2) It effectively addresses the long-tail distribution problem by providing balanced samples across all categories through its three distinct subsets: handwritten, historical and synthetic subsets; (3) Comprehensive benchmarking experiments reveal new challenges in mega-category scenarios, including increased storage demands, morphologically similar character recognition, and zero-shot learning difficulties, while also unlocking substantial opportunities for future research. To the best of our knowledge, the MetaHan97K is likely the dataset with the largest classes not only in the field of OCR but may also in the broader domain of pattern recognition. The dataset is available at https://github.com/SCUT-DLVCLab/MegaHan97K.', 'score': 0, 'issue_id': 4212, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': 'd9e2ab51265af386', 'authors': ['Yuyi Zhang', 'Yongxin Shi', 'Peirong Zhang', 'Yixin Zhao', 'Zhenhua Yang', 'Lianwen Jin'], 'affiliations': ['SCUT-Zhuhai Institute of Modern Industrial Innovation, Zhuhai, China', 'School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.04807.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#long_context', '#synthetic'], 'emoji': '🈶', 'ru': {'title': 'MegaHan97K: Революция в распознавании китайских иероглифов', 'desc': 'MegaHan97K - это крупномасштабный датасет для распознавания более 97 000 китайских иероглифов. Он решает проблему распределения с длинным хвостом и раскрывает новые вызовы в оптическом распознавании символов (OCR) с мега-категориями. Датасет включает рукописные, исторические и синтетические подмножества, обеспечивая сбалансированные выборки по всем категориям. Эксперименты по бенчмаркингу выявляют такие проблемы, как повышенные требования к хранению, распознавание морфологически схожих символов и трудности обучения с нулевым выстрелом.'}, 'en': {'title': 'Unlocking the Future of Chinese Character Recognition with MegaHan97K', 'desc': 'MegaHan97K is a groundbreaking dataset designed for recognizing over 97,000 Chinese characters, addressing the challenges of mega-category Optical Character Recognition (OCR). This dataset is the first to align with the latest GB18030-2022 standard, significantly expanding the number of character categories available for training models. It tackles the long-tail distribution problem by offering balanced samples across three subsets: handwritten, historical, and synthetic. Additionally, it highlights new challenges in mega-category recognition, such as increased storage needs and difficulties in zero-shot learning, while paving the way for future advancements in the field.'}, 'zh': {'title': 'MegaHan97K：超大类汉字识别的新里程碑', 'desc': 'MegaHan97K是一个大规模的数据集，旨在识别超过97,000个汉字，解决了长尾分布问题，并揭示了超大类OCR的新挑战。该数据集支持最新的GB18030-2022标准，提供了比现有数据集多六倍的类别。通过手写、历史和合成三个子集，MegaHan97K有效地平衡了各类别的样本。我们的研究还揭示了在超大类场景中面临的新挑战，如存储需求增加、形态相似字符的识别和零样本学习的困难。'}}}, {'id': 'https://huggingface.co/papers/2505.23473', 'title': 'EVOREFUSE: Evolutionary Prompt Optimization for Evaluation and\n  Mitigation of LLM Over-Refusal to Pseudo-Malicious Instructions', 'url': 'https://huggingface.co/papers/2505.23473', 'abstract': 'EVOREFUSE, an evolutionary algorithm, generates diverse pseudo-malicious instructions to optimize LLM refusal training, improving user experience without compromising safety.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) frequently refuse to respond to pseudo-malicious instructions: semantically harmless input queries triggering unnecessary LLM refusals due to conservative safety alignment, significantly impairing user experience. Collecting such instructions is crucial for evaluating and mitigating over-refusals, but existing instruction curation methods, like manual creation or instruction rewriting, either lack scalability or fail to produce sufficiently diverse and effective refusal-inducing prompts. To address these limitations, we introduce EVOREFUSE, a prompt optimization approach that generates diverse pseudo-malicious instructions consistently eliciting confident refusals across LLMs. EVOREFUSE employs an evolutionary algorithm exploring the instruction space in more diverse directions than existing methods via mutation strategies and recombination, and iteratively evolves seed instructions to maximize evidence lower bound on LLM refusal probability. Using EVOREFUSE, we create two novel datasets: EVOREFUSE-TEST, a benchmark of 582 pseudo-malicious instructions that outperforms the next-best benchmark with 140.41% higher average refusal triggering rate across 9 LLMs, 34.86% greater lexical diversity, and 40.03% improved LLM response confidence scores; and EVOREFUSE-ALIGN, which provides 3,000 pseudo-malicious instructions with responses for supervised and preference-based alignment training. LLAMA3.1-8B-INSTRUCT supervisedly fine-tuned on EVOREFUSE-ALIGN achieves up to 14.31% fewer over-refusals than models trained on the second-best alignment dataset, without compromising safety. Our analysis with EVOREFUSE-TEST reveals models trigger over-refusals by overly focusing on sensitive keywords while ignoring broader context.', 'score': 0, 'issue_id': 4212, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': 'dc6dd3911616cbf5', 'authors': ['Xiaorui Wu', 'Xiaofeng Mao', 'Xin Zhang', 'Fei Li', 'Chong Teng', 'Yuxiang Peng', 'Li Zheng', 'Donghong Ji', 'Zhuang Li'], 'affiliations': ['Ant Group', 'Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, China', 'School of Computing Technologies, Royal Melbourne Institute of Technology, Australia'], 'pdf_title_img': 'assets/pdf/title_img/2505.23473.jpg', 'data': {'categories': ['#hallucinations', '#optimization', '#alignment', '#dataset', '#training', '#data'], 'emoji': '🧬', 'ru': {'title': 'Эволюция инструкций для умных отказов ИИ', 'desc': 'EVOREFUSE - это эволюционный алгоритм для генерации разнообразных псевдо-вредоносных инструкций, оптимизирующий обучение отказам в больших языковых моделях (LLM). Алгоритм создает наборы данных EVOREFUSE-TEST и EVOREFUSE-ALIGN, которые превосходят существующие бенчмарки по разнообразию и эффективности вызова отказов. Обучение на этих данных позволяет LLM снизить количество ложных отказов без ущерба безопасности. Анализ показывает, что модели часто вызывают избыточные отказы, фокусируясь на отдельных чувствительных словах и игнорируя более широкий контекст.'}, 'en': {'title': 'EVOREFUSE: Enhancing LLM Refusal Training with Evolutionary Algorithms', 'desc': 'EVOREFUSE is an innovative evolutionary algorithm designed to enhance the training of large language models (LLMs) by generating a variety of pseudo-malicious instructions. These instructions help to optimize refusal training, allowing LLMs to respond more effectively to potentially harmful queries without sacrificing user safety. By employing mutation strategies and recombination, EVOREFUSE explores the instruction space more thoroughly than traditional methods, resulting in a significant increase in the diversity and effectiveness of refusal-inducing prompts. The approach has led to the creation of two new datasets that improve LLM performance, reducing unnecessary refusals while maintaining safety standards.'}, 'zh': {'title': 'EVOREFUSE：优化LLM拒绝训练的进化算法', 'desc': 'EVOREFUSE是一种进化算法，旨在生成多样化的伪恶意指令，以优化大型语言模型（LLM）的拒绝训练，从而提升用户体验而不影响安全性。现有的指令策划方法如手动创建或重写指令，缺乏可扩展性或无法产生足够多样和有效的拒绝诱导提示。EVOREFUSE通过变异策略和重组探索指令空间，迭代演化种子指令，以最大化LLM拒绝概率的证据下界。使用EVOREFUSE，我们创建了两个新数据集，显著提高了拒绝触发率和响应信心分数，减少了过度拒绝的情况。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (2)', '#agi', '#alignment (4)', '#architecture (2)', '#audio', '#benchmark (10)', '#cv (5)', '#data (7)', '#dataset (12)', '#diffusion (1)', '#ethics', '#games (4)', '#graphs (1)', '#hallucinations (4)', '#healthcare (1)', '#inference (5)', '#interpretability (4)', '#leakage', '#long_context (5)', '#low_resource', '#machine_translation', '#math (1)', '#multilingual', '#multimodal (8)', '#open_source (9)', '#optimization (13)', '#plp', '#rag', '#reasoning (10)', '#rl (7)', '#rlhf (4)', '#robotics (1)', '#science (2)', '#security (1)', '#small_models (2)', '#story_generation', '#survey (1)', '#synthetic (5)', '#training (17)', '#transfer_learning (2)', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-06-10 08:16',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-06-10 08:16')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-06-10 08:16')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    