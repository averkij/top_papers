{
    "date": {
        "ru": "8 июля",
        "en": "July 8",
        "zh": "7月8日"
    },
    "time_utc": "2025-07-08 05:14",
    "weekday": 1,
    "issue_id": 4695,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2507.03724",
            "title": "MemOS: A Memory OS for AI System",
            "url": "https://huggingface.co/papers/2507.03724",
            "abstract": "MemOS is proposed as a memory operating system for Large Language Models to enhance memory management, enabling efficient storage and retrieval, and facilitating continual learning and personalized modeling.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have become an essential infrastructure for Artificial General Intelligence (AGI), yet their lack of well-defined memory management systems hinders the development of long-context reasoning, continual personalization, and knowledge consistency.Existing models mainly rely on static parameters and short-lived contextual states, limiting their ability to track user preferences or update knowledge over extended periods.While Retrieval-Augmented Generation (RAG) introduces external knowledge in plain text, it remains a stateless workaround without lifecycle control or integration with persistent representations.Recent work has modeled the training and inference cost of LLMs from a memory hierarchy perspective, showing that introducing an explicit memory layer between parameter memory and external retrieval can substantially reduce these costs by externalizing specific knowledge. Beyond computational efficiency, LLMs face broader challenges arising from how information is distributed over time and context, requiring systems capable of managing heterogeneous knowledge spanning different temporal scales and sources. To address this challenge, we propose MemOS, a memory operating system that treats memory as a manageable system resource. It unifies the representation, scheduling, and evolution of plaintext, activation-based, and parameter-level memories, enabling cost-efficient storage and retrieval. As the basic unit, a MemCube encapsulates both memory content and metadata such as provenance and versioning. MemCubes can be composed, migrated, and fused over time, enabling flexible transitions between memory types and bridging retrieval with parameter-based learning. MemOS establishes a memory-centric system framework that brings controllability, plasticity, and evolvability to LLMs, laying the foundation for continual learning and personalized modeling.",
            "score": 41,
            "issue_id": 4693,
            "pub_date": "2025-07-04",
            "pub_date_card": {
                "ru": "4 июля",
                "en": "July 4",
                "zh": "7月4日"
            },
            "hash": "5a64c779be945671",
            "authors": [
                "Zhiyu Li",
                "Shichao Song",
                "Chenyang Xi",
                "Hanyu Wang",
                "Chen Tang",
                "Simin Niu",
                "Ding Chen",
                "Jiawei Yang",
                "Chunyu Li",
                "Qingchen Yu",
                "Jihao Zhao",
                "Yezhaohui Wang",
                "Peng Liu",
                "Zehao Lin",
                "Pengyuan Wang",
                "Jiahao Huo",
                "Tianyi Chen",
                "Kai Chen",
                "Kehang Li",
                "Zhen Tao",
                "Junpeng Ren",
                "Huayi Lai",
                "Hao Wu",
                "Bo Tang",
                "Zhenren Wang",
                "Zhaoxin Fan",
                "Ningyu Zhang",
                "Linfeng Zhang",
                "Junchi Yan",
                "Mingchuan Yang",
                "Tong Xu",
                "Wei Xu",
                "Huajun Chen",
                "Haofeng Wang",
                "Hongkang Yang",
                "Wentao Zhang",
                "Zhi-Qin John Xu",
                "Siheng Chen",
                "Feiyu Xiong"
            ],
            "affiliations": [
                "Beihang University",
                "Institute for Advanced Algorithms Research, Shanghai",
                "MemTensor (Shanghai) Technology Co., Ltd.",
                "Peking University",
                "Renmin University of China",
                "Research Institute of China Telecom",
                "Shanghai Jiao Tong University",
                "Tongji University",
                "University of Science and Technology of China",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.03724.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#agi",
                    "#rag",
                    "#long_context",
                    "#optimization",
                    "#data"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "MemOS: операционная система памяти для более умных и адаптивных языковых моделей",
                    "desc": "MemOS - это операционная система памяти для больших языковых моделей (LLM), предложенная для улучшения управления памятью. Она позволяет эффективно хранить и извлекать информацию, а также способствует непрерывному обучению и персонализированному моделированию. MemOS вводит концепцию MemCube как базовой единицы памяти, содержащей как контент, так и метаданные. Система объединяет представление, планирование и эволюцию различных типов памяти, обеспечивая гибкость и эффективность работы LLM."
                },
                "en": {
                    "title": "MemOS: Revolutionizing Memory Management for LLMs",
                    "desc": "MemOS is a proposed memory operating system designed to improve memory management in Large Language Models (LLMs). It addresses the limitations of existing models that rely on static parameters and short-term context by introducing a structured memory layer that enhances storage and retrieval capabilities. The system utilizes MemCubes, which encapsulate memory content along with metadata, allowing for flexible transitions between different memory types. This approach not only increases computational efficiency but also supports continual learning and personalized modeling by managing knowledge across various temporal scales."
                },
                "zh": {
                    "title": "MemOS：为大型语言模型提供智能内存管理",
                    "desc": "MemOS是一种为大型语言模型（LLMs）设计的内存操作系统，旨在改善内存管理。它通过统一表示、调度和演变不同类型的内存，支持高效的存储和检索。MemOS引入了MemCube作为基本单元，封装了内存内容和元数据，允许灵活的内存类型转换。该系统为LLMs提供了可控性、可塑性和可演化性，促进了持续学习和个性化建模。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.05163",
            "title": "4DSloMo: 4D Reconstruction for High Speed Scene with Asynchronous\n  Capture",
            "url": "https://huggingface.co/papers/2507.05163",
            "abstract": "A high-speed 4D capturing system using low FPS cameras with asynchronous capture and video-diffusion-based artifact correction enhances reconstruction quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Reconstructing fast-dynamic scenes from multi-view videos is crucial for high-speed motion analysis and realistic 4D reconstruction. However, the majority of 4D capture systems are limited to frame rates below 30 FPS (frames per second), and a direct 4D reconstruction of high-speed motion from low FPS input may lead to undesirable results. In this work, we propose a high-speed 4D capturing system only using low FPS cameras, through novel capturing and processing modules. On the capturing side, we propose an asynchronous capture scheme that increases the effective frame rate by staggering the start times of cameras. By grouping cameras and leveraging a base frame rate of 25 FPS, our method achieves an equivalent frame rate of 100-200 FPS without requiring specialized high-speed cameras. On processing side, we also propose a novel generative model to fix artifacts caused by 4D sparse-view reconstruction, as asynchrony reduces the number of viewpoints at each timestamp. Specifically, we propose to train a video-diffusion-based artifact-fix model for sparse 4D reconstruction, which refines missing details, maintains temporal consistency, and improves overall reconstruction quality. Experimental results demonstrate that our method significantly enhances high-speed 4D reconstruction compared to synchronous capture.",
            "score": 18,
            "issue_id": 4693,
            "pub_date": "2025-07-07",
            "pub_date_card": {
                "ru": "7 июля",
                "en": "July 7",
                "zh": "7月7日"
            },
            "hash": "e1f4c8e83495db53",
            "authors": [
                "Yutian Chen",
                "Shi Guo",
                "Tianshuo Yang",
                "Lihe Ding",
                "Xiuyuan Yu",
                "Jinwei Gu",
                "Tianfan Xue"
            ],
            "affiliations": [
                "NVIDIA",
                "Shanghai AI Laboratory",
                "The Chinese University of Hong Kong",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.05163.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#diffusion",
                    "#3d",
                    "#video"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Высокоскоростная 4D-съемка обычными камерами",
                    "desc": "Предлагается система высокоскоростной 4D-съемки с использованием камер с низкой частотой кадров и асинхронным захватом. Система повышает эффективную частоту кадров до 100-200 FPS путем смещения времени начала съемки для разных камер. Для устранения артефактов, вызванных реконструкцией по малому числу ракурсов, применяется генеративная модель на основе видео-диффузии. Экспериментальные результаты показывают значительное улучшение качества высокоскоростной 4D-реконструкции по сравнению с синхронной съемкой."
                },
                "en": {
                    "title": "Revolutionizing 4D Capture: High-Speed Reconstruction with Low FPS Cameras",
                    "desc": "This paper presents a novel system for capturing high-speed 4D scenes using low frame rate cameras. It introduces an asynchronous capture technique that effectively increases the frame rate by staggering the start times of multiple cameras, achieving rates of 100-200 FPS from a base of 25 FPS. Additionally, the authors propose a video-diffusion-based generative model to correct artifacts in the sparse 4D reconstruction, ensuring better detail and temporal consistency. Experimental results show that this approach significantly improves the quality of high-speed 4D reconstructions compared to traditional synchronous methods."
                },
                "zh": {
                    "title": "低帧率相机实现高速度4D重建的创新方案",
                    "desc": "本研究提出了一种高速度的4D捕捉系统，利用低帧率相机进行异步捕捉和视频扩散基础的伪影修正，从而提高重建质量。传统的4D捕捉系统通常帧率低于30 FPS，直接从低帧率输入进行高速度运动的4D重建会导致不理想的结果。我们的方法通过异步捕捉方案，将相机的启动时间错开，提升了有效帧率，达到100-200 FPS的效果。处理方面，我们提出了一种基于视频扩散的生成模型，修复4D稀疏视图重建中产生的伪影，显著改善了重建的细节和时间一致性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.05197",
            "title": "Pre-Trained Policy Discriminators are General Reward Models",
            "url": "https://huggingface.co/papers/2507.05197",
            "abstract": "A scalable reward modeling method, Policy Discriminative Learning (POLAR), enhances reward model performance and generalizes robustly in reinforcement learning through policy comparison.  \t\t\t\t\tAI-generated summary \t\t\t\t We offer a novel perspective on reward modeling by formulating it as a policy discriminator, which quantifies the difference between two policies to generate a reward signal, guiding the training policy towards a target policy with desired behaviors. Based on this conceptual insight, we propose a scalable pre-training method named Policy Discriminative Learning (POLAR), which trains a reward model (RM) to discern identical policies and discriminate different ones. Unlike traditional reward modeling methods relying on absolute preferences, POLAR captures the relative difference between one policy and an arbitrary target policy, which is a scalable, high-level optimization objective suitable for modeling generic ranking relationships. Leveraging the POLAR pre-training paradigm, we present a series of RMs with parameter scales from 1.8B to 7B. Empirical results show that POLAR substantially outperforms traditional non-pre-trained methods, significantly enhancing RM performance. For instance, POLAR-7B could improve preference accuracy from 54.8% to 81.0% on STEM tasks and from 57.9% to 85.5% on creative writing tasks compared to SOTA baselines. POLAR also shows robust generalization capabilities in RLHF using Reinforcement Fine-tuning (RFT), providing reliable reward signals and markedly enhancing policy performance--improving LLaMa3.1-8B from an average of 47.36% to 56.33% and Qwen2.5-32B from 64.49% to 70.47% on 20 benchmarks. Moreover, scaling experiments reveal a clear power-law relationship between computation and performance, supported by linear correlation coefficients approaching 0.99. The impressive performance, strong generalization, and scaling properties suggest that POLAR is a promising direction for developing general and strong reward models.",
            "score": 15,
            "issue_id": 4694,
            "pub_date": "2025-07-07",
            "pub_date_card": {
                "ru": "7 июля",
                "en": "July 7",
                "zh": "7月7日"
            },
            "hash": "88d62db0ed894120",
            "authors": [
                "Shihan Dou",
                "Shichun Liu",
                "Yuming Yang",
                "Yicheng Zou",
                "Yunhua Zhou",
                "Shuhao Xing",
                "Chenhao Huang",
                "Qiming Ge",
                "Demin Song",
                "Haijun Lv",
                "Songyang Gao",
                "Chengqi Lv",
                "Enyu Zhou",
                "Honglin Guo",
                "Zhiheng Xi",
                "Wenwei Zhang",
                "Qipeng Guo",
                "Qi Zhang",
                "Xipeng Qiu",
                "Xuanjing Huang",
                "Tao Gui",
                "Kai Chen"
            ],
            "affiliations": [
                "Fudan University",
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.05197.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#rlhf",
                    "#training",
                    "#rl"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "POLAR: Революция в моделировании вознаграждений для обучения с подкреплением",
                    "desc": "В статье представлен новый метод моделирования вознаграждений в обучении с подкреплением, названный Policy Discriminative Learning (POLAR). POLAR обучает модель вознаграждения различать идентичные политики и дискриминировать различные, что позволяет захватывать относительную разницу между политиками. Эмпирические результаты показывают, что POLAR значительно превосходит традиционные методы, улучшая точность предпочтений и производительность политик в различных задачах. Метод демонстрирует надежные возможности обобщения и масштабирования, что делает его перспективным направлением для разработки сильных и общих моделей вознаграждения."
                },
                "en": {
                    "title": "POLAR: Revolutionizing Reward Modeling through Policy Comparison",
                    "desc": "The paper introduces Policy Discriminative Learning (POLAR), a new method for reward modeling in reinforcement learning that focuses on comparing policies rather than relying on absolute preferences. By treating reward modeling as a policy discriminator, POLAR effectively generates reward signals that guide the training policy towards a target policy with desired behaviors. This approach allows for scalable pre-training of reward models, which can discern between similar and different policies, enhancing their performance significantly. Empirical results demonstrate that POLAR outperforms traditional methods, showing improved accuracy and robust generalization in various tasks."
                },
                "zh": {
                    "title": "POLAR：提升奖励模型性能的新方法",
                    "desc": "POLAR是一种可扩展的奖励建模方法，通过策略比较来增强奖励模型的性能并在强化学习中实现稳健的泛化。它将奖励建模视为策略鉴别器，量化两个策略之间的差异，以生成奖励信号，指导训练策略朝向具有期望行为的目标策略。与传统的绝对偏好方法不同，POLAR捕捉一个策略与任意目标策略之间的相对差异，适合于建模通用的排名关系。实验结果表明，POLAR显著优于传统的非预训练方法，提升了奖励模型的表现，展示了其在强化学习中的强大泛化能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.03483",
            "title": "BMMR: A Large-Scale Bilingual Multimodal Multi-Discipline Reasoning\n  Dataset",
            "url": "https://huggingface.co/papers/2507.03483",
            "abstract": "A large-scale dataset and verification tool are introduced for assessing and improving cross-disciplinary reasoning capabilities in multimodal models.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we introduce BMMR, a large-scale bilingual, multimodal, multi-disciplinary reasoning dataset for the community to develop and evaluate large multimodal models (LMMs). BMMR comprises 110k college-level questions spanning 300 UNESCO-defined subjects, spanning diverse formats-multiple-choice, fill-in-the-blank, and open-ended QA-and sourced from both print and digital media such as books, exams, and quizzes. All data are curated and filtered via a human-in-the-loop and scalable framework, and each instance is paired with a high-quality reasoning path. The dataset is organized into two parts: BMMR-Eval that comprises 20,458 high-quality instances to comprehensively assess LMMs' knowledge and reasoning across multiple disciplines in both Chinese and English; and BMMR-Train that contains 88,991 instances to support further research and development, extending the current focus on mathematical reasoning to diverse disciplines and domains. In addition, we propose the process-based multi-discipline verifier (i.e., BMMR-Verifier) for accurate and fine-grained evaluation of reasoning paths. Extensive experiments on 24 models reveal that (i) even SOTA models (e.g., o3 and Gemini-2.5-Pro) leave substantial headroom on BMMR-Eval; (ii) reasoning models exhibit discipline bias and outperform LMMs only on specific subjects; (iii) open-source models still trail their proprietary counterparts; and (iv) fine-tuning on BMMR-Train narrows this gap. Additionally, we conduct reasoning-chain analyses using BMMR-Verifier and other in-depth studies, uncovering the challenges LMMs currently face in multidisciplinary reasoning. We will release the data, and we hope our work can offer insights and contributions to the community.",
            "score": 11,
            "issue_id": 4693,
            "pub_date": "2025-07-04",
            "pub_date_card": {
                "ru": "4 июля",
                "en": "July 4",
                "zh": "7月4日"
            },
            "hash": "a916ca78a2bd6196",
            "authors": [
                "Zhiheng Xi",
                "Guanyu Li",
                "Yutao Fan",
                "Honglin Guo",
                "Yufang Liu",
                "Xiaoran Fan",
                "Jiaqi Liu",
                "Jingchao Ding",
                "Wangmeng Zuo",
                "Zhenfei Yin",
                "Lei Bai",
                "Tao Ji",
                "Tao Gui",
                "Qi Zhang",
                "Xuanjing Huang"
            ],
            "affiliations": [
                "East China Normal University",
                "Fudan University",
                "Harbin Institute of Technology",
                "Oxford",
                "Shanghai AI Laboratory",
                "University of Sydney",
                "Yimudata"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.03483.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#reasoning",
                    "#dataset",
                    "#benchmark",
                    "#open_source",
                    "#data"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Новый инструмент для оценки мультидисциплинарных рассуждений ИИ",
                    "desc": "Статья представляет BMMR - масштабный двуязычный мультимодальный датасет для оценки рассуждений в различных дисциплинах. Он содержит 110 тысяч вопросов университетского уровня по 300 предметам, включая тесты, вопросы с открытым ответом и заполнение пропусков. Авторы также предлагают BMMR-Verifier для точной оценки цепочек рассуждений моделей. Эксперименты показывают, что даже современные модели оставляют значительный простор для улучшений в мультидисциплинарных рассуждениях."
                },
                "en": {
                    "title": "Enhancing Multimodal Reasoning with BMMR Dataset",
                    "desc": "This paper presents BMMR, a comprehensive dataset designed to enhance the reasoning abilities of large multimodal models (LMMs) across various disciplines. It includes 110,000 college-level questions from 300 subjects, formatted in multiple-choice, fill-in-the-blank, and open-ended styles, sourced from both print and digital media. The dataset is divided into BMMR-Eval for evaluation and BMMR-Train for training, with a focus on improving reasoning in diverse domains beyond just mathematics. Additionally, the authors introduce BMMR-Verifier, a tool for detailed assessment of reasoning paths, revealing significant gaps in current models' performance and highlighting the need for further research in multidisciplinary reasoning."
                },
                "zh": {
                    "title": "推动多模态模型的跨学科推理能力",
                    "desc": "本文介绍了BMMR，一个大规模的双语、多模态、多学科推理数据集，旨在帮助开发和评估大型多模态模型（LMMs）。该数据集包含110,000个大学水平的问题，涵盖300个联合国教科文组织定义的学科，问题形式多样，包括选择题、填空题和开放式问答。数据经过人工筛选和过滤，并为每个实例配备高质量的推理路径，分为BMMR-Eval和BMMR-Train两部分，以支持多学科知识和推理的评估与研究。我们还提出了基于过程的多学科验证器（BMMR-Verifier），用于对推理路径进行准确和细致的评估。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.03253",
            "title": "RefineX: Learning to Refine Pre-training Data at Scale from\n  Expert-Guided Programs",
            "url": "https://huggingface.co/papers/2507.03253",
            "abstract": "RefineX is a scalable framework for improving the quality of large language model pre-training data through programmatic editing, yielding better performance than alternative methods across various downstream tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t The foundational capabilities of large language models (LLMs) are deeply influenced by the quality of their pre-training corpora. However, enhancing data quality at scale remains a significant challenge, primarily due to the trade-off between refinement effectiveness and processing efficiency. While rule-based filtering remains the dominant paradigm, it typically operates at the document level and lacks the granularity needed to refine specific content within documents. Inspired by emerging work such as ProX, we propose RefineX, a novel framework for large-scale, surgical refinement of pre-training data through programmatic editing tasks. RefineX enables efficient and fine-grained data refinement while reliably preserving the diversity and naturalness of raw text. The core strength of RefineX lies in distilling high-quality, expert-guided end-to-end refinement results into minimal edit-based deletion programs. This high-precision distillation pipeline is used to train an efficient and reliable refine model that can systematically improve every instance in the corpus at scale. We evaluate RefineX across from-scratch pre-training at multiple model scales and find that it consistently outperforms models trained on raw, filtered, or alternatively refined data across diverse downstream tasks. On the 750M model, RefineX yields 2.6%-7.2% average gains on lighteval tasks, and achieves comparable performance using significantly fewer training tokens. Further analysis shows that RefineX reliably enhances text quality with both high efficiency and precision, outperforming prior approaches such as end-to-end generation and Prox-C. These results position RefineX as a scalable, effective, and reliable solution for optimizing pre-training data in modern LLM pipelines.",
            "score": 11,
            "issue_id": 4693,
            "pub_date": "2025-07-04",
            "pub_date_card": {
                "ru": "4 июля",
                "en": "July 4",
                "zh": "7月4日"
            },
            "hash": "6f3d1aa17a4188e7",
            "authors": [
                "Baolong Bi",
                "Shenghua Liu",
                "Xingzhang Ren",
                "Dayiheng Liu",
                "Junyang Lin",
                "Yiwei Wang",
                "Lingrui Mei",
                "Junfeng Fang",
                "Jiafeng Guo",
                "Xueqi Cheng"
            ],
            "affiliations": [
                "Alibaba Group",
                "Institute of Computing Technology, Chinese Academy of Sciences",
                "National University of Singapore",
                "University of California, Merced"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.03253.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#data"
                ],
                "emoji": "✂️",
                "ru": {
                    "title": "RefineX: хирургическая точность в улучшении данных для ИИ",
                    "desc": "RefineX - это масштабируемый фреймворк для улучшения качества данных предобучения больших языковых моделей путем программного редактирования. Он позволяет эффективно и точечно улучшать качество данных, сохраняя при этом разнообразие и естественность исходного текста. RefineX обучает модель уточнения, которая может систематически улучшать каждый экземпляр в корпусе в масштабе. Эксперименты показывают, что модели, обученные на данных, улучшенных с помощью RefineX, превосходят модели, обученные на необработанных, отфильтрованных или альтернативно улучшенных данных по различным задачам."
                },
                "en": {
                    "title": "RefineX: Precision Editing for Superior Language Model Training",
                    "desc": "RefineX is a new framework designed to enhance the quality of pre-training data for large language models (LLMs) through targeted programmatic editing. It addresses the challenge of improving data quality at scale by allowing for precise modifications rather than broad document-level changes. This method preserves the diversity and naturalness of the text while ensuring efficient processing. Evaluations show that models trained with RefineX consistently outperform those trained on raw or traditionally refined data across various tasks, demonstrating its effectiveness in optimizing pre-training data."
                },
                "zh": {
                    "title": "RefineX：提升预训练数据质量的可扩展框架",
                    "desc": "RefineX是一个可扩展的框架，旨在通过程序化编辑提高大型语言模型预训练数据的质量。该框架解决了数据质量提升与处理效率之间的权衡问题，能够进行高效且细致的数据精炼。RefineX通过最小化编辑的删除程序，提炼出高质量的专家指导的端到端精炼结果，从而系统性地改善语料库中的每个实例。实验表明，RefineX在多个下游任务中表现优于使用原始、过滤或其他精炼数据训练的模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.04952",
            "title": "ArtifactsBench: Bridging the Visual-Interactive Gap in LLM Code\n  Generation Evaluation",
            "url": "https://huggingface.co/papers/2507.04952",
            "abstract": "ArtifactsBench, a novel benchmark and evaluation framework, automates the assessment of visual code generation quality using temporal screenshots and a multimodal language model judge.  \t\t\t\t\tAI-generated summary \t\t\t\t The generative capabilities of Large Language Models (LLMs) are rapidly expanding from static code to dynamic, interactive visual artifacts. This progress is bottlenecked by a critical evaluation gap: established benchmarks focus on algorithmic correctness and are blind to the visual fidelity and interactive integrity that define modern user experiences. To bridge this gap, we introduce ArtifactsBench, a new benchmark and paradigm for the automated, multimodal evaluation of visual code generation. Our framework programmatically renders each generated artifact and captures its dynamic behavior through temporal screenshots. This visual evidence, alongside the source code, is then assessed by a Multimodal LLM (MLLM)-as-Judge, which is rigorously guided by a fine-grained, per-task checklist to ensure holistic and reproducible scoring. We construct a new benchmark of 1,825 diverse tasks and evaluate over 30 leading LLMs. Our automated evaluation achieves a striking 94.4% ranking consistency with WebDev Arena, the gold-standard for human preference in web development, and over 90% pairwise agreement with human experts. This establishes ArtifactsBench as the first framework to reliably automate the assessment of human-perceived quality at scale. Our analysis provides a high-resolution map of the current SOTA, revealing that generalist models often outperform domain-specific ones. We open-source ArtifactsBench, including the benchmark, evaluation harness, and baseline results at https://artifactsbenchmark.github.io/, to provide the community with a scalable and accurate tool to accelerate the development of user-centric generative models.",
            "score": 3,
            "issue_id": 4695,
            "pub_date": "2025-07-07",
            "pub_date_card": {
                "ru": "7 июля",
                "en": "July 7",
                "zh": "7月7日"
            },
            "hash": "8eed78adf2fbda3a",
            "pdf_title_img": "assets/pdf/title_img/2507.04952.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#games",
                    "#open_source"
                ],
                "emoji": "🖥️",
                "ru": {
                    "title": "Автоматизированная оценка визуального кода с помощью мультимодальных языковых моделей",
                    "desc": "ArtifactsBench - это новый фреймворк для автоматизированной оценки качества генерации визуального кода. Он использует временные скриншоты и мультимодальную языковую модель в качестве судьи для оценки. Фреймворк включает в себя бенчмарк из 1825 разнообразных задач и оценивает более 30 ведущих языковых моделей. ArtifactsBench достигает 94.4% согласованности ранжирования с золотым стандартом WebDev Arena и более 90% попарного согласия с экспертами-людьми."
                },
                "en": {
                    "title": "Automating Quality Assessment in Visual Code Generation",
                    "desc": "ArtifactsBench is a new framework designed to evaluate the quality of visual code generation by using temporal screenshots and a multimodal language model as a judge. It addresses the limitations of existing benchmarks that only focus on algorithmic correctness, ignoring the visual and interactive aspects crucial for user experiences. By programmatically rendering artifacts and capturing their dynamic behavior, ArtifactsBench provides a comprehensive assessment through a detailed checklist. The framework has been tested on 1,825 tasks and shows high consistency with human evaluations, making it a valuable tool for improving generative models in web development."
                },
                "zh": {
                    "title": "ArtifactsBench：自动化视觉代码生成评估的新标准",
                    "desc": "ArtifactsBench是一个新颖的基准和评估框架，旨在自动评估视觉代码生成的质量。它通过时间截图和多模态语言模型评判，捕捉生成的视觉工件的动态行为。该框架使用细致的任务清单来确保评估的全面性和可重复性，并构建了一个包含1825个多样化任务的新基准。我们的自动评估与人类专家的评分高度一致，标志着ArtifactsBench成为可靠的自动化评估人类感知质量的首个框架。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.04590",
            "title": "VLM2Vec-V2: Advancing Multimodal Embedding for Videos, Images, and\n  Visual Documents",
            "url": "https://huggingface.co/papers/2507.04590",
            "abstract": "A unified framework VLM2Vec-V2 is proposed for learning embeddings across diverse visual forms such as videos and documents, demonstrating strong performance on new tasks and improving upon existing benchmarks for images.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal embedding models have been crucial in enabling various downstream tasks such as semantic similarity, information retrieval, and clustering over different modalities. However, existing multimodal embeddings like VLM2Vec, E5-V, GME are predominantly focused on natural images, with limited support for other visual forms such as videos and visual documents. This restricts their applicability in real-world scenarios, including AI agents, multi-modal search and recommendation, and retrieval-augmented generation (RAG). To close this gap, we propose VLM2Vec-V2, a unified framework for learning embeddings across diverse visual forms. First, we introduce MMEB-V2, a comprehensive benchmark that extends MMEB with five new task types: visual document retrieval, video retrieval, temporal grounding, video classification and video question answering - spanning text, image, video, and visual document inputs. Next, we train VLM2Vec-V2, a general-purpose embedding model that supports text, image, video, and visual document inputs. Extensive experiments show that VLM2Vec-V2 achieves strong performance not only on the newly introduced video and document retrieval tasks, but also improves over prior baselines on the original image benchmarks. Through extensive evaluation, our study offers insights into the generalizability of various multimodal embedding models and highlights effective strategies for unified embedding learning, laying the groundwork for more scalable and adaptable representation learning in both research and real-world settings.",
            "score": 1,
            "issue_id": 4694,
            "pub_date": "2025-07-07",
            "pub_date_card": {
                "ru": "7 июля",
                "en": "July 7",
                "zh": "7月7日"
            },
            "hash": "a417297c3b4c5459",
            "authors": [
                "Rui Meng",
                "Ziyan Jiang",
                "Ye Liu",
                "Mingyi Su",
                "Xinyi Yang",
                "Yuepeng Fu",
                "Can Qin",
                "Zeyuan Chen",
                "Ran Xu",
                "Caiming Xiong",
                "Yingbo Zhou",
                "Wenhu Chen",
                "Semih Yavuz"
            ],
            "affiliations": [
                "Salesforce Research",
                "Tsinghua University",
                "UC Santa Barbara",
                "University of Waterloo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.04590.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#rag",
                    "#survey",
                    "#benchmark",
                    "#transfer_learning",
                    "#multimodal"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Единая модель эмбеддингов для всех визуальных форматов",
                    "desc": "VLM2Vec-V2 - это унифицированная система для создания эмбеддингов различных визуальных форматов, включая видео и документы. Модель демонстрирует высокую эффективность на новых задачах и превосходит существующие бенчмарки для изображений. VLM2Vec-V2 обучена на расширенном наборе данных MMEB-V2, который включает задачи поиска визуальных документов, поиска видео, временной привязки, классификации видео и ответов на вопросы по видео. Эксперименты показывают, что модель обобщается на различные мультимодальные задачи и закладывает основу для более масштабируемого и адаптивного обучения представлений."
                },
                "en": {
                    "title": "Unified Embeddings for All Visual Forms!",
                    "desc": "The paper introduces VLM2Vec-V2, a new framework designed to learn embeddings for various visual forms, including videos and documents. This model enhances the capabilities of existing multimodal embedding models, which have primarily focused on natural images. By establishing a comprehensive benchmark called MMEB-V2, the authors evaluate the model on new tasks such as video retrieval and visual document retrieval. The results show that VLM2Vec-V2 not only excels in these new tasks but also outperforms previous models on traditional image benchmarks, demonstrating its versatility and effectiveness in real-world applications."
                },
                "zh": {
                    "title": "统一多模态嵌入学习的新框架",
                    "desc": "本文提出了一种统一框架VLM2Vec-V2，用于学习多种视觉形式（如视频和文档）的嵌入。该模型在新任务上表现出色，并在图像的现有基准上有所提升。我们引入了MMEB-V2基准，扩展了五种新任务类型，包括视觉文档检索和视频分类等。通过广泛的实验，VLM2Vec-V2展示了其在多模态嵌入学习中的强大能力，为未来的研究和实际应用奠定了基础。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.03336",
            "title": "Disambiguation-Centric Finetuning Makes Enterprise Tool-Calling LLMs\n  More Realistic and Less Risky",
            "url": "https://huggingface.co/papers/2507.03336",
            "abstract": "DiaFORGE is a disambiguation framework that enhances large language models' ability to invoke enterprise APIs accurately through dialogue synthesis, supervised fine-tuning, and real-world evaluation.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are increasingly tasked with invoking enterprise APIs, yet they routinely falter when near-duplicate tools vie for the same user intent or when required arguments are left underspecified. We introduce DiaFORGE (Dialogue Framework for Organic Response Generation & Evaluation), a disambiguation-centric, three-stage pipeline that (i) synthesizes persona-driven, multi-turn dialogues in which the assistant must distinguish among highly similar tools, (ii) performs supervised fine-tuning of open-source models with reasoning traces across 3B - 70B parameters, and (iii) evaluates real-world readiness via a dynamic suite that redeploys each model in a live agentic loop and reports end-to-end goal completion alongside conventional static metrics. On our dynamic benchmark DiaBENCH, models trained with DiaFORGE raise tool-invocation success by 27 pp over GPT-4o and by 49 pp over Claude-3.5-Sonnet, both under optimized prompting. To spur further research, we release an open corpus of 5000 production-grade enterprise API specifications paired with rigorously validated, disambiguation-focused dialogues, offering a practical blueprint for building reliable, enterprise-ready tool-calling agents.",
            "score": 1,
            "issue_id": 4694,
            "pub_date": "2025-07-04",
            "pub_date_card": {
                "ru": "4 июля",
                "en": "July 4",
                "zh": "7月4日"
            },
            "hash": "a22f17539601dde1",
            "authors": [
                "Ashutosh Hathidara",
                "Julien Yu",
                "Sebastian Schreiber"
            ],
            "affiliations": [
                "SAP Labs"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.03336.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#open_source",
                    "#dataset",
                    "#training",
                    "#data",
                    "#alignment",
                    "#benchmark",
                    "#agents"
                ],
                "emoji": "🔧",
                "ru": {
                    "title": "DiaFORGE: точные вызовы API через синтез диалогов и дообучение LLM",
                    "desc": "DiaFORGE - это фреймворк для улучшения способности больших языковых моделей (LLM) точно вызывать корпоративные API через синтез диалогов и дообучение с учителем. Он включает трехэтапный процесс: синтез диалогов, дообучение моделей и оценку готовности к реальному использованию. На тестовом наборе DiaBENCH модели, обученные с помощью DiaFORGE, повышают успешность вызова инструментов на 27 процентных пунктов по сравнению с GPT-4 и на 49 пунктов по сравнению с Claude-3.5-Sonnet. Авторы также выпустили открытый корпус из 5000 корпоративных API-спецификаций с проверенными диалогами для дальнейших исследований."
                },
                "en": {
                    "title": "Empowering LLMs to Accurately Invoke APIs with DiaFORGE",
                    "desc": "DiaFORGE is a framework designed to improve how large language models (LLMs) interact with enterprise APIs by resolving ambiguities in user requests. It consists of a three-stage process that includes generating multi-turn dialogues to help the model differentiate between similar tools, fine-tuning the model with supervised learning using reasoning traces, and evaluating the model's performance in real-world scenarios. The results show that models trained with DiaFORGE significantly outperform existing models like GPT-4o and Claude-3.5-Sonnet in successfully invoking tools. Additionally, DiaFORGE provides a valuable resource by releasing a corpus of enterprise API specifications and validated dialogues to aid future research."
                },
                "zh": {
                    "title": "提升API调用准确性的对话框架",
                    "desc": "DiaFORGE是一个消歧义框架，旨在提高大型语言模型在对话中准确调用企业API的能力。该框架包括三个阶段：首先合成以角色为驱动的多轮对话，帮助助手区分相似工具；其次对开源模型进行监督微调，利用3B到70B参数的推理轨迹；最后通过动态评估套件测试模型在真实环境中的表现。通过DiaFORGE训练的模型在工具调用成功率上比GPT-4o提高了27个百分点，比Claude-3.5-Sonnet提高了49个百分点。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.04036",
            "title": "PresentAgent: Multimodal Agent for Presentation Video Generation",
            "url": "https://huggingface.co/papers/2507.04036",
            "abstract": "A multimodal agent transforms documents into detailed presentation videos with audio, evaluated using a comprehensive framework involving vision-language models.  \t\t\t\t\tAI-generated summary \t\t\t\t We present PresentAgent, a multimodal agent that transforms long-form documents into narrated presentation videos. While existing approaches are limited to generating static slides or text summaries, our method advances beyond these limitations by producing fully synchronized visual and spoken content that closely mimics human-style presentations. To achieve this integration, PresentAgent employs a modular pipeline that systematically segments the input document, plans and renders slide-style visual frames, generates contextual spoken narration with large language models and Text-to-Speech models, and seamlessly composes the final video with precise audio-visual alignment. Given the complexity of evaluating such multimodal outputs, we introduce PresentEval, a unified assessment framework powered by Vision-Language Models that comprehensively scores videos across three critical dimensions: content fidelity, visual clarity, and audience comprehension through prompt-based evaluation. Our experimental validation on a curated dataset of 30 document-presentation pairs demonstrates that PresentAgent approaches human-level quality across all evaluation metrics. These results highlight the significant potential of controllable multimodal agents in transforming static textual materials into dynamic, effective, and accessible presentation formats. Code will be available at https://github.com/AIGeeksGroup/PresentAgent.",
            "score": 0,
            "issue_id": 4693,
            "pub_date": "2025-07-05",
            "pub_date_card": {
                "ru": "5 июля",
                "en": "July 5",
                "zh": "7月5日"
            },
            "hash": "79b10f5eed3bd7e4",
            "authors": [
                "Jingwei Shi",
                "Zeyu Zhang",
                "Biao Wu",
                "Yanjie Liang",
                "Meng Fang",
                "Ling Chen",
                "Yang Zhao"
            ],
            "affiliations": [
                "AI Geeks, Australia",
                "Australian Artificial Intelligence Institute, Australia",
                "La Trobe University, Australia",
                "University of Liverpool, United Kingdom"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.04036.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#multimodal",
                    "#agents",
                    "#optimization",
                    "#dataset",
                    "#benchmark",
                    "#games",
                    "#interpretability"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Искусственный интеллект создает презентации на уровне человека",
                    "desc": "Статья представляет PresentAgent - мультимодального агента, преобразующего длинные документы в видеопрезентации с озвучкой. Система использует модульный конвейер для сегментации документа, создания слайдов, генерации речи и компоновки видео. Для оценки качества выходных данных авторы разработали фреймворк PresentEval на основе визуально-языковых моделей. Эксперименты показали, что PresentAgent приближается к уровню человека по всем метрикам оценки."
                },
                "en": {
                    "title": "Transforming Text into Engaging Videos with PresentAgent",
                    "desc": "PresentAgent is a multimodal agent designed to convert long documents into engaging presentation videos with synchronized audio. Unlike traditional methods that only create static slides or text summaries, this approach generates dynamic visual and spoken content that resembles human presentations. It utilizes a modular pipeline for document segmentation, slide rendering, and narration generation, ensuring high-quality audio-visual alignment. The effectiveness of PresentAgent is evaluated using PresentEval, a framework that assesses video quality based on content fidelity, visual clarity, and audience comprehension, demonstrating its potential to enhance the accessibility of information."
                },
                "zh": {
                    "title": "将文档转化为生动演示的智能体",
                    "desc": "本文介绍了一种名为PresentAgent的多模态智能体，它能够将长篇文档转化为带有旁白的演示视频。与现有方法仅能生成静态幻灯片或文本摘要不同，我们的方法能够生成与人类演示风格相似的同步视觉和语音内容。PresentAgent采用模块化流程，系统地对输入文档进行分段，规划和渲染幻灯片风格的视觉框架，并利用大型语言模型和文本转语音模型生成上下文相关的旁白。我们还提出了PresentEval评估框架，通过视觉-语言模型对视频进行全面评分，验证了PresentAgent在内容真实性、视觉清晰度和观众理解力等方面接近人类水平的质量。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.02659",
            "title": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding",
            "url": "https://huggingface.co/papers/2507.02659",
            "abstract": "OmniDraft, a unified framework, addresses cross-vocabulary mismatch and improves decoding speed by allowing a single draft model to interact dynamically with diverse target models in online settings.  \t\t\t\t\tAI-generated summary \t\t\t\t Speculative decoding generally dictates having a small, efficient draft model that is either pretrained or distilled offline to a particular target model series, for instance, Llama or Qwen models. However, within online deployment settings, there are two major challenges: 1) usage of a target model that is incompatible with the draft model; 2) expectation of latency improvements over usage and time. In this work, we propose OmniDraft, a unified framework that enables a single draft model to operate with any target model and adapt dynamically to user data. We introduce an online n-gram cache with hybrid distillation fine-tuning to address the cross-vocabulary mismatch across draft and target models; and further improve decoding speed by leveraging adaptive drafting techniques. OmniDraft is particularly suitable for on-device LLM applications where model cost, efficiency and user customization are the major points of contention. This further highlights the need to tackle the above challenges and motivates the ``one drafter for all'' paradigm. We showcase the proficiency of the OmniDraft framework by performing online learning on math reasoning, coding and text generation tasks. Notably, OmniDraft enables a single Llama-68M model to pair with various target models including Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding; and additionally provides up to 1.5-2x speedup.",
            "score": 0,
            "issue_id": 4694,
            "pub_date": "2025-07-03",
            "pub_date_card": {
                "ru": "3 июля",
                "en": "July 3",
                "zh": "7月3日"
            },
            "hash": "356734d41c5a5e65",
            "authors": [
                "Ramchalam Kinattinkara Ramakrishnan",
                "Zhaocong Yuan",
                "Shaojie Zhuo",
                "Chen Feng",
                "Yicheng Lin",
                "Chenzheng Su",
                "Xiaopeng Zhang"
            ],
            "affiliations": [
                "Qualcomm AI Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.02659.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#games",
                    "#inference",
                    "#training",
                    "#reasoning",
                    "#multimodal"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Один черновик для всех: ускорение и адаптация языковых моделей",
                    "desc": "OmniDraft - это унифицированная система для ускорения работы языковых моделей. Она решает проблему несоответствия словарей между черновой и целевой моделями, используя онлайн-кэш n-грамм и гибридную дистилляцию. OmniDraft позволяет одной черновой модели работать с различными целевыми моделями и адаптироваться к пользовательским данным. Система особенно подходит для LLM-приложений на устройствах, где важны эффективность и кастомизация."
                },
                "en": {
                    "title": "One Draft Model for All Target Models!",
                    "desc": "OmniDraft is a new framework designed to solve the problem of cross-vocabulary mismatch between draft models and target models in machine learning applications. It allows a single draft model to work with different target models dynamically, improving decoding speed and efficiency. The framework uses an online n-gram cache and hybrid distillation fine-tuning to adapt to user data and enhance performance. OmniDraft is particularly beneficial for on-device large language model (LLM) applications, where it can significantly reduce latency and improve user customization."
                },
                "zh": {
                    "title": "一个草稿模型，适配所有目标模型",
                    "desc": "OmniDraft是一个统一框架，旨在解决跨词汇不匹配问题，并通过允许单一草稿模型与多种目标模型动态交互来提高解码速度。该框架特别适用于在线部署环境，能够使单一草稿模型与任何目标模型兼容，并根据用户数据动态调整。通过引入在线n-gram缓存和混合蒸馏微调，OmniDraft有效解决了草稿模型与目标模型之间的词汇不匹配问题。该方法在数学推理、编码和文本生成任务中表现出色，显著提高了解码效率。"
                }
            }
        }
    ],
    "link_prev": "2025-07-07.html",
    "link_next": "2025-07-09.html",
    "link_month": "2025-07.html",
    "short_date_prev": {
        "ru": "07.07",
        "en": "07/07",
        "zh": "7月7日"
    },
    "short_date_next": {
        "ru": "09.07",
        "en": "07/09",
        "zh": "7月9日"
    },
    "categories": {
        "#dataset": 3,
        "#data": 4,
        "#benchmark": 5,
        "#agents": 2,
        "#cv": 1,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 2,
        "#plp": 0,
        "#inference": 1,
        "#3d": 1,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 5,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 5,
        "#robotics": 0,
        "#agi": 1,
        "#games": 4,
        "#interpretability": 1,
        "#reasoning": 2,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 7,
        "#survey": 1,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}