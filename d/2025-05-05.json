{
    "date": {
        "ru": "5 Ğ¼Ğ°Ñ",
        "en": "May 5",
        "zh": "5æœˆ5æ—¥"
    },
    "time_utc": "2025-05-05 05:12",
    "weekday": 0,
    "issue_id": 3582,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2505.01079",
            "title": "Improving Editability in Image Generation with Layer-wise Memory",
            "url": "https://huggingface.co/papers/2505.01079",
            "abstract": "Most real-world image editing tasks require multiple sequential edits to achieve desired results. Current editing approaches, primarily designed for single-object modifications, struggle with sequential editing: especially with maintaining previous edits along with adapting new objects naturally into the existing content. These limitations significantly hinder complex editing scenarios where multiple objects need to be modified while preserving their contextual relationships. We address this fundamental challenge through two key proposals: enabling rough mask inputs that preserve existing content while naturally integrating new elements and supporting consistent editing across multiple modifications. Our framework achieves this through layer-wise memory, which stores latent representations and prompt embeddings from previous edits. We propose Background Consistency Guidance that leverages memorized latents to maintain scene coherence and Multi-Query Disentanglement in cross-attention that ensures natural adaptation to existing content. To evaluate our method, we present a new benchmark dataset incorporating semantic alignment metrics and interactive editing scenarios. Through comprehensive experiments, we demonstrate superior performance in iterative image editing tasks with minimal user effort, requiring only rough masks while maintaining high-quality results throughout multiple editing steps.",
            "score": 2,
            "issue_id": 3582,
            "pub_date": "2025-05-02",
            "pub_date_card": {
                "ru": "2 Ğ¼Ğ°Ñ",
                "en": "May 2",
                "zh": "5æœˆ2æ—¥"
            },
            "hash": "e1aa83ea7926943e",
            "authors": [
                "Daneul Kim",
                "Jaeah Lee",
                "Jaesik Park"
            ],
            "affiliations": [
                "Seoul National University, Republic of Korea"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.01079.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#cv"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹: ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ¿Ñ€Ğ¾ÑˆĞ»Ğ¾Ğµ, Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ½Ğ¾Ğ²Ğ¾Ğµ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ°ÑĞ¾Ğº, Ğ¿Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½ÑƒÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸Ğº Background Consistency Guidance Ğ¸ Multi-Query Disentanglement. Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ÑÑ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Seamless Sequential Image Editing with Context Preservation",
                    "desc": "This paper addresses the challenges of sequential image editing, where multiple edits are needed while keeping previous changes intact. Current methods struggle with integrating new objects into existing images without disrupting the overall context. The authors propose a framework that uses layer-wise memory to store previous edits and ensure consistency across modifications. Their approach includes Background Consistency Guidance and Multi-Query Disentanglement to enhance the natural integration of new elements, leading to improved performance in complex editing tasks with minimal user input."
                },
                "zh": {
                    "title": "å®ç°è‡ªç„¶è¿ç»­çš„å›¾åƒç¼–è¾‘",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†å›¾åƒç¼–è¾‘ä¸­çš„å¤šæ¬¡è¿ç»­ç¼–è¾‘é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤šä¸ªå¯¹è±¡çš„ä¿®æ”¹æ—¶å­˜åœ¨å›°éš¾ï¼Œå°¤å…¶æ˜¯åœ¨ä¿æŒä¹‹å‰ç¼–è¾‘å†…å®¹çš„åŒæ—¶è‡ªç„¶åœ°èå…¥æ–°å¯¹è±¡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤é¡¹å…³é”®æ–¹æ¡ˆï¼šä¸€æ˜¯æ”¯æŒç²—ç•¥çš„æ©è†œè¾“å…¥ï¼Œä»¥ä¿ç•™ç°æœ‰å†…å®¹å¹¶è‡ªç„¶æ•´åˆæ–°å…ƒç´ ï¼›äºŒæ˜¯æ”¯æŒå¤šæ¬¡ä¿®æ”¹çš„ä¸€è‡´æ€§ç¼–è¾‘ã€‚æˆ‘ä»¬çš„æ¡†æ¶é€šè¿‡å±‚çº§è®°å¿†å­˜å‚¨å…ˆå‰ç¼–è¾‘çš„æ½œåœ¨è¡¨ç¤ºå’Œæç¤ºåµŒå…¥ï¼Œåˆ©ç”¨èƒŒæ™¯ä¸€è‡´æ€§å¼•å¯¼ä¿æŒåœºæ™¯çš„è¿è´¯æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è¿­ä»£å›¾åƒç¼–è¾‘ä»»åŠ¡ä¸­è¡¨ç°ä¼˜è¶Šï¼Œç”¨æˆ·åªéœ€æä¾›ç²—ç•¥æ©è†œå³å¯å®ç°é«˜è´¨é‡çš„ç¼–è¾‘æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.00023",
            "title": "CORG: Generating Answers from Complex, Interrelated Contexts",
            "url": "https://huggingface.co/papers/2505.00023",
            "abstract": "In a real-world corpus, knowledge frequently recurs across documents but often contains inconsistencies due to ambiguous naming, outdated information, or errors, leading to complex interrelationships between contexts. Previous research has shown that language models struggle with these complexities, typically focusing on single factors in isolation. We classify these relationships into four types: distracting, ambiguous, counterfactual, and duplicated. Our analysis reveals that no single approach effectively addresses all these interrelationships simultaneously. Therefore, we introduce Context Organizer (CORG), a framework that organizes multiple contexts into independently processed groups. This design allows the model to efficiently find all relevant answers while ensuring disambiguation. CORG consists of three key components: a graph constructor, a reranker, and an aggregator. Our results demonstrate that CORG balances performance and efficiency effectively, outperforming existing grouping methods and achieving comparable results to more computationally intensive, single-context approaches.",
            "score": 2,
            "issue_id": 3582,
            "pub_date": "2025-04-25",
            "pub_date_card": {
                "ru": "25 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 25",
                "zh": "4æœˆ25æ—¥"
            },
            "hash": "46da290a5c894311",
            "authors": [
                "Hyunji Lee",
                "Franck Dernoncourt",
                "Trung Bui",
                "Seunghyun Yoon"
            ],
            "affiliations": [
                "Adobe Research",
                "KAIST AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.00023.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#multimodal",
                    "#graphs",
                    "#architecture",
                    "#data"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "CORG: Ğ£Ğ¼Ğ½Ğ°Ñ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Context Organizer (CORG) Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾ÑĞ²ÑĞ·ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸ Ğ² ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ°Ñ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°. CORG Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñ‹ Ğ² Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğµ Ğ³Ñ€ÑƒĞ¿Ğ¿Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ²ÑĞµ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ñ‚Ñ€ĞµÑ… ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²: ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¾Ñ€Ğ° Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ², Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ° Ğ¸ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ‚Ğ¾Ñ€Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ CORG ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸."
                },
                "en": {
                    "title": "Organizing Contexts for Better Language Understanding",
                    "desc": "This paper addresses the challenges faced by language models when dealing with complex interrelationships in real-world data, which often contain inconsistencies. It categorizes these relationships into four types: distracting, ambiguous, counterfactual, and duplicated, highlighting that existing methods typically fail to handle them all at once. To tackle this issue, the authors propose a new framework called Context Organizer (CORG), which organizes contexts into separate groups for independent processing. CORG includes a graph constructor, a reranker, and an aggregator, and it demonstrates improved performance and efficiency compared to traditional methods."
                },
                "zh": {
                    "title": "ä¸Šä¸‹æ–‡ç»„ç»‡ï¼Œæå‡æ¨¡å‹æ•ˆç‡ä¸å‡†ç¡®æ€§",
                    "desc": "åœ¨ç°å®ä¸–ç•Œçš„è¯­æ–™åº“ä¸­ï¼ŒçŸ¥è¯†ç»å¸¸åœ¨æ–‡æ¡£ä¸­é‡å¤å‡ºç°ï¼Œä½†ç”±äºå‘½åæ¨¡ç³Šã€ä¿¡æ¯è¿‡æ—¶æˆ–é”™è¯¯ï¼Œå¯¼è‡´ä¸Šä¸‹æ–‡ä¹‹é—´å­˜åœ¨å¤æ‚çš„ç›¸äº’å…³ç³»ã€‚ä»¥å¾€çš„ç ”ç©¶è¡¨æ˜ï¼Œè¯­è¨€æ¨¡å‹åœ¨å¤„ç†è¿™äº›å¤æ‚æ€§æ—¶é€šå¸¸åªå…³æ³¨å•ä¸€å› ç´ ã€‚æˆ‘ä»¬å°†è¿™äº›å…³ç³»åˆ†ä¸ºå››ç§ç±»å‹ï¼šå¹²æ‰°ã€æ¨¡ç³Šã€åäº‹å®å’Œé‡å¤ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸Šä¸‹æ–‡ç»„ç»‡å™¨ï¼ˆCORGï¼‰ï¼Œå®ƒå°†å¤šä¸ªä¸Šä¸‹æ–‡ç»„ç»‡æˆç‹¬ç«‹å¤„ç†çš„ç»„ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.00174",
            "title": "Real-World Gaps in AI Governance Research",
            "url": "https://huggingface.co/papers/2505.00174",
            "abstract": "Drawing on 1,178 safety and reliability papers from 9,439 generative AI papers (January 2020 - March 2025), we compare research outputs of leading AI companies (Anthropic, Google DeepMind, Meta, Microsoft, and OpenAI) and AI universities (CMU, MIT, NYU, Stanford, UC Berkeley, and University of Washington). We find that corporate AI research increasingly concentrates on pre-deployment areas -- model alignment and testing & evaluation -- while attention to deployment-stage issues such as model bias has waned. Significant research gaps exist in high-risk deployment domains, including healthcare, finance, misinformation, persuasive and addictive features, hallucinations, and copyright. Without improved observability into deployed AI, growing corporate concentration could deepen knowledge deficits. We recommend expanding external researcher access to deployment data and systematic observability of in-market AI behaviors.",
            "score": 1,
            "issue_id": 3582,
            "pub_date": "2025-04-30",
            "pub_date_card": {
                "ru": "30 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 30",
                "zh": "4æœˆ30æ—¥"
            },
            "hash": "7618edbafcee6b13",
            "authors": [
                "Ilan Strauss",
                "Isobel Moure",
                "Tim O'Reilly",
                "Sruly Rosenblat"
            ],
            "affiliations": [
                "AI Disclosures Project, Social Science Research Council",
                "Institute for Innovation and Public Purpose, University College London",
                "OReilly Media"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.00174.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#ethics",
                    "#alignment",
                    "#healthcare",
                    "#hallucinations",
                    "#data"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞšĞ¾Ñ€Ğ¿Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ˜Ğ˜: Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ² Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ 1178 Ñ€Ğ°Ğ±Ğ¾Ñ‚ Ğ¿Ğ¾ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ· 9439 ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ğ¿Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ˜Ğ˜ Ğ·Ğ° Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´ Ñ ÑĞ½Ğ²Ğ°Ñ€Ñ 2020 Ğ¿Ğ¾ Ğ¼Ğ°Ñ€Ñ‚ 2025 Ğ³Ğ¾Ğ´Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ¸Ñ‚ĞµÑ‚Ğ¾Ğ² Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ˜Ğ˜. ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ ĞºĞ¾Ñ€Ğ¿Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ˜Ğ˜ Ğ²ÑĞµ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ ĞºĞ¾Ğ½Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğº Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°Ğ¼ ÑÑ‚Ğ°Ğ¿Ğ° Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ğ¼ ĞºĞ°Ğº ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾ÑĞ»Ğ°Ğ±ĞµĞ²Ğ°ĞµÑ‚. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒÑÑ‚ Ñ€Ğ°ÑÑˆĞ¸Ñ€Ğ¸Ñ‚ÑŒ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğº Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğµ Ğ·Ğ° Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ˜Ğ˜ Ğ½Ğ° Ñ€Ñ‹Ğ½ĞºĞµ."
                },
                "en": {
                    "title": "Bridging the Gap: Enhancing AI Safety in Deployment",
                    "desc": "This paper analyzes the trends in safety and reliability research within generative AI by examining 1,178 papers from major AI companies and universities. It highlights a shift in focus towards pre-deployment concerns like model alignment and evaluation, while issues related to deployment, such as model bias, are receiving less attention. The authors identify critical research gaps in high-risk areas like healthcare and finance, where the implications of AI deployment can be significant. They advocate for better access to deployment data and enhanced observability of AI systems in real-world applications to address these gaps."
                },
                "zh": {
                    "title": "å…³æ³¨äººå·¥æ™ºèƒ½éƒ¨ç½²é˜¶æ®µçš„ç ”ç©¶ç¼ºå£",
                    "desc": "æœ¬ç ”ç©¶åˆ†æäº†1178ç¯‡å®‰å…¨æ€§å’Œå¯é æ€§è®ºæ–‡ä¸9439ç¯‡ç”Ÿæˆå¼äººå·¥æ™ºèƒ½è®ºæ–‡ï¼Œæ¯”è¾ƒäº†ä¸»è¦äººå·¥æ™ºèƒ½å…¬å¸å’Œå¤§å­¦çš„ç ”ç©¶æˆæœã€‚ç ”ç©¶å‘ç°ï¼Œä¼ä¸šçš„äººå·¥æ™ºèƒ½ç ”ç©¶è¶Šæ¥è¶Šé›†ä¸­åœ¨æ¨¡å‹å¯¹é½å’Œæµ‹è¯•è¯„ä¼°ç­‰é¢„éƒ¨ç½²é¢†åŸŸï¼Œè€Œå¯¹éƒ¨ç½²é˜¶æ®µé—®é¢˜å¦‚æ¨¡å‹åè§çš„å…³æ³¨æœ‰æ‰€å‡å°‘ã€‚é«˜é£é™©éƒ¨ç½²é¢†åŸŸï¼ˆå¦‚åŒ»ç–—ã€é‡‘èã€è™šå‡ä¿¡æ¯ç­‰ï¼‰å­˜åœ¨æ˜¾è‘—çš„ç ”ç©¶ç©ºç™½ã€‚ä¸ºäº†æ”¹å–„å¯¹å·²éƒ¨ç½²äººå·¥æ™ºèƒ½çš„å¯è§‚å¯Ÿæ€§ï¼Œå»ºè®®æ‰©å¤§å¤–éƒ¨ç ”ç©¶äººå‘˜å¯¹éƒ¨ç½²æ•°æ®çš„è®¿é—®ï¼Œå¹¶ç³»ç»ŸåŒ–å¸‚åœºä¸­äººå·¥æ™ºèƒ½è¡Œä¸ºçš„å¯è§‚å¯Ÿæ€§ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-05-02.html",
    "link_next": "2025-05-06.html",
    "link_month": "2025-05.html",
    "short_date_prev": {
        "ru": "02.05",
        "en": "05/02",
        "zh": "5æœˆ2æ—¥"
    },
    "short_date_next": {
        "ru": "06.05",
        "en": "05/06",
        "zh": "5æœˆ6æ—¥"
    },
    "categories": {
        "#dataset": 0,
        "#data": 2,
        "#benchmark": 2,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 1,
        "#training": 0,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 1,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°å…´æŠ€æœ¯ï¼šäº’åŠ¨ç”Ÿæˆè§†é¢‘ï¼ˆIGVï¼‰ã€‚å®ƒç»“åˆç”Ÿæˆå’Œäº’åŠ¨åŠŸèƒ½ï¼Œäº§ç”Ÿé«˜è´¨é‡è§†é¢‘å†…å®¹ï¼Œå¹¶è®©ç”¨æˆ·é€šè¿‡æ§åˆ¶ä¿¡å·å’Œå“åº”åé¦ˆå‚ä¸å…¶ä¸­ã€‚IGVåœ¨æ¸¸æˆã€äººå·¥æ™ºèƒ½å’Œè‡ªåŠ¨é©¾é©¶ä¸‰å¤§é¢†åŸŸæœ‰é‡è¦åº”ç”¨ã€‚æ–‡ç« è¿˜æå‡ºäº†ç†æƒ³IGVç³»ç»Ÿçš„äº”ä¸ªå…³é”®æ¨¡å—ï¼Œå¹¶åˆ†æäº†æ¯ä¸ªæ¨¡å—çš„æŠ€æœ¯æŒ‘æˆ˜å’Œæœªæ¥æ–¹å‘ã€‚",
        "title": "A Survey of Interactive Generative Video",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°å…´æŠ€æœ¯ï¼šäº’åŠ¨ç”Ÿæˆè§†é¢‘ï¼ˆIGVï¼‰ã€‚å®ƒç»“åˆç”Ÿæˆå’Œäº’åŠ¨åŠŸèƒ½ï¼Œäº§ç”Ÿé«˜è´¨é‡è§†é¢‘å†…å®¹ï¼Œå¹¶è®©ç”¨æˆ·é€šè¿‡æ§åˆ¶ä¿¡å·å’Œå“åº”åé¦ˆå‚ä¸å…¶ä¸­ã€‚IGVåœ¨æ¸¸æˆã€äººå·¥æ™ºèƒ½å’Œè‡ªåŠ¨é©¾é©¶ä¸‰å¤§é¢†åŸŸæœ‰é‡è¦åº”ç”¨ã€‚æ–‡ç« è¿˜æå‡ºäº†ç†æƒ³IGVç³»ç»Ÿçš„äº”ä¸ªå…³é”®æ¨¡å—ï¼Œå¹¶åˆ†æäº†æ¯ä¸ªæ¨¡å—çš„æŠ€æœ¯æŒ‘æˆ˜å’Œæœªæ¥æ–¹å‘ã€‚\n\nZhÃ¨ piÄn wÃ©nzhÄng jiÃ¨shÃ o le yÄ« zhÇ’ng xÄ«nxÄ«ng jÃ¬shÃ¹: hÃ¹dÃ²ng shÄ“ngchÃ©ng shÃ¬pÃ­n (IGV). TÄ jiÃ©hÃ© shÄ“ngchÃ©ng hÃ© hÃ¹dÃ²ng gÅngnÃ©ng, chÇnshÄ“ng gÄo zhÃ¬liÃ ng shÃ¬pÃ­n nÃ¨irÃ³ng, bÃ¬ng rÃ ng yÃ²nghÃ¹ tÅngguÃ² kÃ²ngzhÃ¬ xÃ¬nhÃ o hÃ© xiÇngyÃ¬ng fÇnkuÃ¬ cÄnyÃ¹ qÃ­zhÅng. IGV zÃ i yÃ³uxÃ¬, rÃ©ngÅng zhÃ¬nÃ©ng hÃ© zÃ¬dÃ²ng jiÃ shÇ sÄn dÃ  lÇngyÃ¹ yÇ’u zhÃ²ngyÃ o yÃ¬ngyÃ²ng. WÃ©nzhÄng hÃ¡i tÃ­chÅ« le lÇxiÇng IGV xÃ¬tÇ’ng de wÇ” gÃ¨ guÇnjiÃ n mÃ³kuÃ i, bÃ¬ng fÄ“nxi le mÄ›i gÃ¨ mÃ³kuÃ i de jÃ¬shÃ¹ tiÇozhÃ n hÃ© wÃ¨ilÃ¡i fÄngxiÃ ng.",
        "vocab": "[\n    {\"word\": \"æ–°å…´\", \"pinyin\": \"xÄ«n xÄ«ng\", \"trans\": \"emerging\"},\n    {\"word\": \"äº’åŠ¨\", \"pinyin\": \"hÃ¹ dÃ²ng\", \"trans\": \"interactive\"},\n    {\"word\": \"ç”Ÿæˆ\", \"pinyin\": \"shÄ“ng chÃ©ng\", \"trans\": \"generate\"},\n    {\"word\": \"é«˜è´¨é‡\", \"pinyin\": \"gÄo zhÃ¬ liÃ ng\", \"trans\": \"high quality\"},\n    {\"word\": \"æ§åˆ¶\", \"pinyin\": \"kÃ²ng zhÃ¬\", \"trans\": \"control\"},\n    {\"word\": \"ä¿¡å·\", \"pinyin\": \"xÃ¬n hÃ o\", \"trans\": \"signal\"},\n    {\"word\": \"åé¦ˆ\", \"pinyin\": \"fÇn kuÃ¬\", \"trans\": \"feedback\"},\n    {\"word\": \"å‚ä¸\", \"pinyin\": \"cÄn yÃ¹\", \"trans\": \"participate\"},\n    {\"word\": \"äººå·¥æ™ºèƒ½\", \"pinyin\": \"rÃ©n gÅng zhÃ¬ nÃ©ng\", \"trans\": \"artificial intelligence\"},\n    {\"word\": \"è‡ªåŠ¨é©¾é©¶\", \"pinyin\": \"zÃ¬ dÃ²ng jiÃ  shÇ\", \"trans\": \"autonomous driving\"},\n    {\"word\": \"é¢†åŸŸ\", \"pinyin\": \"lÇng yÃ¹\", \"trans\": \"field\"},\n    {\"word\": \"é‡è¦\", \"pinyin\": \"zhÃ²ng yÃ o\", \"trans\": \"important\"},\n    {\"word\": \"åº”ç”¨\", \"pinyin\": \"yÃ¬ng yÃ²ng\", \"trans\": \"application\"},\n    {\"word\": \"ç†æƒ³\", \"pinyin\": \"lÇ xiÇng\", \"trans\": \"ideal\"},\n    {\"word\": \"ç³»ç»Ÿ\", \"pinyin\": \"xÃ¬ tÇ’ng\", \"trans\": \"system\"},\n    {\"word\": \"å…³é”®\", \"pinyin\": \"guÄn jiÃ n\", \"trans\": \"key\"},\n    {\"word\": \"æ¨¡å—\", \"pinyin\": \"mÃ³ kuÃ i\", \"trans\": \"module\"},\n    {\"word\": \"æŠ€æœ¯\", \"pinyin\": \"jÃ¬ shÃ¹\", \"trans\": \"technology\"},\n    {\"word\": \"æŒ‘æˆ˜\", \"pinyin\": \"tiÇo zhÃ n\", \"trans\": \"challenge\"},\n    {\"word\": \"æœªæ¥\", \"pinyin\": \"wÃ¨i lÃ¡i\", \"trans\": \"future\"},\n    {\"word\": \"æ–¹å‘\", \"pinyin\": \"fÄng xiÃ ng\", \"trans\": \"direction\"}\n]",
        "trans": "This article introduces an emerging technology: Interactive Generative Video (IGV). It combines generative and interactive functions to produce high-quality video content and allows users to participate through control signals and response feedback. IGV has important applications in three major areas: gaming, artificial intelligence, and autonomous driving. The article also proposes five key modules for an ideal IGV system and analyzes the technical challenges and future directions for each module.",
        "update_ts": "2025-05-04 12:44"
    }
}