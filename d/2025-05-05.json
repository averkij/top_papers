{
    "date": {
        "ru": "5 Ğ¼Ğ°Ñ",
        "en": "May 5",
        "zh": "5æœˆ5æ—¥"
    },
    "time_utc": "2025-05-05 16:14",
    "weekday": 0,
    "issue_id": 3593,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.20438",
            "title": "PixelHacker: Image Inpainting with Structural and Semantic Consistency",
            "url": "https://huggingface.co/papers/2504.20438",
            "abstract": "Image inpainting is a fundamental research area between image editing and image generation. Recent state-of-the-art (SOTA) methods have explored novel attention mechanisms, lightweight architectures, and context-aware modeling, demonstrating impressive performance. However, they often struggle with complex structure (e.g., texture, shape, spatial relations) and semantics (e.g., color consistency, object restoration, and logical correctness), leading to artifacts and inappropriate generation. To address this challenge, we design a simple yet effective inpainting paradigm called latent categories guidance, and further propose a diffusion-based model named PixelHacker. Specifically, we first construct a large dataset containing 14 million image-mask pairs by annotating foreground and background (potential 116 and 21 categories, respectively). Then, we encode potential foreground and background representations separately through two fixed-size embeddings, and intermittently inject these features into the denoising process via linear attention. Finally, by pre-training on our dataset and fine-tuning on open-source benchmarks, we obtain PixelHacker. Extensive experiments show that PixelHacker comprehensively outperforms the SOTA on a wide range of datasets (Places2, CelebA-HQ, and FFHQ) and exhibits remarkable consistency in both structure and semantics. Project page at https://hustvl.github.io/PixelHacker.",
            "score": 23,
            "issue_id": 3584,
            "pub_date": "2025-04-29",
            "pub_date_card": {
                "ru": "29 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 29",
                "zh": "4æœˆ29æ—¥"
            },
            "hash": "987ce511e3c86e06",
            "authors": [
                "Ziyang Xu",
                "Kangsheng Duan",
                "Xiaolei Shen",
                "Zhifeng Ding",
                "Wenyu Liu",
                "Xiaohu Ruan",
                "Xiaoxin Chen",
                "Xinggang Wang"
            ],
            "affiliations": [
                "Huazhong University of Science and Technology",
                "VIVO AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.20438.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#dataset",
                    "#training",
                    "#open_source",
                    "#optimization",
                    "#diffusion",
                    "#cv"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "PixelHacker: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ PixelHacker. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 14 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ğ¼Ğ°ÑĞºĞ° Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ğ¸ Ğ·Ğ°Ğ´Ğ½ĞµĞ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ°. PixelHacker Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ PixelHacker Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "PixelHacker: Revolutionizing Image Inpainting with Latent Categories Guidance",
                    "desc": "This paper presents a new approach to image inpainting called PixelHacker, which aims to improve the quality of generated images by addressing issues with complex structures and semantics. The authors introduce a large dataset of 14 million image-mask pairs to train their model, focusing on distinguishing between foreground and background categories. They utilize a diffusion-based model that incorporates linear attention to enhance the denoising process, ensuring better consistency in texture and color. Experimental results demonstrate that PixelHacker significantly outperforms existing state-of-the-art methods across various datasets, achieving superior image restoration results."
                },
                "zh": {
                    "title": "PixelHackerï¼šå›¾åƒä¿®å¤çš„æ–°çªç ´",
                    "desc": "å›¾åƒä¿®å¤æ˜¯å›¾åƒç¼–è¾‘ä¸ç”Ÿæˆä¹‹é—´çš„ä¸€ä¸ªé‡è¦ç ”ç©¶é¢†åŸŸã€‚æœ€è¿‘çš„æœ€å…ˆè¿›æ–¹æ³•æ¢ç´¢äº†æ–°é¢–çš„æ³¨æ„åŠ›æœºåˆ¶ã€è½»é‡çº§æ¶æ„å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥å»ºæ¨¡ï¼Œå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨å¤„ç†å¤æ‚ç»“æ„å’Œè¯­ä¹‰æ—¶å¸¸å¸¸é¢ä¸´æŒ‘æˆ˜ï¼Œå¯¼è‡´ç”Ÿæˆçš„å›¾åƒå‡ºç°ä¼ªå½±å’Œä¸å½“ç”Ÿæˆã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„ä¿®å¤èŒƒå¼ï¼Œç§°ä¸ºæ½œåœ¨ç±»åˆ«å¼•å¯¼ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£çš„æ¨¡å‹PixelHackerã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.01079",
            "title": "Improving Editability in Image Generation with Layer-wise Memory",
            "url": "https://huggingface.co/papers/2505.01079",
            "abstract": "Most real-world image editing tasks require multiple sequential edits to achieve desired results. Current editing approaches, primarily designed for single-object modifications, struggle with sequential editing: especially with maintaining previous edits along with adapting new objects naturally into the existing content. These limitations significantly hinder complex editing scenarios where multiple objects need to be modified while preserving their contextual relationships. We address this fundamental challenge through two key proposals: enabling rough mask inputs that preserve existing content while naturally integrating new elements and supporting consistent editing across multiple modifications. Our framework achieves this through layer-wise memory, which stores latent representations and prompt embeddings from previous edits. We propose Background Consistency Guidance that leverages memorized latents to maintain scene coherence and Multi-Query Disentanglement in cross-attention that ensures natural adaptation to existing content. To evaluate our method, we present a new benchmark dataset incorporating semantic alignment metrics and interactive editing scenarios. Through comprehensive experiments, we demonstrate superior performance in iterative image editing tasks with minimal user effort, requiring only rough masks while maintaining high-quality results throughout multiple editing steps.",
            "score": 17,
            "issue_id": 3582,
            "pub_date": "2025-05-02",
            "pub_date_card": {
                "ru": "2 Ğ¼Ğ°Ñ",
                "en": "May 2",
                "zh": "5æœˆ2æ—¥"
            },
            "hash": "e1aa83ea7926943e",
            "authors": [
                "Daneul Kim",
                "Jaeah Lee",
                "Jaesik Park"
            ],
            "affiliations": [
                "Seoul National University, Republic of Korea"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.01079.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#cv"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹: ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ¿Ñ€Ğ¾ÑˆĞ»Ğ¾Ğµ, Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ½Ğ¾Ğ²Ğ¾Ğµ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ°ÑĞ¾Ğº, Ğ¿Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½ÑƒÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸Ğº Background Consistency Guidance Ğ¸ Multi-Query Disentanglement. Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ÑÑ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Seamless Sequential Image Editing with Context Preservation",
                    "desc": "This paper addresses the challenges of sequential image editing, where multiple edits are needed while keeping previous changes intact. Current methods struggle with integrating new objects into existing images without disrupting the overall context. The authors propose a framework that uses layer-wise memory to store previous edits and ensure consistency across modifications. Their approach includes Background Consistency Guidance and Multi-Query Disentanglement to enhance the natural integration of new elements, leading to improved performance in complex editing tasks with minimal user input."
                },
                "zh": {
                    "title": "å®ç°è‡ªç„¶è¿ç»­çš„å›¾åƒç¼–è¾‘",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†å›¾åƒç¼–è¾‘ä¸­çš„å¤šæ¬¡è¿ç»­ç¼–è¾‘é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤šä¸ªå¯¹è±¡çš„ä¿®æ”¹æ—¶å­˜åœ¨å›°éš¾ï¼Œå°¤å…¶æ˜¯åœ¨ä¿æŒä¹‹å‰ç¼–è¾‘å†…å®¹çš„åŒæ—¶è‡ªç„¶åœ°èå…¥æ–°å¯¹è±¡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤é¡¹å…³é”®æ–¹æ¡ˆï¼šä¸€æ˜¯æ”¯æŒç²—ç•¥çš„æ©è†œè¾“å…¥ï¼Œä»¥ä¿ç•™ç°æœ‰å†…å®¹å¹¶è‡ªç„¶æ•´åˆæ–°å…ƒç´ ï¼›äºŒæ˜¯æ”¯æŒå¤šæ¬¡ä¿®æ”¹çš„ä¸€è‡´æ€§ç¼–è¾‘ã€‚æˆ‘ä»¬çš„æ¡†æ¶é€šè¿‡å±‚çº§è®°å¿†å­˜å‚¨å…ˆå‰ç¼–è¾‘çš„æ½œåœ¨è¡¨ç¤ºå’Œæç¤ºåµŒå…¥ï¼Œåˆ©ç”¨èƒŒæ™¯ä¸€è‡´æ€§å¼•å¯¼ä¿æŒåœºæ™¯çš„è¿è´¯æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è¿­ä»£å›¾åƒç¼–è¾‘ä»»åŠ¡ä¸­è¡¨ç°ä¼˜è¶Šï¼Œç”¨æˆ·åªéœ€æä¾›ç²—ç•¥æ©è†œå³å¯å®ç°é«˜è´¨é‡çš„ç¼–è¾‘æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.21117",
            "title": "Beyond One-Size-Fits-All: Inversion Learning for Highly Effective NLG\n  Evaluation Prompts",
            "url": "https://huggingface.co/papers/2504.21117",
            "abstract": "Evaluating natural language generation (NLG) systems is challenging due to the diversity of valid outputs. While human evaluation is the gold standard, it suffers from inconsistencies, lack of standardisation, and demographic biases, limiting reproducibility. LLM-based evaluation offers a scalable alternative but is highly sensitive to prompt design, where small variations can lead to significant discrepancies. In this work, we propose an inversion learning method that learns effective reverse mappings from model outputs back to their input instructions, enabling the automatic generation of highly effective, model-specific evaluation prompts. Our method requires only a single evaluation sample and eliminates the need for time-consuming manual prompt engineering, thereby improving both efficiency and robustness. Our work contributes toward a new direction for more robust and efficient LLM-based evaluation.",
            "score": 11,
            "issue_id": 3587,
            "pub_date": "2025-04-29",
            "pub_date_card": {
                "ru": "29 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 29",
                "zh": "4æœˆ29æ—¥"
            },
            "hash": "2a43e27932acf80e",
            "authors": [
                "Hanhua Hong",
                "Chenghao Xiao",
                "Yang Wang",
                "Yiqi Liu",
                "Wenge Rong",
                "Chenghua Lin"
            ],
            "affiliations": [
                "Beihang University",
                "Durham University",
                "The University of Manchester"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.21117.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#optimization",
                    "#benchmark",
                    "#interpretability"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ NLG ÑĞ¸ÑÑ‚ĞµĞ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° (NLG) Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ¸Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ğ´Ğ¸Ğ½ Ğ¾Ğ±Ñ€Ğ°Ğ·ĞµÑ†. ĞœĞµÑ‚Ğ¾Ğ´ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ² Ñ‚Ñ€ÑƒĞ´Ğ¾ĞµĞ¼ĞºĞ¾Ğ¹ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ², Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ğ¾Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ¹ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM)."
                },
                "en": {
                    "title": "Revolutionizing NLG Evaluation with Inversion Learning",
                    "desc": "This paper addresses the difficulties in evaluating natural language generation (NLG) systems, particularly the inconsistencies and biases in human evaluations. It introduces an inversion learning method that creates effective prompts for evaluating models by learning from their outputs. This approach allows for automatic generation of tailored evaluation prompts, requiring only one sample, which enhances efficiency. The proposed method aims to improve the robustness of LLM-based evaluations, paving the way for more standardized assessment in NLG."
                },
                "zh": {
                    "title": "æå‡è‡ªç„¶è¯­è¨€ç”Ÿæˆè¯„ä¼°çš„æ•ˆç‡ä¸ç¨³å¥æ€§",
                    "desc": "è¯„ä¼°è‡ªç„¶è¯­è¨€ç”Ÿæˆï¼ˆNLGï¼‰ç³»ç»Ÿæ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› ä¸ºæœ‰æ•ˆè¾“å‡ºçš„å¤šæ ·æ€§ä½¿å¾—æ ‡å‡†åŒ–è¯„ä¼°å˜å¾—å›°éš¾ã€‚è™½ç„¶äººå·¥è¯„ä¼°è¢«è®¤ä¸ºæ˜¯é‡‘æ ‡å‡†ï¼Œä½†å…¶å­˜åœ¨ä¸ä¸€è‡´æ€§ã€ç¼ºä¹æ ‡å‡†åŒ–å’Œäººå£åè§ç­‰é—®é¢˜ï¼Œé™åˆ¶äº†å¯é‡å¤æ€§ã€‚åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¯„ä¼°æä¾›äº†ä¸€ç§å¯æ‰©å±•çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†å¯¹æç¤ºè®¾è®¡éå¸¸æ•æ„Ÿï¼Œå¾®å°çš„å˜åŒ–å¯èƒ½å¯¼è‡´æ˜¾è‘—çš„å·®å¼‚ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åæ¼”å­¦ä¹ æ–¹æ³•ï¼Œå¯ä»¥æœ‰æ•ˆåœ°ä»æ¨¡å‹è¾“å‡ºåå‘æ˜ å°„åˆ°è¾“å…¥æŒ‡ä»¤ï¼Œä»è€Œè‡ªåŠ¨ç”Ÿæˆé«˜æ•ˆçš„ã€ç‰¹å®šäºæ¨¡å‹çš„è¯„ä¼°æç¤ºï¼Œæå‡äº†è¯„ä¼°çš„æ•ˆç‡å’Œç¨³å¥æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.00949",
            "title": "Llama-Nemotron: Efficient Reasoning Models",
            "url": "https://huggingface.co/papers/2505.00949",
            "abstract": "We introduce the Llama-Nemotron series of models, an open family of heterogeneous reasoning models that deliver exceptional reasoning capabilities, inference efficiency, and an open license for enterprise use. The family comes in three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs competitively with state-of-the-art reasoning models such as DeepSeek-R1 while offering superior inference throughput and memory efficiency. In this report, we discuss the training procedure for these models, which entails using neural architecture search from Llama 3 models for accelerated inference, knowledge distillation, and continued pretraining, followed by a reasoning-focused post-training stage consisting of two main parts: supervised fine-tuning and large scale reinforcement learning. Llama-Nemotron models are the first open-source models to support a dynamic reasoning toggle, allowing users to switch between standard chat and reasoning modes during inference. To further support open research and facilitate model development, we provide the following resources: 1. We release the Llama-Nemotron reasoning models -- LN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA Open Model License Agreement. 2. We release the complete post-training dataset: Llama-Nemotron-Post-Training-Dataset. 3. We also release our training codebases: NeMo, NeMo-Aligner, and Megatron-LM.",
            "score": 7,
            "issue_id": 3587,
            "pub_date": "2025-05-02",
            "pub_date_card": {
                "ru": "2 Ğ¼Ğ°Ñ",
                "en": "May 2",
                "zh": "5æœˆ2æ—¥"
            },
            "hash": "cbc28025b0c6bde3",
            "authors": [
                "Akhiad Bercovich",
                "Itay Levy",
                "Izik Golan",
                "Mohammad Dabbah",
                "Ran El-Yaniv",
                "Omri Puny",
                "Ido Galil",
                "Zach Moshe",
                "Tomer Ronen",
                "Najeeb Nabwani",
                "Ido Shahaf",
                "Oren Tropp",
                "Ehud Karpas",
                "Ran Zilberstein",
                "Jiaqi Zeng",
                "Soumye Singhal",
                "Alexander Bukharin",
                "Yian Zhang",
                "Tugrul Konuk",
                "Gerald Shen",
                "Ameya Sunil Mahabaleshwarkar",
                "Bilal Kartal",
                "Yoshi Suhara",
                "Olivier Delalleau",
                "Zijia Chen",
                "Zhilin Wang",
                "David Mosallanezhad",
                "Adi Renduchintala",
                "Haifeng Qian",
                "Dima Rekesh",
                "Fei Jia",
                "Somshubra Majumdar",
                "Vahid Noroozi",
                "Wasi Uddin Ahmad",
                "Sean Narenthiran",
                "Aleksander Ficek",
                "Mehrzad Samadi",
                "Jocelyn Huang",
                "Siddhartha Jain",
                "Igor Gitman",
                "Ivan Moshkov",
                "Wei Du",
                "Shubham Toshniwal",
                "George Armstrong",
                "Branislav Kisacanin",
                "Matvei Novikov",
                "Daria Gitman",
                "Evelina Bakhturina",
                "Jane Polak Scowcroft",
                "John Kamalu",
                "Dan Su",
                "Kezhi Kong",
                "Markus Kliegl",
                "Rabeeh Karimi",
                "Ying Lin",
                "Sanjeev Satheesh",
                "Jupinder Parmar",
                "Pritam Gundecha",
                "Brandon Norick",
                "Joseph Jennings",
                "Shrimai Prabhumoye",
                "Syeda Nahida Akter",
                "Mostofa Patwary",
                "Abhinav Khattar",
                "Deepak Narayanan",
                "Roger Waleffe",
                "Jimmy Zhang",
                "Bor-Yiing Su",
                "Guyue Huang",
                "Terry Kong",
                "Parth Chadha",
                "Sahil Jain",
                "Christine Harvey",
                "Elad Segal",
                "Jining Huang",
                "Sergey Kashirsky",
                "Robert McQueen",
                "Izzy Putterman",
                "George Lam",
                "Arun Venkatesan",
                "Sherry Wu",
                "Vinh Nguyen",
                "Manoj Kilaru",
                "Andrew Wang",
                "Anna Warno",
                "Abhilash Somasamudramath",
                "Sandip Bhaskar",
                "Maka Dong",
                "Nave Assaf",
                "Shahar Mor",
                "Omer Ullman Argov",
                "Scot Junkin",
                "Oleksandr Romanenko",
                "Pedro Larroy",
                "Monika Katariya",
                "Marco Rovinelli",
                "Viji Balas",
                "Nicholas Edelman",
                "Anahita Bhiwandiwalla",
                "Muthu Subramaniam",
                "Smita Ithape",
                "Karthik Ramamoorthy",
                "Yuting Wu",
                "Suguna Varshini Velury",
                "Omri Almog",
                "Joyjit Daw",
                "Denys Fridman",
                "Erick Galinkin",
                "Michael Evans",
                "Katherine Luna",
                "Leon Derczynski",
                "Nikki Pope",
                "Eileen Long",
                "Seth Schneider",
                "Guillermo Siman",
                "Tomasz Grzegorzek",
                "Pablo Ribalta",
                "Monika Katariya",
                "Joey Conway",
                "Trisha Saar",
                "Ann Guan",
                "Krzysztof Pawelec",
                "Shyamala Prayaga",
                "Oleksii Kuchaiev",
                "Boris Ginsburg",
                "Oluwatobi Olabiyi",
                "Kari Briski",
                "Jonathan Cohen",
                "Bryan Catanzaro",
                "Jonah Alben",
                "Yonatan Geifman",
                "Eric Chung"
            ],
            "affiliations": [
                "NVIDIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.00949.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#training",
                    "#rl",
                    "#open_source",
                    "#architecture",
                    "#dataset",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞµÑ€Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Llama-Nemotron - ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹ Ğ² Ñ‚Ñ€ĞµÑ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°Ñ… (8B, 49B, 253B) Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº, Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑÑ‚Ğ°Ğ¿ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ Ğ°ĞºÑ†ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ½Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ÑÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ°Ğ¼Ğ¸ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ‡Ğ°Ñ‚Ğ° Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Unlocking Reasoning with Open-Source Efficiency",
                    "desc": "The Llama-Nemotron series introduces a new family of reasoning models designed for efficient inference and strong reasoning capabilities. These models come in three sizes, allowing flexibility for different applications while maintaining competitive performance against leading models. The training process involves advanced techniques like neural architecture search, knowledge distillation, and reinforcement learning to enhance reasoning abilities. Additionally, these models are open-source, providing resources for further research and development in the machine learning community."
                },
                "zh": {
                    "title": "å¼€æ”¾æ¨ç†æ¨¡å‹ï¼Œæå‡æ¨ç†æ•ˆç‡ï¼",
                    "desc": "Llama-Nemotronç³»åˆ—æ¨¡å‹æ˜¯ä¸€ç§å¼€æ”¾çš„å¼‚æ„æ¨ç†æ¨¡å‹ï¼Œå…·æœ‰å“è¶Šçš„æ¨ç†èƒ½åŠ›å’Œé«˜æ•ˆçš„æ¨ç†æ€§èƒ½ã€‚è¯¥ç³»åˆ—åŒ…æ‹¬ä¸‰ç§ä¸åŒè§„æ¨¡çš„æ¨¡å‹ï¼šNanoï¼ˆ8Bï¼‰ã€Superï¼ˆ49Bï¼‰å’ŒUltraï¼ˆ253Bï¼‰ï¼Œåœ¨æ¨ç†é€Ÿåº¦å’Œå†…å­˜æ•ˆç‡ä¸Šä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ã€‚æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹é‡‡ç”¨äº†ç¥ç»æ¶æ„æœç´¢ã€çŸ¥è¯†è’¸é¦å’ŒæŒç»­é¢„è®­ç»ƒï¼Œæœ€åé€šè¿‡ç›‘ç£å¾®è°ƒå’Œå¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ è¿›è¡Œæ¨ç†ä¸“æ³¨çš„åè®­ç»ƒé˜¶æ®µã€‚Llama-Nemotronæ¨¡å‹æ˜¯é¦–ä¸ªæ”¯æŒåŠ¨æ€æ¨ç†åˆ‡æ¢çš„å¼€æºæ¨¡å‹ï¼Œç”¨æˆ·å¯ä»¥åœ¨æ¨ç†è¿‡ç¨‹ä¸­åœ¨æ ‡å‡†èŠå¤©æ¨¡å¼å’Œæ¨ç†æ¨¡å¼ä¹‹é—´åˆ‡æ¢ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.00174",
            "title": "Real-World Gaps in AI Governance Research",
            "url": "https://huggingface.co/papers/2505.00174",
            "abstract": "Drawing on 1,178 safety and reliability papers from 9,439 generative AI papers (January 2020 - March 2025), we compare research outputs of leading AI companies (Anthropic, Google DeepMind, Meta, Microsoft, and OpenAI) and AI universities (CMU, MIT, NYU, Stanford, UC Berkeley, and University of Washington). We find that corporate AI research increasingly concentrates on pre-deployment areas -- model alignment and testing & evaluation -- while attention to deployment-stage issues such as model bias has waned. Significant research gaps exist in high-risk deployment domains, including healthcare, finance, misinformation, persuasive and addictive features, hallucinations, and copyright. Without improved observability into deployed AI, growing corporate concentration could deepen knowledge deficits. We recommend expanding external researcher access to deployment data and systematic observability of in-market AI behaviors.",
            "score": 5,
            "issue_id": 3582,
            "pub_date": "2025-04-30",
            "pub_date_card": {
                "ru": "30 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 30",
                "zh": "4æœˆ30æ—¥"
            },
            "hash": "7618edbafcee6b13",
            "authors": [
                "Ilan Strauss",
                "Isobel Moure",
                "Tim O'Reilly",
                "Sruly Rosenblat"
            ],
            "affiliations": [
                "AI Disclosures Project, Social Science Research Council",
                "Institute for Innovation and Public Purpose, University College London",
                "OReilly Media"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.00174.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#ethics",
                    "#alignment",
                    "#healthcare",
                    "#hallucinations",
                    "#data"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞšĞ¾Ñ€Ğ¿Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ˜Ğ˜: Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ² Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ 1178 Ñ€Ğ°Ğ±Ğ¾Ñ‚ Ğ¿Ğ¾ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ· 9439 ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ğ¿Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ˜Ğ˜ Ğ·Ğ° Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´ Ñ ÑĞ½Ğ²Ğ°Ñ€Ñ 2020 Ğ¿Ğ¾ Ğ¼Ğ°Ñ€Ñ‚ 2025 Ğ³Ğ¾Ğ´Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ¸Ñ‚ĞµÑ‚Ğ¾Ğ² Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ˜Ğ˜. ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ ĞºĞ¾Ñ€Ğ¿Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ˜Ğ˜ Ğ²ÑĞµ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ ĞºĞ¾Ğ½Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğº Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°Ğ¼ ÑÑ‚Ğ°Ğ¿Ğ° Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ğ¼ ĞºĞ°Ğº ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾ÑĞ»Ğ°Ğ±ĞµĞ²Ğ°ĞµÑ‚. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒÑÑ‚ Ñ€Ğ°ÑÑˆĞ¸Ñ€Ğ¸Ñ‚ÑŒ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğº Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğµ Ğ·Ğ° Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ˜Ğ˜ Ğ½Ğ° Ñ€Ñ‹Ğ½ĞºĞµ."
                },
                "en": {
                    "title": "Bridging the Gap: Enhancing AI Safety in Deployment",
                    "desc": "This paper analyzes the trends in safety and reliability research within generative AI by examining 1,178 papers from major AI companies and universities. It highlights a shift in focus towards pre-deployment concerns like model alignment and evaluation, while issues related to deployment, such as model bias, are receiving less attention. The authors identify critical research gaps in high-risk areas like healthcare and finance, where the implications of AI deployment can be significant. They advocate for better access to deployment data and enhanced observability of AI systems in real-world applications to address these gaps."
                },
                "zh": {
                    "title": "å…³æ³¨äººå·¥æ™ºèƒ½éƒ¨ç½²é˜¶æ®µçš„ç ”ç©¶ç¼ºå£",
                    "desc": "æœ¬ç ”ç©¶åˆ†æäº†1178ç¯‡å®‰å…¨æ€§å’Œå¯é æ€§è®ºæ–‡ä¸9439ç¯‡ç”Ÿæˆå¼äººå·¥æ™ºèƒ½è®ºæ–‡ï¼Œæ¯”è¾ƒäº†ä¸»è¦äººå·¥æ™ºèƒ½å…¬å¸å’Œå¤§å­¦çš„ç ”ç©¶æˆæœã€‚ç ”ç©¶å‘ç°ï¼Œä¼ä¸šçš„äººå·¥æ™ºèƒ½ç ”ç©¶è¶Šæ¥è¶Šé›†ä¸­åœ¨æ¨¡å‹å¯¹é½å’Œæµ‹è¯•è¯„ä¼°ç­‰é¢„éƒ¨ç½²é¢†åŸŸï¼Œè€Œå¯¹éƒ¨ç½²é˜¶æ®µé—®é¢˜å¦‚æ¨¡å‹åè§çš„å…³æ³¨æœ‰æ‰€å‡å°‘ã€‚é«˜é£é™©éƒ¨ç½²é¢†åŸŸï¼ˆå¦‚åŒ»ç–—ã€é‡‘èã€è™šå‡ä¿¡æ¯ç­‰ï¼‰å­˜åœ¨æ˜¾è‘—çš„ç ”ç©¶ç©ºç™½ã€‚ä¸ºäº†æ”¹å–„å¯¹å·²éƒ¨ç½²äººå·¥æ™ºèƒ½çš„å¯è§‚å¯Ÿæ€§ï¼Œå»ºè®®æ‰©å¤§å¤–éƒ¨ç ”ç©¶äººå‘˜å¯¹éƒ¨ç½²æ•°æ®çš„è®¿é—®ï¼Œå¹¶ç³»ç»ŸåŒ–å¸‚åœºä¸­äººå·¥æ™ºèƒ½è¡Œä¸ºçš„å¯è§‚å¯Ÿæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.00023",
            "title": "CORG: Generating Answers from Complex, Interrelated Contexts",
            "url": "https://huggingface.co/papers/2505.00023",
            "abstract": "In a real-world corpus, knowledge frequently recurs across documents but often contains inconsistencies due to ambiguous naming, outdated information, or errors, leading to complex interrelationships between contexts. Previous research has shown that language models struggle with these complexities, typically focusing on single factors in isolation. We classify these relationships into four types: distracting, ambiguous, counterfactual, and duplicated. Our analysis reveals that no single approach effectively addresses all these interrelationships simultaneously. Therefore, we introduce Context Organizer (CORG), a framework that organizes multiple contexts into independently processed groups. This design allows the model to efficiently find all relevant answers while ensuring disambiguation. CORG consists of three key components: a graph constructor, a reranker, and an aggregator. Our results demonstrate that CORG balances performance and efficiency effectively, outperforming existing grouping methods and achieving comparable results to more computationally intensive, single-context approaches.",
            "score": 5,
            "issue_id": 3582,
            "pub_date": "2025-04-25",
            "pub_date_card": {
                "ru": "25 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 25",
                "zh": "4æœˆ25æ—¥"
            },
            "hash": "46da290a5c894311",
            "authors": [
                "Hyunji Lee",
                "Franck Dernoncourt",
                "Trung Bui",
                "Seunghyun Yoon"
            ],
            "affiliations": [
                "Adobe Research",
                "KAIST AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.00023.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#multimodal",
                    "#graphs",
                    "#architecture",
                    "#data"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "CORG: Ğ£Ğ¼Ğ½Ğ°Ñ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Context Organizer (CORG) Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾ÑĞ²ÑĞ·ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸ Ğ² ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ°Ñ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°. CORG Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñ‹ Ğ² Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğµ Ğ³Ñ€ÑƒĞ¿Ğ¿Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ²ÑĞµ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ñ‚Ñ€ĞµÑ… ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²: ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¾Ñ€Ğ° Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ², Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ° Ğ¸ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ‚Ğ¾Ñ€Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ CORG ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸."
                },
                "en": {
                    "title": "Organizing Contexts for Better Language Understanding",
                    "desc": "This paper addresses the challenges faced by language models when dealing with complex interrelationships in real-world data, which often contain inconsistencies. It categorizes these relationships into four types: distracting, ambiguous, counterfactual, and duplicated, highlighting that existing methods typically fail to handle them all at once. To tackle this issue, the authors propose a new framework called Context Organizer (CORG), which organizes contexts into separate groups for independent processing. CORG includes a graph constructor, a reranker, and an aggregator, and it demonstrates improved performance and efficiency compared to traditional methods."
                },
                "zh": {
                    "title": "ä¸Šä¸‹æ–‡ç»„ç»‡ï¼Œæå‡æ¨¡å‹æ•ˆç‡ä¸å‡†ç¡®æ€§",
                    "desc": "åœ¨ç°å®ä¸–ç•Œçš„è¯­æ–™åº“ä¸­ï¼ŒçŸ¥è¯†ç»å¸¸åœ¨æ–‡æ¡£ä¸­é‡å¤å‡ºç°ï¼Œä½†ç”±äºå‘½åæ¨¡ç³Šã€ä¿¡æ¯è¿‡æ—¶æˆ–é”™è¯¯ï¼Œå¯¼è‡´ä¸Šä¸‹æ–‡ä¹‹é—´å­˜åœ¨å¤æ‚çš„ç›¸äº’å…³ç³»ã€‚ä»¥å¾€çš„ç ”ç©¶è¡¨æ˜ï¼Œè¯­è¨€æ¨¡å‹åœ¨å¤„ç†è¿™äº›å¤æ‚æ€§æ—¶é€šå¸¸åªå…³æ³¨å•ä¸€å› ç´ ã€‚æˆ‘ä»¬å°†è¿™äº›å…³ç³»åˆ†ä¸ºå››ç§ç±»å‹ï¼šå¹²æ‰°ã€æ¨¡ç³Šã€åäº‹å®å’Œé‡å¤ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸Šä¸‹æ–‡ç»„ç»‡å™¨ï¼ˆCORGï¼‰ï¼Œå®ƒå°†å¤šä¸ªä¸Šä¸‹æ–‡ç»„ç»‡æˆç‹¬ç«‹å¤„ç†çš„ç»„ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.00562",
            "title": "TeLoGraF: Temporal Logic Planning via Graph-encoded Flow Matching",
            "url": "https://huggingface.co/papers/2505.00562",
            "abstract": "Learning to solve complex tasks with signal temporal logic (STL) specifications is crucial to many real-world applications. However, most previous works only consider fixed or parametrized STL specifications due to the lack of a diverse STL dataset and encoders to effectively extract temporal logic information for downstream tasks. In this paper, we propose TeLoGraF, Temporal Logic Graph-encoded Flow, which utilizes Graph Neural Networks (GNN) encoder and flow-matching to learn solutions for general STL specifications. We identify four commonly used STL templates and collect a total of 200K specifications with paired demonstrations. We conduct extensive experiments in five simulation environments ranging from simple dynamical models in the 2D space to high-dimensional 7DoF Franka Panda robot arm and Ant quadruped navigation. Results show that our method outperforms other baselines in the STL satisfaction rate. Compared to classical STL planning algorithms, our approach is 10-100X faster in inference and can work on any system dynamics. Besides, we show our graph-encoding method's capability to solve complex STLs and robustness to out-distribution STL specifications. Code is available at https://github.com/mengyuest/TeLoGraF",
            "score": 2,
            "issue_id": 3583,
            "pub_date": "2025-05-01",
            "pub_date_card": {
                "ru": "1 Ğ¼Ğ°Ñ",
                "en": "May 1",
                "zh": "5æœˆ1æ—¥"
            },
            "hash": "bf5b246f5848fa6e",
            "authors": [
                "Yue Meng",
                "Chuchu Fan"
            ],
            "affiliations": [
                "Department of Aeronautics and Astronautics, MIT, Cambridge, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.00562.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#inference",
                    "#agents",
                    "#robotics",
                    "#graphs",
                    "#optimization"
                ],
                "emoji": "â±ï¸",
                "ru": {
                    "title": "Ğ“Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚Ğ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ‚ĞµĞ¼Ğ¿Ğ¾Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ TeLoGraF - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ñ‚ĞµĞ¼Ğ¿Ğ¾Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¾Ğ¹ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² (STL). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑĞµÑ‚Ğ¸ Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ flow-matching Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ STL-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¸ÑÑŒ Ğ² Ğ¿ÑÑ‚Ğ¸ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…, Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… 2D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ TeLoGraF Ğ½Ğ°Ğ´ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "TeLoGraF: Fast and Robust Solutions for Complex Temporal Logic Tasks",
                    "desc": "This paper introduces TeLoGraF, a novel approach that leverages Graph Neural Networks (GNN) to effectively learn solutions for complex tasks defined by signal temporal logic (STL) specifications. The authors address the limitations of previous methods that relied on fixed STL templates by creating a diverse dataset of 200,000 STL specifications paired with demonstrations. Through extensive experiments across various simulation environments, TeLoGraF demonstrates superior performance in STL satisfaction rates and significantly faster inference times compared to traditional STL planning algorithms. Additionally, the graph-encoding technique shows robustness in handling complex and out-of-distribution STL specifications, making it a versatile tool for real-world applications."
                },
                "zh": {
                    "title": "TeLoGraFï¼šé«˜æ•ˆè§£å†³å¤æ‚æ—¶åºé€»è¾‘ä»»åŠ¡çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•TeLoGraFï¼Œç”¨äºè§£å†³å¤æ‚ä»»åŠ¡çš„ä¿¡å·æ—¶åºé€»è¾‘ï¼ˆSTLï¼‰è§„èŒƒã€‚æˆ‘ä»¬åˆ©ç”¨å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰ç¼–ç å™¨å’ŒæµåŒ¹é…æŠ€æœ¯ï¼Œå­¦ä¹ é€šç”¨STLè§„èŒƒçš„è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡æ”¶é›†20ä¸‡ä¸ªé…å¯¹ç¤ºä¾‹ï¼Œæˆ‘ä»¬åœ¨å¤šä¸ªä»¿çœŸç¯å¢ƒä¸­è¿›è¡Œäº†å¹¿æ³›å®éªŒï¼Œç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨STLæ»¡è¶³ç‡ä¸Šä¼˜äºå…¶ä»–åŸºçº¿ã€‚ä¸ä¼ ç»Ÿçš„STLè§„åˆ’ç®—æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ¨ç†é€Ÿåº¦ä¸Šå¿«10åˆ°100å€ï¼Œå¹¶ä¸”èƒ½å¤Ÿé€‚åº”ä»»ä½•ç³»ç»ŸåŠ¨æ€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.20859",
            "title": "X-Cross: Dynamic Integration of Language Models for Cross-Domain\n  Sequential Recommendation",
            "url": "https://huggingface.co/papers/2504.20859",
            "abstract": "As new products are emerging daily, recommendation systems are required to quickly adapt to possible new domains without needing extensive retraining. This work presents ``X-Cross'' -- a novel cross-domain sequential-recommendation model that recommends products in new domains by integrating several domain-specific language models; each model is fine-tuned with low-rank adapters (LoRA). Given a recommendation prompt, operating layer by layer, X-Cross dynamically refines the representation of each source language model by integrating knowledge from all other models. These refined representations are propagated from one layer to the next, leveraging the activations from each domain adapter to ensure domain-specific nuances are preserved while enabling adaptability across domains. Using Amazon datasets for sequential recommendation, X-Cross achieves performance comparable to a model that is fine-tuned with LoRA, while using only 25% of the additional parameters. In cross-domain tasks, such as adapting from Toys domain to Tools, Electronics or Sports, X-Cross demonstrates robust performance, while requiring about 50%-75% less fine-tuning data than LoRA to make fine-tuning effective. Furthermore, X-Cross achieves significant improvement in accuracy over alternative cross-domain baselines. Overall, X-Cross enables scalable and adaptive cross-domain recommendations, reducing computational overhead and providing an efficient solution for data-constrained environments.",
            "score": 2,
            "issue_id": 3583,
            "pub_date": "2025-04-29",
            "pub_date_card": {
                "ru": "29 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 29",
                "zh": "4æœˆ29æ—¥"
            },
            "hash": "2102f697cfc2375e",
            "authors": [
                "Guy Hadad",
                "Haggai Roitman",
                "Yotam Eshel",
                "Bracha Shapira",
                "Lior Rokach"
            ],
            "affiliations": [
                "Ben-Gurion University of the Negev Beer Sheva, Israel",
                "eBay Netanya, Israel"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.20859.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#transfer_learning",
                    "#low_resource",
                    "#training",
                    "#multimodal"
                ],
                "emoji": "ğŸ”€",
                "ru": {
                    "title": "X-Cross: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ĞºÑ€Ğ¾ÑÑ-Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ X-Cross Ğ´Ğ»Ñ ĞºÑ€Ğ¾ÑÑ-Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ÑƒÑ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ğ¾-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ°Ğ¼Ğ¸ (LoRA). X-Cross Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ· Ğ²ÑĞµÑ… Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Amazon Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ X-Cross Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ¹ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ, Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ LoRA, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ»Ğ¸ÑˆÑŒ 25% Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² ĞºÑ€Ğ¾ÑÑ-Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑ Ğ½Ğ° 50-75% Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ´Ğ¾Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸."
                },
                "en": {
                    "title": "X-Cross: Efficient Cross-Domain Recommendations with Minimal Data",
                    "desc": "The paper introduces 'X-Cross', a new model designed for cross-domain sequential recommendations that can quickly adapt to new product categories without extensive retraining. It utilizes multiple domain-specific language models, each fine-tuned with low-rank adapters (LoRA), to enhance the recommendation process. By refining the representations of these models layer by layer, X-Cross effectively integrates knowledge from different domains while maintaining their unique characteristics. The model shows strong performance on Amazon datasets, requiring significantly less fine-tuning data and parameters compared to traditional methods, making it efficient for data-limited scenarios."
                },
                "zh": {
                    "title": "X-Crossï¼šé«˜æ•ˆçš„è·¨é¢†åŸŸæ¨èè§£å†³æ–¹æ¡ˆ",
                    "desc": "éšç€æ–°äº§å“çš„ä¸æ–­æ¶Œç°ï¼Œæ¨èç³»ç»Ÿéœ€è¦å¿«é€Ÿé€‚åº”æ–°é¢†åŸŸï¼Œè€Œæ— éœ€å¤§é‡é‡æ–°è®­ç»ƒã€‚æœ¬æ–‡æå‡ºäº†â€œX-Crossâ€æ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„è·¨é¢†åŸŸåºåˆ—æ¨èæ¨¡å‹ï¼Œé€šè¿‡æ•´åˆå¤šä¸ªç‰¹å®šé¢†åŸŸçš„è¯­è¨€æ¨¡å‹æ¥æ¨èæ–°é¢†åŸŸçš„äº§å“ã€‚X-Crossé€šè¿‡é€å±‚æ“ä½œåŠ¨æ€åœ°ä¼˜åŒ–æ¯ä¸ªæºè¯­è¨€æ¨¡å‹çš„è¡¨ç¤ºï¼Œç¡®ä¿åœ¨è·¨é¢†åŸŸé€‚åº”æ—¶ä¿ç•™é¢†åŸŸç‰¹æœ‰çš„ç»†å¾®å·®åˆ«ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒX-Crossåœ¨è·¨é¢†åŸŸä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä¸”æ‰€éœ€çš„å¾®è°ƒæ•°æ®é‡æ˜¾è‘—ä½äºä¼ ç»Ÿæ–¹æ³•ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-05-02.html",
    "link_next": "2025-05-06.html",
    "link_month": "2025-05.html",
    "short_date_prev": {
        "ru": "02.05",
        "en": "05/02",
        "zh": "5æœˆ2æ—¥"
    },
    "short_date_next": {
        "ru": "06.05",
        "en": "05/06",
        "zh": "5æœˆ6æ—¥"
    },
    "categories": {
        "#dataset": 4,
        "#data": 2,
        "#benchmark": 3,
        "#agents": 1,
        "#cv": 2,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 1,
        "#training": 3,
        "#robotics": 1,
        "#agi": 1,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 1,
        "#transfer_learning": 1,
        "#graphs": 2,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 4,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 1
    },
    "zh": {
        "text": "å›¾åƒä¿®å¤æ˜¯å›¾åƒç¼–è¾‘ä¸ç”Ÿæˆä¹‹é—´çš„é‡è¦ç ”ç©¶é¢†åŸŸã€‚æœ€æ–°æ–¹æ³•åœ¨æ³¨æ„åŠ›æœºåˆ¶ã€è½»é‡æ¶æ„å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥å»ºæ¨¡æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨å¤„ç†å¤æ‚ç»“æ„å’Œè¯­ä¹‰æ—¶ä»é¢ä¸´æŒ‘æˆ˜ï¼Œå¯¼è‡´ä¼ªå½±å’Œä¸æ°å½“çš„ç”Ÿæˆã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ç§°ä¸ºæ½œåœ¨ç±»åˆ«æŒ‡å¯¼çš„ç®€å•æœ‰æ•ˆèŒƒå¼ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£çš„æ¨¡å‹ï¼Œå‘½åä¸ºPixelHackerã€‚æˆ‘ä»¬é€šè¿‡æ„å»ºåŒ…å«1400ä¸‡å¼ å›¾åƒ-æ©ç å¯¹çš„å¤§å‹æ•°æ®é›†ï¼Œåˆ†åˆ«ç¼–ç å‰æ™¯å’ŒèƒŒæ™¯è¡¨ç¤ºï¼Œå¹¶åœ¨å»å™ªè¿‡ç¨‹ä¸­æ³¨å…¥è¿™äº›ç‰¹å¾ã€‚å®éªŒè¡¨æ˜ï¼ŒPixelHackeråœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç»“æ„å’Œè¯­ä¹‰ä¸€è‡´æ€§æ˜¾è‘—ã€‚é¡¹ç›®é¡µé¢ï¼šhttps://hustvl.github.io/PixelHackerã€‚",
        "title": "PixelHacker: Image Inpainting with Structural and Semantic Consistency",
        "pinyin": "å›¾åƒä¿®å¤æ˜¯å›¾åƒç¼–è¾‘ä¸ç”Ÿæˆä¹‹é—´çš„é‡è¦ç ”ç©¶é¢†åŸŸã€‚æœ€æ–°æ–¹æ³•åœ¨æ³¨æ„åŠ›æœºåˆ¶ã€è½»é‡æ¶æ„å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥å»ºæ¨¡æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨å¤„ç†å¤æ‚ç»“æ„å’Œè¯­ä¹‰æ—¶ä»é¢ä¸´æŒ‘æˆ˜ï¼Œå¯¼è‡´ä¼ªå½±å’Œä¸æ°å½“çš„ç”Ÿæˆã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ç§°ä¸ºæ½œåœ¨ç±»åˆ«æŒ‡å¯¼çš„ç®€å•æœ‰æ•ˆèŒƒå¼ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£çš„æ¨¡å‹ï¼Œå‘½åä¸ºPixelHackerã€‚æˆ‘ä»¬é€šè¿‡æ„å»ºåŒ…å«1400ä¸‡å¼ å›¾åƒ-æ©ç å¯¹çš„å¤§å‹æ•°æ®é›†ï¼Œåˆ†åˆ«ç¼–ç å‰æ™¯å’ŒèƒŒæ™¯è¡¨ç¤ºï¼Œå¹¶åœ¨å»å™ªè¿‡ç¨‹ä¸­æ³¨å…¥è¿™äº›ç‰¹å¾ã€‚å®éªŒè¡¨æ˜ï¼ŒPixelHackeråœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç»“æ„å’Œè¯­ä¹‰ä¸€è‡´æ€§æ˜¾è‘—ã€‚é¡¹ç›®é¡µé¢ï¼šhttps://hustvl.github.io/PixelHackerã€‚\n\ntÃº xiÃ ng xiÅ« fÃ¹ shÃ¬ tÃº xiÃ ng biÄn jÃ­ yÇ” shÄ“ng chÃ©ng zhÄ« jiÄn de zhÃ²ng yÃ o yÃ¡n jiÅ« lÇng yÃ¹. zuÃ¬ xÄ«n fÄng fÇ zÃ i zhÃ¹ yÃ¬ lÃ¬ jÄ« zhÃ¬, qÄ«ng liÃ ng jiÃ  gÃ²u hÃ© shÃ ng xiÃ  tÃ­ng gÇn zhÄ« jiÃ n mÃ³u mÃ³ fÄng miÃ n zhuÅ dÃ o le xiÇn zhÃ¹ jÃ¬n zhÇn, dÃ n zÃ i chÇ” lÇ fÃº zÃ  jiÃ¨ gÃ²u hÃ© yÇ” yÃ¬ shÃ­ rÃ©ng miÃ n duÃ¬ zhÃ n, dÇo zhÃ¬ wÄ›i yÇng hÃ© bÃ¹ qiÃ  dÄng de shÄ“ng chÃ©ng. wÃ¨i jiÄ› juÃ© zhÃ¨ yÄ« wÃ¨n tÃ­, wÇ’ men shÃ¨ jÃ¬ le yÄ« zhÇ’ng chÄ“ng wÃ©i qiÃ¡n zÃ i lÃ¨i biÃ© zhÇ dÇo de jiÇn dÄn yÇ’u xiÃ o fÃ n shÃ¬, bÃ¬ng tÃ­ chÅ« le yÄ« zhÇ’ng jÄ« yÃº kuÃ² sÃ n de mÃ³ xÃ­ng, mÃ¬ng mÃ­ng wÃ©i PixelHacker. wÇ’ men tÅng guÃ² gÃ²u jiÃ n bÄo hÃ¡n 1400 wÃ n zhÄng tÃº xiÃ ng-mÃ³ zhÃ o duÃ¬ de dÃ  xÃ­ng shÃ¹ jÃ¹ jÄ«, fÄ“n biÃ© biÄn mÇ qiÃ¡n jÇng hÃ© bÃ¨i jÇng biÇo shÃ¬, bÃ¬ng zÃ i qÃ¹ zÃ o guÃ² chÃ©ng zhÅng zhÃ¹ yÇn zhÃ¨ xiÄ“ tÃ¨ zhÃ¨ng. shÃ­ yÃ n biÇo mÃ­ng, PixelHacker zÃ i duÅ gÃ¨ shÃ¹ jÃ¹ shÃ ng biÇo xiÃ n yÅu yÃ¡n, jiÃ¨ gÃ²u hÃ© yÇ” yÃ¬ yÄ« zhÃ¬ xÃ­ng xiÇn zhÃ¹. xiÃ ng mÃ¹ yÃ¨ miÃ n: https://hustvl.github.io/PixelHacker.",
        "vocab": "[{'word': 'å›¾åƒä¿®å¤', 'pinyin': 'tÃº xiÃ ng xiÅ« fÃ¹', 'trans': 'image inpainting'},\n{'word': 'é¢†åŸŸ', 'pinyin': 'lÇng yÃ¹', 'trans': 'field'},\n{'word': 'æ³¨æ„åŠ›æœºåˆ¶', 'pinyin': 'zhÃ¹ yÃ¬ lÃ¬ jÄ« zhÃ¬', 'trans': 'attention mechanism'},\n{'word': 'è½»é‡æ¶æ„', 'pinyin': 'qÄ«ng liÃ ng jiÃ  gÃ²u', 'trans': 'lightweight architecture'},\n{'word': 'ä¸Šä¸‹æ–‡æ„ŸçŸ¥', 'pinyin': 'shÃ ng xiÃ  wÃ©n gÇn zhÄ«', 'trans': 'context-aware'},\n{'word': 'å»ºæ¨¡', 'pinyin': 'jiÃ n mÃ³', 'trans': 'modeling'},\n{'word': 'æ˜¾è‘—', 'pinyin': 'xiÇn zhÃ¹', 'trans': 'significant'},\n{'word': 'è¿›å±•', 'pinyin': 'jÃ¬n zhÇn', 'trans': 'progress'},\n{'word': 'å¤æ‚ç»“æ„', 'pinyin': 'fÃ¹ zÃ¡ jiÃ© gÃ²u', 'trans': 'complex structures'},\n{'word': 'è¯­ä¹‰', 'pinyin': 'yÇ” yÃ¬', 'trans': 'semantics'},\n{'word': 'æŒ‘æˆ˜', 'pinyin': 'tiÇo zhÃ n', 'trans': 'challenge'},\n{'word': 'ä¼ªå½±', 'pinyin': 'wÄ›i yÇng', 'trans': 'artifacts'},\n{'word': 'ä¸æ°å½“', 'pinyin': 'bÃ¹ qiÃ  dÃ ng', 'trans': 'inappropriate'},\n{'word': 'ç”Ÿæˆ', 'pinyin': 'shÄ“ng chÃ©ng', 'trans': 'generation'},\n{'word': 'æ½œåœ¨ç±»åˆ«', 'pinyin': 'qiÃ¡n zÃ i lÃ¨i biÃ©', 'trans': 'latent category'},\n{'word': 'æŒ‡å¯¼', 'pinyin': 'zhÇ dÇo', 'trans': 'guidance'},\n{'word': 'èŒƒå¼', 'pinyin': 'fÃ n shÃ¬', 'trans': 'paradigm'},\n{'word': 'ç®€å•æœ‰æ•ˆ', 'pinyin': 'jiÇn dÄn yÇ’u xiÃ o', 'trans': 'simple and effective'},\n{'word': 'æ‰©æ•£', 'pinyin': 'kuÃ² sÃ n', 'trans': 'diffusion'},\n{'word': 'æ¨¡å‹', 'pinyin': 'mÃ³ xÃ­ng', 'trans': 'model'},\n{'word': 'å‘½å', 'pinyin': 'mÃ¬ng mÃ­ng', 'trans': 'named'},\n{'word': 'å‰æ™¯', 'pinyin': 'qiÃ¡n jÇng', 'trans': 'foreground'},\n{'word': 'èƒŒæ™¯', 'pinyin': 'bÃ¨i jÇng', 'trans': 'background'},\n{'word': 'è¡¨ç¤º', 'pinyin': 'biÇo shÃ¬', 'trans': 'representation'},\n{'word': 'æ³¨å…¥', 'pinyin': 'zhÃ¹ rÃ¹', 'trans': 'inject'},\n{'word': 'ç‰¹å¾', 'pinyin': 'tÃ¨ zhÄ“ng', 'trans': 'features'},\n{'word': 'å»å™ª', 'pinyin': 'qÃ¹ zÃ o', 'trans': 'denoising'},\n{'word': 'è¿‡ç¨‹', 'pinyin': 'guÃ² chÃ©ng', 'trans': 'process'},\n{'word': 'ç¼–ç ', 'pinyin': 'biÄn mÇ', 'trans': 'encode'},\n{'word': 'å®éªŒ', 'pinyin': 'shÃ­ yÃ n', 'trans': 'experiment'},\n{'word': 'è¡¨ç°', 'pinyin': 'biÇo xiÃ n', 'trans': 'performance'},\n{'word': 'ä¼˜å¼‚', 'pinyin': 'yÅu yÃ¬', 'trans': 'excellent'},\n{'word': 'ä¸€è‡´æ€§', 'pinyin': 'yÄ« zhÃ¬ xÃ¬ng', 'trans': 'consistency'},\n{'word': 'é¡¹ç›®', 'pinyin': 'xiÃ ng mÃ¹', 'trans': 'project'},\n{'word': 'é¡µé¢', 'pinyin': 'yÃ¨ miÃ n', 'trans': 'page'}]",
        "trans": "Image inpainting is an important research area in image editing and generation. Recent methods have made significant progress in attention mechanisms, lightweight architectures, and context-aware modeling, but they still face challenges in handling complex structures and semantics, leading to artifacts and inappropriate generation. To address this issue, we designed a simple and effective paradigm called latent category guidance and proposed a diffusion-based model named PixelHacker. We constructed a large dataset containing 14 million image-mask pairs, encoding foreground and background representations separately, and injected these features during the denoising process. Experiments demonstrated that PixelHacker performs excellently on multiple datasets, with significant structural and semantic consistency. Project page: https://hustvl.github.io/PixelHacker.",
        "update_ts": "2025-05-05 09:12"
    }
}