{
    "date": {
        "ru": "5 Ğ¼Ğ°Ñ",
        "en": "May 5",
        "zh": "5æœˆ5æ—¥"
    },
    "time_utc": "2025-05-05 08:16",
    "weekday": 0,
    "issue_id": 3585,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.20438",
            "title": "PixelHacker: Image Inpainting with Structural and Semantic Consistency",
            "url": "https://huggingface.co/papers/2504.20438",
            "abstract": "Image inpainting is a fundamental research area between image editing and image generation. Recent state-of-the-art (SOTA) methods have explored novel attention mechanisms, lightweight architectures, and context-aware modeling, demonstrating impressive performance. However, they often struggle with complex structure (e.g., texture, shape, spatial relations) and semantics (e.g., color consistency, object restoration, and logical correctness), leading to artifacts and inappropriate generation. To address this challenge, we design a simple yet effective inpainting paradigm called latent categories guidance, and further propose a diffusion-based model named PixelHacker. Specifically, we first construct a large dataset containing 14 million image-mask pairs by annotating foreground and background (potential 116 and 21 categories, respectively). Then, we encode potential foreground and background representations separately through two fixed-size embeddings, and intermittently inject these features into the denoising process via linear attention. Finally, by pre-training on our dataset and fine-tuning on open-source benchmarks, we obtain PixelHacker. Extensive experiments show that PixelHacker comprehensively outperforms the SOTA on a wide range of datasets (Places2, CelebA-HQ, and FFHQ) and exhibits remarkable consistency in both structure and semantics. Project page at https://hustvl.github.io/PixelHacker.",
            "score": 16,
            "issue_id": 3584,
            "pub_date": "2025-04-29",
            "pub_date_card": {
                "ru": "29 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 29",
                "zh": "4æœˆ29æ—¥"
            },
            "hash": "987ce511e3c86e06",
            "authors": [
                "Ziyang Xu",
                "Kangsheng Duan",
                "Xiaolei Shen",
                "Zhifeng Ding",
                "Wenyu Liu",
                "Xiaohu Ruan",
                "Xiaoxin Chen",
                "Xinggang Wang"
            ],
            "affiliations": [
                "Huazhong University of Science and Technology",
                "VIVO AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.20438.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#dataset",
                    "#training",
                    "#open_source",
                    "#optimization",
                    "#diffusion",
                    "#cv"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "PixelHacker: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ PixelHacker. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 14 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ğ¼Ğ°ÑĞºĞ° Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ğ¸ Ğ·Ğ°Ğ´Ğ½ĞµĞ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ°. PixelHacker Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ PixelHacker Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "PixelHacker: Revolutionizing Image Inpainting with Latent Categories Guidance",
                    "desc": "This paper presents a new approach to image inpainting called PixelHacker, which aims to improve the quality of generated images by addressing issues with complex structures and semantics. The authors introduce a large dataset of 14 million image-mask pairs to train their model, focusing on distinguishing between foreground and background categories. They utilize a diffusion-based model that incorporates linear attention to enhance the denoising process, ensuring better consistency in texture and color. Experimental results demonstrate that PixelHacker significantly outperforms existing state-of-the-art methods across various datasets, achieving superior image restoration results."
                },
                "zh": {
                    "title": "PixelHackerï¼šå›¾åƒä¿®å¤çš„æ–°çªç ´",
                    "desc": "å›¾åƒä¿®å¤æ˜¯å›¾åƒç¼–è¾‘ä¸ç”Ÿæˆä¹‹é—´çš„ä¸€ä¸ªé‡è¦ç ”ç©¶é¢†åŸŸã€‚æœ€è¿‘çš„æœ€å…ˆè¿›æ–¹æ³•æ¢ç´¢äº†æ–°é¢–çš„æ³¨æ„åŠ›æœºåˆ¶ã€è½»é‡çº§æ¶æ„å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥å»ºæ¨¡ï¼Œå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨å¤„ç†å¤æ‚ç»“æ„å’Œè¯­ä¹‰æ—¶å¸¸å¸¸é¢ä¸´æŒ‘æˆ˜ï¼Œå¯¼è‡´ç”Ÿæˆçš„å›¾åƒå‡ºç°ä¼ªå½±å’Œä¸å½“ç”Ÿæˆã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„ä¿®å¤èŒƒå¼ï¼Œç§°ä¸ºæ½œåœ¨ç±»åˆ«å¼•å¯¼ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£çš„æ¨¡å‹PixelHackerã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.01079",
            "title": "Improving Editability in Image Generation with Layer-wise Memory",
            "url": "https://huggingface.co/papers/2505.01079",
            "abstract": "Most real-world image editing tasks require multiple sequential edits to achieve desired results. Current editing approaches, primarily designed for single-object modifications, struggle with sequential editing: especially with maintaining previous edits along with adapting new objects naturally into the existing content. These limitations significantly hinder complex editing scenarios where multiple objects need to be modified while preserving their contextual relationships. We address this fundamental challenge through two key proposals: enabling rough mask inputs that preserve existing content while naturally integrating new elements and supporting consistent editing across multiple modifications. Our framework achieves this through layer-wise memory, which stores latent representations and prompt embeddings from previous edits. We propose Background Consistency Guidance that leverages memorized latents to maintain scene coherence and Multi-Query Disentanglement in cross-attention that ensures natural adaptation to existing content. To evaluate our method, we present a new benchmark dataset incorporating semantic alignment metrics and interactive editing scenarios. Through comprehensive experiments, we demonstrate superior performance in iterative image editing tasks with minimal user effort, requiring only rough masks while maintaining high-quality results throughout multiple editing steps.",
            "score": 10,
            "issue_id": 3582,
            "pub_date": "2025-05-02",
            "pub_date_card": {
                "ru": "2 Ğ¼Ğ°Ñ",
                "en": "May 2",
                "zh": "5æœˆ2æ—¥"
            },
            "hash": "e1aa83ea7926943e",
            "authors": [
                "Daneul Kim",
                "Jaeah Lee",
                "Jaesik Park"
            ],
            "affiliations": [
                "Seoul National University, Republic of Korea"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.01079.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#cv"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹: ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ¿Ñ€Ğ¾ÑˆĞ»Ğ¾Ğµ, Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ½Ğ¾Ğ²Ğ¾Ğµ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ°ÑĞ¾Ğº, Ğ¿Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½ÑƒÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸Ğº Background Consistency Guidance Ğ¸ Multi-Query Disentanglement. Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ÑÑ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Seamless Sequential Image Editing with Context Preservation",
                    "desc": "This paper addresses the challenges of sequential image editing, where multiple edits are needed while keeping previous changes intact. Current methods struggle with integrating new objects into existing images without disrupting the overall context. The authors propose a framework that uses layer-wise memory to store previous edits and ensure consistency across modifications. Their approach includes Background Consistency Guidance and Multi-Query Disentanglement to enhance the natural integration of new elements, leading to improved performance in complex editing tasks with minimal user input."
                },
                "zh": {
                    "title": "å®ç°è‡ªç„¶è¿ç»­çš„å›¾åƒç¼–è¾‘",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†å›¾åƒç¼–è¾‘ä¸­çš„å¤šæ¬¡è¿ç»­ç¼–è¾‘é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤šä¸ªå¯¹è±¡çš„ä¿®æ”¹æ—¶å­˜åœ¨å›°éš¾ï¼Œå°¤å…¶æ˜¯åœ¨ä¿æŒä¹‹å‰ç¼–è¾‘å†…å®¹çš„åŒæ—¶è‡ªç„¶åœ°èå…¥æ–°å¯¹è±¡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤é¡¹å…³é”®æ–¹æ¡ˆï¼šä¸€æ˜¯æ”¯æŒç²—ç•¥çš„æ©è†œè¾“å…¥ï¼Œä»¥ä¿ç•™ç°æœ‰å†…å®¹å¹¶è‡ªç„¶æ•´åˆæ–°å…ƒç´ ï¼›äºŒæ˜¯æ”¯æŒå¤šæ¬¡ä¿®æ”¹çš„ä¸€è‡´æ€§ç¼–è¾‘ã€‚æˆ‘ä»¬çš„æ¡†æ¶é€šè¿‡å±‚çº§è®°å¿†å­˜å‚¨å…ˆå‰ç¼–è¾‘çš„æ½œåœ¨è¡¨ç¤ºå’Œæç¤ºåµŒå…¥ï¼Œåˆ©ç”¨èƒŒæ™¯ä¸€è‡´æ€§å¼•å¯¼ä¿æŒåœºæ™¯çš„è¿è´¯æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è¿­ä»£å›¾åƒç¼–è¾‘ä»»åŠ¡ä¸­è¡¨ç°ä¼˜è¶Šï¼Œç”¨æˆ·åªéœ€æä¾›ç²—ç•¥æ©è†œå³å¯å®ç°é«˜è´¨é‡çš„ç¼–è¾‘æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.00023",
            "title": "CORG: Generating Answers from Complex, Interrelated Contexts",
            "url": "https://huggingface.co/papers/2505.00023",
            "abstract": "In a real-world corpus, knowledge frequently recurs across documents but often contains inconsistencies due to ambiguous naming, outdated information, or errors, leading to complex interrelationships between contexts. Previous research has shown that language models struggle with these complexities, typically focusing on single factors in isolation. We classify these relationships into four types: distracting, ambiguous, counterfactual, and duplicated. Our analysis reveals that no single approach effectively addresses all these interrelationships simultaneously. Therefore, we introduce Context Organizer (CORG), a framework that organizes multiple contexts into independently processed groups. This design allows the model to efficiently find all relevant answers while ensuring disambiguation. CORG consists of three key components: a graph constructor, a reranker, and an aggregator. Our results demonstrate that CORG balances performance and efficiency effectively, outperforming existing grouping methods and achieving comparable results to more computationally intensive, single-context approaches.",
            "score": 3,
            "issue_id": 3582,
            "pub_date": "2025-04-25",
            "pub_date_card": {
                "ru": "25 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 25",
                "zh": "4æœˆ25æ—¥"
            },
            "hash": "46da290a5c894311",
            "authors": [
                "Hyunji Lee",
                "Franck Dernoncourt",
                "Trung Bui",
                "Seunghyun Yoon"
            ],
            "affiliations": [
                "Adobe Research",
                "KAIST AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.00023.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#multimodal",
                    "#graphs",
                    "#architecture",
                    "#data"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "CORG: Ğ£Ğ¼Ğ½Ğ°Ñ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Context Organizer (CORG) Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾ÑĞ²ÑĞ·ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸ Ğ² ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ°Ñ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°. CORG Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñ‹ Ğ² Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğµ Ğ³Ñ€ÑƒĞ¿Ğ¿Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ²ÑĞµ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ñ‚Ñ€ĞµÑ… ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²: ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¾Ñ€Ğ° Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ², Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ° Ğ¸ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ‚Ğ¾Ñ€Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ CORG ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸."
                },
                "en": {
                    "title": "Organizing Contexts for Better Language Understanding",
                    "desc": "This paper addresses the challenges faced by language models when dealing with complex interrelationships in real-world data, which often contain inconsistencies. It categorizes these relationships into four types: distracting, ambiguous, counterfactual, and duplicated, highlighting that existing methods typically fail to handle them all at once. To tackle this issue, the authors propose a new framework called Context Organizer (CORG), which organizes contexts into separate groups for independent processing. CORG includes a graph constructor, a reranker, and an aggregator, and it demonstrates improved performance and efficiency compared to traditional methods."
                },
                "zh": {
                    "title": "ä¸Šä¸‹æ–‡ç»„ç»‡ï¼Œæå‡æ¨¡å‹æ•ˆç‡ä¸å‡†ç¡®æ€§",
                    "desc": "åœ¨ç°å®ä¸–ç•Œçš„è¯­æ–™åº“ä¸­ï¼ŒçŸ¥è¯†ç»å¸¸åœ¨æ–‡æ¡£ä¸­é‡å¤å‡ºç°ï¼Œä½†ç”±äºå‘½åæ¨¡ç³Šã€ä¿¡æ¯è¿‡æ—¶æˆ–é”™è¯¯ï¼Œå¯¼è‡´ä¸Šä¸‹æ–‡ä¹‹é—´å­˜åœ¨å¤æ‚çš„ç›¸äº’å…³ç³»ã€‚ä»¥å¾€çš„ç ”ç©¶è¡¨æ˜ï¼Œè¯­è¨€æ¨¡å‹åœ¨å¤„ç†è¿™äº›å¤æ‚æ€§æ—¶é€šå¸¸åªå…³æ³¨å•ä¸€å› ç´ ã€‚æˆ‘ä»¬å°†è¿™äº›å…³ç³»åˆ†ä¸ºå››ç§ç±»å‹ï¼šå¹²æ‰°ã€æ¨¡ç³Šã€åäº‹å®å’Œé‡å¤ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸Šä¸‹æ–‡ç»„ç»‡å™¨ï¼ˆCORGï¼‰ï¼Œå®ƒå°†å¤šä¸ªä¸Šä¸‹æ–‡ç»„ç»‡æˆç‹¬ç«‹å¤„ç†çš„ç»„ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.00174",
            "title": "Real-World Gaps in AI Governance Research",
            "url": "https://huggingface.co/papers/2505.00174",
            "abstract": "Drawing on 1,178 safety and reliability papers from 9,439 generative AI papers (January 2020 - March 2025), we compare research outputs of leading AI companies (Anthropic, Google DeepMind, Meta, Microsoft, and OpenAI) and AI universities (CMU, MIT, NYU, Stanford, UC Berkeley, and University of Washington). We find that corporate AI research increasingly concentrates on pre-deployment areas -- model alignment and testing & evaluation -- while attention to deployment-stage issues such as model bias has waned. Significant research gaps exist in high-risk deployment domains, including healthcare, finance, misinformation, persuasive and addictive features, hallucinations, and copyright. Without improved observability into deployed AI, growing corporate concentration could deepen knowledge deficits. We recommend expanding external researcher access to deployment data and systematic observability of in-market AI behaviors.",
            "score": 2,
            "issue_id": 3582,
            "pub_date": "2025-04-30",
            "pub_date_card": {
                "ru": "30 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 30",
                "zh": "4æœˆ30æ—¥"
            },
            "hash": "7618edbafcee6b13",
            "authors": [
                "Ilan Strauss",
                "Isobel Moure",
                "Tim O'Reilly",
                "Sruly Rosenblat"
            ],
            "affiliations": [
                "AI Disclosures Project, Social Science Research Council",
                "Institute for Innovation and Public Purpose, University College London",
                "OReilly Media"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.00174.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#ethics",
                    "#alignment",
                    "#healthcare",
                    "#hallucinations",
                    "#data"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞšĞ¾Ñ€Ğ¿Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ˜Ğ˜: Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ² Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ 1178 Ñ€Ğ°Ğ±Ğ¾Ñ‚ Ğ¿Ğ¾ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ· 9439 ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ğ¿Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ˜Ğ˜ Ğ·Ğ° Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´ Ñ ÑĞ½Ğ²Ğ°Ñ€Ñ 2020 Ğ¿Ğ¾ Ğ¼Ğ°Ñ€Ñ‚ 2025 Ğ³Ğ¾Ğ´Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ¸Ñ‚ĞµÑ‚Ğ¾Ğ² Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ˜Ğ˜. ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ ĞºĞ¾Ñ€Ğ¿Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ˜Ğ˜ Ğ²ÑĞµ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ ĞºĞ¾Ğ½Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğº Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°Ğ¼ ÑÑ‚Ğ°Ğ¿Ğ° Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ğ¼ ĞºĞ°Ğº ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾ÑĞ»Ğ°Ğ±ĞµĞ²Ğ°ĞµÑ‚. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒÑÑ‚ Ñ€Ğ°ÑÑˆĞ¸Ñ€Ğ¸Ñ‚ÑŒ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğº Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğµ Ğ·Ğ° Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ˜Ğ˜ Ğ½Ğ° Ñ€Ñ‹Ğ½ĞºĞµ."
                },
                "en": {
                    "title": "Bridging the Gap: Enhancing AI Safety in Deployment",
                    "desc": "This paper analyzes the trends in safety and reliability research within generative AI by examining 1,178 papers from major AI companies and universities. It highlights a shift in focus towards pre-deployment concerns like model alignment and evaluation, while issues related to deployment, such as model bias, are receiving less attention. The authors identify critical research gaps in high-risk areas like healthcare and finance, where the implications of AI deployment can be significant. They advocate for better access to deployment data and enhanced observability of AI systems in real-world applications to address these gaps."
                },
                "zh": {
                    "title": "å…³æ³¨äººå·¥æ™ºèƒ½éƒ¨ç½²é˜¶æ®µçš„ç ”ç©¶ç¼ºå£",
                    "desc": "æœ¬ç ”ç©¶åˆ†æäº†1178ç¯‡å®‰å…¨æ€§å’Œå¯é æ€§è®ºæ–‡ä¸9439ç¯‡ç”Ÿæˆå¼äººå·¥æ™ºèƒ½è®ºæ–‡ï¼Œæ¯”è¾ƒäº†ä¸»è¦äººå·¥æ™ºèƒ½å…¬å¸å’Œå¤§å­¦çš„ç ”ç©¶æˆæœã€‚ç ”ç©¶å‘ç°ï¼Œä¼ä¸šçš„äººå·¥æ™ºèƒ½ç ”ç©¶è¶Šæ¥è¶Šé›†ä¸­åœ¨æ¨¡å‹å¯¹é½å’Œæµ‹è¯•è¯„ä¼°ç­‰é¢„éƒ¨ç½²é¢†åŸŸï¼Œè€Œå¯¹éƒ¨ç½²é˜¶æ®µé—®é¢˜å¦‚æ¨¡å‹åè§çš„å…³æ³¨æœ‰æ‰€å‡å°‘ã€‚é«˜é£é™©éƒ¨ç½²é¢†åŸŸï¼ˆå¦‚åŒ»ç–—ã€é‡‘èã€è™šå‡ä¿¡æ¯ç­‰ï¼‰å­˜åœ¨æ˜¾è‘—çš„ç ”ç©¶ç©ºç™½ã€‚ä¸ºäº†æ”¹å–„å¯¹å·²éƒ¨ç½²äººå·¥æ™ºèƒ½çš„å¯è§‚å¯Ÿæ€§ï¼Œå»ºè®®æ‰©å¤§å¤–éƒ¨ç ”ç©¶äººå‘˜å¯¹éƒ¨ç½²æ•°æ®çš„è®¿é—®ï¼Œå¹¶ç³»ç»ŸåŒ–å¸‚åœºä¸­äººå·¥æ™ºèƒ½è¡Œä¸ºçš„å¯è§‚å¯Ÿæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.00562",
            "title": "TeLoGraF: Temporal Logic Planning via Graph-encoded Flow Matching",
            "url": "https://huggingface.co/papers/2505.00562",
            "abstract": "Learning to solve complex tasks with signal temporal logic (STL) specifications is crucial to many real-world applications. However, most previous works only consider fixed or parametrized STL specifications due to the lack of a diverse STL dataset and encoders to effectively extract temporal logic information for downstream tasks. In this paper, we propose TeLoGraF, Temporal Logic Graph-encoded Flow, which utilizes Graph Neural Networks (GNN) encoder and flow-matching to learn solutions for general STL specifications. We identify four commonly used STL templates and collect a total of 200K specifications with paired demonstrations. We conduct extensive experiments in five simulation environments ranging from simple dynamical models in the 2D space to high-dimensional 7DoF Franka Panda robot arm and Ant quadruped navigation. Results show that our method outperforms other baselines in the STL satisfaction rate. Compared to classical STL planning algorithms, our approach is 10-100X faster in inference and can work on any system dynamics. Besides, we show our graph-encoding method's capability to solve complex STLs and robustness to out-distribution STL specifications. Code is available at https://github.com/mengyuest/TeLoGraF",
            "score": 1,
            "issue_id": 3583,
            "pub_date": "2025-05-01",
            "pub_date_card": {
                "ru": "1 Ğ¼Ğ°Ñ",
                "en": "May 1",
                "zh": "5æœˆ1æ—¥"
            },
            "hash": "bf5b246f5848fa6e",
            "authors": [
                "Yue Meng",
                "Chuchu Fan"
            ],
            "affiliations": [
                "Department of Aeronautics and Astronautics, MIT, Cambridge, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.00562.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#inference",
                    "#agents",
                    "#robotics",
                    "#graphs",
                    "#optimization"
                ],
                "emoji": "â±ï¸",
                "ru": {
                    "title": "Ğ“Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚Ğ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ‚ĞµĞ¼Ğ¿Ğ¾Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ TeLoGraF - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ñ‚ĞµĞ¼Ğ¿Ğ¾Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¾Ğ¹ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² (STL). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑĞµÑ‚Ğ¸ Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ flow-matching Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ STL-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¸ÑÑŒ Ğ² Ğ¿ÑÑ‚Ğ¸ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…, Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… 2D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ TeLoGraF Ğ½Ğ°Ğ´ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "TeLoGraF: Fast and Robust Solutions for Complex Temporal Logic Tasks",
                    "desc": "This paper introduces TeLoGraF, a novel approach that leverages Graph Neural Networks (GNN) to effectively learn solutions for complex tasks defined by signal temporal logic (STL) specifications. The authors address the limitations of previous methods that relied on fixed STL templates by creating a diverse dataset of 200,000 STL specifications paired with demonstrations. Through extensive experiments across various simulation environments, TeLoGraF demonstrates superior performance in STL satisfaction rates and significantly faster inference times compared to traditional STL planning algorithms. Additionally, the graph-encoding technique shows robustness in handling complex and out-of-distribution STL specifications, making it a versatile tool for real-world applications."
                },
                "zh": {
                    "title": "TeLoGraFï¼šé«˜æ•ˆè§£å†³å¤æ‚æ—¶åºé€»è¾‘ä»»åŠ¡çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•TeLoGraFï¼Œç”¨äºè§£å†³å¤æ‚ä»»åŠ¡çš„ä¿¡å·æ—¶åºé€»è¾‘ï¼ˆSTLï¼‰è§„èŒƒã€‚æˆ‘ä»¬åˆ©ç”¨å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰ç¼–ç å™¨å’ŒæµåŒ¹é…æŠ€æœ¯ï¼Œå­¦ä¹ é€šç”¨STLè§„èŒƒçš„è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡æ”¶é›†20ä¸‡ä¸ªé…å¯¹ç¤ºä¾‹ï¼Œæˆ‘ä»¬åœ¨å¤šä¸ªä»¿çœŸç¯å¢ƒä¸­è¿›è¡Œäº†å¹¿æ³›å®éªŒï¼Œç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨STLæ»¡è¶³ç‡ä¸Šä¼˜äºå…¶ä»–åŸºçº¿ã€‚ä¸ä¼ ç»Ÿçš„STLè§„åˆ’ç®—æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ¨ç†é€Ÿåº¦ä¸Šå¿«10åˆ°100å€ï¼Œå¹¶ä¸”èƒ½å¤Ÿé€‚åº”ä»»ä½•ç³»ç»ŸåŠ¨æ€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.20859",
            "title": "X-Cross: Dynamic Integration of Language Models for Cross-Domain\n  Sequential Recommendation",
            "url": "https://huggingface.co/papers/2504.20859",
            "abstract": "As new products are emerging daily, recommendation systems are required to quickly adapt to possible new domains without needing extensive retraining. This work presents ``X-Cross'' -- a novel cross-domain sequential-recommendation model that recommends products in new domains by integrating several domain-specific language models; each model is fine-tuned with low-rank adapters (LoRA). Given a recommendation prompt, operating layer by layer, X-Cross dynamically refines the representation of each source language model by integrating knowledge from all other models. These refined representations are propagated from one layer to the next, leveraging the activations from each domain adapter to ensure domain-specific nuances are preserved while enabling adaptability across domains. Using Amazon datasets for sequential recommendation, X-Cross achieves performance comparable to a model that is fine-tuned with LoRA, while using only 25% of the additional parameters. In cross-domain tasks, such as adapting from Toys domain to Tools, Electronics or Sports, X-Cross demonstrates robust performance, while requiring about 50%-75% less fine-tuning data than LoRA to make fine-tuning effective. Furthermore, X-Cross achieves significant improvement in accuracy over alternative cross-domain baselines. Overall, X-Cross enables scalable and adaptive cross-domain recommendations, reducing computational overhead and providing an efficient solution for data-constrained environments.",
            "score": 1,
            "issue_id": 3583,
            "pub_date": "2025-04-29",
            "pub_date_card": {
                "ru": "29 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 29",
                "zh": "4æœˆ29æ—¥"
            },
            "hash": "2102f697cfc2375e",
            "authors": [
                "Guy Hadad",
                "Haggai Roitman",
                "Yotam Eshel",
                "Bracha Shapira",
                "Lior Rokach"
            ],
            "affiliations": [
                "Ben-Gurion University of the Negev Beer Sheva, Israel",
                "eBay Netanya, Israel"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.20859.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#transfer_learning",
                    "#low_resource",
                    "#training",
                    "#multimodal"
                ],
                "emoji": "ğŸ”€",
                "ru": {
                    "title": "X-Cross: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ĞºÑ€Ğ¾ÑÑ-Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ X-Cross Ğ´Ğ»Ñ ĞºÑ€Ğ¾ÑÑ-Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ÑƒÑ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ğ¾-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ°Ğ¼Ğ¸ (LoRA). X-Cross Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ· Ğ²ÑĞµÑ… Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Amazon Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ X-Cross Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ¹ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ, Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ LoRA, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ»Ğ¸ÑˆÑŒ 25% Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² ĞºÑ€Ğ¾ÑÑ-Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑ Ğ½Ğ° 50-75% Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ´Ğ¾Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸."
                },
                "en": {
                    "title": "X-Cross: Efficient Cross-Domain Recommendations with Minimal Data",
                    "desc": "The paper introduces 'X-Cross', a new model designed for cross-domain sequential recommendations that can quickly adapt to new product categories without extensive retraining. It utilizes multiple domain-specific language models, each fine-tuned with low-rank adapters (LoRA), to enhance the recommendation process. By refining the representations of these models layer by layer, X-Cross effectively integrates knowledge from different domains while maintaining their unique characteristics. The model shows strong performance on Amazon datasets, requiring significantly less fine-tuning data and parameters compared to traditional methods, making it efficient for data-limited scenarios."
                },
                "zh": {
                    "title": "X-Crossï¼šé«˜æ•ˆçš„è·¨é¢†åŸŸæ¨èè§£å†³æ–¹æ¡ˆ",
                    "desc": "éšç€æ–°äº§å“çš„ä¸æ–­æ¶Œç°ï¼Œæ¨èç³»ç»Ÿéœ€è¦å¿«é€Ÿé€‚åº”æ–°é¢†åŸŸï¼Œè€Œæ— éœ€å¤§é‡é‡æ–°è®­ç»ƒã€‚æœ¬æ–‡æå‡ºäº†â€œX-Crossâ€æ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„è·¨é¢†åŸŸåºåˆ—æ¨èæ¨¡å‹ï¼Œé€šè¿‡æ•´åˆå¤šä¸ªç‰¹å®šé¢†åŸŸçš„è¯­è¨€æ¨¡å‹æ¥æ¨èæ–°é¢†åŸŸçš„äº§å“ã€‚X-Crossé€šè¿‡é€å±‚æ“ä½œåŠ¨æ€åœ°ä¼˜åŒ–æ¯ä¸ªæºè¯­è¨€æ¨¡å‹çš„è¡¨ç¤ºï¼Œç¡®ä¿åœ¨è·¨é¢†åŸŸé€‚åº”æ—¶ä¿ç•™é¢†åŸŸç‰¹æœ‰çš„ç»†å¾®å·®åˆ«ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒX-Crossåœ¨è·¨é¢†åŸŸä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä¸”æ‰€éœ€çš„å¾®è°ƒæ•°æ®é‡æ˜¾è‘—ä½äºä¼ ç»Ÿæ–¹æ³•ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-05-02.html",
    "link_next": "2025-05-06.html",
    "link_month": "2025-05.html",
    "short_date_prev": {
        "ru": "02.05",
        "en": "05/02",
        "zh": "5æœˆ2æ—¥"
    },
    "short_date_next": {
        "ru": "06.05",
        "en": "05/06",
        "zh": "5æœˆ6æ—¥"
    },
    "categories": {
        "#dataset": 3,
        "#data": 2,
        "#benchmark": 2,
        "#agents": 1,
        "#cv": 2,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 1,
        "#training": 2,
        "#robotics": 1,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 1,
        "#graphs": 2,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 1
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°å…´æŠ€æœ¯ï¼šäº’åŠ¨ç”Ÿæˆè§†é¢‘ï¼ˆIGVï¼‰ã€‚å®ƒç»“åˆç”Ÿæˆå’Œäº’åŠ¨åŠŸèƒ½ï¼Œäº§ç”Ÿé«˜è´¨é‡è§†é¢‘å†…å®¹ï¼Œå¹¶è®©ç”¨æˆ·é€šè¿‡æ§åˆ¶ä¿¡å·å’Œå“åº”åé¦ˆå‚ä¸å…¶ä¸­ã€‚IGVåœ¨æ¸¸æˆã€äººå·¥æ™ºèƒ½å’Œè‡ªåŠ¨é©¾é©¶ä¸‰å¤§é¢†åŸŸæœ‰é‡è¦åº”ç”¨ã€‚æ–‡ç« è¿˜æå‡ºäº†ç†æƒ³IGVç³»ç»Ÿçš„äº”ä¸ªå…³é”®æ¨¡å—ï¼Œå¹¶åˆ†æäº†æ¯ä¸ªæ¨¡å—çš„æŠ€æœ¯æŒ‘æˆ˜å’Œæœªæ¥æ–¹å‘ã€‚",
        "title": "A Survey of Interactive Generative Video",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°å…´æŠ€æœ¯ï¼šäº’åŠ¨ç”Ÿæˆè§†é¢‘ï¼ˆIGVï¼‰ã€‚å®ƒç»“åˆç”Ÿæˆå’Œäº’åŠ¨åŠŸèƒ½ï¼Œäº§ç”Ÿé«˜è´¨é‡è§†é¢‘å†…å®¹ï¼Œå¹¶è®©ç”¨æˆ·é€šè¿‡æ§åˆ¶ä¿¡å·å’Œå“åº”åé¦ˆå‚ä¸å…¶ä¸­ã€‚IGVåœ¨æ¸¸æˆã€äººå·¥æ™ºèƒ½å’Œè‡ªåŠ¨é©¾é©¶ä¸‰å¤§é¢†åŸŸæœ‰é‡è¦åº”ç”¨ã€‚æ–‡ç« è¿˜æå‡ºäº†ç†æƒ³IGVç³»ç»Ÿçš„äº”ä¸ªå…³é”®æ¨¡å—ï¼Œå¹¶åˆ†æäº†æ¯ä¸ªæ¨¡å—çš„æŠ€æœ¯æŒ‘æˆ˜å’Œæœªæ¥æ–¹å‘ã€‚\n\nZhÃ¨ piÄn wÃ©nzhÄng jiÃ¨shÃ o le yÄ« zhÇ’ng xÄ«nxÄ«ng jÃ¬shÃ¹: hÃ¹dÃ²ng shÄ“ngchÃ©ng shÃ¬pÃ­n (IGV). TÄ jiÃ©hÃ© shÄ“ngchÃ©ng hÃ© hÃ¹dÃ²ng gÅngnÃ©ng, chÇnshÄ“ng gÄo zhÃ¬liÃ ng shÃ¬pÃ­n nÃ¨irÃ³ng, bÃ¬ng rÃ ng yÃ²nghÃ¹ tÅngguÃ² kÃ²ngzhÃ¬ xÃ¬nhÃ o hÃ© xiÇngyÃ¬ng fÇnkuÃ¬ cÄnyÃ¹ qÃ­zhÅng. IGV zÃ i yÃ³uxÃ¬, rÃ©ngÅng zhÃ¬nÃ©ng hÃ© zÃ¬dÃ²ng jiÃ shÇ sÄn dÃ  lÇngyÃ¹ yÇ’u zhÃ²ngyÃ o yÃ¬ngyÃ²ng. WÃ©nzhÄng hÃ¡i tÃ­chÅ« le lÇxiÇng IGV xÃ¬tÇ’ng de wÇ” gÃ¨ guÇnjiÃ n mÃ³kuÃ i, bÃ¬ng fÄ“nxi le mÄ›i gÃ¨ mÃ³kuÃ i de jÃ¬shÃ¹ tiÇozhÃ n hÃ© wÃ¨ilÃ¡i fÄngxiÃ ng.",
        "vocab": "[\n    {\"word\": \"æ–°å…´\", \"pinyin\": \"xÄ«n xÄ«ng\", \"trans\": \"emerging\"},\n    {\"word\": \"äº’åŠ¨\", \"pinyin\": \"hÃ¹ dÃ²ng\", \"trans\": \"interactive\"},\n    {\"word\": \"ç”Ÿæˆ\", \"pinyin\": \"shÄ“ng chÃ©ng\", \"trans\": \"generate\"},\n    {\"word\": \"é«˜è´¨é‡\", \"pinyin\": \"gÄo zhÃ¬ liÃ ng\", \"trans\": \"high quality\"},\n    {\"word\": \"æ§åˆ¶\", \"pinyin\": \"kÃ²ng zhÃ¬\", \"trans\": \"control\"},\n    {\"word\": \"ä¿¡å·\", \"pinyin\": \"xÃ¬n hÃ o\", \"trans\": \"signal\"},\n    {\"word\": \"åé¦ˆ\", \"pinyin\": \"fÇn kuÃ¬\", \"trans\": \"feedback\"},\n    {\"word\": \"å‚ä¸\", \"pinyin\": \"cÄn yÃ¹\", \"trans\": \"participate\"},\n    {\"word\": \"äººå·¥æ™ºèƒ½\", \"pinyin\": \"rÃ©n gÅng zhÃ¬ nÃ©ng\", \"trans\": \"artificial intelligence\"},\n    {\"word\": \"è‡ªåŠ¨é©¾é©¶\", \"pinyin\": \"zÃ¬ dÃ²ng jiÃ  shÇ\", \"trans\": \"autonomous driving\"},\n    {\"word\": \"é¢†åŸŸ\", \"pinyin\": \"lÇng yÃ¹\", \"trans\": \"field\"},\n    {\"word\": \"é‡è¦\", \"pinyin\": \"zhÃ²ng yÃ o\", \"trans\": \"important\"},\n    {\"word\": \"åº”ç”¨\", \"pinyin\": \"yÃ¬ng yÃ²ng\", \"trans\": \"application\"},\n    {\"word\": \"ç†æƒ³\", \"pinyin\": \"lÇ xiÇng\", \"trans\": \"ideal\"},\n    {\"word\": \"ç³»ç»Ÿ\", \"pinyin\": \"xÃ¬ tÇ’ng\", \"trans\": \"system\"},\n    {\"word\": \"å…³é”®\", \"pinyin\": \"guÄn jiÃ n\", \"trans\": \"key\"},\n    {\"word\": \"æ¨¡å—\", \"pinyin\": \"mÃ³ kuÃ i\", \"trans\": \"module\"},\n    {\"word\": \"æŠ€æœ¯\", \"pinyin\": \"jÃ¬ shÃ¹\", \"trans\": \"technology\"},\n    {\"word\": \"æŒ‘æˆ˜\", \"pinyin\": \"tiÇo zhÃ n\", \"trans\": \"challenge\"},\n    {\"word\": \"æœªæ¥\", \"pinyin\": \"wÃ¨i lÃ¡i\", \"trans\": \"future\"},\n    {\"word\": \"æ–¹å‘\", \"pinyin\": \"fÄng xiÃ ng\", \"trans\": \"direction\"}\n]",
        "trans": "This article introduces an emerging technology: Interactive Generative Video (IGV). It combines generative and interactive functions to produce high-quality video content and allows users to participate through control signals and response feedback. IGV has important applications in three major areas: gaming, artificial intelligence, and autonomous driving. The article also proposes five key modules for an ideal IGV system and analyzes the technical challenges and future directions for each module.",
        "update_ts": "2025-05-04 12:44"
    }
}