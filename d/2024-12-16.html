
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 26 papers. December 16.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">16 декабря</span> | <span id="title-articles-count">26 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2024-12-13.html">⬅️ <span id="prev-date">13.12</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-12-17.html">➡️ <span id="next-date">17.12</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2024-12.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '16 декабря', 'en': 'December 16', 'zh': '12月16日'};
        let feedDateNext = {'ru': '17.12', 'en': '12/17', 'zh': '12月17日'};
        let feedDatePrev = {'ru': '13.12', 'en': '12/13', 'zh': '12月13日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2412.09596', 'title': 'InternLM-XComposer2.5-OmniLive: A Comprehensive Multimodal System for Long-term Streaming Video and Audio Interactions', 'url': 'https://huggingface.co/papers/2412.09596', 'abstract': 'Creating AI systems that can interact with environments over long periods, similar to human cognition, has been a longstanding research goal. Recent advancements in multimodal large language models (MLLMs) have made significant strides in open-world understanding. However, the challenge of continuous and simultaneous streaming perception, memory, and reasoning remains largely unexplored. Current MLLMs are constrained by their sequence-to-sequence architecture, which limits their ability to process inputs and generate responses simultaneously, akin to being unable to think while perceiving. Furthermore, relying on long contexts to store historical data is impractical for long-term interactions, as retaining all information becomes costly and inefficient. Therefore, rather than relying on a single foundation model to perform all functions, this project draws inspiration from the concept of the Specialized Generalist AI and introduces disentangled streaming perception, reasoning, and memory mechanisms, enabling real-time interaction with streaming video and audio input. The proposed framework InternLM-XComposer2.5-OmniLive (IXC2.5-OL) consists of three key modules: (1) Streaming Perception Module: Processes multimodal information in real-time, storing key details in memory and triggering reasoning in response to user queries. (2) Multi-modal Long Memory Module: Integrates short-term and long-term memory, compressing short-term memories into long-term ones for efficient retrieval and improved accuracy. (3) Reasoning Module: Responds to queries and executes reasoning tasks, coordinating with the perception and memory modules. This project simulates human-like cognition, enabling multimodal large language models to provide continuous and adaptive service over time.', 'score': 75, 'issue_id': 1103, 'pub_date': '2024-12-12', 'pub_date_card': {'ru': '12 декабря', 'en': 'December 12', 'zh': '12月12日'}, 'hash': 'f14f0c4b833358e7', 'authors': ['Pan Zhang', 'Xiaoyi Dong', 'Yuhang Cao', 'Yuhang Zang', 'Rui Qian', 'Xilin Wei', 'Lin Chen', 'Yifei Li', 'Junbo Niu', 'Shuangrui Ding', 'Qipeng Guo', 'Haodong Duan', 'Xin Chen', 'Han Lv', 'Zheng Nie', 'Min Zhang', 'Bin Wang', 'Wenwei Zhang', 'Xinyue Zhang', 'Jiaye Ge', 'Wei Li', 'Jingwen Li', 'Zhongying Tu', 'Conghui He', 'Xingcheng Zhang', 'Kai Chen', 'Yu Qiao', 'Dahua Lin', 'Jiaqi Wang'], 'affiliations': ['Beihang University', 'Fudan University', 'SenseTime Group', 'Shanghai Artificial Intelligence Laboratory', 'The Chinese University of Hong Kong', 'Tsinghua University', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2412.09596.jpg', 'data': {'categories': ['#long_context', '#audio', '#cv', '#multimodal', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Непрерывное восприятие и рассуждение: новый шаг к человекоподобному ИИ', 'desc': 'Данная статья представляет новый подход к созданию систем искусственного интеллекта, способных к длительному взаимодействию с окружающей средой. Авторы предлагают фреймворк InternLM-XComposer2.5-OmniLive, состоящий из трех ключевых модулей: потокового восприятия, мультимодальной долговременной памяти и рассуждения. Система способна обрабатывать потоковые видео и аудио данные в реальном времени, сохраняя ключевую информацию в памяти и выполняя рассуждения по запросу пользователя. Этот подход имитирует человеческое познание, позволяя мультимодальным большим языковым моделям предоставлять непрерывные и адаптивные услуги в течение длительного времени.'}, 'en': {'title': 'Empowering AI with Human-like Cognition for Real-time Interaction', 'desc': "This paper presents a new approach to creating AI systems that can interact with their environments over extended periods, similar to human thinking. It introduces a framework called InternLM-XComposer2.5-OmniLive (IXC2.5-OL), which features three modules: a Streaming Perception Module for real-time processing of multimodal inputs, a Multi-modal Long Memory Module for efficient memory management, and a Reasoning Module for responding to queries. The framework aims to overcome limitations of current multimodal large language models (MLLMs) by enabling simultaneous perception, memory, and reasoning. This approach allows for continuous and adaptive interactions, enhancing the AI's ability to function in dynamic environments."}, 'zh': {'title': '模拟人类认知的流式交互AI系统', 'desc': '本论文探讨了创建能够长时间与环境互动的人工智能系统，类似于人类的认知能力。尽管多模态大型语言模型（MLLMs）在开放世界理解方面取得了进展，但在连续和同时的感知、记忆和推理方面仍面临挑战。当前的MLLMs受限于序列到序列的架构，无法同时处理输入和生成响应。为了解决这一问题，本文提出了InternLM-XComposer2.5-OmniLive框架，通过分离的流式感知、推理和记忆机制，模拟人类认知，实现实时的多模态交互。'}}}, {'id': 'https://huggingface.co/papers/2412.08905', 'title': 'Phi-4 Technical Report', 'url': 'https://huggingface.co/papers/2412.08905', 'abstract': 'We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.', 'score': 65, 'issue_id': 1105, 'pub_date': '2024-12-12', 'pub_date_card': {'ru': '12 декабря', 'en': 'December 12', 'zh': '12月12日'}, 'hash': '5b5d18f4e7e9fad9', 'authors': ['Marah Abdin', 'Jyoti Aneja', 'Harkirat Behl', 'Sébastien Bubeck', 'Ronen Eldan', 'Suriya Gunasekar', 'Michael Harrison', 'Russell J. Hewett', 'Mojan Javaheripi', 'Piero Kauffmann', 'James R. Lee', 'Yin Tat Lee', 'Yuanzhi Li', 'Weishung Liu', 'Caio C. T. Mendes', 'Anh Nguyen', 'Eric Price', 'Gustavo de Rosa', 'Olli Saarikivi', 'Adil Salim', 'Shital Shah', 'Xin Wang', 'Rachel Ward', 'Yue Wu', 'Dingli Yu', 'Cyril Zhang', 'Yi Zhang'], 'affiliations': ['Microsoft Research'], 'pdf_title_img': 'assets/pdf/title_img/2412.08905.jpg', 'data': {'categories': ['#data', '#reasoning', '#synthetic', '#training', '#benchmark', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Качество данных - ключ к превосходству языковой модели', 'desc': 'Статья представляет phi-4 - языковую модель с 14 миллиардами параметров, разработанную с акцентом на качество данных. В отличие от большинства моделей, phi-4 стратегически использует синтетические данные на протяжении всего процесса обучения. Модель превосходит своего учителя GPT-4 в задачах вопросов-ответов по STEM-тематике, что свидетельствует о эффективности методов генерации данных и пост-обучения. Несмотря на минимальные изменения в архитектуре, phi-4 достигает высокой производительности благодаря улучшенным данным, учебной программе и инновациям в схеме пост-обучения.'}, 'en': {'title': 'Elevating Language Models with Quality Data', 'desc': 'The phi-4 model is a large language model with 14 billion parameters that emphasizes the importance of high-quality data in its training process. Unlike typical models that rely mainly on organic data from the internet, phi-4 integrates synthetic data to enhance its learning. This approach allows phi-4 to outperform its predecessor, GPT-4, particularly in STEM-related question answering tasks, demonstrating the effectiveness of its unique data generation and training methods. Additionally, phi-4 maintains a similar architecture to phi-3 but achieves superior performance on reasoning tasks due to advancements in data quality and training strategies.'}, 'zh': {'title': '数据质量驱动的语言模型phi-4', 'desc': 'phi-4是一个拥有140亿参数的语言模型，专注于数据质量的训练方法。与大多数语言模型主要依赖于网络内容或代码等自然数据源不同，phi-4在训练过程中战略性地融入了合成数据。尽管phi-4的架构与phi-3几乎没有变化，但由于改进的数据、训练课程和后训练方案，它在推理相关的基准测试中表现出色，超越了其教师模型GPT-4。该模型在STEM领域的问答能力上显著提升，证明了其数据生成和后训练技术的有效性。'}}}, {'id': 'https://huggingface.co/papers/2412.08737', 'title': 'Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions', 'url': 'https://huggingface.co/papers/2412.08737', 'abstract': "Multimodal large language models (MLLMs) have made rapid progress in recent years, yet continue to struggle with low-level visual perception (LLVP) -- particularly the ability to accurately describe the geometric details of an image. This capability is crucial for applications in areas such as robotics, medical image analysis, and manufacturing. In this paper, we first introduce Geoperception, a benchmark designed to evaluate an MLLM's ability to accurately transcribe 2D geometric information from an image. Using this benchmark, we demonstrate the limitations of leading MLLMs, and then conduct a comprehensive empirical study to explore strategies for improving their performance on geometric tasks. Our findings highlight the benefits of certain model architectures, training techniques, and data strategies, including the use of high-fidelity synthetic data and multi-stage training with a data curriculum. Notably, we find that a data curriculum enables models to learn challenging geometry understanding tasks which they fail to learn from scratch. Leveraging these insights, we develop Euclid, a family of models specifically optimized for strong low-level geometric perception. Although purely trained on synthetic multimodal data, Euclid shows strong generalization ability to novel geometry shapes. For instance, Euclid outperforms the best closed-source model, Gemini-1.5-Pro, by up to 58.56% on certain Geoperception benchmark tasks and 10.65% on average across all tasks.", 'score': 35, 'issue_id': 1104, 'pub_date': '2024-12-11', 'pub_date_card': {'ru': '11 декабря', 'en': 'December 11', 'zh': '12月11日'}, 'hash': 'a3850f5730e90caa', 'authors': ['Jiarui Zhang', 'Ollie Liu', 'Tianyu Yu', 'Jinyi Hu', 'Willie Neiswanger'], 'affiliations': ['Tsinghua University', 'University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2412.08737.jpg', 'data': {'categories': ['#benchmark', '#architecture', '#low_resource', '#synthetic', '#multimodal', '#optimization', '#training', '#data'], 'emoji': '📐', 'ru': {'title': 'Прорыв в геометрическом восприятии для мультимодальных ИИ-моделей', 'desc': 'Статья представляет новый бенчмарк Geoperception для оценки способности мультимодальных больших языковых моделей (MLLM) точно описывать геометрические детали изображений. Авторы демонстрируют ограничения существующих MLLM в задачах низкоуровневого визуального восприятия и проводят эмпирическое исследование стратегий улучшения их производительности. На основе полученных результатов разработано семейство моделей Euclid, оптимизированных для геометрического восприятия. Модели Euclid, обученные только на синтетических данных, показывают сильную обобщающую способность и превосходят лучшие закрытые модели на бенчмарке Geoperception.'}, 'en': {'title': 'Enhancing Geometric Perception in MLLMs with Euclid', 'desc': "This paper addresses the challenges faced by multimodal large language models (MLLMs) in low-level visual perception, specifically in accurately describing geometric details in images. It introduces a new benchmark called Geoperception to evaluate MLLMs' performance in transcribing 2D geometric information. The authors conduct an empirical study to identify effective strategies for enhancing MLLMs' geometric task performance, including model architecture improvements and the use of synthetic data. They present Euclid, a model family optimized for geometric perception, which demonstrates significant performance gains over existing models on the Geoperception benchmark."}, 'zh': {'title': '提升几何感知能力的关键策略', 'desc': '多模态大型语言模型（MLLMs）在近年来取得了快速进展，但在低级视觉感知（LLVP）方面仍然存在困难，特别是在准确描述图像的几何细节方面。本文首先介绍了Geoperception基准，用于评估MLLM从图像中准确转录二维几何信息的能力。通过这一基准，我们展示了领先的MLLM的局限性，并进行了全面的实证研究，以探索提高其几何任务性能的策略。我们的研究结果强调了特定模型架构、训练技术和数据策略的好处，包括使用高保真合成数据和多阶段训练的数据课程。'}}}, {'id': 'https://huggingface.co/papers/2412.08635', 'title': 'Multimodal Latent Language Modeling with Next-Token Diffusion', 'url': 'https://huggingface.co/papers/2412.08635', 'abstract': 'Multimodal generative models require a unified approach to handle both discrete data (e.g., text and code) and continuous data (e.g., image, audio, video). In this work, we propose Latent Language Modeling (LatentLM), which seamlessly integrates continuous and discrete data using causal Transformers. Specifically, we employ a variational autoencoder (VAE) to represent continuous data as latent vectors and introduce next-token diffusion for autoregressive generation of these vectors. Additionally, we develop sigma-VAE to address the challenges of variance collapse, which is crucial for autoregressive modeling. Extensive experiments demonstrate the effectiveness of LatentLM across various modalities. In image generation, LatentLM surpasses Diffusion Transformers in both performance and scalability. When integrated into multimodal large language models, LatentLM provides a general-purpose interface that unifies multimodal generation and understanding. Experimental results show that LatentLM achieves favorable performance compared to Transfusion and vector quantized models in the setting of scaling up training tokens. In text-to-speech synthesis, LatentLM outperforms the state-of-the-art VALL-E 2 model in speaker similarity and robustness, while requiring 10x fewer decoding steps. The results establish LatentLM as a highly effective and scalable approach to advance large multimodal models.', 'score': 30, 'issue_id': 1103, 'pub_date': '2024-12-11', 'pub_date_card': {'ru': '11 декабря', 'en': 'December 11', 'zh': '12月11日'}, 'hash': 'a04f1d532626975e', 'authors': ['Yutao Sun', 'Hangbo Bao', 'Wenhui Wang', 'Zhiliang Peng', 'Li Dong', 'Shaohan Huang', 'Jianyong Wang', 'Furu Wei'], 'affiliations': ['Microsoft Research', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2412.08635.jpg', 'data': {'categories': ['#diffusion', '#optimization', '#audio', '#video', '#cv', '#multimodal', '#training'], 'emoji': '🧠', 'ru': {'title': 'LatentLM: Объединяя дискретное и непрерывное в мультимодальных моделях', 'desc': 'Статья представляет Latent Language Modeling (LatentLM) - унифицированный подход к обработке дискретных и непрерывных данных в мультимодальных генеративных моделях. LatentLM использует вариационный автоэнкодер (VAE) для представления непрерывных данных в виде латентных векторов и вводит диффузию следующего токена для авторегрессивной генерации этих векторов. Модель демонстрирует превосходные результаты в генерации изображений, мультимодальном понимании и генерации, а также в синтезе речи по тексту. LatentLM представляет собой эффективный и масштабируемый подход для развития крупных мультимодальных моделей.'}, 'en': {'title': 'Unifying Discrete and Continuous Data with LatentLM', 'desc': 'This paper introduces Latent Language Modeling (LatentLM), a novel framework that effectively combines discrete data like text and code with continuous data such as images and audio. It utilizes causal Transformers and a variational autoencoder (VAE) to convert continuous data into latent vectors, enhancing the generation process through next-token diffusion. The sigma-VAE is introduced to tackle variance collapse, which is essential for improving autoregressive modeling. Experimental results show that LatentLM excels in various tasks, outperforming existing models in image generation and text-to-speech synthesis while maintaining scalability and efficiency.'}, 'zh': {'title': '潜在语言建模：统一多模态生成的创新之路', 'desc': '本研究提出了一种新的多模态生成模型，称为潜在语言建模（LatentLM），旨在统一处理离散数据（如文本和代码）与连续数据（如图像、音频和视频）。我们使用变分自编码器（VAE）将连续数据表示为潜在向量，并引入下一标记扩散方法进行自回归生成。这种方法解决了自回归建模中的方差崩溃问题，提升了模型的性能和可扩展性。实验结果表明，LatentLM在多种模态下表现优异，尤其在图像生成和文本到语音合成任务中超越了现有的最先进模型。'}}}, {'id': 'https://huggingface.co/papers/2412.09605', 'title': 'AgentTrek: Agent Trajectory Synthesis via Guiding Replay with Web Tutorials', 'url': 'https://huggingface.co/papers/2412.09605', 'abstract': 'Graphical User Interface (GUI) agents hold great potential for automating complex tasks across diverse digital environments, from web applications to desktop software. However, the development of such agents is hindered by the lack of high-quality, multi-step trajectory data required for effective training. Existing approaches rely on expensive and labor-intensive human annotation, making them unsustainable at scale. To address this challenge, we propose AgentTrek, a scalable data synthesis pipeline that generates high-quality GUI agent trajectories by leveraging web tutorials. Our method automatically gathers tutorial-like texts from the internet, transforms them into task goals with step-by-step instructions, and employs a visual-language model agent to simulate their execution in a real digital environment. A VLM-based evaluator ensures the correctness of the generated trajectories. We demonstrate that training GUI agents with these synthesized trajectories significantly improves their grounding and planning performance over the current models. Moreover, our approach is more cost-efficient compared to traditional human annotation methods. This work underscores the potential of guided replay with web tutorials as a viable strategy for large-scale GUI agent training, paving the way for more capable and autonomous digital agents.', 'score': 21, 'issue_id': 1106, 'pub_date': '2024-12-12', 'pub_date_card': {'ru': '12 декабря', 'en': 'December 12', 'zh': '12月12日'}, 'hash': '298173f67e05442e', 'authors': ['Yiheng Xu', 'Dunjie Lu', 'Zhennan Shen', 'Junli Wang', 'Zekun Wang', 'Yuchen Mao', 'Caiming Xiong', 'Tao Yu'], 'affiliations': ['Salesforce Research', 'University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2412.09605.jpg', 'data': {'categories': ['#data', '#dataset', '#training', '#optimization', '#agents', '#synthetic'], 'emoji': '🤖', 'ru': {'title': 'AgentTrek: Революция в обучении GUI-агентов с помощью веб-руководств', 'desc': 'Статья представляет AgentTrek - масштабируемый конвейер для синтеза данных, который генерирует высококачественные траектории агентов графического интерфейса пользователя (GUI) с помощью веб-руководств. Метод автоматически собирает тексты, похожие на учебные пособия, из интернета, преобразует их в цели задач с пошаговыми инструкциями и использует агента на основе визуально-языковой модели для симуляции их выполнения в реальной цифровой среде. Обучение GUI-агентов с использованием этих синтезированных траекторий значительно улучшает их производительность по сравнению с текущими моделями. Этот подход более экономичен по сравнению с традиционными методами аннотации данных человеком.'}, 'en': {'title': 'Automating GUI Agent Training with Web Tutorials', 'desc': 'This paper introduces AgentTrek, a novel approach for generating high-quality training data for Graphical User Interface (GUI) agents. It addresses the challenge of obtaining multi-step trajectory data by automatically synthesizing it from web tutorials, thus eliminating the need for costly human annotation. The method involves transforming tutorial texts into actionable task goals and using a visual-language model to simulate the execution of these tasks. The results show that training GUI agents with these synthesized trajectories enhances their performance in grounding and planning, making the process more efficient and scalable.'}, 'zh': {'title': '利用网络教程提升GUI代理训练效率', 'desc': '本文提出了一种名为AgentTrek的数据合成管道，用于生成高质量的图形用户界面（GUI）代理轨迹。该方法通过自动收集网络教程文本，将其转化为任务目标和逐步指令，并利用视觉语言模型代理在真实数字环境中模拟执行。与传统的人类标注方法相比，我们的方法在成本上更具效率，同时显著提高了GUI代理的基础和规划性能。此研究展示了利用网络教程进行引导重放的潜力，为大规模GUI代理训练开辟了新的可能性。'}}}, {'id': 'https://huggingface.co/papers/2412.09618', 'title': 'EasyRef: Omni-Generalized Group Image Reference for Diffusion Models via Multimodal LLM', 'url': 'https://huggingface.co/papers/2412.09618', 'abstract': "Significant achievements in personalization of diffusion models have been witnessed. Conventional tuning-free methods mostly encode multiple reference images by averaging their image embeddings as the injection condition, but such an image-independent operation cannot perform interaction among images to capture consistent visual elements within multiple references. Although the tuning-based Low-Rank Adaptation (LoRA) can effectively extract consistent elements within multiple images through the training process, it necessitates specific finetuning for each distinct image group. This paper introduces EasyRef, a novel plug-and-play adaptation method that enables diffusion models to be conditioned on multiple reference images and the text prompt. To effectively exploit consistent visual elements within multiple images, we leverage the multi-image comprehension and instruction-following capabilities of the multimodal large language model (MLLM), prompting it to capture consistent visual elements based on the instruction. Besides, injecting the MLLM's representations into the diffusion process through adapters can easily generalize to unseen domains, mining the consistent visual elements within unseen data. To mitigate computational costs and enhance fine-grained detail preservation, we introduce an efficient reference aggregation strategy and a progressive training scheme. Finally, we introduce MRBench, a new multi-reference image generation benchmark. Experimental results demonstrate EasyRef surpasses both tuning-free methods like IP-Adapter and tuning-based methods like LoRA, achieving superior aesthetic quality and robust zero-shot generalization across diverse domains.", 'score': 20, 'issue_id': 1104, 'pub_date': '2024-12-12', 'pub_date_card': {'ru': '12 декабря', 'en': 'December 12', 'zh': '12月12日'}, 'hash': 'ad7e249fb9dcb420', 'authors': ['Zhuofan Zong', 'Dongzhi Jiang', 'Bingqi Ma', 'Guanglu Song', 'Hao Shao', 'Dazhong Shen', 'Yu Liu', 'Hongsheng Li'], 'affiliations': ['CUHK MMLab', 'SenseTime Research', 'Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2412.09618.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#multimodal', '#diffusion', '#optimization', '#training'], 'emoji': '🖼️', 'ru': {'title': 'EasyRef: умная адаптация диффузионных моделей к нескольким референсам', 'desc': 'Статья представляет EasyRef - новый метод адаптации для диффузионных моделей, позволяющий учитывать несколько референсных изображений и текстовый запрос. Авторы используют мультимодальную языковую модель (MLLM) для выявления согласованных визуальных элементов в нескольких изображениях. Метод превосходит как безнастроечные подходы вроде IP-Adapter, так и методы с дообучением типа LoRA, демонстрируя лучшее качество и обобщение на новые домены. Также представлен новый бенчмарк MRBench для оценки генерации изображений по нескольким референсам.'}, 'en': {'title': 'EasyRef: Enhancing Diffusion Models with Multi-Image Conditioning', 'desc': 'This paper presents EasyRef, a new method for personalizing diffusion models using multiple reference images and text prompts. Unlike traditional methods that simply average image embeddings, EasyRef captures consistent visual elements by leveraging the capabilities of multimodal large language models (MLLMs). It introduces a novel reference aggregation strategy and a progressive training scheme to enhance detail preservation while reducing computational costs. Experimental results show that EasyRef outperforms existing methods, achieving better aesthetic quality and generalization across various domains.'}, 'zh': {'title': 'EasyRef：多图像参考的个性化扩散模型', 'desc': '本文介绍了一种名为EasyRef的新方法，用于在扩散模型中实现多图像参考的个性化。传统的方法通过平均图像嵌入来编码多个参考图像，但无法捕捉图像之间的一致视觉元素。EasyRef利用多模态大语言模型（MLLM）的能力，能够根据指令提取一致的视觉元素，并通过适配器将其注入扩散过程中。实验结果表明，EasyRef在美学质量和零样本泛化能力上优于现有的调优无关和调优相关的方法。'}}}, {'id': 'https://huggingface.co/papers/2412.09619', 'title': 'SnapGen: Taming High-Resolution Text-to-Image Models for Mobile Devices with Efficient Architectures and Training', 'url': 'https://huggingface.co/papers/2412.09619', 'abstract': 'Existing text-to-image (T2I) diffusion models face several limitations, including large model sizes, slow runtime, and low-quality generation on mobile devices. This paper aims to address all of these challenges by developing an extremely small and fast T2I model that generates high-resolution and high-quality images on mobile platforms. We propose several techniques to achieve this goal. First, we systematically examine the design choices of the network architecture to reduce model parameters and latency, while ensuring high-quality generation. Second, to further improve generation quality, we employ cross-architecture knowledge distillation from a much larger model, using a multi-level approach to guide the training of our model from scratch. Third, we enable a few-step generation by integrating adversarial guidance with knowledge distillation. For the first time, our model SnapGen, demonstrates the generation of 1024x1024 px images on a mobile device around 1.4 seconds. On ImageNet-1K, our model, with only 372M parameters, achieves an FID of 2.06 for 256x256 px generation. On T2I benchmarks (i.e., GenEval and DPG-Bench), our model with merely 379M parameters, surpasses large-scale models with billions of parameters at a significantly smaller size (e.g., 7x smaller than SDXL, 14x smaller than IF-XL).', 'score': 19, 'issue_id': 1106, 'pub_date': '2024-12-12', 'pub_date_card': {'ru': '12 декабря', 'en': 'December 12', 'zh': '12月12日'}, 'hash': '335b0aa017bc9495', 'authors': ['Dongting Hu', 'Jierun Chen', 'Xijie Huang', 'Huseyin Coskun', 'Arpit Sahni', 'Aarush Gupta', 'Anujraaj Goyal', 'Dishani Lahiri', 'Rajesh Singh', 'Yerlan Idelbayev', 'Junli Cao', 'Yanyu Li', 'Kwang-Ting Cheng', 'S. -H. Gary Chan', 'Mingming Gong', 'Sergey Tulyakov', 'Anil Kag', 'Yanwu Xu', 'Jian Ren'], 'affiliations': ['HKUST', 'MBZUAI', 'Snap Inc.', 'The University of Melbourne'], 'pdf_title_img': 'assets/pdf/title_img/2412.09619.jpg', 'data': {'categories': ['#architecture', '#small_models', '#training', '#optimization', '#diffusion', '#benchmark'], 'emoji': '📱', 'ru': {'title': 'Компактная и быстрая генерация изображений на мобильных устройствах', 'desc': 'Статья представляет новую модель генерации изображений по тексту, SnapGen, разработанную для мобильных устройств. Модель отличается малым размером (372-379 миллионов параметров) и высокой скоростью работы, генерируя изображения 1024x1024 пикселей за 1.4 секунды на мобильных устройствах. Авторы применили оптимизацию архитектуры, дистилляцию знаний и адверсариальное обучение для достижения высокого качества генерации. SnapGen превосходит по качеству генерации крупные модели, имея при этом значительно меньший размер.'}, 'en': {'title': 'SnapGen: High-Quality Image Generation on Mobile in Seconds!', 'desc': "This paper presents SnapGen, a compact and efficient text-to-image diffusion model designed for mobile devices. It addresses the common issues of large model sizes and slow runtimes by optimizing the network architecture to reduce parameters and latency while maintaining high-quality image generation. The authors utilize cross-architecture knowledge distillation to enhance the model's performance, allowing it to learn effectively from a larger model. SnapGen achieves impressive results, generating 1024x1024 pixel images in approximately 1.4 seconds, outperforming larger models with significantly fewer parameters."}, 'zh': {'title': '小而快的文本到图像生成模型', 'desc': '现有的文本到图像（T2I）扩散模型存在一些限制，如模型体积大、运行速度慢以及在移动设备上生成图像质量低。本文旨在通过开发一个极小且快速的T2I模型，解决这些挑战，使其能够在移动平台上生成高分辨率和高质量的图像。我们提出了几种技术来实现这一目标，包括优化网络架构设计以减少模型参数和延迟，同时确保生成质量。我们的模型SnapGen首次在移动设备上以约1.4秒的时间生成1024x1024像素的图像，并在多个基准测试中超越了大型模型。'}}}, {'id': 'https://huggingface.co/papers/2412.09501', 'title': 'Lyra: An Efficient and Speech-Centric Framework for Omni-Cognition', 'url': 'https://huggingface.co/papers/2412.09501', 'abstract': 'As Multi-modal Large Language Models (MLLMs) evolve, expanding beyond single-domain capabilities is essential to meet the demands for more versatile and efficient AI. However, previous omni-models have insufficiently explored speech, neglecting its integration with multi-modality. We introduce Lyra, an efficient MLLM that enhances multimodal abilities, including advanced long-speech comprehension, sound understanding, cross-modality efficiency, and seamless speech interaction. To achieve efficiency and speech-centric capabilities, Lyra employs three strategies: (1) leveraging existing open-source large models and a proposed multi-modality LoRA to reduce training costs and data requirements; (2) using a latent multi-modality regularizer and extractor to strengthen the relationship between speech and other modalities, thereby enhancing model performance; and (3) constructing a high-quality, extensive dataset that includes 1.5M multi-modal (language, vision, audio) data samples and 12K long speech samples, enabling Lyra to handle complex long speech inputs and achieve more robust omni-cognition. Compared to other omni-methods, Lyra achieves state-of-the-art performance on various vision-language, vision-speech, and speech-language benchmarks, while also using fewer computational resources and less training data.', 'score': 17, 'issue_id': 1106, 'pub_date': '2024-12-12', 'pub_date_card': {'ru': '12 декабря', 'en': 'December 12', 'zh': '12月12日'}, 'hash': 'a125d02e4026bd64', 'authors': ['Zhisheng Zhong', 'Chengyao Wang', 'Yuqi Liu', 'Senqiao Yang', 'Longxiang Tang', 'Yuechen Zhang', 'Jingyao Li', 'Tianyuan Qu', 'Yanwei Li', 'Yukang Chen', 'Shaozuo Yu', 'Sitong Wu', 'Eric Lo', 'Shu Liu', 'Jiaya Jia'], 'affiliations': ['CUHK', 'HKUST', 'SmartMore'], 'pdf_title_img': 'assets/pdf/title_img/2412.09501.jpg', 'data': {'categories': ['#open_source', '#dataset', '#long_context', '#multimodal', '#benchmark'], 'emoji': '🎭', 'ru': {'title': 'Lyra: Эффективная мультимодальная модель для комплексного понимания речи и изображений', 'desc': 'Lyra - это эффективная мультимодальная большая языковая модель (MLLM), которая улучшает способности в области понимания длинной речи, звука и кросс-модального взаимодействия. Модель использует стратегии, включающие использование существующих открытых моделей, мультимодальную LoRA и латентный мультимодальный регуляризатор для повышения эффективности. Lyra обучена на большом наборе данных, содержащем 1,5 млн мультимодальных образцов и 12 тыс. образцов длинной речи. Модель достигает передовых результатов на различных тестах, связанных с обработкой изображений, речи и языка, при этом используя меньше вычислительных ресурсов и данных для обучения.'}, 'en': {'title': 'Lyra: Revolutionizing Multi-modal AI with Speech Integration', 'desc': 'The paper presents Lyra, a Multi-modal Large Language Model (MLLM) designed to improve the integration of speech with other modalities like vision and text. It addresses the limitations of previous omni-models by enhancing long-speech comprehension and sound understanding. Lyra employs innovative strategies such as a multi-modality LoRA for efficient training, a latent regularizer to strengthen modality relationships, and a comprehensive dataset with 1.5 million samples. As a result, Lyra outperforms existing models in various benchmarks while being more resource-efficient.'}, 'zh': {'title': 'Lyra：高效的多模态语言模型', 'desc': '随着多模态大型语言模型（MLLM）的发展，超越单一领域的能力变得至关重要。Lyra是一种高效的MLLM，专注于增强多模态能力，特别是在长语音理解和声音理解方面。它通过利用现有的开源大模型、引入多模态LoRA和构建高质量数据集来提高效率和语音中心能力。与其他全能模型相比，Lyra在多个基准测试中表现出色，同时使用更少的计算资源和训练数据。'}}}, {'id': 'https://huggingface.co/papers/2412.09569', 'title': 'JuStRank: Benchmarking LLM Judges for System Ranking', 'url': 'https://huggingface.co/papers/2412.09569', 'abstract': "Given the rapid progress of generative AI, there is a pressing need to systematically compare and choose between the numerous models and configurations available. The scale and versatility of such evaluations make the use of LLM-based judges a compelling solution for this challenge. Crucially, this approach requires first to validate the quality of the LLM judge itself. Previous work has focused on instance-based assessment of LLM judges, where a judge is evaluated over a set of responses, or response pairs, while being agnostic to their source systems. We argue that this setting overlooks critical factors affecting system-level ranking, such as a judge's positive or negative bias towards certain systems. To address this gap, we conduct the first large-scale study of LLM judges as system rankers. System scores are generated by aggregating judgment scores over multiple system outputs, and the judge's quality is assessed by comparing the resulting system ranking to a human-based ranking. Beyond overall judge assessment, our analysis provides a fine-grained characterization of judge behavior, including their decisiveness and bias.", 'score': 15, 'issue_id': 1110, 'pub_date': '2024-12-12', 'pub_date_card': {'ru': '12 декабря', 'en': 'December 12', 'zh': '12月12日'}, 'hash': '1615862b75e39998', 'authors': ['Ariel Gera', 'Odellia Boni', 'Yotam Perlitz', 'Roy Bar-Haim', 'Lilach Eden', 'Asaf Yehudai'], 'affiliations': ['IBM Research'], 'pdf_title_img': 'assets/pdf/title_img/2412.09569.jpg', 'data': {'categories': ['#interpretability', '#alignment', '#benchmark', '#rlhf'], 'emoji': '⚖️', 'ru': {'title': 'LLM как беспристрастные судьи ИИ-систем', 'desc': 'Это исследование посвящено оценке качества языковых моделей (LLM) в роли судей для сравнения и выбора между различными генеративными ИИ-системами. Авторы предлагают новый подход к оценке LLM-судей на уровне ранжирования систем, а не отдельных ответов. Они проводят масштабное исследование, сравнивая ранжирование систем LLM-судьями с ранжированием, основанным на оценках людей. Анализ включает детальную характеристику поведения судей, включая их решительность и предвзятость.'}, 'en': {'title': 'Evaluating AI Models with LLM Judges: A Systematic Approach', 'desc': 'This paper addresses the challenge of evaluating generative AI models by proposing the use of large language model (LLM) judges for systematic comparisons. It highlights the importance of validating the quality of these LLM judges, as previous methods have only assessed them based on individual responses without considering their biases towards different systems. The authors conduct a large-scale study to evaluate LLM judges as system rankers, comparing their rankings to those made by humans. Additionally, the study provides insights into the behavior of LLM judges, including their decisiveness and potential biases, which are crucial for accurate model evaluation.'}, 'zh': {'title': '系统排名的新视角：评估LLM的质量与偏见', 'desc': '随着生成性人工智能的快速发展，系统比较和选择众多模型和配置的需求日益迫切。本文提出使用基于大型语言模型（LLM）的评估者来解决这一挑战，但首先需要验证LLM评估者的质量。以往的研究主要关注于对LLM评估者的实例评估，而忽视了影响系统级排名的重要因素，如评估者对某些系统的偏见。我们进行了一项大规模研究，评估LLM评估者作为系统排名者的表现，并通过与人类排名的比较来评估其质量。'}}}, {'id': 'https://huggingface.co/papers/2412.09593', 'title': 'Neural LightRig: Unlocking Accurate Object Normal and Material Estimation with Multi-Light Diffusion', 'url': 'https://huggingface.co/papers/2412.09593', 'abstract': 'Recovering the geometry and materials of objects from a single image is challenging due to its under-constrained nature. In this paper, we present Neural LightRig, a novel framework that boosts intrinsic estimation by leveraging auxiliary multi-lighting conditions from 2D diffusion priors. Specifically, 1) we first leverage illumination priors from large-scale diffusion models to build our multi-light diffusion model on a synthetic relighting dataset with dedicated designs. This diffusion model generates multiple consistent images, each illuminated by point light sources in different directions. 2) By using these varied lighting images to reduce estimation uncertainty, we train a large G-buffer model with a U-Net backbone to accurately predict surface normals and materials. Extensive experiments validate that our approach significantly outperforms state-of-the-art methods, enabling accurate surface normal and PBR material estimation with vivid relighting effects. Code and dataset are available on our project page at https://projects.zxhezexin.com/neural-lightrig.', 'score': 15, 'issue_id': 1103, 'pub_date': '2024-12-12', 'pub_date_card': {'ru': '12 декабря', 'en': 'December 12', 'zh': '12月12日'}, 'hash': '8a03e5d63e2849d3', 'authors': ['Zexin He', 'Tengfei Wang', 'Xin Huang', 'Xingang Pan', 'Ziwei Liu'], 'affiliations': ['Nanyang Technological University', 'Shanghai AI Lab', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2412.09593.jpg', 'data': {'categories': ['#diffusion', '#synthetic', '#open_source', '#cv', '#3d', '#dataset'], 'emoji': '💡', 'ru': {'title': 'Улучшение оценки 3D-геометрии с помощью синтетического освещения', 'desc': 'Статья представляет Neural LightRig - новый подход к оценке внутренних свойств объектов на изображении. Метод использует диффузионные модели для генерации вспомогательных изображений с разным освещением. На основе этих изображений обучается U-Net модель для точного предсказания нормалей поверхности и материалов. Эксперименты показывают, что подход значительно превосходит современные методы в задаче оценки нормалей и PBR-материалов.'}, 'en': {'title': 'Neural LightRig: Enhancing Object Geometry and Material Estimation with Multi-lighting Diffusion', 'desc': 'This paper introduces Neural LightRig, a framework designed to improve the estimation of object geometry and materials from a single image. It utilizes a multi-lighting approach by generating various images under different lighting conditions using a diffusion model trained on a synthetic dataset. The framework employs a U-Net backbone to create a G-buffer model that predicts surface normals and materials more accurately. The results show that Neural LightRig outperforms existing methods, providing enhanced surface normal and physically-based rendering (PBR) material estimation with realistic lighting effects.'}, 'zh': {'title': '从单图像中恢复物体几何与材料的创新方法', 'desc': '本论文提出了一种名为Neural LightRig的新框架，旨在从单张图像中恢复物体的几何形状和材料。我们利用大规模扩散模型的光照先验，构建了一个多光照扩散模型，以生成多个一致的图像，这些图像由不同方向的点光源照明。通过使用这些多样化的光照图像来减少估计的不确定性，我们训练了一个大型G-buffer模型，以准确预测表面法线和材料。实验结果表明，我们的方法在表面法线和PBR材料估计方面显著优于现有的最先进方法。'}}}, {'id': 'https://huggingface.co/papers/2412.05994', 'title': 'PIG: Physics-Informed Gaussians as Adaptive Parametric Mesh Representations', 'url': 'https://huggingface.co/papers/2412.05994', 'abstract': 'The approximation of Partial Differential Equations (PDEs) using neural networks has seen significant advancements through Physics-Informed Neural Networks (PINNs). Despite their straightforward optimization framework and flexibility in implementing various PDEs, PINNs often suffer from limited accuracy due to the spectral bias of Multi-Layer Perceptrons (MLPs), which struggle to effectively learn high-frequency and non-linear components. Recently, parametric mesh representations in combination with neural networks have been investigated as a promising approach to eliminate the inductive biases of neural networks. However, they usually require very high-resolution grids and a large number of collocation points to achieve high accuracy while avoiding overfitting issues. In addition, the fixed positions of the mesh parameters restrict their flexibility, making it challenging to accurately approximate complex PDEs. To overcome these limitations, we propose Physics-Informed Gaussians (PIGs), which combine feature embeddings using Gaussian functions with a lightweight neural network. Our approach uses trainable parameters for the mean and variance of each Gaussian, allowing for dynamic adjustment of their positions and shapes during training. This adaptability enables our model to optimally approximate PDE solutions, unlike models with fixed parameter positions. Furthermore, the proposed approach maintains the same optimization framework used in PINNs, allowing us to benefit from their excellent properties. Experimental results show the competitive performance of our model across various PDEs, demonstrating its potential as a robust tool for solving complex PDEs. Our project page is available at https://namgyukang.github.io/Physics-Informed-Gaussians/', 'score': 11, 'issue_id': 1106, 'pub_date': '2024-12-08', 'pub_date_card': {'ru': '8 декабря', 'en': 'December 8', 'zh': '12月8日'}, 'hash': 'e96069cfa12605a2', 'authors': ['Namgyu Kang', 'Jaemin Oh', 'Youngjoon Hong', 'Eunbyung Park'], 'affiliations': ['Department of Artificial Intelligence, Sungkyunkwan University', 'Department of Electrical and Computer Engineering, Sungkyunkwan University', 'Department of Mathematical Sciences, KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2412.05994.jpg', 'data': {'categories': ['#optimization', '#math', '#architecture'], 'emoji': '📊', 'ru': {'title': 'Адаптивные гауссовы функции для точного решения сложных дифференциальных уравнений', 'desc': 'В статье представлен новый подход к аппроксимации дифференциальных уравнений в частных производных (ДУЧП) с использованием нейронных сетей, называемый Physics-Informed Gaussians (PIGs). Этот метод объединяет вложения признаков с использованием гауссовых функций и легковесную нейронную сеть, что позволяет динамически настраивать позиции и формы гауссиан во время обучения. PIGs преодолевает ограничения традиционных Physics-Informed Neural Networks (PINNs), такие как спектральное смещение и трудности с обучением высокочастотным компонентам. Экспериментальные результаты демонстрируют конкурентоспособность модели PIGs при решении различных ДУЧП.'}, 'en': {'title': 'Dynamic Gaussian Adaptation for Enhanced PDE Solutions', 'desc': "This paper introduces Physics-Informed Gaussians (PIGs), a novel method for approximating Partial Differential Equations (PDEs) using neural networks. PIGs enhance the traditional Physics-Informed Neural Networks (PINNs) by incorporating Gaussian functions with trainable parameters, allowing for dynamic adjustments during training. This flexibility addresses the limitations of fixed mesh parameters and improves the model's ability to capture high-frequency and non-linear components of PDEs. Experimental results indicate that PIGs perform competitively across various PDEs, showcasing their potential as an effective tool for complex PDE solutions."}, 'zh': {'title': '灵活高效的偏微分方程求解新方法', 'desc': '本文提出了一种新的方法，称为物理信息高斯（PIGs），用于近似偏微分方程（PDEs）。该方法结合了高斯函数的特征嵌入和轻量级神经网络，允许在训练过程中动态调整高斯的均值和方差。与固定参数位置的模型不同，PIGs能够更灵活地逼近复杂的PDE解决方案。实验结果表明，该模型在多种PDE问题上表现出色，展示了其作为解决复杂PDE的强大工具的潜力。'}}}, {'id': 'https://huggingface.co/papers/2412.08687', 'title': 'VisionArena: 230K Real World User-VLM Conversations with Preference Labels', 'url': 'https://huggingface.co/papers/2412.08687', 'abstract': 'With the growing adoption and capabilities of vision-language models (VLMs) comes the need for benchmarks that capture authentic user-VLM interactions. In response, we create VisionArena, a dataset of 230K real-world conversations between users and VLMs. Collected from Chatbot Arena - an open-source platform where users interact with VLMs and submit preference votes - VisionArena spans 73K unique users, 45 VLMs, and 138 languages. Our dataset contains three subsets: VisionArena-Chat, 200k single and multi-turn conversations between a user and a VLM; VisionArena-Battle, 30K conversations comparing two anonymous VLMs with user preference votes; and VisionArena-Bench, an automatic benchmark of 500 diverse user prompts that efficiently approximate the live Chatbot Arena model rankings. Additionally, we highlight the types of question asked by users, the influence of response style on preference, and areas where models often fail. We find open-ended tasks like captioning and humor are highly style-dependent, and current VLMs struggle with spatial reasoning and planning tasks. Lastly, we show finetuning the same base model on VisionArena-Chat outperforms Llava-Instruct-158K, with a 17-point gain on MMMU and a 46-point gain on the WildVision benchmark. Dataset at https://huggingface.co/lmarena-ai', 'score': 10, 'issue_id': 1119, 'pub_date': '2024-12-11', 'pub_date_card': {'ru': '11 декабря', 'en': 'December 11', 'zh': '12月11日'}, 'hash': '28a9eea48c8cc79e', 'authors': ['Christopher Chou', 'Lisa Dunlap', 'Koki Mashita', 'Krishna Mandal', 'Trevor Darrell', 'Ion Stoica', 'Joseph E. Gonzalez', 'Wei-Lin Chiang'], 'affiliations': ['Stanford', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2412.08687.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#open_source', '#games', '#training', '#reasoning', '#interpretability', '#dataset'], 'emoji': '🖼️', 'ru': {'title': 'VisionArena: реальные диалоги для оценки зрительно-языковых моделей', 'desc': 'Статья представляет VisionArena - набор данных из 230 тысяч реальных диалогов между пользователями и мультимодальными языковыми моделями (VLM). Датасет включает три подмножества: чаты, сравнения моделей и автоматический бенчмарк. Исследователи анализируют типы вопросов пользователей, влияние стиля ответов на предпочтения и области, где модели часто ошибаются. Дообучение базовой модели на данных VisionArena-Chat значительно улучшает ее производительность на нескольких бенчмарках.'}, 'en': {'title': 'VisionArena: Benchmarking User Interactions with Vision-Language Models', 'desc': 'This paper introduces VisionArena, a comprehensive dataset designed to evaluate user interactions with vision-language models (VLMs). It consists of 230,000 real-world conversations collected from an open-source platform, featuring diverse user inputs across 138 languages and 45 different VLMs. The dataset is divided into three subsets: VisionArena-Chat for general conversations, VisionArena-Battle for comparative analysis of VLMs, and VisionArena-Bench for benchmarking model performance. The findings reveal that VLMs struggle with tasks requiring spatial reasoning and planning, while performance improves significantly when models are fine-tuned on the VisionArena-Chat data.'}, 'zh': {'title': '真实互动的视觉语言模型基准测试', 'desc': '随着视觉语言模型（VLM）的广泛应用，用户与VLM之间真实互动的基准测试变得越来越重要。为此，我们创建了VisionArena，这是一个包含23万条用户与VLM之间真实对话的数据集。该数据集来自Chatbot Arena平台，涵盖了73,000名独特用户、45个VLM和138种语言。我们发现，开放式任务如图像描述和幽默感对响应风格高度依赖，而当前的VLM在空间推理和规划任务上表现不佳。'}}}, {'id': 'https://huggingface.co/papers/2412.09585', 'title': 'OLA-VLM: Elevating Visual Perception in Multimodal LLMs with Auxiliary Embedding Distillation', 'url': 'https://huggingface.co/papers/2412.09585', 'abstract': "The standard practice for developing contemporary MLLMs is to feed features from vision encoder(s) into the LLM and train with natural language supervision. In this work, we posit an overlooked opportunity to optimize the intermediate LLM representations through a vision perspective (objective), i.e., solely natural language supervision is sub-optimal for the MLLM's visual understanding ability. To that end, we propose OLA-VLM, the first approach distilling knowledge into the LLM's hidden representations from a set of target visual representations. Firstly, we formulate the objective during the pretraining stage in MLLMs as a coupled optimization of predictive visual embedding and next text-token prediction. Secondly, we investigate MLLMs trained solely with natural language supervision and identify a positive correlation between the quality of visual representations within these models and their downstream performance. Moreover, upon probing our OLA-VLM, we observe improved representation quality owing to the embedding optimization. Thirdly, we demonstrate that our OLA-VLM outperforms the single and multi-encoder baselines, proving our approach's superiority over explicitly feeding the corresponding features to the LLM. Particularly, OLA-VLM boosts performance by an average margin of up to 2.5% on various benchmarks, with a notable improvement of 8.7% on the Depth task in CV-Bench. Our code is open-sourced at https://github.com/SHI-Labs/OLA-VLM .", 'score': 10, 'issue_id': 1103, 'pub_date': '2024-12-12', 'pub_date_card': {'ru': '12 декабря', 'en': 'December 12', 'zh': '12月12日'}, 'hash': 'c24690a112662ed6', 'authors': ['Jitesh Jain', 'Zhengyuan Yang', 'Humphrey Shi', 'Jianfeng Gao', 'Jianwei Yang'], 'affiliations': ['Microsoft Research, Redmond', 'SHI Labs @ Georgia Tech'], 'pdf_title_img': 'assets/pdf/title_img/2412.09585.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#architecture', '#open_source', '#cv', '#multimodal'], 'emoji': '👁️', 'ru': {'title': 'Оптимизация визуального понимания языковых моделей через призму зрения', 'desc': 'Статья представляет новый подход к обучению мультимодальных языковых моделей (MLLM) под названием OLA-VLM. В отличие от стандартной практики, авторы предлагают оптимизировать промежуточные представления языковой модели с точки зрения зрения, а не только с помощью естественного языка. Метод OLA-VLM включает в себя дистилляцию знаний из целевых визуальных представлений в скрытые представления языковой модели. Результаты показывают, что OLA-VLM превосходит базовые модели с одним и несколькими энкодерами, улучшая производительность в среднем на 2.5% на различных бенчмарках.'}, 'en': {'title': 'Enhancing Visual Understanding in MLLMs with OLA-VLM', 'desc': "This paper introduces OLA-VLM, a novel method for enhancing the visual understanding of multi-modal large language models (MLLMs). The authors argue that relying solely on natural language supervision is insufficient for optimizing the visual representations within these models. By coupling the optimization of visual embeddings with text-token prediction during pretraining, OLA-VLM improves the quality of the LLM's hidden representations. The results show that OLA-VLM outperforms existing methods, achieving significant performance gains on various benchmarks, particularly in visual tasks."}, 'zh': {'title': '优化视觉理解，提升多模态模型表现', 'desc': '本论文提出了一种新的方法，称为OLA-VLM，用于优化多模态语言模型（MLLM）的视觉理解能力。我们认为，仅依靠自然语言监督来训练MLLM的视觉表示是不够的，因此我们引入了视觉目标的优化。通过在预训练阶段同时优化视觉嵌入和文本预测，我们发现这种方法显著提高了模型的表示质量。实验结果表明，OLA-VLM在多个基准测试中表现优于传统方法，尤其在深度任务上提升了8.7%。'}}}, {'id': 'https://huggingface.co/papers/2412.09405', 'title': 'Learned Compression for Compressed Learning', 'url': 'https://huggingface.co/papers/2412.09405', 'abstract': "Modern sensors produce increasingly rich streams of high-resolution data. Due to resource constraints, machine learning systems discard the vast majority of this information via resolution reduction. Compressed-domain learning allows models to operate on compact latent representations, allowing higher effective resolution for the same budget. However, existing compression systems are not ideal for compressed learning. Linear transform coding and end-to-end learned compression systems reduce bitrate, but do not uniformly reduce dimensionality; thus, they do not meaningfully increase efficiency. Generative autoencoders reduce dimensionality, but their adversarial or perceptual objectives lead to significant information loss. To address these limitations, we introduce WaLLoC (Wavelet Learned Lossy Compression), a neural codec architecture that combines linear transform coding with nonlinear dimensionality-reducing autoencoders. WaLLoC sandwiches a shallow, asymmetric autoencoder and entropy bottleneck between an invertible wavelet packet transform. Across several key metrics, WaLLoC outperforms the autoencoders used in state-of-the-art latent diffusion models. WaLLoC does not require perceptual or adversarial losses to represent high-frequency detail, providing compatibility with modalities beyond RGB images and stereo audio. WaLLoC's encoder consists almost entirely of linear operations, making it exceptionally efficient and suitable for mobile computing, remote sensing, and learning directly from compressed data. We demonstrate WaLLoC's capability for compressed-domain learning across several tasks, including image classification, colorization, document understanding, and music source separation. Our code, experiments, and pre-trained audio and image codecs are available at https://ut-sysml.org/walloc", 'score': 9, 'issue_id': 1104, 'pub_date': '2024-12-12', 'pub_date_card': {'ru': '12 декабря', 'en': 'December 12', 'zh': '12月12日'}, 'hash': '6b9d7578ed48f6b3', 'authors': ['Dan Jacobellis', 'Neeraja J. Yadwadkar'], 'affiliations': ['University of Texas at Austin'], 'pdf_title_img': 'assets/pdf/title_img/2412.09405.jpg', 'data': {'categories': ['#architecture', '#inference', '#synthetic', '#dataset', '#optimization', '#multimodal'], 'emoji': '🗜️', 'ru': {'title': 'WaLLoC: эффективное сжатие для машинного обучения на данных высокого разрешения', 'desc': 'Статья представляет WaLLoC - новую архитектуру нейронного кодека для сжатия данных с потерями. WaLLoC объединяет линейное преобразование и нелинейное автоэнкодерное сжатие размерности, что позволяет эффективно работать с данными высокого разрешения. Архитектура превосходит автоэнкодеры, используемые в современных моделях латентной диффузии, по ключевым метрикам. WaLLoC особенно эффективен для мобильных вычислений и обучения непосредственно на сжатых данных.'}, 'en': {'title': 'Efficient Compressed-Domain Learning with WaLLoC', 'desc': 'This paper presents WaLLoC, a novel neural codec architecture designed for compressed-domain learning, which efficiently processes high-resolution data from modern sensors. It combines linear transform coding with nonlinear dimensionality-reducing autoencoders to enhance data representation without significant information loss. Unlike traditional methods that either reduce bitrate or dimensionality, WaLLoC maintains high-frequency detail and is compatible with various data modalities. The architecture is highly efficient, making it suitable for applications in mobile computing and remote sensing, and it demonstrates strong performance across multiple tasks such as image classification and music source separation.'}, 'zh': {'title': 'WaLLoC：高效的压缩域学习解决方案', 'desc': '现代传感器生成高分辨率数据流，但由于资源限制，机器学习系统通常会丢弃大部分信息。压缩域学习允许模型在紧凑的潜在表示上操作，从而在相同预算下实现更高的有效分辨率。我们提出的WaLLoC（小波学习有损压缩）结合了线性变换编码和非线性降维自编码器，克服了现有压缩系统的不足。WaLLoC在多个关键指标上超越了当前最先进的潜在扩散模型，适用于移动计算、遥感和直接从压缩数据中学习。'}}}, {'id': 'https://huggingface.co/papers/2412.08972', 'title': 'RuleArena: A Benchmark for Rule-Guided Reasoning with LLMs in Real-World Scenarios', 'url': 'https://huggingface.co/papers/2412.08972', 'abstract': "This paper introduces RuleArena, a novel and challenging benchmark designed to evaluate the ability of large language models (LLMs) to follow complex, real-world rules in reasoning. Covering three practical domains -- airline baggage fees, NBA transactions, and tax regulations -- RuleArena assesses LLMs' proficiency in handling intricate natural language instructions that demand long-context understanding, logical reasoning, and accurate mathematical computation. Two key attributes distinguish RuleArena from traditional rule-based reasoning benchmarks: (1) it extends beyond standard first-order logic representations, and (2) it is grounded in authentic, practical scenarios, providing insights into the suitability and reliability of LLMs for real-world applications. Our findings reveal several notable limitations in LLMs: (1) they struggle to identify and apply the appropriate rules, frequently becoming confused by similar but distinct regulations, (2) they cannot consistently perform accurate mathematical computations, even when they correctly identify the relevant rules, and (3) in general, they perform poorly in the benchmark. These results highlight significant challenges in advancing LLMs' rule-guided reasoning capabilities in real-life applications.", 'score': 8, 'issue_id': 1106, 'pub_date': '2024-12-12', 'pub_date_card': {'ru': '12 декабря', 'en': 'December 12', 'zh': '12月12日'}, 'hash': '354368732f492039', 'authors': ['Ruiwen Zhou', 'Wenyue Hua', 'Liangming Pan', 'Sitao Cheng', 'Xiaobao Wu', 'En Yu', 'William Yang Wang'], 'affiliations': ['Nanyang Technological University', 'University of Arizona', 'University of California, Santa Barbara'], 'pdf_title_img': 'assets/pdf/title_img/2412.08972.jpg', 'data': {'categories': ['#long_context', '#math', '#benchmark', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Большие языковые модели спотыкаются на сложных правилах реального мира', 'desc': 'RuleArena - это новый сложный бенчмарк для оценки способности больших языковых моделей следовать сложным правилам в рассуждениях. Он охватывает три практические области: тарифы на багаж авиакомпаний, сделки в НБА и налоговые правила. Бенчмарк оценивает способность моделей обрабатывать сложные инструкции на естественном языке, требующие понимания длинного контекста, логических рассуждений и точных математических вычислений. Результаты показывают, что большие языковые модели испытывают трудности с идентификацией и применением правил, а также с выполнением точных вычислений.'}, 'en': {'title': 'Testing LLMs: Real-World Rules, Real-World Challenges', 'desc': 'This paper presents RuleArena, a new benchmark for testing large language models (LLMs) on their ability to follow complex rules in real-world situations. It focuses on three areas: airline baggage fees, NBA transactions, and tax regulations, requiring LLMs to understand long-context instructions and perform logical reasoning and math. Unlike traditional benchmarks, RuleArena uses real-life scenarios and goes beyond basic logic, revealing how well LLMs can handle practical tasks. The study finds that LLMs often struggle with rule identification, mathematical accuracy, and overall performance, indicating significant challenges in their reasoning abilities for real-world applications.'}, 'zh': {'title': 'RuleArena：评估语言模型的推理能力新基准', 'desc': '本文介绍了RuleArena，这是一个新颖且具有挑战性的基准，用于评估大型语言模型（LLMs）在推理中遵循复杂现实规则的能力。RuleArena涵盖了三个实际领域——航空行李费用、NBA交易和税收法规，评估LLMs处理复杂自然语言指令的能力，这些指令需要长上下文理解、逻辑推理和准确的数学计算。与传统的基于规则的推理基准不同，RuleArena不仅扩展了标准的一阶逻辑表示，还基于真实的实际场景，提供了对LLMs在现实应用中适用性和可靠性的洞察。我们的研究发现LLMs存在几个显著的局限性，包括难以识别和应用适当的规则、无法始终进行准确的数学计算，以及在基准测试中整体表现不佳，这些结果突显了在现实应用中提升LLMs规则引导推理能力的重大挑战。'}}}, {'id': 'https://huggingface.co/papers/2412.09622', 'title': 'LoRACLR: Contrastive Adaptation for Customization of Diffusion Models', 'url': 'https://huggingface.co/papers/2412.09622', 'abstract': 'Recent advances in text-to-image customization have enabled high-fidelity, context-rich generation of personalized images, allowing specific concepts to appear in a variety of scenarios. However, current methods struggle with combining multiple personalized models, often leading to attribute entanglement or requiring separate training to preserve concept distinctiveness. We present LoRACLR, a novel approach for multi-concept image generation that merges multiple LoRA models, each fine-tuned for a distinct concept, into a single, unified model without additional individual fine-tuning. LoRACLR uses a contrastive objective to align and merge the weight spaces of these models, ensuring compatibility while minimizing interference. By enforcing distinct yet cohesive representations for each concept, LoRACLR enables efficient, scalable model composition for high-quality, multi-concept image synthesis. Our results highlight the effectiveness of LoRACLR in accurately merging multiple concepts, advancing the capabilities of personalized image generation.', 'score': 7, 'issue_id': 1108, 'pub_date': '2024-12-12', 'pub_date_card': {'ru': '12 декабря', 'en': 'December 12', 'zh': '12月12日'}, 'hash': '79f410a319d1e84a', 'authors': ['Enis Simsar', 'Thomas Hofmann', 'Federico Tombari', 'Pinar Yanardag'], 'affiliations': ['ETH Zurich', 'Google', 'TU Munich', 'Virginia Tech'], 'pdf_title_img': 'assets/pdf/title_img/2412.09622.jpg', 'data': {'categories': ['#training', '#multimodal', '#cv'], 'emoji': '🎨', 'ru': {'title': 'LoRACLR: Гармоничное слияние концепций для персонализированной генерации изображений', 'desc': 'LoRACLR - это новый подход к генерации изображений с множественными концепциями, объединяющий несколько моделей LoRA без дополнительной индивидуальной доработки. Метод использует контрастивную цель для выравнивания и слияния весовых пространств моделей, обеспечивая их совместимость при минимизации интерференции. LoRACLR позволяет эффективно объединять несколько персонализированных моделей для высококачественного синтеза изображений с множественными концепциями. Результаты демонстрируют эффективность LoRACLR в точном объединении нескольких концепций, продвигая возможности персонализированной генерации изображений.'}, 'en': {'title': 'Seamless Multi-Concept Image Generation with LoRACLR', 'desc': 'This paper introduces LoRACLR, a new method for generating images that combine multiple personalized concepts without needing separate training for each one. It addresses the problem of attribute entanglement by merging different LoRA models, which are fine-tuned for specific concepts, into a single model. LoRACLR employs a contrastive objective to align the weight spaces of these models, ensuring they work together effectively while maintaining their distinctiveness. The results demonstrate that LoRACLR can produce high-quality images that accurately reflect multiple concepts, enhancing personalized image generation capabilities.'}, 'zh': {'title': 'LoRACLR：高效合并多概念图像生成模型', 'desc': '最近，文本到图像的定制技术取得了显著进展，可以生成高保真、丰富上下文的个性化图像。然而，现有方法在结合多个个性化模型时面临挑战，常常导致属性纠缠或需要单独训练以保持概念的独特性。我们提出了LoRACLR，这是一种新颖的多概念图像生成方法，可以将多个针对不同概念微调的LoRA模型合并为一个统一模型，而无需额外的单独微调。LoRACLR通过对比目标来对齐和合并这些模型的权重空间，确保兼容性并最小化干扰，从而实现高质量的多概念图像合成。'}}}, {'id': 'https://huggingface.co/papers/2412.09573', 'title': 'FreeSplatter: Pose-free Gaussian Splatting for Sparse-view 3D Reconstruction', 'url': 'https://huggingface.co/papers/2412.09573', 'abstract': "Existing sparse-view reconstruction models heavily rely on accurate known camera poses. However, deriving camera extrinsics and intrinsics from sparse-view images presents significant challenges. In this work, we present FreeSplatter, a highly scalable, feed-forward reconstruction framework capable of generating high-quality 3D Gaussians from uncalibrated sparse-view images and recovering their camera parameters in mere seconds. FreeSplatter is built upon a streamlined transformer architecture, comprising sequential self-attention blocks that facilitate information exchange among multi-view image tokens and decode them into pixel-wise 3D Gaussian primitives. The predicted Gaussian primitives are situated in a unified reference frame, allowing for high-fidelity 3D modeling and instant camera parameter estimation using off-the-shelf solvers. To cater to both object-centric and scene-level reconstruction, we train two model variants of FreeSplatter on extensive datasets. In both scenarios, FreeSplatter outperforms state-of-the-art baselines in terms of reconstruction quality and pose estimation accuracy. Furthermore, we showcase FreeSplatter's potential in enhancing the productivity of downstream applications, such as text/image-to-3D content creation.", 'score': 7, 'issue_id': 1103, 'pub_date': '2024-12-12', 'pub_date_card': {'ru': '12 декабря', 'en': 'December 12', 'zh': '12月12日'}, 'hash': '2a5c88d514179442', 'authors': ['Jiale Xu', 'Shenghua Gao', 'Ying Shan'], 'affiliations': ['ARC Lab, Tencent PCG', 'School of Computing and Data Science, The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2412.09573.jpg', 'data': {'categories': ['#architecture', '#3d'], 'emoji': '🔍', 'ru': {'title': 'Революция в 3D-реконструкции: от неоткалиброванных изображений к точным моделям', 'desc': 'FreeSplatter - это инновационная модель реконструкции 3D-объектов, работающая с неоткалиброванными изображениями с разных ракурсов. Она использует архитектуру трансформера для создания высококачественных 3D-гауссианов и оценки параметров камеры за считанные секунды. FreeSplatter превосходит современные методы по качеству реконструкции и точности оценки позы камеры. Модель демонстрирует потенциал для улучшения downstream-задач, таких как создание 3D-контента на основе текста или изображений.'}, 'en': {'title': 'Revolutionizing 3D Reconstruction from Sparse Views with FreeSplatter', 'desc': 'The paper introduces FreeSplatter, a novel framework for reconstructing 3D models from sparse-view images without needing precise camera parameters. It utilizes a transformer architecture with self-attention blocks to efficiently process and convert multi-view image data into 3D Gaussian representations. FreeSplatter not only generates high-quality 3D models but also estimates camera parameters quickly, making it suitable for various applications. The framework demonstrates superior performance compared to existing methods in both reconstruction quality and pose estimation accuracy.'}, 'zh': {'title': 'FreeSplatter：从稀疏视图快速重建高质量3D模型', 'desc': '本论文提出了一种名为FreeSplatter的重建框架，能够从未校准的稀疏视图图像中生成高质量的3D高斯模型，并在几秒钟内恢复相机参数。该框架基于流线型的变换器架构，利用自注意力机制在多视图图像之间进行信息交换，将其解码为像素级的3D高斯原语。FreeSplatter在重建质量和姿态估计精度上超越了现有的最先进基线，适用于物体中心和场景级重建。该方法还展示了在文本/图像到3D内容创建等下游应用中提升生产力的潜力。'}}}, {'id': 'https://huggingface.co/papers/2412.09013', 'title': 'Arbitrary-steps Image Super-resolution via Diffusion Inversion', 'url': 'https://huggingface.co/papers/2412.09013', 'abstract': 'This study presents a new image super-resolution (SR) technique based on diffusion inversion, aiming at harnessing the rich image priors encapsulated in large pre-trained diffusion models to improve SR performance. We design a Partial noise Prediction strategy to construct an intermediate state of the diffusion model, which serves as the starting sampling point. Central to our approach is a deep noise predictor to estimate the optimal noise maps for the forward diffusion process. Once trained, this noise predictor can be used to initialize the sampling process partially along the diffusion trajectory, generating the desirable high-resolution result. Compared to existing approaches, our method offers a flexible and efficient sampling mechanism that supports an arbitrary number of sampling steps, ranging from one to five. Even with a single sampling step, our method demonstrates superior or comparable performance to recent state-of-the-art approaches. The code and model are publicly available at https://github.com/zsyOAOA/InvSR.', 'score': 6, 'issue_id': 1108, 'pub_date': '2024-12-12', 'pub_date_card': {'ru': '12 декабря', 'en': 'December 12', 'zh': '12月12日'}, 'hash': 'a08e0484c6b1e7bd', 'authors': ['Zongsheng Yue', 'Kang Liao', 'Chen Change Loy'], 'affiliations': ['S-Lab, Nanyang Technological University'], 'pdf_title_img': 'assets/pdf/title_img/2412.09013.jpg', 'data': {'categories': ['#cv', '#open_source', '#dataset', '#architecture', '#diffusion'], 'emoji': '🔍', 'ru': {'title': 'Улучшение сверхразрешения изображений с помощью инверсии диффузии', 'desc': 'Это исследование представляет новую технику сверхразрешения изображений, основанную на инверсии диффузии. Метод использует богатые априорные знания об изображениях, заключенные в предобученных диффузионных моделях. Ключевым элементом подхода является глубокий предиктор шума для оценки оптимальных шумовых карт в процессе прямой диффузии. Метод демонстрирует превосходную или сопоставимую производительность по сравнению с современными подходами, даже при использовании всего одного шага сэмплирования.'}, 'en': {'title': 'Enhancing Image Resolution with Diffusion Inversion', 'desc': 'This paper introduces a novel image super-resolution technique that utilizes diffusion inversion to leverage the capabilities of large pre-trained diffusion models. The method employs a Partial noise Prediction strategy to create an intermediate state, which acts as a starting point for the sampling process. A deep noise predictor is central to this approach, as it estimates optimal noise maps for the forward diffusion, enhancing the quality of the generated high-resolution images. The proposed technique is flexible, allowing for varying numbers of sampling steps, and shows competitive performance even with just one sampling step compared to existing state-of-the-art methods.'}, 'zh': {'title': '基于扩散反演的高效图像超分辨率技术', 'desc': '本研究提出了一种基于扩散反演的新图像超分辨率(SR)技术，旨在利用大型预训练扩散模型中丰富的图像先验来提高SR性能。我们设计了一种部分噪声预测策略，以构建扩散模型的中间状态，作为采样的起始点。我们的方法的核心是一个深度噪声预测器，用于估计前向扩散过程的最佳噪声图。与现有方法相比，我们的方法提供了一种灵活高效的采样机制，支持从一个到五个的任意采样步骤，甚至在单个采样步骤下也能展现出优越或可比的性能。'}}}, {'id': 'https://huggingface.co/papers/2412.09370', 'title': 'Word Sense Linking: Disambiguating Outside the Sandbox', 'url': 'https://huggingface.co/papers/2412.09370', 'abstract': 'Word Sense Disambiguation (WSD) is the task of associating a word in a given context with its most suitable meaning among a set of possible candidates. While the task has recently witnessed renewed interest, with systems achieving performances above the estimated inter-annotator agreement, at the time of writing it still struggles to find downstream applications. We argue that one of the reasons behind this is the difficulty of applying WSD to plain text. Indeed, in the standard formulation, models work under the assumptions that a) all the spans to disambiguate have already been identified, and b) all the possible candidate senses of each span are provided, both of which are requirements that are far from trivial. In this work, we present a new task called Word Sense Linking (WSL) where, given an input text and a reference sense inventory, systems have to both identify which spans to disambiguate and then link them to their most suitable meaning.We put forward a transformer-based architecture for the task and thoroughly evaluate both its performance and those of state-of-the-art WSD systems scaled to WSL, iteratively relaxing the assumptions of WSD. We hope that our work will foster easier integration of lexical semantics into downstream applications.', 'score': 6, 'issue_id': 1106, 'pub_date': '2024-12-12', 'pub_date_card': {'ru': '12 декабря', 'en': 'December 12', 'zh': '12月12日'}, 'hash': '01bab47ebd783b40', 'authors': ['Andrei Stefan Bejgu', 'Edoardo Barba', 'Luigi Procopio', 'Alberte Fernández-Castro', 'Roberto Navigli'], 'affiliations': ['Babelscape, Italy', 'Litus AI', 'Roma Tre University', 'Sapienza NLP Group, Sapienza University of Rome'], 'pdf_title_img': 'assets/pdf/title_img/2412.09370.jpg', 'data': {'categories': ['#multilingual', '#reasoning', '#dataset', '#architecture'], 'emoji': '🔗', 'ru': {'title': 'От WSD к WSL: Новый подход к пониманию смысла слов в тексте', 'desc': 'Статья представляет новую задачу под названием Word Sense Linking (WSL), которая расширяет традиционную задачу Word Sense Disambiguation (WSD). В отличие от WSD, где spans и возможные значения слов заранее определены, WSL требует от систем самостоятельно идентифицировать spans для disambiguate и связывать их с наиболее подходящими значениями. Авторы предлагают архитектуру на основе трансформеров для решения этой задачи. Они проводят тщательную оценку производительности своей модели и сравнивают ее с современными системами WSD, адаптированными для WSL.'}, 'en': {'title': 'Revolutionizing Word Sense Disambiguation with Word Sense Linking', 'desc': 'This paper introduces a new task called Word Sense Linking (WSL), which aims to improve the process of Word Sense Disambiguation (WSD) by allowing models to identify and link words to their meanings in a given context. Unlike traditional WSD, which assumes that all words to disambiguate are pre-identified and that all possible meanings are provided, WSL addresses these challenges directly. The authors propose a transformer-based architecture to tackle this task and evaluate its performance against existing WSD systems. The goal is to make it easier to apply lexical semantics in real-world applications, enhancing the utility of WSD techniques.'}, 'zh': {'title': '词义链接：提升词义消歧的应用潜力', 'desc': '词义消歧（WSD）是将特定上下文中的单词与其最合适的意义进行关联的任务。尽管最近对该任务的关注有所增加，但在实际应用中仍然面临挑战。本文提出了一种新任务，称为词义链接（WSL），要求系统在给定文本和参考意义清单的情况下，识别需要消歧的词组并将其链接到最合适的意义。我们提出了一种基于变换器的架构，并对其性能进行了全面评估，以促进词汇语义在下游应用中的整合。'}}}, {'id': 'https://huggingface.co/papers/2412.06745', 'title': 'ONEBench to Test Them All: Sample-Level Benchmarking Over Open-Ended Capabilities', 'url': 'https://huggingface.co/papers/2412.06745', 'abstract': 'Traditional fixed test sets fall short in evaluating open-ended capabilities of foundation models. To address this, we propose ONEBench(OpeN-Ended Benchmarking), a new testing paradigm that consolidates individual evaluation datasets into a unified, ever-expanding sample pool. ONEBench allows users to generate custom, open-ended evaluation benchmarks from this pool, corresponding to specific capabilities of interest. By aggregating samples across test sets, ONEBench enables the assessment of diverse capabilities beyond those covered by the original test sets, while mitigating overfitting and dataset bias. Most importantly, it frames model evaluation as a collective process of selecting and aggregating sample-level tests.   The shift from task-specific benchmarks to ONEBench introduces two challenges: (1)heterogeneity and (2)incompleteness. Heterogeneity refers to the aggregation over diverse metrics, while incompleteness describes comparing models evaluated on different data subsets. To address these challenges, we explore algorithms to aggregate sparse measurements into reliable model scores. Our aggregation algorithm ensures identifiability(asymptotically recovering ground-truth scores) and rapid convergence, enabling accurate model ranking with less data. On homogenous datasets, we show our aggregation algorithm provides rankings that highly correlate with those produced by average scores. We also demonstrate robustness to ~95% of measurements missing, reducing evaluation cost by up to 20x with little-to-no change in model rankings. We introduce ONEBench-LLM for language models and ONEBench-LMM for vision-language models, unifying evaluations across these domains. Overall, we present a technique for open-ended evaluation, which can aggregate over incomplete, heterogeneous sample-level measurements to continually grow a benchmark alongside the rapidly developing foundation models.', 'score': 5, 'issue_id': 1119, 'pub_date': '2024-12-09', 'pub_date_card': {'ru': '9 декабря', 'en': 'December 9', 'zh': '12月9日'}, 'hash': '794891c842afdb0b', 'authors': ['Adhiraj Ghosh', 'Sebastian Dziadzio', 'Ameya Prabhu', 'Vishaal Udandarao', 'Samuel Albanie', 'Matthias Bethge'], 'affiliations': ['Open-Ψ (Open-Sci) Collective', 'Tubingen AI Center, University of Tubingen', 'University of Cambridge'], 'pdf_title_img': 'assets/pdf/title_img/2412.06745.jpg', 'data': {'categories': ['#survey', '#optimization', '#benchmark', '#dataset'], 'emoji': '🧪', 'ru': {'title': 'Единый открытый бенчмарк для комплексной оценки фундаментальных моделей', 'desc': 'ONEBench - это новая парадигма тестирования, объединяющая отдельные наборы данных для оценки в единый расширяемый пул образцов. Она позволяет генерировать пользовательские тесты для оценки конкретных возможностей моделей, решая проблемы гетерогенности и неполноты данных. Предложенный алгоритм агрегации обеспечивает надежное ранжирование моделей даже при отсутствии до 95% измерений. ONEBench представлен в версиях для языковых и мультимодальных моделей, объединяя оценки в этих областях.'}, 'en': {'title': 'ONEBench: Evolving Evaluation for Foundation Models', 'desc': 'This paper introduces ONEBench, a new approach for evaluating foundation models that overcomes the limitations of traditional fixed test sets. ONEBench creates a flexible and expanding pool of evaluation samples, allowing users to design custom benchmarks that assess various capabilities of models. The method addresses challenges like heterogeneity and incompleteness by using algorithms that aggregate sparse data into reliable scores, ensuring accurate model rankings even with missing measurements. This innovative framework supports ongoing evaluation as models evolve, making it suitable for both language and vision-language models.'}, 'zh': {'title': 'ONEBench：开放式评估的新范式', 'desc': '传统的固定测试集无法有效评估基础模型的开放能力。为了解决这个问题，我们提出了ONEBench（开放式基准测试），这是一种新的测试范式，将各个评估数据集整合成一个统一且不断扩展的样本池。ONEBench允许用户从这个样本池中生成自定义的开放式评估基准，以对应特定的能力需求。通过聚合不同测试集的样本，ONEBench能够评估超出原始测试集覆盖范围的多样化能力，同时减轻过拟合和数据集偏差的问题。'}}}, {'id': 'https://huggingface.co/papers/2412.09349', 'title': 'DisPose: Disentangling Pose Guidance for Controllable Human Image Animation', 'url': 'https://huggingface.co/papers/2412.09349', 'abstract': 'Controllable human image animation aims to generate videos from reference images using driving videos. Due to the limited control signals provided by sparse guidance (e.g., skeleton pose), recent works have attempted to introduce additional dense conditions (e.g., depth map) to ensure motion alignment. However, such strict dense guidance impairs the quality of the generated video when the body shape of the reference character differs significantly from that of the driving video. In this paper, we present DisPose to mine more generalizable and effective control signals without additional dense input, which disentangles the sparse skeleton pose in human image animation into motion field guidance and keypoint correspondence. Specifically, we generate a dense motion field from a sparse motion field and the reference image, which provides region-level dense guidance while maintaining the generalization of the sparse pose control. We also extract diffusion features corresponding to pose keypoints from the reference image, and then these point features are transferred to the target pose to provide distinct identity information. To seamlessly integrate into existing models, we propose a plug-and-play hybrid ControlNet that improves the quality and consistency of generated videos while freezing the existing model parameters. Extensive qualitative and quantitative experiments demonstrate the superiority of DisPose compared to current methods. Code: https://github.com/lihxxx/DisPose{https://github.com/lihxxx/DisPose}.', 'score': 5, 'issue_id': 1110, 'pub_date': '2024-12-12', 'pub_date_card': {'ru': '12 декабря', 'en': 'December 12', 'zh': '12月12日'}, 'hash': '9b776f7ceb75ac14', 'authors': ['Hongxiang Li', 'Yaowei Li', 'Yuhang Yang', 'Junjie Cao', 'Zhihong Zhu', 'Xuxin Cheng', 'Long Chen'], 'affiliations': ['Hong Kong University of Science and Technology', 'Peking University', 'Tsinghua University', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2412.09349.jpg', 'data': {'categories': ['#cv', '#multimodal', '#video'], 'emoji': '🎭', 'ru': {'title': 'DisPose: Улучшенный контроль анимации человека без дополнительных входных данных', 'desc': 'Статья представляет DisPose - новый метод для контролируемой анимации изображений человека. Авторы предлагают разделить скелетную позу на поле движения и соответствие ключевых точек, что позволяет генерировать более качественные видео без дополнительных плотных входных данных. DisPose использует генерацию плотного поля движения и извлечение диффузионных признаков для сохранения идентичности. Метод интегрируется в существующие модели с помощью гибридного ControlNet, улучшая качество и согласованность генерируемых видео.'}, 'en': {'title': 'Enhancing Human Animation with DisPose: More Control, Less Complexity!', 'desc': 'This paper introduces DisPose, a method for controllable human image animation that enhances video generation from reference images using driving videos. It addresses the limitations of sparse guidance by creating a dense motion field from a sparse skeleton pose, allowing for better motion alignment without requiring additional dense inputs. DisPose also extracts and transfers diffusion features from reference images to target poses, ensuring distinct identity representation. The proposed hybrid ControlNet integrates seamlessly with existing models, improving video quality and consistency while keeping model parameters unchanged.'}, 'zh': {'title': '无密集输入的可控人像动画', 'desc': '本论文提出了一种名为DisPose的方法，用于在不需要额外密集输入的情况下，挖掘更具普遍性和有效性的控制信号。我们将稀疏的骨架姿势分解为运动场引导和关键点对应，从而生成密集的运动场，以提供区域级的密集指导，同时保持稀疏姿势控制的泛化能力。通过提取与姿势关键点对应的扩散特征，并将这些特征转移到目标姿势上，我们为生成的视频提供了独特的身份信息。实验结果表明，DisPose在视频生成的质量和一致性方面优于现有方法。'}}}, {'id': 'https://huggingface.co/papers/2412.09586', 'title': 'Gaze-LLE: Gaze Target Estimation via Large-Scale Learned Encoders', 'url': 'https://huggingface.co/papers/2412.09586', 'abstract': "We address the problem of gaze target estimation, which aims to predict where a person is looking in a scene. Predicting a person's gaze target requires reasoning both about the person's appearance and the contents of the scene. Prior works have developed increasingly complex, hand-crafted pipelines for gaze target estimation that carefully fuse features from separate scene encoders, head encoders, and auxiliary models for signals like depth and pose. Motivated by the success of general-purpose feature extractors on a variety of visual tasks, we propose Gaze-LLE, a novel transformer framework that streamlines gaze target estimation by leveraging features from a frozen DINOv2 encoder. We extract a single feature representation for the scene, and apply a person-specific positional prompt to decode gaze with a lightweight module. We demonstrate state-of-the-art performance across several gaze benchmarks and provide extensive analysis to validate our design choices. Our code is available at: http://github.com/fkryan/gazelle .", 'score': 5, 'issue_id': 1103, 'pub_date': '2024-12-12', 'pub_date_card': {'ru': '12 декабря', 'en': 'December 12', 'zh': '12月12日'}, 'hash': 'af3315f58d9a0bd3', 'authors': ['Fiona Ryan', 'Ajay Bati', 'Sangmin Lee', 'Daniel Bolya', 'Judy Hoffman', 'James M. Rehg'], 'affiliations': ['Georgia Institute of Technology', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2412.09586.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#architecture', '#cv', '#reasoning'], 'emoji': '👀', 'ru': {'title': 'Простой и эффективный метод оценки направления взгляда с помощью трансформеров', 'desc': 'Статья представляет Gaze-LLE - новый метод оценки направления взгляда человека на основе трансформеров. В отличие от предыдущих сложных подходов, Gaze-LLE использует единое представление сцены, извлеченное замороженным энкодером DINOv2. Метод применяет позиционный промпт для каждого человека и легковесный декодер для определения направления взгляда. Gaze-LLE демонстрирует лучшие результаты на нескольких эталонных наборах данных по оценке направления взгляда.'}, 'en': {'title': 'Streamlining Gaze Estimation with Gaze-LLE', 'desc': 'This paper presents Gaze-LLE, a new approach for estimating where a person is looking in a scene. It simplifies the gaze target estimation process by using a frozen DINOv2 encoder to extract features from the scene, rather than relying on complex, hand-crafted pipelines. The method incorporates a person-specific positional prompt to accurately decode gaze direction with a lightweight module. The authors demonstrate that Gaze-LLE achieves state-of-the-art performance on various gaze estimation benchmarks, validating their design choices through extensive analysis.'}, 'zh': {'title': '简化注视目标估计的创新框架', 'desc': '本文研究了注视目标估计的问题，旨在预测一个人在场景中注视的方向。为了预测一个人的注视目标，需要同时考虑个人的外观和场景的内容。以往的研究采用了复杂的手工设计流程，结合了来自不同场景编码器和头部编码器的特征。我们提出了Gaze-LLE，一个新的变换器框架，通过利用冻结的DINOv2编码器的特征，简化了注视目标估计的过程，并在多个基准测试中展示了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2412.06329', 'title': 'Normalizing Flows are Capable Generative Models', 'url': 'https://huggingface.co/papers/2412.06329', 'abstract': 'Normalizing Flows (NFs) are likelihood-based models for continuous inputs. They have demonstrated promising results on both density estimation and generative modeling tasks, but have received relatively little attention in recent years. In this work, we demonstrate that NFs are more powerful than previously believed. We present TarFlow: a simple and scalable architecture that enables highly performant NF models. TarFlow can be thought of as a Transformer-based variant of Masked Autoregressive Flows (MAFs): it consists of a stack of autoregressive Transformer blocks on image patches, alternating the autoregression direction between layers. TarFlow is straightforward to train end-to-end, and capable of directly modeling and generating pixels. We also propose three key techniques to improve sample quality: Gaussian noise augmentation during training, a post training denoising procedure, and an effective guidance method for both class-conditional and unconditional settings. Putting these together, TarFlow sets new state-of-the-art results on likelihood estimation for images, beating the previous best methods by a large margin, and generates samples with quality and diversity comparable to diffusion models, for the first time with a stand-alone NF model. We make our code available at https://github.com/apple/ml-tarflow.', 'score': 4, 'issue_id': 1120, 'pub_date': '2024-12-09', 'pub_date_card': {'ru': '9 декабря', 'en': 'December 9', 'zh': '12月9日'}, 'hash': '3354deedd6063181', 'authors': ['Shuangfei Zhai', 'Ruixiang Zhang', 'Preetum Nakkiran', 'David Berthelot', 'Jiatao Gu', 'Huangjie Zheng', 'Tianrong Chen', 'Miguel Angel Bautista', 'Navdeep Jaitly', 'Josh Susskind'], 'affiliations': ['Apple'], 'pdf_title_img': 'assets/pdf/title_img/2412.06329.jpg', 'data': {'categories': ['#architecture', '#diffusion', '#optimization', '#training', '#open_source'], 'emoji': '🌊', 'ru': {'title': 'TarFlow: Возрождение нормализующих потоков для высококачественной генерации изображений', 'desc': 'TarFlow - это новая архитектура нормализующих потоков, основанная на трансформерах. Она демонстрирует высокую производительность в задачах оценки плотности вероятности и генеративного моделирования изображений. Авторы предлагают несколько техник для улучшения качества генерируемых образцов, включая аугментацию гауссовым шумом и процедуру шумоподавления. TarFlow устанавливает новый state-of-the-art в оценке правдоподобия для изображений и генерирует образцы, сравнимые по качеству с диффузионными моделями.'}, 'en': {'title': 'TarFlow: Transforming Normalizing Flows for Superior Image Generation', 'desc': 'This paper introduces TarFlow, a novel architecture for Normalizing Flows (NFs) that enhances their performance in density estimation and generative modeling. TarFlow utilizes a Transformer-based approach, employing autoregressive blocks on image patches to improve the modeling of continuous inputs. The authors implement three innovative techniques, including Gaussian noise augmentation and a post-training denoising process, to significantly enhance sample quality. As a result, TarFlow achieves state-of-the-art performance in likelihood estimation for images and generates high-quality samples that rival those produced by diffusion models.'}, 'zh': {'title': 'TarFlow：归一化流的新突破', 'desc': '归一化流（NFs）是一种基于似然的连续输入模型，近年来受到的关注相对较少。本文提出了一种新的架构TarFlow，它是一种简单且可扩展的归一化流模型，能够实现高性能的生成和密度估计。TarFlow结合了自回归Transformer块，能够直接建模和生成图像像素，并通过三种关键技术提高样本质量。最终，TarFlow在图像的似然估计上设定了新的最先进结果，并且生成的样本质量和多样性与扩散模型相当。'}}}, {'id': 'https://huggingface.co/papers/2412.09460', 'title': 'The Impact of Copyrighted Material on Large Language Models: A Norwegian Perspective', 'url': 'https://huggingface.co/papers/2412.09460', 'abstract': 'The use of copyrighted materials in training generative language models raises critical legal and ethical questions. This paper presents a framework for and the results of empirically assessing the impact of copyrighted materials on the performance of large language models (LLMs) for Norwegian. We found that both books and newspapers contribute positively when the models are evaluated on a diverse set of Norwegian benchmarks, while fiction works possibly lead to decreased performance. Our experiments could inform the creation of a compensation scheme for authors whose works contribute to AI development.', 'score': 4, 'issue_id': 1112, 'pub_date': '2024-12-12', 'pub_date_card': {'ru': '12 декабря', 'en': 'December 12', 'zh': '12月12日'}, 'hash': 'ef4555fb21c8153d', 'authors': ['Javier de la Rosa', 'Vladislav Mikhailov', 'Lemei Zhang', 'Freddy Wetjen', 'David Samuel', 'Peng Liu', 'Rolv-Arild Braaten', 'Petter Mæhlum', 'Magnus Breder Birkenes', 'Andrey Kutuzov', 'Tita Enstad', 'Svein Arne Brygfjeld', 'Jon Atle Gulla', 'Stephan Oepen', 'Erik Velldal', 'Wilfred Østgulen', 'Liljia Øvrelid', 'Aslak Sira Myhre'], 'affiliations': ['National Library of Norway', 'Norwegian University of Science and Technology', 'University of Oslo'], 'pdf_title_img': 'assets/pdf/title_img/2412.09460.jpg', 'data': {'categories': ['#low_resource', '#multilingual', '#dataset', '#benchmark', '#ethics'], 'emoji': '📚', 'ru': {'title': 'Авторское право в эпоху искусственного интеллекта: оценка влияния на языковые модели', 'desc': 'Статья исследует влияние защищенных авторским правом материалов на эффективность больших языковых моделей (LLM) для норвежского языка. Авторы разработали методологию для эмпирической оценки этого влияния. Результаты показывают, что книги и газеты положительно влияют на производительность моделей при оценке на различных норвежских бенчмарках. Однако художественная литература, возможно, приводит к снижению эффективности LLM.'}, 'en': {'title': 'Balancing AI Training and Copyright: A Path Forward', 'desc': 'This paper investigates how copyrighted materials affect the performance of large language models (LLMs) specifically for the Norwegian language. It introduces a framework to empirically assess the impact of different types of texts, such as books and newspapers, on model performance. The findings indicate that while books and newspapers enhance model capabilities on various benchmarks, fiction may hinder performance. The results aim to guide the development of a compensation system for authors whose works are utilized in training AI models.'}, 'zh': {'title': '评估版权材料对语言模型性能的影响', 'desc': '本论文探讨了在训练生成语言模型时使用受版权保护材料所带来的法律和伦理问题。我们提出了一个框架，并通过实证研究评估这些材料对挪威大型语言模型（LLMs）性能的影响。研究发现，书籍和报纸在多样化的挪威基准测试中对模型性能有积极贡献，而小说作品可能导致性能下降。我们的实验结果可以为制定补偿方案提供参考，以支持那些对人工智能发展有贡献的作者。'}}}, {'id': 'https://huggingface.co/papers/2412.09025', 'title': 'Shiksha: A Technical Domain focused Translation Dataset and Model for Indian Languages', 'url': 'https://huggingface.co/papers/2412.09025', 'abstract': 'Neural Machine Translation (NMT) models are typically trained on datasets with limited exposure to Scientific, Technical and Educational domains. Translation models thus, in general, struggle with tasks that involve scientific understanding or technical jargon. Their performance is found to be even worse for low-resource Indian languages. Finding a translation dataset that tends to these domains in particular, poses a difficult challenge. In this paper, we address this by creating a multilingual parallel corpus containing more than 2.8 million rows of English-to-Indic and Indic-to-Indic high-quality translation pairs across 8 Indian languages. We achieve this by bitext mining human-translated transcriptions of NPTEL video lectures. We also finetune and evaluate NMT models using this corpus and surpass all other publicly available models at in-domain tasks. We also demonstrate the potential for generalizing to out-of-domain translation tasks by improving the baseline by over 2 BLEU on average for these Indian languages on the Flores+ benchmark. We are pleased to release our model and dataset via this link: https://huggingface.co/SPRINGLab.', 'score': 4, 'issue_id': 1104, 'pub_date': '2024-12-12', 'pub_date_card': {'ru': '12 декабря', 'en': 'December 12', 'zh': '12月12日'}, 'hash': '757a034f902441cd', 'authors': ['Advait Joglekar', 'Srinivasan Umesh'], 'affiliations': ['SPRING Lab, Indian Institute of Technology Madras, India'], 'pdf_title_img': 'assets/pdf/title_img/2412.09025.jpg', 'data': {'categories': ['#benchmark', '#multilingual', '#low_resource', '#machine_translation', '#dataset', '#open_source', '#training'], 'emoji': '🌏', 'ru': {'title': 'Новый корпус для улучшения машинного перевода индийских языков', 'desc': 'Эта статья представляет новый многоязычный параллельный корпус для машинного перевода, содержащий более 2,8 миллиона высококачественных пар переводов между английским и 8 индийскими языками. Корпус создан путем извлечения двуязычных текстов из переведенных человеком транскрипций видеолекций NPTEL. Авторы дообучили и оценили модели нейронного машинного перевода на этом корпусе, превзойдя все другие публично доступные модели на задачах в предметной области. Результаты также показали улучшение качества перевода на 2 BLEU в среднем на тестовом наборе Flores+ для индийских языков.'}, 'en': {'title': 'Empowering Indian Languages with High-Quality Scientific Translation', 'desc': 'This paper addresses the limitations of Neural Machine Translation (NMT) models in translating scientific and technical content, especially for low-resource Indian languages. The authors create a large multilingual parallel corpus with over 2.8 million high-quality translation pairs from English to eight Indic languages, sourced from NPTEL video lectures. They fine-tune NMT models on this dataset and achieve superior performance compared to existing models on in-domain translation tasks. Additionally, their approach shows promise for improving out-of-domain translation tasks, as evidenced by significant BLEU score improvements on the Flores+ benchmark.'}, 'zh': {'title': '提升印度语言翻译的科学技术能力', 'desc': '本论文针对神经机器翻译（NMT）模型在科学、技术和教育领域的翻译困难进行了研究。我们创建了一个多语言平行语料库，包含超过280万条高质量的英印和印印翻译对，特别关注低资源的印度语言。通过挖掘NPTEL视频讲座的人类翻译文本，我们成功地训练和评估了NMT模型，并在领域内任务中超越了所有其他公开可用的模型。我们的研究表明，该模型在跨领域翻译任务中也具有良好的泛化能力，平均提高了2 BLEU分数。'}}}, {'id': 'https://huggingface.co/papers/2412.05552', 'title': 'SAME: Learning Generic Language-Guided Visual Navigation with State-Adaptive Mixture of Experts', 'url': 'https://huggingface.co/papers/2412.05552', 'abstract': 'The academic field of learning instruction-guided visual navigation can be generally categorized into high-level category-specific search and low-level language-guided navigation, depending on the granularity of language instruction, in which the former emphasizes the exploration process, while the latter concentrates on following detailed textual commands. Despite the differing focuses of these tasks, the underlying requirements of interpreting instructions, comprehending the surroundings, and inferring action decisions remain consistent. This paper consolidates diverse navigation tasks into a unified and generic framework -- we investigate the core difficulties of sharing general knowledge and exploiting task-specific capabilities in learning navigation and propose a novel State-Adaptive Mixture of Experts (SAME) model that effectively enables an agent to infer decisions based on different-granularity language and dynamic observations. Powered by SAME, we present a versatile agent capable of addressing seven navigation tasks simultaneously that outperforms or achieves highly comparable performance to task-specific agents.', 'score': 3, 'issue_id': 1103, 'pub_date': '2024-12-07', 'pub_date_card': {'ru': '7 декабря', 'en': 'December 7', 'zh': '12月7日'}, 'hash': '5575b6dc7fe05f4c', 'authors': ['Gengze Zhou', 'Yicong Hong', 'Zun Wang', 'Chongyang Zhao', 'Mohit Bansal', 'Qi Wu'], 'affiliations': ['Adobe Research', 'The University of Adelaide', 'UNC, Chapel Hill', 'UNSW Sydney'], 'pdf_title_img': 'assets/pdf/title_img/2412.05552.jpg', 'data': {'categories': ['#reasoning', '#multimodal', '#agents', '#optimization'], 'emoji': '🧭', 'ru': {'title': 'Универсальный агент для визуальной навигации с языковыми инструкциями', 'desc': 'Это исследование предлагает универсальную модель для различных задач визуальной навигации с языковыми инструкциями. Авторы разработали State-Adaptive Mixture of Experts (SAME) - подход, позволяющий агенту принимать решения на основе инструкций разной детализации и динамических наблюдений. Модель успешно справляется с семью различными навигационными задачами одновременно. SAME демонстрирует производительность на уровне или выше специализированных агентов для конкретных задач.'}, 'en': {'title': 'Unified Navigation with Adaptive Language Understanding', 'desc': 'This paper addresses the challenges in visual navigation tasks that rely on language instructions, categorizing them into high-level and low-level navigation. It introduces a unified framework that combines various navigation tasks, focusing on the common needs of understanding instructions and making decisions based on the environment. The authors propose a novel model called State-Adaptive Mixture of Experts (SAME), which allows an agent to adaptively infer actions from different levels of language input and real-time observations. The results show that the SAME-powered agent can effectively handle multiple navigation tasks at once, often outperforming specialized agents designed for individual tasks.'}, 'zh': {'title': '统一导航任务的智能代理模型', 'desc': '本论文探讨了学习指导下的视觉导航领域，主要分为高层次的类别特定搜索和低层次的语言指导导航。尽管这两种任务的重点不同，但它们在理解指令、理解环境和推断行动决策方面的基本要求是一致的。我们提出了一种新的状态自适应专家混合模型（SAME），该模型能够根据不同粒度的语言和动态观察有效推断决策。通过SAME，我们展示了一种多功能代理，能够同时处理七个导航任务，其性能优于或与特定任务代理相当。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (2)', '#agi', '#alignment (1)', '#architecture (11)', '#audio (2)', '#benchmark (13)', '#cv (8)', '#data (3)', '#dataset (11)', '#diffusion (6)', '#ethics (1)', '#games (1)', '#graphs', '#hallucinations', '#healthcare', '#inference (1)', '#interpretability (2)', '#leakage', '#long_context (3)', '#low_resource (3)', '#machine_translation (1)', '#math (2)', '#multilingual (3)', '#multimodal (11)', '#open_source (7)', '#optimization (12)', '#plp', '#rag', '#reasoning (7)', '#rl', '#rlhf (1)', '#robotics', '#science', '#security', '#small_models (1)', '#story_generation', '#survey (1)', '#synthetic (5)', '#training (10)', '#transfer_learning', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2024-12-16 00:53',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-12-16 00:53')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-12-16 00:53')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    