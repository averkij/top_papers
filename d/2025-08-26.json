{
    "date": {
        "ru": "26 августа",
        "en": "August 26",
        "zh": "8月26日"
    },
    "time_utc": "2025-08-26 02:27",
    "weekday": 1,
    "issue_id": 5539,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2508.17580",
            "title": "UQ: Assessing Language Models on Unsolved Questions",
            "url": "https://huggingface.co/papers/2508.17580",
            "abstract": "UQ is a benchmark for evaluating AI models on unsolved questions, combining difficulty and realism to assess capabilities like reasoning, factuality, and browsing.  \t\t\t\t\tAI-generated summary \t\t\t\t Benchmarks shape progress in AI research. A useful benchmark should be both difficult and realistic: questions should challenge frontier models while also reflecting real-world usage. Yet, current paradigms face a difficulty-realism tension: exam-style benchmarks are often made artificially difficult with limited real-world value, while benchmarks based on real user interaction often skew toward easy, high-frequency problems. In this work, we explore a radically different paradigm: assessing models on unsolved questions. Rather than a static benchmark scored once, we curate unsolved questions and evaluate models asynchronously over time with validator-assisted screening and community verification. We introduce UQ, a testbed of 500 challenging, diverse questions sourced from Stack Exchange, spanning topics from CS theory and math to sci-fi and history, probing capabilities including reasoning, factuality, and browsing. UQ is difficult and realistic by construction: unsolved questions are often hard and naturally arise when humans seek answers, thus solving them yields direct real-world value. Our contributions are threefold: (1) UQ-Dataset and its collection pipeline combining rule-based filters, LLM judges, and human review to ensure question quality (e.g., well-defined and difficult); (2) UQ-Validators, compound validation strategies that leverage the generator-validator gap to provide evaluation signals and pre-screen candidate solutions for human review; and (3) UQ-Platform, an open platform where experts collectively verify questions and solutions. The top model passes UQ-validation on only 15% of questions, and preliminary human verification has already identified correct answers among those that passed. UQ charts a path for evaluating frontier models on real-world, open-ended challenges, where success pushes the frontier of human knowledge. We release UQ at https://uq.stanford.edu.",
            "score": 1,
            "issue_id": 5539,
            "pub_date": "2025-08-25",
            "pub_date_card": {
                "ru": "25 августа",
                "en": "August 25",
                "zh": "8月25日"
            },
            "hash": "9f8a8ba45f2a8eca",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#data",
                    "#survey",
                    "#reasoning",
                    "#open_source"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "UQ: Оценка ИИ на грани человеческих знаний",
                    "desc": "UQ - это новый бенчмарк для оценки моделей искусственного интеллекта на нерешенных вопросах, сочетающий сложность и реалистичность. Он включает 500 разнообразных вопросов из Stack Exchange, охватывающих темы от теории информатики до научной фантастики. UQ использует валидаторы для предварительной проверки решений и открытую платформу для экспертной верификации. Этот подход позволяет оценивать передовые возможности моделей ИИ в решении реальных открытых задач."
                },
                "en": {
                    "title": "UQ: Evaluating AI on Real-World Unsolved Questions",
                    "desc": "The paper introduces UQ, a new benchmark for evaluating AI models on unsolved questions, which combines difficulty and realism to better assess capabilities like reasoning and factuality. Unlike traditional benchmarks that often present artificially difficult questions, UQ focuses on real-world challenges that arise from genuine human inquiries. The UQ framework includes a dataset of 500 diverse questions, validation strategies, and a collaborative platform for community verification. This innovative approach aims to push the boundaries of AI performance by addressing open-ended challenges that reflect actual knowledge gaps."
                },
                "zh": {
                    "title": "UQ：评估AI模型的新标准",
                    "desc": "UQ是一个用于评估人工智能模型在未解决问题上的基准，结合了难度和现实性，以评估推理、事实性和浏览等能力。当前的基准往往面临难度与现实性之间的矛盾，而UQ通过评估未解决的问题，提供了一种新的评估范式。我们构建了一个包含500个来自Stack Exchange的挑战性问题的数据集，并引入了验证者辅助筛选和社区验证的方法。UQ为评估前沿模型在真实世界中的表现提供了新的路径，推动人类知识的前沿。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.17188",
            "title": "PosterGen: Aesthetic-Aware Paper-to-Poster Generation via Multi-Agent\n  LLMs",
            "url": "https://huggingface.co/papers/2508.17188",
            "abstract": "PosterGen, a multi-agent framework using large language models, automates paper-to-poster generation with high design quality and minimal manual refinement.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-agent systems built upon large language models (LLMs) have demonstrated remarkable capabilities in tackling complex compositional tasks. In this work, we apply this paradigm to the paper-to-poster generation problem, a practical yet time-consuming process faced by researchers preparing for conferences. While recent approaches have attempted to automate this task, most neglect core design and aesthetic principles, resulting in posters that require substantial manual refinement. To address these design limitations, we propose PosterGen, a multi-agent framework that mirrors the workflow of professional poster designers. It consists of four collaborative specialized agents: (1) Parser and Curator agents extract content from the paper and organize storyboard; (2) Layout agent maps the content into a coherent spatial layout; (3) Stylist agents apply visual design elements such as color and typography; and (4) Renderer composes the final poster. Together, these agents produce posters that are both semantically grounded and visually appealing. To evaluate design quality, we introduce a vision-language model (VLM)-based rubric that measures layout balance, readability, and aesthetic coherence. Experimental results show that PosterGen consistently matches in content fidelity, and significantly outperforms existing methods in visual designs, generating posters that are presentation-ready with minimal human refinements.",
            "score": 0,
            "issue_id": 5539,
            "pub_date": "2025-08-24",
            "pub_date_card": {
                "ru": "24 августа",
                "en": "August 24",
                "zh": "8月24日"
            },
            "hash": "fb29b368892da03a",
            "authors": [
                "Zhilin Zhang",
                "Xiang Zhang",
                "Jiaqi Wei",
                "Yiwei Xu",
                "Chenyu You"
            ],
            "affiliations": [
                "New York University",
                "Stony Brook University",
                "University of British Columbia",
                "University of California, Los Angeles",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.17188.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#agents",
                    "#optimization",
                    "#agi",
                    "#cv"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "PosterGen: ИИ-дизайнер научных постеров",
                    "desc": "PosterGen - это мультиагентная система на основе больших языковых моделей (LLM) для автоматического создания постеров из научных статей. Система состоит из четырех специализированных агентов, которые извлекают содержание, создают макет, применяют визуальный дизайн и формируют итоговый постер. PosterGen превосходит существующие методы по качеству визуального дизайна, создавая постеры, готовые к презентации с минимальными доработками. Для оценки качества дизайна авторы предложили рубрику на основе мультимодальной языковой модели (VLM)."
                },
                "en": {
                    "title": "Automating Academic Posters with PosterGen: Design Meets Efficiency",
                    "desc": "PosterGen is a multi-agent framework that automates the process of creating academic posters from research papers using large language models. It consists of four specialized agents that work together: Parser and Curator agents extract and organize content, the Layout agent arranges this content spatially, the Stylist agents enhance the visual design, and the Renderer composes the final poster. This system not only ensures that the posters are semantically accurate but also visually appealing, addressing the shortcomings of previous automation methods. The framework is evaluated using a vision-language model to ensure high design quality, demonstrating superior performance in generating ready-to-present posters with minimal manual adjustments."
                },
                "zh": {
                    "title": "PosterGen：自动生成高质量学术海报的智能框架",
                    "desc": "PosterGen是一个基于大型语言模型的多智能体框架，旨在自动生成高质量的学术海报。该系统通过四个协作的专门代理，分别负责内容提取、布局设计、视觉风格应用和最终海报合成。与传统方法相比，PosterGen在设计美学和内容准确性上表现更佳，能够生成几乎无需人工修改的海报。通过引入视觉-语言模型评估设计质量，PosterGen确保了海报的可读性和视觉一致性。"
                }
            }
        }
    ],
    "link_prev": "2025-08-25.html",
    "link_next": "2025-08-27.html",
    "link_month": "2025-08.html",
    "short_date_prev": {
        "ru": "25.08",
        "en": "08/25",
        "zh": "8月25日"
    },
    "short_date_next": {
        "ru": "27.08",
        "en": "08/27",
        "zh": "8月27日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 1,
        "#benchmark": 1,
        "#agents": 1,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 0,
        "#robotics": 0,
        "#agi": 1,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 1,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}