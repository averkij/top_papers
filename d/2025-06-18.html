
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 34 papers. June 18.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">18 Ğ¸ÑĞ½Ñ</span> | <span id="title-articles-count">34 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-06-17.html">â¬…ï¸ <span id="prev-date">17.06</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-06-19.html">â¡ï¸ <span id="next-date">19.06</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-06.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '18 Ğ¸ÑĞ½Ñ', 'en': 'June 18', 'zh': '6æœˆ18æ—¥'};
        let feedDateNext = {'ru': '19.06', 'en': '06/19', 'zh': '6æœˆ19æ—¥'};
        let feedDatePrev = {'ru': '17.06', 'en': '06/17', 'zh': '6æœˆ17æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2506.14028', 'title': 'MultiFinBen: A Multilingual, Multimodal, and Difficulty-Aware Benchmark\n  for Financial LLM Evaluation', 'url': 'https://huggingface.co/papers/2506.14028', 'abstract': 'MultiFinBen is a multilingual and multimodal benchmark for financial domain tasks, evaluating LLMs across modalities and linguistic settings, revealing challenges in complex cross-lingual and multimodal financial reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models (LLMs) have accelerated progress in financial NLP and applications, yet existing benchmarks remain limited to monolingual and unimodal settings, often over-relying on simple tasks and failing to reflect the complexity of real-world financial communication. We introduce MultiFinBen, the first multilingual and multimodal benchmark tailored to the global financial domain, evaluating LLMs across modalities (text, vision, audio) and linguistic settings (monolingual, bilingual, multilingual) on domain-specific tasks. We introduce two novel tasks, including PolyFiQA-Easy and PolyFiQA-Expert, the first multilingual financial benchmarks requiring models to perform complex reasoning over mixed-language inputs; and EnglishOCR and SpanishOCR, the first OCR-embedded financial QA tasks challenging models to extract and reason over information from visual-text financial documents. Moreover, we propose a dynamic, difficulty-aware selection mechanism and curate a compact, balanced benchmark rather than simple aggregation existing datasets. Extensive evaluation of 22 state-of-the-art models reveals that even the strongest models, despite their general multimodal and multilingual capabilities, struggle dramatically when faced with complex cross-lingual and multimodal tasks in financial domain. MultiFinBen is publicly released to foster transparent, reproducible, and inclusive progress in financial studies and applications.', 'score': 65, 'issue_id': 4360, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 Ğ¸ÑĞ½Ñ', 'en': 'June 16', 'zh': '6æœˆ16æ—¥'}, 'hash': 'e94d60496c8d96d1', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#multilingual', '#benchmark', '#multimodal', '#reasoning', '#financial', '#dataset'], 'emoji': 'ğŸ’¹', 'ru': {'title': 'MultiFinBen: ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ°Ñ€ÑŒĞµÑ€Ñ‹ Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¼ Ğ˜Ğ˜', 'desc': 'MultiFinBen - ÑÑ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¹ ÑÑ„ĞµÑ€Ğµ, Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ… Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ°Ñ…. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº PolyFiQA Ğ¸ OCR-Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğµ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ°Ğ´ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¹, ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞÑ†ĞµĞ½ĞºĞ° 22 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ ÑĞ°Ğ¼Ñ‹Ğµ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ· Ğ½Ğ¸Ñ… Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼ĞµĞ¶ÑŠÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸.'}, 'en': {'title': 'MultiFinBen: Bridging Multilingual and Multimodal Gaps in Financial AI', 'desc': 'MultiFinBen is a new benchmark designed to test large language models (LLMs) in the financial sector using multiple languages and types of data, such as text, images, and audio. It addresses the limitations of previous benchmarks that only focused on single languages and simple tasks, which do not reflect the complexities of real-world financial communication. The benchmark includes innovative tasks that require models to understand and reason with mixed-language inputs and to extract information from visual financial documents. Evaluation of various advanced models shows that even the best-performing ones struggle with these challenging tasks, highlighting the need for improved capabilities in financial reasoning across different languages and modalities.'}, 'zh': {'title': 'å¤šè¯­è¨€å¤šæ¨¡æ€é‡‘èåŸºå‡†ï¼Œæ¨åŠ¨é‡‘èæ™ºèƒ½è¿›æ­¥', 'desc': 'MultiFinBenæ˜¯ä¸€ä¸ªé’ˆå¯¹é‡‘èé¢†åŸŸä»»åŠ¡çš„å¤šè¯­è¨€å’Œå¤šæ¨¡æ€åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸åŒæ¨¡æ€å’Œè¯­è¨€ç¯å¢ƒä¸‹çš„è¡¨ç°ã€‚è¯¥åŸºå‡†æ­ç¤ºäº†åœ¨å¤æ‚çš„è·¨è¯­è¨€å’Œå¤šæ¨¡æ€é‡‘èæ¨ç†ä¸­å­˜åœ¨çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸¤ä¸ªæ–°ä»»åŠ¡ï¼ŒPolyFiQA-Easyå’ŒPolyFiQA-Expertï¼Œè¦æ±‚æ¨¡å‹åœ¨æ··åˆè¯­è¨€è¾“å…¥ä¸Šè¿›è¡Œå¤æ‚æ¨ç†ã€‚æ­¤å¤–ï¼ŒMultiFinBenè¿˜æä¾›äº†ä¸€ç§åŠ¨æ€çš„ã€å…³æ³¨éš¾åº¦çš„é€‰æ‹©æœºåˆ¶ï¼Œä»¥ç¡®ä¿åŸºå‡†çš„å¹³è¡¡æ€§å’Œæœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.12928', 'title': 'Scaling Test-time Compute for LLM Agents', 'url': 'https://huggingface.co/papers/2506.12928', 'abstract': "Systematic exploration of test-time scaling methods in large language agents reveals that computational scaling improves performance, especially through parallel sampling, sequential revision, effective verification, and increased rollout diversity.  \t\t\t\t\tAI-generated summary \t\t\t\t Scaling test time compute has shown remarkable success in improving the reasoning abilities of large language models (LLMs). In this work, we conduct the first systematic exploration of applying test-time scaling methods to language agents and investigate the extent to which it improves their effectiveness. Specifically, we explore different test-time scaling strategies, including: (1) parallel sampling algorithms; (2) sequential revision strategies; (3) verifiers and merging methods; (4)strategies for diversifying rollouts.We carefully analyze and ablate the impact of different design strategies on applying test-time scaling on language agents, and have follow findings: 1. Scaling test time compute could improve the performance of agents. 2. Knowing when to reflect is important for agents. 3. Among different verification and result merging approaches, the list-wise method performs best. 4. Increasing diversified rollouts exerts a positive effect on the agent's task performance.", 'score': 36, 'issue_id': 4351, 'pub_date': '2025-06-15', 'pub_date_card': {'ru': '15 Ğ¸ÑĞ½Ñ', 'en': 'June 15', 'zh': '6æœˆ15æ—¥'}, 'hash': '39c8f3e831e90d93', 'authors': ['King Zhu', 'Hanhao Li', 'Siwei Wu', 'Tianshun Xing', 'Dehua Ma', 'Xiangru Tang', 'Minghao Liu', 'Jian Yang', 'Jiaheng Liu', 'Yuchen Eleanor Jiang', 'Changwang Zhang', 'Chenghua Lin', 'Jun Wang', 'Ge Zhang', 'Wangchunshu Zhou'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2506.12928.jpg', 'data': {'categories': ['#training', '#reasoning', '#agents', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¸Ñ… ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ‘Ñ‹Ğ»Ğ¸ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ñ‹ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½ÑƒÑ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºÑƒ, Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²ĞºÑƒ, Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¸ Ğ´Ğ¸Ğ²ĞµÑ€ÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². ĞÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ÑÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑĞ¿Ğ¸ÑĞ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğ¹.'}, 'en': {'title': 'Boosting Language Agents with Test-Time Scaling', 'desc': 'This paper investigates how increasing computational resources at test time can enhance the performance of large language models (LLMs). It systematically examines various test-time scaling methods, such as parallel sampling, sequential revisions, and verification techniques. The findings indicate that scaling up computation not only boosts reasoning capabilities but also highlights the importance of strategic reflection and diverse rollouts. Notably, the study reveals that the list-wise verification method yields the best results among different merging approaches.'}, 'zh': {'title': 'æµ‹è¯•æ—¶é—´æ‰©å±•æå‡è¯­è¨€ä»£ç†æ€§èƒ½', 'desc': 'æœ¬æ–‡ç³»ç»Ÿæ¢è®¨äº†åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­åº”ç”¨æµ‹è¯•æ—¶é—´æ‰©å±•æ–¹æ³•çš„æ•ˆæœã€‚ç ”ç©¶è¡¨æ˜ï¼Œè®¡ç®—æ‰©å±•èƒ½å¤Ÿæ˜¾è‘—æå‡è¯­è¨€ä»£ç†çš„æ¨ç†èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯é€šè¿‡å¹¶è¡Œé‡‡æ ·ã€é¡ºåºä¿®è®¢ã€æœ‰æ•ˆéªŒè¯å’Œå¢åŠ å¤šæ ·åŒ–çš„å›æ»šç­–ç•¥ã€‚æˆ‘ä»¬åˆ†æäº†ä¸åŒè®¾è®¡ç­–ç•¥å¯¹è¯­è¨€ä»£ç†æ€§èƒ½çš„å½±å“ï¼Œå¹¶å‘ç°æµ‹è¯•æ—¶é—´è®¡ç®—çš„æ‰©å±•ç¡®å®èƒ½æé«˜ä»£ç†çš„è¡¨ç°ã€‚ç‰¹åˆ«æ˜¯ï¼Œé‡‡ç”¨åˆ—è¡¨å¼éªŒè¯æ–¹æ³•æ•ˆæœæœ€ä½³ï¼Œè€Œå¤šæ ·åŒ–çš„å›æ»šç­–ç•¥ä¹Ÿå¯¹ä»»åŠ¡è¡¨ç°æœ‰ç§¯æå½±å“ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.12285', 'title': 'CMI-Bench: A Comprehensive Benchmark for Evaluating Music Instruction\n  Following', 'url': 'https://huggingface.co/papers/2506.12285', 'abstract': 'CMI-Bench introduces a comprehensive instruction-following benchmark for audio-text LLMs to evaluate them on a diverse range of music information retrieval tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in audio-text large language models (LLMs) have opened new possibilities for music understanding and generation. However, existing benchmarks are limited in scope, often relying on simplified tasks or multi-choice evaluations that fail to reflect the complexity of real-world music analysis. We reinterpret a broad range of traditional MIR annotations as instruction-following formats and introduce CMI-Bench, a comprehensive music instruction following benchmark designed to evaluate audio-text LLMs on a diverse set of music information retrieval (MIR) tasks. These include genre classification, emotion regression, emotion tagging, instrument classification, pitch estimation, key detection, lyrics transcription, melody extraction, vocal technique recognition, instrument performance technique detection, music tagging, music captioning, and (down)beat tracking: reflecting core challenges in MIR research. Unlike previous benchmarks, CMI-Bench adopts standardized evaluation metrics consistent with previous state-of-the-art MIR models, ensuring direct comparability with supervised approaches. We provide an evaluation toolkit supporting all open-source audio-textual LLMs, including LTU, Qwen-audio, SALMONN, MusiLingo, etc. Experiment results reveal significant performance gaps between LLMs and supervised models, along with their culture, chronological and gender bias, highlighting the potential and limitations of current models in addressing MIR tasks. CMI-Bench establishes a unified foundation for evaluating music instruction following, driving progress in music-aware LLMs.', 'score': 34, 'issue_id': 4360, 'pub_date': '2025-06-14', 'pub_date_card': {'ru': '14 Ğ¸ÑĞ½Ñ', 'en': 'June 14', 'zh': '6æœˆ14æ—¥'}, 'hash': 'c0c91a24a40dfd14', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#open_source', '#survey', '#audio', '#benchmark', '#ethics'], 'emoji': 'ğŸµ', 'ru': {'title': 'CMI-Bench: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… LLM', 'desc': 'CMI-Bench Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¹ ÑĞ¿ĞµĞºÑ‚Ñ€ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¶Ğ°Ğ½Ñ€Ğ¾Ğ², Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹, Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ². CMI-Bench Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€ÑĞ¼Ğ¾Ğµ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ LLM Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ, Ñ…Ñ€Ğ¾Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ Ğ³ĞµĞ½Ğ´ĞµÑ€Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ LLM.'}, 'en': {'title': 'CMI-Bench: Advancing Music Understanding with LLMs', 'desc': 'CMI-Bench is a new benchmark designed to evaluate audio-text large language models (LLMs) on various music information retrieval (MIR) tasks. It addresses the limitations of existing benchmarks by providing a comprehensive set of instruction-following tasks that reflect real-world music analysis complexities. The benchmark includes diverse tasks such as genre classification, emotion tagging, and melody extraction, using standardized evaluation metrics for consistency with state-of-the-art models. Results from experiments show performance gaps between LLMs and supervised models, revealing both the potential and limitations of current LLMs in handling MIR challenges.'}, 'zh': {'title': 'CMI-Benchï¼šéŸ³ä¹ä¿¡æ¯æ£€ç´¢çš„æ–°åŸºå‡†', 'desc': 'CMI-Benchæ˜¯ä¸€ä¸ªå…¨é¢çš„éŸ³é¢‘æ–‡æœ¬å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æŒ‡ä»¤è·ŸéšåŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°å®ƒä»¬åœ¨å¤šæ ·åŒ–çš„éŸ³ä¹ä¿¡æ¯æ£€ç´¢ï¼ˆMIRï¼‰ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚è¯¥åŸºå‡†é‡æ–°è§£é‡Šäº†ä¼ ç»ŸMIRæ³¨é‡Šä¸ºæŒ‡ä»¤è·Ÿéšæ ¼å¼ï¼Œæ¶µç›–äº†å¦‚æµæ´¾åˆ†ç±»ã€æƒ…æ„Ÿå›å½’ã€ä¹å™¨åˆ†ç±»ç­‰å¤šé¡¹ä»»åŠ¡ã€‚ä¸ä»¥å¾€çš„åŸºå‡†ä¸åŒï¼ŒCMI-Benché‡‡ç”¨æ ‡å‡†åŒ–è¯„ä¼°æŒ‡æ ‡ï¼Œç¡®ä¿ä¸ç°æœ‰çš„ç›‘ç£å­¦ä¹ æ¨¡å‹ç›´æ¥å¯æ¯”ã€‚å®éªŒç»“æœæ˜¾ç¤ºLLMä¸ç›‘ç£æ¨¡å‹ä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ï¼Œæ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨MIRä»»åŠ¡ä¸­çš„æ½œåŠ›å’Œå±€é™æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.14429', 'title': 'LongLLaDA: Unlocking Long Context Capabilities in Diffusion LLMs', 'url': 'https://huggingface.co/papers/2506.14429', 'abstract': 'This study investigates long-context performance of diffusion LLMs compared to auto-regressive LLMs, identifies their unique characteristics, and proposes LongLLaDA, a training-free method for extending context windows.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Diffusion Models, or diffusion LLMs, have emerged as a significant focus in NLP research, with substantial effort directed toward understanding their scalability and downstream task performance. However, their long-context capabilities remain unexplored, lacking systematic analysis or methods for context extension. In this work, we present the first systematic investigation comparing the long-context performance of diffusion LLMs and traditional auto-regressive LLMs. We first identify a unique characteristic of diffusion LLMs, unlike auto-regressive LLMs, they maintain remarkably \\textit{stable perplexity} during direct context extrapolation. Furthermore, where auto-regressive models fail outright during the Needle-In-A-Haystack task with context exceeding their pretrained length, we discover diffusion LLMs exhibit a distinct \\textit{local perception} phenomenon, enabling successful retrieval from recent context segments. We explain both phenomena through the lens of Rotary Position Embedding (RoPE) scaling theory. Building on these observations, we propose LongLLaDA, a training-free method that integrates LLaDA with the NTK-based RoPE extrapolation. Our results validate that established extrapolation scaling laws remain effective for extending the context windows of diffusion LLMs. Furthermore, we identify long-context tasks where diffusion LLMs outperform auto-regressive LLMs and others where they fall short. Consequently, this study establishes the first context extrapolation method for diffusion LLMs while providing essential theoretical insights and empirical benchmarks critical for advancing future research on long-context diffusion LLMs.', 'score': 33, 'issue_id': 4347, 'pub_date': '2025-06-17', 'pub_date_card': {'ru': '17 Ğ¸ÑĞ½Ñ', 'en': 'June 17', 'zh': '6æœˆ17æ—¥'}, 'hash': 'd0032538675516d6', 'authors': ['Xiaoran Liu', 'Zhigeng Liu', 'Zengfeng Huang', 'Qipeng Guo', 'Ziwei He', 'Xipeng Qiu'], 'affiliations': ['School of Computer Science, Fudan University', 'Shanghai AI Lab', 'Shanghai Innovation Institute'], 'pdf_title_img': 'assets/pdf/title_img/2506.14429.jpg', 'data': {'categories': ['#training', '#long_context', '#architecture', '#benchmark', '#diffusion', '#rl'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ñ‹ Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°', 'desc': "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½ÑƒÑ Ğ¿ĞµÑ€Ğ¿Ğ»ĞµĞºÑĞ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ÑÑ‚ Ñ„ĞµĞ½Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ¼ 'Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ'. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ñ‹Ğ» Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ LongLLaDA Ğ´Ğ»Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾ĞºĞ½Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ² ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼."}, 'en': {'title': 'Unlocking Long Contexts in Diffusion LLMs with LongLLaDA', 'desc': 'This paper explores how diffusion large language models (LLMs) perform with long contexts compared to traditional auto-regressive LLMs. It highlights that diffusion LLMs maintain stable perplexity when extending context, unlike their auto-regressive counterparts, which struggle with longer inputs. The authors introduce LongLLaDA, a method that allows for context window extension without additional training, leveraging insights from Rotary Position Embedding (RoPE) scaling. The findings reveal specific tasks where diffusion LLMs excel and others where they do not, paving the way for future research in long-context applications.'}, 'zh': {'title': 'æ‰©æ•£æ¨¡å‹çš„é•¿ä¸Šä¸‹æ–‡æ–°æ–¹æ³•ï¼šLongLLaDA', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹ï¼ˆdiffusion LLMsï¼‰ä¸è‡ªå›å½’å¤§è¯­è¨€æ¨¡å‹ï¼ˆauto-regressive LLMsï¼‰åœ¨é•¿ä¸Šä¸‹æ–‡æ€§èƒ½æ–¹é¢çš„æ¯”è¾ƒï¼Œè¯†åˆ«äº†å®ƒä»¬çš„ç‹¬ç‰¹ç‰¹æ€§ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ— è®­ç»ƒçš„æ–¹æ³•LongLLaDAæ¥æ‰©å±•ä¸Šä¸‹æ–‡çª—å£ã€‚ç ”ç©¶å‘ç°ï¼Œæ‰©æ•£LLMsåœ¨ç›´æ¥ä¸Šä¸‹æ–‡å¤–æ¨æ—¶ä¿æŒäº†æ˜¾è‘—ç¨³å®šçš„å›°æƒ‘åº¦ï¼Œè€Œè‡ªå›å½’æ¨¡å‹åœ¨ä¸Šä¸‹æ–‡è¶…å‡ºé¢„è®­ç»ƒé•¿åº¦æ—¶åˆ™è¡¨ç°ä¸ä½³ã€‚æ‰©æ•£LLMså±•ç°å‡ºç‹¬ç‰¹çš„å±€éƒ¨æ„ŸçŸ¥ç°è±¡ï¼Œä½¿å…¶èƒ½å¤ŸæˆåŠŸä»æœ€è¿‘çš„ä¸Šä¸‹æ–‡ç‰‡æ®µä¸­æ£€ç´¢ä¿¡æ¯ã€‚é€šè¿‡æ—‹è½¬ä½ç½®åµŒå…¥ï¼ˆRoPEï¼‰ç¼©æ”¾ç†è®ºï¼Œæˆ‘ä»¬è§£é‡Šäº†è¿™äº›ç°è±¡ï¼Œå¹¶éªŒè¯äº†æ‰©æ•£LLMsçš„ä¸Šä¸‹æ–‡å¤–æ¨æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.14245', 'title': 'Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes\n  Correct Reasoning in Base LLMs', 'url': 'https://huggingface.co/papers/2506.14245', 'abstract': 'RLVR advances machine reasoning by incentivizing correct and logical thought chains, addressing limitations identified by a more precise evaluation metric, $CoT$-$Pass@K$.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a promising paradigm for advancing the reasoning capabilities of Large Language Models (LLMs). However, a critical paradox clouds its efficacy: RLVR-tuned models often underperform their base models on the Pass@K metric for solution-finding, leading to the hypothesis that RLVR merely re-weights existing reasoning paths at the cost of reasoning diversity. In this work, we resolve this contradiction by identifying the source of the problem: the Pass@K metric itself is a flawed measure of reasoning, as it credits correct final answers that probably arise from inaccurate or incomplete chains of thought (CoTs). To address this, we introduce a more precise evaluation metric, CoT-Pass@K, which mandates that both the reasoning path and the final answer be correct. We provide a new theoretical foundation that formalizes how RLVR, unlike traditional RL, is uniquely structured to incentivize logical integrity. Our empirical results are supportive: using CoT-Pass@K, we observe that RLVR can incentivize the generalization of correct reasoning for all values of K. Furthermore, by analyzing the training dynamics, we find that this enhanced reasoning capability emerges early in the training process and smoothly generalizes. Our work provides a clear perspective on the role of RLVR, offers a more reliable method for its evaluation, and confirms its potential to genuinely advance machine reasoning.', 'score': 25, 'issue_id': 4348, 'pub_date': '2025-06-17', 'pub_date_card': {'ru': '17 Ğ¸ÑĞ½Ñ', 'en': 'June 17', 'zh': '6æœˆ17æ—¥'}, 'hash': 'c78cc63a970ea4e9', 'authors': ['Xumeng Wen', 'Zihan Liu', 'Shun Zheng', 'Zhijian Xu', 'Shengyu Ye', 'Zhirong Wu', 'Xiao Liang', 'Yang Wang', 'Junjie Li', 'Ziming Miao', 'Jiang Bian', 'Mao Yang'], 'affiliations': ['Microsoft Research Asia', 'Peking University', 'The Chinese University of Hong Kong', 'University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2506.14245.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#training', '#rl'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'RLVR: Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ - Reinforcement Learning with Verifiable Rewards (RLVR). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ CoT-Pass@K, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ°Ğº Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ‚Ğ°Ğº Ğ¸ ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ RLVR Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹ K. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» RLVR Ğ´Ğ»Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing Machine Reasoning with RLVR and CoT-Pass@K', 'desc': 'Reinforcement Learning with Verifiable Rewards (RLVR) enhances the reasoning abilities of Large Language Models (LLMs) by promoting logical thought processes. The study identifies a flaw in the existing evaluation metric, Pass@K, which inaccurately rewards correct answers that may stem from faulty reasoning paths. To improve this, the authors propose a new metric, CoT-Pass@K, that ensures both the reasoning chain and the final answer are accurate. The findings demonstrate that RLVR can effectively encourage correct reasoning from the early stages of training, leading to better generalization across various scenarios.'}, 'zh': {'title': 'RLVRï¼šæ¨åŠ¨æœºå™¨æ¨ç†çš„æ–°æ–¹æ³•', 'desc': 'RLVRï¼ˆå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼‰é€šè¿‡æ¿€åŠ±æ­£ç¡®å’Œé€»è¾‘çš„æ€ç»´é“¾ï¼Œæ¨åŠ¨äº†æœºå™¨æ¨ç†çš„å‘å±•ã€‚ç ”ç©¶å‘ç°ï¼Œä¼ ç»Ÿçš„è¯„ä¼°æŒ‡æ ‡Pass@Kå­˜åœ¨ç¼ºé™·ï¼Œå¯èƒ½ä¼šé”™è¯¯åœ°è®¤å¯ä¸å®Œæ•´çš„æ€ç»´é“¾æ‰€å¾—åˆ°çš„æ­£ç¡®ç­”æ¡ˆã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†æ›´ç²¾ç¡®çš„è¯„ä¼°æŒ‡æ ‡CoT-Pass@Kï¼Œè¦æ±‚æ¨ç†è·¯å¾„å’Œæœ€ç»ˆç­”æ¡ˆéƒ½å¿…é¡»æ­£ç¡®ã€‚æˆ‘ä»¬çš„å®éªŒè¯æ˜ï¼ŒRLVRèƒ½å¤Ÿæœ‰æ•ˆæ¿€åŠ±æ­£ç¡®æ¨ç†çš„æ³›åŒ–ï¼Œå¹¶ä¸”è¿™ç§å¢å¼ºçš„æ¨ç†èƒ½åŠ›åœ¨è®­ç»ƒæ—©æœŸå°±èƒ½æ˜¾ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.14234', 'title': 'Xolver: Multi-Agent Reasoning with Holistic Experience Learning Just\n  Like an Olympiad Team', 'url': 'https://huggingface.co/papers/2506.14234', 'abstract': "Xolver, a multi-agent reasoning framework, enhances large language models with persistent memory and diverse experience modalities, improving performance on complex reasoning tasks by avoiding generating solutions from scratch.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite impressive progress on complex reasoning, current large language models (LLMs) typically operate in isolation - treating each problem as an independent attempt, without accumulating or integrating experiential knowledge. In contrast, expert problem solvers - such as Olympiad or programming contest teams - leverage a rich tapestry of experiences: absorbing mentorship from coaches, developing intuition from past problems, leveraging knowledge of tool usage and library functionality, adapting strategies based on the expertise and experiences of peers, continuously refining their reasoning through trial and error, and learning from other related problems even during competition. We introduce Xolver, a training-free multi-agent reasoning framework that equips a black-box LLM with a persistent, evolving memory of holistic experience. Xolver integrates diverse experience modalities, including external and self-retrieval, tool use, collaborative interactions, agent-driven evaluation, and iterative refinement. By learning from relevant strategies, code fragments, and abstract reasoning patterns at inference time, Xolver avoids generating solutions from scratch - marking a transition from isolated inference toward experience-aware language agents. Built on both open-weight and proprietary models, Xolver consistently outperforms specialized reasoning agents. Even with lightweight backbones (e.g., QWQ-32B), it often surpasses advanced models including Qwen3-235B, Gemini 2.5 Pro, o3, and o4-mini-high. With o3-mini-high, it achieves new best results on GSM8K (98.1%), AIME'24 (94.4%), AIME'25 (93.7%), Math-500 (99.8%), and LiveCodeBench-V5 (91.6%) - highlighting holistic experience learning as a key step toward generalist agents capable of expert-level reasoning. Code and data are available at https://kagnlp.github.io/xolver.github.io/.", 'score': 24, 'issue_id': 4348, 'pub_date': '2025-06-17', 'pub_date_card': {'ru': '17 Ğ¸ÑĞ½Ñ', 'en': 'June 17', 'zh': '6æœˆ17æ—¥'}, 'hash': '70ebdc96484832ea', 'authors': ['Md Tanzib Hosain', 'Salman Rahman', 'Md Kishor Morol', 'Md Rizwan Parvez'], 'affiliations': ['American International University-Bangladesh', 'Cornell University', 'Qatar Computing Research Institute', 'University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2506.14234.jpg', 'data': {'categories': ['#training', '#agents', '#agi', '#open_source', '#reasoning', '#multimodal'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Xolver: ĞĞ¿Ñ‹Ñ‚-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ', 'desc': 'Xolver - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ·Ğ° ÑÑ‡ĞµÑ‚ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¾Ğ¿Ñ‹Ñ‚Ğ°. ĞĞ½ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ½Ğ°ĞºĞ°Ğ¿Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞºÑĞ¿ĞµÑ€Ğ¸ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ, Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼-Ñ€ĞµÑˆĞ°Ñ‚ĞµĞ»ÑĞ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡. Xolver Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ¿Ñ‹Ñ‚Ğ°, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¹ Ğ¸ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ñ‹Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¸ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ. Ğ”Ğ°Ğ¶Ğµ Ñ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Xolver Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ….'}, 'en': {'title': 'Empowering Language Models with Experience-Aware Reasoning', 'desc': 'Xolver is a multi-agent reasoning framework designed to enhance large language models (LLMs) by incorporating persistent memory and diverse experience modalities. Unlike traditional LLMs that treat each problem independently, Xolver allows agents to accumulate knowledge from past experiences, similar to expert problem solvers. This framework integrates various methods such as self-retrieval, tool usage, and collaborative interactions to refine reasoning and improve performance on complex tasks. As a result, Xolver consistently outperforms specialized reasoning agents, achieving state-of-the-art results on several benchmarks, demonstrating the importance of experience-aware learning in AI.'}, 'zh': {'title': 'Xolverï¼šç»éªŒé©±åŠ¨çš„æ¨ç†æ¡†æ¶', 'desc': 'Xolveræ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“æ¨ç†æ¡†æ¶ï¼Œé€šè¿‡æŒä¹…è®°å¿†å’Œå¤šæ ·åŒ–çš„ç»éªŒæ¨¡å¼å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œä»è€Œæé«˜å¤æ‚æ¨ç†ä»»åŠ¡çš„è¡¨ç°ã€‚ä¸ä¼ ç»Ÿçš„LLMå­¤ç«‹å¤„ç†æ¯ä¸ªé—®é¢˜ä¸åŒï¼ŒXolverèƒ½å¤Ÿæ•´åˆå’Œç§¯ç´¯ç»éªŒçŸ¥è¯†ï¼Œæ¨¡æ‹Ÿä¸“å®¶é—®é¢˜è§£å†³è€…çš„æ€ç»´æ–¹å¼ã€‚å®ƒé€šè¿‡å¤–éƒ¨å’Œè‡ªæˆ‘æ£€ç´¢ã€å·¥å…·ä½¿ç”¨ã€åä½œäº’åŠ¨ç­‰å¤šç§ç»éªŒæ¨¡å¼ï¼Œé¿å…ä»å¤´ç”Ÿæˆè§£å†³æ–¹æ¡ˆã€‚Xolveråœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå±•ç¤ºäº†æ•´ä½“ç»éªŒå­¦ä¹ åœ¨å®ç°é€šç”¨æ™ºèƒ½ä½“æ–¹é¢çš„é‡è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.13363', 'title': 'Efficient Medical VIE via Reinforcement Learning', 'url': 'https://huggingface.co/papers/2506.13363', 'abstract': 'An RLVR framework using fine-tuned Qwen2.5-VL-7B achieves state-of-the-art performance in medical VIE with limited annotated samples, enhancing reasoning and balance between precision and recall.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual Information Extraction (VIE) converts unstructured document images into structured formats like JSON, critical for medical applications such as report analysis and online consultations. Traditional methods rely on OCR and language models, while end-to-end multimodal models offer direct JSON generation. However, domain-specific schemas and high annotation costs limit their effectiveness in medical VIE. We base our approach on the Reinforcement Learning with Verifiable Rewards (RLVR) framework to address these challenges using only 100 annotated samples. Our approach ensures dataset diversity, a balanced precision-recall reward mechanism to reduce hallucinations and improve field coverage, and innovative sampling strategies to enhance reasoning capabilities. Fine-tuning Qwen2.5-VL-7B with our RLVR method, we achieve state-of-the-art performance on medical VIE tasks, significantly improving F1, precision, and recall. While our models excel on tasks similar to medical datasets, performance drops on dissimilar tasks, highlighting the need for domain-specific optimization. Case studies further demonstrate the value of reasoning during training and inference for VIE.', 'score': 22, 'issue_id': 4348, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 Ğ¸ÑĞ½Ñ', 'en': 'June 16', 'zh': '6æœˆ16æ—¥'}, 'hash': '29de6bf10e7470ad', 'authors': ['Lijun Liu', 'Ruiyang Li', 'Zhaocheng Liu', 'Chenglin Zhu', 'Chong Li', 'Jiehan Cheng', 'Qiang Ju', 'Jian Xie'], 'affiliations': ['Baichuan Inc.', 'Peking University', 'Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2506.13363.jpg', 'data': {'categories': ['#hallucinations', '#training', '#optimization', '#healthcare', '#reasoning', '#rl', '#multimodal'], 'emoji': 'ğŸ¥', 'ru': {'title': 'RLVR: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° RLVR Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen2.5-VL-7B Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². RLVR ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ñƒ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ½Ğ¾ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ².'}, 'en': {'title': 'Revolutionizing Medical VIE with Limited Data and Enhanced Reasoning', 'desc': "This paper presents a Reinforcement Learning with Verifiable Rewards (RLVR) framework that utilizes a fine-tuned Qwen2.5-VL-7B model to enhance Visual Information Extraction (VIE) in medical contexts. By leveraging only 100 annotated samples, the framework effectively balances precision and recall, addressing the challenges posed by limited annotated data and high annotation costs. The approach incorporates innovative sampling strategies and a balanced reward mechanism to improve reasoning capabilities and reduce hallucinations in the output. The results show significant improvements in F1 score, precision, and recall, although the model's performance varies with the similarity of the tasks to the training data."}, 'zh': {'title': 'åŒ»ç–—è§†è§‰ä¿¡æ¯æå–çš„åˆ›æ–°çªç ´', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æ¡†æ¶çš„æ–¹æ³•ï¼Œåˆ©ç”¨å¾®è°ƒçš„Qwen2.5-VL-7Bæ¨¡å‹ï¼Œåœ¨åŒ»ç–—è§†è§‰ä¿¡æ¯æå–ï¼ˆVIEï¼‰ä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•ä»…ä½¿ç”¨100ä¸ªæ ‡æ³¨æ ·æœ¬ï¼Œè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•åœ¨åŒ»ç–—é¢†åŸŸé¢ä¸´çš„é«˜æ ‡æ³¨æˆæœ¬å’Œé¢†åŸŸç‰¹å®šæ¨¡å¼çš„é—®é¢˜ã€‚é€šè¿‡ç¡®ä¿æ•°æ®é›†çš„å¤šæ ·æ€§å’Œå¹³è¡¡çš„ç²¾ç¡®ç‡-å¬å›ç‡å¥–åŠ±æœºåˆ¶ï¼Œå‡å°‘äº†æ¨¡å‹çš„å¹»è§‰ç°è±¡ï¼Œå¹¶æé«˜äº†é¢†åŸŸè¦†ç›–ç‡ã€‚æ¡ˆä¾‹ç ”ç©¶è¿›ä¸€æ­¥è¯æ˜äº†åœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­æ¨ç†èƒ½åŠ›çš„é‡è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.13642', 'title': 'Stream-Omni: Simultaneous Multimodal Interactions with Large\n  Language-Vision-Speech Model', 'url': 'https://huggingface.co/papers/2506.13642', 'abstract': 'Stream-Omni, a large multimodal model, integrates text, vision, and speech by efficiently aligning modalities using sequence-dimension concatenation for vision and layer-dimension mapping for speech, achieving strong performance with less data.  \t\t\t\t\tAI-generated summary \t\t\t\t The emergence of GPT-4o-like large multimodal models (LMMs) has raised the exploration of integrating text, vision, and speech modalities to support more flexible multimodal interaction. Existing LMMs typically concatenate representation of modalities along the sequence dimension and feed them into a large language model (LLM) backbone. While sequence-dimension concatenation is straightforward for modality integration, it often relies heavily on large-scale data to learn modality alignments. In this paper, we aim to model the relationships between modalities more purposefully, thereby achieving more efficient and flexible modality alignments. To this end, we propose Stream-Omni, a large language-vision-speech model with efficient modality alignments, which can simultaneously support interactions under various modality combinations. Stream-Omni employs LLM as the backbone and aligns the vision and speech to the text based on their relationships. For vision that is semantically complementary to text, Stream-Omni uses sequence-dimension concatenation to achieve vision-text alignment. For speech that is semantically consistent with text, Stream-Omni introduces a CTC-based layer-dimension mapping to achieve speech-text alignment. In this way, Stream-Omni can achieve modality alignments with less data (especially speech), enabling the transfer of text capabilities to other modalities. Experiments on various benchmarks demonstrate that Stream-Omni achieves strong performance on visual understanding, speech interaction, and vision-grounded speech interaction tasks. Owing to the layer-dimensional mapping, Stream-Omni can simultaneously provide intermediate text outputs (such as ASR transcriptions and model responses) during speech interaction, offering users a comprehensive multimodal experience.', 'score': 21, 'issue_id': 4347, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 Ğ¸ÑĞ½Ñ', 'en': 'June 16', 'zh': '6æœˆ16æ—¥'}, 'hash': '0d0624980a111254', 'authors': ['Shaolei Zhang', 'Shoutao Guo', 'Qingkai Fang', 'Yan Zhou', 'Yang Feng'], 'affiliations': ['Key Laboratory of AI Safety, Chinese Academy of Sciences', 'Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences (ICT/CAS)', 'University of Chinese Academy of Sciences, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.13642.jpg', 'data': {'categories': ['#multimodal', '#audio', '#transfer_learning', '#cv', '#benchmark', '#agi'], 'emoji': 'ğŸ”€', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ»Ñ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Stream-Omni - ÑÑ‚Ğ¾ ĞºÑ€ÑƒĞ¿Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ñ‚ĞµĞºÑÑ‚, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€ĞµÑ‡ÑŒ. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ½ĞºĞ°Ñ‚ĞµĞ½Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾ĞµĞ² Ğ´Ğ»Ñ Ñ€ĞµÑ‡Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸. Stream-Omni Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ñ€ĞµÑ‡ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¸ Ñ€ĞµÑ‡ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Stream-Omni: Efficient Multimodal Integration for Enhanced Interaction', 'desc': "Stream-Omni is a large multimodal model that effectively integrates text, vision, and speech by using innovative alignment techniques. It employs sequence-dimension concatenation for aligning vision with text and a layer-dimension mapping for aligning speech with text, which allows for more efficient learning of modality relationships. This approach reduces the reliance on large datasets, particularly for speech, while still achieving strong performance across various multimodal tasks. The model's design enables it to provide intermediate outputs during speech interactions, enhancing the overall user experience in multimodal applications."}, 'zh': {'title': 'Stream-Omniï¼šé«˜æ•ˆçš„å¤šæ¨¡æ€æ•´åˆæ¨¡å‹', 'desc': 'Stream-Omniæ˜¯ä¸€ç§å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ•´åˆæ–‡æœ¬ã€è§†è§‰å’Œè¯­éŸ³ã€‚å®ƒé€šè¿‡åºåˆ—ç»´åº¦è¿æ¥å®ç°è§†è§‰ä¸æ–‡æœ¬çš„å¯¹é½ï¼Œå¹¶é€šè¿‡åŸºäºCTCçš„å±‚ç»´åº¦æ˜ å°„å®ç°è¯­éŸ³ä¸æ–‡æœ¬çš„å¯¹é½ï¼Œä»è€Œåœ¨æ•°æ®è¾ƒå°‘çš„æƒ…å†µä¸‹ä¹Ÿèƒ½è¾¾åˆ°è‰¯å¥½çš„æ€§èƒ½ã€‚è¯¥æ¨¡å‹æ”¯æŒå¤šç§æ¨¡æ€ç»„åˆçš„äº¤äº’ï¼Œèƒ½å¤Ÿåœ¨è§†è§‰ç†è§£ã€è¯­éŸ³äº¤äº’å’Œè§†è§‰å¼•å¯¼çš„è¯­éŸ³äº¤äº’ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚Stream-Omniçš„è®¾è®¡ä½¿å¾—ç”¨æˆ·åœ¨è¯­éŸ³äº¤äº’æ—¶å¯ä»¥åŒæ—¶è·å¾—ä¸­é—´æ–‡æœ¬è¾“å‡ºï¼Œæä¾›äº†å…¨é¢çš„å¤šæ¨¡æ€ä½“éªŒã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.14758', 'title': 'Reasoning with Exploration: An Entropy Perspective', 'url': 'https://huggingface.co/papers/2506.14758', 'abstract': 'Introducing an entropy-based term to the advantage function in reinforcement learning enhances exploratory reasoning in language models, leading to improved performance on complex reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Balancing exploration and exploitation is a central goal in reinforcement learning (RL). Despite recent advances in enhancing language model (LM) reasoning, most methods lean toward exploitation, and increasingly encounter performance plateaus. In this work, we revisit entropy -- a signal of exploration in RL -- and examine its relationship to exploratory reasoning in LMs. Through empirical analysis, we uncover strong positive correlations between high-entropy regions and three types of exploratory reasoning actions: (1) pivotal tokens that determine or connect logical steps, (2) reflective actions such as self-verification and correction, and (3) rare behaviors under-explored by the base LMs. Motivated by this, we introduce a minimal modification to standard RL with only one line of code: augmenting the advantage function with an entropy-based term. Unlike traditional maximum-entropy methods which encourage exploration by promoting uncertainty, we encourage exploration by promoting longer and deeper reasoning chains. Notably, our method achieves significant gains on the Pass@K metric -- an upper-bound estimator of LM reasoning capabilities -- even when evaluated with extremely large K values, pushing the boundaries of LM reasoning.', 'score': 17, 'issue_id': 4349, 'pub_date': '2025-06-17', 'pub_date_card': {'ru': '17 Ğ¸ÑĞ½Ñ', 'en': 'June 17', 'zh': '6æœˆ17æ—¥'}, 'hash': '14595ff25bf8a37c', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#optimization', '#rlhf', '#training', '#rl', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ğ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ ĞºĞ°Ğº ĞºĞ»ÑÑ‡ Ğº Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¹Ğ½Ñ‹Ğ¹ Ñ‡Ğ»ĞµĞ½ Ğ² Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ°, Ñ‡Ñ‚Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼Ñƒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ Pass@K. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ°Ñ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ ĞºĞ¾Ñ€Ñ€ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ñ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸, Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ñ€ĞµĞ´ĞºĞ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Enhancing Language Model Reasoning through Entropy-Driven Exploration', 'desc': 'This paper introduces a new approach to enhance exploratory reasoning in language models (LMs) by modifying the advantage function in reinforcement learning (RL) with an entropy-based term. The authors highlight that traditional methods often focus on exploitation, leading to performance plateaus, and argue that incorporating entropy can promote better exploration. Their empirical analysis shows that high-entropy regions correlate with key reasoning actions, such as pivotal tokens and reflective behaviors. The proposed method not only encourages deeper reasoning chains but also significantly improves performance on the Pass@K metric, demonstrating its effectiveness in advancing LM reasoning capabilities.'}, 'zh': {'title': 'å¢å¼ºè¯­è¨€æ¨¡å‹æ¨ç†çš„æ¢ç´¢æ€§', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç†µçš„æœ¯è¯­ï¼Œåº”ç”¨äºå¼ºåŒ–å­¦ä¹ ä¸­çš„ä¼˜åŠ¿å‡½æ•°ï¼Œä»¥å¢å¼ºè¯­è¨€æ¨¡å‹çš„æ¢ç´¢æ€§æ¨ç†èƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•é€šè¿‡å¼•å…¥ç†µä¿¡å·ï¼Œä¿ƒè¿›äº†æ¢ç´¢ä¸åˆ©ç”¨ä¹‹é—´çš„å¹³è¡¡ï¼Œè§£å†³äº†è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­æ€§èƒ½åœæ»çš„é—®é¢˜ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé«˜ç†µåŒºåŸŸä¸ä¸‰ç§æ¢ç´¢æ€§æ¨ç†è¡Œä¸ºä¹‹é—´å­˜åœ¨å¼ºæ­£ç›¸å…³ï¼ŒåŒ…æ‹¬å…³é”®æ ‡è®°ã€åæ€æ€§è¡Œä¸ºå’Œç¨€æœ‰è¡Œä¸ºã€‚é€šè¿‡ç®€å•çš„ä»£ç ä¿®æ”¹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æé«˜äº†è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå°¤å…¶åœ¨Pass@KæŒ‡æ ‡ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.14603', 'title': 'Align Your Flow: Scaling Continuous-Time Flow Map Distillation', 'url': 'https://huggingface.co/papers/2506.14603', 'abstract': 'Flow maps, introduced with new continuous-time objectives and training techniques, achieve state-of-the-art performance in few-step image and text-to-image generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion- and flow-based models have emerged as state-of-the-art generative modeling approaches, but they require many sampling steps. Consistency models can distill these models into efficient one-step generators; however, unlike flow- and diffusion-based methods, their performance inevitably degrades when increasing the number of steps, which we show both analytically and empirically. Flow maps generalize these approaches by connecting any two noise levels in a single step and remain effective across all step counts. In this paper, we introduce two new continuous-time objectives for training flow maps, along with additional novel training techniques, generalizing existing consistency and flow matching objectives. We further demonstrate that autoguidance can improve performance, using a low-quality model for guidance during distillation, and an additional boost can be achieved by adversarial finetuning, with minimal loss in sample diversity. We extensively validate our flow map models, called Align Your Flow, on challenging image generation benchmarks and achieve state-of-the-art few-step generation performance on both ImageNet 64x64 and 512x512, using small and efficient neural networks. Finally, we show text-to-image flow map models that outperform all existing non-adversarially trained few-step samplers in text-conditioned synthesis.', 'score': 13, 'issue_id': 4347, 'pub_date': '2025-06-17', 'pub_date_card': {'ru': '17 Ğ¸ÑĞ½Ñ', 'en': 'June 17', 'zh': '6æœˆ17æ—¥'}, 'hash': 'c235653dff87ea28', 'authors': ['Amirmojtaba Sabour', 'Sanja Fidler', 'Karsten Kreis'], 'affiliations': ['NVIDIA', 'University of Toronto', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2506.14603.jpg', 'data': {'categories': ['#training', '#dataset', '#cv', '#benchmark', '#optimization', '#diffusion', '#small_models'], 'emoji': 'ğŸŒŠ', 'ru': {'title': 'Flow maps: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 'flow maps'. Ğ­Ñ‚Ğ° Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑĞ¾ĞµĞ´Ğ¸Ğ½ÑÑ‚ÑŒ Ğ»ÑĞ±Ñ‹Ğµ Ğ´Ğ²Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ñ ÑˆÑƒĞ¼Ğ° Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ ÑˆĞ°Ğ³, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑˆĞ°Ğ³Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğµ Ğ¿Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ flow maps. ĞœĞ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ·Ğ° Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑˆĞ°Ğ³Ğ¾Ğ²."}, 'en': {'title': 'Flow Maps: Efficient Few-Step Generative Modeling', 'desc': 'This paper introduces flow maps, a new approach in generative modeling that connects different noise levels in a single step, allowing for efficient image and text-to-image generation. Unlike traditional diffusion and consistency models, which require many sampling steps and degrade in performance with increased steps, flow maps maintain effectiveness across all step counts. The authors propose two continuous-time training objectives and novel techniques that enhance the training of flow maps, including autoguidance and adversarial finetuning. The results demonstrate that their method, called Align Your Flow, achieves state-of-the-art performance in few-step generation tasks on various benchmarks, outperforming existing models in both image and text-conditioned synthesis.'}, 'zh': {'title': 'æµå›¾æ¨¡å‹ï¼šé«˜æ•ˆçš„å°‘æ­¥éª¤ç”Ÿæˆæ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æµå›¾æ¨¡å‹ï¼Œæ—¨åœ¨æé«˜å›¾åƒå’Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„æ•ˆç‡ã€‚æµå›¾é€šè¿‡åœ¨å•ä¸€æ­¥éª¤ä¸­è¿æ¥ä»»æ„ä¸¤ä¸ªå™ªå£°æ°´å¹³ï¼Œå…‹æœäº†ä¼ ç»Ÿæ‰©æ•£å’Œæµæ¨¡å‹åœ¨å¤šæ­¥éª¤é‡‡æ ·ä¸­çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ç§æ–°çš„è¿ç»­æ—¶é—´ç›®æ ‡å’Œè®­ç»ƒæŠ€æœ¯ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–äº†æµå›¾çš„è®­ç»ƒè¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæµå›¾æ¨¡å‹åœ¨å›¾åƒç”ŸæˆåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯åœ¨å°‘æ­¥éª¤ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.12860', 'title': 'QFFT, Question-Free Fine-Tuning for Adaptive Reasoning', 'url': 'https://huggingface.co/papers/2506.12860', 'abstract': 'Question-Free Fine-Tuning (QFFT) improves efficiency and adaptability in cognitive models by leveraging both short and long chain-of-thought patterns, reducing response length while maintaining performance across various scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in Long Chain-of-Thought (CoT) reasoning models have improved performance on complex tasks, but they suffer from overthinking, which generates redundant reasoning steps, especially for simple questions. This paper revisits the reasoning patterns of Long and Short CoT models, observing that the Short CoT patterns offer concise reasoning efficiently, while the Long CoT patterns excel in challenging scenarios where the Short CoT patterns struggle. To enable models to leverage both patterns, we propose Question-Free Fine-Tuning (QFFT), a fine-tuning approach that removes the input question during training and learns exclusively from Long CoT responses. This approach enables the model to adaptively employ both reasoning patterns: it prioritizes the Short CoT patterns and activates the Long CoT patterns only when necessary. Experiments on various mathematical datasets demonstrate that QFFT reduces average response length by more than 50\\%, while achieving performance comparable to Supervised Fine-Tuning (SFT). Additionally, QFFT exhibits superior performance compared to SFT in noisy, out-of-domain, and low-resource scenarios.', 'score': 13, 'issue_id': 4348, 'pub_date': '2025-06-15', 'pub_date_card': {'ru': '15 Ğ¸ÑĞ½Ñ', 'en': 'June 15', 'zh': '6æœˆ15æ—¥'}, 'hash': '4e8d6c1da3d2fdd1', 'authors': ['Wanlong Liu', 'Junxiao Xu', 'Fei Yu', 'Yukang Lin', 'Ke Ji', 'Wenyu Chen', 'Yan Xu', 'Yasheng Wang', 'Lifeng Shang', 'Benyou Wang'], 'affiliations': ['Huawei Noahs Ark Lab', 'The Chinese University of Hong Kong, Shenzhen', 'University of Electronic Science and Technology of China, Chengdu, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.12860.jpg', 'data': {'categories': ['#training', '#math', '#long_context', '#reasoning', '#low_resource'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'QFFT: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜ Ğ³Ğ¸Ğ±ĞºĞ¾Ğ¼Ñƒ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ - Question-Free Fine-Tuning (QFFT). QFFT Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ğº ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ ÑÑ€ĞµĞ´Ğ½ÑÑ Ğ´Ğ»Ğ¸Ğ½Ñƒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ½Ğ° 50%, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼. QFFT Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ñ ÑˆÑƒĞ¼Ğ¾Ğ¼, Ğ²Ğ½Ğµ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ° Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ°Ñ….'}, 'en': {'title': 'Efficient Reasoning with Question-Free Fine-Tuning', 'desc': 'This paper introduces Question-Free Fine-Tuning (QFFT), a method that enhances cognitive models by combining short and long chain-of-thought reasoning patterns. QFFT addresses the issue of overthinking in Long Chain-of-Thought models, which can lead to unnecessary complexity in responses. By training models without input questions, QFFT allows them to learn from Long CoT responses while primarily using Short CoT patterns for efficiency. The results show that QFFT significantly reduces response length and performs well across various challenging scenarios, outperforming traditional Supervised Fine-Tuning methods in specific contexts.'}, 'zh': {'title': 'æ— é—®å¾®è°ƒï¼šé«˜æ•ˆé€‚åº”çš„æ¨ç†æ–°æ–¹æ³•', 'desc': 'è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¾®è°ƒæ–¹æ³•ï¼Œç§°ä¸ºæ— é—®å¾®è°ƒï¼ˆQFFTï¼‰ï¼Œæ—¨åœ¨æé«˜è®¤çŸ¥æ¨¡å‹çš„æ•ˆç‡å’Œé€‚åº”æ€§ã€‚é€šè¿‡ç»“åˆçŸ­é“¾å’Œé•¿é“¾æ¨ç†æ¨¡å¼ï¼ŒQFFTèƒ½å¤Ÿåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶å‡å°‘å“åº”é•¿åº¦ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒçŸ­é“¾æ¨ç†åœ¨ç®€å•é—®é¢˜ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè€Œé•¿é“¾æ¨ç†åœ¨å¤æ‚ä»»åŠ¡ä¸­æ›´å…·ä¼˜åŠ¿ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒQFFTåœ¨å¤šä¸ªæ•°å­¦æ•°æ®é›†ä¸Šå¹³å‡å“åº”é•¿åº¦å‡å°‘è¶…è¿‡50%ï¼Œå¹¶åœ¨å™ªå£°ã€åŸŸå¤–å’Œä½èµ„æºåœºæ™¯ä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.12278', 'title': 'Can LLMs Generate High-Quality Test Cases for Algorithm Problems?\n  TestCase-Eval: A Systematic Evaluation of Fault Coverage and Exposure', 'url': 'https://huggingface.co/papers/2506.12278', 'abstract': 'TestCase-Eval is a benchmark for evaluating LLMs in generating comprehensive and targeted test cases for algorithm problems.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce TestCase-Eval, a new benchmark for systematic evaluation of LLMs in test-case generation. TestCase-Eval includes 500 algorithm problems and 100,000 human-crafted solutions from the Codeforces platform. It focuses on two pivotal tasks: (1) Fault Coverage, which measures how well LLM-generated test sets probe diverse input scenarios and cover a wide range of potential failure modes. (2) Fault Exposure, which evaluates whether LLMs can craft a tailored test input that reveals a specific incorrect code implementation. We provide a comprehensive assessment of 19 state-of-the-art open-source and proprietary LLMs on TestCase-Eval, offering insights into their strengths and limitations in generating effective test cases for algorithm problems.', 'score': 13, 'issue_id': 4348, 'pub_date': '2025-06-13', 'pub_date_card': {'ru': '13 Ğ¸ÑĞ½Ñ', 'en': 'June 13', 'zh': '6æœˆ13æ—¥'}, 'hash': 'c852db550c523453', 'authors': ['Zheyuan Yang', 'Zexi Kuang', 'Xue Xia', 'Yilun Zhao'], 'affiliations': ['HKUST', 'Northeastern University', 'Tongji University', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2506.12278.jpg', 'data': {'categories': ['#open_source', '#optimization', '#benchmark'], 'emoji': 'ğŸ§ª', 'ru': {'title': 'TestCase-Eval: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¯Ğœ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµÑÑ‚Ğ¾Ğ²', 'desc': 'TestCase-Eval - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (Ğ¯Ğœ) Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğµ ÑĞ»ÑƒÑ‡Ğ°Ğ¸ Ğ´Ğ»Ñ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 500 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ 100 000 Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Codeforces. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ´Ğ²ÑƒÑ… ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…: Ğ¿Ğ¾ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¸ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºÑƒ 19 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¯Ğœ Ğ½Ğ° TestCase-Eval, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ² Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¸Ñ… ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ ÑĞ»Ğ°Ğ±Ñ‹Ñ… ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ĞµĞ².'}, 'en': {'title': 'Evaluating LLMs for Effective Test Case Generation', 'desc': 'TestCase-Eval is a benchmark designed to assess the performance of large language models (LLMs) in generating effective test cases for algorithmic problems. It consists of 500 algorithm problems paired with 100,000 human-created solutions sourced from the Codeforces platform. The evaluation focuses on two main aspects: Fault Coverage, which checks how well the generated test cases explore various input scenarios, and Fault Exposure, which determines the ability of LLMs to create specific test inputs that can uncover flaws in code implementations. The study evaluates 19 different LLMs, providing valuable insights into their capabilities and limitations in this area.'}, 'zh': {'title': 'è¯„ä¼°LLMç”Ÿæˆæµ‹è¯•ç”¨ä¾‹çš„æ–°åŸºå‡†', 'desc': 'TestCase-Evalæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆç®—æ³•é—®é¢˜æµ‹è¯•ç”¨ä¾‹çš„æ–°åŸºå‡†ã€‚å®ƒåŒ…å«500ä¸ªç®—æ³•é—®é¢˜å’Œæ¥è‡ªCodeforceså¹³å°çš„100,000ä¸ªäººå·¥è§£å†³æ–¹æ¡ˆã€‚è¯¥åŸºå‡†å…³æ³¨ä¸¤ä¸ªå…³é”®ä»»åŠ¡ï¼šæ•…éšœè¦†ç›–æ€§ï¼Œè¯„ä¼°LLMç”Ÿæˆçš„æµ‹è¯•é›†æ˜¯å¦èƒ½å¤Ÿæ¢æµ‹å¤šæ ·çš„è¾“å…¥åœºæ™¯ï¼›æ•…éšœæš´éœ²æ€§ï¼Œè¯„ä¼°LLMæ˜¯å¦èƒ½å¤Ÿç”Ÿæˆç‰¹å®šçš„æµ‹è¯•è¾“å…¥ä»¥æ­ç¤ºä»£ç å®ç°ä¸­çš„é”™è¯¯ã€‚æˆ‘ä»¬å¯¹19ä¸ªæœ€å…ˆè¿›çš„å¼€æºå’Œä¸“æœ‰LLMåœ¨TestCase-Evalä¸Šçš„è¡¨ç°è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œæä¾›äº†å®ƒä»¬åœ¨ç”Ÿæˆæœ‰æ•ˆæµ‹è¯•ç”¨ä¾‹æ–¹é¢çš„ä¼˜ç¼ºç‚¹çš„è§è§£ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.14606', 'title': 'Guaranteed Guess: A Language Modeling Approach for CISC-to-RISC\n  Transpilation with Testing Guarantees', 'url': 'https://huggingface.co/papers/2506.14606', 'abstract': 'A novel ISA-centric transpilation pipeline using LLMs and software testing achieves high correctness and efficiency in translating between complex and reduced hardware architectures.  \t\t\t\t\tAI-generated summary \t\t\t\t The hardware ecosystem is rapidly evolving, with increasing interest in translating low-level programs across different instruction set architectures (ISAs) in a quick, flexible, and correct way to enhance the portability and longevity of existing code. A particularly challenging class of this transpilation problem is translating between complex- (CISC) and reduced- (RISC) hardware architectures, due to fundamental differences in instruction complexity, memory models, and execution paradigms. In this work, we introduce GG (Guaranteed Guess), an ISA-centric transpilation pipeline that combines the translation power of pre-trained large language models (LLMs) with the rigor of established software testing constructs. Our method generates candidate translations using an LLM from one ISA to another, and embeds such translations within a software-testing framework to build quantifiable confidence in the translation. We evaluate our GG approach over two diverse datasets, enforce high code coverage (>98%) across unit tests, and achieve functional/semantic correctness of 99% on HumanEval programs and 49% on BringupBench programs, respectively. Further, we compare our approach to the state-of-the-art Rosetta 2 framework on Apple Silicon, showcasing 1.73x faster runtime performance, 1.47x better energy efficiency, and 2.41x better memory usage for our transpiled code, demonstrating the effectiveness of GG for real-world CISC-to-RISC translation tasks. We will open-source our codes, data, models, and benchmarks to establish a common foundation for ISA-level code translation research.', 'score': 10, 'issue_id': 4347, 'pub_date': '2025-06-17', 'pub_date_card': {'ru': '17 Ğ¸ÑĞ½Ñ', 'en': 'June 17', 'zh': '6æœˆ17æ—¥'}, 'hash': 'c414a1f31e0417da', 'authors': ['Ahmed Heakl', 'Sarim Hashmi', 'Chaimaa Abi', 'Celine Lee', 'Abdulrahman Mahmoud'], 'affiliations': ['Cornell University', 'MBZUAI'], 'pdf_title_img': 'assets/pdf/title_img/2506.14606.jpg', 'data': {'categories': ['#open_source', '#dataset', '#architecture', '#benchmark', '#data', '#science'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Ğ¯ĞœĞ‘ Ğ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‚ÑÑ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¸Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ñ€Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¸Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ñ€Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ GG (Guaranteed Guess) Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ñ‹ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° ĞºĞ¾Ğ´Ğ° Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ½Ğ° Ğ´Ñ€ÑƒĞ³ÑƒÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¯ĞœĞ‘ Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ Ğ¸Ñ… ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° (99% Ğ´Ğ»Ñ HumanEval Ğ¸ 49% Ğ´Ğ»Ñ BringupBench) Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑĞ½ĞµÑ€Ğ³Ğ¾ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€ÑƒÑÑ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ÑŒ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´, Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸.'}, 'en': {'title': 'Efficient ISA Translation with Guaranteed Guess', 'desc': 'This paper presents a new transpilation pipeline called GG (Guaranteed Guess) that focuses on translating programs between complex (CISC) and reduced (RISC) instruction set architectures (ISAs). By leveraging large language models (LLMs) for generating translation candidates, the pipeline integrates software testing to ensure high correctness and efficiency. The authors demonstrate that their approach achieves over 99% functional correctness on specific benchmarks and outperforms the existing Rosetta 2 framework in terms of runtime speed, energy efficiency, and memory usage. The research aims to enhance the portability of code across different hardware architectures and will provide open-source resources for further exploration in ISA-level code translation.'}, 'zh': {'title': 'é«˜æ•ˆå‡†ç¡®çš„ISAè½¬è¯‘æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ISAä¸­å¿ƒçš„è½¬è¯‘ç®¡é“ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œè½¯ä»¶æµ‹è¯•æŠ€æœ¯ï¼Œå®ç°äº†åœ¨å¤æ‚å’Œç®€åŒ–ç¡¬ä»¶æ¶æ„ä¹‹é—´çš„é«˜æ•ˆä¸”æ­£ç¡®çš„ä»£ç è½¬æ¢ã€‚è¯¥æ–¹æ³•é€šè¿‡LLMç”Ÿæˆå€™é€‰ç¿»è¯‘ï¼Œå¹¶å°†å…¶åµŒå…¥è½¯ä»¶æµ‹è¯•æ¡†æ¶ä¸­ï¼Œä»¥æé«˜ç¿»è¯‘çš„å¯é æ€§ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªä¸åŒçš„æ•°æ®é›†ä¸Šè¯„ä¼°äº†è¯¥æ–¹æ³•ï¼Œè¾¾åˆ°äº†99%çš„åŠŸèƒ½å’Œè¯­ä¹‰æ­£ç¡®ç‡ï¼Œå¹¶ä¸”åœ¨æ€§èƒ½ä¸Šä¼˜äºç°æœ‰çš„Rosetta 2æ¡†æ¶ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬å°†å¼€æºä»£ç ã€æ•°æ®ã€æ¨¡å‹å’ŒåŸºå‡†ï¼Œä»¥æ¨åŠ¨ISAçº§ä»£ç ç¿»è¯‘ç ”ç©¶çš„å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.13977', 'title': 'CRITICTOOL: Evaluating Self-Critique Capabilities of Large Language\n  Models in Tool-Calling Error Scenarios', 'url': 'https://huggingface.co/papers/2506.13977', 'abstract': 'A comprehensive benchmark, CRITICTOOL, evaluates and enhances the robustness of large language models in handling errors during tool usage.  \t\t\t\t\tAI-generated summary \t\t\t\t The ability of large language models (LLMs) to utilize external tools has enabled them to tackle an increasingly diverse range of tasks. However, as the tasks become more complex and long-horizon, the intricate tool utilization process may trigger various unexpected errors. Therefore, how to effectively handle such errors, including identifying, diagnosing, and recovering from them, has emerged as a key research direction for advancing tool learning. In this work, we first extensively analyze the types of errors encountered during the function-calling process on several competitive tool evaluation benchmarks. Based on it, we introduce CRITICTOOL, a comprehensive critique evaluation benchmark specialized for tool learning. Building upon a novel evolutionary strategy for dataset construction, CRITICTOOL holds diverse tool-use errors with varying complexities, which better reflects real-world scenarios. We conduct extensive experiments on CRITICTOOL, and validate the generalization and effectiveness of our constructed benchmark strategy. We also provide an in-depth analysis of the tool reflection ability on various LLMs, offering a new perspective on the field of tool learning in LLMs. The code is available at https://github.com/Shellorley0513/CriticTool{https://github.com/Shellorley0513/CriticTool}.', 'score': 8, 'issue_id': 4351, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 Ğ¸ÑĞ½Ñ', 'en': 'June 11', 'zh': '6æœˆ11æ—¥'}, 'hash': 'd72fbcfec0e28e92', 'authors': ['Shiting Huang', 'Zhen Fang', 'Zehui Chen', 'Siyu Yuan', 'Junjie Ye', 'Yu Zeng', 'Lin Chen', 'Qi Mao', 'Feng Zhao'], 'affiliations': ['Communication University of China', 'Fudan University', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2506.13977.jpg', 'data': {'categories': ['#interpretability', '#benchmark', '#dataset', '#optimization'], 'emoji': 'ğŸ› ï¸', 'ru': {'title': 'ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸', 'desc': 'CRITICTOOL - ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ½ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº, Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‰Ğ¸Ñ… Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ° Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹, Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. CRITICTOOL Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ LLM Ğº Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ­Ñ‚Ğ¾Ñ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Enhancing LLM Robustness with CRITICTOOL', 'desc': 'This paper introduces CRITICTOOL, a benchmark designed to assess and improve the robustness of large language models (LLMs) when using external tools. It identifies and categorizes various errors that can occur during the function-calling process, especially as tasks become more complex. The benchmark employs an innovative evolutionary strategy for dataset construction, ensuring a wide range of tool-use errors that mimic real-world challenges. Through extensive experiments, the authors demonstrate the effectiveness of CRITICTOOL in enhancing the error-handling capabilities of LLMs and provide insights into their tool reflection abilities.'}, 'zh': {'title': 'æå‡å¤§å‹è¯­è¨€æ¨¡å‹å·¥å…·ä½¿ç”¨çš„é²æ£’æ€§', 'desc': 'æœ¬æ–‡ä»‹ç»äº†CRITICTOOLï¼Œä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•å·¥å…·ï¼Œç”¨äºè¯„ä¼°å’Œå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä½¿ç”¨å·¥å…·æ—¶å¤„ç†é”™è¯¯çš„èƒ½åŠ›ã€‚éšç€ä»»åŠ¡çš„å¤æ‚æ€§å¢åŠ ï¼Œå·¥å…·ä½¿ç”¨è¿‡ç¨‹ä¸­å¯èƒ½ä¼šå‡ºç°å„ç§æ„å¤–é”™è¯¯ï¼Œå› æ­¤æœ‰æ•ˆå¤„ç†è¿™äº›é”™è¯¯æˆä¸ºäº†ä¸€ä¸ªé‡è¦çš„ç ”ç©¶æ–¹å‘ã€‚æˆ‘ä»¬åˆ†æäº†åœ¨å¤šä¸ªç«äº‰æ€§å·¥å…·è¯„ä¼°åŸºå‡†ä¸­é‡åˆ°çš„é”™è¯¯ç±»å‹ï¼Œå¹¶åŸºäºæ­¤æ„å»ºäº†CRITICTOOLï¼Œä¸“æ³¨äºå·¥å…·å­¦ä¹ çš„æ‰¹åˆ¤æ€§è¯„ä¼°ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬éªŒè¯äº†è¯¥åŸºå‡†ç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼Œå¹¶æä¾›äº†å¯¹ä¸åŒå¤§å‹è¯­è¨€æ¨¡å‹å·¥å…·ååº”èƒ½åŠ›çš„æ·±å…¥åˆ†æã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.09985', 'title': 'V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction\n  and Planning', 'url': 'https://huggingface.co/papers/2506.09985', 'abstract': 'A self-supervised approach combining internet video data and minimal robot interaction achieves strong performances in motion understanding, action anticipation, video question-answering, and robotic planning without task-specific training or reward.  \t\t\t\t\tAI-generated summary \t\t\t\t A major challenge for modern AI is to learn to understand the world and learn to act largely by observation. This paper explores a self-supervised approach that combines internet-scale video data with a small amount of interaction data (robot trajectories), to develop models capable of understanding, predicting, and planning in the physical world. We first pre-train an action-free joint-embedding-predictive architecture, V-JEPA 2, on a video and image dataset comprising over 1 million hours of internet video. V-JEPA 2 achieves strong performance on motion understanding (77.3 top-1 accuracy on Something-Something v2) and state-of-the-art performance on human action anticipation (39.7 recall-at-5 on Epic-Kitchens-100) surpassing previous task-specific models. Additionally, after aligning V-JEPA 2 with a large language model, we demonstrate state-of-the-art performance on multiple video question-answering tasks at the 8 billion parameter scale (e.g., 84.0 on PerceptionTest, 76.9 on TempCompass). Finally, we show how self-supervised learning can be applied to robotic planning tasks by post-training a latent action-conditioned world model, V-JEPA 2-AC, using less than 62 hours of unlabeled robot videos from the Droid dataset. We deploy V-JEPA 2-AC zero-shot on Franka arms in two different labs and enable picking and placing of objects using planning with image goals. Notably, this is achieved without collecting any data from the robots in these environments, and without any task-specific training or reward. This work demonstrates how self-supervised learning from web-scale data and a small amount of robot interaction data can yield a world model capable of planning in the physical world.', 'score': 7, 'issue_id': 4359, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 Ğ¸ÑĞ½Ñ', 'en': 'June 11', 'zh': '6æœˆ11æ—¥'}, 'hash': '2a9d0d368d8caa0a', 'authors': ['Mido Assran', 'Adrien Bardes', 'David Fan', 'Quentin Garrido', 'Russell Howes', 'Mojtaba', 'Komeili', 'Matthew Muckley', 'Ammar Rizvi', 'Claire Roberts', 'Koustuv Sinha', 'Artem Zholus', 'Sergio Arnaud', 'Abha Gejji', 'Ada Martin', 'Francois Robert Hogan', 'Daniel Dugas', 'Piotr Bojanowski', 'Vasil Khalidov', 'Patrick Labatut', 'Francisco Massa', 'Marc Szafraniec', 'Kapil Krishnakumar', 'Yong Li', 'Xiaodong Ma', 'Sarath Chandar', 'Franziska Meier', 'Yann LeCun', 'Michael Rabbat', 'Nicolas Ballas'], 'affiliations': ['FAIR at Meta', 'Mila Quebec AI Institute and Polytechnique MontrÃ©al'], 'pdf_title_img': 'assets/pdf/title_img/2506.09985.jpg', 'data': {'categories': ['#multimodal', '#games', '#transfer_learning', '#dataset', '#agi', '#cv', '#robotics', '#rl'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ÑÑ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğ° Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ĞµĞ¼ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ V-JEPA 2 Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 1 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğµ Ñ‡Ğ°ÑĞ¾Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞŸĞ¾ÑĞ»Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ, V-JEPA 2 Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° 62 Ñ‡Ğ°ÑĞ°Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ V-JEPA 2-AC Ğ¾ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ»ÑÑ‚ÑŒ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ±ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ.'}, 'en': {'title': 'Learning to Act by Watching: Self-Supervised Motion Understanding and Planning', 'desc': 'This paper presents a self-supervised learning method that leverages vast amounts of internet video data alongside minimal robot interaction to enhance motion understanding, action anticipation, and robotic planning. The authors introduce V-JEPA 2, a joint-embedding-predictive architecture pre-trained on over 1 million hours of video, achieving impressive results in various tasks without the need for specific training or rewards. By aligning V-JEPA 2 with a large language model, they also achieve state-of-the-art performance in video question-answering tasks. Furthermore, they demonstrate the application of this approach in robotic planning, enabling robots to perform tasks like object manipulation using learned models without additional data collection.'}, 'zh': {'title': 'è‡ªç›‘ç£å­¦ä¹ ï¼šä»è§†é¢‘åˆ°æœºå™¨äººè§„åˆ’çš„çªç ´', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œç»“åˆäº†äº’è”ç½‘è§†é¢‘æ•°æ®å’Œå°‘é‡æœºå™¨äººäº¤äº’æ•°æ®ï¼Œä»¥å®ç°è¿åŠ¨ç†è§£ã€åŠ¨ä½œé¢„æµ‹ã€è§†é¢‘é—®ç­”å’Œæœºå™¨äººè§„åˆ’ç­‰ä»»åŠ¡ã€‚æˆ‘ä»¬é¦–å…ˆåœ¨è¶…è¿‡100ä¸‡å°æ—¶çš„è§†é¢‘æ•°æ®é›†ä¸Šé¢„è®­ç»ƒäº†ä¸€ä¸ªæ— åŠ¨ä½œçš„è”åˆåµŒå…¥é¢„æµ‹æ¶æ„V-JEPA 2ï¼Œå–å¾—äº†è¿åŠ¨ç†è§£å’Œäººç±»åŠ¨ä½œé¢„æµ‹çš„ä¼˜å¼‚è¡¨ç°ã€‚é€šè¿‡ä¸å¤§å‹è¯­è¨€æ¨¡å‹å¯¹é½ï¼ŒV-JEPA 2åœ¨å¤šä¸ªè§†é¢‘é—®ç­”ä»»åŠ¡ä¸Šä¹Ÿè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æœ€åï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•å°†è‡ªç›‘ç£å­¦ä¹ åº”ç”¨äºæœºå™¨äººè§„åˆ’ä»»åŠ¡ï¼ŒæˆåŠŸå®ç°äº†åœ¨ä¸åŒå®éªŒå®¤ä¸­ä½¿ç”¨V-JEPA 2-ACè¿›è¡Œç‰©ä½“çš„æŠ“å–å’Œæ”¾ç½®ï¼Œè€Œæ— éœ€ç‰¹å®šä»»åŠ¡çš„è®­ç»ƒæˆ–å¥–åŠ±ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.13651', 'title': 'xbench: Tracking Agents Productivity Scaling with Profession-Aligned\n  Real-World Evaluations', 'url': 'https://huggingface.co/papers/2506.13651', 'abstract': "We introduce xbench, a dynamic, profession-aligned evaluation suite designed to bridge the gap between AI agent capabilities and real-world productivity. While existing benchmarks often focus on isolated technical skills, they may not accurately reflect the economic value agents deliver in professional settings. To address this, xbench targets commercially significant domains with evaluation tasks defined by industry professionals. Our framework creates metrics that strongly correlate with productivity value, enables prediction of Technology-Market Fit (TMF), and facilitates tracking of product capabilities over time. As our initial implementations, we present two benchmarks: Recruitment and Marketing. For Recruitment, we collect 50 tasks from real-world headhunting business scenarios to evaluate agents' abilities in company mapping, information retrieval, and talent sourcing. For Marketing, we assess agents' ability to match influencers with advertiser needs, evaluating their performance across 50 advertiser requirements using a curated pool of 836 candidate influencers. We present initial evaluation results for leading contemporary agents, establishing a baseline for these professional domains. Our continuously updated evalsets and evaluations are available at https://xbench.org.", 'score': 6, 'issue_id': 4351, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 Ğ¸ÑĞ½Ñ', 'en': 'June 16', 'zh': '6æœˆ16æ—¥'}, 'hash': 'ce4f94d367671b84', 'authors': ['Kaiyuan Chen', 'Yixin Ren', 'Yang Liu', 'Xiaobo Hu', 'Haotong Tian', 'Tianbao Xie', 'Fangfu Liu', 'Haoye Zhang', 'Hongzhang Liu', 'Yuan Gong', 'Chen Sun', 'Han Hou', 'Hui Yang', 'James Pan', 'Jianan Lou', 'Jiayi Mao', 'Jizheng Liu', 'Jinpeng Li', 'Kangyi Liu', 'Kenkun Liu', 'Rui Wang', 'Run Li', 'Tong Niu', 'Wenlong Zhang', 'Wenqi Yan', 'Xuanzheng Wang', 'Yuchen Zhang', 'Yi-Hsin Hung', 'Yuan Jiang', 'Zexuan Liu', 'Zihan Yin', 'Zijian Ma', 'Zhiwen Mo'], 'affiliations': ['Carnegie Mellon University', 'Fudan University', 'Imperial College London', 'Massachusetts Institute of Technology', 'National University of Singapore', 'Peking University', 'Shanghai Jiao Tong University', 'Stanford University', 'The Chinese University of Hong Kong (Shenzhen)', 'The Ohio State University', 'Tsinghua University', 'University of Chinese Academy of Sciences', 'University of Oxford', 'University of Pennsylvania', 'University of Science and Technology of China', 'University of Sydney', 'University of Toronto', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2506.13651.jpg', 'data': {'categories': ['#benchmark', '#agents'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'xbench: Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ xbench - Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ´Ğ»Ñ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ², xbench Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸, Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»Ğ°Ğ¼Ğ¸ Ğ¸Ğ½Ğ´ÑƒÑÑ‚Ñ€Ğ¸Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸, ĞºĞ¾Ñ€Ñ€ĞµĞ»Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ñ Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¹ Ñ€Ñ‹Ğ½ĞºÑƒ Ğ¸ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚Ğ¾Ğ². ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ´Ğ²Ğ° Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°: Ğ´Ğ»Ñ Ñ€ĞµĞºÑ€ÑƒÑ‚Ğ¸Ğ½Ğ³Ğ° Ğ¸ Ğ¼Ğ°Ñ€ĞºĞµÑ‚Ğ¸Ğ½Ğ³Ğ°, Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¸Ğ·Ğ½ĞµÑ-ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ….'}, 'en': {'title': 'Bridging AI Performance with Real-World Productivity', 'desc': "The paper introduces xbench, a new evaluation suite aimed at assessing AI agents in real-world professional contexts. Unlike traditional benchmarks that focus on isolated skills, xbench emphasizes the economic impact of AI agents in industries like recruitment and marketing. It includes tasks defined by industry experts to ensure relevance and creates metrics that correlate with productivity value. The initial benchmarks evaluate agents' performance in real-world scenarios, providing a baseline for future assessments and tracking improvements over time."}, 'zh': {'title': 'xbenchï¼šè¿æ¥AIèƒ½åŠ›ä¸çœŸå®ç”Ÿäº§åŠ›çš„æ¡¥æ¢', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†xbenchï¼Œè¿™æ˜¯ä¸€ä¸ªåŠ¨æ€çš„ã€ä¸èŒä¸šç›¸å…³çš„è¯„ä¼°å¥—ä»¶ï¼Œæ—¨åœ¨å¼¥åˆäººå·¥æ™ºèƒ½ä»£ç†èƒ½åŠ›ä¸ç°å®ä¸–ç•Œç”Ÿäº§åŠ›ä¹‹é—´çš„å·®è·ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•é€šå¸¸å…³æ³¨å­¤ç«‹çš„æŠ€æœ¯æŠ€èƒ½ï¼Œä½†å¯èƒ½æ— æ³•å‡†ç¡®åæ˜ ä»£ç†åœ¨ä¸“ä¸šç¯å¢ƒä¸­æ‰€å¸¦æ¥çš„ç»æµä»·å€¼ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œxbenché’ˆå¯¹å•†ä¸šä¸Šé‡è¦çš„é¢†åŸŸï¼Œè¯„ä¼°ä»»åŠ¡ç”±è¡Œä¸šä¸“ä¸šäººå£«å®šä¹‰ã€‚æˆ‘ä»¬çš„æ¡†æ¶åˆ›å»ºäº†ä¸ç”Ÿäº§åŠ›ä»·å€¼é«˜åº¦ç›¸å…³çš„æŒ‡æ ‡ï¼Œèƒ½å¤Ÿé¢„æµ‹æŠ€æœ¯å¸‚åœºå¥‘åˆåº¦ï¼ˆTMFï¼‰ï¼Œå¹¶ä¾¿äºè·Ÿè¸ªäº§å“èƒ½åŠ›çš„å˜åŒ–ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.10100', 'title': 'EfficientVLA: Training-Free Acceleration and Compression for\n  Vision-Language-Action Models', 'url': 'https://huggingface.co/papers/2506.10100', 'abstract': 'EfficientVLA accelerates Vision-Language-Action models by pruning language layers, optimizing visual token selection, and caching intermediate features in the diffusion-based action head.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language-Action (VLA) models, particularly diffusion-based architectures, demonstrate transformative potential for embodied intelligence but are severely hampered by high computational and memory demands stemming from extensive inherent and inference-time redundancies. While existing acceleration efforts often target isolated inefficiencies, such piecemeal solutions typically fail to holistically address the varied computational and memory bottlenecks across the entire VLA pipeline, thereby limiting practical deployability. We introduce EfficientVLA, a structured and training-free inference acceleration framework that systematically eliminates these barriers by cohesively exploiting multifaceted redundancies. EfficientVLA synergistically integrates three targeted strategies: (1) pruning of functionally inconsequential layers from the language module, guided by an analysis of inter-layer redundancies; (2) optimizing the visual processing pathway through a task-aware strategy that selects a compact, diverse set of visual tokens, balancing task-criticality with informational coverage; and (3) alleviating temporal computational redundancy within the iterative diffusion-based action head by strategically caching and reusing key intermediate features. We apply our method to a standard VLA model CogACT, yielding a 1.93X inference speedup and reduces FLOPs to 28.9%, with only a 0.6% success rate drop in the SIMPLER benchmark.', 'score': 6, 'issue_id': 4348, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 Ğ¸ÑĞ½Ñ', 'en': 'June 11', 'zh': '6æœˆ11æ—¥'}, 'hash': '6a877c4c5f1d1f72', 'authors': ['Yantai Yang', 'Yuhao Wang', 'Zichen Wen', 'Luo Zhongwei', 'Chang Zou', 'Zhipeng Zhang', 'Chuan Wen', 'Linfeng Zhang'], 'affiliations': ['Harbin Institute of Technology', 'School of Artificial Intelligence, Shanghai Jiao Tong University', 'University of Electronic Science and Technology of China', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2506.10100.jpg', 'data': {'categories': ['#diffusion', '#optimization', '#architecture', '#inference', '#multimodal'], 'emoji': 'ğŸš€', 'ru': {'title': 'EfficientVLA: ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ VLA Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°', 'desc': 'EfficientVLA - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Vision-Language-Action (VLA). ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸: Ğ¾Ğ±Ñ€ĞµĞ·Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑĞ»Ğ¾ĞµĞ² Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ÑƒĞ»Ğµ, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ EfficientVLA Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ CogACT Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ² 1,93 Ñ€Ğ°Ğ·Ğ° Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ FLOP Ğ½Ğ° 71,1% Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ±Ğ°Ñ€ÑŒĞµÑ€Ñ‹ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… VLA.'}, 'en': {'title': 'Accelerating VLA Models with EfficientVLA', 'desc': 'EfficientVLA is a framework designed to speed up Vision-Language-Action (VLA) models by addressing their high computational and memory requirements. It achieves this by pruning unnecessary language layers, optimizing the selection of visual tokens, and caching important features during the action generation process. This approach not only reduces the overall processing time but also minimizes the number of floating-point operations (FLOPs) needed for inference. As a result, EfficientVLA significantly enhances the efficiency of VLA models while maintaining a high level of performance.'}, 'zh': {'title': 'é«˜æ•ˆåŠ é€Ÿè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„è§£å†³æ–¹æ¡ˆ', 'desc': 'EfficientVLAæ˜¯ä¸€ç§åŠ é€Ÿè§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹çš„æ¡†æ¶ï¼Œé€šè¿‡ä¿®å‰ªè¯­è¨€å±‚ã€ä¼˜åŒ–è§†è§‰æ ‡è®°é€‰æ‹©å’Œç¼“å­˜ä¸­é—´ç‰¹å¾æ¥æé«˜æ•ˆç‡ã€‚è¯¥æ–¹æ³•ç³»ç»Ÿæ€§åœ°æ¶ˆé™¤äº†è®¡ç®—å’Œå†…å­˜ç“¶é¢ˆï¼Œè§£å†³äº†ç°æœ‰åŠ é€Ÿæ–¹æ³•æ— æ³•å…¨é¢åº”å¯¹çš„é—®é¢˜ã€‚é€šè¿‡åˆ†æå±‚é—´å†—ä½™ï¼ŒEfficientVLAå»é™¤äº†åŠŸèƒ½ä¸é‡è¦çš„è¯­è¨€æ¨¡å—å±‚ï¼Œå¹¶é‡‡ç”¨ä»»åŠ¡æ„ŸçŸ¥ç­–ç•¥ä¼˜åŒ–è§†è§‰å¤„ç†è·¯å¾„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåº”ç”¨EfficientVLAåï¼Œæ ‡å‡†VLAæ¨¡å‹CogACTçš„æ¨ç†é€Ÿåº¦æé«˜äº†1.93å€ï¼ŒFLOPså‡å°‘è‡³28.9%ï¼ŒæˆåŠŸç‡ä»…ä¸‹é™0.6%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.14002', 'title': 'Taming Polysemanticity in LLMs: Provable Feature Recovery via Sparse\n  Autoencoders', 'url': 'https://huggingface.co/papers/2506.14002', 'abstract': "A new statistical framework and training algorithm, Group Bias Adaptation, enhance Sparse Autoencoders for recovering monosemantic features in Large Language Models, offering theoretical guarantees and superior performance.  \t\t\t\t\tAI-generated summary \t\t\t\t We study the challenge of achieving theoretically grounded feature recovery using Sparse Autoencoders (SAEs) for the interpretation of Large Language Models. Existing SAE training algorithms often lack rigorous mathematical guarantees and suffer from practical limitations such as hyperparameter sensitivity and instability. To address these issues, we first propose a novel statistical framework for the feature recovery problem, which includes a new notion of feature identifiability by modeling polysemantic features as sparse mixtures of underlying monosemantic concepts. Building on this framework, we introduce a new SAE training algorithm based on ``bias adaptation'', a technique that adaptively adjusts neural network bias parameters to ensure appropriate activation sparsity. We theoretically prove that this algorithm correctly recovers all monosemantic features when input data is sampled from our proposed statistical model. Furthermore, we develop an improved empirical variant, Group Bias Adaptation (GBA), and demonstrate its superior performance against benchmark methods when applied to LLMs with up to 1.5 billion parameters. This work represents a foundational step in demystifying SAE training by providing the first SAE algorithm with theoretical recovery guarantees, thereby advancing the development of more transparent and trustworthy AI systems through enhanced mechanistic interpretability.", 'score': 5, 'issue_id': 4347, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 Ğ¸ÑĞ½Ñ', 'en': 'June 16', 'zh': '6æœˆ16æ—¥'}, 'hash': 'f27daadffcce7200', 'authors': ['Siyu Chen', 'Heejune Sheen', 'Xuyuan Xiong', 'Tianhao Wang', 'Zhuoran Yang'], 'affiliations': ['Antai College of Economics and Management, Shanghai Jiao Tong University', 'Department of Statistics and Data Science, Yale University', 'Toyota Technological Institute at Chicago'], 'pdf_title_img': 'assets/pdf/title_img/2506.14002.jpg', 'data': {'categories': ['#training', '#architecture', '#interpretability', '#math', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Group Bias Adaptation Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² (Sparse Autoencoders). Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ½Ğ¾ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ¸Ğ· Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (Large Language Models) Ñ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ğ¾Ğµ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ¸Ğµ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑ Ğ¿Ğ¾Ğ»Ğ¸ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ ĞºĞ°Ğº Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ¼ĞµÑĞ¸ Ğ¼Ğ¾Ğ½Ğ¾ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ½Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ Ğ´Ğ¾ 1,5 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Enhancing Feature Recovery in Language Models with Group Bias Adaptation', 'desc': 'This paper introduces a new method called Group Bias Adaptation (GBA) to improve Sparse Autoencoders (SAEs) for extracting clear features from Large Language Models (LLMs). The authors address the limitations of existing SAE training methods, which often lack solid mathematical backing and can be unstable. They propose a statistical framework that models complex features as combinations of simpler, clear concepts, ensuring better feature recovery. The new training algorithm not only provides theoretical guarantees for recovering these features but also shows better performance compared to traditional methods when tested on large models.'}, 'zh': {'title': 'ç¾¤ä½“åå·®é€‚åº”ï¼šæå‡ç¨€ç–è‡ªç¼–ç å™¨çš„å•ä¹‰ç‰¹å¾æ¢å¤èƒ½åŠ›', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç»Ÿè®¡æ¡†æ¶å’Œè®­ç»ƒç®—æ³•ï¼Œç§°ä¸ºç¾¤ä½“åå·®é€‚åº”ï¼ˆGroup Bias Adaptationï¼‰ï¼Œæ—¨åœ¨å¢å¼ºç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSparse Autoencodersï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„å•ä¹‰ç‰¹å¾æ¢å¤èƒ½åŠ›ã€‚ç°æœ‰çš„ç¨€ç–è‡ªç¼–ç å™¨è®­ç»ƒç®—æ³•ç¼ºä¹ä¸¥æ ¼çš„æ•°å­¦ä¿è¯ï¼Œå¹¶ä¸”åœ¨è¶…å‚æ•°æ•æ„Ÿæ€§å’Œä¸ç¨³å®šæ€§æ–¹é¢å­˜åœ¨å®é™…é™åˆ¶ã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥ç‰¹å¾å¯è¯†åˆ«æ€§çš„æ–°æ¦‚å¿µï¼Œè§£å†³äº†ç‰¹å¾æ¢å¤é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºåå·®é€‚åº”çš„æ–°è®­ç»ƒç®—æ³•ã€‚ç†è®ºè¯æ˜è¯¥ç®—æ³•èƒ½å¤Ÿåœ¨ç‰¹å®šç»Ÿè®¡æ¨¡å‹ä¸‹æ­£ç¡®æ¢å¤æ‰€æœ‰å•ä¹‰ç‰¹å¾ï¼Œä»è€Œä¸ºç¨€ç–è‡ªç¼–ç å™¨çš„è®­ç»ƒæä¾›äº†ç†è®ºæ”¯æŒã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.10038', 'title': 'Ambient Diffusion Omni: Training Good Models with Bad Data', 'url': 'https://huggingface.co/papers/2506.10038', 'abstract': 'Ambient Diffusion Omni framework leverages low-quality images to enhance diffusion models by utilizing properties of natural images and shows improvements in ImageNet FID and text-to-image quality.  \t\t\t\t\tAI-generated summary \t\t\t\t We show how to use low-quality, synthetic, and out-of-distribution images to improve the quality of a diffusion model. Typically, diffusion models are trained on curated datasets that emerge from highly filtered data pools from the Web and other sources. We show that there is immense value in the lower-quality images that are often discarded. We present Ambient Diffusion Omni, a simple, principled framework to train diffusion models that can extract signal from all available images during training. Our framework exploits two properties of natural images -- spectral power law decay and locality. We first validate our framework by successfully training diffusion models with images synthetically corrupted by Gaussian blur, JPEG compression, and motion blur. We then use our framework to achieve state-of-the-art ImageNet FID, and we show significant improvements in both image quality and diversity for text-to-image generative modeling. The core insight is that noise dampens the initial skew between the desired high-quality distribution and the mixed distribution we actually observe. We provide rigorous theoretical justification for our approach by analyzing the trade-off between learning from biased data versus limited unbiased data across diffusion times.', 'score': 5, 'issue_id': 4349, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 Ğ¸ÑĞ½Ñ', 'en': 'June 10', 'zh': '6æœˆ10æ—¥'}, 'hash': 'f746e9dd50fb7b78', 'authors': ['Giannis Daras', 'Adrian Rodriguez-Munoz', 'Adam Klivans', 'Antonio Torralba', 'Constantinos Daskalakis'], 'affiliations': ['Massachusetts Institute of Technology', 'The University of Texas at Austin'], 'pdf_title_img': 'assets/pdf/title_img/2506.10038.jpg', 'data': {'categories': ['#training', '#cv', '#dataset', '#synthetic', '#diffusion', '#data'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ñ‹ Ğ¸Ğ· ÑˆÑƒĞ¼Ğ°: ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ½Ğ¸Ğ·ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ambient Diffusion Omni, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ¸Ğ·ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ¾Ñ‚Ğ±Ñ€Ğ°ĞºĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ñ‹ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ·Ğ°ĞºĞ¾Ğ½ Ğ·Ğ°Ñ‚ÑƒÑ…Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ FID Ğ½Ğ° ImageNet Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ. Ğ¢ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° ÑĞ¼ĞµÑ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ½ĞµÑĞ¼ĞµÑ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Unlocking Potential: Enhancing Diffusion Models with Low-Quality Images', 'desc': 'The Ambient Diffusion Omni framework enhances diffusion models by effectively utilizing low-quality images, which are often overlooked. It demonstrates that these lower-quality images can significantly improve model performance on tasks like text-to-image generation. The framework leverages natural image properties, such as spectral power law decay and locality, to extract valuable signals during training. By validating its approach with various synthetic corruptions, the framework achieves state-of-the-art results in ImageNet FID, showcasing improved image quality and diversity.'}, 'zh': {'title': 'åˆ©ç”¨ä½è´¨é‡å›¾åƒæå‡æ‰©æ•£æ¨¡å‹çš„è´¨é‡', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºAmbient Diffusion Omniçš„æ¡†æ¶ï¼Œåˆ©ç”¨ä½è´¨é‡å›¾åƒæ¥æå‡æ‰©æ•£æ¨¡å‹çš„æ€§èƒ½ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé€šå¸¸è¢«ä¸¢å¼ƒçš„ä½è´¨é‡å›¾åƒå®é™…ä¸Šå…·æœ‰å¾ˆå¤§çš„ä»·å€¼ï¼Œå¯ä»¥æ”¹å–„æ¨¡å‹çš„è®­ç»ƒæ•ˆæœã€‚è¯¥æ¡†æ¶åˆ©ç”¨è‡ªç„¶å›¾åƒçš„ä¸¤ä¸ªç‰¹æ€§â€”â€”è°±åŠŸç‡æ³•åˆ™è¡°å‡å’Œå±€éƒ¨æ€§ï¼ŒæˆåŠŸåœ°ä»åˆæˆæ¨¡ç³Šã€JPEGå‹ç¼©å’Œè¿åŠ¨æ¨¡ç³Šçš„å›¾åƒä¸­æå–ä¿¡å·ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬åœ¨ImageNet FIDä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œå¹¶æ˜¾è‘—æé«˜äº†æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹çš„å›¾åƒè´¨é‡å’Œå¤šæ ·æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.05336', 'title': 'VideoMolmo: Spatio-Temporal Grounding Meets Pointing', 'url': 'https://huggingface.co/papers/2506.05336', 'abstract': 'VideoMolmo, a multimodal model incorporating a temporal attention mechanism and SAM2 for mask fusion, enhances spatio-temporal pointing accuracy and reasoning capabilities in diverse real-world scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Spatio-temporal localization is vital for precise interactions across diverse domains, from biological research to autonomous navigation and interactive interfaces. Current video-based approaches, while proficient in tracking, lack the sophisticated reasoning capabilities of large language models, limiting their contextual understanding and generalization. We introduce VideoMolmo, a large multimodal model tailored for fine-grained spatio-temporal pointing conditioned on textual descriptions. Building upon the Molmo architecture, VideoMolmo incorporates a temporal module utilizing an attention mechanism to condition each frame on preceding frames, ensuring temporal consistency. Additionally, our novel temporal mask fusion pipeline employs SAM2 for bidirectional point propagation, significantly enhancing coherence across video sequences. This two-step decomposition, i.e., first using the LLM to generate precise pointing coordinates, then relying on a sequential mask-fusion module to produce coherent segmentation, not only simplifies the task for the language model but also enhances interpretability. Due to the lack of suitable datasets, we curate a comprehensive dataset comprising 72k video-caption pairs annotated with 100k object points. To evaluate the generalization of VideoMolmo, we introduce VPoS-Bench, a challenging out-of-distribution benchmark spanning five real-world scenarios: Cell Tracking, Egocentric Vision, Autonomous Driving, Video-GUI Interaction, and Robotics. We also evaluate our model on Referring Video Object Segmentation (Refer-VOS) and Reasoning VOS tasks. In comparison to existing models, VideoMolmo substantially improves spatio-temporal pointing accuracy and reasoning capability. Our code and models are publicly available at https://github.com/mbzuai-oryx/VideoMolmo.', 'score': 5, 'issue_id': 4348, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 Ğ¸ÑĞ½Ñ', 'en': 'June 5', 'zh': '6æœˆ5æ—¥'}, 'hash': 'def5ee56ea3b6157', 'authors': ['Ghazi Shazan Ahmad', 'Ahmed Heakl', 'Hanan Gani', 'Abdelrahman Shaker', 'Zhiqiang Shen', 'Ranjay Krishna', 'Fahad Shahbaz Khan', 'Salman Khan'], 'affiliations': ['Allen Institute for Artificial Intelligence', 'Australian National University', 'LinkÃ¶ping University', 'Mohamed Bin Zayed University of Artificial Intelligence', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2506.05336.jpg', 'data': {'categories': ['#dataset', '#interpretability', '#benchmark', '#open_source', '#reasoning', '#video', '#multimodal'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'VideoMolmo - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ SAM2 Ğ´Ğ»Ñ Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡ĞµĞº, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ°ÑĞ¾Ğº Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ…. VideoMolmo Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°.'}, 'en': {'title': 'Enhancing Spatio-Temporal Reasoning with VideoMolmo', 'desc': 'VideoMolmo is a multimodal model designed to improve spatio-temporal pointing accuracy and reasoning in various real-world applications. It combines a temporal attention mechanism with a novel mask fusion technique called SAM2, which enhances the coherence of video sequences. By generating precise pointing coordinates through a large language model and then refining them with a mask-fusion module, VideoMolmo simplifies the task and improves interpretability. The model is evaluated on a newly curated dataset and a challenging benchmark, demonstrating significant advancements over existing video-based approaches.'}, 'zh': {'title': 'VideoMolmoï¼šæå‡æ—¶ç©ºæŒ‡å‘ä¸æ¨ç†èƒ½åŠ›çš„å¤šæ¨¡æ€æ¨¡å‹', 'desc': 'VideoMolmoæ˜¯ä¸€ç§å¤šæ¨¡æ€æ¨¡å‹ï¼Œç»“åˆäº†æ—¶é—´æ³¨æ„æœºåˆ¶å’ŒSAM2è¿›è¡Œæ©è†œèåˆï¼Œæ˜¾è‘—æé«˜äº†æ—¶ç©ºæŒ‡å‘çš„å‡†ç¡®æ€§å’Œæ¨ç†èƒ½åŠ›ã€‚è¯¥æ¨¡å‹ä¸“ä¸ºåŸºäºæ–‡æœ¬æè¿°çš„ç»†ç²’åº¦æ—¶ç©ºæŒ‡å‘è€Œè®¾è®¡ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒçš„çœŸå®åœºæ™¯ä¸­è¿›è¡Œç²¾ç¡®äº¤äº’ã€‚é€šè¿‡å¼•å…¥æ—¶é—´æ¨¡å—å’ŒåŒå‘ç‚¹ä¼ æ’­çš„æ©è†œèåˆç®¡é“ï¼ŒVideoMolmoç¡®ä¿äº†è§†é¢‘åºåˆ—çš„æ—¶é—´ä¸€è‡´æ€§å’Œè¿è´¯æ€§ã€‚æˆ‘ä»¬è¿˜åˆ›å»ºäº†ä¸€ä¸ªåŒ…å«72,000ä¸ªè§†é¢‘-å­—å¹•å¯¹çš„æ•°æ®é›†ï¼Œä»¥è¯„ä¼°æ¨¡å‹åœ¨å¤šç§çœŸå®åœºæ™¯ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.14755', 'title': 'Optimizing Length Compression in Large Reasoning Models', 'url': 'https://huggingface.co/papers/2506.14755', 'abstract': 'LC-R1, a post-training method guided by Brevity and Sufficiency principles, reduces unnecessary reasoning in Large Reasoning Models with minimal accuracy loss.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Reasoning Models (LRMs) have achieved remarkable success, yet they often suffer from producing unnecessary and verbose reasoning chains. We identify a core aspect of this issue as "invalid thinking" -- models tend to repeatedly double-check their work after having derived the correct answer. To address this specific inefficiency, we move beyond the general principles of Efficacy and Efficiency to propose two new, fine-grained principles: Brevity, which advocates for eliminating redundancy, and Sufficiency, which ensures critical reasoning steps are preserved. Guided by these principles, we introduce LC-R1, a post-training method based on Group Relative Policy Optimization (GRPO). LC-R1 employs a novel combination of a Length Reward for overall conciseness and a Compress Reward that is specifically designed to remove the invalid portion of the thinking process. Extensive experiments on multiple reasoning benchmarks demonstrate that LC-R1 achieves a significant reduction in sequence length (~50%) with only a marginal (~2%) drop in accuracy, achieving a favorable trade-off point on the Pareto frontier that prioritizes high compression. Our analysis further validates the robustness of LC-R1 and provides valuable insights for developing more powerful yet computationally efficient LRMs. Our code is released at https://github.com/zxiangx/LC-R1.', 'score': 4, 'issue_id': 4347, 'pub_date': '2025-06-17', 'pub_date_card': {'ru': '17 Ğ¸ÑĞ½Ñ', 'en': 'June 17', 'zh': '6æœˆ17æ—¥'}, 'hash': '837a56d067dd6e74', 'authors': ['Zhengxiang Cheng', 'Dongping Chen', 'Mingyang Fu', 'Tianyi Zhou'], 'affiliations': ['University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2506.14755.jpg', 'data': {'categories': ['#reasoning', '#training', '#architecture', '#benchmark', '#optimization'], 'emoji': 'âœ‚ï¸', 'ru': {'title': 'LC-R1: ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°', 'desc': 'LC-R1 - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ÑÑ‚-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (LRM), Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ°Ñ… ĞºÑ€Ğ°Ñ‚ĞºĞ¾ÑÑ‚Ğ¸ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ½ Ğ½Ğ°Ñ†ĞµĞ»ĞµĞ½ Ğ½Ğ° ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. LC-R1 Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ğ·Ğ° Ğ´Ğ»Ğ¸Ğ½Ñƒ Ğ¸ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ğ¾Ğ¹ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ (GRPO). Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ´Ğ»Ğ¸Ğ½Ñƒ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ¾ Ğ½Ğ° 50% Ğ¿Ñ€Ğ¸ Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²ÑĞµĞ³Ğ¾ Ğ½Ğ° 2%.'}, 'en': {'title': 'Streamlining Reasoning: LC-R1 for Efficient Large Models', 'desc': "The paper introduces LC-R1, a post-training method aimed at improving Large Reasoning Models (LRMs) by reducing unnecessary reasoning while maintaining accuracy. It identifies 'invalid thinking' as a key issue where models redundantly verify correct answers, leading to verbosity. To combat this, the authors propose two principles: Brevity, which focuses on cutting out redundant reasoning, and Sufficiency, which ensures essential reasoning steps are retained. Through experiments, LC-R1 demonstrates a significant reduction in reasoning sequence length by about 50% with only a slight accuracy drop of around 2%, showcasing an effective balance between compression and performance."}, 'zh': {'title': 'ç®€åŒ–æ¨ç†ï¼Œæå‡æ•ˆç‡ï¼', 'desc': 'LC-R1æ˜¯ä¸€ç§åè®­ç»ƒæ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡ç®€æ´æ€§å’Œå……åˆ†æ€§åŸåˆ™æ¥å‡å°‘å¤§å‹æ¨ç†æ¨¡å‹ä¸­çš„ä¸å¿…è¦æ¨ç†ï¼ŒåŒæ—¶ä¿æŒè¾ƒå°çš„å‡†ç¡®æ€§æŸå¤±ã€‚è¯¥æ–¹æ³•è¯†åˆ«å‡ºæ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­å­˜åœ¨çš„â€œæ— æ•ˆæ€ç»´â€é—®é¢˜ï¼Œå³æ¨¡å‹åœ¨å¾—å‡ºæ­£ç¡®ç­”æ¡ˆåä»ç„¶åå¤æ£€æŸ¥ã€‚ä¸ºäº†è§£å†³è¿™ä¸€ä½æ•ˆé—®é¢˜ï¼ŒLC-R1æå‡ºäº†ä¸¤ä¸ªæ–°åŸåˆ™ï¼šç®€æ´æ€§ï¼Œå¼ºè°ƒæ¶ˆé™¤å†—ä½™ï¼›å……åˆ†æ€§ï¼Œç¡®ä¿å…³é”®æ¨ç†æ­¥éª¤å¾—ä»¥ä¿ç•™ã€‚é€šè¿‡å¯¹å¤šä¸ªæ¨ç†åŸºå‡†çš„å¹¿æ³›å®éªŒï¼ŒLC-R1å®ç°äº†åºåˆ—é•¿åº¦çš„æ˜¾è‘—å‡å°‘ï¼ˆçº¦50%ï¼‰ï¼Œè€Œå‡†ç¡®æ€§ä»…ä¸‹é™çº¦2%ï¼Œåœ¨é«˜å‹ç¼©ç‡å’Œå‡†ç¡®æ€§ä¹‹é—´è¾¾æˆäº†è‰¯å¥½çš„å¹³è¡¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.09033', 'title': 'Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2506.09033', 'abstract': 'Router-R1, a reinforcement learning-based framework, improves multi-LLM routing by interleaving think and route actions, optimizing performance-cost trade-offs, and generalizing to unseen models.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid emergence of diverse large language models (LLMs) has spurred the development of LLM routers that assign user queries to the most suitable model. However, existing LLM routers typically perform a single-round, one-to-one mapping (i.e., assigning each query to a single model in isolation), which limits their capability to tackle complex tasks that demand the complementary strengths of multiple LLMs. In this paper, we present Router-R1, a reinforcement learning (RL)-based framework that formulates multi-LLM routing and aggregation as a sequential decision process. Router-R1 instantiates the router itself as a capable LLM, leveraging its reasoning ability to interleave "think" actions (internal deliberation) with "route" actions (dynamic model invocation), and integrates each response into its evolving context. To guide learning, we employ a lightweight rule-based reward comprising format rewards, final outcome rewards, and a novel cost reward for performance and cost trade-off optimization, opening a pathway toward optimizing performance-cost tradeoffs via RL. Router-R1 also conditions only on simple model descriptors such as pricing, latency, and example performance, enabling strong generalization to unseen model selection. Experiments on seven general and multi-hop QA benchmarks show that Router-R1 outperforms over several strong baselines, achieving superior performance while maintaining robust generalization and cost management.Code is available at https://github.com/ulab-uiuc/Router-R1.', 'score': 3, 'issue_id': 4357, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 Ğ¸ÑĞ½Ñ', 'en': 'June 10', 'zh': '6æœˆ10æ—¥'}, 'hash': '6f6eee917a3ef0d9', 'authors': ['Haozhen Zhang', 'Tao Feng', 'Jiaxuan You'], 'affiliations': ['University of Illinois at Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2506.09033.jpg', 'data': {'categories': ['#rlhf', '#rl', '#multimodal', '#optimization', '#reasoning', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ°Ñ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸', 'desc': 'Router-R1 - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞĞ½ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹, Ñ‡ĞµÑ€ĞµĞ´ÑƒÑ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¾Ğ±Ğ´ÑƒĞ¼Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Router-R1 Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑĞ¿Ğ¾ÑĞ¾Ğ±ĞµĞ½ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ¿Ğ¸Ñ€Ğ°ÑÑÑŒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğµ Ğ´ĞµÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ€Ğ¾Ğ´Ğµ Ñ†ĞµĞ½Ñ‹ Ğ¸ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¸.'}, 'en': {'title': 'Optimizing Multi-LLM Routing with Reinforcement Learning', 'desc': "Router-R1 is a reinforcement learning framework designed to enhance the routing of user queries among multiple large language models (LLMs). Unlike traditional routers that assign queries to a single model, Router-R1 interleaves 'think' and 'route' actions, allowing it to consider multiple models' strengths for complex tasks. It uses a rule-based reward system to optimize the balance between performance and cost, making it efficient in selecting the best model for each query. The framework demonstrates strong generalization capabilities, performing well on various benchmarks while managing costs effectively."}, 'zh': {'title': 'æ™ºèƒ½è·¯ç”±ï¼Œä¼˜åŒ–æ€§èƒ½ä¸æˆæœ¬çš„å¹³è¡¡', 'desc': 'Router-R1 æ˜¯ä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¡†æ¶ï¼Œæ—¨åœ¨æ”¹å–„å¤šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è·¯ç”±ã€‚å®ƒé€šè¿‡äº¤æ›¿è¿›è¡Œæ€è€ƒå’Œè·¯ç”±åŠ¨ä½œï¼Œä¼˜åŒ–æ€§èƒ½ä¸æˆæœ¬ä¹‹é—´çš„æƒè¡¡ï¼Œå¹¶èƒ½å¤Ÿæ¨å¹¿åˆ°æœªè§è¿‡çš„æ¨¡å‹ã€‚ä¸ä¼ ç»Ÿçš„å•è½®ä¸€å¯¹ä¸€æ˜ å°„ä¸åŒï¼ŒRouter-R1 å°†è·¯ç”±å’Œèšåˆè§†ä¸ºä¸€ä¸ªåºåˆ—å†³ç­–è¿‡ç¨‹ï¼Œåˆ©ç”¨å…¶æ¨ç†èƒ½åŠ›è¿›è¡ŒåŠ¨æ€æ¨¡å‹è°ƒç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRouter-R1 åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå±•ç°äº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›å’Œæˆæœ¬ç®¡ç†èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.14761', 'title': 'From Bytes to Ideas: Language Modeling with Autoregressive U-Nets', 'url': 'https://huggingface.co/papers/2506.14761', 'abstract': 'An autoregressive U-Net learns to embed its own tokens during training, enabling a multi-scale view of text sequences and improved handling of character-level tasks and low-resource languages.  \t\t\t\t\tAI-generated summary \t\t\t\t Tokenization imposes a fixed granularity on the input text, freezing how a language model operates on data and how far in the future it predicts. Byte Pair Encoding (BPE) and similar schemes split text once, build a static vocabulary, and leave the model stuck with that choice. We relax this rigidity by introducing an autoregressive U-Net that learns to embed its own tokens as it trains. The network reads raw bytes, pools them into words, then pairs of words, then up to 4 words, giving it a multi-scale view of the sequence. At deeper stages, the model must predict further into the future -- anticipating the next few words rather than the next byte -- so deeper stages focus on broader semantic patterns while earlier stages handle fine details. When carefully tuning and controlling pretraining compute, shallow hierarchies tie strong BPE baselines, and deeper hierarchies have a promising trend. Because tokenization now lives inside the model, the same system can handle character-level tasks and carry knowledge across low-resource languages.', 'score': 2, 'issue_id': 4362, 'pub_date': '2025-06-17', 'pub_date_card': {'ru': '17 Ğ¸ÑĞ½Ñ', 'en': 'June 17', 'zh': '6æœˆ17æ—¥'}, 'hash': '24f72bf9457e3acf', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#architecture', '#data', '#optimization', '#low_resource', '#multilingual'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ“Ğ¸Ğ±ĞºĞ°Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€Ğ°ÑÑĞ¸Ğ²Ğ½ÑƒÑ U-Net Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ² Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ñ Ğ¼Ğ°Ğ»Ğ¾Ñ€ĞµÑÑƒÑ€ÑĞ½Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ°Ğ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‡Ğ¸Ñ‚Ğ°ĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ±Ğ°Ğ¹Ñ‚Ñ‹ Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¸Ñ… Ğ² ÑĞ»Ğ¾Ğ²Ğ° Ğ¸ Ñ„Ñ€Ğ°Ğ·Ñ‹, Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ.'}, 'en': {'title': 'Flexible Tokenization for Enhanced Text Understanding', 'desc': "This paper presents an autoregressive U-Net model that learns to create its own token embeddings during training, allowing for a flexible approach to text processing. By reading raw bytes and progressively pooling them into larger units, the model gains a multi-scale perspective on text sequences. This design enables the model to predict further into the future at deeper layers, focusing on broader semantic patterns while earlier layers manage finer details. The approach not only improves performance on character-level tasks but also enhances the model's ability to work with low-resource languages by integrating tokenization within the model itself."}, 'zh': {'title': 'è‡ªå›å½’U-Netï¼šçµæ´»å¤„ç†æ–‡æœ¬çš„æœªæ¥', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§è‡ªå›å½’U-Netæ¨¡å‹ï¼Œå®ƒåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å­¦ä¹ åµŒå…¥è‡ªå·±çš„æ ‡è®°ï¼Œä»è€Œå®ç°å¯¹æ–‡æœ¬åºåˆ—çš„å¤šå°ºåº¦è§†å›¾ã€‚è¿™ç§æ–¹æ³•æ‰“ç ´äº†ä¼ ç»Ÿæ ‡è®°åŒ–çš„å›ºå®šç²’åº¦é™åˆ¶ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ›´çµæ´»åœ°å¤„ç†æ•°æ®ã€‚é€šè¿‡è¯»å–åŸå§‹å­—èŠ‚å¹¶é€æ­¥èšåˆæˆè¯ï¼Œæ¨¡å‹åœ¨ä¸åŒæ·±åº¦é˜¶æ®µé¢„æµ‹æ›´è¿œçš„æœªæ¥ï¼Œå…³æ³¨æ›´å¹¿æ³›çš„è¯­ä¹‰æ¨¡å¼ã€‚æœ€ç»ˆï¼Œè¿™ç§æ¨¡å‹ä¸ä»…èƒ½å¤„ç†å­—ç¬¦çº§ä»»åŠ¡ï¼Œè¿˜èƒ½åœ¨ä½èµ„æºè¯­è¨€ä¸­ä¼ é€’çŸ¥è¯†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.14731', 'title': 'Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning\n  for LLMs', 'url': 'https://huggingface.co/papers/2506.14731', 'abstract': 'Ring-lite uses a MoE architecture and reinforcement learning to efficiently match SOTA reasoning models while activating fewer parameters and addressing challenges specific to MoE training.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Ring-lite, a Mixture-of-Experts (MoE)-based large language model optimized via reinforcement learning (RL) to achieve efficient and robust reasoning capabilities. Built upon the publicly available Ling-lite model, a 16.8 billion parameter model with 2.75 billion activated parameters, our approach matches the performance of state-of-the-art (SOTA) small-scale reasoning models on challenging benchmarks (e.g., AIME, LiveCodeBench, GPQA-Diamond) while activating only one-third of the parameters required by comparable models. To accomplish this, we introduce a joint training pipeline integrating distillation with RL, revealing undocumented challenges in MoE RL training. First, we identify optimization instability during RL training, and we propose Constrained Contextual Computation Policy Optimization(C3PO), a novel approach that enhances training stability and improves computational throughput via algorithm-system co-design methodology. Second, we empirically demonstrate that selecting distillation checkpoints based on entropy loss for RL training, rather than validation metrics, yields superior performance-efficiency trade-offs in subsequent RL training. Finally, we develop a two-stage training paradigm to harmonize multi-domain data integration, addressing domain conflicts that arise in training with mixed dataset. We will release the model, dataset, and code.', 'score': 2, 'issue_id': 4350, 'pub_date': '2025-06-17', 'pub_date_card': {'ru': '17 Ğ¸ÑĞ½Ñ', 'en': 'June 17', 'zh': '6æœˆ17æ—¥'}, 'hash': '7c1c5a66d6e8f898', 'authors': ['Ring Team', 'Bin Hu', 'Cai Chen', 'Deng Zhao', 'Ding Liu', 'Dingnan Jin', 'Feng Zhu', 'Hao Dai', 'Hongzhi Luan', 'Jia Guo', 'Jiaming Liu', 'Jiewei Wu', 'Jun Mei', 'Jun Zhou', 'Junbo Zhao', 'Junwu Xiong', 'Kaihong Zhang', 'Kuan Xu', 'Lei Liang', 'Liang Jiang', 'Liangcheng Fu', 'Longfei Zheng', 'Qiang Gao', 'Qing Cui', 'Quan Wan', 'Shaomian Zheng', 'Shuaicheng Li', 'Tongkai Yang', 'Wang Ren', 'Xiaodong Yan', 'Xiaopei Wan', 'Xiaoyun Feng', 'Xin Zhao', 'Xinxing Yang', 'Xinyu Kong', 'Xuemin Yang', 'Yang Li', 'Yingting Wu', 'Yongkang Liu', 'Zhankai Xu', 'Zhenduo Zhang', 'Zhenglei Zhou', 'Zhenyu Huang', 'Zhiqiang Zhang', 'Zihao Wang', 'Zujie Wen'], 'affiliations': ['Ring Team, Inclusion AI'], 'pdf_title_img': 'assets/pdf/title_img/2506.14731.jpg', 'data': {'categories': ['#dataset', '#optimization', '#training', '#reasoning', '#open_source', '#rl', '#architecture'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸', 'desc': 'Ring-lite - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Mixture-of-Experts (MoE), Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€ÑƒÑ Ğ»Ğ¸ÑˆÑŒ Ñ‚Ñ€ĞµÑ‚ÑŒ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ C3PO Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ².'}, 'en': {'title': 'Efficient Reasoning with Fewer Parameters: Introducing Ring-lite', 'desc': 'Ring-lite is a large language model that uses a Mixture-of-Experts (MoE) architecture combined with reinforcement learning (RL) to enhance reasoning capabilities while minimizing parameter activation. It builds on the Ling-lite model, achieving state-of-the-art performance on various reasoning benchmarks with only a fraction of the parameters activated compared to similar models. The paper introduces a novel training method called Constrained Contextual Computation Policy Optimization (C3PO) to improve stability during RL training and optimize computational efficiency. Additionally, it highlights the importance of selecting distillation checkpoints based on entropy loss for better performance in RL training and proposes a two-stage training approach to manage domain conflicts in mixed datasets.'}, 'zh': {'title': 'é«˜æ•ˆæ¨ç†ï¼Œæ¿€æ´»æ›´å°‘å‚æ•°çš„Ring-lite', 'desc': 'Ring-liteæ˜¯ä¸€ç§åŸºäºä¸“å®¶æ··åˆï¼ˆMoEï¼‰æ¶æ„çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œä¼˜åŒ–ï¼Œä»¥å®ç°é«˜æ•ˆä¸”ç¨³å¥çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¨¡å‹åœ¨Ling-liteçš„åŸºç¡€ä¸Šæ„å»ºï¼Œå…·æœ‰168äº¿ä¸ªå‚æ•°ï¼Œä½†ä»…æ¿€æ´»2.75äº¿ä¸ªå‚æ•°ï¼Œèƒ½å¤Ÿåœ¨å¤šä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸­ä¸å°è§„æ¨¡çš„æœ€å…ˆè¿›æ¨ç†æ¨¡å‹ç›¸åŒ¹é…ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§è”åˆè®­ç»ƒæµç¨‹ï¼Œå°†è’¸é¦ä¸å¼ºåŒ–å­¦ä¹ ç»“åˆï¼Œè§£å†³äº†MoEå¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸­çš„ä¸€äº›æœªè®°å½•çš„æŒ‘æˆ˜ã€‚é€šè¿‡å¼•å…¥å—é™ä¸Šä¸‹æ–‡è®¡ç®—ç­–ç•¥ä¼˜åŒ–ï¼ˆC3POï¼‰ï¼Œæˆ‘ä»¬æé«˜äº†è®­ç»ƒçš„ç¨³å®šæ€§ï¼Œå¹¶é€šè¿‡ç®—æ³•ä¸ç³»ç»Ÿçš„å…±åŒè®¾è®¡æ–¹æ³•æ”¹å–„äº†è®¡ç®—ååé‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.14702', 'title': 'Treasure Hunt: Real-time Targeting of the Long Tail using Training-Time\n  Markers', 'url': 'https://huggingface.co/papers/2506.14702', 'abstract': 'A principled approach to fine-tuning models for better performance and controllability on underrepresented use cases is developed through automatic inference of generation attributes.  \t\t\t\t\tAI-generated summary \t\t\t\t One of the most profound challenges of modern machine learning is performing well on the long-tail of rare and underrepresented features. Large general-purpose models are trained for many tasks, but work best on high-frequency use cases. After training, it is hard to adapt a model to perform well on specific use cases underrepresented in the training corpus. Relying on prompt engineering or few-shot examples to maximize the output quality on a particular test case can be frustrating, as models can be highly sensitive to small changes, react in unpredicted ways or rely on a fixed system prompt for maintaining performance. In this work, we ask: "Can we optimize our training protocols to both improve controllability and performance on underrepresented use cases at inference time?" We revisit the divide between training and inference techniques to improve long-tail performance while providing users with a set of control levers the model is trained to be responsive to. We create a detailed taxonomy of data characteristics and task provenance to explicitly control generation attributes and implicitly condition generations at inference time. We fine-tune a base model to infer these markers automatically, which makes them optional at inference time. This principled and flexible approach yields pronounced improvements in performance, especially on examples from the long tail of the training distribution. While we observe an average lift of 5.7% win rates in open-ended generation quality with our markers, we see over 9.1% gains in underrepresented domains. We also observe relative lifts of up to 14.1% on underrepresented tasks like CodeRepair and absolute improvements of 35.3% on length instruction following evaluations.', 'score': 2, 'issue_id': 4350, 'pub_date': '2025-06-17', 'pub_date_card': {'ru': '17 Ğ¸ÑĞ½Ñ', 'en': 'June 17', 'zh': '6æœˆ17æ—¥'}, 'hash': '2fbff0f4b562f92e', 'authors': ["Daniel D'souza", 'Julia Kreutzer', 'Adrien Morisot', 'Ahmet ÃœstÃ¼n', 'Sara Hooker'], 'affiliations': ['Cohere', 'Cohere Labs'], 'pdf_title_img': 'assets/pdf/title_img/2506.14702.jpg', 'data': {'categories': ['#long_context', '#optimization', '#training'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ñ€ĞµĞ´ĞºĞ¸Ñ… ÑĞ»ÑƒÑ‡Ğ°ĞµĞ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€ĞµĞ´ĞºĞ¸Ñ… Ğ¸ Ğ½ĞµĞ´Ğ¾Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ÑÑ… Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸Ğº Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ´Ğ»Ñ ÑĞ²Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ ÑÑ‚Ğ¸ Ğ¼Ğ°Ñ€ĞºĞµÑ€Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ¸Ñ… Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ… Ğ¸Ğ· Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ…Ğ²Ğ¾ÑÑ‚Ğ° Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Optimizing Model Performance for Rare Use Cases', 'desc': "This paper addresses the challenge of improving machine learning model performance on rare and underrepresented use cases, often referred to as the long tail. It proposes a method for fine-tuning models that enhances both controllability and performance by automatically inferring generation attributes during inference. The authors introduce a taxonomy of data characteristics to help guide the model's output, allowing for better adaptation to specific tasks without relying heavily on prompt engineering. Their approach demonstrates significant performance improvements, particularly in underrepresented domains, achieving notable gains in generation quality and task-specific evaluations."}, 'zh': {'title': 'ä¼˜åŒ–æ¨¡å‹ä»¥æå‡ç¨€æœ‰ç”¨ä¾‹çš„æ€§èƒ½ä¸å¯æ§æ€§', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ç³»ç»Ÿçš„æ–¹æ³•ï¼Œé€šè¿‡è‡ªåŠ¨æ¨æ–­ç”Ÿæˆå±æ€§æ¥å¾®è°ƒæ¨¡å‹ï¼Œä»¥æé«˜åœ¨ç¨€æœ‰å’Œæœªå……åˆ†ä»£è¡¨çš„ç”¨ä¾‹ä¸Šçš„æ€§èƒ½å’Œå¯æ§æ€§ã€‚ç°ä»£æœºå™¨å­¦ä¹ é¢ä¸´çš„ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜æ˜¯å¦‚ä½•åœ¨é•¿å°¾ç‰¹å¾ä¸Šè¡¨ç°è‰¯å¥½ï¼Œå°¤å…¶æ˜¯åœ¨è®­ç»ƒæ•°æ®ä¸­è¾ƒå°‘å‡ºç°çš„ç‰¹å¾ã€‚æˆ‘ä»¬é‡æ–°å®¡è§†è®­ç»ƒå’Œæ¨ç†æŠ€æœ¯ä¹‹é—´çš„å·®è·ï¼Œä»¥æ”¹å–„é•¿å°¾æ€§èƒ½ï¼Œå¹¶ä¸ºç”¨æˆ·æä¾›ä¸€ç»„å¯æ§çš„ç”Ÿæˆå±æ€§ã€‚é€šè¿‡å¯¹åŸºç¡€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œæˆ‘ä»¬å®ç°äº†åœ¨æ¨ç†æ—¶è‡ªåŠ¨æ¨æ–­è¿™äº›æ ‡è®°ï¼Œä»è€Œåœ¨æœªå……åˆ†ä»£è¡¨çš„é¢†åŸŸä¸­æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.13599', 'title': 'CAMS: A CityGPT-Powered Agentic Framework for Urban Human Mobility\n  Simulation', 'url': 'https://huggingface.co/papers/2506.13599', 'abstract': 'CAMS integrates an agentic framework with urban-knowledgeable large language models to simulate human mobility more realistically by modeling individual and collective patterns.  \t\t\t\t\tAI-generated summary \t\t\t\t Human mobility simulation plays a crucial role in various real-world applications. Recently, to address the limitations of traditional data-driven approaches, researchers have explored leveraging the commonsense knowledge and reasoning capabilities of large language models (LLMs) to accelerate human mobility simulation. However, these methods suffer from several critical shortcomings, including inadequate modeling of urban spaces and poor integration with both individual mobility patterns and collective mobility distributions. To address these challenges, we propose CityGPT-Powered Agentic framework for Mobility Simulation (CAMS), an agentic framework that leverages the language based urban foundation model to simulate human mobility in urban space. CAMS comprises three core modules, including MobExtractor to extract template mobility patterns and synthesize new ones based on user profiles, GeoGenerator to generate anchor points considering collective knowledge and generate candidate urban geospatial knowledge using an enhanced version of CityGPT, TrajEnhancer to retrieve spatial knowledge based on mobility patterns and generate trajectories with real trajectory preference alignment via DPO. Experiments on real-world datasets show that CAMS achieves superior performance without relying on externally provided geospatial information. Moreover, by holistically modeling both individual mobility patterns and collective mobility constraints, CAMS generates more realistic and plausible trajectories. In general, CAMS establishes a new paradigm that integrates the agentic framework with urban-knowledgeable LLMs for human mobility simulation.', 'score': 2, 'issue_id': 4348, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 Ğ¸ÑĞ½Ñ', 'en': 'June 16', 'zh': '6æœˆ16æ—¥'}, 'hash': '0b0a6282d1310e1b', 'authors': ['Yuwei Du', 'Jie Feng', 'Jian Yuan', 'Yong Li'], 'affiliations': ['Department of Electronic Engineering, BRNist, Tsinghua University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.13599.jpg', 'data': {'categories': ['#agents', '#synthetic', '#reasoning', '#multimodal'], 'emoji': 'ğŸ™ï¸', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ³Ğ¾Ñ€Ğ¾Ğ´ÑĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'CAMS (CityGPT-Powered Agentic framework for Mobility Simulation) - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ³Ğ¾Ñ€Ğ¾Ğ´ÑĞºĞ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ³Ğ¾Ñ€Ğ¾Ğ´ÑĞºĞ¾Ğ¹ ÑÑ€ĞµĞ´Ñ‹ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ ĞºĞ¾Ğ»Ğ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² Ğ¿ĞµÑ€ĞµĞ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. CAMS Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ: MobExtractor Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ğ¾Ğ² Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, GeoGenerator Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ¿Ğ¾Ñ€Ğ½Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº, Ğ¸ TrajEnhancer Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ CAMS Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Revolutionizing Urban Mobility Simulation with CAMS', 'desc': 'CAMS introduces a novel framework that combines agent-based modeling with large language models to enhance the simulation of human mobility in urban environments. It addresses the limitations of traditional methods by integrating individual and collective mobility patterns, allowing for more realistic trajectory generation. The framework consists of three main components: MobExtractor for mobility pattern extraction, GeoGenerator for urban geospatial knowledge generation, and TrajEnhancer for trajectory refinement. Experiments demonstrate that CAMS outperforms existing approaches by generating plausible mobility trajectories without needing external geospatial data.'}, 'zh': {'title': 'åŸå¸‚ç§»åŠ¨æ¨¡æ‹Ÿçš„æ–°èŒƒå¼', 'desc': 'CAMSæ˜¯ä¸€ä¸ªç»“åˆäº†ä»£ç†æ¡†æ¶å’ŒåŸå¸‚çŸ¥è¯†çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç”¨äºæ›´çœŸå®åœ°æ¨¡æ‹Ÿäººç±»çš„ç§»åŠ¨è¡Œä¸ºã€‚å®ƒé€šè¿‡ä¸‰ä¸ªæ ¸å¿ƒæ¨¡å—æ¥å®ç°è¿™ä¸€ç›®æ ‡ï¼šMobExtractoræå–å’Œåˆæˆç”¨æˆ·çš„ç§»åŠ¨æ¨¡å¼ï¼ŒGeoGeneratorç”Ÿæˆè€ƒè™‘é›†ä½“çŸ¥è¯†çš„åŸå¸‚åœ°ç†ä¿¡æ¯ï¼ŒTrajEnhanceræ ¹æ®ç§»åŠ¨æ¨¡å¼ç”Ÿæˆç¬¦åˆçœŸå®åå¥½çš„è½¨è¿¹ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒCAMSåœ¨ä¸ä¾èµ–å¤–éƒ¨åœ°ç†ä¿¡æ¯çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°å»ºæ¨¡ä¸ªä½“å’Œé›†ä½“çš„ç§»åŠ¨æ¨¡å¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCAMSç”Ÿæˆçš„è½¨è¿¹æ›´åŠ çœŸå®å¯ä¿¡ï¼Œå¼€åˆ›äº†äººç±»ç§»åŠ¨æ¨¡æ‹Ÿçš„æ–°èŒƒå¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.05426', 'title': 'Mixture-of-Experts Meets In-Context Reinforcement Learning', 'url': 'https://huggingface.co/papers/2506.05426', 'abstract': 'T2MIR, a framework using token-wise and task-wise MoE in transformer-based decision models, enhances in-context reinforcement learning by addressing multi-modality and task diversity.  \t\t\t\t\tAI-generated summary \t\t\t\t In-context reinforcement learning (ICRL) has emerged as a promising paradigm for adapting RL agents to downstream tasks through prompt conditioning. However, two notable challenges remain in fully harnessing in-context learning within RL domains: the intrinsic multi-modality of the state-action-reward data and the diverse, heterogeneous nature of decision tasks. To tackle these challenges, we propose T2MIR (Token- and Task-wise MoE for In-context RL), an innovative framework that introduces architectural advances of mixture-of-experts (MoE) into transformer-based decision models. T2MIR substitutes the feedforward layer with two parallel layers: a token-wise MoE that captures distinct semantics of input tokens across multiple modalities, and a task-wise MoE that routes diverse tasks to specialized experts for managing a broad task distribution with alleviated gradient conflicts. To enhance task-wise routing, we introduce a contrastive learning method that maximizes the mutual information between the task and its router representation, enabling more precise capture of task-relevant information. The outputs of two MoE components are concatenated and fed into the next layer. Comprehensive experiments show that T2MIR significantly facilitates in-context learning capacity and outperforms various types of baselines. We bring the potential and promise of MoE to ICRL, offering a simple and scalable architectural enhancement to advance ICRL one step closer toward achievements in language and vision communities. Our code is available at https://github.com/NJU-RL/T2MIR.', 'score': 2, 'issue_id': 4356, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 Ğ¸ÑĞ½Ñ', 'en': 'June 5', 'zh': '6æœˆ5æ—¥'}, 'hash': '4e79eb4ebca225c7', 'authors': ['Wenhao Wu', 'Fuhong Liu', 'Haoru Li', 'Zican Hu', 'Daoyi Dong', 'Chunlin Chen', 'Zhi Wang'], 'affiliations': ['Australian Artificial Intelligence Institute, University of Technology Sydney', 'Nanjing University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05426.jpg', 'data': {'categories': ['#optimization', '#rl', '#games', '#multimodal', '#architecture'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'T2MIR: Ğ¡Ğ¼ĞµÑÑŒ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ', 'desc': 'T2MIR - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ (ICRL), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ ÑĞ¼ĞµÑÑŒ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² (MoE) Ğ² Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‚Ğ¾ĞºĞµĞ½-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ MoE. T2MIR Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'Enhancing In-Context Learning with T2MIR: A Mixture-of-Experts Approach', 'desc': 'The paper introduces T2MIR, a novel framework that enhances in-context reinforcement learning (ICRL) by integrating mixture-of-experts (MoE) into transformer-based decision models. It addresses the challenges of multi-modality in state-action-reward data and the diversity of decision tasks by implementing token-wise and task-wise MoE layers. The token-wise MoE captures different meanings of input tokens, while the task-wise MoE directs tasks to specialized experts, reducing conflicts during training. Experimental results demonstrate that T2MIR improves the learning capacity of ICRL and outperforms existing methods, showcasing its potential for advancements in both language and vision tasks.'}, 'zh': {'title': 'T2MIRï¼šæå‡ä¸Šä¸‹æ–‡å¼ºåŒ–å­¦ä¹ çš„ä¸“å®¶æ··åˆæ¡†æ¶', 'desc': 'T2MIRæ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œåˆ©ç”¨åŸºäºTransformerçš„å†³ç­–æ¨¡å‹ä¸­çš„é€tokenå’Œé€ä»»åŠ¡çš„ä¸“å®¶æ··åˆï¼ˆMoEï¼‰æ–¹æ³•ï¼Œå¢å¼ºäº†ä¸Šä¸‹æ–‡å¼ºåŒ–å­¦ä¹ ï¼ˆICRLï¼‰ã€‚è¯¥æ¡†æ¶è§£å†³äº†çŠ¶æ€-åŠ¨ä½œ-å¥–åŠ±æ•°æ®çš„å¤šæ¨¡æ€æ€§å’Œå†³ç­–ä»»åŠ¡çš„å¤šæ ·æ€§é—®é¢˜ã€‚T2MIRé€šè¿‡å¼•å…¥ä¸¤ä¸ªå¹¶è¡Œå±‚ï¼Œåˆ†åˆ«æ˜¯é€tokençš„MoEå’Œé€ä»»åŠ¡çš„MoEï¼Œæ¥æ•æ‰è¾“å…¥tokençš„ä¸åŒè¯­ä¹‰ï¼Œå¹¶å°†å¤šæ ·åŒ–çš„ä»»åŠ¡åˆ†é…ç»™ä¸“é—¨çš„ä¸“å®¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒT2MIRæ˜¾è‘—æé«˜äº†ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼Œå¹¶åœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.13901', 'title': 'Alignment Quality Index (AQI) : Beyond Refusals: AQI as an Intrinsic\n  Alignment Diagnostic via Latent Geometry, Cluster Divergence, and Layer wise\n  Pooled Representations', 'url': 'https://huggingface.co/papers/2506.13901', 'abstract': "A new evaluation metric called Alignment Quality Index (AQI) assesses the alignment of large language models by analyzing latent space activations, capturing clustering quality to detect misalignments and fake alignment, and complementing existing behavioral proxies.  \t\t\t\t\tAI-generated summary \t\t\t\t Alignment is no longer a luxury, it is a necessity. As large language models (LLMs) enter high-stakes domains like education, healthcare, governance, and law, their behavior must reliably reflect human-aligned values and safety constraints. Yet current evaluations rely heavily on behavioral proxies such as refusal rates, G-Eval scores, and toxicity classifiers, all of which have critical blind spots. Aligned models are often vulnerable to jailbreaking, stochasticity of generation, and alignment faking.   To address this issue, we introduce the Alignment Quality Index (AQI). This novel geometric and prompt-invariant metric empirically assesses LLM alignment by analyzing the separation of safe and unsafe activations in latent space. By combining measures such as the Davies-Bouldin Score (DBS), Dunn Index (DI), Xie-Beni Index (XBI), and Calinski-Harabasz Index (CHI) across various formulations, AQI captures clustering quality to detect hidden misalignments and jailbreak risks, even when outputs appear compliant. AQI also serves as an early warning signal for alignment faking, offering a robust, decoding invariant tool for behavior agnostic safety auditing.   Additionally, we propose the LITMUS dataset to facilitate robust evaluation under these challenging conditions. Empirical tests on LITMUS across different models trained under DPO, GRPO, and RLHF conditions demonstrate AQI's correlation with external judges and ability to reveal vulnerabilities missed by refusal metrics. We make our implementation publicly available to foster future research in this area.", 'score': 1, 'issue_id': 4351, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 Ğ¸ÑĞ½Ñ', 'en': 'June 16', 'zh': '6æœˆ16æ—¥'}, 'hash': '5492fc2feb0ae2f3', 'authors': ['Abhilekh Borah', 'Chhavi Sharma', 'Danush Khanna', 'Utkarsh Bhatt', 'Gurpreet Singh', 'Hasnat Md Abdullah', 'Raghav Kaushik Ravi', 'Vinija Jain', 'Jyoti Patel', 'Shubham Singh', 'Vasu Sharma', 'Arpita Vats', 'Rahul Raja', 'Aman Chadha', 'Amitava Das'], 'affiliations': ['Amazon AI', 'BITS Goa, India', 'Evalueserve', 'IIIT Guwahati, India', 'IIT Kharagpur, India', 'LinkedIn', 'Manipal University Jaipur, India', 'Meta AI', 'New York University, USA', 'Texas A&M University, USA', 'Vellore Institute of Technology, Chennai, India'], 'pdf_title_img': 'assets/pdf/title_img/2506.13901.jpg', 'data': {'categories': ['#security', '#rlhf', '#alignment', '#benchmark', '#dataset', '#open_source'], 'emoji': 'ğŸ¯', 'ru': {'title': 'AQI: Ğ“ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ (alignment) Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ - Ğ˜Ğ½Ğ´ĞµĞºÑ ĞšĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ’Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ (AQI). AQI Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ² ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ñ„Ğ°Ğ»ÑŒÑˆĞ¸Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ. ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ° Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾ĞºÑĞ¸-Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ° Ğ¾Ñ‚ĞºĞ°Ğ·Ğ¾Ğ² Ğ¸ Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ. AQI Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ÑŒ Ñ€Ğ°Ğ½Ğ½Ğ¸Ğ¼ Ğ¸Ğ½Ğ´Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğº Ğ¾Ğ±Ñ…Ğ¾Ğ´Ğ° Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ°ÑƒĞ´Ğ¸Ñ‚Ğ° Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Ensuring True Alignment in Language Models with AQI', 'desc': "The paper introduces a new evaluation metric called the Alignment Quality Index (AQI) to assess the alignment of large language models (LLMs). AQI analyzes latent space activations to measure clustering quality, helping to identify misalignments and instances of alignment faking that traditional behavioral proxies may overlook. By utilizing established clustering metrics like the Davies-Bouldin Score and Dunn Index, AQI provides a more reliable assessment of model safety and alignment in high-stakes applications. The authors also present the LITMUS dataset to support rigorous evaluation, demonstrating AQI's effectiveness in revealing vulnerabilities that other metrics fail to detect."}, 'zh': {'title': 'å¯¹é½è´¨é‡æŒ‡æ•°ï¼šç¡®ä¿å¤§å‹è¯­è¨€æ¨¡å‹çš„å®‰å…¨ä¸å¯é ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æŒ‡æ ‡ï¼Œç§°ä¸ºå¯¹é½è´¨é‡æŒ‡æ•°ï¼ˆAQIï¼‰ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„å¯¹é½æƒ…å†µã€‚AQIé€šè¿‡åˆ†ææ½œåœ¨ç©ºé—´ä¸­çš„æ¿€æ´»åˆ†ç¦»ï¼Œæ•æ‰èšç±»è´¨é‡ï¼Œä»¥æ£€æµ‹æ¨¡å‹çš„é”™è¯¯å¯¹é½å’Œä¼ªå¯¹é½ç°è±¡ã€‚ä¸ç°æœ‰çš„è¡Œä¸ºä»£ç†ç›¸æ¯”ï¼ŒAQIèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°è¯†åˆ«æ¨¡å‹çš„å®‰å…¨æ€§å’Œæ½œåœ¨é£é™©ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†LITMUSæ•°æ®é›†ï¼Œä»¥æ”¯æŒåœ¨å¤æ‚æ¡ä»¶ä¸‹çš„ç¨³å¥è¯„ä¼°ï¼Œå¹¶å±•ç¤ºäº†AQIä¸å¤–éƒ¨è¯„å®¡è€…çš„ç›¸å…³æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.13387', 'title': 'TR2M: Transferring Monocular Relative Depth to Metric Depth with\n  Language Descriptions and Scale-Oriented Contrast', 'url': 'https://huggingface.co/papers/2506.13387', 'abstract': "A framework, TR2M, uses multimodal inputs to rescale relative depth to metric depth, enhancing performance across various datasets through cross-modality attention and contrastive learning.  \t\t\t\t\tAI-generated summary \t\t\t\t This work presents a generalizable framework to transfer relative depth to metric depth. Current monocular depth estimation methods are mainly divided into metric depth estimation (MMDE) and relative depth estimation (MRDE). MMDEs estimate depth in metric scale but are often limited to a specific domain. MRDEs generalize well across different domains, but with uncertain scales which hinders downstream applications. To this end, we aim to build up a framework to solve scale uncertainty and transfer relative depth to metric depth. Previous methods used language as input and estimated two factors for conducting rescaling. Our approach, TR2M, utilizes both text description and image as inputs and estimates two rescale maps to transfer relative depth to metric depth at pixel level. Features from two modalities are fused with a cross-modality attention module to better capture scale information. A strategy is designed to construct and filter confident pseudo metric depth for more comprehensive supervision. We also develop scale-oriented contrastive learning to utilize depth distribution as guidance to enforce the model learning about intrinsic knowledge aligning with the scale distribution. TR2M only exploits a small number of trainable parameters to train on datasets in various domains and experiments not only demonstrate TR2M's great performance in seen datasets but also reveal superior zero-shot capabilities on five unseen datasets. We show the huge potential in pixel-wise transferring relative depth to metric depth with language assistance. (Code is available at: https://github.com/BeileiCui/TR2M)", 'score': 1, 'issue_id': 4347, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 Ğ¸ÑĞ½Ñ', 'en': 'June 16', 'zh': '6æœˆ16æ—¥'}, 'hash': '6d0fc497ae4dcfd0', 'authors': ['Beilei Cui', 'Yiming Huang', 'Long Bai', 'Hongliang Ren'], 'affiliations': ['The Chinese University of Hong Kong, Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2506.13387.jpg', 'data': {'categories': ['#transfer_learning', '#cv', '#multimodal'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'TR2M - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ² Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ½ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. TR2M Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ°Ğº Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ²Ğµ ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ² Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ĞºĞ°Ğº Ğ½Ğ° Ğ·Ğ½Ğ°ĞºĞ¾Ğ¼Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ² Ğ¿Ğ¾Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Transforming Relative Depth to Metric Depth with TR2M', 'desc': 'The paper introduces TR2M, a framework that effectively converts relative depth information into metric depth using multimodal inputs, specifically images and text. It addresses the limitations of existing monocular depth estimation methods by combining the strengths of metric and relative depth estimation. TR2M employs cross-modality attention to enhance feature fusion and utilizes contrastive learning to improve scale alignment. The framework demonstrates strong performance across various datasets, including impressive zero-shot capabilities on unseen data, showcasing its versatility and effectiveness in depth estimation tasks.'}, 'zh': {'title': 'TR2Mï¼šç›¸å¯¹æ·±åº¦åˆ°åº¦é‡æ·±åº¦çš„æ™ºèƒ½è½¬æ¢', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºTR2Mçš„æ¡†æ¶ï¼Œæ—¨åœ¨å°†ç›¸å¯¹æ·±åº¦è½¬æ¢ä¸ºåº¦é‡æ·±åº¦ï¼Œåˆ©ç”¨å¤šæ¨¡æ€è¾“å…¥æå‡åœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚å½“å‰çš„å•ç›®æ·±åº¦ä¼°è®¡æ–¹æ³•ä¸»è¦åˆ†ä¸ºåº¦é‡æ·±åº¦ä¼°è®¡å’Œç›¸å¯¹æ·±åº¦ä¼°è®¡ï¼Œå‰è€…åœ¨ç‰¹å®šé¢†åŸŸè¡¨ç°è‰¯å¥½ï¼Œä½†å±€é™æ€§è¾ƒå¤§ï¼Œè€Œåè€…åœ¨ä¸åŒé¢†åŸŸå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†å­˜åœ¨å°ºåº¦ä¸ç¡®å®šæ€§çš„é—®é¢˜ã€‚TR2Mé€šè¿‡èåˆæ–‡æœ¬æè¿°å’Œå›¾åƒè¾“å…¥ï¼Œåˆ©ç”¨äº¤å‰æ¨¡æ€æ³¨æ„åŠ›æ¨¡å—å’Œå¯¹æ¯”å­¦ä¹ ç­–ç•¥ï¼Œæ„å»ºäº†ä¸¤ä¸ªé‡æ ‡å®šå›¾ä»¥åœ¨åƒç´ çº§åˆ«ä¸Šè¿›è¡Œæ·±åº¦è½¬æ¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTR2Måœ¨å·²çŸ¥æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¹¶åœ¨äº”ä¸ªæœªçŸ¥æ•°æ®é›†ä¸Šå±•ç°å‡ºå“è¶Šçš„é›¶æ ·æœ¬èƒ½åŠ›ï¼Œæ˜¾ç¤ºå‡ºåœ¨åƒç´ çº§åˆ«ä¸Šåˆ©ç”¨è¯­è¨€è¾…åŠ©è¿›è¡Œæ·±åº¦è½¬æ¢çš„å·¨å¤§æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.12880', 'title': 'Universal Jailbreak Suffixes Are Strong Attention Hijackers', 'url': 'https://huggingface.co/papers/2506.12880', 'abstract': "Suffix-based jailbreaks exploit adversarial suffixes to hijack large language models, with effectiveness linked to suffix universality; the method can be enhanced and mitigated with minimal computational or utility cost.  \t\t\t\t\tAI-generated summary \t\t\t\t We study suffix-based jailbreaksx2013a powerful family of attacks against large language models (LLMs) that optimize adversarial suffixes to circumvent safety alignment. Focusing on the widely used foundational GCG attack (Zou et al., 2023), we observe that suffixes vary in efficacy: some markedly more universalx2013generalizing to many unseen harmful instructionsx2013than others. We first show that GCG's effectiveness is driven by a shallow, critical mechanism, built on the information flow from the adversarial suffix to the final chat template tokens before generation. Quantifying the dominance of this mechanism during generation, we find GCG irregularly and aggressively hijacks the contextualization process. Crucially, we tie hijacking to the universality phenomenon, with more universal suffixes being stronger hijackers. Subsequently, we show that these insights have practical implications: GCG universality can be efficiently enhanced (up to times5 in some cases) at no additional computational cost, and can also be surgically mitigated, at least halving attack success with minimal utility loss. We release our code and data at http://github.com/matanbt/interp-jailbreak.", 'score': 1, 'issue_id': 4357, 'pub_date': '2025-06-15', 'pub_date_card': {'ru': '15 Ğ¸ÑĞ½Ñ', 'en': 'June 15', 'zh': '6æœˆ15æ—¥'}, 'hash': 'b2ad9af4ff256800', 'authors': ['Matan Ben-Tov', 'Mor Geva', 'Mahmood Sharif'], 'affiliations': ['Blavatnik School of Computer Science and AI, Tel Aviv University'], 'pdf_title_img': 'assets/pdf/title_img/2506.12880.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#security', '#hallucinations', '#multimodal', '#open_source', '#alignment', '#data'], 'emoji': 'ğŸ”“', 'ru': {'title': 'Ğ£ÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ°Ñ‚Ğ°ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑƒÑ„Ñ„Ğ¸ĞºÑĞ¾Ğ²', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑƒÑ„Ñ„Ğ¸ĞºÑĞ¾Ğ² Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ñ€Ğ°Ğ¶Ğ´ĞµĞ±Ğ½Ñ‹Ğµ ÑÑƒÑ„Ñ„Ğ¸ĞºÑÑ‹ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ…Ğ¾Ğ´Ğ° Ğ¼ĞµÑ€ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°Ñ‚Ğ°ĞºĞ¸ GCG Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑÑƒÑ„Ñ„Ğ¸ĞºÑĞ¾Ğ² Ğ¸ Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ‘Ğ¾Ğ»ĞµĞµ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑƒÑ„Ñ„Ğ¸ĞºÑÑ‹ Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ÑÑŒ ÑĞ¸Ğ»ÑŒĞ½ĞµĞµ Ğ² Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ñ‚Ğ°Ğº Ğ¸ Ğ¸Ñ… ÑĞ¼ÑĞ³Ñ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Unlocking and Mitigating Suffix-Based Attacks on Language Models', 'desc': "This paper investigates suffix-based jailbreaks, which are attacks on large language models (LLMs) that use specific suffixes to bypass safety measures. The authors focus on the GCG attack, revealing that some suffixes are more effective than others due to their universality, meaning they can apply to a wider range of harmful instructions. They identify a key mechanism in how these suffixes influence the model's output, showing that more universal suffixes are better at hijacking the model's contextualization process. The study also presents methods to enhance the effectiveness of these attacks without extra computational costs and suggests ways to mitigate them while maintaining model utility."}, 'zh': {'title': 'åç¼€æ”»å‡»ï¼šç»•è¿‡å®‰å…¨æœºåˆ¶çš„æ–°æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡ç ”ç©¶äº†ä¸€ç§åŸºäºåç¼€çš„æ”»å‡»æ–¹æ³•ï¼Œæ—¨åœ¨ç»•è¿‡å¤§å‹è¯­è¨€æ¨¡å‹çš„å®‰å…¨æœºåˆ¶ã€‚æˆ‘ä»¬å‘ç°ï¼Œä¸åŒçš„åç¼€åœ¨æ”»å‡»æ•ˆæœä¸Šå­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼ŒæŸäº›åç¼€å…·æœ‰æ›´å¼ºçš„é€šç”¨æ€§ï¼Œèƒ½å¤Ÿé€‚ç”¨äºæ›´å¤šæœªè§è¿‡çš„æœ‰å®³æŒ‡ä»¤ã€‚é€šè¿‡åˆ†æä¿¡æ¯æµåŠ¨ï¼Œæˆ‘ä»¬æ­ç¤ºäº†æ”»å‡»çš„å…³é”®æœºåˆ¶ï¼Œå¹¶æŒ‡å‡ºæ›´é€šç”¨çš„åç¼€èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åŠ«æŒä¸Šä¸‹æ–‡å¤„ç†è¿‡ç¨‹ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†å¢å¼ºå’Œç¼“è§£è¿™ç§æ”»å‡»çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨ä¸å¢åŠ è®¡ç®—æˆæœ¬çš„æƒ…å†µä¸‹æ˜¾è‘—æé«˜é˜²å¾¡æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.14629', 'title': 'VisText-Mosquito: A Multimodal Dataset and Benchmark for AI-Based\n  Mosquito Breeding Site Detection and Reasoning', 'url': 'https://huggingface.co/papers/2506.14629', 'abstract': 'VisText-Mosquito is a multimodal dataset combining visual and textual data for automated mosquito breeding site detection, segmentation, and reasoning, utilizing YOLOv9s, YOLOv11n-Seg, and a fine-tuned BLIP model.  \t\t\t\t\tAI-generated summary \t\t\t\t Mosquito-borne diseases pose a major global health risk, requiring early detection and proactive control of breeding sites to prevent outbreaks. In this paper, we present VisText-Mosquito, a multimodal dataset that integrates visual and textual data to support automated detection, segmentation, and reasoning for mosquito breeding site analysis. The dataset includes 1,828 annotated images for object detection, 142 images for water surface segmentation, and natural language reasoning texts linked to each image. The YOLOv9s model achieves the highest precision of 0.92926 and mAP@50 of 0.92891 for object detection, while YOLOv11n-Seg reaches a segmentation precision of 0.91587 and mAP@50 of 0.79795. For reasoning generation, our fine-tuned BLIP model achieves a final loss of 0.0028, with a BLEU score of 54.7, BERTScore of 0.91, and ROUGE-L of 0.87. This dataset and model framework emphasize the theme "Prevention is Better than Cure", showcasing how AI-based detection can proactively address mosquito-borne disease risks. The dataset and implementation code are publicly available at GitHub: https://github.com/adnanul-islam-jisun/VisText-Mosquito', 'score': 0, 'issue_id': 4358, 'pub_date': '2025-06-17', 'pub_date_card': {'ru': '17 Ğ¸ÑĞ½Ñ', 'en': 'June 17', 'zh': '6æœˆ17æ—¥'}, 'hash': '3308b767733e40b9', 'authors': ['Md. Adnanul Islam', 'Md. Faiyaz Abdullah Sayeedi', 'Md. Asaduzzaman Shuvo', 'Muhammad Ziaur Rahman', 'Shahanur Rahman Bappy', 'Raiyan Rahman', 'Swakkhar Shatabda'], 'affiliations': ['BRAC University, Bangladesh', 'United International University, Bangladesh', 'University of Portsmouth, United Kingdom'], 'pdf_title_img': 'assets/pdf/title_img/2506.14629.jpg', 'data': {'categories': ['#games', '#dataset', '#open_source', '#multimodal', '#healthcare', '#reasoning'], 'emoji': 'ğŸ¦Ÿ', 'ru': {'title': 'Ğ˜Ğ˜ Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ğ¶Ğµ Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²ÑŒÑ: Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ ÑƒĞ³Ñ€Ğ¾Ğ·Ñ‹ ĞºĞ¾Ğ¼Ğ°Ñ€Ğ¾Ğ²', 'desc': 'VisText-Mosquito - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ, ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¼ĞµÑÑ‚ Ñ€Ğ°Ğ·Ğ¼Ğ½Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ¼Ğ°Ñ€Ğ¾Ğ². Ğ’ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ÑÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ YOLOv9s Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², YOLOv11n-Seg Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ BLIP Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 1828 Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², 142 Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ñ‚ĞµĞºÑÑ‚Ñ‹ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ, ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»Ğ°ĞºÑ‚Ğ¸ĞºĞ¸ Ğ·Ğ°Ğ±Ğ¾Ğ»ĞµĞ²Ğ°Ğ½Ğ¸Ğ¹, Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ğ¼Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ°Ñ€Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Harnessing AI for Mosquito Control: Detect, Segment, Reason!', 'desc': 'The paper introduces VisText-Mosquito, a unique dataset that combines images and text to help identify and analyze mosquito breeding sites. It includes 1,828 annotated images for detecting objects and 142 images specifically for segmenting water surfaces, along with reasoning texts for each image. The study employs advanced models like YOLOv9s and YOLOv11n-Seg for detection and segmentation tasks, achieving high precision scores. Additionally, a fine-tuned BLIP model is used for generating natural language reasoning, demonstrating the effectiveness of multimodal approaches in combating mosquito-borne diseases.'}, 'zh': {'title': 'å¤šæ¨¡æ€æ•°æ®åŠ©åŠ›èšŠå­æ»‹ç”Ÿåœ°è‡ªåŠ¨æ£€æµ‹', 'desc': 'VisText-Mosquitoæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€æ•°æ®é›†ï¼Œç»“åˆäº†è§†è§‰å’Œæ–‡æœ¬æ•°æ®ï¼Œç”¨äºè‡ªåŠ¨åŒ–æ£€æµ‹å’Œåˆ†æèšŠå­æ»‹ç”Ÿåœ°ã€‚è¯¥æ•°æ®é›†åŒ…å«1828å¼ æ ‡æ³¨å›¾åƒç”¨äºç›®æ ‡æ£€æµ‹ï¼Œ142å¼ å›¾åƒç”¨äºæ°´é¢åˆ†å‰²ï¼Œä»¥åŠä¸æ¯å¼ å›¾åƒç›¸å…³çš„è‡ªç„¶è¯­è¨€æ¨ç†æ–‡æœ¬ã€‚ä½¿ç”¨YOLOv9sæ¨¡å‹è¿›è¡Œç›®æ ‡æ£€æµ‹æ—¶ï¼Œè¾¾åˆ°äº†æœ€é«˜çš„ç²¾åº¦0.92926ï¼Œè€ŒYOLOv11n-Segåœ¨åˆ†å‰²ä»»åŠ¡ä¸­è¾¾åˆ°äº†0.91587çš„ç²¾åº¦ã€‚é€šè¿‡å¾®è°ƒçš„BLIPæ¨¡å‹ï¼Œæˆ‘ä»¬åœ¨æ¨ç†ç”Ÿæˆæ–¹é¢å–å¾—äº†è‰¯å¥½çš„æ•ˆæœï¼Œå¼ºè°ƒäº†â€œé¢„é˜²èƒœäºæ²»ç–—â€çš„ä¸»é¢˜ï¼Œå±•ç¤ºäº†åŸºäºAIçš„æ£€æµ‹å¦‚ä½•ä¸»åŠ¨åº”å¯¹èšŠåª’ç–¾ç—…é£é™©ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.13922', 'title': 'DynaGuide: Steering Diffusion Polices with Active Dynamic Guidance', 'url': 'https://huggingface.co/papers/2506.13922', 'abstract': 'DynaGuide, a steering method using an external dynamics model, enhances diffusion policies by allowing them to adapt to multiple objectives and maintain robustness, outperforming goal-conditioning especially with low-quality objectives.  \t\t\t\t\tAI-generated summary \t\t\t\t Deploying large, complex policies in the real world requires the ability to steer them to fit the needs of a situation. Most common steering approaches, like goal-conditioning, require training the robot policy with a distribution of test-time objectives in mind. To overcome this limitation, we present DynaGuide, a steering method for diffusion policies using guidance from an external dynamics model during the diffusion denoising process. DynaGuide separates the dynamics model from the base policy, which gives it multiple advantages, including the ability to steer towards multiple objectives, enhance underrepresented base policy behaviors, and maintain robustness on low-quality objectives. The separate guidance signal also allows DynaGuide to work with off-the-shelf pretrained diffusion policies. We demonstrate the performance and features of DynaGuide against other steering approaches in a series of simulated and real experiments, showing an average steering success of 70% on a set of articulated CALVIN tasks and outperforming goal-conditioning by 5.4x when steered with low-quality objectives. We also successfully steer an off-the-shelf real robot policy to express preference for particular objects and even create novel behavior. Videos and more can be found on the project website: https://dynaguide.github.io', 'score': 0, 'issue_id': 4359, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 Ğ¸ÑĞ½Ñ', 'en': 'June 16', 'zh': '6æœˆ16æ—¥'}, 'hash': 'bc27538b9a430f57', 'authors': ['Maximilian Du', 'Shuran Song'], 'affiliations': ['Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2506.13922.jpg', 'data': {'categories': ['#agents', '#diffusion', '#optimization', '#training', '#robotics'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'DynaGuide: Ğ³Ğ¸Ğ±ĞºĞ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸', 'desc': 'DynaGuide - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ°Ğ¼Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸. ĞĞ½ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğº Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ñ†ĞµĞ»ÑĞ¼ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ†ĞµĞ»ÑĞ¼Ğ¸, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ»ÑÑ…. DynaGuide Ğ¾Ñ‚Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ¾Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°ĞµÑ‚ Ñ€ÑĞ´ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ» ÑÑ€ĞµĞ´Ğ½ÑÑ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚ÑŒ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ 70% Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡ CALVIN Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞµĞ» Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ†ĞµĞ»ÑĞ¼Ğ¸ Ğ² 5,4 Ñ€Ğ°Ğ·Ğ° Ğ¿Ñ€Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ñ Ğ½Ğ¸Ğ·ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ†ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'Steering Policies with DynaGuide: Adapting to Multiple Goals Robustly!', 'desc': 'DynaGuide is a novel steering method that enhances diffusion policies by utilizing an external dynamics model. This approach allows the policies to adapt to various objectives while ensuring robustness, particularly when dealing with low-quality objectives. Unlike traditional goal-conditioning methods, DynaGuide separates the dynamics model from the base policy, enabling it to steer towards multiple goals and improve underrepresented behaviors. The effectiveness of DynaGuide is demonstrated through experiments, achieving a 70% steering success rate and significantly outperforming existing methods.'}, 'zh': {'title': 'DynaGuideï¼šå¤šç›®æ ‡å¼•å¯¼çš„æ™ºèƒ½ç­–ç•¥', 'desc': 'DynaGuideæ˜¯ä¸€ç§ä½¿ç”¨å¤–éƒ¨åŠ¨æ€æ¨¡å‹çš„å¼•å¯¼æ–¹æ³•ï¼Œæ—¨åœ¨å¢å¼ºæ‰©æ•£ç­–ç•¥çš„é€‚åº”æ€§ã€‚å®ƒå…è®¸ç­–ç•¥é€‚åº”å¤šä¸ªç›®æ ‡ï¼Œå¹¶åœ¨ä½è´¨é‡ç›®æ ‡ä¸‹ä¿æŒé²æ£’æ€§ï¼Œè¡¨ç°ä¼˜äºä¼ ç»Ÿçš„ç›®æ ‡æ¡ä»¶æ–¹æ³•ã€‚é€šè¿‡å°†åŠ¨æ€æ¨¡å‹ä¸åŸºç¡€ç­–ç•¥åˆ†ç¦»ï¼ŒDynaGuideèƒ½å¤Ÿå¼•å¯¼ç­–ç•¥æœå‘å¤šä¸ªç›®æ ‡ï¼Œå¹¶å¢å¼ºåŸºç¡€ç­–ç•¥çš„è¡¨ç°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDynaGuideåœ¨å¤šç§ä»»åŠ¡ä¸­å–å¾—äº†70%çš„å¼•å¯¼æˆåŠŸç‡ï¼Œå°¤å…¶åœ¨ä½è´¨é‡ç›®æ ‡ä¸‹æ¯”ç›®æ ‡æ¡ä»¶æ–¹æ³•æé«˜äº†5.4å€çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.12015', 'title': 'EMLoC: Emulator-based Memory-efficient Fine-tuning with LoRA Correction', 'url': 'https://huggingface.co/papers/2506.12015', 'abstract': 'EMLoC, an memory-efficient fine-tuning framework using activation-aware SVD and LoRA, allows model adaptation within inference memory constraints for diverse applications.  \t\t\t\t\tAI-generated summary \t\t\t\t Open-source foundation models have seen rapid adoption and development, enabling powerful general-purpose capabilities across diverse domains. However, fine-tuning large foundation models for domain-specific or personalized tasks remains prohibitively expensive for most users due to the significant memory overhead beyond that of inference. We introduce EMLoC, an Emulator-based Memory-efficient fine-tuning framework with LoRA Correction, which enables model fine-tuning within the same memory budget required for inference. EMLoC constructs a task-specific light-weight emulator using activation-aware singular value decomposition (SVD) on a small downstream calibration set. Fine-tuning then is performed on this lightweight emulator via LoRA. To tackle the misalignment between the original model and the compressed emulator, we propose a novel compensation algorithm to correct the fine-tuned LoRA module, which thus can be merged into the original model for inference. EMLoC supports flexible compression ratios and standard training pipelines, making it adaptable to a wide range of applications. Extensive experiments demonstrate that EMLoC outperforms other baselines across multiple datasets and modalities. Moreover, without quantization, EMLoC enables fine-tuning of a 38B model on a single 24GB consumer GPU-bringing efficient and practical model adaptation to individual users.', 'score': 0, 'issue_id': 4356, 'pub_date': '2025-06-13', 'pub_date_card': {'ru': '13 Ğ¸ÑĞ½Ñ', 'en': 'June 13', 'zh': '6æœˆ13æ—¥'}, 'hash': 'd559dcf057099acf', 'authors': ['Hsi-Che Lin', 'Yu-Chu Yu', 'Kai-Po Chang', 'Yu-Chiang Frank Wang'], 'affiliations': ['NVIDIA', 'National Taiwan University'], 'pdf_title_img': 'assets/pdf/title_img/2506.12015.jpg', 'data': {'categories': ['#optimization', '#training', '#dataset', '#open_source', '#inference'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¢Ğ¾Ğ½ĞºĞ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ³Ğ¸Ğ³Ğ°Ğ½Ñ‚ÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğ¼ ĞŸĞš', 'desc': 'EMLoC - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ SVD Ğ¸ LoRA Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ğ¾Ğ³Ğ¾ ÑĞ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… Ñ‚ĞµÑ… Ğ¶Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğ¼ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ. EMLoC Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ…, Ğ´ĞµĞ»Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ñ Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Efficient Fine-Tuning for Everyone with EMLoC!', 'desc': 'EMLoC is a memory-efficient framework designed for fine-tuning large foundation models while adhering to inference memory limits. It utilizes activation-aware singular value decomposition (SVD) to create a lightweight emulator from a small calibration dataset, allowing for effective model adaptation. Fine-tuning is achieved through Low-Rank Adaptation (LoRA), which is then corrected with a novel compensation algorithm to ensure alignment with the original model. This approach enables users to fine-tune large models on standard consumer hardware, making advanced machine learning accessible and practical for diverse applications.'}, 'zh': {'title': 'EMLoCï¼šé«˜æ•ˆå†…å­˜å¾®è°ƒçš„æ–°é€‰æ‹©', 'desc': 'EMLoCæ˜¯ä¸€ç§å†…å­˜é«˜æ•ˆçš„å¾®è°ƒæ¡†æ¶ï¼Œåˆ©ç”¨æ¿€æ´»æ„ŸçŸ¥çš„å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰å’ŒLoRAæŠ€æœ¯ï¼Œä½¿å¾—åœ¨æ¨ç†å†…å­˜é™åˆ¶ä¸‹è¿›è¡Œæ¨¡å‹é€‚åº”æˆä¸ºå¯èƒ½ã€‚è¯¥æ¡†æ¶é€šè¿‡åœ¨å°å‹ä¸‹æ¸¸æ ¡å‡†é›†ä¸Šæ„å»ºä»»åŠ¡ç‰¹å®šçš„è½»é‡çº§æ¨¡æ‹Ÿå™¨ï¼Œæ¥å®ç°å¾®è°ƒã€‚ä¸ºäº†çº æ­£åŸå§‹æ¨¡å‹ä¸å‹ç¼©æ¨¡æ‹Ÿå™¨ä¹‹é—´çš„é”™ä½ï¼ŒEMLoCæå‡ºäº†ä¸€ç§æ–°é¢–çš„è¡¥å¿ç®—æ³•ï¼Œä½¿å¾—å¾®è°ƒåçš„LoRAæ¨¡å—å¯ä»¥åˆå¹¶å›åŸå§‹æ¨¡å‹ä¸­è¿›è¡Œæ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEMLoCåœ¨å¤šä¸ªæ•°æ®é›†å’Œæ¨¡æ€ä¸Šä¼˜äºå…¶ä»–åŸºçº¿ï¼Œä¸”æ— éœ€é‡åŒ–å³å¯åœ¨å•ä¸ª24GBçš„æ¶ˆè´¹çº§GPUä¸Šå¾®è°ƒ38Bæ¨¡å‹ï¼Œæå¤§åœ°æé«˜äº†æ¨¡å‹é€‚åº”çš„æ•ˆç‡å’Œå®ç”¨æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.03939', 'title': 'Graph Counselor: Adaptive Graph Exploration via Multi-Agent Synergy to Enhance LLM Reasoning', 'url': 'https://huggingface.co/papers/2506.03939', 'abstract': 'Graph Counselor enhances Large Language Models by using multi-agent collaboration and adaptive reasoning to integrate knowledge effectively, improving factual accuracy and generation quality in specialized domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Graph Retrieval Augmented Generation (GraphRAG) effectively enhances external knowledge integration capabilities by explicitly modeling knowledge relationships, thereby improving the factual accuracy and generation quality of Large Language Models (LLMs) in specialized domains. However, existing methods suffer from two inherent limitations: 1) Inefficient Information Aggregation: They rely on a single agent and fixed iterative patterns, making it difficult to adaptively capture multi-level textual, structural, and degree information within graph data. 2) Rigid Reasoning Mechanism: They employ preset reasoning schemes, which cannot dynamically adjust reasoning depth nor achieve precise semantic correction. To overcome these limitations, we propose Graph Counselor, an GraphRAG method based on multi-agent collaboration. This method uses the Adaptive Graph Information Extraction Module (AGIEM), where Planning, Thought, and Execution Agents work together to precisely model complex graph structures and dynamically adjust information extraction strategies, addressing the challenges of multi-level dependency modeling and adaptive reasoning depth. Additionally, the Self-Reflection with Multiple Perspectives (SR) module improves the accuracy and semantic consistency of reasoning results through self-reflection and backward reasoning mechanisms. Experiments demonstrate that Graph Counselor outperforms existing methods in multiple graph reasoning tasks, exhibiting higher reasoning accuracy and generalization ability. Our code is available at https://github.com/gjq100/Graph-Counselor.git.', 'score': 0, 'issue_id': 4362, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 Ğ¸ÑĞ½Ñ', 'en': 'June 4', 'zh': '6æœˆ4æ—¥'}, 'hash': '44afebcf0fc38495', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#reasoning', '#multimodal', '#agents', '#graphs', '#rag'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ñ‹Ğµ Ğ³Ñ€Ğ°Ñ„Ñ‹ Ğ´Ğ»Ñ ÑƒĞ¼Ğ½Ñ‹Ñ… Ğ¼Ğ°ÑˆĞ¸Ğ½: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜', 'desc': 'Graph Counselor - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ½ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ· Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑĞ°Ğ¼Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Graph Counselor Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ³Ñ€Ğ°Ñ„Ğ°Ñ….'}, 'en': {'title': 'Empowering LLMs with Adaptive Multi-Agent Collaboration', 'desc': "Graph Counselor is a novel approach that enhances Large Language Models (LLMs) by employing multi-agent collaboration and adaptive reasoning techniques. It addresses the limitations of existing methods in knowledge integration by utilizing an Adaptive Graph Information Extraction Module (AGIEM) that allows agents to work together to model complex graph structures. This method dynamically adjusts information extraction strategies, improving the model's ability to handle multi-level dependencies and reasoning depth. Additionally, the Self-Reflection with Multiple Perspectives (SR) module ensures higher accuracy and semantic consistency in reasoning results, leading to superior performance in graph reasoning tasks."}, 'zh': {'title': 'å›¾é¡¾é—®ï¼šæå‡è¯­è¨€æ¨¡å‹çš„çŸ¥è¯†æ•´åˆèƒ½åŠ›', 'desc': 'Graph Counselor æ˜¯ä¸€ç§åŸºäºå¤šæ™ºèƒ½ä½“åä½œçš„å›¾æ£€ç´¢å¢å¼ºç”Ÿæˆæ–¹æ³•ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸“ä¸šé¢†åŸŸçš„çŸ¥è¯†æ•´åˆèƒ½åŠ›ã€‚è¯¥æ–¹æ³•é€šè¿‡è‡ªé€‚åº”å›¾ä¿¡æ¯æå–æ¨¡å—ï¼ˆAGIEMï¼‰ï¼Œä½¿è§„åˆ’ã€æ€è€ƒå’Œæ‰§è¡Œæ™ºèƒ½ä½“ååŒå·¥ä½œï¼Œä»è€Œç²¾ç¡®å»ºæ¨¡å¤æ‚çš„å›¾ç»“æ„å¹¶åŠ¨æ€è°ƒæ•´ä¿¡æ¯æå–ç­–ç•¥ã€‚å®ƒè¿˜å¼•å…¥äº†å¤šè§†è§’è‡ªæˆ‘åæ€æ¨¡å—ï¼ˆSRï¼‰ï¼Œé€šè¿‡è‡ªæˆ‘åæ€å’Œé€†å‘æ¨ç†æœºåˆ¶æé«˜æ¨ç†ç»“æœçš„å‡†ç¡®æ€§å’Œè¯­ä¹‰ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGraph Counselor åœ¨å¤šä¸ªå›¾æ¨ç†ä»»åŠ¡ä¸­ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå±•ç°å‡ºæ›´é«˜çš„æ¨ç†å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d', '#agents (6)', '#agi (3)', '#alignment (2)', '#architecture (8)', '#audio (2)', '#benchmark (14)', '#cv (5)', '#data (4)', '#dataset (12)', '#diffusion (5)', '#ethics (1)', '#games (3)', '#graphs (1)', '#hallucinations (2)', '#healthcare (2)', '#inference (2)', '#interpretability (3)', '#leakage', '#long_context (3)', '#low_resource (2)', '#machine_translation', '#math (2)', '#multilingual (2)', '#multimodal (14)', '#open_source (10)', '#optimization (16)', '#plp', '#rag (1)', '#reasoning (14)', '#rl (8)', '#rlhf (3)', '#robotics (2)', '#science (1)', '#security (2)', '#small_models (1)', '#story_generation', '#survey (1)', '#synthetic (2)', '#training (16)', '#transfer_learning (3)', '#video (1)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-06-18 22:11',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-06-18 22:11')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-06-18 22:11')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    