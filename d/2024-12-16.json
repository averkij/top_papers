{
    "date": {
        "ru": "16 декабря",
        "en": "December 16",
        "zh": "12月16日"
    },
    "time_utc": "2024-12-16 04:13",
    "weekday": 0,
    "issue_id": 1136,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.09624",
            "title": "GenEx: Generating an Explorable World",
            "url": "https://huggingface.co/papers/2412.09624",
            "abstract": "Understanding, navigating, and exploring the 3D physical real world has long been a central challenge in the development of artificial intelligence. In this work, we take a step toward this goal by introducing GenEx, a system capable of planning complex embodied world exploration, guided by its generative imagination that forms priors (expectations) about the surrounding environments. GenEx generates an entire 3D-consistent imaginative environment from as little as a single RGB image, bringing it to life through panoramic video streams. Leveraging scalable 3D world data curated from Unreal Engine, our generative model is rounded in the physical world. It captures a continuous 360-degree environment with little effort, offering a boundless landscape for AI agents to explore and interact with. GenEx achieves high-quality world generation, robust loop consistency over long trajectories, and demonstrates strong 3D capabilities such as consistency and active 3D mapping. Powered by generative imagination of the world, GPT-assisted agents are equipped to perform complex embodied tasks, including both goal-agnostic exploration and goal-driven navigation. These agents utilize predictive expectation regarding unseen parts of the physical world to refine their beliefs, simulate different outcomes based on potential decisions, and make more informed choices. In summary, we demonstrate that GenEx provides a transformative platform for advancing embodied AI in imaginative spaces and brings potential for extending these capabilities to real-world exploration.",
            "score": 17,
            "issue_id": 1135,
            "pub_date": "2024-12-12",
            "pub_date_card": {
                "ru": "12 декабря",
                "en": "December 12",
                "zh": "12月12日"
            },
            "hash": "c4524ac73801b5cd",
            "authors": [
                "Taiming Lu",
                "Tianmin Shu",
                "Junfei Xiao",
                "Luoxin Ye",
                "Jiahao Wang",
                "Cheng Peng",
                "Chen Wei",
                "Daniel Khashabi",
                "Rama Chellappa",
                "Alan Yuille",
                "Jieneng Chen"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2412.09624.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#games",
                    "#agi",
                    "#3d"
                ],
                "emoji": "🌎",
                "ru": {
                    "title": "Генеративное воображение для исследования 3D-мира",
                    "desc": "GenEx - это система, способная планировать сложное исследование физического мира с помощью генеративного воображения. Она создает целостную трехмерную среду на основе всего одного RGB-изображения, генерируя панорамные видеопотоки. Модель обучена на данных из Unreal Engine и демонстрирует высокое качество генерации мира, согласованность при длительных траекториях и возможности активного 3D-картирования. Агенты на основе GPT используют GenEx для выполнения сложных задач в воображаемом пространстве, что открывает перспективы для исследования реального мира."
                },
                "en": {
                    "title": "GenEx: Empowering AI Exploration with Generative Imagination",
                    "desc": "This paper presents GenEx, a novel system designed to enhance artificial intelligence's ability to explore and navigate 3D environments. GenEx utilizes generative imagination to create realistic 3D environments from a single RGB image, enabling AI agents to visualize and interact with these spaces through panoramic video. The system is built on extensive 3D data from Unreal Engine, ensuring high-quality world generation and robust consistency during exploration. By leveraging predictive expectations, GPT-assisted agents can perform complex tasks, improving their decision-making in both exploratory and navigational contexts."
                },
                "zh": {
                    "title": "GenEx：开启AI探索3D世界的新篇章",
                    "desc": "本研究介绍了GenEx系统，它能够通过生成想象来规划复杂的3D世界探索。GenEx从单张RGB图像生成一致的3D环境，并通过全景视频流将其呈现出来。该系统利用来自虚幻引擎的可扩展3D世界数据，捕捉360度的环境，为AI代理提供了广阔的探索空间。GenEx展示了高质量的世界生成和强大的3D能力，使得AI代理能够执行复杂的任务，包括无目标探索和目标驱动导航。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.10047",
            "title": "Large Action Models: From Inception to Implementation",
            "url": "https://huggingface.co/papers/2412.10047",
            "abstract": "As AI continues to advance, there is a growing demand for systems that go beyond language-based assistance and move toward intelligent agents capable of performing real-world actions. This evolution requires the transition from traditional Large Language Models (LLMs), which excel at generating textual responses, to Large Action Models (LAMs), designed for action generation and execution within dynamic environments. Enabled by agent systems, LAMs hold the potential to transform AI from passive language understanding to active task completion, marking a significant milestone in the progression toward artificial general intelligence.   In this paper, we present a comprehensive framework for developing LAMs, offering a systematic approach to their creation, from inception to deployment. We begin with an overview of LAMs, highlighting their unique characteristics and delineating their differences from LLMs. Using a Windows OS-based agent as a case study, we provide a detailed, step-by-step guide on the key stages of LAM development, including data collection, model training, environment integration, grounding, and evaluation. This generalizable workflow can serve as a blueprint for creating functional LAMs in various application domains. We conclude by identifying the current limitations of LAMs and discussing directions for future research and industrial deployment, emphasizing the challenges and opportunities that lie ahead in realizing the full potential of LAMs in real-world applications.   The code for the data collection process utilized in this paper is publicly available at: https://github.com/microsoft/UFO/tree/main/dataflow, and comprehensive documentation can be found at https://microsoft.github.io/UFO/dataflow/overview/.",
            "score": 6,
            "issue_id": 1135,
            "pub_date": "2024-12-13",
            "pub_date_card": {
                "ru": "13 декабря",
                "en": "December 13",
                "zh": "12月13日"
            },
            "hash": "be65080464153291",
            "authors": [
                "Lu Wang",
                "Fangkai Yang",
                "Chaoyun Zhang",
                "Junting Lu",
                "Jiaxu Qian",
                "Shilin He",
                "Pu Zhao",
                "Bo Qiao",
                "Ray Huang",
                "Si Qin",
                "Qisheng Su",
                "Jiayi Ye",
                "Yudi Zhang",
                "Jian-Guang Lou",
                "Qingwei Lin",
                "Saravan Rajmohan",
                "Dongmei Zhang",
                "Qi Zhang"
            ],
            "affiliations": [
                "Eindhoven University of Technology",
                "Microsoft",
                "Peking University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.10047.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#data",
                    "#training",
                    "#open_source",
                    "#agi"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "От слов к делу: новый этап в развитии искусственного интеллекта",
                    "desc": "Статья представляет концепцию Крупномасштабных Моделей Действий (LAM), которые призваны перевести ИИ от пассивного понимания языка к активному выполнению задач. Авторы предлагают комплексную структуру для разработки LAM, включая сбор данных, обучение модели и интеграцию с окружением. На примере агента для Windows OS демонстрируется процесс создания LAM. Статья также обсуждает текущие ограничения LAM и направления будущих исследований."
                },
                "en": {
                    "title": "From Language to Action: Advancing AI with Large Action Models",
                    "desc": "This paper discusses the shift from Large Language Models (LLMs) to Large Action Models (LAMs), which are designed to perform actions in real-world environments rather than just generating text. LAMs are enabled by agent systems and represent a step towards achieving artificial general intelligence by allowing AI to complete tasks actively. The authors present a framework for developing LAMs, detailing the stages from data collection to model training and evaluation, using a Windows OS-based agent as an example. They also address the limitations of current LAMs and suggest future research directions to enhance their capabilities in practical applications."
                },
                "zh": {
                    "title": "从语言理解到行动执行的智能转型",
                    "desc": "随着人工智能的不断进步，市场对能够执行实际操作的智能代理系统的需求日益增加。本文提出了一种大型行动模型（LAMs）的综合框架，旨在从传统的大型语言模型（LLMs）转变为能够在动态环境中生成和执行行动的模型。我们通过Windows操作系统的代理作为案例，详细介绍了LAM开发的关键阶段，包括数据收集、模型训练、环境集成和评估。最后，我们讨论了LAMs的当前局限性以及未来研究和工业应用的方向，强调了实现LAMs在实际应用中潜力的挑战与机遇。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.09626",
            "title": "FreeScale: Unleashing the Resolution of Diffusion Models via Tuning-Free Scale Fusion",
            "url": "https://huggingface.co/papers/2412.09626",
            "abstract": "Visual diffusion models achieve remarkable progress, yet they are typically trained at limited resolutions due to the lack of high-resolution data and constrained computation resources, hampering their ability to generate high-fidelity images or videos at higher resolutions. Recent efforts have explored tuning-free strategies to exhibit the untapped potential higher-resolution visual generation of pre-trained models. However, these methods are still prone to producing low-quality visual content with repetitive patterns. The key obstacle lies in the inevitable increase in high-frequency information when the model generates visual content exceeding its training resolution, leading to undesirable repetitive patterns deriving from the accumulated errors. To tackle this challenge, we propose FreeScale, a tuning-free inference paradigm to enable higher-resolution visual generation via scale fusion. Specifically, FreeScale processes information from different receptive scales and then fuses it by extracting desired frequency components. Extensive experiments validate the superiority of our paradigm in extending the capabilities of higher-resolution visual generation for both image and video models. Notably, compared with the previous best-performing method, FreeScale unlocks the generation of 8k-resolution images for the first time.",
            "score": 3,
            "issue_id": 1135,
            "pub_date": "2024-12-12",
            "pub_date_card": {
                "ru": "12 декабря",
                "en": "December 12",
                "zh": "12月12日"
            },
            "hash": "1551e9966255aa0a",
            "authors": [
                "Haonan Qiu",
                "Shiwei Zhang",
                "Yujie Wei",
                "Ruihang Chu",
                "Hangjie Yuan",
                "Xiang Wang",
                "Yingya Zhang",
                "Ziwei Liu"
            ],
            "affiliations": [
                "Alibaba Group",
                "Fudan University",
                "Nanyang Technological University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.09626.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#inference",
                    "#diffusion",
                    "#video",
                    "#cv"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "FreeScale: Прорыв в генерации визуального контента сверхвысокого разрешения",
                    "desc": "Статья представляет FreeScale - новый метод генерации высококачественных изображений и видео высокого разрешения без дополнительного обучения моделей. Авторы решают проблему появления повторяющихся паттернов при увеличении разрешения, используя слияние информации с разных масштабов и извлечение нужных частотных компонентов. Эксперименты показывают превосходство FreeScale над существующими методами для моделей изображений и видео. Метод позволяет впервые генерировать изображения с разрешением 8K."
                },
                "en": {
                    "title": "Unlocking High-Resolution Visuals with FreeScale",
                    "desc": "This paper introduces FreeScale, a new method for generating high-resolution images and videos using visual diffusion models. Traditional models struggle with high-resolution outputs due to limited training data and computational resources, often resulting in low-quality visuals with repetitive patterns. FreeScale addresses this by employing a tuning-free inference approach that fuses information from various scales, allowing the model to better handle high-frequency details. Experimental results demonstrate that FreeScale significantly enhances the quality of generated visuals, achieving 8k-resolution outputs for the first time."
                },
                "zh": {
                    "title": "FreeScale：无调优的高分辨率视觉生成新范式",
                    "desc": "视觉扩散模型在生成高保真图像或视频时面临分辨率限制的问题，主要是由于缺乏高分辨率数据和计算资源。最近的研究尝试了无调优策略，以展示预训练模型在高分辨率视觉生成方面的潜力，但仍然容易产生低质量的视觉内容和重复模式。我们提出了FreeScale，这是一种无调优推理范式，通过尺度融合实现更高分辨率的视觉生成。实验结果表明，FreeScale在图像和视频模型的高分辨率生成能力上优于以往的方法，首次实现了8k分辨率图像的生成。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.10319",
            "title": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods",
            "url": "https://huggingface.co/papers/2412.10319",
            "abstract": "Long-context LLMs have enabled numerous downstream applications but also introduced significant challenges related to computational and memory efficiency. To address these challenges, optimizations for long-context inference have been developed, centered around the KV cache. However, existing benchmarks often evaluate in single-request, neglecting the full lifecycle of the KV cache in real-world use. This oversight is particularly critical, as KV cache reuse has become widely adopted in LLMs inference frameworks, such as vLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft, Google, and Anthropic. To address this gap, we introduce SCBench(SharedContextBench), a comprehensive benchmark for evaluating long-context methods from a KV cachecentric perspective: 1) KV cache generation, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache loading. Specifically, SCBench uses test examples with shared context, ranging 12 tasks with two shared context modes, covering four categories of long-context capabilities: string retrieval, semantic retrieval, global information, and multi-task. With it, we provide an extensive KV cache-centric analysis of eight categories long-context solutions, including Gated Linear RNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention, KV cache dropping, quantization, retrieval, loading, and prompt compression. The evaluation is conducted on 8 long-context LLMs. Our findings show that sub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding with O(n) memory and sub-O(n^2) pre-filling computation perform robustly. Dynamic sparsity yields more expressive KV caches than static patterns, and layer-level sparsity in hybrid architectures reduces memory usage with strong performance. Additionally, we identify attention distribution shift issues in long-generation scenarios. https://aka.ms/SCBench.",
            "score": 1,
            "issue_id": 1135,
            "pub_date": "2024-12-13",
            "pub_date_card": {
                "ru": "13 декабря",
                "en": "December 13",
                "zh": "12月13日"
            },
            "hash": "a6269882457435d4",
            "authors": [
                "Yucheng Li",
                "Huiqiang Jiang",
                "Qianhui Wu",
                "Xufang Luo",
                "Surin Ahn",
                "Chengruidong Zhang",
                "Amir H. Abdi",
                "Dongsheng Li",
                "Jianfeng Gao",
                "Yuqing Yang",
                "Lili Qiu"
            ],
            "affiliations": [
                "Microsoft Corporation",
                "University of Surrey"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.10319.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#long_context",
                    "#inference",
                    "#benchmark",
                    "#architecture"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "SCBench: новый взгляд на оценку длинноконтекстных языковых моделей через призму KV-кэша",
                    "desc": "Статья представляет новый бенчмарк SCBench для оценки методов работы с длинным контекстом в языковых моделях с акцентом на KV-кэш. SCBench оценивает генерацию, сжатие, извлечение и загрузку KV-кэша на 12 задачах с общим контекстом. Авторы проанализировали 8 категорий решений для длинного контекста на 8 языковых моделях. Результаты показывают, что методы с субквадратичной памятью уступают в многоходовых сценариях, а динамическое прореживание дает более выразительные KV-кэши."
                },
                "en": {
                    "title": "Optimizing Long-Context LLMs with SCBench: A KV Cache Revolution",
                    "desc": "This paper introduces SCBench, a new benchmark designed to evaluate long-context methods in large language models (LLMs) with a focus on the key-value (KV) cache. It addresses the limitations of existing benchmarks by considering the entire lifecycle of the KV cache, including its generation, compression, retrieval, and loading. The study analyzes various long-context solutions, revealing that memory-efficient methods can struggle in multi-turn scenarios, while certain sparse encoding techniques perform well. The findings also highlight the importance of dynamic sparsity and layer-level sparsity in optimizing memory usage and performance in LLMs."
                },
                "zh": {
                    "title": "优化长上下文的KV缓存评估",
                    "desc": "本文介绍了SCBench（SharedContextBench），一个针对长上下文方法的基准测试，重点关注KV缓存的生命周期。研究表明，现有的基准测试往往只关注单次请求，而忽视了KV缓存的重用，这在实际应用中至关重要。SCBench涵盖了KV缓存的生成、压缩、检索和加载等四个方面，并通过12个任务的共享上下文进行评估。我们的研究发现，动态稀疏性在KV缓存中表现更好，而混合架构中的层级稀疏性则有效降低了内存使用，同时保持了强大的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.07517",
            "title": "FireFlow: Fast Inversion of Rectified Flow for Image Semantic Editing",
            "url": "https://huggingface.co/papers/2412.07517",
            "abstract": "Though Rectified Flows (ReFlows) with distillation offers a promising way for fast sampling, its fast inversion transforms images back to structured noise for recovery and following editing remains unsolved. This paper introduces FireFlow, a simple yet effective zero-shot approach that inherits the startling capacity of ReFlow-based models (such as FLUX) in generation while extending its capabilities to accurate inversion and editing in 8 steps. We first demonstrate that a carefully designed numerical solver is pivotal for ReFlow inversion, enabling accurate inversion and reconstruction with the precision of a second-order solver while maintaining the practical efficiency of a first-order Euler method. This solver achieves a 3times runtime speedup compared to state-of-the-art ReFlow inversion and editing techniques, while delivering smaller reconstruction errors and superior editing results in a training-free mode. The code is available at https://github.com/HolmesShuan/FireFlow{this URL}.",
            "score": 1,
            "issue_id": 1135,
            "pub_date": "2024-12-10",
            "pub_date_card": {
                "ru": "10 декабря",
                "en": "December 10",
                "zh": "12月10日"
            },
            "hash": "70f97a4533ea4ebb",
            "authors": [
                "Yingying Deng",
                "Xiangyu He",
                "Changwang Mei",
                "Peisong Wang",
                "Fan Tang"
            ],
            "affiliations": [
                "Institute of Automation, Chinese Academy of Sciences, Beijing, China",
                "Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.07517.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#open_source",
                    "#cv",
                    "#architecture"
                ],
                "emoji": "🔥",
                "ru": {
                    "title": "FireFlow: Молниеносная инверсия и редактирование изображений в ReFlow моделях",
                    "desc": "Статья представляет FireFlow - новый подход к быстрой инверсии и редактированию изображений в моделях на основе Rectified Flows (ReFlows). Авторы разработали специальный численный решатель, который позволяет точно инвертировать и реконструировать изображения за 8 шагов. Метод FireFlow работает в 3 раза быстрее существующих техник инверсии ReFlow, обеспечивая при этом меньшие ошибки реконструкции и лучшие результаты редактирования. Важно отметить, что FireFlow не требует дополнительного обучения и может применяться к уже обученным моделям."
                },
                "en": {
                    "title": "FireFlow: Fast and Accurate Image Inversion and Editing",
                    "desc": "This paper presents FireFlow, a new method that improves the inversion and editing capabilities of Rectified Flows (ReFlows) in machine learning. FireFlow uses a specially designed numerical solver that combines the efficiency of first-order methods with the accuracy of second-order methods, allowing for faster and more precise image reconstruction. The approach achieves a threefold increase in speed compared to existing ReFlow techniques while reducing errors and enhancing editing quality without requiring additional training. Overall, FireFlow enhances the usability of ReFlow models for generating and manipulating images effectively."
                },
                "zh": {
                    "title": "FireFlow：高效的图像反演与编辑新方法",
                    "desc": "本文介绍了一种名为FireFlow的新方法，它在快速采样的基础上，解决了图像反演和编辑的问题。FireFlow继承了基于ReFlow模型的强大生成能力，并在8个步骤内实现了准确的反演和编辑。我们设计了一种数值求解器，使得ReFlow的反演过程更加精确，同时保持了高效性。与现有的ReFlow反演和编辑技术相比，该求解器在运行速度上提高了3倍，并且在训练无关的模式下，重建误差更小，编辑效果更佳。"
                }
            }
        }
    ],
    "link_prev": "2024-12-13.html",
    "link_next": "2024-12-17.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "13.12",
        "en": "12/13",
        "zh": "12月13日"
    },
    "short_date_next": {
        "ru": "17.12",
        "en": "12/17",
        "zh": "12月17日"
    },
    "categories": {
        "#dataset": 0,
        "#data": 1,
        "#benchmark": 1,
        "#agents": 2,
        "#cv": 2,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 1,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 2,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章讨论了创建能够长时间与环境互动的人工智能系统的研究目标。近期，多模态大型语言模型（MLLMs）在开放世界理解方面取得了显著进展。然而，连续和同时进行的感知、记忆和推理仍然是一个未探索的挑战。当前的MLLMs受限于其序列到序列的架构，无法同时处理输入和生成响应。因此，这个项目引入了分离的感知、推理和记忆机制，使得模型能够实时处理视频和音频输入。提出的框架IXC2.5-OL包括三个关键模块：实时处理多模态信息的感知模块，整合短期和长期记忆的多模态长期记忆模块，以及响应查询和执行推理任务的推理模块。这个项目模拟了人类认知，使多模态大型语言模型能够提供持续和适应性的服务。",
        "title": "InternLM-XComposer2.5-OmniLive: A Comprehensive Multimodal System for Long-term Streaming Video and Audio Interactions",
        "pinyin": "Zhè piān wénzhāng tǎolùnle chuàngjiàn nénggòu cháng shíjiān yǔ huánjìng hùdòng de réngōng zhìnéng xìtǒng de yánjiū mùbiāo. Jìnqī, duō móshuǎi dàxíng yǔyán móxíng (MLLMs) zài kāifàng shìjiè lǐjiě fāngmiàn quèdéle xiǎnzhù jìnbù. Rán'ér, liánxù hé tóngshí jìnxíng de gǎnjué, jìyì hé tuíli shì yīgè wèi tànsuǒ de tiáozhàn. Dāngqián de MLLMs shòuxiàn yú qí xùliè dào xùliè de jiàgòu, wúfǎ tóngshí chǔlǐ shūrù hé shēngchéng xiǎngyìng. Yīncǐ, zhègè xiàngmù yǐnrùle fēnliè de gǎnjué, tuíli hé jìyì jīzhì, shǐdé móxíng nénggòu shíshí chǔlǐ shìpǐn hé yīnpǐn shūrù. Tíchū de kuàngjià IXC2.5-OL bāokuò sān gè guǎnjiàn módù: shíshí chǔlǐ duō móshuǎi xìnxī de gǎnjué módù, zhěnghé duǎnqī hé chángqī jìyì de duō móshuǎi chángqī jìyì módù, yǐjiǎ xiǎngyìng cháxún hé zhíxíng tuíli rènwù de tuíli módù. Zhègè xiàngmù mónǐle rénlèi rèngōng, shǐ duō móshuǎi dàxíng yǔyán móxíng nénggòu tígōng chíxù hé shìyìngxìng de fúwù.",
        "vocab": "[\n    {\"word\": \"讨论\", \"pinyin\": \"tǎo lùn\", \"trans\": \"discuss\"},\n    {\"word\": \"创建\", \"pinyin\": \"chuàng jiàn\", \"trans\": \"create\"},\n    {\"word\": \"能够\", \"pinyin\": \"néng gòu\", \"trans\": \"be able to\"},\n    {\"word\": \"长时间\", \"pinyin\": \"cháng shí jiān\", \"trans\": \"long period of time\"},\n    {\"word\": \"互动\", \"pinyin\": \"hù dòng\", \"trans\": \"interact\"},\n    {\"word\": \"人工智能\", \"pinyin\": \"rén gōng zhì néng\", \"trans\": \"artificial intelligence\"},\n    {\"word\": \"系统\", \"pinyin\": \"xì tǒng\", \"trans\": \"system\"},\n    {\"word\": \"研究\", \"pinyin\": \"yán jiū\", \"trans\": \"research\"},\n    {\"word\": \"目标\", \"pinyin\": \"mù biāo\", \"trans\": \"goal\"},\n    {\"word\": \"多模态\", \"pinyin\": \"duō mó shuài\", \"trans\": \"multimodal\"},\n    {\"word\": \"大型\", \"pinyin\": \"dà xíng\", \"trans\": \"large-scale\"},\n    {\"word\": \"语言\", \"pinyin\": \"yǔ yán\", \"trans\": \"language\"},\n    {\"word\": \"模型\", \"pinyin\": \"mó xíng\", \"trans\": \"model\"},\n    {\"word\": \"开放\", \"pinyin\": \"kāi fàng\", \"trans\": \"open\"},\n    {\"word\": \"世界\", \"pinyin\": \"shì jiè\", \"trans\": \"world\"},\n    {\"word\": \"理解\", \"pinyin\": \"lǐ jiě\", \"trans\": \"understand\"},\n    {\"word\": \"方面\", \"pinyin\": \"fāng miàn\", \"trans\": \"aspect\"},\n    {\"word\": \"取得\", \"pinyin\": \"qǔ dé\", \"trans\": \"achieve\"},\n    {\"word\": \"显著\", \"pinyin\": \"xiǎn zhù\", \"trans\": \"significant\"},\n    {\"word\": \"进展\", \"pinyin\": \"jìn zhǎn\", \"trans\": \"progress\"},\n    {\"word\": \"然而\", \"pinyin\": \"rán ér\", \"trans\": \"however\"},\n    {\"word\": \"连续\", \"pinyin\": \"lián xù\", \"trans\": \"continuous\"},\n    {\"word\": \"同时\", \"pinyin\": \"tóng shí\", \"trans\": \"simultaneous\"},\n    {\"word\": \"感知\", \"pinyin\": \"gǎn zhī\", \"trans\": \"perception\"},\n    {\"word\": \"记忆\", \"pinyin\": \"jì yì\", \"trans\": \"memory\"},\n    {\"word\": \"推理\", \"pinyin\": \"tuī lǐ\", \"trans\": \"reasoning\"},\n    {\"word\": \"挑战\", \"pinyin\": \"tiǎo zhàn\", \"trans\": \"challenge\"},\n    {\"word\": \"未\", \"pinyin\": \"wèi\", \"trans\": \"not yet\"},\n    {\"word\": \"探索\", \"pinyin\": \"tàn suǒ\", \"trans\": \"explore\"},\n    {\"word\": \"受限于\", \"pinyin\": \"shòu xiàn yú\", \"trans\": \"be limited to\"},\n    {\"word\": \"其\", \"pinyin\": \"qí\", \"trans\": \"its\"},\n    {\"word\": \"序列\", \"pinyin\": \"xù liè\", \"trans\": \"sequence\"},\n    {\"word\": \"架构\", \"pinyin\": \"jià gòu\", \"trans\": \"architecture\"},\n    {\"word\": \"无法\", \"pinyin\": \"wú fǎ\", \"trans\": \"unable to\"},\n    {\"word\": \"处理\", \"pinyin\": \"chǔ lǐ\", \"trans\": \"process\"},\n    {\"word\": \"输入\", \"pinyin\": \"shū rù\", \"trans\": \"input\"},\n    {\"word\": \"生成\", \"pinyin\": \"shēng chéng\", \"trans\": \"generate\"},\n    {\"word\": \"响应\", \"pinyin\": \"xiǎng yìng\", \"trans\": \"response\"},\n    {\"word\": \"引入\", \"pinyin\": \"yǐn rù\", \"trans\": \"introduce\"},\n    {\"word\": \"分离\", \"pinyin\": \"fēn lí\", \"trans\": \"separate\"},\n    {\"word\": \"机制\", \"pinyin\": \"jī zhì\", \"trans\": \"mechanism\"},\n    {\"word\": \"实时\", \"pinyin\": \"shí shí\", \"trans\": \"real-time\"},\n    {\"word\": \"视频\", \"pinyin\": \"shì pín\", \"trans\": \"video\"},\n    {\"word\": \"音频\", \"pinyin\": \"yīn pín\", \"trans\": \"audio\"},\n    {\"word\": \"提出\", \"pinyin\": \"tí chū\", \"trans\": \"propose\"},\n    {\"word\": \"框架\", \"pinyin\": \"kuàng jià\", \"trans\": \"framework\"},\n    {\"word\": \"IXC2.5-OL\", \"pinyin\": \"IXC2.5-OL\", \"trans\": \"IXC2.5-OL\"},\n    {\"word\": \"包括\", \"pinyin\": \"bāo kuò\", \"trans\": \"include\"},\n    {\"word\": \"关键\", \"pinyin\": \"guǎn jiàn\", \"trans\": \"key\"},\n    {\"word\": \"模块\", \"pinyin\": \"mó kuài\", \"trans\": \"module\"},\n    {\"word\": \"整合\", \"pinyin\": \"zhěng hé\", \"trans\": \"integrate\"},\n    {\"word\": \"短期\", \"pinyin\": \"duǎn qī\", \"trans\": \"short-term\"},\n    {\"word\": \"长期\", \"pinyin\": \"cháng qī\", \"trans\": \"long-term\"},\n    {\"word\": \"多模态长期记忆\", \"pinyin\": \"duō mó shuài cháng qī jì yì\", \"trans\": \"multimodal long-term memory\"},\n    {\"word\": \"响应查询\", \"pinyin\": \"xiǎng yìng chá xún\", \"trans\": \"respond to queries\"},\n    {\"word\": \"执行\", \"pinyin\": \"zhí xíng\", \"trans\": \"execute\"},\n    {\"word\": \"任务\", \"pinyin\": \"rèn wù\", \"trans\": \"task\"},\n    {\"word\": \"项目\", \"pinyin\": \"xiàng mù\", \"trans\": \"project\"},\n    {\"word\": \"模拟\", \"pinyin\": \"mó nǐ\", \"trans\": \"simulate\"},\n    {\"word\": \"人类\", \"pinyin\": \"rén lèi\", \"trans\": \"human\"},\n    {\"word\": \"认知\", \"pinyin\": \"rèn zhī\", \"trans\": \"cognition\"},\n    {\"word\": \"使\", \"pinyin\": \"shǐ\", \"trans\": \"make\"},\n    {\"word\": \"提供\", \"pinyin\": \"tí gōng\", \"trans\": \"provide\"},\n    {\"word\": \"持续\", \"pinyin\": \"chí xù\", \"trans\": \"continuous\"},\n    {\"word\": \"适应性\", \"pinyin\": \"shì yìng xìng\", \"trans\": \"adaptability\"},\n    {\"word\": \"服务\", \"pinyin\": \"fú wù\", \"trans\": \"service\"}\n]",
        "trans": "This article discusses the research goal of creating artificial intelligence systems capable of long-term interaction with their environment. Recently, multimodal large language models (MLLMs) have made significant progress in understanding the open world. However, continuous and simultaneous perception, memory, and reasoning remain unexplored challenges. Current MLLMs are limited by their sequence-to-sequence architecture, which prevents them from processing inputs and generating responses simultaneously. Therefore, this project introduces separate mechanisms for perception, reasoning, and memory, allowing the model to process video and audio inputs in real-time. The proposed framework, IXC2.5-OL, includes three key modules: a perception module for real-time processing of multimodal information, a multimodal long-term memory module that integrates short-term and long-term memory, and a reasoning module for responding to queries and performing reasoning tasks. This project simulates human cognition, enabling multimodal large language models to provide continuous and adaptive services.",
        "update_ts": "2024-12-15 12:42"
    }
}