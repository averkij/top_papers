{
    "date": {
        "ru": "16 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
        "en": "December 16",
        "zh": "12æœˆ16æ—¥"
    },
    "time_utc": "2024-12-16 04:13",
    "weekday": 0,
    "issue_id": 1136,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.09624",
            "title": "GenEx: Generating an Explorable World",
            "url": "https://huggingface.co/papers/2412.09624",
            "abstract": "Understanding, navigating, and exploring the 3D physical real world has long been a central challenge in the development of artificial intelligence. In this work, we take a step toward this goal by introducing GenEx, a system capable of planning complex embodied world exploration, guided by its generative imagination that forms priors (expectations) about the surrounding environments. GenEx generates an entire 3D-consistent imaginative environment from as little as a single RGB image, bringing it to life through panoramic video streams. Leveraging scalable 3D world data curated from Unreal Engine, our generative model is rounded in the physical world. It captures a continuous 360-degree environment with little effort, offering a boundless landscape for AI agents to explore and interact with. GenEx achieves high-quality world generation, robust loop consistency over long trajectories, and demonstrates strong 3D capabilities such as consistency and active 3D mapping. Powered by generative imagination of the world, GPT-assisted agents are equipped to perform complex embodied tasks, including both goal-agnostic exploration and goal-driven navigation. These agents utilize predictive expectation regarding unseen parts of the physical world to refine their beliefs, simulate different outcomes based on potential decisions, and make more informed choices. In summary, we demonstrate that GenEx provides a transformative platform for advancing embodied AI in imaginative spaces and brings potential for extending these capabilities to real-world exploration.",
            "score": 17,
            "issue_id": 1135,
            "pub_date": "2024-12-12",
            "pub_date_card": {
                "ru": "12 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 12",
                "zh": "12æœˆ12æ—¥"
            },
            "hash": "c4524ac73801b5cd",
            "authors": [
                "Taiming Lu",
                "Tianmin Shu",
                "Junfei Xiao",
                "Luoxin Ye",
                "Jiahao Wang",
                "Cheng Peng",
                "Chen Wei",
                "Daniel Khashabi",
                "Rama Chellappa",
                "Alan Yuille",
                "Jieneng Chen"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2412.09624.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#games",
                    "#agi",
                    "#3d"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ 3D-Ğ¼Ğ¸Ñ€Ğ°",
                    "desc": "GenEx - ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ° ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½ÑƒÑ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½ÑƒÑ ÑÑ€ĞµĞ´Ñƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²ÑĞµĞ³Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ RGB-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Unreal Engine Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¸Ñ€Ğ°, ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑÑ… Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ 3D-ĞºĞ°Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ³ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ GPT Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ GenEx Ğ´Ğ»Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ğ²Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ¶Ğ°ĞµĞ¼Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹ Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°."
                },
                "en": {
                    "title": "GenEx: Empowering AI Exploration with Generative Imagination",
                    "desc": "This paper presents GenEx, a novel system designed to enhance artificial intelligence's ability to explore and navigate 3D environments. GenEx utilizes generative imagination to create realistic 3D environments from a single RGB image, enabling AI agents to visualize and interact with these spaces through panoramic video. The system is built on extensive 3D data from Unreal Engine, ensuring high-quality world generation and robust consistency during exploration. By leveraging predictive expectations, GPT-assisted agents can perform complex tasks, improving their decision-making in both exploratory and navigational contexts."
                },
                "zh": {
                    "title": "GenExï¼šå¼€å¯AIæ¢ç´¢3Dä¸–ç•Œçš„æ–°ç¯‡ç« ",
                    "desc": "æœ¬ç ”ç©¶ä»‹ç»äº†GenExç³»ç»Ÿï¼Œå®ƒèƒ½å¤Ÿé€šè¿‡ç”Ÿæˆæƒ³è±¡æ¥è§„åˆ’å¤æ‚çš„3Dä¸–ç•Œæ¢ç´¢ã€‚GenExä»å•å¼ RGBå›¾åƒç”Ÿæˆä¸€è‡´çš„3Dç¯å¢ƒï¼Œå¹¶é€šè¿‡å…¨æ™¯è§†é¢‘æµå°†å…¶å‘ˆç°å‡ºæ¥ã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨æ¥è‡ªè™šå¹»å¼•æ“çš„å¯æ‰©å±•3Dä¸–ç•Œæ•°æ®ï¼Œæ•æ‰360åº¦çš„ç¯å¢ƒï¼Œä¸ºAIä»£ç†æä¾›äº†å¹¿é˜”çš„æ¢ç´¢ç©ºé—´ã€‚GenExå±•ç¤ºäº†é«˜è´¨é‡çš„ä¸–ç•Œç”Ÿæˆå’Œå¼ºå¤§çš„3Dèƒ½åŠ›ï¼Œä½¿å¾—AIä»£ç†èƒ½å¤Ÿæ‰§è¡Œå¤æ‚çš„ä»»åŠ¡ï¼ŒåŒ…æ‹¬æ— ç›®æ ‡æ¢ç´¢å’Œç›®æ ‡é©±åŠ¨å¯¼èˆªã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.10047",
            "title": "Large Action Models: From Inception to Implementation",
            "url": "https://huggingface.co/papers/2412.10047",
            "abstract": "As AI continues to advance, there is a growing demand for systems that go beyond language-based assistance and move toward intelligent agents capable of performing real-world actions. This evolution requires the transition from traditional Large Language Models (LLMs), which excel at generating textual responses, to Large Action Models (LAMs), designed for action generation and execution within dynamic environments. Enabled by agent systems, LAMs hold the potential to transform AI from passive language understanding to active task completion, marking a significant milestone in the progression toward artificial general intelligence.   In this paper, we present a comprehensive framework for developing LAMs, offering a systematic approach to their creation, from inception to deployment. We begin with an overview of LAMs, highlighting their unique characteristics and delineating their differences from LLMs. Using a Windows OS-based agent as a case study, we provide a detailed, step-by-step guide on the key stages of LAM development, including data collection, model training, environment integration, grounding, and evaluation. This generalizable workflow can serve as a blueprint for creating functional LAMs in various application domains. We conclude by identifying the current limitations of LAMs and discussing directions for future research and industrial deployment, emphasizing the challenges and opportunities that lie ahead in realizing the full potential of LAMs in real-world applications.   The code for the data collection process utilized in this paper is publicly available at: https://github.com/microsoft/UFO/tree/main/dataflow, and comprehensive documentation can be found at https://microsoft.github.io/UFO/dataflow/overview/.",
            "score": 6,
            "issue_id": 1135,
            "pub_date": "2024-12-13",
            "pub_date_card": {
                "ru": "13 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 13",
                "zh": "12æœˆ13æ—¥"
            },
            "hash": "be65080464153291",
            "authors": [
                "Lu Wang",
                "Fangkai Yang",
                "Chaoyun Zhang",
                "Junting Lu",
                "Jiaxu Qian",
                "Shilin He",
                "Pu Zhao",
                "Bo Qiao",
                "Ray Huang",
                "Si Qin",
                "Qisheng Su",
                "Jiayi Ye",
                "Yudi Zhang",
                "Jian-Guang Lou",
                "Qingwei Lin",
                "Saravan Rajmohan",
                "Dongmei Zhang",
                "Qi Zhang"
            ],
            "affiliations": [
                "Eindhoven University of Technology",
                "Microsoft",
                "Peking University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.10047.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#data",
                    "#training",
                    "#open_source",
                    "#agi"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞÑ‚ ÑĞ»Ğ¾Ğ² Ğº Ğ´ĞµĞ»Ñƒ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ¿ Ğ² Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ ĞšÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… ĞœĞ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ”ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ (LAM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ²Ğ°Ğ½Ñ‹ Ğ¿ĞµÑ€ĞµĞ²ĞµÑÑ‚Ğ¸ Ğ˜Ğ˜ Ğ¾Ñ‚ Ğ¿Ğ°ÑÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ° Ğº Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ LAM, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ñ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ Windows OS Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ LAM. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ĞµÑ‚ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ LAM Ğ¸ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "From Language to Action: Advancing AI with Large Action Models",
                    "desc": "This paper discusses the shift from Large Language Models (LLMs) to Large Action Models (LAMs), which are designed to perform actions in real-world environments rather than just generating text. LAMs are enabled by agent systems and represent a step towards achieving artificial general intelligence by allowing AI to complete tasks actively. The authors present a framework for developing LAMs, detailing the stages from data collection to model training and evaluation, using a Windows OS-based agent as an example. They also address the limitations of current LAMs and suggest future research directions to enhance their capabilities in practical applications."
                },
                "zh": {
                    "title": "ä»è¯­è¨€ç†è§£åˆ°è¡ŒåŠ¨æ‰§è¡Œçš„æ™ºèƒ½è½¬å‹",
                    "desc": "éšç€äººå·¥æ™ºèƒ½çš„ä¸æ–­è¿›æ­¥ï¼Œå¸‚åœºå¯¹èƒ½å¤Ÿæ‰§è¡Œå®é™…æ“ä½œçš„æ™ºèƒ½ä»£ç†ç³»ç»Ÿçš„éœ€æ±‚æ—¥ç›Šå¢åŠ ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤§å‹è¡ŒåŠ¨æ¨¡å‹ï¼ˆLAMsï¼‰çš„ç»¼åˆæ¡†æ¶ï¼Œæ—¨åœ¨ä»ä¼ ç»Ÿçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è½¬å˜ä¸ºèƒ½å¤Ÿåœ¨åŠ¨æ€ç¯å¢ƒä¸­ç”Ÿæˆå’Œæ‰§è¡Œè¡ŒåŠ¨çš„æ¨¡å‹ã€‚æˆ‘ä»¬é€šè¿‡Windowsæ“ä½œç³»ç»Ÿçš„ä»£ç†ä½œä¸ºæ¡ˆä¾‹ï¼Œè¯¦ç»†ä»‹ç»äº†LAMå¼€å‘çš„å…³é”®é˜¶æ®µï¼ŒåŒ…æ‹¬æ•°æ®æ”¶é›†ã€æ¨¡å‹è®­ç»ƒã€ç¯å¢ƒé›†æˆå’Œè¯„ä¼°ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†LAMsçš„å½“å‰å±€é™æ€§ä»¥åŠæœªæ¥ç ”ç©¶å’Œå·¥ä¸šåº”ç”¨çš„æ–¹å‘ï¼Œå¼ºè°ƒäº†å®ç°LAMsåœ¨å®é™…åº”ç”¨ä¸­æ½œåŠ›çš„æŒ‘æˆ˜ä¸æœºé‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.09626",
            "title": "FreeScale: Unleashing the Resolution of Diffusion Models via Tuning-Free Scale Fusion",
            "url": "https://huggingface.co/papers/2412.09626",
            "abstract": "Visual diffusion models achieve remarkable progress, yet they are typically trained at limited resolutions due to the lack of high-resolution data and constrained computation resources, hampering their ability to generate high-fidelity images or videos at higher resolutions. Recent efforts have explored tuning-free strategies to exhibit the untapped potential higher-resolution visual generation of pre-trained models. However, these methods are still prone to producing low-quality visual content with repetitive patterns. The key obstacle lies in the inevitable increase in high-frequency information when the model generates visual content exceeding its training resolution, leading to undesirable repetitive patterns deriving from the accumulated errors. To tackle this challenge, we propose FreeScale, a tuning-free inference paradigm to enable higher-resolution visual generation via scale fusion. Specifically, FreeScale processes information from different receptive scales and then fuses it by extracting desired frequency components. Extensive experiments validate the superiority of our paradigm in extending the capabilities of higher-resolution visual generation for both image and video models. Notably, compared with the previous best-performing method, FreeScale unlocks the generation of 8k-resolution images for the first time.",
            "score": 3,
            "issue_id": 1135,
            "pub_date": "2024-12-12",
            "pub_date_card": {
                "ru": "12 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 12",
                "zh": "12æœˆ12æ—¥"
            },
            "hash": "1551e9966255aa0a",
            "authors": [
                "Haonan Qiu",
                "Shiwei Zhang",
                "Yujie Wei",
                "Ruihang Chu",
                "Hangjie Yuan",
                "Xiang Wang",
                "Yingya Zhang",
                "Ziwei Liu"
            ],
            "affiliations": [
                "Alibaba Group",
                "Fudan University",
                "Nanyang Technological University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.09626.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#inference",
                    "#diffusion",
                    "#video",
                    "#cv"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "FreeScale: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° ÑĞ²ĞµÑ€Ñ…Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ FreeScale - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¿Ğ¾ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ÑÑÑ‰Ğ¸Ñ…ÑÑ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² Ğ¿Ñ€Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¾Ğ² Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ½ÑƒĞ¶Ğ½Ñ‹Ñ… Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ FreeScale Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼ 8K."
                },
                "en": {
                    "title": "Unlocking High-Resolution Visuals with FreeScale",
                    "desc": "This paper introduces FreeScale, a new method for generating high-resolution images and videos using visual diffusion models. Traditional models struggle with high-resolution outputs due to limited training data and computational resources, often resulting in low-quality visuals with repetitive patterns. FreeScale addresses this by employing a tuning-free inference approach that fuses information from various scales, allowing the model to better handle high-frequency details. Experimental results demonstrate that FreeScale significantly enhances the quality of generated visuals, achieving 8k-resolution outputs for the first time."
                },
                "zh": {
                    "title": "FreeScaleï¼šæ— è°ƒä¼˜çš„é«˜åˆ†è¾¨ç‡è§†è§‰ç”Ÿæˆæ–°èŒƒå¼",
                    "desc": "è§†è§‰æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜ä¿çœŸå›¾åƒæˆ–è§†é¢‘æ—¶é¢ä¸´åˆ†è¾¨ç‡é™åˆ¶çš„é—®é¢˜ï¼Œä¸»è¦æ˜¯ç”±äºç¼ºä¹é«˜åˆ†è¾¨ç‡æ•°æ®å’Œè®¡ç®—èµ„æºã€‚æœ€è¿‘çš„ç ”ç©¶å°è¯•äº†æ— è°ƒä¼˜ç­–ç•¥ï¼Œä»¥å±•ç¤ºé¢„è®­ç»ƒæ¨¡å‹åœ¨é«˜åˆ†è¾¨ç‡è§†è§‰ç”Ÿæˆæ–¹é¢çš„æ½œåŠ›ï¼Œä½†ä»ç„¶å®¹æ˜“äº§ç”Ÿä½è´¨é‡çš„è§†è§‰å†…å®¹å’Œé‡å¤æ¨¡å¼ã€‚æˆ‘ä»¬æå‡ºäº†FreeScaleï¼Œè¿™æ˜¯ä¸€ç§æ— è°ƒä¼˜æ¨ç†èŒƒå¼ï¼Œé€šè¿‡å°ºåº¦èåˆå®ç°æ›´é«˜åˆ†è¾¨ç‡çš„è§†è§‰ç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFreeScaleåœ¨å›¾åƒå’Œè§†é¢‘æ¨¡å‹çš„é«˜åˆ†è¾¨ç‡ç”Ÿæˆèƒ½åŠ›ä¸Šä¼˜äºä»¥å¾€çš„æ–¹æ³•ï¼Œé¦–æ¬¡å®ç°äº†8kåˆ†è¾¨ç‡å›¾åƒçš„ç”Ÿæˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.10319",
            "title": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods",
            "url": "https://huggingface.co/papers/2412.10319",
            "abstract": "Long-context LLMs have enabled numerous downstream applications but also introduced significant challenges related to computational and memory efficiency. To address these challenges, optimizations for long-context inference have been developed, centered around the KV cache. However, existing benchmarks often evaluate in single-request, neglecting the full lifecycle of the KV cache in real-world use. This oversight is particularly critical, as KV cache reuse has become widely adopted in LLMs inference frameworks, such as vLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft, Google, and Anthropic. To address this gap, we introduce SCBench(SharedContextBench), a comprehensive benchmark for evaluating long-context methods from a KV cachecentric perspective: 1) KV cache generation, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache loading. Specifically, SCBench uses test examples with shared context, ranging 12 tasks with two shared context modes, covering four categories of long-context capabilities: string retrieval, semantic retrieval, global information, and multi-task. With it, we provide an extensive KV cache-centric analysis of eight categories long-context solutions, including Gated Linear RNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention, KV cache dropping, quantization, retrieval, loading, and prompt compression. The evaluation is conducted on 8 long-context LLMs. Our findings show that sub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding with O(n) memory and sub-O(n^2) pre-filling computation perform robustly. Dynamic sparsity yields more expressive KV caches than static patterns, and layer-level sparsity in hybrid architectures reduces memory usage with strong performance. Additionally, we identify attention distribution shift issues in long-generation scenarios. https://aka.ms/SCBench.",
            "score": 1,
            "issue_id": 1135,
            "pub_date": "2024-12-13",
            "pub_date_card": {
                "ru": "13 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 13",
                "zh": "12æœˆ13æ—¥"
            },
            "hash": "a6269882457435d4",
            "authors": [
                "Yucheng Li",
                "Huiqiang Jiang",
                "Qianhui Wu",
                "Xufang Luo",
                "Surin Ahn",
                "Chengruidong Zhang",
                "Amir H. Abdi",
                "Dongsheng Li",
                "Jianfeng Gao",
                "Yuqing Yang",
                "Lili Qiu"
            ],
            "affiliations": [
                "Microsoft Corporation",
                "University of Surrey"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.10319.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#long_context",
                    "#inference",
                    "#benchmark",
                    "#architecture"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "SCBench: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ğ·Ğ¼Ñƒ KV-ĞºÑÑˆĞ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº SCBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ Ğ°ĞºÑ†ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ½Ğ° KV-ĞºÑÑˆ. SCBench Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ, ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ, Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºÑƒ KV-ĞºÑÑˆĞ° Ğ½Ğ° 12 Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ¾Ğ±Ñ‰Ğ¸Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ 8 ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ° 8 ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ ÑÑƒĞ±ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ ÑƒÑÑ‚ÑƒĞ¿Ğ°ÑÑ‚ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…, Ğ° Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ñ€Ğ¾Ñ€ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ KV-ĞºÑÑˆĞ¸."
                },
                "en": {
                    "title": "Optimizing Long-Context LLMs with SCBench: A KV Cache Revolution",
                    "desc": "This paper introduces SCBench, a new benchmark designed to evaluate long-context methods in large language models (LLMs) with a focus on the key-value (KV) cache. It addresses the limitations of existing benchmarks by considering the entire lifecycle of the KV cache, including its generation, compression, retrieval, and loading. The study analyzes various long-context solutions, revealing that memory-efficient methods can struggle in multi-turn scenarios, while certain sparse encoding techniques perform well. The findings also highlight the importance of dynamic sparsity and layer-level sparsity in optimizing memory usage and performance in LLMs."
                },
                "zh": {
                    "title": "ä¼˜åŒ–é•¿ä¸Šä¸‹æ–‡çš„KVç¼“å­˜è¯„ä¼°",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†SCBenchï¼ˆSharedContextBenchï¼‰ï¼Œä¸€ä¸ªé’ˆå¯¹é•¿ä¸Šä¸‹æ–‡æ–¹æ³•çš„åŸºå‡†æµ‹è¯•ï¼Œé‡ç‚¹å…³æ³¨KVç¼“å­˜çš„ç”Ÿå‘½å‘¨æœŸã€‚ç ”ç©¶è¡¨æ˜ï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•å¾€å¾€åªå…³æ³¨å•æ¬¡è¯·æ±‚ï¼Œè€Œå¿½è§†äº†KVç¼“å­˜çš„é‡ç”¨ï¼Œè¿™åœ¨å®é™…åº”ç”¨ä¸­è‡³å…³é‡è¦ã€‚SCBenchæ¶µç›–äº†KVç¼“å­˜çš„ç”Ÿæˆã€å‹ç¼©ã€æ£€ç´¢å’ŒåŠ è½½ç­‰å››ä¸ªæ–¹é¢ï¼Œå¹¶é€šè¿‡12ä¸ªä»»åŠ¡çš„å…±äº«ä¸Šä¸‹æ–‡è¿›è¡Œè¯„ä¼°ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼ŒåŠ¨æ€ç¨€ç–æ€§åœ¨KVç¼“å­˜ä¸­è¡¨ç°æ›´å¥½ï¼Œè€Œæ··åˆæ¶æ„ä¸­çš„å±‚çº§ç¨€ç–æ€§åˆ™æœ‰æ•ˆé™ä½äº†å†…å­˜ä½¿ç”¨ï¼ŒåŒæ—¶ä¿æŒäº†å¼ºå¤§çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.07517",
            "title": "FireFlow: Fast Inversion of Rectified Flow for Image Semantic Editing",
            "url": "https://huggingface.co/papers/2412.07517",
            "abstract": "Though Rectified Flows (ReFlows) with distillation offers a promising way for fast sampling, its fast inversion transforms images back to structured noise for recovery and following editing remains unsolved. This paper introduces FireFlow, a simple yet effective zero-shot approach that inherits the startling capacity of ReFlow-based models (such as FLUX) in generation while extending its capabilities to accurate inversion and editing in 8 steps. We first demonstrate that a carefully designed numerical solver is pivotal for ReFlow inversion, enabling accurate inversion and reconstruction with the precision of a second-order solver while maintaining the practical efficiency of a first-order Euler method. This solver achieves a 3times runtime speedup compared to state-of-the-art ReFlow inversion and editing techniques, while delivering smaller reconstruction errors and superior editing results in a training-free mode. The code is available at https://github.com/HolmesShuan/FireFlow{this URL}.",
            "score": 1,
            "issue_id": 1135,
            "pub_date": "2024-12-10",
            "pub_date_card": {
                "ru": "10 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 10",
                "zh": "12æœˆ10æ—¥"
            },
            "hash": "70f97a4533ea4ebb",
            "authors": [
                "Yingying Deng",
                "Xiangyu He",
                "Changwang Mei",
                "Peisong Wang",
                "Fan Tang"
            ],
            "affiliations": [
                "Institute of Automation, Chinese Academy of Sciences, Beijing, China",
                "Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.07517.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#open_source",
                    "#cv",
                    "#architecture"
                ],
                "emoji": "ğŸ”¥",
                "ru": {
                    "title": "FireFlow: ĞœĞ¾Ğ»Ğ½Ğ¸ĞµĞ½Ğ¾ÑĞ½Ğ°Ñ Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ¸Ñ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² ReFlow Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ FireFlow - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Rectified Flows (ReFlows). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¸Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ·Ğ° 8 ÑˆĞ°Ğ³Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ FireFlow Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² 3 Ñ€Ğ°Ğ·Ğ° Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ñ‚ĞµÑ…Ğ½Ğ¸Ğº Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ¸Ğ¸ ReFlow, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ’Ğ°Ğ¶Ğ½Ğ¾ Ğ¾Ñ‚Ğ¼ĞµÑ‚Ğ¸Ñ‚ÑŒ, Ñ‡Ñ‚Ğ¾ FireFlow Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒÑÑ Ğº ÑƒĞ¶Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼."
                },
                "en": {
                    "title": "FireFlow: Fast and Accurate Image Inversion and Editing",
                    "desc": "This paper presents FireFlow, a new method that improves the inversion and editing capabilities of Rectified Flows (ReFlows) in machine learning. FireFlow uses a specially designed numerical solver that combines the efficiency of first-order methods with the accuracy of second-order methods, allowing for faster and more precise image reconstruction. The approach achieves a threefold increase in speed compared to existing ReFlow techniques while reducing errors and enhancing editing quality without requiring additional training. Overall, FireFlow enhances the usability of ReFlow models for generating and manipulating images effectively."
                },
                "zh": {
                    "title": "FireFlowï¼šé«˜æ•ˆçš„å›¾åƒåæ¼”ä¸ç¼–è¾‘æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºFireFlowçš„æ–°æ–¹æ³•ï¼Œå®ƒåœ¨å¿«é€Ÿé‡‡æ ·çš„åŸºç¡€ä¸Šï¼Œè§£å†³äº†å›¾åƒåæ¼”å’Œç¼–è¾‘çš„é—®é¢˜ã€‚FireFlowç»§æ‰¿äº†åŸºäºReFlowæ¨¡å‹çš„å¼ºå¤§ç”Ÿæˆèƒ½åŠ›ï¼Œå¹¶åœ¨8ä¸ªæ­¥éª¤å†…å®ç°äº†å‡†ç¡®çš„åæ¼”å’Œç¼–è¾‘ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ•°å€¼æ±‚è§£å™¨ï¼Œä½¿å¾—ReFlowçš„åæ¼”è¿‡ç¨‹æ›´åŠ ç²¾ç¡®ï¼ŒåŒæ—¶ä¿æŒäº†é«˜æ•ˆæ€§ã€‚ä¸ç°æœ‰çš„ReFlowåæ¼”å’Œç¼–è¾‘æŠ€æœ¯ç›¸æ¯”ï¼Œè¯¥æ±‚è§£å™¨åœ¨è¿è¡Œé€Ÿåº¦ä¸Šæé«˜äº†3å€ï¼Œå¹¶ä¸”åœ¨è®­ç»ƒæ— å…³çš„æ¨¡å¼ä¸‹ï¼Œé‡å»ºè¯¯å·®æ›´å°ï¼Œç¼–è¾‘æ•ˆæœæ›´ä½³ã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-12-13.html",
    "link_next": "2024-12-17.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "13.12",
        "en": "12/13",
        "zh": "12æœˆ13æ—¥"
    },
    "short_date_next": {
        "ru": "17.12",
        "en": "12/17",
        "zh": "12æœˆ17æ—¥"
    },
    "categories": {
        "#dataset": 0,
        "#data": 1,
        "#benchmark": 1,
        "#agents": 2,
        "#cv": 2,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 1,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 2,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†åˆ›å»ºèƒ½å¤Ÿé•¿æ—¶é—´ä¸ç¯å¢ƒäº’åŠ¨çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„ç ”ç©¶ç›®æ ‡ã€‚è¿‘æœŸï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¼€æ”¾ä¸–ç•Œç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œè¿ç»­å’ŒåŒæ—¶è¿›è¡Œçš„æ„ŸçŸ¥ã€è®°å¿†å’Œæ¨ç†ä»ç„¶æ˜¯ä¸€ä¸ªæœªæ¢ç´¢çš„æŒ‘æˆ˜ã€‚å½“å‰çš„MLLMså—é™äºå…¶åºåˆ—åˆ°åºåˆ—çš„æ¶æ„ï¼Œæ— æ³•åŒæ—¶å¤„ç†è¾“å…¥å’Œç”Ÿæˆå“åº”ã€‚å› æ­¤ï¼Œè¿™ä¸ªé¡¹ç›®å¼•å…¥äº†åˆ†ç¦»çš„æ„ŸçŸ¥ã€æ¨ç†å’Œè®°å¿†æœºåˆ¶ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå®æ—¶å¤„ç†è§†é¢‘å’ŒéŸ³é¢‘è¾“å…¥ã€‚æå‡ºçš„æ¡†æ¶IXC2.5-OLåŒ…æ‹¬ä¸‰ä¸ªå…³é”®æ¨¡å—ï¼šå®æ—¶å¤„ç†å¤šæ¨¡æ€ä¿¡æ¯çš„æ„ŸçŸ¥æ¨¡å—ï¼Œæ•´åˆçŸ­æœŸå’Œé•¿æœŸè®°å¿†çš„å¤šæ¨¡æ€é•¿æœŸè®°å¿†æ¨¡å—ï¼Œä»¥åŠå“åº”æŸ¥è¯¢å’Œæ‰§è¡Œæ¨ç†ä»»åŠ¡çš„æ¨ç†æ¨¡å—ã€‚è¿™ä¸ªé¡¹ç›®æ¨¡æ‹Ÿäº†äººç±»è®¤çŸ¥ï¼Œä½¿å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿæä¾›æŒç»­å’Œé€‚åº”æ€§çš„æœåŠ¡ã€‚",
        "title": "InternLM-XComposer2.5-OmniLive: A Comprehensive Multimodal System for Long-term Streaming Video and Audio Interactions",
        "pinyin": "ZhÃ¨ piÄn wÃ©nzhÄng tÇolÃ¹nle chuÃ ngjiÃ n nÃ©nggÃ²u chÃ¡ng shÃ­jiÄn yÇ” huÃ¡njÃ¬ng hÃ¹dÃ²ng de rÃ©ngÅng zhÃ¬nÃ©ng xÃ¬tÇ’ng de yÃ¡njiÅ« mÃ¹biÄo. JÃ¬nqÄ«, duÅ mÃ³shuÇi dÃ xÃ­ng yÇ”yÃ¡n mÃ³xÃ­ng (MLLMs) zÃ i kÄifÃ ng shÃ¬jiÃ¨ lÇjiÄ› fÄngmiÃ n quÃ¨dÃ©le xiÇnzhÃ¹ jÃ¬nbÃ¹. RÃ¡n'Ã©r, liÃ¡nxÃ¹ hÃ© tÃ³ngshÃ­ jÃ¬nxÃ­ng de gÇnjuÃ©, jÃ¬yÃ¬ hÃ© tuÃ­li shÃ¬ yÄ«gÃ¨ wÃ¨i tÃ nsuÇ’ de tiÃ¡ozhÃ n. DÄngqiÃ¡n de MLLMs shÃ²uxiÃ n yÃº qÃ­ xÃ¹liÃ¨ dÃ o xÃ¹liÃ¨ de jiÃ gÃ²u, wÃºfÇ tÃ³ngshÃ­ chÇ”lÇ shÅ«rÃ¹ hÃ© shÄ“ngchÃ©ng xiÇngyÃ¬ng. YÄ«ncÇ, zhÃ¨gÃ¨ xiÃ ngmÃ¹ yÇnrÃ¹le fÄ“nliÃ¨ de gÇnjuÃ©, tuÃ­li hÃ© jÃ¬yÃ¬ jÄ«zhÃ¬, shÇdÃ© mÃ³xÃ­ng nÃ©nggÃ²u shÃ­shÃ­ chÇ”lÇ shÃ¬pÇn hÃ© yÄ«npÇn shÅ«rÃ¹. TÃ­chÅ« de kuÃ ngjiÃ  IXC2.5-OL bÄokuÃ² sÄn gÃ¨ guÇnjiÃ n mÃ³dÃ¹: shÃ­shÃ­ chÇ”lÇ duÅ mÃ³shuÇi xÃ¬nxÄ« de gÇnjuÃ© mÃ³dÃ¹, zhÄ›nghÃ© duÇnqÄ« hÃ© chÃ¡ngqÄ« jÃ¬yÃ¬ de duÅ mÃ³shuÇi chÃ¡ngqÄ« jÃ¬yÃ¬ mÃ³dÃ¹, yÇjiÇ xiÇngyÃ¬ng chÃ¡xÃºn hÃ© zhÃ­xÃ­ng tuÃ­li rÃ¨nwÃ¹ de tuÃ­li mÃ³dÃ¹. ZhÃ¨gÃ¨ xiÃ ngmÃ¹ mÃ³nÇle rÃ©nlÃ¨i rÃ¨ngÅng, shÇ duÅ mÃ³shuÇi dÃ xÃ­ng yÇ”yÃ¡n mÃ³xÃ­ng nÃ©nggÃ²u tÃ­gÅng chÃ­xÃ¹ hÃ© shÃ¬yÃ¬ngxÃ¬ng de fÃºwÃ¹.",
        "vocab": "[\n    {\"word\": \"è®¨è®º\", \"pinyin\": \"tÇo lÃ¹n\", \"trans\": \"discuss\"},\n    {\"word\": \"åˆ›å»º\", \"pinyin\": \"chuÃ ng jiÃ n\", \"trans\": \"create\"},\n    {\"word\": \"èƒ½å¤Ÿ\", \"pinyin\": \"nÃ©ng gÃ²u\", \"trans\": \"be able to\"},\n    {\"word\": \"é•¿æ—¶é—´\", \"pinyin\": \"chÃ¡ng shÃ­ jiÄn\", \"trans\": \"long period of time\"},\n    {\"word\": \"äº’åŠ¨\", \"pinyin\": \"hÃ¹ dÃ²ng\", \"trans\": \"interact\"},\n    {\"word\": \"äººå·¥æ™ºèƒ½\", \"pinyin\": \"rÃ©n gÅng zhÃ¬ nÃ©ng\", \"trans\": \"artificial intelligence\"},\n    {\"word\": \"ç³»ç»Ÿ\", \"pinyin\": \"xÃ¬ tÇ’ng\", \"trans\": \"system\"},\n    {\"word\": \"ç ”ç©¶\", \"pinyin\": \"yÃ¡n jiÅ«\", \"trans\": \"research\"},\n    {\"word\": \"ç›®æ ‡\", \"pinyin\": \"mÃ¹ biÄo\", \"trans\": \"goal\"},\n    {\"word\": \"å¤šæ¨¡æ€\", \"pinyin\": \"duÅ mÃ³ shuÃ i\", \"trans\": \"multimodal\"},\n    {\"word\": \"å¤§å‹\", \"pinyin\": \"dÃ  xÃ­ng\", \"trans\": \"large-scale\"},\n    {\"word\": \"è¯­è¨€\", \"pinyin\": \"yÇ” yÃ¡n\", \"trans\": \"language\"},\n    {\"word\": \"æ¨¡å‹\", \"pinyin\": \"mÃ³ xÃ­ng\", \"trans\": \"model\"},\n    {\"word\": \"å¼€æ”¾\", \"pinyin\": \"kÄi fÃ ng\", \"trans\": \"open\"},\n    {\"word\": \"ä¸–ç•Œ\", \"pinyin\": \"shÃ¬ jiÃ¨\", \"trans\": \"world\"},\n    {\"word\": \"ç†è§£\", \"pinyin\": \"lÇ jiÄ›\", \"trans\": \"understand\"},\n    {\"word\": \"æ–¹é¢\", \"pinyin\": \"fÄng miÃ n\", \"trans\": \"aspect\"},\n    {\"word\": \"å–å¾—\", \"pinyin\": \"qÇ” dÃ©\", \"trans\": \"achieve\"},\n    {\"word\": \"æ˜¾è‘—\", \"pinyin\": \"xiÇn zhÃ¹\", \"trans\": \"significant\"},\n    {\"word\": \"è¿›å±•\", \"pinyin\": \"jÃ¬n zhÇn\", \"trans\": \"progress\"},\n    {\"word\": \"ç„¶è€Œ\", \"pinyin\": \"rÃ¡n Ã©r\", \"trans\": \"however\"},\n    {\"word\": \"è¿ç»­\", \"pinyin\": \"liÃ¡n xÃ¹\", \"trans\": \"continuous\"},\n    {\"word\": \"åŒæ—¶\", \"pinyin\": \"tÃ³ng shÃ­\", \"trans\": \"simultaneous\"},\n    {\"word\": \"æ„ŸçŸ¥\", \"pinyin\": \"gÇn zhÄ«\", \"trans\": \"perception\"},\n    {\"word\": \"è®°å¿†\", \"pinyin\": \"jÃ¬ yÃ¬\", \"trans\": \"memory\"},\n    {\"word\": \"æ¨ç†\", \"pinyin\": \"tuÄ« lÇ\", \"trans\": \"reasoning\"},\n    {\"word\": \"æŒ‘æˆ˜\", \"pinyin\": \"tiÇo zhÃ n\", \"trans\": \"challenge\"},\n    {\"word\": \"æœª\", \"pinyin\": \"wÃ¨i\", \"trans\": \"not yet\"},\n    {\"word\": \"æ¢ç´¢\", \"pinyin\": \"tÃ n suÇ’\", \"trans\": \"explore\"},\n    {\"word\": \"å—é™äº\", \"pinyin\": \"shÃ²u xiÃ n yÃº\", \"trans\": \"be limited to\"},\n    {\"word\": \"å…¶\", \"pinyin\": \"qÃ­\", \"trans\": \"its\"},\n    {\"word\": \"åºåˆ—\", \"pinyin\": \"xÃ¹ liÃ¨\", \"trans\": \"sequence\"},\n    {\"word\": \"æ¶æ„\", \"pinyin\": \"jiÃ  gÃ²u\", \"trans\": \"architecture\"},\n    {\"word\": \"æ— æ³•\", \"pinyin\": \"wÃº fÇ\", \"trans\": \"unable to\"},\n    {\"word\": \"å¤„ç†\", \"pinyin\": \"chÇ” lÇ\", \"trans\": \"process\"},\n    {\"word\": \"è¾“å…¥\", \"pinyin\": \"shÅ« rÃ¹\", \"trans\": \"input\"},\n    {\"word\": \"ç”Ÿæˆ\", \"pinyin\": \"shÄ“ng chÃ©ng\", \"trans\": \"generate\"},\n    {\"word\": \"å“åº”\", \"pinyin\": \"xiÇng yÃ¬ng\", \"trans\": \"response\"},\n    {\"word\": \"å¼•å…¥\", \"pinyin\": \"yÇn rÃ¹\", \"trans\": \"introduce\"},\n    {\"word\": \"åˆ†ç¦»\", \"pinyin\": \"fÄ“n lÃ­\", \"trans\": \"separate\"},\n    {\"word\": \"æœºåˆ¶\", \"pinyin\": \"jÄ« zhÃ¬\", \"trans\": \"mechanism\"},\n    {\"word\": \"å®æ—¶\", \"pinyin\": \"shÃ­ shÃ­\", \"trans\": \"real-time\"},\n    {\"word\": \"è§†é¢‘\", \"pinyin\": \"shÃ¬ pÃ­n\", \"trans\": \"video\"},\n    {\"word\": \"éŸ³é¢‘\", \"pinyin\": \"yÄ«n pÃ­n\", \"trans\": \"audio\"},\n    {\"word\": \"æå‡º\", \"pinyin\": \"tÃ­ chÅ«\", \"trans\": \"propose\"},\n    {\"word\": \"æ¡†æ¶\", \"pinyin\": \"kuÃ ng jiÃ \", \"trans\": \"framework\"},\n    {\"word\": \"IXC2.5-OL\", \"pinyin\": \"IXC2.5-OL\", \"trans\": \"IXC2.5-OL\"},\n    {\"word\": \"åŒ…æ‹¬\", \"pinyin\": \"bÄo kuÃ²\", \"trans\": \"include\"},\n    {\"word\": \"å…³é”®\", \"pinyin\": \"guÇn jiÃ n\", \"trans\": \"key\"},\n    {\"word\": \"æ¨¡å—\", \"pinyin\": \"mÃ³ kuÃ i\", \"trans\": \"module\"},\n    {\"word\": \"æ•´åˆ\", \"pinyin\": \"zhÄ›ng hÃ©\", \"trans\": \"integrate\"},\n    {\"word\": \"çŸ­æœŸ\", \"pinyin\": \"duÇn qÄ«\", \"trans\": \"short-term\"},\n    {\"word\": \"é•¿æœŸ\", \"pinyin\": \"chÃ¡ng qÄ«\", \"trans\": \"long-term\"},\n    {\"word\": \"å¤šæ¨¡æ€é•¿æœŸè®°å¿†\", \"pinyin\": \"duÅ mÃ³ shuÃ i chÃ¡ng qÄ« jÃ¬ yÃ¬\", \"trans\": \"multimodal long-term memory\"},\n    {\"word\": \"å“åº”æŸ¥è¯¢\", \"pinyin\": \"xiÇng yÃ¬ng chÃ¡ xÃºn\", \"trans\": \"respond to queries\"},\n    {\"word\": \"æ‰§è¡Œ\", \"pinyin\": \"zhÃ­ xÃ­ng\", \"trans\": \"execute\"},\n    {\"word\": \"ä»»åŠ¡\", \"pinyin\": \"rÃ¨n wÃ¹\", \"trans\": \"task\"},\n    {\"word\": \"é¡¹ç›®\", \"pinyin\": \"xiÃ ng mÃ¹\", \"trans\": \"project\"},\n    {\"word\": \"æ¨¡æ‹Ÿ\", \"pinyin\": \"mÃ³ nÇ\", \"trans\": \"simulate\"},\n    {\"word\": \"äººç±»\", \"pinyin\": \"rÃ©n lÃ¨i\", \"trans\": \"human\"},\n    {\"word\": \"è®¤çŸ¥\", \"pinyin\": \"rÃ¨n zhÄ«\", \"trans\": \"cognition\"},\n    {\"word\": \"ä½¿\", \"pinyin\": \"shÇ\", \"trans\": \"make\"},\n    {\"word\": \"æä¾›\", \"pinyin\": \"tÃ­ gÅng\", \"trans\": \"provide\"},\n    {\"word\": \"æŒç»­\", \"pinyin\": \"chÃ­ xÃ¹\", \"trans\": \"continuous\"},\n    {\"word\": \"é€‚åº”æ€§\", \"pinyin\": \"shÃ¬ yÃ¬ng xÃ¬ng\", \"trans\": \"adaptability\"},\n    {\"word\": \"æœåŠ¡\", \"pinyin\": \"fÃº wÃ¹\", \"trans\": \"service\"}\n]",
        "trans": "This article discusses the research goal of creating artificial intelligence systems capable of long-term interaction with their environment. Recently, multimodal large language models (MLLMs) have made significant progress in understanding the open world. However, continuous and simultaneous perception, memory, and reasoning remain unexplored challenges. Current MLLMs are limited by their sequence-to-sequence architecture, which prevents them from processing inputs and generating responses simultaneously. Therefore, this project introduces separate mechanisms for perception, reasoning, and memory, allowing the model to process video and audio inputs in real-time. The proposed framework, IXC2.5-OL, includes three key modules: a perception module for real-time processing of multimodal information, a multimodal long-term memory module that integrates short-term and long-term memory, and a reasoning module for responding to queries and performing reasoning tasks. This project simulates human cognition, enabling multimodal large language models to provide continuous and adaptive services.",
        "update_ts": "2024-12-15 12:42"
    }
}