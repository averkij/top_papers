{
    "date": {
        "ru": "16 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
        "en": "December 16",
        "zh": "12æœˆ16æ—¥"
    },
    "time_utc": "2024-12-16 18:14",
    "weekday": 0,
    "issue_id": 1150,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.10360",
            "title": "Apollo: An Exploration of Video Understanding in Large Multimodal Models",
            "url": "https://huggingface.co/papers/2412.10360",
            "abstract": "Despite the rapid integration of video perception capabilities into Large Multimodal Models (LMMs), the underlying mechanisms driving their video understanding remain poorly understood. Consequently, many design decisions in this domain are made without proper justification or analysis. The high computational cost of training and evaluating such models, coupled with limited open research, hinders the development of video-LMMs. To address this, we present a comprehensive study that helps uncover what effectively drives video understanding in LMMs.   We begin by critically examining the primary contributors to the high computational requirements associated with video-LMM research and discover Scaling Consistency, wherein design and training decisions made on smaller models and datasets (up to a critical size) effectively transfer to larger models. Leveraging these insights, we explored many video-specific aspects of video-LMMs, including video sampling, architectures, data composition, training schedules, and more. For example, we demonstrated that fps sampling during training is vastly preferable to uniform frame sampling and which vision encoders are the best for video representation.   Guided by these findings, we introduce Apollo, a state-of-the-art family of LMMs that achieve superior performance across different model sizes. Our models can perceive hour-long videos efficiently, with Apollo-3B outperforming most existing 7B models with an impressive 55.1 on LongVideoBench. Apollo-7B is state-of-the-art compared to 7B LMMs with a 70.9 on MLVU, and 63.3 on Video-MME.",
            "score": 68,
            "issue_id": 1137,
            "pub_date": "2024-12-13",
            "pub_date_card": {
                "ru": "13 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 13",
                "zh": "12æœˆ13æ—¥"
            },
            "hash": "780ae1aa1dc1af24",
            "authors": [
                "Orr Zohar",
                "Xiaohan Wang",
                "Yann Dubois",
                "Nikhil Mehta",
                "Tong Xiao",
                "Philippe Hansen-Estruch",
                "Licheng Yu",
                "Xiaofang Wang",
                "Felix Juefei-Xu",
                "Ning Zhang",
                "Serena Yeung-Levy",
                "Xide Xia"
            ],
            "affiliations": [
                "Meta",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.10360.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#video",
                    "#architecture",
                    "#multimodal",
                    "#transfer_learning",
                    "#optimization"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°Ñ ÑĞµĞºÑ€ĞµÑ‚Ñ‹ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LMM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ñ‚ÑŒ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ñ Ğ¼Ğ°Ğ»ĞµĞ½ÑŒĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ. ĞĞ½Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹ Ğ²Ğ¸Ğ´ĞµĞ¾-LMM, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºÑƒ ĞºĞ°Ğ´Ñ€Ğ¾Ğ², Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ ÑĞ¾ÑÑ‚Ğ°Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¾ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Apollo, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‰ĞµĞµ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Unlocking Video Understanding in Large Multimodal Models with Apollo",
                    "desc": "This paper investigates the mechanisms behind video understanding in Large Multimodal Models (LMMs), which are complex AI systems that process both video and text. The authors identify a principle called Scaling Consistency, which shows that insights from smaller models can be applied to larger ones, helping to reduce computational costs. They also explore various aspects of video-LMMs, such as video sampling methods and architecture choices, to improve performance. The result of their research is Apollo, a new family of LMMs that significantly outperforms existing models in video perception tasks."
                },
                "zh": {
                    "title": "æ­ç¤ºè§†é¢‘ç†è§£çš„å…³é”®æœºåˆ¶",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨è§†é¢‘ç†è§£æ–¹é¢çš„æœºåˆ¶ï¼ŒæŒ‡å‡ºç›®å‰å¯¹å…¶ç†è§£ä»ç„¶ä¸è¶³ã€‚ç ”ç©¶å‘ç°ï¼Œç¼©æ”¾ä¸€è‡´æ€§æ˜¯å½±å“è§†é¢‘-LMMç ”ç©¶è®¡ç®—éœ€æ±‚çš„ä¸»è¦å› ç´ ï¼Œè¾ƒå°æ¨¡å‹å’Œæ•°æ®é›†çš„è®¾è®¡å’Œè®­ç»ƒå†³ç­–å¯ä»¥æœ‰æ•ˆè½¬ç§»åˆ°æ›´å¤§æ¨¡å‹ä¸Šã€‚é€šè¿‡å¯¹è§†é¢‘ç‰¹å®šæ–¹é¢çš„æ·±å…¥ç ”ç©¶ï¼Œæå‡ºäº†Apolloç³»åˆ—æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹åœ¨ä¸åŒè§„æ¨¡ä¸‹è¡¨ç°ä¼˜å¼‚ï¼Œèƒ½å¤Ÿé«˜æ•ˆå¤„ç†é•¿è¾¾ä¸€å°æ—¶çš„è§†é¢‘ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒApollo-3Båœ¨LongVideoBenchä¸Šè¶…è¶Šäº†å¤§å¤šæ•°ç°æœ‰çš„7Bæ¨¡å‹ï¼ŒApollo-7Båœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¹Ÿè¡¨ç°å‡ºè‰²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.09624",
            "title": "GenEx: Generating an Explorable World",
            "url": "https://huggingface.co/papers/2412.09624",
            "abstract": "Understanding, navigating, and exploring the 3D physical real world has long been a central challenge in the development of artificial intelligence. In this work, we take a step toward this goal by introducing GenEx, a system capable of planning complex embodied world exploration, guided by its generative imagination that forms priors (expectations) about the surrounding environments. GenEx generates an entire 3D-consistent imaginative environment from as little as a single RGB image, bringing it to life through panoramic video streams. Leveraging scalable 3D world data curated from Unreal Engine, our generative model is rounded in the physical world. It captures a continuous 360-degree environment with little effort, offering a boundless landscape for AI agents to explore and interact with. GenEx achieves high-quality world generation, robust loop consistency over long trajectories, and demonstrates strong 3D capabilities such as consistency and active 3D mapping. Powered by generative imagination of the world, GPT-assisted agents are equipped to perform complex embodied tasks, including both goal-agnostic exploration and goal-driven navigation. These agents utilize predictive expectation regarding unseen parts of the physical world to refine their beliefs, simulate different outcomes based on potential decisions, and make more informed choices. In summary, we demonstrate that GenEx provides a transformative platform for advancing embodied AI in imaginative spaces and brings potential for extending these capabilities to real-world exploration.",
            "score": 51,
            "issue_id": 1135,
            "pub_date": "2024-12-12",
            "pub_date_card": {
                "ru": "12 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 12",
                "zh": "12æœˆ12æ—¥"
            },
            "hash": "c4524ac73801b5cd",
            "authors": [
                "Taiming Lu",
                "Tianmin Shu",
                "Junfei Xiao",
                "Luoxin Ye",
                "Jiahao Wang",
                "Cheng Peng",
                "Chen Wei",
                "Daniel Khashabi",
                "Rama Chellappa",
                "Alan Yuille",
                "Jieneng Chen"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2412.09624.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#games",
                    "#agi",
                    "#3d"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ 3D-Ğ¼Ğ¸Ñ€Ğ°",
                    "desc": "GenEx - ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ° ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½ÑƒÑ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½ÑƒÑ ÑÑ€ĞµĞ´Ñƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²ÑĞµĞ³Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ RGB-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Unreal Engine Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¸Ñ€Ğ°, ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑÑ… Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ 3D-ĞºĞ°Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ³ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ GPT Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ GenEx Ğ´Ğ»Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ğ²Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ¶Ğ°ĞµĞ¼Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹ Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°."
                },
                "en": {
                    "title": "GenEx: Empowering AI Exploration with Generative Imagination",
                    "desc": "This paper presents GenEx, a novel system designed to enhance artificial intelligence's ability to explore and navigate 3D environments. GenEx utilizes generative imagination to create realistic 3D environments from a single RGB image, enabling AI agents to visualize and interact with these spaces through panoramic video. The system is built on extensive 3D data from Unreal Engine, ensuring high-quality world generation and robust consistency during exploration. By leveraging predictive expectations, GPT-assisted agents can perform complex tasks, improving their decision-making in both exploratory and navigational contexts."
                },
                "zh": {
                    "title": "GenExï¼šå¼€å¯AIæ¢ç´¢3Dä¸–ç•Œçš„æ–°ç¯‡ç« ",
                    "desc": "æœ¬ç ”ç©¶ä»‹ç»äº†GenExç³»ç»Ÿï¼Œå®ƒèƒ½å¤Ÿé€šè¿‡ç”Ÿæˆæƒ³è±¡æ¥è§„åˆ’å¤æ‚çš„3Dä¸–ç•Œæ¢ç´¢ã€‚GenExä»å•å¼ RGBå›¾åƒç”Ÿæˆä¸€è‡´çš„3Dç¯å¢ƒï¼Œå¹¶é€šè¿‡å…¨æ™¯è§†é¢‘æµå°†å…¶å‘ˆç°å‡ºæ¥ã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨æ¥è‡ªè™šå¹»å¼•æ“çš„å¯æ‰©å±•3Dä¸–ç•Œæ•°æ®ï¼Œæ•æ‰360åº¦çš„ç¯å¢ƒï¼Œä¸ºAIä»£ç†æä¾›äº†å¹¿é˜”çš„æ¢ç´¢ç©ºé—´ã€‚GenExå±•ç¤ºäº†é«˜è´¨é‡çš„ä¸–ç•Œç”Ÿæˆå’Œå¼ºå¤§çš„3Dèƒ½åŠ›ï¼Œä½¿å¾—AIä»£ç†èƒ½å¤Ÿæ‰§è¡Œå¤æ‚çš„ä»»åŠ¡ï¼ŒåŒ…æ‹¬æ— ç›®æ ‡æ¢ç´¢å’Œç›®æ ‡é©±åŠ¨å¯¼èˆªã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.09604",
            "title": "SynerGen-VL: Towards Synergistic Image Understanding and Generation with Vision Experts and Token Folding",
            "url": "https://huggingface.co/papers/2412.09604",
            "abstract": "The remarkable success of Large Language Models (LLMs) has extended to the multimodal domain, achieving outstanding performance in image understanding and generation. Recent efforts to develop unified Multimodal Large Language Models (MLLMs) that integrate these capabilities have shown promising results. However, existing approaches often involve complex designs in model architecture or training pipeline, increasing the difficulty of model training and scaling. In this paper, we propose SynerGen-VL, a simple yet powerful encoder-free MLLM capable of both image understanding and generation. To address challenges identified in existing encoder-free unified MLLMs, we introduce the token folding mechanism and the vision-expert-based progressive alignment pretraining strategy, which effectively support high-resolution image understanding while reducing training complexity. After being trained on large-scale mixed image-text data with a unified next-token prediction objective, SynerGen-VL achieves or surpasses the performance of existing encoder-free unified MLLMs with comparable or smaller parameter sizes, and narrows the gap with task-specific state-of-the-art models, highlighting a promising path toward future unified MLLMs. Our code and models shall be released.",
            "score": 24,
            "issue_id": 1142,
            "pub_date": "2024-12-12",
            "pub_date_card": {
                "ru": "12 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 12",
                "zh": "12æœˆ12æ—¥"
            },
            "hash": "1c0941190b24e85f",
            "authors": [
                "Hao Li",
                "Changyao Tian",
                "Jie Shao",
                "Xizhou Zhu",
                "Zhaokai Wang",
                "Jinguo Zhu",
                "Wenhan Dou",
                "Xiaogang Wang",
                "Hongsheng Li",
                "Lewei Lu",
                "Jifeng Dai"
            ],
            "affiliations": [
                "Beijing National Research Center for Information Science and Technology",
                "MMLab, The Chinese University of Hong Kong",
                "Nanjing University",
                "OpenGVLab, Shanghai AI Laboratory",
                "SenseTime Research",
                "Shanghai Jiao Tong University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.09604.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#agi",
                    "#multimodal",
                    "#architecture",
                    "#open_source"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾ÑÑ‚Ğ°Ñ, Ğ½Ğ¾ Ğ¼Ğ¾Ñ‰Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SynerGen-VL - Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ±ĞµĞ· ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑĞ²Ğ¾Ñ€Ğ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. SynerGen-VL Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ĞµĞ¹ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¸Ğ»Ğ¸ Ğ²Ñ‹ÑˆĞµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ğ¿Ñ€Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ¼ Ğ¸Ğ»Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¼ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğµ."
                },
                "en": {
                    "title": "Simplifying Multimodal Learning with SynerGen-VL",
                    "desc": "This paper presents SynerGen-VL, a new type of Multimodal Large Language Model (MLLM) that simplifies the process of image understanding and generation without the need for complex encoders. It introduces innovative techniques like token folding and a vision-expert-based pretraining strategy to enhance performance while minimizing training difficulties. By training on a large dataset of mixed image and text, SynerGen-VL achieves competitive results compared to existing models, even with fewer parameters. This work suggests a more efficient approach to developing unified MLLMs, paving the way for future advancements in the field."
                },
                "zh": {
                    "title": "SynerGen-VLï¼šç®€åŒ–çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSynerGen-VLçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œå®ƒèƒ½å¤ŸåŒæ—¶è¿›è¡Œå›¾åƒç†è§£å’Œç”Ÿæˆã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•è€Œå¼ºå¤§çš„æ— ç¼–ç å™¨è®¾è®¡ï¼Œæ—¨åœ¨é™ä½æ¨¡å‹è®­ç»ƒçš„å¤æ‚æ€§ã€‚é€šè¿‡å¼•å…¥ä»¤ç‰ŒæŠ˜å æœºåˆ¶å’ŒåŸºäºè§†è§‰ä¸“å®¶çš„æ¸è¿›å¯¹é½é¢„è®­ç»ƒç­–ç•¥ï¼ŒSynerGen-VLåœ¨é«˜åˆ†è¾¨ç‡å›¾åƒç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ç»è¿‡å¤§è§„æ¨¡æ··åˆå›¾åƒ-æ–‡æœ¬æ•°æ®çš„è®­ç»ƒï¼ŒSynerGen-VLåœ¨æ€§èƒ½ä¸Šä¸ç°æœ‰æ¨¡å‹ç›¸å½“æˆ–æ›´ä¼˜ï¼Œå±•ç¤ºäº†æœªæ¥ç»Ÿä¸€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.07769",
            "title": "BiMediX2: Bio-Medical EXpert LMM for Diverse Medical Modalities",
            "url": "https://huggingface.co/papers/2412.07769",
            "abstract": "This paper introduces BiMediX2, a bilingual (Arabic-English) Bio-Medical EXpert Large Multimodal Model (LMM) with a unified architecture that integrates text and visual modalities, enabling advanced image understanding and medical applications. BiMediX2 leverages the Llama3.1 architecture and integrates text and visual capabilities to facilitate seamless interactions in both English and Arabic, supporting text-based inputs and multi-turn conversations involving medical images. The model is trained on an extensive bilingual healthcare dataset consisting of 1.6M samples of diverse medical interactions for both text and image modalities, mixed in Arabic and English. We also propose the first bilingual GPT-4o based medical LMM benchmark named BiMed-MBench. BiMediX2 is benchmarked on both text-based and image-based tasks, achieving state-of-the-art performance across several medical benchmarks. It outperforms recent state-of-the-art models in medical LLM evaluation benchmarks. Our model also sets a new benchmark in multimodal medical evaluations with over 9% improvement in English and over 20% in Arabic evaluations. Additionally, it surpasses GPT-4 by around 9% in UPHILL factual accuracy evaluations and excels in various medical Visual Question Answering, Report Generation, and Report Summarization tasks. The project page including source code and the trained model, is available at https://github.com/mbzuai-oryx/BiMediX2.",
            "score": 20,
            "issue_id": 1140,
            "pub_date": "2024-12-10",
            "pub_date_card": {
                "ru": "10 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 10",
                "zh": "12æœˆ10æ—¥"
            },
            "hash": "046676af13bd3252",
            "authors": [
                "Sahal Shaji Mullappilly",
                "Mohammed Irfan Kurpath",
                "Sara Pieri",
                "Saeed Yahya Alseiari",
                "Shanavas Cholakkal",
                "Khaled Aldahmani",
                "Fahad Khan",
                "Rao Anwer",
                "Salman Khan",
                "Timothy Baldwin",
                "Hisham Cholakkal"
            ],
            "affiliations": [
                "Govt Medical College Kozhikode",
                "Linkoping University",
                "Mohamed Bin Zayed University of Artificial Intelligence (MBZUAI)",
                "Shaikh Tahnoon bin Mohammed Medical City (STMC)",
                "Sheikh Shakhbout Medical City (SSMC)",
                "Tawam Hospital"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.07769.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#architecture",
                    "#benchmark",
                    "#open_source",
                    "#science",
                    "#healthcare",
                    "#machine_translation",
                    "#multimodal"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¼ Ğ˜Ğ˜: Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ BiMediX2",
                    "desc": "BiMediX2 - ÑÑ‚Ğ¾ Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ğ°Ñ (Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¾-Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ°Ñ) Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ±Ğ¸Ğ¾Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Llama3.1 Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ğ¾Ğ¼ Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 1,6 Ğ¼Ğ»Ğ½ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. BiMediX2 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº BiMed-MBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "BiMediX2: Bridging Bilingual Medical Understanding with Multimodal Intelligence",
                    "desc": "This paper presents BiMediX2, a bilingual large multimodal model designed for biomedical applications, capable of processing both text and images in Arabic and English. It utilizes the Llama3.1 architecture to enhance interactions in medical contexts, allowing for complex conversations that involve medical imagery. The model is trained on a substantial bilingual dataset of 1.6 million samples, achieving superior performance on various medical benchmarks, particularly in multimodal evaluations. BiMediX2 not only surpasses existing models like GPT-4 in accuracy but also sets new standards for bilingual medical language models."
                },
                "zh": {
                    "title": "åŒè¯­ç”Ÿç‰©åŒ»å­¦æ¨¡å‹çš„åˆ›æ–°çªç ´",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†BiMediX2ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒè¯­ï¼ˆé˜¿æ‹‰ä¼¯è¯­-è‹±è¯­ï¼‰ç”Ÿç‰©åŒ»å­¦ä¸“å®¶å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰ï¼Œå…·æœ‰ç»Ÿä¸€æ¶æ„ï¼Œèƒ½å¤Ÿæ•´åˆæ–‡æœ¬å’Œè§†è§‰æ¨¡æ€ï¼Œä»è€Œå®ç°é«˜çº§å›¾åƒç†è§£å’ŒåŒ»ç–—åº”ç”¨ã€‚BiMediX2åˆ©ç”¨Llama3.1æ¶æ„ï¼Œæ”¯æŒé˜¿æ‹‰ä¼¯è¯­å’Œè‹±è¯­çš„æ— ç¼äº¤äº’ï¼Œèƒ½å¤Ÿå¤„ç†åŸºäºæ–‡æœ¬çš„è¾“å…¥å’Œæ¶‰åŠåŒ»ç–—å›¾åƒçš„å¤šè½®å¯¹è¯ã€‚è¯¥æ¨¡å‹åœ¨ä¸€ä¸ªåŒ…å«160ä¸‡æ ·æœ¬çš„åŒè¯­åŒ»ç–—æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæ¶µç›–äº†å¤šç§åŒ»ç–—äº¤äº’ï¼Œæ–‡æœ¬å’Œå›¾åƒæ¨¡æ€æ··åˆã€‚BiMediX2åœ¨å¤šä¸ªåŒ»ç–—åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶åœ¨è‹±è¯­å’Œé˜¿æ‹‰ä¼¯è¯­çš„å¤šæ¨¡æ€åŒ»ç–—è¯„ä¼°ä¸­ï¼Œåˆ†åˆ«æé«˜äº†è¶…è¿‡9%å’Œ20%çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.10047",
            "title": "Large Action Models: From Inception to Implementation",
            "url": "https://huggingface.co/papers/2412.10047",
            "abstract": "As AI continues to advance, there is a growing demand for systems that go beyond language-based assistance and move toward intelligent agents capable of performing real-world actions. This evolution requires the transition from traditional Large Language Models (LLMs), which excel at generating textual responses, to Large Action Models (LAMs), designed for action generation and execution within dynamic environments. Enabled by agent systems, LAMs hold the potential to transform AI from passive language understanding to active task completion, marking a significant milestone in the progression toward artificial general intelligence.   In this paper, we present a comprehensive framework for developing LAMs, offering a systematic approach to their creation, from inception to deployment. We begin with an overview of LAMs, highlighting their unique characteristics and delineating their differences from LLMs. Using a Windows OS-based agent as a case study, we provide a detailed, step-by-step guide on the key stages of LAM development, including data collection, model training, environment integration, grounding, and evaluation. This generalizable workflow can serve as a blueprint for creating functional LAMs in various application domains. We conclude by identifying the current limitations of LAMs and discussing directions for future research and industrial deployment, emphasizing the challenges and opportunities that lie ahead in realizing the full potential of LAMs in real-world applications.   The code for the data collection process utilized in this paper is publicly available at: https://github.com/microsoft/UFO/tree/main/dataflow, and comprehensive documentation can be found at https://microsoft.github.io/UFO/dataflow/overview/.",
            "score": 17,
            "issue_id": 1135,
            "pub_date": "2024-12-13",
            "pub_date_card": {
                "ru": "13 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 13",
                "zh": "12æœˆ13æ—¥"
            },
            "hash": "be65080464153291",
            "authors": [
                "Lu Wang",
                "Fangkai Yang",
                "Chaoyun Zhang",
                "Junting Lu",
                "Jiaxu Qian",
                "Shilin He",
                "Pu Zhao",
                "Bo Qiao",
                "Ray Huang",
                "Si Qin",
                "Qisheng Su",
                "Jiayi Ye",
                "Yudi Zhang",
                "Jian-Guang Lou",
                "Qingwei Lin",
                "Saravan Rajmohan",
                "Dongmei Zhang",
                "Qi Zhang"
            ],
            "affiliations": [
                "Eindhoven University of Technology",
                "Microsoft",
                "Peking University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.10047.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#data",
                    "#training",
                    "#open_source",
                    "#agi"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞÑ‚ ÑĞ»Ğ¾Ğ² Ğº Ğ´ĞµĞ»Ñƒ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ¿ Ğ² Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ ĞšÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… ĞœĞ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ”ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ (LAM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ²Ğ°Ğ½Ñ‹ Ğ¿ĞµÑ€ĞµĞ²ĞµÑÑ‚Ğ¸ Ğ˜Ğ˜ Ğ¾Ñ‚ Ğ¿Ğ°ÑÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ° Ğº Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ LAM, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ñ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ Windows OS Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ LAM. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ĞµÑ‚ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ LAM Ğ¸ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "From Language to Action: Advancing AI with Large Action Models",
                    "desc": "This paper discusses the shift from Large Language Models (LLMs) to Large Action Models (LAMs), which are designed to perform actions in real-world environments rather than just generating text. LAMs are enabled by agent systems and represent a step towards achieving artificial general intelligence by allowing AI to complete tasks actively. The authors present a framework for developing LAMs, detailing the stages from data collection to model training and evaluation, using a Windows OS-based agent as an example. They also address the limitations of current LAMs and suggest future research directions to enhance their capabilities in practical applications."
                },
                "zh": {
                    "title": "ä»è¯­è¨€ç†è§£åˆ°è¡ŒåŠ¨æ‰§è¡Œçš„æ™ºèƒ½è½¬å‹",
                    "desc": "éšç€äººå·¥æ™ºèƒ½çš„ä¸æ–­è¿›æ­¥ï¼Œå¸‚åœºå¯¹èƒ½å¤Ÿæ‰§è¡Œå®é™…æ“ä½œçš„æ™ºèƒ½ä»£ç†ç³»ç»Ÿçš„éœ€æ±‚æ—¥ç›Šå¢åŠ ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤§å‹è¡ŒåŠ¨æ¨¡å‹ï¼ˆLAMsï¼‰çš„ç»¼åˆæ¡†æ¶ï¼Œæ—¨åœ¨ä»ä¼ ç»Ÿçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è½¬å˜ä¸ºèƒ½å¤Ÿåœ¨åŠ¨æ€ç¯å¢ƒä¸­ç”Ÿæˆå’Œæ‰§è¡Œè¡ŒåŠ¨çš„æ¨¡å‹ã€‚æˆ‘ä»¬é€šè¿‡Windowsæ“ä½œç³»ç»Ÿçš„ä»£ç†ä½œä¸ºæ¡ˆä¾‹ï¼Œè¯¦ç»†ä»‹ç»äº†LAMå¼€å‘çš„å…³é”®é˜¶æ®µï¼ŒåŒ…æ‹¬æ•°æ®æ”¶é›†ã€æ¨¡å‹è®­ç»ƒã€ç¯å¢ƒé›†æˆå’Œè¯„ä¼°ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†LAMsçš„å½“å‰å±€é™æ€§ä»¥åŠæœªæ¥ç ”ç©¶å’Œå·¥ä¸šåº”ç”¨çš„æ–¹å‘ï¼Œå¼ºè°ƒäº†å®ç°LAMsåœ¨å®é™…åº”ç”¨ä¸­æ½œåŠ›çš„æŒ‘æˆ˜ä¸æœºé‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.09283",
            "title": "InstanceCap: Improving Text-to-Video Generation via Instance-aware Structured Caption",
            "url": "https://huggingface.co/papers/2412.09283",
            "abstract": "Text-to-video generation has evolved rapidly in recent years, delivering remarkable results. Training typically relies on video-caption paired data, which plays a crucial role in enhancing generation performance. However, current video captions often suffer from insufficient details, hallucinations and imprecise motion depiction, affecting the fidelity and consistency of generated videos. In this work, we propose a novel instance-aware structured caption framework, termed InstanceCap, to achieve instance-level and fine-grained video caption for the first time. Based on this scheme, we design an auxiliary models cluster to convert original video into instances to enhance instance fidelity. Video instances are further used to refine dense prompts into structured phrases, achieving concise yet precise descriptions. Furthermore, a 22K InstanceVid dataset is curated for training, and an enhancement pipeline that tailored to InstanceCap structure is proposed for inference. Experimental results demonstrate that our proposed InstanceCap significantly outperform previous models, ensuring high fidelity between captions and videos while reducing hallucinations.",
            "score": 16,
            "issue_id": 1137,
            "pub_date": "2024-12-12",
            "pub_date_card": {
                "ru": "12 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 12",
                "zh": "12æœˆ12æ—¥"
            },
            "hash": "8a8c6d346689077b",
            "authors": [
                "Tiehan Fan",
                "Kepan Nan",
                "Rui Xie",
                "Penghao Zhou",
                "Zhenheng Yang",
                "Chaoyou Fu",
                "Xiang Li",
                "Jian Yang",
                "Ying Tai"
            ],
            "affiliations": [
                "ByteDance",
                "Nanjing University",
                "Nankai University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.09283.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#inference",
                    "#diffusion",
                    "#data",
                    "#video",
                    "#dataset",
                    "#multimodal",
                    "#hallucinations"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "InstanceCap: Ñ‚Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ InstanceCap. Ğ­Ñ‚Ğ° Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾. InstanceCap Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ InstanceVid Ğ¸Ğ· 22 Ñ‚Ñ‹ÑÑÑ‡ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Enhancing Video Generation with Instance-Level Captions",
                    "desc": "This paper introduces InstanceCap, a new framework for generating detailed video captions that improve the quality of text-to-video generation. It addresses common issues in existing video captions, such as lack of detail and inaccuracies in motion representation. By focusing on instance-level descriptions, InstanceCap enhances the fidelity of video generation through a structured captioning approach. The authors also present a new dataset, InstanceVid, and an enhancement pipeline that together improve the alignment between video content and generated captions, leading to better overall performance."
                },
                "zh": {
                    "title": "å®ä¾‹æ„ŸçŸ¥ï¼Œæå‡è§†é¢‘ç”Ÿæˆè´¨é‡",
                    "desc": "è¿‘å¹´æ¥ï¼Œæ–‡æœ¬åˆ°è§†é¢‘ç”ŸæˆæŠ€æœ¯è¿…é€Ÿå‘å±•ï¼Œå–å¾—äº†æ˜¾è‘—æˆæœã€‚è®­ç»ƒé€šå¸¸ä¾èµ–äºè§†é¢‘å’Œå­—å¹•é…å¯¹çš„æ•°æ®ï¼Œè¿™å¯¹æé«˜ç”Ÿæˆæ€§èƒ½è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è§†é¢‘å­—å¹•å¾€å¾€ç¼ºä¹ç»†èŠ‚ï¼Œå­˜åœ¨å¹»è§‰å’Œä¸ç²¾ç¡®çš„è¿åŠ¨æç»˜ï¼Œå½±å“ç”Ÿæˆè§†é¢‘çš„çœŸå®æ„Ÿå’Œä¸€è‡´æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„å®ä¾‹æ„ŸçŸ¥ç»“æ„åŒ–å­—å¹•æ¡†æ¶ï¼Œç§°ä¸ºInstanceCapï¼Œé¦–æ¬¡å®ç°äº†å®ä¾‹çº§å’Œç»†ç²’åº¦çš„è§†é¢‘å­—å¹•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.09626",
            "title": "FreeScale: Unleashing the Resolution of Diffusion Models via Tuning-Free Scale Fusion",
            "url": "https://huggingface.co/papers/2412.09626",
            "abstract": "Visual diffusion models achieve remarkable progress, yet they are typically trained at limited resolutions due to the lack of high-resolution data and constrained computation resources, hampering their ability to generate high-fidelity images or videos at higher resolutions. Recent efforts have explored tuning-free strategies to exhibit the untapped potential higher-resolution visual generation of pre-trained models. However, these methods are still prone to producing low-quality visual content with repetitive patterns. The key obstacle lies in the inevitable increase in high-frequency information when the model generates visual content exceeding its training resolution, leading to undesirable repetitive patterns deriving from the accumulated errors. To tackle this challenge, we propose FreeScale, a tuning-free inference paradigm to enable higher-resolution visual generation via scale fusion. Specifically, FreeScale processes information from different receptive scales and then fuses it by extracting desired frequency components. Extensive experiments validate the superiority of our paradigm in extending the capabilities of higher-resolution visual generation for both image and video models. Notably, compared with the previous best-performing method, FreeScale unlocks the generation of 8k-resolution images for the first time.",
            "score": 11,
            "issue_id": 1135,
            "pub_date": "2024-12-12",
            "pub_date_card": {
                "ru": "12 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 12",
                "zh": "12æœˆ12æ—¥"
            },
            "hash": "1551e9966255aa0a",
            "authors": [
                "Haonan Qiu",
                "Shiwei Zhang",
                "Yujie Wei",
                "Ruihang Chu",
                "Hangjie Yuan",
                "Xiang Wang",
                "Yingya Zhang",
                "Ziwei Liu"
            ],
            "affiliations": [
                "Alibaba Group",
                "Fudan University",
                "Nanyang Technological University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.09626.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#inference",
                    "#diffusion",
                    "#video",
                    "#cv"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "FreeScale: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° ÑĞ²ĞµÑ€Ñ…Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ FreeScale - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¿Ğ¾ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ÑÑÑ‰Ğ¸Ñ…ÑÑ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² Ğ¿Ñ€Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¾Ğ² Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ½ÑƒĞ¶Ğ½Ñ‹Ñ… Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ FreeScale Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼ 8K."
                },
                "en": {
                    "title": "Unlocking High-Resolution Visuals with FreeScale",
                    "desc": "This paper introduces FreeScale, a new method for generating high-resolution images and videos using visual diffusion models. Traditional models struggle with high-resolution outputs due to limited training data and computational resources, often resulting in low-quality visuals with repetitive patterns. FreeScale addresses this by employing a tuning-free inference approach that fuses information from various scales, allowing the model to better handle high-frequency details. Experimental results demonstrate that FreeScale significantly enhances the quality of generated visuals, achieving 8k-resolution outputs for the first time."
                },
                "zh": {
                    "title": "FreeScaleï¼šæ— è°ƒä¼˜çš„é«˜åˆ†è¾¨ç‡è§†è§‰ç”Ÿæˆæ–°èŒƒå¼",
                    "desc": "è§†è§‰æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜ä¿çœŸå›¾åƒæˆ–è§†é¢‘æ—¶é¢ä¸´åˆ†è¾¨ç‡é™åˆ¶çš„é—®é¢˜ï¼Œä¸»è¦æ˜¯ç”±äºç¼ºä¹é«˜åˆ†è¾¨ç‡æ•°æ®å’Œè®¡ç®—èµ„æºã€‚æœ€è¿‘çš„ç ”ç©¶å°è¯•äº†æ— è°ƒä¼˜ç­–ç•¥ï¼Œä»¥å±•ç¤ºé¢„è®­ç»ƒæ¨¡å‹åœ¨é«˜åˆ†è¾¨ç‡è§†è§‰ç”Ÿæˆæ–¹é¢çš„æ½œåŠ›ï¼Œä½†ä»ç„¶å®¹æ˜“äº§ç”Ÿä½è´¨é‡çš„è§†è§‰å†…å®¹å’Œé‡å¤æ¨¡å¼ã€‚æˆ‘ä»¬æå‡ºäº†FreeScaleï¼Œè¿™æ˜¯ä¸€ç§æ— è°ƒä¼˜æ¨ç†èŒƒå¼ï¼Œé€šè¿‡å°ºåº¦èåˆå®ç°æ›´é«˜åˆ†è¾¨ç‡çš„è§†è§‰ç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFreeScaleåœ¨å›¾åƒå’Œè§†é¢‘æ¨¡å‹çš„é«˜åˆ†è¾¨ç‡ç”Ÿæˆèƒ½åŠ›ä¸Šä¼˜äºä»¥å¾€çš„æ–¹æ³•ï¼Œé¦–æ¬¡å®ç°äº†8kåˆ†è¾¨ç‡å›¾åƒçš„ç”Ÿæˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.08645",
            "title": "ObjectMate: A Recurrence Prior for Object Insertion and Subject-Driven Generation",
            "url": "https://huggingface.co/papers/2412.08645",
            "abstract": "This paper introduces a tuning-free method for both object insertion and subject-driven generation. The task involves composing an object, given multiple views, into a scene specified by either an image or text. Existing methods struggle to fully meet the task's challenging objectives: (i) seamlessly composing the object into the scene with photorealistic pose and lighting, and (ii) preserving the object's identity. We hypothesize that achieving these goals requires large scale supervision, but manually collecting sufficient data is simply too expensive. The key observation in this paper is that many mass-produced objects recur across multiple images of large unlabeled datasets, in different scenes, poses, and lighting conditions. We use this observation to create massive supervision by retrieving sets of diverse views of the same object. This powerful paired dataset enables us to train a straightforward text-to-image diffusion architecture to map the object and scene descriptions to the composited image. We compare our method, ObjectMate, with state-of-the-art methods for object insertion and subject-driven generation, using a single or multiple references. Empirically, ObjectMate achieves superior identity preservation and more photorealistic composition. Differently from many other multi-reference methods, ObjectMate does not require slow test-time tuning.",
            "score": 8,
            "issue_id": 1142,
            "pub_date": "2024-12-11",
            "pub_date_card": {
                "ru": "11 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 11",
                "zh": "12æœˆ11æ—¥"
            },
            "hash": "3a06f3f96c756398",
            "authors": [
                "Daniel Winter",
                "Asaf Shul",
                "Matan Cohen",
                "Dana Berman",
                "Yael Pritch",
                "Alex Rav-Acha",
                "Yedid Hoshen"
            ],
            "affiliations": [
                "Google",
                "The Hebrew University of Jerusalem"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.08645.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#training",
                    "#dataset",
                    "#multimodal",
                    "#optimization",
                    "#cv"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ¤Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ°Ñ Ğ²ÑÑ‚Ğ°Ğ²ĞºĞ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ´Ğ»Ñ Ğ²ÑÑ‚Ğ°Ğ²ĞºĞ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñƒ. ĞœĞµÑ‚Ğ¾Ğ´ ObjectMate Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ´Ğ°Ğ¼Ğ¸ Ğ¾Ğ´Ğ½Ğ¸Ñ… Ğ¸ Ñ‚ĞµÑ… Ğ¶Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. ObjectMate Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ğ»Ğ³Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ ĞµĞ³Ğ¾ Ğ¾Ñ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ñ… Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¾Ğ¿Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "ObjectMate: Seamless Object Insertion Without Tuning",
                    "desc": "This paper presents ObjectMate, a novel method for object insertion and subject-driven image generation without the need for tuning. It addresses the challenges of seamlessly integrating objects into scenes while maintaining their identity and achieving photorealistic results. The authors leverage large unlabeled datasets to create a diverse set of views for mass-produced objects, which serves as a powerful source of supervision. By employing a text-to-image diffusion architecture, ObjectMate outperforms existing methods in both identity preservation and composition quality, all without requiring time-consuming adjustments during testing."
                },
                "zh": {
                    "title": "æ— è°ƒä¼˜çš„ç‰©ä½“æ’å…¥ä¸ç”Ÿæˆæ–¹æ³•",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ— éœ€è°ƒä¼˜çš„æ–¹æ³•ï¼Œç”¨äºç‰©ä½“æ’å…¥å’ŒåŸºäºä¸»é¢˜çš„ç”Ÿæˆã€‚è¯¥ä»»åŠ¡æ¶‰åŠå°†å¤šä¸ªè§†è§’çš„ç‰©ä½“åˆæˆåˆ°ç”±å›¾åƒæˆ–æ–‡æœ¬æŒ‡å®šçš„åœºæ™¯ä¸­ã€‚ç°æœ‰æ–¹æ³•åœ¨å®ç°æ— ç¼åˆæˆç‰©ä½“ã€ä¿æŒçœŸå®çš„å§¿æ€å’Œå…‰ç…§æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºçš„ObjectMateæ–¹æ³•é€šè¿‡åˆ©ç”¨å¤§è§„æ¨¡æœªæ ‡è®°æ•°æ®é›†ä¸­é‡å¤å‡ºç°çš„ç‰©ä½“è§†è§’ï¼Œåˆ›å»ºäº†å¼ºå¤§çš„é…å¯¹æ•°æ®é›†ï¼Œä»è€Œå®ç°äº†æ›´å¥½çš„èº«ä»½ä¿ç•™å’Œæ›´é€¼çœŸçš„åˆæˆæ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.07517",
            "title": "FireFlow: Fast Inversion of Rectified Flow for Image Semantic Editing",
            "url": "https://huggingface.co/papers/2412.07517",
            "abstract": "Though Rectified Flows (ReFlows) with distillation offers a promising way for fast sampling, its fast inversion transforms images back to structured noise for recovery and following editing remains unsolved. This paper introduces FireFlow, a simple yet effective zero-shot approach that inherits the startling capacity of ReFlow-based models (such as FLUX) in generation while extending its capabilities to accurate inversion and editing in 8 steps. We first demonstrate that a carefully designed numerical solver is pivotal for ReFlow inversion, enabling accurate inversion and reconstruction with the precision of a second-order solver while maintaining the practical efficiency of a first-order Euler method. This solver achieves a 3times runtime speedup compared to state-of-the-art ReFlow inversion and editing techniques, while delivering smaller reconstruction errors and superior editing results in a training-free mode. The code is available at https://github.com/HolmesShuan/FireFlow{this URL}.",
            "score": 7,
            "issue_id": 1135,
            "pub_date": "2024-12-10",
            "pub_date_card": {
                "ru": "10 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 10",
                "zh": "12æœˆ10æ—¥"
            },
            "hash": "70f97a4533ea4ebb",
            "authors": [
                "Yingying Deng",
                "Xiangyu He",
                "Changwang Mei",
                "Peisong Wang",
                "Fan Tang"
            ],
            "affiliations": [
                "Institute of Automation, Chinese Academy of Sciences, Beijing, China",
                "Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.07517.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#open_source",
                    "#cv",
                    "#architecture"
                ],
                "emoji": "ğŸ”¥",
                "ru": {
                    "title": "FireFlow: ĞœĞ¾Ğ»Ğ½Ğ¸ĞµĞ½Ğ¾ÑĞ½Ğ°Ñ Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ¸Ñ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² ReFlow Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ FireFlow - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Rectified Flows (ReFlows). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¸Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ·Ğ° 8 ÑˆĞ°Ğ³Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ FireFlow Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² 3 Ñ€Ğ°Ğ·Ğ° Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ñ‚ĞµÑ…Ğ½Ğ¸Ğº Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ¸Ğ¸ ReFlow, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ’Ğ°Ğ¶Ğ½Ğ¾ Ğ¾Ñ‚Ğ¼ĞµÑ‚Ğ¸Ñ‚ÑŒ, Ñ‡Ñ‚Ğ¾ FireFlow Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒÑÑ Ğº ÑƒĞ¶Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼."
                },
                "en": {
                    "title": "FireFlow: Fast and Accurate Image Inversion and Editing",
                    "desc": "This paper presents FireFlow, a new method that improves the inversion and editing capabilities of Rectified Flows (ReFlows) in machine learning. FireFlow uses a specially designed numerical solver that combines the efficiency of first-order methods with the accuracy of second-order methods, allowing for faster and more precise image reconstruction. The approach achieves a threefold increase in speed compared to existing ReFlow techniques while reducing errors and enhancing editing quality without requiring additional training. Overall, FireFlow enhances the usability of ReFlow models for generating and manipulating images effectively."
                },
                "zh": {
                    "title": "FireFlowï¼šé«˜æ•ˆçš„å›¾åƒåæ¼”ä¸ç¼–è¾‘æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºFireFlowçš„æ–°æ–¹æ³•ï¼Œå®ƒåœ¨å¿«é€Ÿé‡‡æ ·çš„åŸºç¡€ä¸Šï¼Œè§£å†³äº†å›¾åƒåæ¼”å’Œç¼–è¾‘çš„é—®é¢˜ã€‚FireFlowç»§æ‰¿äº†åŸºäºReFlowæ¨¡å‹çš„å¼ºå¤§ç”Ÿæˆèƒ½åŠ›ï¼Œå¹¶åœ¨8ä¸ªæ­¥éª¤å†…å®ç°äº†å‡†ç¡®çš„åæ¼”å’Œç¼–è¾‘ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ•°å€¼æ±‚è§£å™¨ï¼Œä½¿å¾—ReFlowçš„åæ¼”è¿‡ç¨‹æ›´åŠ ç²¾ç¡®ï¼ŒåŒæ—¶ä¿æŒäº†é«˜æ•ˆæ€§ã€‚ä¸ç°æœ‰çš„ReFlowåæ¼”å’Œç¼–è¾‘æŠ€æœ¯ç›¸æ¯”ï¼Œè¯¥æ±‚è§£å™¨åœ¨è¿è¡Œé€Ÿåº¦ä¸Šæé«˜äº†3å€ï¼Œå¹¶ä¸”åœ¨è®­ç»ƒæ— å…³çš„æ¨¡å¼ä¸‹ï¼Œé‡å»ºè¯¯å·®æ›´å°ï¼Œç¼–è¾‘æ•ˆæœæ›´ä½³ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.09428",
            "title": "Multimodal Music Generation with Explicit Bridges and Retrieval Augmentation",
            "url": "https://huggingface.co/papers/2412.09428",
            "abstract": "Multimodal music generation aims to produce music from diverse input modalities, including text, videos, and images. Existing methods use a common embedding space for multimodal fusion. Despite their effectiveness in other modalities, their application in multimodal music generation faces challenges of data scarcity, weak cross-modal alignment, and limited controllability. This paper addresses these issues by using explicit bridges of text and music for multimodal alignment. We introduce a novel method named Visuals Music Bridge (VMB). Specifically, a Multimodal Music Description Model converts visual inputs into detailed textual descriptions to provide the text bridge; a Dual-track Music Retrieval module that combines broad and targeted retrieval strategies to provide the music bridge and enable user control. Finally, we design an Explicitly Conditioned Music Generation framework to generate music based on the two bridges. We conduct experiments on video-to-music, image-to-music, text-to-music, and controllable music generation tasks, along with experiments on controllability. The results demonstrate that VMB significantly enhances music quality, modality, and customization alignment compared to previous methods. VMB sets a new standard for interpretable and expressive multimodal music generation with applications in various multimedia fields. Demos and code are available at https://github.com/wbs2788/VMB.",
            "score": 5,
            "issue_id": 1139,
            "pub_date": "2024-12-12",
            "pub_date_card": {
                "ru": "12 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 12",
                "zh": "12æœˆ12æ—¥"
            },
            "hash": "3887e6d6a5eecb26",
            "authors": [
                "Baisen Wang",
                "Le Zhuo",
                "Zhaokai Wang",
                "Chenxi Bao",
                "Wu Chengjing",
                "Xuecheng Nie",
                "Jiao Dai",
                "Jizhong Han",
                "Yue Liao",
                "Si Liu"
            ],
            "affiliations": [
                "Beihang University",
                "Institute of Information Engineering, Chinese Academy of Sciences",
                "MT Lab, Meitu Inc.",
                "School of Cyberspace Security, University of Chinese Academy of Sciences",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "University of Edinburgh"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.09428.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#audio"
                ],
                "emoji": "ğŸ¼",
                "ru": {
                    "title": "VMB: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Visuals Music Bridge (VMB). VMB Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾ÑÑ‚Ñ‹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¸ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¾Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸, Ğ´Ğ²ÑƒÑ…Ñ‚Ñ€ĞµĞºĞ¾Ğ²Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸ Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸ Ñ ÑĞ²Ğ½Ñ‹Ğ¼Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑĞ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ VMB Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸, Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Bridging Modalities for Enhanced Music Generation",
                    "desc": "This paper presents a new approach to multimodal music generation, which creates music from different types of inputs like text, images, and videos. The authors introduce the Visuals Music Bridge (VMB) method, which improves the alignment between these modalities by using explicit connections between text and music. They develop a Multimodal Music Description Model to transform visual inputs into text descriptions and a Dual-track Music Retrieval module to enhance user control over the music generation process. Experimental results show that VMB significantly improves the quality and customization of generated music compared to existing methods, making it a valuable tool for multimedia applications."
                },
                "zh": {
                    "title": "è§†è§‰éŸ³ä¹æ¡¥ï¼šå¤šæ¨¡æ€éŸ³ä¹ç”Ÿæˆçš„æ–°æ ‡å‡†",
                    "desc": "å¤šæ¨¡æ€éŸ³ä¹ç”Ÿæˆæ—¨åœ¨ä»å¤šç§è¾“å…¥æ¨¡æ€ï¼ˆå¦‚æ–‡æœ¬ã€è§†é¢‘å’Œå›¾åƒï¼‰ä¸­ç”ŸæˆéŸ³ä¹ã€‚ç°æœ‰æ–¹æ³•ä½¿ç”¨å…±åŒçš„åµŒå…¥ç©ºé—´è¿›è¡Œå¤šæ¨¡æ€èåˆï¼Œä½†åœ¨å¤šæ¨¡æ€éŸ³ä¹ç”Ÿæˆä¸­é¢ä¸´æ•°æ®ç¨€ç¼ºã€è·¨æ¨¡æ€å¯¹é½å¼±å’Œå¯æ§æ€§æœ‰é™ç­‰æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œç§°ä¸ºè§†è§‰éŸ³ä¹æ¡¥ï¼ˆVMBï¼‰ï¼Œé€šè¿‡æ–‡æœ¬å’ŒéŸ³ä¹çš„æ˜¾å¼æ¡¥æ¢æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVMBåœ¨éŸ³ä¹è´¨é‡ã€æ¨¡æ€å’Œå®šåˆ¶å¯¹é½æ–¹é¢æ˜¾è‘—ä¼˜äºä¹‹å‰çš„æ–¹æ³•ï¼Œè®¾å®šäº†å¯è§£é‡Šå’Œå¯Œæœ‰è¡¨ç°åŠ›çš„å¤šæ¨¡æ€éŸ³ä¹ç”Ÿæˆçš„æ–°æ ‡å‡†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.09611",
            "title": "FluxSpace: Disentangled Semantic Editing in Rectified Flow Transformers",
            "url": "https://huggingface.co/papers/2412.09611",
            "abstract": "Rectified flow models have emerged as a dominant approach in image generation, showcasing impressive capabilities in high-quality image synthesis. However, despite their effectiveness in visual generation, rectified flow models often struggle with disentangled editing of images. This limitation prevents the ability to perform precise, attribute-specific modifications without affecting unrelated aspects of the image. In this paper, we introduce FluxSpace, a domain-agnostic image editing method leveraging a representation space with the ability to control the semantics of images generated by rectified flow transformers, such as Flux. By leveraging the representations learned by the transformer blocks within the rectified flow models, we propose a set of semantically interpretable representations that enable a wide range of image editing tasks, from fine-grained image editing to artistic creation. This work offers a scalable and effective image editing approach, along with its disentanglement capabilities.",
            "score": 4,
            "issue_id": 1143,
            "pub_date": "2024-12-12",
            "pub_date_card": {
                "ru": "12 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 12",
                "zh": "12æœˆ12æ—¥"
            },
            "hash": "a5d955bf540c4f06",
            "authors": [
                "Yusuf Dalva",
                "Kavana Venkatesh",
                "Pinar Yanardag"
            ],
            "affiliations": [
                "Virginia Tech"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.09611.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#diffusion",
                    "#3d"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "FluxSpace: ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€ĞµĞºÑ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ FluxSpace - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€ĞµĞºÑ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ°. Ğ­Ñ‚Ğ° Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºÑƒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸Ğ· Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ±Ğ»Ğ¾ĞºĞ¾Ğ². FluxSpace Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¹ ÑĞ¿ĞµĞºÑ‚Ñ€ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ - Ğ¾Ñ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ´Ğ¾ Ñ…ÑƒĞ´Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ğ²Ğ¾Ñ€Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ÑÑ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğº Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²."
                },
                "en": {
                    "title": "FluxSpace: Precision Editing in Image Generation",
                    "desc": "This paper presents FluxSpace, a new method for image editing that improves upon rectified flow models, which are known for generating high-quality images. While these models excel at creating images, they often fail to allow for specific edits without altering other unrelated features. FluxSpace addresses this issue by utilizing a representation space that enables precise control over the semantics of the images. The method leverages the learned representations from transformer blocks in rectified flow models, facilitating a variety of editing tasks, from detailed adjustments to creative artistic modifications."
                },
                "zh": {
                    "title": "FluxSpaceï¼šè§£è€¦å›¾åƒç¼–è¾‘çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºFluxSpaceçš„å›¾åƒç¼–è¾‘æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰çš„ä¿®æ­£æµæ¨¡å‹åœ¨å›¾åƒç¼–è¾‘ä¸­çš„å±€é™æ€§ã€‚å°½ç®¡ä¿®æ­£æµæ¨¡å‹åœ¨é«˜è´¨é‡å›¾åƒåˆæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬åœ¨è¿›è¡Œå›¾åƒçš„è§£è€¦ç¼–è¾‘æ—¶å¸¸å¸¸é‡åˆ°å›°éš¾ã€‚FluxSpaceåˆ©ç”¨ä¿®æ­£æµå˜æ¢å™¨ä¸­å­¦ä¹ åˆ°çš„è¡¨ç¤ºï¼Œæä¾›äº†ä¸€ç§è¯­ä¹‰å¯è§£é‡Šçš„è¡¨ç¤ºç©ºé—´ï¼Œä½¿å¾—ç”¨æˆ·èƒ½å¤Ÿè¿›è¡Œç²¾ç¡®çš„å±æ€§ç‰¹å®šä¿®æ”¹ã€‚è¯¥æ–¹æ³•ä¸ä»…æ”¯æŒç»†ç²’åº¦çš„å›¾åƒç¼–è¾‘ï¼Œè¿˜èƒ½å®ç°è‰ºæœ¯åˆ›ä½œï¼Œå±•ç°äº†è‰¯å¥½çš„å¯æ‰©å±•æ€§å’Œæœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.09856",
            "title": "LinGen: Towards High-Resolution Minute-Length Text-to-Video Generation with Linear Computational Complexity",
            "url": "https://huggingface.co/papers/2412.09856",
            "abstract": "Text-to-video generation enhances content creation but is highly computationally intensive: The computational cost of Diffusion Transformers (DiTs) scales quadratically in the number of pixels. This makes minute-length video generation extremely expensive, limiting most existing models to generating videos of only 10-20 seconds length. We propose a Linear-complexity text-to-video Generation (LinGen) framework whose cost scales linearly in the number of pixels. For the first time, LinGen enables high-resolution minute-length video generation on a single GPU without compromising quality. It replaces the computationally-dominant and quadratic-complexity block, self-attention, with a linear-complexity block called MATE, which consists of an MA-branch and a TE-branch. The MA-branch targets short-to-long-range correlations, combining a bidirectional Mamba2 block with our token rearrangement method, Rotary Major Scan, and our review tokens developed for long video generation. The TE-branch is a novel TEmporal Swin Attention block that focuses on temporal correlations between adjacent tokens and medium-range tokens. The MATE block addresses the adjacency preservation issue of Mamba and improves the consistency of generated videos significantly. Experimental results show that LinGen outperforms DiT (with a 75.6% win rate) in video quality with up to 15times (11.5times) FLOPs (latency) reduction. Furthermore, both automatic metrics and human evaluation demonstrate our LinGen-4B yields comparable video quality to state-of-the-art models (with a 50.5%, 52.1%, 49.1% win rate with respect to Gen-3, LumaLabs, and Kling, respectively). This paves the way to hour-length movie generation and real-time interactive video generation. We provide 68s video generation results and more examples in our project website: https://lineargen.github.io/.",
            "score": 3,
            "issue_id": 1149,
            "pub_date": "2024-12-13",
            "pub_date_card": {
                "ru": "13 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 13",
                "zh": "12æœˆ13æ—¥"
            },
            "hash": "d981bcdb0d035c11",
            "authors": [
                "Hongjie Wang",
                "Chih-Yao Ma",
                "Yen-Cheng Liu",
                "Ji Hou",
                "Tao Xu",
                "Jialiang Wang",
                "Felix Juefei-Xu",
                "Yaqiao Luo",
                "Peizhao Zhang",
                "Tingbo Hou",
                "Peter Vajda",
                "Niraj K. Jha",
                "Xiaoliang Dai"
            ],
            "affiliations": [
                "Meta",
                "Princeton University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.09856.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#diffusion",
                    "#architecture",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "LinGen: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ LinGen. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ğ¸Ğ½Ğ¾Ğ¹ Ğ´Ğ¾ Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ GPU, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. LinGen Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ»Ğ¾Ğº MATE Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑĞ°Ğ¼Ğ¾Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½ÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LinGen Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ²."
                },
                "en": {
                    "title": "Revolutionizing Video Generation with Linear Complexity",
                    "desc": "This paper introduces LinGen, a new framework for text-to-video generation that significantly reduces computational costs. Unlike traditional methods that scale quadratically with pixel count, LinGen operates with linear complexity, allowing for the generation of high-resolution videos up to a minute long on a single GPU. The framework utilizes a novel MATE block, which combines two branches to effectively capture both short and long-range correlations in video data. Experimental results show that LinGen not only reduces latency but also maintains high video quality, outperforming existing models in various evaluations."
                },
                "zh": {
                    "title": "çº¿æ€§å¤æ‚åº¦ï¼Œåˆ†é’Ÿçº§è§†é¢‘ç”Ÿæˆæ–°çªç ´ï¼",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§çº¿æ€§å¤æ‚åº¦çš„æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆæ¡†æ¶LinGenï¼Œæ—¨åœ¨é™ä½ç”Ÿæˆè§†é¢‘çš„è®¡ç®—æˆæœ¬ã€‚ä¼ ç»Ÿçš„æ‰©æ•£å˜æ¢å™¨åœ¨ç”Ÿæˆè§†é¢‘æ—¶ï¼Œè®¡ç®—æˆæœ¬éšç€åƒç´ æ•°é‡çš„å¢åŠ è€Œå‘ˆå¹³æ–¹å¢é•¿ï¼Œé™åˆ¶äº†è§†é¢‘é•¿åº¦ã€‚LinGené€šè¿‡å¼•å…¥MATEæ¨¡å—ï¼Œæ›¿ä»£äº†è®¡ç®—å¯†é›†å‹çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œä½¿å¾—ç”Ÿæˆé«˜åˆ†è¾¨ç‡çš„åˆ†é’Ÿçº§è§†é¢‘æˆä¸ºå¯èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLinGenåœ¨è§†é¢‘è´¨é‡ä¸Šä¼˜äºç°æœ‰æ¨¡å‹ï¼Œå¹¶æ˜¾è‘—å‡å°‘äº†è®¡ç®—å»¶è¿Ÿï¼Œæ¨åŠ¨äº†é•¿æ—¶é—´è§†é¢‘ç”Ÿæˆçš„å¯èƒ½æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.10319",
            "title": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods",
            "url": "https://huggingface.co/papers/2412.10319",
            "abstract": "Long-context LLMs have enabled numerous downstream applications but also introduced significant challenges related to computational and memory efficiency. To address these challenges, optimizations for long-context inference have been developed, centered around the KV cache. However, existing benchmarks often evaluate in single-request, neglecting the full lifecycle of the KV cache in real-world use. This oversight is particularly critical, as KV cache reuse has become widely adopted in LLMs inference frameworks, such as vLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft, Google, and Anthropic. To address this gap, we introduce SCBench(SharedContextBench), a comprehensive benchmark for evaluating long-context methods from a KV cachecentric perspective: 1) KV cache generation, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache loading. Specifically, SCBench uses test examples with shared context, ranging 12 tasks with two shared context modes, covering four categories of long-context capabilities: string retrieval, semantic retrieval, global information, and multi-task. With it, we provide an extensive KV cache-centric analysis of eight categories long-context solutions, including Gated Linear RNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention, KV cache dropping, quantization, retrieval, loading, and prompt compression. The evaluation is conducted on 8 long-context LLMs. Our findings show that sub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding with O(n) memory and sub-O(n^2) pre-filling computation perform robustly. Dynamic sparsity yields more expressive KV caches than static patterns, and layer-level sparsity in hybrid architectures reduces memory usage with strong performance. Additionally, we identify attention distribution shift issues in long-generation scenarios. https://aka.ms/SCBench.",
            "score": 3,
            "issue_id": 1135,
            "pub_date": "2024-12-13",
            "pub_date_card": {
                "ru": "13 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 13",
                "zh": "12æœˆ13æ—¥"
            },
            "hash": "a6269882457435d4",
            "authors": [
                "Yucheng Li",
                "Huiqiang Jiang",
                "Qianhui Wu",
                "Xufang Luo",
                "Surin Ahn",
                "Chengruidong Zhang",
                "Amir H. Abdi",
                "Dongsheng Li",
                "Jianfeng Gao",
                "Yuqing Yang",
                "Lili Qiu"
            ],
            "affiliations": [
                "Microsoft Corporation",
                "University of Surrey"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.10319.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#long_context",
                    "#inference",
                    "#benchmark",
                    "#architecture"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "SCBench: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ğ·Ğ¼Ñƒ KV-ĞºÑÑˆĞ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº SCBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ Ğ°ĞºÑ†ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ½Ğ° KV-ĞºÑÑˆ. SCBench Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ, ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ, Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºÑƒ KV-ĞºÑÑˆĞ° Ğ½Ğ° 12 Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ¾Ğ±Ñ‰Ğ¸Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ 8 ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ° 8 ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ ÑÑƒĞ±ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ ÑƒÑÑ‚ÑƒĞ¿Ğ°ÑÑ‚ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…, Ğ° Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ñ€Ğ¾Ñ€ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ KV-ĞºÑÑˆĞ¸."
                },
                "en": {
                    "title": "Optimizing Long-Context LLMs with SCBench: A KV Cache Revolution",
                    "desc": "This paper introduces SCBench, a new benchmark designed to evaluate long-context methods in large language models (LLMs) with a focus on the key-value (KV) cache. It addresses the limitations of existing benchmarks by considering the entire lifecycle of the KV cache, including its generation, compression, retrieval, and loading. The study analyzes various long-context solutions, revealing that memory-efficient methods can struggle in multi-turn scenarios, while certain sparse encoding techniques perform well. The findings also highlight the importance of dynamic sparsity and layer-level sparsity in optimizing memory usage and performance in LLMs."
                },
                "zh": {
                    "title": "ä¼˜åŒ–é•¿ä¸Šä¸‹æ–‡çš„KVç¼“å­˜è¯„ä¼°",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†SCBenchï¼ˆSharedContextBenchï¼‰ï¼Œä¸€ä¸ªé’ˆå¯¹é•¿ä¸Šä¸‹æ–‡æ–¹æ³•çš„åŸºå‡†æµ‹è¯•ï¼Œé‡ç‚¹å…³æ³¨KVç¼“å­˜çš„ç”Ÿå‘½å‘¨æœŸã€‚ç ”ç©¶è¡¨æ˜ï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•å¾€å¾€åªå…³æ³¨å•æ¬¡è¯·æ±‚ï¼Œè€Œå¿½è§†äº†KVç¼“å­˜çš„é‡ç”¨ï¼Œè¿™åœ¨å®é™…åº”ç”¨ä¸­è‡³å…³é‡è¦ã€‚SCBenchæ¶µç›–äº†KVç¼“å­˜çš„ç”Ÿæˆã€å‹ç¼©ã€æ£€ç´¢å’ŒåŠ è½½ç­‰å››ä¸ªæ–¹é¢ï¼Œå¹¶é€šè¿‡12ä¸ªä»»åŠ¡çš„å…±äº«ä¸Šä¸‹æ–‡è¿›è¡Œè¯„ä¼°ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼ŒåŠ¨æ€ç¨€ç–æ€§åœ¨KVç¼“å­˜ä¸­è¡¨ç°æ›´å¥½ï¼Œè€Œæ··åˆæ¶æ„ä¸­çš„å±‚çº§ç¨€ç–æ€§åˆ™æœ‰æ•ˆé™ä½äº†å†…å­˜ä½¿ç”¨ï¼ŒåŒæ—¶ä¿æŒäº†å¼ºå¤§çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.08347",
            "title": "SmolTulu: Higher Learning Rate to Batch Size Ratios Can Lead to Better Reasoning in SLMs",
            "url": "https://huggingface.co/papers/2412.08347",
            "abstract": "We present SmolTulu-1.7b-Instruct, referenced in this report as SmolTulu-DPO-1130, an instruction-tuned language model that adapts AllenAI's Tulu 3 post-training pipeline to enhance Huggingface's SmolLM2-1.7B base model. Through comprehensive empirical analysis using a 135M parameter model, we demonstrate that the relationship between learning rate and batch size significantly impacts model performance in a task-dependent manner. Our findings reveal a clear split: reasoning tasks like ARC and GSM8K benefit from higher learning rate to batch size ratios, while pattern recognition tasks such as HellaSwag and IFEval show optimal performance with lower ratios. These insights informed the development of SmolTulu, which achieves state-of-the-art performance among sub-2B parameter models on instruction following, scoring 67.7% on IFEval (Delta11%), and mathematical reasoning with 51.6% on GSM8K (Delta3.4%), with an alternate version achieving scoring 57.1% on ARC (Delta5.4%). We release our model, training recipes, and ablation studies to facilitate further research in efficient model alignment, demonstrating that careful adaptation of optimization dynamics can help bridge the capability gap between small and large language models.",
            "score": 2,
            "issue_id": 1141,
            "pub_date": "2024-12-11",
            "pub_date_card": {
                "ru": "11 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 11",
                "zh": "12æœˆ11æ—¥"
            },
            "hash": "a7238338dc7e3853",
            "authors": [
                "Sultan Alrashed"
            ],
            "affiliations": [
                "Saudi Data & Artificial Intelligence Authority"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.08347.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#optimization",
                    "#open_source",
                    "#science",
                    "#architecture",
                    "#small_models",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ğ»Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ñ… ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ SmolTulu-1.7b-Instruct, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ğ°Ñ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ pipeline Tulu 3 Ğ¾Ñ‚ AllenAI Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ SmolLM2-1.7B Ğ¾Ñ‚ Huggingface. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğµ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ±Ğ°Ñ‚Ñ‡Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ²Ğ»Ğ¸ÑĞµÑ‚ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ”Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğµ ÑĞ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ, Ğ° Ğ´Ğ»Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² - Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ¸Ğ·ĞºĞ¸Ğµ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ»Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ SmolTulu, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³ÑˆĞ°Ñ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ ÑÑ€ĞµĞ´Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¾ 2 Ğ¼Ğ»Ñ€Ğ´ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Optimizing Learning Dynamics for Enhanced Language Model Performance",
                    "desc": "The paper introduces SmolTulu-1.7b-Instruct, an instruction-tuned language model that enhances the SmolLM2-1.7B base model using AllenAI's Tulu 3 post-training pipeline. It highlights the importance of the learning rate and batch size relationship, showing that different tasks require different optimization strategies for optimal performance. For reasoning tasks, a higher learning rate to batch size ratio is beneficial, while pattern recognition tasks perform better with lower ratios. The model achieves state-of-the-art results among sub-2B parameter models, providing valuable insights and resources for future research in model alignment and optimization."
                },
                "zh": {
                    "title": "ä¼˜åŒ–å°æ¨¡å‹ï¼Œæå‡å¤§èƒ½åŠ›",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†SmolTulu-1.7b-Instructï¼Œè¿™æ˜¯ä¸€ä¸ªç»è¿‡æŒ‡ä»¤è°ƒä¼˜çš„è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨æå‡Huggingfaceçš„SmolLM2-1.7BåŸºç¡€æ¨¡å‹çš„æ€§èƒ½ã€‚é€šè¿‡å¯¹135Må‚æ•°æ¨¡å‹çš„å…¨é¢å®è¯åˆ†æï¼Œæˆ‘ä»¬å‘ç°å­¦ä¹ ç‡ä¸æ‰¹é‡å¤§å°ä¹‹é—´çš„å…³ç³»å¯¹æ¨¡å‹æ€§èƒ½æœ‰æ˜¾è‘—å½±å“ï¼Œä¸”è¿™ç§å½±å“å› ä»»åŠ¡è€Œå¼‚ã€‚æ¨ç†ä»»åŠ¡å¦‚ARCå’ŒGSM8Kåœ¨è¾ƒé«˜çš„å­¦ä¹ ç‡ä¸æ‰¹é‡å¤§å°æ¯”ç‡ä¸‹è¡¨ç°æ›´å¥½ï¼Œè€Œæ¨¡å¼è¯†åˆ«ä»»åŠ¡å¦‚HellaSwagå’ŒIFEvalåˆ™åœ¨è¾ƒä½çš„æ¯”ç‡ä¸‹è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç ”ç©¶æˆæœä¸ºSmolTuluçš„å‘å±•æä¾›äº†æŒ‡å¯¼ï¼Œä½¿å…¶åœ¨æŒ‡ä»¤è·Ÿéšå’Œæ•°å­¦æ¨ç†ä»»åŠ¡ä¸­åœ¨å°äº2Bå‚æ•°æ¨¡å‹ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.10345",
            "title": "TraceVLA: Visual Trace Prompting Enhances Spatial-Temporal Awareness for Generalist Robotic Policies",
            "url": "https://huggingface.co/papers/2412.10345",
            "abstract": "Although large vision-language-action (VLA) models pretrained on extensive robot datasets offer promising generalist policies for robotic learning, they still struggle with spatial-temporal dynamics in interactive robotics, making them less effective in handling complex tasks, such as manipulation. In this work, we introduce visual trace prompting, a simple yet effective approach to facilitate VLA models' spatial-temporal awareness for action prediction by encoding state-action trajectories visually. We develop a new TraceVLA model by finetuning OpenVLA on our own collected dataset of 150K robot manipulation trajectories using visual trace prompting. Evaluations of TraceVLA across 137 configurations in SimplerEnv and 4 tasks on a physical WidowX robot demonstrate state-of-the-art performance, outperforming OpenVLA by 10% on SimplerEnv and 3.5x on real-robot tasks and exhibiting robust generalization across diverse embodiments and scenarios. To further validate the effectiveness and generality of our method, we present a compact VLA model based on 4B Phi-3-Vision, pretrained on the Open-X-Embodiment and finetuned on our dataset, rivals the 7B OpenVLA baseline while significantly improving inference efficiency.",
            "score": 1,
            "issue_id": 1148,
            "pub_date": "2024-12-13",
            "pub_date_card": {
                "ru": "13 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 13",
                "zh": "12æœˆ13æ—¥"
            },
            "hash": "cf176777ca0c3426",
            "authors": [
                "Ruijie Zheng",
                "Yongyuan Liang",
                "Shuaiyi Huang",
                "Jianfeng Gao",
                "Hal DaumÃ© III",
                "Andrey Kolobov",
                "Furong Huang",
                "Jianwei Yang"
            ],
            "affiliations": [
                "Microsoft Research",
                "University of Maryland, College Park"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.10345.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#robotics",
                    "#training",
                    "#dataset",
                    "#optimization",
                    "#cv"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°ÑÑĞ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ vision-language-action (VLA) Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ TraceVLA, Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ² OpenVLA Ğ½Ğ° ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 150 Ñ‚Ñ‹ÑÑÑ‡ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ². TraceVLA Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞ»Ğ° OpenVLA Ğ½Ğ° 10% Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğµ SimplerEnv Ğ¸ Ğ² 3,5 Ñ€Ğ°Ğ·Ğ° Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğµ WidowX. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ±Ñ‹Ğ»Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ° ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ VLA-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Phi-3-Vision, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ° Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ OpenVLA, Ğ½Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞµ."
                },
                "en": {
                    "title": "Enhancing Robotic Action Prediction with Visual Trace Prompting",
                    "desc": "This paper addresses the limitations of large vision-language-action (VLA) models in understanding spatial-temporal dynamics for robotic manipulation tasks. The authors propose a novel technique called visual trace prompting, which enhances the models' ability to predict actions by visually encoding state-action trajectories. They introduce the TraceVLA model, which is fine-tuned on a dataset of 150,000 robot manipulation trajectories, leading to significant performance improvements over the baseline OpenVLA model. The results show that TraceVLA achieves state-of-the-art performance in various environments and tasks, demonstrating its effectiveness and generalization capabilities in real-world robotic applications."
                },
                "zh": {
                    "title": "æå‡æœºå™¨äººæ“ä½œçš„æ—¶ç©ºæ„ŸçŸ¥èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºè§†è§‰è½¨è¿¹æç¤ºï¼Œæ—¨åœ¨æé«˜å¤§å‹è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹åœ¨æœºå™¨äººäº¤äº’ä¸­çš„æ—¶ç©ºæ„ŸçŸ¥èƒ½åŠ›ã€‚é€šè¿‡å¯¹çŠ¶æ€-åŠ¨ä½œè½¨è¿¹è¿›è¡Œè§†è§‰ç¼–ç ï¼Œæˆ‘ä»¬å¼€å‘äº†æ–°çš„TraceVLAæ¨¡å‹ï¼Œå¹¶åœ¨150Kæœºå™¨äººæ“ä½œè½¨è¿¹çš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¾®è°ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTraceVLAåœ¨å¤šä¸ªé…ç½®å’Œä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†OpenVLAæ¨¡å‹ï¼Œå°¤å…¶åœ¨çœŸå®æœºå™¨äººä»»åŠ¡ä¸­è¡¨ç°å‡º3.5å€çš„æå‡ã€‚æˆ‘ä»¬çš„ç´§å‡‘å‹VLAæ¨¡å‹åœ¨æ¨ç†æ•ˆç‡ä¸Šä¹Ÿæœ‰æ˜¾è‘—æ”¹å–„ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.09910",
            "title": "Prompt2Perturb (P2P): Text-Guided Diffusion-Based Adversarial Attacks on Breast Ultrasound Images",
            "url": "https://huggingface.co/papers/2412.09910",
            "abstract": "Deep neural networks (DNNs) offer significant promise for improving breast cancer diagnosis in medical imaging. However, these models are highly susceptible to adversarial attacks--small, imperceptible changes that can mislead classifiers--raising critical concerns about their reliability and security. Traditional attacks rely on fixed-norm perturbations, misaligning with human perception. In contrast, diffusion-based attacks require pre-trained models, demanding substantial data when these models are unavailable, limiting practical use in data-scarce scenarios. In medical imaging, however, this is often unfeasible due to the limited availability of datasets. Building on recent advancements in learnable prompts, we propose Prompt2Perturb (P2P), a novel language-guided attack method capable of generating meaningful attack examples driven by text instructions. During the prompt learning phase, our approach leverages learnable prompts within the text encoder to create subtle, yet impactful, perturbations that remain imperceptible while guiding the model towards targeted outcomes. In contrast to current prompt learning-based approaches, our P2P stands out by directly updating text embeddings, avoiding the need for retraining diffusion models. Further, we leverage the finding that optimizing only the early reverse diffusion steps boosts efficiency while ensuring that the generated adversarial examples incorporate subtle noise, thus preserving ultrasound image quality without introducing noticeable artifacts. We show that our method outperforms state-of-the-art attack techniques across three breast ultrasound datasets in FID and LPIPS. Moreover, the generated images are both more natural in appearance and more effective compared to existing adversarial attacks. Our code will be publicly available https://github.com/yasamin-med/P2P.",
            "score": 1,
            "issue_id": 1140,
            "pub_date": "2024-12-13",
            "pub_date_card": {
                "ru": "13 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 13",
                "zh": "12æœˆ13æ—¥"
            },
            "hash": "bda49360304ea17e",
            "authors": [
                "Yasamin Medghalchi",
                "Moein Heidari",
                "Clayton Allard",
                "Leonid Sigal",
                "Ilker Hacihaliloglu"
            ],
            "affiliations": [
                "University of British Columbia, Vancouver, BC, Canada",
                "Vector Institute for AI, Toronto, ON, Canada"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.09910.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#open_source",
                    "#security",
                    "#healthcare",
                    "#diffusion"
                ],
                "emoji": "ğŸ©»",
                "ru": {
                    "title": "Ğ¢ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸ ĞºĞ°Ğº Ğ¾Ñ€ÑƒĞ¶Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² Ğ˜Ğ˜ Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞµ",
                    "desc": "Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ñ‚Ğ°ĞºĞ¸ Ğ½Ğ° Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑĞµÑ‚Ğ¸ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ñ€Ğ°ĞºĞ° Ğ³Ñ€ÑƒĞ´Ğ¸ Ğ¿Ğ¾ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Prompt2Perturb (P2P), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ½ĞµĞ·Ğ°Ğ¼ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ² Ğ·Ğ°Ğ±Ğ»ÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², P2P Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ½Ğ° Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ P2P Ğ½Ğ°Ğ´ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ°Ñ‚Ğ°Ğº Ğ½Ğ° Ñ‚Ñ€ĞµÑ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑƒĞ»ÑŒÑ‚Ñ€Ğ°Ğ·Ğ²ÑƒĞºĞ¾Ğ²Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ³Ñ€ÑƒĞ´Ğ¸."
                },
                "en": {
                    "title": "Enhancing Breast Cancer Diagnosis with Smart Adversarial Attacks",
                    "desc": "This paper introduces Prompt2Perturb (P2P), a novel method for generating adversarial attacks on deep neural networks used in breast cancer diagnosis. Unlike traditional methods that rely on fixed perturbations, P2P utilizes learnable prompts to create subtle changes in medical images based on text instructions. This approach allows for the generation of effective adversarial examples without the need for extensive datasets or retraining of models. The results demonstrate that P2P outperforms existing techniques, producing more natural-looking images while maintaining the quality of ultrasound diagnostics."
                },
                "zh": {
                    "title": "åˆ©ç”¨è¯­è¨€æŒ‡å¯¼çš„å¯¹æŠ—æ”»å‡»æå‡åŒ»å­¦å½±åƒå®‰å…¨æ€§",
                    "desc": "æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰åœ¨åŒ»å­¦å½±åƒä¸­çš„ä¹³è…ºç™Œè¯Šæ–­ä¸­å…·æœ‰å¾ˆå¤§æ½œåŠ›ï¼Œä½†å®ƒä»¬å®¹æ˜“å—åˆ°å¯¹æŠ—æ”»å‡»çš„å½±å“ï¼Œè¿™äº›æ”»å‡»é€šè¿‡å¾®å°çš„ã€ä¸å¯å¯Ÿè§‰çš„å˜åŒ–æ¥è¯¯å¯¼åˆ†ç±»å™¨ã€‚ä¼ ç»Ÿçš„å¯¹æŠ—æ”»å‡»ä¾èµ–äºå›ºå®šèŒƒæ•°çš„æ‰°åŠ¨ï¼Œè¿™ä¸äººç±»çš„æ„ŸçŸ¥ä¸ä¸€è‡´ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•Prompt2Perturbï¼ˆP2Pï¼‰ï¼Œå®ƒåˆ©ç”¨å¯å­¦ä¹ çš„æç¤ºç”Ÿæˆæœ‰æ„ä¹‰çš„æ”»å‡»ç¤ºä¾‹ï¼Œé¿å…äº†å¯¹æŠ—æ”»å‡»ä¸­å¸¸è§çš„é‡è®­ç»ƒéœ€æ±‚ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒP2Påœ¨ä¹³è…ºè¶…å£°æ•°æ®é›†ä¸Šä¼˜äºç°æœ‰çš„å¯¹æŠ—æ”»å‡»æŠ€æœ¯ï¼Œç”Ÿæˆçš„å›¾åƒæ›´è‡ªç„¶ä¸”æ›´æœ‰æ•ˆã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-12-13.html",
    "link_next": "2024-12-17.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "13.12",
        "en": "12/13",
        "zh": "12æœˆ13æ—¥"
    },
    "short_date_next": {
        "ru": "17.12",
        "en": "12/17",
        "zh": "12æœˆ17æ—¥"
    },
    "categories": {
        "#dataset": 5,
        "#data": 2,
        "#benchmark": 3,
        "#agents": 2,
        "#cv": 5,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 3,
        "#3d": 2,
        "#audio": 1,
        "#video": 4,
        "#multimodal": 6,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 7,
        "#healthcare": 2,
        "#training": 9,
        "#robotics": 1,
        "#agi": 3,
        "#games": 2,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 8,
        "#survey": 0,
        "#diffusion": 6,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 1,
        "#leakage": 0,
        "#open_source": 6,
        "#small_models": 1,
        "#science": 2,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†GenExç³»ç»Ÿï¼Œå®ƒèƒ½é€šè¿‡å•å¼ RGBå›¾åƒç”Ÿæˆ3Dä¸€è‡´çš„æƒ³è±¡ç¯å¢ƒã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨è™šå¹»å¼•æ“çš„3Dæ•°æ®ï¼Œå¸®åŠ©AIä»£ç†åœ¨å¤æ‚ä»»åŠ¡ä¸­è¿›è¡Œæ¢ç´¢å’Œå¯¼èˆªã€‚GenExå±•ç¤ºäº†é«˜è´¨é‡çš„ä¸–ç•Œç”Ÿæˆå’Œå¼ºå¤§çš„3Dèƒ½åŠ›ï¼Œå¦‚3Dæ˜ å°„å’Œå¾ªç¯ä¸€è‡´æ€§ã€‚æ–‡ç« æ€»ç»“è¯´ï¼ŒGenExä¸ºæå‡æƒ³è±¡ç©ºé—´ä¸­çš„ embodied AI æä¾›äº†å˜é©æ€§å¹³å°ï¼Œå¹¶æœ‰æ½œåŠ›æ‰©å±•åˆ°ç°å®ä¸–ç•Œæ¢ç´¢ã€‚",
        "title": "GenEx: Generating an Explorable World",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†GenExç³»ç»Ÿï¼Œå®ƒèƒ½é€šè¿‡å•å¼ RGBå›¾åƒç”Ÿæˆ3Dä¸€è‡´çš„æƒ³è±¡ç¯å¢ƒã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨è™šå¹»å¼•æ“çš„3Dæ•°æ®ï¼Œå¸®åŠ©AIä»£ç†åœ¨å¤æ‚ä»»åŠ¡ä¸­è¿›è¡Œæ¢ç´¢å’Œå¯¼èˆªã€‚GenExå±•ç¤ºäº†é«˜è´¨é‡çš„ä¸–ç•Œç”Ÿæˆå’Œå¼ºå¤§çš„3Dèƒ½åŠ›ï¼Œå¦‚3Dæ˜ å°„å’Œå¾ªç¯ä¸€è‡´æ€§ã€‚æ–‡ç« æ€»ç»“è¯´ï¼ŒGenExä¸ºæå‡æƒ³è±¡ç©ºé—´ä¸­çš„ embodied AI æä¾›äº†å˜é©æ€§å¹³å°ï¼Œå¹¶æœ‰æ½œåŠ›æ‰©å±•åˆ°ç°å®ä¸–ç•Œæ¢ç´¢ã€‚\n\nzhÃ¨ piÄn wÃ©n zhÄng jiÃ¨ shÃ o le GenEx xÃ¬ tÇ’ng, tÄ nÃ©ng tÅng guÃ² dÄn zhÄng RGB tÃº xÃ­ng shÄ“ng chÃ©ng 3D yÄ« zhÃ¬ de xiÇng xiÃ ng huÃ¡n jÃ¬ng. gÇi xÃ¬ tÇ’ng lÃ¬ yÃ²ng xÅ« huÃ n yÇn qÃ­ng de 3D shÃ¹ jÃ¹, bÄng zhÃ¹ AI dÃ i lÇ zÃ i fÃº zÃ  rÃ¨n wÃ¹ zhÅng jÃ¬n xÃ­ng tÃ n suÇ’ hÃ© dÇo hÃ¡ng. GenEx zhÇn shÃ¬ le gÄo zhÃ¬ liÃ ng de shÃ¬ jiÃ¨ shÄ“ng chÃ©ng hÃ© qiÃ¡ng dÃ  de 3D nÃ©ng lÃ¬, rÃº 3D yÇng shÃ¨ hÃ© xÃºn huÃ¡n yÄ« zhÃ¬ xÃ¬ng. wÃ©n zhÄng zÇ’ng jiÃ¨ shuÅ, GenEx wÃ¨i tÃ­ shÄ“ng xiÇng xiÃ ng kÅng zhÅng de embodied AI tÃ­ gÅng le biÃ n gÃ© xÃ¬ng pÃ­ng tÃ¡i, bÃ¬ng yÇ’u qiÃ¡n lÃ¬ kuÃ² zhÇn dÃ o xiÃ n shÃ­ shÃ¬ jiÃ¨ tÃ n suÇ’.",
        "vocab": "[{'word': 'GenEx', 'pinyin': '', 'trans': 'GenEx'}, {'word': 'ç³»ç»Ÿ', 'pinyin': 'xÃ¬tÇ’ng', 'trans': 'system'}, {'word': 'é€šè¿‡', 'pinyin': 'tÅngguÃ²', 'trans': 'through'}, {'word': 'å•å¼ ', 'pinyin': 'dÄn zhÄng', 'trans': 'single'}, {'word': 'RGB', 'pinyin': '', 'trans': 'RGB'}, {'word': 'å›¾åƒ', 'pinyin': 'tÃºxiÃ ng', 'trans': 'image'}, {'word': 'ç”Ÿæˆ', 'pinyin': 'shÄ“ngchÃ©ng', 'trans': 'generate'}, {'word': '3D', 'pinyin': '', 'trans': '3D'}, {'word': 'ä¸€è‡´', 'pinyin': 'yÄ«zhÃ¬', 'trans': 'consistent'}, {'word': 'æƒ³è±¡', 'pinyin': 'xiÇngxiÃ ng', 'trans': 'imaginary'}, {'word': 'ç¯å¢ƒ', 'pinyin': 'huÃ¡njÃ¬ng', 'trans': 'environment'}, {'word': 'åˆ©ç”¨', 'pinyin': 'lÃ¬yÃ²ng', 'trans': 'utilize'}, {'word': 'è™šå¹»', 'pinyin': 'xÅ«huÃ n', 'trans': 'virtual'}, {'word': 'å¼•æ“', 'pinyin': 'yÇnqÃ­ng', 'trans': 'engine'}, {'word': 'æ•°æ®', 'pinyin': 'shÃ¹jÃ¹', 'trans': 'data'}, {'word': 'å¸®åŠ©', 'pinyin': 'bÄngzhÃ¹', 'trans': 'help'}, {'word': 'AI', 'pinyin': '', 'trans': 'AI'}, {'word': 'ä»£ç†', 'pinyin': 'dÃ ilÇ', 'trans': 'agent'}, {'word': 'å¤æ‚', 'pinyin': 'fÃ¹zÃ¡', 'trans': 'complex'}, {'word': 'ä»»åŠ¡', 'pinyin': 'rÃ¨nwÃ¹', 'trans': 'task'}, {'word': 'è¿›è¡Œ', 'pinyin': 'jÃ¬nxÃ­ng', 'trans': 'conduct'}, {'word': 'æ¢ç´¢', 'pinyin': 'tÃ nsuÇ’', 'trans': 'explore'}, {'word': 'å¯¼èˆª', 'pinyin': 'dÇohÃ¡ng', 'trans': 'navigate'}, {'word': 'å±•ç¤º', 'pinyin': 'zhÇnshÃ¬', 'trans': 'demonstrate'}, {'word': 'é«˜è´¨é‡', 'pinyin': 'gÄo zhÃ¬liÃ ng', 'trans': 'high quality'}, {'word': 'ä¸–ç•Œ', 'pinyin': 'shÃ¬jiÃ¨', 'trans': 'world'}, {'word': 'å¼ºå¤§', 'pinyin': 'qiÃ¡ngdÃ ', 'trans': 'powerful'}, {'word': 'èƒ½åŠ›', 'pinyin': 'nÃ©nglÃ¬', 'trans': 'ability'}, {'word': 'æ˜ å°„', 'pinyin': 'yÃ¬ngshÃ¨', 'trans': 'mapping'}, {'word': 'å¾ªç¯', 'pinyin': 'xÃºnhuÃ¡n', 'trans': 'cyclic'}, {'word': 'ä¸€è‡´æ€§', 'pinyin': 'yÄ«zhÃ¬xÃ¬ng', 'trans': 'consistency'}, {'word': 'æ€»ç»“', 'pinyin': 'zÇ’ngjiÃ©', 'trans': 'summarize'}, {'word': 'è¯´', 'pinyin': 'shuÅ', 'trans': 'say'}, {'word': 'æå‡', 'pinyin': 'tÃ­shÄ“ng', 'trans': 'enhance'}, {'word': 'ç©ºé—´', 'pinyin': 'kÅngjiÄn', 'trans': 'space'}, {'word': 'embodied', 'pinyin': '', 'trans': 'embodied'}, {'word': 'å˜é©æ€§', 'pinyin': 'biÃ ngÃ©xÃ¬ng', 'trans': 'transformative'}, {'word': 'å¹³å°', 'pinyin': 'pÃ­ngtÃ¡i', 'trans': 'platform'}, {'word': 'æ½œåŠ›', 'pinyin': 'qiÃ¡nlÃ¬', 'trans': 'potential'}, {'word': 'æ‰©å±•', 'pinyin': 'kuÃ²zhÇn', 'trans': 'expand'}, {'word': 'ç°å®', 'pinyin': 'xiÃ nshÃ­', 'trans': 'real'}, {'word': 'ä¸–ç•Œ', 'pinyin': 'shÃ¬jiÃ¨', 'trans': 'world'}, {'word': 'æ¢ç´¢', 'pinyin': 'tÃ nsuÇ’', 'trans': 'explore'}]",
        "trans": "This article introduces the GenEx system, which can generate a 3D-consistent imagined environment from a single RGB image. The system leverages 3D data from the Unreal Engine to assist AI agents in exploration and navigation in complex tasks. GenEx demonstrates high-quality world generation and powerful 3D capabilities, such as 3D mapping and cyclic consistency. The article concludes that GenEx provides a transformative platform for enhancing embodied AI in imagined spaces and has the potential to extend to real-world exploration.",
        "update_ts": "2024-12-16 09:12"
    }
}