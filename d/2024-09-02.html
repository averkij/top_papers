
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 16 papers. September 2.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #0989eacf;
            --secondary-color: #fff;
            --background-color: #f5f5f5;
            --text-color: #333333;
            --header-color: #0989eacf;
            --body-color: #f5f5f5;
            --menu-color: #002370;
        }        
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 0;
            line-height: 1;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 2em;
            padding: 10px 0 20px 0;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.tags {
            color: #555;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);
            transition: background-color 0.2s ease;
            display: flex;
            flex-direction: column;
            position: relative;
        }
        .article-content {
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
        }
        body.dark-theme>div>main>article {
            background-color: #444;
        }
        body.light-theme>div>main>article {
            background-color: #fff;
        }
        body.dark-theme>div>main>article:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
        }
        .pub-date {
            font-size: 0.9em;
            margin-bottom: 0.8em;
            font-weight: 300;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 1em;
            position: absolute;
            bottom: 10px;
            font-weight: 300;
            font-family: 'Roboto Slab';
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 80px;
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        a:hover {
            color: #e73838;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            padding: 1em 0;
            margin-top: 2em;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 0px;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }
        
        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">2 сентября</span> | <span id="title-articles-count">16 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item" id="nav-prev"><a href="/d/2024-08-30.html">⬅️ <span id="prev-date">30.08</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-09-03.html">➡️ <span id="next-date">03.09</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2024-09.html">📈 <span id='top-month-label'>Топ за месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '2 сентября', 'en': 'September 2', 'zh': '9月2日'};
        let feedDateNext = {'ru': '03.09', 'en': '09/03', 'zh': '9月3日'};
        let feedDatePrev = {'ru': '30.08', 'en': '08/30', 'zh': '8月30日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'Статья от ', 'en': 'Published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Топ за месяц', 'en': 'Top by Month', 'zh': '月度热门论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2408.15545', 'title': 'SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding', 'url': 'https://huggingface.co/papers/2408.15545', 'abstract': 'Scientific literature understanding is crucial for extracting targeted information and garnering insights, thereby significantly advancing scientific discovery. Despite the remarkable success of Large Language Models (LLMs), they face challenges in scientific literature understanding, primarily due to (1) a lack of scientific knowledge and (2) unfamiliarity with specialized scientific tasks.   To develop an LLM specialized in scientific literature understanding, we propose a hybrid strategy that integrates continual pre-training (CPT) and supervised fine-tuning (SFT), to simultaneously infuse scientific domain knowledge and enhance instruction-following capabilities for domain-specific tasks.cIn this process, we identify two key challenges: (1) constructing high-quality CPT corpora, and (2) generating diverse SFT instructions. We address these challenges through a meticulous pipeline, including PDF text extraction, parsing content error correction, quality filtering, and synthetic instruction creation. Applying this strategy, we present a suite of LLMs: SciLitLLM, specialized in scientific literature understanding. These models demonstrate promising performance on scientific literature understanding benchmarks.   Our contributions are threefold: (1) We present an effective framework that integrates CPT and SFT to adapt LLMs to scientific literature understanding, which can also be easily adapted to other domains. (2) We propose an LLM-based synthesis method to generate diverse and high-quality scientific instructions, resulting in a new instruction set -- SciLitIns -- for supervised fine-tuning in less-represented scientific domains. (3) SciLitLLM achieves promising performance improvements on scientific literature understanding benchmarks.', 'score': 34, 'issue_id': 1, 'pub_date': '2024-08-28', 'pub_date_card': {'ru': '28 августа', 'en': 'August 28', 'zh': '8月28日'}, 'hash': '3a813f632e634481', 'data': {'categories': ['#dataset', '#data', '#training', '#benchmark', '#multilingual'], 'emoji': '🧠', 'ru': {'title': 'Улучшение понимания научных текстов с помощью специализированных языковых моделей', 'desc': 'Статья представляет новый подход к обучению больших языковых моделей (LLM) для понимания научной литературы. Авторы предлагают гибридную стратегию, сочетающую непрерывное предобучение и контролируемую тонкую настройку для улучшения знаний в научной области и выполнения специализированных задач. Они разработали модель SciLitLLM, которая показывает многообещающие результаты на тестах по пониманию научной литературы. Статья также представляет новый набор инструкций SciLitIns для тонкой настройки в малопредставленных научных областях.'}, 'en': {'title': 'Empowering LLMs for Scientific Literature Mastery', 'desc': "This paper addresses the challenges faced by Large Language Models (LLMs) in understanding scientific literature due to their lack of domain-specific knowledge and unfamiliarity with specialized tasks. The authors propose a hybrid approach that combines continual pre-training (CPT) and supervised fine-tuning (SFT) to enhance LLMs' capabilities in this area. They tackle the difficulties of creating high-quality training data and generating diverse instructions through a detailed pipeline that includes content extraction and error correction. The resulting model, SciLitLLM, shows significant improvements on benchmarks for scientific literature understanding, demonstrating the effectiveness of their proposed framework."}, 'zh': {'title': '提升科学文献理解的智能模型', 'desc': '本文提出了一种混合策略，旨在提高大型语言模型（LLM）在科学文献理解方面的能力。通过持续预训练（CPT）和监督微调（SFT）的结合，模型能够同时吸收科学领域知识并增强对特定任务的指令遵循能力。我们识别了构建高质量CPT语料库和生成多样化SFT指令的两个主要挑战，并通过精细的流程来解决这些问题。最终，我们推出了专门用于科学文献理解的模型SciLitLLM，并在相关基准测试中表现出色。'}}}, {'id': 'https://huggingface.co/papers/2408.17267', 'title': 'UrBench: A Comprehensive Benchmark for Evaluating Large Multimodal Models in Multi-View Urban Scenarios', 'url': 'https://huggingface.co/papers/2408.17267', 'abstract': "Recent evaluations of Large Multimodal Models (LMMs) have explored their capabilities in various domains, with only few benchmarks specifically focusing on urban environments. Moreover, existing urban benchmarks have been limited to evaluating LMMs with basic region-level urban tasks under singular views, leading to incomplete evaluations of LMMs' abilities in urban environments. To address these issues, we present UrBench, a comprehensive benchmark designed for evaluating LMMs in complex multi-view urban scenarios. UrBench contains 11.6K meticulously curated questions at both region-level and role-level that cover 4 task dimensions: Geo-Localization, Scene Reasoning, Scene Understanding, and Object Understanding, totaling 14 task types. In constructing UrBench, we utilize data from existing datasets and additionally collect data from 11 cities, creating new annotations using a cross-view detection-matching method. With these images and annotations, we then integrate LMM-based, rule-based, and human-based methods to construct large-scale high-quality questions. Our evaluations on 21 LMMs show that current LMMs struggle in the urban environments in several aspects. Even the best performing GPT-4o lags behind humans in most tasks, ranging from simple tasks such as counting to complex tasks such as orientation, localization and object attribute recognition, with an average performance gap of 17.4%. Our benchmark also reveals that LMMs exhibit inconsistent behaviors with different urban views, especially with respect to understanding cross-view relations. UrBench datasets and benchmark results will be publicly available at https://opendatalab.github.io/UrBench/.", 'score': 23, 'issue_id': 1, 'pub_date': '2024-08-30', 'pub_date_card': {'ru': '30 августа', 'en': 'August 30', 'zh': '8月30日'}, 'hash': '46054a14f1990e6c', 'data': {'categories': ['#benchmark', '#dataset', '#multimodal'], 'emoji': '🏙️', 'ru': {'title': 'UrBench: новый рубеж в оценке мультимодальных моделей для городских сред', 'desc': 'В статье представлен UrBench - комплексный бенчмарк для оценки больших мультимодальных моделей (LMM) в сложных многоракурсных городских сценариях. Бенчмарк содержит 11.6 тысяч тщательно подобранных вопросов, охватывающих 4 измерения задач и 14 типов задач. Оценка 21 LMM показала, что современные модели испытывают трудности в городских средах, отставая от людей в большинстве задач в среднем на 17.4%. UrBench также выявил непоследовательность в поведении LMM при работе с различными городскими ракурсами.'}, 'en': {'title': 'UrBench: Elevating LMM Evaluation in Urban Landscapes', 'desc': 'This paper introduces UrBench, a new benchmark specifically designed to evaluate Large Multimodal Models (LMMs) in urban environments. It addresses the limitations of existing benchmarks by providing a comprehensive set of 11.6K questions that assess LMMs across various task dimensions, including Geo-Localization and Scene Understanding. The authors collected data from 11 cities and used a cross-view detection-matching method to create high-quality annotations for their tasks. The evaluation results indicate that current LMMs, including the top performer GPT-4o, significantly underperform compared to human capabilities in urban scenarios, highlighting the need for improved model training in this domain.'}, 'zh': {'title': '全面评估城市环境中的大型多模态模型', 'desc': '本文介绍了一个名为UrBench的基准测试，旨在评估大型多模态模型（LMMs）在复杂城市环境中的能力。现有的城市基准测试主要集中在单一视角的基本区域任务，导致对LMMs能力的评估不够全面。UrBench包含11600个精心策划的问题，涵盖地理定位、场景推理、场景理解和物体理解等四个任务维度。通过对11个城市的数据收集和交叉视角检测匹配方法的应用，UrBench为LMMs提供了更全面的评估，结果显示当前LMMs在城市环境中表现不佳，尤其是在理解不同视角关系方面。'}}}, {'id': 'https://huggingface.co/papers/2408.15914', 'title': 'CoRe: Context-Regularized Text Embedding Learning for Text-to-Image Personalization', 'url': 'https://huggingface.co/papers/2408.15914', 'abstract': "Recent advances in text-to-image personalization have enabled high-quality and controllable image synthesis for user-provided concepts. However, existing methods still struggle to balance identity preservation with text alignment. Our approach is based on the fact that generating prompt-aligned images requires a precise semantic understanding of the prompt, which involves accurately processing the interactions between the new concept and its surrounding context tokens within the CLIP text encoder. To address this, we aim to embed the new concept properly into the input embedding space of the text encoder, allowing for seamless integration with existing tokens. We introduce Context Regularization (CoRe), which enhances the learning of the new concept's text embedding by regularizing its context tokens in the prompt. This is based on the insight that appropriate output vectors of the text encoder for the context tokens can only be achieved if the new concept's text embedding is correctly learned. CoRe can be applied to arbitrary prompts without requiring the generation of corresponding images, thus improving the generalization of the learned text embedding. Additionally, CoRe can serve as a test-time optimization technique to further enhance the generations for specific prompts. Comprehensive experiments demonstrate that our method outperforms several baseline methods in both identity preservation and text alignment. Code will be made publicly available.", 'score': 21, 'issue_id': 1, 'pub_date': '2024-08-28', 'pub_date_card': {'ru': '28 августа', 'en': 'August 28', 'zh': '8月28日'}, 'hash': '837ea41abf0b83ee', 'data': {'categories': ['#cv', '#multimodal', '#training'], 'emoji': '🖼️', 'ru': {'title': 'CoRe: улучшение персонализации text-to-image моделей через контекстную регуляризацию', 'desc': 'Статья описывает новый подход к персонализации генерации изображений по текстовому описанию. Авторы представляют метод Context Regularization (CoRe), который улучшает встраивание нового концепта в пространство входных эмбеддингов текстового энкодера. CoRe регуляризует контекстные токены в промпте, что позволяет лучше интегрировать новый концепт с существующими токенами. Эксперименты показывают, что данный метод превосходит базовые подходы как в сохранении идентичности, так и в соответствии текстовому описанию.'}, 'en': {'title': 'Enhancing Text-to-Image Synthesis with Context Regularization', 'desc': "This paper presents a new method for improving text-to-image synthesis by focusing on how new concepts are integrated into existing text prompts. The authors introduce Context Regularization (CoRe), which helps the model better understand the relationships between a new concept and its context in the text. By embedding the new concept effectively within the input space of the CLIP text encoder, CoRe enhances the learning of the concept's text representation. The results show that this approach leads to better identity preservation and text alignment in generated images compared to previous methods."}, 'zh': {'title': '提升文本到图像生成的个性化与对齐', 'desc': '本文提出了一种新的文本到图像个性化方法，旨在提高用户提供概念的图像合成质量和可控性。我们的方法通过精确理解提示的语义，处理新概念与上下文标记之间的交互，来实现更好的图像生成。我们引入了上下文正则化（CoRe），通过正则化提示中的上下文标记，增强新概念的文本嵌入学习。实验结果表明，我们的方法在身份保留和文本对齐方面优于多种基线方法。'}}}, {'id': 'https://huggingface.co/papers/2408.14765', 'title': 'CrossViewDiff: A Cross-View Diffusion Model for Satellite-to-Street View Synthesis', 'url': 'https://huggingface.co/papers/2408.14765', 'abstract': 'Satellite-to-street view synthesis aims at generating a realistic street-view image from its corresponding satellite-view image. Although stable diffusion models have exhibit remarkable performance in a variety of image generation applications, their reliance on similar-view inputs to control the generated structure or texture restricts their application to the challenging cross-view synthesis task. In this work, we propose CrossViewDiff, a cross-view diffusion model for satellite-to-street view synthesis. To address the challenges posed by the large discrepancy across views, we design the satellite scene structure estimation and cross-view texture mapping modules to construct the structural and textural controls for street-view image synthesis. We further design a cross-view control guided denoising process that incorporates the above controls via an enhanced cross-view attention module. To achieve a more comprehensive evaluation of the synthesis results, we additionally design a GPT-based scoring method as a supplement to standard evaluation metrics. We also explore the effect of different data sources (e.g., text, maps, building heights, and multi-temporal satellite imagery) on this task. Results on three public cross-view datasets show that CrossViewDiff outperforms current state-of-the-art on both standard and GPT-based evaluation metrics, generating high-quality street-view panoramas with more realistic structures and textures across rural, suburban, and urban scenes. The code and models of this work will be released at https://opendatalab.github.io/CrossViewDiff/.', 'score': 14, 'issue_id': 1, 'pub_date': '2024-08-27', 'pub_date_card': {'ru': '27 августа', 'en': 'August 27', 'zh': '8月27日'}, 'hash': '2b0e5d03556e552c', 'data': {'categories': ['#cv', '#3d', '#benchmark'], 'emoji': '🛰️', 'ru': {'title': 'От спутника к улице: революционный синтез изображений с помощью CrossViewDiff', 'desc': 'Статья представляет CrossViewDiff - диффузионную модель для синтеза изображений с уровня улицы из спутниковых снимков. Авторы разработали модули оценки структуры сцены и кросс-видового маппинга текстур для управления синтезом. Модель использует улучшенный механизм кросс-видового внимания в процессе шумоподавления. Результаты на трех наборах данных показывают превосходство CrossViewDiff над современными методами в генерации высококачественных панорам улиц.'}, 'en': {'title': 'Bridging Views: Realistic Street-View Synthesis from Satellite Imagery', 'desc': 'This paper introduces CrossViewDiff, a novel cross-view diffusion model designed for synthesizing street-view images from satellite-view images. The model addresses the challenges of significant differences between the two views by incorporating satellite scene structure estimation and cross-view texture mapping modules. Additionally, it employs a cross-view control guided denoising process that utilizes an enhanced attention mechanism to improve the quality of the generated images. The results demonstrate that CrossViewDiff surpasses existing methods in generating realistic street-view panoramas across various environments, supported by both traditional and GPT-based evaluation metrics.'}, 'zh': {'title': '跨视图扩散模型，生成真实街景！', 'desc': '卫星到街景合成旨在从卫星视图生成逼真的街景图像。尽管稳定扩散模型在多种图像生成应用中表现出色，但它们对相似视图输入的依赖限制了在跨视图合成任务中的应用。为了解决视图之间的巨大差异，我们提出了CrossViewDiff模型，设计了卫星场景结构估计和跨视图纹理映射模块，以构建街景图像合成的结构和纹理控制。实验结果表明，CrossViewDiff在多个公共跨视图数据集上超越了现有的最先进技术，生成了更高质量的街景全景图。'}}}, {'id': 'https://huggingface.co/papers/2408.17024', 'title': 'InkubaLM: A small language model for low-resource African languages', 'url': 'https://huggingface.co/papers/2408.17024', 'abstract': 'High-resource language models often fall short in the African context, where there is a critical need for models that are efficient, accessible, and locally relevant, even amidst significant computing and data constraints. This paper introduces InkubaLM, a small language model with 0.4 billion parameters, which achieves performance comparable to models with significantly larger parameter counts and more extensive training data on tasks such as machine translation, question-answering, AfriMMLU, and the AfriXnli task. Notably, InkubaLM outperforms many larger models in sentiment analysis and demonstrates remarkable consistency across multiple languages. This work represents a pivotal advancement in challenging the conventional paradigm that effective language models must rely on substantial resources. Our model and datasets are publicly available \\url{https://huggingface.co/lelapa} to encourage research and development on low-resource languages.', 'score': 12, 'issue_id': 1, 'pub_date': '2024-08-30', 'pub_date_card': {'ru': '30 августа', 'en': 'August 30', 'zh': '8月30日'}, 'hash': '16f2149ded7ace2d', 'data': {'categories': ['#multilingual', '#dataset', '#translation'], 'emoji': '🌍', 'ru': {'title': 'Маленькая модель - большие возможности для африканских языков', 'desc': 'InkubaLM - это малая языковая модель с 0,4 миллиардами параметров, разработанная для африканского контекста. Она достигает производительности, сравнимой с гораздо более крупными моделями, в задачах машинного перевода, ответов на вопросы и других. InkubaLM превосходит многие большие модели в анализе тональности и демонстрирует стабильность на нескольких языках. Это исследование оспаривает идею о том, что эффективные языковые модели должны опираться на значительные ресурсы.'}, 'en': {'title': 'InkubaLM: Efficient Language Modeling for Africa', 'desc': 'This paper presents InkubaLM, a compact language model designed specifically for the African context, which operates efficiently with only 0.4 billion parameters. Despite its smaller size, InkubaLM performs comparably to larger models on various tasks, including machine translation and question-answering, showcasing its effectiveness in low-resource settings. The model excels particularly in sentiment analysis and maintains strong performance across multiple languages, challenging the notion that high-resource models are necessary for effective language processing. By making InkubaLM and its datasets publicly available, the authors aim to foster further research and development in the area of low-resource languages.'}, 'zh': {'title': 'InkubaLM：小型语言模型的巨大潜力', 'desc': '本论文介绍了InkubaLM，这是一种小型语言模型，参数量为4亿。尽管参数量较小，InkubaLM在机器翻译、问答、AfriMMLU和AfriXnli任务上表现出与大型模型相当的性能。特别是在情感分析方面，InkubaLM的表现优于许多更大的模型，并且在多种语言中展现出显著的一致性。该研究挑战了有效语言模型必须依赖大量资源的传统观念，并提供了公开的数据集以促进低资源语言的研究和开发。'}}}, {'id': 'https://huggingface.co/papers/2408.17131', 'title': 'VQ4DiT: Efficient Post-Training Vector Quantization for Diffusion Transformers', 'url': 'https://huggingface.co/papers/2408.17131', 'abstract': 'The Diffusion Transformers Models (DiTs) have transitioned the network architecture from traditional UNets to transformers, demonstrating exceptional capabilities in image generation. Although DiTs have been widely applied to high-definition video generation tasks, their large parameter size hinders inference on edge devices. Vector quantization (VQ) can decompose model weight into a codebook and assignments, allowing extreme weight quantization and significantly reducing memory usage. In this paper, we propose VQ4DiT, a fast post-training vector quantization method for DiTs. We found that traditional VQ methods calibrate only the codebook without calibrating the assignments. This leads to weight sub-vectors being incorrectly assigned to the same assignment, providing inconsistent gradients to the codebook and resulting in a suboptimal result. To address this challenge, VQ4DiT calculates the candidate assignment set for each weight sub-vector based on Euclidean distance and reconstructs the sub-vector based on the weighted average. Then, using the zero-data and block-wise calibration method, the optimal assignment from the set is efficiently selected while calibrating the codebook. VQ4DiT quantizes a DiT XL/2 model on a single NVIDIA A100 GPU within 20 minutes to 5 hours depending on the different quantization settings. Experiments show that VQ4DiT establishes a new state-of-the-art in model size and performance trade-offs, quantizing weights to 2-bit precision while retaining acceptable image generation quality.', 'score': 11, 'issue_id': 1, 'pub_date': '2024-08-30', 'pub_date_card': {'ru': '30 августа', 'en': 'August 30', 'zh': '8月30日'}, 'hash': '8d6caa5c8402ee57', 'data': {'categories': ['#inference', '#architecture', '#cv', '#diffusion'], 'emoji': '🎨', 'ru': {'title': 'Эффективное сжатие моделей DiT без потери качества', 'desc': 'Статья представляет VQ4DiT - метод быстрого пост-обучения векторного квантования для моделей Diffusion Transformers (DiTs). Традиционные методы VQ калибруют только кодовую книгу, что приводит к неоптимальным результатам. VQ4DiT решает эту проблему, вычисляя набор кандидатов на присвоение для каждого подвектора веса и реконструируя подвектор на основе взвешенного среднего. Метод позволяет квантовать веса до 2-битной точности, сохраняя приемлемое качество генерации изображений, и устанавливает новый state-of-the-art в соотношении размера модели и производительности.'}, 'en': {'title': 'VQ4DiT: Optimizing Diffusion Transformers for Efficient Image Generation', 'desc': 'The paper introduces VQ4DiT, a novel post-training vector quantization method specifically designed for Diffusion Transformers Models (DiTs) to enhance their efficiency for edge device deployment. Traditional vector quantization methods only focus on calibrating the codebook, which can lead to suboptimal weight assignments and inconsistent gradients. VQ4DiT improves this by calculating candidate assignments based on Euclidean distance and reconstructing weight sub-vectors through a weighted average approach. The method achieves significant model size reduction while maintaining high-quality image generation, setting a new benchmark in the trade-off between model size and performance.'}, 'zh': {'title': 'VQ4DiT：提升扩散变换器模型的量化效率', 'desc': '本文介绍了一种新的快速后训练向量量化方法VQ4DiT，专门用于扩散变换器模型（DiTs）。传统的向量量化方法只校准了代码本，而没有校准分配，导致权重子向量被错误分配，从而影响了模型性能。VQ4DiT通过计算每个权重子向量的候选分配集，并基于加权平均重构子向量，来解决这一问题。实验表明，VQ4DiT在模型大小和性能之间建立了新的最佳平衡，能够将权重量化到2位精度，同时保持良好的图像生成质量。'}}}, {'id': 'https://huggingface.co/papers/2408.16444', 'title': 'SurveySum: A Dataset for Summarizing Multiple Scientific Articles into a Survey Section', 'url': 'https://huggingface.co/papers/2408.16444', 'abstract': 'Document summarization is a task to shorten texts into concise and informative summaries. This paper introduces a novel dataset designed for summarizing multiple scientific articles into a section of a survey. Our contributions are: (1) SurveySum, a new dataset addressing the gap in domain-specific summarization tools; (2) two specific pipelines to summarize scientific articles into a section of a survey; and (3) the evaluation of these pipelines using multiple metrics to compare their performance. Our results highlight the importance of high-quality retrieval stages and the impact of different configurations on the quality of generated summaries.', 'score': 8, 'issue_id': 1, 'pub_date': '2024-08-29', 'pub_date_card': {'ru': '29 августа', 'en': 'August 29', 'zh': '8月29日'}, 'hash': '1fb964fde1f5ed8a', 'data': {'categories': ['#dataset', '#data', '#benchmark'], 'emoji': '📚', 'ru': {'title': 'SurveySum: Революция в автоматическом создании научных обзоров', 'desc': 'Эта статья представляет новый набор данных SurveySum для обобщения нескольких научных статей в раздел обзора. Авторы предлагают две специализированные модели для выполнения этой задачи. Исследование включает оценку эффективности предложенных моделей с использованием различных метрик. Результаты подчеркивают важность качественного этапа извлечения информации и влияние различных конфигураций на качество генерируемых резюме.'}, 'en': {'title': 'Enhancing Scientific Summarization with SurveySum Dataset', 'desc': 'This paper focuses on document summarization, specifically creating concise summaries from multiple scientific articles for surveys. It introduces SurveySum, a new dataset that fills a gap in tools for summarizing domain-specific content. The authors present two distinct pipelines designed to effectively summarize scientific articles into survey sections. Their evaluation of these pipelines reveals the significance of high-quality retrieval processes and how various configurations can influence the quality of the summaries produced.'}, 'zh': {'title': '提升科学文章摘要质量的新方法', 'desc': '本文介绍了一种文档摘要的任务，旨在将文本缩短为简洁且信息丰富的摘要。我们提出了一个新数据集SurveySum，专门用于将多篇科学文章总结为调查报告的一部分。我们还设计了两种特定的处理流程来实现这一目标，并使用多种评估指标对其性能进行了比较。研究结果强调了高质量检索阶段的重要性以及不同配置对生成摘要质量的影响。'}}}, {'id': 'https://huggingface.co/papers/2408.14886', 'title': 'The VoxCeleb Speaker Recognition Challenge: A Retrospective', 'url': 'https://huggingface.co/papers/2408.14886', 'abstract': "The VoxCeleb Speaker Recognition Challenges (VoxSRC) were a series of challenges and workshops that ran annually from 2019 to 2023. The challenges primarily evaluated the tasks of speaker recognition and diarisation under various settings including: closed and open training data; as well as supervised, self-supervised, and semi-supervised training for domain adaptation. The challenges also provided publicly available training and evaluation datasets for each task and setting, with new test sets released each year. In this paper, we provide a review of these challenges that covers: what they explored; the methods developed by the challenge participants and how these evolved; and also the current state of the field for speaker verification and diarisation. We chart the progress in performance over the five installments of the challenge on a common evaluation dataset and provide a detailed analysis of how each year's special focus affected participants' performance. This paper is aimed both at researchers who want an overview of the speaker recognition and diarisation field, and also at challenge organisers who want to benefit from the successes and avoid the mistakes of the VoxSRC challenges. We end with a discussion of the current strengths of the field and open challenges. Project page : https://mm.kaist.ac.kr/datasets/voxceleb/voxsrc/workshop.html", 'score': 8, 'issue_id': 1, 'pub_date': '2024-08-27', 'pub_date_card': {'ru': '27 августа', 'en': 'August 27', 'zh': '8月27日'}, 'hash': '6a8cd96b781f16f0', 'data': {'categories': ['#benchmark', '#audio', '#survey'], 'emoji': '🎤', 'ru': {'title': 'VoxSRC: пять лет инноваций в распознавании говорящего и диаризации', 'desc': 'VoxCeleb Speaker Recognition Challenges (VoxSRC) - это серия ежегодных соревнований и семинаров, проводившихся с 2019 по 2023 год. Они фокусировались на задачах распознавания говорящего и диаризации в различных условиях, включая закрытые и открытые наборы данных для обучения, а также супервизорное, самосупервизорное и полусупервизорное обучение для адаптации к домену. В статье представлен обзор этих соревнований, включая методы, разработанные участниками, и их эволюцию. Авторы анализируют прогресс в производительности на общем наборе данных для оценки и обсуждают текущие сильные стороны и открытые проблемы в области.'}, 'en': {'title': 'Advancing Speaker Recognition: Insights from VoxSRC Challenges', 'desc': 'The VoxCeleb Speaker Recognition Challenges (VoxSRC) were annual competitions from 2019 to 2023 that focused on improving speaker recognition and diarisation techniques. Participants used various training methods, including supervised and self-supervised learning, to adapt models to different domains. The paper reviews the evolution of methods used in these challenges and tracks performance improvements over the years using a common evaluation dataset. It serves as a resource for researchers and challenge organizers, highlighting successes, mistakes, and ongoing challenges in the field.'}, 'zh': {'title': '说话人识别的进步与挑战', 'desc': 'VoxCeleb说话人识别挑战（VoxSRC）是一个从2019年到2023年每年举行的系列挑战和研讨会。该挑战主要评估在不同设置下的说话人识别和分段任务，包括封闭和开放训练数据，以及监督、自监督和半监督训练的领域适应。本文回顾了这些挑战，探讨了参与者开发的方法及其演变，并分析了说话人验证和分段领域的当前状态。我们还展示了在五届挑战中，参与者在共同评估数据集上的性能进展，以及每年特别关注点对参与者表现的影响。'}}}, {'id': 'https://huggingface.co/papers/2408.16176', 'title': 'VLM4Bio: A Benchmark Dataset to Evaluate Pretrained Vision-Language Models for Trait Discovery from Biological Images', 'url': 'https://huggingface.co/papers/2408.16176', 'abstract': 'Images are increasingly becoming the currency for documenting biodiversity on the planet, providing novel opportunities for accelerating scientific discoveries in the field of organismal biology, especially with the advent of large vision-language models (VLMs). We ask if pre-trained VLMs can aid scientists in answering a range of biologically relevant questions without any additional fine-tuning. In this paper, we evaluate the effectiveness of 12 state-of-the-art (SOTA) VLMs in the field of organismal biology using a novel dataset, VLM4Bio, consisting of 469K question-answer pairs involving 30K images from three groups of organisms: fishes, birds, and butterflies, covering five biologically relevant tasks. We also explore the effects of applying prompting techniques and tests for reasoning hallucination on the performance of VLMs, shedding new light on the capabilities of current SOTA VLMs in answering biologically relevant questions using images. The code and datasets for running all the analyses reported in this paper can be found at https://github.com/sammarfy/VLM4Bio.', 'score': 7, 'issue_id': 1, 'pub_date': '2024-08-28', 'pub_date_card': {'ru': '28 августа', 'en': 'August 28', 'zh': '8月28日'}, 'hash': 'd0a805442038e4da', 'data': {'categories': ['#dataset', '#cv', '#multimodal', '#reasoning', '#hallucinations'], 'emoji': '🦋', 'ru': {'title': 'Мультимодальные модели на страже биоразнообразия', 'desc': 'В статье исследуется применение предобученных мультимодальных моделей (VLM) для анализа изображений в биологии организмов. Авторы оценивают эффективность 12 современных VLM на новом наборе данных VLM4Bio, содержащем 469 тысяч пар вопрос-ответ для 30 тысяч изображений рыб, птиц и бабочек. Рассматриваются пять биологически значимых задач, а также влияние техник промптинга и проверка на галлюцинации. Результаты проливают свет на возможности современных VLM в ответах на биологические вопросы по изображениям.'}, 'en': {'title': 'Unlocking Biodiversity Insights with Vision-Language Models', 'desc': "This paper investigates the use of pre-trained vision-language models (VLMs) to assist scientists in answering biological questions without needing further training. It evaluates 12 state-of-the-art VLMs on a new dataset called VLM4Bio, which includes 469,000 question-answer pairs related to images of fishes, birds, and butterflies. The study examines how different prompting techniques and reasoning tests affect the models' performance in biological contexts. The findings highlight the potential of VLMs to enhance research in organismal biology by leveraging large datasets of images and questions."}, 'zh': {'title': '利用视觉语言模型加速生物多样性研究', 'desc': '本论文探讨了大型视觉语言模型（VLMs）在生物学领域的应用，特别是在回答与生物相关的问题方面。我们评估了12种最先进的VLM在处理包含469K问答对和30K图像的数据集VLM4Bio中的表现。研究涵盖了鱼类、鸟类和蝴蝶三组生物，涉及五个生物学相关任务。通过应用提示技术和推理幻觉测试，我们揭示了当前VLM在利用图像回答生物问题时的能力。'}}}, {'id': 'https://huggingface.co/papers/2408.15993', 'title': 'ClimDetect: A Benchmark Dataset for Climate Change Detection and Attribution', 'url': 'https://huggingface.co/papers/2408.15993', 'abstract': 'Detecting and attributing temperature increases due to climate change is crucial for understanding global warming and guiding adaptation strategies. The complexity of distinguishing human-induced climate signals from natural variability has challenged traditional detection and attribution (D&A) approaches, which seek to identify specific "fingerprints" in climate response variables. Deep learning offers potential for discerning these complex patterns in expansive spatial datasets. However, lack of standard protocols has hindered consistent comparisons across studies. We introduce ClimDetect, a standardized dataset of over 816k daily climate snapshots, designed to enhance model accuracy in identifying climate change signals. ClimDetect integrates various input and target variables used in past research, ensuring comparability and consistency. We also explore the application of vision transformers (ViT) to climate data, a novel and modernizing approach in this context. Our open-access data and code serve as a benchmark for advancing climate science through improved model evaluations. ClimDetect is publicly accessible via Huggingface dataet respository at: https://huggingface.co/datasets/ClimDetect/ClimDetect.', 'score': 7, 'issue_id': 1, 'pub_date': '2024-08-28', 'pub_date_card': {'ru': '28 августа', 'en': 'August 28', 'zh': '8月28日'}, 'hash': '75efd489e981a0a5', 'data': {'categories': ['#dataset', '#benchmark', '#cv'], 'emoji': '🌡️', 'ru': {'title': 'ClimDetect: Революция в обнаружении сигналов изменения климата с помощью глубокого обучения', 'desc': 'ClimDetect - это стандартизированный набор данных из более чем 816 тысяч ежедневных климатических снимков, созданный для повышения точности моделей в выявлении сигналов изменения климата. Он интегрирует различные входные и целевые переменные, используемые в предыдущих исследованиях, обеспечивая сопоставимость и согласованность. Авторы исследуют применение vision transformers (ViT) к климатическим данным, что является новаторским подходом в этой области. ClimDetect служит эталоном для продвижения климатической науки через улучшенные оценки моделей.'}, 'en': {'title': 'ClimDetect: Standardizing Climate Change Detection with Deep Learning', 'desc': 'This paper presents ClimDetect, a new standardized dataset aimed at improving the detection and attribution of climate change signals. It addresses the challenges of distinguishing human-induced climate changes from natural variability by providing over 816,000 daily climate snapshots. The study also explores the use of vision transformers (ViT) to analyze climate data, which represents a modern approach to understanding complex climate patterns. By offering open-access data and code, ClimDetect aims to enhance model evaluations and foster advancements in climate science.'}, 'zh': {'title': 'ClimDetect：提升气候变化信号识别的标准化数据集', 'desc': '本论文介绍了ClimDetect，这是一个标准化的数据集，包含超过816,000个每日气候快照，旨在提高气候变化信号识别的模型准确性。传统的检测和归因方法在区分人类引起的气候信号与自然变异性方面面临挑战，而深度学习能够帮助识别这些复杂模式。我们还探讨了视觉变换器（ViT）在气候数据中的应用，这是一种新颖的现代化方法。我们的开放访问数据和代码为气候科学的进步提供了基准，促进了模型评估的改进。'}}}, {'id': 'https://huggingface.co/papers/2408.14572', 'title': 'CURLoRA: Stable LLM Continual Fine-Tuning and Catastrophic Forgetting Mitigation', 'url': 'https://huggingface.co/papers/2408.14572', 'abstract': "This paper introduces CURLoRA, a novel approach to fine-tuning large language models (LLMs) that leverages CUR matrix decomposition in the context of Low-Rank Adaptation (LoRA). Our method addresses two critical challenges in LLM fine-tuning: mitigating catastrophic forgetting during continual learning and reducing the number of trainable parameters. We propose a unique modification to the CUR decomposition process, utilizing inverted probabilities for column and row selection which acts as an implicit regularization, and initializing the U matrix as a zero matrix, and only fine-tuning it. We demonstrate through experiments on multiple datasets that CURLoRA outperforms standard LoRA in mitigating catastrophic forgetting. It maintains model stability and performance across tasks while significantly reducing the number of trainable parameters. Our results show that CURLoRA achieves very good and stable task accuracy while maintaining base model's perplexity scores fixed compared to LoRA upon continual fine-tuning, particularly in scenarios with limited data.", 'score': 7, 'issue_id': 1, 'pub_date': '2024-08-26', 'pub_date_card': {'ru': '26 августа', 'en': 'August 26', 'zh': '8月26日'}, 'hash': '7e1ffebe22276d0d', 'data': {'categories': ['#training', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'CURLoRA: Эффективное дообучение языковых моделей без забывания', 'desc': 'Статья представляет CURLoRA - новый подход к дообучению больших языковых моделей (LLM), использующий CUR-разложение матриц в контексте Low-Rank Adaptation (LoRA). Метод решает две ключевые проблемы: смягчение катастрофического забывания при непрерывном обучении и уменьшение числа обучаемых параметров. CURLoRA модифицирует процесс CUR-разложения, используя обратные вероятности для выбора столбцов и строк, что действует как неявная регуляризация. Эксперименты показывают, что CURLoRA превосходит стандартный LoRA в смягчении катастрофического забывания, сохраняя стабильность модели при уменьшении числа параметров.'}, 'en': {'title': 'CURLoRA: Fine-Tuning LLMs with Stability and Efficiency', 'desc': "CURLoRA is a new method for fine-tuning large language models that uses CUR matrix decomposition within the Low-Rank Adaptation framework. It tackles the problems of catastrophic forgetting during continual learning and the need to reduce the number of parameters that need to be trained. The approach modifies the CUR decomposition by using inverted probabilities for selecting rows and columns, which helps in regularization, and initializes the U matrix as a zero matrix to focus on fine-tuning. Experiments show that CURLoRA not only outperforms traditional LoRA in maintaining task accuracy but also keeps the model's perplexity scores stable, especially when data is limited."}, 'zh': {'title': 'CURLoRA：微调大型语言模型的新方法', 'desc': '本文介绍了一种名为CURLoRA的新方法，用于微调大型语言模型（LLMs），该方法利用CUR矩阵分解和低秩适应（LoRA）。CURLoRA解决了LLM微调中的两个关键挑战：在持续学习中减轻灾难性遗忘和减少可训练参数的数量。我们对CUR分解过程进行了独特的修改，使用反向概率进行列和行选择，作为隐式正则化，并将U矩阵初始化为零矩阵，仅对其进行微调。实验结果表明，CURLoRA在减轻灾难性遗忘方面优于标准的LoRA，同时在多个任务中保持模型的稳定性和性能。'}}}, {'id': 'https://huggingface.co/papers/2408.16672', 'title': 'Jina-ColBERT-v2: A General-Purpose Multilingual Late Interaction Retriever', 'url': 'https://huggingface.co/papers/2408.16672', 'abstract': "Multi-vector dense models, such as ColBERT, have proven highly effective in information retrieval. ColBERT's late interaction scoring approximates the joint query-document attention seen in cross-encoders while maintaining inference efficiency closer to traditional dense retrieval models, thanks to its bi-encoder architecture and recent optimizations in indexing and search. In this paper, we introduce several improvements to the ColBERT model architecture and training pipeline, leveraging techniques successful in the more established single-vector embedding model paradigm, particularly those suited for heterogeneous multilingual data. Our new model, Jina-ColBERT-v2, demonstrates strong performance across a range of English and multilingual retrieval tasks, while also cutting storage requirements by up to 50% compared to previous models.", 'score': 6, 'issue_id': 1, 'pub_date': '2024-08-29', 'pub_date_card': {'ru': '29 августа', 'en': 'August 29', 'zh': '8月29日'}, 'hash': 'de191a3e234adaf4', 'data': {'categories': ['#architecture', '#training', '#multilingual', '#dataset'], 'emoji': '🔍', 'ru': {'title': 'Jina-ColBERT-v2: Эффективный многоязычный поиск с меньшими затратами', 'desc': 'Статья представляет усовершенствованную модель Jina-ColBERT-v2 для информационного поиска. Она основана на архитектуре ColBERT, которая эффективно сочетает преимущества кросс-энкодеров и плотных моделей ретривера. Авторы внедрили улучшения в архитектуру модели и процесс обучения, адаптировав техники из парадигмы одновекторных эмбеддингов. Новая модель демонстрирует высокую производительность в задачах поиска на английском и многоязычных данных, при этом сокращая требования к хранилищу до 50%.'}, 'en': {'title': 'Efficient Multilingual Retrieval with Jina-ColBERT-v2', 'desc': 'This paper presents Jina-ColBERT-v2, an enhanced version of the ColBERT model designed for information retrieval. It combines the efficiency of bi-encoder architectures with improvements from single-vector embedding techniques, particularly for multilingual data. The model achieves high performance in various retrieval tasks while significantly reducing storage needs by up to 50%. These advancements make Jina-ColBERT-v2 a competitive option for both English and multilingual information retrieval applications.'}, 'zh': {'title': '提升信息检索效率的新模型', 'desc': '多向量密集模型，如ColBERT，在信息检索中表现出色。ColBERT通过后期交互评分，近似于交叉编码器中的联合查询-文档注意力，同时保持了接近传统密集检索模型的推理效率。本文介绍了对ColBERT模型架构和训练流程的多项改进，特别是针对异构多语言数据的技术。我们的新模型Jina-ColBERT-v2在多种英语和多语言检索任务中表现强劲，同时相比于之前的模型，存储需求减少了多达50%。'}}}, {'id': 'https://huggingface.co/papers/2408.15827', 'title': 'Automatic Differential Diagnosis using Transformer-Based Multi-Label Sequence Classification', 'url': 'https://huggingface.co/papers/2408.15827', 'abstract': "As the field of artificial intelligence progresses, assistive technologies are becoming more widely used across all industries. The healthcare industry is no different, with numerous studies being done to develop assistive tools for healthcare professionals. Automatic diagnostic systems are one such beneficial tool that can assist with a variety of tasks, including collecting patient information, analyzing test results, and diagnosing patients. However, the idea of developing systems that can provide a differential diagnosis has been largely overlooked in most of these research studies. In this study, we propose a transformer-based approach for providing differential diagnoses based on a patient's age, sex, medical history, and symptoms. We use the DDXPlus dataset, which provides differential diagnosis information for patients based on 49 disease types. Firstly, we propose a method to process the tabular patient data from the dataset and engineer them into patient reports to make them suitable for our research. In addition, we introduce two data modification modules to diversify the training data and consequently improve the robustness of the models. We approach the task as a multi-label classification problem and conduct extensive experiments using four transformer models. All the models displayed promising results by achieving over 97% F1 score on the held-out test set. Moreover, we design additional behavioral tests to get a broader understanding of the models. In particular, for one of our test cases, we prepared a custom test set of 100 samples with the assistance of a doctor. The results on the custom set showed that our proposed data modification modules improved the model's generalization capabilities. We hope our findings will provide future researchers with valuable insights and inspire them to develop reliable systems for automatic differential diagnosis.", 'score': 6, 'issue_id': 1, 'pub_date': '2024-08-28', 'pub_date_card': {'ru': '28 августа', 'en': 'August 28', 'zh': '8月28日'}, 'hash': '4341ae6b1eb888e2', 'data': {'categories': ['#dataset', '#data', '#medicine', '#training'], 'emoji': '🩺', 'ru': {'title': 'Искусственный интеллект на страже дифференциальной диагностики', 'desc': 'Исследователи предложили подход на основе трансформеров для предоставления дифференциальных диагнозов, используя информацию о пациенте. Они использовали набор данных DDXPlus и разработали методы обработки табличных данных пациентов, а также модули модификации данных для улучшения обучения моделей. Эксперименты с четырьмя моделями трансформеров показали многообещающие результаты с F1-мерой более 97% на тестовом наборе. Дополнительные поведенческие тесты, включая пользовательский набор из 100 образцов, подготовленный с помощью врача, продемонстрировали улучшение обобщающей способности моделей.'}, 'en': {'title': 'Transforming Healthcare: Accurate Differential Diagnosis with AI', 'desc': 'This paper presents a novel transformer-based approach for automatic differential diagnosis in healthcare, addressing a gap in existing assistive technologies. The authors utilize the DDXPlus dataset to train models that analyze patient data, including age, sex, medical history, and symptoms, to generate differential diagnoses. They introduce data modification techniques to enhance the diversity of training data, which improves model robustness. The results demonstrate that their models achieve over 97% F1 score, indicating high accuracy, and the study aims to inspire further research in reliable diagnostic systems.'}, 'zh': {'title': '智能辅助诊断，助力医疗决策', 'desc': '本研究提出了一种基于变换器的自动化辅助诊断系统，旨在根据患者的年龄、性别、病史和症状提供鉴别诊断。我们使用DDXPlus数据集，该数据集包含49种疾病类型的鉴别诊断信息。通过对患者数据进行处理和工程化，我们构建了适合研究的患者报告，并引入了数据修改模块以增强训练数据的多样性。实验结果显示，所提出的模型在测试集上达到了超过97%的F1分数，表明其在自动化鉴别诊断中的潜力。'}}}, {'id': 'https://huggingface.co/papers/2408.16245', 'title': 'Large-Scale Multi-omic Biosequence Transformers for Modeling Peptide-Nucleotide Interactions', 'url': 'https://huggingface.co/papers/2408.16245', 'abstract': 'The transformer architecture has revolutionized bioinformatics and driven progress in the understanding and prediction of the properties of biomolecules. Almost all research on large-scale biosequence transformers has focused on one domain at a time (single-omic), usually nucleotides or peptides. These models have seen incredible success in downstream tasks in each domain and have achieved particularly noteworthy breakthroughs in sequences of peptides and structural modeling. However, these single-omic models are naturally incapable of modeling multi-omic tasks, one of the most biologically critical being nucleotide-peptide interactions.   We present our work training the first multi-omic nucleotide-peptide foundation models. We show that these multi-omic models (MOMs) can learn joint representations between various single-omic distributions that are emergently consistent with the Central Dogma of molecular biology, despite only being trained on unlabeled biosequences. We further demonstrate that MOMs can be fine-tuned to achieve state-of-the-art results on peptide-nucleotide interaction tasks, namely predicting the change in Gibbs free energy ({\\Delta}G) of the binding interaction between a given oligonucleotide and peptide, as well as the effect on this binding interaction due to mutations in the oligonucleotide sequence ({\\Delta}{\\Delta}G).   Remarkably, we show that multi-omic biosequence transformers emergently learn useful structural information without any prior structural training, allowing us to predict which peptide residues are most involved in the peptide-nucleotide binding interaction. Lastly, we provide evidence that multi-omic biosequence models are non-inferior to foundation models trained on single-omics distributions, suggesting a more generalized or foundational approach to building these models.', 'score': 4, 'issue_id': 1, 'pub_date': '2024-08-29', 'pub_date_card': {'ru': '29 августа', 'en': 'August 29', 'zh': '8月29日'}, 'hash': 'e76539fcf9b40424', 'data': {'categories': ['#architecture', '#medicine', '#training', '#transfer_learning'], 'emoji': '🧬', 'ru': {'title': 'Мультиомные трансформеры: революция в моделировании биомолекул', 'desc': 'Исследователи представили первые мультиомные модели-основания для нуклеотидов и пептидов. Эти модели, обученные только на немеченных биопоследовательностях, способны создавать совместные представления, соответствующие Центральной догме молекулярной биологии. После дообучения модели достигают передовых результатов в задачах взаимодействия пептидов и нуклеотидов, включая предсказание изменения свободной энергии Гиббса при связывании. Примечательно, что мультиомные трансформеры также обучаются полезной структурной информации без специального обучения на структурных данных.'}, 'en': {'title': 'Revolutionizing Bioinformatics with Multi-Omic Models', 'desc': 'This paper introduces a novel multi-omic model (MOM) that integrates nucleotide and peptide data to enhance the understanding of their interactions. Unlike traditional single-omic models, which focus on one type of biological sequence, MOMs can learn joint representations from both nucleotides and peptides, aligning with the Central Dogma of molecular biology. The authors demonstrate that these models can predict the Gibbs free energy changes in binding interactions and the effects of mutations, achieving state-of-the-art results in these tasks. Additionally, the study reveals that MOMs can extract structural information without prior training, indicating their potential as foundational models in bioinformatics.'}, 'zh': {'title': '多组学模型：生物信息学的新突破', 'desc': '这篇论文介绍了一种新的多组学模型，能够同时处理核苷酸和肽的相互作用。与以往只关注单一组学的模型不同，这些多组学模型可以学习不同组学之间的联合表示，并与分子生物学的中心法则一致。研究表明，这些模型在预测核苷酸与肽的结合自由能变化方面表现出色，甚至在没有结构训练的情况下也能学习到有用的结构信息。最终，结果显示多组学模型在性能上不逊色于单组学模型，表明它们在生物信息学中的广泛应用潜力。'}}}, {'id': 'https://huggingface.co/papers/2408.15300', 'title': 'GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs', 'url': 'https://huggingface.co/papers/2408.15300', 'abstract': 'Parameter Efficient Fine-Tuning (PEFT) methods have gained popularity and democratized the usage of Large Language Models (LLMs). Recent studies have shown that a small subset of weights significantly impacts performance. Based on this observation, we introduce a novel PEFT method, called Gaussian noise Injected Fine Tuning of Salient Weights (GIFT-SW). Our method updates only salient columns, while injecting Gaussian noise into non-salient ones. To identify these columns, we developeda generalized sensitivity metric that extends and unifies metrics from previous studies. Experiments with LLaMA models demonstrate that GIFT-SW outperforms full fine-tuning and modern PEFT methods under the same computational budget. Moreover, GIFT-SW offers practical advantages to recover performance of models subjected to mixed-precision quantization with keeping salient weights in full precision.', 'score': 3, 'issue_id': 1, 'pub_date': '2024-08-27', 'pub_date_card': {'ru': '27 августа', 'en': 'August 27', 'zh': '8月27日'}, 'hash': '97d0a5f80506ca74', 'data': {'categories': ['#training', '#optimization', '#inference'], 'emoji': '🎁', 'ru': {'title': 'Точечная настройка критических параметров улучшает языковые модели', 'desc': 'Статья представляет новый метод эффективной настройки параметров для больших языковых моделей под названием GIFT-SW. Этот подход обновляет только важные веса модели, добавляя гауссовский шум в менее значимые. Авторы разработали обобщенную метрику чувствительности для определения важных весов. Эксперименты показали, что GIFT-SW превосходит полную дообучение и современные методы PEFT при том же вычислительном бюджете.'}, 'en': {'title': 'Efficient Fine-Tuning with GIFT-SW: Focus on What Matters!', 'desc': 'This paper presents a new method called Gaussian noise Injected Fine Tuning of Salient Weights (GIFT-SW) for efficiently fine-tuning Large Language Models (LLMs). The method focuses on updating only the most important weights, known as salient columns, while adding Gaussian noise to the less important weights. A new sensitivity metric is introduced to identify these salient weights, improving upon previous metrics. Experiments show that GIFT-SW not only outperforms traditional fine-tuning methods but also maintains model performance when using mixed-precision quantization.'}, 'zh': {'title': '显著权重微调：高效与性能的完美结合', 'desc': '参数高效微调（PEFT）方法在大型语言模型（LLM）的使用中变得越来越流行。研究表明，少量重要的权重对模型性能有显著影响。我们提出了一种新的PEFT方法，称为显著权重的高斯噪声注入微调（GIFT-SW），该方法仅更新显著列，同时对非显著列注入高斯噪声。实验结果表明，GIFT-SW在相同计算预算下优于完全微调和现代PEFT方法，并且在混合精度量化的情况下能够有效恢复模型性能。'}}}, {'id': 'https://huggingface.co/papers/2408.16667', 'title': 'Iterative Graph Alignment', 'url': 'https://huggingface.co/papers/2408.16667', 'abstract': "By compressing diverse narratives, LLMs go beyond memorization, achieving intelligence by capturing generalizable causal relationships. However, they suffer from local 'representation gaps' due to insufficient training data diversity, limiting their real-world utility, especially in tasks requiring strict alignment to rules. Traditional alignment methods relying on heavy human annotations are inefficient and unscalable. Recent self-alignment techniques also fall short, as they often depend on self-selection based prompting and memorization-based learning. To address these issues, we introduce Iterative Graph Alignment (IGA), an annotation-free rule-based alignment algorithm. A teacher model (VLM) employs Iterative Graph Prompting (IGP) to create logical graphs and reference answers. The student model (LLM) identifies local knowledge gaps by attempting to align its responses with these references, collaborating with helper models to generate diverse answers. These aligned responses are then used for iterative supervised fine-tuning (SFT). Our evaluations across five rule-based scenarios demonstrate IGP's effectiveness, with a 73.12\\% alignment improvement in Claude Sonnet 3.5, and Llama3-8B-Instruct achieving an 86.20\\% improvement, outperforming Claude Sonnet 3.5 in rule-based alignment.", 'score': 2, 'issue_id': 1, 'pub_date': '2024-08-29', 'pub_date_card': {'ru': '29 августа', 'en': 'August 29', 'zh': '8月29日'}, 'hash': 'a3fdd3c0b33eb72a', 'data': {'categories': ['#alignment', '#training', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Выравнивание языковых моделей без аннотаций: новый подход к заполнению пробелов в знаниях', 'desc': 'Статья представляет новый метод выравнивания языковых моделей под названием Iterative Graph Alignment (IGA). Этот метод использует визуально-языковую модель для создания логических графов и эталонных ответов, с которыми затем сравниваются ответы обучаемой модели. IGA позволяет идентифицировать и заполнять локальные пробелы в знаниях модели без необходимости ручной аннотации данных. Эксперименты показали значительное улучшение выравнивания моделей в задачах, требующих строгого соблюдения правил.'}, 'en': {'title': 'Bridging Gaps in Rule-Based Alignment with Iterative Graphs', 'desc': 'This paper discusses the limitations of large language models (LLMs) in understanding and applying rules due to local representation gaps caused by a lack of diverse training data. It introduces Iterative Graph Alignment (IGA), a new method that does not require human annotations for aligning LLMs with rule-based tasks. The approach uses a teacher model to create logical graphs and reference answers, allowing the student model to identify and fill knowledge gaps. The results show significant improvements in alignment performance across various scenarios, demonstrating the effectiveness of the proposed method.'}, 'zh': {'title': '迭代图对齐：提升语言模型的规则对齐能力', 'desc': '本论文提出了一种新的无注释规则对齐算法，称为迭代图对齐（IGA），旨在解决大型语言模型（LLM）在训练数据多样性不足时出现的局部表示差距问题。通过使用教师模型（VLM）生成逻辑图和参考答案，学生模型（LLM）能够识别其响应中的知识缺口，并与辅助模型合作生成多样化的答案。该方法通过迭代监督微调（SFT）来提高模型的对齐能力。实验结果表明，IGA在多个基于规则的场景中显著提高了对齐效果，展示了其在实际应用中的潜力。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents', '#agi', '#alignment (1)', '#architecture (3)', '#audio (1)', '#benchmark (6)', '#cv (5)', '#data (3)', '#dataset (8)', '#diffusion (1)', '#edge_computing', '#ethics', '#games', '#graphs', '#hallucinations (1)', '#inference (2)', '#interpretability', '#long_context', '#math', '#medicine (2)', '#multilingual (3)', '#multimodal (3)', '#optimization (2)', '#plp', '#rag', '#reasoning (2)', '#rl', '#rlhf', '#robotics', '#security', '#story_generation', '#survey (1)', '#synthetic', '#training (8)', '#transfer_learning (1)', '#translation (1)', '#video'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].join(" ");
                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="background-digit">${index + 1}</div>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <h2>${item['data']['emoji']} ${item['title']}</h2>
                            <p class="meta"><svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path transform="translate(0, 2)" fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> ${item['score']}. ${title}</p>
                            <p class="pub-date">📝 ${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>
                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>
                            <p class="tags">${cats}</p>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2024-09-02 09:00',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-09-02 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-09-02 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    