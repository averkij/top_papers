{
    "date": {
        "ru": "14 –∞–ø—Ä–µ–ª—è",
        "en": "April 14",
        "zh": "4Êúà14Êó•"
    },
    "time_utc": "2025-04-14 07:12",
    "weekday": 0,
    "issue_id": 3218,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.08685",
            "title": "Seaweed-7B: Cost-Effective Training of Video Generation Foundation Model",
            "url": "https://huggingface.co/papers/2504.08685",
            "abstract": "This technical report presents a cost-efficient strategy for training a video generation foundation model. We present a mid-sized research model with approximately 7 billion parameters (7B) called Seaweed-7B trained from scratch using 665,000 H100 GPU hours. Despite being trained with moderate computational resources, Seaweed-7B demonstrates highly competitive performance compared to contemporary video generation models of much larger size. Design choices are especially crucial in a resource-constrained setting. This technical report highlights the key design decisions that enhance the performance of the medium-sized diffusion model. Empirically, we make two observations: (1) Seaweed-7B achieves performance comparable to, or even surpasses, larger models trained on substantially greater GPU resources, and (2) our model, which exhibits strong generalization ability, can be effectively adapted across a wide range of downstream applications either by lightweight fine-tuning or continue training. See the project page at https://seaweed.video/",
            "score": 32,
            "issue_id": 3213,
            "pub_date": "2025-04-11",
            "pub_date_card": {
                "ru": "11 –∞–ø—Ä–µ–ª—è",
                "en": "April 11",
                "zh": "4Êúà11Êó•"
            },
            "hash": "43b42333b796033b",
            "authors": [
                "Team Seawead",
                "Ceyuan Yang",
                "Zhijie Lin",
                "Yang Zhao",
                "Shanchuan Lin",
                "Zhibei Ma",
                "Haoyuan Guo",
                "Hao Chen",
                "Lu Qi",
                "Sen Wang",
                "Feng Cheng",
                "Feilong Zuo Xuejiao Zeng",
                "Ziyan Yang",
                "Fangyuan Kong",
                "Zhiwu Qing",
                "Fei Xiao",
                "Meng Wei",
                "Tuyen Hoang",
                "Siyu Zhang",
                "Peihao Zhu",
                "Qi Zhao",
                "Jiangqiao Yan",
                "Liangke Gui",
                "Sheng Bi",
                "Jiashi Li",
                "Yuxi Ren",
                "Rui Wang",
                "Huixia Li",
                "Xuefeng Xiao",
                "Shu Liu",
                "Feng Ling",
                "Heng Zhang",
                "Houmin Wei",
                "Huafeng Kuang",
                "Jerry Duncan",
                "Junda Zhang",
                "Junru Zheng",
                "Li Sun",
                "Manlin Zhang",
                "Renfei Sun",
                "Xiaobin Zhuang",
                "Xiaojie Li",
                "Xin Xia",
                "Xuyan Chi",
                "Yanghua Peng",
                "Yuping Wang",
                "Yuxuan Wang",
                "Zhongkai Zhao",
                "Zhuo Chen",
                "Zuquan Song",
                "Zhenheng Yang",
                "Jiashi Feng",
                "Jianchao Yang",
                "Lu Jiang"
            ],
            "affiliations": [
                "ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.08685.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#optimization",
                    "#transfer_learning",
                    "#small_models",
                    "#diffusion",
                    "#training",
                    "#architecture"
                ],
                "emoji": "üåä",
                "ru": {
                    "title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏",
                    "desc": "–≠—Ç–æ—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –æ—Ç—á–µ—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –æ–±—É—á–µ–Ω–∏—è –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –º–æ–¥–µ–ª—å —Å—Ä–µ–¥–Ω–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ —Å –ø—Ä–∏–º–µ—Ä–Ω–æ 7 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (7B), –Ω–∞–∑–≤–∞–Ω–Ω—É—é Seaweed-7B, –æ–±—É—á–µ–Ω–Ω—É—é —Å –Ω—É–ª—è –∑–∞ 665 000 —á–∞—Å–æ–≤ —Ä–∞–±–æ—Ç—ã GPU H100. –ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ —É–º–µ—Ä–µ–Ω–Ω—ã–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã, Seaweed-7B –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫–æ–∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –≥–æ—Ä–∞–∑–¥–æ –±–æ–ª—å—à–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞. –í –æ—Ç—á–µ—Ç–µ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞—é—Ç—Å—è –∫–ª—é—á–µ–≤—ã–µ –ø—Ä–æ–µ–∫—Ç–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–≤—ã—à–∞—é—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ —Å—Ä–µ–¥–Ω–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞."
                },
                "en": {
                    "title": "Efficient Video Generation with Seaweed-7B: Small Size, Big Impact!",
                    "desc": "This paper introduces Seaweed-7B, a mid-sized video generation model with 7 billion parameters, trained efficiently using 665,000 GPU hours. Despite its smaller size, Seaweed-7B achieves competitive performance against larger models, showcasing the importance of strategic design choices in resource-limited environments. The model demonstrates strong generalization capabilities, allowing it to adapt effectively to various downstream tasks through lightweight fine-tuning or continued training. Overall, the findings suggest that a well-designed medium-sized model can rival larger counterparts while being more cost-effective to train."
                },
                "zh": {
                    "title": "ÁªèÊµéÈ´òÊïàÁöÑËßÜÈ¢ëÁîüÊàêÊ®°ÂûãËÆ≠ÁªÉÁ≠ñÁï•",
                    "desc": "Êú¨ÊäÄÊúØÊä•ÂëäÊèêÂá∫‰∫Ü‰∏ÄÁßçÁªèÊµéÈ´òÊïàÁöÑËßÜÈ¢ëÁîüÊàêÂü∫Á°ÄÊ®°ÂûãËÆ≠ÁªÉÁ≠ñÁï•„ÄÇÊàë‰ª¨‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫Seaweed-7BÁöÑ‰∏≠ÂûãÁ†îÁ©∂Ê®°ÂûãÔºåÂÖ∑ÊúâÁ∫¶70‰∫ø‰∏™ÂèÇÊï∞Ôºå‰ΩøÁî®665,000‰∏™H100 GPUÂ∞èÊó∂‰ªéÈõ∂ÂºÄÂßãËÆ≠ÁªÉ„ÄÇÂ∞ΩÁÆ°ËÆ≠ÁªÉËµÑÊ∫êÈÄÇ‰∏≠ÔºåSeaweed-7BÁöÑÊÄßËÉΩ‰∏éÊõ¥Â§ßËßÑÊ®°ÁöÑÁé∞‰ª£ËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÁõ∏ÊØî‰ªçÁÑ∂ÂÖ∑ÊúâÁ´û‰∫âÂäõ„ÄÇÊä•ÂëäÂº∫Ë∞É‰∫ÜÂú®ËµÑÊ∫êÂèóÈôêÁéØÂ¢É‰∏≠Â¢ûÂº∫‰∏≠ÂûãÊâ©Êï£Ê®°ÂûãÊÄßËÉΩÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂÜ≥Á≠ñ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.08736",
            "title": "GigaTok: Scaling Visual Tokenizers to 3 Billion Parameters for\n  Autoregressive Image Generation",
            "url": "https://huggingface.co/papers/2504.08736",
            "abstract": "In autoregressive (AR) image generation, visual tokenizers compress images into compact discrete latent tokens, enabling efficient training of downstream autoregressive models for visual generation via next-token prediction. While scaling visual tokenizers improves image reconstruction quality, it often degrades downstream generation quality -- a challenge not adequately addressed in existing literature. To address this, we introduce GigaTok, the first approach to simultaneously improve image reconstruction, generation, and representation learning when scaling visual tokenizers. We identify the growing complexity of latent space as the key factor behind the reconstruction vs. generation dilemma. To mitigate this, we propose semantic regularization, which aligns tokenizer features with semantically consistent features from a pre-trained visual encoder. This constraint prevents excessive latent space complexity during scaling, yielding consistent improvements in both reconstruction and downstream autoregressive generation. Building on semantic regularization, we explore three key practices for scaling tokenizers:(1) using 1D tokenizers for better scalability, (2) prioritizing decoder scaling when expanding both encoder and decoder, and (3) employing entropy loss to stabilize training for billion-scale tokenizers. By scaling to 3 space billion parameters, GigaTok achieves state-of-the-art performance in reconstruction, downstream AR generation, and downstream AR representation quality.",
            "score": 16,
            "issue_id": 3216,
            "pub_date": "2025-04-11",
            "pub_date_card": {
                "ru": "11 –∞–ø—Ä–µ–ª—è",
                "en": "April 11",
                "zh": "4Êúà11Êó•"
            },
            "hash": "4af199758c238fd4",
            "authors": [
                "Tianwei Xiong",
                "Jun Hao Liew",
                "Zilong Huang",
                "Jiashi Feng",
                "Xihui Liu"
            ],
            "affiliations": [
                "ByteDance",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.08736.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#cv",
                    "#training"
                ],
                "emoji": "üñºÔ∏è",
                "ru": {
                    "title": "GigaTok: –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤ –±–µ–∑ –∫–æ–º–ø—Ä–æ–º–∏—Å—Å–æ–≤",
                    "desc": "GigaTok - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—é –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤ –¥–ª—è –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –û–Ω —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —É—Ö—É–¥—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–∏ —É–ª—É—á—à–µ–Ω–∏–∏ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø—É—Ç–µ–º –≤–≤–µ–¥–µ–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏. –ú–µ—Ç–æ–¥ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–≥–æ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ —ç–Ω–∫–æ–¥–µ—Ä–∞. GigaTok –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏, –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ –∫–∞—á–µ—Å—Ç–≤–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –¥–ª—è –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π."
                },
                "en": {
                    "title": "GigaTok: Balancing Image Quality and Generation in Autoregressive Models",
                    "desc": "This paper presents GigaTok, a novel approach to enhance autoregressive image generation by improving visual tokenizers. It addresses the challenge of balancing image reconstruction quality with downstream generation quality, which often deteriorates when scaling tokenizers. The authors introduce semantic regularization to align tokenizer features with those from a pre-trained visual encoder, reducing latent space complexity. By implementing key practices for scaling, GigaTok achieves state-of-the-art results in image reconstruction and generation tasks."
                },
                "zh": {
                    "title": "GigaTokÔºöÊèêÂçáÂõæÂÉèÁîüÊàê‰∏éÈáçÂª∫ÁöÑÂàõÊñ∞ÊñπÊ≥ï",
                    "desc": "Âú®Ëá™ÂõûÂΩíÂõæÂÉèÁîüÊàê‰∏≠ÔºåËßÜËßâÊ†áËÆ∞Âô®Â∞ÜÂõæÂÉèÂéãÁº©‰∏∫Á¥ßÂáëÁöÑÁ¶ªÊï£ÊΩúÂú®Ê†áËÆ∞Ôºå‰ªéËÄåÂÆûÁé∞È´òÊïàÁöÑ‰∏ãÊ∏∏Ëá™ÂõûÂΩíÊ®°ÂûãËÆ≠ÁªÉ„ÄÇÂ∞ΩÁÆ°Êâ©Â§ßËßÜËßâÊ†áËÆ∞Âô®ÂèØ‰ª•ÊèêÈ´òÂõæÂÉèÈáçÂª∫Ë¥®ÈáèÔºå‰ΩÜÂæÄÂæÄ‰ºöÈôç‰Ωé‰∏ãÊ∏∏ÁîüÊàêË¥®ÈáèÔºåËøôÊòØÁé∞ÊúâÊñáÁåÆ‰∏≠Êú™ËÉΩÂÖÖÂàÜËß£ÂÜ≥ÁöÑÊåëÊàò„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜGigaTokÔºåËøôÊòØ‰∏ÄÁßçÂú®Êâ©Â§ßËßÜËßâÊ†áËÆ∞Âô®Êó∂ÂêåÊó∂ÊîπÂñÑÂõæÂÉèÈáçÂª∫„ÄÅÁîüÊàêÂíåË°®Á§∫Â≠¶‰π†ÁöÑÈ¶ñ‰∏™ÊñπÊ≥ï„ÄÇÊàë‰ª¨ÈÄöËøáËØ≠‰πâÊ≠£ÂàôÂåñÊù•ÂáèËΩªÊΩúÂú®Á©∫Èó¥Â§çÊùÇÊÄßÔºå‰ªéËÄåÂú®ÈáçÂª∫Âíå‰∏ãÊ∏∏ÁîüÊàê‰πãÈó¥ÂèñÂæó‰∏ÄËá¥ÁöÑÊîπËøõ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.08388",
            "title": "MineWorld: a Real-Time and Open-Source Interactive World Model on\n  Minecraft",
            "url": "https://huggingface.co/papers/2504.08388",
            "abstract": "World modeling is a crucial task for enabling intelligent agents to effectively interact with humans and operate in dynamic environments. In this work, we propose MineWorld, a real-time interactive world model on Minecraft, an open-ended sandbox game which has been utilized as a common testbed for world modeling. MineWorld is driven by a visual-action autoregressive Transformer, which takes paired game scenes and corresponding actions as input, and generates consequent new scenes following the actions. Specifically, by transforming visual game scenes and actions into discrete token ids with an image tokenizer and an action tokenizer correspondingly, we consist the model input with the concatenation of the two kinds of ids interleaved. The model is then trained with next token prediction to learn rich representations of game states as well as the conditions between states and actions simultaneously. In inference, we develop a novel parallel decoding algorithm that predicts the spatial redundant tokens in each frame at the same time, letting models in different scales generate 4 to 7 frames per second and enabling real-time interactions with game players. In evaluation, we propose new metrics to assess not only visual quality but also the action following capacity when generating new scenes, which is crucial for a world model. Our comprehensive evaluation shows the efficacy of MineWorld, outperforming SoTA open-sourced diffusion based world models significantly. The code and model have been released.",
            "score": 4,
            "issue_id": 3215,
            "pub_date": "2025-04-11",
            "pub_date_card": {
                "ru": "11 –∞–ø—Ä–µ–ª—è",
                "en": "April 11",
                "zh": "4Êúà11Êó•"
            },
            "hash": "f87f4b2c67ff61ca",
            "authors": [
                "Junliang Guo",
                "Yang Ye",
                "Tianyu He",
                "Haoyu Wu",
                "Yushu Jiang",
                "Tim Pearce",
                "Jiang Bian"
            ],
            "affiliations": [
                "Microsoft Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.08388.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#benchmark",
                    "#diffusion",
                    "#cv",
                    "#open_source",
                    "#agents",
                    "#games"
                ],
                "emoji": "üïπÔ∏è",
                "ru": {
                    "title": "MineWorld: –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –º–∏—Ä–∞ Minecraft –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏",
                    "desc": "MineWorld - —ç—Ç–æ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å –º–∏—Ä–∞ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è Minecraft, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–æ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–µ. –ú–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –ø–∞—Ä–∞—Ö –∏–≥—Ä–æ–≤—ã—Ö —Å—Ü–µ–Ω –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –¥–µ–π—Å—Ç–≤–∏–π, –≥–µ–Ω–µ—Ä–∏—Ä—É—è –ø–æ—Å–ª–µ–¥—É—é—â–∏–µ —Å—Ü–µ–Ω—ã. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 4-7 –∫–∞–¥—Ä–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω—ã –Ω–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —Å–ª–µ–¥–æ–≤–∞—Ç—å –¥–µ–π—Å—Ç–≤–∏—è–º, MineWorld –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–∏."
                },
                "en": {
                    "title": "MineWorld: Real-Time World Modeling in Minecraft",
                    "desc": "This paper introduces MineWorld, a real-time interactive world model designed for the game Minecraft, which serves as a platform for testing world modeling techniques. The model utilizes a visual-action autoregressive Transformer that processes paired game scenes and actions to predict subsequent scenes based on player interactions. By converting visual inputs and actions into discrete token IDs, MineWorld learns to represent game states and the relationships between actions and outcomes effectively. The authors also present a new parallel decoding algorithm that enhances the model's speed, allowing it to generate multiple frames per second while maintaining high visual quality and action coherence."
                },
                "zh": {
                    "title": "MineWorldÔºöÂÆûÊó∂‰∫íÂä®ÁöÑÊô∫ËÉΩ‰∏ñÁïåÊ®°Âûã",
                    "desc": "Êú¨Á†îÁ©∂ÊèêÂá∫‰∫ÜMineWorldÔºåËøôÊòØ‰∏Ä‰∏™Âü∫‰∫éMinecraftÁöÑÂÆûÊó∂‰∫íÂä®‰∏ñÁïåÊ®°Âûã„ÄÇËØ•Ê®°Âûã‰ΩøÁî®ËßÜËßâ-Âä®‰ΩúËá™ÂõûÂΩíTransformerÔºåËÉΩÂ§üÊ†πÊçÆÊ∏∏ÊàèÂú∫ÊôØÂíåÁõ∏Â∫îÁöÑÂä®‰ΩúÁîüÊàêÊñ∞ÁöÑÂú∫ÊôØ„ÄÇÈÄöËøáÂ∞ÜËßÜËßâÂú∫ÊôØÂíåÂä®‰ΩúËΩ¨Êç¢‰∏∫Á¶ªÊï£ÁöÑÊ†áËÆ∞IDÔºåÊ®°ÂûãËÉΩÂ§üÂ≠¶‰π†Ê∏∏ÊàèÁä∂ÊÄÅÁöÑ‰∏∞ÂØåË°®Á§∫ÂèäÁä∂ÊÄÅ‰∏éÂä®‰Ωú‰πãÈó¥ÁöÑÂÖ≥Á≥ª„ÄÇÂú®ËØÑ‰º∞‰∏≠ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÊñ∞ÁöÑÊåáÊ†áÊù•ËØÑ‰º∞ÁîüÊàêÊñ∞Âú∫ÊôØÁöÑËßÜËßâË¥®ÈáèÂíåÂä®‰ΩúË∑üÈöèËÉΩÂäõÔºåÊòæÁ§∫MineWorldÂú®ÊÄßËÉΩ‰∏äÊòæËëó‰ºò‰∫éÁé∞ÊúâÁöÑ‰∏ñÁïåÊ®°Âûã„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.07963",
            "title": "PixelFlow: Pixel-Space Generative Models with Flow",
            "url": "https://huggingface.co/papers/2504.07963",
            "abstract": "We present PixelFlow, a family of image generation models that operate directly in the raw pixel space, in contrast to the predominant latent-space models. This approach simplifies the image generation process by eliminating the need for a pre-trained Variational Autoencoder (VAE) and enabling the whole model end-to-end trainable. Through efficient cascade flow modeling, PixelFlow achieves affordable computation cost in pixel space. It achieves an FID of 1.98 on 256times256 ImageNet class-conditional image generation benchmark. The qualitative text-to-image results demonstrate that PixelFlow excels in image quality, artistry, and semantic control. We hope this new paradigm will inspire and open up new opportunities for next-generation visual generation models. Code and models are available at https://github.com/ShoufaChen/PixelFlow.",
            "score": 4,
            "issue_id": 3217,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 –∞–ø—Ä–µ–ª—è",
                "en": "April 10",
                "zh": "4Êúà10Êó•"
            },
            "hash": "4cdb0cfa27fd5251",
            "authors": [
                "Shoufa Chen",
                "Chongjian Ge",
                "Shilong Zhang",
                "Peize Sun",
                "Ping Luo"
            ],
            "affiliations": [
                "Adobe",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.07963.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#benchmark",
                    "#architecture",
                    "#cv",
                    "#open_source"
                ],
                "emoji": "üé®",
                "ru": {
                    "title": "PixelFlow: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø—Ä—è–º–æ –≤ –ø–∏–∫—Å–µ–ª—å–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ",
                    "desc": "PixelFlow - —ç—Ç–æ —Å–µ–º–µ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —Ä–∞–±–æ—Ç–∞—é—â–∏—Ö –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –ø–∏–∫—Å–µ–ª–µ–π, –≤ –æ—Ç–ª–∏—á–∏–µ –æ—Ç –ø—Ä–µ–æ–±–ª–∞–¥–∞—é—â–∏—Ö –º–æ–¥–µ–ª–µ–π –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ —É–ø—Ä–æ—â–∞–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —É—Å—Ç—Ä–∞–Ω—è—è –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –≤ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω–æ–º –≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–æ–º –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–µ (VAE) –∏ –ø–æ–∑–≤–æ–ª—è—è –æ–±—É—á–∞—Ç—å –≤—Å—é –º–æ–¥–µ–ª—å —Å–∫–≤–æ–∑–Ω—ã–º –æ–±—Ä–∞–∑–æ–º. –ë–ª–∞–≥–æ–¥–∞—Ä—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º—É –∫–∞—Å–∫–∞–¥–Ω–æ–º—É –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—é –ø–æ—Ç–æ–∫–∞, PixelFlow –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ø—Ä–∏–µ–º–ª–µ–º—ã—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç –≤ –ø–∏–∫—Å–µ–ª—å–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ. –ú–æ–¥–µ–ª—å –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è FID 1.98 –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ —É—Å–ª–æ–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π ImageNet —Ä–∞–∑–º–µ—Ä–æ–º 256x256."
                },
                "en": {
                    "title": "PixelFlow: Revolutionizing Image Generation in Raw Pixel Space",
                    "desc": "PixelFlow is a new type of image generation model that works directly with raw pixels instead of using latent spaces like many existing models. This method simplifies the process by removing the need for a pre-trained Variational Autoencoder (VAE), allowing the entire model to be trained in one go. By using efficient cascade flow modeling, PixelFlow maintains low computational costs while generating high-quality images. It has shown impressive results, achieving a low FID score of 1.98 on the ImageNet benchmark, and demonstrates strong performance in generating artistic and semantically controlled images from text prompts."
                },
                "zh": {
                    "title": "PixelFlowÔºöÊñ∞‰∏Ä‰ª£ÂõæÂÉèÁîüÊàêÊ®°ÂûãÁöÑÁ™ÅÁ†¥",
                    "desc": "PixelFlowÊòØ‰∏ÄÁßçÁõ¥Êé•Âú®ÂéüÂßãÂÉèÁ¥†Á©∫Èó¥‰∏≠Êìç‰ΩúÁöÑÂõæÂÉèÁîüÊàêÊ®°ÂûãÔºå‰∏é‰∏ªÊµÅÁöÑÊΩúÂú®Á©∫Èó¥Ê®°Âûã‰∏çÂêå„ÄÇËøôÁßçÊñπÊ≥ïÁÆÄÂåñ‰∫ÜÂõæÂÉèÁîüÊàêËøáÁ®ãÔºåÊ∂àÈô§‰∫ÜÂØπÈ¢ÑËÆ≠ÁªÉÂèòÂàÜËá™ÁºñÁ†ÅÂô®ÔºàVAEÔºâÁöÑÈúÄÊ±ÇÔºå‰ΩøÊï¥‰∏™Ê®°ÂûãÂèØ‰ª•Á´ØÂà∞Á´ØËÆ≠ÁªÉ„ÄÇÈÄöËøáÈ´òÊïàÁöÑÁ∫ßËÅîÊµÅÂª∫Ê®°ÔºåPixelFlowÂú®ÂÉèÁ¥†Á©∫Èó¥‰∏≠ÂÆûÁé∞‰∫ÜÂèØÊâøÂèóÁöÑËÆ°ÁÆóÊàêÊú¨ÔºåÂπ∂Âú®256x256ÁöÑImageNetÊù°‰ª∂ÂõæÂÉèÁîüÊàêÂü∫ÂáÜ‰∏äËææÂà∞‰∫Ü1.98ÁöÑFIDÂÄº„ÄÇÂÖ∂ÊñáÊú¨Âà∞ÂõæÂÉèÁöÑÁîüÊàêÁªìÊûúÊòæÁ§∫ÔºåPixelFlowÂú®ÂõæÂÉèË¥®Èáè„ÄÅËâ∫ÊúØÊÄßÂíåËØ≠‰πâÊéßÂà∂ÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.07405",
            "title": "FlexIP: Dynamic Control of Preservation and Personality for Customized\n  Image Generation",
            "url": "https://huggingface.co/papers/2504.07405",
            "abstract": "With the rapid advancement of 2D generative models, preserving subject identity while enabling diverse editing has emerged as a critical research focus. Existing methods typically face inherent trade-offs between identity preservation and personalized manipulation. We introduce FlexIP, a novel framework that decouples these objectives through two dedicated components: a Personalization Adapter for stylistic manipulation and a Preservation Adapter for identity maintenance. By explicitly injecting both control mechanisms into the generative model, our framework enables flexible parameterized control during inference through dynamic tuning of the weight adapter. Experimental results demonstrate that our approach breaks through the performance limitations of conventional methods, achieving superior identity preservation while supporting more diverse personalized generation capabilities (Project Page: https://flexip-tech.github.io/flexip/).",
            "score": 3,
            "issue_id": 3213,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 –∞–ø—Ä–µ–ª—è",
                "en": "April 10",
                "zh": "4Êúà10Êó•"
            },
            "hash": "fb73f6a8f480a7a1",
            "authors": [
                "Linyan Huang",
                "Haonan Lin",
                "Yanning Zhou",
                "Kaiwen Xiao"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2504.07405.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#cv",
                    "#multimodal"
                ],
                "emoji": "üé≠",
                "ru": {
                    "title": "–ì–∏–±–∫–∏–π –∫–æ–Ω—Ç—Ä–æ–ª—å –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏ –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö",
                    "desc": "FlexIP - —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è 2D –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏ –æ–±—ä–µ–∫—Ç–∞ –ø—Ä–∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω–æ–º —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤–∞ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞: –∞–¥–∞–ø—Ç–µ—Ä –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–ª—è —Å—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–π –∏ –∞–¥–∞–ø—Ç–µ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∞–Ω–∏—è –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏. –§—Ä–µ–π–º–≤–æ—Ä–∫ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≥–∏–±–∫–æ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä–æ–≤–∞—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –ø—É—Ç–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≤–µ—Å–æ–≤–æ–≥–æ –∞–¥–∞–ø—Ç–µ—Ä–∞ –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ FlexIP –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –ª—É—á—à–µ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –±–æ–ª–µ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è—Ö –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏."
                },
                "en": {
                    "title": "FlexIP: Balancing Identity and Personalization in 2D Generative Models",
                    "desc": "This paper presents FlexIP, a new framework designed to improve 2D generative models by separating the tasks of identity preservation and stylistic manipulation. It introduces two components: a Personalization Adapter that allows for diverse editing styles and a Preservation Adapter that ensures the subject's identity remains intact. By integrating these components, FlexIP enables users to dynamically adjust the balance between identity and personalization during the generation process. Experimental results show that FlexIP outperforms traditional methods, providing better identity retention while allowing for a wider range of personalized outputs."
                },
                "zh": {
                    "title": "ÁÅµÊ¥ªÁöÑË∫´‰ªΩ‰øùÊåÅ‰∏é‰∏™ÊÄßÂåñÁºñËæë",
                    "desc": "ÈöèÁùÄ‰∫åÁª¥ÁîüÊàêÊ®°ÂûãÁöÑÂø´ÈÄüÂèëÂ±ïÔºå‰øùÊåÅ‰∏ª‰ΩìË∫´‰ªΩÂêåÊó∂ÂÆûÁé∞Â§öÊ†∑ÂåñÁºñËæëÊàê‰∏∫‰∫Ü‰∏Ä‰∏™ÈáçË¶ÅÁöÑÁ†îÁ©∂ÊñπÂêë„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏Âú®Ë∫´‰ªΩ‰øùÊåÅÂíå‰∏™ÊÄßÂåñÊìç‰Ωú‰πãÈó¥Â≠òÂú®Âõ∫ÊúâÁöÑÊùÉË°°„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜFlexIPÔºå‰∏Ä‰∏™Êñ∞È¢ñÁöÑÊ°ÜÊû∂ÔºåÈÄöËøá‰∏§‰∏™‰∏ìÈó®ÁöÑÁªÑ‰ª∂Ëß£ËÄ¶Ëøô‰∫õÁõÆÊ†áÔºö‰∏™ÊÄßÂåñÈÄÇÈÖçÂô®Áî®‰∫éÈ£éÊ†ºÂåñÊìç‰ΩúÔºå‰øùÊåÅÈÄÇÈÖçÂô®Áî®‰∫éË∫´‰ªΩÁª¥Êä§„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÁ™ÅÁ†¥‰∫Ü‰º†ÁªüÊñπÊ≥ïÁöÑÊÄßËÉΩÈôêÂà∂ÔºåÂÆûÁé∞‰∫ÜÊõ¥‰ºòÁöÑË∫´‰ªΩ‰øùÊåÅÔºåÂêåÊó∂ÊîØÊåÅÊõ¥Â§öÊ†∑ÂåñÁöÑ‰∏™ÊÄßÂåñÁîüÊàêËÉΩÂäõ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.08716",
            "title": "ModernBERT or DeBERTaV3? Examining Architecture and Data Influence on\n  Transformer Encoder Models Performance",
            "url": "https://huggingface.co/papers/2504.08716",
            "abstract": "Pretrained transformer-encoder models like DeBERTaV3 and ModernBERT introduce architectural advancements aimed at improving efficiency and performance. Although the authors of ModernBERT report improved performance over DeBERTaV3 on several benchmarks, the lack of disclosed training data and the absence of comparisons using a shared dataset make it difficult to determine whether these gains are due to architectural improvements or differences in training data. In this work, we conduct a controlled study by pretraining ModernBERT on the same dataset as CamemBERTaV2, a DeBERTaV3 French model, isolating the effect of model design. Our results show that the previous model generation remains superior in sample efficiency and overall benchmark performance, with ModernBERT's primary advantage being faster training and inference speed. However, the new proposed model still provides meaningful architectural improvements compared to earlier models such as BERT and RoBERTa. Additionally, we observe that high-quality pre-training data accelerates convergence but does not significantly improve final performance, suggesting potential benchmark saturation. These findings show the importance of disentangling pretraining data from architectural innovations when evaluating transformer models.",
            "score": 2,
            "issue_id": 3217,
            "pub_date": "2025-04-11",
            "pub_date_card": {
                "ru": "11 –∞–ø—Ä–µ–ª—è",
                "en": "April 11",
                "zh": "4Êúà11Êó•"
            },
            "hash": "50998c3f1c1cc54d",
            "authors": [
                "Wissam Antoun",
                "Beno√Æt Sagot",
                "Djam√© Seddah"
            ],
            "affiliations": [
                "Inria, Paris, France"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.08716.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#benchmark",
                    "#training"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ vs –¥–∞–Ω–Ω—ã–µ: —á—Ç–æ –≤–∞–∂–Ω–µ–µ –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π?",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –≤ –º–æ–¥–µ–ª—è—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ ModernBERT –∏ DeBERTaV3. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª–∏ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—ã–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç, –ø—Ä–µ–¥–æ–±—É—á–∏–≤ ModernBERT –Ω–∞ —Ç–æ–º –∂–µ –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö, —á—Ç–æ –∏ CamemBERTaV2 (–º–æ–¥–µ–ª—å DeBERTaV3 –¥–ª—è —Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–æ–≥–æ —è–∑—ã–∫–∞). –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –ø—Ä–µ–¥—ã–¥—É—â–µ–µ –ø–æ–∫–æ–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –æ—Å—Ç–∞–µ—Ç—Å—è –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º –ø–æ –≤—ã–±–æ—Ä–∫–µ –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, –Ω–æ ModernBERT –±—ã—Å—Ç—Ä–µ–µ –≤ –æ–±—É—á–µ–Ω–∏–∏ –∏ –≤—ã–≤–æ–¥–µ. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç–∞–∫–∂–µ –≤—ã—è–≤–∏–ª–æ, —á—Ç–æ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è —É—Å–∫–æ—Ä—è—é—Ç —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å, –Ω–æ –Ω–µ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞—é—Ç –∫–æ–Ω–µ—á–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å."
                },
                "en": {
                    "title": "Disentangling Architecture from Data in Transformer Models",
                    "desc": "This paper investigates the performance of ModernBERT compared to DeBERTaV3 by controlling for training data. The authors find that while ModernBERT shows faster training and inference speeds, DeBERTaV3 outperforms it in terms of sample efficiency and overall benchmark results. The study highlights that high-quality pre-training data can speed up the training process but does not necessarily enhance final model performance. Ultimately, the research emphasizes the need to separate the effects of model architecture from the quality of training data when assessing transformer models."
                },
                "zh": {
                    "title": "Êû∂ÊûÑÂàõÊñ∞‰∏éÈ¢ÑËÆ≠ÁªÉÊï∞ÊçÆÁöÑÂàÜÁ¶ªËØÑ‰º∞",
                    "desc": "Êú¨ÊñáÁ†îÁ©∂‰∫ÜÈ¢ÑËÆ≠ÁªÉÁöÑÂèòÊç¢Âô®ÁºñÁ†ÅÂô®Ê®°ÂûãÔºåÂ¶ÇDeBERTaV3ÂíåModernBERTÔºåÊó®Âú®ÊèêÈ´òÊïàÁéáÂíåÊÄßËÉΩ„ÄÇÂ∞ΩÁÆ°ModernBERTÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºò‰∫éDeBERTaV3Ôºå‰ΩÜÁî±‰∫éÁº∫‰πèÂÖ¨ÂºÄÁöÑËÆ≠ÁªÉÊï∞ÊçÆÂíåÂÖ±‰∫´Êï∞ÊçÆÈõÜÁöÑÊØîËæÉÔºåÈöæ‰ª•Âà§Êñ≠Ëøô‰∫õÊèêÂçáÊòØÁî±‰∫éÊû∂ÊûÑÊîπËøõËøòÊòØËÆ≠ÁªÉÊï∞ÊçÆÁöÑÂ∑ÆÂºÇ„ÄÇÈÄöËøáÂú®‰∏éCamemBERTaV2Áõ∏ÂêåÁöÑÊï∞ÊçÆÈõÜ‰∏äÈ¢ÑËÆ≠ÁªÉModernBERTÔºåÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÊóß‰∏Ä‰ª£Ê®°ÂûãÂú®Ê†∑Êú¨ÊïàÁéáÂíåÊï¥‰ΩìÂü∫ÂáÜÊÄßËÉΩ‰∏ä‰ªçÁÑ∂‰ºò‰∫éÊñ∞Ê®°ÂûãÔºåModernBERTÁöÑ‰∏ªË¶Å‰ºòÂäøÂú®‰∫éÊõ¥Âø´ÁöÑËÆ≠ÁªÉÂíåÊé®ÁêÜÈÄüÂ∫¶„ÄÇÁ†îÁ©∂ÁªìÊûúÂº∫Ë∞É‰∫ÜÂú®ËØÑ‰º∞ÂèòÊç¢Âô®Ê®°ÂûãÊó∂ÔºåÂ∞ÜÈ¢ÑËÆ≠ÁªÉÊï∞ÊçÆ‰∏éÊû∂ÊûÑÂàõÊñ∞ÂàÜÂºÄËÄÉËôëÁöÑÈáçË¶ÅÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.08600",
            "title": "SQL-R1: Training Natural Language to SQL Reasoning Model By\n  Reinforcement Learning",
            "url": "https://huggingface.co/papers/2504.08600",
            "abstract": "Natural Language to SQL (NL2SQL) enables intuitive interactions with databases by transforming natural language queries into structured SQL statements. Despite recent advancements in enhancing human-computer interaction within database applications, significant challenges persist, particularly regarding the inference performance in complex scenarios involving multi-table joins and nested queries. Current methodologies primarily utilize supervised fine-tuning (SFT) to train the NL2SQL model, which may limit adaptability and interpretability in new environments (e.g., finance and healthcare). In order to enhance the reasoning performance of the NL2SQL model in the above complex situations, we introduce SQL-R1, a novel NL2SQL reasoning model trained by the reinforcement learning (RL) algorithms. We design a specialized RL-based reward function tailored for NL2SQL tasks and discussed the impact of cold start on the effectiveness of intensive training. In addition, we achieve competitive accuracy using only a tiny amount of synthetic NL2SQL data for augmented training and further explore data engineering for RL. In existing experiments, SQL-R1 achieves execution accuracy of 88.6% and 66.6% on the benchmark Spider and BIRD, respectively, only using the 7B base model.",
            "score": 2,
            "issue_id": 3217,
            "pub_date": "2025-04-11",
            "pub_date_card": {
                "ru": "11 –∞–ø—Ä–µ–ª—è",
                "en": "April 11",
                "zh": "4Êúà11Êó•"
            },
            "hash": "6153f561c5040630",
            "authors": [
                "Peixian Ma",
                "Xialie Zhuang",
                "Chengjin Xu",
                "Xuhui Jiang",
                "Ran Chen",
                "Jian Guo"
            ],
            "affiliations": [
                "DataArc Tech Ltd.",
                "IDEA Research, International Digital Economy Academy",
                "The Hong Kong University of Science and Technology (Guangzhou)",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.08600.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#benchmark",
                    "#training",
                    "#rl",
                    "#dataset",
                    "#reasoning"
                ],
                "emoji": "ü§ñ",
                "ru": {
                    "title": "–û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —É–ª—É—á—à–∞–µ—Ç –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ –≤ SQL",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å SQL-R1 –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ –≤ SQL-–∑–∞–ø—Ä–æ—Å—ã. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ —Å–ª–æ–∂–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö —Å –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç–∞–±–ª–∏—Ü –∏ –≤–ª–æ–∂–µ–Ω–Ω—ã–º–∏ –∑–∞–ø—Ä–æ—Å–∞–º–∏. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –¥–ª—è –∑–∞–¥–∞—á NL2SQL –∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–ª–∏ –≤–ª–∏—è–Ω–∏–µ —Ö–æ–ª–æ–¥–Ω–æ–≥–æ —Å—Ç–∞—Ä—Ç–∞ –Ω–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. SQL-R1 –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –Ω–∞ —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö Spider –∏ BIRD, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å —Ä–∞–∑–º–µ—Ä–æ–º 7 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤."
                },
                "en": {
                    "title": "Transforming Natural Language Queries with Reinforcement Learning",
                    "desc": "This paper presents SQL-R1, a new model designed to convert natural language queries into SQL statements more effectively, especially in complex scenarios like multi-table joins. It addresses the limitations of traditional supervised fine-tuning methods by employing reinforcement learning (RL) to improve reasoning performance. The authors introduce a specialized reward function for NL2SQL tasks and explore the challenges of cold start in training. SQL-R1 demonstrates impressive execution accuracy on benchmark datasets, achieving 88.6% on Spider and 66.6% on BIRD, using minimal synthetic data for training."
                },
                "zh": {
                    "title": "ÊèêÂçáNL2SQLÊé®ÁêÜÊÄßËÉΩÁöÑÊñ∞ÊñπÊ≥ï",
                    "desc": "Ëá™ÁÑ∂ËØ≠Ë®ÄËΩ¨SQLÔºàNL2SQLÔºâ‰ΩøÁî®Êà∑ËÉΩÂ§üÈÄöËøáËá™ÁÑ∂ËØ≠Ë®ÄÊü•ËØ¢‰∏éÊï∞ÊçÆÂ∫ìËøõË°åÁõ¥ËßÇ‰∫§‰∫í„ÄÇÂ∞ΩÁÆ°Âú®Êï∞ÊçÆÂ∫ìÂ∫îÁî®‰∏≠‰∫∫Êú∫‰∫§‰∫íÊñπÈù¢ÂèñÂæó‰∫Ü‰∏Ä‰∫õËøõÂ±ïÔºå‰ΩÜÂú®Â§çÊùÇÂú∫ÊôØ‰∏ãÁöÑÊé®ÁêÜÊÄßËÉΩ‰ªçÁÑ∂Èù¢‰∏¥ÈáçÂ§ßÊåëÊàòÔºåÂ∞§ÂÖ∂ÊòØÊ∂âÂèäÂ§öË°®ËøûÊé•ÂíåÂµåÂ•óÊü•ËØ¢ÁöÑÊÉÖÂÜµ„ÄÇÂΩìÂâçÁöÑÊñπÊ≥ï‰∏ªË¶Å‰æùËµñÁõëÁù£ÂæÆË∞ÉÔºàSFTÔºâÊù•ËÆ≠ÁªÉNL2SQLÊ®°ÂûãÔºåËøôÂèØËÉΩÈôêÂà∂‰∫ÜÂÖ∂Âú®Êñ∞ÁéØÂ¢É‰∏≠ÁöÑÈÄÇÂ∫îÊÄßÂíåÂèØËß£ÈáäÊÄß„ÄÇ‰∏∫ÊèêÈ´òNL2SQLÊ®°ÂûãÂú®Â§çÊùÇÊÉÖÂÜµ‰∏ãÁöÑÊé®ÁêÜÊÄßËÉΩÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜSQL-R1ÔºåËøôÊòØ‰∏ÄÁßçÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÁÆóÊ≥ïËÆ≠ÁªÉÁöÑÊñ∞ÂûãNL2SQLÊé®ÁêÜÊ®°Âûã„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.08366",
            "title": "In-2-4D: Inbetweening from Two Single-View Images to 4D Generation",
            "url": "https://huggingface.co/papers/2504.08366",
            "abstract": "We propose a new problem, In-2-4D, for generative 4D (i.e., 3D + motion) inbetweening from a minimalistic input setting: two single-view images capturing an object in two distinct motion states. Given two images representing the start and end states of an object in motion, our goal is to generate and reconstruct the motion in 4D. We utilize a video interpolation model to predict the motion, but large frame-to-frame motions can lead to ambiguous interpretations. To overcome this, we employ a hierarchical approach to identify keyframes that are visually close to the input states and show significant motion, then generate smooth fragments between them. For each fragment, we construct the 3D representation of the keyframe using Gaussian Splatting. The temporal frames within the fragment guide the motion, enabling their transformation into dynamic Gaussians through a deformation field. To improve temporal consistency and refine 3D motion, we expand the self-attention of multi-view diffusion across timesteps and apply rigid transformation regularization. Finally, we merge the independently generated 3D motion segments by interpolating boundary deformation fields and optimizing them to align with the guiding video, ensuring smooth and flicker-free transitions. Through extensive qualitative and quantitiave experiments as well as a user study, we show the effectiveness of our method and its components. The project page is available at https://in-2-4d.github.io/",
            "score": 2,
            "issue_id": 3213,
            "pub_date": "2025-04-11",
            "pub_date_card": {
                "ru": "11 –∞–ø—Ä–µ–ª—è",
                "en": "April 11",
                "zh": "4Êúà11Êó•"
            },
            "hash": "df65a8f6baab7f84",
            "authors": [
                "Sauradip Nag",
                "Daniel Cohen-Or",
                "Hao Zhang",
                "Ali Mahdavi-Amiri"
            ],
            "affiliations": [
                "Simon Fraser University, Canada",
                "Tel Aviv University, Israel"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.08366.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#diffusion",
                    "#3d",
                    "#optimization"
                ],
                "emoji": "üéûÔ∏è",
                "ru": {
                    "title": "–ò–∑ 2D –≤ 4D: –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–≥–æ –¥–≤–∏–∂–µ–Ω–∏—è –ø–æ –¥–≤—É–º –∫–∞–¥—Ä–∞–º",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –∑–∞–¥–∞—á—É In-2-4D –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ 4D-–∏–Ω—Ç–µ—Ä–ø–æ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–≤—É—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –æ–±—ä–µ–∫—Ç–∞ –≤ —Ä–∞–∑–Ω—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏—è—Ö –¥–≤–∏–∂–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç –º–æ–¥–µ–ª—å –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏–∏ –≤–∏–¥–µ–æ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏—è –∏ –ø—Ä–∏–º–µ–Ω—è—é—Ç –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö –∫–∞–¥—Ä–æ–≤. –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–µ—Ç—Å—è 3D-–ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é Gaussian Splatting, –∞ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∫–∞–¥—Ä—ã —É–ø—Ä–∞–≤–ª—è—é—Ç –¥–≤–∏–∂–µ–Ω–∏–µ–º —á–µ—Ä–µ–∑ –ø–æ–ª–µ –¥–µ—Ñ–æ—Ä–º–∞—Ü–∏–∏. –ú–µ—Ç–æ–¥ —É–ª—É—á—à–∞–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å, —Ä–∞—Å—à–∏—Ä—è—è —Å–∞–º–æ–≤–Ω–∏–º–∞–Ω–∏–µ –º—É–ª—å—Ç–∏—Ä–∞–∫—É—Ä—Å–Ω–æ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏ –∏ –ø—Ä–∏–º–µ–Ω—è—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é –∂–µ—Å—Ç–∫–æ–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏."
                },
                "en": {
                    "title": "Transforming 2D Images into Smooth 4D Motion!",
                    "desc": "This paper introduces the In-2-4D problem, which focuses on generating 4D representations of motion from just two images of an object in different states. The authors propose a hierarchical method that identifies keyframes to create smooth transitions between these states, addressing challenges posed by large frame-to-frame motions. They utilize Gaussian Splatting to construct 3D representations and apply a deformation field to guide the transformation of these representations into dynamic forms. The approach enhances temporal consistency and smoothness by merging motion segments and optimizing them to align with the original video, demonstrating its effectiveness through various experiments."
                },
                "zh": {
                    "title": "‰ªéÈùôÊÄÅÂà∞Âä®ÊÄÅÔºö4DËøêÂä®ÊèíÂÄºÁöÑÊñ∞ÊñπÊ≥ï",
                    "desc": "Êàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑÈóÆÈ¢òÔºåIn-2-4DÔºåÊó®Âú®‰ªé‰∏§‰∏™‰∏çÂêåËøêÂä®Áä∂ÊÄÅÁöÑÂçïËßÜÂõæÂõæÂÉè‰∏≠ÁîüÊàê4DÔºàÂç≥3D + Âä®‰ΩúÔºâÊèíÂÄº„ÄÇÁªôÂÆöË°®Á§∫Áâ©‰ΩìËøêÂä®Ëµ∑ÂßãÂíåÁªìÊùüÁä∂ÊÄÅÁöÑ‰∏§ÂπÖÂõæÂÉèÔºåÊàë‰ª¨ÁöÑÁõÆÊ†áÊòØÁîüÊàêÂíåÈáçÂª∫4D‰∏≠ÁöÑËøêÂä®„ÄÇÊàë‰ª¨ÈááÁî®ËßÜÈ¢ëÊèíÂÄºÊ®°ÂûãÊù•È¢ÑÊµãËøêÂä®Ôºå‰ΩÜÂ§ßÂπÖÂ∫¶ÁöÑÂ∏ßÈó¥ËøêÂä®ÂèØËÉΩÂØºËá¥Ê®°Á≥äÁöÑËß£Èáä„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨‰ΩøÁî®ÂàÜÂ±ÇÊñπÊ≥ïËØÜÂà´‰∏éËæìÂÖ•Áä∂ÊÄÅËßÜËßâ‰∏äÊé•Ëøë‰∏îËøêÂä®ÊòæËëóÁöÑÂÖ≥ÈîÆÂ∏ßÔºåÁÑ∂ÂêéÂú®ÂÆÉ‰ª¨‰πãÈó¥ÁîüÊàêÂπ≥ÊªëÁöÑÁâáÊÆµ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.08192",
            "title": "SAEs Can Improve Unlearning: Dynamic Sparse Autoencoder\n  Guardrails for Precision Unlearning in LLMs",
            "url": "https://huggingface.co/papers/2504.08192",
            "abstract": "Machine unlearning is a promising approach to improve LLM safety by removing unwanted knowledge from the model. However, prevailing gradient-based unlearning methods suffer from issues such as high computational costs, hyperparameter instability, poor sequential unlearning capability, vulnerability to relearning attacks, low data efficiency, and lack of interpretability. While Sparse Autoencoders are well-suited to improve these aspects by enabling targeted activation-based unlearning, prior approaches underperform gradient-based methods. This work demonstrates that, contrary to these earlier findings, SAEs can significantly improve unlearning when employed dynamically. We introduce Dynamic DAE Guardrails (DSG), a novel method for precision unlearning that leverages principled feature selection and a dynamic classifier. Our experiments show DSG substantially outperforms leading unlearning methods, achieving superior forget-utility trade-offs. DSG addresses key drawbacks of gradient-based approaches for unlearning -- offering enhanced computational efficiency and stability, robust performance in sequential unlearning, stronger resistance to relearning attacks, better data efficiency including zero-shot settings, and more interpretable unlearning.",
            "score": 1,
            "issue_id": 3218,
            "pub_date": "2025-04-11",
            "pub_date_card": {
                "ru": "11 –∞–ø—Ä–µ–ª—è",
                "en": "April 11",
                "zh": "4Êúà11Êó•"
            },
            "hash": "e6e92f7d8a3d930d",
            "authors": [
                "Aashiq Muhamed",
                "Jacopo Bonato",
                "Mona Diab",
                "Virginia Smith"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "Leonardo Labs"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.08192.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#security",
                    "#interpretability",
                    "#optimization"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–¢–æ—á–Ω–æ–µ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–∞–∑–æ–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π —Å –ø–æ–º–æ—â—å—é –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–æ–≤",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –º–∞—à–∏–Ω–Ω–æ–≥–æ —Ä–∞–∑–æ–±—É—á–µ–Ω–∏—è –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Dynamic DAE Guardrails (DSG), –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã—Ö –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–∞—Ö. DSG –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —É–¥–∞–ª—è—Ç—å –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞–Ω–∏—è –∏–∑ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –ø–æ —Ä—è–¥—É –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –ú–µ—Ç–æ–¥ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ª—É—á—à–∏–π –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –∑–∞–±—ã–≤–∞–Ω–∏–µ–º –∏ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç—å—é, –ø–æ–≤—ã—à–µ–Ω–Ω—É—é –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å. DSG —Ç–∞–∫–∂–µ –æ–±–ª–∞–¥–∞–µ—Ç —É–ª—É—á—à–µ–Ω–Ω–æ–π —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å—é –∫ –∞—Ç–∞–∫–∞–º –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ –±–æ–ª—å—à–µ–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å—é –ø—Ä–æ—Ü–µ—Å—Å–∞ —Ä–∞–∑–æ–±—É—á–µ–Ω–∏—è."
                },
                "en": {
                    "title": "Dynamic Unlearning: Enhancing Safety in LLMs with DSG",
                    "desc": "This paper discusses a new method called Dynamic DAE Guardrails (DSG) for improving machine unlearning in large language models (LLMs). Traditional gradient-based unlearning methods face several challenges, including high computational costs and vulnerability to relearning attacks. The authors propose using Sparse Autoencoders (SAEs) to enhance unlearning efficiency and stability, demonstrating that DSG can outperform existing methods. Their experiments show that DSG provides better performance in terms of forget-utility trade-offs, making unlearning more effective and interpretable."
                },
                "zh": {
                    "title": "Âä®ÊÄÅÂéªÂô™Ëá™ÁºñÁ†ÅÂô®ÔºöÊèêÂçáÊú∫Âô®ÈÅóÂøòÁöÑÊïàÁéá‰∏éÁ®≥ÂÆöÊÄß",
                    "desc": "Êú∫Âô®ÈÅóÂøòÊòØ‰∏ÄÁßçÊúâÂâçÊôØÁöÑÊñπÊ≥ïÔºåÂèØ‰ª•ÈÄöËøá‰ªéÊ®°Âûã‰∏≠ÁßªÈô§‰∏çÂøÖË¶ÅÁöÑÁü•ËØÜÊù•ÊèêÈ´òÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÂÆâÂÖ®ÊÄß„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÁöÑÂü∫‰∫éÊ¢ØÂ∫¶ÁöÑÈÅóÂøòÊñπÊ≥ïÂ≠òÂú®ËÆ°ÁÆóÊàêÊú¨È´ò„ÄÅË∂ÖÂèÇÊï∞‰∏çÁ®≥ÂÆö„ÄÅÈ°∫Â∫èÈÅóÂøòËÉΩÂäõÂ∑ÆÁ≠âÈóÆÈ¢ò„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂä®ÊÄÅÂéªÂô™Ëá™ÁºñÁ†ÅÂô®ÊñπÊ≥ïÔºàDSGÔºâÔºåÈÄöËøáÁ≤æÁ°ÆÁöÑÁâπÂæÅÈÄâÊã©ÂíåÂä®ÊÄÅÂàÜÁ±ªÂô®Êù•ÂÆûÁé∞Êõ¥ÊúâÊïàÁöÑÈÅóÂøò„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåDSGÂú®ÈÅóÂøòÊïàÁéáÂíåÂÆûÁî®ÊÄß‰πãÈó¥ÂèñÂæó‰∫ÜÊòæËëóÁöÑÂπ≥Ë°°ÔºåË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÈÅóÂøòÊñπÊ≥ï„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.01883",
            "title": "CoRAG: Collaborative Retrieval-Augmented Generation",
            "url": "https://huggingface.co/papers/2504.01883",
            "abstract": "Retrieval-Augmented Generation (RAG) models excel in knowledge-intensive tasks, especially under few-shot learning constraints. We introduce CoRAG, a framework extending RAG to collaborative settings, where clients jointly train a shared model using a collaborative passage store. To evaluate CoRAG, we introduce CRAB, a benchmark for collaborative homogeneous open-domain question answering. Our experiments demonstrate that CoRAG consistently outperforms both parametric collaborative learning methods and locally trained RAG models in low-resource scenarios. Further analysis reveals the critical importance of relevant passages within the shared store, the surprising benefits of incorporating irrelevant passages, and the potential for hard negatives to negatively impact performance. This introduces a novel consideration in collaborative RAG: the trade-off between leveraging a collectively enriched knowledge base and the potential risk of incorporating detrimental passages from other clients. Our findings underscore the viability of CoRAG, while also highlighting key design challenges and promising avenues for future research.",
            "score": 1,
            "issue_id": 3218,
            "pub_date": "2025-04-02",
            "pub_date_card": {
                "ru": "2 –∞–ø—Ä–µ–ª—è",
                "en": "April 2",
                "zh": "4Êúà2Êó•"
            },
            "hash": "f3bd4bbb45b0315a",
            "authors": [
                "Aashiq Muhamed",
                "Mona Diab",
                "Virginia Smith"
            ],
            "affiliations": [
                "Language Technologies Institute, Carnegie Mellon University",
                "Machine Learning Department, Carnegie Mellon University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.01883.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#rag",
                    "#benchmark",
                    "#low_resource"
                ],
                "emoji": "ü§ù",
                "ru": {
                    "title": "CoRAG: –°–æ–≤–º–µ—Å—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç CoRAG - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ RAG. –ê–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ CRAB –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–æ–ª–ª–∞–±–æ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ CoRAG –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –¥—Ä—É–≥–∏–µ –º–µ—Ç–æ–¥—ã –≤ —É—Å–ª–æ–≤–∏—è—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–ª—è–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –ø–∞—Å—Å–∞–∂–µ–π –≤ –æ–±—â–µ–º —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –∏ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ —Ä–∏—Å–∫–∏ –≤–∫–ª—é—á–µ–Ω–∏—è –≤—Ä–µ–¥–Ω—ã—Ö –ø–∞—Å—Å–∞–∂–µ–π –æ—Ç –¥—Ä—É–≥–∏—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤."
                },
                "en": {
                    "title": "Collaborative Learning with CoRAG: Enhancing RAG Models Together!",
                    "desc": "The paper presents CoRAG, a new framework that enhances Retrieval-Augmented Generation (RAG) models for collaborative learning environments. In this setup, multiple clients work together to train a shared model using a common passage store, which improves performance on knowledge-intensive tasks. The authors introduce CRAB, a benchmark for evaluating collaborative question answering, and show that CoRAG outperforms traditional methods in low-resource situations. Key insights include the importance of relevant passages, the unexpected advantages of irrelevant ones, and the risks posed by hard negatives, highlighting the complexities of collaborative knowledge sharing."
                },
                "zh": {
                    "title": "Âçè‰ΩúÂ¢ûÂº∫ÁîüÊàêÔºöÂÖ±‰∫´Áü•ËØÜÁöÑÂäõÈáè‰∏éÊåëÊàò",
                    "desc": "Êú¨ËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫CoRAGÁöÑÊ°ÜÊû∂ÔºåÂÆÉÊâ©Â±ï‰∫ÜÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÔºàRAGÔºâÊ®°ÂûãÔºå‰ΩøÂÖ∂ËÉΩÂ§üÂú®Âçè‰ΩúÁéØÂ¢É‰∏≠ÂÖ±ÂêåËÆ≠ÁªÉÂÖ±‰∫´Ê®°Âûã„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜCRABÂü∫ÂáÜÔºåÁî®‰∫éËØÑ‰º∞Âçè‰ΩúÂêåË¥®ÂºÄÊîæÂüüÈóÆÁ≠îÁöÑÊÄßËÉΩ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåCoRAGÂú®‰ΩéËµÑÊ∫êÂú∫ÊôØ‰∏ãÁöÑË°®Áé∞‰ºò‰∫é‰º†ÁªüÁöÑÂèÇÊï∞Âçè‰ΩúÂ≠¶‰π†ÊñπÊ≥ïÂíåÊú¨Âú∞ËÆ≠ÁªÉÁöÑRAGÊ®°Âûã„ÄÇÁ†îÁ©∂ËøòÊè≠Á§∫‰∫ÜÂÖ±‰∫´Áü•ËØÜÂ∫ì‰∏≠Áõ∏ÂÖ≥ÊÆµËêΩÁöÑÈáçË¶ÅÊÄßÔºå‰ª•ÂèäÂºïÂÖ•Êó†ÂÖ≥ÊÆµËêΩÁöÑÊÑèÂ§ñÂ•ΩÂ§ÑÂíåÂõ∞ÈöæË¥üÊ†∑Êú¨ÂØπÊÄßËÉΩÁöÑÊΩúÂú®Ë¥üÈù¢ÂΩ±Âìç„ÄÇ"
                }
            }
        }
    ],
    "link_prev": "2025-04-11.html",
    "link_next": "2025-04-15.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "11.04",
        "en": "04/11",
        "zh": "4Êúà11Êó•"
    },
    "short_date_next": {
        "ru": "15.04",
        "en": "04/15",
        "zh": "4Êúà15Êó•"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 5,
        "#agents": 1,
        "#cv": 4,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 2,
        "#3d": 1,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 4,
        "#healthcare": 0,
        "#training": 5,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 1,
        "#transfer_learning": 2,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 5,
        "#survey": 0,
        "#diffusion": 4,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 1
    },
    "zh": {
        "text": "Êàë‰ª¨‰ªãÁªç‰∫ÜKimi-VLÔºå‰∏Ä‰∏™ÂºÄÊ∫êÁöÑÊ∑∑Âêà‰∏ìÂÆ∂ËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã„ÄÇÂÆÉÂÖ∑ÊúâÂÖàËøõÁöÑÂ§öÊ®°ÊÄÅÊé®ÁêÜ„ÄÅÈïø‰∏ä‰∏ãÊñáÁêÜËß£ÂíåÂº∫Â§ßÁöÑ‰ª£ÁêÜËÉΩÂäõÔºåÂè™ÊøÄÊ¥ª2.8B‰∏™ËØ≠Ë®ÄËß£Á†ÅÂô®ÂèÇÊï∞„ÄÇKimi-VLÂú®Â§öËΩÆ‰ª£ÁêÜ‰ªªÂä°ÂíåÂ§öÁßçËßÜËßâËØ≠Ë®Ä‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇÂÆÉËøòËÉΩÂ§ÑÁêÜÈïø‰∏ä‰∏ãÊñáÂíåÈ´òÂàÜËæ®ÁéáËßÜËßâËæìÂÖ•„ÄÇÂü∫‰∫éKimi-VLÔºåÊàë‰ª¨ÂºÄÂèë‰∫ÜKimi-VL-ThinkingÔºåÂÖ∑ÊúâÂº∫Â§ßÁöÑÈïøÊó∂Èó¥Êé®ÁêÜËÉΩÂäõ„ÄÇ‰ª£Á†ÅÂíåÊ®°ÂûãÂú®https://github.com/MoonshotAI/Kimi-VLÂÖ¨ÂºÄ„ÄÇ",
        "title": "Kimi-VL Technical Report",
        "pinyin": "Êàë‰ª¨‰ªãÁªç‰∫ÜKimi-VLÔºå‰∏Ä‰∏™ÂºÄÊ∫êÁöÑÊ∑∑Âêà‰∏ìÂÆ∂ËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã„ÄÇÂÆÉÂÖ∑ÊúâÂÖàËøõÁöÑÂ§öÊ®°ÊÄÅÊé®ÁêÜ„ÄÅÈïø‰∏ä‰∏ãÊñáÁêÜËß£ÂíåÂº∫Â§ßÁöÑ‰ª£ÁêÜËÉΩÂäõÔºåÂè™ÊøÄÊ¥ª2.8B‰∏™ËØ≠Ë®ÄËß£Á†ÅÂô®ÂèÇÊï∞„ÄÇKimi-VLÂú®Â§öËΩÆ‰ª£ÁêÜ‰ªªÂä°ÂíåÂ§öÁßçËßÜËßâËØ≠Ë®Ä‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇÂÆÉËøòËÉΩÂ§ÑÁêÜÈïø‰∏ä‰∏ãÊñáÂíåÈ´òÂàÜËæ®ÁéáËßÜËßâËæìÂÖ•„ÄÇÂü∫‰∫éKimi-VLÔºåÊàë‰ª¨ÂºÄÂèë‰∫ÜKimi-VL-ThinkingÔºåÂÖ∑ÊúâÂº∫Â§ßÁöÑÈïøÊó∂Èó¥Êé®ÁêÜËÉΩÂäõ„ÄÇ‰ª£Á†ÅÂíåÊ®°ÂûãÂú®https://github.com/MoonshotAI/Kimi-VLÂÖ¨ÂºÄ„ÄÇ\n\nW«ímen ji√®sh√†o le Kimi-VL, yƒ´g√® kƒÅiyu√°n de h√πn h√© zhuƒÅnjiƒÅ sh√¨jiu√®-y«îy√°n m√≥x√≠ng. TƒÅ j√πy«íu xiƒÅnj√¨n de du≈ç m√≥sh√¨ tu√≠ l«ê, ch√°ng sh√†ng xi√† w√©n l«êjiƒõ h√© qi√°ngd√† de d√†il«ê n√©ngl√¨, zh«ê jƒ´hu√≥ 2.8B g√® y«îy√°n jiƒõm«éq√¨ cƒÅnsh√π. Kimi-VL z√†i du≈ç l√∫n d√†il«ê r√®nw√π h√© du≈ç zh«íng sh√¨jiu√® y«îy√°n r√®nw√π zh≈çng bi«éoxi√†n ch≈´s√®. TƒÅ h√°i n√©ng ch«îl«ê ch√°ng sh√†ng xi√† w√©n h√© gƒÅo fƒìnbi√†ol«ú sh√¨jiu√® sh≈´r√π. Jƒ´y√∫ Kimi-VL, w«ímen kƒÅifƒÅ le Kimi-VL-Thinking, j√πy«íu qi√°ngd√† de ch√°ng sh√≠jiƒÅn tu√≠ l«ê n√©ngl√¨. D√†im«é h√© m√≥x√≠ng z√†i https://github.com/MoonshotAI/Kimi-VL g≈çngkƒÅi.",
        "vocab": "[\n    {\"word\": \"‰ªãÁªç\", \"pinyin\": \"ji√® sh√†o\", \"trans\": \"introduce\"},\n    {\"word\": \"ÂºÄÊ∫ê\", \"pinyin\": \"kƒÅi yu√°n\", \"trans\": \"open source\"},\n    {\"word\": \"Ê∑∑Âêà\", \"pinyin\": \"h√πn h√©\", \"trans\": \"hybrid\"},\n    {\"word\": \"‰∏ìÂÆ∂\", \"pinyin\": \"zhuƒÅn jiƒÅ\", \"trans\": \"expert\"},\n    {\"word\": \"ËßÜËßâ\", \"pinyin\": \"sh√¨ ju√©\", \"trans\": \"visual\"},\n    {\"word\": \"ËØ≠Ë®Ä\", \"pinyin\": \"y«î y√°n\", \"trans\": \"language\"},\n    {\"word\": \"Ê®°Âûã\", \"pinyin\": \"m√≥ x√≠ng\", \"trans\": \"model\"},\n    {\"word\": \"ÂÖàËøõ\", \"pinyin\": \"xiƒÅn j√¨n\", \"trans\": \"advanced\"},\n    {\"word\": \"Â§öÊ®°ÊÄÅ\", \"pinyin\": \"du≈ç m√≥ t√†i\", \"trans\": \"multimodal\"},\n    {\"word\": \"Êé®ÁêÜ\", \"pinyin\": \"tuƒ´ l«ê\", \"trans\": \"reasoning\"},\n    {\"word\": \"Èïø\", \"pinyin\": \"ch√°ng\", \"trans\": \"long\"},\n    {\"word\": \"‰∏ä‰∏ãÊñá\", \"pinyin\": \"sh√†ng xi√† w√©n\", \"trans\": \"context\"},\n    {\"word\": \"ÁêÜËß£\", \"pinyin\": \"l«ê jiƒõ\", \"trans\": \"understanding\"},\n    {\"word\": \"Âº∫Â§ß\", \"pinyin\": \"qi√°ng d√†\", \"trans\": \"powerful\"},\n    {\"word\": \"‰ª£ÁêÜ\", \"pinyin\": \"d√†i l«ê\", \"trans\": \"agent\"},\n    {\"word\": \"ËÉΩÂäõ\", \"pinyin\": \"n√©ng l√¨\", \"trans\": \"ability\"},\n    {\"word\": \"ÊøÄÊ¥ª\", \"pinyin\": \"jƒ´ hu√≥\", \"trans\": \"activate\"},\n    {\"word\": \"Ëß£Á†ÅÂô®\", \"pinyin\": \"jiƒõ m«é q√¨\", \"trans\": \"decoder\"},\n    {\"word\": \"ÂèÇÊï∞\", \"pinyin\": \"cƒÅn sh√π\", \"trans\": \"parameters\"},\n    {\"word\": \"Â§öËΩÆ\", \"pinyin\": \"du≈ç l√∫n\", \"trans\": \"multi-turn\"},\n    {\"word\": \"‰ªªÂä°\", \"pinyin\": \"r√®n w√π\", \"trans\": \"task\"},\n    {\"word\": \"Ë°®Áé∞\", \"pinyin\": \"bi«éo xi√†n\", \"trans\": \"performance\"},\n    {\"word\": \"Âá∫Ëâ≤\", \"pinyin\": \"ch≈´ s√®\", \"trans\": \"outstanding\"},\n    {\"word\": \"Â§ÑÁêÜ\", \"pinyin\": \"ch«î l«ê\", \"trans\": \"handle\"},\n    {\"word\": \"È´òÂàÜËæ®Áéá\", \"pinyin\": \"gƒÅo fƒìn b√†i l«ú\", \"trans\": \"high resolution\"},\n    {\"word\": \"ËæìÂÖ•\", \"pinyin\": \"sh≈´ r√π\", \"trans\": \"input\"},\n    {\"word\": \"Âü∫‰∫é\", \"pinyin\": \"jƒ´ y√∫\", \"trans\": \"based on\"},\n    {\"word\": \"ÂºÄÂèë\", \"pinyin\": \"kƒÅi fƒÅ\", \"trans\": \"develop\"},\n    {\"word\": \"ÈïøÊó∂Èó¥\", \"pinyin\": \"ch√°ng sh√≠ jiƒÅn\", \"trans\": \"long-term\"},\n    {\"word\": \"ÂÖ¨ÂºÄ\", \"pinyin\": \"g≈çng kƒÅi\", \"trans\": \"public\"},\n    {\"word\": \"‰ª£Á†Å\", \"pinyin\": \"d√†i m«é\", \"trans\": \"code\"}\n]",
        "trans": "We introduce Kimi-VL, an open-source hybrid expert visual-language model. It features advanced multimodal reasoning, long-context understanding, and powerful agent capabilities, activating only 2.8B language decoder parameters. Kimi-VL performs excellently in multi-turn agent tasks and various visual-language tasks. It can also handle long-context and high-resolution visual inputs. Based on Kimi-VL, we developed Kimi-VL-Thinking, which has strong long-term reasoning capabilities. The code and model are available at https://github.com/MoonshotAI/Kimi-VL.",
        "update_ts": "2025-04-13 12:41"
    }
}