
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 10 papers. March 13.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">13 марта</span> | <span id="title-articles-count">10 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-03-12.html">⬅️ <span id="prev-date">12.03</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-03-14.html">➡️ <span id="next-date">14.03</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-03.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'};
        let feedDateNext = {'ru': '14.03', 'en': '03/14', 'zh': '3月14日'};
        let feedDatePrev = {'ru': '12.03', 'en': '03/12', 'zh': '3月12日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2503.09566', 'title': 'TPDiff: Temporal Pyramid Video Diffusion Model', 'url': 'https://huggingface.co/papers/2503.09566', 'abstract': 'The development of video diffusion models unveils a significant challenge: the substantial computational demands. To mitigate this challenge, we note that the reverse process of diffusion exhibits an inherent entropy-reducing nature. Given the inter-frame redundancy in video modality, maintaining full frame rates in high-entropy stages is unnecessary. Based on this insight, we propose TPDiff, a unified framework to enhance training and inference efficiency. By dividing diffusion into several stages, our framework progressively increases frame rate along the diffusion process with only the last stage operating on full frame rate, thereby optimizing computational efficiency. To train the multi-stage diffusion model, we introduce a dedicated training framework: stage-wise diffusion. By solving the partitioned probability flow ordinary differential equations (ODE) of diffusion under aligned data and noise, our training strategy is applicable to various diffusion forms and further enhances training efficiency. Comprehensive experimental evaluations validate the generality of our method, demonstrating 50% reduction in training cost and 1.5x improvement in inference efficiency.', 'score': 23, 'issue_id': 2681, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': '6952e94de20936ce', 'authors': ['Lingmin Ran', 'Mike Zheng Shou'], 'affiliations': ['Show Lab, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.09566.jpg', 'data': {'categories': ['#inference', '#video', '#diffusion', '#training', '#optimization'], 'emoji': '🎬', 'ru': {'title': 'Ускорение видео-диффузии: эффективность через поэтапность', 'desc': 'Статья представляет TPDiff - унифицированную систему для повышения эффективности обучения и вывода видео-диффузионных моделей. Авторы предлагают разделить процесс диффузии на несколько этапов, постепенно увеличивая частоту кадров, что оптимизирует вычислительные ресурсы. Для обучения многоэтапной диффузионной модели вводится специальная структура обучения: поэтапная диффузия. Экспериментальные оценки подтверждают универсальность метода, демонстрируя сокращение затрат на обучение на 50% и повышение эффективности вывода в 1,5 раза.'}, 'en': {'title': 'Optimizing Video Diffusion with TPDiff: Efficiency Unleashed!', 'desc': 'This paper addresses the high computational costs associated with video diffusion models by introducing TPDiff, a framework that optimizes training and inference efficiency. The authors leverage the entropy-reducing nature of the diffusion process and the redundancy between video frames to reduce the need for full frame rates during high-entropy stages. TPDiff operates in multiple stages, gradually increasing the frame rate, with only the final stage using the full frame rate, thus enhancing computational efficiency. The proposed stage-wise diffusion training framework further improves efficiency by solving partitioned probability flow ordinary differential equations, leading to significant reductions in training costs and improvements in inference speed.'}, 'zh': {'title': '优化视频扩散模型的计算效率', 'desc': '视频扩散模型的发展面临着巨大的计算需求。为了缓解这一挑战，我们注意到扩散的反向过程具有固有的减少熵的特性。考虑到视频模态中的帧间冗余，在高熵阶段保持全帧率是没有必要的。基于这一见解，我们提出了TPDiff框架，通过将扩散过程分为多个阶段，逐步提高帧率，从而优化计算效率。'}}}, {'id': 'https://huggingface.co/papers/2503.09151', 'title': 'Reangle-A-Video: 4D Video Generation as Video-to-Video Translation', 'url': 'https://huggingface.co/papers/2503.09151', 'abstract': 'We introduce Reangle-A-Video, a unified framework for generating synchronized multi-view videos from a single input video. Unlike mainstream approaches that train multi-view video diffusion models on large-scale 4D datasets, our method reframes the multi-view video generation task as video-to-videos translation, leveraging publicly available image and video diffusion priors. In essence, Reangle-A-Video operates in two stages. (1) Multi-View Motion Learning: An image-to-video diffusion transformer is synchronously fine-tuned in a self-supervised manner to distill view-invariant motion from a set of warped videos. (2) Multi-View Consistent Image-to-Images Translation: The first frame of the input video is warped and inpainted into various camera perspectives under an inference-time cross-view consistency guidance using DUSt3R, generating multi-view consistent starting images. Extensive experiments on static view transport and dynamic camera control show that Reangle-A-Video surpasses existing methods, establishing a new solution for multi-view video generation. We will publicly release our code and data. Project page: https://hyeonho99.github.io/reangle-a-video/', 'score': 19, 'issue_id': 2681, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': 'e1463c182b0fe8e9', 'authors': ['Hyeonho Jeong', 'Suhyeon Lee', 'Jong Chul Ye'], 'affiliations': ['KAIST AI'], 'pdf_title_img': 'assets/pdf/title_img/2503.09151.jpg', 'data': {'categories': ['#open_source', '#video', '#multimodal', '#diffusion'], 'emoji': '🎥', 'ru': {'title': 'Революция в многоракурсной видеогенерации без 4D-датасетов', 'desc': 'Reangle-A-Video - это новый подход к генерации синхронизированных многоракурсных видео из одного входного видео. Метод использует двухэтапный процесс: обучение многоракурсному движению и согласованный перевод изображения в изображения с разных ракурсов. В отличие от традиционных методов, Reangle-A-Video не требует больших 4D-датасетов, а использует существующие модели диффузии для изображений и видео. Эксперименты показывают, что этот метод превосходит существующие подходы в задачах статического переноса ракурса и динамического управления камерой.'}, 'en': {'title': 'Transforming Single Videos into Multi-View Masterpieces!', 'desc': 'Reangle-A-Video is a novel framework designed to create synchronized multi-view videos from a single input video. It innovatively approaches the multi-view video generation task by treating it as a video-to-video translation problem, utilizing existing image and video diffusion models. The process consists of two main stages: first, it learns motion patterns from warped videos in a self-supervised manner, and second, it generates consistent multi-view images by warping and inpainting the initial frame under specific guidance. This method outperforms current techniques in both static view transport and dynamic camera control, marking a significant advancement in multi-view video generation.'}, 'zh': {'title': 'Reangle-A-Video：单视频生成多视角同步视频的新方法', 'desc': '我们提出了Reangle-A-Video，这是一个统一框架，用于从单个输入视频生成同步的多视角视频。与主流方法不同，我们的方法将多视角视频生成任务重新定义为视频到视频的转换，利用公开可用的图像和视频扩散先验。Reangle-A-Video的操作分为两个阶段：首先，通过自监督方式对图像到视频的扩散变换器进行同步微调，以提取视角不变的运动；其次，在推理时使用DUSt3R进行跨视角一致性指导，将输入视频的第一帧变形并修复为不同的摄像机视角，生成多视角一致的起始图像。'}}}, {'id': 'https://huggingface.co/papers/2503.09573', 'title': 'Block Diffusion: Interpolating Between Autoregressive and Diffusion\n  Language Models', 'url': 'https://huggingface.co/papers/2503.09573', 'abstract': 'Diffusion language models offer unique benefits over autoregressive models due to their potential for parallelized generation and controllability, yet they lag in likelihood modeling and are limited to fixed-length generation. In this work, we introduce a class of block diffusion language models that interpolate between discrete denoising diffusion and autoregressive models. Block diffusion overcomes key limitations of both approaches by supporting flexible-length generation and improving inference efficiency with KV caching and parallel token sampling. We propose a recipe for building effective block diffusion models that includes an efficient training algorithm, estimators of gradient variance, and data-driven noise schedules to minimize the variance. Block diffusion sets a new state-of-the-art performance among diffusion models on language modeling benchmarks and enables generation of arbitrary-length sequences. We provide the code, along with the model weights and blog post on the project page: https://m-arriola.com/bd3lms/', 'score': 11, 'issue_id': 2678, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': '32f097e93cbf5f3a', 'authors': ['Marianne Arriola', 'Aaron Gokaslan', 'Justin T Chiu', 'Zhihan Yang', 'Zhixuan Qi', 'Jiaqi Han', 'Subham Sekhar Sahoo', 'Volodymyr Kuleshov'], 'affiliations': ['Cohere, NY, USA', 'Cornell Tech, NY, USA', 'Stanford University, CA, USA'], 'pdf_title_img': 'assets/pdf/title_img/2503.09573.jpg', 'data': {'categories': ['#open_source', '#diffusion', '#benchmark', '#training', '#architecture'], 'emoji': '🧩', 'ru': {'title': 'Блочные диффузионные модели: лучшее из двух миров в языковом моделировании', 'desc': 'Статья представляет новый класс языковых моделей - блочные диффузионные модели. Они объединяют преимущества диффузионных и авторегрессивных моделей, позволяя генерировать тексты произвольной длины и повышая эффективность вывода. Авторы предлагают эффективный алгоритм обучения, оценки дисперсии градиента и расписания шума для минимизации дисперсии. Блочные диффузионные модели достигают нового уровня производительности среди диффузионных моделей в задачах языкового моделирования.'}, 'en': {'title': 'Block Diffusion: The Future of Flexible Language Generation', 'desc': 'This paper presents block diffusion language models, which combine the strengths of diffusion models and autoregressive models. These models allow for flexible-length text generation and improve efficiency during inference by using techniques like KV caching and parallel token sampling. The authors introduce a comprehensive approach for training these models, including methods to reduce gradient variance and optimize noise schedules. As a result, block diffusion models achieve state-of-the-art performance in language modeling tasks and can generate sequences of varying lengths.'}, 'zh': {'title': '块扩散模型：灵活生成与高效推理的结合', 'desc': '扩散语言模型相比自回归模型具有独特的优势，如并行生成和可控性，但在似然建模方面表现较差，并且生成长度固定。本文提出了一类块扩散语言模型，结合了离散去噪扩散和自回归模型的优点。块扩散克服了这两种方法的关键限制，支持灵活长度的生成，并通过KV缓存和并行令牌采样提高推理效率。我们提出了一种构建有效块扩散模型的方案，包括高效的训练算法、梯度方差估计器和数据驱动的噪声调度，以最小化方差。'}}}, {'id': 'https://huggingface.co/papers/2503.04388', 'title': 'More Documents, Same Length: Isolating the Challenge of Multiple\n  Documents in RAG', 'url': 'https://huggingface.co/papers/2503.04388', 'abstract': 'Retrieval-augmented generation (RAG) provides LLMs with relevant documents. Although previous studies noted that retrieving many documents can degrade performance, they did not isolate how the quantity of documents affects performance while controlling for context length. We evaluate various language models on custom datasets derived from a multi-hop QA task. We keep the context length and position of relevant information constant while varying the number of documents, and find that increasing the document count in RAG settings poses significant challenges for LLMs. Additionally, our results indicate that processing multiple documents is a separate challenge from handling long contexts. We also make the datasets and code available: https://github.com/shaharl6000/MoreDocsSameLen .', 'score': 4, 'issue_id': 2682, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': '9b3fb8251a206c0d', 'authors': ['Shahar Levy', 'Nir Mazor', 'Lihi Shalmon', 'Michael Hassid', 'Gabriel Stanovsky'], 'affiliations': ['School of Computer Science and Engineering The Hebrew University of Jerusalem, Jerusalem, Israel'], 'pdf_title_img': 'assets/pdf/title_img/2503.04388.jpg', 'data': {'categories': ['#long_context', '#dataset', '#open_source', '#rag'], 'emoji': '📚', 'ru': {'title': 'Больше документов - больше проблем для языковых моделей', 'desc': 'Статья исследует влияние количества документов на производительность языковых моделей в задачах генерации с извлечением (RAG). Авторы создали специальные наборы данных, где сохраняли длину контекста и позицию релевантной информации постоянными, но варьировали число документов. Результаты показали, что увеличение количества документов в RAG создает значительные трудности для языковых моделей. Также было установлено, что обработка множества документов - это отдельная проблема от работы с длинными контекстами.'}, 'en': {'title': 'More Documents, More Challenges for LLMs!', 'desc': 'This paper investigates how the number of documents retrieved in Retrieval-augmented generation (RAG) impacts the performance of large language models (LLMs). The authors conduct experiments using custom datasets focused on multi-hop question answering, ensuring that context length remains constant while varying the number of documents. Their findings reveal that increasing the number of documents can significantly hinder the performance of LLMs, indicating that managing multiple documents presents unique challenges distinct from those associated with long contexts. The study contributes to the understanding of RAG by providing datasets and code for further research.'}, 'zh': {'title': '文档数量影响LLM性能的挑战', 'desc': '检索增强生成（RAG）为大型语言模型（LLMs）提供相关文档。尽管之前的研究指出检索过多文档可能会降低性能，但并未明确控制上下文长度来研究文档数量对性能的影响。我们在多跳问答任务的自定义数据集上评估了各种语言模型，发现增加文档数量在RAG设置中对LLMs造成了显著挑战。此外，我们的结果表明，处理多个文档与处理长上下文是两个不同的挑战。'}}}, {'id': 'https://huggingface.co/papers/2503.09402', 'title': 'VLog: Video-Language Models by Generative Retrieval of Narration\n  Vocabulary', 'url': 'https://huggingface.co/papers/2503.09402', 'abstract': "Human daily activities can be concisely narrated as sequences of routine events (e.g., turning off an alarm) in video streams, forming an event vocabulary. Motivated by this, we introduce VLog, a novel video understanding framework that define video narrations as vocabulary, going beyond the typical subword vocabularies in existing generative video-language models. Built on the lightweight language model GPT-2, VLog feature three key innovations: (i) A generative retrieval model, marrying language model's complex reasoning capabilities with contrastive retrieval's efficient similarity search. (ii) A hierarchical vocabulary derived from large-scale video narrations using our narration pair encoding algorithm, enabling efficient indexing of specific events (e.g., cutting a tomato) by identifying broader scenarios (e.g., kitchen) with expressive postfixes (e.g., by the left hand). (iii) A vocabulary update strategy leveraging generative models to extend the vocabulary for novel events encountered during inference. To validate our approach, we introduce VidCap-Eval, a development set requiring concise narrations with reasoning relationships (e.g., before and after). Experiments on EgoSchema, COIN, and HiREST further demonstrate the effectiveness of VLog, highlighting its ability to generate concise, contextually accurate, and efficient narrations, offering a novel perspective on video understanding. Codes are released at https://github.com/showlab/VLog.", 'score': 2, 'issue_id': 2681, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': '6a25ed0e2c069e4f', 'authors': ['Kevin Qinghong Lin', 'Mike Zheng Shou'], 'affiliations': ['Show Lab, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.09402.jpg', 'data': {'categories': ['#dataset', '#multimodal', '#games', '#reasoning', '#video', '#benchmark'], 'emoji': '🎥', 'ru': {'title': 'VLog: Пересказ видео через словарь событий', 'desc': 'VLog - это новая система понимания видео, которая использует словарь событий для описания повседневной деятельности человека. Она основана на языковой модели GPT-2 и включает в себя генеративную модель поиска, иерархический словарь и стратегию обновления словаря. VLog способна генерировать краткие и точные описания видео, учитывая контекст и логические связи между событиями. Эффективность системы была продемонстрирована на нескольких наборах данных, включая специально созданный набор VidCap-Eval.'}, 'en': {'title': 'VLog: Revolutionizing Video Narration with Hierarchical Vocabulary', 'desc': 'The paper presents VLog, a new framework for understanding videos by narrating daily activities as sequences of events. It introduces a hierarchical vocabulary that allows for efficient indexing of specific actions within broader contexts, enhancing the way video content is interpreted. VLog combines a generative retrieval model with a lightweight language model, enabling complex reasoning and efficient similarity searches. Additionally, it features a vocabulary update strategy that adapts to new events during inference, demonstrating its effectiveness through experiments on various datasets.'}, 'zh': {'title': 'VLog：视频理解的新视角', 'desc': '本论文介绍了一种名为VLog的视频理解框架，旨在将视频叙述定义为词汇，超越现有生成视频语言模型中的子词词汇。VLog基于轻量级语言模型GPT-2，具有三项关键创新：生成检索模型、层次词汇和词汇更新策略。生成检索模型结合了语言模型的复杂推理能力和对比检索的高效相似性搜索。通过在EgoSchema、COIN和HiREST数据集上的实验，验证了VLog在生成简洁、上下文准确的叙述方面的有效性。'}}}, {'id': 'https://huggingface.co/papers/2503.06955', 'title': 'Motion Anything: Any to Motion Generation', 'url': 'https://huggingface.co/papers/2503.06955', 'abstract': 'Conditional motion generation has been extensively studied in computer vision, yet two critical challenges remain. First, while masked autoregressive methods have recently outperformed diffusion-based approaches, existing masking models lack a mechanism to prioritize dynamic frames and body parts based on given conditions. Second, existing methods for different conditioning modalities often fail to integrate multiple modalities effectively, limiting control and coherence in generated motion. To address these challenges, we propose Motion Anything, a multimodal motion generation framework that introduces an Attention-based Mask Modeling approach, enabling fine-grained spatial and temporal control over key frames and actions. Our model adaptively encodes multimodal conditions, including text and music, improving controllability. Additionally, we introduce Text-Music-Dance (TMD), a new motion dataset consisting of 2,153 pairs of text, music, and dance, making it twice the size of AIST++, thereby filling a critical gap in the community. Extensive experiments demonstrate that Motion Anything surpasses state-of-the-art methods across multiple benchmarks, achieving a 15% improvement in FID on HumanML3D and showing consistent performance gains on AIST++ and TMD. See our project website https://steve-zeyu-zhang.github.io/MotionAnything', 'score': 2, 'issue_id': 2680, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '9199d2d99b75d862', 'authors': ['Zeyu Zhang', 'Yiran Wang', 'Wei Mao', 'Danning Li', 'Rui Zhao', 'Biao Wu', 'Zirui Song', 'Bohan Zhuang', 'Ian Reid', 'Richard Hartley'], 'affiliations': ['ANU', 'Google', 'JD.com', 'MBZUAI', 'McGill', 'Tencent', 'USYD', 'UTS', 'ZJU'], 'pdf_title_img': 'assets/pdf/title_img/2503.06955.jpg', 'data': {'categories': ['#games', '#synthetic', '#optimization', '#benchmark', '#multimodal', '#cv', '#dataset'], 'emoji': '🕺', 'ru': {'title': 'Универсальная генерация движений с мультимодальным контролем', 'desc': 'Статья представляет Motion Anything - новую мультимодальную систему генерации движений, использующую маскирование на основе внимания для точного контроля над ключевыми кадрами и действиями. Модель адаптивно кодирует различные условия, включая текст и музыку, что улучшает управляемость генерируемых движений. Авторы также представляют новый датасет Text-Music-Dance (TMD), содержащий 2153 пары текста, музыки и танца. Эксперименты показывают, что Motion Anything превосходит современные методы на нескольких бенчмарках, достигая 15% улучшения FID на HumanML3D.'}, 'en': {'title': 'Revolutionizing Motion Generation with Multimodal Control', 'desc': 'This paper presents Motion Anything, a new framework for generating motion that effectively combines multiple input types like text and music. It addresses two main challenges in motion generation: the need for prioritizing dynamic elements and the integration of different conditioning modalities. The proposed Attention-based Mask Modeling allows for better control over key frames and actions, enhancing the quality of generated motions. Additionally, the introduction of the Text-Music-Dance dataset provides a larger resource for training, leading to significant improvements in performance compared to existing methods.'}, 'zh': {'title': '多模态运动生成的新突破', 'desc': '本文提出了一种名为Motion Anything的多模态运动生成框架，旨在解决现有方法在动态帧和身体部位优先级方面的不足。我们引入了一种基于注意力的掩模建模方法，使得对关键帧和动作的空间和时间控制更加精细。该模型能够自适应编码文本和音乐等多模态条件，从而提高生成运动的可控性。此外，我们还创建了一个新的运动数据集Text-Music-Dance (TMD)，包含2153对文本、音乐和舞蹈，填补了社区中的重要空白。'}}}, {'id': 'https://huggingface.co/papers/2503.09579', 'title': 'Cost-Optimal Grouped-Query Attention for Long-Context LLMs', 'url': 'https://huggingface.co/papers/2503.09579', 'abstract': 'Building effective and efficient Transformer-based large language models (LLMs) has recently become a research focus, requiring maximizing model language capabilities and minimizing training and deployment costs. Existing efforts have primarily described complex relationships among model performance, parameter size, and data size, as well as searched for the optimal compute allocation to train LLMs. However, they overlook the impacts of context length and attention head configuration (the number of query and key-value heads in grouped-query attention) on training and inference. In this paper, we systematically compare models with different parameter sizes, context lengths, and attention head configurations in terms of model performance, computational cost, and memory cost. Then, we extend the existing scaling methods, which are based solely on parameter size and training compute, to guide the construction of cost-optimal LLMs during both training and inference. Our quantitative scaling studies show that, when processing sufficiently long sequences, a larger model with fewer attention heads can achieve a lower loss while incurring lower computational and memory costs. Our findings provide valuable insights for developing practical LLMs, especially in long-context processing scenarios. We will publicly release our code and data.', 'score': 1, 'issue_id': 2682, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': '2c884c8c6aab1cc4', 'authors': ['Yingfa Chen', 'Yutong Wu', 'Xu Han', 'Zhiyuan Liu', 'Maosong Sun'], 'affiliations': ['NLP Group, DCST, IAI, BNRIST, Tsinghua University, Beijing, China', 'SIST, University of Science and Technology Beijing, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.09579.jpg', 'data': {'categories': ['#optimization', '#architecture', '#open_source', '#long_context', '#inference', '#training'], 'emoji': '🧠', 'ru': {'title': 'Оптимизация LLM: больше параметров, меньше головок внимания', 'desc': 'Статья исследует влияние длины контекста и конфигурации головок внимания на эффективность и производительность больших языковых моделей (LLM). Авторы проводят систематическое сравнение моделей с различными размерами параметров, длинами контекста и конфигурациями головок внимания. Они расширяют существующие методы масштабирования, чтобы оптимизировать LLM как на этапе обучения, так и при выводе. Результаты показывают, что при обработке длинных последовательностей большая модель с меньшим количеством головок внимания может достичь лучших результатов при меньших вычислительных затратах и использовании памяти.'}, 'en': {'title': 'Optimizing LLMs: Less Heads, More Efficiency!', 'desc': 'This paper investigates how different configurations of Transformer-based large language models (LLMs) affect their performance and resource efficiency. It highlights the importance of context length and attention head settings, which have been largely ignored in previous research. By comparing various model sizes and configurations, the authors propose new scaling methods that optimize both training and inference costs. Their results indicate that larger models with fewer attention heads can perform better and use less computational and memory resources when handling long sequences.'}, 'zh': {'title': '优化大型语言模型的构建与成本', 'desc': '本文探讨了如何构建高效的基于Transformer的大型语言模型（LLMs），重点在于最大化模型的语言能力，同时降低训练和部署成本。研究比较了不同参数大小、上下文长度和注意力头配置对模型性能、计算成本和内存成本的影响。结果表明，在处理较长序列时，较大的模型配合较少的注意力头可以实现更低的损失，同时降低计算和内存成本。我们的研究为开发实用的LLMs提供了重要的见解，尤其是在长上下文处理的场景中。'}}}, {'id': 'https://huggingface.co/papers/2503.09427', 'title': 'Multimodal Language Modeling for High-Accuracy Single Cell\n  Transcriptomics Analysis and Generation', 'url': 'https://huggingface.co/papers/2503.09427', 'abstract': 'Pre-trained language models (PLMs) have revolutionized scientific research, yet their application to single-cell analysis remains limited. Text PLMs cannot process single-cell RNA sequencing data, while cell PLMs lack the ability to handle free text, restricting their use in multimodal tasks. Existing efforts to bridge these modalities often suffer from information loss or inadequate single-modal pre-training, leading to suboptimal performances. To address these challenges, we propose Single-Cell MultiModal Generative Pre-trained Transformer (scMMGPT), a unified PLM for joint cell and text modeling. scMMGPT effectively integrates the state-of-the-art cell and text PLMs, facilitating cross-modal knowledge sharing for improved performance. To bridge the text-cell modality gap, scMMGPT leverages dedicated cross-modal projectors, and undergoes extensive pre-training on 27 million cells -- the largest dataset for multimodal cell-text PLMs to date. This large-scale pre-training enables scMMGPT to excel in joint cell-text tasks, achieving an 84\\% relative improvement of textual discrepancy for cell description generation, 20.5\\% higher accuracy for cell type annotation, and 4\\% improvement in k-NN accuracy for text-conditioned pseudo-cell generation, outperforming baselines.', 'score': 1, 'issue_id': 2677, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': '491beb48064068d2', 'authors': ['Yaorui Shi', 'Jiaqi Yang', 'Sihang Li', 'Junfeng Fang', 'Xiang Wang', 'Zhiyuan Liu', 'Yang Zhang'], 'affiliations': ['National University of Singapore', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.09427.jpg', 'data': {'categories': ['#plp', '#transfer_learning', '#science', '#multimodal', '#dataset', '#training'], 'emoji': '🧬', 'ru': {'title': 'Единая модель для анализа клеток и текста', 'desc': 'scMMGPT - это новая языковая модель, объединяющая анализ одиночных клеток и текста. Она решает проблему ограниченности существующих моделей, которые специализируются только на одной из этих модальностей. scMMGPT использует специальные проекторы для преодоления разрыва между клеточными и текстовыми данными. Модель была предобучена на 27 миллионах клеток, что является крупнейшим датасетом для мультимодальных клеточно-текстовых моделей на сегодняшний день.'}, 'en': {'title': 'Bridging Cells and Text: The Power of scMMGPT', 'desc': 'This paper introduces the Single-Cell MultiModal Generative Pre-trained Transformer (scMMGPT), a novel model designed to integrate single-cell RNA sequencing data with textual information. Traditional pre-trained language models struggle with this integration due to their inability to process both modalities effectively. scMMGPT addresses these limitations by utilizing cross-modal projectors and extensive pre-training on a large dataset of 27 million cells, enhancing its performance in joint tasks. The results demonstrate significant improvements in cell description generation, cell type annotation accuracy, and text-conditioned pseudo-cell generation compared to existing models.'}, 'zh': {'title': '单细胞多模态生成预训练变换器的创新应用', 'desc': '预训练语言模型（PLMs）在科学研究中带来了革命性的变化，但在单细胞分析中的应用仍然有限。现有的文本PLMs无法处理单细胞RNA测序数据，而细胞PLMs又无法处理自由文本，这限制了它们在多模态任务中的使用。为了解决这些问题，我们提出了单细胞多模态生成预训练变换器（scMMGPT），这是一个用于细胞和文本联合建模的统一PLM。scMMGPT通过专门的跨模态投影器和在2700万个细胞上进行的大规模预训练，显著提高了细胞描述生成和细胞类型注释的准确性。'}}}, {'id': 'https://huggingface.co/papers/2503.09419', 'title': 'Alias-Free Latent Diffusion Models:Improving Fractional Shift\n  Equivariance of Diffusion Latent Space', 'url': 'https://huggingface.co/papers/2503.09419', 'abstract': 'Latent Diffusion Models (LDMs) are known to have an unstable generation process, where even small perturbations or shifts in the input noise can lead to significantly different outputs. This hinders their applicability in applications requiring consistent results. In this work, we redesign LDMs to enhance consistency by making them shift-equivariant. While introducing anti-aliasing operations can partially improve shift-equivariance, significant aliasing and inconsistency persist due to the unique challenges in LDMs, including 1) aliasing amplification during VAE training and multiple U-Net inferences, and 2) self-attention modules that inherently lack shift-equivariance. To address these issues, we redesign the attention modules to be shift-equivariant and propose an equivariance loss that effectively suppresses the frequency bandwidth of the features in the continuous domain. The resulting alias-free LDM (AF-LDM) achieves strong shift-equivariance and is also robust to irregular warping. Extensive experiments demonstrate that AF-LDM produces significantly more consistent results than vanilla LDM across various applications, including video editing and image-to-image translation. Code is available at: https://github.com/SingleZombie/AFLDM', 'score': 1, 'issue_id': 2680, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': '1c497a991b18da6a', 'authors': ['Yifan Zhou', 'Zeqi Xiao', 'Shuai Yang', 'Xingang Pan'], 'affiliations': ['S-Lab, Nanyang Technological University', 'Wangxuan Institute of Computer Technology, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2503.09419.jpg', 'data': {'categories': ['#video', '#diffusion', '#training', '#optimization', '#architecture', '#cv'], 'emoji': '🔄', 'ru': {'title': 'Стабильная генерация изображений с помощью эквивариантных латентных диффузионных моделей', 'desc': 'Эта статья представляет новый подход к улучшению стабильности латентных диффузионных моделей (LDM). Авторы предлагают модифицированную архитектуру AF-LDM, которая обладает свойством эквивариантности к сдвигу, что повышает согласованность результатов генерации. Ключевые изменения включают переработку модулей внимания и введение специальной функции потерь для подавления частотной полосы признаков. Эксперименты показывают, что AF-LDM значительно превосходит стандартные LDM по стабильности результатов в различных задачах, таких как редактирование видео и перевод изображений.'}, 'en': {'title': 'Achieving Consistency in Latent Diffusion Models with Shift-Equivariance', 'desc': 'Latent Diffusion Models (LDMs) often produce inconsistent outputs due to their sensitivity to input noise variations. This paper presents a redesign of LDMs to improve their consistency by implementing shift-equivariance. The authors address challenges such as aliasing during VAE training and the limitations of self-attention modules by introducing new shift-equivariant attention mechanisms and an equivariance loss. The resulting alias-free LDM (AF-LDM) shows enhanced robustness and consistency in applications like video editing and image translation compared to traditional LDMs.'}, 'zh': {'title': '提升潜在扩散模型的一致性', 'desc': '潜在扩散模型（LDMs）在生成过程中存在不稳定性，输入噪声的微小变化可能导致输出结果显著不同，这限制了其在需要一致性结果的应用中的适用性。本文通过重新设计LDMs，使其具备平移等变性，从而增强一致性。我们提出了一种新的注意力模块，使其具备平移等变性，并引入了一种等变损失，有效抑制特征在连续域中的频率带宽。最终，得到的无别名LDM（AF-LDM）在多个应用中表现出更强的一致性，尤其是在视频编辑和图像到图像转换任务中。'}}}, {'id': 'https://huggingface.co/papers/2503.08525', 'title': 'GTR: Guided Thought Reinforcement Prevents Thought Collapse in RL-based\n  VLM Agent Training', 'url': 'https://huggingface.co/papers/2503.08525', 'abstract': "Reinforcement learning with verifiable outcome rewards (RLVR) has effectively scaled up chain-of-thought (CoT) reasoning in large language models (LLMs). Yet, its efficacy in training vision-language model (VLM) agents for goal-directed action reasoning in visual environments is less established. This work investigates this problem through extensive experiments on complex card games, such as 24 points, and embodied tasks from ALFWorld. We find that when rewards are based solely on action outcomes, RL fails to incentivize CoT reasoning in VLMs, instead leading to a phenomenon we termed thought collapse, characterized by a rapid loss of diversity in the agent's thoughts, state-irrelevant and incomplete reasoning, and subsequent invalid actions, resulting in negative rewards. To counteract thought collapse, we highlight the necessity of process guidance and propose an automated corrector that evaluates and refines the agent's reasoning at each RL step. This simple and scalable GTR (Guided Thought Reinforcement) framework trains reasoning and action simultaneously without the need for dense, per-step human labeling. Our experiments demonstrate that GTR significantly enhances the performance and generalization of the LLaVA-7b model across various visual environments, achieving 3-5 times higher task success rates compared to SoTA models with notably smaller model sizes.", 'score': 1, 'issue_id': 2681, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': 'e82260bba3e835b4', 'authors': ['Tong Wei', 'Yijun Yang', 'Junliang Xing', 'Yuanchun Shi', 'Zongqing Lu', 'Deheng Ye'], 'affiliations': ['Peking University', 'Tencent', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.08525.jpg', 'data': {'categories': ['#agents', '#games', '#reasoning', '#video', '#training', '#optimization', '#rl'], 'emoji': '🧠', 'ru': {'title': 'Управляемое обучение рассуждениям для визуально-языковых моделей', 'desc': "Исследование посвящено применению обучения с подкреплением для улучшения рассуждений в визуально-языковых моделях. Авторы обнаружили феномен 'коллапса мыслей' при использовании наград, основанных только на результатах действий. Для решения этой проблемы предложен метод GTR (Guided Thought Reinforcement), который автоматически оценивает и уточняет рассуждения агента на каждом шаге обучения. Эксперименты показали, что GTR значительно улучшает производительность и обобщающую способность модели LLaVA-7b в различных визуальных средах."}, 'en': {'title': 'Enhancing VLMs with Guided Thought Reinforcement', 'desc': "This paper explores the challenges of using reinforcement learning (RL) to train vision-language models (VLMs) for reasoning in visual tasks. It identifies a problem called 'thought collapse', where the model's reasoning becomes less diverse and leads to incorrect actions when rewards are based only on outcomes. To address this, the authors propose a Guided Thought Reinforcement (GTR) framework that provides process guidance to improve the reasoning of VLMs during training. Their experiments show that GTR significantly boosts the performance of the LLaVA-7b model, achieving much higher success rates in complex tasks compared to state-of-the-art models."}, 'zh': {'title': '引导思维强化：提升视觉语言模型的推理能力', 'desc': '本研究探讨了可验证结果奖励的强化学习（RLVR）在视觉语言模型（VLM）中的应用，尤其是在复杂的视觉环境中进行目标导向的推理。我们发现，当奖励仅基于行动结果时，RL无法有效激励VLM的思维链推理，导致思维崩溃现象，表现为代理的思维多样性迅速下降和推理不完整。为了解决这一问题，我们提出了一种自动纠正器，能够在每个RL步骤中评估和改进代理的推理过程。通过实验，我们的引导思维强化（GTR）框架显著提高了LLaVA-7b模型在各种视觉环境中的表现和泛化能力。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d', '#agents (1)', '#agi', '#alignment', '#architecture (3)', '#audio', '#benchmark (3)', '#cv (2)', '#data', '#dataset (4)', '#diffusion (4)', '#ethics', '#games (3)', '#graphs', '#hallucinations', '#healthcare', '#inference (2)', '#interpretability', '#leakage', '#long_context (2)', '#low_resource', '#machine_translation', '#math', '#multilingual', '#multimodal (4)', '#open_source (4)', '#optimization (5)', '#plp (1)', '#rag (1)', '#reasoning (2)', '#rl (1)', '#rlhf', '#robotics', '#science (1)', '#security', '#small_models', '#story_generation', '#survey', '#synthetic (1)', '#training (6)', '#transfer_learning (1)', '#video (5)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-03-13 08:15',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-03-13 08:15')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-03-13 08:15')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    