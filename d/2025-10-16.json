{
    "date": {
        "ru": "16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
        "en": "October 16",
        "zh": "10æœˆ16æ—¥"
    },
    "time_utc": "2025-10-16 07:12",
    "weekday": 3,
    "issue_id": 6449,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2510.13554",
            "title": "Attention Illuminates LLM Reasoning: The Preplan-and-Anchor Rhythm\n  Enables Fine-Grained Policy Optimization",
            "url": "https://huggingface.co/papers/2510.13554",
            "abstract": "Attention mechanisms in LLMs are analyzed to reveal reasoning patterns, leading to novel RL strategies that improve performance by focusing on critical tokens.  \t\t\t\t\tAI-generated summary \t\t\t\t The reasoning pattern of Large language models (LLMs) remains opaque, and Reinforcement learning (RL) typically applies uniform credit across an entire generation, blurring the distinction between pivotal and routine steps. This work positions attention as a privileged substrate that renders the internal logic of LLMs legible, not merely as a byproduct of computation, but as a mechanistic blueprint of reasoning itself. We first distinguish attention heads between locally and globally focused information processing and reveal that locally focused heads produce a sawtooth pattern near the diagonal indicating phrasal chunks, while globally focused heads expose tokens that exert broad downstream influence over future tokens. We formalize these with two metrics: 1) Windowed Average Attention Distance, which measures the extent of backward attention within a clipped window; 2) Future Attention Influence, which quantifies a token's global importance as the average attention it receives from subsequent tokens. Taken together, these signals reveal a recurring preplan-and-anchor mechanism, where the model first performs a long-range contextual reference to generate an introductory token, which is immediately followed by or coincides with a semantic anchor token that organizes subsequent reasoning. Leveraging these insights, we introduce three novel RL strategies that dynamically perform targeted credit assignment to critical nodes (preplan tokens, anchor tokens, and their temporal coupling) and show consistent performance gains across various reasoning tasks. By aligning optimization with the model's intrinsic reasoning rhythm, we aim to transform opaque optimization into an actionable structure-aware process, hoping to offer a potential step toward more transparent and effective optimization of LLM reasoning.",
            "score": 36,
            "issue_id": 6445,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 15",
                "zh": "10æœˆ15æ—¥"
            },
            "hash": "ae0c2f486679f7f1",
            "authors": [
                "Yang Li",
                "Zhichen Dong",
                "Yuhan Sun",
                "Weixun Wang",
                "Shaopan Xiong",
                "Yijia Luo",
                "Jiashun Liu",
                "Han Lu",
                "Jiamang Wang",
                "Wenbo Su",
                "Bo Zheng",
                "Junchi Yan"
            ],
            "affiliations": [
                "Alibaba Group",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13554.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#reasoning",
                    "#optimization",
                    "#interpretability",
                    "#training"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ’Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ĞºĞ°Ğº ĞºĞ°Ñ€Ñ‚Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹: Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ attention Ğ² LLM, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿Ğ¾Ğ½ÑÑ‚ÑŒ, ĞºĞ°Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°ÑÑ‚, Ğ²Ñ‹ÑĞ²Ğ»ÑÑ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½ Â«Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ-Ğ¸-ÑĞºĞ¾Ñ€ÑŒÂ». ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑÑ‚ attention heads Ğ½Ğ° Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾ Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾ ÑÑ„Ğ¾ĞºÑƒÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ¸Ğ·Ğ¼ĞµÑ€ÑÑ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸. ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ´ĞµĞ»Ğ°ÑÑ‚ Ğ´Ğ°Ğ»ÑŒĞ½Ğ¸Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ²Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°, Ğ·Ğ°Ñ‚ĞµĞ¼ ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Â«ÑĞºĞ¾Ñ€ÑŒÂ», Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… Ğ¸Ğ½ÑĞ°Ğ¹Ñ‚Ğ¾Ğ² Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ñ‚Ñ€Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğµ RL-ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€Ğ¸ÑĞ²Ğ°Ğ¸Ğ²Ğ°ÑÑ‚ reward Ğ¸Ğ¼ĞµĞ½Ğ½Ğ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Unlocking LLM Reasoning with Targeted Attention Strategies",
                    "desc": "This paper investigates how attention mechanisms in Large Language Models (LLMs) can clarify their reasoning processes. It identifies two types of attention heads: those that focus locally on specific phrases and those that have a global influence on future tokens. By introducing metrics like Windowed Average Attention Distance and Future Attention Influence, the authors reveal a structured reasoning pattern that involves preplanning and anchoring tokens. The study proposes new Reinforcement Learning strategies that assign credit to these critical tokens, leading to improved performance in reasoning tasks by aligning optimization with the model's inherent reasoning structure."
                },
                "zh": {
                    "title": "æ­ç¤ºæ¨ç†æ¨¡å¼ï¼Œæå‡æ¨¡å‹æ€§èƒ½çš„åˆ›æ–°ç­–ç•¥",
                    "desc": "æœ¬æ–‡åˆ†æäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥æ­ç¤ºå…¶æ¨ç†æ¨¡å¼ï¼Œå¹¶æå‡ºäº†æ–°çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç­–ç•¥ï¼Œé€šè¿‡å…³æ³¨å…³é”®æ ‡è®°æ¥æé«˜æ€§èƒ½ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ³¨æ„åŠ›ä¸ä»…æ˜¯è®¡ç®—çš„å‰¯äº§å“ï¼Œè€Œæ˜¯æ¨ç†çš„æœºåˆ¶è“å›¾ã€‚æˆ‘ä»¬åŒºåˆ†äº†å±€éƒ¨å’Œå…¨å±€å…³æ³¨çš„ä¿¡æ¯å¤„ç†ï¼Œå‘ç°å±€éƒ¨å…³æ³¨çš„å¤´éƒ¨äº§ç”Ÿé”¯é½¿å½¢æ¨¡å¼ï¼Œè€Œå…¨å±€å…³æ³¨çš„å¤´éƒ¨åˆ™æ­ç¤ºäº†å¯¹æœªæ¥æ ‡è®°çš„å¹¿æ³›å½±å“ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸‰ç§æ–°çš„RLç­–ç•¥ï¼ŒåŠ¨æ€åœ°å¯¹å…³é”®èŠ‚ç‚¹è¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„ä¿¡ç”¨åˆ†é…ï¼Œä»è€Œåœ¨å„ç§æ¨ç†ä»»åŠ¡ä¸­å®ç°äº†ä¸€è‡´çš„æ€§èƒ½æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13344",
            "title": "UniMoE-Audio: Unified Speech and Music Generation with Dynamic-Capacity\n  MoE",
            "url": "https://huggingface.co/papers/2510.13344",
            "abstract": "UniMoE-Audio, a unified speech and music generation model using a Dynamic-Capacity Mixture-of-Experts framework, addresses data imbalance and task conflicts, achieving state-of-the-art performance and enhanced cross-domain synergy.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in unified multimodal models indicate a clear trend towards comprehensive content generation. However, the auditory domain remains a significant challenge, with music and speech often developed in isolation, hindering progress towards universal audio synthesis. This separation stems from inherent task conflicts and severe data imbalances, which impede the development of a truly unified audio generation model. To address this challenge, we propose UniMoE-Audio, a unified speech and music generation model within a novel Dynamic-Capacity Mixture-of-Experts (MoE) framework. Architecturally, UniMoE-Audio introduces a Top-P routing strategy for dynamic expert number allocation, and a hybrid expert design comprising routed experts for domain-specific knowledge, shared experts for domain-agnostic features, and null experts for adaptive computation skipping. To tackle data imbalance, we introduce a three-stage training curriculum: 1) Independent Specialist Training leverages original datasets to instill domain-specific knowledge into each \"proto-expert\" without interference; 2) MoE Integration and Warmup incorporates these specialists into the UniMoE-Audio architecture, warming up the gate module and shared expert using a subset of balanced dataset; and 3) Synergistic Joint Training trains the entire model end-to-end on the fully balanced dataset, fostering enhanced cross-domain synergy. Extensive experiments show that UniMoE-Audio not only achieves state-of-the-art performance on major speech and music generation benchmarks, but also demonstrates superior synergistic learning, mitigating the performance degradation typically seen in naive joint training. Our findings highlight the substantial potential of specialized MoE architecture and curated training strategies in advancing the field of universal audio generation. Homepage: https://mukioxun.github.io/Uni-MoE-site/home.html",
            "score": 33,
            "issue_id": 6445,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 15",
                "zh": "10æœˆ15æ—¥"
            },
            "hash": "a52820c83c08467e",
            "authors": [
                "Zhenyu Liu",
                "Yunxin Li",
                "Xuanyu Zhang",
                "Qixun Teng",
                "Shenyuan Jiang",
                "Xinyu Chen",
                "Haoyuan Shi",
                "Jinchao Li",
                "Qi Wang",
                "Haolan Chen",
                "Fanbo Meng",
                "Mingjun Zhao",
                "Yu Xu",
                "Yancheng He",
                "Baotian Hu",
                "Min Zhang"
            ],
            "affiliations": [
                "Department of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China",
                "Shenzhen Loop Area Institute, Shenzhen, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13344.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#games",
                    "#benchmark",
                    "#optimization",
                    "#multimodal",
                    "#audio",
                    "#training"
                ],
                "emoji": "ğŸµ",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€ĞµÑ‡Ğ¸ Ğ¸ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ Mixture-of-Experts",
                    "desc": "UniMoE-Audio â€” ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµÑ‡Ğ¸ Ğ¸ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Dynamic-Capacity Mixture-of-Experts Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ´Ğ¸ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ñ‚Ñ€Ñ‘Ñ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼ÑƒÑ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºÑƒ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸ÑÑ‚Ğ¾Ğ², Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ² MoE-Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¸ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Top-P routing, domain-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ñ‹, Ğ¾Ğ±Ñ‰Ğ¸Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ñ‹ Ğ¸ null-ÑĞºÑĞ¿ĞµÑ€Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ° Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµÑ‡Ğ¸ Ğ¸ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ ĞºÑ€Ğ¾ÑÑ-Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ¸Ğ½ĞµÑ€Ğ³Ğ¸Ñ Ğ±ĞµĞ· Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Unifying Speech and Music Generation with Dynamic Experts",
                    "desc": "UniMoE-Audio is a novel model designed for generating both speech and music using a Dynamic-Capacity Mixture-of-Experts framework. It addresses the challenges of data imbalance and task conflicts that have historically separated these two domains. The model employs a Top-P routing strategy to dynamically allocate experts, allowing for both domain-specific and shared knowledge. Through a three-stage training process, UniMoE-Audio achieves state-of-the-art performance while enhancing synergy between speech and music generation tasks."
                },
                "zh": {
                    "title": "ç»Ÿä¸€éŸ³é¢‘ç”Ÿæˆçš„æœªæ¥",
                    "desc": "UniMoE-Audio æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„è¯­éŸ³å’ŒéŸ³ä¹ç”Ÿæˆæ¨¡å‹ï¼Œé‡‡ç”¨åŠ¨æ€å®¹é‡æ··åˆä¸“å®¶æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ•°æ®ä¸å¹³è¡¡å’Œä»»åŠ¡å†²çªçš„é—®é¢˜ã€‚è¯¥æ¨¡å‹é€šè¿‡å¼•å…¥ Top-P è·¯ç”±ç­–ç•¥ï¼Œå®ç°åŠ¨æ€ä¸“å®¶æ•°é‡åˆ†é…ï¼Œå¹¶è®¾è®¡äº†æ··åˆä¸“å®¶ç»“æ„ï¼Œä»¥ä¾¿äºå¤„ç†ç‰¹å®šé¢†åŸŸå’Œé€šç”¨ç‰¹å¾ã€‚ä¸ºäº†åº”å¯¹æ•°æ®ä¸å¹³è¡¡ï¼ŒUniMoE-Audio é‡‡ç”¨äº†ä¸‰é˜¶æ®µçš„è®­ç»ƒè¯¾ç¨‹ï¼Œé€æ­¥æå‡æ¨¡å‹çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨è¯­éŸ³å’ŒéŸ³ä¹ç”Ÿæˆçš„ä¸»è¦åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶ä¸”åœ¨è·¨é¢†åŸŸååŒå­¦ä¹ æ–¹é¢ä¼˜äºä¼ ç»Ÿçš„è”åˆè®­ç»ƒæ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13678",
            "title": "FlashWorld: High-quality 3D Scene Generation within Seconds",
            "url": "https://huggingface.co/papers/2510.13678",
            "abstract": "FlashWorld generates 3D scenes from single images or text prompts quickly and with high quality by combining MV-oriented and 3D-oriented generation methods.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose FlashWorld, a generative model that produces 3D scenes from a single image or text prompt in seconds, 10~100times faster than previous works while possessing superior rendering quality. Our approach shifts from the conventional multi-view-oriented (MV-oriented) paradigm, which generates multi-view images for subsequent 3D reconstruction, to a 3D-oriented approach where the model directly produces 3D Gaussian representations during multi-view generation. While ensuring 3D consistency, 3D-oriented method typically suffers poor visual quality. FlashWorld includes a dual-mode pre-training phase followed by a cross-mode post-training phase, effectively integrating the strengths of both paradigms. Specifically, leveraging the prior from a video diffusion model, we first pre-train a dual-mode multi-view diffusion model, which jointly supports MV-oriented and 3D-oriented generation modes. To bridge the quality gap in 3D-oriented generation, we further propose a cross-mode post-training distillation by matching distribution from consistent 3D-oriented mode to high-quality MV-oriented mode. This not only enhances visual quality while maintaining 3D consistency, but also reduces the required denoising steps for inference. Also, we propose a strategy to leverage massive single-view images and text prompts during this process to enhance the model's generalization to out-of-distribution inputs. Extensive experiments demonstrate the superiority and efficiency of our method.",
            "score": 32,
            "issue_id": 6444,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 15",
                "zh": "10æœˆ15æ—¥"
            },
            "hash": "ef944b6ef4d97bd1",
            "authors": [
                "Xinyang Li",
                "Tengfei Wang",
                "Zixiao Gu",
                "Shengchuan Zhang",
                "Chunchao Guo",
                "Liujuan Cao"
            ],
            "affiliations": [
                "Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University",
                "Tencent",
                "Yes Lab, Fudan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13678.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#optimization",
                    "#diffusion"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "ĞœĞ³Ğ½Ğ¾Ğ²ĞµĞ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ 3D-Ğ¼Ğ¸Ñ€Ğ¾Ğ²: ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ğ¼ĞµÑÑ‚Ğµ",
                    "desc": "FlashWorld â€” ÑÑ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ 3D-ÑÑ†ĞµĞ½Ñ‹ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ° Ğ·Ğ° ÑÑ‡Ğ¸Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞµĞºÑƒĞ½Ğ´Ñ‹, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ Ğ² 10-100 Ñ€Ğ°Ğ· Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ². ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğµ Ğ¾Ñ‚ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ multi-view Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğº Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¼Ñƒ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ 3D Gaussian Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° dual-mode pre-training Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ¾Ğ±Ğ¾Ğ¸Ñ… Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ¾Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ·Ğ°Ñ‚ĞµĞ¼ cross-mode post-training Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ°Ğ¼Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ â€” Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ°Ñ 3D-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒÑ."
                },
                "en": {
                    "title": "FlashWorld: Fast and High-Quality 3D Scene Generation",
                    "desc": "FlashWorld is a generative model that creates high-quality 3D scenes from single images or text prompts in a fraction of the time compared to previous methods. It innovatively combines multi-view-oriented and 3D-oriented generation techniques, allowing for faster rendering while maintaining 3D consistency. The model employs a dual-mode pre-training phase and a cross-mode post-training phase to enhance visual quality and reduce denoising steps during inference. By utilizing a large dataset of single-view images and text prompts, FlashWorld improves its ability to generalize to new inputs effectively."
                },
                "zh": {
                    "title": "FlashWorldï¼šå¿«é€Ÿç”Ÿæˆé«˜è´¨é‡3Dåœºæ™¯çš„åˆ›æ–°æ¨¡å‹",
                    "desc": "FlashWorldæ˜¯ä¸€ç§ç”Ÿæˆæ¨¡å‹ï¼Œå¯ä»¥å¿«é€Ÿä»å•å¼ å›¾åƒæˆ–æ–‡æœ¬æç¤ºç”Ÿæˆé«˜è´¨é‡çš„3Dåœºæ™¯ã€‚å®ƒç»“åˆäº†å¤šè§†è§’å¯¼å‘å’Œ3Då¯¼å‘çš„ç”Ÿæˆæ–¹æ³•ï¼Œä½¿å¾—ç”Ÿæˆé€Ÿåº¦æ¯”ä»¥å¾€å¿«10åˆ°100å€ï¼ŒåŒæ—¶ä¿æŒä¼˜è¶Šçš„æ¸²æŸ“è´¨é‡ã€‚è¯¥æ¨¡å‹é€šè¿‡åŒæ¨¡å¼é¢„è®­ç»ƒå’Œäº¤å‰æ¨¡å¼åè®­ç»ƒï¼Œæœ‰æ•ˆæ•´åˆäº†ä¸¤ç§æ–¹æ³•çš„ä¼˜ç‚¹ï¼Œç¡®ä¿äº†3Dä¸€è‡´æ€§å¹¶æå‡äº†è§†è§‰è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFlashWorldåœ¨ç”Ÿæˆæ•ˆç‡å’Œæ•ˆæœä¸Šéƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13795",
            "title": "Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully\n  Open MLLMs",
            "url": "https://huggingface.co/papers/2510.13795",
            "abstract": "A new dataset and pipeline for data curation improve the performance of fully open multimodal large language models, achieving state-of-the-art results competitive with semi-open models.  \t\t\t\t\tAI-generated summary \t\t\t\t Fully open multimodal large language models (MLLMs) currently lag behind proprietary counterparts, primarily due to a significant gap in data quality for supervised fine-tuning (SFT). Existing open-source datasets are often plagued by widespread noise and a critical deficit in complex reasoning data, such as Chain-of-Thought (CoT), which hinders the development of advanced model capabilities. Addressing these challenges, our work makes three primary contributions. First, we introduce Honey-Data-15M, a new SFT dataset comprising approximately 15 million QA pairs, processed through multiple cleaning techniques and enhanced with a novel dual-level (short and long) CoT enrichment strategy. Second, we introduce HoneyPipe, the data curation pipeline, and its underlying framework DataStudio, providing the community with a transparent and adaptable methodology for data curation that moves beyond static dataset releases. Finally, to validate our dataset and pipeline, we train Bee-8B, an 8B model on Honey-Data-15M. Experiments show that Bee-8B establishes a new state-of-the-art (SOTA) for fully open MLLMs, achieving performance that is competitive with, and in some cases surpasses, recent semi-open models such as InternVL3.5-8B. Our work delivers to the community a suite of foundational resources, including: the Honey-Data-15M corpus; the full-stack suite comprising HoneyPipe and DataStudio; training recipes; an evaluation harness; and the model weights. This effort demonstrates that a principled focus on data quality is a key pathway to developing fully open MLLMs that are highly competitive with their semi-open counterparts.",
            "score": 30,
            "issue_id": 6445,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 15",
                "zh": "10æœˆ15æ—¥"
            },
            "hash": "9896f21bc1e1b5ba",
            "authors": [
                "Yi Zhang",
                "Bolin Ni",
                "Xin-Sheng Chen",
                "Heng-Rui Zhang",
                "Yongming Rao",
                "Houwen Peng",
                "Qinglin Lu",
                "Han Hu",
                "Meng-Hao Guo",
                "Shi-Min Hu"
            ],
            "affiliations": [
                "Beihang University",
                "Tencent Hunyuan Team",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13795.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#transfer_learning",
                    "#dataset",
                    "#optimization",
                    "#open_source"
                ],
                "emoji": "ğŸ",
                "ru": {
                    "title": "ĞšĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ â€” ĞºĞ»ÑÑ‡ Ğº Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Honey-Data-15M Ğ¸Ğ· 15 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… LLM. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Chain-of-Thought Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ²ÑƒÑ… ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾ÑˆÑ‘Ğ» Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºÑƒ Ğ¾Ñ‚ ÑˆÑƒĞ¼Ğ°. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Bee-8B, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ÑÑ€ĞµĞ´Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ñ„Ğ¾ĞºÑƒÑ Ğ½Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ Ğ¿Ğ¾Ğ»ÑƒĞ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ°Ğ¼Ğ¸ Ğ²Ñ€Ğ¾Ğ´Ğµ InternVL3.5-8B."
                },
                "en": {
                    "title": "Elevating Open Models with Quality Data",
                    "desc": "This paper presents a new dataset called Honey-Data-15M, which contains 15 million question-answer pairs designed to improve the performance of fully open multimodal large language models (MLLMs). The dataset is enhanced with a dual-level Chain-of-Thought (CoT) enrichment strategy and processed through a data curation pipeline named HoneyPipe, which is part of the DataStudio framework. By training the Bee-8B model on this curated dataset, the authors achieve state-of-the-art results that rival those of semi-open models. The work emphasizes the importance of high-quality data for supervised fine-tuning to enhance the capabilities of open-source MLLMs."
                },
                "zh": {
                    "title": "æå‡å¼€æ”¾æ¨¡å‹æ€§èƒ½çš„å…³é”®åœ¨äºæ•°æ®è´¨é‡",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®é›†å’Œæ•°æ®å¤„ç†ç®¡é“ï¼Œä»¥æé«˜å®Œå…¨å¼€æ”¾çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ€§èƒ½ã€‚æˆ‘ä»¬å¼•å…¥äº†Honey-Data-15Mæ•°æ®é›†ï¼ŒåŒ…å«çº¦1500ä¸‡ä¸ªé—®ç­”å¯¹ï¼Œå¹¶é€šè¿‡å¤šç§æ¸…æ´—æŠ€æœ¯å’ŒåŒå±‚æ€ç»´é“¾ï¼ˆCoTï¼‰å¢å¼ºç­–ç•¥è¿›è¡Œå¤„ç†ã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†HoneyPipeæ•°æ®å¤„ç†ç®¡é“å’ŒDataStudioæ¡†æ¶ï¼Œä¸ºç¤¾åŒºæä¾›é€æ˜ä¸”å¯é€‚åº”çš„æ•°æ®å¤„ç†æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºHoney-Data-15Mè®­ç»ƒçš„Bee-8Bæ¨¡å‹åœ¨å®Œå…¨å¼€æ”¾çš„MLLMsä¸­è¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ï¼Œæ€§èƒ½ä¸åŠå¼€æ”¾æ¨¡å‹ç›¸å½“ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹è¶…è¶Šäº†å®ƒä»¬ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13747",
            "title": "InteractiveOmni: A Unified Omni-modal Model for Audio-Visual Multi-turn\n  Dialogue",
            "url": "https://huggingface.co/papers/2510.13747",
            "abstract": "InteractiveOmni is a unified omni-modal large language model for audio-visual multi-turn interactions, offering comprehensive understanding and speech generation capabilities with efficient parameter usage.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce InteractiveOmni, a unified and open-source omni-modal large language model for audio-visual multi-turn interaction, ranging from 4B to 8B parameters, designed to lead the field of lightweight models by offering comprehensive omni-modal understanding and speech generation capabilities. To achieve this, we integrate the vision encoder, audio encoder, large language model, and speech decoder into a unified model for understanding and generation tasks. We design a multi-stage training strategy to ensure robust cross-modal capabilities, including pre-training for omni-modal understanding, followed by post-training with speech conversation and audio-visual interaction. To enable human-like long-term conversational ability, we meticulously curate a multi-turn training dataset that enhances the model's ability to handle complex and multi-turn interactions. To effectively evaluate the multi-turn memory and speech interaction capabilities, we construct the multi-modal multi-turn memory benchmark and the multi-turn speech interaction benchmark. Experiments demonstrate that InteractiveOmni significantly outperforms leading open-source models and provides a more intelligent multi-turn audio-visual experience, particularly in its long-term memory capabilities. Notably, InteractiveOmni-4B is comparable to the much larger model like Qwen2.5-Omni-7B on general benchmarks, and it can retain 97% of the performance of the InteractiveOmni-8B while utilizing only 50% of the model size. Achieving state-of-the-art results against similarly sized models across image, audio, video understanding, and speech generation tasks, InteractiveOmni is an accessible, open-source foundation for next-generation intelligent interactive systems.",
            "score": 25,
            "issue_id": 6445,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 15",
                "zh": "10æœˆ15æ—¥"
            },
            "hash": "4bcb34dcd1975ae1",
            "authors": [
                "Wenwen Tong",
                "Hewei Guo",
                "Dongchuan Ran",
                "Jiangnan Chen",
                "Jiefan Lu",
                "Kaibin Wang",
                "Keqiang Li",
                "Xiaoxu Zhu",
                "Jiakui Li",
                "Kehan Li",
                "Xueheng Li",
                "Lumin Li",
                "Chenxu Guo",
                "Jiasheng Zhou",
                "Jiandong Chen",
                "Xianye Wu",
                "Jiahao Wang",
                "Silei Wu",
                "Lei Chen",
                "Hanming Deng",
                "Yuxuan Song",
                "Dinghao Zhou",
                "Guiping Zhong",
                "Ken Zheng",
                "Shiyin Kang",
                "Lewei Lu"
            ],
            "affiliations": [
                "SenseTime Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13747.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#small_models",
                    "#long_context",
                    "#dataset",
                    "#benchmark",
                    "#multimodal",
                    "#audio",
                    "#open_source",
                    "#training"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "Ğ›ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ğ°Ñ Ğ¾Ğ¼Ğ½Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ Ğ´Ğ»Ñ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ",
                    "desc": "InteractiveOmni â€” ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¾Ğ¼Ğ½Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ Ğ¾Ñ‚ 4B Ğ´Ğ¾ 8B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€, Ğ°ÑƒĞ´Ğ¸Ğ¾-ÑĞ½ĞºĞ¾Ğ´ĞµÑ€, LLM Ğ¸ Ñ€ĞµÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ¿Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ĞµĞ¹ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ğ¼Ğ½Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°Ñ… Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñ‹ Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ÑƒĞ½Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ¾Ğ»Ğ³Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ñ€ĞµÑ‡ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. InteractiveOmni-4B Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ñ‹Ñ… Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ² Ğ´Ğ²Ğ° Ñ€Ğ°Ğ·Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ°ÑƒĞ´Ğ¸Ğ¾, Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµÑ‡Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Multi-Turn Interactions with InteractiveOmni",
                    "desc": "InteractiveOmni is a cutting-edge omni-modal large language model designed for audio-visual interactions, capable of understanding and generating speech across multiple turns. It integrates various components like vision and audio encoders with a language model and speech decoder, allowing it to perform complex tasks efficiently. The model employs a multi-stage training strategy to enhance its cross-modal capabilities, ensuring it can handle intricate conversations effectively. With its impressive performance and reduced parameter size, InteractiveOmni sets a new standard for lightweight models in the field of interactive AI systems."
                },
                "zh": {
                    "title": "å…¨æ¨¡æ€äº¤äº’çš„æ™ºèƒ½æ–°çºªå…ƒ",
                    "desc": "InteractiveOmniæ˜¯ä¸€ç§ç»Ÿä¸€çš„å…¨æ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä¸“æ³¨äºéŸ³é¢‘-è§†è§‰çš„å¤šè½®äº¤äº’ã€‚è¯¥æ¨¡å‹é›†æˆäº†è§†è§‰ç¼–ç å™¨ã€éŸ³é¢‘ç¼–ç å™¨ã€å¤§å‹è¯­è¨€æ¨¡å‹å’Œè¯­éŸ³è§£ç å™¨ï¼Œæ—¨åœ¨å®ç°å…¨é¢çš„ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚é€šè¿‡å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼ŒInteractiveOmnièƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å¤æ‚çš„å¤šè½®å¯¹è¯ï¼Œå¹¶åœ¨é•¿æ—¶é—´è®°å¿†å’Œè¯­éŸ³äº¤äº’æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒInteractiveOmniåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†é¢†å…ˆçš„å¼€æºæ¨¡å‹ï¼Œæä¾›äº†æ›´æ™ºèƒ½çš„éŸ³é¢‘-è§†è§‰äº¤äº’ä½“éªŒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13626",
            "title": "LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action\n  Models",
            "url": "https://huggingface.co/papers/2510.13626",
            "abstract": "State-of-the-art Visual-Language-Action models show high benchmark scores but are brittle to various perturbations, particularly in camera viewpoints and robot initial states, and often ignore language instructions.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual-Language-Action (VLA) models report impressive success rates on robotic manipulation benchmarks, yet these results may mask fundamental weaknesses in robustness. We perform a systematic vulnerability analysis by introducing controlled perturbations across seven dimensions: objects layout, camera viewpoints, robot initial states, language instructions, light conditions, background textures and sensor noise. We comprehensively analyzed multiple state-of-the-art models and revealed consistent brittleness beneath apparent competence. Our analysis exposes critical weaknesses: models exhibit extreme sensitivity to perturbation factors, including camera viewpoints and robot initial states, with performance dropping from 95% to below 30% under modest perturbations. Surprisingly, models are largely insensitive to language variations, with further experiments revealing that models tend to ignore language instructions completely. Our findings challenge the assumption that high benchmark scores equate to true competency and highlight the need for evaluation practices that assess reliability under realistic variation.",
            "score": 21,
            "issue_id": 6445,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 15",
                "zh": "10æœˆ15æ—¥"
            },
            "hash": "2e3c1031d59c2710",
            "authors": [
                "Senyu Fei",
                "Siyin Wang",
                "Junhao Shi",
                "Zihao Dai",
                "Jikun Cai",
                "Pengfang Qian",
                "Li Ji",
                "Xinzhe He",
                "Shiduo Zhang",
                "Zhaoye Fei",
                "Jinlan Fu",
                "Jingjing Gong",
                "Xipeng Qiu"
            ],
            "affiliations": [
                "Fudan University",
                "National University of Singapore",
                "Shanghai Innovation Institute",
                "Tongji University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13626.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#benchmark",
                    "#video",
                    "#interpretability",
                    "#security"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ¥Ñ€ÑƒĞ¿ĞºĞ¾ÑÑ‚ÑŒ VLA Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸ ÑĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Visual-Language-Action (VLA) Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… (Ğ´Ğ¾ 95% ÑƒÑĞ¿ĞµÑ…Ğ°), Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ÑÑŒ ĞºÑ€Ğ°Ğ¹Ğ½Ğµ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹ Ğº Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸ÑĞ¼: Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ĞºÑƒÑ€ÑĞ° ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸Ğ»Ğ¸ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ·Ñ‹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ° ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¾ 30%. ĞÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ñ‚Ñ€ĞµĞ²Ğ¾Ğ¶Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸Ğ³Ğ½Ğ¾Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸, Ñ…Ğ¾Ñ‚Ñ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ñ‹ Ğ½Ğ° Ğ½Ğ¸Ñ… Ğ¾Ğ¿Ğ¸Ñ€Ğ°Ñ‚ÑŒÑÑ. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ½Ğµ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€ÑƒÑÑ‚ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ÑÑ Ğ¿ĞµÑ€ĞµÑĞ¼Ğ¾Ñ‚Ñ€ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞ¸ VLA Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Unmasking the Fragility of Visual-Language-Action Models",
                    "desc": "This paper investigates the vulnerabilities of state-of-the-art Visual-Language-Action (VLA) models in robotic manipulation tasks. Despite achieving high benchmark scores, these models show significant weaknesses when faced with changes in camera angles and robot starting positions, leading to drastic drops in performance. The study systematically tests various perturbations, revealing that the models are particularly sensitive to environmental changes while largely ignoring language instructions. The findings suggest that high performance on benchmarks does not guarantee robustness, emphasizing the need for better evaluation methods that reflect real-world conditions."
                },
                "zh": {
                    "title": "é«˜åˆ†ä¸ç­‰äºé«˜å¯é æ€§ï¼ŒVLAæ¨¡å‹éœ€æ›´ä¸¥è°¨è¯„ä¼°",
                    "desc": "æœ¬ç ”ç©¶åˆ†æäº†è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹åœ¨æœºå™¨äººæ“ä½œä¸­çš„è„†å¼±æ€§ã€‚å°½ç®¡è¿™äº›æ¨¡å‹åœ¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ä¸åŒçš„æ‰°åŠ¨ä¸‹ï¼Œå°¤å…¶æ˜¯ç›¸æœºè§†è§’å’Œæœºå™¨äººåˆå§‹çŠ¶æ€çš„å˜åŒ–æ—¶ï¼Œè¡¨ç°å´æä¸ºä¸ç¨³å®šã€‚æˆ‘ä»¬çš„åˆ†ææ˜¾ç¤ºï¼Œæ¨¡å‹å¯¹æ‰°åŠ¨å› ç´ æä¸ºæ•æ„Ÿï¼Œæ€§èƒ½åœ¨è½»å¾®æ‰°åŠ¨ä¸‹å¯èƒ½ä»95%é™è‡³30%ä»¥ä¸‹ã€‚æ›´ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæ¨¡å‹å¯¹è¯­è¨€æŒ‡ä»¤çš„å˜åŒ–å‡ ä¹æ²¡æœ‰ååº”ï¼Œå¾€å¾€å®Œå…¨å¿½è§†è¿™äº›æŒ‡ä»¤ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.07944",
            "title": "CVD-STORM: Cross-View Video Diffusion with Spatial-Temporal\n  Reconstruction Model for Autonomous Driving",
            "url": "https://huggingface.co/papers/2510.07944",
            "abstract": "CVD-STORM, a cross-view video diffusion model with a spatial-temporal reconstruction VAE, enhances video generation quality and provides depth estimation for dynamic scenes.  \t\t\t\t\tAI-generated summary \t\t\t\t Generative models have been widely applied to world modeling for environment simulation and future state prediction. With advancements in autonomous driving, there is a growing demand not only for high-fidelity video generation under various controls, but also for producing diverse and meaningful information such as depth estimation. To address this, we propose CVD-STORM, a cross-view video diffusion model utilizing a spatial-temporal reconstruction Variational Autoencoder (VAE) that generates long-term, multi-view videos with 4D reconstruction capabilities under various control inputs. Our approach first fine-tunes the VAE with an auxiliary 4D reconstruction task, enhancing its ability to encode 3D structures and temporal dynamics. Subsequently, we integrate this VAE into the video diffusion process to significantly improve generation quality. Experimental results demonstrate that our model achieves substantial improvements in both FID and FVD metrics. Additionally, the jointly-trained Gaussian Splatting Decoder effectively reconstructs dynamic scenes, providing valuable geometric information for comprehensive scene understanding.",
            "score": 20,
            "issue_id": 6445,
            "pub_date": "2025-10-09",
            "pub_date_card": {
                "ru": "9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 9",
                "zh": "10æœˆ9æ—¥"
            },
            "hash": "73e48c906fb522f5",
            "authors": [
                "Tianrui Zhang",
                "Yichen Liu",
                "Zilin Guo",
                "Yuxin Guo",
                "Jingcheng Ni",
                "Chenjing Ding",
                "Dan Xu",
                "Lewei Lu",
                "Zehuan Wu"
            ],
            "affiliations": [
                "Sensetime Research",
                "The Hong Kong University of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.07944.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#3d",
                    "#optimization",
                    "#video",
                    "#multimodal",
                    "#diffusion"
                ],
                "emoji": "ğŸš—",
                "ru": {
                    "title": "4D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ CVD-STORM â€” Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñƒ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ»Ğ¸ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¹ VAE, Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ¸Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ 4D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ. Ğ˜Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ°ĞºĞ¾Ğ³Ğ¾ VAE Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼ FID Ğ¸ FVD. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ° Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ³Ğ´Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ÑÑ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ½Ğ¾ Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ ÑÑ†ĞµĞ½Ğµ."
                },
                "en": {
                    "title": "Enhancing Video Generation with Depth Estimation Using CVD-STORM",
                    "desc": "CVD-STORM is a novel cross-view video diffusion model designed to improve the quality of video generation while also providing depth estimation for dynamic scenes. It employs a spatial-temporal reconstruction Variational Autoencoder (VAE) that enhances the model's ability to capture 3D structures and temporal dynamics through a fine-tuning process. By integrating this VAE into the video diffusion framework, CVD-STORM achieves significant improvements in video generation metrics such as FID and FVD. The model also utilizes a Gaussian Splatting Decoder to effectively reconstruct dynamic scenes, offering valuable geometric insights for better scene understanding."
                },
                "zh": {
                    "title": "CVD-STORMï¼šæå‡è§†é¢‘ç”Ÿæˆä¸æ·±åº¦ä¼°è®¡çš„åˆ›æ–°æ¨¡å‹",
                    "desc": "CVD-STORMæ˜¯ä¸€ç§è·¨è§†è§’è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œç»“åˆäº†æ—¶ç©ºé‡å»ºçš„å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ï¼Œæ—¨åœ¨æå‡è§†é¢‘ç”Ÿæˆè´¨é‡å¹¶ä¸ºåŠ¨æ€åœºæ™¯æä¾›æ·±åº¦ä¼°è®¡ã€‚è¯¥æ¨¡å‹é€šè¿‡è¾…åŠ©çš„4Dé‡å»ºä»»åŠ¡å¯¹VAEè¿›è¡Œå¾®è°ƒï¼Œä»è€Œå¢å¼ºå…¶ç¼–ç ä¸‰ç»´ç»“æ„å’Œæ—¶é—´åŠ¨æ€çš„èƒ½åŠ›ã€‚éšåï¼Œå°†è¯¥VAEé›†æˆåˆ°è§†é¢‘æ‰©æ•£è¿‡ç¨‹ä¸­ï¼Œæ˜¾è‘—æé«˜äº†ç”Ÿæˆè´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨FIDå’ŒFVDæŒ‡æ ‡ä¸Šå‡æœ‰æ˜¾è‘—æå‡ï¼ŒåŒæ—¶è”åˆè®­ç»ƒçš„é«˜æ–¯ç‚¹äº‘è§£ç å™¨æœ‰æ•ˆé‡å»ºåŠ¨æ€åœºæ™¯ï¼Œä¸ºå…¨é¢ç†è§£åœºæ™¯æä¾›äº†æœ‰ä»·å€¼çš„å‡ ä½•ä¿¡æ¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13809",
            "title": "PhysMaster: Mastering Physical Representation for Video Generation via\n  Reinforcement Learning",
            "url": "https://huggingface.co/papers/2510.13809",
            "abstract": "PhysMaster enhances video generation by integrating physical knowledge through PhysEncoder, using reinforcement learning and Direct Preference Optimization to improve physics-awareness.  \t\t\t\t\tAI-generated summary \t\t\t\t Video generation models nowadays are capable of generating visually realistic videos, but often fail to adhere to physical laws, limiting their ability to generate physically plausible videos and serve as ''world models''. To address this issue, we propose PhysMaster, which captures physical knowledge as a representation for guiding video generation models to enhance their physics-awareness. Specifically, PhysMaster is based on the image-to-video task where the model is expected to predict physically plausible dynamics from the input image. Since the input image provides physical priors like relative positions and potential interactions of objects in the scenario, we devise PhysEncoder to encode physical information from it as an extra condition to inject physical knowledge into the video generation process. The lack of proper supervision on the model's physical performance beyond mere appearance motivates PhysEncoder to apply reinforcement learning with human feedback to physical representation learning, which leverages feedback from generation models to optimize physical representations with Direct Preference Optimization (DPO) in an end-to-end manner. PhysMaster provides a feasible solution for improving physics-awareness of PhysEncoder and thus of video generation, proving its ability on a simple proxy task and generalizability to wide-ranging physical scenarios. This implies that our PhysMaster, which unifies solutions for various physical processes via representation learning in the reinforcement learning paradigm, can act as a generic and plug-in solution for physics-aware video generation and broader applications.",
            "score": 18,
            "issue_id": 6444,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 15",
                "zh": "10æœˆ15æ—¥"
            },
            "hash": "74cd3b0ff8137389",
            "authors": [
                "Sihui Ji",
                "Xi Chen",
                "Xin Tao",
                "Pengfei Wan",
                "Hengshuang Zhao"
            ],
            "affiliations": [
                "Kling Team, Kuaishou Technology",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13809.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#games",
                    "#rl",
                    "#video",
                    "#multimodal",
                    "#optimization"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ·Ğ°ĞºĞ¾Ğ½Ğ°Ğ¼ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ",
                    "desc": "PhysMaster ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑÑ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ PhysEncoder. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ reinforcement learning Ğ¸ Direct Preference Optimization, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ½Ğ°ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. PhysEncoder Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ (Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¸Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ) Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞµÑ‘ ĞºĞ°Ğº Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğµ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ­Ñ‚Ğ¾ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ plug-in Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑĞ¼, Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶Ğ°Ñ video generation Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ñ€Ğ¾Ğ»Ğ¸ Ğ½Ğ°ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ñ… world models."
                },
                "en": {
                    "title": "Enhancing Video Realism with Physics-Aware Generation",
                    "desc": "PhysMaster is a novel approach to video generation that incorporates physical knowledge to enhance the realism of generated videos. It utilizes a component called PhysEncoder, which extracts physical information from input images to guide the video generation process. By employing reinforcement learning and Direct Preference Optimization, PhysMaster optimizes the model's understanding of physical dynamics, ensuring that the generated videos are not only visually appealing but also adhere to the laws of physics. This method demonstrates the potential for creating more accurate world models that can be applied to various physical scenarios."
                },
                "zh": {
                    "title": "PhysMasterï¼šæå‡è§†é¢‘ç”Ÿæˆçš„ç‰©ç†æ„è¯†",
                    "desc": "PhysMaster æ˜¯ä¸€ç§é€šè¿‡æ•´åˆç‰©ç†çŸ¥è¯†æ¥å¢å¼ºè§†é¢‘ç”Ÿæˆçš„æ¨¡å‹ã€‚å®ƒä½¿ç”¨ PhysEncoder ç¼–ç ç‰©ç†ä¿¡æ¯ï¼Œä»¥æé«˜è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„ç‰©ç†æ„è¯†ã€‚è¯¥æ¨¡å‹é‡‡ç”¨å¼ºåŒ–å­¦ä¹ å’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æ¥ä¼˜åŒ–ç‰©ç†è¡¨ç¤ºï¼Œç¡®ä¿ç”Ÿæˆçš„è§†é¢‘ç¬¦åˆç‰©ç†è§„å¾‹ã€‚PhysMaster æä¾›äº†ä¸€ç§é€šç”¨çš„è§£å†³æ–¹æ¡ˆï¼Œé€‚ç”¨äºå„ç§ç‰©ç†è¿‡ç¨‹çš„è¡¨ç¤ºå­¦ä¹ ï¼Œèƒ½å¤Ÿå¹¿æ³›åº”ç”¨äºç‰©ç†æ„è¯†è§†é¢‘ç”Ÿæˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13804",
            "title": "Generative Universal Verifier as Multimodal Meta-Reasoner",
            "url": "https://huggingface.co/papers/2510.13804",
            "abstract": "Generative Universal Verifier enhances multimodal reasoning by providing reliable visual verification through ViVerBench, OmniVerifier-7B, and OmniVerifier-TTS, improving generation and refinement capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Generative Universal Verifier, a novel concept and plugin designed for next-generation multimodal reasoning in vision-language models and unified multimodal models, providing the fundamental capability of reflection and refinement on visual outcomes during the reasoning and generation process. This work makes three main contributions: (1) We build ViVerBench, a comprehensive benchmark spanning 16 categories of critical tasks for evaluating visual outcomes in multimodal reasoning. Results show that existing VLMs consistently underperform across these tasks, underscoring a substantial gap from human-level capability in reliable visual verification. (2) We design two automated pipelines to construct large-scale visual verification data and train OmniVerifier-7B, the first omni-capable generative verifier trained for universal visual verification and achieves notable gains on ViVerBench(+8.3). Through training, we identify three atomic capabilities in visual verification and demonstrate how they generalize and interact synergistically. (3) We propose OmniVerifier-TTS, a sequential test-time scaling paradigm that leverages the universal verifier to bridge image generation and editing within unified models, enhancing the upper bound of generative ability through iterative fine-grained optimization. Beyond generation, we extend universal verifier to broader world-modeling interleaved reasoning scenarios. Empirically, OmniVerifier-TTS achieves improvements on T2I-ReasonBench(+3.7), and GenEval++(+4.3), outperforming existing parallel test-time scaling methods, such as Best-of-N. By endowing multimodal reasoning with reliable visual verification, OmniVerifier advances both reliable reflection during generation and scalable test-time refinement, marking a step toward more trustworthy and controllable next-generation reasoning systems.",
            "score": 18,
            "issue_id": 6444,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 15",
                "zh": "10æœˆ15æ—¥"
            },
            "hash": "85e2081e6a4e6525",
            "authors": [
                "Xinchen Zhang",
                "Xiaoying Zhang",
                "Youbin Wu",
                "Yanbin Cao",
                "Renrui Zhang",
                "Ruihang Chu",
                "Ling Yang",
                "Yujiu Yang"
            ],
            "affiliations": [
                "ByteDance Seed",
                "Princeton University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13804.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#dataset",
                    "#multimodal",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Generative Universal Verifier â€” Ğ½Ğ¾Ğ²ÑƒÑ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ViVerBench Ğ¸Ğ· 16 ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ»Ğ¸ OmniVerifier-7B â€” Ğ¿ĞµÑ€Ğ²ÑƒÑ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ²ÑˆÑƒÑ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ +8.3 Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° OmniVerifier-TTS Ğ´Ğ»Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ test-time scaling, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ñ‚Ğ¸Ğ¿Ğ° Best-of-N Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… T2I-ReasonBench (+3.7) Ğ¸ GenEval++ (+4.3), Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğµ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Enhancing Multimodal Reasoning with Reliable Visual Verification",
                    "desc": "The Generative Universal Verifier is a new tool that improves how machines understand and generate visual information alongside text. It introduces ViVerBench, a benchmark for testing how well models can verify visual outcomes, revealing that current models struggle compared to human performance. The paper also presents OmniVerifier-7B, a generative verifier that enhances visual verification capabilities and shows significant improvements in benchmark scores. Additionally, OmniVerifier-TTS offers a method for refining image generation and editing, leading to better overall performance in multimodal reasoning tasks."
                },
                "zh": {
                    "title": "æå‡å¤šæ¨¡æ€æ¨ç†çš„å¯é æ€§ä¸ç”Ÿæˆèƒ½åŠ›",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°æ¦‚å¿µâ€”â€”ç”Ÿæˆé€šç”¨éªŒè¯å™¨ï¼ˆGenerative Universal Verifierï¼‰ï¼Œæ—¨åœ¨æå‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬æ„å»ºäº†ViVerBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ¶µç›–16ç±»å…³é”®ä»»åŠ¡çš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤šæ¨¡æ€æ¨ç†ä¸­çš„è§†è§‰ç»“æœã€‚é€šè¿‡è®­ç»ƒOmniVerifier-7Bï¼Œæˆ‘ä»¬è¯†åˆ«å‡ºè§†è§‰éªŒè¯ä¸­çš„ä¸‰ç§åŸºæœ¬èƒ½åŠ›ï¼Œå¹¶å±•ç¤ºå®ƒä»¬çš„ååŒä½œç”¨ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†OmniVerifier-TTSï¼Œé€šè¿‡è¿­ä»£ä¼˜åŒ–æå‡ç”Ÿæˆèƒ½åŠ›ï¼Œæ¨åŠ¨äº†æ›´å¯é çš„å¤šæ¨¡æ€æ¨ç†ç³»ç»Ÿçš„å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.04767",
            "title": "ParallelBench: Understanding the Trade-offs of Parallel Decoding in\n  Diffusion LLMs",
            "url": "https://huggingface.co/papers/2510.04767",
            "abstract": "Parallel decoding in diffusion LLMs degrades generation quality due to ignored token dependencies, highlighting the need for new decoding methods and benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t While most autoregressive LLMs are constrained to one-by-one decoding, diffusion LLMs (dLLMs) have attracted growing interest for their potential to dramatically accelerate inference through parallel decoding. Despite this promise, the conditional independence assumption in dLLMs causes parallel decoding to ignore token dependencies, inevitably degrading generation quality when these dependencies are strong. However, existing works largely overlook these inherent challenges, and evaluations on standard benchmarks (e.g., math and coding) are not sufficient to capture the quality degradation caused by parallel decoding. To address this gap, we first provide an information-theoretic analysis of parallel decoding. We then conduct case studies on analytically tractable synthetic list operations from both data distribution and decoding strategy perspectives, offering quantitative insights that highlight the fundamental limitations of parallel decoding. Building on these insights, we propose ParallelBench, the first benchmark specifically designed for dLLMs, featuring realistic tasks that are trivial for humans and autoregressive LLMs yet exceptionally challenging for dLLMs under parallel decoding. Using ParallelBench, we systematically analyze both dLLMs and autoregressive LLMs, revealing that: (i) dLLMs under parallel decoding can suffer dramatic quality degradation in real-world scenarios, and (ii) current parallel decoding strategies struggle to adapt their degree of parallelism based on task difficulty, thus failing to achieve meaningful speedup without compromising quality. Our findings underscore the pressing need for innovative decoding methods that can overcome the current speed-quality trade-off. We release our benchmark to help accelerate the development of truly efficient dLLMs.",
            "score": 18,
            "issue_id": 6445,
            "pub_date": "2025-10-06",
            "pub_date_card": {
                "ru": "6 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 6",
                "zh": "10æœˆ6æ—¥"
            },
            "hash": "b9d87711f8ddf258",
            "authors": [
                "Wonjun Kang",
                "Kevin Galim",
                "Seunghyuk Oh",
                "Minjae Lee",
                "Yuchen Zeng",
                "Shuibai Zhang",
                "Coleman Hooper",
                "Yuezhou Hu",
                "Hyung Il Koo",
                "Nam Ik Cho",
                "Kangwook Lee"
            ],
            "affiliations": [
                "FuriosaAI",
                "KRAFTON AI",
                "Microsoft Research",
                "Seoul National University",
                "UC Berkeley",
                "UW-Madison"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.04767.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#diffusion",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… LLM",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (dLLM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¾Ğ± ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ³Ğ½Ğ¾Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ½ĞµĞ¸Ğ·Ğ±ĞµĞ¶Ğ½Ğ¾ ÑƒÑ…ÑƒĞ´ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ParallelBench, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ dLLM Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€Ğ¸Ğ²Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ»ÑĞ´ĞµĞ¹ Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼, Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°Ñ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… dLLM."
                },
                "en": {
                    "title": "Enhancing Quality in Fast Decoding: The Need for Innovation in dLLMs",
                    "desc": "This paper discusses the challenges of using parallel decoding in diffusion language models (dLLMs), which can lead to a decline in the quality of generated text due to overlooked token dependencies. The authors highlight that while dLLMs can speed up inference, the assumption of conditional independence can harm performance when token relationships are strong. They introduce ParallelBench, a new benchmark designed to evaluate dLLMs under realistic tasks that are easy for humans but difficult for dLLMs using parallel decoding. The study reveals that current parallel decoding methods do not effectively adjust to task complexity, resulting in significant quality loss, emphasizing the need for better decoding strategies."
                },
                "zh": {
                    "title": "å¹³è¡Œè§£ç çš„æŒ‘æˆ˜ä¸æ–°åŸºå‡†çš„å¿…è¦æ€§",
                    "desc": "åœ¨æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹ï¼ˆdLLMsï¼‰ä¸­ï¼Œå¹³è¡Œè§£ç ä¼šå¯¼è‡´ç”Ÿæˆè´¨é‡ä¸‹é™ï¼Œå› ä¸ºå®ƒå¿½ç•¥äº†ä»¤ç‰Œä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚è¿™ç§æ¡ä»¶ç‹¬ç«‹å‡è®¾ä½¿å¾—åœ¨å¼ºä¾èµ–å…³ç³»çš„æƒ…å†µä¸‹ï¼Œå¹³è¡Œè§£ç çš„æ•ˆæœä¸ä½³ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡é¦–å…ˆè¿›è¡Œäº†ä¿¡æ¯è®ºåˆ†æï¼Œå¹¶æå‡ºäº†ParallelBenchåŸºå‡†ï¼Œä¸“é—¨ç”¨äºè¯„ä¼°dLLMsåœ¨å¹³è¡Œè§£ç ä¸‹çš„è¡¨ç°ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œç°æœ‰çš„å¹³è¡Œè§£ç ç­–ç•¥åœ¨ä»»åŠ¡éš¾åº¦å˜åŒ–æ—¶éš¾ä»¥è°ƒæ•´å¹¶ä¿æŒè´¨é‡ï¼Œå¼ºè°ƒäº†å¼€å‘æ–°è§£ç æ–¹æ³•çš„å¿…è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13802",
            "title": "Trace Anything: Representing Any Video in 4D via Trajectory Fields",
            "url": "https://huggingface.co/papers/2510.13802",
            "abstract": "Trace Anything, a neural network, predicts video trajectories in a single pass, achieving state-of-the-art performance and demonstrating efficiency and emergent abilities like motion forecasting.  \t\t\t\t\tAI-generated summary \t\t\t\t Effective spatio-temporal representation is fundamental to modeling, understanding, and predicting dynamics in videos. The atomic unit of a video, the pixel, traces a continuous 3D trajectory over time, serving as the primitive element of dynamics. Based on this principle, we propose representing any video as a Trajectory Field: a dense mapping that assigns a continuous 3D trajectory function of time to each pixel in every frame. With this representation, we introduce Trace Anything, a neural network that predicts the entire trajectory field in a single feed-forward pass. Specifically, for each pixel in each frame, our model predicts a set of control points that parameterizes a trajectory (i.e., a B-spline), yielding its 3D position at arbitrary query time instants. We trained the Trace Anything model on large-scale 4D data, including data from our new platform, and our experiments demonstrate that: (i) Trace Anything achieves state-of-the-art performance on our new benchmark for trajectory field estimation and performs competitively on established point-tracking benchmarks; (ii) it offers significant efficiency gains thanks to its one-pass paradigm, without requiring iterative optimization or auxiliary estimators; and (iii) it exhibits emergent abilities, including goal-conditioned manipulation, motion forecasting, and spatio-temporal fusion. Project page: https://trace-anything.github.io/.",
            "score": 16,
            "issue_id": 6444,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 15",
                "zh": "10æœˆ15æ—¥"
            },
            "hash": "2a519ed83bf3da2a",
            "authors": [
                "Xinhang Liu",
                "Yuxi Xiao",
                "Donny Y. Chen",
                "Jiashi Feng",
                "Yu-Wing Tai",
                "Chi-Keung Tang",
                "Bingyi Kang"
            ],
            "affiliations": [
                "ByteDance Seed",
                "Dartmouth College",
                "HKUST",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13802.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#games",
                    "#video",
                    "#dataset",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "ĞÑ‚ÑĞ»ĞµĞ´Ğ¸Ñ‚ÑŒ Ğ²ÑÑ‘: Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ²ÑĞµÑ… Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Trace Anything â€” Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²ÑĞµÑ… Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾ ĞºĞ°Ğº Ğ¿Ğ¾Ğ»Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹, Ğ³Ğ´Ğµ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼Ñƒ Ğ¿Ğ¸ĞºÑĞµĞ»Ñ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ°Ñ 3D-Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ, Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ B-ÑĞ¿Ğ»Ğ°Ğ¹Ğ½Ğ°Ğ¼Ğ¸ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¾Ñ‡ĞºĞ°Ğ¼Ğ¸. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… 4D Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ĞµĞ¹ Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ emergent abilities, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ¼ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Predicting Video Trajectories with Efficiency and Precision",
                    "desc": "The paper introduces Trace Anything, a neural network designed to predict video trajectories efficiently in a single pass. It utilizes a novel representation called Trajectory Field, which maps each pixel in a video to a continuous 3D trajectory function over time. This approach allows the model to generate control points for B-splines, enabling accurate trajectory predictions at any moment. The results show that Trace Anything not only achieves state-of-the-art performance but also demonstrates significant efficiency and advanced capabilities like motion forecasting and goal-conditioned manipulation."
                },
                "zh": {
                    "title": "å•æ¬¡é¢„æµ‹ï¼Œè½¨è¿¹è¿½è¸ªçš„æœªæ¥",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºTrace Anythingçš„ç¥ç»ç½‘ç»œï¼Œç”¨äºåœ¨å•æ¬¡å‰å‘ä¼ æ’­ä¸­é¢„æµ‹è§†é¢‘çš„è½¨è¿¹åœºã€‚è¯¥æ–¹æ³•é€šè¿‡å°†è§†é¢‘è¡¨ç¤ºä¸ºæ¯ä¸ªåƒç´ çš„è¿ç»­ä¸‰ç»´è½¨è¿¹å‡½æ•°ï¼Œæ¥æœ‰æ•ˆå»ºæ¨¡å’Œé¢„æµ‹è§†é¢‘ä¸­çš„åŠ¨æ€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTrace Anythingåœ¨è½¨è¿¹åœºä¼°è®¡çš„åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶åœ¨ç‚¹è·Ÿè¸ªåŸºå‡†ä¸Šä¹Ÿå…·æœ‰ç«äº‰åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨æ•ˆç‡ä¸Šæœ‰æ˜¾è‘—æå‡ï¼Œèƒ½å¤Ÿå®ç°ç›®æ ‡æ¡ä»¶çš„æ“ä½œã€è¿åŠ¨é¢„æµ‹å’Œæ—¶ç©ºèåˆç­‰æ–°å…´èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13621",
            "title": "The Role of Computing Resources in Publishing Foundation Model Research",
            "url": "https://huggingface.co/papers/2510.13621",
            "abstract": "Increased computing resources are correlated with national funding and citations in foundation model research, but not with research environment, domain, or methodology.  \t\t\t\t\tAI-generated summary \t\t\t\t Cutting-edge research in Artificial Intelligence (AI) requires considerable resources, including Graphics Processing Units (GPUs), data, and human resources. In this paper, we evaluate of the relationship between these resources and the scientific advancement of foundation models (FM). We reviewed 6517 FM papers published between 2022 to 2024, and surveyed 229 first-authors to the impact of computing resources on scientific output. We find that increased computing is correlated with national funding allocations and citations, but our findings don't observe the strong correlations with research environment (academic or industrial), domain, or study methodology. We advise that individuals and institutions focus on creating shared and affordable computing opportunities to lower the entry barrier for under-resourced researchers. These steps can help expand participation in FM research, foster diversity of ideas and contributors, and sustain innovation and progress in AI. The data will be available at: https://mit-calc.csail.mit.edu/",
            "score": 10,
            "issue_id": 6444,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 15",
                "zh": "10æœˆ15æ—¥"
            },
            "hash": "f9f73f3064dbf2e4",
            "authors": [
                "Yuexing Hao",
                "Yue Huang",
                "Haoran Zhang",
                "Chenyang Zhao",
                "Zhenwen Liang",
                "Paul Pu Liang",
                "Yue Zhao",
                "Lichao Sun",
                "Saleh Kalantari",
                "Xiangliang Zhang",
                "Marzyeh Ghassemi"
            ],
            "affiliations": [
                "CSE, University of Notre Dame, South Bend, 46556, USA",
                "Computer Science Department, Lehigh University, Bethlehem, 18015, USA",
                "Computer Science Department, University of California, Los Angeles, 90095, USA",
                "Cornell University, Ithaca, 14850, USA",
                "EECS, MIT, Cambridge, 02135, USA",
                "School of Advanced Computing, University of Southern California, Los Angeles, 90007, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13621.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#ethics"
                ],
                "emoji": "ğŸ’°",
                "ru": {
                    "title": "Ğ”ĞµĞ½ÑŒĞ³Ğ¸ Ñ€ĞµÑˆĞ°ÑÑ‚: ĞºĞ°Ğº Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑÑƒÑ€ÑÑ‹ Ğ²Ğ»Ğ¸ÑÑÑ‚ Ğ½Ğ° Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ foundation models",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ²ÑĞ·ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸ Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¾Ğ¼ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ foundation models (Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ·ÑƒÑ‡Ğ¸Ğ»Ğ¸ 6517 ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ğ¸ Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¸Ğ»Ğ¸ 229 Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹, Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ² ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ¾Ğ¼ Ğº GPU Ğ¸ Ğ½Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ñ†Ğ¸Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒÑ Ñ€Ğ°Ğ±Ğ¾Ñ‚. Ğ˜Ğ½Ñ‚ĞµÑ€ĞµÑĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ½Ğµ Ğ±Ñ‹Ğ»Ğ¾ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ ÑĞ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸ Ğ¸ Ñ‚Ğ¸Ğ¿Ğ¾Ğ¼ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ (Ğ°ĞºĞ°Ğ´ĞµĞ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ğ°Ñ), Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ¼ Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸ĞµĞ¹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒÑÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±Ñ‰ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¿Ğ¾ Ñ†ĞµĞ½Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñ‹, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ğ±Ğ°Ñ€ÑŒĞµÑ€ Ğ²Ñ…Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Empowering AI Research Through Shared Computing Resources",
                    "desc": "This paper investigates how computing resources, such as GPUs and funding, influence the progress of foundation model research in AI. By analyzing 6517 papers and surveying 229 authors, the study finds a strong correlation between increased computing resources and national funding and citations. However, it reveals that these resources do not significantly impact the research environment, domain, or methodology. The authors recommend creating shared computing resources to support under-resourced researchers, promoting diversity and innovation in the field."
                },
                "zh": {
                    "title": "è®¡ç®—èµ„æºä¸åŸºç¡€æ¨¡å‹ç ”ç©¶çš„å…³ç³»",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†è®¡ç®—èµ„æºä¸åŸºç¡€æ¨¡å‹ç ”ç©¶çš„ç§‘å­¦è¿›å±•ä¹‹é—´çš„å…³ç³»ã€‚æˆ‘ä»¬åˆ†æäº†6517ç¯‡2022è‡³2024å¹´é—´å‘è¡¨çš„åŸºç¡€æ¨¡å‹è®ºæ–‡ï¼Œå¹¶è°ƒæŸ¥äº†229ä½ç¬¬ä¸€ä½œè€…å¯¹è®¡ç®—èµ„æºå½±å“çš„çœ‹æ³•ã€‚ç ”ç©¶å‘ç°ï¼Œè®¡ç®—èµ„æºçš„å¢åŠ ä¸å›½å®¶èµ„é‡‘åˆ†é…å’Œå¼•ç”¨æ¬¡æ•°ç›¸å…³ï¼Œä½†ä¸ç ”ç©¶ç¯å¢ƒã€é¢†åŸŸæˆ–ç ”ç©¶æ–¹æ³•æ²¡æœ‰æ˜¾è‘—ç›¸å…³æ€§ã€‚æˆ‘ä»¬å»ºè®®ä¸ªäººå’Œæœºæ„åº”ä¸“æ³¨äºåˆ›å»ºå…±äº«å’Œå¯è´Ÿæ‹…çš„è®¡ç®—æœºä¼šï¼Œä»¥é™ä½èµ„æºä¸è¶³ç ”ç©¶è€…çš„è¿›å…¥é—¨æ§›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13515",
            "title": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning",
            "url": "https://huggingface.co/papers/2510.13515",
            "abstract": "A novel Universal Multimodal Embedding (UniME-V2) model uses MLLMs to enhance representation learning by identifying diverse, high-quality hard negatives and improving discriminative capacity through soft semantic matching scores.  \t\t\t\t\tAI-generated summary \t\t\t\t Universal multimodal embedding models are foundational to various tasks. Existing approaches typically employ in-batch negative mining by measuring the similarity of query-candidate pairs. However, these methods often struggle to capture subtle semantic differences among candidates and lack diversity in negative samples. Moreover, the embeddings exhibit limited discriminative ability in distinguishing false and hard negatives. In this paper, we leverage the advanced understanding capabilities of MLLMs to enhance representation learning and present a novel Universal Multimodal Embedding (UniME-V2) model. Our approach first constructs a potential hard negative set through global retrieval. We then introduce the MLLM-as-a-Judge mechanism, which utilizes MLLMs to assess the semantic alignment of query-candidate pairs and generate soft semantic matching scores. These scores serve as a foundation for hard negative mining, mitigating the impact of false negatives and enabling the identification of diverse, high-quality hard negatives. Furthermore, the semantic matching scores are used as soft labels to mitigate the rigid one-to-one mapping constraint. By aligning the similarity matrix with the soft semantic matching score matrix, the model learns semantic distinctions among candidates, significantly enhancing its discriminative capacity. To further improve performance, we propose UniME-V2-Reranker, a reranking model trained on our mined hard negatives through a joint pairwise and listwise optimization approach. We conduct comprehensive experiments on the MMEB benchmark and multiple retrieval tasks, demonstrating that our method achieves state-of-the-art performance on average across all tasks.",
            "score": 10,
            "issue_id": 6444,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 15",
                "zh": "10æœˆ15æ—¥"
            },
            "hash": "4e7810d5695a73ea",
            "authors": [
                "Tiancheng Gu",
                "Kaicheng Yang",
                "Kaichen Zhang",
                "Xiang An",
                "Ziyong Feng",
                "Yueyi Zhang",
                "Weidong Cai",
                "Jiankang Deng",
                "Lidong Bing"
            ],
            "affiliations": [
                "Imperial College London",
                "LMMs-Lab Team",
                "M.R.L. Team",
                "MiroMind AI",
                "The University of Sydney"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13515.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#benchmark",
                    "#training",
                    "#multimodal"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "MLLM ĞºĞ°Ğº ÑÑƒĞ´ÑŒÑ Ğ´Ğ»Ñ ÑƒĞ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°Ğ¹Ğ½Ğ¸Ğ½Ğ³Ğ° hard negatives Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ°Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ UniME-V2 â€” ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (MLLMs) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ MLLM Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑÑƒĞ´ÑŒĞ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¿Ğ°Ñ€ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ-ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑĞ³ĞºĞ¸Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº ÑĞ¾Ğ²Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ÑÑ‚ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ hard negatives, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ»ÑƒĞ¶Ğ°Ñ‚ Ğ¼ÑĞ³ĞºĞ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚ĞºĞ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°Ñ‚ÑŒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ±Ğ»Ğ¸Ğ·ĞºĞ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ MMEB Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ°, Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-Ñ€Ğ°Ğ½ĞºĞµÑ€ UniME-V2-Reranker Ğ´Ğ»Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Multimodal Learning with Smart Negative Mining",
                    "desc": "The paper introduces a new model called Universal Multimodal Embedding version 2 (UniME-V2) that improves how machines understand and represent different types of data. It uses advanced machine learning language models (MLLMs) to find and evaluate hard negative examples, which are crucial for training. By generating soft semantic matching scores, the model can better distinguish between similar candidates and improve its ability to identify relevant information. The results show that UniME-V2 outperforms existing methods in various tasks, making it a significant advancement in multimodal representation learning."
                },
                "zh": {
                    "title": "æå‡å¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ çš„åˆ›æ–°æ¨¡å‹",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„é€šç”¨å¤šæ¨¡æ€åµŒå…¥æ¨¡å‹ï¼ˆUniME-V2ï¼‰ï¼Œæ—¨åœ¨é€šè¿‡è¯†åˆ«å¤šæ ·åŒ–çš„é«˜è´¨é‡å›°éš¾è´Ÿæ ·æœ¬æ¥å¢å¼ºè¡¨ç¤ºå­¦ä¹ ã€‚è¯¥æ¨¡å‹åˆ©ç”¨å¤šè¯­è¨€å¤§æ¨¡å‹ï¼ˆMLLMsï¼‰æ¥è¯„ä¼°æŸ¥è¯¢-å€™é€‰å¯¹çš„è¯­ä¹‰å¯¹é½ï¼Œå¹¶ç”Ÿæˆè½¯è¯­ä¹‰åŒ¹é…åˆ†æ•°ï¼Œä»è€Œæ”¹å–„åŒºåˆ†èƒ½åŠ›ã€‚é€šè¿‡æ„å»ºæ½œåœ¨çš„å›°éš¾è´Ÿæ ·æœ¬é›†ï¼ŒUniME-V2èƒ½å¤Ÿæœ‰æ•ˆå‡è½»å‡è´Ÿæ ·æœ¬çš„å½±å“ï¼Œå¹¶è¯†åˆ«å‡ºå¤šæ ·åŒ–çš„é«˜è´¨é‡å›°éš¾è´Ÿæ ·æœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ£€ç´¢ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13759",
            "title": "Uni-MMMU: A Massive Multi-discipline Multimodal Unified Benchmark",
            "url": "https://huggingface.co/papers/2510.13759",
            "abstract": "Uni-MMMU is a benchmark that evaluates the bidirectional synergy between visual understanding and generation across multiple domains, providing insights into their integration and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Unified multimodal models aim to jointly enable visual understanding and generation, yet current benchmarks rarely examine their true integration. Existing evaluations either treat the two abilities in isolation or overlook tasks that inherently couple them. To address this gap, we present Uni-MMMU, a comprehensive and discipline-aware benchmark that systematically unfolds the bidirectional synergy between generation and understanding across eight reasoning-centric domains, including science, coding, mathematics, and puzzles. Each task is bidirectionally coupled, demanding models to (i) leverage conceptual understanding to guide precise visual synthesis, or (ii) utilize generation as a cognitive scaffold for analytical reasoning. Uni-MMMU incorporates verifiable intermediate reasoning steps, unique ground truths, and a reproducible scoring protocol for both textual and visual outputs. Through extensive evaluation of state-of-the-art unified, generation-only, and understanding-only models, we reveal substantial performance disparities and cross-modal dependencies, offering new insights into when and how these abilities reinforce one another, and establishing a reliable foundation for advancing unified models.",
            "score": 9,
            "issue_id": 6445,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 15",
                "zh": "10æœˆ15æ—¥"
            },
            "hash": "9e262f2fe4ddde55",
            "authors": [
                "Kai Zou",
                "Ziqi Huang",
                "Yuhao Dong",
                "Shulin Tian",
                "Dian Zheng",
                "Hongbo Liu",
                "Jingwen He",
                "Bin Liu",
                "Yu Qiao",
                "Ziwei Liu"
            ],
            "affiliations": [
                "S-Lab, Nanyang Technological University",
                "Shanghai Artificial Intelligence Laboratory",
                "The Chinese University of Hong Kong",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13759.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#survey",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ”Ğ²ÑƒÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½ÑÑ ÑĞ¸Ğ½ĞµÑ€Ğ³Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Uni-MMMU â€” ÑÑ‚Ğ¾ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ unified Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ÑÑ‚ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. Ğ¡ÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ‚ĞµÑÑ‚Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ ÑÑ‚Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ·Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾, Ğ½Ğ¾ Uni-MMMU Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ Ğ¸Ñ… Ñ€ĞµĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ² Ğ²Ğ¾ÑÑŒĞ¼Ğ¸ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ½Ğ°ÑƒĞºÑƒ, Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºÑƒ. ĞšĞ°Ğ¶Ğ´Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¾Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ»Ğ¸Ğ±Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ»Ğ¸Ğ±Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ°Ğº Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¸ ĞºĞ°Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ÑÑ‚ Ğ´Ñ€ÑƒĞ³ Ğ´Ñ€ÑƒĞ³Ğ°."
                },
                "en": {
                    "title": "Bridging Visual Understanding and Generation with Uni-MMMU",
                    "desc": "Uni-MMMU is a new benchmark designed to assess how well visual understanding and generation work together across different fields. It focuses on tasks that require both skills, rather than evaluating them separately. The benchmark includes eight domains, such as science and mathematics, where models must use their understanding to create visuals or use visuals to enhance reasoning. By testing various models, Uni-MMMU highlights important differences in performance and shows how these two abilities can support each other, paving the way for better unified models."
                },
                "zh": {
                    "title": "è§†è§‰ç†è§£ä¸ç”Ÿæˆçš„åŒå‘ååŒè¯„ä¼°",
                    "desc": "Uni-MMMUæ˜¯ä¸€ä¸ªåŸºå‡†æµ‹è¯•ï¼Œè¯„ä¼°è§†è§‰ç†è§£ä¸ç”Ÿæˆä¹‹é—´çš„åŒå‘ååŒï¼Œæ¶µç›–å¤šä¸ªé¢†åŸŸã€‚è¯¥åŸºå‡†ç³»ç»Ÿåœ°æ­ç¤ºäº†ç”Ÿæˆä¸ç†è§£ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨ç§‘å­¦ã€ç¼–ç¨‹ã€æ•°å­¦å’Œè°œé¢˜ç­‰æ¨ç†ä¸­å¿ƒé¢†åŸŸã€‚æ¯ä¸ªä»»åŠ¡éƒ½è¦æ±‚æ¨¡å‹åˆ©ç”¨æ¦‚å¿µç†è§£æ¥æŒ‡å¯¼ç²¾ç¡®çš„è§†è§‰åˆæˆï¼Œæˆ–åˆ©ç”¨ç”Ÿæˆä½œä¸ºåˆ†ææ¨ç†çš„è®¤çŸ¥æ”¯æ¶ã€‚é€šè¿‡å¯¹æœ€å…ˆè¿›çš„ç»Ÿä¸€æ¨¡å‹è¿›è¡Œå¹¿æ³›è¯„ä¼°ï¼ŒUni-MMMUæ­ç¤ºäº†æ€§èƒ½å·®å¼‚å’Œè·¨æ¨¡æ€ä¾èµ–ï¼Œä¸ºç»Ÿä¸€æ¨¡å‹çš„è¿›ä¸€æ­¥å‘å±•æä¾›äº†å¯é çš„åŸºç¡€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.10977",
            "title": "Revisiting Model Interpolation for Efficient Reasoning",
            "url": "https://huggingface.co/papers/2510.10977",
            "abstract": "Model merging, typically on Instruct and Thinking models, has shown remarkable performance for efficient reasoning. In this paper, we systematically revisit the simplest merging method that interpolates two weights directly. Particularly, we observe that model interpolation follows a three-stage evolutionary paradigm with distinct behaviors on the reasoning trajectory. These dynamics provide a principled guide for navigating the performance-cost trade-off. Empirical results demonstrate that a strategically interpolated model surprisingly surpasses sophisticated model merging baselines on both efficiency and effectiveness. We further validate our findings with extensive ablation studies on model layers, modules, and decoding strategies. Ultimately, this work demystifies model interpolation and offers a practical framework for crafting models with precisely targeted reasoning capabilities. Code is available at https://github.com/wutaiqiang/MI{Github}.",
            "score": 8,
            "issue_id": 6444,
            "pub_date": "2025-10-13",
            "pub_date_card": {
                "ru": "13 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 13",
                "zh": "10æœˆ13æ—¥"
            },
            "hash": "bdc53b166ac44504",
            "authors": [
                "Taiqiang Wu",
                "Runming Yang",
                "Tao Liu",
                "Jiahao Wang",
                "Ngai Wong"
            ],
            "affiliations": [
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.10977.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#reasoning",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ”€",
                "ru": {
                    "title": "Ğ¢Ñ€Ğ¸ ÑÑ‚Ğ°Ğ´Ğ¸Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸: Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ±ĞµĞ¶Ğ´Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚ĞµĞ¹ÑˆĞ¸Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ - Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ²ĞµÑĞ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Instruct Ğ¸ Thinking Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞĞ½Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‡ĞµÑ€ĞµĞ· Ñ‚Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ´Ğ¸Ğ¸ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ñ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ¡Ñ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ğ°Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½ĞµĞ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ baseline Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ ĞºĞ°Ğº Ğ¿Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ¿Ñ€Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚."
                },
                "en": {
                    "title": "Unlocking Efficient Reasoning through Model Interpolation",
                    "desc": "This paper explores the concept of model merging, particularly focusing on Instruct and Thinking models, to enhance reasoning efficiency. The authors analyze a basic method of directly interpolating weights from two models, revealing a three-stage evolution in the reasoning process. Their findings indicate that a well-interpolated model can outperform more complex merging techniques in terms of both efficiency and effectiveness. The research provides a structured approach to model interpolation, enabling the development of models with specific reasoning strengths, supported by comprehensive experiments and ablation studies."
                },
                "zh": {
                    "title": "æ¨¡å‹æ’å€¼ï¼šé«˜æ•ˆæ¨ç†çš„æ–°è·¯å¾„",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†æ¨¡å‹åˆå¹¶ï¼Œç‰¹åˆ«æ˜¯åœ¨æŒ‡ä»¤å’Œæ€ç»´æ¨¡å‹ä¸Šçš„åº”ç”¨ï¼Œå±•ç¤ºäº†å…¶åœ¨é«˜æ•ˆæ¨ç†æ–¹é¢çš„å“è¶Šè¡¨ç°ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°å›é¡¾äº†æœ€ç®€å•çš„åˆå¹¶æ–¹æ³•ï¼Œå³ç›´æ¥æ’å€¼ä¸¤ä¸ªæƒé‡ï¼Œå¹¶è§‚å¯Ÿåˆ°æ¨¡å‹æ’å€¼éµå¾ªä¸‰é˜¶æ®µçš„æ¼”å˜èŒƒå¼ï¼Œå…·æœ‰ä¸åŒçš„æ¨ç†è½¨è¿¹ç‰¹å¾ã€‚è¿™äº›åŠ¨æ€ä¸ºåœ¨æ€§èƒ½ä¸æˆæœ¬ä¹‹é—´çš„æƒè¡¡æä¾›äº†åŸåˆ™æ€§æŒ‡å¯¼ã€‚å®è¯ç»“æœè¡¨æ˜ï¼Œç»è¿‡æˆ˜ç•¥æ€§æ’å€¼çš„æ¨¡å‹åœ¨æ•ˆç‡å’Œæœ‰æ•ˆæ€§ä¸Šè¶…è¶Šäº†å¤æ‚çš„æ¨¡å‹åˆå¹¶åŸºçº¿ï¼Œè¿›ä¸€æ­¥é€šè¿‡å¹¿æ³›çš„æ¶ˆèç ”ç©¶éªŒè¯äº†æˆ‘ä»¬çš„å‘ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.10921",
            "title": "FG-CLIP 2: A Bilingual Fine-grained Vision-Language Alignment Model",
            "url": "https://huggingface.co/papers/2510.10921",
            "abstract": "FG-CLIP 2, a bilingual vision-language model, enhances fine-grained alignment for English and Chinese through rich supervision and a new TIC loss, achieving state-of-the-art performance across multiple datasets and tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Fine-grained vision-language understanding requires precise alignment between visual content and linguistic descriptions, a capability that remains limited in current models, particularly in non-English settings. While models like CLIP perform well on global alignment, they often struggle to capture fine-grained details in object attributes, spatial relations, and linguistic expressions, with limited support for bilingual comprehension. To address these challenges, we introduce FG-CLIP 2, a bilingual vision-language model designed to advance fine-grained alignment for both English and Chinese. Our approach leverages rich fine-grained supervision, including region-text matching and long-caption modeling, alongside multiple discriminative objectives. We further introduce the Textual Intra-modal Contrastive (TIC) loss to better distinguish semantically similar captions. Trained on a carefully curated mixture of large-scale English and Chinese data, FG-CLIP 2 achieves powerful bilingual performance. To enable rigorous evaluation, we present a new benchmark for Chinese multimodal understanding, featuring long-caption retrieval and bounding box classification. Extensive experiments on 29 datasets across 8 tasks show that FG-CLIP 2 outperforms existing methods, achieving state-of-the-art results in both languages. We release the model, code, and benchmark to facilitate future research on bilingual fine-grained alignment.",
            "score": 8,
            "issue_id": 6445,
            "pub_date": "2025-10-13",
            "pub_date_card": {
                "ru": "13 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 13",
                "zh": "10æœˆ13æ—¥"
            },
            "hash": "48bb15d507d38753",
            "authors": [
                "Chunyu Xie",
                "Bin Wang",
                "Fanjing Kong",
                "Jincheng Li",
                "Dawei Liang",
                "Ji Ao",
                "Dawei Leng",
                "Yuhui Yin"
            ],
            "affiliations": [
                "360.cn"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.10921.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#dataset",
                    "#benchmark",
                    "#multimodal",
                    "#alignment",
                    "#open_source"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ”ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°",
                    "desc": "FG-CLIP 2 â€” ÑÑ‚Ğ¾ Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ğ°Ñ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ³Ğ¾ Ğ¸ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ğ¹ fine-grained supervision, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ½Ğ¾Ğ²ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ TIC (Textual Intra-modal Contrastive) Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ CLIP, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼, FG-CLIP 2 Ğ»ÑƒÑ‡ÑˆĞµ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¸Ñ… Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ñ‹ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° 29 Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… Ğ² 8 Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ³Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Bilingual Vision-Language Alignment with FG-CLIP 2",
                    "desc": "FG-CLIP 2 is a bilingual vision-language model that improves the alignment between visual content and text descriptions in both English and Chinese. It addresses the limitations of existing models by using rich supervision techniques, such as region-text matching and long-caption modeling, to enhance fine-grained understanding. The model introduces a new loss function called Textual Intra-modal Contrastive (TIC) loss, which helps differentiate between similar captions more effectively. With extensive training on a diverse dataset and rigorous evaluation, FG-CLIP 2 achieves state-of-the-art performance across various tasks and datasets, making it a significant advancement in bilingual multimodal understanding."
                },
                "zh": {
                    "title": "åŒè¯­è§†è§‰-è¯­è¨€æ¨¡å‹çš„ç»†ç²’åº¦å¯¹é½æ–°çªç ´",
                    "desc": "FG-CLIP 2 æ˜¯ä¸€ç§åŒè¯­è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨æé«˜è‹±è¯­å’Œä¸­æ–‡ä¹‹é—´çš„ç»†ç²’åº¦å¯¹é½èƒ½åŠ›ã€‚è¯¥æ¨¡å‹é€šè¿‡ä¸°å¯Œçš„ç›‘ç£å­¦ä¹ å’Œæ–°çš„æ–‡æœ¬å†…å¯¹æ¯”æŸå¤±ï¼ˆTICæŸå¤±ï¼‰æ¥å®ç°è¿™ä¸€ç›®æ ‡ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰è§†è§‰å†…å®¹ä¸è¯­è¨€æè¿°ä¹‹é—´çš„ç»†å¾®å·®åˆ«ã€‚FG-CLIP 2 åœ¨å¤šä¸ªæ•°æ®é›†å’Œä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒè¯­ç†è§£æ–¹é¢ã€‚æˆ‘ä»¬è¿˜æ¨å‡ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œä»¥è¯„ä¼°ä¸­æ–‡çš„å¤šæ¨¡æ€ç†è§£èƒ½åŠ›ï¼Œæ¨åŠ¨æœªæ¥çš„ç ”ç©¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13778",
            "title": "InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for\n  Generalist Robot Policy",
            "url": "https://huggingface.co/papers/2510.13778",
            "abstract": "A unified framework for spatial grounding and robot control, InternVLA-M1, enhances instruction-following robots through spatially guided vision-language-action training, achieving significant improvements across various tasks and simulations.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce InternVLA-M1, a unified framework for spatial grounding and robot control that advances instruction-following robots toward scalable, general-purpose intelligence. Its core idea is spatially guided vision-language-action training, where spatial grounding serves as the critical link between instructions and robot actions. InternVLA-M1 employs a two-stage pipeline: (i) spatial grounding pre-training on over 2.3M spatial reasoning data to determine ``where to act'' by aligning instructions with visual, embodiment-agnostic positions, and (ii) spatially guided action post-training to decide ``how to act'' by generating embodiment-aware actions through plug-and-play spatial prompting. This spatially guided training recipe yields consistent gains: InternVLA-M1 outperforms its variant without spatial guidance by +14.6% on SimplerEnv Google Robot, +17% on WidowX, and +4.3% on LIBERO Franka, while demonstrating stronger spatial reasoning capability in box, point, and trace prediction. To further scale instruction following, we built a simulation engine to collect 244K generalizable pick-and-place episodes, enabling a 6.2% average improvement across 200 tasks and 3K+ objects. In real-world clustered pick-and-place, InternVLA-M1 improved by 7.3%, and with synthetic co-training, achieved +20.6% on unseen objects and novel configurations. Moreover, in long-horizon reasoning-intensive scenarios, it surpassed existing works by over 10%. These results highlight spatially guided training as a unifying principle for scalable and resilient generalist robots. Code and models are available at https://github.com/InternRobotics/InternVLA-M1.",
            "score": 7,
            "issue_id": 6444,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 15",
                "zh": "10æœˆ15æ—¥"
            },
            "hash": "9f1a9178757021cf",
            "authors": [
                "Xinyi Chen",
                "Yilun Chen",
                "Yanwei Fu",
                "Ning Gao",
                "Jiaya Jia",
                "Weiyang Jin",
                "Hao Li",
                "Yao Mu",
                "Jiangmiao Pang",
                "Yu Qiao",
                "Yang Tian",
                "Bin Wang",
                "Bolun Wang",
                "Fangjing Wang",
                "Hanqing Wang",
                "Tai Wang",
                "Ziqin Wang",
                "Xueyuan Wei",
                "Chao Wu",
                "Shuai Yang",
                "Jinhui Ye",
                "Junqiu Yu",
                "Jia Zeng",
                "Jingjing Zhang",
                "Jinyu Zhang",
                "Shi Zhang",
                "Feng Zheng",
                "Bowen Zhou",
                "Yangkun Zhu"
            ],
            "affiliations": [
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13778.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#agents",
                    "#agi",
                    "#optimization",
                    "#robotics"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ Ğ¾Ğ±Ğ¾Ñ‚Ñ‹ ÑƒÑ‡Ğ°Ñ‚ÑÑ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°",
                    "desc": "InternVLA-M1 â€” ÑÑ‚Ğ¾ ĞµĞ´Ğ¸Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑÑ†ĞµĞ½Ñ‹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ´Ğ²Ğ° ÑÑ‚Ğ°Ğ¿Ğ°: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ Â«Ğ³Ğ´Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒÂ» Ğ½Ğ° 2.3 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ·Ğ°Ñ‚ĞµĞ¼ ÑƒÑ‡Ğ¸Ñ‚ÑÑ Â«ĞºĞ°Ğº Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒÂ» Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ 4% Ğ´Ğ¾ 20% Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸ÑÑ… Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ½Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ â€” Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ spatial grounding ĞºĞ°Ğº Ğ¼Ğ¾ÑÑ‚Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ°Ğ¼Ğ¸ Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°."
                },
                "en": {
                    "title": "Empowering Robots with Spatially Guided Intelligence",
                    "desc": "The paper presents InternVLA-M1, a framework that enhances robots' ability to follow instructions by integrating spatial grounding with vision-language-action training. This approach involves a two-stage process: first, pre-training the model on a large dataset to understand where to act based on spatial reasoning, and second, fine-tuning it to determine how to act using spatial prompts. The results show significant performance improvements in various robotic tasks, demonstrating the effectiveness of spatial guidance in robot control. Overall, this work emphasizes the importance of spatially informed training for developing versatile and intelligent robots."
                },
                "zh": {
                    "title": "ç©ºé—´å¼•å¯¼è®­ç»ƒï¼šæå‡æœºå™¨äººæ™ºèƒ½çš„å…³é”®",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºInternVLA-M1çš„ç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨æå‡éµå¾ªæŒ‡ä»¤çš„æœºå™¨äººæ™ºèƒ½ã€‚è¯¥æ¡†æ¶é€šè¿‡ç©ºé—´å¼•å¯¼çš„è§†è§‰-è¯­è¨€-è¡ŒåŠ¨è®­ç»ƒï¼Œå»ºç«‹äº†æŒ‡ä»¤ä¸æœºå™¨äººåŠ¨ä½œä¹‹é—´çš„å…³é”®è”ç³»ã€‚InternVLA-M1é‡‡ç”¨ä¸¤é˜¶æ®µæµç¨‹ï¼šé¦–å…ˆè¿›è¡Œç©ºé—´å¼•å¯¼çš„é¢„è®­ç»ƒï¼Œä»¥ç¡®å®šâ€œåœ¨å“ªé‡Œè¡ŒåŠ¨â€ï¼›ç„¶åè¿›è¡Œç©ºé—´å¼•å¯¼çš„åè®­ç»ƒï¼Œä»¥ç”Ÿæˆâ€œå¦‚ä½•è¡ŒåŠ¨â€çš„å…·ä½“åŠ¨ä½œã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªä»»åŠ¡å’Œæ¨¡æ‹Ÿä¸­æ˜¾è‘—æé«˜äº†æœºå™¨äººçš„è¡¨ç°ï¼Œå±•ç¤ºäº†ç©ºé—´å¼•å¯¼è®­ç»ƒåœ¨å¯æ‰©å±•å’Œé€šç”¨æœºå™¨äººä¸­çš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.10274",
            "title": "X-VLA: Soft-Prompted Transformer as Scalable Cross-Embodiment\n  Vision-Language-Action Model",
            "url": "https://huggingface.co/papers/2510.10274",
            "abstract": "A novel Soft Prompt approach enhances Vision-Language-Action models by using learnable embeddings for diverse robotic data, enabling superior performance across simulations and real-world robots.  \t\t\t\t\tAI-generated summary \t\t\t\t Successful generalist Vision-Language-Action (VLA) models rely on effective training across diverse robotic platforms with large-scale, cross-embodiment, heterogeneous datasets. To facilitate and leverage the heterogeneity in rich, diverse robotic data sources, we propose a novel Soft Prompt approach with minimally added parameters, by infusing prompt learning concepts into cross-embodiment robot learning and introducing separate sets of learnable embeddings for each distinct data source. These embeddings serve as embodiment-specific prompts, which in unity empower VLA models with effective exploitation of varying cross-embodiment features. Our new X-VLA, a neat flow-matching-based VLA architecture, relies exclusively on soft-prompted standard Transformer encoders, enjoying both scalability and simplicity. Evaluated across 6 simulations as well as 3 real-world robots, our 0.9B instantiation-X-VLA-0.9B simultaneously achieves SOTA performance over a sweep of benchmarks, demonstrating superior results on a wide axes of capabilities, from flexible dexterity to quick adaptation across embodiments, environments, and tasks. Website: https://thu-air-dream.github.io/X-VLA/",
            "score": 5,
            "issue_id": 6446,
            "pub_date": "2025-10-11",
            "pub_date_card": {
                "ru": "11 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 11",
                "zh": "10æœˆ11æ—¥"
            },
            "hash": "5a799bed54a78313",
            "authors": [
                "Jinliang Zheng",
                "Jianxiong Li",
                "Zhihao Wang",
                "Dongxiu Liu",
                "Xirui Kang",
                "Yuchun Feng",
                "Yinan Zheng",
                "Jiayin Zou",
                "Yilun Chen",
                "Jia Zeng",
                "Ya-Qin Zhang",
                "Jiangmiao Pang",
                "Jingjing Liu",
                "Tai Wang",
                "Xianyuan Zhan"
            ],
            "affiliations": [
                "Institute for AI Industry Research (AIR), Tsinghua University",
                "Peking University",
                "Shanghai AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.10274.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#agi",
                    "#agents",
                    "#training",
                    "#3d",
                    "#optimization"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞœÑĞ³ĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Soft Prompt Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Vision-Language-Action Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° X-VLA Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° flow-matching Ğ¸ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Transformer ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ°Ñ… Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 0.9B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² 6 ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸ÑÑ… Ğ¸ Ğ½Ğ° 3 Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ñ…, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒ Ğ¸ Ğ±Ñ‹ÑÑ‚Ñ€ÑƒÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼."
                },
                "en": {
                    "title": "Enhancing VLA Models with Soft Prompts for Diverse Robotics",
                    "desc": "This paper introduces a new Soft Prompt method that improves Vision-Language-Action (VLA) models by using learnable embeddings tailored for different robotic data sources. By incorporating prompt learning into cross-embodiment robot learning, the approach allows for better utilization of diverse datasets with minimal additional parameters. The proposed X-VLA architecture employs soft-prompted Transformer encoders, which enhances scalability and simplicity. The results show that X-VLA achieves state-of-the-art performance across various simulations and real-world robots, demonstrating its effectiveness in adapting to different tasks and environments."
                },
                "zh": {
                    "title": "è½¯æç¤ºåŠ©åŠ›è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„çªç ´",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è½¯æç¤ºæ–¹æ³•ï¼Œæ—¨åœ¨å¢å¼ºè§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹çš„æ€§èƒ½ã€‚é€šè¿‡ä¸ºä¸åŒçš„æœºå™¨äººæ•°æ®æºå¼•å…¥å¯å­¦ä¹ çš„åµŒå…¥ï¼Œæ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨å¤šæ ·åŒ–çš„æœºå™¨äººæ•°æ®ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªä»¿çœŸå’ŒçœŸå®æœºå™¨äººä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œæ˜¾ç¤ºå‡ºåœ¨çµæ´»æ€§å’Œé€‚åº”æ€§æ–¹é¢çš„å“è¶Šè¡¨ç°ã€‚æˆ‘ä»¬çš„X-VLAæ¶æ„é€šè¿‡è½¯æç¤ºçš„æ ‡å‡†Transformerç¼–ç å™¨å®ç°äº†é«˜æ•ˆçš„æ€§èƒ½å’Œç®€å•çš„è®¾è®¡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.11958",
            "title": "Direct Multi-Token Decoding",
            "url": "https://huggingface.co/papers/2510.11958",
            "abstract": "Direct Multi-Token Decoding (DMTD) accelerates large language model inference by using only late layers for token generation, achieving significant speedup with minimal performance loss.  \t\t\t\t\tAI-generated summary \t\t\t\t Decoder-only transformers have become the standard architecture for large language models (LLMs) due to their strong performance. Recent studies suggest that, in pre-trained LLMs, early, middle, and late layers may serve distinct roles: Early layers focus on understanding the input context, middle layers handle task-specific processing, and late layers convert abstract representations into output tokens. We hypothesize that once representations have been processed by the early and middle layers, the resulting hidden states may encapsulate sufficient information to support the generation of multiple tokens using only the late layers, eliminating the need to repeatedly traverse the early and middle layers. We refer to this inference paradigm as Direct Multi-Token Decoding (DMTD). Unlike speculative decoding, our method introduces no additional parameters, auxiliary routines, or post-generation verification. Despite being trained on a limited dataset, a fine-tuned DMTD Qwen3-4B model has already demonstrated promising results, achieving up to a 2x speedup with only minor performance loss. Moreover, as shown in our scaling analysis, its performance is expected to further improve with larger training datasets.",
            "score": 4,
            "issue_id": 6444,
            "pub_date": "2025-10-13",
            "pub_date_card": {
                "ru": "13 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 13",
                "zh": "10æœˆ13æ—¥"
            },
            "hash": "a9246dd44d38f230",
            "authors": [
                "Xuan Luo",
                "Weizhi Wang",
                "Xifeng Yan"
            ],
            "affiliations": [
                "Department of Computer Science, UC Santa Barbara"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.11958.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#inference",
                    "#optimization"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾Ğ·Ğ´Ğ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Direct Multi-Token Decoding (DMTD), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ inference Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·Ğµ, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°Ğ½Ğ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ÑÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚, ÑÑ€ĞµĞ´Ğ½Ğ¸Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ, Ğ° Ğ¿Ğ¾Ğ·Ğ´Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒÑÑ‚ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ñ‹. ĞŸĞ¾ÑĞ»Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ€Ğ°Ğ½Ğ½Ğ¸Ğ¼Ğ¸ Ğ¸ ÑÑ€ĞµĞ´Ğ½Ğ¸Ğ¼Ğ¸ ÑĞ»Ğ¾ÑĞ¼Ğ¸ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ¾Ğ·Ğ´Ğ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ¸, Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ²ÑÑ ÑĞµÑ‚ÑŒ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» Ğ´Ğ²ÑƒĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen3-4B Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€ĞµĞ¹ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°."
                },
                "en": {
                    "title": "Speed Up Language Generation with DMTD!",
                    "desc": "Direct Multi-Token Decoding (DMTD) is a new method that speeds up the process of generating text with large language models by only using the late layers of the model for token generation. This approach takes advantage of the fact that early and middle layers have already processed the input, allowing the late layers to efficiently produce multiple tokens without reprocessing. DMTD does not require any extra parameters or complex routines, making it a straightforward enhancement to existing models. Initial results show that a fine-tuned DMTD model can achieve up to a 2x increase in speed with minimal impact on performance, and its effectiveness is expected to grow with larger datasets."
                },
                "zh": {
                    "title": "ç›´æ¥å¤šæ ‡è®°è§£ç ï¼šåŠ é€Ÿæ¨ç†çš„æ–°æ–¹æ³•",
                    "desc": "ç›´æ¥å¤šæ ‡è®°è§£ç ï¼ˆDMTDï¼‰é€šè¿‡ä»…ä½¿ç”¨åå±‚è¿›è¡Œæ ‡è®°ç”Ÿæˆï¼ŒåŠ é€Ÿäº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†ï¼Œæ˜¾è‘—æé«˜äº†é€Ÿåº¦ä¸”æ€§èƒ½æŸå¤±æå°ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æ—©æœŸã€ä¸­æœŸå’ŒåæœŸå±‚å„è‡ªæ‰¿æ‹…ä¸åŒçš„è§’è‰²ã€‚æˆ‘ä»¬çš„å‡è®¾æ˜¯ï¼Œä¸€æ—¦æ—©æœŸå’Œä¸­æœŸå±‚å¤„ç†å®Œè¾“å…¥ï¼Œç”Ÿæˆçš„éšè—çŠ¶æ€å°±è¶³ä»¥æ”¯æŒä»…ä½¿ç”¨åæœŸå±‚ç”Ÿæˆå¤šä¸ªæ ‡è®°ï¼Œä»è€Œé¿å…é‡å¤éå†æ—©æœŸå’Œä¸­æœŸå±‚ã€‚DMTDæ–¹æ³•åœ¨ä¸å¢åŠ é¢å¤–å‚æ•°æˆ–è¾…åŠ©ç¨‹åºçš„æƒ…å†µä¸‹ï¼Œå·²åœ¨æœ‰é™æ•°æ®é›†ä¸Šå±•ç¤ºå‡ºè‰¯å¥½çš„æ•ˆæœï¼Œé€Ÿåº¦æå‡å¯è¾¾2å€ï¼Œä¸”æ€§èƒ½æŸå¤±å¾ˆå°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13602",
            "title": "NOSA: Native and Offloadable Sparse Attention",
            "url": "https://huggingface.co/papers/2510.13602",
            "abstract": "NOSA, a trainable sparse attention framework, enhances decoding throughput by enabling efficient KV cache offloading without compromising performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Trainable sparse attention has emerged as a promising solution to address the decoding efficiency bottleneck of LLMs in long-context processing, significantly saving memory accesses while minimally impacting task performance. However, existing sparse attention methods leave a crucial limitation unresolved: the size of the key-value (KV) cache remains unreduced, which constrains on-GPU batch sizes and throttles decoding throughput, especially in large-scale batched inference. In this paper, we show that trainable sparse attention naturally exhibits strong locality in token selection across adjacent decoding steps, thereby enabling KV cache offloading without altering the underlying attention computation. However, the inherent locality remains insufficient to achieve efficient offloading, as the transfer of selected KV pairs between the CPU and GPU continues to dominate the overall decoding cost. Building on this insight, we present NOSA, a trainable sparse attention framework designed to natively support KV cache offloading. NOSA introduces explicit locality constraints by decomposing token selection into query-aware and query-agnostic components, thereby reducing KV transfers while preserving the same attention computation as used during training. We pretrain a 1B-parameter model with NOSA and conduct extensive benchmarks, showing that it preserves near-lossless performance while achieving up to a 2.3x improvement in decoding throughput compared with the vanilla trainable sparse attention baseline (InfLLM-V2).",
            "score": 3,
            "issue_id": 6445,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 15",
                "zh": "10æœˆ15æ—¥"
            },
            "hash": "2b3732fd1262cb04",
            "authors": [
                "Yuxiang Huang",
                "Chaojun Xiao",
                "Xu Han",
                "Zhiyuan Liu"
            ],
            "affiliations": [
                "Department of Computer Science and Technology, Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13602.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#inference",
                    "#long_context",
                    "#benchmark",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ LLM Ñ‡ĞµÑ€ĞµĞ· Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ",
                    "desc": "NOSA â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ (sparse attention), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¾ÑĞµĞ´Ğ½Ğ¸Ğ¼Ğ¸ ÑˆĞ°Ğ³Ğ°Ğ¼Ğ¸ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ñ‹Ğ³Ñ€ÑƒĞ¶Ğ°Ñ‚ÑŒ KV-ĞºÑÑˆ Ñ GPU Ğ½Ğ° CPU. NOSA Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ²Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹, Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‰Ğ¸Ğµ Ğ¸ Ğ½Ğµ Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‰Ğ¸Ğµ Ğ¾Ñ‚ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°, Ñ‡Ñ‚Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµĞ¶Ğ´Ñƒ CPU Ğ¸ GPU. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¾ 2.3 Ñ€Ğ°Ğ·Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "NOSA: Boosting Decoding Efficiency with Sparse Attention",
                    "desc": "NOSA is a novel framework that improves the efficiency of decoding in large language models (LLMs) by enabling effective offloading of key-value (KV) caches. It leverages trainable sparse attention to minimize memory access while maintaining high performance during long-context processing. The framework introduces locality constraints in token selection, allowing for reduced KV transfers between CPU and GPU, which is crucial for enhancing throughput. Experimental results demonstrate that NOSA achieves significant improvements in decoding speed without sacrificing the quality of the model's outputs."
                },
                "zh": {
                    "title": "NOSAï¼šé«˜æ•ˆè§£ç çš„æ–°æ–¹æ³•",
                    "desc": "NOSAæ˜¯ä¸€ç§å¯è®­ç»ƒçš„ç¨€ç–æ³¨æ„åŠ›æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜è§£ç æ•ˆç‡ï¼Œå…è®¸é«˜æ•ˆçš„é”®å€¼ï¼ˆKVï¼‰ç¼“å­˜å¸è½½è€Œä¸å½±å“æ€§èƒ½ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨ç›¸é‚»è§£ç æ­¥éª¤ä¸­é€‰æ‹©å…·æœ‰å¼ºå±€éƒ¨æ€§çš„tokenï¼Œæ¥å®ç°KVç¼“å­˜çš„å¸è½½ï¼Œä»è€Œå‡å°‘å†…å­˜è®¿é—®ã€‚å°½ç®¡ç°æœ‰çš„ç¨€ç–æ³¨æ„åŠ›æ–¹æ³•æœªèƒ½æœ‰æ•ˆå‡å°‘KVç¼“å­˜çš„å¤§å°ï¼ŒNOSAé€šè¿‡å¼•å…¥æ˜¾å¼çš„å±€éƒ¨æ€§çº¦æŸï¼Œä¼˜åŒ–äº†tokené€‰æ‹©è¿‡ç¨‹ï¼Œé™ä½äº†KVä¼ è¾“çš„æˆæœ¬ã€‚ç»è¿‡é¢„è®­ç»ƒï¼ŒNOSAåœ¨è§£ç ååé‡ä¸Šå®ç°äº†é«˜è¾¾2.3å€çš„æå‡ï¼ŒåŒæ—¶ä¿æŒäº†æ¥è¿‘æ— æŸçš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.12560",
            "title": "CoIRL-AD: Collaborative-Competitive Imitation-Reinforcement Learning in\n  Latent World Models for Autonomous Driving",
            "url": "https://huggingface.co/papers/2510.12560",
            "abstract": "End-to-end autonomous driving models trained solely with imitation learning (IL) often suffer from poor generalization. In contrast, reinforcement learning (RL) promotes exploration through reward maximization but faces challenges such as sample inefficiency and unstable convergence. A natural solution is to combine IL and RL. Moving beyond the conventional two-stage paradigm (IL pretraining followed by RL fine-tuning), we propose CoIRL-AD, a competitive dual-policy framework that enables IL and RL agents to interact during training. CoIRL-AD introduces a competition-based mechanism that facilitates knowledge exchange while preventing gradient conflicts. Experiments on the nuScenes dataset show an 18% reduction in collision rate compared to baselines, along with stronger generalization and improved performance on long-tail scenarios. Code is available at: https://github.com/SEU-zxj/CoIRL-AD.",
            "score": 3,
            "issue_id": 6445,
            "pub_date": "2025-10-14",
            "pub_date_card": {
                "ru": "14 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 14",
                "zh": "10æœˆ14æ—¥"
            },
            "hash": "7b32811bacd13120",
            "authors": [
                "Xiaoji Zheng",
                "Ziyuan Yang",
                "Yanhao Chen",
                "Yuhang Peng",
                "Yuanrong Tang",
                "Gengyuan Liu",
                "Bokui Chen",
                "Jiangtao Gong"
            ],
            "affiliations": [
                "Beijing Jiaotong University",
                "The Hong Kong Polytechnic University",
                "Tsinghua University",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.12560.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#agents",
                    "#games",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸï¸",
                "ru": {
                    "title": "ĞšĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ²ÑƒÑ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ¿Ğ¸Ğ»Ğ¾Ñ‚Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ¼Ñƒ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ CoIRL-AD, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ imitation learning Ğ¸ reinforcement learning Ğ² ĞµĞ´Ğ¸Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ñ Ğ´Ğ²ÑƒĞ¼Ñ ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°, Ğ³Ğ´Ğµ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ IL, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ RL fine-tuning, Ğ·Ğ´ĞµÑÑŒ Ğ¾Ğ±Ğ° Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒÑÑ‚ Ğ¸ Ğ¾Ğ±Ğ¼ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ†Ğ¸Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ° Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ ÑĞ»Ğ°Ğ±Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ IL-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ RL. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ nuScenes Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑÑ‚Ğ¾Ğ»ĞºĞ½Ğ¾Ğ²ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° 18% Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ğ² Ñ€ĞµĞ´ĞºĞ¸Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Revolutionizing Autonomous Driving with CoIRL-AD: A Dual-Policy Approach",
                    "desc": "This paper presents CoIRL-AD, a novel framework that integrates imitation learning (IL) and reinforcement learning (RL) for training autonomous driving models. Unlike traditional methods that use IL for pretraining followed by RL fine-tuning, CoIRL-AD allows IL and RL agents to interact during the training process. This dual-policy approach promotes knowledge sharing and mitigates issues like gradient conflicts, leading to better performance. The results demonstrate a significant reduction in collision rates and improved generalization on challenging driving scenarios, showcasing the effectiveness of this combined training strategy."
                },
                "zh": {
                    "title": "ç»“åˆæ¨¡ä»¿å­¦ä¹ ä¸å¼ºåŒ–å­¦ä¹ çš„è‡ªåŠ¨é©¾é©¶æ–°æ–¹æ³•",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºCoIRL-ADçš„åŒç­–ç•¥æ¡†æ¶ï¼Œæ—¨åœ¨ç»“åˆæ¨¡ä»¿å­¦ä¹ ï¼ˆILï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä»¥æé«˜è‡ªåŠ¨é©¾é©¶æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ä¼ ç»Ÿçš„ä¸¤é˜¶æ®µæ–¹æ³•ä¸åŒï¼ŒCoIRL-ADå…è®¸ILå’ŒRLä»£ç†åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç›¸äº’ä½œç”¨ï¼Œä»è€Œä¿ƒè¿›çŸ¥è¯†çš„äº¤æµã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸€ç§åŸºäºç«äº‰çš„æœºåˆ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆé˜²æ­¢æ¢¯åº¦å†²çªã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸åŸºçº¿ç›¸æ¯”ï¼ŒCoIRL-ADåœ¨nuScenesæ•°æ®é›†ä¸Šå‡å°‘äº†18%çš„ç¢°æ’ç‡ï¼Œå¹¶åœ¨é•¿å°¾åœºæ™¯ä¸­è¡¨ç°å‡ºæ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›å’Œæ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13744",
            "title": "Hard2Verify: A Step-Level Verification Benchmark for Open-Ended Frontier\n  Math",
            "url": "https://huggingface.co/papers/2510.13744",
            "abstract": "Hard2Verify, a human-annotated benchmark, evaluates step-level verifiers for LLM-based mathematical reasoning systems, highlighting the challenges and performance gaps between open-source and closed-source models.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language model (LLM)-based reasoning systems have recently achieved gold medal-level performance in the IMO 2025 competition, writing mathematical proofs where, to receive full credit, each step must be not only correct but also sufficiently supported. To train LLM-based reasoners in such challenging, open-ended settings, strong verifiers capable of catching step-level mistakes are necessary prerequisites. We introduce Hard2Verify, a human-annotated, step-level verification benchmark produced with over 500 hours of human labor. Hard2Verify is designed to rigorously assess step-level verifiers at the frontier: Verifiers must provide step-level annotations or identify the first error in responses generated by frontier LLMs for very recent, challenging, and open-ended math questions. We evaluate 29 generative critics and process reward models, demonstrating that, beyond a few standouts, open-source verifiers lag closed source models. We subsequently analyze what drives poor performance in step-level verification, the impacts of scaling verifier compute, as well as fundamental questions such as self-verification and verification-generation dynamics.",
            "score": 2,
            "issue_id": 6444,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 15",
                "zh": "10æœˆ15æ—¥"
            },
            "hash": "ac775fe89e3a3d8b",
            "authors": [
                "Shrey Pandit",
                "Austin Xu",
                "Xuan-Phi Nguyen",
                "Yifei Ming",
                "Caiming Xiong",
                "Shafiq Joty"
            ],
            "affiliations": [
                "Salesforce AI Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13744.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#math",
                    "#benchmark",
                    "#interpretability"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ AI Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Hard2Verify â€” Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ LLM, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ 500 Ñ‡Ğ°ÑĞ¾Ğ² Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸. Ğ’ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ñ‹ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸ÑÑ… ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 29 Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ñ‚ÑÑ‚Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ open-source Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¾Ñ‚ closed-source ÑĞ¸ÑÑ‚ĞµĞ¼. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ñ‹ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸, Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ¸ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ ÑĞ°Ğ¼Ğ¾Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ AI-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Hard2Verify: Bridging the Gap in LLM Verification",
                    "desc": "The paper introduces Hard2Verify, a benchmark designed to evaluate step-level verifiers for large language model (LLM)-based mathematical reasoning systems. It highlights the importance of strong verifiers that can accurately identify mistakes in mathematical proofs, which are crucial for achieving high performance in competitions like IMO 2025. The benchmark was created through extensive human annotation, involving over 500 hours of labor, to rigorously assess the capabilities of various verification models. The study reveals significant performance gaps between open-source and closed-source verifiers, while also exploring factors that contribute to these discrepancies and the dynamics of verification processes."
                },
                "zh": {
                    "title": "Hard2Verifyï¼šè¯„ä¼°æ•°å­¦æ¨ç†çš„é€æ­¥éªŒè¯å™¨",
                    "desc": "Hard2Verifyæ˜¯ä¸€ä¸ªäººç±»æ ‡æ³¨çš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ•°å­¦æ¨ç†ç³»ç»Ÿçš„é€æ­¥éªŒè¯å™¨ã€‚è¯¥åŸºå‡†å¼ºè°ƒäº†å¼€æºæ¨¡å‹å’Œé—­æºæ¨¡å‹ä¹‹é—´çš„æŒ‘æˆ˜å’Œæ€§èƒ½å·®è·ã€‚ä¸ºäº†åœ¨å¤æ‚çš„å¼€æ”¾å¼ç¯å¢ƒä¸­è®­ç»ƒLLMæ¨ç†å™¨ï¼Œå¼ºå¤§çš„éªŒè¯å™¨æ˜¯å¿…ä¸å¯å°‘çš„ï¼Œå®ƒä»¬èƒ½å¤Ÿæ•æ‰é€æ­¥é”™è¯¯ã€‚æˆ‘ä»¬è¯„ä¼°äº†29ç§ç”Ÿæˆæ€§æ‰¹è¯„è€…å’Œè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼Œç»“æœæ˜¾ç¤ºï¼Œé™¤äº†å°‘æ•°ä¼˜ç§€çš„æ¨¡å‹å¤–ï¼Œå¼€æºéªŒè¯å™¨çš„è¡¨ç°æ™®éè½åäºé—­æºæ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13282",
            "title": "Universal Image Restoration Pre-training via Masked Degradation\n  Classification",
            "url": "https://huggingface.co/papers/2510.13282",
            "abstract": "A Masked Degradation Classification Pre-Training method enhances image restoration by using degradation type classification and image reconstruction, improving performance across CNNs and Transformers.  \t\t\t\t\tAI-generated summary \t\t\t\t This study introduces a Masked Degradation Classification Pre-Training method (MaskDCPT), designed to facilitate the classification of degradation types in input images, leading to comprehensive image restoration pre-training. Unlike conventional pre-training methods, MaskDCPT uses the degradation type of the image as an extremely weak supervision, while simultaneously leveraging the image reconstruction to enhance performance and robustness. MaskDCPT includes an encoder and two decoders: the encoder extracts features from the masked low-quality input image. The classification decoder uses these features to identify the degradation type, whereas the reconstruction decoder aims to reconstruct a corresponding high-quality image. This design allows the pre-training to benefit from both masked image modeling and contrastive learning, resulting in a generalized representation suited for restoration tasks. Benefit from the straightforward yet potent MaskDCPT, the pre-trained encoder can be used to address universal image restoration and achieve outstanding performance. Implementing MaskDCPT significantly improves performance for both convolution neural networks (CNNs) and Transformers, with a minimum increase in PSNR of 3.77 dB in the 5D all-in-one restoration task and a 34.8% reduction in PIQE compared to baseline in real-world degradation scenarios. It also emergences strong generalization to previously unseen degradation types and levels. In addition, we curate and release the UIR-2.5M dataset, which includes 2.5 million paired restoration samples across 19 degradation types and over 200 degradation levels, incorporating both synthetic and real-world data. The dataset, source code, and models are available at https://github.com/MILab-PKU/MaskDCPT.",
            "score": 2,
            "issue_id": 6447,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 15",
                "zh": "10æœˆ15æ—¥"
            },
            "hash": "692908ce4f9b0d08",
            "authors": [
                "JiaKui Hu",
                "Zhengjian Yao",
                "Lujia Jin",
                "Yinghao Chen",
                "Yanye Lu"
            ],
            "affiliations": [
                "Biomedical Engineering Department, College of Future Technology, Peking University, Beijing, China",
                "College of Electronic Engineering, National University of Defense Technology, Changsha, China",
                "Institute of Medical Technology, Peking University Health Science Center, Peking University, Beijing, China",
                "JIUTIAN Research, Beijing, China",
                "National Biomedical Imaging Center, Peking University, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13282.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#data",
                    "#dataset",
                    "#synthetic",
                    "#optimization",
                    "#cv"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ MaskDCPT Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ğº ÑĞ»Ğ°Ğ±Ñ‹Ğ¹ Ğ½Ğ°Ğ´Ğ·Ğ¾Ñ€ Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸Ğ· Ğ·Ğ°Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ğ´Ğ²Ğ° Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ°: Ğ¾Ğ´Ğ¸Ğ½ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ğ¸Ğ¿ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸, Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¹ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° masked image modeling Ğ¸ contrastive learning, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ restoration. MaskDCPT Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ CNN Ğ¸ Transformer Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ (Ğ¼Ğ¸Ğ½Ğ¸Ğ¼ÑƒĞ¼ +3.77 dB PSNR) Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ UIR-2.5M Ñ 2.5 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ 19 Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Enhancing Image Restoration with Masked Degradation Classification",
                    "desc": "The paper presents a novel Masked Degradation Classification Pre-Training method (MaskDCPT) that enhances image restoration by classifying degradation types and reconstructing images. This approach utilizes weak supervision from degradation classification while simultaneously improving image quality through reconstruction. The architecture consists of an encoder for feature extraction and two decoders for classification and reconstruction tasks, allowing for effective pre-training. Results show significant performance improvements in both CNNs and Transformers, with better generalization to new degradation types and a large dataset of 2.5 million samples for training and evaluation."
                },
                "zh": {
                    "title": "Masked Degradation Classificationï¼šæå‡å›¾åƒæ¢å¤çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºMasked Degradation Classification Pre-Trainingï¼ˆMaskDCPTï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡å¯¹è¾“å…¥å›¾åƒçš„é™è´¨ç±»å‹è¿›è¡Œåˆ†ç±»æ¥å¢å¼ºå›¾åƒæ¢å¤çš„æ•ˆæœã€‚ä¸ä¼ ç»Ÿçš„é¢„è®­ç»ƒæ–¹æ³•ä¸åŒï¼ŒMaskDCPTåˆ©ç”¨å›¾åƒçš„é™è´¨ç±»å‹ä½œä¸ºæå¼±çš„ç›‘ç£ä¿¡å·ï¼ŒåŒæ—¶ç»“åˆå›¾åƒé‡å»ºæ¥æé«˜æ€§èƒ½å’Œé²æ£’æ€§ã€‚è¯¥æ–¹æ³•åŒ…æ‹¬ä¸€ä¸ªç¼–ç å™¨å’Œä¸¤ä¸ªè§£ç å™¨ï¼Œç¼–ç å™¨ä»ä½è´¨é‡çš„è¾“å…¥å›¾åƒä¸­æå–ç‰¹å¾ï¼Œåˆ†ç±»è§£ç å™¨ç”¨äºè¯†åˆ«é™è´¨ç±»å‹ï¼Œè€Œé‡å»ºè§£ç å™¨åˆ™æ—¨åœ¨é‡å»ºç›¸åº”çš„é«˜è´¨é‡å›¾åƒã€‚é€šè¿‡è¿™ç§è®¾è®¡ï¼ŒMaskDCPTèƒ½å¤Ÿæœ‰æ•ˆåœ°è¿›è¡Œå›¾åƒæ¢å¤ä»»åŠ¡ï¼Œå¹¶åœ¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰å’Œå˜æ¢å™¨ï¼ˆTransformersï¼‰ä¸Šæ˜¾è‘—æå‡æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.11062",
            "title": "Stronger Together: On-Policy Reinforcement Learning for Collaborative\n  LLMs",
            "url": "https://huggingface.co/papers/2510.11062",
            "abstract": "AT-GRPO, a tailored RL algorithm for multi-agent systems, significantly enhances performance across various tasks by addressing unique challenges in on-policy RL.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-agent systems (MAS) and reinforcement learning (RL) are widely used to enhance the agentic capabilities of large language models (LLMs). MAS improves task performance through role-based orchestration, while RL uses environmental rewards to learn stronger policies, such as GRPO-style optimization. However, applying on-policy RL to MAS remains underexplored and presents unique challenges. Algorithmically, standard GRPO grouping assumptions break down because prompts vary by role and by turn. System-wise, the training stack must support MAS-workflow rollouts and on-policy updates for both single-policy and multi-policy models.   We propose AT-GRPO, which includes (i) an agent- and turn-wise grouped RL algorithm tailored to MAS and (ii) a training system that supports both single- and multi-policy regimes. Across game, planning, coding, and math tasks, AT-GRPO delivers substantial gains. On long-horizon planning, it increases accuracy from a 14.0 to 47.0 percent single-agent RL baseline to 96.0 to 99.5 percent. It also improves reasoning performance, with average gains of 3.87 to 7.62 percent on coding tasks and 9.0 to 17.93 percent on math. Code and environments are available at: https://github.com/pettingllms-ai/PettingLLMs.",
            "score": 2,
            "issue_id": 6448,
            "pub_date": "2025-10-13",
            "pub_date_card": {
                "ru": "13 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 13",
                "zh": "10æœˆ13æ—¥"
            },
            "hash": "54cb332759f2b6bc",
            "authors": [
                "Yujie Zhao",
                "Lanxiang Hu",
                "Yang Wang",
                "Minmin Hou",
                "Hao Zhang",
                "Ke Ding",
                "Jishen Zhao"
            ],
            "affiliations": [
                "Intel Corporation",
                "University of California, San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.11062.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#rl",
                    "#optimization",
                    "#reasoning",
                    "#games",
                    "#training"
                ],
                "emoji": "ğŸ¤",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ AT-GRPO â€” Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ LLM. Ğ¢Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ on-policy RL Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº GRPO, Ğ¿Ğ»Ğ¾Ñ…Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…, Ğ³Ğ´Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°ÑÑ‚ÑÑ Ğ¿Ğ¾ Ñ€Ğ¾Ğ»ÑĞ¼ Ğ¸ Ñ…Ğ¾Ğ´Ğ°Ğ¼ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². AT-GRPO Ñ€ĞµÑˆĞ°ĞµÑ‚ ÑÑ‚Ñƒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ‡ĞµÑ€ĞµĞ· Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€Ğ¾Ğ²ĞºÑƒ Ğ¿Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ¸ Ñ…Ğ¾Ğ´Ğ°Ğ¼, Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°Ñ ĞºĞ°Ğº Ğ¾Ğ´Ğ½Ğ¾-, Ñ‚Ğ°Ğº Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸-policy Ñ€ĞµĞ¶Ğ¸Ğ¼Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹: Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹Ñ€Ğ¾ÑĞ»Ğ° Ñ 14-47% Ğ´Ğ¾ 96-99.5%, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞµ (9-18%) Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ (4-8%)."
                },
                "en": {
                    "title": "Boosting Multi-Agent Performance with AT-GRPO",
                    "desc": "The paper introduces AT-GRPO, a specialized reinforcement learning (RL) algorithm designed for multi-agent systems (MAS). It addresses the challenges of applying on-policy RL in MAS by implementing a grouped RL approach that considers agent roles and interaction turns. The proposed algorithm significantly improves task performance, achieving remarkable accuracy in long-horizon planning and enhancing reasoning capabilities in coding and math tasks. Overall, AT-GRPO demonstrates substantial performance gains across various applications, showcasing its effectiveness in optimizing multi-agent workflows."
                },
                "zh": {
                    "title": "AT-GRPOï¼šå¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–°çªç ´",
                    "desc": "AT-GRPOæ˜¯ä¸€ç§ä¸“ä¸ºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿè®¾è®¡çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œæ—¨åœ¨è§£å†³åœ¨ç­–ç•¥å­¦ä¹ ä¸­é¢ä¸´çš„ç‹¬ç‰¹æŒ‘æˆ˜ã€‚è¯¥ç®—æ³•é€šè¿‡è§’è‰²å’Œå›åˆçš„åˆ†ç»„æ–¹å¼ï¼Œä¼˜åŒ–äº†å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„ç­–ç•¥å­¦ä¹ è¿‡ç¨‹ã€‚AT-GRPOåœ¨å¤šä¸ªä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä¾‹å¦‚åœ¨é•¿æ—¶é—´è§„åˆ’ä»»åŠ¡ä¸­ï¼Œå‡†ç¡®ç‡ä»14.0%æå‡è‡³96.0%è‡³99.5%ã€‚æ­¤å¤–ï¼Œåœ¨ç¼–ç å’Œæ•°å­¦ä»»åŠ¡ä¸­ï¼ŒAT-GRPOä¹Ÿæé«˜äº†æ¨ç†æ€§èƒ½ï¼Œå¹³å‡å¢å¹…åˆ†åˆ«ä¸º3.87%è‡³7.62%å’Œ9.0%è‡³17.93%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.12831",
            "title": "MTSQL-R1: Towards Long-Horizon Multi-Turn Text-to-SQL via Agentic\n  Training",
            "url": "https://huggingface.co/papers/2510.12831",
            "abstract": "MTSQL-R1, an agentic training framework, improves multi-turn Text-to-SQL by treating it as an MDP with iterative propose-execute-verify-refine cycles, enhancing coherence and execution.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-turn Text-to-SQL aims to translate a user's conversational utterances into executable SQL while preserving dialogue coherence and grounding to the target schema. However, most existing systems only regard this task as a simple text translation task and follow a short-horizon paradigm, generating a query per turn without execution, explicit verification, and refinement, which leads to non-executable or incoherent outputs. We present MTSQL-R1, an agentic training framework for long-horizon multi-turn Text-to-SQL. We cast the task as a Markov Decision Process (MDP) in which an agent interacts with (i) a database for execution feedback and (ii) a persistent dialogue memory for coherence verification, performing an iterative propose to execute -> verify -> refine cycle until all checks pass. Experiments on COSQL and SPARC demonstrate that MTSQL-R1 consistently outperforms strong baselines, highlighting the importance of environment-driven verification and memory-guided refinement for conversational semantic parsing. Full recipes (including code, trained models, logs, reasoning trajectories, etc.) will be released after the internal review to contribute to community research.",
            "score": 2,
            "issue_id": 6445,
            "pub_date": "2025-10-12",
            "pub_date_card": {
                "ru": "12 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 12",
                "zh": "10æœˆ12æ—¥"
            },
            "hash": "54ec2109d031b622",
            "authors": [
                "Taicheng Guo",
                "Hai Wang",
                "ChaoChun Liu",
                "Mohsen Golalikhani",
                "Xin Chen",
                "Xiangliang Zhang",
                "Chandan K. Reddy"
            ],
            "affiliations": [
                "Amazon",
                "University of Notre Dame"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.12831.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#agents",
                    "#reasoning",
                    "#optimization",
                    "#survey"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ˜Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ SQL Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ¹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "MTSQL-R1 â€” ÑÑ‚Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² SQL Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ĞºĞ°Ğº Ğ¼Ğ°Ñ€ĞºĞ¾Ğ²ÑĞºĞ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹, Ğ³Ğ´Ğµ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ SQL-Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ, Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ ĞµĞ³Ğ¾, Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒĞµÑ‚ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ¹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ ÑĞ²ÑĞ·Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… COSQL Ğ¸ SPARC Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ñ†Ğ¸ĞºĞ»Ñƒ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°."
                },
                "en": {
                    "title": "Transforming Multi-Turn Text-to-SQL with Iterative Learning",
                    "desc": "The paper introduces MTSQL-R1, a novel training framework designed to enhance multi-turn Text-to-SQL tasks by treating them as a Markov Decision Process (MDP). This approach allows the system to engage in iterative cycles of proposing, executing, verifying, and refining SQL queries, which improves the coherence and correctness of the generated outputs. Unlike traditional methods that generate a single query per turn without execution or verification, MTSQL-R1 leverages feedback from database interactions and maintains a dialogue memory for coherence checks. Experimental results on datasets like COSQL and SPARC show that MTSQL-R1 significantly outperforms existing models, emphasizing the value of environment-driven verification and memory-guided refinement in conversational semantic parsing."
                },
                "zh": {
                    "title": "æå‡å¤šè½®å¯¹è¯SQLè½¬æ¢çš„æ™ºèƒ½æ¡†æ¶",
                    "desc": "MTSQL-R1æ˜¯ä¸€ç§ä»£ç†è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨æ”¹å–„å¤šè½®æ–‡æœ¬åˆ°SQLçš„è½¬æ¢ã€‚å®ƒå°†è¯¥ä»»åŠ¡è§†ä¸ºä¸€ä¸ªé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ï¼Œé€šè¿‡è¿­ä»£çš„æè®®-æ‰§è¡Œ-éªŒè¯-ç²¾ç‚¼å¾ªç¯æ¥å¢å¼ºå¯¹è¯çš„ä¸€è‡´æ€§å’Œæ‰§è¡Œèƒ½åŠ›ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒMTSQL-R1åœ¨æ¯ä¸ªå›åˆä¸­ä¸ä»…ç”ŸæˆæŸ¥è¯¢ï¼Œè¿˜ä¸æ•°æ®åº“è¿›è¡Œäº¤äº’ä»¥è·å–æ‰§è¡Œåé¦ˆï¼Œå¹¶åˆ©ç”¨æŒä¹…çš„å¯¹è¯è®°å¿†è¿›è¡Œä¸€è‡´æ€§éªŒè¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMTSQL-R1åœ¨COSQLå’ŒSPARCæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºå¼ºåŸºçº¿ï¼Œå¼ºè°ƒäº†ç¯å¢ƒé©±åŠ¨çš„éªŒè¯å’Œè®°å¿†å¼•å¯¼çš„ç²¾ç‚¼åœ¨å¯¹è¯è¯­ä¹‰è§£æä¸­çš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.10611",
            "title": "HyperAgent: Leveraging Hypergraphs for Topology Optimization in\n  Multi-Agent Communication",
            "url": "https://huggingface.co/papers/2510.10611",
            "abstract": "HyperAgent, a hypergraph-based framework, optimizes communication topologies and captures group collaboration patterns, improving performance and efficiency in multi-agent systems.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language model-powered multi-agent systems have demonstrated remarkable collective intelligence through effective communication. However, existing approaches face two primary challenges: (i) Ineffective group collaboration modeling, as they rely on pairwise edge representations in graph structures, limiting their ability to capture relationships among multiple agents; and (ii) Limited task-adaptiveness in communication topology design, leading to excessive communication cost for simple tasks and insufficient coordination for complex scenarios. These issues restrict the scalability and practical deployment of adaptive collaboration frameworks. To address these challenges, we propose HyperAgent, a hypergraph-based framework that optimizes communication topologies and effectively captures group collaboration patterns using direct hyperedge representations. Unlike edge-based approaches, HyperAgent uses hyperedges to link multiple agents within the same subtask and employs hypergraph convolutional layers to achieve one-step information aggregation in collaboration groups. Additionally, it incorporates a variational autoencoder framework with sparsity regularization to dynamically adjust hypergraph topologies based on task complexity. Experiments highlight the superiority of HyperAgent in both performance and efficiency. For instance, on GSM8K, HyperAgent achieves 95.07\\% accuracy while reducing token consumption by 25.33\\%, demonstrating the potential of hypergraph-based optimization for multi-agent communication.",
            "score": 2,
            "issue_id": 6444,
            "pub_date": "2025-10-12",
            "pub_date_card": {
                "ru": "12 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 12",
                "zh": "10æœˆ12æ—¥"
            },
            "hash": "38135ea1ec6a6548",
            "authors": [
                "Heng Zhang",
                "Yuling Shi",
                "Xiaodong Gu",
                "Zijian Zhang",
                "Haochen You",
                "Lubin Gan",
                "Yilei Yuan",
                "Jin Huang"
            ],
            "affiliations": [
                "Columbia University, USA",
                "Shanghai Jiao Tong University, China",
                "South China Normal University, China",
                "University of Michigan, USA",
                "University of Pennsylvania, USA",
                "University of Science and Technology of China, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.10611.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#optimization",
                    "#graphs",
                    "#games"
                ],
                "emoji": "ğŸ•¸ï¸",
                "ru": {
                    "title": "Ğ“Ğ¸Ğ¿ĞµÑ€Ğ³Ñ€Ğ°Ñ„Ñ‹ Ğ´Ğ»Ñ ÑƒĞ¼Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ HyperAgent â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ³Ğ¸Ğ¿ĞµÑ€Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ñ LLM. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ°Ñ€Ğ½Ñ‹Ğµ ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸, HyperAgent Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ³Ğ¸Ğ¿ĞµÑ€Ñ€Ñ‘Ğ±Ñ€Ğ° Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ğ¾Ğ¹ ĞºĞ¾Ğ»Ğ»Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğº ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ñ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ GSM8K Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ 95.07% Ğ¿Ñ€Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° 25.33%, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³Ğ¸Ğ¿ĞµÑ€Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "HyperAgent: Enhancing Multi-Agent Collaboration with Hypergraphs",
                    "desc": "HyperAgent is a new framework that uses hypergraphs to improve how multiple agents communicate and work together. Traditional methods struggle with modeling group collaboration and adapting communication for different tasks, which can lead to inefficiencies. By using hyperedges, HyperAgent can connect several agents at once, allowing for better information sharing and coordination. The framework also adjusts its communication structure based on the complexity of the task, resulting in better performance and reduced communication costs."
                },
                "zh": {
                    "title": "è¶…å›¾ä¼˜åŒ–ï¼šæå‡å¤šæ™ºèƒ½ä½“åä½œçš„åˆ©å™¨",
                    "desc": "HyperAgentæ˜¯ä¸€ä¸ªåŸºäºè¶…å›¾çš„æ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜åŒ–å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„é€šä¿¡æ‹“æ‰‘ï¼Œå¹¶æ•æ‰ç¾¤ä½“åä½œæ¨¡å¼ï¼Œä»è€Œæé«˜æ€§èƒ½å’Œæ•ˆç‡ã€‚è¯¥æ¡†æ¶è§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨å»ºæ¨¡ç¾¤ä½“åä½œæ—¶çš„ä¸è¶³ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°è¡¨ç¤ºå¤šä¸ªæ™ºèƒ½ä½“ä¹‹é—´çš„å…³ç³»ã€‚HyperAgenté€šè¿‡è¶…è¾¹è¿æ¥åŒä¸€å­ä»»åŠ¡ä¸­çš„å¤šä¸ªæ™ºèƒ½ä½“ï¼Œå¹¶åˆ©ç”¨è¶…å›¾å·ç§¯å±‚å®ç°åä½œç»„å†…çš„ä¸€æ­¥ä¿¡æ¯èšåˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHyperAgentåœ¨æ€§èƒ½å’Œæ•ˆç‡ä¸Šå‡ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå±•ç¤ºäº†è¶…å›¾ä¼˜åŒ–åœ¨å¤šæ™ºèƒ½ä½“é€šä¿¡ä¸­çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.10581",
            "title": "GraphTracer: Graph-Guided Failure Tracing in LLM Agents for Robust\n  Multi-Turn Deep Search",
            "url": "https://huggingface.co/papers/2510.10581",
            "abstract": "GraphTracer addresses multi-agent system failure attribution by constructing Information Dependency Graphs to trace information flow and improve debugging accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-agent systems powered by Large Language Models excel at complex tasks through coordinated collaboration, yet they face high failure rates in multi-turn deep search scenarios. Existing temporal attribution methods struggle to accurately diagnose root causes, particularly when errors propagate across multiple agents. Attempts to automate failure attribution by analyzing action sequences remain ineffective due to their inability to account for information dependencies that span agents. This paper identifies two core challenges: (i) distinguishing symptoms from root causes in multi-agent error propagation, and (ii) tracing information dependencies beyond temporal order. To address these issues, we introduce GraphTracer, a framework that redefines failure attribution through information flow analysis. GraphTracer constructs Information Dependency Graphs (IDGs) to explicitly capture how agents reference and build on prior outputs. It localizes root causes by tracing through these dependency structures instead of relying on temporal sequences. GraphTracer also uses graph-aware synthetic data generation to target critical nodes, creating realistic failure scenarios. Evaluations on the Who\\&When benchmark and integration into production systems demonstrate that GraphTracer-8B achieves up to 18.18\\% higher attribution accuracy compared to state-of-the-art models and enables 4.8\\% to 14.2\\% performance improvements in deployed multi-agent frameworks, establishing a robust solution for multi-agent system debugging.",
            "score": 2,
            "issue_id": 6444,
            "pub_date": "2025-10-12",
            "pub_date_card": {
                "ru": "12 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 12",
                "zh": "10æœˆ12æ—¥"
            },
            "hash": "4456157e4d3ee0bf",
            "authors": [
                "Heng Zhang",
                "Yuling Shi",
                "Xiaodong Gu",
                "Haochen You",
                "Zijian Zhang",
                "Lubin Gan",
                "Yilei Yuan",
                "Jin Huang"
            ],
            "affiliations": [
                "Columbia University",
                "Shanghai Jiao Tong University",
                "South China Normal University",
                "University of Michigan",
                "University of Pennsylvania",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.10581.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#synthetic",
                    "#dataset",
                    "#graphs",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "ğŸ•¸ï¸",
                "ru": {
                    "title": "Ğ“Ñ€Ğ°Ñ„Ñ‹ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° ĞºĞ¾Ñ€Ğ½ĞµĞ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ GraphTracer â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ LLM. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ñ‚ÑŒ ÑĞ¸Ğ¼Ğ¿Ñ‚Ğ¾Ğ¼Ñ‹ Ğ¾Ñ‚ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¾Ğ½Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑÑ‚ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. GraphTracer ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ Ğ³Ñ€Ğ°Ñ„Ñ‹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ (IDG), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°ÑÑ‚, ĞºĞ°Ğº Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ´Ñ€ÑƒĞ³ Ğ´Ñ€ÑƒĞ³Ğ°, Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ½Ğ° 18% Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ½Ğ° 5-14%."
                },
                "en": {
                    "title": "Revolutionizing Multi-Agent Debugging with GraphTracer",
                    "desc": "GraphTracer is a novel framework designed to improve failure attribution in multi-agent systems by utilizing Information Dependency Graphs (IDGs). It addresses the challenges of identifying root causes of errors that propagate across multiple agents and distinguishing them from mere symptoms. By analyzing information flow rather than just temporal sequences, GraphTracer enhances debugging accuracy and provides a clearer understanding of how agents interact. The framework has shown significant improvements in attribution accuracy and performance in real-world applications, making it a valuable tool for developers working with complex multi-agent systems."
                },
                "zh": {
                    "title": "GraphTracerï¼šæå‡å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæ•…éšœå½’å› çš„å‡†ç¡®æ€§",
                    "desc": "GraphTracer æ˜¯ä¸€ç§é’ˆå¯¹å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæ•…éšœå½’å› çš„æ–¹æ³•ï¼Œé€šè¿‡æ„å»ºä¿¡æ¯ä¾èµ–å›¾æ¥è¿½è¸ªä¿¡æ¯æµï¼Œä»è€Œæé«˜è°ƒè¯•çš„å‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•è§£å†³äº†åœ¨å¤šæ™ºèƒ½ä½“é”™è¯¯ä¼ æ’­ä¸­åŒºåˆ†ç—‡çŠ¶ä¸æ ¹æœ¬åŸå› çš„æŒ‘æˆ˜ï¼Œå¹¶èƒ½å¤Ÿè¶…è¶Šæ—¶é—´é¡ºåºè¿½è¸ªä¿¡æ¯ä¾èµ–ã€‚GraphTracer é€šè¿‡åˆ†æä¿¡æ¯æµï¼Œé‡æ–°å®šä¹‰äº†æ•…éšœå½’å› ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å®šä½æ ¹æœ¬åŸå› ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGraphTracer åœ¨æ•…éšœå½’å› å‡†ç¡®æ€§ä¸Šæ¯”ç°æœ‰æ¨¡å‹æé«˜äº† 18.18%ï¼Œå¹¶åœ¨å¤šæ™ºèƒ½ä½“æ¡†æ¶ä¸­å®ç°äº† 4.8% åˆ° 14.2% çš„æ€§èƒ½æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.13586",
            "title": "Deflanderization for Game Dialogue: Balancing Character Authenticity\n  with Task Execution in LLM-based NPCs",
            "url": "https://huggingface.co/papers/2510.13586",
            "abstract": "Participants in the CPDC 2025 used lightweight prompting and fine-tuned large models to achieve high rankings in task-oriented and context-aware dialogue challenges.  \t\t\t\t\tAI-generated summary \t\t\t\t The emergence of large language models (LLMs) has opened new opportunities for cre- ating dynamic non-player characters (NPCs) in gaming environments, enabling both func- tional task execution and persona-consistent dialogue generation. In this paper, we (Tu_Character_lab) report our participation in the Commonsense Persona-Grounded Dialogue Challenge (CPDC) 2025 Round 2, which eval- uates agents across three tracks: task-oriented dialogue, context-aware dialogue, and their integration. Our approach combines two complementary strategies: (i) lightweight prompting techniques in the API track, including a Deflanderization prompting method to suppress excessive role-play and improve task fidelity, and (ii) fine-tuned large models in the GPU track, leveraging Qwen3-14B with supervisedfinetuning (SFT) and Low-Rank Adaptation(LoRA). Our best submissions ranked 2nd on Task 1, 2nd on Task 3 (API track), and 4th on Task 3 (GPU track).",
            "score": 1,
            "issue_id": 6444,
            "pub_date": "2025-10-15",
            "pub_date_card": {
                "ru": "15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 15",
                "zh": "10æœˆ15æ—¥"
            },
            "hash": "b4af6fcdd0d708b9",
            "authors": [
                "Pasin Buakhaw",
                "Kun Kerdthaisong",
                "Phuree Phenhiran",
                "Pitikorn Khlaisamniang",
                "Supasate Vorathammathorn",
                "Piyalitt Ittichaiwong",
                "Nutchanon Yongsatianchot"
            ],
            "affiliations": [
                "Artificial Intelligence Association of Thailand",
                "Department of Computer Engineering and Digital Technology, Faculty of Engineering, Chulalongkorn University",
                "School of Biomedical Engineering & Imaging Sciences, Kings College London",
                "Siriraj Informatics and Data Innovation Center (SIData+), Faculty of Medicine, Siriraj Hospital, Mahidol University",
                "Thammasat School of Engineering, Thammasat University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.13586.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#optimization",
                    "#training",
                    "#games"
                ],
                "emoji": "ğŸ®",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ñ‹Ğµ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ğµ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³ Ğ¸ Ñ„Ğ°Ğ¹Ğ½-Ñ‚ÑĞ½Ğ¸Ğ½Ğ³",
                    "desc": "ĞšĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ° Tu_Character_lab Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ… NPC Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ² Ğ¸Ğ³Ñ€Ğ°Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ÑÑŒ Ğ´Ğ²Ğ° Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°: Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³ Ñ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¾Ğ¹ Deflanderization Ğ´Ğ»Ñ API Ñ‚Ñ€ĞµĞºĞ° Ğ¸ Ñ„Ğ°Ğ¹Ğ½-Ñ‚ÑĞ½Ğ¸Ğ½Ğ³ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen3-14B Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ SFT Ğ¸ LoRA Ğ´Ğ»Ñ GPU Ñ‚Ñ€ĞµĞºĞ°. Ğ¢ĞµÑ…Ğ½Ğ¸ĞºĞ° Deflanderization Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ¸Ğ·Ğ»Ğ¸ÑˆĞ½ÑÑ Ñ€Ğ¾Ğ»ĞµĞ²ÑƒÑ Ğ¸Ğ³Ñ€Ñƒ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¼. Ğ ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ½ÑĞ»Ğ¸ Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¼ĞµÑÑ‚Ğ¾ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… 1 Ğ¸ 3 (API) Ğ¸ Ñ‡ĞµÑ‚Ğ²ĞµÑ€Ñ‚Ğ¾Ğµ Ğ¼ĞµÑÑ‚Ğ¾ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ 3 (GPU) Ğ½Ğ° ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ CPDC 2025."
                },
                "en": {
                    "title": "Elevating NPC Dialogue with Fine-Tuned Models and Smart Prompting",
                    "desc": "This paper discusses the participation of the Tu_Character_lab team in the CPDC 2025, focusing on creating advanced non-player characters (NPCs) for dialogue challenges. The team utilized lightweight prompting techniques and fine-tuned large language models to enhance dialogue generation and task execution. They implemented a unique Deflanderization method to maintain task fidelity while minimizing excessive role-play. Their strategies led to impressive rankings, achieving 2nd place in both Task 1 and Task 3 of the API track, and 4th place in Task 3 of the GPU track."
                },
                "zh": {
                    "title": "è½»é‡çº§æç¤ºä¸å¾®è°ƒæ¨¡å‹çš„å®Œç¾ç»“åˆ",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†æˆ‘ä»¬åœ¨2025å¹´å¸¸è¯†äººæ ¼å¯¹è¯æŒ‘æˆ˜èµ›ï¼ˆCPDCï¼‰ä¸­çš„å‚ä¸æƒ…å†µã€‚æˆ‘ä»¬ä½¿ç”¨è½»é‡çº§æç¤ºæŠ€æœ¯å’Œå¾®è°ƒçš„å¤§å‹æ¨¡å‹ï¼ŒæˆåŠŸåœ°åœ¨ä»»åŠ¡å¯¼å‘å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥å¯¹è¯æŒ‘æˆ˜ä¸­å–å¾—äº†é«˜æ’åã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†Deflanderizationæç¤ºæ–¹æ³•æ¥æŠ‘åˆ¶è¿‡åº¦è§’è‰²æ‰®æ¼”ï¼Œå¹¶æé«˜ä»»åŠ¡çš„å‡†ç¡®æ€§ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬åˆ©ç”¨Qwen3-14Bæ¨¡å‹è¿›è¡Œç›‘ç£å¾®è°ƒå’Œä½ç§©é€‚åº”ï¼Œæå‡äº†å¯¹è¯ç”Ÿæˆçš„è´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.11715",
            "title": "Point Prompting: Counterfactual Tracking with Video Diffusion Models",
            "url": "https://huggingface.co/papers/2510.11715",
            "abstract": "Pretrained video diffusion models can perform zero-shot point tracking by visually marking points and regenerating video frames, outperforming prior methods and handling occlusions.  \t\t\t\t\tAI-generated summary \t\t\t\t Trackers and video generators solve closely related problems: the former analyze motion, while the latter synthesize it. We show that this connection enables pretrained video diffusion models to perform zero-shot point tracking by simply prompting them to visually mark points as they move over time. We place a distinctively colored marker at the query point, then regenerate the rest of the video from an intermediate noise level. This propagates the marker across frames, tracing the point's trajectory. To ensure that the marker remains visible in this counterfactual generation, despite such markers being unlikely in natural videos, we use the unedited initial frame as a negative prompt. Through experiments with multiple image-conditioned video diffusion models, we find that these \"emergent\" tracks outperform those of prior zero-shot methods and persist through occlusions, often obtaining performance that is competitive with specialized self-supervised models.",
            "score": 1,
            "issue_id": 6449,
            "pub_date": "2025-10-13",
            "pub_date_card": {
                "ru": "13 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 13",
                "zh": "10æœˆ13æ—¥"
            },
            "hash": "25082e4025961f42",
            "authors": [
                "Ayush Shrivastava",
                "Sanyam Mehta",
                "Daniel Geng",
                "Andrew Owens"
            ],
            "affiliations": [
                "Cornell University",
                "University of Michigan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.11715.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#cv",
                    "#games",
                    "#diffusion"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ¢Ñ€ĞµĞºĞ¸Ğ½Ğ³ Ñ‚Ğ¾Ñ‡ĞµĞº Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ñ‚Ñ€ĞµĞºĞ¸Ğ½Ğ³ Ñ‚Ğ¾Ñ‡ĞµĞº Ğ² zero-shot Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ, Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ¼ĞµÑ‡Ğ°Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ñ†Ğ²ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°Ñ€ĞºĞµÑ€Ğ° Ğ½Ğ° Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡ĞºĞµ Ğ¸ Ñ€ĞµĞ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ÑÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ ÑˆÑƒĞ¼Ğ°, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¼Ğ°Ñ€ĞºĞµÑ€ Ğ¿Ğ¾ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. Ğ”Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ°Ñ€ĞºĞµÑ€Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ ĞºĞ°Ğ´Ñ€ ĞºĞ°Ğº negative prompt, Ğ¿Ğ¾ÑĞºĞ¾Ğ»ÑŒĞºÑƒ ÑÑ€ĞºĞ¸Ğµ Ğ¼Ğ°Ñ€ĞºĞµÑ€Ñ‹ Ğ½ĞµÑ‚Ğ¸Ğ¿Ğ¸Ñ‡Ğ½Ñ‹ Ğ´Ğ»Ñ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ zero-shot Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ Ñ Ğ¾ĞºĞºĞ»ÑĞ·Ğ¸ÑĞ¼Ğ¸ Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ self-supervised Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ‚Ñ€ĞµĞºĞ¸Ğ½Ğ³Ğ°."
                },
                "en": {
                    "title": "Revolutionizing Point Tracking with Video Diffusion Models",
                    "desc": "This paper presents a novel approach using pretrained video diffusion models for zero-shot point tracking, which involves marking points visually and regenerating video frames. By placing a distinctively colored marker at the query point, the model can trace the point's trajectory across frames, even in the presence of occlusions. The method leverages the relationship between motion analysis and video synthesis, allowing for effective tracking without prior training on specific tracking tasks. Experimental results demonstrate that this approach outperforms existing zero-shot tracking methods and competes well with specialized models."
                },
                "zh": {
                    "title": "é¢„è®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡å‹å®ç°é›¶-shotç‚¹è·Ÿè¸ª",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œå¯ä»¥é€šè¿‡è§†è§‰æ ‡è®°ç‚¹æ¥å®ç°é›¶-shotç‚¹è·Ÿè¸ªï¼Œå¹¶ä¸”åœ¨å¤„ç†é®æŒ¡æ—¶è¡¨ç°ä¼˜äºä¹‹å‰çš„æ–¹æ³•ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè·Ÿè¸ªå™¨å’Œè§†é¢‘ç”Ÿæˆå™¨ä¹‹é—´å­˜åœ¨å¯†åˆ‡çš„è”ç³»ï¼Œå‰è€…åˆ†æè¿åŠ¨ï¼Œåè€…åˆæˆè¿åŠ¨ã€‚é€šè¿‡åœ¨æŸ¥è¯¢ç‚¹æ”¾ç½®ä¸€ä¸ªç‹¬ç‰¹é¢œè‰²çš„æ ‡è®°ï¼Œå¹¶ä»ä¸­é—´å™ªå£°æ°´å¹³é‡æ–°ç”Ÿæˆè§†é¢‘ï¼Œå…¶æ ‡è®°å¯ä»¥åœ¨å¸§ä¹‹é—´ä¼ æ’­ï¼Œè¿½è¸ªç‚¹çš„è½¨è¿¹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¿™ç§â€œæ¶Œç°â€è½¨è¿¹çš„æ€§èƒ½è¶…è¿‡äº†ä¹‹å‰çš„é›¶-shotæ–¹æ³•ï¼Œå¹¶ä¸”åœ¨é®æŒ¡æƒ…å†µä¸‹ä¾ç„¶æœ‰æ•ˆï¼Œè¡¨ç°ä¸ä¸“é—¨çš„è‡ªç›‘ç£æ¨¡å‹ç›¸å½“ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-10-15.html",
    "link_next": "2025-10-17.html",
    "link_month": "2025-10.html",
    "short_date_prev": {
        "ru": "15.10",
        "en": "10/15",
        "zh": "10æœˆ15æ—¥"
    },
    "short_date_next": {
        "ru": "17.10",
        "en": "10/17",
        "zh": "10æœˆ17æ—¥"
    },
    "categories": {
        "#dataset": 7,
        "#data": 2,
        "#benchmark": 12,
        "#agents": 9,
        "#cv": 2,
        "#rl": 5,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 3,
        "#3d": 3,
        "#audio": 2,
        "#video": 6,
        "#multimodal": 8,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 6,
        "#healthcare": 0,
        "#training": 12,
        "#robotics": 1,
        "#agi": 2,
        "#games": 8,
        "#interpretability": 3,
        "#reasoning": 7,
        "#transfer_learning": 1,
        "#graphs": 2,
        "#ethics": 1,
        "#security": 1,
        "#optimization": 22,
        "#survey": 2,
        "#diffusion": 4,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 2,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 5,
        "#small_models": 1,
        "#science": 1,
        "#low_resource": 0
    }
}