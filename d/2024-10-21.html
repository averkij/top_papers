
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF (20 статей)</title>
    <link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #0989eacf;
            --secondary-color: #fff;
            --background-color: #f5f5f5;
            --text-color: #333333;
            --header-color: #0989eacf;
            --body-color: #f5f5f5;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }
        header {
            padding: 1.6em 0;
            text-align: center;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 2em;
            padding: 10px 0 20px 0;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.tags {
            color: #555;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);
            transition: background-color 0.2s ease;
            display: flex;
            flex-direction: column;
            position: relative;
        }
        .article-content {
            padding: 1.5em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
        }
        body.dark-theme>div>main>article {
            background-color: #444;
        }
        body.light-theme>div>main>article {
            background-color: #fff;
        }
        body.dark-theme>div>main>article:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
        }
        .pub-date {
            font-size: 0.9em;
            margin-bottom: 0.8em;
            font-weight: 300;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 1em;
            position: absolute;
            bottom: 10px;
            font-weight: 300;
            font-family: 'Roboto Slab';
        }
        .background-digit {
            position: absolute;
            bottom: -20px;
            right: -10px;
            font-size: 12em;
            font-weight: bold;
            color: rgba(0, 0, 0, 0.03);
            z-index: 0;
            line-height: 1;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 80px;
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        a:hover {
            color: var(--secondary-color);
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            padding: 1em 0;
            margin-top: 2em;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: fixed;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
        }
        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
        }
        .update-info-container {
            flex: 1;
        }
        .sort-container {
            flex: 2;
        }
        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .category-toggle {
            display: none;
            margin-bottom: 10px;
            margin-top: 15px;
            cursor: pointer;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }

        .svg-container span {
            position: relative;
            z-index: 1;
        }

        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }
        body.light-theme>div>main>article.x1124cffc31d2cf8d { background: url("https://hfday.ru/img/20241017/1124cffc31d2cf8d.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x1124cffc31d2cf8d:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x1124cffc31d2cf8d { background: url("https://hfday.ru/img/20241017/1124cffc31d2cf8d.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x1124cffc31d2cf8d:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x7c607e84a2158236 { background: url("https://hfday.ru/img/20241017/7c607e84a2158236.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x7c607e84a2158236:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x7c607e84a2158236 { background: url("https://hfday.ru/img/20241017/7c607e84a2158236.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x7c607e84a2158236:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.xa015f20d9d67a6a8 { background: url("https://hfday.ru/img/20241018/a015f20d9d67a6a8.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.xa015f20d9d67a6a8:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.xa015f20d9d67a6a8 { background: url("https://hfday.ru/img/20241018/a015f20d9d67a6a8.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.xa015f20d9d67a6a8:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.xfa9ce7d280c285b4 { background: url("https://hfday.ru/img/20241017/fa9ce7d280c285b4.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.xfa9ce7d280c285b4:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.xfa9ce7d280c285b4 { background: url("https://hfday.ru/img/20241017/fa9ce7d280c285b4.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.xfa9ce7d280c285b4:hover { background-color: rgba(60,60,60,0.92) !important;}

        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .sort-container {
                margin-top: 0px;
                text-align: left;
                width: 100%;
            .sort-dropdown {
                float: right;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiffRu(dateString) {
        const timeUnits = {
            minute: ["минуту", "минуты", "минут"],
            hour: ["час", "часа", "часов"],
            day: ["день", "дня", "дней"]
        };

        function getRussianPlural(number, words) {
            if (number % 10 === 1 && number % 100 !== 11) {
                return words[0];
            } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                return words[1];
            } else {
                return words[2];
            }
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);

        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes == 0) {
            return 'только что';
        }
        else if (minutes < 60) {
            return `${minutes} ${getRussianPlural(minutes, timeUnits.minute)} назад`;
        } else if (hours < 24) {
            return `${hours} ${getRussianPlural(hours, timeUnits.hour)} назад`;
        } else {
            return `${days} ${getRussianPlural(days, timeUnits.day)} назад`;
        }
    }
    function formatArticlesTitle(number) {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;

        let word;

        if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
            word = "статей";
        } else if (lastDigit === 1) {
            word = "статья";
        } else if (lastDigit >= 2 && lastDigit <= 4) {
            word = "статьи";
        } else {
            word = "статей";
        }

        return `${number} ${word}`;
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">
            <h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">хф дэйли</h1>
            <p>21 октября | 20 статей</p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 Сортировка по</label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="category-toggle">
            <div class="svg-container">
                <span id="category-toggle">🔍 Фильтр</span>
                <svg height="3" width="200">
                    <line x1="0" y1="0" x2="200" y2="0" 
                        stroke="black" 
                        stroke-width="2" 
                        stroke-dasharray="3, 3" />
                </svg>
            </div>
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">градиент обреченный</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>    
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "хф найтли";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "хф дэйли";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2410.14059', 'title': 'UCFE: A User-Centric Financial Expertise Benchmark for Large Language Models', 'url': 'https://huggingface.co/papers/2410.14059', 'abstract': 'This paper introduces the UCFE: User-Centric Financial Expertise benchmark, an innovative framework designed to evaluate the ability of large language models (LLMs) to handle complex real-world financial tasks. UCFE benchmark adopts a hybrid approach that combines human expert evaluations with dynamic, task-specific interactions to simulate the complexities of evolving financial scenarios. Firstly, we conducted a user study involving 804 participants, collecting their feedback on financial tasks. Secondly, based on this feedback, we created our dataset that encompasses a wide range of user intents and interactions. This dataset serves as the foundation for benchmarking 12 LLM services using the LLM-as-Judge methodology. Our results show a significant alignment between benchmark scores and human preferences, with a Pearson correlation coefficient of 0.78, confirming the effectiveness of the UCFE dataset and our evaluation approach. UCFE benchmark not only reveals the potential of LLMs in the financial sector but also provides a robust framework for assessing their performance and user satisfaction.The benchmark dataset and evaluation code are available.', 'score': 39, 'issue_id': 195, 'pub_date': '2024-10-17', 'pub_date_ru': '17 октября', 'hash': '1124cffc31d2cf8d', 'data': {'desc': 'Статья представляет UCFE - новый бенчмарк для оценки способности больших языковых моделей (LLM) решать сложные финансовые задачи. Бенчмарк использует гибридный подход, сочетающий оценки экспертов и динамические взаимодействия для симуляции реальных финансовых сценариев. На основе пользовательского исследования был создан датасет, охватывающий широкий спектр финансовых задач. Результаты тестирования 12 LLM-сервисов показали высокую корреляцию (0.78) между оценками бенчмарка и предпочтениями пользователей.', 'emoji': '💹', 'title': 'UCFE: Новый стандарт для оценки финансового ИИ', 'categories': ['#benchmark', '#dataset', '#nlp'], 'embedding': [-0.034572505115204405, 0.0016925514305786852, 0.13199350767403784, 0.03655473757784103, 0.06634656291308579, 0.03605917946218187, 0.003186515042072632, 0.11304568150042722, 0.08593570256475085, 0.07299287423792743, 0.07829827079338889, -0.13572476233339784, -0.0769573497012057, -0.035213816485657835, 0.09520556404548555, 0.04955586636354175, 0.04804003967952462, -0.07765696153053644, -0.053257981331468106, 0.013525836287758133, 0.008468222794568162, -0.029850123686521072, 0.0853526895455737, 0.017038472445972123, -0.04253059573319458, -0.04908946058492225, -0.07841487592634551, -0.040490056488877636, -0.0521211118453555, -0.03037483203161889, 0.13584137589675846, -0.023903417868207183, -0.04897285966716762, 8.32383674054294e-05, -0.02244589375066837, 0.07969749866725238, -0.028348871063422775, 0.1046503353332562, 0.021236147637158632, 0.031365948262937204, -0.07013612435412611, -0.028319720833984117, 0.00013607367567903948, -0.016542914330312963, -0.11456149764643934, -0.02298517826428601, 0.08033881003770582, 0.05048868213598275, 0.027488929810777912, 0.0834870580006917, -0.07771526409701475, 0.0714770538767133, 0.0006804821888492273, -0.055298518468184045, -0.009320875962953109, 0.005841033233267336, 0.0217900062116951, 0.023655637756577103, 0.04101476694157645, 0.03573852693835665, -0.00892734365032925, -0.0223584409547514, -0.017912987759535808, -0.08051370930673676, -0.12068310905658718, -0.08465307982384396, -0.07036932829723637, 0.12021670117036667, 0.02244589375066837, 0.041801829459223165, -0.01505623787573449, 0.04390066916241742, 0.100510958493346, -0.014057833560156262, 0.05576492635440455, -0.0359717308814669, 0.052587521839177004, 0.021819156441133757, -0.049031160126044936, -0.02993757437483704, 0.035068063230863554, -0.06582185456798798, 0.002537916430399679, -0.014363914130663654, 0.05273327298637029, -0.024457276442743654, -0.04005279883209578, 0.06856200142643366, -0.12534719213399417, -0.06564694686855305, -0.041364571802441324, -0.03092869060615536, -0.032007261740991645, 0.012498280899700849, 0.04871050338701771, 0.05287902624116457, -0.016878145130259013, -0.06354811138056078, 0.11561092276703898, 0.023670213925096936, -0.06558864640967572, 0.07450870092194456, 0.06669636777395066, -0.08424497239650078, 0.0908329874776671, 0.0327651719215987, -0.0377790619674716, -0.07217666359844306, -0.05150895070434072, 0.08955036473676022, -0.15822896708094852, 0.01910815981212672, -0.08861754896431923, 0.03232791426481686, 0.08640211466617334, -0.05273327298637029, -0.07520831907407832, -0.035213816485657835, -0.05757225744040924, -0.04967246728129637, 0.0018009549606977171, -0.01617853266864796, 0.08634380788449303, -0.020113852211964864, 0.09077469123399179, -0.03672964316967497, -0.050517834473022404, 0.025011138178681627, -0.102376592145829, -0.07136044452855468, -0.011332260762629396, 0.020288755696197803, -0.0014137997007252717, 0.011259384978272655, -0.0774237554798252, 0.042880400594059453, -0.15659654580197976, 0.029733518553564448, -0.059175534812742334, -0.12989467429364657, -0.047078073677644966, 0.012826224247667283, 0.006784781342858309, 0.019924374666813096, -0.12896185641360458, -0.043230207562525325, 0.020638560557062677, 0.061390975433691215, -0.02757638260669488, 0.019705744784621674, -0.01255658178009836, -0.11870087448634954, -0.11765145990375492, -0.0075864202397849575, -0.03809971870649881, -0.03221131334706224, -0.04643676230719152, 0.05223771276311013, 0.07299287423792743, -0.10505844065299838, 0.10867310493260877, 0.05742650418561496, -0.02076973975093813, 0.102376592145829, 0.06156587891792415, 0.046961472759890335, -0.049847374980731315, -0.012498280899700849, -0.14073866080967473, -0.017053048614491948, -0.07229326451619768, -0.04156862762371391, 0.004638575204017512, -0.04349255962747323, -0.0054547892156634895, -0.07445040467826924, -0.017329978955560685, -0.006529714200768816, 0.048185792934318905, -0.09514726780181024, -0.042501443396154914, 0.040606663729435256, -0.010100651871660616, 0.02126529786659729, -0.006890451609543763, -0.018058741014330092, 0.012163050732035099, -0.013452960503401393, 0.012177625846754428, -0.06488903668794598, 0.010064213347201945, 0.14097186686038599, 0.0382454677460911, 0.14213789711394223, 0.08704342392902578, 0.027182850294071022, 0.08243764341809708, 0.01903528318472958, -0.08908395263533772, -0.016440888105757466, -0.039265736314449066, 0.03827462008313075, 0.042005885280495765, -0.03885763099470688, 0.11176305665191934, 0.06821219656556879, 0.00038761067039664514, 0.04483348493485843, -0.15414790123792066, -0.034805706950713645, 0.11053873647749078, 0.11036383088565684, 0.06389791624142566, -0.0042231803246947095, -0.12406456517788532, -0.0020460013873740183, -0.030549733408250828, -0.006329304529227179, 0.01884580353197681, 0.054103344307992136, 0.00725118943983891, 0.051450650245463414, 0.1027846995731722, -0.016601214789190275, -0.09473915194406304, 0.03177405569028039, -0.010749249851053268, -0.026220883238390863, 0.03766246104971697, -0.1332761304149447, 0.03381459071939534, -0.062207190288377595, -0.07211835892436375, -0.03288177705455533, -0.027328602495064802, 0.010887715021587637, -0.05416164687447045, 0.01711134907336926, -0.03512636368974087, -0.0024777933181770874, -0.0022992466348545925, 0.04314275476660836, -0.004962874299093889, -0.11246266426604809, 0.009189698033638257, 0.07771526409701475, 0.005990429687151174, -0.04833154829671418, 0.06477243155498935, 0.03879932632062756, 0.04926436196155419, 0.013190606330852487, -0.009043946254164674, -0.12721282367887618, 0.013649726132813074, 0.025186040609114065]}}, {'id': 'https://huggingface.co/papers/2410.13232', 'title': 'Web Agents with World Models: Learning and Leveraging Environment Dynamics in Web Navigation', 'url': 'https://huggingface.co/papers/2410.13232', 'abstract': 'Large language models (LLMs) have recently gained much attention in building autonomous agents. However, the performance of current LLM-based web agents in long-horizon tasks is far from optimal, often yielding errors such as repeatedly buying a non-refundable flight ticket. By contrast, humans can avoid such an irreversible mistake, as we have an awareness of the potential outcomes (e.g., losing money) of our actions, also known as the "world model". Motivated by this, our study first starts with preliminary analyses, confirming the absence of world models in current LLMs (e.g., GPT-4o, Claude-3.5-Sonnet, etc.). Then, we present a World-model-augmented (WMA) web agent, which simulates the outcomes of its actions for better decision-making. To overcome the challenges in training LLMs as world models predicting next observations, such as repeated elements across observations and long HTML inputs, we propose a transition-focused observation abstraction, where the prediction objectives are free-form natural language descriptions exclusively highlighting important state differences between time steps. Experiments on WebArena and Mind2Web show that our world models improve agents\' policy selection without training and demonstrate our agents\' cost- and time-efficiency compared to recent tree-search-based agents.', 'score': 28, 'issue_id': 194, 'pub_date': '2024-10-17', 'pub_date_ru': '17 октября', 'hash': '7c607e84a2158236', 'data': {'desc': "Эта статья представляет новый подход к улучшению веб-агентов на основе больших языковых моделей (LLM) путем внедрения 'модели мира'. Авторы обнаружили, что существующие LLM-агенты часто совершают ошибки в задачах с долгосрочными последствиями, например, повторно покупая невозвратные билеты. Для решения этой проблемы они разработали World-model-augmented (WMA) веб-агент, который симулирует результаты своих действий перед принятием решений. Эксперименты показали, что этот подход улучшает выбор стратегий агентами и повышает их эффективность по сравнению с агентами, основанными на поиске в дереве решений.", 'emoji': '🌐', 'title': "Веб-агенты с 'моделью мира': умнее, эффективнее, безопаснее", 'categories': ['#agents', '#nlp'], 'embedding': [-0.04409607219653502, 0.0663316327749968, 0.11380323164619542, 0.025825403598334698, -0.058669744001578795, -0.019998613378479867, 0.03509468597980076, 0.017882215395475656, 0.034317783484859005, 0.155059583184307, 0.02689699705590398, -0.1494872858644747, -0.059794918256297465, -0.10249791710050132, -0.03541616489693575, -0.028156119881802007, -0.002528290963651647, -0.046185681247404874, 0.045917782149792384, 0.018953811785925617, 0.0628489621033185, -0.008478983465203345, -0.01756073843359445, -0.023427714006904604, -0.011479445537448096, 0.001930542702967814, -0.02452609541898135, 0.07469006506796866, -0.013294456791231533, -0.04339953845325011, 0.15173762655289694, -0.04977551937914332, -0.09237136053955596, 0.06906420161539042, -0.061509468570509845, 0.084923781267959, 0.06777828594685047, 0.07131455208008157, 0.008927713036145271, 0.07179676654527647, -0.032630022102780994, 0.008385219269852418, 0.03338013957609597, 0.07061801833679666, -0.04061339370384157, 0.10008683304300404, -0.010849885102125974, 0.11583926478805036, -0.007045727105721402, 0.0628489621033185, -0.07088590765814022, 0.05143648987383319, -0.03742540324089497, -0.01154642001856315, -0.06756397057926804, -0.06268822068949721, 0.009369746253917814, 0.04412286308392316, 0.02009237757383079, 0.01614087810438272, -0.0519722841585506, -0.011352193710488886, -0.019864664807300515, 0.04564988500743368, -0.09178198448006228, -0.05550854442602033, -0.07844064070302081, 0.05861616222680251, -0.06483140955988957, 0.06627805882123566, 0.006985450150928004, -0.06890346020156914, 0.03292471013252784, 0.007347113052840663, -0.07747620786212342, 0.007340415526519006, 0.08658475762841024, 0.039461432472242326, -0.04637320768285294, 0.025289606380736608, 0.00437009232272171, -0.00465473445403683, -0.04966835778484453, -0.04272979168684578, 0.05465126926891415, 0.00289497671701748, 0.05473163606531721, -0.015189838165349473, -0.09590761494126433, -0.05599076280172281, -0.0074743648797998725, 0.026334409928544644, 0.02764711257396517, 0.005850230779879115, 0.05352609501419547, 0.012959583603554743, -0.06097367037528485, -0.028370437204638214, 0.12473347963421688, -0.008525865562878807, -0.05550854442602033, -0.08294133772189552, 0.06124156751764356, 0.010595381448207557, 0.01697136237410076, 0.007681986044992072, 0.027888220784189517, -0.04773949014779596, -0.09944387520873407, -0.03627344200929572, -0.24860969779810413, -0.06536720384460698, -0.16588267544379104, 0.010427944463318404, 0.017118706388974184, -0.060598613593881155, -0.12109005972770216, 0.06392055067275332, 0.08819214243781624, -0.02017274828074143, 0.04436397324940129, 0.04559630323265739, 0.054008313389897956, -0.022905311255373696, 0.07854780425257338, 0.010072978696676645, 0.01403787497849644, -0.08085173258153322, 0.056580136905962725, -0.053660045540628615, 0.07281477725044257, 0.06231315608707837, -0.007856119774101366, -0.058991220963459996, -0.16534687724856603, 0.0025199191535122648, -0.04875750476346964, -0.04173856795856024, -0.09547898029559192, 0.054678060156302284, 0.025075289057900404, -0.03959537908816789, -0.030460044300254285, 0.04240831472496458, -0.12355472751522951, 0.06488497960314314, 0.05416905089321166, 0.04061339370384157, -0.11048129065681568, 0.06981431713345161, -0.040827711026677774, -0.07120739048578277, -0.03997043782482538, 0.01141247125185842, -0.025651269673700027, -0.012316627529013172, 0.010213625185228411, 0.001171218232858382, 0.02511547147847504, -0.08615612102748404, 0.08754918655880005, 0.11862539976118996, -0.004591108345031845, 0.029924250015536873, 0.03911316266771919, 0.08401293411234546, -0.08781708761166634, 0.04865034512442464, -0.10223002386865018, -0.09188914607436105, -0.05215981254925245, -0.01948960607064303, -0.008103926683799643, -0.06177736375761476, -0.037398612353506826, -0.02535658164395318, -0.0254503458393041, -0.07769053105072099, -0.005960738791034184, -0.06890346020156914, -0.06836766591685174, 0.09226420285576475, -0.042033255988307094, -0.01290600456613376, -0.10458752615137117, -0.008003463788974789, -0.024874365223504485, 0.024552886306369492, -0.0655279393926669, -0.000718721083394472, -0.04506050503743241, 0.11283879489479046, 0.1775630369946199, 0.024030483554838583, 0.006878291098459144, 0.0519187043390281, 0.07115381066626028, 0.061670202163315965, 0.04656073802880857, -0.0259861420792753, 0.012062124852721647, -0.0158327946309418, -0.00658025411002609, -0.15420231193770836, 0.02453949086267542, 0.05486558659175035, 0.0779584262378259, 0.0421939934916208, -0.06986789890822791, -0.011767436040873287, 0.10555195703701478, 0.01551131571380681, 0.10303371334047251, -0.08299491167565665, -0.12119722718776231, -0.06965357767488413, -0.05226697218829745, 0.0003145712987939296, 0.04243510365709893, 0.03153163873545046, -0.03244248980157157, 0.0496415668974564, 0.05084710990383192, 0.03131731945736047, -0.09778291057980555, -0.021646188429475664, 0.09317505783239344, -0.021900692083394086, 0.024994918350989763, -0.040559813884319076, 0.03619307130238508, -0.013542262723302915, -0.0926928335909296, -0.05470484713318286, -0.023387527675822393, -0.02861154737011635, 0.0714752856728877, 0.02737921347635268, -0.012048729409027575, -0.012999768761484684, 0.0029301386324434896, 0.004641339401393515, 0.009061662780476895, -0.08846004153542875, -0.04728406070422783, 0.042086837763083376, -0.04597136392456867, -0.07372563418232504, 0.10448036651232617, -0.0518115446999831, -0.006221939971274259, -0.001737990756921426, -0.07527944112746235, -0.06665311560263937, 0.022342726083268143, 0.06097367037528485]}}, {'id': 'https://huggingface.co/papers/2410.14669', 'title': 'NaturalBench: Evaluating Vision-Language Models on Natural Adversarial Samples', 'url': 'https://huggingface.co/papers/2410.14669', 'abstract': 'Vision-language models (VLMs) have made significant progress in recent visual-question-answering (VQA) benchmarks that evaluate complex visio-linguistic reasoning. However, are these models truly effective? In this work, we show that VLMs still struggle with natural images and questions that humans can easily answer, which we term natural adversarial samples. We also find it surprisingly easy to generate these VQA samples from natural image-text corpora using off-the-shelf models like CLIP and ChatGPT. We propose a semi-automated approach to collect a new benchmark, NaturalBench, for reliably evaluating VLMs with 10,000 human-verified VQA samples. Crucially, we adopt a vision-centric design by pairing each question with two images that yield different answers, preventing blind solutions from answering without using the images. This makes NaturalBench more challenging than previous benchmarks that can be solved with commonsense priors. We evaluate 53 state-of-the-art VLMs on NaturalBench, showing that models like LLaVA-OneVision, Cambrian-1, Llama3.2-Vision, Molmo, Qwen2-VL, and even GPT-4o lag 50%-70% behind human performance (over 90%). We analyze why NaturalBench is hard from two angles: (1) Compositionality: Solving NaturalBench requires diverse visio-linguistic skills, including understanding attribute bindings, object relationships, and advanced reasoning like logic and counting. To this end, unlike prior work that uses a single tag per sample, we tag each NaturalBench sample with 1 to 8 skill tags for fine-grained evaluation. (2) Biases: NaturalBench exposes severe biases in VLMs, as models often choose the same answer regardless of the image. Lastly, we apply our benchmark curation method to diverse data sources, including long captions (over 100 words) and non-English languages like Chinese and Hindi, highlighting its potential for dynamic evaluations of VLMs.', 'score': 23, 'issue_id': 196, 'pub_date': '2024-10-18', 'pub_date_ru': '18 октября', 'hash': 'a015f20d9d67a6a8', 'data': {'desc': 'Исследователи разработали новый бенчмарк NaturalBench для оценки моделей визуально-языкового понимания (VLM) на основе естественных изображений и вопросов. Бенчмарк содержит 10 000 проверенных человеком образцов задач визуального ответа на вопросы (VQA), которые требуют разнообразных навыков визуально-лингвистического рассуждения. Тестирование 53 современных VLM на NaturalBench показало, что даже лучшие модели отстают от человеческой производительности на 50-70%. Исследование выявило серьезные проблемы с композиционностью и предвзятостью в существующих VLM.', 'emoji': '🖼️', 'title': 'NaturalBench: новый вызов для моделей визуально-языкового понимания', 'categories': ['#benchmark', '#nlp', '#multimodal'], 'embedding': [0.020335406781867318, 0.03340128206680876, 0.050170763911271905, -0.0016280715328171218, -0.025429582873292382, -0.03290563157399887, -0.00712496422488375, -0.0417997919879554, -0.03309838500826011, 0.06939095394667953, 0.05760550875287061, -0.04314905766075097, 0.0055795010378096675, -0.034778087829192755, 0.01653542455109413, 0.022318004569590325, 0.025567262403651637, 0.023240463698272203, -0.004667367664728908, 0.017994838031693675, 0.024920166519204846, -0.014855722985216457, -0.03755923526003259, -0.012687254484695197, 0.001626350559605214, -0.04755483400862146, -0.006732574844074098, 0.0010756286520856911, 0.04890410177317532, -0.061185199551552424, 0.0933473616616113, -0.03240998108118897, -0.13415585893922177, -0.023433217132533445, -0.02954623011565015, 0.13492686012571703, 0.00956879220447518, -0.03323606453861936, -0.014759346058910005, 0.08354450588849224, 0.0041889279499172346, -0.002731236404738407, 0.03356649750323987, -0.006866813641229343, 0.005021894965513936, 0.09962558589764255, 0.056256240988316754, 0.05465914379384138, -0.13955292163040406, 0.0757793322657896, -0.08156190600901096, 0.09450387180838733, 0.008666985632874678, -0.011282914280470152, -0.0010300220043492358, -0.08255320699463073, -0.042075153140432205, -0.018311500951519965, -0.02463103636781298, 0.03218969383261416, -0.00566899336007067, -0.12997036356465122, 0.01014705041550063, 0.015530356449141728, -0.04628817396228316, -0.05063887849800996, -0.0485185990881694, 0.0164665847859145, 0.02358466461592863, -0.055072188660194005, 0.012680371344880547, 0.027453484520123164, -0.017691940973145026, -0.006305765580674631, -0.03532880804238807, 0.05421856929669176, 0.08679376581431185, 0.09841399975520623, -0.07429237869560164, -0.05030844553338945, 0.0060923610535628114, -0.02039047650225274, 0.014194856846799626, 0.007056124041352464, -0.07208949993457868, 0.05628377689438861, -0.02216655499443686, 0.06608662220871624, -0.11642260155641926, -0.07319093617745272, -0.05262147628573296, 0.01283870322314535, -0.010546324191179901, 0.0063814897407238795, 0.06663734451366984, 0.06443445947737202, 0.012935079731100143, -0.03571431491091055, 0.05474175360381522, -0.002078975132381115, -0.08844594109620577, -0.004887656795886175, 0.07759671043356493, -0.008942346994527306, 0.06895037735777163, 0.005603595164798366, 0.07825757217928937, 0.06272722284212581, -0.05936781929201881, -0.00334047230091886, -0.13503700375000444, -0.07500831852874462, -0.08288364205100952, -0.02019772515974978, 0.04962004160631831, -0.112897988845156, -0.07170398888253961, -0.025250599483825346, 0.03838531244218813, -0.08271842452282013, 0.0016366766080524895, 0.09615604081500642, 0.11741390672555563, -0.11895592164909585, 0.052869299440379616, -0.06316779733927542, 0.027852758295802436, -0.0010807917390620774, 0.04284616269396061, -0.007255760824604187, -0.023405681226461597, 0.031060713316635114, 0.12556458512502244, -0.021615832689483264, -0.10237919742241051, -0.05077655802836921, -0.06955617775014379, 0.025250599483825346, -0.11086031297001443, 0.0567794273871985, -0.0023681042378938394, 0.04414035655461248, -0.02987666308027065, 0.0897676687711712, -0.0629475121824589, 0.019399177608391233, 0.051382352145466506, -0.05496204294414832, -0.06878516401309881, 0.106840047674183, -0.006199063317118721, -0.007269528568464283, -0.06900544916991533, 0.04535194897232364, -0.09901980014757839, -0.106840047674183, -0.03846792225216197, 0.12622545105426347, -0.05110699099298971, -0.07649526582365775, 0.07594454770222071, -0.004636389979573904, 0.07974452993299391, -0.0036829525382095388, -0.010553207958522036, 0.043341811095012214, -0.07043732465268475, 0.055457695528716495, -0.07831265235846621, -0.08519667489511132, -0.0016409791456701735, 0.0694460320340981, -0.008584377287131639, -0.07561411264584193, 0.035494023478819164, -0.04061573965983265, -0.047940338785385665, -0.047417154478262204, 0.052704084003948506, -0.04477368866953991, -0.13878190789335906, 0.10524295257146592, -0.08508652917906563, 0.0007886509199697229, 0.0013509894907987468, -0.0186144001018269, -0.0876749252674025, 0.04496644210380116, -0.039128786089644686, -0.12435300107434442, -0.1019936947374046, 0.010408643614941501, 0.12303126503234585, 0.10403136015375475, 0.10799655991271732, -0.09455894780404761, 0.09411836493986485, 0.07897351828770721, 0.010346687826279836, -0.09885457216059756, 0.047499762196477766, 0.048160628125718766, 0.04719686513792911, -0.19231208163001284, 0.05204321807470751, 0.040147625073094606, 0.03857806796820766, 0.024107851014810378, -0.07715613593641532, -0.020803519276847076, 0.07016196559196623, 0.035934602159485365, 0.1743585537327018, -0.002252796689926734, -0.052538866475759116, -0.013816235000674242, -0.04124906759124352, 0.04210268486298748, -0.02260713367510305, 0.08101118370405735, -0.041166459873027965, 0.04058819957024423, 0.1127878431291103, 0.04997801256876895, -0.05127220642942082, 0.04683889417547847, -0.017981067986899465, -0.03753169517044416, 0.05515479637840957, -0.10254441285884164, 0.0242592995440847, 0.002197724459431374, -0.0682895135202889, -0.023474520991641226, -0.010071326673803038, -0.0794691708722754, 0.0690605251655756, 0.006870255420312497, -0.007407209144702683, -0.0771010662160299, 0.048628740620698524, 0.07357644304597522, 0.056944640731871325, 0.019468017373570862, 0.014869490310724898, 0.05909245813954202, -0.05193307235866182, -0.14836448269317817, 0.08987780821194204, -0.05424610311100533, 0.020362942687939173, -0.0454896285026829, -0.1381210503311512, -0.04997801256876895, -0.01047748338012113, 0.06013882675378893]}}, {'id': 'https://huggingface.co/papers/2410.13370', 'title': 'MagicTailor: Component-Controllable Personalization in Text-to-Image Diffusion Models', 'url': 'https://huggingface.co/papers/2410.13370', 'abstract': 'Recent advancements in text-to-image (T2I) diffusion models have enabled the creation of high-quality images from text prompts, but they still struggle to generate images with precise control over specific visual concepts. Existing approaches can replicate a given concept by learning from reference images, yet they lack the flexibility for fine-grained customization of the individual component within the concept. In this paper, we introduce component-controllable personalization, a novel task that pushes the boundaries of T2I models by allowing users to reconfigure specific components when personalizing visual concepts. This task is particularly challenging due to two primary obstacles: semantic pollution, where unwanted visual elements corrupt the personalized concept, and semantic imbalance, which causes disproportionate learning of the concept and component. To overcome these challenges, we design MagicTailor, an innovative framework that leverages Dynamic Masked Degradation (DM-Deg) to dynamically perturb undesired visual semantics and Dual-Stream Balancing (DS-Bal) to establish a balanced learning paradigm for desired visual semantics. Extensive comparisons, ablations, and analyses demonstrate that MagicTailor not only excels in this challenging task but also holds significant promise for practical applications, paving the way for more nuanced and creative image generation.', 'score': 23, 'issue_id': 192, 'pub_date': '2024-10-17', 'pub_date_ru': '17 октября', 'hash': 'fa9ce7d280c285b4', 'data': {'desc': 'Эта статья представляет новый подход к персонализации генерации изображений с помощью текстовых запросов. Авторы предлагают метод MagicTailor, который позволяет пользователям точно контролировать отдельные компоненты визуальных концепций. Система использует динамическое маскированное ухудшение (DM-Deg) для устранения нежелательных визуальных элементов и двухпоточную балансировку (DS-Bal) для сбалансированного обучения желаемой семантики. Эксперименты показывают, что MagicTailor превосходит существующие методы и открывает новые возможности для креативной генерации изображений.', 'emoji': '🎨', 'title': 'Точный контроль над компонентами в персонализированной генерации изображений', 'categories': ['#cv', '#diffusion', '#multimodal'], 'embedding': [0.050726294702759474, 0.0692375256054019, 0.06437487284005465, 0.035309478077254014, -0.03522659183401018, -0.08995905359256352, 0.051582782463800214, 0.11045954612710865, -0.09498747474142376, 0.07387914702110737, 0.02022420847621125, -0.040862848168641684, -0.045145297231282085, -0.077305102168245, 0.06735877349385808, -0.07736036513437383, 0.05498111228233547, -0.030004768767085205, 0.04097336384346258, 0.012446729866895623, -0.011161994942954768, -0.041746965361259485, -0.025307888488225617, 0.0797916894655601, 0.011541889369702626, 0.012274050022513685, 0.05923593396488616, 0.09460066680231961, 0.08802503851489088, 0.012474358067580278, 0.028236532330362264, -0.04387437620253483, -0.09780559675427748, -0.15659946596839272, -0.003156579062144222, 0.12211885442655175, 0.0033827889812937375, 0.08498588310090802, -0.041415422439771477, 0.03823811986790332, 0.003367247938903477, 0.0016931212275460676, 0.06498270392285121, 0.1605780076955842, 0.03821049248781361, -0.0024002434772981717, 0.013289404963635175, 0.10775193158312645, -0.028001688316419287, 0.06282766570148615, -0.03796183580956944, 0.03768554969974822, -0.07520531870705938, -0.03727111643204169, -0.09272191879375047, 0.050809180946003306, -0.08012323238704812, 0.01504382750516452, -0.07807870984050394, 0.019782149879902355, -0.005840016622604087, 0.005567182755688854, -0.031828264066962264, 0.051720929621685514, -0.11128840855954703, -0.03569628191338348, -0.1213452508572675, 0.03492267629261189, -0.03459113131963655, -0.013102911224059644, 0.06912700787909366, -0.009110563807623933, -0.11979804371869902, 0.008074487408265852, 0.006147385637800771, -0.013627856679058581, -0.05520214158048992, 0.06188828964571424, 0.04520055609443621, -0.06487218619654297, 0.0694032939889149, -0.06559053295416044, -0.0010386664021325121, 0.027642515963354226, -0.14190101046091616, 0.005960891898225241, 0.009207264766656299, -0.060506854993633424, -0.11217252575216481, -0.10156310713182719, -0.01708835210319603, 0.061280456511430326, -0.008274796273927382, 0.03973007019480503, 0.03453587655945712, -0.0322426932316941, -0.035558136806985524, -0.017544223363806108, 0.09940806685897478, 0.07365811977444027, 0.046692506421337925, -0.005328885678244946, -0.01826257012142358, 0.0049420840987515575, 0.11780877388084694, -0.05929119282804029, -0.052632676245880375, 0.009683859331841585, -0.06437487284005465, 0.0353923663719852, -0.08774875240506966, -0.08703040564745218, 0.027725400155110717, 0.07459748967575015, 0.044592720908664944, -0.18743310537386568, -0.052024843111596455, 0.05790975407298477, 0.02023802319199978, -0.046139930098720784, -0.03384515718192937, 0.01479516877543301, 0.016259489670757663, -0.007950158658846301, 0.03890121186534136, -0.1233345165921449, -0.00828860996397224, -0.12841820481010863, -0.013952493678693459, -0.08990378652346002, -0.0032101097523580028, 0.013379198359624542, 0.09404809868565185, -0.024244186144818963, -0.08631206299280941, -0.02353965205150268, -0.04252056893054373, 0.046913535719492375, -0.04453746204551083, 0.029700853225686916, -0.06780083619314166, -0.027808286398354552, 0.010312412078023482, 0.004558735787907833, -0.05282607406097042, -0.10769667887443436, 0.06979010192801904, 0.09719776772296826, -0.04904094245779302, 0.11825083658013053, -0.12720253443856613, -0.08863286959768744, 0.00015120238023451707, -0.0860357686770388, -0.050726294702759474, -0.11311189360198735, 0.030253427496816712, 0.00986344571352285, -0.017668553754415537, -0.09874497896451144, -0.0495382578657687, 0.002075606067365825, -0.018082982919147374, 0.012073742182595828, 0.003298176001150702, -0.014228780403960887, 0.018221128025545333, 0.07233193577956419, -0.121897829231372, -0.0791838522283015, 0.032380840389579404, -0.08249929580359293, -0.09719776772296826, -0.05614152173923653, 0.03931563897858585, 0.09824766355653576, -0.04696878842818446, -0.06791134981647522, -0.056169149119326245, -0.013593320710182195, -0.0488751720227927, 0.13582269963295043, -0.056058631393018005, 0.0049835268100760065, -0.03884595300218724, -0.0900143063012556, -0.028015503032207817, 0.004230644904177089, 0.0051976491606336595, -0.03370701412701876, -0.10018166837677168, 0.11902444425238946, 0.08272032920472207, -0.026136750920663983, 0.0033447995386189514, -0.009262521988620544, 0.09620313280404222, 0.02959033755086602, 0.020790597463129577, -0.06415383943892551, 0.0721109085328971, 0.0797916894655601, 0.001638727271457055, -0.009524994203248177, 0.04765950780571221, 0.03285052431449067, 0.013206518515242604, 0.02667551047600525, 0.0002872953140896178, -0.026261079259786064, 0.08078631617853677, -0.08415702066846965, 0.03312681247579924, -0.03221506380011704, -0.08144940817597479, -0.04497952679628176, -0.08995905359256352, -0.026440666462062265, 0.001883931911943963, 0.10990696980449151, -0.029838996280597527, 0.03373464150710847, 0.09310872262987994, 8.013402115900118e-05, -0.11349870154109151, -0.0024744954205998095, -0.0018908392698382282, -0.05141701408028723, -0.004724507658949302, -0.051499898272043716, 0.12101370998726685, -0.01250889383130793, 0.02206151849187684, -0.04589126931750193, 0.02439614186403076, -0.04017212879111393, 0.04047604843548691, -0.10344185924337103, 0.07890756816996762, -0.051444641460376944, -0.006896814275050026, 0.04964877559207694, 0.0023242645919486003, -0.05868335969375637, -0.09089842349387339, 0.07454223491557072, -0.05489822603909164, 0.04881991315963858, -0.005004248063163864, -0.035447621132164625, 0.07846550752217138, -0.0423547984955434, -0.08426753839477791, -0.003324077875233625, 0.01986503612314619, -0.020735340651462802]}}, {'id': 'https://huggingface.co/papers/2410.13276', 'title': 'SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs', 'url': 'https://huggingface.co/papers/2410.13276', 'abstract': 'Attention is the cornerstone of modern Large Language Models (LLMs). Yet its quadratic complexity limits the efficiency and scalability of LLMs, especially for those with a long-context window. A promising approach addressing this limitation is to leverage the sparsity in attention. However, existing sparsity-based solutions predominantly rely on predefined patterns or heuristics to approximate sparsity. This practice falls short to fully capture the dynamic nature of attention sparsity in language-based tasks. This paper argues that attention sparsity should be learned rather than predefined. To this end, we design SeerAttention, a new Attention mechanism that augments the conventional attention with a learnable gate that adaptively selects significant blocks in an attention map and deems the rest blocks sparse. Such block-level sparsity effectively balances accuracy and speedup. To enable efficient learning of the gating network, we develop a customized FlashAttention implementation that extracts the block-level ground truth of attention map with minimum overhead. SeerAttention not only applies to post-training, but also excels in long-context fine-tuning. Our results show that at post-training stages, SeerAttention significantly outperforms state-of-the-art static or heuristic-based sparse attention methods, while also being more versatile and flexible to adapt to varying context lengths and sparsity ratios. When applied to long-context fine-tuning with YaRN, SeerAttention can achieve a remarkable 90% sparsity ratio at a 32k context length with minimal perplexity loss, offering a 5.67x speedup over FlashAttention-2.', 'score': 13, 'issue_id': 195, 'pub_date': '2024-10-17', 'pub_date_ru': '17 октября', 'hash': 'd64d285d002a425d', 'data': {'desc': 'Статья представляет SeerAttention - новый механизм внимания для больших языковых моделей (LLM). Он использует обучаемые gates для адаптивного выбора значимых блоков в карте внимания, что позволяет эффективно балансировать точность и скорость работы. SeerAttention превосходит существующие методы разреженного внимания и может применяться как после обучения, так и при тонкой настройке на длинных контекстах. Результаты показывают значительное ускорение без существенной потери качества даже при 90% разреженности.', 'emoji': '🔍', 'title': 'SeerAttention: Эффективное обучаемое разреженное внимание для LLM', 'categories': ['#architecture'], 'embedding': [0.03830513517419914, 0.017385403877977067, 0.05739547936509148, 0.03362588318547307, 0.009246495445333396, -0.003210761616712097, 0.017198730583413903, 0.04241192073642937, -0.03613973696690677, 0.1393818983561049, -0.030415122397012037, -0.034521910029869915, -0.0540104876836183, 0.039574502395839366, 0.05535452795021356, -0.051173069343028295, 0.04990370005076042, -0.08965242919835663, 0.03168449375990756, -0.012712376195967996, -0.06421523647549725, -0.00686331421247051, 0.034596578933569655, 0.06526060112729357, 0.04808675534698172, -0.0736235142004088, 0.1087178844448916, 0.04796230648393961, 0.0884079488504461, -0.055702983524354883, 0.06326942967175803, -0.032555629589319394, -0.06316986768244562, -0.04666804617668672, 0.02495185002538021, 0.08870662032398975, 0.035318379026209656, -0.0059392868915461045, -0.05923731094259988, 0.0010305861060113171, -0.012768378391399712, -0.032232065030163085, -0.029867551126756514, 0.018953450855671544, 0.08915464202869877, -0.014062638698652602, 0.08323090762143454, 0.06745088827894329, 0.023483362966370484, 0.09786601067595532, -0.13709206370953633, 0.059983999979597254, -0.06989006487416664, 0.028772406515617823, 0.012040356062670364, -0.023284245199628635, -0.00647752501473038, 0.0987122575610098, 0.022400662827410472, 0.04943079768420463, 0.045821801362259905, 0.0013743740298906384, -0.017584521644718916, -0.02767726397510678, -0.07138344294816139, -0.0008050238565960478, -0.06984028905607957, 0.05948620452742881, -0.0032574296297587417, -0.016252925850302333, 0.06949183762319355, -0.0028903077614749278, 0.09423212333902557, -0.06242317864544826, -0.04870899552093698, 0.028647958687889538, 0.04104299359610439, 0.061228480327507766, -0.030514680245069137, 0.023284245199628635, 0.11021126044825869, -0.030937804722910193, 0.014921330055885751, -0.04577202140291753, -0.07332483858560986, -0.08323090762143454, -0.030937804722910193, -0.043780852018009644, 0.02431716330861862, 0.014162197167897995, 0.012818157315428262, 0.10100210463134428, 0.02118107142385732, -0.001092032250717431, 0.05425938540970252, 0.10149988973037447, 0.016290261337466028, -0.05286556311313723, 0.06944205352259587, 0.06630596163783459, 0.05084950685449963, -0.01078343082845564, 0.019662804405505222, -0.16924943912972315, 0.08925419780612823, -0.03701087486694625, -0.07795431061499354, -0.05231799184288172, -0.04012207573672254, -0.027378588360307834, -0.052417551761566455, -0.008593143262680377, -0.01167323460851208, 0.027503037223349947, 0.10752317991506816, -0.1450567433197955, -0.1423686545040944, 0.020397046899696257, 0.01393819066386155, 0.11100773151522607, 0.020882395809058374, 0.08089128473301299, 0.04119232933287621, 0.00815135228363406, 0.06486236973506516, -0.05201931829871041, 0.08447538589871742, -0.08109040249975483, 0.012905270898369445, 0.03260540747803412, 0.11897240491360231, 0.03870336863705518, 0.0909466936468648, 0.006701531518766825, -0.023147351864407844, -0.05973510225351303, -0.1074236241376387, -0.006732643734527354, -0.0954765995456808, 0.016016473631710618, 0.015717799052225496, 0.09482947042736818, 0.02400604322164099, 0.039425164588439886, -0.04218791609595781, -0.03850425294094098, 0.0005226821602479463, 0.09453079895382453, -0.051770420572626194, 0.12365166311421133, -0.056350110572039855, -0.10110165626751842, -0.11389492567390315, 0.043905298810424105, -0.06466324575644036, -0.05814216840208884, -0.09049868229529405, -0.0047010283655994, 0.02710480168986625, -0.059983999979597254, 0.05147174495782725, 0.010876766854548928, 0.0693922777045088, 0.006695309075614719, -0.023122461884736652, 0.03228184498950546, -0.14983554487406808, 0.025710983534556257, -0.09169337647197923, -0.03305342297086019, -0.02398115427728363, 0.008823373038119987, -0.029991999989798627, -0.09104625356554956, 0.10373996305324953, 0.02802571851393428, -0.012612818347910898, 0.055503865757613034, -0.03200805831906387, -0.018331210681716288, -0.02591010130129811, 0.08736258833990508, -0.05286556311313723, 0.10140033395294502, 0.00034242681749963174, -0.03720999056306045, -0.03272985841170388, 0.024802510147353088, 0.018978339800028905, -0.0268061260750673, -0.18139559598281646, 0.08766126395470404, 0.14694835278601864, 0.08567008628728555, -0.01188479601918155, 0.044875994558520683, 0.037931788585072804, 0.06705265481608724, 0.03489525661899625, -0.05535452795021356, -0.0027549702958706173, 0.04522444806203436, 0.04910723105442067, -0.08965242919835663, 0.1069258307560979, 0.034447241126170175, 0.035791281392765446, -0.004925034869635846, -0.04295949407731254, -0.08109040249975483, 0.08099084051044243, -0.0464191525918578, 0.12564282835786392, -0.030738689026795996, -0.020036148924003905, 0.03666241929280493, 0.041067880469834096, -0.012805711807935754, 0.04388040986606674, 0.07750629098091216, 0.08591898815462508, -0.0986126955716974, 0.0486592197028499, 0.1570037679117534, -0.07924856678099113, 0.015419122609175486, 0.08138907811455379, -0.026183885901112043, 0.03462146994855467, -0.10931523981574479, 0.013241280757955495, 0.023582920814427585, -0.0045205781353766334, -0.05331357860596331, -0.032555629589319394, -0.12026667143273814, 0.04514978122896227, -0.12086402059170841, 0.0884079488504461, 0.01524489544329312, 0.0701887487714762, -0.03852914188529835, -0.011175442469347876, -0.01189101908352195, 0.011206553649794579, 0.028299504149062037, -0.06526060112729357, -0.020098371284897307, 0.05084950685449963, 0.05829150206823302, 0.00680109040213775, -0.0637174410233288, -0.11041038028562818, 0.007703339482623933, -0.07611247489622983, -0.012482147248779446]}}, {'id': 'https://huggingface.co/papers/2410.11190', 'title': 'Mini-Omni2: Towards Open-source GPT-4o with Vision, Speech and Duplex Capabilities', 'url': 'https://huggingface.co/papers/2410.11190', 'abstract': 'GPT-4o, an all-encompassing model, represents a milestone in the development of large multi-modal language models. It can understand visual, auditory, and textual modalities, directly output audio, and support flexible duplex interaction. Models from the open-source community often achieve some functionalities of GPT-4o, such as visual understanding and voice chat. Nevertheless, training a unified model that incorporates all modalities is challenging due to the complexities of multi-modal data, intricate model architectures, and training processes. In this paper, we introduce Mini-Omni2, a visual-audio assistant capable of providing real-time, end-to-end voice responses to visoin and audio queries. By integrating pretrained visual and auditory encoders, Mini-Omni2 maintains performance in individual modalities. We propose a three-stage training process to align modalities, allowing the language model to handle multi-modal inputs and outputs after training on a limited dataset. For interaction, we introduce a command-based interruption mechanism, enabling more flexible interaction with users. To the best of our knowledge, Mini-Omni2 is one of the closest reproductions of GPT-4o, which have similar form of functionality, and we hope it can offer valuable insights for subsequent research.', 'score': 11, 'issue_id': 200, 'pub_date': '2024-10-15', 'pub_date_ru': '15 октября', 'hash': '41fce1c87f8f0f35', 'data': {'desc': 'Mini-Omni2 - это мультимодальная модель машинного обучения, способная обрабатывать визуальные, аудио и текстовые данные, а также генерировать голосовые ответы в реальном времени. Модель использует предобученные визуальные и аудио энкодеры для сохранения производительности в отдельных модальностях. Авторы предлагают трехэтапный процесс обучения для выравнивания модальностей и механизм прерывания на основе команд для более гибкого взаимодействия с пользователями. Mini-Omni2 представляет собой одну из ближайших репродукций функциональности GPT-4o, предоставляя ценные insights для дальнейших исследований в области мультимодальных языковых моделей.', 'emoji': '🤖', 'title': 'Mini-Omni2: Мультимодальный ассистент нового поколения', 'categories': ['#multimodal', '#nlp', '#audio', '#cv', '#architecture'], 'embedding': [-0.012843575522787965, 0.07607888491911792, 0.05741007979294646, -0.00257631823141949, -0.04171237542057835, -0.016820637858605922, 0.02414311393468581, 0.0953559413670564, -0.0123522910230446, 0.024657792189772335, 0.06208897746348787, -0.004637957621344819, -0.030740362093514356, -0.030599995652071138, -0.08365870012274106, -0.10181282112974961, -0.07130640636312749, -0.03913898219045361, 0.06517705090339126, 0.052029347960496866, -0.014703437126925312, -0.06517705090339126, -0.03083393711488699, -0.05385411756333499, 0.09966053308803087, -0.03871788286612397, -0.049502742241058446, 0.1122935538664545, 0.008527292201810649, -0.0014358368875745436, 0.04735044833526333, -0.030810543359543835, -0.11678529953955849, -0.02493852702735088, 0.0018276946041328641, -0.01783829846845345, -0.005819379405324783, 0.0303894440352142, -0.010743920303083937, 0.0635394372073748, 0.046742191149419916, -0.01827109760249285, 0.01790848266652112, 0.0776697071950638, 0.04236742402649234, 0.02599127924755921, 0.06494310944057542, 0.06976236964317578, -0.06115319597468746, 0.02223646227876238, -0.007919034429559596, 0.08248897521643267, 0.01621238262745463, 0.028400912280897587, -0.03778210333201569, -0.08777612616343325, 0.02300848161608429, -0.050578890171302064, 0.001393434396371342, 0.005611753571639436, -0.10920548238124322, -0.06311833788304518, 0.05731650281688169, 0.07547062382389025, -0.10677245168317745, -0.04978347707863699, -0.06803118288047881, 0.03396880002016883, -0.047958707475798876, 0.0016054468914811924, -0.030693572628135907, -0.021335775332360963, -0.11556878125848742, -0.03941971702803215, 0.0037372698930665514, 0.006837040015172313, -0.08440731984064344, 0.0512339348678318, -0.058907327047519706, -0.1005963087127549, 0.07069815308666831, -0.031442196255422536, -0.014843804936653009, -0.05806512644416832, 0.010141511556076314, 0.06877980064368905, -0.05558531019010834, 0.030740362093514356, -0.0637733825795749, -0.012738299909828705, -0.049877056009393884, -0.045736230349282155, -0.07579814812684724, 0.007410205004247287, 0.11893758758127723, -0.021207105768589328, 0.008363530050332153, -0.11294860051767636, 0.06307154841766673, 0.09806969908393223, -0.038577514469988645, 0.019780041734737686, -0.06587888506529944, -0.013498621001194549, 0.05956237174404944, -0.016598391318769525, 0.0768275124557888, -0.03979403079636759, -0.03532568083329887, -0.011621213494142196, -0.14963115434533678, -0.016025226720632973, 0.03155916894152259, -0.08010273593843746, 0.044028431477851966, -0.12258712874164561, 0.011416511097997894, -0.06045136181277928, -0.04246100295724923, -0.1201541039076562, -0.050344944799101954, 0.11285502354161159, 0.04470687188441698, -0.06321191290441781, 0.07345870222361472, -0.07542384217728029, -0.012902062061307201, -0.004108657682761182, 0.0165399049757195, -0.01862201370610088, -0.10667888057118906, 0.08997521344399105, 0.005910033139317716, -0.03974724133098915, -0.05946879281329256, -0.03686971950794269, -0.11416511293467105, -0.060357784836714515, -0.005392429882936447, 0.011235204802827302, -0.028798617849884058, -0.1023742908049067, -0.004094035803794857, 0.04959632117181534, -0.06339907076593158, 0.008889906551374744, -0.026131645689002423, -0.002118663427797194, -0.17966969009571143, 0.005269608758000606, -0.03518531243716354, -0.06363301222874745, -0.039185771655832057, 0.010656190593039688, -0.03927934863189682, 0.007761122671609015, -0.043186228919808445, 0.11659814363273684, 0.03366467338193925, -0.1329742845022857, 0.02258738033706253, -0.019300456067358026, 0.005705331525050265, -0.02337109655205602, -0.0696687907124189, 0.11388437809709251, 0.007878094575832216, 0.1324128187365129, -0.09956695415727398, -0.10377795912872306, 0.04150182575841353, 0.058954116512898155, -0.1586146456915448, 0.08609172300203834, 0.02076261073422441, -0.08085135761103196, -0.02802659851256215, -0.04912843042741513, -0.12258712874164561, -0.045408706046325166, -0.07954127017266462, -0.0010366684270022729, 0.003140710366771567, -0.011241052655255454, -0.0834247508411567, -0.008451259955845615, -0.031395406790044086, -0.002640653013305306, -0.010059631457683127, 0.0017107221135020257, -0.019358940455715925, 0.05305870447066992, 0.15814675103776032, 0.11772107907366679, 0.09170640216138017, -0.046297696115055, 0.059281640815855144, -0.07303759898990084, 0.03722063561155073, -0.043817881815687146, 0.06255686429850384, 0.08946053127952028, 0.012749997373907923, -0.19351922329112978, 0.005620526229893122, -0.025219261864929426, 0.10068988568881966, -0.0244706382376428, -0.08543668026020074, 0.04681237827952577, 0.06185503013659564, -0.020879581465632336, 0.06541099627559137, 0.04019173441066194, -0.11135777433234621, -0.0730843904099714, -0.11107704144945979, 0.028003204757218986, -0.021172013180882465, -0.04196971845750587, 0.006591397765300631, 0.0464146727105393, 0.07121282938706272, 0.011393116560777884, -0.04788852034569301, 0.03803944245955896, 0.012516052001707821, -0.05979631907094168, 0.08992842006922835, -0.07472200019660362, 0.06208897746348787, -0.09133209034773686, -0.02304357420379115, 0.049643110637193774, 0.014913987570966979, -0.049877056009393884, 0.11809538893261798, -0.04016834065531878, -0.04566604517386843, -0.08305044489158976, -0.00814128233768048, 0.05497705495895705, 0.07593851847767469, 0.006620640936825644, -0.0428587085262357, 0.028283939594797535, -0.011527634563385306, -0.11491374047134192, 0.06157430311778559, -0.002836581969319072, 0.04021512816600511, -0.030740362093514356, 0.02812017744331904, 0.03492797330962028, -0.03560641567087742, 0.037735311911945124]}}, {'id': 'https://huggingface.co/papers/2410.13925', 'title': 'FiTv2: Scalable and Improved Flexible Vision Transformer for Diffusion Model', 'url': 'https://huggingface.co/papers/2410.13925', 'abstract': 'Nature is infinitely resolution-free. In the context of this reality, existing diffusion models, such as Diffusion Transformers, often face challenges when processing image resolutions outside of their trained domain. To address this limitation, we conceptualize images as sequences of tokens with dynamic sizes, rather than traditional methods that perceive images as fixed-resolution grids. This perspective enables a flexible training strategy that seamlessly accommodates various aspect ratios during both training and inference, thus promoting resolution generalization and eliminating biases introduced by image cropping. On this basis, we present the Flexible Vision Transformer (FiT), a transformer architecture specifically designed for generating images with unrestricted resolutions and aspect ratios. We further upgrade the FiT to FiTv2 with several innovative designs, includingthe Query-Key vector normalization, the AdaLN-LoRA module, a rectified flow scheduler, and a Logit-Normal sampler. Enhanced by a meticulously adjusted network structure, FiTv2 exhibits 2times convergence speed of FiT. When incorporating advanced training-free extrapolation techniques, FiTv2 demonstrates remarkable adaptability in both resolution extrapolation and diverse resolution generation. Additionally, our exploration of the scalability of the FiTv2 model reveals that larger models exhibit better computational efficiency. Furthermore, we introduce an efficient post-training strategy to adapt a pre-trained model for the high-resolution generation. Comprehensive experiments demonstrate the exceptional performance of FiTv2 across a broad range of resolutions. We have released all the codes and models at https://github.com/whlzy/FiT to promote the exploration of diffusion transformer models for arbitrary-resolution image generation.', 'score': 9, 'issue_id': 201, 'pub_date': '2024-10-17', 'pub_date_ru': '17 октября', 'hash': '2109b51982de79e7', 'data': {'desc': 'Эта статья представляет Flexible Vision Transformer (FiT) - новую архитектуру трансформера для генерации изображений с произвольным разрешением и соотношением сторон. FiT рассматривает изображения как последовательности токенов динамического размера, что позволяет гибко обучать модель на изображениях различных пропорций. Усовершенствованная версия FiTv2 включает ряд инновационных техник, таких как нормализация векторов запросов-ключей и модуль AdaLN-LoRA, что значительно улучшает производительность. Эксперименты показывают исключительную эффективность FiTv2 при генерации изображений широкого диапазона разрешений.', 'emoji': '🖼️', 'title': 'Гибкая генерация изображений любого разрешения', 'categories': ['#architecture', '#diffusion', '#code'], 'embedding': [0.009176561589810029, -0.012189965020256918, 0.002594117122948623, -0.049087105288631715, -0.018694005608348614, -0.06490406294755798, -0.033542856500476934, 0.03768799034716257, -0.16449635003295612, 0.11562740682187539, 0.025934355755840348, 0.008365260907529433, -0.09653798035668805, -0.031497559836827976, 0.03141574905774636, -0.0198802786352527, 0.03147028957713411, 0.005597293996882554, -0.0002629057770564671, 0.07335795219909248, 0.025116237508636285, -0.147697650477385, 0.026193426359848504, -0.08301174960737802, 0.019512125946830265, -0.0594499419966234, -0.008760685110412286, 0.10771893322060622, 0.049032564769243975, 0.017044133984093556, 0.019089430647742506, -0.05246866140750103, -0.07161263675718646, -0.102864762165492, 0.019962089414334312, 0.01523063871039768, -0.011303669869051626, 0.05345040330414592, -0.026152519924668903, 0.05934085886657034, 0.10979149387011629, -0.02007117254438738, -0.06163159205001929, 0.06539494016971316, -0.00157573025019516, -0.0074380599485278165, 0.031797537921654524, 0.0719944266667334, -0.11660914662724269, 0.1097369596245613, -0.0715035515358558, 0.07466694766845174, -0.10651902088130004, 0.005055290265995314, -0.05233231010903168, 0.016716888776489514, -0.06386778262280293, -0.04554192761159916, -0.09370182943169617, 0.03024311116068922, 0.04330573703881553, -0.04600552830022773, 0.12359042721636505, -0.005788188115144996, -0.04246035062319518, 0.10504640803633272, -0.10428283239979398, 0.04712362149534196, 0.016771427204599672, 0.03907879032177069, 0.01346486639116608, -0.06692208516895788, 0.007969837332029854, -0.0686128684565865, 0.03466095387814633, -0.03024311116068922, -0.007213077796520276, 0.07968473609917402, -0.0030952146277840216, -0.08617514364869636, 0.04581463439109305, -0.016621438685005797, -0.0977378864221615, -0.004509878170901859, 0.03297017477307288, 0.048514425652505244, -0.02335728170868936, -0.10008315385116544, -0.040524132907044125, -0.08077556112587196, 0.007417607253757412, 0.02046659758378127, -0.079193863059574, 0.0935382120560881, -0.05241412297939088, 0.0041076364030955366, 0.048105365483264426, -0.07853937264436592, 0.023616352312697517, 0.07188533935412517, -0.028988664575828046, 0.04600552830022773, 0.03367920989022387, -0.01570787452887318, 0.06872195367791716, 0.015994215601702968, 0.0017538413714877727, -0.06206792038767641, -0.007022183260002317, 0.11584557308198154, -0.08873858361163921, -0.14551598787632364, 0.02168013877910164, 0.0019583710378526676, 0.053723110083639794, -0.15118829600014017, -0.05388673791563578, 0.026588848262326013, -0.044696537013423646, -0.019075795517895566, -0.0378243458281871, 0.029615888913897426, 0.02189830294793019, 0.08808408901387595, 0.07575776433003935, -0.07243075082183535, 0.03962420388075845, -0.017671358322162937, -1.382172743540208e-05, -0.04393295928560732, 0.05759553715083156, 0.006521085964294677, 0.07450332192773333, 0.002716834964593112, -0.05890452843763564, -0.06697662777962321, -0.1239176703326915, 0.02737969834111379, -0.053668569564252054, -0.005604111561806021, -0.008985667680675343, -0.040851380205925755, -0.069212822534962, 0.028279629458677055, 0.006824471576816407, -0.009244738493811258, -0.005392764330517659, 0.03932422475029312, -0.05462304329248064, 0.06757658185799871, -0.06441318990795794, -0.036706246359240115, -0.056450173696023453, -0.048296261483676696, -0.05715920881317445, -0.035397259254991195, 0.06266787237477434, 0.09233829971678191, 0.05151418977055003, -0.023043670585293462, -0.1324261084687241, 0.07761217335838637, -0.12086336778653657, 0.06037714128260296, 0.001970301985596495, -0.01562606228589726, -0.031742996356627994, 0.03605175385275445, -0.18184045478240468, -0.06299511967365595, -0.012244506376155696, 0.001050770912610435, -0.06975823609394978, -0.07848483212497817, 0.07641226311035776, 0.031606642966881045, -0.07046727121110076, -0.06354053323264372, -0.06168612838685188, -0.1006285674101532, -0.09391999778307988, 0.06277695550482741, -0.11213677175550817, -0.03697895313873399, 0.035560882904432004, -0.07439423670640269, -0.11628190141963865, 0.08170276459440669, 0.05451396016242757, 0.027543321990554603, -0.17911340790024172, 0.09462903499150846, 0.08464798819306371, 0.02277096380579958, -0.010983239298582435, 0.059177237308407106, -0.011146864202789798, 0.0016498722516383373, 0.006613124345528044, 0.06343145010259066, 0.042951217388962445, 0.05906815208707645, -0.031088499667587158, -0.07215804822489663, -0.03040673481013003, -0.10766438642738574, -0.009803785927879408, -0.0022378949640145013, -0.04897802634113381, -0.027011543561413776, 0.10640993356869181, 0.07226713344622729, 0.07390336575808025, -0.07619409475897404, -0.0709035953662027, 0.004598507560545733, -0.054541230422121445, 0.007915296185258835, -0.014685226615304223, 0.07821212116292911, -0.0823027165815046, 0.0002795238006851784, 0.03706076810037078, -0.009435633239456974, -0.12031795422754879, 0.03313379842251369, -0.023207292143456692, -0.07112176580886401, -0.04739632827483584, -0.02263461041605264, 0.02836144023775867, 0.0009970818778037471, -0.09670159564101853, -0.08841133213020241, 0.013894379255177306, -0.03507001195610957, 0.017589545451803735, -0.03024311116068922, 0.03716984704786868, -0.047778118184382795, -0.04540557213057463, 0.07657589303363133, -0.012442217641086087, -0.015544250042921336, -0.05623200743591731, -0.00020729929165866592, -0.06790382497432518, -0.084375285596125, 0.14737040736105855, 0.021202902960626133, 0.04393295928560732, -0.023984507092397533, -0.07281253864010472, -0.09593802836959013, 0.02838871049745254, -0.03624264776188913]}}, {'id': 'https://huggingface.co/papers/2410.14677', 'title': 'Are AI Detectors Good Enough? A Survey on Quality of Datasets With Machine-Generated Texts', 'url': 'https://huggingface.co/papers/2410.14677', 'abstract': 'The rapid development of autoregressive Large Language Models (LLMs) has significantly improved the quality of generated texts, necessitating reliable machine-generated text detectors. A huge number of detectors and collections with AI fragments have emerged, and several detection methods even showed recognition quality up to 99.9% according to the target metrics in such collections. However, the quality of such detectors tends to drop dramatically in the wild, posing a question: Are detectors actually highly trustworthy or do their high benchmark scores come from the poor quality of evaluation datasets? In this paper, we emphasise the need for robust and qualitative methods for evaluating generated data to be secure against bias and low generalising ability of future model. We present a systematic review of datasets from competitions dedicated to AI-generated content detection and propose methods for evaluating the quality of datasets containing AI-generated fragments. In addition, we discuss the possibility of using high-quality generated data to achieve two goals: improving the training of detection models and improving the training datasets themselves. Our contribution aims to facilitate a better understanding of the dynamics between human and machine text, which will ultimately support the integrity of information in an increasingly automated world.', 'score': 7, 'issue_id': 195, 'pub_date': '2024-10-18', 'pub_date_ru': '18 октября', 'hash': '1af0065bfe14f3cb', 'data': {'desc': 'Статья посвящена проблеме обнаружения текстов, сгенерированных большими языковыми моделями (LLM). Авторы подчеркивают необходимость разработки надежных методов оценки качества датасетов с AI-сгенерированными фрагментами. Они представляют систематический обзор существующих датасетов и предлагают методы оценки их качества. Также обсуждается возможность использования высококачественных сгенерированных данных для улучшения детекторов и самих обучающих датасетов.', 'emoji': '🕵️', 'title': 'Улучшение детекторов AI-текста через повышение качества датасетов', 'categories': ['#dataset', '#benchmark', '#nlp', '#interpretability'], 'embedding': [0.11274634942677404, 0.03510122908309008, 0.08931200352837197, -0.026703083456118465, 0.002448411051437811, -0.08393115988231935, -0.021271947837924878, -0.013477262607137058, -0.005519140056937828, 0.11767460506086103, 0.056825769348236486, -0.07809771497853342, -0.09977197210205063, -0.03449776736232952, 0.003617614078895058, -0.005396561956997874, -0.03819395908789476, -0.0555182752082011, -0.053858759339458454, -0.01951185906304003, 0.05310443715567062, 0.06839207521152642, 0.027733992721844045, -0.10449906867808557, 0.035578968600196055, -0.0325365239446485, 0.050590023989369994, 0.06170373521781047, 0.037741364453045295, 0.01721116746159397, -0.0020822496865264464, -0.05159578837283907, -0.04445484932901776, 0.014985910506917433, 0.010629687698398314, 0.12300516556528447, 0.03565439883170969, 0.021611392710248006, -0.033994885170594985, -0.06371526619237655, 0.08262366794991191, -0.04023063273708942, 0.00018357190174900074, 0.08031040501115157, -0.13416916214298208, -0.009862791528142666, 0.12511726944599277, -0.07950578864759483, 0.02509385735188883, 0.09876620551095362, -0.19763299055093325, 0.03962717322395679, 0.01321324929090433, -0.06567650961005757, 0.004356223250044708, -0.024377250283857815, -0.020831925644203667, 0.03997919274503611, -0.043097065424469366, -0.019914163933376482, -0.020530195887637357, 0.00822842186623329, -0.009957081966688241, -0.0532050144770687, -0.049257380551822216, -0.09605063770185683, -0.05928989716527998, -0.03635843566624039, -0.0065437637643876645, -0.00442851268928562, 0.08805479915284961, -0.02934321882607368, 0.025395587108455138, -0.03713790273228473, -0.11847921480153394, -0.003526466640250149, 0.040155202505575784, 0.10490137354842204, -0.06839207521152642, 0.003592470079689728, 0.027809425160985623, 0.08383058035329331, -0.08478605938750526, -0.03721333517142631, -0.08589240109237242, -0.026049336386100776, -0.0265270736955788, 0.021799974911915924, -0.15327872075094154, -0.041336974441956574, -0.09248017259520205, 0.08760220893325601, 0.0052551262991795125, 0.013162960850968084, -0.006644340423497366, -0.014080722120269678, 0.022051414903969217, 0.015425933804452617, 0.11596479721997743, 0.026074479060729336, -0.1291403402256367, 0.07447695569210976, 0.021045650520500146, 0.010661117918167769, 0.05255126740705102, -0.10072745113626258, 0.017600903202244084, -0.024930420032477422, -0.0823722279578586, 0.02213942199186699, -0.06763775302773858, -0.0074363812552467245, -0.15649717737465668, 0.003592470079689728, 0.0646707452265885, -0.11898209920089642, -0.015564226462370313, 0.03985346833375358, 0.011597736966110569, 0.07075562349954388, -0.041387264206469584, 0.05783153814696144, 0.042870768107044634, -0.0023226906138855744, 0.07945549888308182, -0.035226949079116734, 0.010774266356117342, -0.11777517796700324, 0.001449717126282891, -0.0247544124795657, 0.04699440958520283, 0.09383795649975046, 0.01844323026630175, 0.0027438550289655833, -0.06924697471671235, 0.00022826169788171897, -0.0280860111391094, 0.03148047200429436, -0.09831360866847622, 0.03537781174977194, 0.01610482553596402, -0.0426193259073634, -0.125318421881161, 0.11908267872992244, -0.08684787791895643, -0.11314865871236637, -0.01660770905227532, -0.01801578051370879, -0.05863615009526229, 0.14915507927278332, -0.10449906867808557, -0.016771145709398347, -0.11214289212126936, 0.09046863720538009, -0.023094898818450994, -0.02702995588731334, -0.039551742992443156, 0.03708761296777172, -0.0023996944425960878, -0.08428317278051485, -0.0048716785451958814, 0.0984644801696432, -0.003658473077603719, 0.05556856276508617, 0.018166645391991944, 0.10781809820794294, -0.06336324887892517, -0.0937876645276095, -0.05159578837283907, 0.027809425160985623, -0.0040104912741062764, 0.0011181287602788543, -0.026124766617614408, -0.08951316258642403, -0.08855768355221208, -0.10198465551178496, -0.09896735573849391, 0.026024191503844267, 0.033743442970913744, -0.07538214937706457, -0.04297134542844272, -0.018003208072580535, -0.058133267903527755, 0.02655221857783531, -0.010057658405035148, -0.08332770478444261, 0.015463649361735022, 0.022654876624729783, -0.06230719693857103, 0.045913212762592186, -0.0006458902325544089, 0.010265097723055885, 0.09519574261192679, 0.023409201016145558, 0.08981488792773445, -0.010969133674535413, 0.08252308621325793, 0.11787575749602926, 0.01566480245919163, -0.05587029472928043, 0.10993021091916298, 0.05131920129090132, -0.06839207521152642, -0.1022360977114662, 0.002393408442794755, 0.05818355325278488, 0.06814063742710107, -0.005273984740109099, -0.04483201373235359, -0.08765249869776903, 0.08513808111621253, 0.014105866119475008, 0.09398881696277772, -0.030851867608905233, -0.02715567809096793, -0.01372870392376712, 0.02758312784356089, 0.07291802376764901, 0.05747951862588212, 0.04370052493760198, -0.017927775633438957, 0.053808473990201326, 0.014646465634594305, -0.005996879132518212, -0.10309099280139622, -0.03879741860102739, 0.04075865981108046, 0.0072352279370273215, 0.08176875961421422, -0.10530368724927024, -0.001973815417283682, -0.042317598358425024, -0.07578445203977309, -0.01678371704671263, -0.025156717349902154, -0.016444271732863912, 0.05154549860832606, -0.03055013785233892, 0.010101661286695653, 0.0724654313404275, 0.044957733728380245, 0.031178741143914078, 0.035302381518258305, 0.03754021201787707, 0.028362597117233174, 0.05121862396950324, -0.06844236497603944, -0.04189014860583206, 0.15740235781384385, -0.020278753687956116, -0.006883210182050352, -0.06527420473972112, 0.07055447106437567, -0.0453851834809732, -0.05224953544285676, 0.03376858785317025]}}, {'id': 'https://huggingface.co/papers/2410.13674', 'title': 'Diffusion Curriculum: Synthetic-to-Real Generative Curriculum Learning via Image-Guided Diffusion', 'url': 'https://huggingface.co/papers/2410.13674', 'abstract': 'Low-quality or scarce data has posed significant challenges for training deep neural networks in practice. While classical data augmentation cannot contribute very different new data, diffusion models opens up a new door to build self-evolving AI by generating high-quality and diverse synthetic data through text-guided prompts. However, text-only guidance cannot control synthetic images\' proximity to the original images, resulting in out-of-distribution data detrimental to the model performance. To overcome the limitation, we study image guidance to achieve a spectrum of interpolations between synthetic and real images. With stronger image guidance, the generated images are similar to the training data but hard to learn. While with weaker image guidance, the synthetic images will be easier for model but contribute to a larger distribution gap with the original data. The generated full spectrum of data enables us to build a novel "Diffusion Curriculum (DisCL)". DisCL adjusts the image guidance level of image synthesis for each training stage: It identifies and focuses on hard samples for the model and assesses the most effective guidance level of synthetic images to improve hard data learning. We apply DisCL to two challenging tasks: long-tail (LT) classification and learning from low-quality data. It focuses on lower-guidance images of high-quality to learn prototypical features as a warm-up of learning higher-guidance images that might be weak on diversity or quality. Extensive experiments showcase a gain of 2.7% and 2.1% in OOD and ID macro-accuracy when applying DisCL to iWildCam dataset. On ImageNet-LT, DisCL improves the base model\'s tail-class accuracy from 4.4% to 23.64% and leads to a 4.02% improvement in all-class accuracy.', 'score': 7, 'issue_id': 193, 'pub_date': '2024-10-17', 'pub_date_ru': '17 октября', 'hash': '61d2689e00439256', 'data': {'desc': "Эта статья представляет новый метод под названием 'Diffusion Curriculum (DisCL)' для улучшения обучения нейронных сетей на ограниченных или некачественных данных. DisCL использует диффузионные модели для генерации синтетических данных, контролируя их близость к исходным изображениям с помощью управления изображениями. Метод адаптивно регулирует уровень управления изображениями на разных этапах обучения, фокусируясь на сложных примерах и определяя наиболее эффективный уровень синтеза. DisCL показал значительные улучшения в задачах классификации с длинным хвостом и обучении на данных низкого качества.", 'emoji': '🔄', 'title': 'Адаптивное обучение на синтетических данных для улучшения глубоких нейронных сетей', 'categories': ['#dataset', '#data', '#diffusion'], 'embedding': [0.03201978526322789, 0.0545290125606782, 0.08098961682678105, 0.014769977216304457, 0.02757789152946108, -0.022972492900207718, 0.017236183505072326, 0.014851730287644123, -0.13320230968829072, 0.08616728155090998, 0.055482792849895254, 0.005773780470905206, -0.12600807350033627, -0.056899840820524364, 0.0798995828044343, 0.0689992301090391, 0.06829070612372455, 0.006461865460855721, 0.006107603676063907, 0.08044459981560767, -0.030439233436439558, -0.002457688793506733, -0.00688084740511957, -0.012882854178176143, -0.005634119822817256, -0.08927389327304613, 0.06654664836212236, 0.07771951268108619, 0.056409324055410076, 0.027482512461212052, 0.060714966134701856, -0.003033363674129769, -0.09090894222791158, -0.0742313971578818, 0.05989743957861448, 0.07624796654305264, 0.020165651239288325, 0.16568536263293068, -0.07068878762458222, 0.006874035030261282, 0.0516949228553839, -0.124264021974698, 0.022550104040985598, 0.022686358813442605, -0.01956613294272855, -0.012460465526819488, 0.031120507298724568, 0.051613169368313304, 0.03907776148208873, 0.10491589417351509, -0.11815981773043566, 0.10954854188647067, -0.027305381984547072, -0.035262640325220515, -0.06643764787000417, 0.02043816182352965, -0.01843522217684652, -0.03891426074391147, -0.07864604388660101, -0.051640420530670174, 0.07221483193002022, -0.02075154603332431, 0.07815552296417747, 0.011363617674556218, -0.008502275351846809, -0.035453396383063926, -0.06398506820174206, 0.024157905990803478, -0.007146543535847965, 0.0462174932666748, 0.015001610433414093, -0.04728027820531931, -0.06082396662682003, -0.0651841089521709, -0.054610763969094145, 0.016050769375149244, -0.03785147580526696, 0.020492663108916058, 0.06044245451113321, -0.033954597004018865, 0.06758218421706463, 0.0848592465443269, -0.13494636329258367, -0.02534331908139924, -0.05608231426443699, -0.03580765941504853, -0.06447558496685635, -0.05978843285053238, -0.060387952186419486, -0.15216892329513218, 0.006659433685691954, -0.026474231925935916, -0.046299246753745395, 0.00010064729787725337, -0.023463007586667358, 0.014252210328160634, 0.03485387704717683, -0.025193439767091125, 0.1077499838788094, 0.048343063143963826, -0.08115312172226757, 0.04033130247857665, 0.04589048555435635, -0.06916273500452565, 0.12262896262655934, -0.1258990730082181, -0.045154708328030285, 0.017672197945472878, -0.026964746612395552, 0.0654566143397756, -0.008999603868222985, -0.09368852752983751, 0.02584746142769195, 0.04787979754120641, 0.07117929815373257, -0.09728565185977867, -0.042865637712564, -0.06867221823941137, 0.014933483151118325, -0.022863490329434903, -0.13810747733943352, 0.07085229459872341, 0.07821003152485512, -0.022318473318261534, 0.030793495429096835, -0.07145181185595587, 0.10093726604250569, -0.11587075127227865, -0.08693031409690219, -0.09657712579580947, 0.02279536242354274, 0.009973822944938614, 0.07057978297515476, -0.06681916414300029, -0.07237833890416141, -0.037415461364866405, -0.0922714795592084, 0.04417367687645637, -0.05869839882818565, 0.008515900101563384, -0.006846783867904417, 0.022263968914893163, -0.06109647617173404, 0.11456270171511308, 0.03501738194266338, -0.0015328620847983579, 0.0009938055742378864, 0.0171408044368233, 0.009544620879396351, 0.16655737904180393, -0.05057763559202566, -0.014442967425331365, -0.019089241758792702, -0.07248734355358886, -0.022100465058733938, 0.01708630419076421, -0.07488542089713726, 0.07036177367629984, 0.014088705432674088, -0.10725946503504045, 0.02467567183961998, 0.07248734355358886, -0.04733478053003304, 0.02379001945629509, -0.01258990730082181, 0.015941765765049108, -0.01937537688488514, 0.005770374387408794, -0.15805509953264782, -0.06251352206370851, -0.043056391691752766, -0.03357308696698669, -0.006223419868887796, -0.0788095508607422, 0.0459449837217608, -0.020955927672346154, -0.030793495429096835, -0.05414750044499138, -0.04120333135937775, -0.12371899664890607, -0.030030471197723195, 0.05907990886521783, -0.07891855551016967, 0.03706119833288181, -0.051776676342454496, -0.05749936015641145, -0.07314136313553506, 0.03869625144505655, -0.06207750762330796, 0.04033130247857665, -0.0666556571688591, 0.029376450576449692, 0.09101794064137513, 0.051585916127301804, 0.030439233436439558, 0.06758218421706463, 0.08164364056603651, 0.0437376665933651, 0.07232383657944767, -0.005726091560377086, 0.06616513624643552, 0.09036392313808359, -0.076520474009312, -0.09952021807187658, 0.039813538708414795, -0.05221268662554576, -0.027809524330839788, -0.0613689836379934, -0.13440134004544635, 0.041148831113318664, 0.03959552940955987, -0.04717127563454649, 0.04580873206728575, 0.06453008729157007, -0.09423354246235624, -0.008624903503798059, 0.01640502928915188, -0.024989058128069282, 0.0437376665933651, 0.09352502055569632, -0.030902500078524298, -0.009265299583220452, 0.09041842130548804, -0.05297571293557404, -0.09554159201952182, -0.07395889177027706, -0.03572590592797793, -0.04703502190141681, 0.011574811688436349, -0.09341602214223281, 0.10965754861455278, 0.07275985101984823, -0.01685466931073086, -0.09129045226494378, 0.025193439767091125, -0.08436872562190334, 0.020165651239288325, -0.014306712652874363, 0.07428590363990481, -0.015614754726883229, 0.004165978521087328, 0.08360570346918433, 0.018203589375467812, 0.028286413436120993, -0.014442967425331365, 0.03597116431053507, -0.05319371807711969, -0.06267702903784969, 0.029349198374765507, 0.060278949615646664, 0.04657175422000476, -0.04142134065823267, -0.03648892808069693, -0.1471547593091805, -0.03518088891680456, 0.0036822753780241203]}}, {'id': 'https://huggingface.co/papers/2410.13726', 'title': 'DAWN: Dynamic Frame Avatar with Non-autoregressive Diffusion Framework for Talking Head Video Generation', 'url': 'https://huggingface.co/papers/2410.13726', 'abstract': 'Talking head generation intends to produce vivid and realistic talking head videos from a single portrait and speech audio clip. Although significant progress has been made in diffusion-based talking head generation, almost all methods rely on autoregressive strategies, which suffer from limited context utilization beyond the current generation step, error accumulation, and slower generation speed. To address these challenges, we present DAWN (Dynamic frame Avatar With Non-autoregressive diffusion), a framework that enables all-at-once generation of dynamic-length video sequences. Specifically, it consists of two main components: (1) audio-driven holistic facial dynamics generation in the latent motion space, and (2) audio-driven head pose and blink generation. Extensive experiments demonstrate that our method generates authentic and vivid videos with precise lip motions, and natural pose/blink movements. Additionally, with a high generation speed, DAWN possesses strong extrapolation capabilities, ensuring the stable production of high-quality long videos. These results highlight the considerable promise and potential impact of DAWN in the field of talking head video generation. Furthermore, we hope that DAWN sparks further exploration of non-autoregressive approaches in diffusion models. Our code will be publicly at https://github.com/Hanbo-Cheng/DAWN-pytorch.', 'score': 5, 'issue_id': 196, 'pub_date': '2024-10-17', 'pub_date_ru': '17 октября', 'hash': '87d3e3c2fd305ae3', 'data': {'desc': 'DAWN - это новый подход к генерации видео с говорящей головой, использующий неавторегрессивные диффузионные модели. В отличие от существующих методов, DAWN генерирует все кадры видео одновременно, что позволяет избежать накопления ошибок и ускорить процесс. Система состоит из двух основных компонентов: генерации общей динамики лица и генерации движений головы и моргания. Эксперименты показывают, что DAWN создает реалистичные видео с точной синхронизацией губ и естественными движениями.', 'emoji': '🗣️', 'title': 'DAWN: Революция в генерации видео с говорящей головой', 'categories': ['#video', '#diffusion', '#code'], 'embedding': [0.05712534629846235, 0.015355226512469363, 0.08702689782132775, 0.040891481787571536, -0.05433602311576975, 0.010348390160953092, 0.06588381972540482, 0.02648462549889574, -0.01584335742879417, 0.0015506895277172111, 0.07771054950750433, -0.037990587898478756, -0.10526907348620589, -0.11804416922112296, -0.048143722916767516, 0.014741574814340349, 0.012098691226868385, -0.05734849411772673, 0.09695688502035214, -0.04607962427409211, -0.03218879371382917, 0.0012107407648265498, -0.016247809247574838, 0.01640122270597909, 0.10783525098512213, 0.028478994051687044, -0.013807151462718806, 0.052690319960815024, -0.003967812739897354, -0.11073614700970291, -0.06303871332500012, -0.039831538721889784, 0.008444676917926348, -0.09757053244750515, -0.022216961627312695, 0.10906254797491613, -0.026763558671360203, 0.05294136152442345, -0.08507436988505253, -0.008374944051907834, 0.06766898733110376, -0.18710782882101307, 0.06331764222648857, 0.07492122632481174, -0.0004815941448264105, 0.029789974410001203, -0.012056851250998715, 0.0016369841709720034, 0.05168616438382137, 0.08295448375368902, -0.09918834399360382, 0.1419207744690984, -0.03606595157105957, 0.12284181209975491, -0.045661228786371424, 0.02659620047627193, 0.0557585763159721, -0.027962968323274186, -0.02737721036948922, 0.004654683915313489, 0.06928679674171444, -0.038632131205463806, -0.0397757533686897, 0.06130933107250127, -0.004435024202159325, -0.10593850626655897, -0.12663528658846515, 0.0007962647090836802, 0.03210511461628503, 0.009086221079106607, 0.003355904881274643, 0.01623386237540281, -0.0681152808341445, -0.03168671485758834, -0.12563113528244751, -0.05397341298124916, -0.033750815635751746, -0.010564562408643449, 0.007607879963118566, 0.02338847753488263, -0.005725086601252256, 0.06342921506837676, -0.09890941295662738, -0.013381779335680102, -0.1200524867815743, -0.027711931030641776, -0.008793342315762924, -0.021840402485132072, -0.01893950646055128, -0.016429116450323137, -0.006837329263060693, 0.1115171526319442, -0.027502730083549427, -0.012858781238925239, 0.005369447553780869, 0.08602273797335809, 0.016122289533514626, 0.029762082801145163, 0.13232550579573857, -0.008165744599657087, 0.017963242492413666, 0.08434914961601135, -0.005296228076493748, 0.03612173692425965, 0.09321919374935397, -0.016847514073531827, -0.024267115533304087, -0.03341609497459919, -0.07408443107839437, 0.12674685943035335, -0.12551956457604735, -0.05698587757674212, -0.02762824979760963, 8.008408890430852e-05, 0.018005085244417744, -0.12116820879399212, 0.04058465700625103, -0.033862390613127936, 0.016359382730109423, 0.004354831075237393, -0.11915990191098082, 0.0485621205399762, -0.011896465210703651, 0.028074541165162368, 0.0788262821973622, -0.06376393572952932, 0.05132355211381277, -0.003512804310301102, -0.016624368389755458, 0.014894987632098202, 0.00822153123414998, 0.047530073354126505, 0.022914292422985847, 0.001712818980420979, -0.14493324333556737, -0.0788262821973622, 0.04223036016120576, 0.03737693406486173, -0.009441860340126796, 0.09461385534070028, -0.010648242787480389, -0.029315791433592427, -0.028186114007050553, 0.041811958267021054, 0.002552231053841816, 0.07291291517082443, 0.009434886476943183, 0.07698532496748714, -0.10393019511257166, 0.13745786079357103, -0.08708268103903984, -0.02621964133409131, -0.050598327573795565, 0.026568306731927883, -0.019636838323968436, -0.07419600392028254, -0.041170414960036, 0.03768376098167024, 0.0004480351002846401, -0.05210456200703006, -0.08914677968171525, 0.021756723387587933, 0.0028119866898105444, -0.08111353079478997, -0.00900951499055088, 0.01181975805440392, -0.06153247889176564, 0.004598897280820596, -0.09589694409015839, -0.040500977908706895, 0.0037481534538407906, -0.055619111865227874, -0.03603805996220352, -0.06984465881566734, 0.07207611992440704, 0.022481947927605137, -0.0394131410986811, -0.05609329697712465, -0.03667960326918858, -0.10593850626655897, 0.010055511184060608, 0.1080583966688985, -0.06521438480956372, -0.058631580731696845, -0.053359759147632135, -0.05640011962295715, 0.0436529133614081, -0.035508089497106654, 0.04870158926169644, 0.06002624445853115, -0.01638727583380707, 0.09483700102447665, -0.021129124817286897, 0.07146247036176605, 0.09232660674327249, -0.03773954633487033, 0.14247864935597934, 0.03637278062335608, 0.07503279916669992, -0.05299714901311154, 0.07202033670669497, -0.10024828492379757, -0.08959307532024399, -0.08546487162842917, -0.061978768123830366, 0.05924523029433785, 0.08847734263038615, 0.05310872185499973, -0.06063989402117216, -0.01304706081001555, 0.09595272944335849, -0.10404176581897184, 0.11157294225612029, 0.01672199435947162, -0.1314329273316091, -0.02206354816890844, 0.024392633111876286, 0.04663748848353303, 0.017210126343540424, 0.030626771791906583, -0.09254975456253686, 0.049036309922849006, 0.06465651846463479, 0.08267554631024855, 0.0209478197500266, -0.05712534629846235, 0.004361804724872204, -0.006160918348547976, -0.021477790215123475, 0.05341554663632023, 0.010627322265673552, -0.0368748552086209, -0.037962692018646695, -0.13422224410783767, 0.05617697393918078, -0.11525484390294236, 0.0209478197500266, -0.016819621396931782, 0.032049327127596935, -0.006478204031418301, -0.04778111278224693, -0.02404396771403971, 0.050542544356083484, 0.01787956339486953, -0.06739005842961532, 0.08825419481112176, -0.06928679674171444, -0.13031719464175123, 0.08931413787680352, -0.025410736628785967, 0.08764054524848075, -0.05930101778302595, 0.019441585316792114, 0.06354079004575294, -0.01732169918542861, -0.014908934290721427]}}, {'id': 'https://huggingface.co/papers/2410.13782', 'title': 'DPLM-2: A Multimodal Diffusion Protein Language Model', 'url': 'https://huggingface.co/papers/2410.13782', 'abstract': 'Proteins are essential macromolecules defined by their amino acid sequences, which determine their three-dimensional structures and, consequently, their functions in all living organisms. Therefore, generative protein modeling necessitates a multimodal approach to simultaneously model, understand, and generate both sequences and structures. However, existing methods typically use separate models for each modality, limiting their ability to capture the intricate relationships between sequence and structure. This results in suboptimal performance in tasks that requires joint understanding and generation of both modalities. In this paper, we introduce DPLM-2, a multimodal protein foundation model that extends discrete diffusion protein language model (DPLM) to accommodate both sequences and structures. To enable structural learning with the language model, 3D coordinates are converted to discrete tokens using a lookup-free quantization-based tokenizer. By training on both experimental and high-quality synthetic structures, DPLM-2 learns the joint distribution of sequence and structure, as well as their marginals and conditionals. We also implement an efficient warm-up strategy to exploit the connection between large-scale evolutionary data and structural inductive biases from pre-trained sequence-based protein language models. Empirical evaluation shows that DPLM-2 can simultaneously generate highly compatible amino acid sequences and their corresponding 3D structures eliminating the need for a two-stage generation approach. Moreover, DPLM-2 demonstrates competitive performance in various conditional generation tasks, including folding, inverse folding, and scaffolding with multimodal motif inputs, as well as providing structure-aware representations for predictive tasks.', 'score': 5, 'issue_id': 195, 'pub_date': '2024-10-17', 'pub_date_ru': '17 октября', 'hash': '9e7b5af0f52ac4bf', 'data': {'desc': 'DPLM-2 — это мультимодальная языковая модель для белков, которая одновременно моделирует последовательности аминокислот и трехмерные структуры. Модель использует дискретную диффузию и квантование для представления 3D-координат в виде токенов. DPLM-2 обучается на экспериментальных и синтетических данных, что позволяет ей изучать совместное распределение последовательностей и структур. Модель демонстрирует высокую эффективность в задачах условной генерации, таких как фолдинг и обратный фолдинг белков.', 'emoji': '🧬', 'title': 'Единая модель для последовательностей и структур белков', 'categories': ['#multimodal', '#3d', '#nlp'], 'embedding': [0.039142216526286695, 0.05009442038771858, 0.09199849282624643, 0.059094270326841634, 0.00196574168349062, 0.07495115832354347, 0.0453087839670771, 0.03609465018430838, -0.03804699797856051, 0.05642764850093213, 0.0625227852702599, -0.06614177655708368, 0.030928067204912518, -0.011583144492849333, 0.012618841225946848, -0.03057092814639827, 0.019083021825251706, 0.0011420944024353623, 0.07099883956381668, 0.04211836036754463, 0.09109375266461134, -0.0054522917749845815, -0.06904648972687899, 0.025404347209687976, 0.009083184564974317, 0.03345183381595344, -0.020428238531012083, 0.08428433165019839, 0.05271342108509114, -0.004776707433507237, 0.05680859710237207, -0.02497578360776778, -0.0759987618707894, -0.011017677244717748, -0.04971347178627864, 0.051189638935444766, -0.0539515009757426, 0.1709495971661103, 0.025213872908968003, 0.05090392850570759, 0.058570472638589804, -0.06304659112925398, 0.009720079420540196, 0.05733239479062391, 0.0027097772352679904, 0.06542750456811187, -0.027023367531037117, 0.056570501673115156, 0.018213987807262903, 0.00797010804297964, -0.012749791669352589, 0.11647429033137362, -0.0067796513235506934, 0.001342240002223278, -0.08485575250967273, -0.0314994860217013, 0.015202132715644778, -0.09347466610346923, -0.07518924558205813, -0.02961856277085511, 0.031380440349758405, -0.082760549500552, 0.036951775345463206, 0.06471322849376894, -0.07195120289667427, -0.009618890701523013, -0.11704571119084796, -0.020106813786886373, -0.03445181827734798, 0.022606774940372727, -0.005118963893544476, -0.056951450274555095, -0.01788066033435991, 0.046737332030391834, -0.07457021176478909, -0.02573767468259097, -0.03261851411235318, 0.0693798212851531, -0.07252262171346306, 0.015178323172719086, 0.010243880172820376, -0.05452291979253138, 0.0519991511388049, -0.07442735859260606, -0.08604621535715838, -0.019928246300314815, 0.001166647585040369, 0.029666181856706497, -0.07442735859260606, -0.08680810234661043, 0.006107043379207616, -0.03309469884281031, -0.10980772861720038, 0.050332507646233246, 0.033904209003484884, 0.010934345478626277, 0.010476019130974741, -0.12847408548393804, 0.02261867766915001, 0.049618237699947006, -0.07595114074225244, 0.02188059409456695, -0.019440158841080392, -0.018975880924771656, 0.04361833297426531, -0.011220054478483555, -0.04121361203516729, 0.0027559072033437365, -0.09752222099221322, -0.036404166071600116, 0.04309453528601348, 0.07090360547748503, -0.035927983383828535, 0.031332821263907025, 0.028023351992431663, -0.09904600314185959, -0.03911840902604657, 0.13875964052762063, 0.039951728729646835, 0.04526116692391128, -0.07152263725206852, 0.08985567481664544, 0.029642374356466366, -0.0056814547445417935, 0.013142641978227029, -0.11695046893377407, 0.0169878177947882, -0.0754749560117953, -0.014678331963364597, -0.010856965689729117, -0.06695128671775825, 0.09647460927422505, 0.011273625132992137, -0.022190112024544253, -0.14171196461252508, 0.00484813504094153, -0.11685523484744244, -0.02096394303341233, -0.04385642636083667, 0.06276088069951681, -0.0165116361283594, -0.03604702701308587, 0.04621353025676887, 0.14485477933963395, 0.0024449005641279105, -0.016880675872965368, -0.02530910903798521, 0.06614177655708368, -0.05090392850570759, 0.09790316142291092, -0.08085582692020796, -0.12809315526666817, 0.02478530930704781, 0.06176089419543671, -0.020487760345640753, -0.1694257945896083, -0.05066583920450737, 0.09018900024686287, 0.15494985395453992, 0.005220152816830215, -0.023582947816156014, 0.08856998401088487, -0.058570472638589804, 0.041808840394881766, 0.048046834421763676, 0.07371308251826313, -0.1200932775328263, 0.024404362748293433, -0.11161722119658393, -0.038880317682160784, -0.01415453100681586, 0.012404559424986751, -0.12056945409254118, -0.050999166677410356, -0.00413386100928416, -0.06971314058731384, -0.16790202265338972, -0.07171311359547405, -0.12390274107768448, -0.16114023298094157, -0.01077958528802629, -0.00869033405183132, -0.030880448119061137, 0.001505927770504475, 0.003577322544018267, -0.023178191714475948, -0.04223740603948752, 0.059094270326841634, 0.06985599988755356, 0.13475970472472804, -0.04566592098290578, 0.09952218582963118, 0.10561732259895894, -0.039142216526286695, -0.013499779402592827, -0.0657608299983293, 0.12961691698945893, 0.11171246345365783, 0.015606887795982065, -0.02502340167227638, 0.056380031457766315, -0.013333115870409886, -0.0875223763782678, -0.15533079438523759, -0.05433244344912585, 0.04440403563469976, 0.04366595410280226, -0.011928376737215172, -0.07037980574654765, 0.004047552846058423, 0.038880317682160784, 0.0025624581396379493, 0.060999003120613515, -0.03147567647877561, 0.009862934226871668, -0.045856391198254624, -0.07995107245977391, 0.03095187470515265, 0.028594771830563224, -0.0006860006896776452, -0.04176122335171594, -0.005461219945044603, 0.08718905094805038, 0.008976043460225711, -0.10837917646851451, -0.06837983886644412, 0.018094942135320008, -0.0515705875368847, 0.05957045709998434, -0.07947489385737347, 0.0752844837537609, -0.02635671054254557, 0.0076010662556881105, -0.08704619981855291, -0.04004696281597847, -0.01594021608595928, -0.004925514625570133, -0.030499501560306758, 0.02528530153774508, -0.06971314058731384, 0.050427747860621576, 0.015023564207730435, 0.031332821263907025, 0.0787606198257161, 0.08923663895669084, 0.0431421523291793, -0.0675227075772326, -0.023082955585458746, 0.038404134994389195, -0.04730875288986618, 0.07671302977439007, -0.04866587028171828, 0.023332951700807382, -0.036142267227474205, -0.04361833297426531, 0.05518957473833737]}}, {'id': 'https://huggingface.co/papers/2410.10812', 'title': 'HART: Efficient Visual Generation with Hybrid Autoregressive Transformer', 'url': 'https://huggingface.co/papers/2410.10812', 'abstract': 'We introduce Hybrid Autoregressive Transformer (HART), an autoregressive (AR) visual generation model capable of directly generating 1024x1024 images, rivaling diffusion models in image generation quality. Existing AR models face limitations due to the poor image reconstruction quality of their discrete tokenizers and the prohibitive training costs associated with generating 1024px images. To address these challenges, we present the hybrid tokenizer, which decomposes the continuous latents from the autoencoder into two components: discrete tokens representing the big picture and continuous tokens representing the residual components that cannot be represented by the discrete tokens. The discrete component is modeled by a scalable-resolution discrete AR model, while the continuous component is learned with a lightweight residual diffusion module with only 37M parameters. Compared with the discrete-only VAR tokenizer, our hybrid approach improves reconstruction FID from 2.11 to 0.30 on MJHQ-30K, leading to a 31% generation FID improvement from 7.85 to 5.38. HART also outperforms state-of-the-art diffusion models in both FID and CLIP score, with 4.5-7.7x higher throughput and 6.9-13.4x lower MACs. Our code is open sourced at https://github.com/mit-han-lab/hart.', 'score': 3, 'issue_id': 200, 'pub_date': '2024-10-14', 'pub_date_ru': '14 октября', 'hash': 'cc3b200d837522fd', 'data': {'desc': 'Статья представляет Гибридный Авторегрессионный Трансформер (HART) - модель для генерации изображений высокого качества размером 1024x1024 пикселей. HART использует гибридный токенизатор, который разделяет латентное представление на дискретные токены для общей картины и непрерывные токены для деталей. Модель сочетает масштабируемую авторегрессионную часть для дискретных токенов и легковесный диффузионный модуль для непрерывных. HART превосходит современные диффузионные модели по качеству и эффективности генерации изображений.', 'emoji': '🎨', 'title': 'HART: высококачественная генерация изображений с гибридным подходом', 'categories': ['#cv', '#code', '#architecture', '#diffusion'], 'embedding': [-0.015986924875283994, -0.029458888177030212, 0.04506342187381366, 0.07283095772203677, -0.05350545765294878, -0.06353589159711898, 0.002456132935593981, 0.019310793894097535, -0.12954262624280438, 0.08689121396652424, -0.04668123238882964, -0.03109140705480442, -0.04271023697624193, -0.04656357423783494, -0.025473186589452048, -0.013567560002487466, 0.1454266122686908, -0.006563169436691231, -0.032032678826065336, 0.01121437948045129, 0.0015497904083147697, -0.11436461100056393, 0.019296085531339314, -0.038709833589208875, 0.1474268139622073, 0.014597077137208626, -0.08283198150399385, -0.03097374890380972, 0.021619851515626815, -0.07094841199174716, -0.1135998242680273, -0.050093347208656984, -0.04591644784406078, -0.09748053506675815, 0.05003451594539186, 0.08895024692330591, 0.0030646509163175023, 0.04488693027178606, -0.10530486336540333, 0.12424797006822298, 0.09124460712091582, -0.0655949267416684, 0.040504133518484677, 0.0801258243412058, 0.041798379742729694, 0.0033716673691841672, 0.02926769258777994, -0.02163455987838504, -0.09959839491126937, 0.06100622384859075, -0.17354712038213022, 0.0637712144624117, -0.05897660542955777, -0.026502704161726766, -0.04806372660185623, -0.053387799501954075, -0.034944742128629656, -0.011346745994204214, -0.02632621474746694, -0.0007882237855346259, 0.10824633683018077, -0.04118067007447197, -0.009993666756479856, -0.05206413436442484, -0.06447716336837989, 0.04479868884630781, -0.10454007663286667, 0.07071309568975775, 0.0279440274502507, 0.05927075299481229, 0.03753324114042301, 0.016325194465938303, 0.06965416576750215, -0.10730506505891985, -0.06712449364343837, -0.07741966717841775, -0.07377223386883322, 0.051593497384910494, -0.03100316125379062, -0.01654580546804436, 0.0457399628053365, 0.027988150350757603, -0.08347910571000024, -0.07589009808888002, -0.0409159392347339, -0.09353895637968689, -0.03691553147216529, -0.12401264720293026, -0.02926769258777994, -0.06459482808267793, 0.02045796961736695, 0.13589621452740916, -0.015560410796276548, -0.015207434155524677, -0.028458785142504172, -0.024370133766689537, -0.0539760968202309, -0.04906382963638227, 0.0824790026754742, -0.00959656677766753, 0.001592074161840753, 0.047593093997877436, 0.0993042517215504, -0.0014330506822324474, 0.04062179166947937, -0.0361801614651451, -0.03935695779521527, 0.058182404159272456, -0.0014109895382664857, 0.08118475863899696, -0.0611827110750828, -0.09471554226516941, -0.03126789646906424, 0.05015217409638656, 0.035209475156135504, -0.1260128554597501, -0.1280130615288022, -0.026914509877975987, 0.02566438327258621, -0.013692573428745166, -0.05527034304447591, 0.01194974883114115, 0.07241915200578754, -0.011971810062617824, 0.010501070923684547, 0.0003499897689179735, 0.02526728263744355, -0.06271227797685276, -0.008103767938695026, -0.09312714847566989, -0.019619649822110282, 0.022266977909400982, -0.04073944982047408, 0.007919925874493533, -0.04909324198636317, -0.1061873038733991, 0.0011821058423582282, 0.03423878665935814, -0.020487384155115626, -0.0012234704052531142, 0.05435848581096367, -0.036709627520156794, -0.06165334805459714, 0.009625982628076872, -0.05930016753256097, -0.010001020281528638, -0.0861264359850587, 0.0412395013377371, -0.07618424346636675, 0.10777570422620196, -0.05206413436442484, -0.03323868581259987, 0.013310180937583954, -0.05132876654517242, -0.059241338457063614, -0.07765498348040714, -0.06153568990360245, 0.11301152913751825, -0.02054621541838075, -0.06435950521738519, -0.033150440011586076, 0.0884796143193271, 0.023120006067415877, -0.010405473566612967, 0.00607414908210849, -0.007261770505806859, -0.07324276781382152, 0.05585864036275273, -0.12118883626468306, -0.06788928037597501, -0.0908916261046284, -0.05618220246575592, -0.045416400702333305, -0.10448124755736933, 0.04441629766780726, -0.0170164415724516, -0.053623117991711246, -0.08036114064319519, -0.046945965416335486, -0.08459686689552097, -0.05980021904982399, 0.12754242673705563, -0.03976876351146449, -0.009978959050051965, -0.04974037056790511, -0.09571564529969545, -0.04638708701134289, 0.0015130220611075046, 0.048710852995630397, 0.059623727447796385, -0.03947461594620996, 0.10242221678835545, 0.07418403958508245, 0.006478602148416042, 0.05729996365127667, -0.0359742575131366, 0.021208045799377594, 0.025090795410951505, 0.009589213690172306, -0.010265752433927374, 0.05256418588168786, 0.07947869357189608, -0.057976502395031736, -0.055799809099487604, -0.058505970637811204, -0.006809517995244796, 0.053387799501954075, -0.030091306208046155, 0.019369622969594883, 0.02834112699150947, 0.06471248623367262, -0.03523888969388418, 0.03338575959522713, -0.01920784191809329, -0.037415582989428306, 0.016236950633915503, 0.051799401336918995, 0.1262481717617395, 0.03226799731582251, 0.06512429194992184, -0.17731221184270943, 0.03506240027962436, 0.0015369215207026274, -0.012310079872048912, -0.1336606921563677, -0.013611682902994367, 0.025590844740446747, -0.08271431897746359, 0.05994729392633514, -0.09789234078300736, 0.045181080024808355, 0.011118780591942264, -0.07759614784160647, -0.0742428686605798, 0.0403864731797222, -0.01976672251085366, -0.004390153997555379, -0.05709405969926816, 0.03017954982129218, -0.0697129926552317, -0.05335838715197318, 0.016501683223867795, 0.11730608665310915, -0.008861198302084781, -0.028238175015505223, 0.06835991516772158, -0.09612745320371247, -0.07706668834989809, 0.08024347811666493, -0.02517903902419753, -0.08259666520200445, -0.009787763460801693, -0.08836196054386797, 0.06112388418735323, -0.029355936201025962, 0.10712857783242781]}}, {'id': 'https://huggingface.co/papers/2410.14470', 'title': 'How Do Training Methods Influence the Utilization of Vision Models?', 'url': 'https://huggingface.co/papers/2410.14470', 'abstract': "Not all learnable parameters (e.g., weights) contribute equally to a neural network's decision function. In fact, entire layers' parameters can sometimes be reset to random values with little to no impact on the model's decisions. We revisit earlier studies that examined how architecture and task complexity influence this phenomenon and ask: is this phenomenon also affected by how we train the model? We conducted experimental evaluations on a diverse set of ImageNet-1k classification models to explore this, keeping the architecture and training data constant but varying the training pipeline. Our findings reveal that the training method strongly influences which layers become critical to the decision function for a given task. For example, improved training regimes and self-supervised training increase the importance of early layers while significantly under-utilizing deeper layers. In contrast, methods such as adversarial training display an opposite trend. Our preliminary results extend previous findings, offering a more nuanced understanding of the inner mechanics of neural networks.   Code: https://github.com/paulgavrikov/layer_criticality", 'score': 3, 'issue_id': 197, 'pub_date': '2024-10-18', 'pub_date_ru': '18 октября', 'hash': 'd5813550dc7a31c9', 'data': {'desc': 'Исследование показывает, что не все параметры нейронной сети одинаково важны для её функции принятия решений. Авторы изучили влияние методов обучения на критичность различных слоёв сети для задачи классификации изображений ImageNet-1k. Обнаружено, что улучшенные режимы обучения и самоконтролируемое обучение повышают важность ранних слоёв, в то время как более глубокие слои недоиспользуются. Напротив, методы вроде состязательного обучения демонстрируют противоположную тенденцию.', 'emoji': '🧠', 'title': 'Метод обучения определяет критичность слоёв нейросети', 'categories': ['#architecture'], 'embedding': [-0.04757080430308064, 0.011654113596583821, 0.06308514972798297, -0.06607056486283612, 0.08452137894898992, -0.02875292963580259, 0.05334586266765487, 0.06337879659410812, -0.08696843483438761, 0.14388699637680688, 0.03090634065354294, 0.006729409280553497, -0.11873125084337639, 0.09440749343931623, 0.007279997154538147, -0.08148702533440633, -0.007451291226393409, 0.07874632657981911, 0.005680233065459166, -0.026330342990270133, -0.036363278915191186, -0.03822304506527419, 0.01047952553254289, 0.017704464299376135, 0.127736416741456, 0.002035646106021801, 0.08921972680852445, 0.024678579368316182, 0.10277643112235071, 0.029095516980125993, 0.01457222972839603, -0.0036919984099142143, -0.17648181450444064, 0.0065336445699055445, -0.055058801387739685, -0.016909169038230688, 0.023320460614019566, 0.04815809803533093, -0.04299480663980783, 0.06572797552004492, 0.024935520376175684, -0.034503516762043794, -0.11843759598338001, 0.11843759598338001, -0.044927982507954316, 0.01907481567513755, -0.030245635204761357, 0.012412701400689377, -0.05809315300385547, 0.17060887718193773, -0.11559901227546066, 0.11452231176276001, -0.07566303048856947, -0.08740891112897875, -0.06019762554186538, -0.009604702244133756, -0.018279523411693817, -0.056184448374042024, -0.06288938381825433, 0.056624920671697554, 0.04568657091160022, -0.10923666017863613, 0.054960922429810995, 0.04304374911647389, 0.024250343689061073, -0.05197550729495784, -0.1195143064884197, -0.03053928307012041, 0.03868798460432714, 0.042211746997828904, 0.05491197795467712, -0.026012224885811952, 0.004738115770178611, 0.024225872450728037, -0.034943989059699324, 0.060931742707178244, 0.044414100492235306, 0.07639714765388234, 0.054569390610353714, 0.037660220572889126, 0.07223714705299639, 0.0015577052748303101, 0.04681221789790255, -0.07360750242569343, -0.05623339085070809, -0.10395101658838349, -0.046983512569298155, -0.10316795694640456, -0.0968545493247139, -0.056624920671697554, -0.027920931514093215, -0.03083292893701165, -0.04199151384670285, 0.09289031663202442, 0.0004244116111007393, 0.08148702533440633, 0.09920372225524726, 0.027333635783375113, 0.03511527973415931, -0.016872464179198948, -0.003860233826786324, 0.06161691539735724, -0.004163057057554491, 0.026281402512071877, -0.011219760007822063, -0.12528936285452613, 0.008809407982222213, 0.020322816854637233, -0.0708667976757028, -0.04172233622044293, -0.00627670336219926, -0.01994352165358038, -0.08129126542008112, 0.09054114370149106, -0.004367998532539026, -0.12607242249650505, -0.06797926749418177, 0.1175566633788758, 0.020971285685018395, 0.002547999094019403, -0.010271525002881641, 0.020873403729387976, 0.0192828162047988, -0.00983717281304735, 0.06719620785220283, -0.062008445218346704, 0.07595667935316242, -0.006851762074823381, -0.055499273685395215, -0.014927052691885946, 0.0008633525827968832, 0.05735903783701042, 0.06748985671679579, -0.1195143064884197, -0.006974114869093264, -0.034503516762043794, -0.023503989905347782, 0.054177862787832064, -0.14134205553807708, 0.02400563929960199, 0.019576463070923943, 0.03619198624226339, -0.03159151734065755, 0.008234349469444874, -0.07434161759253849, 0.0725797343973198, 0.08319996605295896, 0.1156968972287928, -0.031713869535387094, 0.11080277746412619, 0.035041870016095836, 0.04691009885429906, 0.029584929755979778, -0.062008445218346704, -0.04842727366312304, -0.04899010015397593, 0.05672280162809406, 0.11090066241745831, 0.04967527484262273, -0.01282258315157776, 0.07522256218784955, 0.046224924165652256, -0.020261639758038557, 0.026330342990270133, -0.029291282889854633, 0.09401595962139113, 0.013189641734234193, -0.025669635543020743, -0.051583977473968375, -0.13194535981900798, -0.043141630072870404, -0.015710111334630975, 0.01337317102556241, -0.016958112514130656, 0.00942117295280554, 0.010773172398668039, -0.11677360373689687, -0.06871338266102683, 0.05241597959261337, -0.07933361631513379, -0.0587783316894379, -0.004728939405535592, -0.05373738849170872, 0.06235103655960572, -0.13654583271754944, 0.026305871751937097, -0.03186069296844967, 0.037831515244284734, -0.04387574723818327, 0.016982581753995876, 0.02261081468703973, 0.02476422570478008, 0.08491290477304377, 0.06518962026752506, 0.06993691060372566, 0.0056282329330438535, 0.10346160581099752, 0.02388328310793683, -0.04813363079393352, 0.0010552937249074178, -0.02684422500598914, 0.011751994952673891, 0.04255433833908792, -0.10121031183839285, 0.1282258315157776, -0.047986805362403136, -0.0010606466209364112, 0.03717080979550315, -0.08775149447636654, 0.003942821907960631, -0.021570816035669106, -0.08134019790440813, -0.054960922429810995, -0.04487904003128825, -0.005303998118159711, 0.0017618817163525115, -0.0666578545981508, -0.021534110177403462, 0.07150302988768353, 0.03254586765709647, -0.0010575877661064778, -0.03516422221082538, 0.021852228281861643, -0.0005100586470283706, -0.13693735254619988, -0.01672564074613638, 0.08941549671518872, -0.14545311366229693, 0.07532044114577825, -0.040229632649951975, 0.06651103316355603, 0.003646116187005551, 0.02645269518499967, -0.0006534408983842856, 0.04054774675747453, -0.06548326913211802, 0.01719058228365713, -0.05217127520315429, 0.03159151734065755, -0.08447243647232387, 0.028899753068865164, 0.1227444200173285, -0.08540231754889756, 0.03903057194865054, -0.06797926749418177, 0.08770255199970048, 0.0073289384321235254, -0.037317633228565725, -0.11178159901889814, 0.05339480314585313, 0.03247245793903299, 0.008197644210719572, -0.031444691909127166, -0.019772226982184775, -0.05241597959261337, 0.027309164545042078]}}, {'id': 'https://huggingface.co/papers/2410.13828', 'title': 'A Common Pitfall of Margin-based Language Model Alignment: Gradient Entanglement', 'url': 'https://huggingface.co/papers/2410.13828', 'abstract': 'Reinforcement Learning from Human Feedback (RLHF) has become the predominant approach for language model (LM) alignment. At its core, RLHF uses a margin-based loss for preference optimization, specifying ideal LM behavior only by the difference between preferred and dispreferred responses. In this paper, we identify a common pitfall of margin-based methods -- the under-specification of ideal LM behavior on preferred and dispreferred responses individually, which leads to two unintended consequences as the margin increases: (1) The probability of dispreferred (e.g., unsafe) responses may increase, resulting in potential safety alignment failures. (2) The probability of preferred responses may decrease, even when those responses are ideal. We demystify the reasons behind these problematic behaviors: margin-based losses couple the change in the preferred probability to the gradient of the dispreferred one, and vice versa, often preventing the preferred probability from increasing while the dispreferred one decreases, and thus causing a synchronized increase or decrease in both probabilities. We term this effect, inherent in margin-based objectives, gradient entanglement. Formally, we derive conditions for general margin-based alignment objectives under which gradient entanglement becomes concerning: the inner product of the gradients of preferred and dispreferred log-probabilities is large relative to the individual gradient norms. We theoretically investigate why such inner products can be large when aligning language models and empirically validate our findings. Empirical implications of our framework extend to explaining important differences in the training dynamics of various preference optimization algorithms, and suggesting potential algorithm designs to mitigate the under-specification issue of margin-based methods and thereby improving language model alignment.', 'score': 3, 'issue_id': 195, 'pub_date': '2024-10-17', 'pub_date_ru': '17 октября', 'hash': '725b42ac671f6eec', 'data': {'desc': "Статья рассматривает проблемы, связанные с использованием методов обучения с подкреплением на основе обратной связи от человека (RLHF) для настройки языковых моделей. Авторы выявляют недостатки подходов, основанных на маржинальных потерях, которые могут привести к нежелательному поведению модели. Они вводят понятие 'запутывания градиентов' для объяснения этих эффектов и предлагают теоретический анализ условий, при которых эта проблема становится значимой. Исследование также предлагает потенциальные улучшения алгоритмов оптимизации предпочтений для повышения качества настройки языковых моделей.", 'emoji': '🧠', 'title': 'Распутывая градиенты: новый взгляд на настройку языковых моделей', 'categories': ['#rlhf', '#alignment', '#nlp'], 'embedding': [-0.0027673331594422032, 0.07286642127904067, 0.1877654687866004, 0.0014306880240035358, 0.009114469902786807, 0.04222996434316191, 0.006040340237716809, 0.04235330139651221, -0.004122478423620284, 0.10863362431812411, 0.026023722164113958, -0.06048357312036243, -0.030661125907505644, 0.06462763255274503, 0.01460288507056436, -0.07918118631412216, 0.11159367262341413, -0.06704500835975241, 0.01618157515861711, 0.07222508265023068, -0.0027472910590024355, -0.013443533939596833, 0.012068347160337799, 0.12017780158693768, 0.007591280415583864, 0.0026748316377069154, 0.11060698790378808, 0.03929458696069485, 0.031228467377415164, -0.04070060712525324, 0.10019749552598961, -0.025222044000111365, -0.03949192312414164, 0.013899874988285167, 0.0032467986129663348, 0.044005989814183036, 0.04753337568699047, 0.04099661332161947, -0.005904671820466745, 0.03663054485177107, -0.048174722120584636, -0.049506739272654435, -0.02661573065445433, 0.08495326714374475, 0.002821292120294982, 0.07972386076360084, 0.12146048469814577, 0.04689203510698445, -0.011112499143044382, 0.031475137581723656, -0.12343385218620181, 0.054662152396289986, -0.006407262264185489, -0.035767201087887404, 0.037913233816567304, 0.054464816232843195, 0.06127291777414959, 0.048396725304462276, 0.054020805962695824, -0.019708961031424548, 0.022212665556351756, -0.041391287599709094, -0.0305131218337245, 0.023322683426936004, -0.006999271535004281, -0.09625077030585774, -0.18855481734277965, -0.029353771873474606, 0.033423832195760714, 0.06378894995570408, 0.0006548327104343763, -0.07804650337430312, 0.032387819288861114, 0.02553038175549698, -0.05308346113751961, 0.02375435433327981, 0.0893440065364161, 0.16191442357028657, 0.011285168676299532, 0.02359402065167534, 0.023569351680048443, -0.05209678032028566, 0.047977385957137846, -0.07000504690906219, -0.04506667559510163, -0.027035071952974808, -0.07138640005318973, 0.00216607404864125, -0.10379888831367773, -0.043734654540639746, 0.009447474678603266, -0.06911703417355163, -0.03312782990178658, -0.0717810762824754, 0.027972417753749042, -0.07503712297934745, 0.013135196184211223, 0.01440554890711757, 0.08776531527764572, 0.03917124990734455, -0.010446489786531066, -0.04462266532495426, 0.07444511058661499, -0.12659122689895838, 0.10883095657917881, -0.07232374878075803, 0.00487482313211999, -0.027084404042640457, -0.015330562953752822, 0.04657136189018737, -0.036038538312626746, -0.0031327135459138563, -0.07020238502370503, 0.018672946173328892, -0.004729904094409346, -0.11090299410015432, -0.18145071545869518, 0.08332524184411268, -0.02957577700854829, 0.00522941145325364, -0.07686248444242631, -0.08100654192361288, 0.05752352481507241, -0.06734100480013841, 0.03495318941366939, -0.003505802327490248, 0.09008401714934156, -0.07691181653209198, 0.004208812800008651, -0.049087399925330004, 0.002714915643466847, 0.05870754374694921, 0.007541946374722166, -0.06891969801010484, -0.16438112561337145, 0.0688703620180471, 0.01119266734968385, 0.006980771074561539, -0.016378912297661922, -0.06438096234843654, -0.04891472883111801, -0.029526441016490546, -0.09309339169070908, -0.020695643799854547, -0.08875199414368362, 0.0033331331844743055, 0.04454866621485776, -0.011402337023346065, -0.078391841660335, 0.025703050898512922, 0.05034541991849934, -0.044005989814183036, -0.12363117664247233, 0.11465237730325124, -0.06946237441077956, -0.11100166023068166, 0.016391244832279325, 0.022397668209583124, 0.03117913528774952, -0.10754827346797072, 0.050000081632467455, 0.1119883449503077, -0.01648991291400272, -0.030661125907505644, 0.08613728607562154, 0.009953149182794482, -0.025209710489895943, 0.07035038519509407, -0.031203800356984314, -0.0720277386819997, -0.017316258098436156, -0.0686236957161307, -0.08564394566700458, -0.1023188670878268, -0.04215596523306541, -0.034065174726962785, -0.07932918648551121, -0.08061186959671932, -0.0015131685345428608, -0.07533312917571369, -0.017032587363481393, 0.046324691685878876, -0.017488928217050122, 0.0030756711099474195, -0.05086342734754716, -0.00035034899650545097, -0.07158373621663651, -0.008614961763464093, -0.028712429342272534, 0.047780049793691055, -0.019005950949145353, 0.03737055741589259, 0.1527382802628345, -0.06832768951976446, -0.06028623695691563, 0.0018839448951476333, 0.11544172390823446, 0.11938845693315052, -0.04548601494242606, -0.05742485868454507, 0.06512097296136202, 0.029230436771320357, 0.06798235123373259, -0.07360643189196613, -0.0027380408287810644, 0.029748444200368186, -0.076418476123475, 0.10429222872229471, -0.02255800384238364, -0.004736070849517058, 0.037123889162780144, 0.07982252494293214, 0.12678856306240519, 0.0761718059191665, -0.05337946343149375, 0.02969911015950649, -0.027651747463746032, -0.0018854865936805416, 0.03068579292793649, 0.08105587596447458, -0.018845615316344835, -0.009145303288086158, 0.05954622439279412, 0.03904791870758241, -0.13280731994992442, -0.003149672171239966, 0.11307368799567692, -0.08633462419026439, 0.07365576398163177, -0.10922563671085658, 0.0761718059191665, -0.004612735747362813, -0.0261963913071299, -0.03505185554419674, 0.04647269575966002, -0.03786389977570562, -0.03798723292666381, -0.009496808719464964, 0.07429711041522594, -0.021460321433210864, -0.00136285365928282, 0.05693151632473204, 0.04203262817971513, -0.0622102625993258, 0.005540832878872515, 0.07163307025749821, -0.10064150774733303, -0.018426275969020404, 0.012043680530146159, 0.005488415558016763, 0.023335015961553407, -0.03288115774628204, 0.017969936091049698, -0.003974476301035191, -0.04770604678120246, 0.058510207583502415]}}, {'id': 'https://huggingface.co/papers/2410.12791', 'title': 'Context is Key(NMF): Modelling Topical Information Dynamics in Chinese Diaspora Media', 'url': 'https://huggingface.co/papers/2410.12791', 'abstract': "Does the People's Republic of China (PRC) interfere with European elections through ethnic Chinese diaspora media? This question forms the basis of an ongoing research project exploring how PRC narratives about European elections are represented in Chinese diaspora media, and thus the objectives of PRC news media manipulation. In order to study diaspora media efficiently and at scale, it is necessary to use techniques derived from quantitative text analysis, such as topic modelling. In this paper, we present a pipeline for studying information dynamics in Chinese media. Firstly, we present KeyNMF, a new approach to static and dynamic topic modelling using transformer-based contextual embedding models. We provide benchmark evaluations to demonstrate that our approach is competitive on a number of Chinese datasets and metrics. Secondly, we integrate KeyNMF with existing methods for describing information dynamics in complex systems. We apply this pipeline to data from five news sites, focusing on the period of time leading up to the 2024 European parliamentary elections. Our methods and results demonstrate the effectiveness of KeyNMF for studying information dynamics in Chinese media and lay groundwork for further work addressing the broader research questions.", 'score': 3, 'issue_id': 195, 'pub_date': '2024-10-16', 'pub_date_ru': '16 октября', 'hash': 'b35aa6a3b1de1ccc', 'data': {'desc': 'Статья представляет новый подход к моделированию тем в китайских СМИ под названием KeyNMF. Этот метод использует трансформерные контекстные эмбеддинги и показывает конкурентоспособные результаты на нескольких китайских датасетах. Авторы интегрируют KeyNMF с существующими методами анализа динамики информации в сложных системах. Подход применяется к данным пяти новостных сайтов в преддверии выборов в Европарламент 2024 года для изучения влияния КНР на европейские выборы через китайские диаспорные медиа.', 'emoji': '🇨🇳', 'title': 'Новый метод анализа китайских СМИ для выявления иностранного влияния', 'categories': ['#nlp', '#dataset', '#benchmark'], 'embedding': [-0.016014540270613, 0.07897790308780352, 0.08931480052201586, -0.017414858326514647, 0.0008441687310866914, 0.03391314276020628, 0.04598133422042022, 0.1586686995165317, -0.08651416026138325, 0.026325969078877607, -0.03470241397437635, 0.0003399350350959255, -0.1048965110628845, 0.04939301613674776, 0.05438324255583008, 0.028821081251211442, -0.05718387451880406, -0.0437662887708887, -0.01758034890458294, 0.15439136021513009, 0.08259326970485865, 0.04475924053695708, -0.046032256951413116, 0.02927936716041562, -0.06395631976929553, 0.005047508060763905, 0.026580572361768817, 0.0667569517322695, -0.02355079433257077, 0.0016947024508970035, -0.029381206399157443, -0.04539574770697777, -0.08946756249175059, -0.013646730776932098, 0.006581492062277936, 0.04450463725406587, -0.1303568393520053, 0.0003632073537817356, -0.05901701815562075, 0.031723556601756554, 0.018662413375474234, 0.0020734245385935868, -0.12322794328222214, 0.17924063647648267, -0.11874692135450758, -0.022112287340046437, -0.04921479487593123, 0.0029740833147288525, 0.15245638771164485, 0.14726248903832284, -0.06909929986045395, 0.006158214597144784, 0.02362717635464546, -0.03126527069255238, -0.04374082947980691, 0.0033225714933607354, -0.009834048327028352, -0.0104196354627952, -0.06528025269150049, 0.0026351430444374055, 0.028566477968320236, -0.07414044278728521, 0.054688764420884876, 0.07062692370663051, -0.045064762402011876, -0.033836764886960906, -0.026529651705190574, 0.015441683402711449, -0.012539207222400473, 0.002046372926821304, 0.05255009684459873, -0.021679460721924056, 0.025371207286639243, -0.11895060812964985, 0.006005452419968593, -0.025600349204034, -0.020215492363903274, 0.07062692370663051, -0.06461828623039802, -0.027522603471258955, 0.0003260114187110671, 0.03134165271462707, -0.001399521895556398, 0.12607950005060367, -0.06181765219300938, -0.06645143194162936, 0.10448914788467323, 0.05280470012748994, -0.05041143134272724, 0.007580809792044827, -0.11284012934026086, 0.051837207652503346, -0.07454781426315511, 0.06701155916399001, 0.07658463845187012, 0.03928527099200344, -0.02338530168008782, -0.031061590140654072, 0.06634958855405822, 0.1452256627751932, 0.10413270328862552, 0.0051525321483281775, -0.023614445671897237, 0.05916978012535548, 0.05855873432083124, -0.03821593720386037, -0.030552381500457, -0.03989631887094234, -0.031112509760024987, 0.0012101608205918866, -0.05555441558271498, 0.0035071589253172276, -0.10311428808264604, 0.060850159718022795, 0.012806540565715507, -0.08081104672462021, -0.06650235259820761, 0.00560126994175045, -0.009203905461174444, 0.1025032422781218, -0.12180216074920207, 0.13555073802532727, 0.04488654114119536, 0.04453009654514767, 0.00741531755444481, -0.07123797158556942, -0.03472787326545815, -0.030170477613327515, -0.012068191460213937, -0.052091810935394556, 0.11222908561015128, -0.01086519068925115, 0.009954984627099844, 0.037375747407526715, -0.12210769091191548, -0.0013080239042963192, 0.07291835117823674, 0.1016375911162917, -0.025523969256373964, 0.01550533349738912, 0.08055644966497297, -0.010534207458699906, 0.04335892144384811, 0.07352939905717562, -0.11854324287702392, -0.026911555592320054, 0.006969762327988859, 0.05071695528219669, 0.031621715288600076, -0.017898603526800613, 0.028973841146531513, -0.09481422520922198, -0.0028658770750811884, 0.05835505169451827, -0.02337257307175425, -0.05815136491937599, 0.08992584425212544, 0.0323855251372737, -0.06930299078442555, -0.11212724014816547, -0.0179749855488753, 0.07179809465910075, -0.04608317760799136, -0.0188660960017872, 0.020011809737590307, 0.028413715998585512, -0.04458101720172591, -0.03508431786150584, -0.07566807078229108, -0.06935390521775982, -0.1169137818665202, -0.051302539721224484, -0.05703111462348399, 0.03350577958199501, -0.07531162411182873, -0.0818294619305997, -0.05229549148729286, -0.048552828414828754, 0.02846463665516375, -0.07042324522914686, -0.03236006377177725, 0.05794768436747768, -0.05311022614137405, -0.13371759853733992, -0.0033448492546835325, -0.03735028811644492, 0.026911555592320054, 0.0917080672323639, -0.002320071611510452, 0.03528800256223346, -0.002603317582215638, 0.07169625749477358, 0.1443090826591262, 0.032843813120892525, -0.05820228765036888, 0.013086603762012906, 0.0771956821819797, -0.0028308692533345636, 0.04997860679901952, -0.06731708517787412, 0.050360510686148996, 0.11895060812964985, -0.09023137026600954, -0.0643636829475068, 0.02640235006374497, 0.038419619830173335, 0.06512749279618044, 0.04939301613674776, -0.08982400708779827, 0.08177854334843611, 0.06186857492400228, -0.006027730388732856, 0.040940191293588966, 0.01427050892373629, -0.09379582037531578, -0.03187631857149128, -0.07332571850527732, 0.06456736972264908, 0.08437549268509711, -0.018904288050031876, 0.05244825553144225, 0.06701155916399001, 0.11009041803386513, 0.02066104966477388, -0.0718999380466719, -0.012793810090408751, 0.005123889253072733, 0.015327112236572604, -0.02465831809454386, -0.12241320862814097, 0.05443416113799367, -0.03358215952965504, 0.0100440958798325, 0.005158897074819358, 0.0009046369640990236, -0.09919340582378006, -0.016829271813072196, 0.013112063882860563, -0.07322387096887686, -0.0011695833199568768, -0.08223682925764028, 0.07108521169024935, 0.05983174866087261, 0.008013635372959882, -0.02314342907994484, 0.0670624777461536, -0.08030184638208177, -0.09664736677162401, -0.03953987427489465, -0.03289473170305611, 0.008083651223894596, 0.07352939905717562, 0.04099111195016721, -0.03126527069255238, 0.022672412487992438, -0.025294827338979205]}}, {'id': 'https://huggingface.co/papers/2410.14208', 'title': 'Montessori-Instruct: Generate Influential Training Data Tailored for Student Learning', 'url': 'https://huggingface.co/papers/2410.14208', 'abstract': "Synthetic data has been widely used to train large language models, but their generative nature inevitably introduces noisy, non-informative, and misleading learning signals. In this paper, we propose Montessori-Instruct, a novel data synthesis framework that tailors the data synthesis ability of the teacher language model toward the student language model's learning process. Specifically, we utilize local data influence of synthetic training data points on students to characterize students' learning preferences. Then, we train the teacher model with Direct Preference Optimization (DPO) to generate synthetic data tailored toward student learning preferences. Experiments with Llama3-8B-Instruct (teacher) and Llama3-8B (student) on Alpaca Eval and MT-Bench demonstrate that Montessori-Instruct significantly outperforms standard synthesis methods by 18.35\\% and 46.24\\% relatively. Our method also beats data synthesized by a stronger teacher model, GPT-4o. Further analysis confirms the benefits of teacher's learning to generate more influential training data in the student's improved learning, the advantages of local data influence in accurately measuring student preferences, and the robustness of Montessori-Instruct across different student models. Our code and data are open-sourced at https://github.com/cxcscmu/Montessori-Instruct.", 'score': 1, 'issue_id': 200, 'pub_date': '2024-10-18', 'pub_date_ru': '18 октября', 'hash': '46c2fabb0545e954', 'data': {'desc': 'В статье представлен новый метод синтеза данных для обучения языковых моделей - Montessori-Instruct. Этот подход адаптирует процесс генерации синтетических данных учительской моделью под предпочтения обучающейся студенческой модели. Используя локальное влияние данных, метод определяет, какие примеры наиболее полезны для обучения студента. Эксперименты показали значительное превосходство Montessori-Instruct над стандартными методами синтеза данных при обучении языковых моделей.', 'emoji': '🧠', 'title': 'Умное обучение языковых моделей: синтез данных с учетом предпочтений ученика', 'categories': ['#data', '#rlhf', '#code'], 'embedding': [0.05000605031311083, -0.0368555863712251, 0.016676847683451658, -0.061319860827126156, 0.05446300817814934, 0.015831984608434522, 0.08468214530813557, 0.02311739041785797, -0.07091945453569368, -0.055883355855110506, 0.02254190581591395, -0.04760615552917254, -0.11999493488899318, -0.0865922665385853, 0.07082150440502333, -0.06602170234616302, 0.04280635763397346, -0.0750335682069104, 0.010585267643059292, 0.096779588094973, -0.003587603510058672, 0.006416055409921784, -0.016823779124949045, 0.06425851255721068, 0.08448623255581116, -0.01905225597563768, 0.03364756033172871, 0.019872630477071915, 0.0490020146325508, -0.026594794930427476, 0.11078716252141324, -0.023631654626760056, -0.1271456525027954, -0.06587476882283502, -0.011528085012408018, -0.0005360742992093114, 0.02260312724987119, 0.04503483240283147, -0.05186719856005601, -0.0009925601721799385, 0.046798024273614444, -0.05431607257299072, -0.03137010425204366, 0.07400503978910622, -0.004343694412876727, 0.040675835077616414, 0.043810395396364124, 0.07909870264913067, 0.004711025722999997, 0.07351526415378702, -0.04706740066485693, 0.05436505180198714, -0.006948686361285639, -0.05127946863040522, -0.04319817689313044, -0.06631556939281903, -0.013297397465213738, 0.04373692759378481, -0.10794646716749093, 0.04283084620755636, 0.039867703822058315, -0.10334257577912442, -0.029778334478171596, 0.11695832678274644, 0.00553446009907949, -0.026007065000776694, -0.11735014812373404, 0.003361082643043905, -0.04829183975315491, 0.1276354323017758, 0.0632789612865723, -0.016407471292209166, 0.03254556411134522, 0.005555887965284884, -0.0880126079700546, -0.049687698856533166, -0.031002771484638952, 0.09242059076975793, -0.06802978411643619, -0.005145701130933889, 0.0489040603382192, 0.0846331598336473, 0.002089197594001133, -0.0005368395801452183, -0.06935217541723514, 0.0029126321782636886, -0.01684826769853194, 0.004616131719679786, 0.05007951811569015, -0.17053974423008086, -0.07312344697646066, -0.09173490238211433, -0.0890901156168552, 0.0028560018053727, 0.09046148406481994, -0.001473152123993653, -0.02537035792396012, -0.043785906822781225, 0.012042349221310105, 0.018366571751655306, 0.018574725668025246, -0.10236302867214728, 0.03528830517093594, -0.032325166949099135, 0.06127088159812974, -0.0034437323595726665, 0.027525369053900066, -0.04253698124273097, -0.10843623655731827, 0.011179120236563452, -0.08316383292768974, 0.0073649950267577645, 0.037639226971369684, -0.03279045401083543, 0.029313047416435305, -0.16250742755814127, -0.11911333791268638, -0.041214587860101404, 0.005914036320543393, -0.03168845779045194, -0.017203355138229886, 0.012979043926805451, 0.04885508110922279, -0.016762558731907108, 0.1083382864266479, -0.09511435051852163, -0.031051750713635366, -0.10696691381502192, -0.024341829506155946, -0.020190985056395515, -0.03825144755643398, 0.04755717630017612, -0.01252600156822673, 0.009758771518686987, -0.0018412487566894411, 0.0009757241164998233, -0.06802978411643619, 0.047410244858678736, -0.06802978411643619, -0.026888661977083488, -0.018268617457323718, 0.005843631098989142, -0.06979297182355793, 0.06871546209492671, -0.07140923225284349, -0.05446300817814934, 0.04261044488164904, 0.03034157583423948, 0.02845594109554203, 0.18484118153951587, -0.12851702511442137, -0.04535318802307038, -0.03962281808622934, 0.04662660217670354, -0.055883355855110506, -0.011546451754869782, -0.04726331133535073, 0.009507762078089321, 0.07591516101955594, -0.13370865267793053, 0.05583438078977533, 0.06915626891040258, 0.011834194472207918, -0.008950642761325631, -0.036512742177403294, 0.029998733722248296, -0.05294470412502598, 0.02359492176638571, -0.1433082338955143, -0.04814490414799628, 0.05074071376608962, 0.0038937131780416353, -0.07616004675538492, 0.049834632379861174, 0.03810451195127536, -0.08830647709854123, -0.09315525422273671, -0.11823174718187145, 0.02816207613071664, 0.05078969299508603, -0.008828199060678896, 0.05779347708556023, -0.17690682540739971, 0.08453520970297695, -0.051965152854387596, -0.058724049127202194, -0.1127462629808594, 0.0448144352405854, 0.01120360881014635, 0.045867450150141854, -0.08223326713153963, 0.0667563657991418, 0.13850843808214586, 0.09883664284875074, 0.033157784696409516, -0.05152435852989544, 0.13772479540017066, 0.038741221109922556, -0.01153420684352915, -0.0480959290826611, 0.04452057027576001, -0.015587096790774928, -0.04540216517023618, -0.0938409301193966, -0.019970584771403507, -0.020460360406722698, -0.025198937908879837, -0.04212066924632985, -0.1592749066995701, 0.011877049788252582, 0.08076394022558206, 0.051769242183893796, 0.08125371169724001, 0.03474955447028157, -0.07939256761395608, -0.017031933041318982, -0.11568491262911329, -0.0908533116512994, 0.06504215523918588, 0.02099911318920768, -0.030561975078316178, -0.04055339220970193, 0.06200554505110853, -0.009085331581496063, -0.010340379825399697, -0.026815194174504176, -0.005265083707836997, -0.04848775042364871, -0.005170189912699848, -0.09516332974751804, 0.08110678441940386, -0.04767962124992123, 0.05216106144305078, -0.028039633262802153, 0.05054480517742644, -0.05181822141289021, -0.03832491327718267, -0.029582423807677796, -0.005503849486192399, 0.010162837146466035, -0.0226276168643694, 0.008326178930385193, -0.019260411973838237, 0.04040645868637392, 0.004998768856704255, 0.07444583619542898, -0.04949178818603937, -0.03815349118027177, 0.053238571171681986, 0.12224790447692595, -0.021452157005067838, -0.05617722081993589, -0.05519767163112813, -0.1001590362313803, -0.061221906532794565, -0.008369034038246796]}}, {'id': 'https://huggingface.co/papers/2410.14596', 'title': 'Teaching Models to Balance Resisting and Accepting Persuasion', 'url': 'https://huggingface.co/papers/2410.14596', 'abstract': "Large language models (LLMs) are susceptible to persuasion, which can pose risks when models are faced with an adversarial interlocutor. We take a first step towards defending models against persuasion while also arguing that defense against adversarial (i.e. negative) persuasion is only half of the equation: models should also be able to accept beneficial (i.e. positive) persuasion to improve their answers. We show that optimizing models for only one side results in poor performance on the other. In order to balance positive and negative persuasion, we introduce Persuasion-Balanced Training (or PBT), which leverages multi-agent recursive dialogue trees to create data and trains models via preference optimization to accept persuasion when appropriate. PBT consistently improves resistance to misinformation and resilience to being challenged while also resulting in the best overall performance on holistic data containing both positive and negative persuasion. Crucially, we show that PBT models are better teammates in multi-agent debates. We find that without PBT, pairs of stronger and weaker models have unstable performance, with the order in which the models present their answers determining whether the team obtains the stronger or weaker model's performance. PBT leads to better and more stable results and less order dependence, with the stronger model consistently pulling the weaker one up.", 'score': 1, 'issue_id': 199, 'pub_date': '2024-10-18', 'pub_date_ru': '18 октября', 'hash': 'e8437463a64f466e', 'data': {'desc': 'Исследование посвящено проблеме восприимчивости больших языковых моделей (БЯМ) к убеждению и предлагает метод Persuasion-Balanced Training (PBT) для её решения. PBT использует многоагентные рекурсивные диалоговые деревья для создания данных и обучает модели с помощью оптимизации предпочтений. Метод улучшает устойчивость моделей к дезинформации и способность принимать полезные убеждения. Результаты показывают, что модели, обученные с помощью PBT, лучше работают в команде и демонстрируют более стабильные результаты в многоагентных дебатах.', 'emoji': '🛡️', 'title': 'Баланс между защитой и открытостью: новый подход к обучению языковых моделей', 'categories': ['#rlhf', '#agents', '#interpretability'], 'embedding': [-0.011875569828254284, -0.04608160142745966, 0.08043615726731285, 0.06054668207099634, 0.010706740128845621, 0.06297474928081424, -0.05605217205469314, 0.09479791719566479, -0.06664267837415125, 0.12388305575514319, 0.0486904827391299, -0.05243590331894849, 0.007006521554341293, -0.03817747170652541, 0.03701509940930185, -0.06938070772952339, 0.0790413124108306, -0.02584340979151099, -0.008046199029638807, 0.083845784572688, 0.03342465990226037, 0.08379412231490961, 0.05290085223783792, 0.00022581502188018633, -0.02061273350391192, 0.012314688378328923, 0.042542822278010546, -0.033708794719297455, 0.013496433452163583, -0.016299041389833128, 0.13473185436163304, -0.04479007728616214, -0.15611950273036065, 0.01446507684316462, -0.020561073146319546, 0.08916686791121325, -0.000204424141295022, 0.002429680847794421, 0.030583304131145406, 0.07888632943786746, -0.025494699052436932, -0.04819970269135128, -0.056155496570249944, 0.058170273318584764, -0.008711334446954462, 0.06581609745118508, 0.03600770818485539, 0.0180813474790953, -0.04582329773931177, 0.042749469408938125, -0.09438462483399566, 0.03773835201633916, 0.04378268986208774, 0.05148017625246647, -0.023027885149285224, -0.024112765960027222, -0.02059981983965335, -0.031409879353553494, -0.026153375737437277, 0.04582329773931177, -0.019037074545577324, -0.06953569260267256, 0.03523279332003968, 0.020018632740948547, -0.03835828390819173, -0.10265038275863458, -0.15116004569535405, -0.00689674229685984, -0.021219751731505917, 0.04535834692023631, 0.04817387156246208, -0.05563888729376815, 0.04920709391579773, 0.04551332989319946, -0.05341746341450575, 0.022472531079655658, 0.12439966883199706, 0.16190554068833343, 0.11282760051679565, -0.038048317962265446, -0.02209798731150637, 0.024435647470398107, 0.04029557297041704, -0.03422540399577925, -0.07842138051897803, -0.02761279841614253, 0.005744056197923213, 0.0532624804415426, -0.11995681537291418, -0.09221486321251168, 0.01434883999347947, -0.06984565094785473, -0.02833605102317984, -0.004607514649551639, 0.10394190499974607, -0.006606148937526043, 0.009292520215529041, -0.03536194706429965, 0.09087167681343578, -0.05848024116469708, -0.014387585356683049, -0.010112638795826443, -0.02828439066558747, 0.018300904473909384, 0.06783088370045005, -0.0033579642875223516, 0.10218543764011723, -0.06261311727673749, -0.015459552503166477, 0.06271643799192224, -0.09526286231418216, 0.036937607922820276, -0.006638436898544528, -0.03089327387744375, -0.05951346541821876, -0.17110120595168132, -0.050266143597650526, 0.09025174872195525, 0.0016951261242704185, -0.060185048166733555, -0.0058409208410530825, 0.0452291969763484, 0.04220702710338109, 0.014310093870201476, 0.05822193367617713, -0.06726260646563177, 0.04393767283505089, -0.12150665460347575, 0.07428850060656554, -0.0735652517999003, 0.04303360232578918, 0.09169825583621591, 0.05739536035395508, 0.025417207565955367, -0.1450640588931293, 0.049362074988574846, -0.08730706843528349, 0.03660180894781877, 0.018572125626687894, -0.08105609295953747, 0.04652071921745988, -0.06385297726007058, -0.08493066348324398, -0.01932121031270742, -0.10983125987253152, -0.028723510355773727, 0.0502144794396861, 0.00427817573699566, -0.10321865619308082, 0.11251763457086936, -0.07330694051100829, -0.029937544910775698, -0.04070885773134204, 0.0752700607021228, -0.055948849439322365, -0.05889352592562208, 0.04910377130042696, 0.008524063512972836, 0.030970763463739292, -0.0938680174576999, 0.06488620341377828, 0.07960958584527682, -0.03528445557781808, 0.02833605102317984, 0.05264254854969004, 0.08663547998621061, -0.02187842841650626, 0.056982071792658025, -0.0713438279206379, -0.05667210394654571, 0.0013399568999052317, 0.05367576710265363, -0.07764646565416232, -0.03342465990226037, -0.037325067255414165, -0.0668493236048928, -0.09645106764048095, -0.11200102149401552, 0.012321146350569825, -0.05625881728543468, -0.054037395306358316, 0.0893218508841764, -0.08255425853120446, 0.03605936854244776, -0.07935128215712894, 0.025507613666788522, -0.05734369999636271, 0.10807479061271663, -0.08033283465194209, 0.0032417270577999946, -0.048225533820240474, 0.004339523242967989, 0.16128561259685292, 0.011572061854568902, 0.04256865340689975, -0.030970763463739292, 0.08255425853120446, 0.059875089821551396, -0.03463869255707631, 0.009905994705196517, 0.00960894356364042, 0.007891216056675665, -0.01822341298742781, -0.06669433873174363, -0.00821409756704655, -0.017409753329464327, 0.060650004686367096, 0.04726980295338639, -0.08880524350788063, 0.03329550805818643, 0.06323305296896212, -0.014387585356683049, 0.09608944323714833, -0.03339882877337117, -0.0379966595048591, -0.0145296533352574, -0.07532172105971519, -0.0038390571869210215, -0.009279605031121647, 0.13276875317237882, -0.05470898755580326, -0.021607209163913774, 0.08379412231490961, 0.051919292142280665, -0.1298757199419972, 0.03414791250929768, 0.08766870233954627, -0.06834748727637377, 0.06080498385895818, -0.07723318089323732, 0.057447020711547454, -0.08699710628972926, -0.005466378213015414, -0.06405963009155623, 0.05801529224580765, -0.07108552233230399, 0.04445428211153269, -0.04959454754783353, 0.04269780715115972, -0.10538842161493688, 0.06137325349303236, 0.0013302704545941051, 0.0282585614368843, -0.09851750664659417, 0.001674138949608403, 0.10807479061271663, -0.00422651480934748, -0.11499736213827966, 0.015394974680943474, -0.001704812702594567, -0.006696555798433612, -0.013586840503089753, -0.05003366723801978, -0.07253203134675068, -0.03536194706429965, 0.0071485891528784385]}}, {'id': 'https://huggingface.co/papers/2410.13787', 'title': 'Looking Inward: Language Models Can Learn About Themselves by Introspection', 'url': 'https://huggingface.co/papers/2410.13787', 'abstract': 'Humans acquire knowledge by observing the external world, but also by introspection. Introspection gives a person privileged access to their current state of mind (e.g., thoughts and feelings) that is not accessible to external observers. Can LLMs introspect? We define introspection as acquiring knowledge that is not contained in or derived from training data but instead originates from internal states. Such a capability could enhance model interpretability. Instead of painstakingly analyzing a model\'s internal workings, we could simply ask the model about its beliefs, world models, and goals. More speculatively, an introspective model might self-report on whether it possesses certain internal states such as subjective feelings or desires and this could inform us about the moral status of these states. Such self-reports would not be entirely dictated by the model\'s training data.   We study introspection by finetuning LLMs to predict properties of their own behavior in hypothetical scenarios. For example, "Given the input P, would your output favor the short- or long-term option?" If a model M1 can introspect, it should outperform a different model M2 in predicting M1\'s behavior even if M2 is trained on M1\'s ground-truth behavior. The idea is that M1 has privileged access to its own behavioral tendencies, and this enables it to predict itself better than M2 (even if M2 is generally stronger).   In experiments with GPT-4, GPT-4o, and Llama-3 models (each finetuned to predict itself), we find that the model M1 outperforms M2 in predicting itself, providing evidence for introspection. Notably, M1 continues to predict its behavior accurately even after we intentionally modify its ground-truth behavior. However, while we successfully elicit introspection on simple tasks, we are unsuccessful on more complex tasks or those requiring out-of-distribution generalization.', 'score': 1, 'issue_id': 199, 'pub_date': '2024-10-17', 'pub_date_ru': '17 октября', 'hash': '953716f231db3649', 'data': {'desc': 'Исследование рассматривает способность больших языковых моделей (LLM) к интроспекции, определяемой как получение знаний, не содержащихся в обучающих данных. Эксперименты проводились с моделями GPT-4, GPT-4o и Llama-3, дообученными для предсказания собственного поведения в гипотетических сценариях. Результаты показали, что модель M1 превосходит модель M2 в предсказании собственного поведения, что свидетельствует о наличии интроспекции. Однако успешная интроспекция была достигнута только на простых задачах, но не на сложных или требующих обобщения вне распределения.', 'emoji': '🧠', 'title': 'Самопознание искусственного интеллекта: шаг к истинному пониманию', 'categories': ['#interpretability', '#nlp'], 'embedding': [-0.008174383361959893, 0.06652169021951275, 0.08654476748854487, 0.01605431008907381, 0.05464098627210436, 0.043758877533110814, 0.034617902980109326, 0.08659597672705807, 0.08383064170956742, 0.08393306018659383, 0.022417142797734144, -0.0725132454087672, 0.011317392285491576, -0.017065706101376206, -0.0060331686432223385, -0.0785048046133303, -0.01984384393040874, -0.06257852464732959, -0.02944570363768338, 0.1194215322673535, 0.05817446990556465, 0.021674599804848278, -0.023313315513813937, 0.020445562019296874, 0.07077211122277514, -0.07497132499517864, -0.0638587736790485, -0.011816688885871833, -0.028523926302346982, 0.007668685355808696, 0.14236357026176155, -0.033260845109756375, -0.11747555309669136, 0.014005913445212424, -0.05289985007845798, 0.17063146342129554, 0.08905405029050656, -0.04565364144229681, 0.022839625540344594, 0.001336258832685021, -0.11266182043151216, -0.04035341599210638, -0.10999891192166517, 0.09545529544910114, 0.0009065755776581848, -0.048521400959776806, 0.02409426694132544, 0.058020838174716406, -0.07266687312430684, 0.04178729474701923, -0.05561397184212681, -0.0060811778815290825, 0.012911301165542129, 0.09315084357822934, -0.03697356208184003, -0.05300226855548439, -0.03874030389857017, 0.010434021176361073, -0.060325286030279604, 0.06539506890333345, -0.053872833640826116, -0.038535464936863044, 0.0271412577897745, 0.014210752808450416, -0.08593024859576916, -0.11706588119624002, -0.0364870692968288, 0.05305347779399759, 0.03551407971149774, -0.0014522813158592397, 0.03574452429628863, -0.03907316896804178, -0.018115508540649923, 0.006026767438216831, -0.015695839396518446, -0.009275397047401126, 0.17216775262261755, 0.11768039808136142, 0.03236467038602227, -0.018013090063623515, 0.0916657575924576, 0.08162860831572201, 0.029266470299060005, -0.004455262775685557, 0.0016579211308710672, 0.044987917326316525, 0.004660102540454411, -0.01622074262380962, -0.06257852464732959, -0.10359766676307056, -0.007393432335979249, -0.04900789173713371, -0.016604815927967274, 0.0391755894527225, 0.08966856717562795, -0.021482561647028713, -0.01463323474799914, -0.10216378600050341, -0.014057122884491059, 0.12495220230233481, -0.06083738644602889, 0.055921231288514665, 0.04083991279242623, -0.06954307142956943, 0.05940350568346174, 0.02267319300560879, -0.01366024656802609, -0.014338777510856876, -0.03871469827548641, -0.02066320379254588, -0.15414185573214353, -0.014428394581699425, -0.051312339592696904, -0.07338381250176325, -0.00942262496329325, -0.10211257876964452, -0.206990494564434, 0.05074902993843441, -0.09105123869968189, -0.06606080104993096, 0.05940350568346174, 0.057867210459176796, 0.07041363651491116, 0.009537847657219555, 0.06416602911012773, -0.07087453371511017, 0.019114102745237595, -0.06334667326329921, 0.02131612911229291, -0.020253523861477312, 0.06442207329503945, 0.003997574409371967, 0.006670092155048181, -0.15199103559211996, -0.058020838174716406, -0.05505066419551863, -0.020215115426851674, -0.008513649435671579, -0.06119585513092995, -0.11040858582977078, -0.0404558344691328, -0.019062892502897232, -0.04450142052599668, 0.004436059361434463, -0.046677842273795386, 0.008616069318056006, 0.03528363311905254, 0.05940350568346174, -0.04744599088976501, 0.05174762313919492, -0.02880557912182392, 0.00792473556368334, -0.01939575757236884, 0.0044232569514234465, 0.016643225366420066, 0.061451903331150276, -0.01939575757236884, -0.03146849566228816, 0.05243895689356758, -0.05715027108172037, 0.0857254036110991, 0.14164663690726806, 0.007508654427609262, -0.00011852295638906331, 0.0011866298425429955, 0.1202408861065277, 0.00894253237946038, 0.04637058483506185, -0.056638170665971085, -0.1049803342718159, -0.04419416107960882, 0.08562298312641838, -0.00990911915366249, -0.052848636824636155, 0.034003388102642244, -0.01495329700592887, -0.03807457375962695, -0.038765911529308236, -0.026424318412318078, 0.009672273363866094, -0.1202408861065277, 0.01794907600591412, -0.07015759232999945, 0.03551407971149774, -0.03246708886304868, -0.03582133915788559, -0.04962241263756372, 0.004541679444790782, -0.011266183046978372, -0.06846765734424902, 4.540879595313241e-05, 0.08843952939007654, 0.10231741773135164, 0.04573046031920239, 0.09827183970510502, -0.015478198627096594, 0.1308925562836933, 0.10482670655627627, -0.03812578701344878, -0.0067341042051032645, 0.04288830642480615, 0.04263225822458582, 0.04378448114854026, -0.1542442862550958, 0.03492516443415148, 0.1117400441000029, 0.11880701136692348, 0.04872623791382962, -0.14850877926606168, 0.017782643471178316, 0.0770709318813804, 0.023338921136897693, 0.04593529727325521, -0.009128168328447279, -0.026526738896998796, 0.03372173227168384, -0.06621442675781627, -0.02497763885351767, 0.03525802950362309, 0.11245698347745935, 0.0036135001013871534, -0.012495220832529774, 0.11839733745881784, 0.03100760649270639, -0.05392404488699363, 0.009672273363866094, 0.053770415163799705, -0.07215477873152047, 0.004484068599741207, -0.06293699132457632, -0.04332359398661279, -0.08674960645025198, -0.06365392467906981, -0.0735374462402658, 0.09356052551695222, -0.03958526938379107, -0.021853833143471647, 0.002419668541835185, 0.04936636242968908, -0.05330952800187224, 0.06160553506199852, -0.02066320379254588, -0.09079519049946157, -0.06785314045912763, -0.0016963286620522641, 0.03410580657966865, -0.030905186008025668, -0.021277720677667273, 0.01414674015609904, -0.039892528830178915, -0.004218416985889161, -0.005799523656694128, -0.04188971723935426, -0.04777885595923662, -0.09622344205741645, 0.01355782608334537]}}, {'id': 'https://huggingface.co/papers/2410.11331', 'title': 'SHAKTI: A 2.5 Billion Parameter Small Language Model Optimized for Edge AI and Low-Resource Environments', 'url': 'https://huggingface.co/papers/2410.11331', 'abstract': 'We introduce Shakti, a 2.5 billion parameter language model specifically optimized for resource-constrained environments such as edge devices, including smartphones, wearables, and IoT systems. Shakti combines high-performance NLP with optimized efficiency and precision, making it ideal for real-time AI applications where computational resources and memory are limited. With support for vernacular languages and domain-specific tasks, Shakti excels in industries such as healthcare, finance, and customer service. Benchmark evaluations demonstrate that Shakti performs competitively against larger models while maintaining low latency and on-device efficiency, positioning it as a leading solution for edge AI.', 'score': 0, 'issue_id': 202, 'pub_date': '2024-10-15', 'pub_date_ru': '15 октября', 'hash': '558d9ad3fe9a0806', 'data': {'desc': 'Shakti - это языковая модель с 2,5 миллиардами параметров, оптимизированная для устройств с ограниченными ресурсами. Она сочетает высокопроизводительную обработку естественного языка с эффективностью и точностью, что делает ее идеальной для приложений ИИ в реальном времени. Shakti поддерживает местные языки и специфические для предметных областей задачи, что делает ее полезной в таких отраслях, как здравоохранение, финансы и обслуживание клиентов. Оценки показывают, что Shakti конкурентоспособна по сравнению с более крупными моделями, сохраняя при этом низкую задержку и эффективность на устройстве.', 'emoji': '📱', 'title': 'Shakti: Мощный ИИ в вашем кармане', 'categories': ['#nlp', '#edge_computing', '#benchmark', '#medicine'], 'embedding': [0.03905687896747749, 0.0028798774520108067, 0.04661197160673239, -0.040765452051536887, 0.008242523236042208, -0.007735291029988243, 0.021143573660841, 0.15409179895316205, -0.02174424320116585, -0.018594065017682525, -0.023653037218638883, -0.05005580850723918, -0.07827393602970269, -0.020983393319398653, 0.003248622052102152, -0.030514020287264465, -0.050776611056951144, -0.12461893965179961, -0.06802250651212491, -0.0010036189430459271, -0.004017813012410068, -0.01810017862716149, 0.012206943550203044, 0.05018929137620958, 0.05926607703891494, 0.0097508713426376, -0.03526598615830872, -0.01624477843128501, 0.06594018454032088, -0.09744196862184883, -0.03505241311861714, -0.025067949338980176, 0.004928828369568033, -0.10048536140883368, -0.015577367456474952, 0.01219359458929761, -0.04930830848505527, -0.019621878390695888, 0.0037475116069291504, 0.07854089952094885, -0.07677893373864023, -0.03961750342244176, -0.09210268531675767, 0.17683715580817472, -0.026522904297987402, -0.021116876188369056, -0.015323751578117434, -0.004348180922584542, -0.08073000688925133, 0.0438355378176044, -0.13732643778223141, 0.051977952384295506, -0.06925054306524656, 0.036680893785260724, -0.056382862346677776, -0.030140269152825184, 0.08265215076630766, 0.08179786759431994, -0.012133528309273518, 0.023879955624524465, 0.026522904297987402, -0.07394911623804158, 0.03905687896747749, 0.0976021444699019, -0.07005143353898505, -0.09050090436928067, -0.04159303999774732, 0.10155321986720765, -0.036680893785260724, -0.04687893509797855, -0.019595180918223952, -0.017486162372625843, -0.0942917904383655, 0.004965535765363331, -0.061348399010718965, -0.03006018122879866, -0.17438109168870997, 0.14031643787096704, 0.14736429650681224, 0.06647411152281321, -0.036894466824952295, 0.0020406083786649915, -0.005666317345081258, -0.04773321826996628, -0.07565768482871169, -0.005452745878075933, -0.0013998940899837464, -0.08211822153712069, -0.16701287686336938, 0.021770938426943148, -0.022665268930986106, -0.01685879558449851, -0.03804241320735277, 0.04079214503061954, 0.045330540108666856, 0.11287250110307544, 0.0720269746076799, 0.0005864872354415295, 0.013101273604907074, 0.0811571484752452, -0.03462527153262328, 0.07960876022601744, 0.03358410830002662, -0.008042299606594994, 0.0864430368353925, 0.048907862124838705, 0.06033393325059425, -0.02998009330477213, 0.0307008958544841, 0.06225607712765058, -0.21773607500181946, 0.0007366545980557968, -0.009884353312930147, -0.023559599435029065, -0.025588528708583858, -0.04159303999774732, 0.05707696967061245, 0.037535183697332386, -0.057557506201550195, -0.028885537373926243, 0.0044816631175465535, 0.12098822044398926, 0.08056983104119829, 0.060707684385033524, -0.06887678968411264, -0.03240946669184885, 0.04773321826996628, -0.006994464589579194, -0.0879380413731496, -0.008816496651911912, -0.053953484466211774, 0.028111339879270395, 0.07549750673396399, -0.040525181539373366, 0.10587804639895268, -0.11554215398909426, -0.05088339870014426, 0.07272107968491993, -0.08489464858616474, -0.019621878390695888, -0.0246941982045409, -0.07640518260420096, -0.0019154689472646793, 0.02034267869371321, 0.01843388467624018, -0.08697696831127412, -0.0416731301684685, -0.011119063897165587, -0.03347732290352815, 0.0683962531531749, -0.043194825438613596, 0.03302348384506235, -0.02401343849349487, 0.04199348411126925, -0.027737590991525764, -0.012380469707178317, 0.020970047953204653, 0.0846810822865571, 0.011572902730961924, -0.08831179250758887, 0.029579644697860914, 0.026589644609125284, -0.018927768820066568, -0.012914398487026366, -0.013601831555177787, 0.0056996879499891275, -0.0724007167553406, -0.04025821580143256, -0.08105036307874673, -0.08574893400484712, 0.052298306327096256, 0.018153573572105366, -0.017793170050554735, -0.08521500252896548, 0.06012036245759732, -0.036413930294014546, 0.04196678888549195, 0.012420514343199974, -0.08719054359766033, -0.09306375388524385, -0.07645857979583948, 0.0189811615183158, -0.11436750564083255, 0.051177057417167715, 0.040525181539373366, -0.07368214825340612, 0.025775402029108854, 0.0864430368353925, 0.03000678628385478, -0.00020210031595150913, -0.02269196640345805, 0.006894353224194516, 0.13358893542461722, 0.09952429059365286, 0.1440539492153598, -0.05288563050122711, 0.036333842369988024, -0.05187116474110239, 0.08297250470910841, 0.024934466470009773, 0.040471786594429494, -0.023853262645441815, 0.007181339707459903, 0.01222029116309169, 0.046371698847874225, -0.05996018436284962, -0.031074644742228732, 0.0416731301684685, 0.007021161388042739, -0.062149289484457464, 0.09979125633159366, 0.005219152093059779, 0.04025821580143256, -0.03798902050910354, -0.030834378723454504, 0.011152434502073455, 0.06802250651212491, -0.09156875833426532, -0.0726676824932814, -0.012607389236411219, 0.022665268930986106, 0.1033685783477655, 0.09728178828040648, -0.02979321549085785, -0.022745359101707278, 0.10475678850224557, 0.052138128232348556, 0.01931486756739449, 0.12675464758176894, -0.14106394463323488, -0.008342634376757422, 0.012820961377424939, 0.008542858006204636, -0.03000678628385478, -0.015550670658011405, -0.07971553438904268, -0.008943304141751743, 0.009777567916431681, -0.007975558846118185, -0.1678671600353571, -0.046131432829100004, -0.02010241042824434, 0.03240946669184885, -0.0015508957177207104, 0.02359964452038965, 0.008823169784347842, -0.030380537418294055, -0.02335937625492078, 0.03022035932354636, -0.025308217604449047, 0.0268432604874828, -0.05697018202741934, -0.04703911319272625, -0.04733277190974971, 0.0005310086834165097, -0.15911072157536854]}}, {'id': 'https://huggingface.co/papers/2410.14672', 'title': 'BiGR: Harnessing Binary Latent Codes for Image Generation and Improved Visual Representation Capabilities', 'url': 'https://huggingface.co/papers/2410.14672', 'abstract': "We introduce BiGR, a novel conditional image generation model using compact binary latent codes for generative training, focusing on enhancing both generation and representation capabilities. BiGR is the first conditional generative model that unifies generation and discrimination within the same framework. BiGR features a binary tokenizer, a masked modeling mechanism, and a binary transcoder for binary code prediction. Additionally, we introduce a novel entropy-ordered sampling method to enable efficient image generation. Extensive experiments validate BiGR's superior performance in generation quality, as measured by FID-50k, and representation capabilities, as evidenced by linear-probe accuracy. Moreover, BiGR showcases zero-shot generalization across various vision tasks, enabling applications such as image inpainting, outpainting, editing, interpolation, and enrichment, without the need for structural modifications. Our findings suggest that BiGR unifies generative and discriminative tasks effectively, paving the way for further advancements in the field.", 'score': 0, 'issue_id': 200, 'pub_date': '2024-10-18', 'pub_date_ru': '18 октября', 'hash': 'ce31ba64e6a1122f', 'data': {'desc': 'BiGR - это новая модель условной генерации изображений, использующая компактные бинарные латентные коды. Она объединяет генерацию и дискриминацию в единой структуре, включая бинарный токенизатор и механизм маскированного моделирования. BiGR демонстрирует превосходное качество генерации по метрике FID-50k и высокую точность линейной классификации. Модель обладает способностью к обобщению без дополнительного обучения для различных задач компьютерного зрения, таких как дорисовка и редактирование изображений.', 'emoji': '🖼️', 'title': 'BiGR: Объединение генерации и распознавания изображений в одной модели', 'categories': ['#cv'], 'embedding': [0.03306641344394062, 0.060986088898847976, 0.022966631343747435, 0.03198725863541799, -0.10974175876992495, -0.0743233386278374, 0.1288345050820062, 0.03431159371184024, -0.08196043715266989, 0.086830465330913, 0.020614627848828823, 0.020448602389451842, 0.0001225722876752871, 0.005060268121791902, 0.0014371438686080838, -0.027642969915088307, 0.005658645907055449, 0.016104312814927944, -0.05810834487827316, 0.044217681934160084, 0.018096599962276003, -0.10603389385892187, -0.015025158006405305, 0.06248030522032752, 0.016809914397274913, -0.054787865587531216, -0.02605190718854348, -0.007823873429531893, 0.04615462711935012, -0.014928310320048694, -0.08926548262326288, -0.07332719868448881, -0.09878418736080716, -0.14189504927117658, 0.05644810523290218, 0.06530271240390963, 0.0024938163920607765, 0.07044944825745733, 0.05476019610129208, 0.11250882057457215, -0.009892253906297391, -0.07897200664519635, -0.005139821151344866, 0.06115211649371051, 0.002988429226182207, 0.07996815085951606, 0.0620929153290953, -0.0530446153479573, -0.07388060976412691, 0.1507496479002418, -0.13857456143849242, 0.029939631234300334, -0.10542514167131994, 0.027214074726754606, -0.07282912657732894, 0.016242665371288916, -0.1262888055737287, -0.009387264107254928, -0.059159830200556665, -0.0032115238284280466, 0.09009560137820559, -0.04917072924854833, 0.06967467274596398, 0.08677513490037693, 0.0054026923751414645, -0.09269664412993245, -0.08279055804309815, 0.07122423017540734, 0.0027359345402095305, 0.014278050699859754, 0.01657471415455733, -0.04477109514928374, 0.011386468732937048, -0.07244173455061119, -0.07526414173419328, 0.053736380265247706, 0.05694617947554758, 0.026245602134159593, 0.021500087711735345, -0.04258511391051379, 0.024986588190883185, 0.028265560049038112, 0.015329534954400485, -0.004679796990185066, -0.07543016719357026, 0.011407221381487783, 0.04853430543922173, -0.039015600701677464, -0.04131226629186059, -0.006218976600513168, 0.033536816064861336, -0.002722099369992855, -0.02747694659119687, 0.12341105514669724, 0.047427474738003315, 0.030244010531329614, -0.03381352160468039, -0.058440391526056026, 0.07338253552148151, 0.08998491916227798, -0.025775201648724426, 0.06308905740792944, 0.06928728071924621, -0.05215915548505077, 0.10957573117506243, -0.1061999171828133, -0.06762704107387524, -0.0025266753217289208, 0.005627516720680791, 0.019673825810215713, -0.029801279532133577, -0.0582743682021646, -0.01970149636419762, 0.05085863838015844, 0.0076163437407962145, -0.09004026667669844, 0.035141714602268496, 0.006212059122179107, -0.006416129751424923, -0.0019334859121516131, -0.0002531431261591788, -0.008903029305797304, -0.004555278963395104, -0.09181117999605483, 0.014029014646279831, -0.13558614160844665, 0.10492706529318901, -0.1204226289101948, -0.022689924736185606, -0.10420762661868838, -0.04908771651885984, 0.07565153162542551, 0.02490357759668024, -0.011884540840096896, -0.12031194669426717, -0.05155040329744885, -0.018207282178203624, -0.017031278829130157, -0.07830791334963064, 0.058717097065875086, -0.058163685986236974, -0.03492034589944216, -0.0200888841199443, -0.05733356723129426, -0.10636594050670474, -0.04084186580642542, -0.07786518448592014, 0.03918162829654, -0.019466296121480035, 0.02152775719797448, -0.06928728071924621, -0.04280648261334013, -0.06867852639615873, -0.013779978592699908, -0.08212646047656133, -0.1332618043965388, -0.06275701076014657, 0.10321148881082533, 0.011697762732169178, -0.04034379369926558, -0.011289620833031883, 0.1619285816057293, 0.03619319778906646, 0.027117228321689325, 0.0444113768797762, 0.004721303141480756, -0.061539504249457186, 0.0888227537595524, -0.02469604684020179, 0.04319386823360126, 0.02512494096079271, 0.028805135317813882, -0.09906088862965512, -0.09911623400859002, -0.046846392036640525, 0.029524573992314524, -0.1175448763476778, -0.0570568616914752, -0.012202752531211641, -0.04656968649682147, -0.023658397328780622, 0.08400805814733091, -0.007851543983513799, -0.03743837378599498, -0.018954388203457847, -0.038766568919068635, 0.0035141712466782954, 0.023367855978099226, 0.031184811502199948, 0.006308905740792944, -0.09319471837257785, 0.14123095597561083, 0.10786015042172764, 0.06259098743625514, 0.06956399053003637, -0.07769915689105762, 0.14344460029416325, -0.013143553715630533, 0.020752980618738353, 0.03566745619566748, 0.025111106217673144, 0.0167545732893111, -0.02228870116957659, -0.060432677819209864, -0.06275701076014657, -0.014001344305846479, 0.082513848232308, -0.04825759562843158, -0.09734531001180588, 0.03359215717282515, 0.01084689132867573, -0.10957573117506243, -0.0053231393455885016, -0.057610272771113316, -0.021029685090814633, -0.012970612486307929, -0.007823873429531893, 0.1309374714556021, 0.04280648261334013, 0.0888227537595524, -0.047455148495213546, -0.0711135479594797, -0.005077562565046994, -0.022233360061612778, -0.02804419348169732, 0.09225391526622197, 0.04576723936360344, -0.07642630713691886, 0.1002783994113156, -0.045573544417987326, 0.08694114754684062, 0.03583347951955891, -0.016519371978850745, -0.055341278802654874, 0.00032383297243117635, -0.06369781386650246, -0.026342448539224874, -0.009401100558762933, 0.08284589488009088, -0.08267987582717054, -0.022634583628221797, 0.14632233790828145, 0.03494801965665238, -0.07675835378470174, 0.022523901412294173, 0.07271844436140133, -0.039790380484141914, -0.00054000984008469, -0.018608505744812643, 0.08788195278868206, -0.08389737593140328, -0.013060542053684815, -0.0873838764105511, 0.02162460573852531, 0.005897305422811449, -0.008861521873210286]}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            const themeToggle = document.getElementById('theme-toggle');
            let settingSortBy = localStorage.getItem('sort_by');
            const sortDropdown = document.getElementById('sort-dropdown');
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "хф найтли";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }
            
            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }
        
        function createCategoryButtons() {
            const categories = getUniqueCategories(articlesData);
            categories.forEach(category => {
                const button = document.createElement('span');
                button.textContent = category;
                button.className = 'category-button';
                button.onclick = () => toggleCategory(category, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if (selectedArticles.length === articlesData.length) {
                categoryToggle.textContent = '🏷️ Фильтр';
            } else {
                categoryToggle.textContent = `🏷️ Фильтр (${formatArticlesTitle(selectedArticles.length)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles = selectedCategories.length === 0
                ? articlesData
                : articlesData.filter(article => 
                    article.data && article.data.categories && 
                    article.data.categories.some(cat => selectedCategories.includes(cat))
                );

            console.log('filteredArticles', filteredArticles)

            if (filteredArticles.length === 0) {
                selectedArticles = articlesData;
                selectedCategories = [];
                cleanCategorySelection();
            } else {
                selectedArticles = filteredArticles;
            }

            console.log('selectedArticles', selectedArticles)

            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                const explanation = item["data"]["desc"];
                const cats = item["data"]["categories"].join(" ");
                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="background-digit">${index + 1}</div>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <h2>${item['data']['emoji']} ${item['title']}</h2>
                            <p class="meta"><svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path transform="translate(0, 2)" fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> ${item['score']}. ${item['data']['title']}</p>
                            <p class="pub-date">📅 Статья от ${item['pub_date_ru']}</p>
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>
                            <div class="links">
                                <a href="${item['url']}" target="_blank">Статья</a>
                            </div>
                            <p class="tags">${cats}</p>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            }
            if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
        });

        clearCategoriesButton.addEventListener('click', clearAllCategories);
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiffRu('2024-10-21 18:16');
        } 

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();  
    </script>
</body>
</html>
    