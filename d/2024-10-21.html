
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF (20 статей)</title>
    <link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #0989eacf;
            --secondary-color: #fff;
            --background-color: #f5f5f5;
            --text-color: #333333;
            --header-color: #0989eacf;
            --body-color: #f5f5f5;
            --menu-color: #002370;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 2em;
            padding: 10px 0 20px 0;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.tags {
            color: #555;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);
            transition: background-color 0.2s ease;
            display: flex;
            flex-direction: column;
            position: relative;
        }
        .article-content {
            padding: 1.5em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
        }
        body.dark-theme>div>main>article {
            background-color: #444;
        }
        body.light-theme>div>main>article {
            background-color: #fff;
        }
        body.dark-theme>div>main>article:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
        }
        .pub-date {
            font-size: 0.9em;
            margin-bottom: 0.8em;
            font-weight: 300;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 1em;
            position: absolute;
            bottom: 10px;
            font-weight: 300;
            font-family: 'Roboto Slab';
        }
        .background-digit {
            position: absolute;
            bottom: -20px;
            right: -10px;
            font-size: 12em;
            font-weight: bold;
            color: rgba(0, 0, 0, 0.03);
            z-index: 0;
            line-height: 1;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 80px;
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        a:hover {
            color: #e73838;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            padding: 1em 0;
            margin-top: 2em;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: fixed;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
        }
        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
        }
        .update-info-container {
            flex: 1;
        }
        .sort-container {
            flex: 2;
        }
        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
                display: block;
                margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .category-toggle {
            display: inline-block;
            margin-bottom: 10px;
            margin-top: 15px;
            cursor: pointer;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 0px;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }
        
        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        body.light-theme>div>main>article.x1124cffc31d2cf8d { background: url("https://hfday.ru/img/20241017/1124cffc31d2cf8d.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x1124cffc31d2cf8d:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x1124cffc31d2cf8d { background: url("https://hfday.ru/img/20241017/1124cffc31d2cf8d.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x1124cffc31d2cf8d:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x7c607e84a2158236 { background: url("https://hfday.ru/img/20241017/7c607e84a2158236.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x7c607e84a2158236:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x7c607e84a2158236 { background: url("https://hfday.ru/img/20241017/7c607e84a2158236.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x7c607e84a2158236:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.xa015f20d9d67a6a8 { background: url("https://hfday.ru/img/20241018/a015f20d9d67a6a8.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.xa015f20d9d67a6a8:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.xa015f20d9d67a6a8 { background: url("https://hfday.ru/img/20241018/a015f20d9d67a6a8.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.xa015f20d9d67a6a8:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.xfa9ce7d280c285b4 { background: url("https://hfday.ru/img/20241017/fa9ce7d280c285b4.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.xfa9ce7d280c285b4:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.xfa9ce7d280c285b4 { background: url("https://hfday.ru/img/20241017/fa9ce7d280c285b4.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.xfa9ce7d280c285b4:hover { background-color: rgba(60,60,60,0.92) !important;}

        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .sort-container {
                margin-top: 0px;
                text-align: left;
                width: 100%;
            .sort-dropdown {
                float: right;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiffRu(dateString) {
        const timeUnits = {
            minute: ["минуту", "минуты", "минут"],
            hour: ["час", "часа", "часов"],
            day: ["день", "дня", "дней"]
        };

        function getRussianPlural(number, words) {
            if (number % 10 === 1 && number % 100 !== 11) {
                return words[0];
            } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                return words[1];
            } else {
                return words[2];
            }
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);

        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes == 0) {
            return 'только что';
        }
        else if (minutes < 60) {
            return `${minutes} ${getRussianPlural(minutes, timeUnits.minute)} назад`;
        } else if (hours < 24) {
            return `${hours} ${getRussianPlural(hours, timeUnits.hour)} назад`;
        } else {
            return `${days} ${getRussianPlural(days, timeUnits.day)} назад`;
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function formatArticlesTitle(number) {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;

        let word;

        if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
            word = "статей";
        } else if (lastDigit === 1) {
            word = "статья";
        } else if (lastDigit >= 2 && lastDigit <= 4) {
            word = "статьи";
        } else {
            word = "статей";
        }

        return `${number} ${word}`;
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p>21 октября | 20 статей</p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item" id="nav-prev"><a href="/d/2024-10-18.html">⬅️ 18.10</a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-10-22.html">➡️ 22.10</a></span>
            <!--<span class="nav-item" id="nav-weekly">Топ за неделю</span>
            <span class="nav-item" id="nav-weekly">Топ за месяц</span>-->
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 Сортировка по</label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="category-toggle">
            <div class="svg-container">
                <span id="category-toggle">🏷️ Фильтр</span>
                <svg height="3" width="200">
                    <line x1="0" y1="0" x2="200" y2="0" 
                        stroke="black" 
                        stroke-width="2" 
                        stroke-dasharray="3, 3" />
                </svg>
            </div>
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">градиент обреченный</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'ru';
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2410.14059', 'title': 'UCFE: A User-Centric Financial Expertise Benchmark for Large Language Models', 'url': 'https://huggingface.co/papers/2410.14059', 'abstract': 'This paper introduces the UCFE: User-Centric Financial Expertise benchmark, an innovative framework designed to evaluate the ability of large language models (LLMs) to handle complex real-world financial tasks. UCFE benchmark adopts a hybrid approach that combines human expert evaluations with dynamic, task-specific interactions to simulate the complexities of evolving financial scenarios. Firstly, we conducted a user study involving 804 participants, collecting their feedback on financial tasks. Secondly, based on this feedback, we created our dataset that encompasses a wide range of user intents and interactions. This dataset serves as the foundation for benchmarking 12 LLM services using the LLM-as-Judge methodology. Our results show a significant alignment between benchmark scores and human preferences, with a Pearson correlation coefficient of 0.78, confirming the effectiveness of the UCFE dataset and our evaluation approach. UCFE benchmark not only reveals the potential of LLMs in the financial sector but also provides a robust framework for assessing their performance and user satisfaction.The benchmark dataset and evaluation code are available.', 'score': 39, 'issue_id': 195, 'pub_date': '2024-10-17', 'pub_date_ru': '17 октября', 'hash': '1124cffc31d2cf8d', 'data': {'categories': ['#benchmark', '#dataset'], 'emoji': '💹', 'ru': {'title': 'UCFE: Новый стандарт для оценки финансового ИИ', 'desc': 'Статья представляет UCFE - новый бенчмарк для оценки способности больших языковых моделей (LLM) решать сложные финансовые задачи. Бенчмарк использует гибридный подход, сочетающий оценки экспертов и динамические взаимодействия для симуляции реальных финансовых сценариев. На основе пользовательского исследования был создан датасет, охватывающий широкий спектр финансовых задач. Результаты тестирования 12 LLM-сервисов показали высокую корреляцию (0.78) между оценками бенчмарка и предпочтениями пользователей.'}, 'en': {'title': 'Empowering LLMs for Financial Mastery', 'desc': 'The paper introduces the UCFE benchmark, a new framework to test how well large language models (LLMs) can handle complex financial tasks. It uses a mix of human expert evaluations and dynamic interactions to mimic real-world financial scenarios. A user study with 804 participants helped create a dataset that reflects various user intents, which was then used to test 12 LLM services. The results showed a strong correlation between the benchmark scores and human preferences, indicating the effectiveness of the UCFE framework in evaluating LLMs in finance.'}, 'zh': {'title': 'UCFE：评估金融领域大型语言模型的新基准', 'desc': '这篇论文介绍了UCFE：用户中心金融专业基准，这是一个创新框架，用于评估大型语言模型处理复杂金融任务的能力。UCFE基准采用混合方法，结合人类专家评估和动态任务特定交互，模拟不断变化的金融场景。研究中，我们收集了804名参与者对金融任务的反馈，并基于此创建了一个包含广泛用户意图和交互的数据集。结果显示，基准分数与人类偏好之间有显著的一致性，表明UCFE数据集和评估方法的有效性。'}}}, {'id': 'https://huggingface.co/papers/2410.13232', 'title': 'Web Agents with World Models: Learning and Leveraging Environment Dynamics in Web Navigation', 'url': 'https://huggingface.co/papers/2410.13232', 'abstract': 'Large language models (LLMs) have recently gained much attention in building autonomous agents. However, the performance of current LLM-based web agents in long-horizon tasks is far from optimal, often yielding errors such as repeatedly buying a non-refundable flight ticket. By contrast, humans can avoid such an irreversible mistake, as we have an awareness of the potential outcomes (e.g., losing money) of our actions, also known as the "world model". Motivated by this, our study first starts with preliminary analyses, confirming the absence of world models in current LLMs (e.g., GPT-4o, Claude-3.5-Sonnet, etc.). Then, we present a World-model-augmented (WMA) web agent, which simulates the outcomes of its actions for better decision-making. To overcome the challenges in training LLMs as world models predicting next observations, such as repeated elements across observations and long HTML inputs, we propose a transition-focused observation abstraction, where the prediction objectives are free-form natural language descriptions exclusively highlighting important state differences between time steps. Experiments on WebArena and Mind2Web show that our world models improve agents\' policy selection without training and demonstrate our agents\' cost- and time-efficiency compared to recent tree-search-based agents.', 'score': 29, 'issue_id': 194, 'pub_date': '2024-10-17', 'pub_date_ru': '17 октября', 'hash': '7c607e84a2158236', 'data': {'categories': ['#agents', '#rl'], 'emoji': '🌐', 'ru': {'title': "Веб-агенты с 'моделью мира': умнее, эффективнее, безопаснее", 'desc': "Эта статья представляет новый подход к улучшению веб-агентов на основе больших языковых моделей (LLM) путем внедрения 'модели мира'. Авторы обнаружили, что существующие LLM-агенты часто совершают ошибки в задачах с долгосрочными последствиями, например, повторно покупая невозвратные билеты. Для решения этой проблемы они разработали World-model-augmented (WMA) веб-агент, который симулирует результаты своих действий перед принятием решений. Эксперименты показали, что этот подход улучшает выбор стратегий агентами и повышает их эффективность по сравнению с агентами, основанными на поиске в дереве решений."}, 'en': {'title': 'Empowering AI with World Models for Smarter Decisions', 'desc': "The paper discusses the limitations of large language models (LLMs) in performing long-horizon tasks autonomously, highlighting their lack of a 'world model' to predict the outcomes of actions. To address this, the authors introduce a World-model-augmented (WMA) web agent that simulates potential outcomes to improve decision-making. They propose a novel approach using transition-focused observation abstraction to train LLMs, which involves predicting important state changes in natural language. Experiments demonstrate that this method enhances policy selection and efficiency compared to traditional tree-search-based agents."}, 'zh': {'title': '增强世界模型：让AI决策更聪明', 'desc': '这篇论文研究了大型语言模型在长时间任务中的表现问题，发现它们缺乏“世界模型”的能力，容易犯不可逆的错误。为了解决这个问题，研究者提出了一种增强世界模型的网络代理，通过模拟行动结果来改进决策。为了克服训练中的挑战，他们设计了一种专注于状态变化的观察抽象方法。实验表明，这种方法提高了代理的决策效率，并且在成本和时间上优于其他方法。'}}}, {'id': 'https://huggingface.co/papers/2410.14669', 'title': 'NaturalBench: Evaluating Vision-Language Models on Natural Adversarial Samples', 'url': 'https://huggingface.co/papers/2410.14669', 'abstract': 'Vision-language models (VLMs) have made significant progress in recent visual-question-answering (VQA) benchmarks that evaluate complex visio-linguistic reasoning. However, are these models truly effective? In this work, we show that VLMs still struggle with natural images and questions that humans can easily answer, which we term natural adversarial samples. We also find it surprisingly easy to generate these VQA samples from natural image-text corpora using off-the-shelf models like CLIP and ChatGPT. We propose a semi-automated approach to collect a new benchmark, NaturalBench, for reliably evaluating VLMs with 10,000 human-verified VQA samples. Crucially, we adopt a vision-centric design by pairing each question with two images that yield different answers, preventing blind solutions from answering without using the images. This makes NaturalBench more challenging than previous benchmarks that can be solved with commonsense priors. We evaluate 53 state-of-the-art VLMs on NaturalBench, showing that models like LLaVA-OneVision, Cambrian-1, Llama3.2-Vision, Molmo, Qwen2-VL, and even GPT-4o lag 50%-70% behind human performance (over 90%). We analyze why NaturalBench is hard from two angles: (1) Compositionality: Solving NaturalBench requires diverse visio-linguistic skills, including understanding attribute bindings, object relationships, and advanced reasoning like logic and counting. To this end, unlike prior work that uses a single tag per sample, we tag each NaturalBench sample with 1 to 8 skill tags for fine-grained evaluation. (2) Biases: NaturalBench exposes severe biases in VLMs, as models often choose the same answer regardless of the image. Lastly, we apply our benchmark curation method to diverse data sources, including long captions (over 100 words) and non-English languages like Chinese and Hindi, highlighting its potential for dynamic evaluations of VLMs.', 'score': 23, 'issue_id': 196, 'pub_date': '2024-10-18', 'pub_date_ru': '18 октября', 'hash': 'a015f20d9d67a6a8', 'data': {'categories': ['#benchmark', '#multimodal'], 'emoji': '🖼️', 'ru': {'title': 'NaturalBench: новый вызов для моделей визуально-языкового понимания', 'desc': 'Исследователи разработали новый бенчмарк NaturalBench для оценки моделей визуально-языкового понимания (VLM) на основе естественных изображений и вопросов. Бенчмарк содержит 10 000 проверенных человеком образцов задач визуального ответа на вопросы (VQA), которые требуют разнообразных навыков визуально-лингвистического рассуждения. Тестирование 53 современных VLM на NaturalBench показало, что даже лучшие модели отстают от человеческой производительности на 50-70%. Исследование выявило серьезные проблемы с композиционностью и предвзятостью в существующих VLM.'}, 'en': {'title': 'Challenging VLMs: Beyond Simple Answers', 'desc': 'Vision-language models (VLMs) have shown progress in visual-question-answering tasks but still struggle with natural adversarial samples, which are questions humans can easily answer. The paper introduces NaturalBench, a new benchmark with 10,000 human-verified samples, designed to challenge VLMs by pairing questions with two images that require different answers. Evaluations show that current VLMs perform significantly worse than humans, highlighting issues with compositionality and biases in these models. The study also demonstrates the potential of using diverse data sources for dynamic evaluations of VLMs.'}, 'zh': {'title': '挑战视觉语言模型的极限：NaturalBench的诞生', 'desc': '这篇论文探讨了视觉语言模型（VLMs）在处理自然图像和问题时的不足，尽管它们在视觉问答（VQA）基准测试中取得了进展。研究者提出了一个新的基准测试，称为NaturalBench，通过配对问题和不同答案的图像来提高测试难度。结果显示，许多先进的VLMs在这个基准测试中表现不佳，与人类表现相差50%-70%。论文还分析了VLMs在组合性和偏见方面的挑战，并展示了如何利用多样化的数据源进行动态评估。'}}}, {'id': 'https://huggingface.co/papers/2410.13370', 'title': 'MagicTailor: Component-Controllable Personalization in Text-to-Image Diffusion Models', 'url': 'https://huggingface.co/papers/2410.13370', 'abstract': 'Recent advancements in text-to-image (T2I) diffusion models have enabled the creation of high-quality images from text prompts, but they still struggle to generate images with precise control over specific visual concepts. Existing approaches can replicate a given concept by learning from reference images, yet they lack the flexibility for fine-grained customization of the individual component within the concept. In this paper, we introduce component-controllable personalization, a novel task that pushes the boundaries of T2I models by allowing users to reconfigure specific components when personalizing visual concepts. This task is particularly challenging due to two primary obstacles: semantic pollution, where unwanted visual elements corrupt the personalized concept, and semantic imbalance, which causes disproportionate learning of the concept and component. To overcome these challenges, we design MagicTailor, an innovative framework that leverages Dynamic Masked Degradation (DM-Deg) to dynamically perturb undesired visual semantics and Dual-Stream Balancing (DS-Bal) to establish a balanced learning paradigm for desired visual semantics. Extensive comparisons, ablations, and analyses demonstrate that MagicTailor not only excels in this challenging task but also holds significant promise for practical applications, paving the way for more nuanced and creative image generation.', 'score': 23, 'issue_id': 192, 'pub_date': '2024-10-17', 'pub_date_ru': '17 октября', 'hash': 'fa9ce7d280c285b4', 'data': {'categories': ['#cv', '#diffusion', '#multimodal'], 'emoji': '🎨', 'ru': {'title': 'Точный контроль над компонентами в персонализированной генерации изображений', 'desc': 'Эта статья представляет новый подход к персонализации генерации изображений с помощью текстовых запросов. Авторы предлагают метод MagicTailor, который позволяет пользователям точно контролировать отдельные компоненты визуальных концепций. Система использует динамическое маскированное ухудшение (DM-Deg) для устранения нежелательных визуальных элементов и двухпоточную балансировку (DS-Bal) для сбалансированного обучения желаемой семантики. Эксперименты показывают, что MagicTailor превосходит существующие методы и открывает новые возможности для креативной генерации изображений.'}, 'en': {'title': 'MagicTailor: Precision in Text-to-Image Creativity', 'desc': 'The paper introduces a new task called component-controllable personalization for text-to-image diffusion models, which allows users to adjust specific parts of a visual concept. This task addresses two main challenges: semantic pollution, where unwanted elements interfere with the concept, and semantic imbalance, where the learning of the concept and its components is uneven. To tackle these issues, the authors propose MagicTailor, a framework using Dynamic Masked Degradation to remove unwanted semantics and Dual-Stream Balancing to ensure balanced learning. The results show that MagicTailor significantly improves the precision and flexibility of image generation, offering new possibilities for creative applications.'}, 'zh': {'title': 'MagicTailor：实现图像生成的精细化控制', 'desc': '近年来，文本到图像扩散模型在从文本生成高质量图像方面取得了显著进展，但在精确控制特定视觉概念上仍存在困难。现有方法可以通过学习参考图像来复制给定概念，但缺乏对概念内各个组件的细粒度定制能力。本文提出了一种新的任务——组件可控个性化，允许用户在个性化视觉概念时重新配置特定组件。为解决语义污染和语义不平衡的问题，我们设计了MagicTailor框架，通过动态掩码降解和双流平衡技术实现更精细的图像生成。'}}}, {'id': 'https://huggingface.co/papers/2410.13276', 'title': 'SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs', 'url': 'https://huggingface.co/papers/2410.13276', 'abstract': 'Attention is the cornerstone of modern Large Language Models (LLMs). Yet its quadratic complexity limits the efficiency and scalability of LLMs, especially for those with a long-context window. A promising approach addressing this limitation is to leverage the sparsity in attention. However, existing sparsity-based solutions predominantly rely on predefined patterns or heuristics to approximate sparsity. This practice falls short to fully capture the dynamic nature of attention sparsity in language-based tasks. This paper argues that attention sparsity should be learned rather than predefined. To this end, we design SeerAttention, a new Attention mechanism that augments the conventional attention with a learnable gate that adaptively selects significant blocks in an attention map and deems the rest blocks sparse. Such block-level sparsity effectively balances accuracy and speedup. To enable efficient learning of the gating network, we develop a customized FlashAttention implementation that extracts the block-level ground truth of attention map with minimum overhead. SeerAttention not only applies to post-training, but also excels in long-context fine-tuning. Our results show that at post-training stages, SeerAttention significantly outperforms state-of-the-art static or heuristic-based sparse attention methods, while also being more versatile and flexible to adapt to varying context lengths and sparsity ratios. When applied to long-context fine-tuning with YaRN, SeerAttention can achieve a remarkable 90% sparsity ratio at a 32k context length with minimal perplexity loss, offering a 5.67x speedup over FlashAttention-2.', 'score': 14, 'issue_id': 195, 'pub_date': '2024-10-17', 'pub_date_ru': '17 октября', 'hash': 'd64d285d002a425d', 'data': {'categories': ['#architecture'], 'emoji': '🔍', 'ru': {'title': 'SeerAttention: Эффективное обучаемое разреженное внимание для LLM', 'desc': 'Статья представляет SeerAttention - новый механизм внимания для больших языковых моделей (LLM). Он использует обучаемые gates для адаптивного выбора значимых блоков в карте внимания, что позволяет эффективно балансировать точность и скорость работы. SeerAttention превосходит существующие методы разреженного внимания и может применяться как после обучения, так и при тонкой настройке на длинных контекстах. Результаты показывают значительное ускорение без существенной потери качества даже при 90% разреженности.'}, 'en': {'title': '"SeerAttention: Learning to Focus, Speeding Up Language Models"', 'desc': 'The paper introduces SeerAttention, a novel attention mechanism designed to improve the efficiency of Large Language Models by learning attention sparsity dynamically rather than relying on predefined patterns. SeerAttention uses a learnable gate to identify and focus on significant blocks in the attention map, enhancing both accuracy and processing speed. The authors also developed a customized FlashAttention implementation to efficiently learn the gating network with minimal computational overhead. Results demonstrate that SeerAttention outperforms existing sparse attention methods, particularly in long-context fine-tuning, achieving high sparsity ratios with minimal loss in performance.'}, 'zh': {'title': 'SeerAttention：自适应学习注意力稀疏性的新方法', 'desc': '这篇论文讨论了大型语言模型中的注意力机制，指出其二次复杂性限制了效率和可扩展性。为了解决这个问题，作者提出了一种新的注意力机制SeerAttention，它通过可学习的门控机制自适应地选择注意力图中的重要块。这样的方法在保持准确性的同时提高了速度。实验结果表明，SeerAttention在长上下文微调中表现出色，能够在高稀疏率下保持低困惑度。'}}}, {'id': 'https://huggingface.co/papers/2410.13925', 'title': 'FiTv2: Scalable and Improved Flexible Vision Transformer for Diffusion Model', 'url': 'https://huggingface.co/papers/2410.13925', 'abstract': 'Nature is infinitely resolution-free. In the context of this reality, existing diffusion models, such as Diffusion Transformers, often face challenges when processing image resolutions outside of their trained domain. To address this limitation, we conceptualize images as sequences of tokens with dynamic sizes, rather than traditional methods that perceive images as fixed-resolution grids. This perspective enables a flexible training strategy that seamlessly accommodates various aspect ratios during both training and inference, thus promoting resolution generalization and eliminating biases introduced by image cropping. On this basis, we present the Flexible Vision Transformer (FiT), a transformer architecture specifically designed for generating images with unrestricted resolutions and aspect ratios. We further upgrade the FiT to FiTv2 with several innovative designs, includingthe Query-Key vector normalization, the AdaLN-LoRA module, a rectified flow scheduler, and a Logit-Normal sampler. Enhanced by a meticulously adjusted network structure, FiTv2 exhibits 2times convergence speed of FiT. When incorporating advanced training-free extrapolation techniques, FiTv2 demonstrates remarkable adaptability in both resolution extrapolation and diverse resolution generation. Additionally, our exploration of the scalability of the FiTv2 model reveals that larger models exhibit better computational efficiency. Furthermore, we introduce an efficient post-training strategy to adapt a pre-trained model for the high-resolution generation. Comprehensive experiments demonstrate the exceptional performance of FiTv2 across a broad range of resolutions. We have released all the codes and models at https://github.com/whlzy/FiT to promote the exploration of diffusion transformer models for arbitrary-resolution image generation.', 'score': 12, 'issue_id': 201, 'pub_date': '2024-10-17', 'pub_date_ru': '17 октября', 'hash': '2109b51982de79e7', 'data': {'categories': ['#architecture', '#diffusion'], 'emoji': '🖼️', 'ru': {'title': 'Гибкая генерация изображений любого разрешения', 'desc': 'Эта статья представляет Flexible Vision Transformer (FiT) - новую архитектуру трансформера для генерации изображений с произвольным разрешением и соотношением сторон. FiT рассматривает изображения как последовательности токенов динамического размера, что позволяет гибко обучать модель на изображениях различных пропорций. Усовершенствованная версия FiTv2 включает ряд инновационных техник, таких как нормализация векторов запросов-ключей и модуль AdaLN-LoRA, что значительно улучшает производительность. Эксперименты показывают исключительную эффективность FiTv2 при генерации изображений широкого диапазона разрешений.'}, 'en': {'title': 'Breaking Free from Fixed Resolutions: The Future of Image Generation', 'desc': 'The paper introduces the Flexible Vision Transformer (FiT), a new model that treats images as sequences of tokens with dynamic sizes, allowing it to handle various resolutions and aspect ratios without bias. This approach improves upon traditional diffusion models that struggle with images outside their trained resolution. The upgraded version, FiTv2, incorporates several innovative features, enhancing convergence speed and adaptability for high-resolution image generation. Experiments show that FiTv2 performs exceptionally well across different resolutions, and the authors have made their code available for further exploration.'}, 'zh': {'title': '灵活视觉变换器：突破分辨率限制的图像生成', 'desc': '这篇论文介绍了一种新的图像生成模型，称为灵活视觉变换器（FiT），它可以处理任意分辨率和长宽比的图像。通过将图像视为动态大小的序列，而不是固定分辨率的网格，FiT能够在训练和推理过程中灵活适应不同的图像比例。升级版FiTv2引入了多项创新设计，使其在分辨率外推和多样化分辨率生成方面表现出色。实验结果表明，FiTv2在各种分辨率下的性能都非常出色，并且代码和模型已公开以促进进一步研究。'}}}, {'id': 'https://huggingface.co/papers/2410.11190', 'title': 'Mini-Omni2: Towards Open-source GPT-4o with Vision, Speech and Duplex Capabilities', 'url': 'https://huggingface.co/papers/2410.11190', 'abstract': 'GPT-4o, an all-encompassing model, represents a milestone in the development of large multi-modal language models. It can understand visual, auditory, and textual modalities, directly output audio, and support flexible duplex interaction. Models from the open-source community often achieve some functionalities of GPT-4o, such as visual understanding and voice chat. Nevertheless, training a unified model that incorporates all modalities is challenging due to the complexities of multi-modal data, intricate model architectures, and training processes. In this paper, we introduce Mini-Omni2, a visual-audio assistant capable of providing real-time, end-to-end voice responses to visoin and audio queries. By integrating pretrained visual and auditory encoders, Mini-Omni2 maintains performance in individual modalities. We propose a three-stage training process to align modalities, allowing the language model to handle multi-modal inputs and outputs after training on a limited dataset. For interaction, we introduce a command-based interruption mechanism, enabling more flexible interaction with users. To the best of our knowledge, Mini-Omni2 is one of the closest reproductions of GPT-4o, which have similar form of functionality, and we hope it can offer valuable insights for subsequent research.', 'score': 12, 'issue_id': 200, 'pub_date': '2024-10-15', 'pub_date_ru': '15 октября', 'hash': '41fce1c87f8f0f35', 'data': {'categories': ['#multimodal', '#audio', '#cv', '#architecture'], 'emoji': '🤖', 'ru': {'title': 'Mini-Omni2: Мультимодальный ассистент нового поколения', 'desc': 'Mini-Omni2 - это мультимодальная модель машинного обучения, способная обрабатывать визуальные, аудио и текстовые данные, а также генерировать голосовые ответы в реальном времени. Модель использует предобученные визуальные и аудио энкодеры для сохранения производительности в отдельных модальностях. Авторы предлагают трехэтапный процесс обучения для выравнивания модальностей и механизм прерывания на основе команд для более гибкого взаимодействия с пользователями. Mini-Omni2 представляет собой одну из ближайших репродукций функциональности GPT-4o, предоставляя ценные insights для дальнейших исследований в области мультимодальных языковых моделей.'}, 'en': {'title': 'Mini-Omni2: Bridging Modalities for Real-Time Interaction', 'desc': 'The paper introduces Mini-Omni2, a model designed to handle visual and auditory inputs and provide real-time voice responses. It uses pretrained encoders for visual and auditory data to maintain high performance across these modalities. A three-stage training process is employed to align these modalities, allowing the model to process multi-modal inputs and outputs effectively. Additionally, a command-based interruption mechanism is introduced to enhance user interaction flexibility.'}, 'zh': {'title': 'Mini-Omni2：多模态交互的未来', 'desc': '这篇论文介绍了Mini-Omni2，一个能够实时处理视觉和音频查询的多模态助手。通过整合预训练的视觉和听觉编码器，Mini-Omni2在单一模态下保持了良好的性能。论文提出了一个三阶段的训练过程，使语言模型能够处理多模态输入和输出。Mini-Omni2还引入了基于命令的中断机制，提供了更灵活的用户交互方式。'}}}, {'id': 'https://huggingface.co/papers/2410.13674', 'title': 'Diffusion Curriculum: Synthetic-to-Real Generative Curriculum Learning via Image-Guided Diffusion', 'url': 'https://huggingface.co/papers/2410.13674', 'abstract': 'Low-quality or scarce data has posed significant challenges for training deep neural networks in practice. While classical data augmentation cannot contribute very different new data, diffusion models opens up a new door to build self-evolving AI by generating high-quality and diverse synthetic data through text-guided prompts. However, text-only guidance cannot control synthetic images\' proximity to the original images, resulting in out-of-distribution data detrimental to the model performance. To overcome the limitation, we study image guidance to achieve a spectrum of interpolations between synthetic and real images. With stronger image guidance, the generated images are similar to the training data but hard to learn. While with weaker image guidance, the synthetic images will be easier for model but contribute to a larger distribution gap with the original data. The generated full spectrum of data enables us to build a novel "Diffusion Curriculum (DisCL)". DisCL adjusts the image guidance level of image synthesis for each training stage: It identifies and focuses on hard samples for the model and assesses the most effective guidance level of synthetic images to improve hard data learning. We apply DisCL to two challenging tasks: long-tail (LT) classification and learning from low-quality data. It focuses on lower-guidance images of high-quality to learn prototypical features as a warm-up of learning higher-guidance images that might be weak on diversity or quality. Extensive experiments showcase a gain of 2.7% and 2.1% in OOD and ID macro-accuracy when applying DisCL to iWildCam dataset. On ImageNet-LT, DisCL improves the base model\'s tail-class accuracy from 4.4% to 23.64% and leads to a 4.02% improvement in all-class accuracy.', 'score': 9, 'issue_id': 193, 'pub_date': '2024-10-17', 'pub_date_ru': '17 октября', 'hash': '61d2689e00439256', 'data': {'categories': ['#dataset', '#data', '#diffusion'], 'emoji': '🔄', 'ru': {'title': 'Адаптивное обучение на синтетических данных для улучшения глубоких нейронных сетей', 'desc': "Эта статья представляет новый метод под названием 'Diffusion Curriculum (DisCL)' для улучшения обучения нейронных сетей на ограниченных или некачественных данных. DisCL использует диффузионные модели для генерации синтетических данных, контролируя их близость к исходным изображениям с помощью управления изображениями. Метод адаптивно регулирует уровень управления изображениями на разных этапах обучения, фокусируясь на сложных примерах и определяя наиболее эффективный уровень синтеза. DisCL показал значительные улучшения в задачах классификации с длинным хвостом и обучении на данных низкого качества."}, 'en': {'title': '"Diffusion Curriculum: Bridging the Data Gap with Smart Synthesis"', 'desc': 'The paper addresses the challenge of training deep neural networks with low-quality or scarce data by using diffusion models to generate diverse synthetic data. It introduces a method called Diffusion Curriculum (DisCL) that adjusts the level of image guidance during training to balance between similarity to real data and ease of learning. DisCL focuses on identifying hard samples and optimizes the guidance level to improve learning from difficult data. The approach shows significant improvements in classification tasks, particularly in long-tail and low-quality data scenarios.'}, 'zh': {'title': '扩散课程：提升深度学习的自我进化能力', 'desc': '这篇论文研究了如何利用扩散模型生成高质量和多样化的合成数据来改善深度神经网络的训练。通过图像引导，研究者们能够在合成图像和真实图像之间实现不同程度的插值，从而克服文本引导的局限性。论文提出了一种新的“扩散课程（DisCL）”方法，通过调整图像合成的引导水平来提高模型对困难样本的学习能力。实验结果表明，DisCL在处理长尾分类和低质量数据学习任务时，显著提高了模型的准确性。'}}}, {'id': 'https://huggingface.co/papers/2410.14677', 'title': 'Are AI Detectors Good Enough? A Survey on Quality of Datasets With Machine-Generated Texts', 'url': 'https://huggingface.co/papers/2410.14677', 'abstract': 'The rapid development of autoregressive Large Language Models (LLMs) has significantly improved the quality of generated texts, necessitating reliable machine-generated text detectors. A huge number of detectors and collections with AI fragments have emerged, and several detection methods even showed recognition quality up to 99.9% according to the target metrics in such collections. However, the quality of such detectors tends to drop dramatically in the wild, posing a question: Are detectors actually highly trustworthy or do their high benchmark scores come from the poor quality of evaluation datasets? In this paper, we emphasise the need for robust and qualitative methods for evaluating generated data to be secure against bias and low generalising ability of future model. We present a systematic review of datasets from competitions dedicated to AI-generated content detection and propose methods for evaluating the quality of datasets containing AI-generated fragments. In addition, we discuss the possibility of using high-quality generated data to achieve two goals: improving the training of detection models and improving the training datasets themselves. Our contribution aims to facilitate a better understanding of the dynamics between human and machine text, which will ultimately support the integrity of information in an increasingly automated world.', 'score': 7, 'issue_id': 195, 'pub_date': '2024-10-18', 'pub_date_ru': '18 октября', 'hash': '1af0065bfe14f3cb', 'data': {'categories': ['#dataset', '#benchmark', '#interpretability', '#survey'], 'emoji': '🕵️', 'ru': {'title': 'Улучшение детекторов AI-текста через повышение качества датасетов', 'desc': 'Статья посвящена проблеме обнаружения текстов, сгенерированных большими языковыми моделями (LLM). Авторы подчеркивают необходимость разработки надежных методов оценки качества датасетов с AI-сгенерированными фрагментами. Они представляют систематический обзор существующих датасетов и предлагают методы оценки их качества. Также обсуждается возможность использования высококачественных сгенерированных данных для улучшения детекторов и самих обучающих датасетов.'}, 'en': {'title': 'Ensuring Trust in AI Text Detection: Beyond Benchmark Scores', 'desc': 'The paper discusses the challenges in detecting machine-generated text from autoregressive Large Language Models (LLMs), highlighting that current detectors often fail in real-world scenarios despite high benchmark scores. It suggests that these scores may be misleading due to poor evaluation datasets, emphasizing the need for robust evaluation methods to ensure reliable detection. The authors review existing datasets and propose new methods to assess their quality, aiming to improve both detection models and training datasets. Ultimately, the paper seeks to enhance the understanding of human and machine text interactions to maintain information integrity.'}, 'zh': {'title': '提升AI文本检测的可靠性：从数据集质量入手', 'desc': '近年来，自回归大型语言模型的发展显著提升了生成文本的质量，因此需要可靠的机器生成文本检测器。虽然许多检测器在特定数据集上的识别率高达99.9%，但在实际应用中效果往往大幅下降。这篇论文强调了评估生成数据的稳健和高质量方法的重要性，以防止未来模型的偏差和低泛化能力。我们提出了评估AI生成内容检测数据集质量的方法，并探讨了利用高质量生成数据来改进检测模型和训练数据集的可能性。'}}}, {'id': 'https://huggingface.co/papers/2410.13726', 'title': 'DAWN: Dynamic Frame Avatar with Non-autoregressive Diffusion Framework for Talking Head Video Generation', 'url': 'https://huggingface.co/papers/2410.13726', 'abstract': 'Talking head generation intends to produce vivid and realistic talking head videos from a single portrait and speech audio clip. Although significant progress has been made in diffusion-based talking head generation, almost all methods rely on autoregressive strategies, which suffer from limited context utilization beyond the current generation step, error accumulation, and slower generation speed. To address these challenges, we present DAWN (Dynamic frame Avatar With Non-autoregressive diffusion), a framework that enables all-at-once generation of dynamic-length video sequences. Specifically, it consists of two main components: (1) audio-driven holistic facial dynamics generation in the latent motion space, and (2) audio-driven head pose and blink generation. Extensive experiments demonstrate that our method generates authentic and vivid videos with precise lip motions, and natural pose/blink movements. Additionally, with a high generation speed, DAWN possesses strong extrapolation capabilities, ensuring the stable production of high-quality long videos. These results highlight the considerable promise and potential impact of DAWN in the field of talking head video generation. Furthermore, we hope that DAWN sparks further exploration of non-autoregressive approaches in diffusion models. Our code will be publicly at https://github.com/Hanbo-Cheng/DAWN-pytorch.', 'score': 6, 'issue_id': 196, 'pub_date': '2024-10-17', 'pub_date_ru': '17 октября', 'hash': '87d3e3c2fd305ae3', 'data': {'categories': ['#video', '#diffusion'], 'emoji': '🗣️', 'ru': {'title': 'DAWN: Революция в генерации видео с говорящей головой', 'desc': 'DAWN - это новый подход к генерации видео с говорящей головой, использующий неавторегрессивные диффузионные модели. В отличие от существующих методов, DAWN генерирует все кадры видео одновременно, что позволяет избежать накопления ошибок и ускорить процесс. Система состоит из двух основных компонентов: генерации общей динамики лица и генерации движений головы и моргания. Эксперименты показывают, что DAWN создает реалистичные видео с точной синхронизацией губ и естественными движениями.'}, 'en': {'title': '"DAWN: Revolutionizing Talking Head Videos with Non-Autoregressive Diffusion"', 'desc': 'The paper introduces DAWN, a new framework for generating talking head videos using non-autoregressive diffusion models. Unlike traditional methods, DAWN generates entire video sequences at once, improving speed and reducing errors. It uses audio cues to create realistic facial dynamics, head poses, and blinks. Experiments show DAWN produces high-quality, long videos with accurate lip-sync and natural movements, showcasing its potential in the field.'}, 'zh': {'title': 'DAWN：非自回归扩散技术革新说话人视频生成', 'desc': '这篇论文介绍了一种名为DAWN的新方法，用于生成逼真的说话人视频。与传统的自回归方法不同，DAWN采用非自回归扩散技术，可以一次性生成动态长度的视频序列。DAWN通过音频驱动的面部动态生成和头部姿态/眨眼生成，能够快速生成高质量的视频。实验表明，DAWN在生成真实生动的视频方面表现出色，并且具有很强的外推能力。'}}}, {'id': 'https://huggingface.co/papers/2410.13782', 'title': 'DPLM-2: A Multimodal Diffusion Protein Language Model', 'url': 'https://huggingface.co/papers/2410.13782', 'abstract': 'Proteins are essential macromolecules defined by their amino acid sequences, which determine their three-dimensional structures and, consequently, their functions in all living organisms. Therefore, generative protein modeling necessitates a multimodal approach to simultaneously model, understand, and generate both sequences and structures. However, existing methods typically use separate models for each modality, limiting their ability to capture the intricate relationships between sequence and structure. This results in suboptimal performance in tasks that requires joint understanding and generation of both modalities. In this paper, we introduce DPLM-2, a multimodal protein foundation model that extends discrete diffusion protein language model (DPLM) to accommodate both sequences and structures. To enable structural learning with the language model, 3D coordinates are converted to discrete tokens using a lookup-free quantization-based tokenizer. By training on both experimental and high-quality synthetic structures, DPLM-2 learns the joint distribution of sequence and structure, as well as their marginals and conditionals. We also implement an efficient warm-up strategy to exploit the connection between large-scale evolutionary data and structural inductive biases from pre-trained sequence-based protein language models. Empirical evaluation shows that DPLM-2 can simultaneously generate highly compatible amino acid sequences and their corresponding 3D structures eliminating the need for a two-stage generation approach. Moreover, DPLM-2 demonstrates competitive performance in various conditional generation tasks, including folding, inverse folding, and scaffolding with multimodal motif inputs, as well as providing structure-aware representations for predictive tasks.', 'score': 5, 'issue_id': 195, 'pub_date': '2024-10-17', 'pub_date_ru': '17 октября', 'hash': '9e7b5af0f52ac4bf', 'data': {'categories': ['#multimodal', '#3d', '#rlhf'], 'emoji': '🧬', 'ru': {'title': 'Единая модель для последовательностей и структур белков', 'desc': 'DPLM-2 — это мультимодальная языковая модель для белков, которая одновременно моделирует последовательности аминокислот и трехмерные структуры. Модель использует дискретную диффузию и квантование для представления 3D-координат в виде токенов. DPLM-2 обучается на экспериментальных и синтетических данных, что позволяет ей изучать совместное распределение последовательностей и структур. Модель демонстрирует высокую эффективность в задачах условной генерации, таких как фолдинг и обратный фолдинг белков.'}, 'en': {'title': "Unifying Protein Sequence and Structure: DPLM-2's Multimodal Mastery", 'desc': 'The paper introduces DPLM-2, a multimodal protein foundation model that integrates both protein sequences and structures into a single framework. By converting 3D structures into discrete tokens, the model learns the joint distribution of sequences and structures, improving the generation and understanding of proteins. DPLM-2 eliminates the need for separate models for sequence and structure, enhancing performance in tasks like folding and scaffolding. The model leverages evolutionary data and structural biases, showing competitive results in generating compatible sequences and structures.'}, 'zh': {'title': 'DPLM-2：同时生成蛋白质序列和结构的多模态模型', 'desc': '蛋白质是由氨基酸序列决定的三维结构的基本大分子，影响其在生物体中的功能。现有的方法通常使用单独的模型来处理序列和结构，限制了对两者之间复杂关系的捕捉能力。本文介绍了一种新的多模态蛋白质基础模型DPLM-2，它可以同时生成和理解蛋白质的序列和结构。通过将3D坐标转换为离散标记，DPLM-2能够在实验和高质量合成结构上进行训练，展示了在多种条件生成任务中的竞争性能。'}}}, {'id': 'https://huggingface.co/papers/2410.10812', 'title': 'HART: Efficient Visual Generation with Hybrid Autoregressive Transformer', 'url': 'https://huggingface.co/papers/2410.10812', 'abstract': 'We introduce Hybrid Autoregressive Transformer (HART), an autoregressive (AR) visual generation model capable of directly generating 1024x1024 images, rivaling diffusion models in image generation quality. Existing AR models face limitations due to the poor image reconstruction quality of their discrete tokenizers and the prohibitive training costs associated with generating 1024px images. To address these challenges, we present the hybrid tokenizer, which decomposes the continuous latents from the autoencoder into two components: discrete tokens representing the big picture and continuous tokens representing the residual components that cannot be represented by the discrete tokens. The discrete component is modeled by a scalable-resolution discrete AR model, while the continuous component is learned with a lightweight residual diffusion module with only 37M parameters. Compared with the discrete-only VAR tokenizer, our hybrid approach improves reconstruction FID from 2.11 to 0.30 on MJHQ-30K, leading to a 31% generation FID improvement from 7.85 to 5.38. HART also outperforms state-of-the-art diffusion models in both FID and CLIP score, with 4.5-7.7x higher throughput and 6.9-13.4x lower MACs. Our code is open sourced at https://github.com/mit-han-lab/hart.', 'score': 4, 'issue_id': 200, 'pub_date': '2024-10-14', 'pub_date_ru': '14 октября', 'hash': 'cc3b200d837522fd', 'data': {'categories': ['#cv', '#architecture', '#diffusion'], 'emoji': '🎨', 'ru': {'title': 'HART: высококачественная генерация изображений с гибридным подходом', 'desc': 'Статья представляет Гибридный Авторегрессионный Трансформер (HART) - модель для генерации изображений высокого качества размером 1024x1024 пикселей. HART использует гибридный токенизатор, который разделяет латентное представление на дискретные токены для общей картины и непрерывные токены для деталей. Модель сочетает масштабируемую авторегрессионную часть для дискретных токенов и легковесный диффузионный модуль для непрерывных. HART превосходит современные диффузионные модели по качеству и эффективности генерации изображений.'}, 'en': {'title': 'HART: Revolutionizing Image Generation with Hybrid Tokenization', 'desc': 'The paper introduces the Hybrid Autoregressive Transformer (HART), a model that generates high-quality 1024x1024 images, competing with diffusion models. It addresses the limitations of existing autoregressive models by using a hybrid tokenizer that combines discrete and continuous tokens for better image reconstruction. This approach significantly improves the Fréchet Inception Distance (FID) scores, indicating better image quality and efficiency. HART also demonstrates superior performance in terms of throughput and computational efficiency compared to state-of-the-art diffusion models.'}, 'zh': {'title': 'HART：突破图像生成极限的混合自回归模型', 'desc': '我们介绍了一种名为混合自回归变压器（HART）的视觉生成模型，可以直接生成1024x1024的高质量图像。传统的自回归模型在图像重建质量和训练成本上存在局限性。为了解决这些问题，我们提出了混合分词器，将连续潜变量分解为离散和连续两部分。与仅使用离散分词器的模型相比，我们的方法在图像重建和生成质量上都有显著提升。'}}}, {'id': 'https://huggingface.co/papers/2410.14470', 'title': 'How Do Training Methods Influence the Utilization of Vision Models?', 'url': 'https://huggingface.co/papers/2410.14470', 'abstract': "Not all learnable parameters (e.g., weights) contribute equally to a neural network's decision function. In fact, entire layers' parameters can sometimes be reset to random values with little to no impact on the model's decisions. We revisit earlier studies that examined how architecture and task complexity influence this phenomenon and ask: is this phenomenon also affected by how we train the model? We conducted experimental evaluations on a diverse set of ImageNet-1k classification models to explore this, keeping the architecture and training data constant but varying the training pipeline. Our findings reveal that the training method strongly influences which layers become critical to the decision function for a given task. For example, improved training regimes and self-supervised training increase the importance of early layers while significantly under-utilizing deeper layers. In contrast, methods such as adversarial training display an opposite trend. Our preliminary results extend previous findings, offering a more nuanced understanding of the inner mechanics of neural networks.   Code: https://github.com/paulgavrikov/layer_criticality", 'score': 3, 'issue_id': 197, 'pub_date': '2024-10-18', 'pub_date_ru': '18 октября', 'hash': 'd5813550dc7a31c9', 'data': {'categories': ['#architecture'], 'emoji': '🧠', 'ru': {'title': 'Метод обучения определяет критичность слоёв нейросети', 'desc': 'Исследование показывает, что не все параметры нейронной сети одинаково важны для её функции принятия решений. Авторы изучили влияние методов обучения на критичность различных слоёв сети для задачи классификации изображений ImageNet-1k. Обнаружено, что улучшенные режимы обучения и самоконтролируемое обучение повышают важность ранних слоёв, в то время как более глубокие слои недоиспользуются. Напротив, методы вроде состязательного обучения демонстрируют противоположную тенденцию.'}, 'en': {'title': 'Training Techniques Shape Neural Network Layer Importance', 'desc': 'This paper explores how different training methods affect which layers in a neural network are crucial for making decisions. By experimenting with various training techniques on ImageNet-1k models, the study finds that training methods like self-supervised learning make early layers more important, while adversarial training emphasizes deeper layers. The research highlights that not all layers are equally important, and their significance can change based on how the model is trained. These insights provide a deeper understanding of neural network mechanics and how training influences layer criticality.'}, 'zh': {'title': '训练方法决定神经网络层的重要性', 'desc': '这篇论文研究了神经网络中不同层的参数对决策功能的重要性。研究发现，训练方法会显著影响哪些层对特定任务变得关键。通过实验，作者发现改进的训练方法和自监督训练增加了早期层的重要性，而对深层的利用较少。相反，对抗训练则显示出相反的趋势。'}}}, {'id': 'https://huggingface.co/papers/2410.13828', 'title': 'A Common Pitfall of Margin-based Language Model Alignment: Gradient Entanglement', 'url': 'https://huggingface.co/papers/2410.13828', 'abstract': 'Reinforcement Learning from Human Feedback (RLHF) has become the predominant approach for language model (LM) alignment. At its core, RLHF uses a margin-based loss for preference optimization, specifying ideal LM behavior only by the difference between preferred and dispreferred responses. In this paper, we identify a common pitfall of margin-based methods -- the under-specification of ideal LM behavior on preferred and dispreferred responses individually, which leads to two unintended consequences as the margin increases: (1) The probability of dispreferred (e.g., unsafe) responses may increase, resulting in potential safety alignment failures. (2) The probability of preferred responses may decrease, even when those responses are ideal. We demystify the reasons behind these problematic behaviors: margin-based losses couple the change in the preferred probability to the gradient of the dispreferred one, and vice versa, often preventing the preferred probability from increasing while the dispreferred one decreases, and thus causing a synchronized increase or decrease in both probabilities. We term this effect, inherent in margin-based objectives, gradient entanglement. Formally, we derive conditions for general margin-based alignment objectives under which gradient entanglement becomes concerning: the inner product of the gradients of preferred and dispreferred log-probabilities is large relative to the individual gradient norms. We theoretically investigate why such inner products can be large when aligning language models and empirically validate our findings. Empirical implications of our framework extend to explaining important differences in the training dynamics of various preference optimization algorithms, and suggesting potential algorithm designs to mitigate the under-specification issue of margin-based methods and thereby improving language model alignment.', 'score': 3, 'issue_id': 195, 'pub_date': '2024-10-17', 'pub_date_ru': '17 октября', 'hash': '725b42ac671f6eec', 'data': {'categories': ['#rlhf', '#alignment'], 'emoji': '🧠', 'ru': {'title': 'Распутывая градиенты: новый взгляд на настройку языковых моделей', 'desc': "Статья рассматривает проблемы, связанные с использованием методов обучения с подкреплением на основе обратной связи от человека (RLHF) для настройки языковых моделей. Авторы выявляют недостатки подходов, основанных на маржинальных потерях, которые могут привести к нежелательному поведению модели. Они вводят понятие 'запутывания градиентов' для объяснения этих эффектов и предлагают теоретический анализ условий, при которых эта проблема становится значимой. Исследование также предлагает потенциальные улучшения алгоритмов оптимизации предпочтений для повышения качества настройки языковых моделей."}, 'en': {'title': 'Untangling Gradients for Safer AI', 'desc': "This paper explores the challenges of using Reinforcement Learning from Human Feedback (RLHF) for aligning language models, focusing on the limitations of margin-based loss methods. It identifies a problem called 'gradient entanglement,' where the probabilities of preferred and dispreferred responses are linked, causing both to increase or decrease together. This can lead to unsafe responses becoming more likely and ideal responses becoming less likely. The authors provide theoretical insights and empirical evidence to explain these issues and suggest ways to improve language model alignment by addressing the under-specification problem in margin-based methods."}, 'zh': {'title': '解决语言模型对齐中的梯度纠缠问题', 'desc': '这篇论文讨论了人类反馈强化学习（RLHF）在语言模型对齐中的应用。作者指出，基于边际损失的方法可能导致理想行为的定义不明确，从而增加不安全响应的概率，并减少理想响应的概率。论文提出了“梯度纠缠”这一概念，解释了偏好和非偏好响应概率变化的相互影响。通过理论分析和实验证明，作者建议改进算法设计以解决这些问题。'}}}, {'id': 'https://huggingface.co/papers/2410.12791', 'title': 'Context is Key(NMF): Modelling Topical Information Dynamics in Chinese Diaspora Media', 'url': 'https://huggingface.co/papers/2410.12791', 'abstract': "Does the People's Republic of China (PRC) interfere with European elections through ethnic Chinese diaspora media? This question forms the basis of an ongoing research project exploring how PRC narratives about European elections are represented in Chinese diaspora media, and thus the objectives of PRC news media manipulation. In order to study diaspora media efficiently and at scale, it is necessary to use techniques derived from quantitative text analysis, such as topic modelling. In this paper, we present a pipeline for studying information dynamics in Chinese media. Firstly, we present KeyNMF, a new approach to static and dynamic topic modelling using transformer-based contextual embedding models. We provide benchmark evaluations to demonstrate that our approach is competitive on a number of Chinese datasets and metrics. Secondly, we integrate KeyNMF with existing methods for describing information dynamics in complex systems. We apply this pipeline to data from five news sites, focusing on the period of time leading up to the 2024 European parliamentary elections. Our methods and results demonstrate the effectiveness of KeyNMF for studying information dynamics in Chinese media and lay groundwork for further work addressing the broader research questions.", 'score': 3, 'issue_id': 195, 'pub_date': '2024-10-16', 'pub_date_ru': '16 октября', 'hash': 'b35aa6a3b1de1ccc', 'data': {'categories': ['#dataset', '#benchmark', '#multilingual'], 'emoji': '🇨🇳', 'ru': {'title': 'Новый метод анализа китайских СМИ для выявления иностранного влияния', 'desc': 'Статья представляет новый подход к моделированию тем в китайских СМИ под названием KeyNMF. Этот метод использует трансформерные контекстные эмбеддинги и показывает конкурентоспособные результаты на нескольких китайских датасетах. Авторы интегрируют KeyNMF с существующими методами анализа динамики информации в сложных системах. Подход применяется к данным пяти новостных сайтов в преддверии выборов в Европарламент 2024 года для изучения влияния КНР на европейские выборы через китайские диаспорные медиа.'}, 'en': {'title': "Unveiling Media Influence: KeyNMF's Role in Decoding PRC Narratives", 'desc': "The paper investigates how the People's Republic of China might influence European elections through Chinese diaspora media. It introduces KeyNMF, a novel method for topic modeling that uses transformer-based contextual embeddings to analyze media content. The study applies this method to data from five news sites, focusing on the period before the 2024 European parliamentary elections. The results show that KeyNMF is effective in understanding information dynamics, providing a foundation for further research on media manipulation."}, 'zh': {'title': '揭示中国媒体对欧洲选举的潜在影响', 'desc': '这篇论文研究了中国如何通过华人媒体影响欧洲选举。研究使用了一种新的主题建模方法，称为KeyNMF，结合了变压器模型的上下文嵌入。通过对五个新闻网站的数据分析，展示了KeyNMF在研究信息动态方面的有效性。此研究为进一步探讨中国媒体操控的广泛问题奠定了基础。'}}}, {'id': 'https://huggingface.co/papers/2410.14672', 'title': 'BiGR: Harnessing Binary Latent Codes for Image Generation and Improved Visual Representation Capabilities', 'url': 'https://huggingface.co/papers/2410.14672', 'abstract': "We introduce BiGR, a novel conditional image generation model using compact binary latent codes for generative training, focusing on enhancing both generation and representation capabilities. BiGR is the first conditional generative model that unifies generation and discrimination within the same framework. BiGR features a binary tokenizer, a masked modeling mechanism, and a binary transcoder for binary code prediction. Additionally, we introduce a novel entropy-ordered sampling method to enable efficient image generation. Extensive experiments validate BiGR's superior performance in generation quality, as measured by FID-50k, and representation capabilities, as evidenced by linear-probe accuracy. Moreover, BiGR showcases zero-shot generalization across various vision tasks, enabling applications such as image inpainting, outpainting, editing, interpolation, and enrichment, without the need for structural modifications. Our findings suggest that BiGR unifies generative and discriminative tasks effectively, paving the way for further advancements in the field.", 'score': 2, 'issue_id': 200, 'pub_date': '2024-10-18', 'pub_date_ru': '18 октября', 'hash': 'ce31ba64e6a1122f', 'data': {'categories': ['#cv'], 'emoji': '🖼️', 'ru': {'title': 'BiGR: Объединение генерации и распознавания изображений в одной модели', 'desc': 'BiGR - это новая модель условной генерации изображений, использующая компактные бинарные латентные коды. Она объединяет генерацию и дискриминацию в единой структуре, включая бинарный токенизатор и механизм маскированного моделирования. BiGR демонстрирует превосходное качество генерации по метрике FID-50k и высокую точность линейной классификации. Модель обладает способностью к обобщению без дополнительного обучения для различных задач компьютерного зрения, таких как дорисовка и редактирование изображений.'}, 'en': {'title': 'BiGR: Unifying Image Generation and Understanding with Binary Codes', 'desc': 'BiGR is a new model for generating images using compact binary codes, which improves both how images are created and understood. It combines the tasks of generating and discriminating images in one system, using tools like a binary tokenizer and a masked modeling mechanism. The model also introduces a new way to sample images efficiently, leading to high-quality results. BiGR can handle various image tasks without changing its structure, showing strong generalization abilities.'}, 'zh': {'title': 'BiGR：统一生成与判别的新纪元', 'desc': '这篇论文介绍了一种新的条件图像生成模型BiGR，它使用紧凑的二进制潜在编码来增强生成和表示能力。BiGR是第一个在同一框架内统一生成和判别的条件生成模型。它具有二进制标记器、掩码建模机制和二进制转码器，并引入了一种新的熵排序采样方法以提高图像生成效率。实验表明，BiGR在生成质量和表示能力上表现优异，并能在多种视觉任务中实现零样本泛化。'}}}, {'id': 'https://huggingface.co/papers/2410.13787', 'title': 'Looking Inward: Language Models Can Learn About Themselves by Introspection', 'url': 'https://huggingface.co/papers/2410.13787', 'abstract': 'Humans acquire knowledge by observing the external world, but also by introspection. Introspection gives a person privileged access to their current state of mind (e.g., thoughts and feelings) that is not accessible to external observers. Can LLMs introspect? We define introspection as acquiring knowledge that is not contained in or derived from training data but instead originates from internal states. Such a capability could enhance model interpretability. Instead of painstakingly analyzing a model\'s internal workings, we could simply ask the model about its beliefs, world models, and goals. More speculatively, an introspective model might self-report on whether it possesses certain internal states such as subjective feelings or desires and this could inform us about the moral status of these states. Such self-reports would not be entirely dictated by the model\'s training data.   We study introspection by finetuning LLMs to predict properties of their own behavior in hypothetical scenarios. For example, "Given the input P, would your output favor the short- or long-term option?" If a model M1 can introspect, it should outperform a different model M2 in predicting M1\'s behavior even if M2 is trained on M1\'s ground-truth behavior. The idea is that M1 has privileged access to its own behavioral tendencies, and this enables it to predict itself better than M2 (even if M2 is generally stronger).   In experiments with GPT-4, GPT-4o, and Llama-3 models (each finetuned to predict itself), we find that the model M1 outperforms M2 in predicting itself, providing evidence for introspection. Notably, M1 continues to predict its behavior accurately even after we intentionally modify its ground-truth behavior. However, while we successfully elicit introspection on simple tasks, we are unsuccessful on more complex tasks or those requiring out-of-distribution generalization.', 'score': 2, 'issue_id': 199, 'pub_date': '2024-10-17', 'pub_date_ru': '17 октября', 'hash': '953716f231db3649', 'data': {'categories': ['#interpretability'], 'emoji': '🧠', 'ru': {'title': 'Самопознание искусственного интеллекта: шаг к истинному пониманию', 'desc': 'Исследование рассматривает способность больших языковых моделей (LLM) к интроспекции, определяемой как получение знаний, не содержащихся в обучающих данных. Эксперименты проводились с моделями GPT-4, GPT-4o и Llama-3, дообученными для предсказания собственного поведения в гипотетических сценариях. Результаты показали, что модель M1 превосходит модель M2 в предсказании собственного поведения, что свидетельствует о наличии интроспекции. Однако успешная интроспекция была достигнута только на простых задачах, но не на сложных или требующих обобщения вне распределения.'}, 'en': {'title': '"Unlocking the Inner Voice of AI: Can Machines Reflect on Themselves?"', 'desc': 'The paper explores whether large language models (LLMs) can introspect, meaning they can understand and report on their internal states beyond what is derived from training data. By finetuning models to predict their own behavior, the study finds that a model can indeed outperform another model in predicting its own actions, suggesting some level of introspection. This ability could improve model interpretability by allowing models to self-report on their beliefs and goals. However, the study notes that while introspection works for simple tasks, it struggles with more complex scenarios.'}, 'zh': {'title': '探索大型语言模型的自我反思能力', 'desc': '这篇论文探讨了大型语言模型（LLM）是否能够进行自我反思，即获取不依赖于训练数据的内部状态知识。研究通过微调模型来预测其在假设场景中的行为，发现模型能够比其他模型更好地预测自身行为，证明了自我反思的可能性。实验表明，尽管在简单任务中成功实现了自我反思，但在复杂任务中仍然存在挑战。自我反思能力可能有助于提高模型的可解释性和道德状态评估。'}}}, {'id': 'https://huggingface.co/papers/2410.11331', 'title': 'SHAKTI: A 2.5 Billion Parameter Small Language Model Optimized for Edge AI and Low-Resource Environments', 'url': 'https://huggingface.co/papers/2410.11331', 'abstract': 'We introduce Shakti, a 2.5 billion parameter language model specifically optimized for resource-constrained environments such as edge devices, including smartphones, wearables, and IoT systems. Shakti combines high-performance NLP with optimized efficiency and precision, making it ideal for real-time AI applications where computational resources and memory are limited. With support for vernacular languages and domain-specific tasks, Shakti excels in industries such as healthcare, finance, and customer service. Benchmark evaluations demonstrate that Shakti performs competitively against larger models while maintaining low latency and on-device efficiency, positioning it as a leading solution for edge AI.', 'score': 1, 'issue_id': 202, 'pub_date': '2024-10-15', 'pub_date_ru': '15 октября', 'hash': '558d9ad3fe9a0806', 'data': {'categories': ['#edge_computing', '#benchmark', '#medicine'], 'emoji': '📱', 'ru': {'title': 'Shakti: Мощный ИИ в вашем кармане', 'desc': 'Shakti - это языковая модель с 2,5 миллиардами параметров, оптимизированная для устройств с ограниченными ресурсами. Она сочетает высокопроизводительную обработку естественного языка с эффективностью и точностью, что делает ее идеальной для приложений ИИ в реальном времени. Shakti поддерживает местные языки и специфические для предметных областей задачи, что делает ее полезной в таких отраслях, как здравоохранение, финансы и обслуживание клиентов. Оценки показывают, что Shakti конкурентоспособна по сравнению с более крупными моделями, сохраняя при этом низкую задержку и эффективность на устройстве.'}, 'en': {'title': 'Shakti: Powering AI on the Edge with Efficiency and Precision', 'desc': 'Shakti is a 2.5 billion parameter language model designed for use in environments with limited computational resources, like smartphones and IoT devices. It offers high-performance natural language processing with optimized efficiency, making it suitable for real-time applications. Shakti supports multiple languages and specific industry tasks, excelling in fields like healthcare and finance. Benchmark tests show that Shakti competes well with larger models, providing low latency and efficient on-device processing.'}, 'zh': {'title': 'Shakti：边缘设备的高效语言模型', 'desc': 'Shakti 是一个拥有 25 亿参数的语言模型，专为资源受限的环境如边缘设备优化。它结合了高性能的自然语言处理和优化的效率与精度，适合实时 AI 应用。Shakti 支持本地语言和特定领域任务，在医疗、金融和客户服务等行业表现出色。基准评估显示，Shakti 在保持低延迟和设备效率的同时，与更大模型竞争力相当。'}}}, {'id': 'https://huggingface.co/papers/2410.14208', 'title': 'Montessori-Instruct: Generate Influential Training Data Tailored for Student Learning', 'url': 'https://huggingface.co/papers/2410.14208', 'abstract': "Synthetic data has been widely used to train large language models, but their generative nature inevitably introduces noisy, non-informative, and misleading learning signals. In this paper, we propose Montessori-Instruct, a novel data synthesis framework that tailors the data synthesis ability of the teacher language model toward the student language model's learning process. Specifically, we utilize local data influence of synthetic training data points on students to characterize students' learning preferences. Then, we train the teacher model with Direct Preference Optimization (DPO) to generate synthetic data tailored toward student learning preferences. Experiments with Llama3-8B-Instruct (teacher) and Llama3-8B (student) on Alpaca Eval and MT-Bench demonstrate that Montessori-Instruct significantly outperforms standard synthesis methods by 18.35\\% and 46.24\\% relatively. Our method also beats data synthesized by a stronger teacher model, GPT-4o. Further analysis confirms the benefits of teacher's learning to generate more influential training data in the student's improved learning, the advantages of local data influence in accurately measuring student preferences, and the robustness of Montessori-Instruct across different student models. Our code and data are open-sourced at https://github.com/cxcscmu/Montessori-Instruct.", 'score': 1, 'issue_id': 200, 'pub_date': '2024-10-18', 'pub_date_ru': '18 октября', 'hash': '46c2fabb0545e954', 'data': {'categories': ['#data', '#rlhf'], 'emoji': '🧠', 'ru': {'title': 'Умное обучение языковых моделей: синтез данных с учетом предпочтений ученика', 'desc': 'В статье представлен новый метод синтеза данных для обучения языковых моделей - Montessori-Instruct. Этот подход адаптирует процесс генерации синтетических данных учительской моделью под предпочтения обучающейся студенческой модели. Используя локальное влияние данных, метод определяет, какие примеры наиболее полезны для обучения студента. Эксперименты показали значительное превосходство Montessori-Instruct над стандартными методами синтеза данных при обучении языковых моделей.'}, 'en': {'title': 'Tailored Learning: Optimizing Synthetic Data for Better AI Training', 'desc': "The paper introduces Montessori-Instruct, a new framework for creating synthetic data that better aligns with the learning needs of student language models. By analyzing how synthetic data influences the student's learning, the framework adjusts the teacher model's data generation process using Direct Preference Optimization. This approach significantly improves the performance of student models compared to traditional methods, as shown in experiments with Llama3-8B models. The study highlights the importance of tailoring synthetic data to student preferences to enhance learning outcomes."}, 'zh': {'title': 'Montessori-Instruct：优化学生学习的合成数据框架', 'desc': '这篇论文介绍了一种名为Montessori-Instruct的新型数据合成框架，用于改善学生语言模型的学习过程。通过分析合成数据对学生模型的局部影响，来了解学生的学习偏好。然后，使用直接偏好优化（DPO）训练教师模型，以生成符合学生学习偏好的合成数据。实验结果表明，Montessori-Instruct在多个评估中显著优于传统方法，并且在不同学生模型中表现出强大的鲁棒性。'}}}, {'id': 'https://huggingface.co/papers/2410.14596', 'title': 'Teaching Models to Balance Resisting and Accepting Persuasion', 'url': 'https://huggingface.co/papers/2410.14596', 'abstract': "Large language models (LLMs) are susceptible to persuasion, which can pose risks when models are faced with an adversarial interlocutor. We take a first step towards defending models against persuasion while also arguing that defense against adversarial (i.e. negative) persuasion is only half of the equation: models should also be able to accept beneficial (i.e. positive) persuasion to improve their answers. We show that optimizing models for only one side results in poor performance on the other. In order to balance positive and negative persuasion, we introduce Persuasion-Balanced Training (or PBT), which leverages multi-agent recursive dialogue trees to create data and trains models via preference optimization to accept persuasion when appropriate. PBT consistently improves resistance to misinformation and resilience to being challenged while also resulting in the best overall performance on holistic data containing both positive and negative persuasion. Crucially, we show that PBT models are better teammates in multi-agent debates. We find that without PBT, pairs of stronger and weaker models have unstable performance, with the order in which the models present their answers determining whether the team obtains the stronger or weaker model's performance. PBT leads to better and more stable results and less order dependence, with the stronger model consistently pulling the weaker one up.", 'score': 1, 'issue_id': 199, 'pub_date': '2024-10-18', 'pub_date_ru': '18 октября', 'hash': 'e8437463a64f466e', 'data': {'categories': ['#rlhf', '#agents', '#interpretability'], 'emoji': '🛡️', 'ru': {'title': 'Баланс между защитой и открытостью: новый подход к обучению языковых моделей', 'desc': 'Исследование посвящено проблеме восприимчивости больших языковых моделей (LLM) к убеждению и предлагает метод Persuasion-Balanced Training (PBT) для её решения. PBT использует многоагентные рекурсивные диалоговые деревья для создания данных и обучает модели с помощью оптимизации предпочтений. Метод улучшает устойчивость моделей к дезинформации и способность принимать полезные убеждения. Результаты показывают, что модели, обученные с помощью PBT, лучше работают в команде и демонстрируют более стабильные результаты в многоагентных дебатах.'}, 'en': {'title': 'Balancing Persuasion: Training Models to Discern and Respond', 'desc': "This paper explores how large language models (LLMs) can be trained to handle both positive and negative persuasion effectively. The authors introduce Persuasion-Balanced Training (PBT), a method that uses multi-agent recursive dialogue trees to optimize models for balanced persuasion. PBT improves models' resistance to misinformation and enhances their performance in multi-agent debates by ensuring that stronger models can consistently uplift weaker ones. The study highlights the importance of training models to discern and appropriately respond to different types of persuasion for better overall performance."}, 'zh': {'title': '说服平衡训练：提升模型抗干扰能力', 'desc': '大型语言模型容易受到说服的影响，这在面对对抗性对话者时可能带来风险。为了平衡正面和负面说服，我们引入了说服平衡训练（PBT），通过多代理递归对话树生成数据，并通过偏好优化训练模型。PBT提高了模型抵抗错误信息的能力，并在包含正负说服的整体数据上表现最佳。PBT使得在多代理辩论中，模型之间的合作更加稳定，强模型能有效提升弱模型的表现。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            const themeToggle = document.getElementById('theme-toggle');
            let settingSortBy = localStorage.getItem('sort_by');
            const sortDropdown = document.getElementById('sort-dropdown');
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }
            
            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents (2)', '#agi', '#alignment (1)', '#architecture (5)', '#audio (1)', '#benchmark (5)', '#cv (4)', '#data (2)', '#dataset (4)', '#diffusion (5)', '#edge_computing (1)', '#ethics', '#games', '#graphs', '#hallucinations', '#inference', '#interpretability (3)', '#math', '#medicine (1)', '#multilingual (1)', '#multimodal (4)', '#optimization', '#plp', '#quantum', '#rag', '#reasoning', '#rl (1)', '#rlhf (4)', '#robotics', '#security', '#story_generation', '#survey (1)', '#training (2)', '#transfer_learning', '#video (1)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = '🏷️ Фильтр';
            } else {
                categoryToggle.textContent = `🏷️ Фильтр (${formatArticlesTitle(selectedArticles.length)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles = selectedCategories.length === 0
                ? articlesData
                : articlesData.filter(article => 
                    article.data && article.data.categories && 
                    article.data.categories.some(cat => selectedCategories.includes(cat))
                );

            console.log('filteredArticles', filteredArticles)

            //if (filteredArticles.length === 0) {
            //    selectedArticles = articlesData;
            //    selectedCategories = [];
            //    cleanCategorySelection();
            //} else {
            //    selectedArticles = filteredArticles;
            //}

            selectedArticles = filteredArticles;

            console.log('selectedArticles', selectedArticles)

            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            console.log(articles);
            let lang = 'ru'
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].join(" ");
                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="background-digit">${index + 1}</div>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <h2>${item['data']['emoji']} ${item['title']}</h2>
                            <p class="meta"><svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path transform="translate(0, 2)" fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> ${item['score']}. ${title}</p>
                            <p class="pub-date">📅 Статья от ${item['pub_date_ru']}</p>
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>
                            <div class="links">
                                <a href="${item['url']}" target="_blank">Статья</a>
                            </div>
                            <p class="tags">${cats}</p>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            }
            if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
        });

        clearCategoriesButton.addEventListener('click', clearAllCategories);
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiffRu('2024-10-21 20:14');
        } 
        function hideNextLink() {
            if (isToday('2024-10-21 20:14')) {
                const element = document.getElementById('nav-next');
                if (element) {    
                    element.style.display = 'none';
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink(); 
        initializeLanguageFlags();
    </script>
</body>
</html>
    