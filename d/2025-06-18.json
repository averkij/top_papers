{
    "date": {
        "ru": "18 –∏—é–Ω—è",
        "en": "June 18",
        "zh": "6Êúà18Êó•"
    },
    "time_utc": "2025-06-18 02:42",
    "weekday": 2,
    "issue_id": 4347,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2506.13642",
            "title": "Stream-Omni: Simultaneous Multimodal Interactions with Large\n  Language-Vision-Speech Model",
            "url": "https://huggingface.co/papers/2506.13642",
            "abstract": "Stream-Omni, a large multimodal model, integrates text, vision, and speech by efficiently aligning modalities using sequence-dimension concatenation for vision and layer-dimension mapping for speech, achieving strong performance with less data.  \t\t\t\t\tAI-generated summary \t\t\t\t The emergence of GPT-4o-like large multimodal models (LMMs) has raised the exploration of integrating text, vision, and speech modalities to support more flexible multimodal interaction. Existing LMMs typically concatenate representation of modalities along the sequence dimension and feed them into a large language model (LLM) backbone. While sequence-dimension concatenation is straightforward for modality integration, it often relies heavily on large-scale data to learn modality alignments. In this paper, we aim to model the relationships between modalities more purposefully, thereby achieving more efficient and flexible modality alignments. To this end, we propose Stream-Omni, a large language-vision-speech model with efficient modality alignments, which can simultaneously support interactions under various modality combinations. Stream-Omni employs LLM as the backbone and aligns the vision and speech to the text based on their relationships. For vision that is semantically complementary to text, Stream-Omni uses sequence-dimension concatenation to achieve vision-text alignment. For speech that is semantically consistent with text, Stream-Omni introduces a CTC-based layer-dimension mapping to achieve speech-text alignment. In this way, Stream-Omni can achieve modality alignments with less data (especially speech), enabling the transfer of text capabilities to other modalities. Experiments on various benchmarks demonstrate that Stream-Omni achieves strong performance on visual understanding, speech interaction, and vision-grounded speech interaction tasks. Owing to the layer-dimensional mapping, Stream-Omni can simultaneously provide intermediate text outputs (such as ASR transcriptions and model responses) during speech interaction, offering users a comprehensive multimodal experience.",
            "score": 13,
            "issue_id": 4347,
            "pub_date": "2025-06-16",
            "pub_date_card": {
                "ru": "16 –∏—é–Ω—è",
                "en": "June 16",
                "zh": "6Êúà16Êó•"
            },
            "hash": "0d0624980a111254",
            "authors": [
                "Shaolei Zhang",
                "Shoutao Guo",
                "Qingkai Fang",
                "Yan Zhou",
                "Yang Feng"
            ],
            "affiliations": [
                "Key Laboratory of AI Safety, Chinese Academy of Sciences",
                "Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences (ICT/CAS)",
                "University of Chinese Academy of Sciences, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.13642.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#audio",
                    "#transfer_learning",
                    "#cv",
                    "#benchmark",
                    "#agi"
                ],
                "emoji": "üîÄ",
                "ru": {
                    "title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª—è –º–æ—â–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò-–º–æ–¥–µ–ª–µ–π",
                    "desc": "Stream-Omni - —ç—Ç–æ –∫—Ä—É–ø–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è —Ç–µ–∫—Å—Ç, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ —Ä–µ—á—å. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏—é –ø–æ –∏–∑–º–µ—Ä–µ–Ω–∏—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–æ –∏–∑–º–µ—Ä–µ–Ω–∏—é —Å–ª–æ–µ–≤ –¥–ª—è —Ä–µ—á–∏, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞—Ç—å –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏. –ú–æ–¥–µ–ª—å –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –≤—ã—Å–æ–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å –º–µ–Ω—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –¥–∞–Ω–Ω—ã—Ö –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –ø–æ–¥—Ö–æ–¥–∞–º–∏. Stream-Omni –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Å–∏–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –∑–∞–¥–∞—á–∞—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è, —Ä–µ—á–µ–≤–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –∏ —Ä–µ—á–µ–≤–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π."
                },
                "en": {
                    "title": "Stream-Omni: Efficient Multimodal Integration for Enhanced Interaction",
                    "desc": "Stream-Omni is a large multimodal model that effectively integrates text, vision, and speech by using innovative alignment techniques. It employs sequence-dimension concatenation for aligning vision with text and a layer-dimension mapping for aligning speech with text, which allows for more efficient learning of modality relationships. This approach reduces the reliance on large datasets, particularly for speech, while still achieving strong performance across various multimodal tasks. The model's design enables it to provide intermediate outputs during speech interactions, enhancing the overall user experience in multimodal applications."
                },
                "zh": {
                    "title": "Stream-OmniÔºöÈ´òÊïàÁöÑÂ§öÊ®°ÊÄÅÊï¥ÂêàÊ®°Âûã",
                    "desc": "Stream-OmniÊòØ‰∏ÄÁßçÂ§ßÂûãÂ§öÊ®°ÊÄÅÊ®°ÂûãÔºåËÉΩÂ§üÊúâÊïàÊï¥ÂêàÊñáÊú¨„ÄÅËßÜËßâÂíåËØ≠Èü≥„ÄÇÂÆÉÈÄöËøáÂ∫èÂàóÁª¥Â∫¶ËøûÊé•ÂÆûÁé∞ËßÜËßâ‰∏éÊñáÊú¨ÁöÑÂØπÈΩêÔºåÂπ∂ÈÄöËøáÂü∫‰∫éCTCÁöÑÂ±ÇÁª¥Â∫¶Êò†Â∞ÑÂÆûÁé∞ËØ≠Èü≥‰∏éÊñáÊú¨ÁöÑÂØπÈΩêÔºå‰ªéËÄåÂú®Êï∞ÊçÆËæÉÂ∞ëÁöÑÊÉÖÂÜµ‰∏ã‰πüËÉΩËææÂà∞ËâØÂ•ΩÁöÑÊÄßËÉΩ„ÄÇËØ•Ê®°ÂûãÊîØÊåÅÂ§öÁßçÊ®°ÊÄÅÁªÑÂêàÁöÑ‰∫§‰∫íÔºåËÉΩÂ§üÂú®ËßÜËßâÁêÜËß£„ÄÅËØ≠Èü≥‰∫§‰∫íÂíåËßÜËßâÂºïÂØºÁöÑËØ≠Èü≥‰∫§‰∫í‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇStream-OmniÁöÑËÆæËÆ°‰ΩøÂæóÁî®Êà∑Âú®ËØ≠Èü≥‰∫§‰∫íÊó∂ÂèØ‰ª•ÂêåÊó∂Ëé∑Âæó‰∏≠Èó¥ÊñáÊú¨ËæìÂá∫ÔºåÊèê‰æõ‰∫ÜÂÖ®Èù¢ÁöÑÂ§öÊ®°ÊÄÅ‰ΩìÈ™å„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.14606",
            "title": "Guaranteed Guess: A Language Modeling Approach for CISC-to-RISC\n  Transpilation with Testing Guarantees",
            "url": "https://huggingface.co/papers/2506.14606",
            "abstract": "A novel ISA-centric transpilation pipeline using LLMs and software testing achieves high correctness and efficiency in translating between complex and reduced hardware architectures.  \t\t\t\t\tAI-generated summary \t\t\t\t The hardware ecosystem is rapidly evolving, with increasing interest in translating low-level programs across different instruction set architectures (ISAs) in a quick, flexible, and correct way to enhance the portability and longevity of existing code. A particularly challenging class of this transpilation problem is translating between complex- (CISC) and reduced- (RISC) hardware architectures, due to fundamental differences in instruction complexity, memory models, and execution paradigms. In this work, we introduce GG (Guaranteed Guess), an ISA-centric transpilation pipeline that combines the translation power of pre-trained large language models (LLMs) with the rigor of established software testing constructs. Our method generates candidate translations using an LLM from one ISA to another, and embeds such translations within a software-testing framework to build quantifiable confidence in the translation. We evaluate our GG approach over two diverse datasets, enforce high code coverage (>98%) across unit tests, and achieve functional/semantic correctness of 99% on HumanEval programs and 49% on BringupBench programs, respectively. Further, we compare our approach to the state-of-the-art Rosetta 2 framework on Apple Silicon, showcasing 1.73x faster runtime performance, 1.47x better energy efficiency, and 2.41x better memory usage for our transpiled code, demonstrating the effectiveness of GG for real-world CISC-to-RISC translation tasks. We will open-source our codes, data, models, and benchmarks to establish a common foundation for ISA-level code translation research.",
            "score": 10,
            "issue_id": 4347,
            "pub_date": "2025-06-17",
            "pub_date_card": {
                "ru": "17 –∏—é–Ω—è",
                "en": "June 17",
                "zh": "6Êúà17Êó•"
            },
            "hash": "c414a1f31e0417da",
            "authors": [
                "Ahmed Heakl",
                "Sarim Hashmi",
                "Chaimaa Abi",
                "Celine Lee",
                "Abdulrahman Mahmoud"
            ],
            "affiliations": [
                "Cornell University",
                "MBZUAI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.14606.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#dataset",
                    "#architecture",
                    "#benchmark",
                    "#data",
                    "#science"
                ],
                "emoji": "üîÑ",
                "ru": {
                    "title": "–Ø–ú–ë –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—ä–µ–¥–∏–Ω—è—é—Ç—Å—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π —Ç—Ä–∞–Ω—Å–ø–∏–ª—è—Ü–∏–∏ –º–µ–∂–¥—É –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞–º–∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ç—Ä–∞–Ω—Å–ø–∏–ª—è—Ü–∏–∏ –ø—Ä–æ–≥—Ä–∞–º–º –º–µ–∂–¥—É —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞–º–∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ –º–µ—Ç–æ–¥–æ–≤ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è. –ú–µ—Ç–æ–¥ GG (Guaranteed Guess) –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤–∞—Ä–∏–∞–Ω—Ç—ã –ø–µ—Ä–µ–≤–æ–¥–∞ –∫–æ–¥–∞ —Å –æ–¥–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –Ω–∞ –¥—Ä—É–≥—É—é —Å –ø–æ–º–æ—â—å—é –Ø–ú–ë –∏ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –∏—Ö –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å —á–µ—Ä–µ–∑ –≤—Å—Ç—Ä–æ–µ–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –≤—ã—Å–æ–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –ø–µ—Ä–µ–≤–æ–¥–∞ (99% –¥–ª—è HumanEval –∏ 49% –¥–ª—è BringupBench) –∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ –Ω–∞–¥ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ —Ä–µ—à–µ–Ω–∏—è–º–∏ –ø–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, —ç–Ω–µ—Ä–≥–æ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –ø–∞–º—è—Ç–∏. –ê–≤—Ç–æ—Ä—ã –ø–ª–∞–Ω–∏—Ä—É—é—Ç –æ—Ç–∫—Ä—ã—Ç—å –∏—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥, –¥–∞–Ω–Ω—ã–µ –∏ –º–æ–¥–µ–ª–∏ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏."
                },
                "en": {
                    "title": "Efficient ISA Translation with Guaranteed Guess",
                    "desc": "This paper presents a new transpilation pipeline called GG (Guaranteed Guess) that focuses on translating programs between complex (CISC) and reduced (RISC) instruction set architectures (ISAs). By leveraging large language models (LLMs) for generating translation candidates, the pipeline integrates software testing to ensure high correctness and efficiency. The authors demonstrate that their approach achieves over 99% functional correctness on specific benchmarks and outperforms the existing Rosetta 2 framework in terms of runtime speed, energy efficiency, and memory usage. The research aims to enhance the portability of code across different hardware architectures and will provide open-source resources for further exploration in ISA-level code translation."
                },
                "zh": {
                    "title": "È´òÊïàÂáÜÁ°ÆÁöÑISAËΩ¨ËØëÊñ∞ÊñπÊ≥ï",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑISA‰∏≠ÂøÉÁöÑËΩ¨ËØëÁÆ°ÈÅìÔºåÂà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂíåËΩØ‰ª∂ÊµãËØïÊäÄÊúØÔºåÂÆûÁé∞‰∫ÜÂú®Â§çÊùÇÂíåÁÆÄÂåñÁ°¨‰ª∂Êû∂ÊûÑ‰πãÈó¥ÁöÑÈ´òÊïà‰∏îÊ≠£Á°ÆÁöÑ‰ª£Á†ÅËΩ¨Êç¢„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáLLMÁîüÊàêÂÄôÈÄâÁøªËØëÔºåÂπ∂Â∞ÜÂÖ∂ÂµåÂÖ•ËΩØ‰ª∂ÊµãËØïÊ°ÜÊû∂‰∏≠Ôºå‰ª•ÊèêÈ´òÁøªËØëÁöÑÂèØÈù†ÊÄß„ÄÇÊàë‰ª¨Âú®‰∏§‰∏™‰∏çÂêåÁöÑÊï∞ÊçÆÈõÜ‰∏äËØÑ‰º∞‰∫ÜËØ•ÊñπÊ≥ïÔºåËææÂà∞‰∫Ü99%ÁöÑÂäüËÉΩÂíåËØ≠‰πâÊ≠£Á°ÆÁéáÔºåÂπ∂‰∏îÂú®ÊÄßËÉΩ‰∏ä‰ºò‰∫éÁé∞ÊúâÁöÑRosetta 2Ê°ÜÊû∂„ÄÇÊúÄÁªàÔºåÊàë‰ª¨Â∞ÜÂºÄÊ∫ê‰ª£Á†Å„ÄÅÊï∞ÊçÆ„ÄÅÊ®°ÂûãÂíåÂü∫ÂáÜÔºå‰ª•Êé®Âä®ISAÁ∫ß‰ª£Á†ÅÁøªËØëÁ†îÁ©∂ÁöÑÂèëÂ±ï„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.14429",
            "title": "LongLLaDA: Unlocking Long Context Capabilities in Diffusion LLMs",
            "url": "https://huggingface.co/papers/2506.14429",
            "abstract": "This study investigates long-context performance of diffusion LLMs compared to auto-regressive LLMs, identifies their unique characteristics, and proposes LongLLaDA, a training-free method for extending context windows.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Diffusion Models, or diffusion LLMs, have emerged as a significant focus in NLP research, with substantial effort directed toward understanding their scalability and downstream task performance. However, their long-context capabilities remain unexplored, lacking systematic analysis or methods for context extension. In this work, we present the first systematic investigation comparing the long-context performance of diffusion LLMs and traditional auto-regressive LLMs. We first identify a unique characteristic of diffusion LLMs, unlike auto-regressive LLMs, they maintain remarkably \\textit{stable perplexity} during direct context extrapolation. Furthermore, where auto-regressive models fail outright during the Needle-In-A-Haystack task with context exceeding their pretrained length, we discover diffusion LLMs exhibit a distinct \\textit{local perception} phenomenon, enabling successful retrieval from recent context segments. We explain both phenomena through the lens of Rotary Position Embedding (RoPE) scaling theory. Building on these observations, we propose LongLLaDA, a training-free method that integrates LLaDA with the NTK-based RoPE extrapolation. Our results validate that established extrapolation scaling laws remain effective for extending the context windows of diffusion LLMs. Furthermore, we identify long-context tasks where diffusion LLMs outperform auto-regressive LLMs and others where they fall short. Consequently, this study establishes the first context extrapolation method for diffusion LLMs while providing essential theoretical insights and empirical benchmarks critical for advancing future research on long-context diffusion LLMs.",
            "score": 4,
            "issue_id": 4347,
            "pub_date": "2025-06-17",
            "pub_date_card": {
                "ru": "17 –∏—é–Ω—è",
                "en": "June 17",
                "zh": "6Êúà17Êó•"
            },
            "hash": "d0032538675516d6",
            "authors": [
                "Xiaoran Liu",
                "Zhigeng Liu",
                "Zengfeng Huang",
                "Qipeng Guo",
                "Ziwei He",
                "Xipeng Qiu"
            ],
            "affiliations": [
                "School of Computer Science, Fudan University",
                "Shanghai AI Lab",
                "Shanghai Innovation Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.14429.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#long_context",
                    "#architecture",
                    "#benchmark",
                    "#diffusion",
                    "#rl"
                ],
                "emoji": "üî¨",
                "ru": {
                    "title": "–î–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏: –Ω–æ–≤—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞",
                    "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –∏ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —Å—Ç–∞–±–∏–ª—å–Ω—É—é –ø–µ—Ä–ø–ª–µ–∫—Å–∏–≤–Ω–æ—Å—Ç—å –ø—Ä–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ –æ–±–ª–∞–¥–∞—é—Ç —Ñ–µ–Ω–æ–º–µ–Ω–æ–º '–ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è'. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–∏—Ö –Ω–∞–±–ª—é–¥–µ–Ω–∏–π –±—ã–ª —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω –º–µ—Ç–æ–¥ LongLLaDA –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –æ–∫–Ω–∞ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç–∞–∫–∂–µ –≤—ã—è–≤–∏–ª–æ –∑–∞–¥–∞—á–∏, –≤ –∫–æ—Ç–æ—Ä—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–µ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º."
                },
                "en": {
                    "title": "Unlocking Long Contexts in Diffusion LLMs with LongLLaDA",
                    "desc": "This paper explores how diffusion large language models (LLMs) perform with long contexts compared to traditional auto-regressive LLMs. It highlights that diffusion LLMs maintain stable perplexity when extending context, unlike their auto-regressive counterparts, which struggle with longer inputs. The authors introduce LongLLaDA, a method that allows for context window extension without additional training, leveraging insights from Rotary Position Embedding (RoPE) scaling. The findings reveal specific tasks where diffusion LLMs excel and others where they do not, paving the way for future research in long-context applications."
                },
                "zh": {
                    "title": "Êâ©Êï£Ê®°ÂûãÁöÑÈïø‰∏ä‰∏ãÊñáÊñ∞ÊñπÊ≥ïÔºöLongLLaDA",
                    "desc": "Êú¨Á†îÁ©∂Êé¢ËÆ®‰∫ÜÊâ©Êï£Â§ßËØ≠Ë®ÄÊ®°ÂûãÔºàdiffusion LLMsÔºâ‰∏éËá™ÂõûÂΩíÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàauto-regressive LLMsÔºâÂú®Èïø‰∏ä‰∏ãÊñáÊÄßËÉΩÊñπÈù¢ÁöÑÊØîËæÉÔºåËØÜÂà´‰∫ÜÂÆÉ‰ª¨ÁöÑÁã¨ÁâπÁâπÊÄßÔºåÂπ∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊó†ËÆ≠ÁªÉÁöÑÊñπÊ≥ïLongLLaDAÊù•Êâ©Â±ï‰∏ä‰∏ãÊñáÁ™óÂè£„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÊâ©Êï£LLMsÂú®Áõ¥Êé•‰∏ä‰∏ãÊñáÂ§ñÊé®Êó∂‰øùÊåÅ‰∫ÜÊòæËëóÁ®≥ÂÆöÁöÑÂõ∞ÊÉëÂ∫¶ÔºåËÄåËá™ÂõûÂΩíÊ®°ÂûãÂú®‰∏ä‰∏ãÊñáË∂ÖÂá∫È¢ÑËÆ≠ÁªÉÈïøÂ∫¶Êó∂ÂàôË°®Áé∞‰∏ç‰Ω≥„ÄÇÊâ©Êï£LLMsÂ±ïÁé∞Âá∫Áã¨ÁâπÁöÑÂ±ÄÈÉ®ÊÑüÁü•Áé∞Ë±°Ôºå‰ΩøÂÖ∂ËÉΩÂ§üÊàêÂäü‰ªéÊúÄËøëÁöÑ‰∏ä‰∏ãÊñáÁâáÊÆµ‰∏≠Ê£ÄÁ¥¢‰ø°ÊÅØ„ÄÇÈÄöËøáÊóãËΩ¨‰ΩçÁΩÆÂµåÂÖ•ÔºàRoPEÔºâÁº©ÊîæÁêÜËÆ∫ÔºåÊàë‰ª¨Ëß£Èáä‰∫ÜËøô‰∫õÁé∞Ë±°ÔºåÂπ∂È™åËØÅ‰∫ÜÊâ©Êï£LLMsÁöÑ‰∏ä‰∏ãÊñáÂ§ñÊé®ÊñπÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.14002",
            "title": "Taming Polysemanticity in LLMs: Provable Feature Recovery via Sparse\n  Autoencoders",
            "url": "https://huggingface.co/papers/2506.14002",
            "abstract": "A new statistical framework and training algorithm, Group Bias Adaptation, enhance Sparse Autoencoders for recovering monosemantic features in Large Language Models, offering theoretical guarantees and superior performance.  \t\t\t\t\tAI-generated summary \t\t\t\t We study the challenge of achieving theoretically grounded feature recovery using Sparse Autoencoders (SAEs) for the interpretation of Large Language Models. Existing SAE training algorithms often lack rigorous mathematical guarantees and suffer from practical limitations such as hyperparameter sensitivity and instability. To address these issues, we first propose a novel statistical framework for the feature recovery problem, which includes a new notion of feature identifiability by modeling polysemantic features as sparse mixtures of underlying monosemantic concepts. Building on this framework, we introduce a new SAE training algorithm based on ``bias adaptation'', a technique that adaptively adjusts neural network bias parameters to ensure appropriate activation sparsity. We theoretically prove that this algorithm correctly recovers all monosemantic features when input data is sampled from our proposed statistical model. Furthermore, we develop an improved empirical variant, Group Bias Adaptation (GBA), and demonstrate its superior performance against benchmark methods when applied to LLMs with up to 1.5 billion parameters. This work represents a foundational step in demystifying SAE training by providing the first SAE algorithm with theoretical recovery guarantees, thereby advancing the development of more transparent and trustworthy AI systems through enhanced mechanistic interpretability.",
            "score": 3,
            "issue_id": 4347,
            "pub_date": "2025-06-16",
            "pub_date_card": {
                "ru": "16 –∏—é–Ω—è",
                "en": "June 16",
                "zh": "6Êúà16Êó•"
            },
            "hash": "f27daadffcce7200",
            "authors": [
                "Siyu Chen",
                "Heejune Sheen",
                "Xuyuan Xiong",
                "Tianhao Wang",
                "Zhuoran Yang"
            ],
            "affiliations": [
                "Antai College of Economics and Management, Shanghai Jiao Tong University",
                "Department of Statistics and Data Science, Yale University",
                "Toyota Technological Institute at Chicago"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.14002.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#interpretability",
                    "#math",
                    "#optimization"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–ü—Ä–æ—Ä—ã–≤ –≤ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π: —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –Ω–æ–≤—ã–π —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –∏ –∞–ª–≥–æ—Ä–∏—Ç–º –æ–±—É—á–µ–Ω–∏—è –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Group Bias Adaptation –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã—Ö –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–æ–≤ (Sparse Autoencoders). –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–∑–≤–ª–µ–∫–∞—Ç—å –º–æ–Ω–æ—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (Large Language Models) —Å —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–º–∏ –≥–∞—Ä–∞–Ω—Ç–∏—è–º–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–æ–≤–æ–µ –ø–æ–Ω—è—Ç–∏–µ –∏–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä—É–µ–º–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –º–æ–¥–µ–ª–∏—Ä—É—è –ø–æ–ª–∏—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∫–∞–∫ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ —Å–º–µ—Å–∏ –º–æ–Ω–æ—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–µ—Ç–æ–¥–∞ –Ω–∞ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö —Å –¥–æ 1,5 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —ç—Ç–∞–ª–æ–Ω–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏."
                },
                "en": {
                    "title": "Enhancing Feature Recovery in Language Models with Group Bias Adaptation",
                    "desc": "This paper introduces a new method called Group Bias Adaptation (GBA) to improve Sparse Autoencoders (SAEs) for extracting clear features from Large Language Models (LLMs). The authors address the limitations of existing SAE training methods, which often lack solid mathematical backing and can be unstable. They propose a statistical framework that models complex features as combinations of simpler, clear concepts, ensuring better feature recovery. The new training algorithm not only provides theoretical guarantees for recovering these features but also shows better performance compared to traditional methods when tested on large models."
                },
                "zh": {
                    "title": "Áæ§‰ΩìÂÅèÂ∑ÆÈÄÇÂ∫îÔºöÊèêÂçáÁ®ÄÁñèËá™ÁºñÁ†ÅÂô®ÁöÑÂçï‰πâÁâπÂæÅÊÅ¢Â§çËÉΩÂäõ",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÁªüËÆ°Ê°ÜÊû∂ÂíåËÆ≠ÁªÉÁÆóÊ≥ïÔºåÁß∞‰∏∫Áæ§‰ΩìÂÅèÂ∑ÆÈÄÇÂ∫îÔºàGroup Bias AdaptationÔºâÔºåÊó®Âú®Â¢ûÂº∫Á®ÄÁñèËá™ÁºñÁ†ÅÂô®ÔºàSparse AutoencodersÔºâÂú®Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑÂçï‰πâÁâπÂæÅÊÅ¢Â§çËÉΩÂäõ„ÄÇÁé∞ÊúâÁöÑÁ®ÄÁñèËá™ÁºñÁ†ÅÂô®ËÆ≠ÁªÉÁÆóÊ≥ïÁº∫‰πè‰∏•Ê†ºÁöÑÊï∞Â≠¶‰øùËØÅÔºåÂπ∂‰∏îÂú®Ë∂ÖÂèÇÊï∞ÊïèÊÑüÊÄßÂíå‰∏çÁ®≥ÂÆöÊÄßÊñπÈù¢Â≠òÂú®ÂÆûÈôÖÈôêÂà∂„ÄÇÊàë‰ª¨ÈÄöËøáÂºïÂÖ•ÁâπÂæÅÂèØËØÜÂà´ÊÄßÁöÑÊñ∞Ê¶ÇÂøµÔºåËß£ÂÜ≥‰∫ÜÁâπÂæÅÊÅ¢Â§çÈóÆÈ¢òÔºåÂπ∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÂÅèÂ∑ÆÈÄÇÂ∫îÁöÑÊñ∞ËÆ≠ÁªÉÁÆóÊ≥ï„ÄÇÁêÜËÆ∫ËØÅÊòéËØ•ÁÆóÊ≥ïËÉΩÂ§üÂú®ÁâπÂÆöÁªüËÆ°Ê®°Âûã‰∏ãÊ≠£Á°ÆÊÅ¢Â§çÊâÄÊúâÂçï‰πâÁâπÂæÅÔºå‰ªéËÄå‰∏∫Á®ÄÁñèËá™ÁºñÁ†ÅÂô®ÁöÑËÆ≠ÁªÉÊèê‰æõ‰∫ÜÁêÜËÆ∫ÊîØÊåÅ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.14603",
            "title": "Align Your Flow: Scaling Continuous-Time Flow Map Distillation",
            "url": "https://huggingface.co/papers/2506.14603",
            "abstract": "Flow maps, introduced with new continuous-time objectives and training techniques, achieve state-of-the-art performance in few-step image and text-to-image generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion- and flow-based models have emerged as state-of-the-art generative modeling approaches, but they require many sampling steps. Consistency models can distill these models into efficient one-step generators; however, unlike flow- and diffusion-based methods, their performance inevitably degrades when increasing the number of steps, which we show both analytically and empirically. Flow maps generalize these approaches by connecting any two noise levels in a single step and remain effective across all step counts. In this paper, we introduce two new continuous-time objectives for training flow maps, along with additional novel training techniques, generalizing existing consistency and flow matching objectives. We further demonstrate that autoguidance can improve performance, using a low-quality model for guidance during distillation, and an additional boost can be achieved by adversarial finetuning, with minimal loss in sample diversity. We extensively validate our flow map models, called Align Your Flow, on challenging image generation benchmarks and achieve state-of-the-art few-step generation performance on both ImageNet 64x64 and 512x512, using small and efficient neural networks. Finally, we show text-to-image flow map models that outperform all existing non-adversarially trained few-step samplers in text-conditioned synthesis.",
            "score": 2,
            "issue_id": 4347,
            "pub_date": "2025-06-17",
            "pub_date_card": {
                "ru": "17 –∏—é–Ω—è",
                "en": "June 17",
                "zh": "6Êúà17Êó•"
            },
            "hash": "c235653dff87ea28",
            "authors": [
                "Amirmojtaba Sabour",
                "Sanja Fidler",
                "Karsten Kreis"
            ],
            "affiliations": [
                "NVIDIA",
                "University of Toronto",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.14603.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#cv",
                    "#benchmark",
                    "#optimization",
                    "#diffusion",
                    "#small_models"
                ],
                "emoji": "üåä",
                "ru": {
                    "title": "Flow maps: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–º—É –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—é –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º 'flow maps'. –≠—Ç–∞ —Ç–µ—Ö–Ω–∏–∫–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Å–æ–µ–¥–∏–Ω—è—Ç—å –ª—é–±—ã–µ –¥–≤–∞ —É—Ä–æ–≤–Ω—è —à—É–º–∞ –∑–∞ –æ–¥–∏–Ω —à–∞–≥, —Å–æ—Ö—Ä–∞–Ω—è—è –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø—Ä–∏ —Ä–∞–∑–ª–∏—á–Ω–æ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ —à–∞–≥–æ–≤. –ê–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç –Ω–æ–≤—ã–µ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–µ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ —Ü–µ–ª–µ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∏ —Ç–µ—Ö–Ω–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è flow maps. –ú–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ —Å –ø–æ–º–æ—â—å—é —ç—Ç–æ–≥–æ –º–µ—Ç–æ–¥–∞, –¥–æ—Å—Ç–∏–≥–∞—é—Ç –Ω–∞–∏–ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–∏ —Ç–µ–∫—Å—Ç–∞ –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∑–∞ –Ω–µ–±–æ–ª—å—à–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤."
                },
                "en": {
                    "title": "Flow Maps: Efficient Few-Step Generative Modeling",
                    "desc": "This paper introduces flow maps, a new approach in generative modeling that connects different noise levels in a single step, allowing for efficient image and text-to-image generation. Unlike traditional diffusion and consistency models, which require many sampling steps and degrade in performance with increased steps, flow maps maintain effectiveness across all step counts. The authors propose two continuous-time training objectives and novel techniques that enhance the training of flow maps, including autoguidance and adversarial finetuning. The results demonstrate that their method, called Align Your Flow, achieves state-of-the-art performance in few-step generation tasks on various benchmarks, outperforming existing models in both image and text-conditioned synthesis."
                },
                "zh": {
                    "title": "ÊµÅÂõæÊ®°ÂûãÔºöÈ´òÊïàÁöÑÂ∞ëÊ≠•È™§ÁîüÊàêÊñ∞ÊñπÊ≥ï",
                    "desc": "Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊµÅÂõæÊ®°ÂûãÔºåÊó®Âú®ÊèêÈ´òÂõæÂÉèÂíåÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàêÁöÑÊïàÁéá„ÄÇÊµÅÂõæÈÄöËøáÂú®Âçï‰∏ÄÊ≠•È™§‰∏≠ËøûÊé•‰ªªÊÑè‰∏§‰∏™Âô™Â£∞Ê∞¥Âπ≥ÔºåÂÖãÊúç‰∫Ü‰º†ÁªüÊâ©Êï£ÂíåÊµÅÊ®°ÂûãÂú®Â§öÊ≠•È™§ÈááÊ†∑‰∏≠ÁöÑÊÄßËÉΩ‰∏ãÈôçÈóÆÈ¢ò„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏§ÁßçÊñ∞ÁöÑËøûÁª≠Êó∂Èó¥ÁõÆÊ†áÂíåËÆ≠ÁªÉÊäÄÊúØÔºåËøõ‰∏ÄÊ≠•‰ºòÂåñ‰∫ÜÊµÅÂõæÁöÑËÆ≠ÁªÉËøáÁ®ã„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊµÅÂõæÊ®°ÂûãÂú®ÂõæÂÉèÁîüÊàêÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂ∞§ÂÖ∂ÊòØÂú®Â∞ëÊ≠•È™§ÁîüÊàê‰ªªÂä°‰∏≠ÔºåËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.14755",
            "title": "Optimizing Length Compression in Large Reasoning Models",
            "url": "https://huggingface.co/papers/2506.14755",
            "abstract": "LC-R1, a post-training method guided by Brevity and Sufficiency principles, reduces unnecessary reasoning in Large Reasoning Models with minimal accuracy loss.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Reasoning Models (LRMs) have achieved remarkable success, yet they often suffer from producing unnecessary and verbose reasoning chains. We identify a core aspect of this issue as \"invalid thinking\" -- models tend to repeatedly double-check their work after having derived the correct answer. To address this specific inefficiency, we move beyond the general principles of Efficacy and Efficiency to propose two new, fine-grained principles: Brevity, which advocates for eliminating redundancy, and Sufficiency, which ensures critical reasoning steps are preserved. Guided by these principles, we introduce LC-R1, a post-training method based on Group Relative Policy Optimization (GRPO). LC-R1 employs a novel combination of a Length Reward for overall conciseness and a Compress Reward that is specifically designed to remove the invalid portion of the thinking process. Extensive experiments on multiple reasoning benchmarks demonstrate that LC-R1 achieves a significant reduction in sequence length (~50%) with only a marginal (~2%) drop in accuracy, achieving a favorable trade-off point on the Pareto frontier that prioritizes high compression. Our analysis further validates the robustness of LC-R1 and provides valuable insights for developing more powerful yet computationally efficient LRMs. Our code is released at https://github.com/zxiangx/LC-R1.",
            "score": 0,
            "issue_id": 4347,
            "pub_date": "2025-06-17",
            "pub_date_card": {
                "ru": "17 –∏—é–Ω—è",
                "en": "June 17",
                "zh": "6Êúà17Êó•"
            },
            "hash": "837a56d067dd6e74",
            "authors": [
                "Zhengxiang Cheng",
                "Dongping Chen",
                "Mingyang Fu",
                "Tianyi Zhou"
            ],
            "affiliations": [
                "University of Maryland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.14755.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#architecture",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "‚úÇÔ∏è",
                "ru": {
                    "title": "LC-R1: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞",
                    "desc": "LC-R1 - —ç—Ç–æ –º–µ—Ç–æ–¥ –ø–æ—Å—Ç-–æ–±—É—á–µ–Ω–∏—è –¥–ª—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (LRM), –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –ø—Ä–∏–Ω—Ü–∏–ø–∞—Ö –∫—Ä–∞—Ç–∫–æ—Å—Ç–∏ –∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ—Å—Ç–∏. –û–Ω –Ω–∞—Ü–µ–ª–µ–Ω –Ω–∞ —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –∏–∑–±—ã—Ç–æ—á–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –≤—ã–≤–æ–¥–∞—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –ø–æ—Ç–µ—Ä–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏. LC-R1 –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–æ–º–±–∏–Ω–∞—Ü–∏—é –Ω–∞–≥—Ä–∞–¥ –∑–∞ –¥–ª–∏–Ω—É –∏ —Å–∂–∞—Ç–∏–µ –≤ —Ä–∞–º–∫–∞—Ö –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≥—Ä—É–ø–ø–æ–≤–æ–π –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–π –ø–æ–ª–∏—Ç–∏–∫–∏ (GRPO). –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–µ—Ç–æ–¥ —Å–æ–∫—Ä–∞—â–∞–µ—Ç –¥–ª–∏–Ω—É –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –ø—Ä–∏–º–µ—Ä–Ω–æ –Ω–∞ 50% –ø—Ä–∏ –ø–∞–¥–µ–Ω–∏–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ –≤—Å–µ–≥–æ –Ω–∞ 2%."
                },
                "en": {
                    "title": "Streamlining Reasoning: LC-R1 for Efficient Large Models",
                    "desc": "The paper introduces LC-R1, a post-training method aimed at improving Large Reasoning Models (LRMs) by reducing unnecessary reasoning while maintaining accuracy. It identifies 'invalid thinking' as a key issue where models redundantly verify correct answers, leading to verbosity. To combat this, the authors propose two principles: Brevity, which focuses on cutting out redundant reasoning, and Sufficiency, which ensures essential reasoning steps are retained. Through experiments, LC-R1 demonstrates a significant reduction in reasoning sequence length by about 50% with only a slight accuracy drop of around 2%, showcasing an effective balance between compression and performance."
                },
                "zh": {
                    "title": "ÁÆÄÂåñÊé®ÁêÜÔºåÊèêÂçáÊïàÁéáÔºÅ",
                    "desc": "LC-R1ÊòØ‰∏ÄÁßçÂêéËÆ≠ÁªÉÊñπÊ≥ïÔºåÊó®Âú®ÈÄöËøáÁÆÄÊ¥ÅÊÄßÂíåÂÖÖÂàÜÊÄßÂéüÂàôÊù•ÂáèÂ∞ëÂ§ßÂûãÊé®ÁêÜÊ®°Âûã‰∏≠ÁöÑ‰∏çÂøÖË¶ÅÊé®ÁêÜÔºåÂêåÊó∂‰øùÊåÅËæÉÂ∞èÁöÑÂáÜÁ°ÆÊÄßÊçüÂ§±„ÄÇËØ•ÊñπÊ≥ïËØÜÂà´Âá∫Ê®°ÂûãÂú®Êé®ÁêÜËøáÁ®ã‰∏≠Â≠òÂú®ÁöÑ‚ÄúÊó†ÊïàÊÄùÁª¥‚ÄùÈóÆÈ¢òÔºåÂç≥Ê®°ÂûãÂú®ÂæóÂá∫Ê≠£Á°ÆÁ≠îÊ°àÂêé‰ªçÁÑ∂ÂèçÂ§çÊ£ÄÊü•„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏Ä‰ΩéÊïàÈóÆÈ¢òÔºåLC-R1ÊèêÂá∫‰∫Ü‰∏§‰∏™Êñ∞ÂéüÂàôÔºöÁÆÄÊ¥ÅÊÄßÔºåÂº∫Ë∞ÉÊ∂àÈô§ÂÜó‰ΩôÔºõÂÖÖÂàÜÊÄßÔºåÁ°Æ‰øùÂÖ≥ÈîÆÊé®ÁêÜÊ≠•È™§Âæó‰ª•‰øùÁïô„ÄÇÈÄöËøáÂØπÂ§ö‰∏™Êé®ÁêÜÂü∫ÂáÜÁöÑÂπøÊ≥õÂÆûÈ™åÔºåLC-R1ÂÆûÁé∞‰∫ÜÂ∫èÂàóÈïøÂ∫¶ÁöÑÊòæËëóÂáèÂ∞ëÔºàÁ∫¶50%ÔºâÔºåËÄåÂáÜÁ°ÆÊÄß‰ªÖ‰∏ãÈôçÁ∫¶2%ÔºåÂú®È´òÂéãÁº©ÁéáÂíåÂáÜÁ°ÆÊÄß‰πãÈó¥ËææÊàê‰∫ÜËâØÂ•ΩÁöÑÂπ≥Ë°°„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.13387",
            "title": "TR2M: Transferring Monocular Relative Depth to Metric Depth with\n  Language Descriptions and Scale-Oriented Contrast",
            "url": "https://huggingface.co/papers/2506.13387",
            "abstract": "A framework, TR2M, uses multimodal inputs to rescale relative depth to metric depth, enhancing performance across various datasets through cross-modality attention and contrastive learning.  \t\t\t\t\tAI-generated summary \t\t\t\t This work presents a generalizable framework to transfer relative depth to metric depth. Current monocular depth estimation methods are mainly divided into metric depth estimation (MMDE) and relative depth estimation (MRDE). MMDEs estimate depth in metric scale but are often limited to a specific domain. MRDEs generalize well across different domains, but with uncertain scales which hinders downstream applications. To this end, we aim to build up a framework to solve scale uncertainty and transfer relative depth to metric depth. Previous methods used language as input and estimated two factors for conducting rescaling. Our approach, TR2M, utilizes both text description and image as inputs and estimates two rescale maps to transfer relative depth to metric depth at pixel level. Features from two modalities are fused with a cross-modality attention module to better capture scale information. A strategy is designed to construct and filter confident pseudo metric depth for more comprehensive supervision. We also develop scale-oriented contrastive learning to utilize depth distribution as guidance to enforce the model learning about intrinsic knowledge aligning with the scale distribution. TR2M only exploits a small number of trainable parameters to train on datasets in various domains and experiments not only demonstrate TR2M's great performance in seen datasets but also reveal superior zero-shot capabilities on five unseen datasets. We show the huge potential in pixel-wise transferring relative depth to metric depth with language assistance. (Code is available at: https://github.com/BeileiCui/TR2M)",
            "score": 0,
            "issue_id": 4347,
            "pub_date": "2025-06-16",
            "pub_date_card": {
                "ru": "16 –∏—é–Ω—è",
                "en": "June 16",
                "zh": "6Êúà16Êó•"
            },
            "hash": "6d0fc497ae4dcfd0",
            "authors": [
                "Beilei Cui",
                "Yiming Huang",
                "Long Bai",
                "Hongliang Ren"
            ],
            "affiliations": [
                "The Chinese University of Hong Kong, Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.13387.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#cv",
                    "#multimodal"
                ],
                "emoji": "üîç",
                "ru": {
                    "title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≥–ª—É–±–∏–Ω—ã —Å –ø–æ–º–æ—â—å—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è",
                    "desc": "TR2M - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–π –≥–ª—É–±–∏–Ω—ã –≤ –º–µ—Ç—Ä–∏—á–µ—Å–∫—É—é —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –û–Ω –ø—Ä–∏–º–µ–Ω—è–µ—Ç –∫—Ä–æ—Å—Å-–º–æ–¥–∞–ª—å–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –∏ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö. TR2M –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–∞–∫ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ, —Ç–∞–∫ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –¥–≤–µ –∫–∞—Ä—Ç—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–π –≥–ª—É–±–∏–Ω—ã –≤ –º–µ—Ç—Ä–∏—á–µ—Å–∫—É—é –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø–∏–∫—Å–µ–ª–µ–π. –ú–æ–¥–µ–ª—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –æ—Ç–ª–∏—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–∞–∫ –Ω–∞ –∑–Ω–∞–∫–æ–º—ã—Ö, —Ç–∞–∫ –∏ –Ω–∞ –Ω–æ–≤—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö, –ø–æ–∫–∞–∑—ã–≤–∞—è –±–æ–ª—å—à–æ–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –≤ –ø–æ–ø–∏–∫—Å–µ–ª—å–Ω–æ–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–∏ –≥–ª—É–±–∏–Ω—ã —Å –ø–æ–º–æ—â—å—é —è–∑—ã–∫–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏."
                },
                "en": {
                    "title": "Transforming Relative Depth to Metric Depth with TR2M",
                    "desc": "The paper introduces TR2M, a framework that effectively converts relative depth information into metric depth using multimodal inputs, specifically images and text. It addresses the limitations of existing monocular depth estimation methods by combining the strengths of metric and relative depth estimation. TR2M employs cross-modality attention to enhance feature fusion and utilizes contrastive learning to improve scale alignment. The framework demonstrates strong performance across various datasets, including impressive zero-shot capabilities on unseen data, showcasing its versatility and effectiveness in depth estimation tasks."
                },
                "zh": {
                    "title": "TR2MÔºöÁõ∏ÂØπÊ∑±Â∫¶Âà∞Â∫¶ÈáèÊ∑±Â∫¶ÁöÑÊô∫ËÉΩËΩ¨Êç¢",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫TR2MÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®Â∞ÜÁõ∏ÂØπÊ∑±Â∫¶ËΩ¨Êç¢‰∏∫Â∫¶ÈáèÊ∑±Â∫¶ÔºåÂà©Áî®Â§öÊ®°ÊÄÅËæìÂÖ•ÊèêÂçáÂú®‰∏çÂêåÊï∞ÊçÆÈõÜ‰∏äÁöÑË°®Áé∞„ÄÇÂΩìÂâçÁöÑÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°ÊñπÊ≥ï‰∏ªË¶ÅÂàÜ‰∏∫Â∫¶ÈáèÊ∑±Â∫¶‰º∞ËÆ°ÂíåÁõ∏ÂØπÊ∑±Â∫¶‰º∞ËÆ°ÔºåÂâçËÄÖÂú®ÁâπÂÆöÈ¢ÜÂüüË°®Áé∞ËâØÂ•ΩÔºå‰ΩÜÂ±ÄÈôêÊÄßËæÉÂ§ßÔºåËÄåÂêéËÄÖÂú®‰∏çÂêåÈ¢ÜÂüüÂÖ∑ÊúâËâØÂ•ΩÁöÑÊ≥õÂåñËÉΩÂäõÔºå‰ΩÜÂ≠òÂú®Â∞∫Â∫¶‰∏çÁ°ÆÂÆöÊÄßÁöÑÈóÆÈ¢ò„ÄÇTR2MÈÄöËøáËûçÂêàÊñáÊú¨ÊèèËø∞ÂíåÂõæÂÉèËæìÂÖ•ÔºåÂà©Áî®‰∫§ÂèâÊ®°ÊÄÅÊ≥®ÊÑèÂäõÊ®°ÂùóÂíåÂØπÊØîÂ≠¶‰π†Á≠ñÁï•ÔºåÊûÑÂª∫‰∫Ü‰∏§‰∏™ÈáçÊ†áÂÆöÂõæ‰ª•Âú®ÂÉèÁ¥†Á∫ßÂà´‰∏äËøõË°åÊ∑±Â∫¶ËΩ¨Êç¢„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåTR2MÂú®Â∑≤Áü•Êï∞ÊçÆÈõÜ‰∏äË°®Áé∞‰ºòÂºÇÔºåÂπ∂Âú®‰∫î‰∏™Êú™Áü•Êï∞ÊçÆÈõÜ‰∏äÂ±ïÁé∞Âá∫ÂçìË∂äÁöÑÈõ∂Ê†∑Êú¨ËÉΩÂäõÔºåÊòæÁ§∫Âá∫Âú®ÂÉèÁ¥†Á∫ßÂà´‰∏äÂà©Áî®ËØ≠Ë®ÄËæÖÂä©ËøõË°åÊ∑±Â∫¶ËΩ¨Êç¢ÁöÑÂ∑®Â§ßÊΩúÂäõ„ÄÇ"
                }
            }
        }
    ],
    "link_prev": "2025-06-17.html",
    "link_next": "2025-06-19.html",
    "link_month": "2025-06.html",
    "short_date_prev": {
        "ru": "17.06",
        "en": "06/17",
        "zh": "6Êúà17Êó•"
    },
    "short_date_next": {
        "ru": "19.06",
        "en": "06/19",
        "zh": "6Êúà19Êó•"
    },
    "categories": {
        "#dataset": 2,
        "#data": 1,
        "#benchmark": 5,
        "#agents": 0,
        "#cv": 3,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 1,
        "#video": 0,
        "#multimodal": 2,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 4,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 0,
        "#agi": 1,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 1,
        "#transfer_learning": 2,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 1,
        "#science": 1,
        "#low_resource": 0
    }
}