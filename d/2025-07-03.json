{
    "date": {
        "ru": "3 июля",
        "en": "July 3",
        "zh": "7月3日"
    },
    "time_utc": "2025-07-03 07:13",
    "weekday": 3,
    "issue_id": 4620,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2507.01949",
            "title": "Kwai Keye-VL Technical Report",
            "url": "https://huggingface.co/papers/2507.01949",
            "abstract": "While Multimodal Large Language Models (MLLMs) demonstrate remarkable capabilities on static images, they often fall short in comprehending dynamic, information-dense short-form videos, a dominant medium in today's digital landscape. To bridge this gap, we introduce Kwai Keye-VL, an 8-billion-parameter multimodal foundation model engineered for leading-edge performance in short-video understanding while maintaining robust general-purpose vision-language abilities. The development of Keye-VL rests on two core pillars: a massive, high-quality dataset exceeding 600 billion tokens with a strong emphasis on video, and an innovative training recipe. This recipe features a four-stage pre-training process for solid vision-language alignment, followed by a meticulous two-phase post-training process. The first post-training stage enhances foundational capabilities like instruction following, while the second phase focuses on stimulating advanced reasoning. In this second phase, a key innovation is our five-mode ``cold-start'' data mixture, which includes ``thinking'', ``non-thinking'', ``auto-think'', ``think with image'', and high-quality video data. This mixture teaches the model to decide when and how to reason. Subsequent reinforcement learning (RL) and alignment steps further enhance these reasoning capabilities and correct abnormal model behaviors, such as repetitive outputs. To validate our approach, we conduct extensive evaluations, showing that Keye-VL achieves state-of-the-art results on public video benchmarks and remains highly competitive on general image-based tasks (Figure 1). Furthermore, we develop and release the KC-MMBench, a new benchmark tailored for real-world short-video scenarios, where Keye-VL shows a significant advantage.",
            "score": 71,
            "issue_id": 4615,
            "pub_date": "2025-07-02",
            "pub_date_card": {
                "ru": "2 июля",
                "en": "July 2",
                "zh": "7月2日"
            },
            "hash": "ca23195c7fa1bb87",
            "authors": [
                "Kwai Keye Team",
                "Biao Yang",
                "Bin Wen",
                "Changyi Liu",
                "Chenglong Chu",
                "Chengru Song",
                "Chongling Rao",
                "Chuan Yi",
                "Da Li",
                "Dunju Zang",
                "Fan Yang",
                "Guorui Zhou",
                "Hao Peng",
                "Haojie Ding",
                "Jiaming Huang",
                "Jiangxia Cao",
                "Jiankang Chen",
                "Jingyun Hua",
                "Jin Ouyang",
                "Kaibing Chen",
                "Kaiyu Jiang",
                "Kaiyu Tang",
                "Kun Gai",
                "Shengnan Zhang",
                "Siyang Mao",
                "Sui Huang",
                "Tianke Zhang",
                "Tingting Gao",
                "Wei Chen",
                "Wei Yuan",
                "Xiangyu Wu",
                "Xiao Hu",
                "Xingyu Lu",
                "Yang Zhou",
                "Yi-Fan Zhang",
                "Yiping Yang",
                "Yulong Chen",
                "Zhenhua Wu",
                "Zhenyu Li",
                "Zhixin Ling",
                "Ziming Li",
                "Dehua Ma",
                "Di Xu",
                "Haixuan Gao",
                "Hang Li",
                "Jiawei Guo",
                "Jing Wang",
                "Lejian Ren",
                "Muhao Wei",
                "Qianqian Wang",
                "Qigen Hu",
                "Shiyao Wang",
                "Tao Yu",
                "Xinchen Luo",
                "Yan Li",
                "Yiming Liang",
                "Yuhang Hu",
                "Zeyi Lu",
                "Zhuoran Yang",
                "Zixing Zhang"
            ],
            "affiliations": [
                "Kuaishou Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.01949.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#rl",
                    "#video",
                    "#dataset",
                    "#benchmark",
                    "#training",
                    "#multimodal",
                    "#alignment"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Kwai Keye-VL: Прорыв в понимании коротких видео с помощью мультимодального ИИ",
                    "desc": "Статья представляет Kwai Keye-VL - мультимодальную языковую модель с 8 миллиардами параметров, разработанную для понимания коротких видео. Модель обучена на массивном наборе данных объемом более 600 миллиардов токенов с акцентом на видеоконтент. Инновационный процесс обучения включает четырехэтапное предварительное обучение и двухфазное пост-обучение, направленное на улучшение рассуждений и следование инструкциям. Keye-VL достигает передовых результатов на эталонных тестах по видео, сохраняя высокую производительность в задачах обработки изображений."
                },
                "en": {
                    "title": "Revolutionizing Short-Video Understanding with Keye-VL",
                    "desc": "This paper presents Kwai Keye-VL, a multimodal large language model designed to improve understanding of short-form videos, which are increasingly popular. Keye-VL is built on a vast dataset of over 600 billion tokens, focusing on video content, and employs a unique training strategy that includes a four-stage pre-training and a two-phase post-training process. The model enhances its reasoning abilities through a novel data mixture that encourages different modes of thinking, followed by reinforcement learning to refine its outputs. Evaluations demonstrate that Keye-VL outperforms existing models on video benchmarks while maintaining strong performance on general image tasks."
                },
                "zh": {
                    "title": "短视频理解的新突破：Kwai Keye-VL",
                    "desc": "本论文介绍了一种名为Kwai Keye-VL的多模态大语言模型，专注于短视频理解。该模型拥有80亿个参数，旨在提升对动态、信息密集型短视频的理解能力，同时保持强大的通用视觉-语言能力。Keye-VL的开发基于两个核心支柱：一个超过6000亿个标记的高质量数据集，特别强调视频内容，以及一种创新的训练方法，包括四阶段的预训练和两阶段的后训练过程。通过强化学习和对齐步骤，Keye-VL在公共视频基准测试中取得了最先进的结果，并在一般图像任务中保持竞争力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.01945",
            "title": "LongAnimation: Long Animation Generation with Dynamic Global-Local\n  Memory",
            "url": "https://huggingface.co/papers/2507.01945",
            "abstract": "Animation colorization is a crucial part of real animation industry production. Long animation colorization has high labor costs. Therefore, automated long animation colorization based on the video generation model has significant research value. Existing studies are limited to short-term colorization. These studies adopt a local paradigm, fusing overlapping features to achieve smooth transitions between local segments. However, the local paradigm neglects global information, failing to maintain long-term color consistency. In this study, we argue that ideal long-term color consistency can be achieved through a dynamic global-local paradigm, i.e., dynamically extracting global color-consistent features relevant to the current generation. Specifically, we propose LongAnimation, a novel framework, which mainly includes a SketchDiT, a Dynamic Global-Local Memory (DGLM), and a Color Consistency Reward. The SketchDiT captures hybrid reference features to support the DGLM module. The DGLM module employs a long video understanding model to dynamically compress global historical features and adaptively fuse them with the current generation features. To refine the color consistency, we introduce a Color Consistency Reward. During inference, we propose a color consistency fusion to smooth the video segment transition. Extensive experiments on both short-term (14 frames) and long-term (average 500 frames) animations show the effectiveness of LongAnimation in maintaining short-term and long-term color consistency for open-domain animation colorization task. The code can be found at https://cn-makers.github.io/long_animation_web/.",
            "score": 45,
            "issue_id": 4615,
            "pub_date": "2025-07-02",
            "pub_date_card": {
                "ru": "2 июля",
                "en": "July 2",
                "zh": "7月2日"
            },
            "hash": "cf167e3958c2df99",
            "authors": [
                "Nan Chen",
                "Mengqi Huang",
                "Yihao Meng",
                "Zhendong Mao"
            ],
            "affiliations": [
                "Hong Kong University of Science and Technology",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.01945.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#open_source",
                    "#optimization"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "LongAnimation: Революция в автоматической колоризации длинных анимаций",
                    "desc": "Исследователи представили новый подход к автоматической колоризации длинных анимаций под названием LongAnimation. Эта система использует динамическую глобально-локальную парадигму для достижения согласованности цветов в долгосрочной перспективе. LongAnimation включает в себя модуль SketchDiT для извлечения гибридных эталонных признаков, динамическую глобально-локальную память (DGLM) для адаптивного слияния глобальных и локальных особенностей, а также функцию поощрения согласованности цветов. Эксперименты показали эффективность системы как для коротких (14 кадров), так и для длинных (в среднем 500 кадров) анимаций в задаче колоризации анимации с открытым доменом."
                },
                "en": {
                    "title": "Achieving Long-Term Color Consistency in Animation Colorization",
                    "desc": "This paper presents LongAnimation, a framework designed to automate the colorization of long animations, addressing the high labor costs associated with traditional methods. It critiques existing approaches that focus on short-term colorization and local feature fusion, which often overlook the importance of global color consistency. The proposed method utilizes a Dynamic Global-Local Memory (DGLM) to dynamically integrate global features with current generation data, ensuring a cohesive color palette throughout the animation. Additionally, a Color Consistency Reward is introduced to enhance the smoothness of transitions between video segments, demonstrating effectiveness in both short-term and long-term animation colorization tasks."
                },
                "zh": {
                    "title": "动态全局-局部范式实现动画色彩一致性",
                    "desc": "动画上色是动画产业生产中的重要环节，长时间动画的上色成本高昂。因此，基于视频生成模型的自动化长时间动画上色具有重要的研究价值。现有研究主要集中在短期上色，采用局部范式来实现局部片段之间的平滑过渡，但忽视了全局信息，导致长期色彩一致性不足。本研究提出了一种动态全局-局部范式，通过动态提取与当前生成相关的全局色彩一致特征，提出了LongAnimation框架，有效维护了短期和长期的色彩一致性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.01634",
            "title": "Depth Anything at Any Condition",
            "url": "https://huggingface.co/papers/2507.01634",
            "abstract": "We present Depth Anything at Any Condition (DepthAnything-AC), a foundation monocular depth estimation (MDE) model capable of handling diverse environmental conditions. Previous foundation MDE models achieve impressive performance across general scenes but not perform well in complex open-world environments that involve challenging conditions, such as illumination variations, adverse weather, and sensor-induced distortions. To overcome the challenges of data scarcity and the inability of generating high-quality pseudo-labels from corrupted images, we propose an unsupervised consistency regularization finetuning paradigm that requires only a relatively small amount of unlabeled data. Furthermore, we propose the Spatial Distance Constraint to explicitly enforce the model to learn patch-level relative relationships, resulting in clearer semantic boundaries and more accurate details. Experimental results demonstrate the zero-shot capabilities of DepthAnything-AC across diverse benchmarks, including real-world adverse weather benchmarks, synthetic corruption benchmarks, and general benchmarks.   Project Page: https://ghost233lism.github.io/depthanything-AC-page   Code: https://github.com/HVision-NKU/DepthAnythingAC",
            "score": 19,
            "issue_id": 4615,
            "pub_date": "2025-07-02",
            "pub_date_card": {
                "ru": "2 июля",
                "en": "July 2",
                "zh": "7月2日"
            },
            "hash": "48dee9247e2393f0",
            "authors": [
                "Boyuan Sun",
                "Modi Jin",
                "Bowen Yin",
                "Qibin Hou"
            ],
            "affiliations": [
                "VCIP, School of Computer Science, Nankai University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.01634.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#data",
                    "#training",
                    "#cv"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Универсальная оценка глубины изображений в любых условиях",
                    "desc": "DepthAnything-AC - это модель монокулярной оценки глубины, способная работать в различных условиях окружающей среды. Она решает проблемы предыдущих моделей, которые плохо справлялись со сложными условиями освещения, погоды и искажениями датчиков. Авторы предлагают парадигму обучения с регуляризацией согласованности без учителя, требующую небольшого количества немаркированных данных. Также они вводят ограничение пространственного расстояния для улучшения семантических границ и деталей."
                },
                "en": {
                    "title": "Mastering Depth Estimation in Any Environment",
                    "desc": "DepthAnything-AC is a monocular depth estimation model designed to perform well in various challenging environmental conditions. It addresses the limitations of previous models that struggle with issues like lighting changes and weather effects. The model uses an unsupervised consistency regularization approach, allowing it to learn effectively from a small amount of unlabeled data. Additionally, it incorporates a Spatial Distance Constraint to improve the accuracy of depth estimation by focusing on the relationships between different image patches."
                },
                "zh": {
                    "title": "在任何条件下的深度估计新突破",
                    "desc": "本文介绍了一种名为Depth Anything at Any Condition（DepthAnything-AC）的单目深度估计模型，能够在多种环境条件下进行有效的深度估计。以往的深度估计模型在一般场景中表现良好，但在复杂的开放世界环境中，如光照变化和恶劣天气下，表现不佳。为了解决数据稀缺和从受损图像生成高质量伪标签的困难，本文提出了一种无监督一致性正则化微调方法，仅需少量未标记数据。实验结果表明，DepthAnything-AC在多种基准测试中展现了零样本能力，包括真实世界的恶劣天气基准和合成损坏基准。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.01925",
            "title": "A Survey on Vision-Language-Action Models: An Action Tokenization\n  Perspective",
            "url": "https://huggingface.co/papers/2507.01925",
            "abstract": "The remarkable advancements of vision and language foundation models in multimodal understanding, reasoning, and generation has sparked growing efforts to extend such intelligence to the physical world, fueling the flourishing of vision-language-action (VLA) models. Despite seemingly diverse approaches, we observe that current VLA models can be unified under a single framework: vision and language inputs are processed by a series of VLA modules, producing a chain of action tokens that progressively encode more grounded and actionable information, ultimately generating executable actions. We further determine that the primary design choice distinguishing VLA models lies in how action tokens are formulated, which can be categorized into language description, code, affordance, trajectory, goal state, latent representation, raw action, and reasoning. However, there remains a lack of comprehensive understanding regarding action tokens, significantly impeding effective VLA development and obscuring future directions. Therefore, this survey aims to categorize and interpret existing VLA research through the lens of action tokenization, distill the strengths and limitations of each token type, and identify areas for improvement. Through this systematic review and analysis, we offer a synthesized outlook on the broader evolution of VLA models, highlight underexplored yet promising directions, and contribute guidance for future research, hoping to bring the field closer to general-purpose intelligence.",
            "score": 11,
            "issue_id": 4618,
            "pub_date": "2025-07-02",
            "pub_date_card": {
                "ru": "2 июля",
                "en": "July 2",
                "zh": "7月2日"
            },
            "hash": "28708b74dd1e7612",
            "authors": [
                "Yifan Zhong",
                "Fengshuo Bai",
                "Shaofei Cai",
                "Xuchuan Huang",
                "Zhang Chen",
                "Xiaowei Zhang",
                "Yuanfei Wang",
                "Shaoyang Guo",
                "Tianrui Guan",
                "Ka Nam Lui",
                "Zhiquan Qi",
                "Yitao Liang",
                "Yuanpei Chen",
                "Yaodong Yang"
            ],
            "affiliations": [
                "Institute for AI, Peking University",
                "PKU-PsiBot Joint Lab",
                "School of Computer Science, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.01925.jpg",
            "data": {
                "categories": [
                    "#survey",
                    "#reasoning",
                    "#multimodal",
                    "#agents"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Единый взгляд на модели зрения-языка-действия через призму токенизации действий",
                    "desc": "Эта статья посвящена моделям зрения-языка-действия (VLA) в машинном обучении. Авторы предлагают унифицированную структуру для понимания различных подходов к VLA, основанную на концепции токенов действий. Они выделяют восемь типов токенов действий, включая языковое описание, код, возможности, траектории и другие. Исследование анализирует сильные и слабые стороны каждого типа токенов и определяет перспективные направления для будущих исследований в области VLA."
                },
                "en": {
                    "title": "Unifying Vision-Language-Action Models through Action Tokenization",
                    "desc": "This paper discusses the progress of vision-language-action (VLA) models, which integrate visual and linguistic inputs to perform actions in the physical world. It identifies a common framework among these models, where a series of VLA modules process inputs to generate action tokens that convey actionable information. The authors categorize these action tokens into various types, such as language descriptions and trajectories, highlighting the importance of how they are formulated. The survey aims to clarify the role of action tokens in VLA development, assess their strengths and weaknesses, and suggest future research directions to enhance the effectiveness of VLA models."
                },
                "zh": {
                    "title": "统一行动标记，推动VLA模型发展",
                    "desc": "本文探讨了视觉-语言-行动（VLA）模型在多模态理解、推理和生成方面的进展。尽管当前的VLA模型方法多样，但可以统一在一个框架下，处理视觉和语言输入，生成逐步编码的行动标记。文章还指出，VLA模型的主要设计选择在于行动标记的形式化方式，包括语言描述、代码、可用性、轨迹、目标状态等。通过对现有VLA研究的分类和解读，本文旨在识别改进领域，并为未来研究提供指导。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.01953",
            "title": "FreeMorph: Tuning-Free Generalized Image Morphing with Diffusion Model",
            "url": "https://huggingface.co/papers/2507.01953",
            "abstract": "We present FreeMorph, the first tuning-free method for image morphing that accommodates inputs with different semantics or layouts. Unlike existing methods that rely on finetuning pre-trained diffusion models and are limited by time constraints and semantic/layout discrepancies, FreeMorph delivers high-fidelity image morphing without requiring per-instance training. Despite their efficiency and potential, tuning-free methods face challenges in maintaining high-quality results due to the non-linear nature of the multi-step denoising process and biases inherited from the pre-trained diffusion model. In this paper, we introduce FreeMorph to address these challenges by integrating two key innovations. 1) We first propose a guidance-aware spherical interpolation design that incorporates explicit guidance from the input images by modifying the self-attention modules, thereby addressing identity loss and ensuring directional transitions throughout the generated sequence. 2) We further introduce a step-oriented variation trend that blends self-attention modules derived from each input image to achieve controlled and consistent transitions that respect both inputs. Our extensive evaluations demonstrate that FreeMorph outperforms existing methods, being 10x ~ 50x faster and establishing a new state-of-the-art for image morphing.",
            "score": 5,
            "issue_id": 4617,
            "pub_date": "2025-07-02",
            "pub_date_card": {
                "ru": "2 июля",
                "en": "July 2",
                "zh": "7月2日"
            },
            "hash": "de2dfee54bea9af5",
            "authors": [
                "Yukang Cao",
                "Chenyang Si",
                "Jinghao Wang",
                "Ziwei Liu"
            ],
            "affiliations": [
                "Nanjing University",
                "S-Lab, Nanyang Technological University",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.01953.jpg",
            "data": {
                "categories": [
                    "#cv"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "FreeMorph: Революция в морфинге изображений без дополнительного обучения",
                    "desc": "FreeMorph - это первый метод морфинга изображений, не требующий дополнительной настройки и способный работать с изображениями различной семантики и компоновки. В отличие от существующих методов, основанных на дообучении предобученных диффузионных моделей, FreeMorph обеспечивает высококачественный морфинг без необходимости обучения для каждого конкретного случая. Метод включает в себя два ключевых нововведения: сферическую интерполяцию с учетом управления и ориентированную на шаги тенденцию изменения. Эксперименты показывают, что FreeMorph превосходит существующие методы, работая в 10-50 раз быстрее и устанавливая новый стандарт качества в области морфинга изображений."
                },
                "en": {
                    "title": "Revolutionizing Image Morphing with Tuning-Free Efficiency",
                    "desc": "FreeMorph is a novel method for image morphing that does not require tuning, making it efficient and effective for images with different meanings or layouts. It overcomes limitations of previous techniques that needed fine-tuning of diffusion models, which could lead to time delays and inconsistencies. The method introduces a guidance-aware spherical interpolation to enhance the quality of transitions and reduce identity loss, while also employing a step-oriented variation trend for smoother morphing. Evaluations show that FreeMorph is significantly faster and achieves superior results compared to existing methods, setting a new benchmark in the field."
                },
                "zh": {
                    "title": "FreeMorph：无需调优的高效图像变形方法",
                    "desc": "本文介绍了FreeMorph，这是一种首个无需调优的图像变形方法，能够处理具有不同语义或布局的输入。与依赖于微调预训练扩散模型的现有方法不同，FreeMorph无需针对每个实例进行训练，能够高保真地实现图像变形。尽管调优自由的方法在效率上具有优势，但由于多步去噪过程的非线性特性和预训练扩散模型的偏差，保持高质量结果仍然面临挑战。我们通过引入指导感知的球面插值设计和步骤导向的变化趋势，成功解决了这些问题，使FreeMorph在速度上比现有方法快10到50倍，并建立了图像变形的新标准。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.23552",
            "title": "JAM-Flow: Joint Audio-Motion Synthesis with Flow Matching",
            "url": "https://huggingface.co/papers/2506.23552",
            "abstract": "The intrinsic link between facial motion and speech is often overlooked in generative modeling, where talking head synthesis and text-to-speech (TTS) are typically addressed as separate tasks. This paper introduces JAM-Flow, a unified framework to simultaneously synthesize and condition on both facial motion and speech. Our approach leverages flow matching and a novel Multi-Modal Diffusion Transformer (MM-DiT) architecture, integrating specialized Motion-DiT and Audio-DiT modules. These are coupled via selective joint attention layers and incorporate key architectural choices, such as temporally aligned positional embeddings and localized joint attention masking, to enable effective cross-modal interaction while preserving modality-specific strengths. Trained with an inpainting-style objective, JAM-Flow supports a wide array of conditioning inputs-including text, reference audio, and reference motion-facilitating tasks such as synchronized talking head generation from text, audio-driven animation, and much more, within a single, coherent model. JAM-Flow significantly advances multi-modal generative modeling by providing a practical solution for holistic audio-visual synthesis. project page: https://joonghyuk.com/jamflow-web",
            "score": 2,
            "issue_id": 4615,
            "pub_date": "2025-06-30",
            "pub_date_card": {
                "ru": "30 июня",
                "en": "June 30",
                "zh": "6月30日"
            },
            "hash": "69157bacab4dea7d",
            "authors": [
                "Mingi Kwon",
                "Joonghyuk Shin",
                "Jaeseok Jung",
                "Jaesik Park",
                "Youngjung Uh"
            ],
            "affiliations": [
                "Seoul National University",
                "Yonsei University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.23552.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#multimodal",
                    "#architecture"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "Единая модель для синтеза речи и анимации лица",
                    "desc": "JAM-Flow - это унифицированная модель для одновременного синтеза лицевой анимации и речи. Она использует технологию flow matching и новую архитектуру Multi-Modal Diffusion Transformer (MM-DiT) с модулями Motion-DiT и Audio-DiT. Модель обучается с помощью инпейнтинга и поддерживает различные входные данные, включая текст, аудио и референсное движение. JAM-Flow представляет собой значительный прогресс в мультимодальном генеративном моделировании, обеспечивая целостный аудиовизуальный синтез."
                },
                "en": {
                    "title": "Unified Synthesis of Speech and Facial Motion with JAM-Flow",
                    "desc": "This paper presents JAM-Flow, a new framework that combines facial motion and speech synthesis into one model. It uses advanced techniques like flow matching and a Multi-Modal Diffusion Transformer (MM-DiT) to allow for effective interaction between audio and visual data. The model includes specialized components for handling both motion and audio, ensuring that each modality retains its unique characteristics while working together. By training with a unique inpainting-style objective, JAM-Flow can generate synchronized talking heads from various inputs, making it a significant advancement in multi-modal generative modeling."
                },
                "zh": {
                    "title": "统一面部运动与语音的生成模型",
                    "desc": "这篇论文提出了一种名为JAM-Flow的统一框架，旨在同时合成面部运动和语音。该方法利用流匹配和新颖的多模态扩散变换器（MM-DiT）架构，集成了专门的运动和音频模块。通过选择性联合注意力层，这些模块实现了有效的跨模态交互，同时保留了各自模态的优势。JAM-Flow支持多种条件输入，能够在单一模型中实现文本驱动的同步人头生成和音频驱动的动画等任务。"
                }
            }
        }
    ],
    "link_prev": "2025-07-02.html",
    "link_next": "2025-07-04.html",
    "link_month": "2025-07.html",
    "short_date_prev": {
        "ru": "02.07",
        "en": "07/02",
        "zh": "7月2日"
    },
    "short_date_next": {
        "ru": "04.07",
        "en": "07/04",
        "zh": "7月4日"
    },
    "categories": {
        "#dataset": 2,
        "#data": 1,
        "#benchmark": 2,
        "#agents": 1,
        "#cv": 2,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 1,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}