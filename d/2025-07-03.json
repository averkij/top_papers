{
    "date": {
        "ru": "3 июля",
        "en": "July 3",
        "zh": "7月3日"
    },
    "time_utc": "2025-07-03 18:16",
    "weekday": 3,
    "issue_id": 4631,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2507.01949",
            "title": "Kwai Keye-VL Technical Report",
            "url": "https://huggingface.co/papers/2507.01949",
            "abstract": "While Multimodal Large Language Models (MLLMs) demonstrate remarkable capabilities on static images, they often fall short in comprehending dynamic, information-dense short-form videos, a dominant medium in today's digital landscape. To bridge this gap, we introduce Kwai Keye-VL, an 8-billion-parameter multimodal foundation model engineered for leading-edge performance in short-video understanding while maintaining robust general-purpose vision-language abilities. The development of Keye-VL rests on two core pillars: a massive, high-quality dataset exceeding 600 billion tokens with a strong emphasis on video, and an innovative training recipe. This recipe features a four-stage pre-training process for solid vision-language alignment, followed by a meticulous two-phase post-training process. The first post-training stage enhances foundational capabilities like instruction following, while the second phase focuses on stimulating advanced reasoning. In this second phase, a key innovation is our five-mode ``cold-start'' data mixture, which includes ``thinking'', ``non-thinking'', ``auto-think'', ``think with image'', and high-quality video data. This mixture teaches the model to decide when and how to reason. Subsequent reinforcement learning (RL) and alignment steps further enhance these reasoning capabilities and correct abnormal model behaviors, such as repetitive outputs. To validate our approach, we conduct extensive evaluations, showing that Keye-VL achieves state-of-the-art results on public video benchmarks and remains highly competitive on general image-based tasks (Figure 1). Furthermore, we develop and release the KC-MMBench, a new benchmark tailored for real-world short-video scenarios, where Keye-VL shows a significant advantage.",
            "score": 92,
            "issue_id": 4615,
            "pub_date": "2025-07-02",
            "pub_date_card": {
                "ru": "2 июля",
                "en": "July 2",
                "zh": "7月2日"
            },
            "hash": "ca23195c7fa1bb87",
            "authors": [
                "Kwai Keye Team",
                "Biao Yang",
                "Bin Wen",
                "Changyi Liu",
                "Chenglong Chu",
                "Chengru Song",
                "Chongling Rao",
                "Chuan Yi",
                "Da Li",
                "Dunju Zang",
                "Fan Yang",
                "Guorui Zhou",
                "Hao Peng",
                "Haojie Ding",
                "Jiaming Huang",
                "Jiangxia Cao",
                "Jiankang Chen",
                "Jingyun Hua",
                "Jin Ouyang",
                "Kaibing Chen",
                "Kaiyu Jiang",
                "Kaiyu Tang",
                "Kun Gai",
                "Shengnan Zhang",
                "Siyang Mao",
                "Sui Huang",
                "Tianke Zhang",
                "Tingting Gao",
                "Wei Chen",
                "Wei Yuan",
                "Xiangyu Wu",
                "Xiao Hu",
                "Xingyu Lu",
                "Yang Zhou",
                "Yi-Fan Zhang",
                "Yiping Yang",
                "Yulong Chen",
                "Zhenhua Wu",
                "Zhenyu Li",
                "Zhixin Ling",
                "Ziming Li",
                "Dehua Ma",
                "Di Xu",
                "Haixuan Gao",
                "Hang Li",
                "Jiawei Guo",
                "Jing Wang",
                "Lejian Ren",
                "Muhao Wei",
                "Qianqian Wang",
                "Qigen Hu",
                "Shiyao Wang",
                "Tao Yu",
                "Xinchen Luo",
                "Yan Li",
                "Yiming Liang",
                "Yuhang Hu",
                "Zeyi Lu",
                "Zhuoran Yang",
                "Zixing Zhang"
            ],
            "affiliations": [
                "Kuaishou Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.01949.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#rl",
                    "#video",
                    "#dataset",
                    "#benchmark",
                    "#training",
                    "#multimodal",
                    "#alignment"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Kwai Keye-VL: Прорыв в понимании коротких видео с помощью мультимодального ИИ",
                    "desc": "Статья представляет Kwai Keye-VL - мультимодальную языковую модель с 8 миллиардами параметров, разработанную для понимания коротких видео. Модель обучена на массивном наборе данных объемом более 600 миллиардов токенов с акцентом на видеоконтент. Инновационный процесс обучения включает четырехэтапное предварительное обучение и двухфазное пост-обучение, направленное на улучшение рассуждений и следование инструкциям. Keye-VL достигает передовых результатов на эталонных тестах по видео, сохраняя высокую производительность в задачах обработки изображений."
                },
                "en": {
                    "title": "Revolutionizing Short-Video Understanding with Keye-VL",
                    "desc": "This paper presents Kwai Keye-VL, a multimodal large language model designed to improve understanding of short-form videos, which are increasingly popular. Keye-VL is built on a vast dataset of over 600 billion tokens, focusing on video content, and employs a unique training strategy that includes a four-stage pre-training and a two-phase post-training process. The model enhances its reasoning abilities through a novel data mixture that encourages different modes of thinking, followed by reinforcement learning to refine its outputs. Evaluations demonstrate that Keye-VL outperforms existing models on video benchmarks while maintaining strong performance on general image tasks."
                },
                "zh": {
                    "title": "短视频理解的新突破：Kwai Keye-VL",
                    "desc": "本论文介绍了一种名为Kwai Keye-VL的多模态大语言模型，专注于短视频理解。该模型拥有80亿个参数，旨在提升对动态、信息密集型短视频的理解能力，同时保持强大的通用视觉-语言能力。Keye-VL的开发基于两个核心支柱：一个超过6000亿个标记的高质量数据集，特别强调视频内容，以及一种创新的训练方法，包括四阶段的预训练和两阶段的后训练过程。通过强化学习和对齐步骤，Keye-VL在公共视频基准测试中取得了最先进的结果，并在一般图像任务中保持竞争力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.01945",
            "title": "LongAnimation: Long Animation Generation with Dynamic Global-Local\n  Memory",
            "url": "https://huggingface.co/papers/2507.01945",
            "abstract": "Animation colorization is a crucial part of real animation industry production. Long animation colorization has high labor costs. Therefore, automated long animation colorization based on the video generation model has significant research value. Existing studies are limited to short-term colorization. These studies adopt a local paradigm, fusing overlapping features to achieve smooth transitions between local segments. However, the local paradigm neglects global information, failing to maintain long-term color consistency. In this study, we argue that ideal long-term color consistency can be achieved through a dynamic global-local paradigm, i.e., dynamically extracting global color-consistent features relevant to the current generation. Specifically, we propose LongAnimation, a novel framework, which mainly includes a SketchDiT, a Dynamic Global-Local Memory (DGLM), and a Color Consistency Reward. The SketchDiT captures hybrid reference features to support the DGLM module. The DGLM module employs a long video understanding model to dynamically compress global historical features and adaptively fuse them with the current generation features. To refine the color consistency, we introduce a Color Consistency Reward. During inference, we propose a color consistency fusion to smooth the video segment transition. Extensive experiments on both short-term (14 frames) and long-term (average 500 frames) animations show the effectiveness of LongAnimation in maintaining short-term and long-term color consistency for open-domain animation colorization task. The code can be found at https://cn-makers.github.io/long_animation_web/.",
            "score": 60,
            "issue_id": 4615,
            "pub_date": "2025-07-02",
            "pub_date_card": {
                "ru": "2 июля",
                "en": "July 2",
                "zh": "7月2日"
            },
            "hash": "cf167e3958c2df99",
            "authors": [
                "Nan Chen",
                "Mengqi Huang",
                "Yihao Meng",
                "Zhendong Mao"
            ],
            "affiliations": [
                "Hong Kong University of Science and Technology",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.01945.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#open_source",
                    "#optimization"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "LongAnimation: Революция в автоматической колоризации длинных анимаций",
                    "desc": "Исследователи представили новый подход к автоматической колоризации длинных анимаций под названием LongAnimation. Эта система использует динамическую глобально-локальную парадигму для достижения согласованности цветов в долгосрочной перспективе. LongAnimation включает в себя модуль SketchDiT для извлечения гибридных эталонных признаков, динамическую глобально-локальную память (DGLM) для адаптивного слияния глобальных и локальных особенностей, а также функцию поощрения согласованности цветов. Эксперименты показали эффективность системы как для коротких (14 кадров), так и для длинных (в среднем 500 кадров) анимаций в задаче колоризации анимации с открытым доменом."
                },
                "en": {
                    "title": "Achieving Long-Term Color Consistency in Animation Colorization",
                    "desc": "This paper presents LongAnimation, a framework designed to automate the colorization of long animations, addressing the high labor costs associated with traditional methods. It critiques existing approaches that focus on short-term colorization and local feature fusion, which often overlook the importance of global color consistency. The proposed method utilizes a Dynamic Global-Local Memory (DGLM) to dynamically integrate global features with current generation data, ensuring a cohesive color palette throughout the animation. Additionally, a Color Consistency Reward is introduced to enhance the smoothness of transitions between video segments, demonstrating effectiveness in both short-term and long-term animation colorization tasks."
                },
                "zh": {
                    "title": "动态全局-局部范式实现动画色彩一致性",
                    "desc": "动画上色是动画产业生产中的重要环节，长时间动画的上色成本高昂。因此，基于视频生成模型的自动化长时间动画上色具有重要的研究价值。现有研究主要集中在短期上色，采用局部范式来实现局部片段之间的平滑过渡，但忽视了全局信息，导致长期色彩一致性不足。本研究提出了一种动态全局-局部范式，通过动态提取与当前生成相关的全局色彩一致特征，提出了LongAnimation框架，有效维护了短期和长期的色彩一致性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.01634",
            "title": "Depth Anything at Any Condition",
            "url": "https://huggingface.co/papers/2507.01634",
            "abstract": "We present Depth Anything at Any Condition (DepthAnything-AC), a foundation monocular depth estimation (MDE) model capable of handling diverse environmental conditions. Previous foundation MDE models achieve impressive performance across general scenes but not perform well in complex open-world environments that involve challenging conditions, such as illumination variations, adverse weather, and sensor-induced distortions. To overcome the challenges of data scarcity and the inability of generating high-quality pseudo-labels from corrupted images, we propose an unsupervised consistency regularization finetuning paradigm that requires only a relatively small amount of unlabeled data. Furthermore, we propose the Spatial Distance Constraint to explicitly enforce the model to learn patch-level relative relationships, resulting in clearer semantic boundaries and more accurate details. Experimental results demonstrate the zero-shot capabilities of DepthAnything-AC across diverse benchmarks, including real-world adverse weather benchmarks, synthetic corruption benchmarks, and general benchmarks.   Project Page: https://ghost233lism.github.io/depthanything-AC-page   Code: https://github.com/HVision-NKU/DepthAnythingAC",
            "score": 34,
            "issue_id": 4615,
            "pub_date": "2025-07-02",
            "pub_date_card": {
                "ru": "2 июля",
                "en": "July 2",
                "zh": "7月2日"
            },
            "hash": "48dee9247e2393f0",
            "authors": [
                "Boyuan Sun",
                "Modi Jin",
                "Bowen Yin",
                "Qibin Hou"
            ],
            "affiliations": [
                "VCIP, School of Computer Science, Nankai University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.01634.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#data",
                    "#training",
                    "#cv"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Универсальная оценка глубины изображений в любых условиях",
                    "desc": "DepthAnything-AC - это модель монокулярной оценки глубины, способная работать в различных условиях окружающей среды. Она решает проблемы предыдущих моделей, которые плохо справлялись со сложными условиями освещения, погоды и искажениями датчиков. Авторы предлагают парадигму обучения с регуляризацией согласованности без учителя, требующую небольшого количества немаркированных данных. Также они вводят ограничение пространственного расстояния для улучшения семантических границ и деталей."
                },
                "en": {
                    "title": "Mastering Depth Estimation in Any Environment",
                    "desc": "DepthAnything-AC is a monocular depth estimation model designed to perform well in various challenging environmental conditions. It addresses the limitations of previous models that struggle with issues like lighting changes and weather effects. The model uses an unsupervised consistency regularization approach, allowing it to learn effectively from a small amount of unlabeled data. Additionally, it incorporates a Spatial Distance Constraint to improve the accuracy of depth estimation by focusing on the relationships between different image patches."
                },
                "zh": {
                    "title": "在任何条件下的深度估计新突破",
                    "desc": "本文介绍了一种名为Depth Anything at Any Condition（DepthAnything-AC）的单目深度估计模型，能够在多种环境条件下进行有效的深度估计。以往的深度估计模型在一般场景中表现良好，但在复杂的开放世界环境中，如光照变化和恶劣天气下，表现不佳。为了解决数据稀缺和从受损图像生成高质量伪标签的困难，本文提出了一种无监督一致性正则化微调方法，仅需少量未标记数据。实验结果表明，DepthAnything-AC在多种基准测试中展现了零样本能力，包括真实世界的恶劣天气基准和合成损坏基准。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.01925",
            "title": "A Survey on Vision-Language-Action Models: An Action Tokenization\n  Perspective",
            "url": "https://huggingface.co/papers/2507.01925",
            "abstract": "The remarkable advancements of vision and language foundation models in multimodal understanding, reasoning, and generation has sparked growing efforts to extend such intelligence to the physical world, fueling the flourishing of vision-language-action (VLA) models. Despite seemingly diverse approaches, we observe that current VLA models can be unified under a single framework: vision and language inputs are processed by a series of VLA modules, producing a chain of action tokens that progressively encode more grounded and actionable information, ultimately generating executable actions. We further determine that the primary design choice distinguishing VLA models lies in how action tokens are formulated, which can be categorized into language description, code, affordance, trajectory, goal state, latent representation, raw action, and reasoning. However, there remains a lack of comprehensive understanding regarding action tokens, significantly impeding effective VLA development and obscuring future directions. Therefore, this survey aims to categorize and interpret existing VLA research through the lens of action tokenization, distill the strengths and limitations of each token type, and identify areas for improvement. Through this systematic review and analysis, we offer a synthesized outlook on the broader evolution of VLA models, highlight underexplored yet promising directions, and contribute guidance for future research, hoping to bring the field closer to general-purpose intelligence.",
            "score": 18,
            "issue_id": 4618,
            "pub_date": "2025-07-02",
            "pub_date_card": {
                "ru": "2 июля",
                "en": "July 2",
                "zh": "7月2日"
            },
            "hash": "28708b74dd1e7612",
            "authors": [
                "Yifan Zhong",
                "Fengshuo Bai",
                "Shaofei Cai",
                "Xuchuan Huang",
                "Zhang Chen",
                "Xiaowei Zhang",
                "Yuanfei Wang",
                "Shaoyang Guo",
                "Tianrui Guan",
                "Ka Nam Lui",
                "Zhiquan Qi",
                "Yitao Liang",
                "Yuanpei Chen",
                "Yaodong Yang"
            ],
            "affiliations": [
                "Institute for AI, Peking University",
                "PKU-PsiBot Joint Lab",
                "School of Computer Science, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.01925.jpg",
            "data": {
                "categories": [
                    "#survey",
                    "#reasoning",
                    "#multimodal",
                    "#agents"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Единый взгляд на модели зрения-языка-действия через призму токенизации действий",
                    "desc": "Эта статья посвящена моделям зрения-языка-действия (VLA) в машинном обучении. Авторы предлагают унифицированную структуру для понимания различных подходов к VLA, основанную на концепции токенов действий. Они выделяют восемь типов токенов действий, включая языковое описание, код, возможности, траектории и другие. Исследование анализирует сильные и слабые стороны каждого типа токенов и определяет перспективные направления для будущих исследований в области VLA."
                },
                "en": {
                    "title": "Unifying Vision-Language-Action Models through Action Tokenization",
                    "desc": "This paper discusses the progress of vision-language-action (VLA) models, which integrate visual and linguistic inputs to perform actions in the physical world. It identifies a common framework among these models, where a series of VLA modules process inputs to generate action tokens that convey actionable information. The authors categorize these action tokens into various types, such as language descriptions and trajectories, highlighting the importance of how they are formulated. The survey aims to clarify the role of action tokens in VLA development, assess their strengths and weaknesses, and suggest future research directions to enhance the effectiveness of VLA models."
                },
                "zh": {
                    "title": "统一行动标记，推动VLA模型发展",
                    "desc": "本文探讨了视觉-语言-行动（VLA）模型在多模态理解、推理和生成方面的进展。尽管当前的VLA模型方法多样，但可以统一在一个框架下，处理视觉和语言输入，生成逐步编码的行动标记。文章还指出，VLA模型的主要设计选择在于行动标记的形式化方式，包括语言描述、代码、可用性、轨迹、目标状态等。通过对现有VLA研究的分类和解读，本文旨在识别改进领域，并为未来研究提供指导。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.01953",
            "title": "FreeMorph: Tuning-Free Generalized Image Morphing with Diffusion Model",
            "url": "https://huggingface.co/papers/2507.01953",
            "abstract": "We present FreeMorph, the first tuning-free method for image morphing that accommodates inputs with different semantics or layouts. Unlike existing methods that rely on finetuning pre-trained diffusion models and are limited by time constraints and semantic/layout discrepancies, FreeMorph delivers high-fidelity image morphing without requiring per-instance training. Despite their efficiency and potential, tuning-free methods face challenges in maintaining high-quality results due to the non-linear nature of the multi-step denoising process and biases inherited from the pre-trained diffusion model. In this paper, we introduce FreeMorph to address these challenges by integrating two key innovations. 1) We first propose a guidance-aware spherical interpolation design that incorporates explicit guidance from the input images by modifying the self-attention modules, thereby addressing identity loss and ensuring directional transitions throughout the generated sequence. 2) We further introduce a step-oriented variation trend that blends self-attention modules derived from each input image to achieve controlled and consistent transitions that respect both inputs. Our extensive evaluations demonstrate that FreeMorph outperforms existing methods, being 10x ~ 50x faster and establishing a new state-of-the-art for image morphing.",
            "score": 11,
            "issue_id": 4617,
            "pub_date": "2025-07-02",
            "pub_date_card": {
                "ru": "2 июля",
                "en": "July 2",
                "zh": "7月2日"
            },
            "hash": "de2dfee54bea9af5",
            "authors": [
                "Yukang Cao",
                "Chenyang Si",
                "Jinghao Wang",
                "Ziwei Liu"
            ],
            "affiliations": [
                "Nanjing University",
                "S-Lab, Nanyang Technological University",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.01953.jpg",
            "data": {
                "categories": [
                    "#cv"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "FreeMorph: Революция в морфинге изображений без дополнительного обучения",
                    "desc": "FreeMorph - это первый метод морфинга изображений, не требующий дополнительной настройки и способный работать с изображениями различной семантики и компоновки. В отличие от существующих методов, основанных на дообучении предобученных диффузионных моделей, FreeMorph обеспечивает высококачественный морфинг без необходимости обучения для каждого конкретного случая. Метод включает в себя два ключевых нововведения: сферическую интерполяцию с учетом управления и ориентированную на шаги тенденцию изменения. Эксперименты показывают, что FreeMorph превосходит существующие методы, работая в 10-50 раз быстрее и устанавливая новый стандарт качества в области морфинга изображений."
                },
                "en": {
                    "title": "Revolutionizing Image Morphing with Tuning-Free Efficiency",
                    "desc": "FreeMorph is a novel method for image morphing that does not require tuning, making it efficient and effective for images with different meanings or layouts. It overcomes limitations of previous techniques that needed fine-tuning of diffusion models, which could lead to time delays and inconsistencies. The method introduces a guidance-aware spherical interpolation to enhance the quality of transitions and reduce identity loss, while also employing a step-oriented variation trend for smoother morphing. Evaluations show that FreeMorph is significantly faster and achieves superior results compared to existing methods, setting a new benchmark in the field."
                },
                "zh": {
                    "title": "FreeMorph：无需调优的高效图像变形方法",
                    "desc": "本文介绍了FreeMorph，这是一种首个无需调优的图像变形方法，能够处理具有不同语义或布局的输入。与依赖于微调预训练扩散模型的现有方法不同，FreeMorph无需针对每个实例进行训练，能够高保真地实现图像变形。尽管调优自由的方法在效率上具有优势，但由于多步去噪过程的非线性特性和预训练扩散模型的偏差，保持高质量结果仍然面临挑战。我们通过引入指导感知的球面插值设计和步骤导向的变化趋势，成功解决了这些问题，使FreeMorph在速度上比现有方法快10到50倍，并建立了图像变形的新标准。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.01957",
            "title": "Locality-aware Parallel Decoding for Efficient Autoregressive Image\n  Generation",
            "url": "https://huggingface.co/papers/2507.01957",
            "abstract": "We present Locality-aware Parallel Decoding (LPD) to accelerate autoregressive image generation. Traditional autoregressive image generation relies on next-patch prediction, a memory-bound process that leads to high latency. Existing works have tried to parallelize next-patch prediction by shifting to multi-patch prediction to accelerate the process, but only achieved limited parallelization. To achieve high parallelization while maintaining generation quality, we introduce two key techniques: (1) Flexible Parallelized Autoregressive Modeling, a novel architecture that enables arbitrary generation ordering and degrees of parallelization. It uses learnable position query tokens to guide generation at target positions while ensuring mutual visibility among concurrently generated tokens for consistent parallel decoding. (2) Locality-aware Generation Ordering, a novel schedule that forms groups to minimize intra-group dependencies and maximize contextual support, enhancing generation quality. With these designs, we reduce the generation steps from 256 to 20 (256times256 res.) and 1024 to 48 (512times512 res.) without compromising quality on the ImageNet class-conditional generation, and achieving at least 3.4times lower latency than previous parallelized autoregressive models.",
            "score": 10,
            "issue_id": 4621,
            "pub_date": "2025-07-02",
            "pub_date_card": {
                "ru": "2 июля",
                "en": "July 2",
                "zh": "7月2日"
            },
            "hash": "b2594c8c1eebcb0c",
            "authors": [
                "Zhuoyang Zhang",
                "Luke J. Huang",
                "Chengyue Wu",
                "Shang Yang",
                "Kelly Peng",
                "Yao Lu",
                "Song Han"
            ],
            "affiliations": [
                "First Intelligence",
                "MIT",
                "NVIDIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.01957.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#training"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Революция в скорости генерации изображений без потери качества",
                    "desc": "Статья представляет новый метод Locality-aware Parallel Decoding (LPD) для ускорения авторегрессивной генерации изображений. Авторы вводят две ключевые техники: гибкое параллелизованное авторегрессивное моделирование и локально-ориентированное упорядочивание генерации. Эти техники позволяют значительно сократить количество шагов генерации без ухудшения качества изображений. В результате достигается как минимум 3.4-кратное снижение задержки по сравнению с предыдущими параллелизованными авторегрессивными моделями."
                },
                "en": {
                    "title": "Accelerating Image Generation with Locality-aware Parallel Decoding",
                    "desc": "This paper introduces Locality-aware Parallel Decoding (LPD) to improve the speed of autoregressive image generation. Traditional methods face high latency due to memory constraints when predicting the next patch of an image. The authors propose two innovative techniques: a flexible architecture for parallelized autoregressive modeling and a locality-aware generation ordering that optimizes the order of patch generation. These advancements significantly reduce the number of generation steps and latency while maintaining high image quality, outperforming previous models."
                },
                "zh": {
                    "title": "加速自回归图像生成的新方法",
                    "desc": "我们提出了一种局部感知并行解码（LPD）方法，以加速自回归图像生成。传统的自回归图像生成依赖于下一个补丁的预测，这一过程受内存限制，导致延迟较高。我们引入了灵活的并行自回归建模和局部感知生成顺序两项关键技术，以实现高并行性并保持生成质量。通过这些设计，我们将生成步骤从256减少到20，并在不影响质量的情况下，显著降低了延迟。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.01544",
            "title": "MARVIS: Modality Adaptive Reasoning over VISualizations",
            "url": "https://huggingface.co/papers/2507.01544",
            "abstract": "Scientific applications of machine learning often rely on small, specialized models tuned to particular domains. Such models often achieve excellent performance, but lack flexibility. Foundation models offer versatility, but typically underperform specialized approaches, especially on non-traditional modalities and long-tail domains. We propose MARVIS (Modality Adaptive Reasoning over VISualizations), a training-free method that enables even small vision-language models to predict any data modality with high accuracy. MARVIS transforms latent embedding spaces into visual representations and then leverages the spatial and fine-grained reasoning skills of VLMs to successfully interpret and utilize them. MARVIS achieves competitive performance on vision, audio, biological, and tabular domains using a single 3B parameter model, achieving results that beat Gemini by 16\\% on average and approach specialized methods, without exposing personally identifiable information (P.I.I.) or requiring any domain-specific training. We open source our code and datasets at https://github.com/penfever/marvis",
            "score": 7,
            "issue_id": 4624,
            "pub_date": "2025-07-02",
            "pub_date_card": {
                "ru": "2 июля",
                "en": "July 2",
                "zh": "7月2日"
            },
            "hash": "547bee35865d6ddd",
            "authors": [
                "Benjamin Feuer",
                "Lennart Purucker",
                "Oussama Elachqar",
                "Chinmay Hegde"
            ],
            "affiliations": [
                "NYU",
                "Oumi.AI",
                "University of Freiburg"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.01544.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#small_models",
                    "#training",
                    "#open_source",
                    "#dataset",
                    "#science"
                ],
                "emoji": "🔮",
                "ru": {
                    "title": "MARVIS: универсальный метод для точных предсказаний в разных модальностях без дополнительного обучения",
                    "desc": "MARVIS - это метод, позволяющий небольшим мультимодальным моделям предсказывать данные различных модальностей с высокой точностью. Он преобразует латентные пространства в визуальные представления, используя навыки пространственных рассуждений VLM для их интерпретации. MARVIS достигает конкурентоспособных результатов в различных доменах, превосходя Gemini на 16% в среднем, без необходимости в специфическом обучении. Метод не раскрывает персональную информацию и доступен в открытом исходном коде."
                },
                "en": {
                    "title": "Unlocking Versatility in Vision-Language Models with MARVIS",
                    "desc": "This paper introduces MARVIS, a novel method that enhances small vision-language models by allowing them to predict various data modalities without the need for training. MARVIS converts latent embeddings into visual representations, enabling the models to apply their reasoning capabilities effectively across different domains. The approach demonstrates competitive performance in areas like vision, audio, and biological data, outperforming existing models like Gemini by 16% on average. Importantly, MARVIS maintains privacy by not requiring any domain-specific training or exposing personally identifiable information."
                },
                "zh": {
                    "title": "MARVIS：小型模型的多模态预测新方法",
                    "desc": "本论文提出了一种名为MARVIS的方法，旨在提高小型视觉语言模型在多种数据模态上的预测准确性。MARVIS通过将潜在嵌入空间转化为视觉表示，利用视觉语言模型的空间和细粒度推理能力，成功解读和利用这些表示。该方法无需特定领域的训练，能够在视觉、音频、生物和表格数据等领域中实现竞争性的性能。MARVIS在不暴露个人可识别信息的情况下，使用单个3B参数模型的表现超越了Gemini，接近专业方法的效果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.22868",
            "title": "STR-Match: Matching SpatioTemporal Relevance Score for Training-Free\n  Video Editing",
            "url": "https://huggingface.co/papers/2506.22868",
            "abstract": "STR-Match uses latent optimization and a novel STR score to produce spatiotemporally coherent and visually appealing edited videos by leveraging 2D spatial and 1D temporal attention in T2V diffusion models.  \t\t\t\t\tAI-generated summary \t\t\t\t Previous text-guided video editing methods often suffer from temporal inconsistency, motion distortion, and-most notably-limited domain transformation. We attribute these limitations to insufficient modeling of spatiotemporal pixel relevance during the editing process. To address this, we propose STR-Match, a training-free video editing algorithm that produces visually appealing and spatiotemporally coherent videos through latent optimization guided by our novel STR score. The score captures spatiotemporal pixel relevance across adjacent frames by leveraging 2D spatial attention and 1D temporal modules in text-to-video (T2V) diffusion models, without the overhead of computationally expensive 3D attention mechanisms. Integrated into a latent optimization framework with a latent mask, STR-Match generates temporally consistent and visually faithful videos, maintaining strong performance even under significant domain transformations while preserving key visual attributes of the source. Extensive experiments demonstrate that STR-Match consistently outperforms existing methods in both visual quality and spatiotemporal consistency.",
            "score": 4,
            "issue_id": 4621,
            "pub_date": "2025-06-28",
            "pub_date_card": {
                "ru": "28 июня",
                "en": "June 28",
                "zh": "6月28日"
            },
            "hash": "94371810be905c93",
            "authors": [
                "Junsung Lee",
                "Junoh Kang",
                "Bohyung Han"
            ],
            "affiliations": [
                "Seoul National University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.22868.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#optimization",
                    "#diffusion"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "STR-Match: Революция в редактировании видео с помощью ИИ",
                    "desc": "STR-Match - это алгоритм редактирования видео без обучения, использующий латентную оптимизацию и новую метрику STR. Он использует 2D пространственное и 1D временное внимание в диффузионных моделях текст-в-видео для создания пространственно-временно согласованных и визуально привлекательных отредактированных видео. STR-Match превосходит существующие методы по визуальному качеству и пространственно-временной согласованности. Алгоритм способен сохранять ключевые визуальные атрибуты исходного видео даже при значительных преобразованиях домена."
                },
                "en": {
                    "title": "Enhancing Video Editing with STR-Match: Coherence Meets Quality",
                    "desc": "STR-Match is a novel video editing algorithm that enhances the quality and coherence of AI-generated videos. It utilizes a unique STR score to assess the relevance of pixels across time and space, ensuring that the edited videos maintain visual appeal and temporal consistency. By employing 2D spatial attention and 1D temporal attention, STR-Match avoids the complexity of 3D attention mechanisms while still achieving impressive results. The method is training-free and effectively handles significant domain transformations, outperforming existing techniques in visual quality and spatiotemporal coherence."
                },
                "zh": {
                    "title": "STR-Match：时空一致的视觉视频编辑新方法",
                    "desc": "STR-Match是一种无训练的视频编辑算法，旨在生成视觉上吸引人且时空一致的视频。它通过引入新颖的STR评分，利用2D空间注意力和1D时间模块来捕捉相邻帧之间的像素相关性。与传统方法相比，STR-Match避免了计算开销大的3D注意力机制，能够在显著的领域转换下保持视频的关键视觉特征。实验结果表明，STR-Match在视觉质量和时空一致性方面始终优于现有方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.23552",
            "title": "JAM-Flow: Joint Audio-Motion Synthesis with Flow Matching",
            "url": "https://huggingface.co/papers/2506.23552",
            "abstract": "The intrinsic link between facial motion and speech is often overlooked in generative modeling, where talking head synthesis and text-to-speech (TTS) are typically addressed as separate tasks. This paper introduces JAM-Flow, a unified framework to simultaneously synthesize and condition on both facial motion and speech. Our approach leverages flow matching and a novel Multi-Modal Diffusion Transformer (MM-DiT) architecture, integrating specialized Motion-DiT and Audio-DiT modules. These are coupled via selective joint attention layers and incorporate key architectural choices, such as temporally aligned positional embeddings and localized joint attention masking, to enable effective cross-modal interaction while preserving modality-specific strengths. Trained with an inpainting-style objective, JAM-Flow supports a wide array of conditioning inputs-including text, reference audio, and reference motion-facilitating tasks such as synchronized talking head generation from text, audio-driven animation, and much more, within a single, coherent model. JAM-Flow significantly advances multi-modal generative modeling by providing a practical solution for holistic audio-visual synthesis. project page: https://joonghyuk.com/jamflow-web",
            "score": 3,
            "issue_id": 4615,
            "pub_date": "2025-06-30",
            "pub_date_card": {
                "ru": "30 июня",
                "en": "June 30",
                "zh": "6月30日"
            },
            "hash": "69157bacab4dea7d",
            "authors": [
                "Mingi Kwon",
                "Joonghyuk Shin",
                "Jaeseok Jung",
                "Jaesik Park",
                "Youngjung Uh"
            ],
            "affiliations": [
                "Seoul National University",
                "Yonsei University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.23552.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#multimodal",
                    "#architecture"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "Единая модель для синтеза речи и анимации лица",
                    "desc": "JAM-Flow - это унифицированная модель для одновременного синтеза лицевой анимации и речи. Она использует технологию flow matching и новую архитектуру Multi-Modal Diffusion Transformer (MM-DiT) с модулями Motion-DiT и Audio-DiT. Модель обучается с помощью инпейнтинга и поддерживает различные входные данные, включая текст, аудио и референсное движение. JAM-Flow представляет собой значительный прогресс в мультимодальном генеративном моделировании, обеспечивая целостный аудиовизуальный синтез."
                },
                "en": {
                    "title": "Unified Synthesis of Speech and Facial Motion with JAM-Flow",
                    "desc": "This paper presents JAM-Flow, a new framework that combines facial motion and speech synthesis into one model. It uses advanced techniques like flow matching and a Multi-Modal Diffusion Transformer (MM-DiT) to allow for effective interaction between audio and visual data. The model includes specialized components for handling both motion and audio, ensuring that each modality retains its unique characteristics while working together. By training with a unique inpainting-style objective, JAM-Flow can generate synchronized talking heads from various inputs, making it a significant advancement in multi-modal generative modeling."
                },
                "zh": {
                    "title": "统一面部运动与语音的生成模型",
                    "desc": "这篇论文提出了一种名为JAM-Flow的统一框架，旨在同时合成面部运动和语音。该方法利用流匹配和新颖的多模态扩散变换器（MM-DiT）架构，集成了专门的运动和音频模块。通过选择性联合注意力层，这些模块实现了有效的跨模态交互，同时保留了各自模态的优势。JAM-Flow支持多种条件输入，能够在单一模型中实现文本驱动的同步人头生成和音频驱动的动画等任务。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.00472",
            "title": "ARIG: Autoregressive Interactive Head Generation for Real-time\n  Conversations",
            "url": "https://huggingface.co/papers/2507.00472",
            "abstract": "Face-to-face communication, as a common human activity, motivates the research on interactive head generation. A virtual agent can generate motion responses with both listening and speaking capabilities based on the audio or motion signals of the other user and itself. However, previous clip-wise generation paradigm or explicit listener/speaker generator-switching methods have limitations in future signal acquisition, contextual behavioral understanding, and switching smoothness, making it challenging to be real-time and realistic. In this paper, we propose an autoregressive (AR) based frame-wise framework called ARIG to realize the real-time generation with better interaction realism. To achieve real-time generation, we model motion prediction as a non-vector-quantized AR process. Unlike discrete codebook-index prediction, we represent motion distribution using diffusion procedure, achieving more accurate predictions in continuous space. To improve interaction realism, we emphasize interactive behavior understanding (IBU) and detailed conversational state understanding (CSU). In IBU, based on dual-track dual-modal signals, we summarize short-range behaviors through bidirectional-integrated learning and perform contextual understanding over long ranges. In CSU, we use voice activity signals and context features of IBU to understand the various states (interruption, feedback, pause, etc.) that exist in actual conversations. These serve as conditions for the final progressive motion prediction. Extensive experiments have verified the effectiveness of our model.",
            "score": 2,
            "issue_id": 4628,
            "pub_date": "2025-07-01",
            "pub_date_card": {
                "ru": "1 июля",
                "en": "July 1",
                "zh": "7月1日"
            },
            "hash": "083f371459a06cdd",
            "authors": [
                "Ying Guo",
                "Xi Liu",
                "Cheng Zhen",
                "Pengfei Yan",
                "Xiaoming Wei"
            ],
            "affiliations": [
                "Vision AI Department, Meituan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.00472.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#agents",
                    "#optimization",
                    "#multimodal"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "ARIG: Реалистичная генерация движений виртуального собеседника в реальном времени",
                    "desc": "Статья представляет новый подход к генерации интерактивных движений головы виртуального агента в режиме реального времени. Авторы предлагают авторегрессионную модель ARIG, использующую диффузионный процесс для более точного предсказания движений в непрерывном пространстве. Модель включает модули для понимания интерактивного поведения (IBU) и детального понимания состояний разговора (CSU). Эксперименты подтвердили эффективность предложенного подхода для создания реалистичных интерактивных движений виртуального агента."
                },
                "en": {
                    "title": "Real-Time Realism in Virtual Conversations with ARIG",
                    "desc": "This paper presents a new method for generating realistic interactions in virtual agents, focusing on head motion during conversations. The proposed autoregressive framework, called ARIG, allows for real-time motion prediction by modeling it as a continuous process rather than using discrete codes. It enhances interaction realism by incorporating interactive behavior understanding (IBU) and conversational state understanding (CSU), which analyze both short-term and long-term conversational dynamics. The results from extensive experiments demonstrate that ARIG significantly improves the quality and smoothness of virtual agent interactions compared to previous methods."
                },
                "zh": {
                    "title": "提升虚拟代理交互真实感的自回归生成框架",
                    "desc": "本论文研究了交互式头部生成，以提高虚拟代理的实时交互能力。我们提出了一种基于自回归（AR）的框架ARIG，通过非向量量化的AR过程来建模运动预测，从而实现更准确的连续空间预测。为了增强交互的真实感，我们强调了交互行为理解（IBU）和详细的对话状态理解（CSU），通过双轨双模态信号来总结短期行为，并对长期行为进行上下文理解。实验结果验证了我们模型的有效性，显示出在实时生成和交互真实感方面的显著提升。"
                }
            }
        }
    ],
    "link_prev": "2025-07-02.html",
    "link_next": "2025-07-04.html",
    "link_month": "2025-07.html",
    "short_date_prev": {
        "ru": "02.07",
        "en": "07/02",
        "zh": "7月2日"
    },
    "short_date_next": {
        "ru": "04.07",
        "en": "07/04",
        "zh": "7月4日"
    },
    "categories": {
        "#dataset": 3,
        "#data": 1,
        "#benchmark": 2,
        "#agents": 2,
        "#cv": 2,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 3,
        "#multimodal": 5,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 4,
        "#survey": 1,
        "#diffusion": 2,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 1,
        "#science": 1,
        "#low_resource": 0
    }
}