{
    "date": {
        "ru": "24 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
        "en": "October 24",
        "zh": "10æœˆ24æ—¥"
    },
    "time_utc": "2025-10-24 05:12",
    "weekday": 4,
    "issue_id": 6593,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2510.20579",
            "title": "Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal\n  Evidence",
            "url": "https://huggingface.co/papers/2510.20579",
            "abstract": "Open-o3 Video integrates spatio-temporal evidence into video reasoning, achieving state-of-the-art performance on multiple benchmarks and providing valuable reasoning traces for test-time scaling.  \t\t\t\t\tAI-generated summary \t\t\t\t Most video reasoning models only generate textual reasoning traces without indicating when and where key evidence appears. Recent models such as OpenAI-o3 have sparked wide interest in evidence-centered reasoning for images, yet extending this ability to videos is more challenging, as it requires joint temporal tracking and spatial localization across dynamic scenes. We introduce Open-o3 Video, a non-agent framework that integrates explicit spatio-temporal evidence into video reasoning, and carefully collect training data and design training strategies to address the aforementioned challenges. The model highlights key timestamps, objects, and bounding boxes alongside its answers, allowing reasoning to be grounded in concrete visual observations. To enable this functionality, we first curate and build two high-quality datasets, STGR-CoT-30k for SFT and STGR-RL-36k for RL, with carefully constructed temporal and spatial annotations, since most existing datasets offer either temporal spans for videos or spatial boxes on images, lacking unified spatio-temporal supervision and reasoning traces. Then, we adopt a cold-start reinforcement learning strategy with multiple specially designed rewards that jointly encourage answer accuracy, temporal alignment, and spatial precision. On V-STAR benchmark, Open-o3 Video achieves state-of-the-art performance, raising mAM by 14.4% and mLGM by 24.2% on the Qwen2.5-VL baseline. Consistent improvements are also observed on a broad range of video understanding benchmarks, including VideoMME, WorldSense, VideoMMMU, and TVGBench. Beyond accuracy, the reasoning traces produced by Open-o3 Video also provide valuable signals for test-time scaling, enabling confidence-aware verification and improving answer reliability.",
            "score": 21,
            "issue_id": 6590,
            "pub_date": "2025-10-23",
            "pub_date_card": {
                "ru": "23 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 23",
                "zh": "10æœˆ23æ—¥"
            },
            "hash": "fecf89fc3cf72cdb",
            "authors": [
                "Jiahao Meng",
                "Xiangtai Li",
                "Haochen Wang",
                "Yue Tan",
                "Tao Zhang",
                "Lingdong Kong",
                "Yunhai Tong",
                "Anran Wang",
                "Zhiyang Teng",
                "Yujing Wang",
                "Zhuochen Wang"
            ],
            "affiliations": [
                "ByteDance",
                "CASIA",
                "NUS",
                "Peking University",
                "WHU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.20579.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#video",
                    "#training",
                    "#benchmark",
                    "#reasoning",
                    "#games"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ’Ğ¸Ğ´ĞµĞ¾-Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°Ğ¼Ğ¸",
                    "desc": "Open-o3 Video â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ½Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ, Ğ½Ğ¾ Ğ¸ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚ĞºĞ¸ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ (bounding boxes), Ğ³Ğ´Ğµ Ğ½Ğ°Ñ…Ğ¾Ğ´ÑÑ‚ÑÑ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ²Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ reinforcement learning Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ V-STAR, ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ² Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Qwen2.5-VL Ğ½Ğ° 14.4% Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ mAM Ğ¸ Ğ½Ğ° 24.2% Ğ¿Ğ¾ mLGM. Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ reasoning traces Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ñ‹ Ğ´Ğ»Ñ test-time scaling, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Grounding Video Reasoning in Spatio-Temporal Evidence",
                    "desc": "Open-o3 Video is a novel framework that enhances video reasoning by incorporating spatio-temporal evidence, allowing it to achieve top performance on various benchmarks. Unlike previous models that only provide textual reasoning, this approach highlights specific timestamps, objects, and bounding boxes, grounding its conclusions in visual data. The model is trained on two meticulously curated datasets that offer unified spatio-temporal annotations, addressing the challenges of tracking and localizing evidence in dynamic video scenes. By employing a cold-start reinforcement learning strategy with tailored rewards, Open-o3 Video not only improves accuracy but also generates reasoning traces that aid in confidence-aware verification during test-time scaling."
                },
                "zh": {
                    "title": "æ—¶ç©ºè¯æ®é©±åŠ¨çš„è§†é¢‘æ¨ç†æ–°çªç ´",
                    "desc": "Open-o3 Video æ˜¯ä¸€ç§è§†é¢‘æ¨ç†æ¨¡å‹ï¼Œå®ƒå°†æ—¶ç©ºè¯æ®æ•´åˆåˆ°æ¨ç†è¿‡ç¨‹ä¸­ï¼Œä»è€Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿçªå‡ºæ˜¾ç¤ºå…³é”®æ—¶é—´æˆ³ã€å¯¹è±¡å’Œè¾¹ç•Œæ¡†ï¼Œä½¿æ¨ç†åŸºäºå…·ä½“çš„è§†è§‰è§‚å¯Ÿã€‚ä¸ºäº†å…‹æœè§†é¢‘æ¨ç†ä¸­çš„æŒ‘æˆ˜ï¼ŒOpen-o3 Video é‡‡ç”¨äº†å†·å¯åŠ¨å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œå¹¶è®¾è®¡äº†å¤šç§å¥–åŠ±æœºåˆ¶ï¼Œä»¥æé«˜ç­”æ¡ˆçš„å‡†ç¡®æ€§å’Œæ—¶ç©ºç²¾åº¦ã€‚æ­¤å¤–ï¼Œæ¨¡å‹ç”Ÿæˆçš„æ¨ç†è½¨è¿¹ä¸ºæµ‹è¯•æ—¶çš„æ‰©å±•æä¾›äº†æœ‰ä»·å€¼çš„ä¿¡å·ï¼Œå¢å¼ºäº†ç­”æ¡ˆçš„å¯é æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.20822",
            "title": "HoloCine: Holistic Generation of Cinematic Multi-Shot Long Video\n  Narratives",
            "url": "https://huggingface.co/papers/2510.20822",
            "abstract": "HoloCine generates coherent multi-shot narratives using a Window Cross-Attention mechanism and Sparse Inter-Shot Self-Attention, enabling end-to-end cinematic creation.  \t\t\t\t\tAI-generated summary \t\t\t\t State-of-the-art text-to-video models excel at generating isolated clips but fall short of creating the coherent, multi-shot narratives, which are the essence of storytelling. We bridge this \"narrative gap\" with HoloCine, a model that generates entire scenes holistically to ensure global consistency from the first shot to the last. Our architecture achieves precise directorial control through a Window Cross-Attention mechanism that localizes text prompts to specific shots, while a Sparse Inter-Shot Self-Attention pattern (dense within shots but sparse between them) ensures the efficiency required for minute-scale generation. Beyond setting a new state-of-the-art in narrative coherence, HoloCine develops remarkable emergent abilities: a persistent memory for characters and scenes, and an intuitive grasp of cinematic techniques. Our work marks a pivotal shift from clip synthesis towards automated filmmaking, making end-to-end cinematic creation a tangible future. Our code is available at: https://holo-cine.github.io/.",
            "score": 12,
            "issue_id": 6590,
            "pub_date": "2025-10-23",
            "pub_date_card": {
                "ru": "23 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 23",
                "zh": "10æœˆ23æ—¥"
            },
            "hash": "d7e5dd1a530e0b1e",
            "authors": [
                "Yihao Meng",
                "Hao Ouyang",
                "Yue Yu",
                "Qiuyu Wang",
                "Wen Wang",
                "Ka Leong Cheng",
                "Hanlin Wang",
                "Yixuan Li",
                "Cheng Chen",
                "Yanhong Zeng",
                "Yujun Shen",
                "Huamin Qu"
            ],
            "affiliations": [
                "Ant Group",
                "CUHK",
                "HKUST",
                "NTU",
                "ZJU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.20822.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#story_generation",
                    "#video",
                    "#cv"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞÑ‚ ĞºĞ»Ğ¸Ğ¿Ğ¾Ğ² Ğº ĞºĞ¸Ğ½Ğ¾: Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ½Ğ°Ñ€Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ¾Ğ²",
                    "desc": "HoloCine â€” ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ²ÑĞ·Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ½Ğ°Ñ€Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ¾Ğ², ÑĞ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ñ… Ğ¸Ğ· Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° ĞºĞ°Ğ´Ñ€Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… text-to-video Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ ĞºĞ»Ğ¸Ğ¿Ñ‹, Ğ½Ğ¾ Ğ½Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ²Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ Ñ†ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Window Cross-Attention Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğº ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¼ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼ Ğ¸ Sparse Inter-Shot Self-Attention Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ´Ğ¾ Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ emergent abilities: ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²ÑƒÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ¾ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ°Ñ… Ğ¸ ÑÑ†ĞµĞ½Ğ°Ñ…, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ĞºĞ¸Ğ½ĞµĞ¼Ğ°Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµÑ…Ğ½Ğ¸Ğº. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ´Ğ²Ğ¸Ğ³ Ğ¾Ñ‚ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ĞºĞ»Ğ¸Ğ¿Ğ¾Ğ² Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼Ñƒ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ†ĞµĞ½Ğ½Ñ‹Ñ… Ñ„Ğ¸Ğ»ÑŒĞ¼Ğ¾Ğ²."
                },
                "en": {
                    "title": "Bridging the Narrative Gap in AI Filmmaking",
                    "desc": "HoloCine is a machine learning model designed to create coherent multi-shot narratives for video generation. It uses a Window Cross-Attention mechanism to focus on specific text prompts for each shot, ensuring that the generated scenes are consistent and aligned with the narrative. Additionally, the Sparse Inter-Shot Self-Attention allows for efficient processing by maintaining dense attention within shots while being sparse between them. This innovative approach not only enhances narrative coherence but also introduces advanced capabilities like character memory and an understanding of cinematic techniques, paving the way for automated filmmaking."
                },
                "zh": {
                    "title": "HoloCineï¼šä»ç‰‡æ®µåˆæˆåˆ°è‡ªåŠ¨ç”µå½±åˆ¶ä½œçš„è½¬å˜",
                    "desc": "HoloCineæ˜¯ä¸€ç§ç”Ÿæˆè¿è´¯å¤šé•œå¤´å™äº‹çš„æ¨¡å‹ï¼Œé‡‡ç”¨äº†çª—å£äº¤å‰æ³¨æ„åŠ›æœºåˆ¶å’Œç¨€ç–é•œå¤´é—´è‡ªæ³¨æ„åŠ›æœºåˆ¶ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿç¡®ä¿ä»ç¬¬ä¸€é•œå¤´åˆ°æœ€åä¸€é•œå¤´çš„å…¨å±€ä¸€è‡´æ€§ï¼Œè§£å†³äº†ç°æœ‰æ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹åœ¨å™äº‹è¿è´¯æ€§ä¸Šçš„ä¸è¶³ã€‚HoloCineé€šè¿‡ç²¾ç¡®çš„å¯¼æ¼”æ§åˆ¶ï¼Œèƒ½å¤Ÿå°†æ–‡æœ¬æç¤ºå®šä½åˆ°ç‰¹å®šé•œå¤´ï¼ŒåŒæ—¶åœ¨é•œå¤´ä¹‹é—´ä¿æŒé«˜æ•ˆçš„ç”Ÿæˆèƒ½åŠ›ã€‚è¯¥æ¨¡å‹ä¸ä»…åœ¨å™äº‹è¿è´¯æ€§ä¸Šè®¾ç«‹äº†æ–°çš„æ ‡å‡†ï¼Œè¿˜å±•ç°äº†å¯¹è§’è‰²å’Œåœºæ™¯çš„æŒä¹…è®°å¿†ä»¥åŠå¯¹ç”µå½±æŠ€æœ¯çš„ç›´è§‚ç†è§£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.19365",
            "title": "The Massive Legal Embedding Benchmark (MLEB)",
            "url": "https://huggingface.co/papers/2510.19365",
            "abstract": "MLEB is the largest open-source benchmark for legal information retrieval, encompassing multiple jurisdictions, document types, and task types.  \t\t\t\t\tAI-generated summary \t\t\t\t We present the Massive Legal Embedding Benchmark (MLEB), the largest, most diverse, and most comprehensive open-source benchmark for legal information retrieval to date. MLEB consists of ten expert-annotated datasets spanning multiple jurisdictions (the US, UK, EU, Australia, Ireland, and Singapore), document types (cases, legislation, regulatory guidance, contracts, and literature), and task types (search, zero-shot classification, and question answering). Seven of the datasets in MLEB were newly constructed in order to fill domain and jurisdictional gaps in the open-source legal information retrieval landscape. We document our methodology in building MLEB and creating the new constituent datasets, and release our code, results, and data openly to assist with reproducible evaluations.",
            "score": 12,
            "issue_id": 6592,
            "pub_date": "2025-10-22",
            "pub_date_card": {
                "ru": "22 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 22",
                "zh": "10æœˆ22æ—¥"
            },
            "hash": "4a2df25a69dc60ad",
            "authors": [
                "Umar Butler",
                "Abdur-Rahman Butler",
                "Adrian Lucas Malec"
            ],
            "affiliations": [
                "Isaacus"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.19365.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#open_source",
                    "#data"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "Ğ­Ñ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ²Ğ¾ Ğ²ÑĞµÑ… Ğ¿Ñ€Ğ°Ğ²Ğ¾Ğ²Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ MLEB â€” ĞºÑ€ÑƒĞ¿Ğ½ĞµĞ¹ÑˆĞ¸Ğ¹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ² ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑÑ„ĞµÑ€Ğµ. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´ĞµÑÑÑ‚ÑŒ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ¸Ğ· ÑˆĞµÑÑ‚Ğ¸ ÑÑ€Ğ¸ÑĞ´Ğ¸ĞºÑ†Ğ¸Ğ¹ (Ğ¡Ğ¨Ğ, Ğ’ĞµĞ»Ğ¸ĞºĞ¾Ğ±Ñ€Ğ¸Ñ‚Ğ°Ğ½Ğ¸Ñ, Ğ•Ğ¡, ĞĞ²ÑÑ‚Ñ€Ğ°Ğ»Ğ¸Ñ, Ğ˜Ñ€Ğ»Ğ°Ğ½Ğ´Ğ¸Ñ, Ğ¡Ğ¸Ğ½Ğ³Ğ°Ğ¿ÑƒÑ€) Ğ¸ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²: ÑÑƒĞ´ĞµĞ±Ğ½Ñ‹Ğµ Ğ´ĞµĞ»Ğ°, Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ´Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ¾, Ñ€ĞµĞ³ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹, ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ĞºÑ‚Ñ‹ Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½ÑƒÑ Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñƒ. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ñ‚ĞµÑÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ°, zero-shot ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹. Ğ¡ĞµĞ¼ÑŒ Ğ¸Ğ· Ğ´ĞµÑÑÑ‚Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ±Ñ‹Ğ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ñ‹ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ´Ğ»Ñ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ğ¾Ğ² Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ°Ñ… Ğ´Ğ»Ñ ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ AI."
                },
                "en": {
                    "title": "Unlocking Legal Insights with MLEB: A Comprehensive Benchmark for Information Retrieval",
                    "desc": "The Massive Legal Embedding Benchmark (MLEB) is a comprehensive open-source resource designed for legal information retrieval. It includes ten expert-annotated datasets that cover various jurisdictions, document types, and retrieval tasks. MLEB aims to address gaps in existing legal datasets by introducing seven newly constructed datasets. The authors provide detailed documentation of their methodology and make their code and results publicly available to promote reproducibility in evaluations."
                },
                "zh": {
                    "title": "æ³•å¾‹ä¿¡æ¯æ£€ç´¢çš„æœ€å¤§å¼€æºåŸºå‡†",
                    "desc": "MLEBæ˜¯è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„å¼€æºæ³•å¾‹ä¿¡æ¯æ£€ç´¢åŸºå‡†ï¼Œæ¶µç›–å¤šä¸ªæ³•åŸŸã€æ–‡æ¡£ç±»å‹å’Œä»»åŠ¡ç±»å‹ã€‚å®ƒåŒ…å«åä¸ªç”±ä¸“å®¶æ ‡æ³¨çš„æ•°æ®é›†ï¼Œæ¶‰åŠç¾å›½ã€è‹±å›½ã€æ¬§ç›Ÿã€æ¾³å¤§åˆ©äºšã€çˆ±å°”å…°å’Œæ–°åŠ å¡ç­‰å¤šä¸ªæ³•åŸŸã€‚MLEBçš„ä¸ƒä¸ªæ•°æ®é›†æ˜¯æ–°æ„å»ºçš„ï¼Œæ—¨åœ¨å¡«è¡¥å¼€æºæ³•å¾‹ä¿¡æ¯æ£€ç´¢é¢†åŸŸçš„ç©ºç™½ã€‚æˆ‘ä»¬è¯¦ç»†è®°å½•äº†æ„å»ºMLEBå’Œæ–°æ•°æ®é›†çš„æ–¹æ³•ï¼Œå¹¶å…¬å¼€å‘å¸ƒä»£ç ã€ç»“æœå’Œæ•°æ®ï¼Œä»¥æ”¯æŒå¯é‡å¤çš„è¯„ä¼°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.19779",
            "title": "AdaSPEC: Selective Knowledge Distillation for Efficient Speculative\n  Decoders",
            "url": "https://huggingface.co/papers/2510.19779",
            "abstract": "AdaSPEC enhances speculative decoding by selectively filtering tokens during knowledge distillation, improving token acceptance rates without sacrificing generation quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Speculative Decoding (SD) accelerates large language model inference by employing a small draft model to generate predictions, which are then verified by a larger target model. The effectiveness of SD hinges on the alignment between these models, which is typically enhanced by Knowledge Distillation (KD). However, conventional KD methods aim to minimize the KL divergence between the draft and target models across all tokens, a goal that is misaligned with the true objective of SD, which is to maximize token acceptance rate. Therefore, draft models often struggle to fully assimilate the target model's knowledge due to capacity constraints, leading to suboptimal performance. To address this challenge, we propose AdaSPEC, a novel method that incorporates selective token filtering into the KD process. AdaSPEC utilizes a reference model to identify and filter out difficult-to-fit tokens, enabling the distillation of a draft model that better aligns with the target model on simpler tokens. This approach improves the overall token acceptance rate without compromising generation quality. We evaluate AdaSPEC across diverse tasks, including arithmetic reasoning, instruction-following, coding, and summarization, using model configurations of 31M/1.4B and 350M/2.7B parameters. Our results demonstrate that AdaSPEC consistently outperforms the state-of-the-art DistillSpec method, achieving higher acceptance rates across all tasks (up to 15\\%). The code is publicly available at https://github.com/yuezhouhu/adaspec.",
            "score": 11,
            "issue_id": 6592,
            "pub_date": "2025-10-22",
            "pub_date_card": {
                "ru": "22 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 22",
                "zh": "10æœˆ22æ—¥"
            },
            "hash": "15a4753734871a9f",
            "authors": [
                "Yuezhou Hu",
                "Jiaxin Guo",
                "Xinyu Feng",
                "Tuo Zhao"
            ],
            "affiliations": [
                "Georgia Institute of Technology",
                "Tsinghua University",
                "University of California, Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.19779.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#optimization",
                    "#alignment",
                    "#inference",
                    "#training"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ°Ñ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°",
                    "desc": "Speculative Decoding ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼Ğ°Ğ»ĞµĞ½ÑŒĞºÑƒÑ draft-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ Ñ†ĞµĞ»ĞµĞ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ. AdaSPEC ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Knowledge Distillation, Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€ÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ draft-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¾Ğ½Ğ° Ğ»ÑƒÑ‡ÑˆĞµ ÑƒÑ‡Ğ¸Ğ»Ğ°ÑÑŒ Ğ½Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ…. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ DistillSpec Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ¾ 15% Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ½Ğ° ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸ÑÑ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ñ‚ 31M Ğ´Ğ¾ 2.7B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing Token Acceptance with AdaSPEC",
                    "desc": "AdaSPEC is a new method that improves speculative decoding by filtering out challenging tokens during knowledge distillation. This selective filtering helps the draft model learn better from the target model, focusing on easier tokens to enhance performance. By doing so, AdaSPEC increases the token acceptance rate, which is crucial for effective generation. The method has been tested on various tasks and shows significant improvements over previous techniques, making it a valuable advancement in the field of machine learning."
                },
                "zh": {
                    "title": "AdaSPECï¼šæå‡æ¨æµ‹è§£ç çš„ä»¤ç‰Œæ¥å—ç‡",
                    "desc": "AdaSPECæ˜¯ä¸€ç§æ”¹è¿›çš„çŸ¥è¯†è’¸é¦æ–¹æ³•ï¼Œé€šè¿‡é€‰æ‹©æ€§è¿‡æ»¤ä»¤ç‰Œæ¥å¢å¼ºæ¨æµ‹è§£ç ã€‚å®ƒè§£å†³äº†ä¼ ç»ŸçŸ¥è¯†è’¸é¦æ–¹æ³•åœ¨ä»¤ç‰Œæ¥å—ç‡å’Œç”Ÿæˆè´¨é‡ä¹‹é—´çš„çŸ›ç›¾ã€‚é€šè¿‡è¯†åˆ«å’Œè¿‡æ»¤éš¾ä»¥é€‚åº”çš„ä»¤ç‰Œï¼ŒAdaSPECä½¿å¾—è‰ç¨¿æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ä¸ç›®æ ‡æ¨¡å‹å¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAdaSPECåœ¨å¤šä¸ªä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºç°æœ‰çš„DistillSpecæ–¹æ³•ï¼Œæ¥å—ç‡æé«˜äº†æœ€å¤š15%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.19304",
            "title": "Loopholing Discrete Diffusion: Deterministic Bypass of the Sampling Wall",
            "url": "https://huggingface.co/papers/2510.19304",
            "abstract": "Loopholing Discrete Diffusion Models (LDDMs) enhance text generation by preserving distributional information through a deterministic latent pathway, reducing perplexity and improving coherence and performance on reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Discrete diffusion models offer a promising alternative to autoregressive generation through parallel decoding, but they suffer from a sampling wall: once categorical sampling occurs, rich distributional information collapses into one-hot vectors and cannot be propagated across steps, forcing subsequent steps to operate with limited information. To mitigate this problem, we introduce Loopholing, a novel and simple mechanism that preserves this information via a deterministic latent pathway, leading to Loopholing Discrete Diffusion Models (LDDMs). Trained efficiently with a self-conditioning strategy, LDDMs achieve substantial gains-reducing generative perplexity by up to 61% over prior baselines, closing (and in some cases surpassing) the gap with autoregressive models, and producing more coherent text. Applied to reasoning tasks, LDDMs also improve performance on arithmetic benchmarks such as Countdown and Game of 24. These results also indicate that loopholing mitigates idle steps and oscillations, providing a scalable path toward high-quality non-autoregressive text generation.",
            "score": 11,
            "issue_id": 6590,
            "pub_date": "2025-10-22",
            "pub_date_card": {
                "ru": "22 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 22",
                "zh": "10æœˆ22æ—¥"
            },
            "hash": "4c70e804c9d70b2e",
            "authors": [
                "Mingyu Jo",
                "Jaesik Yoon",
                "Justin Deschenaux",
                "Caglar Gulcehre",
                "Sungjin Ahn"
            ],
            "affiliations": [
                "EPFL",
                "KAIST",
                "Microsoft",
                "NYU",
                "SAP"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.19304.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#multimodal",
                    "#training",
                    "#reasoning",
                    "#diffusion"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‡ĞµÑ€ĞµĞ· Ğ´ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¾Ğ±Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¿ÑƒÑ‚ÑŒ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Loopholing Discrete Diffusion Models (LDDMs) â€” ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾ÑĞ»Ğµ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ğ°Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ ÑÑ…Ğ»Ğ¾Ğ¿Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ Ğ² one-hot Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ñ‹ Ğ¸ Ñ‚ĞµÑ€ÑĞµÑ‚ÑÑ. LDDMs Ñ€ĞµÑˆĞ°ÑÑ‚ ÑÑ‚Ñƒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ‡ĞµÑ€ĞµĞ· Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ loopholing â€” Ğ´ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑˆĞ°Ğ³Ğ°Ğ¼Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ perplexity Ğ´Ğ¾ 61%, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ ĞºĞ¾Ğ³ĞµÑ€ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… reasoning, Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶Ğ°ÑÑÑŒ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğº autoregressive Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼."
                },
                "en": {
                    "title": "Enhancing Text Generation with Loopholing in Diffusion Models",
                    "desc": "Loopholing Discrete Diffusion Models (LDDMs) improve text generation by maintaining important distributional information through a new deterministic pathway. This approach addresses the issue of information loss that occurs during categorical sampling in traditional discrete diffusion models. By using a self-conditioning strategy, LDDMs significantly reduce perplexity and enhance coherence, making them competitive with autoregressive models. Additionally, LDDMs show improved performance on reasoning tasks, demonstrating their effectiveness in generating high-quality text."
                },
                "zh": {
                    "title": "æ¼æ´ç¦»æ•£æ‰©æ•£æ¨¡å‹ï¼šæå‡æ–‡æœ¬ç”Ÿæˆè´¨é‡çš„åˆ›æ–°æœºåˆ¶",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼Œç§°ä¸ºæ¼æ´ç¦»æ•£æ‰©æ•£æ¨¡å‹ï¼ˆLDDMsï¼‰ï¼Œå®ƒé€šè¿‡ç¡®å®šæ€§æ½œåœ¨è·¯å¾„ä¿ç•™åˆ†å¸ƒä¿¡æ¯ï¼Œä»è€Œæé«˜ç”Ÿæˆæ–‡æœ¬çš„è¿è´¯æ€§å’Œæ€§èƒ½ã€‚ä¼ ç»Ÿçš„ç¦»æ•£æ‰©æ•£æ¨¡å‹åœ¨é‡‡æ ·æ—¶ä¼šå¯¼è‡´ä¿¡æ¯ä¸¢å¤±ï¼Œé™åˆ¶äº†åç»­æ­¥éª¤çš„ç”Ÿæˆèƒ½åŠ›ã€‚LDDMsé€šè¿‡è‡ªæˆ‘æ¡ä»¶åŒ–ç­–ç•¥è¿›è¡Œé«˜æ•ˆè®­ç»ƒï¼Œæ˜¾è‘—é™ä½äº†ç”Ÿæˆå›°æƒ‘åº¦ï¼Œå¹¶åœ¨æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ¼æ´æœºåˆ¶æœ‰æ•ˆå‡å°‘äº†æ— æ•ˆæ­¥éª¤å’ŒæŒ¯è¡ï¼Œä¸ºé«˜è´¨é‡çš„éè‡ªå›å½’æ–‡æœ¬ç”Ÿæˆæä¾›äº†å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.20187",
            "title": "Every Question Has Its Own Value: Reinforcement Learning with Explicit\n  Human Values",
            "url": "https://huggingface.co/papers/2510.20187",
            "abstract": "RLEV, a reinforcement learning method, aligns LLM optimization with human value signals, improving value-weighted accuracy and learning value-sensitive termination policies.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose Reinforcement Learning with Explicit Human Values (RLEV), a method that aligns Large Language Model (LLM) optimization directly with quantifiable human value signals. While Reinforcement Learning with Verifiable Rewards (RLVR) effectively trains models in objective domains using binary correctness rewards, it overlooks that not all tasks are equally significant. RLEV extends this framework by incorporating human-defined value signals directly into the reward function. Using exam-style data with explicit ground-truth value labels, RLEV consistently outperforms correctness-only baselines across multiple RL algorithms and model scales. Crucially, RLEV policies not only improve value-weighted accuracy but also learn a value-sensitive termination policy: concise for low-value prompts, thorough for high-value ones. We demonstrate this behavior stems from value-weighted gradient amplification on end-of-sequence tokens. Ablation studies confirm the gain is causally linked to value alignment. RLEV remains robust under noisy value signals, such as difficulty-based labels, demonstrating that optimizing for an explicit utility function offers a practical path to aligning LLMs with human priorities.",
            "score": 10,
            "issue_id": 6590,
            "pub_date": "2025-10-23",
            "pub_date_card": {
                "ru": "23 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 23",
                "zh": "10æœˆ23æ—¥"
            },
            "hash": "99ea059117e075a9",
            "authors": [
                "Dian Yu",
                "Yulai Zhao",
                "Kishan Panaganti",
                "Linfeng Song",
                "Haitao Mi",
                "Dong Yu"
            ],
            "affiliations": [
                "Princeton University",
                "Tencent AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.20187.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#rl",
                    "#alignment",
                    "#rlhf"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ AI Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ RLEV, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ reinforcement learning Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚Ğ¾Ğ². Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ¾ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, RLEV ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ‡ĞµÑ€ĞµĞ· ÑĞ²Ğ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹, Ğ½Ğ¾ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ²Ğ¾Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ: Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ ĞºÑ€Ğ°Ñ‚ĞºĞ¸Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ½Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¸ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ğ½ÑƒÑ‚Ñ‹Ğµ Ğ½Ğ° Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ Ğ½ĞµÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ñ†ĞµĞ½ĞºĞ°Ñ… Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°Ñ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ LLM Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Aligning AI with Human Values through RLEV",
                    "desc": "RLEV is a reinforcement learning method that enhances the training of Large Language Models (LLMs) by integrating human value signals into the optimization process. Unlike traditional methods that focus solely on correctness, RLEV uses value-weighted rewards to prioritize tasks based on their significance to humans. This approach not only improves the accuracy of the models but also enables them to adapt their responses based on the value of the prompts, providing concise answers for low-value queries and detailed responses for high-value ones. The method has shown robustness even with noisy value signals, indicating its effectiveness in aligning LLMs with human priorities."
                },
                "zh": {
                    "title": "å¼ºåŒ–å­¦ä¹ ä¸äººç±»ä»·å€¼çš„å¯¹é½",
                    "desc": "RLEVæ˜¯ä¸€ç§å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œå®ƒå°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¼˜åŒ–ä¸å¯é‡åŒ–çš„äººç±»ä»·å€¼ä¿¡å·å¯¹é½ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºäºŒå…ƒæ­£ç¡®æ€§çš„å¥–åŠ±æœºåˆ¶ä¸åŒï¼ŒRLEVç›´æ¥å°†äººç±»å®šä¹‰çš„ä»·å€¼ä¿¡å·çº³å…¥å¥–åŠ±å‡½æ•°ï¼Œä»è€Œæé«˜äº†ä»·å€¼åŠ æƒçš„å‡†ç¡®æ€§ã€‚é€šè¿‡ä½¿ç”¨å¸¦æœ‰æ˜ç¡®çœŸå®ä»·å€¼æ ‡ç­¾çš„è€ƒè¯•é£æ ¼æ•°æ®ï¼ŒRLEVåœ¨å¤šä¸ªå¼ºåŒ–å­¦ä¹ ç®—æ³•å’Œæ¨¡å‹è§„æ¨¡ä¸Šå‡ä¼˜äºä»…ä¾èµ–æ­£ç¡®æ€§çš„åŸºçº¿ã€‚RLEVè¿˜å­¦ä¹ åˆ°äº†ä¸€ç§ä»·å€¼æ•æ„Ÿçš„ç»ˆæ­¢ç­–ç•¥ï¼Œæ ¹æ®æç¤ºçš„ä»·å€¼é«˜ä½è°ƒæ•´å“åº”çš„è¯¦ç»†ç¨‹åº¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.19600",
            "title": "Human-Agent Collaborative Paper-to-Page Crafting for Under $0.1",
            "url": "https://huggingface.co/papers/2510.19600",
            "abstract": "AutoPage, a multi-agent system, automates the creation of interactive research webpages through a hierarchical process, ensuring high-quality and efficient results.  \t\t\t\t\tAI-generated summary \t\t\t\t In the quest for scientific progress, communicating research is as vital as the discovery itself. Yet, researchers are often sidetracked by the manual, repetitive chore of building project webpages to make their dense papers accessible. While automation has tackled static slides and posters, the dynamic, interactive nature of webpages has remained an unaddressed challenge. To bridge this gap, we reframe the problem, arguing that the solution lies not in a single command, but in a collaborative, hierarchical process. We introduce AutoPage, a novel multi-agent system that embodies this philosophy. AutoPage deconstructs paper-to-page creation into a coarse-to-fine pipeline from narrative planning to multimodal content generation and interactive rendering. To combat AI hallucination, dedicated \"Checker\" agents verify each step against the source paper, while optional human checkpoints ensure the final product aligns perfectly with the author's vision, transforming the system from a mere tool into a powerful collaborative assistant. To rigorously validate our approach, we also construct PageBench, the first benchmark for this new task. Experiments show AutoPage not only generates high-quality, visually appealing pages but does so with remarkable efficiency in under 15 minutes for less than \\0.1. Code and dataset will be released at https://mqleet.github.io/AutoPage_ProjectPage/{Webpage}$.",
            "score": 7,
            "issue_id": 6591,
            "pub_date": "2025-10-22",
            "pub_date_card": {
                "ru": "22 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 22",
                "zh": "10æœˆ22æ—¥"
            },
            "hash": "dcfba991c39cb2ce",
            "authors": [
                "Qianli Ma",
                "Siyu Wang",
                "Yilin Chen",
                "Yinhao Tang",
                "Yixiang Yang",
                "Chang Guo",
                "Bingjie Gao",
                "Zhening Xing",
                "Yanan Sun",
                "Zhipeng Zhang"
            ],
            "affiliations": [
                "AutoLab, SAI, Shanghai Jiao Tong University",
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.19600.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#hallucinations",
                    "#open_source",
                    "#science",
                    "#agents"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ²ĞµĞ±-ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ† Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹",
                    "desc": "AutoPage â€” ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²ĞµĞ±-ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ´Ğ»Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´: Ğ¾Ñ‚ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ´Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ°. Ğ¡Ğ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹-Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€Ñ‰Ğ¸ĞºĞ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒÑÑ‚ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ¿ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµĞ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ±Ğ¾Ñ€Ğ¾Ñ‚ÑŒÑÑ Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ AI. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ²ĞµĞ±-ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ¼ĞµĞ½ĞµĞµ Ñ‡ĞµĞ¼ Ğ·Ğ° 15 Ğ¼Ğ¸Ğ½ÑƒÑ‚ Ğ¸ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒÑ Ğ´Ğ¾ $0.1, Ğ¿Ñ€ĞµĞ²Ñ€Ğ°Ñ‰Ğ°ÑÑÑŒ Ğ¸Ğ· Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ° Ğ² Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ†ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Transforming Research Communication with AutoPage",
                    "desc": "AutoPage is a multi-agent system designed to automate the creation of interactive research webpages, addressing the challenges researchers face in communicating their work. It employs a hierarchical process that breaks down the task into manageable steps, from narrative planning to content generation and rendering. To ensure accuracy and quality, specialized 'Checker' agents verify each stage against the original research paper, with optional human oversight for final adjustments. The system not only enhances efficiency, producing high-quality webpages in under 15 minutes, but also introduces PageBench, a benchmark for evaluating this new task."
                },
                "zh": {
                    "title": "è‡ªåŠ¨åŒ–ç ”ç©¶ç½‘é¡µåˆ›å»ºçš„æ™ºèƒ½åŠ©æ‰‹",
                    "desc": "AutoPageæ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œæ—¨åœ¨é€šè¿‡åˆ†å±‚è¿‡ç¨‹è‡ªåŠ¨åˆ›å»ºäº’åŠ¨ç ”ç©¶ç½‘é¡µï¼Œä»è€Œç¡®ä¿é«˜è´¨é‡å’Œé«˜æ•ˆç‡çš„ç»“æœã€‚è¯¥ç³»ç»Ÿå°†è®ºæ–‡åˆ°ç½‘é¡µçš„åˆ›å»ºè¿‡ç¨‹åˆ†è§£ä¸ºä»å™è¿°è§„åˆ’åˆ°å¤šæ¨¡æ€å†…å®¹ç”Ÿæˆå’Œäº’åŠ¨æ¸²æŸ“çš„ç²—åˆ°ç»†çš„ç®¡é“ã€‚ä¸ºäº†é˜²æ­¢AIå¹»è§‰ï¼Œä¸“é—¨çš„â€œæ£€æŸ¥å™¨â€ä»£ç†ä¼šéªŒè¯æ¯ä¸€æ­¥ä¸æºè®ºæ–‡çš„ä¸€è‡´æ€§ï¼ŒåŒæ—¶å¯é€‰çš„äººç±»æ£€æŸ¥ç‚¹ç¡®ä¿æœ€ç»ˆäº§å“ä¸ä½œè€…çš„æ„¿æ™¯å®Œç¾å¯¹é½ã€‚é€šè¿‡æ„å»ºPageBenchåŸºå‡†ï¼Œæˆ‘ä»¬ä¸¥æ ¼éªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå®éªŒè¡¨æ˜AutoPageèƒ½å¤Ÿåœ¨15åˆ†é’Ÿå†…ä»¥ä½äº0.1ç¾å…ƒçš„æˆæœ¬ç”Ÿæˆé«˜è´¨é‡ã€è§†è§‰å¸å¼•åŠ›å¼ºçš„ç½‘é¡µã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.20820",
            "title": "LayerComposer: Interactive Personalized T2I via Spatially-Aware Layered\n  Canvas",
            "url": "https://huggingface.co/papers/2510.20820",
            "abstract": "LayerComposer provides interactive control over spatial composition and scalability in multi-subject text-to-image generation through a layered canvas and locking mechanism.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite their impressive visual fidelity, existing personalized generative models lack interactive control over spatial composition and scale poorly to multiple subjects. To address these limitations, we present LayerComposer, an interactive framework for personalized, multi-subject text-to-image generation. Our approach introduces two main contributions: (1) a layered canvas, a novel representation in which each subject is placed on a distinct layer, enabling occlusion-free composition; and (2) a locking mechanism that preserves selected layers with high fidelity while allowing the remaining layers to adapt flexibly to the surrounding context. Similar to professional image-editing software, the proposed layered canvas allows users to place, resize, or lock input subjects through intuitive layer manipulation. Our versatile locking mechanism requires no architectural changes, relying instead on inherent positional embeddings combined with a new complementary data sampling strategy. Extensive experiments demonstrate that LayerComposer achieves superior spatial control and identity preservation compared to the state-of-the-art methods in multi-subject personalized image generation.",
            "score": 3,
            "issue_id": 6590,
            "pub_date": "2025-10-23",
            "pub_date_card": {
                "ru": "23 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 23",
                "zh": "10æœˆ23æ—¥"
            },
            "hash": "cd0c9d6f510364a1",
            "authors": [
                "Guocheng Gordon Qian",
                "Ruihang Zhang",
                "Tsai-Shien Chen",
                "Yusuf Dalva",
                "Anujraaj Argo Goyal",
                "Willi Menapace",
                "Ivan Skorokhodov",
                "Meng Dong",
                "Arpit Sahni",
                "Daniil Ostashev",
                "Ju Hu",
                "Sergey Tulyakov",
                "Kuan-Chieh Jackson Wang"
            ],
            "affiliations": [
                "Snap Inc.",
                "UC Merced",
                "University of Toronto",
                "Virginia Tech"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.20820.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#multimodal",
                    "#games",
                    "#cv"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "ĞŸĞ¾ÑĞ»Ğ¾Ğ¹Ğ½Ğ°Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ: Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "LayerComposer â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Ğ¿Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ğ¾Ğ³Ğ¾ Ñ…Ğ¾Ğ»ÑÑ‚Ğ°. ĞšĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‰Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ ÑĞ»Ğ¾Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ¿ĞµÑ€ĞµĞºÑ€Ñ‹Ñ‚Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸Ğ½Ñ‚ÑƒĞ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¸ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸. ĞœĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ±Ğ»Ğ¾ĞºĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸ ÑĞ»Ğ¾Ñ‘Ğ² ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¾ÑÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğº ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñƒ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ text-to-image Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "LayerComposer: Mastering Multi-Subject Image Generation with Layers",
                    "desc": "LayerComposer is a novel framework designed for personalized text-to-image generation that allows users to have interactive control over how multiple subjects are arranged in an image. It introduces a layered canvas where each subject is placed on a separate layer, which helps in achieving clear and occlusion-free compositions. Additionally, it features a locking mechanism that keeps selected layers intact while allowing other layers to adjust to their context, enhancing flexibility. Through extensive testing, LayerComposer shows improved spatial control and better identity preservation compared to existing methods in the field."
                },
                "zh": {
                    "title": "LayerComposerï¼šå¤šä¸»ä½“ç”Ÿæˆçš„ç©ºé—´æ§åˆ¶æ–°æ–¹å¼",
                    "desc": "LayerComposer æ˜¯ä¸€ä¸ªäº¤äº’å¼æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤šä¸»ä½“æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­çš„ç©ºé—´æ„å›¾å’Œå¯æ‰©å±•æ€§é—®é¢˜ã€‚å®ƒå¼•å…¥äº†åˆ†å±‚ç”»å¸ƒçš„æ¦‚å¿µï¼Œæ¯ä¸ªä¸»ä½“éƒ½æ”¾ç½®åœ¨ç‹¬ç«‹çš„å±‚ä¸Šï¼Œä»è€Œå®ç°æ— é®æŒ¡çš„ç»„åˆæ•ˆæœã€‚è¯¥æ¡†æ¶è¿˜åŒ…å«ä¸€ä¸ªé”å®šæœºåˆ¶ï¼Œå¯ä»¥åœ¨ä¿æŒé€‰å®šå±‚é«˜ä¿çœŸçš„åŒæ—¶ï¼Œçµæ´»è°ƒæ•´å…¶ä»–å±‚ä»¥é€‚åº”å‘¨å›´ç¯å¢ƒã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œç”¨æˆ·å¯ä»¥åƒä½¿ç”¨ä¸“ä¸šå›¾åƒç¼–è¾‘è½¯ä»¶ä¸€æ ·ï¼Œç›´è§‚åœ°æ“ä½œè¾“å…¥ä¸»ä½“ï¼Œè¿›è¡Œæ”¾ç½®ã€è°ƒæ•´å¤§å°æˆ–é”å®šã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.20270",
            "title": "ImpossibleBench: Measuring LLMs' Propensity of Exploiting Test Cases",
            "url": "https://huggingface.co/papers/2510.20270",
            "abstract": "ImpossibleBench is a benchmark framework that measures and mitigates LLMs' tendency to exploit test cases by introducing impossible task variants, thereby enhancing model reliability.  \t\t\t\t\tAI-generated summary \t\t\t\t The tendency to find and exploit \"shortcuts\" to complete tasks poses significant risks for reliable assessment and deployment of large language models (LLMs). For example, an LLM agent with access to unit tests may delete failing tests rather than fix the underlying bug. Such behavior undermines both the validity of benchmark results and the reliability of real-world LLM coding assistant deployments.   To quantify, study, and mitigate such behavior, we introduce ImpossibleBench, a benchmark framework that systematically measures LLM agents' propensity to exploit test cases. ImpossibleBench creates \"impossible\" variants of tasks from existing benchmarks like LiveCodeBench and SWE-bench by introducing direct conflicts between the natural-language specification and the unit tests. We measure an agent's \"cheating rate\" as its pass rate on these impossible tasks, where any pass necessarily implies a specification-violating shortcut.   As a practical framework, ImpossibleBench is not just an evaluation but a versatile tool. We demonstrate its utility for: (1) studying model behaviors, revealing more fine-grained details of cheating behaviors from simple test modification to complex operator overloading; (2) context engineering, showing how prompt, test access and feedback loop affect cheating rates; and (3) developing monitoring tools, providing a testbed with verified deceptive solutions. We hope ImpossibleBench serves as a useful framework for building more robust and reliable LLM systems.   Our implementation can be found at https://github.com/safety-research/impossiblebench.",
            "score": 2,
            "issue_id": 6590,
            "pub_date": "2025-10-23",
            "pub_date_card": {
                "ru": "23 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 23",
                "zh": "10æœˆ23æ—¥"
            },
            "hash": "8843fcc2113f2c3a",
            "authors": [
                "Ziqian Zhong",
                "Aditi Raghunathan",
                "Nicholas Carlini"
            ],
            "affiliations": [
                "Carnegie Mellon University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.20270.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#benchmark",
                    "#security"
                ],
                "emoji": "ğŸš«",
                "ru": {
                    "title": "Ğ›Ğ¾Ğ²ÑƒÑˆĞºĞ° Ğ´Ğ»Ñ Ñ‡Ğ¸Ñ‚ĞµÑ€Ğ¾Ğ²: ĞºĞ°Ğº Ğ¿Ğ¾Ğ¹Ğ¼Ğ°Ñ‚ÑŒ LLM Ğ½Ğ° Ğ½ĞµÑ‡ĞµÑÑ‚Ğ½Ğ¾Ğ¼ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡",
                    "desc": "ImpossibleBench â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµÑ‚ ÑĞºĞ»Ğ¾Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½ĞµÑ‡ĞµÑÑ‚Ğ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ¾Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Â«Ğ½ĞµĞ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹ĞµÂ» Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ñ‹ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸Ğ· ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ², Ğ²Ğ²Ğ¾Ğ´Ñ Ğ¿Ñ€ÑĞ¼Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ unit-Ñ‚ĞµÑÑ‚Ğ°Ğ¼Ğ¸. Ğ›ÑĞ±Ğ¾Ğµ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ°ÑˆĞ»Ğ° Ğ¾Ğ±Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‡ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ¸Ğ·ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ñ… AI-ÑĞ¸ÑÑ‚ĞµĞ¼."
                },
                "en": {
                    "title": "Enhancing LLM Reliability with ImpossibleBench",
                    "desc": "ImpossibleBench is a benchmark framework designed to evaluate and reduce the tendency of large language models (LLMs) to exploit shortcuts in task completion. It introduces 'impossible' task variants that create conflicts between natural-language specifications and unit tests, allowing researchers to measure the 'cheating rate' of LLMs. This framework not only assesses model behavior but also aids in context engineering and the development of monitoring tools to enhance model reliability. By systematically addressing these issues, ImpossibleBench aims to foster the creation of more robust LLM systems."
                },
                "zh": {
                    "title": "æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„å¯é æ€§",
                    "desc": "ImpossibleBenchæ˜¯ä¸€ä¸ªåŸºå‡†æ¡†æ¶ï¼Œç”¨äºæµ‹é‡å’Œå‡è½»å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åˆ©ç”¨æµ‹è¯•ç”¨ä¾‹çš„å€¾å‘ï¼Œä»è€Œå¢å¼ºæ¨¡å‹çš„å¯é æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥ä¸è‡ªç„¶è¯­è¨€è§„èŒƒå’Œå•å…ƒæµ‹è¯•ä¹‹é—´çš„ç›´æ¥å†²çªï¼Œåˆ›å»ºäº†ç°æœ‰åŸºå‡†ä»»åŠ¡çš„â€œä¸å¯å®Œæˆâ€å˜ä½“ã€‚æˆ‘ä»¬é€šè¿‡æµ‹é‡æ¨¡å‹åœ¨è¿™äº›ä¸å¯å®Œæˆä»»åŠ¡ä¸Šçš„é€šè¿‡ç‡æ¥é‡åŒ–å…¶â€œä½œå¼Šç‡â€ï¼Œè¿™æ„å‘³ç€ä»»ä½•é€šè¿‡éƒ½æš—ç¤ºäº†è¿åè§„èŒƒçš„æ·å¾„ã€‚ImpossibleBenchä¸ä»…æ˜¯ä¸€ä¸ªè¯„ä¼°å·¥å…·ï¼Œè¿˜æ˜¯ä¸€ä¸ªå¤šåŠŸèƒ½çš„å·¥å…·ï¼Œå¯ä»¥ç”¨äºç ”ç©¶æ¨¡å‹è¡Œä¸ºã€ä¸Šä¸‹æ–‡å·¥ç¨‹å’Œå¼€å‘ç›‘æ§å·¥å…·ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.20803",
            "title": "ARGenSeg: Image Segmentation with Autoregressive Image Generation Model",
            "url": "https://huggingface.co/papers/2510.20803",
            "abstract": "A novel AutoRegressive Generation-based paradigm for image segmentation leverages multimodal large language models and VQ-VAE for dense mask generation, achieving high performance and fast inference.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose a novel AutoRegressive Generation-based paradigm for image Segmentation (ARGenSeg), achieving multimodal understanding and pixel-level perception within a unified framework. Prior works integrating image segmentation into multimodal large language models (MLLMs) typically employ either boundary points representation or dedicated segmentation heads. These methods rely on discrete representations or semantic prompts fed into task-specific decoders, which limits the ability of the MLLM to capture fine-grained visual details. To address these challenges, we introduce a segmentation framework for MLLM based on image generation, which naturally produces dense masks for target objects. We leverage MLLM to output visual tokens and detokenize them into images using an universal VQ-VAE, making the segmentation fully dependent on the pixel-level understanding of the MLLM. To reduce inference latency, we employ a next-scale-prediction strategy to generate required visual tokens in parallel. Extensive experiments demonstrate that our method surpasses prior state-of-the-art approaches on multiple segmentation datasets with a remarkable boost in inference speed, while maintaining strong understanding capabilities.",
            "score": 1,
            "issue_id": 6591,
            "pub_date": "2025-10-23",
            "pub_date_card": {
                "ru": "23 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 23",
                "zh": "10æœˆ23æ—¥"
            },
            "hash": "b7296c78740064e7",
            "authors": [
                "Xiaolong Wang",
                "Lixiang Ru",
                "Ziyuan Huang",
                "Kaixiang Ji",
                "Dandan Zheng",
                "Jingdong Chen",
                "Jun Zhou"
            ],
            "affiliations": [
                "Ant Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.20803.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#multimodal",
                    "#inference",
                    "#games",
                    "#cv"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "Ğ¡ĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ°ÑĞ¾Ğº",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ARGenSeg, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¸Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ², Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğµ Ğ¼Ğ°ÑĞºĞ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¸ VQ-VAE. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ¿Ğ¸Ñ€Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ÑĞ°Ğ¼Ğ¸Ğ¼ LLM, Ğ° ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Revolutionizing Image Segmentation with ARGenSeg!",
                    "desc": "This paper introduces a new method called AutoRegressive Generation-based paradigm for image segmentation (ARGenSeg) that enhances how machines understand and segment images. It combines multimodal large language models (MLLMs) with a technique called VQ-VAE to create detailed masks for objects in images. Unlike previous methods that used discrete representations, this approach generates visual tokens directly, allowing for better pixel-level accuracy. The framework also speeds up the process of generating these masks, achieving faster results while maintaining high performance on various segmentation tasks."
                },
                "zh": {
                    "title": "å›¾åƒåˆ†å‰²çš„æ–°èŒƒå¼ï¼šè‡ªå›å½’ç”Ÿæˆ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŸºäºè‡ªå›å½’ç”Ÿæˆçš„å›¾åƒåˆ†å‰²èŒƒå¼ï¼ˆARGenSegï¼‰ï¼Œå®ç°äº†å¤šæ¨¡æ€ç†è§£å’Œåƒç´ çº§æ„ŸçŸ¥ã€‚ä¸ä»¥å¾€å°†å›¾åƒåˆ†å‰²é›†æˆåˆ°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä¸­çš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ¡†æ¶åŸºäºå›¾åƒç”Ÿæˆï¼Œè‡ªç„¶ç”Ÿæˆç›®æ ‡å¯¹è±¡çš„å¯†é›†æ©ç ã€‚æˆ‘ä»¬åˆ©ç”¨MLLMè¾“å‡ºè§†è§‰æ ‡è®°ï¼Œå¹¶é€šè¿‡é€šç”¨çš„VQ-VAEå°†å…¶è§£ç ä¸ºå›¾åƒï¼Œä»è€Œä½¿åˆ†å‰²å®Œå…¨ä¾èµ–äºMLLMçš„åƒç´ çº§ç†è§£ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªåˆ†å‰²æ•°æ®é›†ä¸Šè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼ŒåŒæ—¶æ˜¾è‘—æé«˜äº†æ¨ç†é€Ÿåº¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.20771",
            "title": "AlphaFlow: Understanding and Improving MeanFlow Models",
            "url": "https://huggingface.co/papers/2510.20771",
            "abstract": "The $\\alpha$-Flow framework improves few-step generative modeling by unifying and disentangling conflicting objectives, leading to better convergence and state-of-the-art performance on ImageNet-1K.  \t\t\t\t\tAI-generated summary \t\t\t\t MeanFlow has recently emerged as a powerful framework for few-step generative modeling trained from scratch, but its success is not yet fully understood. In this work, we show that the MeanFlow objective naturally decomposes into two parts: trajectory flow matching and trajectory consistency. Through gradient analysis, we find that these terms are strongly negatively correlated, causing optimization conflict and slow convergence. Motivated by these insights, we introduce alpha-Flow, a broad family of objectives that unifies trajectory flow matching, Shortcut Model, and MeanFlow under one formulation. By adopting a curriculum strategy that smoothly anneals from trajectory flow matching to MeanFlow, alpha-Flow disentangles the conflicting objectives, and achieves better convergence. When trained from scratch on class-conditional ImageNet-1K 256x256 with vanilla DiT backbones, alpha-Flow consistently outperforms MeanFlow across scales and settings. Our largest alpha-Flow-XL/2+ model achieves new state-of-the-art results using vanilla DiT backbones, with FID scores of 2.58 (1-NFE) and 2.15 (2-NFE).",
            "score": 1,
            "issue_id": 6590,
            "pub_date": "2025-10-23",
            "pub_date_card": {
                "ru": "23 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 23",
                "zh": "10æœˆ23æ—¥"
            },
            "hash": "d6e50ecd48746859",
            "authors": [
                "Huijie Zhang",
                "Aliaksandr Siarohin",
                "Willi Menapace",
                "Michael Vasilkovsky",
                "Sergey Tulyakov",
                "Qing Qu",
                "Ivan Skorokhodov"
            ],
            "affiliations": [
                "Department of EECS, University of Michigan",
                "Snap Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.20771.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#optimization",
                    "#training",
                    "#cv",
                    "#diffusion"
                ],
                "emoji": "ğŸŒŠ",
                "ru": {
                    "title": "Î±-Flow: Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞ¹ Ğ¸ Ğ²Ğ»Ğ°ÑÑ‚Ğ²ÑƒĞ¹ Ğ² Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Î±-Flow â€” ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¹ framework Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ²ÑĞµĞ³Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑˆĞ°Ğ³Ğ¾Ğ² Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ MeanFlow ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚ÑƒÑÑ‰Ğ¸Ğµ Ñ‡Ğ°ÑÑ‚Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ (trajectory flow matching Ğ¸ trajectory consistency), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼ĞµÑˆĞ°ÑÑ‚ Ğ´Ñ€ÑƒĞ³ Ğ´Ñ€ÑƒĞ³Ñƒ. ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ curriculum strategy â€” Ğ¿Ğ»Ğ°Ğ²Ğ½Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğ¼Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ÑĞ¼Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ° Ğ¸ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ. ĞĞ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ ImageNet-1K Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Î±-Flow Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ñ FID 2.58 Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ ÑˆĞ°Ğ³ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ DiT."
                },
                "en": {
                    "title": "Unifying Objectives for Superior Few-Step Generative Modeling",
                    "desc": "The $\beta$-Flow framework enhances few-step generative modeling by addressing and resolving conflicting objectives, which leads to improved convergence rates and top-tier performance on ImageNet-1K. This paper reveals that the MeanFlow objective can be broken down into two components: trajectory flow matching and trajectory consistency, which are negatively correlated and hinder optimization. To tackle this issue, the authors propose alpha-Flow, a comprehensive set of objectives that integrates trajectory flow matching, Shortcut Model, and MeanFlow into a single framework. By employing a curriculum learning approach that gradually transitions from trajectory flow matching to MeanFlow, alpha-Flow effectively disentangles these conflicting objectives, resulting in superior performance compared to MeanFlow."
                },
                "zh": {
                    "title": "Î±-Flowï¼šè§£è€¦å†²çªç›®æ ‡ï¼Œå®ç°æ›´ä¼˜ç”Ÿæˆå»ºæ¨¡",
                    "desc": "æœ¬æ–‡æå‡ºäº†Î±-Flowæ¡†æ¶ï¼Œé€šè¿‡ç»Ÿä¸€å’Œè§£è€¦ç›¸äº’å†²çªçš„ç›®æ ‡ï¼Œæ”¹å–„äº†å°‘æ­¥ç”Ÿæˆå»ºæ¨¡çš„æ•ˆæœã€‚æˆ‘ä»¬å‘ç°MeanFlowç›®æ ‡å¯ä»¥è‡ªç„¶åˆ†è§£ä¸ºè½¨è¿¹æµåŒ¹é…å’Œè½¨è¿¹ä¸€è‡´æ€§ä¸¤ä¸ªéƒ¨åˆ†ï¼Œè¿™ä¸¤ä¸ªéƒ¨åˆ†ä¹‹é—´å­˜åœ¨å¼ºçƒˆçš„è´Ÿç›¸å…³ï¼Œå¯¼è‡´ä¼˜åŒ–å†²çªå’Œæ”¶æ•›ç¼“æ…¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒÎ±-Flowå¼•å…¥äº†ä¸€ç§æ–°çš„ç›®æ ‡å‡½æ•°ï¼Œç»“åˆäº†è½¨è¿¹æµåŒ¹é…ã€Shortcutæ¨¡å‹å’ŒMeanFlowï¼Œå¹¶é‡‡ç”¨é€æ­¥ç­–ç•¥å¹³æ»‘è¿‡æ¸¡ï¼Œä»è€Œè§£è€¦äº†å†²çªç›®æ ‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒÎ±-Flowåœ¨ImageNet-1Kæ•°æ®é›†ä¸Šè®­ç»ƒæ—¶ï¼Œè¡¨ç°ä¼˜äºMeanFlowï¼Œè¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›ç»“æœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.20766",
            "title": "DyPE: Dynamic Position Extrapolation for Ultra High Resolution Diffusion",
            "url": "https://huggingface.co/papers/2510.20766",
            "abstract": "Dynamic Position Extrapolation (DyPE) enhances ultra-high-resolution image generation by dynamically adjusting positional encodings in pre-trained diffusion transformers, achieving state-of-the-art fidelity without additional sampling cost.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion Transformer models can generate images with remarkable fidelity and detail, yet training them at ultra-high resolutions remains extremely costly due to the self-attention mechanism's quadratic scaling with the number of image tokens. In this paper, we introduce Dynamic Position Extrapolation (DyPE), a novel, training-free method that enables pre-trained diffusion transformers to synthesize images at resolutions far beyond their training data, with no additional sampling cost. DyPE takes advantage of the spectral progression inherent to the diffusion process, where low-frequency structures converge early, while high-frequencies take more steps to resolve. Specifically, DyPE dynamically adjusts the model's positional encoding at each diffusion step, matching their frequency spectrum with the current stage of the generative process. This approach allows us to generate images at resolutions that exceed the training resolution dramatically, e.g., 16 million pixels using FLUX. On multiple benchmarks, DyPE consistently improves performance and achieves state-of-the-art fidelity in ultra-high-resolution image generation, with gains becoming even more pronounced at higher resolutions. Project page is available at https://noamissachar.github.io/DyPE/.",
            "score": 1,
            "issue_id": 6592,
            "pub_date": "2025-10-23",
            "pub_date_card": {
                "ru": "23 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 23",
                "zh": "10æœˆ23æ—¥"
            },
            "hash": "eab34606a3b59eda",
            "authors": [
                "Noam Issachar",
                "Guy Yariv",
                "Sagie Benaim",
                "Yossi Adi",
                "Dani Lischinski",
                "Raanan Fattal"
            ],
            "affiliations": [
                "The Hebrew University of Jerusalem"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.20766.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#optimization",
                    "#cv",
                    "#benchmark",
                    "#architecture"
                ],
                "emoji": "ğŸ”­",
                "ru": {
                    "title": "Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑĞ²ĞµÑ€Ñ…Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞºÑÑ‚Ñ€Ğ°Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Dynamic Position Extrapolation (DyPE) - Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ÑĞ²ĞµÑ€Ñ…Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… diffusion transformers Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. DyPE Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ñ: Ğ½Ğ¸Ğ·ĞºĞ¾Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ñ€Ğ°Ğ½Ğ¾, Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ - Ğ¿Ğ¾Ğ·Ğ¶Ğµ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ¾ 16 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹ (Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ FLUX) Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ½Ğ° ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. DyPE Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ÑĞ²ĞµÑ€Ñ…Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, Ğ¿Ñ€Ğ¸Ñ‡Ñ‘Ğ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ÑÑ‚ÑÑ ĞµÑ‰Ñ‘ Ğ±Ğ¾Ğ»ĞµĞµ Ğ·Ğ°Ğ¼ĞµÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Revolutionizing Image Generation with Dynamic Position Extrapolation",
                    "desc": "Dynamic Position Extrapolation (DyPE) is a method that improves ultra-high-resolution image generation using pre-trained diffusion transformers. It works by adjusting positional encodings dynamically during the image synthesis process, allowing the model to generate images at resolutions much higher than it was originally trained on. This technique leverages the diffusion process's spectral properties, ensuring that low-frequency details are resolved quickly while high-frequency details are refined over time. As a result, DyPE achieves state-of-the-art image fidelity without incurring additional sampling costs, making it highly efficient for generating images up to 16 million pixels."
                },
                "zh": {
                    "title": "åŠ¨æ€ä½ç½®å¤–æ¨ï¼šè¶…é«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆçš„æ–°çªç ´",
                    "desc": "åŠ¨æ€ä½ç½®å¤–æ¨ï¼ˆDyPEï¼‰æ˜¯ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œå¯ä»¥åœ¨ä¸å¢åŠ é‡‡æ ·æˆæœ¬çš„æƒ…å†µä¸‹ï¼Œå¢å¼ºè¶…é«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆã€‚å®ƒé€šè¿‡åŠ¨æ€è°ƒæ•´é¢„è®­ç»ƒæ‰©æ•£å˜æ¢å™¨ä¸­çš„ä½ç½®ç¼–ç ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåˆæˆè¶…å‡ºè®­ç»ƒæ•°æ®çš„å›¾åƒåˆ†è¾¨ç‡ã€‚DyPEåˆ©ç”¨æ‰©æ•£è¿‡ç¨‹ä¸­çš„é¢‘è°±è¿›å±•ç‰¹æ€§ï¼ŒåŒ¹é…ç”Ÿæˆè¿‡ç¨‹çš„å½“å‰é˜¶æ®µï¼Œä»è€Œæé«˜å›¾åƒç”Ÿæˆçš„ä¿çœŸåº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDyPEåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨æ›´é«˜åˆ†è¾¨ç‡ä¸‹ï¼Œæ€§èƒ½æå‡æ›´åŠ æ˜¾è‘—ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.20470",
            "title": "Conan: Progressive Learning to Reason Like a Detective over Multi-Scale\n  Visual Evidence",
            "url": "https://huggingface.co/papers/2510.20470",
            "abstract": "Conan, a framework for evidence-grounded multi-step video reasoning, enhances visual grounding and reasoning accuracy through a multi-stage training strategy and outperforms existing models on various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Video reasoning, which requires multi-step deduction across frames, remains a major challenge for multimodal large language models (MLLMs). While reinforcement learning (RL)-based methods enhance reasoning capabilities, they often rely on text-only chains that yield ungrounded or hallucinated conclusions. Conversely, frame-retrieval approaches introduce visual grounding but still struggle with inaccurate evidence localization. To address these challenges, we present Conan, a framework for evidence-grounded multi-step video reasoning. Conan identifies contextual and evidence frames, reasons over cross-frame clues, and adaptively decides when to conclude or explore further. To achieve this, we (1) construct Conan-91K, a large-scale dataset of automatically generated reasoning traces that includes frame identification, evidence reasoning, and action decision, and (2) design a multi-stage progressive cold-start strategy combined with an Identification-Reasoning-Action (AIR) RLVR training framework to jointly enhance multi-step visual reasoning. Extensive experiments on six multi-step reasoning benchmarks demonstrate that Conan surpasses the baseline Qwen2.5-VL-7B-Instruct by an average of over 10% in accuracy, achieving state-of-the-art performance. Furthermore, Conan generalizes effectively to long-video understanding tasks, validating its strong scalability and robustness.",
            "score": 1,
            "issue_id": 6591,
            "pub_date": "2025-10-23",
            "pub_date_card": {
                "ru": "23 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 23",
                "zh": "10æœˆ23æ—¥"
            },
            "hash": "3b033cfc30f5dd2d",
            "authors": [
                "Kun Ouyang",
                "Yuanxin Liu",
                "Linli Yao",
                "Yishuo Cai",
                "Hao Zhou",
                "Jie Zhou",
                "Fandong Meng",
                "Xu Sun"
            ],
            "affiliations": [
                "State Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University",
                "WeChat AI, Tencent Inc., China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.20470.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#reasoning",
                    "#long_context",
                    "#dataset",
                    "#rl",
                    "#video"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Conan: ĞŸĞ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°Ğ¼Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Conan â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¾Ğ¿Ğ¾Ñ€Ğ¾Ğ¹ Ğ½Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ĞºĞ°Ğ´Ñ€Ñ‹, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»Ğ¸ĞºĞ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€ĞµÑˆĞ°ĞµÑ‚, ĞºĞ¾Ğ³Ğ´Ğ° Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞ¸Ñ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾Ğ¸ÑĞº. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Conan-91K Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ñ reinforcement learning, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ, Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. Conan Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Qwen2.5-VL Ğ½Ğ° 10% Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑˆĞµÑÑ‚Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ½ÑƒÑ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Conan: Elevating Video Reasoning with Evidence and Accuracy",
                    "desc": "Conan is a new framework designed to improve video reasoning by using evidence from multiple frames. It tackles the challenges of visual grounding and reasoning accuracy through a multi-stage training approach. By utilizing a large dataset called Conan-91K, it enhances the model's ability to identify relevant frames and make informed decisions based on visual evidence. The results show that Conan outperforms existing models, achieving higher accuracy and demonstrating strong performance in understanding long videos."
                },
                "zh": {
                    "title": "Conanï¼šæå‡è§†é¢‘æ¨ç†çš„è¯æ®åŸºç¡€æ¡†æ¶",
                    "desc": "Conanæ˜¯ä¸€ä¸ªç”¨äºåŸºäºè¯æ®çš„å¤šæ­¥éª¤è§†é¢‘æ¨ç†çš„æ¡†æ¶ï¼Œé€šè¿‡å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥æé«˜è§†è§‰å®šä½å’Œæ¨ç†çš„å‡†ç¡®æ€§ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿè¯†åˆ«ä¸Šä¸‹æ–‡å’Œè¯æ®å¸§ï¼Œå¹¶åœ¨è·¨å¸§çº¿ç´¢ä¸Šè¿›è¡Œæ¨ç†ï¼Œçµæ´»å†³å®šä½•æ—¶ç»“æŸæ¨ç†æˆ–è¿›ä¸€æ­¥æ¢ç´¢ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œç ”ç©¶è€…æ„å»ºäº†ä¸€ä¸ªåä¸ºConan-91Kçš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå¹¶è®¾è®¡äº†ç»“åˆè¯†åˆ«-æ¨ç†-è¡ŒåŠ¨çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¡†æ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒConanåœ¨å¤šä¸ªå¤šæ­¥éª¤æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œå‡†ç¡®ç‡å¹³å‡æé«˜è¶…è¿‡10%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.15804",
            "title": "Emergence of Linear Truth Encodings in Language Models",
            "url": "https://huggingface.co/papers/2510.15804",
            "abstract": "A one-layer transformer model demonstrates how linear subspaces separating true from false statements can emerge in language models through memorization and subsequent linear separation.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent probing studies reveal that large language models exhibit linear subspaces that separate true from false statements, yet the mechanism behind their emergence is unclear. We introduce a transparent, one-layer transformer toy model that reproduces such truth subspaces end-to-end and exposes one concrete route by which they can arise. We study one simple setting in which truth encoding can emerge: a data distribution where factual statements co-occur with other factual statements (and vice-versa), encouraging the model to learn this distinction in order to lower the LM loss on future tokens. We corroborate this pattern with experiments in pretrained language models. Finally, in the toy setting we observe a two-phase learning dynamic: networks first memorize individual factual associations in a few steps, then -- over a longer horizon -- learn to linearly separate true from false, which in turn lowers language-modeling loss. Together, these results provide both a mechanistic demonstration and an empirical motivation for how and why linear truth representations can emerge in language models.",
            "score": 0,
            "issue_id": 6593,
            "pub_date": "2025-10-17",
            "pub_date_card": {
                "ru": "17 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 17",
                "zh": "10æœˆ17æ—¥"
            },
            "hash": "13b71229edbc4573",
            "authors": [
                "Shauli Ravfogel",
                "Gilad Yehudai",
                "Tal Linzen",
                "Joan Bruna",
                "Alberto Bietti"
            ],
            "affiliations": [
                "Flatiron Institute",
                "New York University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.15804.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#reasoning",
                    "#interpretability"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "ĞšĞ°Ğº Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ ÑƒÑ‡Ğ°Ñ‚ÑÑ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ°Ğ²Ğ´Ñƒ Ğ¾Ñ‚ Ğ»Ğ¶Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚ÑƒÑ Ğ¾Ğ´Ğ½Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½ÑƒÑ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, ĞºĞ°Ğº Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‚ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°, Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑÑ‰Ğ¸Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¸Ğ²Ñ‹Ğµ Ğ¸ Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ´Ğ²Ğ° ÑÑ‚Ğ°Ğ¿Ğ°: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°ĞµÑ‚ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ„Ğ°ĞºÑ‚Ñ‹, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑ‚ÑŒ Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ loss Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°ÑÑ‚ÑÑ Ğ²Ğ¼ĞµÑÑ‚Ğµ Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ (Ğ¸ Ğ½Ğ°Ğ¾Ğ±Ğ¾Ñ€Ğ¾Ñ‚ Ğ´Ğ»Ñ Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ñ…), Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ±ÑƒĞ¶Ğ´Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ñ‹ÑƒÑ‡Ğ¸Ñ‚ÑŒ ÑÑ‚Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ğµ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… LLM Ğ¸ Ğ¾Ğ±ÑŠÑÑĞ½ÑÑÑ‚, Ğ¿Ğ¾Ñ‡ĞµĞ¼Ñƒ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ÑĞ²Ğ»ÑÑÑ‚ÑÑ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…."
                },
                "en": {
                    "title": "Unveiling Truth: How Transformers Learn to Separate Facts from Fiction",
                    "desc": "This paper explores how a simple one-layer transformer model can learn to distinguish between true and false statements in language. It shows that through memorization of factual associations, the model can create linear subspaces that separate these statements. The authors demonstrate that this separation occurs in two phases: first, the model memorizes facts, and then it learns to linearly separate true from false statements over time. This research provides insights into the mechanisms behind truth representation in large language models and how they can improve performance in language tasks."
                },
                "zh": {
                    "title": "æ­ç¤ºè¯­è¨€æ¨¡å‹ä¸­çš„çœŸå®ä¸è™šå‡åˆ†ç¦»æœºåˆ¶",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†ä¸€å±‚å˜æ¢å™¨æ¨¡å‹å¦‚ä½•é€šè¿‡è®°å¿†å’Œçº¿æ€§åˆ†ç¦»ï¼Œå½¢æˆåŒºåˆ†çœŸå®ä¸è™šå‡é™ˆè¿°çš„çº¿æ€§å­ç©ºé—´ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ä¸­å­˜åœ¨è¿™æ ·çš„çº¿æ€§å­ç©ºé—´ï¼Œä½†å…¶å½¢æˆæœºåˆ¶å°šä¸æ¸…æ¥šã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé€æ˜çš„ç©å…·æ¨¡å‹ï¼Œå±•ç¤ºäº†çœŸå®ç¼–ç å¦‚ä½•åœ¨ç‰¹å®šæ•°æ®åˆ†å¸ƒä¸­å‡ºç°ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†è¿™ä¸€æ¨¡å¼ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ç½‘ç»œåœ¨å­¦ä¹ è¿‡ç¨‹ä¸­ç»å†äº†ä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆå¿«é€Ÿè®°å¿†ä¸ªåˆ«äº‹å®å…³è”ï¼Œç„¶ååœ¨æ›´é•¿çš„æ—¶é—´å†…å­¦ä¹ çº¿æ€§åˆ†ç¦»çœŸå®ä¸è™šå‡ï¼Œä»è€Œé™ä½è¯­è¨€æ¨¡å‹çš„æŸå¤±ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-10-23.html",
    "link_next": "2025-10-27.html",
    "link_month": "2025-10.html",
    "short_date_prev": {
        "ru": "23.10",
        "en": "10/23",
        "zh": "10æœˆ23æ—¥"
    },
    "short_date_next": {
        "ru": "27.10",
        "en": "10/27",
        "zh": "10æœˆ27æ—¥"
    },
    "categories": {
        "#dataset": 3,
        "#data": 3,
        "#benchmark": 6,
        "#agents": 1,
        "#cv": 5,
        "#rl": 2,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 0,
        "#audio": 0,
        "#video": 3,
        "#multimodal": 5,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 5,
        "#robotics": 0,
        "#agi": 0,
        "#games": 3,
        "#interpretability": 2,
        "#reasoning": 5,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 5,
        "#survey": 0,
        "#diffusion": 3,
        "#alignment": 2,
        "#story_generation": 1,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    }
}