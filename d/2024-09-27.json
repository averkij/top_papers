{
    "date": {
        "ru": "27 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
        "en": "September 27",
        "zh": "9æœˆ27æ—¥"
    },
    "time_utc": "2024-09-27 09:00",
    "weekday": 4,
    "issue_id": 2,
    "home_page_url": "https://huggingface.co/papers?date=2024-09-27",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2409.17481",
            "title": "MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models",
            "url": "https://huggingface.co/papers/2409.17481",
            "abstract": "Large Language Models (LLMs) are distinguished by their massive parameter counts, which typically result in significant redundancy. This work introduces MaskLLM, a learnable pruning method that establishes Semi-structured (or ``N:M'') Sparsity in LLMs, aimed at reducing computational overhead during inference. Instead of developing a new importance criterion, MaskLLM explicitly models N:M patterns as a learnable distribution through Gumbel Softmax sampling. This approach facilitates end-to-end training on large-scale datasets and offers two notable advantages: 1) High-quality Masks - our method effectively scales to large datasets and learns accurate masks; 2) Transferability - the probabilistic modeling of mask distribution enables the transfer learning of sparsity across domains or tasks. We assessed MaskLLM using 2:4 sparsity on various LLMs, including LLaMA-2, Nemotron-4, and GPT-3, with sizes ranging from 843M to 15B parameters, and our empirical results show substantial improvements over state-of-the-art methods. For instance, leading approaches achieve a perplexity (PPL) of 10 or greater on Wikitext compared to the dense model's 5.12 PPL, but MaskLLM achieves a significantly lower 6.72 PPL solely by learning the masks with frozen weights. Furthermore, MaskLLM's learnable nature allows customized masks for lossless application of 2:4 sparsity to downstream tasks or domains. Code is available at https://github.com/NVlabs/MaskLLM.",
            "score": 46,
            "issue_id": 1,
            "pub_date": "2024-09-26",
            "pub_date_card": {
                "ru": "26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 26",
                "zh": "9æœˆ26æ—¥"
            },
            "hash": "9bb73b25aad1001a",
            "authors": [
                "Gongfan Fang",
                "Hongxu Yin",
                "Saurav Muralidharan",
                "Greg Heinrich",
                "Jeff Pool",
                "Jan Kautz",
                "Pavlo Molchanov",
                "Xinchao Wang"
            ],
            "affiliations": [
                "NVIDIA",
                "National University of Singapore"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2409.17481.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#inference",
                    "#optimization",
                    "#transfer_learning",
                    "#open_source",
                    "#architecture"
                ],
                "emoji": "âœ‚ï¸",
                "ru": {
                    "title": "MaskLLM: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MaskLLM - Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ¾Ğ¹ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑƒÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). MaskLLM Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ N:M ĞºĞ°Ğº Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ“ÑƒĞ¼Ğ±ĞµĞ»Ñ-Ğ¡Ğ¾Ñ„Ñ‚Ğ¼Ğ°ĞºÑĞ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ ÑĞºĞ²Ğ¾Ğ·Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ°ÑĞºĞ¸ Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ğ¼Ğ¸ Ğ¸Ğ»Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ 2:4 Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ LLM."
                },
                "en": {
                    "title": "Efficient Pruning of Large Language Models with MaskLLM",
                    "desc": "This paper presents MaskLLM, a novel method for pruning large language models (LLMs) by introducing Semi-structured (N:M) sparsity to reduce computational costs during inference. MaskLLM utilizes Gumbel Softmax sampling to model N:M patterns as a learnable distribution, allowing for end-to-end training on extensive datasets. The method not only generates high-quality masks that scale effectively but also enables transfer learning of sparsity across different tasks. Empirical results demonstrate that MaskLLM outperforms existing methods, achieving lower perplexity scores while maintaining the ability to apply customized masks for various downstream applications."
                },
                "zh": {
                    "title": "MaskLLMï¼šé«˜æ•ˆç¨€ç–åŒ–çš„å¤§å‹è¯­è¨€æ¨¡å‹",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šå¸¸å…·æœ‰å¤§é‡å‚æ•°ï¼Œå¯¼è‡´è®¡ç®—å†—ä½™ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºMaskLLMçš„å¯å­¦ä¹ å‰ªææ–¹æ³•ï¼Œé€šè¿‡å»ºç«‹åŠç»“æ„åŒ–ï¼ˆæˆ–â€œN:Mâ€ï¼‰ç¨€ç–æ€§æ¥å‡å°‘æ¨ç†è¿‡ç¨‹ä¸­çš„è®¡ç®—å¼€é”€ã€‚MaskLLMé€šè¿‡Gumbel Softmaxé‡‡æ ·æ˜¾å¼å»ºæ¨¡N:Mæ¨¡å¼ï¼Œæ”¯æŒåœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMaskLLMåœ¨å¤šä¸ªLLMä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä¸”å…¶å¯å­¦ä¹ ç‰¹æ€§ä½¿å¾—åœ¨ä¸åŒä»»åŠ¡æˆ–é¢†åŸŸé—´çš„ç¨€ç–æ€§è½¬ç§»æˆä¸ºå¯èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.18042",
            "title": "EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions",
            "url": "https://huggingface.co/papers/2409.18042",
            "abstract": "GPT-4o, an omni-modal model that enables vocal conversations with diverse emotions and tones, marks a milestone for omni-modal foundation models. However, empowering Large Language Models to perceive and generate images, texts, and speeches end-to-end with publicly available data remains challenging in the open-source community. Existing vision-language models rely on external tools for the speech processing, while speech-language models still suffer from limited or even without vision-understanding abilities. To address this gap, we propose EMOVA (EMotionally Omni-present Voice Assistant), to enable Large Language Models with end-to-end speech capabilities while maintaining the leading vision-language performance. With a semantic-acoustic disentangled speech tokenizer, we notice surprisingly that omni-modal alignment can further enhance vision-language and speech abilities compared with the corresponding bi-modal aligned counterparts. Moreover, a lightweight style module is proposed for flexible speech style controls (e.g., emotions and pitches). For the first time, EMOVA achieves state-of-the-art performance on both the vision-language and speech benchmarks, and meanwhile, supporting omni-modal spoken dialogue with vivid emotions.",
            "score": 36,
            "issue_id": 1,
            "pub_date": "2024-09-26",
            "pub_date_card": {
                "ru": "26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 26",
                "zh": "9æœˆ26æ—¥"
            },
            "hash": "227cd783a8a6d39c",
            "authors": [
                "Kai Chen",
                "Yunhao Gou",
                "Runhui Huang",
                "Zhili Liu",
                "Daxin Tan",
                "Jing Xu",
                "Chunwei Wang",
                "Yi Zhu",
                "Yihan Zeng",
                "Kuo Yang",
                "Dingdong Wang",
                "Kun Xiang",
                "Haoyuan Li",
                "Haoli Bai",
                "Jianhua Han",
                "Xiaohui Li",
                "Weike Jin",
                "Nian Xie",
                "Yu Zhang",
                "James T. Kwok",
                "Hengshuang Zhao",
                "Xiaodan Liang",
                "Dit-Yan Yeung",
                "Xiao Chen",
                "Zhenguo Li",
                "Wei Zhang",
                "Qun Liu",
                "Jun Yao",
                "Lanqing Hong",
                "Lu Hou",
                "Hang Xu"
            ],
            "affiliations": [
                "Hong Kong University of Science and Technology",
                "Huawei Noahs Ark Lab",
                "Southern University of Science and Technology",
                "Sun Yat-sen University",
                "The Chinese University of Hong Kong",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2409.18042.jpg",
            "data": {
                "categories": [
                    "#audio",
                    "#cv",
                    "#benchmark",
                    "#alignment",
                    "#open_source",
                    "#architecture",
                    "#synthetic",
                    "#multimodal"
                ],
                "emoji": "ğŸ—£ï¸",
                "ru": {
                    "title": "EMOVA: Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¾Ğ¼Ğ½Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ˜Ğ˜ Ñ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ¼",
                    "desc": "EMOVA - ÑÑ‚Ğ¾ Ğ¾Ğ¼Ğ½Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ñ€ĞµÑ‡Ğ¸. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¾-Ğ°ĞºÑƒÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ñ€ĞµÑ‡Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¸ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹. EMOVA Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ĞºĞ°Ğº Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ·Ñ€ĞµĞ½Ğ¸Ñ-ÑĞ·Ñ‹ĞºĞ°, Ñ‚Ğ°Ğº Ğ¸ Ğ² Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ¼Ğ½Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ½Ñ‹Ğ¹ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¼Ğ¾Ñ†Ğ¸ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "EMOVA: Bridging Speech and Vision for Emotionally Intelligent Conversations",
                    "desc": "The paper introduces EMOVA, a new model designed to enhance Large Language Models (LLMs) by integrating speech capabilities with vision-language performance. EMOVA utilizes a semantic-acoustic disentangled speech tokenizer, which allows for better alignment between visual and auditory data, improving overall model performance. Additionally, it features a lightweight style module that enables control over speech styles, such as emotions and pitches. This approach achieves state-of-the-art results in both vision-language and speech tasks, facilitating more expressive and emotionally aware spoken dialogues."
                },
                "zh": {
                    "title": "æƒ…æ„Ÿå…¨èƒ½è¯­éŸ³åŠ©æ‰‹ï¼šæ‰“ç ´æ¨¡æ€ç•Œé™çš„åˆ›æ–°",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†EMOVAï¼ˆæƒ…æ„Ÿå…¨èƒ½è¯­éŸ³åŠ©æ‰‹ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªèƒ½å¤Ÿå®ç°ç«¯åˆ°ç«¯è¯­éŸ³èƒ½åŠ›çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚EMOVAé€šè¿‡è¯­ä¹‰-å£°å­¦è§£è€¦çš„è¯­éŸ³æ ‡è®°å™¨ï¼Œæå‡äº†è§†è§‰-è¯­è¨€å’Œè¯­éŸ³èƒ½åŠ›çš„å¯¹é½æ•ˆæœã€‚ä¸ç°æœ‰çš„åŒæ¨¡æ€æ¨¡å‹ç›¸æ¯”ï¼ŒEMOVAåœ¨è§†è§‰-è¯­è¨€å’Œè¯­éŸ³åŸºå‡†æµ‹è¯•ä¸­éƒ½è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¯¥æ¨¡å‹è¿˜å¼•å…¥äº†è½»é‡çº§é£æ ¼æ¨¡å—ï¼Œæ”¯æŒçµæ´»çš„è¯­éŸ³é£æ ¼æ§åˆ¶ï¼Œå¦‚æƒ…æ„Ÿå’ŒéŸ³è°ƒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.18125",
            "title": "LLaVA-3D: A Simple yet Effective Pathway to Empowering LMMs with 3D-awareness",
            "url": "https://huggingface.co/papers/2409.18125",
            "abstract": "Recent advancements in Large Multimodal Models (LMMs) have greatly enhanced their proficiency in 2D visual understanding tasks, enabling them to effectively process and understand images and videos. However, the development of LMMs with 3D-awareness for 3D scene understanding has been hindered by the lack of large-scale 3D vision-language datasets and powerful 3D encoders. In this paper, we introduce a simple yet effective framework called LLaVA-3D. Leveraging the strong 2D understanding priors from LLaVA, our LLaVA-3D efficiently adapts LLaVA for 3D scene understanding without compromising 2D understanding capabilities. To achieve this, we employ a simple yet effective representation, 3D Patch, which connects 2D CLIP patch features with their corresponding positions in 3D space. By integrating the 3D Patches into 2D LMMs and employing joint 2D and 3D vision-language instruction tuning, we establish a unified architecture for both 2D image understanding and 3D scene understanding. Experimental results show that LLaVA-3D converges 3.5x faster than existing 3D LMMs when trained on 3D vision-language datasets. Moreover, LLaVA-3D not only achieves state-of-the-art performance across various 3D tasks but also maintains comparable 2D image understanding and vision-language conversation capabilities with LLaVA.",
            "score": 33,
            "issue_id": 1,
            "pub_date": "2024-09-26",
            "pub_date_card": {
                "ru": "26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 26",
                "zh": "9æœˆ26æ—¥"
            },
            "hash": "4ca82aa848fc15ec",
            "authors": [
                "Chenming Zhu",
                "Tai Wang",
                "Wenwei Zhang",
                "Jiangmiao Pang",
                "Xihui Liu"
            ],
            "affiliations": [
                "Shanghai AI Laboratory",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2409.18125.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#cv",
                    "#training",
                    "#graphs",
                    "#optimization",
                    "#transfer_learning",
                    "#architecture",
                    "#multimodal",
                    "#3d"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "LLaVA-3D: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´ Ğ¾Ñ‚ 2D Ğº 3D Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ LLaVA-3D - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ 2D Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ 3D Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑÑ†ĞµĞ½. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ 3D Patch, ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°ÑÑ‰ÑƒÑ 2D Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ CLIP Ñ Ğ¸Ñ… Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ² 3D Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. LLaVA-3D Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… 3D Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² 3D Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞŸÑ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ 2D Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ LLaVA."
                },
                "en": {
                    "title": "Bridging 2D and 3D: LLaVA-3D Unifies Visual Understanding",
                    "desc": "This paper presents LLaVA-3D, a framework designed to enhance Large Multimodal Models (LMMs) for 3D scene understanding while retaining their 2D visual comprehension abilities. The authors address the challenge of limited 3D vision-language datasets and the need for robust 3D encoders by introducing a novel representation called 3D Patch, which links 2D features to their 3D spatial locations. By integrating these 3D Patches into existing 2D LMMs and utilizing joint instruction tuning, LLaVA-3D achieves a unified approach for processing both 2D and 3D data. Experimental results demonstrate that LLaVA-3D trains 3.5 times faster than current 3D LMMs and excels in various 3D tasks while maintaining strong performance in 2D image understanding."
                },
                "zh": {
                    "title": "LLaVA-3Dï¼šç»Ÿä¸€2Dä¸3Dåœºæ™¯ç†è§£çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ¡†æ¶LLaVA-3Dï¼Œæ—¨åœ¨æå‡å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨3Dåœºæ™¯ç†è§£æ–¹é¢çš„èƒ½åŠ›ã€‚é€šè¿‡ç»“åˆ2D CLIPç‰¹å¾ä¸3Dç©ºé—´ä½ç½®ï¼ŒLLaVA-3Dæœ‰æ•ˆåœ°å°†2Dç†è§£èƒ½åŠ›æ‰©å±•åˆ°3Dåœºæ™¯ä¸­ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ç®€å•æœ‰æ•ˆçš„3D Patchè¡¨ç¤ºï¼Œå¹¶é€šè¿‡è”åˆçš„2Då’Œ3Dè§†è§‰è¯­è¨€æŒ‡ä»¤è°ƒä¼˜ï¼Œå»ºç«‹äº†ç»Ÿä¸€çš„æ¶æ„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLLaVA-3Dåœ¨è®­ç»ƒé€Ÿåº¦ä¸Šæ¯”ç°æœ‰çš„3D LMMså¿«3.5å€ï¼Œå¹¶åœ¨å¤šä¸ª3Dä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†ä¸LLaVAç›¸å½“çš„2Då›¾åƒç†è§£èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.18124",
            "title": "Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction",
            "url": "https://huggingface.co/papers/2409.18124",
            "abstract": "Leveraging the visual priors of pre-trained text-to-image diffusion models offers a promising solution to enhance zero-shot generalization in dense prediction tasks. However, existing methods often uncritically use the original diffusion formulation, which may not be optimal due to the fundamental differences between dense prediction and image generation. In this paper, we provide a systemic analysis of the diffusion formulation for the dense prediction, focusing on both quality and efficiency. And we find that the original parameterization type for image generation, which learns to predict noise, is harmful for dense prediction; the multi-step noising/denoising diffusion process is also unnecessary and challenging to optimize. Based on these insights, we introduce Lotus, a diffusion-based visual foundation model with a simple yet effective adaptation protocol for dense prediction. Specifically, Lotus is trained to directly predict annotations instead of noise, thereby avoiding harmful variance. We also reformulate the diffusion process into a single-step procedure, simplifying optimization and significantly boosting inference speed. Additionally, we introduce a novel tuning strategy called detail preserver, which achieves more accurate and fine-grained predictions. Without scaling up the training data or model capacity, Lotus achieves SoTA performance in zero-shot depth and normal estimation across various datasets. It also significantly enhances efficiency, being hundreds of times faster than most existing diffusion-based methods.",
            "score": 31,
            "issue_id": 1,
            "pub_date": "2024-09-26",
            "pub_date_card": {
                "ru": "26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 26",
                "zh": "9æœˆ26æ—¥"
            },
            "hash": "55be564bbee47eed",
            "authors": [
                "Jing He",
                "Haodong Li",
                "Wei Yin",
                "Yixun Liang",
                "Leheng Li",
                "Kaiqiang Zhou",
                "Hongbo Zhang",
                "Bingbing Liu",
                "Ying-Cong Chen"
            ],
            "affiliations": [
                "HKUST",
                "HKUST(GZ)",
                "Noahs Ark Lab",
                "University of Adelaide"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2409.18124.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#cv",
                    "#inference",
                    "#optimization",
                    "#transfer_learning",
                    "#diffusion",
                    "#architecture"
                ],
                "emoji": "ğŸŒ¸",
                "ru": {
                    "title": "Lotus: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Lotus - Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ² ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ¹ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ ĞµĞ³Ğ¾ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ. Lotus Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑˆÑƒĞ¼Ğ° Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ´Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ²Ñ‹Ğ²Ğ¾Ğ´. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ĞµĞ¹ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Lotus: Revolutionizing Dense Prediction with Efficient Diffusion",
                    "desc": "This paper presents Lotus, a new diffusion-based visual foundation model designed to improve zero-shot generalization in dense prediction tasks. The authors analyze the limitations of traditional diffusion methods, which are primarily suited for image generation, and highlight their inefficiencies when applied to dense prediction. By directly predicting annotations instead of noise and reformulating the diffusion process into a single-step procedure, Lotus simplifies optimization and enhances inference speed. The model achieves state-of-the-art performance in depth and normal estimation without requiring additional training data or increased model size."
                },
                "zh": {
                    "title": "Lotusï¼šé«˜æ•ˆçš„å¯†é›†é¢„æµ‹æ‰©æ•£æ¨¡å‹",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹æ¥æé«˜å¯†é›†é¢„æµ‹ä»»åŠ¡çš„é›¶-shotæ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬åˆ†æäº†ç°æœ‰æ‰©æ•£æ¨¡å‹åœ¨å¯†é›†é¢„æµ‹ä¸­çš„ä¸è¶³ï¼Œå‘ç°åŸæœ‰çš„å™ªå£°é¢„æµ‹å‚æ•°åŒ–æ–¹å¼å¯¹å¯†é›†é¢„æµ‹æœ‰å®³ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†Lotusæ¨¡å‹ï¼Œç›´æ¥é¢„æµ‹æ ‡æ³¨è€Œéå™ªå£°ï¼Œå¹¶å°†æ‰©æ•£è¿‡ç¨‹ç®€åŒ–ä¸ºå•æ­¥ç¨‹åºï¼Œä»è€Œæé«˜äº†ä¼˜åŒ–æ•ˆç‡å’Œæ¨ç†é€Ÿåº¦ã€‚Lotusåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„é›¶-shotæ·±åº¦å’Œæ³•çº¿ä¼°è®¡æ€§èƒ½ï¼ŒåŒæ—¶åœ¨æ•ˆç‡ä¸Šä¹Ÿå¤§å¹…æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.14254",
            "title": "Instruction Following without Instruction Tuning",
            "url": "https://huggingface.co/papers/2409.14254",
            "abstract": "Instruction tuning commonly means finetuning a language model on instruction-response pairs. We discover two forms of adaptation (tuning) that are deficient compared to instruction tuning, yet still yield instruction following; we call this implicit instruction tuning. We first find that instruction-response pairs are not necessary: training solely on responses, without any corresponding instructions, yields instruction following. This suggests pretrained models have an instruction-response mapping which is revealed by teaching the model the desired distribution of responses. However, we then find it's not necessary to teach the desired distribution of responses: instruction-response training on narrow-domain data like poetry still leads to broad instruction-following behavior like recipe generation. In particular, when instructions are very different from those in the narrow finetuning domain, models' responses do not adhere to the style of the finetuning domain. To begin to explain implicit instruction tuning, we hypothesize that very simple changes to a language model's distribution yield instruction following. We support this by hand-writing a rule-based language model which yields instruction following in a product-of-experts with a pretrained model. The rules are to slowly increase the probability of ending the sequence, penalize repetition, and uniformly change 15 words' probabilities. In summary, adaptations made without being designed to yield instruction following can do so implicitly.",
            "score": 27,
            "issue_id": 1,
            "pub_date": "2024-09-21",
            "pub_date_card": {
                "ru": "21 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 21",
                "zh": "9æœˆ21æ—¥"
            },
            "hash": "928d018d2936e022",
            "authors": [
                "John Hewitt",
                "Nelson F. Liu",
                "Percy Liang",
                "Christopher D. Manning"
            ],
            "affiliations": [
                "Department of Computer Science, Stanford University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2409.14254.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#interpretability",
                    "#alignment",
                    "#architecture"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¡ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ½ĞµÑĞ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ½Ğ°ÑƒÑ‡Ğ¸Ñ‚ÑŒÑÑ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ±ĞµĞ· ÑĞ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¿Ğ°Ñ€Ğ°Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚. Ğ­Ñ‚Ğ¾Ñ‚ Ñ„ĞµĞ½Ğ¾Ğ¼ĞµĞ½ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½ 'Ğ½ĞµÑĞ²Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¾Ğ¹ Ğ½Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸'. Ğ’Ñ‹ÑÑĞ½Ğ¸Ğ»Ğ¾ÑÑŒ, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ñ… Ğ¸Ğ»Ğ¸ Ğ´Ğ°Ğ¶Ğµ Ğ½Ğ° ÑƒĞ·ĞºĞ¾ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ² Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ñ€Ğ¸Ğ²ĞµÑÑ‚Ğ¸ Ğº ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼."
                },
                "en": {
                    "title": "Unlocking Instruction Following Without Explicit Instructions",
                    "desc": "This paper explores a new concept called implicit instruction tuning, which shows that language models can learn to follow instructions even without explicit instruction-response pairs. The authors demonstrate that training a model solely on responses can still lead to effective instruction following, suggesting that pretrained models already have an inherent understanding of instruction-response mappings. They also find that training on narrow-domain data can produce broad instruction-following behavior, indicating that the model can generalize beyond its training context. The study proposes that simple adjustments to a model's output distribution can facilitate this implicit learning process."
                },
                "zh": {
                    "title": "éšå¼æŒ‡ä»¤è°ƒä¼˜ï¼šæ— éœ€æŒ‡ä»¤ä¹Ÿèƒ½å®ç°æŒ‡ä»¤è·Ÿéš",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†æŒ‡ä»¤è°ƒä¼˜çš„æ¦‚å¿µï¼Œå‘ç°æœ‰ä¸¤ç§é€‚åº”å½¢å¼è™½ç„¶ä¸å¦‚æŒ‡ä»¤è°ƒä¼˜æœ‰æ•ˆï¼Œä½†ä»èƒ½å®ç°æŒ‡ä»¤è·Ÿéšã€‚ç ”ç©¶è¡¨æ˜ï¼Œä»…é€šè¿‡å“åº”è¿›è¡Œè®­ç»ƒï¼Œè€Œä¸éœ€è¦å¯¹åº”çš„æŒ‡ä»¤ï¼Œä¹Ÿèƒ½ä½¿æ¨¡å‹éµå¾ªæŒ‡ä»¤ã€‚è¿™è¡¨æ˜é¢„è®­ç»ƒæ¨¡å‹å†…éƒ¨å­˜åœ¨æŒ‡ä»¤ä¸å“åº”çš„æ˜ å°„å…³ç³»ã€‚æ­¤å¤–ï¼Œä½œè€…æå‡ºç®€å•çš„æ¨¡å‹è°ƒæ•´å¯ä»¥å®ç°æŒ‡ä»¤è·Ÿéšï¼Œç”šè‡³åœ¨ç‹­çª„é¢†åŸŸçš„æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒä¹Ÿèƒ½äº§ç”Ÿå¹¿æ³›çš„æŒ‡ä»¤è·Ÿéšè¡Œä¸ºã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.17422",
            "title": "Discovering the Gems in Early Layers: Accelerating Long-Context LLMs with 1000x Input Token Reduction",
            "url": "https://huggingface.co/papers/2409.17422",
            "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in handling long context inputs, but this comes at the cost of increased computational resources and latency. Our research introduces a novel approach for the long context bottleneck to accelerate LLM inference and reduce GPU memory consumption. Our research demonstrates that LLMs can identify relevant tokens in the early layers before generating answers to a query. Leveraging this insight, we propose an algorithm that uses early layers of an LLM as filters to select and compress input tokens, significantly reducing the context length for subsequent processing. Our method, GemFilter, demonstrates substantial improvements in both speed and memory efficiency compared to existing techniques, such as standard attention and SnapKV/H2O. Notably, it achieves a 2.4times speedup and 30\\% reduction in GPU memory usage compared to SOTA methods. Evaluation on the Needle in a Haystack task shows that GemFilter significantly outperforms standard attention, SnapKV and demonstrates comparable performance on the LongBench challenge. GemFilter is simple, training-free, and broadly applicable across different LLMs. Crucially, it provides interpretability by allowing humans to inspect the selected input sequence. These findings not only offer practical benefits for LLM deployment, but also enhance our understanding of LLM internal mechanisms, paving the way for further optimizations in LLM design and inference. Our code is available at https://github.com/SalesforceAIResearch/GemFilter.",
            "score": 24,
            "issue_id": 1,
            "pub_date": "2024-09-25",
            "pub_date_card": {
                "ru": "25 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 25",
                "zh": "9æœˆ25æ—¥"
            },
            "hash": "830f07f8f88f0a79",
            "authors": [
                "Zhenmei Shi",
                "Yifei Ming",
                "Xuan-Phi Nguyen",
                "Yingyu Liang",
                "Shafiq Joty"
            ],
            "affiliations": [],
            "pdf_title_img": "assets\\pdf\\title_img\\2409.17422.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#training",
                    "#inference",
                    "#interpretability",
                    "#optimization",
                    "#open_source",
                    "#architecture"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "GemFilter: Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ LLM Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ GemFilter Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¸ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ GPU Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. GemFilter Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ½Ğ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ¸ LLM Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¸ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°Ñ Ğ´Ğ»Ğ¸Ğ½Ñƒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ°Ğ¼Ğ¸. GemFilter Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ»ÑĞ´ÑĞ¼ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑÑ‚ÑŒ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½ÑƒÑ Ğ²Ñ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "Accelerating LLMs with Efficient Token Filtering",
                    "desc": "This paper presents GemFilter, a new method designed to improve the efficiency of Large Language Models (LLMs) when processing long context inputs. By utilizing early layers of the LLM to filter and compress input tokens, GemFilter reduces the amount of data that needs to be processed in later layers, leading to faster inference times and lower GPU memory usage. The results show that GemFilter achieves a 2.4 times speedup and a 30% reduction in memory consumption compared to state-of-the-art techniques. Additionally, it provides interpretability by allowing users to examine the selected input tokens, enhancing both practical deployment and understanding of LLMs."
                },
                "zh": {
                    "title": "GemFilterï¼šåŠ é€Ÿå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†ä¸å†…å­˜ä¼˜åŒ–",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡è¾“å…¥æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†è¿™éœ€è¦æ›´å¤šçš„è®¡ç®—èµ„æºå’Œå»¶è¿Ÿã€‚æˆ‘ä»¬çš„ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œæ—¨åœ¨åŠ é€ŸLLMæ¨ç†å¹¶å‡å°‘GPUå†…å­˜æ¶ˆè€—ã€‚æˆ‘ä»¬å‘ç°LLMså¯ä»¥åœ¨ç”Ÿæˆç­”æ¡ˆä¹‹å‰ï¼Œåœ¨æ—©æœŸå±‚è¯†åˆ«ç›¸å…³çš„è¾“å…¥æ ‡è®°ã€‚åŸºäºè¿™ä¸€å‘ç°ï¼Œæˆ‘ä»¬æå‡ºçš„GemFilterç®—æ³•åˆ©ç”¨LLMçš„æ—©æœŸå±‚ä½œä¸ºè¿‡æ»¤å™¨ï¼Œé€‰æ‹©å’Œå‹ç¼©è¾“å…¥æ ‡è®°ï¼Œä»è€Œæ˜¾è‘—å‡å°‘åç»­å¤„ç†çš„ä¸Šä¸‹æ–‡é•¿åº¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.17565",
            "title": "Pixel-Space Post-Training of Latent Diffusion Models",
            "url": "https://huggingface.co/papers/2409.17565",
            "abstract": "Latent diffusion models (LDMs) have made significant advancements in the field of image generation in recent years. One major advantage of LDMs is their ability to operate in a compressed latent space, allowing for more efficient training and deployment. However, despite these advantages, challenges with LDMs still remain. For example, it has been observed that LDMs often generate high-frequency details and complex compositions imperfectly. We hypothesize that one reason for these flaws is due to the fact that all pre- and post-training of LDMs are done in latent space, which is typically 8 times 8 lower spatial-resolution than the output images. To address this issue, we propose adding pixel-space supervision in the post-training process to better preserve high-frequency details. Experimentally, we show that adding a pixel-space objective significantly improves both supervised quality fine-tuning and preference-based post-training by a large margin on a state-of-the-art DiT transformer and U-Net diffusion models in both visual quality and visual flaw metrics, while maintaining the same text alignment quality.",
            "score": 19,
            "issue_id": 1,
            "pub_date": "2024-09-26",
            "pub_date_card": {
                "ru": "26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 26",
                "zh": "9æœˆ26æ—¥"
            },
            "hash": "fa618de81a80ad24",
            "authors": [
                "Christina Zhang",
                "Simran Motwani",
                "Matthew Yu",
                "Ji Hou",
                "Felix Juefei-Xu",
                "Sam Tsai",
                "Peter Vajda",
                "Zijian He",
                "Jialiang Wang"
            ],
            "affiliations": [
                "Meta GenAI, Menlo Park, CA",
                "Princeton University, Princeton, NJ"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2409.17565.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#training",
                    "#optimization",
                    "#diffusion",
                    "#architecture"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ›Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LDM) Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒĞ»Ğ¸ÑÑŒ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ½Ğ¾ Ğ²ÑÑ‘ ĞµÑ‰Ñ‘ Ğ¸Ğ¼ĞµÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑĞ¼Ğ¸ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾ ÑĞ²ÑĞ·Ğ°Ğ½Ğ¾ Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ñ Ğ½Ğ¸Ğ·ĞºĞ¸Ğ¼ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ² Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾ÑÑ‚-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´ĞµÑ„ĞµĞºÑ‚Ñ‹ Ğ² ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…."
                },
                "en": {
                    "title": "Enhancing Image Quality in Latent Diffusion Models with Pixel-Space Supervision",
                    "desc": "Latent diffusion models (LDMs) are advanced techniques for generating images, leveraging a compressed latent space for efficient training. However, they struggle with producing high-frequency details and complex compositions accurately. This paper suggests that the issue arises because LDMs operate in a lower resolution latent space during training. To improve the quality of generated images, the authors propose incorporating pixel-space supervision in the post-training phase, which significantly enhances visual quality without compromising text alignment."
                },
                "zh": {
                    "title": "æå‡å›¾åƒç”Ÿæˆè´¨é‡çš„æ½œåœ¨ç©ºé—´ç›‘ç£",
                    "desc": "æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMsï¼‰åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚LDMsçš„ä¸€ä¸ªä¸»è¦ä¼˜ç‚¹æ˜¯èƒ½å¤Ÿåœ¨å‹ç¼©çš„æ½œåœ¨ç©ºé—´ä¸­æ“ä½œï¼Œä»è€Œå®ç°æ›´é«˜æ•ˆçš„è®­ç»ƒå’Œéƒ¨ç½²ã€‚ç„¶è€Œï¼ŒLDMsä»ç„¶é¢ä¸´ä¸€äº›æŒ‘æˆ˜ï¼Œä¾‹å¦‚ç”Ÿæˆé«˜é¢‘ç»†èŠ‚å’Œå¤æ‚æ„å›¾æ—¶çš„ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºåœ¨åæœŸè®­ç»ƒè¿‡ç¨‹ä¸­å¢åŠ åƒç´ ç©ºé—´ç›‘ç£ï¼Œä»¥æ›´å¥½åœ°ä¿ç•™é«˜é¢‘ç»†èŠ‚ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†è¿™ä¸€æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.14195",
            "title": "The Imperative of Conversation Analysis in the Era of LLMs: A Survey of Tasks, Techniques, and Trends",
            "url": "https://huggingface.co/papers/2409.14195",
            "abstract": "In the era of large language models (LLMs), a vast amount of conversation logs will be accumulated thanks to the rapid development trend of language UI. Conversation Analysis (CA) strives to uncover and analyze critical information from conversation data, streamlining manual processes and supporting business insights and decision-making. The need for CA to extract actionable insights and drive empowerment is becoming increasingly prominent and attracting widespread attention. However, the lack of a clear scope for CA leads to a dispersion of various techniques, making it difficult to form a systematic technical synergy to empower business applications. In this paper, we perform a thorough review and systematize CA task to summarize the existing related work. Specifically, we formally define CA task to confront the fragmented and chaotic landscape in this field, and derive four key steps of CA from conversation scene reconstruction, to in-depth attribution analysis, and then to performing targeted training, finally generating conversations based on the targeted training for achieving the specific goals. In addition, we showcase the relevant benchmarks, discuss potential challenges and point out future directions in both industry and academia. In view of current advancements, it is evident that the majority of efforts are still concentrated on the analysis of shallow conversation elements, which presents a considerable gap between the research and business, and with the assist of LLMs, recent work has shown a trend towards research on causality and strategic tasks which are sophisticated and high-level. The analyzed experiences and insights will inevitably have broader application value in business operations that target conversation logs.",
            "score": 11,
            "issue_id": 1,
            "pub_date": "2024-09-21",
            "pub_date_card": {
                "ru": "21 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 21",
                "zh": "9æœˆ21æ—¥"
            },
            "hash": "fc04ee445bfa493b",
            "authors": [
                "Xinghua Zhang",
                "Haiyang Yu",
                "Yongbin Li",
                "Minzheng Wang",
                "Longze Chen",
                "Fei Huang"
            ],
            "affiliations": [
                "Alibaba Group, China"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2409.14195.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#survey",
                    "#training",
                    "#data",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "ğŸ’¬",
                "ru": {
                    "title": "ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¾Ğ²: Ğ¾Ñ‚ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğº Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¾Ğ² (Conversation Analysis, CA) Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ CA, Ğ²Ñ‹Ğ´ĞµĞ»ÑÑ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑÑ‚Ğ°Ğ¿Ğ°: Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ ÑÑ†ĞµĞ½Ñ‹ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ°, Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ğ¹, Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ñ†ĞµĞ»ĞµĞ¹. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºĞ¸, Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¸ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸. ĞÑ‚Ğ¼ĞµÑ‡Ğ°ĞµÑ‚ÑÑ, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ¾Ñ‡ĞµĞ½Ğ¾ Ğ½Ğ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ°, Ğ½Ğ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµÑ‚ÑÑ Ñ‚ĞµĞ½Ğ´ĞµĞ½Ñ†Ğ¸Ñ Ğº Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ², Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ²ÑĞ·Ğ¸ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸."
                },
                "en": {
                    "title": "Empowering Business Insights through Systematic Conversation Analysis",
                    "desc": "This paper reviews the field of Conversation Analysis (CA) in the context of large language models (LLMs) and their ability to process conversation logs. It defines the CA task systematically, outlining four key steps: reconstructing conversation scenes, conducting in-depth attribution analysis, performing targeted training, and generating conversations for specific goals. The authors highlight the current focus on shallow conversation elements and the need for deeper analysis to bridge the gap between research and practical business applications. They also discuss benchmarks, challenges, and future directions for CA in both industry and academia, emphasizing the potential of LLMs to enhance strategic conversation tasks."
                },
                "zh": {
                    "title": "ç³»ç»ŸåŒ–å¯¹è¯åˆ†æï¼Œé©±åŠ¨å•†ä¸šæ´å¯Ÿ",
                    "desc": "åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ—¶ä»£ï¼Œéšç€è¯­è¨€ç”¨æˆ·ç•Œé¢çš„å¿«é€Ÿå‘å±•ï¼Œç§¯ç´¯äº†å¤§é‡çš„å¯¹è¯æ—¥å¿—ã€‚å¯¹è¯åˆ†æï¼ˆCAï¼‰æ—¨åœ¨ä»å¯¹è¯æ•°æ®ä¸­æå–å’Œåˆ†æå…³é”®ä¿¡æ¯ï¼Œä»¥ç®€åŒ–æ‰‹åŠ¨æµç¨‹å¹¶æ”¯æŒå•†ä¸šæ´å¯Ÿå’Œå†³ç­–ã€‚æœ¬æ–‡å¯¹CAä»»åŠ¡è¿›è¡Œäº†å…¨é¢å›é¡¾å’Œç³»ç»ŸåŒ–ï¼Œæ˜ç¡®äº†CAçš„å®šä¹‰ï¼Œå¹¶æå‡ºäº†ä»å¯¹è¯åœºæ™¯é‡å»ºåˆ°æ·±å…¥å½’å› åˆ†æã€å†åˆ°é’ˆå¯¹æ€§è®­ç»ƒçš„å››ä¸ªå…³é”®æ­¥éª¤ã€‚é€šè¿‡å±•ç¤ºç›¸å…³åŸºå‡†å’Œè®¨è®ºæ½œåœ¨æŒ‘æˆ˜ï¼Œæœ¬æ–‡æŒ‡å‡ºäº†è¡Œä¸šå’Œå­¦æœ¯ç•Œæœªæ¥çš„å‘å±•æ–¹å‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.17280",
            "title": "Disco4D: Disentangled 4D Human Generation and Animation from a Single Image",
            "url": "https://huggingface.co/papers/2409.17280",
            "abstract": "We present Disco4D, a novel Gaussian Splatting framework for 4D human generation and animation from a single image. Different from existing methods, Disco4D distinctively disentangles clothings (with Gaussian models) from the human body (with SMPL-X model), significantly enhancing the generation details and flexibility. It has the following technical innovations. 1) Disco4D learns to efficiently fit the clothing Gaussians over the SMPL-X Gaussians. 2) It adopts diffusion models to enhance the 3D generation process, e.g., modeling occluded parts not visible in the input image. 3) It learns an identity encoding for each clothing Gaussian to facilitate the separation and extraction of clothing assets. Furthermore, Disco4D naturally supports 4D human animation with vivid dynamics. Extensive experiments demonstrate the superiority of Disco4D on 4D human generation and animation tasks. Our visualizations can be found in https://disco-4d.github.io/.",
            "score": 9,
            "issue_id": 1,
            "pub_date": "2024-09-25",
            "pub_date_card": {
                "ru": "25 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 25",
                "zh": "9æœˆ25æ—¥"
            },
            "hash": "b076d30e6256f634",
            "authors": [
                "Hui En Pang",
                "Shuai Liu",
                "Zhongang Cai",
                "Lei Yang",
                "Tianwei Zhang",
                "Ziwei Liu"
            ],
            "affiliations": [
                "S-Lab, Nanyang Technological University",
                "SenseTime Research",
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2409.17280.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#games",
                    "#diffusion",
                    "#architecture",
                    "#3d"
                ],
                "emoji": "ğŸ‘•",
                "ru": {
                    "title": "Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ñ 3D-Ğ»ÑĞ´ĞµĞ¹ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ñ„Ğ¾Ñ‚Ğ¾",
                    "desc": "Disco4D - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ»ÑĞ´ĞµĞ¹ Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğµ Gaussian Splatting. ĞĞ½Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¾Ğ´ĞµĞ¶Ğ´Ñƒ Ğ¾Ñ‚ Ñ‚ĞµĞ»Ğ° Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ SMPL-X Ğ´Ğ»Ñ Ñ‚ĞµĞ»Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° 3D-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ° Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹. Disco4D Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½ÑƒÑ 4D-Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ»ÑĞ´ĞµĞ¹ Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing 4D Human Generation with Disco4D",
                    "desc": "Disco4D is a new framework that uses Gaussian Splatting to create and animate 4D human figures from just one image. It separates clothing from the human body using Gaussian models and the SMPL-X model, which improves detail and flexibility in the generated images. The framework incorporates diffusion models to better generate 3D representations, even for parts of the body that are not visible in the original image. Additionally, it includes a unique identity encoding for clothing, allowing for easier management of clothing assets and enabling dynamic 4D animations."
                },
                "zh": {
                    "title": "Disco4Dï¼šä»å•å›¾åƒç”ŸæˆåŠ¨æ€4Däººç±»æ¨¡å‹",
                    "desc": "Disco4Dæ˜¯ä¸€ç§æ–°é¢–çš„é«˜æ–¯ç‚¹äº‘æ¡†æ¶ï¼Œç”¨äºä»å•å¼ å›¾åƒç”Ÿæˆå’ŒåŠ¨ç”»åŒ–4Däººç±»æ¨¡å‹ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒDisco4Då°†æœè£…ï¼ˆä½¿ç”¨é«˜æ–¯æ¨¡å‹ï¼‰ä¸äººä½“ï¼ˆä½¿ç”¨SMPL-Xæ¨¡å‹ï¼‰æœ‰æ•ˆåˆ†ç¦»ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†ç”Ÿæˆçš„ç»†èŠ‚å’Œçµæ´»æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡é«˜æ•ˆæ‹Ÿåˆæœè£…é«˜æ–¯æ¨¡å‹å’ŒSMPL-Xé«˜æ–¯æ¨¡å‹ï¼Œé‡‡ç”¨æ‰©æ•£æ¨¡å‹å¢å¼º3Dç”Ÿæˆè¿‡ç¨‹ï¼Œå¹¶ä¸ºæ¯ä¸ªæœè£…é«˜æ–¯å­¦ä¹ èº«ä»½ç¼–ç ï¼Œä»¥ä¾¿äºåˆ†ç¦»å’Œæå–æœè£…èµ„äº§ã€‚æ­¤å¤–ï¼ŒDisco4Dè‡ªç„¶æ”¯æŒç”ŸåŠ¨çš„4Däººç±»åŠ¨ç”»ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.14683",
            "title": "Reducing the Footprint of Multi-Vector Retrieval with Minimal Performance Impact via Token Pooling",
            "url": "https://huggingface.co/papers/2409.14683",
            "abstract": "Over the last few years, multi-vector retrieval methods, spearheaded by ColBERT, have become an increasingly popular approach to Neural IR. By storing representations at the token level rather than at the document level, these methods have demonstrated very strong retrieval performance, especially in out-of-domain settings. However, the storage and memory requirements necessary to store the large number of associated vectors remain an important drawback, hindering practical adoption. In this paper, we introduce a simple clustering-based token pooling approach to aggressively reduce the number of vectors that need to be stored. This method can reduce the space & memory footprint of ColBERT indexes by 50% with virtually no retrieval performance degradation. This method also allows for further reductions, reducing the vector count by 66%-to-75% , with degradation remaining below 5% on a vast majority of datasets. Importantly, this approach requires no architectural change nor query-time processing, and can be used as a simple drop-in during indexation with any ColBERT-like model.",
            "score": 8,
            "issue_id": 1,
            "pub_date": "2024-09-23",
            "pub_date_card": {
                "ru": "23 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 23",
                "zh": "9æœˆ23æ—¥"
            },
            "hash": "d7dda0c648e6ab9d",
            "authors": [
                "Benjamin ClaviÃ©",
                "Antoine Chaffin",
                "Griffin Adams"
            ],
            "affiliations": [
                "Answer.AI Japan",
                "Answer.AI USA",
                "LightOn France"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2409.14683.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#inference",
                    "#graphs",
                    "#optimization",
                    "#data",
                    "#benchmark"
                ],
                "emoji": "ğŸ—œï¸",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ¸Ğ½Ğ´ĞµĞºÑĞ¾Ğ² ColBERT Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ğ¾Ğ¸ÑĞºĞ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞ¼Ğ° Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ¼Ñ‹Ñ… Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ñ… Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº ColBERT. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞ¼ Ğ¸Ğ½Ğ´ĞµĞºÑĞ¾Ğ² ColBERT Ğ½Ğ° 50% Ğ±ĞµĞ· ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ”Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞµ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ 66-75% Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ½ĞµĞµ Ñ‡ĞµĞ¼ Ğ½Ğ° 5% Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ’Ğ°Ğ¶Ğ½Ğ¾ Ğ¾Ñ‚Ğ¼ĞµÑ‚Ğ¸Ñ‚ÑŒ, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ»ĞµĞ³ĞºĞ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Efficient Token Storage for Enhanced Retrieval Performance",
                    "desc": "This paper presents a new method to improve multi-vector retrieval systems, particularly those based on ColBERT. The authors propose a clustering-based token pooling technique that significantly reduces the number of token-level vectors stored, addressing the high storage and memory demands of existing methods. Their approach can cut the storage requirements by 50% without losing retrieval accuracy, and even achieve reductions of 66% to 75% with minimal performance degradation. Importantly, this method is easy to implement, requiring no changes to the existing architecture or query processing, making it a practical enhancement for ColBERT-like models."
                },
                "zh": {
                    "title": "èšç±»æ± åŒ–ï¼šé«˜æ•ˆå­˜å‚¨ä¸æ£€ç´¢çš„å®Œç¾ç»“åˆ",
                    "desc": "è¿‘å¹´æ¥ï¼Œå¤šå‘é‡æ£€ç´¢æ–¹æ³•åœ¨ç¥ç»ä¿¡æ¯æ£€ç´¢ä¸­è¶Šæ¥è¶Šå—æ¬¢è¿ï¼Œå°¤å…¶æ˜¯ColBERTæ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨æ ‡è®°çº§åˆ«å­˜å‚¨è¡¨ç¤ºï¼Œè€Œä¸æ˜¯åœ¨æ–‡æ¡£çº§åˆ«ï¼Œå±•ç¤ºäº†å¼ºå¤§çš„æ£€ç´¢æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨åŸŸå¤–è®¾ç½®ä¸­ã€‚ç„¶è€Œï¼Œå­˜å‚¨å¤§é‡ç›¸å…³å‘é‡æ‰€éœ€çš„å­˜å‚¨å’Œå†…å­˜è¦æ±‚ä»ç„¶æ˜¯ä¸€ä¸ªé‡è¦ç¼ºç‚¹ï¼Œé™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºèšç±»çš„æ ‡è®°æ± åŒ–æ–¹æ³•ï¼Œå¯ä»¥å¤§å¹…å‡å°‘éœ€è¦å­˜å‚¨çš„å‘é‡æ•°é‡ï¼Œä¸”å‡ ä¹ä¸å½±å“æ£€ç´¢æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.17580",
            "title": "Enhancing Structured-Data Retrieval with GraphRAG: Soccer Data Case Study",
            "url": "https://huggingface.co/papers/2409.17580",
            "abstract": "Extracting meaningful insights from large and complex datasets poses significant challenges, particularly in ensuring the accuracy and relevance of retrieved information. Traditional data retrieval methods such as sequential search and index-based retrieval often fail when handling intricate and interconnected data structures, resulting in incomplete or misleading outputs. To overcome these limitations, we introduce Structured-GraphRAG, a versatile framework designed to enhance information retrieval across structured datasets in natural language queries. Structured-GraphRAG utilizes multiple knowledge graphs, which represent data in a structured format and capture complex relationships between entities, enabling a more nuanced and comprehensive retrieval of information. This graph-based approach reduces the risk of errors in language model outputs by grounding responses in a structured format, thereby enhancing the reliability of results. We demonstrate the effectiveness of Structured-GraphRAG by comparing its performance with that of a recently published method using traditional retrieval-augmented generation. Our findings show that Structured-GraphRAG significantly improves query processing efficiency and reduces response times. While our case study focuses on soccer data, the framework's design is broadly applicable, offering a powerful tool for data analysis and enhancing language model applications across various structured domains.",
            "score": 7,
            "issue_id": 1,
            "pub_date": "2024-09-26",
            "pub_date_card": {
                "ru": "26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 26",
                "zh": "9æœˆ26æ—¥"
            },
            "hash": "c7496beca8061db3",
            "authors": [
                "Zahra Sepasdar",
                "Sushant Gautam",
                "Cise Midoglu",
                "Michael A. Riegler",
                "PÃ¥l Halvorsen"
            ],
            "affiliations": [
                "Forzasys",
                "OsloMet",
                "SimulaMet"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2409.17580.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#graphs",
                    "#rag",
                    "#data",
                    "#interpretability",
                    "#architecture"
                ],
                "emoji": "ğŸ•¸ï¸",
                "ru": {
                    "title": "Ğ“Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Structured-GraphRAG - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ². ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ³Ñ€Ğ°Ñ„Ñ‹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Structured-GraphRAG Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ¿Ğ¸Ñ€Ğ°ÑÑÑŒ Ğ½Ğ° ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Data Retrieval with Structured-GraphRAG",
                    "desc": "This paper presents Structured-GraphRAG, a new framework aimed at improving information retrieval from complex datasets using natural language queries. It addresses the shortcomings of traditional methods like sequential search by leveraging multiple knowledge graphs, which organize data and highlight relationships between entities. By grounding language model outputs in structured data, Structured-GraphRAG enhances the accuracy and relevance of the retrieved information. The framework has been shown to significantly boost query processing efficiency and is applicable to various domains beyond the soccer data case study."
                },
                "zh": {
                    "title": "æå‡ç»“æ„åŒ–æ•°æ®æ£€ç´¢çš„æ•ˆç‡ä¸å‡†ç¡®æ€§",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºStructured-GraphRAGçš„ä¿¡æ¯æ£€ç´¢æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¯¹ç»“æ„åŒ–æ•°æ®é›†çš„æ£€ç´¢æ•ˆç‡ã€‚ä¼ ç»Ÿçš„æ•°æ®æ£€ç´¢æ–¹æ³•åœ¨å¤„ç†å¤æ‚æ•°æ®æ—¶å¸¸å¸¸æ— æ³•æä¾›å‡†ç¡®çš„ä¿¡æ¯ï¼Œå¯¼è‡´ç»“æœä¸å®Œæ•´æˆ–è¯¯å¯¼ã€‚Structured-GraphRAGåˆ©ç”¨å¤šä¸ªçŸ¥è¯†å›¾è°±ï¼Œä»¥ç»“æ„åŒ–çš„æ–¹å¼è¡¨ç¤ºæ•°æ®ï¼Œæ•æ‰å®ä½“ä¹‹é—´çš„å¤æ‚å…³ç³»ï¼Œä»è€Œå®ç°æ›´å…¨é¢çš„ä¿¡æ¯æ£€ç´¢ã€‚é€šè¿‡ä¸ä¼ ç»Ÿçš„æ£€ç´¢å¢å¼ºç”Ÿæˆæ–¹æ³•è¿›è¡Œæ¯”è¾ƒï¼Œæˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼ŒStructured-GraphRAGåœ¨æŸ¥è¯¢å¤„ç†æ•ˆç‡å’Œå“åº”æ—¶é—´ä¸Šéƒ½æœ‰æ˜¾è‘—æ”¹å–„ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.18121",
            "title": "Robot See Robot Do: Imitating Articulated Object Manipulation with Monocular 4D Reconstruction",
            "url": "https://huggingface.co/papers/2409.18121",
            "abstract": "Humans can learn to manipulate new objects by simply watching others; providing robots with the ability to learn from such demonstrations would enable a natural interface specifying new behaviors. This work develops Robot See Robot Do (RSRD), a method for imitating articulated object manipulation from a single monocular RGB human demonstration given a single static multi-view object scan. We first propose 4D Differentiable Part Models (4D-DPM), a method for recovering 3D part motion from a monocular video with differentiable rendering. This analysis-by-synthesis approach uses part-centric feature fields in an iterative optimization which enables the use of geometric regularizers to recover 3D motions from only a single video. Given this 4D reconstruction, the robot replicates object trajectories by planning bimanual arm motions that induce the demonstrated object part motion. By representing demonstrations as part-centric trajectories, RSRD focuses on replicating the demonstration's intended behavior while considering the robot's own morphological limits, rather than attempting to reproduce the hand's motion. We evaluate 4D-DPM's 3D tracking accuracy on ground truth annotated 3D part trajectories and RSRD's physical execution performance on 9 objects across 10 trials each on a bimanual YuMi robot. Each phase of RSRD achieves an average of 87% success rate, for a total end-to-end success rate of 60% across 90 trials. Notably, this is accomplished using only feature fields distilled from large pretrained vision models -- without any task-specific training, fine-tuning, dataset collection, or annotation. Project page: https://robot-see-robot-do.github.io",
            "score": 7,
            "issue_id": 1,
            "pub_date": "2024-09-26",
            "pub_date_card": {
                "ru": "26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 26",
                "zh": "9æœˆ26æ—¥"
            },
            "hash": "1397b774b882bc6c",
            "authors": [
                "Justin Kerr",
                "Chung Min Kim",
                "Mingxuan Wu",
                "Brent Yi",
                "Qianqian Wang",
                "Ken Goldberg",
                "Angjoo Kanazawa"
            ],
            "affiliations": [
                "UC Berkeley"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2409.18121.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#optimization",
                    "#games",
                    "#open_source",
                    "#architecture",
                    "#robotics",
                    "#3d"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ Ğ¾Ğ±Ğ¾Ñ‚Ñ‹ ÑƒÑ‡Ğ°Ñ‚ÑÑ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸, Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°Ñ Ğ·Ğ° Ğ»ÑĞ´ÑŒĞ¼Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Robot See Robot Do (RSRD) Ğ´Ğ»Ñ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹ Ñ ÑˆĞ°Ñ€Ğ½Ğ¸Ñ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ğ·Ğ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ 4D Differentiable Part Models (4D-DPM) Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ‡Ğ°ÑÑ‚ĞµĞ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ğ¸Ğ· Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ°. RSRD Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½ÑƒÑ 4D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°, Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´ÑÑ‰Ğ¸Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ñ‡Ğ°ÑÑ‚ĞµĞ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 60% ÑƒÑĞ¿ĞµÑ…Ğ° Ğ² Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ±ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Learning by Watching: Robots Imitate Human Object Manipulation",
                    "desc": "This paper introduces Robot See Robot Do (RSRD), a method that allows robots to learn how to manipulate objects by observing human demonstrations. It utilizes 4D Differentiable Part Models (4D-DPM) to extract 3D motion information from a single monocular video, enabling the robot to understand and replicate the intended object movements. The approach focuses on part-centric trajectories, allowing the robot to plan its arm motions based on the demonstrated behavior while respecting its own physical capabilities. The method shows promising results, achieving an average success rate of 87% in tracking and 60% in execution across multiple trials without requiring specific training or data collection."
                },
                "zh": {
                    "title": "è®©æœºå™¨äººé€šè¿‡è§‚å¯Ÿå­¦ä¹ æ–°æŠ€èƒ½",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºæœºå™¨äººçœ‹æœºå™¨äººåšï¼ˆRSRDï¼‰çš„æ–¹æ³•ï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿé€šè¿‡è§‚å¯Ÿäººç±»çš„å•ä¸€æ¼”ç¤ºæ¥å­¦ä¹ æ“æ§ç‰©ä½“ã€‚æˆ‘ä»¬é¦–å…ˆå¼•å…¥äº†4Då¯å¾®åˆ†éƒ¨ä»¶æ¨¡å‹ï¼ˆ4D-DPMï¼‰ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿä»å•ç›®è§†é¢‘ä¸­æ¢å¤3Déƒ¨ä»¶è¿åŠ¨ã€‚RSRDé€šè¿‡è§„åˆ’åŒæ‰‹è‡‚è¿åŠ¨æ¥å¤åˆ¶ç‰©ä½“è½¨è¿¹ï¼Œä¸“æ³¨äºå†ç°æ¼”ç¤ºçš„æ„å›¾è¡Œä¸ºï¼Œè€Œä¸æ˜¯ç®€å•æ¨¡ä»¿æ‰‹çš„åŠ¨ä½œã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRSRDåœ¨å¤šä¸ªç‰©ä½“ä¸Šçš„æˆåŠŸç‡è¾¾åˆ°87%ï¼Œå¹¶ä¸”åœ¨æ²¡æœ‰ç‰¹å®šä»»åŠ¡è®­ç»ƒçš„æƒ…å†µä¸‹å®ç°äº†60%çš„æ•´ä½“æˆåŠŸç‡ã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-09-26.html",
    "link_next": "2024-09-30.html",
    "link_month": "2024-09.html",
    "short_date_prev": {
        "ru": "26.09",
        "en": "09/26",
        "zh": "9æœˆ26æ—¥"
    },
    "short_date_next": {
        "ru": "30.09",
        "en": "09/30",
        "zh": "9æœˆ30æ—¥"
    },
    "categories": {
        "#dataset": 3,
        "#data": 3,
        "#benchmark": 3,
        "#agents": 0,
        "#cv": 6,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 2,
        "#plp": 0,
        "#inference": 4,
        "#3d": 3,
        "#audio": 1,
        "#video": 0,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 10,
        "#healthcare": 0,
        "#training": 6,
        "#robotics": 1,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 3,
        "#reasoning": 2,
        "#transfer_learning": 3,
        "#graphs": 3,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 7,
        "#survey": 1,
        "#diffusion": 3,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    }
}