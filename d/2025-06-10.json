{
    "date": {
        "ru": "10 Ğ¸ÑĞ½Ñ",
        "en": "June 10",
        "zh": "6æœˆ10æ—¥"
    },
    "time_utc": "2025-06-10 11:10",
    "weekday": 1,
    "issue_id": 4217,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2506.08007",
            "title": "Reinforcement Pre-Training",
            "url": "https://huggingface.co/papers/2506.08007",
            "abstract": "Reinforcement Pre-Training (RPT) enhances language model pre-training by framing next-token prediction as a reinforcement learning task, improving accuracy and providing a strong foundation for further fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t In this work, we introduce Reinforcement Pre-Training (RPT) as a new scaling paradigm for large language models and reinforcement learning (RL). Specifically, we reframe next-token prediction as a reasoning task trained using RL, where it receives verifiable rewards for correctly predicting the next token for a given context. RPT offers a scalable method to leverage vast amounts of text data for general-purpose RL, rather than relying on domain-specific annotated answers. By incentivizing the capability of next-token reasoning, RPT significantly improves the language modeling accuracy of predicting the next tokens. Moreover, RPT provides a strong pre-trained foundation for further reinforcement fine-tuning. The scaling curves show that increased training compute consistently improves the next-token prediction accuracy. The results position RPT as an effective and promising scaling paradigm to advance language model pre-training.",
            "score": 113,
            "issue_id": 4208,
            "pub_date": "2025-06-09",
            "pub_date_card": {
                "ru": "9 Ğ¸ÑĞ½Ñ",
                "en": "June 9",
                "zh": "6æœˆ9æ—¥"
            },
            "hash": "a1ef74d27d1d9d50",
            "authors": [
                "Qingxiu Dong",
                "Li Dong",
                "Yao Tang",
                "Tianzhu Ye",
                "Yutao Sun",
                "Zhifang Sui",
                "Furu Wei"
            ],
            "affiliations": [
                "Microsoft Research",
                "Peking University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.08007.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#rlhf",
                    "#reasoning",
                    "#rl"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ñ‹ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Reinforcement Pre-Training (RPT). RPT Ğ¿ĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° ĞºĞ°Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ³Ğ´Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ·Ğ° Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ³Ñ€Ğ¾Ğ¼Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞ¼Ñ‹ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ RPT Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ‡Ğ½ÑƒÑ Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞ¹ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸."
                },
                "en": {
                    "title": "Reinforcement Learning Boosts Language Model Training",
                    "desc": "Reinforcement Pre-Training (RPT) is a novel approach that enhances the pre-training of language models by treating next-token prediction as a reinforcement learning (RL) task. This method allows the model to receive rewards for accurately predicting the next token based on the context, which improves its reasoning capabilities. RPT utilizes large amounts of unannotated text data, making it scalable and effective for general-purpose RL applications. The results demonstrate that RPT not only boosts prediction accuracy but also lays a solid groundwork for subsequent fine-tuning in reinforcement learning tasks."
                },
                "zh": {
                    "title": "å¼ºåŒ–é¢„è®­ç»ƒï¼šæå‡è¯­è¨€æ¨¡å‹çš„ä¸‹ä¸€æ­¥é¢„æµ‹èƒ½åŠ›",
                    "desc": "å¼ºåŒ–é¢„è®­ç»ƒï¼ˆRPTï¼‰é€šè¿‡å°†ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹è§†ä¸ºå¼ºåŒ–å­¦ä¹ ä»»åŠ¡ï¼Œå¢å¼ºäº†è¯­è¨€æ¨¡å‹çš„é¢„è®­ç»ƒã€‚è¿™ç§æ–¹æ³•é€šè¿‡ä¸ºæ­£ç¡®é¢„æµ‹ç»™å®šä¸Šä¸‹æ–‡çš„ä¸‹ä¸€ä¸ªæ ‡è®°æä¾›å¯éªŒè¯çš„å¥–åŠ±ï¼Œä»è€Œæé«˜äº†å‡†ç¡®æ€§ã€‚RPTåˆ©ç”¨å¤§é‡æ–‡æœ¬æ•°æ®è¿›è¡Œé€šç”¨å¼ºåŒ–å­¦ä¹ ï¼Œè€Œä¸ä¾èµ–äºç‰¹å®šé¢†åŸŸçš„æ ‡æ³¨ç­”æ¡ˆã€‚é€šè¿‡æ¿€åŠ±ä¸‹ä¸€ä¸ªæ ‡è®°æ¨ç†çš„èƒ½åŠ›ï¼ŒRPTæ˜¾è‘—æé«˜äº†è¯­è¨€å»ºæ¨¡çš„å‡†ç¡®æ€§ï¼Œå¹¶ä¸ºè¿›ä¸€æ­¥çš„å¼ºåŒ–å¾®è°ƒæä¾›äº†åšå®çš„åŸºç¡€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.07044",
            "title": "Lingshu: A Generalist Foundation Model for Unified Multimodal Medical\n  Understanding and Reasoning",
            "url": "https://huggingface.co/papers/2506.07044",
            "abstract": "Lingshu, a medical-specialized MLLM, enhances performance in medical tasks through comprehensive data curation, multi-stage training, and reinforcement learning with verifiable rewards, outperforming existing open-source models.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities in understanding common visual elements, largely due to their large-scale datasets and advanced training strategies. However, their effectiveness in medical applications remains limited due to the inherent discrepancies between data and tasks in medical scenarios and those in the general domain. Concretely, existing medical MLLMs face the following critical limitations: (1) limited coverage of medical knowledge beyond imaging, (2) heightened susceptibility to hallucinations due to suboptimal data curation processes, (3) lack of reasoning capabilities tailored for complex medical scenarios. To address these challenges, we first propose a comprehensive data curation procedure that (1) efficiently acquires rich medical knowledge data not only from medical imaging but also from extensive medical texts and general-domain data; and (2) synthesizes accurate medical captions, visual question answering (VQA), and reasoning samples. As a result, we build a multimodal dataset enriched with extensive medical knowledge. Building on the curated data, we introduce our medical-specialized MLLM: Lingshu. Lingshu undergoes multi-stage training to embed medical expertise and enhance its task-solving capabilities progressively. Besides, we preliminarily explore the potential of applying reinforcement learning with verifiable rewards paradigm to enhance Lingshu's medical reasoning ability. Additionally, we develop MedEvalKit, a unified evaluation framework that consolidates leading multimodal and textual medical benchmarks for standardized, fair, and efficient model assessment. We evaluate the performance of Lingshu on three fundamental medical tasks, multimodal QA, text-based QA, and medical report generation. The results show that Lingshu consistently outperforms the existing open-source multimodal models on most tasks ...",
            "score": 50,
            "issue_id": 4208,
            "pub_date": "2025-06-08",
            "pub_date_card": {
                "ru": "8 Ğ¸ÑĞ½Ñ",
                "en": "June 8",
                "zh": "6æœˆ8æ—¥"
            },
            "hash": "475506f0d0cabb39",
            "authors": [
                "LASA Team",
                "Weiwen Xu",
                "Hou Pong Chan",
                "Long Li",
                "Mahani Aljunied",
                "Ruifeng Yuan",
                "Jianyu Wang",
                "Chenghao Xiao",
                "Guizhen Chen",
                "Chaoqun Liu",
                "Zhaodonghui Li",
                "Yu Sun",
                "Junao Shen",
                "Chaojun Wang",
                "Jie Tan",
                "Deli Zhao",
                "Tingyang Xu",
                "Hao Zhang",
                "Yu Rong"
            ],
            "affiliations": [
                "DAMO Academy, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.07044.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#dataset",
                    "#training",
                    "#hallucinations",
                    "#benchmark",
                    "#medical",
                    "#rl",
                    "#reasoning",
                    "#data",
                    "#healthcare",
                    "#multimodal"
                ],
                "emoji": "ğŸ©º",
                "ru": {
                    "title": "Lingshu: Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½Ñ‹",
                    "desc": "Lingshu - ÑÑ‚Ğ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½Ñ‹, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ĞºÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ñ‚ĞµĞºÑÑ‚Ñ‹ Ğ¸ Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. Lingshu Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Lingshu: Revolutionizing Medical AI with Enhanced Learning and Reasoning",
                    "desc": "Lingshu is a specialized Multimodal Large Language Model (MLLM) designed to improve medical task performance through enhanced data curation and training techniques. It addresses key limitations of existing medical models, such as insufficient medical knowledge coverage and a tendency to produce hallucinations. By utilizing a comprehensive dataset that includes both medical texts and images, Lingshu is trained in multiple stages to develop strong reasoning capabilities for complex medical scenarios. The model is evaluated using the MedEvalKit framework, demonstrating superior performance in tasks like multimodal question answering and medical report generation compared to other open-source models."
                },
                "zh": {
                    "title": "Lingshuï¼šåŒ»ç–—ä»»åŠ¡çš„æ™ºèƒ½åŠ©æ‰‹",
                    "desc": "Lingshuæ˜¯ä¸€ç§ä¸“é—¨é’ˆå¯¹åŒ»ç–—ä»»åŠ¡çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œé€šè¿‡å…¨é¢çš„æ•°æ®æ•´ç†ã€å¤šé˜¶æ®µè®­ç»ƒå’Œå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ æ¥æå‡å…¶æ€§èƒ½ã€‚è¯¥æ¨¡å‹è§£å†³äº†ç°æœ‰åŒ»ç–—MLLMåœ¨çŸ¥è¯†è¦†ç›–ã€æ•°æ®æ•´ç†å’Œæ¨ç†èƒ½åŠ›æ–¹é¢çš„ä¸è¶³ã€‚é€šè¿‡é«˜æ•ˆè·å–ä¸°å¯Œçš„åŒ»ç–—çŸ¥è¯†æ•°æ®ï¼ŒLingshuèƒ½å¤Ÿåœ¨å¤šç§åŒ»ç–—ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚æœ€ç»ˆï¼ŒLingshuåœ¨å¤šæ¨¡æ€é—®ç­”ã€åŸºäºæ–‡æœ¬çš„é—®ç­”å’ŒåŒ»ç–—æŠ¥å‘Šç”Ÿæˆç­‰ä»»åŠ¡ä¸­è¶…è¶Šäº†ç°æœ‰çš„å¼€æºæ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.07900",
            "title": "MiniCPM4: Ultra-Efficient LLMs on End Devices",
            "url": "https://huggingface.co/papers/2506.07900",
            "abstract": "MiniCPM4, a large language model optimized for end-side devices, achieves efficiency and effectiveness through innovations in model architecture, training data, algorithms, and inference systems.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper introduces MiniCPM4, a highly efficient large language model (LLM) designed explicitly for end-side devices. We achieve this efficiency through systematic innovation in four key dimensions: model architecture, training data, training algorithms, and inference systems. Specifically, in terms of model architecture, we propose InfLLM v2, a trainable sparse attention mechanism that accelerates both prefilling and decoding phases for long-context processing. Regarding training data, we propose UltraClean, an efficient and accurate pre-training data filtering and generation strategy, and UltraChat v2, a comprehensive supervised fine-tuning dataset. These datasets enable satisfactory model performance to be achieved using just 8 trillion training tokens. Regarding training algorithms, we propose ModelTunnel v2 for efficient pre-training strategy search, and improve existing post-training methods by introducing chunk-wise rollout for load-balanced reinforcement learning and data-efficient tenary LLM, BitCPM. Regarding inference systems, we propose CPM.cu that integrates sparse attention, model quantization, and speculative sampling to achieve efficient prefilling and decoding. To meet diverse on-device requirements, MiniCPM4 is available in two versions, with 0.5B and 8B parameters, respectively. Sufficient evaluation results show that MiniCPM4 outperforms open-source models of similar size across multiple benchmarks, highlighting both its efficiency and effectiveness. Notably, MiniCPM4-8B demonstrates significant speed improvements over Qwen3-8B when processing long sequences. Through further adaptation, MiniCPM4 successfully powers diverse applications, including trustworthy survey generation and tool use with model context protocol, clearly showcasing its broad usability.",
            "score": 45,
            "issue_id": 4208,
            "pub_date": "2025-06-09",
            "pub_date_card": {
                "ru": "9 Ğ¸ÑĞ½Ñ",
                "en": "June 9",
                "zh": "6æœˆ9æ—¥"
            },
            "hash": "edbb02209bdb677b",
            "authors": [
                "MiniCPM Team",
                "Chaojun Xiao",
                "Yuxuan Li",
                "Xu Han",
                "Yuzhuo Bai",
                "Jie Cai",
                "Haotian Chen",
                "Wentong Chen",
                "Xin Cong",
                "Ganqu Cui",
                "Ning Ding",
                "Shengdan Fan",
                "Yewei Fang",
                "Zixuan Fu",
                "Wenyu Guan",
                "Yitong Guan",
                "Junshao Guo",
                "Yufeng Han",
                "Bingxiang He",
                "Yuxiang Huang",
                "Cunliang Kong",
                "Qiuzuo Li",
                "Siyuan Li",
                "Wenhao Li",
                "Yanghao Li",
                "Yishan Li",
                "Zhen Li",
                "Dan Liu",
                "Biyuan Lin",
                "Yankai Lin",
                "Xiang Long",
                "Quanyu Lu",
                "Yaxi Lu",
                "Peiyan Luo",
                "Hongya Lyu",
                "Litu Ou",
                "Yinxu Pan",
                "Zekai Qu",
                "Qundong Shi",
                "Zijun Song",
                "Jiayuan Su",
                "Zhou Su",
                "Ao Sun",
                "Xianghui Sun",
                "Peijun Tang",
                "Fangzheng Wang",
                "Feng Wang",
                "Shuo Wang",
                "Yudong Wang",
                "Yesai Wu",
                "Zhenyu Xiao",
                "Jie Xie",
                "Zihao Xie",
                "Yukun Yan",
                "Jiarui Yuan",
                "Kaihuo Zhang",
                "Lei Zhang",
                "Linyue Zhang",
                "Xueren Zhang",
                "Yudi Zhang",
                "Hengyu Zhao",
                "Weilin Zhao",
                "Weilun Zhao",
                "Yuanqian Zhao",
                "Zhi Zheng",
                "Ge Zhou",
                "Jie Zhou",
                "Wei Zhou",
                "Zihan Zhou",
                "Zixuan Zhou",
                "Zhiyuan Liu",
                "Guoyang Zeng",
                "Chao Jia",
                "Dahai Li",
                "Maosong Sun"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2506.07900.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#dataset",
                    "#long_context",
                    "#training",
                    "#architecture",
                    "#inference",
                    "#open_source",
                    "#data",
                    "#games"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "MiniCPM4: ĞœĞ¾Ñ‰ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸",
                    "desc": "MiniCPM4 - ÑÑ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (LLM), Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸ÑĞ¼ Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ, Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ°Ñ… Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ InfLLM v2, ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… UltraClean Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… UltraChat v2 Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. MiniCPM4 Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¿Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ğ¼ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹."
                },
                "en": {
                    "title": "MiniCPM4: Efficiency Meets Performance for On-Device AI",
                    "desc": "MiniCPM4 is a large language model specifically designed for efficient use on end-side devices. It incorporates innovations in model architecture, such as a new sparse attention mechanism, and utilizes advanced training data strategies to enhance performance. The model employs unique training algorithms for effective pre-training and post-training, ensuring it can handle diverse tasks with minimal resources. Evaluation results indicate that MiniCPM4 outperforms similar-sized models, demonstrating its speed and effectiveness in real-world applications."
                },
                "zh": {
                    "title": "MiniCPM4ï¼šç»ˆç«¯è®¾å¤‡çš„é«˜æ•ˆè¯­è¨€æ¨¡å‹",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†MiniCPM4ï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸ºç»ˆç«¯è®¾å¤‡ä¼˜åŒ–çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚é€šè¿‡åœ¨æ¨¡å‹æ¶æ„ã€è®­ç»ƒæ•°æ®ã€è®­ç»ƒç®—æ³•å’Œæ¨ç†ç³»ç»Ÿå››ä¸ªå…³é”®æ–¹é¢çš„åˆ›æ–°ï¼ŒMiniCPM4å®ç°äº†é«˜æ•ˆæ€§å’Œæœ‰æ•ˆæ€§ã€‚ç‰¹åˆ«æ˜¯ï¼Œé‡‡ç”¨äº†å¯è®­ç»ƒçš„ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶å’Œé«˜æ•ˆçš„æ•°æ®è¿‡æ»¤ç­–ç•¥ï¼Œä½¿å¾—æ¨¡å‹åœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡æ—¶è¡¨ç°å‡ºè‰²ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒMiniCPM4åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†åŒç±»å¼€æºæ¨¡å‹ï¼Œå±•ç°äº†å…¶å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.06444",
            "title": "Saffron-1: Towards an Inference Scaling Paradigm for LLM Safety\n  Assurance",
            "url": "https://huggingface.co/papers/2506.06444",
            "abstract": "SAFFRON introduces multifurcation reward models to improve safety assurance in large language models by addressing the exploration-efficiency dilemma in inference scaling.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing safety assurance research has primarily focused on training-phase alignment to instill safe behaviors into LLMs. However, recent studies have exposed these methods' susceptibility to diverse jailbreak attacks. Concurrently, inference scaling has significantly advanced LLM reasoning capabilities but remains unexplored in the context of safety assurance. Addressing this gap, our work pioneers inference scaling for robust and effective LLM safety against emerging threats. We reveal that conventional inference scaling techniques, despite their success in reasoning tasks, perform poorly in safety contexts, even falling short of basic approaches like Best-of-N Sampling. We attribute this inefficiency to a newly identified challenge, the exploration--efficiency dilemma, arising from the high computational overhead associated with frequent process reward model (PRM) evaluations. To overcome this dilemma, we propose SAFFRON, a novel inference scaling paradigm tailored explicitly for safety assurance. Central to our approach is the introduction of a multifurcation reward model (MRM) that significantly reduces the required number of reward model evaluations. To operationalize this paradigm, we further propose: (i) a partial supervision training objective for MRM, (ii) a conservative exploration constraint to prevent out-of-distribution explorations, and (iii) a Trie-based key--value caching strategy that facilitates cache sharing across sequences during tree search. Extensive experiments validate the effectiveness of our method. Additionally, we publicly release our trained multifurcation reward model (Saffron-1) and the accompanying token-level safety reward dataset (Safety4M) to accelerate future research in LLM safety. Our code, model, and data are publicly available at https://github.com/q-rz/saffron , and our project homepage is at https://q-rz.github.io/p/saffron .",
            "score": 40,
            "issue_id": 4209,
            "pub_date": "2025-06-06",
            "pub_date_card": {
                "ru": "6 Ğ¸ÑĞ½Ñ",
                "en": "June 6",
                "zh": "6æœˆ6æ—¥"
            },
            "hash": "86bb27c97ab5d915",
            "authors": [
                "Ruizhong Qiu",
                "Gaotang Li",
                "Tianxin Wei",
                "Jingrui He",
                "Hanghang Tong"
            ],
            "affiliations": [
                "University of Illinois Urbana-Champaign, IL, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.06444.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#security",
                    "#dataset",
                    "#alignment",
                    "#rl",
                    "#inference"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "SAFFRON: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SAFFRON - Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ (MRM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµĞ¼Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ñ†ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ MRM, Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½ÑĞµÑ€Ğ²Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµÑ„Ğ¸ĞºÑĞ½Ğ¾Ğ³Ğ¾ Ğ´ĞµÑ€ĞµĞ²Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ² Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ LLM Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‰Ğ¸Ñ… ÑƒĞ³Ñ€Ğ¾Ğ·."
                },
                "en": {
                    "title": "SAFFRON: Enhancing LLM Safety with Multifurcation Reward Models",
                    "desc": "SAFFRON presents a new approach to enhance safety in large language models (LLMs) by introducing multifurcation reward models that tackle the exploration-efficiency dilemma during inference scaling. Traditional safety methods have struggled against jailbreak attacks and have not effectively integrated with the advancements in LLM reasoning capabilities. The paper identifies that existing inference scaling techniques are inadequate for safety assurance, often performing worse than simpler methods. To address this, SAFFRON proposes a novel paradigm that reduces the number of necessary reward model evaluations, ensuring safer and more efficient LLM operations."
                },
                "zh": {
                    "title": "SAFFRONï¼šæå‡å¤§å‹è¯­è¨€æ¨¡å‹å®‰å…¨æ€§çš„åˆ›æ–°æ–¹æ¡ˆ",
                    "desc": "SAFFRONæå‡ºäº†ä¸€ç§å¤šåˆ†å‰å¥–åŠ±æ¨¡å‹ï¼Œä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§ä¿éšœï¼Œè§£å†³æ¨ç†æ‰©å±•ä¸­çš„æ¢ç´¢æ•ˆç‡å›°å¢ƒã€‚ç°æœ‰çš„å®‰å…¨ä¿éšœç ”ç©¶ä¸»è¦é›†ä¸­åœ¨è®­ç»ƒé˜¶æ®µçš„å¯¹é½ï¼Œä»¥åŸ¹å…»LLMçš„å®‰å…¨è¡Œä¸ºï¼Œä½†è¿™äº›æ–¹æ³•åœ¨é¢å¯¹å„ç§æ”»å‡»æ—¶è¡¨ç°å‡ºè„†å¼±æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶é¦–æ¬¡æ¢è®¨äº†æ¨ç†æ‰©å±•åœ¨å®‰å…¨ä¿éšœä¸­çš„åº”ç”¨ï¼Œå‘ç°ä¼ ç»Ÿçš„æ¨ç†æ‰©å±•æŠ€æœ¯åœ¨å®‰å…¨ä¸Šä¸‹æ–‡ä¸­è¡¨ç°ä¸ä½³ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¨ç†æ‰©å±•èŒƒå¼ï¼Œç»“åˆå¤šåˆ†å‰å¥–åŠ±æ¨¡å‹ï¼Œæ˜¾è‘—å‡å°‘äº†å¥–åŠ±æ¨¡å‹è¯„ä¼°çš„æ¬¡æ•°ï¼Œä»è€Œæé«˜äº†å®‰å…¨æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.07977",
            "title": "OneIG-Bench: Omni-dimensional Nuanced Evaluation for Image Generation",
            "url": "https://huggingface.co/papers/2506.07977",
            "abstract": "OneIG-Bench is a benchmark framework that comprehensively evaluates text-to-image models across prompt-image alignment, text rendering, reasoning, stylization, and diversity.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-image (T2I) models have garnered significant attention for generating high-quality images aligned with text prompts. However, rapid T2I model advancements reveal limitations in early benchmarks, lacking comprehensive evaluations, for example, the evaluation on reasoning, text rendering and style. Notably, recent state-of-the-art models, with their rich knowledge modeling capabilities, show promising results on the image generation problems requiring strong reasoning ability, yet existing evaluation systems have not adequately addressed this frontier. To systematically address these gaps, we introduce OneIG-Bench, a meticulously designed comprehensive benchmark framework for fine-grained evaluation of T2I models across multiple dimensions, including prompt-image alignment, text rendering precision, reasoning-generated content, stylization, and diversity. By structuring the evaluation, this benchmark enables in-depth analysis of model performance, helping researchers and practitioners pinpoint strengths and bottlenecks in the full pipeline of image generation. Specifically, OneIG-Bench enables flexible evaluation by allowing users to focus on a particular evaluation subset. Instead of generating images for the entire set of prompts, users can generate images only for the prompts associated with the selected dimension and complete the corresponding evaluation accordingly. Our codebase and dataset are now publicly available to facilitate reproducible evaluation studies and cross-model comparisons within the T2I research community.",
            "score": 35,
            "issue_id": 4208,
            "pub_date": "2025-06-09",
            "pub_date_card": {
                "ru": "9 Ğ¸ÑĞ½Ñ",
                "en": "June 9",
                "zh": "6æœˆ9æ—¥"
            },
            "hash": "f87355dfc9390cc7",
            "authors": [
                "Jingjing Chang",
                "Yixiao Fang",
                "Peng Xing",
                "Shuhan Wu",
                "Wei Cheng",
                "Rui Wang",
                "Xianfang Zeng",
                "Gang Yu",
                "Hai-Bao Chen"
            ],
            "affiliations": [
                "SJTU",
                "StepFun"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.07977.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#cv",
                    "#open_source",
                    "#survey"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ’ÑĞµÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½ÑÑ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‚ĞµĞºÑÑ‚-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ",
                    "desc": "OneIG-Bench - ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ. ĞĞ½Ğ° Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ°Ğ¼, Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ°, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, ÑÑ‚Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ²Ñ‹ÑĞ²Ğ»ÑÑ Ğ¸Ñ… ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ ÑĞ»Ğ°Ğ±Ñ‹Ğµ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñ‹. OneIG-Bench Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ³Ğ¸Ğ±ĞºÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ¾Ñ‡Ğ¸Ñ‚ÑŒÑÑ Ğ½Ğ° ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "OneIG-Bench: A Comprehensive Evaluation for Text-to-Image Models",
                    "desc": "OneIG-Bench is a new framework designed to evaluate text-to-image (T2I) models in a detailed way. It focuses on several important aspects like how well the images match the text prompts, the quality of text rendering, reasoning capabilities, artistic style, and diversity of generated images. This benchmark addresses the shortcomings of previous evaluation methods by providing a structured approach that allows researchers to analyze specific areas of model performance. By making the code and dataset publicly available, OneIG-Bench promotes reproducibility and encourages comparisons among different T2I models."
                },
                "zh": {
                    "title": "å…¨é¢è¯„ä¼°æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„åŸºå‡†æ¡†æ¶",
                    "desc": "OneIG-Benchæ˜¯ä¸€ä¸ªåŸºå‡†æ¡†æ¶ï¼Œæ—¨åœ¨å…¨é¢è¯„ä¼°æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹çš„æ€§èƒ½ã€‚å®ƒæ¶µç›–äº†å¤šä¸ªç»´åº¦ï¼ŒåŒ…æ‹¬æç¤ºä¸å›¾åƒçš„å¯¹é½ã€æ–‡æœ¬æ¸²æŸ“ã€æ¨ç†èƒ½åŠ›ã€é£æ ¼åŒ–å’Œå¤šæ ·æ€§ã€‚é€šè¿‡ç³»ç»ŸåŒ–çš„è¯„ä¼°ï¼ŒOneIG-Benchå¸®åŠ©ç ”ç©¶äººå‘˜æ·±å…¥åˆ†ææ¨¡å‹çš„ä¼˜ç¼ºç‚¹ï¼Œè¯†åˆ«å›¾åƒç”Ÿæˆè¿‡ç¨‹ä¸­çš„ç“¶é¢ˆã€‚è¯¥æ¡†æ¶çš„ä»£ç åº“å’Œæ•°æ®é›†å·²å…¬å¼€ï¼Œä¾¿äºåœ¨T2Iç ”ç©¶ç¤¾åŒºä¸­è¿›è¡Œå¯é‡å¤çš„è¯„ä¼°ç ”ç©¶å’Œè·¨æ¨¡å‹æ¯”è¾ƒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.07491",
            "title": "SpatialLM: Training Large Language Models for Structured Indoor Modeling",
            "url": "https://huggingface.co/papers/2506.07491",
            "abstract": "SpatialLM is a large language model capable of processing 3D point cloud data to produce structured outputs for spatial understanding, outperforming previous methods on layout estimation and object detection tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t SpatialLM is a large language model designed to process 3D point cloud data and generate structured 3D scene understanding outputs. These outputs include architectural elements like walls, doors, windows, and oriented object boxes with their semantic categories. Unlike previous methods which exploit task-specific network designs, our model adheres to the standard multimodal LLM architecture and is fine-tuned directly from open-source LLMs.   To train SpatialLM, we collect a large-scale, high-quality synthetic dataset consisting of the point clouds of 12,328 indoor scenes (54,778 rooms) with ground-truth 3D annotations, and conduct a careful study on various modeling and training decisions. On public benchmarks, our model gives state-of-the-art performance in layout estimation and competitive results in 3D object detection. With that, we show a feasible path for enhancing the spatial understanding capabilities of modern LLMs for applications in augmented reality, embodied robotics, and more.",
            "score": 22,
            "issue_id": 4208,
            "pub_date": "2025-06-09",
            "pub_date_card": {
                "ru": "9 Ğ¸ÑĞ½Ñ",
                "en": "June 9",
                "zh": "6æœˆ9æ—¥"
            },
            "hash": "604b98825a94f8d1",
            "authors": [
                "Yongsen Mao",
                "Junhao Zhong",
                "Chuan Fang",
                "Jia Zheng",
                "Rui Tang",
                "Hao Zhu",
                "Ping Tan",
                "Zihan Zhou"
            ],
            "affiliations": [
                "Hong Kong University of Science and Technology",
                "Manycore Tech Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.07491.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#benchmark",
                    "#3d",
                    "#open_source",
                    "#synthetic",
                    "#multimodal"
                ],
                "emoji": "ğŸ ",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ 3D-ÑÑ†ĞµĞ½ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "SpatialLM - ÑÑ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ĞºĞ° Ñ‚Ğ¾Ñ‡ĞµĞº Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑÑ†ĞµĞ½. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², SpatialLM Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ LLM Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ¸Ğ· Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… LLM. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¿Ğ¾Ğ¼ĞµÑ‰ĞµĞ½Ğ¸Ğ¹ Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Revolutionizing 3D Spatial Understanding with SpatialLM",
                    "desc": "SpatialLM is a large language model that specializes in understanding 3D point cloud data, enabling it to produce structured outputs for spatial comprehension. It identifies and categorizes architectural features such as walls, doors, and windows, as well as oriented object boxes. Unlike earlier approaches that relied on specific network designs for tasks, SpatialLM uses a standard multimodal architecture and is fine-tuned from existing open-source models. The model is trained on a comprehensive dataset of indoor scenes, achieving state-of-the-art results in layout estimation and strong performance in 3D object detection, paving the way for advancements in augmented reality and robotics."
                },
                "zh": {
                    "title": "SpatialLMï¼šæå‡ç©ºé—´ç†è§£çš„è¯­è¨€æ¨¡å‹",
                    "desc": "SpatialLMæ˜¯ä¸€ç§å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿå¤„ç†3Dç‚¹äº‘æ•°æ®ï¼Œå¹¶ç”Ÿæˆç»“æ„åŒ–çš„ç©ºé—´ç†è§£è¾“å‡ºã€‚è¿™äº›è¾“å‡ºåŒ…æ‹¬å»ºç­‘å…ƒç´ ï¼Œå¦‚å¢™å£ã€é—¨ã€çª—æˆ·ä»¥åŠå¸¦æœ‰è¯­ä¹‰ç±»åˆ«çš„å®šå‘ç‰©ä½“æ¡†ã€‚ä¸ä¹‹å‰çš„æ–¹æ³•ä¸åŒï¼ŒSpatialLMéµå¾ªæ ‡å‡†çš„å¤šæ¨¡æ€LLMæ¶æ„ï¼Œå¹¶ç›´æ¥ä»å¼€æºLLMè¿›è¡Œå¾®è°ƒã€‚é€šè¿‡æ”¶é›†åŒ…å«12,328ä¸ªå®¤å†…åœºæ™¯çš„é«˜è´¨é‡åˆæˆæ•°æ®é›†ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å¸ƒå±€ä¼°è®¡å’Œ3Dç‰©ä½“æ£€æµ‹ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.06205",
            "title": "Astra: Toward General-Purpose Mobile Robots via Hierarchical Multimodal\n  Learning",
            "url": "https://huggingface.co/papers/2506.06205",
            "abstract": "Astra, a dual-model architecture for mobile robot navigation, uses a multimodal LLM for global localization and a multitask network for local path planning and odometry estimation, achieving high success rates in diverse indoor environments.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern robot navigation systems encounter difficulties in diverse and complex indoor environments. Traditional approaches rely on multiple modules with small models or rule-based systems and thus lack adaptability to new environments. To address this, we developed Astra, a comprehensive dual-model architecture, Astra-Global and Astra-Local, for mobile robot navigation. Astra-Global, a multimodal LLM, processes vision and language inputs to perform self and goal localization using a hybrid topological-semantic graph as the global map, and outperforms traditional visual place recognition methods. Astra-Local, a multitask network, handles local path planning and odometry estimation. Its 4D spatial-temporal encoder, trained through self-supervised learning, generates robust 4D features for downstream tasks. The planning head utilizes flow matching and a novel masked ESDF loss to minimize collision risks for generating local trajectories, and the odometry head integrates multi-sensor inputs via a transformer encoder to predict the relative pose of the robot. Deployed on real in-house mobile robots, Astra achieves high end-to-end mission success rate across diverse indoor environments.",
            "score": 14,
            "issue_id": 4216,
            "pub_date": "2025-06-06",
            "pub_date_card": {
                "ru": "6 Ğ¸ÑĞ½Ñ",
                "en": "June 6",
                "zh": "6æœˆ6æ—¥"
            },
            "hash": "ff910133c6b6f7f5",
            "authors": [
                "Sheng Chen",
                "Peiyu He",
                "Jiaxin Hu",
                "Ziyang Liu",
                "Yansheng Wang",
                "Tao Xu",
                "Chi Zhang",
                "Chongchong Zhang",
                "Chao An",
                "Shiyu Cai",
                "Duo Cao",
                "Kangping Chen",
                "Shuai Chu",
                "Tianwei Chu",
                "Mingdi Dan",
                "Min Du",
                "Weiwei Fang",
                "Pengyou Fu",
                "Junkai Hu",
                "Xiaowei Jiang",
                "Zhaodi Jiang",
                "Fuxuan Li",
                "Jun Li",
                "Minghui Li",
                "Mingyao Li",
                "Yanchang Li",
                "Zhibin Li",
                "Guangming Liu",
                "Kairui Liu",
                "Lihao Liu",
                "Weizhi Liu",
                "Xiaoshun Liu",
                "Yufei Liu",
                "Yunfei Liu",
                "Qiang Lu",
                "Yuanfei Luo",
                "Xiang Lv",
                "Hongying Ma",
                "Sai Ma",
                "Lingxian Mi",
                "Sha Sa",
                "Hongxiang Shu",
                "Lei Tian",
                "Chengzhi Wang",
                "Jiayu Wang",
                "Kaijie Wang",
                "Qingyi Wang",
                "Renwen Wang",
                "Tao Wang",
                "Wei Wang",
                "Xirui Wang",
                "Chao Wei",
                "Xuguang Wei",
                "Zijun Xia",
                "Zhaohao Xiao",
                "Tingshuai Yan",
                "Liyan Yang",
                "Yifan Yang",
                "Zhikai Yang",
                "Zhong Yin",
                "Li Yuan",
                "Liuchun Yuan",
                "Chi Zhang",
                "Jinyang Zhang",
                "Junhui Zhang",
                "Linge Zhang",
                "Zhenyi Zhang",
                "Zheyu Zhang",
                "Dongjie Zhu",
                "Hang Li",
                "Yangang Zhang"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2506.06205.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#games",
                    "#architecture",
                    "#agents",
                    "#graphs",
                    "#optimization",
                    "#multimodal"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞÑÑ‚Ñ€Ğ°: Ğ´Ğ²ÑƒÑ…Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ˜Ğ˜ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²",
                    "desc": "ĞÑÑ‚Ñ€Ğ° - ÑÑ‚Ğ¾ Ğ´Ğ²ÑƒÑ…Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ². ĞĞ½Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿ÑƒÑ‚Ğ¸ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¾Ğ´Ğ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸. ĞÑÑ‚Ñ€Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ Ñ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾-ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ĞºĞ°Ñ€Ñ‚Ñƒ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ĞºĞ°Ñ€Ñ‚Ñ‹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…."
                },
                "en": {
                    "title": "Astra: Navigating Indoor Spaces with Dual-Model Intelligence",
                    "desc": "Astra is a dual-model architecture designed for mobile robot navigation, addressing challenges in complex indoor environments. It consists of Astra-Global, a multimodal large language model (LLM) that combines vision and language inputs for effective global localization, and Astra-Local, a multitask network focused on local path planning and odometry estimation. The system utilizes advanced techniques like a hybrid topological-semantic graph and a 4D spatial-temporal encoder to enhance performance and adaptability. By integrating self-supervised learning and innovative loss functions, Astra achieves high success rates in navigating diverse indoor settings."
                },
                "zh": {
                    "title": "Astraï¼šæ™ºèƒ½ç§»åŠ¨æœºå™¨äººå¯¼èˆªçš„æ–°çºªå…ƒ",
                    "desc": "Astraæ˜¯ä¸€ç§åŒæ¨¡å‹æ¶æ„ï¼Œä¸“ä¸ºç§»åŠ¨æœºå™¨äººå¯¼èˆªè®¾è®¡ã€‚å®ƒç»“åˆäº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤šä»»åŠ¡ç½‘ç»œï¼Œåˆ†åˆ«ç”¨äºå…¨å±€å®šä½å’Œå±€éƒ¨è·¯å¾„è§„åˆ’ã€‚Astra-Globalé€šè¿‡å¤„ç†è§†è§‰å’Œè¯­è¨€è¾“å…¥ï¼Œåˆ©ç”¨æ··åˆæ‹“æ‰‘-è¯­ä¹‰å›¾è¿›è¡Œè‡ªæˆ‘å’Œç›®æ ‡å®šä½ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„è§†è§‰ä½ç½®è¯†åˆ«æ–¹æ³•ã€‚Astra-Localåˆ™é€šè¿‡è‡ªç›‘ç£å­¦ä¹ ç”Ÿæˆå¼ºå¤§çš„4Dç‰¹å¾ï¼Œç¡®ä¿åœ¨å¤æ‚å®¤å†…ç¯å¢ƒä¸­å®ç°é«˜æˆåŠŸç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.07986",
            "title": "Rethinking Cross-Modal Interaction in Multimodal Diffusion Transformers",
            "url": "https://huggingface.co/papers/2506.07986",
            "abstract": "Temperature-Adjusted Cross-modal Attention (TACA) improves text-image alignment in diffusion-based multimodal transformers by dynamically balancing multimodal interactions and provides a parameter-efficient solution.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Diffusion Transformers (MM-DiTs) have achieved remarkable progress in text-driven visual generation. However, even state-of-the-art MM-DiT models like FLUX struggle with achieving precise alignment between text prompts and generated content. We identify two key issues in the attention mechanism of MM-DiT, namely 1) the suppression of cross-modal attention due to token imbalance between visual and textual modalities and 2) the lack of timestep-aware attention weighting, which hinder the alignment. To address these issues, we propose Temperature-Adjusted Cross-modal Attention (TACA), a parameter-efficient method that dynamically rebalances multimodal interactions through temperature scaling and timestep-dependent adjustment. When combined with LoRA fine-tuning, TACA significantly enhances text-image alignment on the T2I-CompBench benchmark with minimal computational overhead. We tested TACA on state-of-the-art models like FLUX and SD3.5, demonstrating its ability to improve image-text alignment in terms of object appearance, attribute binding, and spatial relationships. Our findings highlight the importance of balancing cross-modal attention in improving semantic fidelity in text-to-image diffusion models. Our codes are publicly available at https://github.com/Vchitect/TACA",
            "score": 11,
            "issue_id": 4209,
            "pub_date": "2025-06-09",
            "pub_date_card": {
                "ru": "9 Ğ¸ÑĞ½Ñ",
                "en": "June 9",
                "zh": "6æœˆ9æ—¥"
            },
            "hash": "54c2bf2395cf58cc",
            "authors": [
                "Zhengyao Lv",
                "Tianlin Pan",
                "Chenyang Si",
                "Zhaoxi Chen",
                "Wangmeng Zuo",
                "Ziwei Liu",
                "Kwan-Yee K. Wong"
            ],
            "affiliations": [
                "Harbin Institute of Technology",
                "Nanjing University",
                "Nanyang Technological University",
                "The University of Hong Kong",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.07986.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#open_source",
                    "#optimization",
                    "#training",
                    "#diffusion",
                    "#benchmark"
                ],
                "emoji": "ğŸ”€",
                "ru": {
                    "title": "Ğ‘Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Temperature-Adjusted Cross-modal Attention (TACA) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ…. TACA Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‚ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸, Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‰ĞµĞ¹ Ğ¾Ñ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑˆĞ°Ğ³Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ñ ÑƒÑ‡ĞµÑ‚Ğ° Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ğ² Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. TACA Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "Enhancing Text-Image Alignment with TACA",
                    "desc": "The paper introduces Temperature-Adjusted Cross-modal Attention (TACA), a novel approach to enhance text-image alignment in multimodal diffusion transformers. TACA addresses two main challenges: the imbalance in token representation between text and images, and the absence of attention weighting that considers the timing of inputs. By implementing temperature scaling and timestep-dependent adjustments, TACA efficiently rebalances interactions between modalities. The results show that TACA, when used with LoRA fine-tuning, significantly improves alignment in state-of-the-art models, leading to better semantic fidelity in generated images."
                },
                "zh": {
                    "title": "æ¸©åº¦è°ƒæ•´è·¨æ¨¡æ€æ³¨æ„åŠ›ï¼šæå‡æ–‡æœ¬ä¸å›¾åƒå¯¹é½çš„åˆ©å™¨",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºæ¸©åº¦è°ƒæ•´è·¨æ¨¡æ€æ³¨æ„åŠ›ï¼ˆTACAï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨æ”¹å–„åŸºäºæ‰©æ•£çš„å¤šæ¨¡æ€å˜æ¢å™¨ä¸­çš„æ–‡æœ¬ä¸å›¾åƒå¯¹é½ã€‚æˆ‘ä»¬å‘ç°ç°æœ‰æ¨¡å‹åœ¨æ–‡æœ¬æç¤ºä¸ç”Ÿæˆå†…å®¹ä¹‹é—´å­˜åœ¨æ³¨æ„åŠ›æœºåˆ¶çš„ä¸è¶³ï¼Œä¸»è¦ä½“ç°åœ¨è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€ä¹‹é—´çš„ä»¤ç‰Œä¸å¹³è¡¡ä»¥åŠç¼ºä¹æ—¶é—´æ­¥æ„ŸçŸ¥çš„æ³¨æ„åŠ›åŠ æƒã€‚TACAé€šè¿‡æ¸©åº¦ç¼©æ”¾å’Œæ—¶é—´æ­¥ä¾èµ–çš„è°ƒæ•´ï¼ŒåŠ¨æ€å¹³è¡¡å¤šæ¨¡æ€äº¤äº’ï¼Œä»è€Œæé«˜æ–‡æœ¬ä¸å›¾åƒçš„å¯¹é½æ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTACAåœ¨FLUXå’ŒSD3.5ç­‰å…ˆè¿›æ¨¡å‹ä¸Šæ˜¾è‘—æå‡äº†å›¾åƒ-æ–‡æœ¬å¯¹é½çš„å‡†ç¡®æ€§ï¼Œä¸”è®¡ç®—å¼€é”€æå°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.07553",
            "title": "GTR-CoT: Graph Traversal as Visual Chain of Thought for Molecular\n  Structure Recognition",
            "url": "https://huggingface.co/papers/2506.07553",
            "abstract": "GTR-Mol-VLM, a vision-language model with graph traversal and data-centric principles, achieves superior accuracy in converting molecular images to machine-readable formats, particularly in handling complex structures and functional group abbreviations.  \t\t\t\t\tAI-generated summary \t\t\t\t Optical Chemical Structure Recognition (OCSR) is crucial for digitizing chemical knowledge by converting molecular images into machine-readable formats. While recent vision-language models (VLMs) have shown potential in this task, their image-captioning approach often struggles with complex molecular structures and inconsistent annotations. To overcome these challenges, we introduce GTR-Mol-VLM, a novel framework featuring two key innovations: (1) the Graph Traversal as Visual Chain of Thought mechanism that emulates human reasoning by incrementally parsing molecular graphs through sequential atom-bond predictions, and (2) the data-centric principle of Faithfully Recognize What You've Seen, which addresses the mismatch between abbreviated structures in images and their expanded annotations. To support model development, we constructed GTR-CoT-1.3M, a large-scale instruction-tuning dataset with meticulously corrected annotations, and introduced MolRec-Bench, the first benchmark designed for a fine-grained evaluation of graph-parsing accuracy in OCSR. Comprehensive experiments demonstrate that GTR-Mol-VLM achieves superior results compared to specialist models, chemistry-domain VLMs, and commercial general-purpose VLMs. Notably, in scenarios involving molecular images with functional group abbreviations, GTR-Mol-VLM outperforms the second-best baseline by approximately 14 percentage points, both in SMILES-based and graph-based metrics. We hope that this work will drive OCSR technology to more effectively meet real-world needs, thereby advancing the fields of cheminformatics and AI for Science. We will release GTR-CoT at https://github.com/opendatalab/GTR-CoT.",
            "score": 11,
            "issue_id": 4208,
            "pub_date": "2025-06-09",
            "pub_date_card": {
                "ru": "9 Ğ¸ÑĞ½Ñ",
                "en": "June 9",
                "zh": "6æœˆ9æ—¥"
            },
            "hash": "379cf5e0efdacd27",
            "authors": [
                "Jingchao Wang",
                "Haote Yang",
                "Jiang Wu",
                "Yifan He",
                "Xingjian Wei",
                "Yinfan Wang",
                "Chengjin Liu",
                "Lingli Ge",
                "Lijun Wu",
                "Bin Wang",
                "Dahua Lin",
                "Conghui He"
            ],
            "affiliations": [
                "Chinese University of Hong Kong",
                "East China Normal University",
                "Fudan University",
                "Northwestern Polytechnical University",
                "Peking University",
                "Shanghai Artificial Intelligence Laboratory",
                "Shanghai University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.07553.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#graphs",
                    "#benchmark",
                    "#reasoning",
                    "#cv",
                    "#science",
                    "#games"
                ],
                "emoji": "ğŸ§ª",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ…Ğ¸Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±Ñ…Ğ¾Ğ´Ğ° Ğ³Ñ€Ğ°Ñ„Ğ° Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "GTR-Mol-VLM - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ´Ğ»Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ…Ğ¸Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€ Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¾Ğ±Ñ…Ğ¾Ğ´Ğ° Ğ³Ñ€Ğ°Ñ„Ğ° Ğ¸ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ ÑƒĞ²Ğ¸Ğ´ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ ÑĞ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸ Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ³Ñ€ÑƒĞ¿Ğ¿. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ñ‹Ğ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ñ‹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… GTR-CoT-1.3M Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº MolRec-Bench."
                },
                "en": {
                    "title": "Transforming Molecular Images into Accurate Data with GTR-Mol-VLM",
                    "desc": "GTR-Mol-VLM is a new vision-language model designed to convert molecular images into machine-readable formats with high accuracy. It uses a unique Graph Traversal mechanism that mimics human reasoning by analyzing molecular graphs step-by-step. Additionally, it employs a data-centric approach to ensure that the model accurately recognizes abbreviated structures in images. The model has been tested against various benchmarks and has shown significant improvements, especially in handling complex molecular structures and functional group abbreviations."
                },
                "zh": {
                    "title": "GTR-Mol-VLMï¼šæå‡åˆ†å­å›¾åƒè¯†åˆ«çš„æ™ºèƒ½æ–°æ–¹æ³•",
                    "desc": "GTR-Mol-VLMæ˜¯ä¸€ç§ç»“åˆå›¾éå†å’Œæ•°æ®ä¸­å¿ƒåŸåˆ™çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿå°†åˆ†å­å›¾åƒå‡†ç¡®è½¬æ¢ä¸ºæœºå™¨å¯è¯»æ ¼å¼ã€‚è¯¥æ¨¡å‹é€šè¿‡å›¾éå†æœºåˆ¶æ¨¡æ‹Ÿäººç±»æ¨ç†ï¼Œé€æ­¥è§£æåˆ†å­å›¾ï¼Œè§£å†³äº†å¤æ‚ç»“æ„å’ŒåŠŸèƒ½åŸºå›¢ç¼©å†™çš„é—®é¢˜ã€‚ä¸ºäº†æ”¯æŒæ¨¡å‹å¼€å‘ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„æŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†GTR-CoT-1.3Mï¼Œå¹¶æ¨å‡ºäº†MolRec-BenchåŸºå‡†æµ‹è¯•ï¼Œä»¥è¯„ä¼°å›¾è§£æçš„å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGTR-Mol-VLMåœ¨å¤„ç†åˆ†å­å›¾åƒæ—¶çš„è¡¨ç°ä¼˜äºå…¶ä»–ä¸“ä¸šæ¨¡å‹å’Œé€šç”¨æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨åŠŸèƒ½åŸºå›¢ç¼©å†™çš„åœºæ™¯ä¸­ï¼Œå‡†ç¡®ç‡æé«˜äº†çº¦14ä¸ªç™¾åˆ†ç‚¹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.07712",
            "title": "Through the Valley: Path to Effective Long CoT Training for Small\n  Language Models",
            "url": "https://huggingface.co/papers/2506.07712",
            "abstract": "Small language models experience significant performance declines when trained on long chain-of-thought data due to error accumulation, impacting downstream reinforcement learning but potentially mitigated by extensive supervised fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Long chain-of-thought (CoT) supervision has become a common strategy to enhance reasoning in language models. While effective for large models, we identify a phenomenon we call Long CoT Degradation, in which small language models (SLMs; <=3B parameters) trained on limited long CoT data experience significant performance deterioration. Through extensive experiments on the Qwen2.5, LLaMA3 and Gemma3 families, we demonstrate that this degradation is widespread across SLMs. In some settings, models trained on only 8k long CoT examples lose up to 75% of their original performance before fine-tuning. Strikingly, we further observe that for some particularly small models, even training on 220k long CoT examples fails to recover or surpass their original performance prior to fine-tuning. Our analysis attributes this effect to error accumulation: while longer responses increase the capacity for multi-step reasoning, they also amplify the risk of compounding mistakes. Furthermore, we find that Long CoT Degradation may negatively impacts downstream reinforcement learning (RL), although this can be alleviated by sufficiently scaled supervised fine-tuning (SFT). Our findings challenge common assumptions about the benefits of long CoT training for SLMs and offer practical guidance for building more effective small-scale reasoning models.",
            "score": 10,
            "issue_id": 4208,
            "pub_date": "2025-06-09",
            "pub_date_card": {
                "ru": "9 Ğ¸ÑĞ½Ñ",
                "en": "June 9",
                "zh": "6æœˆ9æ—¥"
            },
            "hash": "856fa5edc302286a",
            "authors": [
                "Renjie Luo",
                "Jiaxi Li",
                "Chen Huang",
                "Wei Lu"
            ],
            "affiliations": [
                "StatNLP Research Group, Singapore University of Technology and Design"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.07712.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#long_context",
                    "#training",
                    "#reasoning",
                    "#rl"
                ],
                "emoji": "ğŸ“‰",
                "ru": {
                    "title": "ĞœĞ°Ğ»Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑÑ‚Ñ€Ğ°Ğ´Ğ°ÑÑ‚ Ğ¾Ñ‚ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ñ„ĞµĞ½Ğ¾Ğ¼ĞµĞ½ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ°Ğ»Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (CoT). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¼ĞµĞ½ĞµĞµ Ñ‡ĞµĞ¼ 3 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ñ‚ĞµÑ€ÑÑ‚ÑŒ Ğ´Ğ¾ 75% ÑĞ²Ğ¾ĞµĞ¹ Ğ¸Ğ·Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ÑĞ»Ğµ Ñ‚Ğ°ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾ ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑŠÑÑĞ½ÑĞµÑ‚ÑÑ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ñ CoT Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ğ»Ğ¸ÑÑ‚ÑŒ Ğ½Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ñ…Ğ¾Ñ‚Ñ ÑÑ‚Ğ¾Ñ‚ ÑÑ„Ñ„ĞµĞºÑ‚ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ ÑĞ¼ÑĞ³Ñ‡Ğ¸Ñ‚ÑŒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¼ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ´ Ğ¿Ñ€Ğ¸ÑĞ¼Ğ¾Ñ‚Ñ€Ğ¾Ğ¼."
                },
                "en": {
                    "title": "Mitigating Long CoT Degradation in Small Language Models",
                    "desc": "This paper discusses a problem called Long CoT Degradation, which affects small language models (SLMs) when they are trained on long chain-of-thought (CoT) data. The authors found that these models can lose a significant amount of performance due to error accumulation, especially when trained on limited long CoT examples. Their experiments show that even with a large number of training examples, some small models still struggle to recover their original performance. The study suggests that while long CoT training can enhance reasoning, it may not be beneficial for smaller models without adequate supervised fine-tuning to mitigate the negative effects."
                },
                "zh": {
                    "title": "å°å‹æ¨¡å‹çš„é•¿é“¾æ€ç»´é€€åŒ–é—®é¢˜",
                    "desc": "å°å‹è¯­è¨€æ¨¡å‹åœ¨é•¿é“¾æ€ç»´æ•°æ®ä¸Šè®­ç»ƒæ—¶ï¼Œå› é”™è¯¯ç´¯ç§¯è€Œå¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œè¿™ç§ç°è±¡è¢«ç§°ä¸ºé•¿é“¾æ€ç»´é€€åŒ–ã€‚å°½ç®¡é•¿é“¾æ€ç»´ç›‘ç£å¯¹å¤§å‹æ¨¡å‹æœ‰æ•ˆï¼Œä½†å°å‹æ¨¡å‹åœ¨æœ‰é™çš„é•¿é“¾æ€ç»´æ•°æ®ä¸Šè®­ç»ƒæ—¶ï¼Œæ€§èƒ½å¯èƒ½ä¸‹é™é«˜è¾¾75%ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œé”™è¯¯ç´¯ç§¯æ˜¯å¯¼è‡´è¿™ç§é€€åŒ–çš„ä¸»è¦åŸå› ï¼Œé•¿å“åº”è™½ç„¶å¢åŠ äº†å¤šæ­¥æ¨ç†çš„èƒ½åŠ›ï¼Œä½†ä¹ŸåŠ å¤§äº†é”™è¯¯å åŠ çš„é£é™©ã€‚æ­¤å¤–ï¼Œé•¿é“¾æ€ç»´é€€åŒ–è¿˜å¯èƒ½å¯¹åç»­çš„å¼ºåŒ–å­¦ä¹ äº§ç”Ÿè´Ÿé¢å½±å“ï¼Œä½†é€šè¿‡å……åˆ†è§„æ¨¡çš„ç›‘ç£å¾®è°ƒå¯ä»¥ç¼“è§£è¿™ä¸€é—®é¢˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.07530",
            "title": "BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation",
            "url": "https://huggingface.co/papers/2506.07530",
            "abstract": "BitVLA, a 1-bit VLA model with ternary parameters, achieves comparable performance to OpenVLA-OFT on LIBERO while using 29.8% less memory through distillation-aware training.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language-Action (VLA) models have shown impressive capabilities across a wide range of robotics manipulation tasks. However, their growing model size poses significant challenges for deployment on resource-constrained robotic systems. While 1-bit pretraining has proven effective for enhancing the inference efficiency of large language models with minimal performance loss, its application to VLA models remains underexplored. In this work, we present BitVLA, the first 1-bit VLA model for robotics manipulation, in which every parameter is ternary, i.e., {-1, 0, 1}. To further reduce the memory footprint of the vision encoder, we propose the distillation-aware training strategy that compresses the full-precision encoder to 1.58-bit weights. During this process, a full-precision encoder serves as a teacher model to better align latent representations. Despite the lack of large-scale robotics pretraining, BitVLA achieves performance comparable to the state-of-the-art model OpenVLA-OFT with 4-bit post-training quantization on the LIBERO benchmark, while consuming only 29.8% of the memory. These results highlight BitVLA's promise for deployment on memory-constrained edge devices. We release the code and model weights in https://github.com/ustcwhy/BitVLA.",
            "score": 10,
            "issue_id": 4208,
            "pub_date": "2025-06-09",
            "pub_date_card": {
                "ru": "9 Ğ¸ÑĞ½Ñ",
                "en": "June 9",
                "zh": "6æœˆ9æ—¥"
            },
            "hash": "2ff752565d34986b",
            "authors": [
                "Hongyu Wang",
                "Chuyan Xiong",
                "Ruiping Wang",
                "Xilin Chen"
            ],
            "affiliations": [
                "Key Laboratory of AI Safety, Institute of Computing Technology, Chinese Academy of Sciences",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.07530.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#small_models",
                    "#robotics",
                    "#inference",
                    "#open_source"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ 1-Ğ±Ğ¸Ñ‚Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ VLA Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸",
                    "desc": "BitVLA - ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ°Ñ 1-Ğ±Ğ¸Ñ‚Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ VLA Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹, Ğ³Ğ´Ğµ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ñ‚ĞµÑ€Ğ½Ğ°Ñ€Ğ½Ñ‹Ğ¼ (-1, 0, 1). ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸ĞµĞ¹ Ğ´Ğ»Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° Ğ´Ğ¾ 1,58-Ğ±Ğ¸Ñ‚Ğ½Ñ‹Ñ… Ğ²ĞµÑĞ¾Ğ². BitVLA Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ¹ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ OpenVLA-OFT Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ LIBERO, Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ÑÑ Ğ²ÑĞµĞ³Ğ¾ 29,8% Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. Ğ­Ñ‚Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ BitVLA Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ… Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ."
                },
                "en": {
                    "title": "Efficient Robotics with BitVLA: Less Memory, Same Performance!",
                    "desc": "BitVLA is a novel 1-bit Vision-Language-Action (VLA) model designed for robotics manipulation tasks, utilizing ternary parameters to optimize performance. It employs a distillation-aware training strategy that compresses a full-precision vision encoder into 1.58-bit weights, significantly reducing memory usage. Despite not being pretrained on large-scale robotics data, BitVLA achieves performance on par with the state-of-the-art OpenVLA-OFT model while using 29.8% less memory. This advancement makes BitVLA particularly suitable for deployment in resource-limited robotic systems."
                },
                "zh": {
                    "title": "BitVLAï¼šé«˜æ•ˆçš„1ä½è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹",
                    "desc": "BitVLAæ˜¯ä¸€ç§1ä½è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹ï¼Œé‡‡ç”¨ä¸‰å…ƒå‚æ•°ï¼Œæ—¨åœ¨è§£å†³æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­çš„å†…å­˜é™åˆ¶é—®é¢˜ã€‚é€šè¿‡ä½¿ç”¨æ³¨æ„è’¸é¦è®­ç»ƒç­–ç•¥ï¼ŒBitVLAåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—å‡å°‘äº†å†…å­˜å ç”¨ï¼Œè¾¾åˆ°ä»…ä½¿ç”¨29.8%çš„å†…å­˜ã€‚å°½ç®¡ç¼ºä¹å¤§è§„æ¨¡çš„æœºå™¨äººé¢„è®­ç»ƒï¼ŒBitVLAåœ¨LIBEROåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºä¸æœ€å…ˆè¿›çš„OpenVLA-OFTæ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚è¯¥ç ”ç©¶å±•ç¤ºäº†BitVLAåœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šçš„åº”ç”¨æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.07298",
            "title": "Pre-trained Large Language Models Learn Hidden Markov Models In-context",
            "url": "https://huggingface.co/papers/2506.07298",
            "abstract": "LLMs using in-context learning can accurately predict sequences generated by HMMs, showcasing its potential for uncovering hidden structures in complex data.  \t\t\t\t\tAI-generated summary \t\t\t\t Hidden Markov Models (HMMs) are foundational tools for modeling sequential data with latent Markovian structure, yet fitting them to real-world data remains computationally challenging. In this work, we show that pre-trained large language models (LLMs) can effectively model data generated by HMMs via in-context learning (ICL)x2013their ability to infer patterns from examples within a prompt. On a diverse set of synthetic HMMs, LLMs achieve predictive accuracy approaching the theoretical optimum. We uncover novel scaling trends influenced by HMM properties, and offer theoretical conjectures for these empirical observations. We also provide practical guidelines for scientists on using ICL as a diagnostic tool for complex data. On real-world animal decision-making tasks, ICL achieves competitive performance with models designed by human experts. To our knowledge, this is the first demonstration that ICL can learn and predict HMM-generated sequencesx2013an advance that deepens our understanding of in-context learning in LLMs and establishes its potential as a powerful tool for uncovering hidden structure in complex scientific data.",
            "score": 9,
            "issue_id": 4208,
            "pub_date": "2025-06-08",
            "pub_date_card": {
                "ru": "8 Ğ¸ÑĞ½Ñ",
                "en": "June 8",
                "zh": "6æœˆ8æ—¥"
            },
            "hash": "568113a6bc87dd15",
            "authors": [
                "Yijia Dai",
                "Zhaolin Gao",
                "Yahya Satter",
                "Sarah Dean",
                "Jennifer J. Sun"
            ],
            "affiliations": [
                "Cornell University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.07298.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#transfer_learning",
                    "#data",
                    "#science",
                    "#multimodal"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "LLM Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¼Ğ°Ñ€ĞºĞ¾Ğ²ÑĞºĞ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (HMM), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ (ICL). LLM Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ±Ğ»Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğº Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼ÑƒĞ¼Ñƒ, Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… HMM. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ñ‚ĞµĞ½Ğ´ĞµĞ½Ñ†Ğ¸Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‰Ğ¸Ğµ Ğ¾Ñ‚ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² HMM, Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑƒĞ³Ğ»ÑƒĞ±Ğ»ÑÑÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ICL Ğ² LLM Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ĞµĞ³Ğ¾ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» ĞºĞ°Ğº Ğ¼Ğ¾Ñ‰Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Unlocking Hidden Structures with In-Context Learning in LLMs",
                    "desc": "This paper demonstrates that large language models (LLMs) can effectively use in-context learning (ICL) to predict sequences generated by Hidden Markov Models (HMMs). The authors show that LLMs can achieve high predictive accuracy on synthetic HMM data, approaching theoretical limits. They also identify scaling trends related to HMM characteristics and provide insights into using ICL as a diagnostic tool for complex datasets. Additionally, the study highlights that ICL performs competitively on real-world tasks compared to expert-designed models, showcasing its potential in revealing hidden structures in data."
                },
                "zh": {
                    "title": "åˆ©ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ æ­ç¤ºå¤æ‚æ•°æ®çš„æ½œåŠ›",
                    "desc": "æœ¬ç ”ç©¶å±•ç¤ºäº†é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å¤Ÿé€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æœ‰æ•ˆåœ°é¢„æµ‹ç”±éšé©¬å°”å¯å¤«æ¨¡å‹ï¼ˆHMMsï¼‰ç”Ÿæˆçš„åºåˆ—ã€‚è¿™è¡¨æ˜LLMsåœ¨æ­ç¤ºå¤æ‚æ•°æ®ä¸­çš„éšè—ç»“æ„æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚æˆ‘ä»¬åœ¨å¤šç§åˆæˆHMMsä¸Šæµ‹è¯•äº†LLMsï¼Œå‘ç°å…¶é¢„æµ‹å‡†ç¡®æ€§æ¥è¿‘ç†è®ºæœ€ä¼˜å€¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†ä½¿ç”¨ICLä½œä¸ºå¤æ‚æ•°æ®è¯Šæ–­å·¥å…·çš„å®ç”¨æŒ‡å—ï¼Œå¹¶åœ¨çœŸå®çš„åŠ¨ç‰©å†³ç­–ä»»åŠ¡ä¸­å–å¾—äº†ä¸äººç±»ä¸“å®¶è®¾è®¡æ¨¡å‹ç›¸å½“çš„è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.07463",
            "title": "CCI4.0: A Bilingual Pretraining Dataset for Enhancing Reasoning in Large\n  Language Models",
            "url": "https://huggingface.co/papers/2506.07463",
            "abstract": "A large-scale bilingual pre-training dataset, CCI4.0, enhances data quality and diverse reasoning patterns for language models, leading to improved performance in downstream tasks like math and code reflection.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce CCI4.0, a large-scale bilingual pre-training dataset engineered for superior data quality and diverse human-like reasoning trajectory. CCI4.0 occupies roughly 35 TB of disk space and comprises two sub-datasets: CCI4.0-M2-Base and CCI4.0-M2-CoT. CCI4.0-M2-Base combines a 5.2 TB carefully curated Chinese web corpus, a 22.5 TB English subset from Nemotron-CC, and diverse sources from math, wiki, arxiv, and code. Although these data are mostly sourced from well-processed datasets, the quality standards of various domains are dynamic and require extensive expert experience and labor to process. So, we propose a novel pipeline justifying data quality mainly based on models through two-stage deduplication, multiclassifier quality scoring, and domain-aware fluency filtering. We extract 4.5 billion pieces of CoT(Chain-of-Thought) templates, named CCI4.0-M2-CoT. Differing from the distillation of CoT from larger models, our proposed staged CoT extraction exemplifies diverse reasoning patterns and significantly decreases the possibility of hallucination. Empirical evaluations demonstrate that LLMs pre-trained in CCI4.0 benefit from cleaner, more reliable training signals, yielding consistent improvements in downstream tasks, especially in math and code reflection tasks. Our results underscore the critical role of rigorous data curation and human thinking templates in advancing LLM performance, shedding some light on automatically processing pretraining corpora.",
            "score": 7,
            "issue_id": 4209,
            "pub_date": "2025-06-09",
            "pub_date_card": {
                "ru": "9 Ğ¸ÑĞ½Ñ",
                "en": "June 9",
                "zh": "6æœˆ9æ—¥"
            },
            "hash": "219a3c7d2fbb1e22",
            "authors": [
                "Guang Liu",
                "Liangdong Wang",
                "Jijie Li",
                "Yang Yu",
                "Yao Xu",
                "Jiabei Chen",
                "Yu Bai",
                "Feng Liao",
                "Yonghua Lin"
            ],
            "affiliations": [
                "baai.ac.cn"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.07463.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#data",
                    "#dataset",
                    "#reasoning",
                    "#transfer_learning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "CCI4.0: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "CCI4.0 - ÑÑ‚Ğ¾ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ñ€Ğ¾Ğ´Ğµ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¸ Ğ¸ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸ ĞºĞ¾Ğ´Ğ°. ĞĞ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… Ğ¿Ğ¾Ğ´Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ²: CCI4.0-M2-Base Ğ¸ CCI4.0-M2-CoT, Ğ¾Ğ±Ñ‰Ğ¸Ğ¼ Ğ¾Ğ±ÑŠĞµĞ¼Ğ¾Ğ¼ Ğ¾ĞºĞ¾Ğ»Ğ¾ 35 Ğ¢Ğ‘. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ğ´ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚Ğ¾Ğ², Ğ¾Ñ†ĞµĞ½ĞºÑƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ° Ğ¸ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ğ¼."
                },
                "en": {
                    "title": "Enhancing Language Models with CCI4.0: Quality Data for Better Reasoning",
                    "desc": "The paper presents CCI4.0, a large-scale bilingual pre-training dataset designed to improve the quality of data and enhance reasoning capabilities in language models. It consists of two main sub-datasets, CCI4.0-M2-Base and CCI4.0-M2-CoT, which together provide a diverse range of high-quality data from various domains. The authors introduce a novel data curation pipeline that includes deduplication, quality scoring, and fluency filtering to ensure the reliability of the training data. Empirical results show that language models trained on CCI4.0 exhibit significant performance improvements in tasks such as mathematics and code reflection, highlighting the importance of quality data in machine learning."
                },
                "zh": {
                    "title": "æå‡è¯­è¨€æ¨¡å‹æ€§èƒ½çš„åŒè¯­æ•°æ®é›†CCI4.0",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºCCI4.0çš„å¤§è§„æ¨¡åŒè¯­é¢„è®­ç»ƒæ•°æ®é›†ï¼Œæ—¨åœ¨æé«˜æ•°æ®è´¨é‡å’Œå¤šæ ·åŒ–çš„æ¨ç†æ¨¡å¼ã€‚CCI4.0åŒ…å«çº¦35 TBçš„æ•°æ®ï¼Œåˆ†ä¸ºä¸¤ä¸ªå­æ•°æ®é›†ï¼šCCI4.0-M2-Baseå’ŒCCI4.0-M2-CoTã€‚é€šè¿‡ä¸¤é˜¶æ®µå»é‡ã€å¤šåˆ†ç±»å™¨è´¨é‡è¯„åˆ†å’Œé¢†åŸŸæ„ŸçŸ¥æµç•…æ€§è¿‡æ»¤ç­‰æ–°é¢–æµç¨‹ï¼Œç¡®ä¿äº†æ•°æ®çš„é«˜è´¨é‡ã€‚å®éªŒè¯æ˜ï¼Œä½¿ç”¨CCI4.0é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦å’Œä»£ç åå°„ç­‰ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.07434",
            "title": "Well Begun is Half Done: Low-resource Preference Alignment by\n  Weak-to-Strong Decoding",
            "url": "https://huggingface.co/papers/2506.07434",
            "abstract": "A novel Weak-to-Strong Decoding (WSD) framework enhances the alignment of large language models using a small aligned model to draft responses initially, improving alignment and performance without degrading downstream task performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) require alignment with human preferences to avoid generating offensive, false, or meaningless content. Recently, low-resource methods for LLM alignment have been popular, while still facing challenges in obtaining both high-quality and aligned content. Motivated by the observation that the difficulty of generating aligned responses is concentrated at the beginning of decoding, we propose a novel framework, Weak-to-Strong Decoding (WSD), to enhance the alignment ability of base models by the guidance of a small aligned model. The small model first drafts well-aligned beginnings, followed by the large base model to continue the rest, controlled by a well-designed auto-switch mechanism. We also collect a new dataset, GenerAlign, to fine-tune a small-sized Pilot-3B as the draft model, which effectively enhances different base models under the WSD framework to outperform all baseline methods, while avoiding degradation on downstream tasks, termed as the alignment tax. Extensive experiments are further conducted to examine the impact of different settings and time efficiency, as well as analyses on the intrinsic mechanisms of WSD in depth.",
            "score": 7,
            "issue_id": 4208,
            "pub_date": "2025-06-09",
            "pub_date_card": {
                "ru": "9 Ğ¸ÑĞ½Ñ",
                "en": "June 9",
                "zh": "6æœˆ9æ—¥"
            },
            "hash": "11d72d8081b838d5",
            "authors": [
                "Feifan Song",
                "Shaohang Wei",
                "Wen Luo",
                "Yuxuan Fan",
                "Tianyu Liu",
                "Guoyin Wang",
                "Houfeng Wang"
            ],
            "affiliations": [
                "State Key Laboratory of Multimedia Information Processing, School of Computer Science Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.07434.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#dataset",
                    "#training",
                    "#rlhf"
                ],
                "emoji": "ğŸ”€",
                "ru": {
                    "title": "Ğ¡Ğ»Ğ°Ğ±Ñ‹Ğ¹ ÑÑ‚Ğ°Ñ€Ñ‚, ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ„Ğ¸Ğ½Ğ¸Ñˆ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. ĞœĞµÑ‚Ğ¾Ğ´ Weak-to-Strong Decoding (WSD) Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ°Ğ»ĞµĞ½ÑŒĞºÑƒÑ Ğ²Ñ‹Ñ€Ğ¾Ğ²Ğ½ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°, Ğ¿Ğ¾ÑĞ»Ğµ Ñ‡ĞµĞ³Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ±Ñ€Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ GenerAlign Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Pilot-3B, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… WSD. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ±ĞµĞ· ÑƒÑ…ÑƒĞ´ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "Enhancing LLM Alignment with Weak-to-Strong Decoding",
                    "desc": "The paper introduces a new framework called Weak-to-Strong Decoding (WSD) that improves the alignment of large language models (LLMs) with human preferences. It uses a smaller, aligned model to generate the initial part of responses, which helps guide the larger model in producing better outputs. This approach addresses the challenge of generating high-quality and aligned content without negatively impacting the performance on other tasks, known as the alignment tax. The authors also present a new dataset, GenerAlign, which is used to fine-tune the smaller model, leading to superior results compared to existing methods."
                },
                "zh": {
                    "title": "å¼±åˆ°å¼ºè§£ç ï¼šæå‡è¯­è¨€æ¨¡å‹å¯¹é½èƒ½åŠ›çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¼±åˆ°å¼ºè§£ç æ¡†æ¶ï¼ˆWSDï¼‰ï¼Œæ—¨åœ¨é€šè¿‡å°å‹å¯¹é½æ¨¡å‹çš„æŒ‡å¯¼æ¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¯¹é½èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é¦–å…ˆç”±å°æ¨¡å‹è‰æ‹Ÿå‡ºé«˜è´¨é‡çš„å¯¹é½å¼€å¤´ï¼Œç„¶åç”±å¤§å‹åŸºç¡€æ¨¡å‹ç»§ç»­ç”Ÿæˆåç»­å†…å®¹ã€‚é€šè¿‡è®¾è®¡è‰¯å¥½çš„è‡ªåŠ¨åˆ‡æ¢æœºåˆ¶ï¼ŒWSDèƒ½å¤Ÿåœ¨ä¸é™ä½ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—æé«˜æ¨¡å‹çš„å¯¹é½æ•ˆæœã€‚æˆ‘ä»¬è¿˜æ”¶é›†äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†GenerAlignï¼Œä»¥å¾®è°ƒå°å‹Pilot-3Bæ¨¡å‹ï¼Œå®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨å¤šç§åŸºçº¿æ–¹æ³•ä¸­è¡¨ç°ä¼˜è¶Šã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.06941",
            "title": "The Illusion of Thinking: Understanding the Strengths and Limitations of\n  Reasoning Models via the Lens of Problem Complexity",
            "url": "https://huggingface.co/papers/2506.06941",
            "abstract": "Large Reasoning Models (LRMs) undergo accuracy collapse at higher complexities and exhibit unique performance scaling behaviors compared to standard LLMs, with failures in exact computation and inconsistent reasoning across scales.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent generations of language models have introduced Large Reasoning Models (LRMs) that generate detailed thinking processes before providing answers. While these models demonstrate improved performance on reasoning benchmarks, their fundamental capabilities, scaling properties, and limitations remain insufficiently understood. Current evaluations primarily focus on established math and coding benchmarks, emphasizing final answer accuracy. However, this evaluation paradigm often suffers from contamination and does not provide insights into the reasoning traces. In this work, we systematically investigate these gaps with the help of controllable puzzle environments that allow precise manipulation of complexity while maintaining consistent logical structures. This setup enables the analysis of not only final answers but also the internal reasoning traces, offering insights into how LRMs think. Through extensive experiments, we show that LRMs face a complete accuracy collapse beyond certain complexities. Moreover, they exhibit a counterintuitive scaling limit: their reasoning effort increases with problem complexity up to a point, then declines despite having remaining token budget. By comparing LRMs with their standard LLM counterparts under same inference compute, we identify three performance regimes: (1) low-complexity tasks where standard models outperform LRMs, (2) medium-complexity tasks where LRMs demonstrates advantage, and (3) high-complexity tasks where both models face complete collapse. We found that LRMs have limitations in exact computation: they fail to use explicit algorithms and reason inconsistently across scales. We also investigate the reasoning traces in more depth, studying the patterns of explored solutions and analyzing the models' computational behavior, shedding light on their strengths, limitations, and raising questions about their reasoning capabilities.",
            "score": 7,
            "issue_id": 4208,
            "pub_date": "2025-06-07",
            "pub_date_card": {
                "ru": "7 Ğ¸ÑĞ½Ñ",
                "en": "June 7",
                "zh": "6æœˆ7æ—¥"
            },
            "hash": "6f3991ea3357f456",
            "authors": [
                "Parshin Shojaee",
                "Iman Mirzadeh",
                "Keivan Alizadeh",
                "Maxwell Horton",
                "Samy Bengio",
                "Mehrdad Farajtabar"
            ],
            "affiliations": [
                "Apple"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.06941.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#training",
                    "#benchmark",
                    "#reasoning",
                    "#interpretability"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹: Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (LRM) Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ñ€ĞµĞ·ĞºĞ¾Ğµ Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, LRM Ğ¸Ğ¼ĞµÑÑ‚ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾ÑĞ²Ğ»ÑÑÑ‚ Ğ½ĞµĞ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… Ğ¿Ñ€Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ LRM Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ¸Ñ… ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ ÑĞ»Ğ°Ğ±Ñ‹Ğµ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñ‹, Ğ¿Ğ¾Ğ´Ğ½Ğ¸Ğ¼Ğ°Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¾ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑÑ… ÑÑ‚Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Understanding the Limits of Large Reasoning Models in Complex Tasks",
                    "desc": "This paper explores the performance of Large Reasoning Models (LRMs) in relation to their complexity and reasoning capabilities. It reveals that LRMs experience accuracy collapse when faced with high-complexity tasks, which is a significant limitation compared to standard language models (LLMs). The study introduces a controlled environment to analyze both the final answers and the internal reasoning processes of LRMs, highlighting their inconsistent reasoning and failure to perform exact computations. The findings categorize performance into three regimes based on task complexity, providing insights into the strengths and weaknesses of LRMs in reasoning tasks."
                },
                "zh": {
                    "title": "å¤§å‹æ¨ç†æ¨¡å‹çš„å¤æ‚æ€§æŒ‘æˆ˜",
                    "desc": "å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨å¤„ç†æ›´å¤æ‚çš„é—®é¢˜æ—¶ä¼šå‡ºç°å‡†ç¡®æ€§å´©æºƒï¼Œå¹¶ä¸”ä¸æ ‡å‡†è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç›¸æ¯”ï¼Œå®ƒä»¬çš„æ€§èƒ½æ‰©å±•è¡Œä¸ºç‹¬ç‰¹ã€‚å°½ç®¡LRMsåœ¨æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬çš„åŸºæœ¬èƒ½åŠ›å’Œå±€é™æ€§ä»ç„¶ä¸å¤Ÿæ¸…æ¥šã€‚é€šè¿‡å¯æ§çš„éš¾é¢˜ç¯å¢ƒï¼Œæˆ‘ä»¬èƒ½å¤Ÿç²¾ç¡®æ“æ§å¤æ‚æ€§ï¼Œå¹¶åˆ†æLRMsçš„å†…éƒ¨æ¨ç†è¿‡ç¨‹ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒLRMsåœ¨é«˜å¤æ‚æ€§ä»»åŠ¡ä¸­ä¼šå®Œå…¨å´©æºƒï¼Œå¹¶ä¸”åœ¨é—®é¢˜å¤æ‚æ€§å¢åŠ æ—¶ï¼Œå®ƒä»¬çš„æ¨ç†åŠªåŠ›ä¼šå…ˆå¢åŠ åå‡å°‘ï¼Œæ˜¾ç¤ºå‡ºåç›´è§‰çš„æ‰©å±•é™åˆ¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.08012",
            "title": "GUI-Reflection: Empowering Multimodal GUI Models with Self-Reflection\n  Behavior",
            "url": "https://huggingface.co/papers/2506.08012",
            "abstract": "A novel framework, GUI-Reflection, integrates self-reflection and error correction into multimodal GUI models through specialized training stages, enabling more robust and intelligent automation.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) have shown great potential in revolutionizing Graphical User Interface (GUI) automation. However, existing GUI models mostly rely on learning from nearly error-free offline trajectories, thus lacking reflection and error recovery capabilities. To bridge this gap, we propose GUI-Reflection, a novel framework that explicitly integrates self-reflection and error correction capabilities into end-to-end multimodal GUI models throughout dedicated training stages: GUI-specific pre-training, offline supervised fine-tuning (SFT), and online reflection tuning. GUI-reflection enables self-reflection behavior emergence with fully automated data generation and learning processes without requiring any human annotation. Specifically, 1) we first propose scalable data pipelines to automatically construct reflection and error correction data from existing successful trajectories. While existing GUI models mainly focus on grounding and UI understanding ability, we propose the GUI-Reflection Task Suite to learn and evaluate reflection-oriented abilities explicitly. 2) Furthermore, we built a diverse and efficient environment for online training and data collection of GUI models on mobile devices. 3) We also present an iterative online reflection tuning algorithm leveraging the proposed environment, enabling the model to continuously enhance its reflection and error correction abilities. Our framework equips GUI agents with self-reflection and correction capabilities, paving the way for more robust, adaptable, and intelligent GUI automation, with all data, models, environments, and tools to be released publicly.",
            "score": 6,
            "issue_id": 4208,
            "pub_date": "2025-06-09",
            "pub_date_card": {
                "ru": "9 Ğ¸ÑĞ½Ñ",
                "en": "June 9",
                "zh": "6æœˆ9æ—¥"
            },
            "hash": "8052bb442adf3c53",
            "authors": [
                "Penghao Wu",
                "Shengnan Ma",
                "Bo Wang",
                "Jiaheng Yu",
                "Lewei Lu",
                "Ziwei Liu"
            ],
            "affiliations": [
                "S-Lab, Nanyang Technological University",
                "SenseTime Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.08012.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#dataset",
                    "#agents",
                    "#training",
                    "#open_source",
                    "#multimodal"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¸ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² Ğ˜Ğ˜ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ GUI-Reflection, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ°Ğ¼Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¸ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. Ğ­Ñ‚Ğ° ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑÑ‚Ğ°Ğ¿Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ GUI, Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸. GUI-Reflection Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ·Ğ°Ğ´Ğ°Ñ‡ GUI-Reflection Task Suite Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğº Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ÑÑ€ĞµĞ´Ñƒ Ğ´Ğ»Ñ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…."
                },
                "en": {
                    "title": "Empowering GUI Automation with Self-Reflection and Error Correction",
                    "desc": "The paper introduces GUI-Reflection, a new framework that enhances multimodal GUI models by incorporating self-reflection and error correction during training. This approach addresses the limitations of existing models that primarily learn from error-free data, enabling them to recover from mistakes and improve over time. The framework includes specialized training stages such as pre-training, supervised fine-tuning, and online reflection tuning, allowing models to learn from their own errors without human intervention. By automating data generation and focusing on reflection-oriented tasks, GUI-Reflection aims to create more robust and intelligent automation for graphical user interfaces."
                },
                "zh": {
                    "title": "è‡ªæˆ‘åæ€ä¸é”™è¯¯çº æ­£ï¼Œæå‡GUIè‡ªåŠ¨åŒ–æ™ºèƒ½",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ¡†æ¶GUI-Reflectionï¼Œå°†è‡ªæˆ‘åæ€å’Œé”™è¯¯çº æ­£èƒ½åŠ›æ•´åˆåˆ°å¤šæ¨¡æ€å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰æ¨¡å‹ä¸­ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸“é—¨çš„è®­ç»ƒé˜¶æ®µï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨è‡ªåŠ¨åŒ–è¿‡ç¨‹ä¸­æ›´å…·é²æ£’æ€§å’Œæ™ºèƒ½æ€§ã€‚æˆ‘ä»¬è®¾è®¡äº†å¯æ‰©å±•çš„æ•°æ®ç®¡é“ï¼Œè‡ªåŠ¨ç”Ÿæˆåæ€å’Œé”™è¯¯çº æ­£çš„æ•°æ®ï¼Œå¹¶æå‡ºäº†GUI-Reflectionä»»åŠ¡å¥—ä»¶æ¥è¯„ä¼°è¿™äº›èƒ½åŠ›ã€‚æœ€ç»ˆï¼Œæ¡†æ¶ä½¿å¾—GUIä»£ç†å…·å¤‡è‡ªæˆ‘åæ€å’Œçº æ­£èƒ½åŠ›ï¼Œä¸ºæ›´æ™ºèƒ½çš„GUIè‡ªåŠ¨åŒ–å¥ å®šåŸºç¡€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.07309",
            "title": "ConfQA: Answer Only If You Are Confident",
            "url": "https://huggingface.co/papers/2506.07309",
            "abstract": "ConfQA fine-tuning strategy reduces factual statement hallucination in LLMs by 80%, using a dampening prompt and factual statements from knowledge graphs to improve confidence calibration and knowledge selection.  \t\t\t\t\tAI-generated summary \t\t\t\t Can we teach Large Language Models (LLMs) to refrain from hallucinating factual statements? In this paper we present a fine-tuning strategy that we call ConfQA, which can reduce hallucination rate from 20-40% to under 5% across multiple factuality benchmarks. The core idea is simple: when the LLM answers a question correctly, it is trained to continue with the answer; otherwise, it is trained to admit \"I am unsure\". But there are two key factors that make the training highly effective. First, we introduce a dampening prompt \"answer only if you are confident\" to explicitly guide the behavior, without which hallucination remains high as 15%-25%. Second, we leverage simple factual statements, specifically attribute values from knowledge graphs, to help LLMs calibrate the confidence, resulting in robust generalization across domains and question types. Building on this insight, we propose the Dual Neural Knowledge framework, which seamlessly select between internally parameterized neural knowledge and externally recorded symbolic knowledge based on ConfQA's confidence. The framework enables potential accuracy gains to beyond 95%, while reducing unnecessary external retrievals by over 30%.",
            "score": 6,
            "issue_id": 4211,
            "pub_date": "2025-06-08",
            "pub_date_card": {
                "ru": "8 Ğ¸ÑĞ½Ñ",
                "en": "June 8",
                "zh": "6æœˆ8æ—¥"
            },
            "hash": "8a4581c11bf35360",
            "authors": [
                "Yin Huang",
                "Yifan Ethan Xu",
                "Kai Sun",
                "Vera Yan",
                "Alicia Sun",
                "Haidar Khan",
                "Jimmy Nguyen",
                "Mohammad Kachuee",
                "Zhaojiang Lin",
                "Yue Liu",
                "Aaron Colak",
                "Anuj Kumar",
                "Wen-tau Yih",
                "Xin Luna Dong"
            ],
            "affiliations": [
                "FAIR at Meta",
                "Meta Reality Labs"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.07309.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#interpretability",
                    "#hallucinations",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ConfQA: Ğ¾Ğ±ÑƒĞ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² LLM Ñ‡ĞµÑ€ĞµĞ· ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ğµ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ConfQA, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM) Ğ¿Ñ€Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ñ… Ğ½Ğ° Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼Ğ¸ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ ÑĞ²Ğ»ÑÑÑ‚ÑÑ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½ĞµÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¸ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ñ 20-40% Ğ´Ğ¾ Ğ¼ĞµĞ½ĞµĞµ 5% Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Dual Neural Knowledge Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¼Ğ¸ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "ConfQA: Reducing Hallucinations in LLMs with Confidence Calibration",
                    "desc": "This paper introduces a fine-tuning strategy called ConfQA that significantly reduces the occurrence of factual statement hallucinations in Large Language Models (LLMs) by up to 80%. The approach involves training the model to confidently provide answers when it knows them and to express uncertainty when it does not. Key components of this strategy include a dampening prompt that encourages the model to answer only when confident, and the use of factual statements from knowledge graphs to enhance the model's confidence calibration. Additionally, the proposed Dual Neural Knowledge framework allows the model to effectively choose between internal neural knowledge and external symbolic knowledge based on its confidence level, leading to improved accuracy and reduced reliance on external data retrieval."
                },
                "zh": {
                    "title": "å‡å°‘å¹»è§‰ï¼Œæé«˜å‡†ç¡®æ€§ï¼",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºConfQAçš„å¾®è°ƒç­–ç•¥ï¼Œæ—¨åœ¨å‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„äº‹å®é™ˆè¿°å¹»è§‰ç°è±¡ã€‚é€šè¿‡ä½¿ç”¨å‡å¼±æç¤ºå’ŒçŸ¥è¯†å›¾è°±ä¸­çš„äº‹å®é™ˆè¿°ï¼Œè¯¥ç­–ç•¥èƒ½å¤Ÿå°†å¹»è§‰ç‡ä»20-40%é™ä½åˆ°5%ä»¥ä¸‹ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯ï¼Œå½“LLMæ­£ç¡®å›ç­”é—®é¢˜æ—¶ï¼Œç»§ç»­ç»™å‡ºç­”æ¡ˆï¼›å¦åˆ™ï¼Œæ‰¿è®¤â€œä¸ç¡®å®šâ€ã€‚æ­¤å¤–ï¼ŒConfQAæ¡†æ¶é€šè¿‡é€‰æ‹©å†…éƒ¨å‚æ•°åŒ–çš„ç¥ç»çŸ¥è¯†å’Œå¤–éƒ¨è®°å½•çš„ç¬¦å·çŸ¥è¯†ï¼Œè¿›ä¸€æ­¥æé«˜äº†æ¨¡å‹çš„å‡†ç¡®æ€§å’ŒçŸ¥è¯†é€‰æ‹©èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.08010",
            "title": "Vision Transformers Don't Need Trained Registers",
            "url": "https://huggingface.co/papers/2506.08010",
            "abstract": "A training-free method shifts high-norm activations in Vision Transformers to an untrained token, enhancing attention maps and performance across visual tasks, and improving interpretability in vision-language models.  \t\t\t\t\tAI-generated summary \t\t\t\t We investigate the mechanism underlying a previously identified phenomenon in Vision Transformers -- the emergence of high-norm tokens that lead to noisy attention maps. We observe that in multiple models (e.g., CLIP, DINOv2), a sparse set of neurons is responsible for concentrating high-norm activations on outlier tokens, leading to irregular attention patterns and degrading downstream visual processing. While the existing solution for removing these outliers involves retraining models from scratch with additional learned register tokens, we use our findings to create a training-free approach to mitigate these artifacts. By shifting the high-norm activations from our discovered register neurons into an additional untrained token, we can mimic the effect of register tokens on a model already trained without registers. We demonstrate that our method produces cleaner attention and feature maps, enhances performance over base models across multiple downstream visual tasks, and achieves results comparable to models explicitly trained with register tokens. We then extend test-time registers to off-the-shelf vision-language models to improve their interpretability. Our results suggest that test-time registers effectively take on the role of register tokens at test-time, offering a training-free solution for any pre-trained model released without them.",
            "score": 5,
            "issue_id": 4212,
            "pub_date": "2025-06-09",
            "pub_date_card": {
                "ru": "9 Ğ¸ÑĞ½Ñ",
                "en": "June 9",
                "zh": "6æœˆ9æ—¥"
            },
            "hash": "d833304e26f5d1ee",
            "authors": [
                "Nick Jiang",
                "Amil Dravid",
                "Alexei Efros",
                "Yossi Gandelsman"
            ],
            "affiliations": [
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.08010.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#training",
                    "#interpretability"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ… Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ…, Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ² ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ½Ğ¾Ñ€Ğ¼Ğ¾Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ÑˆÑƒĞ¼Ğ½Ñ‹Ğ¼ ĞºĞ°Ñ€Ñ‚Ğ°Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµĞ¼ĞµÑ‰Ğ°ĞµÑ‚ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ½Ğ¾Ñ€Ğ¼Ğ¾Ğ¹ Ğ² Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ½ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼ Ğ¿Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸, Ğ½Ğ¾ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Vision Transformers with Training-Free Token Shifting",
                    "desc": "This paper presents a novel training-free method to improve Vision Transformers by addressing the issue of high-norm activations that lead to noisy attention maps. The authors identify that certain neurons concentrate these high-norm activations on outlier tokens, which disrupts attention patterns and affects visual task performance. Instead of retraining models with additional learned tokens, they propose shifting these activations to an untrained token, effectively simulating the benefits of register tokens without the need for retraining. Their approach not only enhances attention and feature maps but also boosts performance across various visual tasks, making it applicable to existing vision-language models for better interpretability."
                },
                "zh": {
                    "title": "æ— è®­ç»ƒæ–¹æ³•æå‡è§†è§‰æ¨¡å‹æ€§èƒ½ä¸å¯è§£é‡Šæ€§",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— è®­ç»ƒçš„æ–¹æ³•ï¼Œé€šè¿‡å°†é«˜èŒƒæ•°æ¿€æ´»è½¬ç§»åˆ°æœªè®­ç»ƒçš„æ ‡è®°ä¸Šï¼Œæ¥å¢å¼ºè§†è§‰å˜æ¢å™¨ä¸­çš„æ³¨æ„åŠ›å›¾å’Œæ€§èƒ½ã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨å¤šä¸ªæ¨¡å‹ä¸­ï¼Œä¸€å°éƒ¨åˆ†ç¥ç»å…ƒè´Ÿè´£å°†é«˜èŒƒæ•°æ¿€æ´»é›†ä¸­åœ¨å¼‚å¸¸æ ‡è®°ä¸Šï¼Œå¯¼è‡´æ³¨æ„åŠ›æ¨¡å¼ä¸è§„åˆ™ï¼Œå½±å“è§†è§‰å¤„ç†ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹ï¼Œè€Œæ˜¯åˆ©ç”¨å‘ç°çš„æ³¨å†Œç¥ç»å…ƒå°†é«˜èŒƒæ•°æ¿€æ´»è½¬ç§»åˆ°é¢å¤–çš„æœªè®­ç»ƒæ ‡è®°ä¸Šï¼Œä»è€Œæ”¹å–„æ³¨æ„åŠ›å’Œç‰¹å¾å›¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªè§†è§‰ä»»åŠ¡ä¸­æå‡äº†æ€§èƒ½ï¼Œå¹¶ä¸”ä¸æ˜¾å¼è®­ç»ƒçš„æ³¨å†Œæ ‡è®°æ¨¡å‹çš„ç»“æœç›¸å½“ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.08006",
            "title": "Dreamland: Controllable World Creation with Simulator and Generative\n  Models",
            "url": "https://huggingface.co/papers/2506.08006",
            "abstract": "Dreamland integrates physics simulators and large-scale pretrained generative models for controllable and photorealistic video world generation, improving image quality and controllability.  \t\t\t\t\tAI-generated summary \t\t\t\t Large-scale video generative models can synthesize diverse and realistic visual content for dynamic world creation, but they often lack element-wise controllability, hindering their use in editing scenes and training embodied AI agents. We propose Dreamland, a hybrid world generation framework combining the granular control of a physics-based simulator and the photorealistic content output of large-scale pretrained generative models. In particular, we design a layered world abstraction that encodes both pixel-level and object-level semantics and geometry as an intermediate representation to bridge the simulator and the generative model. This approach enhances controllability, minimizes adaptation cost through early alignment with real-world distributions, and supports off-the-shelf use of existing and future pretrained generative models. We further construct a D3Sim dataset to facilitate the training and evaluation of hybrid generation pipelines. Experiments demonstrate that Dreamland outperforms existing baselines with 50.8% improved image quality, 17.9% stronger controllability, and has great potential to enhance embodied agent training. Code and data will be made available.",
            "score": 4,
            "issue_id": 4210,
            "pub_date": "2025-06-09",
            "pub_date_card": {
                "ru": "9 Ğ¸ÑĞ½Ñ",
                "en": "June 9",
                "zh": "6æœˆ9æ—¥"
            },
            "hash": "e9dd998a336df034",
            "authors": [
                "Sicheng Mo",
                "Ziyang Leng",
                "Leon Liu",
                "Weizhen Wang",
                "Honglin He",
                "Bolei Zhou"
            ],
            "affiliations": [
                "University of California, Los Angeles"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.08006.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#dataset",
                    "#games",
                    "#video",
                    "#agents"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ¤Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¸Ñ€Ñ‹ Ğ¿Ğ¾Ğ´ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼",
                    "desc": "Dreamland - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¸Ñ€Ğ¾Ğ², Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ñ‹ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²ÑƒÑ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ñ Ğ¼Ğ¸Ñ€Ğ°, ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‰ÑƒÑ ĞºĞ°Ğº Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½ÑƒÑ, Ñ‚Ğ°Ğº Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ½ÑƒÑ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºÑƒ Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Dreamland Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ ÑÑ‚ĞµĞ¿ĞµĞ½Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ."
                },
                "en": {
                    "title": "Dreamland: Bridging Physics and Generative Models for Enhanced Video World Creation",
                    "desc": "Dreamland is a novel framework that combines physics simulators with large-scale pretrained generative models to create realistic and controllable video worlds. It addresses the limitations of existing video generative models by providing element-wise controllability, which is essential for editing scenes and training AI agents. The framework uses a layered world abstraction that captures both pixel-level and object-level details, allowing for better integration between the simulator and the generative model. Experiments show that Dreamland significantly improves image quality and controllability, making it a promising tool for future AI applications."
                },
                "zh": {
                    "title": "æ¢¦å¢ƒï¼šå¯æ§çš„çœŸå®è§†é¢‘ä¸–ç•Œç”Ÿæˆ",
                    "desc": "Dreamlandæ˜¯ä¸€ä¸ªç»“åˆç‰©ç†æ¨¡æ‹Ÿå™¨å’Œå¤§è§„æ¨¡é¢„è®­ç»ƒç”Ÿæˆæ¨¡å‹çš„æ··åˆä¸–ç•Œç”Ÿæˆæ¡†æ¶ã€‚å®ƒé€šè¿‡è®¾è®¡åˆ†å±‚ä¸–ç•ŒæŠ½è±¡ï¼Œç¼–ç åƒç´ çº§å’Œç‰©ä½“çº§çš„è¯­ä¹‰ä¸å‡ ä½•ä¿¡æ¯ï¼Œä»è€Œæé«˜äº†å›¾åƒè´¨é‡å’Œå¯æ§æ€§ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å®ç°æ›´ç»†ç²’åº¦çš„æ§åˆ¶ï¼Œå‡å°‘é€‚åº”æˆæœ¬ï¼Œå¹¶æ”¯æŒç°æœ‰å’Œæœªæ¥ç”Ÿæˆæ¨¡å‹çš„ä½¿ç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDreamlandåœ¨å›¾åƒè´¨é‡å’Œå¯æ§æ€§æ–¹é¢å‡æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼Œå…·æœ‰æå‡æ™ºèƒ½ä½“è®­ç»ƒçš„å·¨å¤§æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.06266",
            "title": "Cartridges: Lightweight and general-purpose long context representations\n  via self-study",
            "url": "https://huggingface.co/papers/2506.06266",
            "abstract": "Cartridges trained with self-study replicate in-context learning functionality while reducing memory and increasing throughput, and enable longer context lengths and composability at inference.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models are often used to answer queries grounded in large text corpora (e.g. codebases, legal documents, or chat histories) by placing the entire corpus in the context window and leveraging in-context learning (ICL). Although current models support contexts of 100K-1M tokens, this setup is costly to serve because the memory consumption of the KV cache scales with input length. We explore an alternative: training a smaller KV cache offline on each corpus. At inference time, we load this trained KV cache, which we call a Cartridge, and decode a response. Critically, the cost of training a Cartridge can be amortized across all the queries referencing the same corpus. However, we find that the naive approach of training the Cartridge with next-token prediction on the corpus is not competitive with ICL. Instead, we propose self-study, a training recipe in which we generate synthetic conversations about the corpus and train the Cartridge with a context-distillation objective. We find that Cartridges trained with self-study replicate the functionality of ICL, while being significantly cheaper to serve. On challenging long-context benchmarks, Cartridges trained with self-study match ICL performance while using 38.6x less memory and enabling 26.4x higher throughput. Self-study also extends the model's effective context length (e.g. from 128k to 484k tokens on MTOB) and surprisingly, leads to Cartridges that can be composed at inference time without retraining.",
            "score": 4,
            "issue_id": 4211,
            "pub_date": "2025-06-06",
            "pub_date_card": {
                "ru": "6 Ğ¸ÑĞ½Ñ",
                "en": "June 6",
                "zh": "6æœˆ6æ—¥"
            },
            "hash": "0d49d6022b9d6786",
            "authors": [
                "Sabri Eyuboglu",
                "Ryan Ehrlich",
                "Simran Arora",
                "Neel Guha",
                "Dylan Zinsley",
                "Emily Liu",
                "Will Tennien",
                "Atri Rudra",
                "James Zou",
                "Azalia Mirhoseini",
                "Christopher Re"
            ],
            "affiliations": [
                "Caltech",
                "Stanford University",
                "University at Buffalo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.06266.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#data",
                    "#inference",
                    "#synthetic",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ’¾",
                "ru": {
                    "title": "ĞšĞ°Ñ€Ñ‚Ñ€Ğ¸Ğ´Ğ¶Ğ¸: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ 'ĞºĞ°Ñ€Ñ‚Ñ€Ğ¸Ğ´Ğ¶Ğ°Ğ¼Ğ¸'. ĞšĞ°Ñ€Ñ‚Ñ€Ğ¸Ğ´Ğ¶Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ 'ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ ĞºĞ°Ñ€Ñ‚Ñ€Ğ¸Ğ´Ğ¶ĞµĞ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ¾Ğ², Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº ĞºĞ¾Ğ´Ğ¾Ğ²Ñ‹Ğµ Ğ±Ğ°Ğ·Ñ‹ Ğ¸Ğ»Ğ¸ ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹."
                },
                "en": {
                    "title": "Efficient In-Context Learning with Cartridges",
                    "desc": "This paper introduces a novel approach called Cartridges, which are trained using a method called self-study to enhance in-context learning (ICL) in large language models. By creating a smaller key-value (KV) cache for each text corpus, Cartridges significantly reduce memory usage and increase processing speed during inference. The self-study training method involves generating synthetic conversations to improve the Cartridge's performance, allowing it to match ICL capabilities while being more efficient. The results show that Cartridges can handle longer context lengths and can be composed without the need for retraining, making them a powerful tool for various applications."
                },
                "zh": {
                    "title": "è‡ªæˆ‘å­¦ä¹ ï¼šé«˜æ•ˆçš„ä¸Šä¸‹æ–‡å­¦ä¹ è§£å†³æ–¹æ¡ˆ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºCartridgeçš„æŠ€æœ¯ï¼Œé€šè¿‡è‡ªæˆ‘å­¦ä¹ è®­ç»ƒå°å‹KVç¼“å­˜ï¼Œä»¥å®ç°ä¸Šä¸‹æ–‡å­¦ä¹ åŠŸèƒ½ï¼ŒåŒæ—¶é™ä½å†…å­˜æ¶ˆè€—å¹¶æé«˜å¤„ç†é€Ÿåº¦ã€‚ä¼ ç»Ÿçš„å¤§å‹è¯­è¨€æ¨¡å‹éœ€è¦å°†æ•´ä¸ªæ–‡æœ¬è¯­æ–™æ”¾å…¥ä¸Šä¸‹æ–‡çª—å£ï¼Œå¯¼è‡´é«˜æ˜‚çš„å†…å­˜æˆæœ¬ï¼Œè€ŒCartridgeé€šè¿‡ç¦»çº¿è®­ç»ƒç¼“å­˜æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨è‡ªæˆ‘å­¦ä¹ çš„æ–¹æ³•è®­ç»ƒCartridgeï¼Œå¯ä»¥åœ¨ä¿æŒä¸ä¸Šä¸‹æ–‡å­¦ä¹ ç›¸ä¼¼çš„æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—å‡å°‘å†…å­˜ä½¿ç”¨å’Œæé«˜ååé‡ã€‚æœ€ç»ˆï¼ŒCartridgeä¸ä»…æ‰©å±•äº†æ¨¡å‹çš„æœ‰æ•ˆä¸Šä¸‹æ–‡é•¿åº¦ï¼Œè¿˜èƒ½åœ¨æ¨ç†æ—¶è¿›è¡Œç»„åˆï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.06006",
            "title": "Bootstrapping World Models from Dynamics Models in Multimodal Foundation\n  Models",
            "url": "https://huggingface.co/papers/2506.06006",
            "abstract": "Foundation models can be fine-tuned to develop dynamics models more easily than world models, with dynamics models aiding world model development through weak supervision and inference time verification, leading to state-of-the-art performance in action-centric image editing.  \t\t\t\t\tAI-generated summary \t\t\t\t To what extent do vision-and-language foundation models possess a realistic world model (observation times action rightarrow observation) and a dynamics model (observation times observation rightarrow action), when actions are expressed through language? While open-source foundation models struggle with both, we find that fine-tuning them to acquire a dynamics model through supervision is significantly easier than acquiring a world model. In turn, dynamics models can be used to bootstrap world models through two main strategies: 1) weakly supervised learning from synthetic data and 2) inference time verification. Firstly, the dynamics model can annotate actions for unlabelled pairs of video frame observations to expand the training data. We further propose a new objective, where image tokens in observation pairs are weighted by their importance, as predicted by a recognition model. Secondly, the dynamics models can assign rewards to multiple samples of the world model to score them, effectively guiding search at inference time. We evaluate the world models resulting from both strategies through the task of action-centric image editing on Aurora-Bench. Our best model achieves a performance competitive with state-of-the-art image editing models, improving on them by a margin of 15% on real-world subsets according to GPT4o-as-judge, and achieving the best average human evaluation across all subsets of Aurora-Bench.",
            "score": 4,
            "issue_id": 4217,
            "pub_date": "2025-06-06",
            "pub_date_card": {
                "ru": "6 Ğ¸ÑĞ½Ñ",
                "en": "June 6",
                "zh": "6æœˆ6æ—¥"
            },
            "hash": "fc75b0606885fd24",
            "authors": [
                "Yifu Qiu",
                "Yftah Ziser",
                "Anna Korhonen",
                "Shay B. Cohen",
                "Edoardo M. Ponti"
            ],
            "affiliations": [
                "Institute for Language, Cognition and Computation, University of Edinburgh",
                "Language Technology Lab, University of Cambridge",
                "NVIDIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.06006.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#cv",
                    "#training",
                    "#synthetic",
                    "#multimodal",
                    "#open_source"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ°Ğº ĞºĞ»ÑÑ‡ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ˜Ğ˜",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (foundation models) Ğ»ĞµĞ³Ñ‡Ğµ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ‡ĞµĞ¼ Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ÑÑ‚ Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· ÑĞ»Ğ°Ğ±Ñ‹Ğ¹ Ğ½Ğ°Ğ´Ğ·Ğ¾Ñ€ Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Ğ­Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Boosting Image Editing with Dynamics Models",
                    "desc": "This paper explores how foundation models can be fine-tuned to create dynamics models, which are easier to develop than world models. Dynamics models help in building world models by providing weak supervision and verifying actions during inference. The authors propose two strategies for using dynamics models: one involves using synthetic data to label actions in unannotated video frames, and the other uses these models to score and guide the search for better world models during inference. The results show that their approach significantly improves performance in action-centric image editing tasks, outperforming existing models by 15%."
                },
                "zh": {
                    "title": "åŠ¨æ€æ¨¡å‹åŠ©åŠ›ä¸–ç•Œæ¨¡å‹ï¼Œæå‡å›¾åƒç¼–è¾‘æ€§èƒ½",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†åŸºç¡€æ¨¡å‹åœ¨åŠ¨æ€æ¨¡å‹å’Œä¸–ç•Œæ¨¡å‹å¼€å‘ä¸­çš„åº”ç”¨ã€‚ç ”ç©¶å‘ç°ï¼Œé€šè¿‡å¾®è°ƒåŸºç¡€æ¨¡å‹æ¥è·å–åŠ¨æ€æ¨¡å‹æ¯”è·å–ä¸–ç•Œæ¨¡å‹è¦å®¹æ˜“å¾—å¤šã€‚åŠ¨æ€æ¨¡å‹å¯ä»¥é€šè¿‡å¼±ç›‘ç£å­¦ä¹ å’Œæ¨ç†æ—¶éªŒè¯æ¥è¾…åŠ©ä¸–ç•Œæ¨¡å‹çš„å¼€å‘ï¼Œä»è€Œåœ¨ä»¥åŠ¨ä½œä¸ºä¸­å¿ƒçš„å›¾åƒç¼–è¾‘ä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬çš„æœ€ä½³æ¨¡å‹åœ¨Aurora-Benchçš„çœŸå®ä¸–ç•Œå­é›†ä¸Šæ¯”ç°æœ‰çš„å›¾åƒç¼–è¾‘æ¨¡å‹æé«˜äº†15%çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.07848",
            "title": "PolyVivid: Vivid Multi-Subject Video Generation with Cross-Modal\n  Interaction and Enhancement",
            "url": "https://huggingface.co/papers/2506.07848",
            "abstract": "PolyVivid is a multi-subject video customization framework that uses text-image fusion, 3D-RoPE enhancement, attention-inherited identity injection, and MLLM-based data processing to ensure identity consistency and realistic video generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite recent advances in video generation, existing models still lack fine-grained controllability, especially for multi-subject customization with consistent identity and interaction. In this paper, we propose PolyVivid, a multi-subject video customization framework that enables flexible and identity-consistent generation. To establish accurate correspondences between subject images and textual entities, we design a VLLM-based text-image fusion module that embeds visual identities into the textual space for precise grounding. To further enhance identity preservation and subject interaction, we propose a 3D-RoPE-based enhancement module that enables structured bidirectional fusion between text and image embeddings. Moreover, we develop an attention-inherited identity injection module to effectively inject fused identity features into the video generation process, mitigating identity drift. Finally, we construct an MLLM-based data pipeline that combines MLLM-based grounding, segmentation, and a clique-based subject consolidation strategy to produce high-quality multi-subject data, effectively enhancing subject distinction and reducing ambiguity in downstream video generation. Extensive experiments demonstrate that PolyVivid achieves superior performance in identity fidelity, video realism, and subject alignment, outperforming existing open-source and commercial baselines.",
            "score": 2,
            "issue_id": 4214,
            "pub_date": "2025-06-09",
            "pub_date_card": {
                "ru": "9 Ğ¸ÑĞ½Ñ",
                "en": "June 9",
                "zh": "6æœˆ9æ—¥"
            },
            "hash": "e91b69e6dac8c694",
            "authors": [
                "Teng Hu",
                "Zhentao Yu",
                "Zhengguang Zhou",
                "Jiangning Zhang",
                "Yuan Zhou",
                "Qinglin Lu",
                "Ran Yi"
            ],
            "affiliations": [
                "Shanghai Jiao Tong University",
                "Tencent Hunyuan",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.07848.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#video",
                    "#open_source",
                    "#multimodal",
                    "#games",
                    "#optimization"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "PolyVivid: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ ĞºĞ°ÑÑ‚Ğ¾Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾Ğ¼ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¾Ğ²",
                    "desc": "PolyVivid - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ĞºĞ°ÑÑ‚Ğ¾Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ 3D-RoPE, Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ğµ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½Ğ°ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ MLLM. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾. PolyVivid Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ»ÑÑ‚ÑŒ Ğ³Ğ¸Ğ±ĞºÑƒÑ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¿Ğ¾ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ VLLM Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ 3D-RoPE Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "PolyVivid: Consistent Multi-Subject Video Generation Made Easy!",
                    "desc": "PolyVivid is a framework designed for creating videos with multiple subjects while maintaining their identities and interactions. It uses advanced techniques like text-image fusion to accurately link visual identities with textual descriptions. The framework also incorporates a 3D-RoPE enhancement module to improve how text and image data interact, ensuring that identities remain consistent throughout the video. By employing a multi-level language model (MLLM) for data processing, PolyVivid enhances the quality of video generation, achieving better identity fidelity and realism compared to existing models."
                },
                "zh": {
                    "title": "PolyVividï¼šå¤šä¸»ä½“è§†é¢‘å®šåˆ¶çš„æ–°çªç ´",
                    "desc": "PolyVividæ˜¯ä¸€ä¸ªå¤šä¸»ä½“è§†é¢‘å®šåˆ¶æ¡†æ¶ï¼Œåˆ©ç”¨æ–‡æœ¬-å›¾åƒèåˆã€3D-RoPEå¢å¼ºã€æ³¨æ„åŠ›ç»§æ‰¿èº«ä»½æ³¨å…¥å’ŒåŸºäºMLLMçš„æ•°æ®å¤„ç†ï¼Œç¡®ä¿èº«ä»½ä¸€è‡´æ€§å’ŒçœŸå®çš„è§†é¢‘ç”Ÿæˆã€‚è¯¥æ¡†æ¶è§£å†³äº†ç°æœ‰è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨å¤šä¸»ä½“å®šåˆ¶ä¸­ç¼ºä¹ç»†ç²’åº¦å¯æ§æ€§çš„é—®é¢˜ã€‚é€šè¿‡è®¾è®¡åŸºäºVLLMçš„æ–‡æœ¬-å›¾åƒèåˆæ¨¡å—ï¼ŒPolyVividèƒ½å¤Ÿå‡†ç¡®å»ºç«‹ä¸»ä½“å›¾åƒä¸æ–‡æœ¬å®ä½“ä¹‹é—´çš„å¯¹åº”å…³ç³»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPolyVividåœ¨èº«ä»½ä¿çœŸåº¦ã€è§†é¢‘çœŸå®æ„Ÿå’Œä¸»ä½“å¯¹é½æ–¹é¢è¡¨ç°ä¼˜è¶Šï¼Œè¶…è¶Šäº†ç°æœ‰çš„å¼€æºå’Œå•†ä¸šåŸºçº¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.07527",
            "title": "Learning What Reinforcement Learning Can't: Interleaved Online\n  Fine-Tuning for Hardest Questions",
            "url": "https://huggingface.co/papers/2506.07527",
            "abstract": "ReLIFT, a method combining reinforcement learning and supervised fine-tuning, enhances large language model reasoning by addressing limitations of RL through interleaved training, improving performance across benchmarks with minimal data.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language model (LLM) reasoning have shown that sophisticated behaviors such as planning and self-reflection can emerge through reinforcement learning (RL). However, despite these successes, RL in its current form remains insufficient to induce capabilities that exceed the limitations of the base model, as it is primarily optimized based on existing knowledge of the model rather than facilitating the acquisition of new information. To address this limitation, we employ supervised fine-tuning (SFT) to learn what RL cannot, which enables the incorporation of new knowledge and reasoning patterns by leveraging high-quality demonstration data. We analyze the training dynamics of RL and SFT for LLM reasoning and find that RL excels at maintaining and improving performance on questions within the model's original capabilities, while SFT is more effective at enabling progress on questions beyond the current scope of the model. Motivated by the complementary strengths of RL and SFT, we introduce a novel training approach, ReLIFT (Reinforcement Learning Interleaved with Online Fine-Tuning). In ReLIFT, the model is primarily trained using RL, but when it encounters challenging questions, high-quality solutions are collected for fine-tuning, and the training process alternates between RL and fine-tuning to enhance the model's reasoning abilities. ReLIFT achieves an average improvement of over +5.2 points across five competition-level benchmarks and one out-of-distribution benchmark compared to other zero-RL models. Furthermore, we demonstrate that ReLIFT outperforms both RL and SFT while using only 13\\% of the detailed demonstration data, highlighting its scalability. These results provide compelling evidence that ReLIFT overcomes the fundamental limitations of RL and underscores the significant potential.",
            "score": 2,
            "issue_id": 4214,
            "pub_date": "2025-06-09",
            "pub_date_card": {
                "ru": "9 Ğ¸ÑĞ½Ñ",
                "en": "June 9",
                "zh": "6æœˆ9æ—¥"
            },
            "hash": "6978ee3d97028c45",
            "authors": [
                "Lu Ma",
                "Hao Liang",
                "Meiyi Qiang",
                "Lexiang Tang",
                "Xiaochen Ma",
                "Zhen Hao Wong",
                "Junbo Niu",
                "Chengyu Shen",
                "Runming He",
                "Bin Cui",
                "Wentao Zhang"
            ],
            "affiliations": [
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.07527.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#optimization",
                    "#rl",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ReLIFT: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ RL Ğ¸ SFT",
                    "desc": "ReLIFT - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ¸ ÑÑƒĞ¿ĞµÑ€Ğ²Ğ¸Ğ·Ğ¾Ñ€Ğ½ÑƒÑ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ (SFT). ĞĞ½ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ RL, Ñ‡ĞµÑ€ĞµĞ´ÑƒÑ ĞµĞ³Ğ¾ Ñ SFT Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ¾Ğ±Ñ€ĞµÑ‚ĞµĞ½Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. ReLIFT ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 5.2 Ğ¿ÑƒĞ½ĞºÑ‚Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²ÑĞµĞ³Ğ¾ 13% Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "ReLIFT: Enhancing LLM Reasoning with Smart Training Mix",
                    "desc": "ReLIFT is a novel training method that combines reinforcement learning (RL) and supervised fine-tuning (SFT) to improve the reasoning capabilities of large language models (LLMs). It addresses the limitations of traditional RL by interleaving training sessions, allowing the model to learn from high-quality demonstration data when faced with difficult questions. This approach enables the model to acquire new knowledge and reasoning patterns, enhancing its performance on challenging tasks. The results show that ReLIFT significantly outperforms both standalone RL and SFT methods, achieving better results with less training data."
                },
                "zh": {
                    "title": "ReLIFTï¼šå¼ºåŒ–å­¦ä¹ ä¸å¾®è°ƒçš„å®Œç¾ç»“åˆ",
                    "desc": "ReLIFTæ˜¯ä¸€ç§ç»“åˆå¼ºåŒ–å­¦ä¹ å’Œç›‘ç£å¾®è°ƒçš„æ–¹æ³•ï¼Œæ—¨åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ–¹æ³•é€šè¿‡äº¤æ›¿è®­ç»ƒæ¥è§£å†³å¼ºåŒ–å­¦ä¹ çš„å±€é™æ€§ï¼Œä½¿æ¨¡å‹åœ¨é¢å¯¹æŒ‘æˆ˜æ€§é—®é¢˜æ—¶èƒ½å¤Ÿæœ‰æ•ˆåœ°å¸æ”¶æ–°çŸ¥è¯†ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒReLIFTåœ¨äº”ä¸ªç«äº‰çº§åŸºå‡†æµ‹è¯•ä¸­å¹³å‡æé«˜äº†è¶…è¿‡5.2åˆ†ï¼Œå¹¶ä¸”åœ¨ä½¿ç”¨ä»…13%çš„ç¤ºèŒƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ å’Œç›‘ç£å¾®è°ƒæ–¹æ³•ã€‚æ­¤ç»“æœè¡¨æ˜ï¼ŒReLIFTèƒ½å¤Ÿå…‹æœå¼ºåŒ–å­¦ä¹ çš„åŸºæœ¬å±€é™æ€§ï¼Œå±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.07240",
            "title": "Overclocking LLM Reasoning: Monitoring and Controlling Thinking Path\n  Lengths in LLMs",
            "url": "https://huggingface.co/papers/2506.07240",
            "abstract": "LLMs use progressive encoding and visualization to optimize the length of reasoning processes, enhancing accuracy and reducing inference time.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, techniques such as explicit structured reasoning have demonstrated strong test-time scaling behavior by enforcing a separation between the model's internal \"thinking\" process and the final response. A key factor influencing answer quality in this setting is the length of the thinking stage. When the reasoning is too short, the model may fail to capture the complexity of the task. Conversely, when it is too long, the model may overthink, leading to unnecessary computation and degraded performance. This paper explores and exploits the underlying mechanisms by which LLMs understand and regulate the length of their reasoning during explicit thought processes. First, we show that LLMs encode their progress through the reasoning process and introduce an interactive progress bar visualization, which is then used to reveal insights on the model's planning dynamics. Second, we manipulate the internal progress encoding during inference to reduce unnecessary steps and generate a more concise and decisive chain of thoughts. Our empirical results demonstrate that this \"overclocking\" method mitigates overthinking, improves answer accuracy, and reduces inference latency. Our code is publicly available.",
            "score": 2,
            "issue_id": 4211,
            "pub_date": "2025-06-08",
            "pub_date_card": {
                "ru": "8 Ğ¸ÑĞ½Ñ",
                "en": "June 8",
                "zh": "6æœˆ8æ—¥"
            },
            "hash": "25ea5847c07c7881",
            "authors": [
                "Roy Eisenstadt",
                "Itamar Zimerman",
                "Lior Wolf"
            ],
            "affiliations": [
                "IBM Research",
                "Tel-Aviv University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.07240.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#reasoning",
                    "#optimization",
                    "#training"
                ],
                "emoji": "â±ï¸",
                "ru": {
                    "title": "ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ LLM: Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ, Ñ‚Ğ¾Ñ‡Ğ½ĞµĞµ, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹, Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ñ€ĞµĞ³ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ğ´Ğ»Ğ¸Ğ½Ñƒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ LLM ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¾ÑÑ‹ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ°. ĞĞ½Ğ¸ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğ¼ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ° Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ½ĞµĞ½ÑƒĞ¶Ğ½Ñ‹Ğµ ÑˆĞ°Ğ³Ğ¸ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€Ğ°Ñ‚ĞºÑƒÑ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºÑƒ Ğ¼Ñ‹ÑĞ»ĞµĞ¹. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ 'Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ½Ğ°' ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ Ğ¸Ğ·Ğ»Ğ¸ÑˆĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºÑƒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Optimizing Reasoning Length for Enhanced LLM Performance",
                    "desc": "This paper discusses how large language models (LLMs) can improve their reasoning processes by optimizing the length of their internal thought stages. It highlights the importance of balancing the duration of reasoning: too short may miss complexities, while too long can lead to overthinking and inefficiency. The authors introduce a method for encoding reasoning progress and a visualization tool to better understand how LLMs plan their responses. By adjusting this internal encoding, they demonstrate that LLMs can enhance accuracy and speed during inference, leading to better performance overall."
                },
                "zh": {
                    "title": "ä¼˜åŒ–æ¨ç†é•¿åº¦ï¼Œæå‡æ¨¡å‹è¡¨ç°",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¦‚ä½•ä¼˜åŒ–æ€ç»´é•¿åº¦ï¼Œä»¥æé«˜å‡†ç¡®æ€§å¹¶å‡å°‘æ¨ç†æ—¶é—´ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ¨ç†é˜¶æ®µçš„é•¿åº¦å¯¹ç­”æ¡ˆè´¨é‡æœ‰é‡è¦å½±å“ï¼Œè¿‡çŸ­å¯èƒ½æ— æ³•æ•æ‰ä»»åŠ¡å¤æ‚æ€§ï¼Œè¿‡é•¿åˆ™å¯èƒ½å¯¼è‡´ä¸å¿…è¦çš„è®¡ç®—ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§äº¤äº’å¼è¿›åº¦æ¡å¯è§†åŒ–ï¼Œå¸®åŠ©æ­ç¤ºæ¨¡å‹çš„æ€ç»´åŠ¨æ€ï¼Œå¹¶é€šè¿‡æ“æ§å†…éƒ¨è¿›åº¦ç¼–ç æ¥å‡å°‘å†—ä½™æ­¥éª¤ã€‚å®éªŒè¯æ˜ï¼Œè¿™ç§â€œè¶…é¢‘â€æ–¹æ³•æœ‰æ•ˆå‡è½»äº†è¿‡åº¦æ€è€ƒï¼Œæé«˜äº†ç­”æ¡ˆçš„å‡†ç¡®æ€§ï¼Œå¹¶é™ä½äº†æ¨ç†å»¶è¿Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.07160",
            "title": "GeometryZero: Improving Geometry Solving for LLM with Group Contrastive\n  Policy Optimization",
            "url": "https://huggingface.co/papers/2506.07160",
            "abstract": "A new reinforcement learning framework, Group Contrastive Policy Optimization (GCPO), enhances geometric reasoning in large language models with judicious auxiliary constructions, outperforming existing methods on benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models (LLMs) have demonstrated remarkable capabilities across diverse domains, particularly in mathematical reasoning, amid which geometry problem solving remains a challenging area where auxiliary construction plays a enssential role. Existing approaches either achieve suboptimal performance or rely on massive LLMs (e.g., GPT-4o), incurring massive computational costs. We posit that reinforcement learning with verifiable reward (e.g., GRPO) offers a promising direction for training smaller models that effectively combine auxiliary construction with robust geometric reasoning. However, directly applying GRPO to geometric reasoning presents fundamental limitations due to its dependence on unconditional rewards, which leads to indiscriminate and counterproductive auxiliary constructions. To address these challenges, we propose Group Contrastive Policy Optimization (GCPO), a novel reinforcement learning framework featuring two key innovations: (1) Group Contrastive Masking, which adaptively provides positive or negative reward signals for auxiliary construction based on contextual utility, and a (2) length reward that promotes longer reasoning chains. Building on GCPO, we develop GeometryZero, a family of affordable-size geometric reasoning models that judiciously determine when to employ auxiliary construction. Our extensive empirical evaluation across popular geometric benchmarks (Geometry3K, MathVista) demonstrates that GeometryZero models consistently outperform baselines (e.g. GRPO), achieving an average improvement of 4.29% across all benchmarks.",
            "score": 2,
            "issue_id": 4212,
            "pub_date": "2025-06-08",
            "pub_date_card": {
                "ru": "8 Ğ¸ÑĞ½Ñ",
                "en": "June 8",
                "zh": "6æœˆ8æ—¥"
            },
            "hash": "25d25cf0d2a1b0d5",
            "authors": [
                "Yikun Wang",
                "Yibin Wang",
                "Dianyi Wang",
                "Zimian Peng",
                "Qipeng Guo",
                "Dacheng Tao",
                "Jiaqi Wang"
            ],
            "affiliations": [
                "Fudan University",
                "Nanyang Technological University",
                "Shanghai AI Laboratory",
                "Shanghai Innovation Institute",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.07160.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#optimization",
                    "#rl",
                    "#training"
                ],
                "emoji": "ğŸ“",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ñ‹Ğµ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡",
                    "desc": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Group Contrastive Policy Optimization (GCPO), ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğ¹. GCPO Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ğ¾Ğµ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ·Ğ° Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ GCPO Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ¾ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ GeometryZero, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑÑ‚, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ GeometryZero Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "Enhancing Geometric Reasoning with GCPO in Language Models",
                    "desc": "The paper introduces a new reinforcement learning framework called Group Contrastive Policy Optimization (GCPO) that improves geometric reasoning in large language models. It addresses the limitations of existing methods that either underperform or require large models, which are costly to run. GCPO uses innovative techniques like Group Contrastive Masking to provide context-sensitive rewards for auxiliary constructions and a length reward to encourage longer reasoning processes. The results show that models developed using GCPO, named GeometryZero, significantly outperform previous benchmarks in geometric problem-solving tasks."
                },
                "zh": {
                    "title": "ç¾¤ä½“å¯¹æ¯”ç­–ç•¥ä¼˜åŒ–ï¼šæå‡å‡ ä½•æ¨ç†çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç§°ä¸ºç¾¤ä½“å¯¹æ¯”ç­–ç•¥ä¼˜åŒ–ï¼ˆGCPOï¼‰ï¼Œæ—¨åœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨å‡ ä½•æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚GCPOé€šè¿‡é€‚åº”æ€§åœ°æä¾›æ­£è´Ÿå¥–åŠ±ä¿¡å·ï¼Œä¼˜åŒ–è¾…åŠ©æ„é€ çš„ä½¿ç”¨ï¼Œä»è€Œå…‹æœäº†ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†GeometryZeroæ¨¡å‹ç³»åˆ—ï¼Œè¿™äº›æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°ç»“åˆè¾…åŠ©æ„é€ ä¸ç¨³å¥çš„å‡ ä½•æ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGeometryZeroåœ¨å¤šä¸ªå‡ ä½•åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹³å‡æå‡äº†4.29%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.03690",
            "title": "Robust Preference Optimization via Dynamic Target Margins",
            "url": "https://huggingface.co/papers/2506.03690",
            "abstract": "The paper introduces Î³-PO, a dynamic target margin preference optimization algorithm that enhances Large Language Models' alignment by adjusting reward margins at the pairwise level, leading to improved performance with minimal impact on training.  \t\t\t\t\tAI-generated summary \t\t\t\t The alignment of Large Language Models (LLMs) is crucial for ensuring their safety and reliability in practical applications. Direct Preference Optimization (DPO) has emerged as an efficient method that directly optimizes models using preference pairs, significantly reducing resource demands. However, the effectiveness of DPO heavily depends on the data quality, which is frequently compromised by noise. In this work, we propose gamma-PO, a dynamic target margin preference optimization algorithm that adjust reward margins at the pairwise level. By introducing instance-specific margin calibration, gamma-PO strategically prioritizes high-confidence pairs (those demonstrating higher reward margins) while suppressing potential noise from ambiguous pairs. Moreover, gamma-PO is a plug-and-play method, compatible with variants of DPO that rely on reward margin between preference pairs. Across benchmarks such as AlpacaEval2 and Arena-Hard, gamma-PO achieves an average 4.4\\% improvement over other baselines, setting new benchmarks for state-of-the-art performance. Additionally, gamma-PO requires minimal code changes and has a negligible impact on training efficiency, making it a robust solution for enhancing LLMs alignment. Our codes are available at https://github.com/sunjie279/gammaPO{https://github.com/sunjie279/gammaPO}.",
            "score": 2,
            "issue_id": 4208,
            "pub_date": "2025-06-04",
            "pub_date_card": {
                "ru": "4 Ğ¸ÑĞ½Ñ",
                "en": "June 4",
                "zh": "6æœˆ4æ—¥"
            },
            "hash": "a3134e659a450a93",
            "authors": [
                "Jie Sun",
                "Junkang Wu",
                "Jiancan Wu",
                "Zhibo Zhu",
                "Xingyu Lu",
                "Jun Zhou",
                "Lintao Ma",
                "Xiang Wang"
            ],
            "affiliations": [
                "Ant Group",
                "National University of Singapore",
                "Shanghai Key Laboratory of Data Science"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.03690.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#benchmark",
                    "#rlhf",
                    "#alignment"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Î³-PO: Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Î³-PO - Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ¼Ğ°Ñ€Ğ¶Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). Î³-PO ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ°Ñ€Ğ¶Ñƒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¿Ğ¾Ğ¿Ğ°Ñ€Ğ½Ğ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑˆÑƒĞ¼. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼ Ñ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ°Ğ¼Ğ¸ Direct Preference Optimization (DPO) Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ 4.4% ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. Î³-PO Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ğ² ĞºĞ¾Ğ´Ğµ Ğ¸ Ğ½Ğµ Ğ²Ğ»Ğ¸ÑĞµÑ‚ Ğ½Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞµĞ³Ğ¾ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğ¼ Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ LLM."
                },
                "en": {
                    "title": "Enhancing LLM Alignment with Dynamic Margin Optimization",
                    "desc": "The paper presents Î³-PO, a novel algorithm designed to optimize the alignment of Large Language Models (LLMs) by dynamically adjusting reward margins at the pairwise level. This method enhances Direct Preference Optimization (DPO) by focusing on high-confidence preference pairs, which helps to mitigate the effects of noisy data. By implementing instance-specific margin calibration, Î³-PO improves model performance while maintaining training efficiency with minimal code modifications. The results demonstrate an average improvement of 4.4% over existing baselines, establishing new benchmarks in LLM alignment."
                },
                "zh": {
                    "title": "Î³-POï¼šæå‡å¤§å‹è¯­è¨€æ¨¡å‹å¯¹é½æ€§çš„åŠ¨æ€ä¼˜åŒ–ç®—æ³•",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºÎ³-POçš„åŠ¨æ€ç›®æ ‡è¾¹é™…åå¥½ä¼˜åŒ–ç®—æ³•ï¼Œæ—¨åœ¨é€šè¿‡è°ƒæ•´æˆå¯¹çš„å¥–åŠ±è¾¹é™…æ¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¯¹é½æ€§ã€‚è¯¥ç®—æ³•é€šè¿‡å®ä¾‹ç‰¹å®šçš„è¾¹é™…æ ¡å‡†ï¼Œä¼˜å…ˆè€ƒè™‘é«˜ç½®ä¿¡åº¦çš„æˆå¯¹æ•°æ®ï¼ŒåŒæ—¶æŠ‘åˆ¶æ¨¡ç³Šæˆå¯¹æ•°æ®çš„æ½œåœ¨å™ªå£°ã€‚Î³-POä¸ç°æœ‰çš„ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æ–¹æ³•å…¼å®¹ï¼Œèƒ½å¤Ÿåœ¨ä¸æ˜¾è‘—å½±å“è®­ç»ƒæ•ˆç‡çš„æƒ…å†µä¸‹ï¼Œæå‡æ¨¡å‹çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒÎ³-POåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å¹³å‡æé«˜äº†4.4%çš„æ€§èƒ½ï¼Œè®¾å®šäº†æ–°çš„æœ€å…ˆè¿›çš„æ€§èƒ½åŸºå‡†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.04807",
            "title": "MegaHan97K: A Large-Scale Dataset for Mega-Category Chinese Character\n  Recognition with over 97K Categories",
            "url": "https://huggingface.co/papers/2506.04807",
            "abstract": "MegaHan97K, a large-scale dataset for recognizing over 97,000 Chinese characters, addresses the long-tail distribution problem and reveals new challenges in mega-category OCR.  \t\t\t\t\tAI-generated summary \t\t\t\t Foundational to the Chinese language and culture, Chinese characters encompass extraordinarily extensive and ever-expanding categories, with the latest Chinese GB18030-2022 standard containing 87,887 categories. The accurate recognition of this vast number of characters, termed mega-category recognition, presents a formidable yet crucial challenge for cultural heritage preservation and digital applications. Despite significant advances in Optical Character Recognition (OCR), mega-category recognition remains unexplored due to the absence of comprehensive datasets, with the largest existing dataset containing merely 16,151 categories. To bridge this critical gap, we introduce MegaHan97K, a mega-category, large-scale dataset covering an unprecedented 97,455 categories of Chinese characters. Our work offers three major contributions: (1) MegaHan97K is the first dataset to fully support the latest GB18030-2022 standard, providing at least six times more categories than existing datasets; (2) It effectively addresses the long-tail distribution problem by providing balanced samples across all categories through its three distinct subsets: handwritten, historical and synthetic subsets; (3) Comprehensive benchmarking experiments reveal new challenges in mega-category scenarios, including increased storage demands, morphologically similar character recognition, and zero-shot learning difficulties, while also unlocking substantial opportunities for future research. To the best of our knowledge, the MetaHan97K is likely the dataset with the largest classes not only in the field of OCR but may also in the broader domain of pattern recognition. The dataset is available at https://github.com/SCUT-DLVCLab/MegaHan97K.",
            "score": 1,
            "issue_id": 4212,
            "pub_date": "2025-06-05",
            "pub_date_card": {
                "ru": "5 Ğ¸ÑĞ½Ñ",
                "en": "June 5",
                "zh": "6æœˆ5æ—¥"
            },
            "hash": "d9e2ab51265af386",
            "authors": [
                "Yuyi Zhang",
                "Yongxin Shi",
                "Peirong Zhang",
                "Yixin Zhao",
                "Zhenhua Yang",
                "Lianwen Jin"
            ],
            "affiliations": [
                "SCUT-Zhuhai Institute of Modern Industrial Innovation, Zhuhai, China",
                "School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.04807.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#long_context",
                    "#synthetic"
                ],
                "emoji": "ğŸˆ¶",
                "ru": {
                    "title": "MegaHan97K: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğ¸ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ñ… Ğ¸ĞµÑ€Ğ¾Ğ³Ğ»Ğ¸Ñ„Ğ¾Ğ²",
                    "desc": "MegaHan97K - ÑÑ‚Ğ¾ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ 97 000 ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ñ… Ğ¸ĞµÑ€Ğ¾Ğ³Ğ»Ğ¸Ñ„Ğ¾Ğ². ĞĞ½ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ Ñ…Ğ²Ğ¾ÑÑ‚Ğ¾Ğ¼ Ğ¸ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ñ‹ Ğ² Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ² (OCR) Ñ Ğ¼ĞµĞ³Ğ°-ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ€ÑƒĞºĞ¾Ğ¿Ğ¸ÑĞ½Ñ‹Ğµ, Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ°, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ¿Ğ¾ Ğ²ÑĞµĞ¼ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸Ğ½Ğ³Ñƒ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ Ñ‚Ğ°ĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹, ĞºĞ°Ğº Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ½Ñ‹Ğµ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğº Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ, Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ñ€Ñ„Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑÑ…Ğ¾Ğ¶Ğ¸Ñ… ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ² Ğ¸ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ½ÑƒĞ»ĞµĞ²Ñ‹Ğ¼ Ğ²Ñ‹ÑÑ‚Ñ€ĞµĞ»Ğ¾Ğ¼."
                },
                "en": {
                    "title": "Unlocking the Future of Chinese Character Recognition with MegaHan97K",
                    "desc": "MegaHan97K is a groundbreaking dataset designed for recognizing over 97,000 Chinese characters, addressing the challenges of mega-category Optical Character Recognition (OCR). This dataset is the first to align with the latest GB18030-2022 standard, significantly expanding the number of character categories available for training models. It tackles the long-tail distribution problem by offering balanced samples across three subsets: handwritten, historical, and synthetic. Additionally, it highlights new challenges in mega-category recognition, such as increased storage needs and difficulties in zero-shot learning, while paving the way for future advancements in the field."
                },
                "zh": {
                    "title": "MegaHan97Kï¼šè¶…å¤§ç±»æ±‰å­—è¯†åˆ«çš„æ–°é‡Œç¨‹ç¢‘",
                    "desc": "MegaHan97Kæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„æ•°æ®é›†ï¼Œæ—¨åœ¨è¯†åˆ«è¶…è¿‡97,000ä¸ªæ±‰å­—ï¼Œè§£å†³äº†é•¿å°¾åˆ†å¸ƒé—®é¢˜ï¼Œå¹¶æ­ç¤ºäº†è¶…å¤§ç±»OCRçš„æ–°æŒ‘æˆ˜ã€‚è¯¥æ•°æ®é›†æ”¯æŒæœ€æ–°çš„GB18030-2022æ ‡å‡†ï¼Œæä¾›äº†æ¯”ç°æœ‰æ•°æ®é›†å¤šå…­å€çš„ç±»åˆ«ã€‚é€šè¿‡æ‰‹å†™ã€å†å²å’Œåˆæˆä¸‰ä¸ªå­é›†ï¼ŒMegaHan97Kæœ‰æ•ˆåœ°å¹³è¡¡äº†å„ç±»åˆ«çš„æ ·æœ¬ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¿˜æ­ç¤ºäº†åœ¨è¶…å¤§ç±»åœºæ™¯ä¸­é¢ä¸´çš„æ–°æŒ‘æˆ˜ï¼Œå¦‚å­˜å‚¨éœ€æ±‚å¢åŠ ã€å½¢æ€ç›¸ä¼¼å­—ç¬¦çš„è¯†åˆ«å’Œé›¶æ ·æœ¬å­¦ä¹ çš„å›°éš¾ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.07833",
            "title": "Improving large language models with concept-aware fine-tuning",
            "url": "https://huggingface.co/papers/2506.07833",
            "abstract": "Concept-Aware Fine-Tuning (CAFT) enhances large language models by enabling multi-token learning in the fine-tuning phase, leading to improved coherent understanding and better performance across various tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have become the cornerstone of modern AI. However, the existing paradigm of next-token prediction fundamentally limits their ability to form coherent, high-level concepts, making it a critical barrier to human-like understanding and reasoning. Take the phrase \"ribonucleic acid\" as an example: an LLM will first decompose it into tokens, i.e., artificial text fragments (\"rib\", \"on\", ...), then learn each token sequentially, rather than grasping the phrase as a unified, coherent semantic entity. This fragmented representation hinders deeper conceptual understanding and, ultimately, the development of truly intelligent systems. In response, we introduce Concept-Aware Fine-Tuning (CAFT), a novel multi-token training method that redefines how LLMs are fine-tuned. By enabling the learning of sequences that span multiple tokens, this method fosters stronger concept-aware learning. Our experiments demonstrate significant improvements compared to conventional next-token finetuning methods across diverse tasks, including traditional applications like text summarization and domain-specific ones like de novo protein design. Multi-token prediction was previously only possible in the prohibitively expensive pretraining phase; CAFT, to our knowledge, is the first to bring the multi-token setting to the post-training phase, thus effectively democratizing its benefits for the broader community of practitioners and researchers. Finally, the unexpected effectiveness of our proposed method suggests wider implications for the machine learning research community. All code and data are available at https://github.com/michaelchen-lab/caft-llm",
            "score": 0,
            "issue_id": 4217,
            "pub_date": "2025-06-09",
            "pub_date_card": {
                "ru": "9 Ğ¸ÑĞ½Ñ",
                "en": "June 9",
                "zh": "6æœˆ9æ—¥"
            },
            "hash": "33b47665a34ee59a",
            "authors": [
                "Michael K. Chen",
                "Xikun Zhang",
                "Jiaxing Huang",
                "Dacheng Tao"
            ],
            "affiliations": [
                "Nanyang Technological University Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.07833.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#dataset",
                    "#agi",
                    "#open_source",
                    "#survey"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞµ: Ğ¾Ñ‚ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğº ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸ÑĞ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Concept-Aware Fine-Tuning (CAFT). CAFT Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ‚Ğ¾ĞºĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹, Ñ‡Ñ‚Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ CAFT Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ‚Ğ¾ĞºĞµĞ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¿Ğ¾ÑÑ‚-Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°Ğ½ĞµĞµ Ğ±Ñ‹Ğ»Ğ¾ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ñ€Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Unlocking Coherent Understanding with Multi-Token Learning",
                    "desc": "Concept-Aware Fine-Tuning (CAFT) is a new approach that improves large language models (LLMs) by allowing them to learn from multiple tokens at once during the fine-tuning process. This method addresses the limitations of traditional next-token prediction, which often leads to a fragmented understanding of language. By enabling LLMs to grasp phrases as coherent units rather than separate parts, CAFT enhances their ability to understand complex concepts. Our experiments show that CAFT significantly outperforms standard fine-tuning methods in various tasks, making it a valuable advancement for AI development."
                },
                "zh": {
                    "title": "æ¦‚å¿µæ„ŸçŸ¥å¾®è°ƒï¼šæå‡è¯­è¨€æ¨¡å‹çš„ç†è§£åŠ›",
                    "desc": "æ¦‚å¿µæ„ŸçŸ¥å¾®è°ƒï¼ˆCAFTï¼‰é€šè¿‡åœ¨å¾®è°ƒé˜¶æ®µå®ç°å¤šæ ‡è®°å­¦ä¹ ï¼Œå¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œä»è€Œæé«˜äº†å¯¹å„ç§ä»»åŠ¡çš„ç†è§£å’Œè¡¨ç°ã€‚ä¼ ç»Ÿçš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹æ–¹æ³•é™åˆ¶äº†æ¨¡å‹å½¢æˆè¿è´¯é«˜å±‚æ¬¡æ¦‚å¿µçš„èƒ½åŠ›ï¼Œå¯¼è‡´ç†è§£å’Œæ¨ç†çš„éšœç¢ã€‚CAFTæ–¹æ³•å…è®¸æ¨¡å‹å­¦ä¹ è·¨è¶Šå¤šä¸ªæ ‡è®°çš„åºåˆ—ï¼Œä¿ƒè¿›äº†æ›´å¼ºçš„æ¦‚å¿µæ„ŸçŸ¥å­¦ä¹ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿçš„å¾®è°ƒæ–¹æ³•ç›¸æ¯”ï¼ŒCAFTåœ¨æ–‡æœ¬æ‘˜è¦å’Œç‰¹å®šé¢†åŸŸä»»åŠ¡ï¼ˆå¦‚æ–°è›‹ç™½è´¨è®¾è®¡ï¼‰ä¸Šéƒ½æœ‰æ˜¾è‘—æ”¹å–„ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.07803",
            "title": "Image Reconstruction as a Tool for Feature Analysis",
            "url": "https://huggingface.co/papers/2506.07803",
            "abstract": "Image reconstruction reveals that vision encoders retain more image information after image-based tasks and that orthogonal rotations in feature space control color encoding.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision encoders are increasingly used in modern applications, from vision-only models to multimodal systems such as vision-language models. Despite their remarkable success, it remains unclear how these architectures represent features internally. Here, we propose a novel approach for interpreting vision features via image reconstruction. We compare two related model families, SigLIP and SigLIP2, which differ only in their training objective, and show that encoders pre-trained on image-based tasks retain significantly more image information than those trained on non-image tasks such as contrastive learning. We further apply our method to a range of vision encoders, ranking them by the informativeness of their feature representations. Finally, we demonstrate that manipulating the feature space yields predictable changes in reconstructed images, revealing that orthogonal rotations (rather than spatial transformations) control color encoding. Our approach can be applied to any vision encoder, shedding light on the inner structure of its feature space. The code and model weights to reproduce the experiments are available in GitHub.",
            "score": 0,
            "issue_id": 4213,
            "pub_date": "2025-06-09",
            "pub_date_card": {
                "ru": "9 Ğ¸ÑĞ½Ñ",
                "en": "June 9",
                "zh": "6æœˆ9æ—¥"
            },
            "hash": "0bba64237ba10a58",
            "authors": [
                "Eduard Allakhverdov",
                "Dmitrii Tarasov",
                "Elizaveta Goncharova",
                "Andrey Kuznetsov"
            ],
            "affiliations": [
                "AIRI Moscow, Russia",
                "MIPT Dolgoprudny, Russia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.07803.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#interpretability",
                    "#cv",
                    "#multimodal",
                    "#architecture"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ñ‚Ğ°Ğ¹Ğ½ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ² ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ°Ñ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹, Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ± Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¸, Ñ‡ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ ÑĞ²Ğ¾Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ°Ğ¼ Ğ¸ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¸Ñ… Ğ¿Ğ¾ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². Ğ¢Ğ°ĞºĞ¶Ğµ Ğ±Ñ‹Ğ»Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¾Ñ€Ñ‚Ğ¾Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒÑÑ‚ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ†Ğ²ĞµÑ‚Ğ°."
                },
                "en": {
                    "title": "Unlocking the Secrets of Vision Encoders Through Image Reconstruction",
                    "desc": "This paper explores how vision encoders, which are used in various AI applications, represent image features internally. The authors introduce a method for interpreting these features through image reconstruction, comparing two model families with different training objectives. They find that encoders trained on image-based tasks retain more information than those trained on non-image tasks. Additionally, they show that manipulating the feature space through orthogonal rotations can control how colors are encoded in the reconstructed images."
                },
                "zh": {
                    "title": "æ­ç¤ºè§†è§‰ç¼–ç å™¨çš„ç‰¹å¾ç©ºé—´ç»“æ„",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†è§†è§‰ç¼–ç å™¨åœ¨å›¾åƒé‡å»ºä¸­çš„è¡¨ç°ï¼Œå‘ç°ç»è¿‡å›¾åƒä»»åŠ¡è®­ç»ƒçš„ç¼–ç å™¨ä¿ç•™äº†æ›´å¤šçš„å›¾åƒä¿¡æ¯ã€‚æˆ‘ä»¬æ¯”è¾ƒäº†ä¸¤ç§æ¨¡å‹å®¶æ—ï¼ŒSigLIPå’ŒSigLIP2ï¼Œå‘ç°å‰è€…åœ¨å›¾åƒä»»åŠ¡ä¸Šçš„é¢„è®­ç»ƒä½¿å…¶ç‰¹å¾è¡¨ç¤ºæ›´å…·ä¿¡æ¯é‡ã€‚é€šè¿‡å¯¹ç‰¹å¾ç©ºé—´çš„æ“ä½œï¼Œæˆ‘ä»¬å‘ç°æ­£äº¤æ—‹è½¬å¯ä»¥æ§åˆ¶é¢œè‰²ç¼–ç ï¼Œè€Œä¸æ˜¯ç©ºé—´å˜æ¢ã€‚è¯¥æ–¹æ³•é€‚ç”¨äºä»»ä½•è§†è§‰ç¼–ç å™¨ï¼Œæœ‰åŠ©äºç†è§£å…¶ç‰¹å¾ç©ºé—´çš„å†…éƒ¨ç»“æ„ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.07645",
            "title": "Evaluating LLMs Robustness in Less Resourced Languages with Proxy Models",
            "url": "https://huggingface.co/papers/2506.07645",
            "abstract": "Character and word-level attacks can effectively perturb large language models' predictions, especially in low-resource languages like Polish, indicating potential vulnerabilities in their safety mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have demonstrated impressive capabilities across various natural language processing (NLP) tasks in recent years. However, their susceptibility to jailbreaks and perturbations necessitates additional evaluations. Many LLMs are multilingual, but safety-related training data contains mainly high-resource languages like English. This can leave them vulnerable to perturbations in low-resource languages such as Polish. We show how surprisingly strong attacks can be cheaply created by altering just a few characters and using a small proxy model for word importance calculation. We find that these character and word-level attacks drastically alter the predictions of different LLMs, suggesting a potential vulnerability that can be used to circumvent their internal safety mechanisms. We validate our attack construction methodology on Polish, a low-resource language, and find potential vulnerabilities of LLMs in this language. Additionally, we show how it can be extended to other languages. We release the created datasets and code for further research.",
            "score": 0,
            "issue_id": 4215,
            "pub_date": "2025-06-09",
            "pub_date_card": {
                "ru": "9 Ğ¸ÑĞ½Ñ",
                "en": "June 9",
                "zh": "6æœˆ9æ—¥"
            },
            "hash": "e04bb2316fbf41bc",
            "authors": [
                "Maciej ChrabÄ…szcz",
                "Katarzyna Lorenc",
                "Karolina Seweryn"
            ],
            "affiliations": [
                "NASK - National Research Institute, Warsaw, Poland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.07645.jpg",
            "data": {
                "categories": [
                    "#low_resource",
                    "#dataset",
                    "#multilingual",
                    "#security"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Ğ£ÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ LLM Ğ² Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€ĞµÑÑƒÑ€ÑĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ…: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ´Ğ»Ñ Ğ˜Ğ˜-Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ² Ğ¸ ÑĞ»Ğ¾Ğ², Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€ĞµÑÑƒÑ€ÑĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ²Ñ€Ğ¾Ğ´Ğµ Ğ¿Ğ¾Ğ»ÑŒÑĞºĞ¾Ğ³Ğ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ² Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ğ»Ğ¸ÑÑ‚ÑŒ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ LLM. Ğ­Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ² Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ°Ñ… Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ² Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… LLM Ğº Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ğ¼ Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼."
                },
                "en": {
                    "title": "Uncovering Vulnerabilities in Language Models: Small Changes, Big Impact!",
                    "desc": "This paper explores the vulnerabilities of large language models (LLMs) to character and word-level attacks, particularly in low-resource languages like Polish. The authors demonstrate that minor alterations in text can significantly change the predictions made by these models, highlighting a critical safety concern. They utilize a small proxy model to assess word importance, enabling the creation of effective perturbations with minimal effort. The findings suggest that LLMs require more robust safety mechanisms, especially for languages that lack extensive training data."
                },
                "zh": {
                    "title": "æ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä½èµ„æºè¯­è¨€ä¸­çš„è„†å¼±æ€§",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä½èµ„æºè¯­è¨€ï¼ˆå¦‚æ³¢å…°è¯­ï¼‰ä¸­çš„è„†å¼±æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡ç®€å•åœ°æ”¹å˜å‡ ä¸ªå­—ç¬¦ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å¹²æ‰°è¿™äº›æ¨¡å‹çš„é¢„æµ‹ç»“æœã€‚ç”±äºå®‰å…¨ç›¸å…³çš„è®­ç»ƒæ•°æ®ä¸»è¦é›†ä¸­åœ¨é«˜èµ„æºè¯­è¨€ï¼ˆå¦‚è‹±è¯­ï¼‰ï¼Œè¿™ä½¿å¾—LLMsåœ¨ä½èµ„æºè¯­è¨€ä¸­æ›´å®¹æ˜“å—åˆ°æ”»å‡»ã€‚è®ºæ–‡è¿˜å±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨å°å‹ä»£ç†æ¨¡å‹è®¡ç®—å•è¯é‡è¦æ€§ï¼Œä»è€Œæ„å»ºå¼ºå¤§çš„æ”»å‡»æ–¹æ³•ï¼Œå¹¶æä¾›äº†ç›¸å…³æ•°æ®é›†å’Œä»£ç ä¾›è¿›ä¸€æ­¥ç ”ç©¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.05904",
            "title": "Proactive Assistant Dialogue Generation from Streaming Egocentric Videos",
            "url": "https://huggingface.co/papers/2506.05904",
            "abstract": "A framework provides automated data synthesis, evaluation metrics, and an end-to-end model for real-time, proactive conversational AI task guidance using streaming video inputs.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in conversational AI have been substantial, but developing real-time systems for perceptual task guidance remains challenging. These systems must provide interactive, proactive assistance based on streaming visual inputs, yet their development is constrained by the costly and labor-intensive process of data collection and system evaluation. To address these limitations, we present a comprehensive framework with three key contributions. First, we introduce a novel data curation pipeline that synthesizes dialogues from annotated egocentric videos, resulting in \\dataset, a large-scale synthetic dialogue dataset spanning multiple domains. Second, we develop a suite of automatic evaluation metrics, validated through extensive human studies. Third, we propose an end-to-end model that processes streaming video inputs to generate contextually appropriate responses, incorporating novel techniques for handling data imbalance and long-duration videos. This work lays the foundation for developing real-time, proactive AI assistants capable of guiding users through diverse tasks. Project page: https://pro-assist.github.io/",
            "score": 0,
            "issue_id": 4214,
            "pub_date": "2025-06-06",
            "pub_date_card": {
                "ru": "6 Ğ¸ÑĞ½Ñ",
                "en": "June 6",
                "zh": "6æœˆ6æ—¥"
            },
            "hash": "a29f2576c90935c3",
            "authors": [
                "Yichi Zhang",
                "Xin Luna Dong",
                "Zhaojiang Lin",
                "Andrea Madotto",
                "Anuj Kumar",
                "Babak Damavandi",
                "Joyce Chai",
                "Seungwhan Moon"
            ],
            "affiliations": [
                "Meta",
                "University of Michigan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.05904.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#long_context",
                    "#data",
                    "#synthetic",
                    "#cv",
                    "#multimodal"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ğ¸Ğ· Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ñ ÑƒÑ‡Ğ°ÑÑ‚Ğ¸ĞµĞ¼ Ğ»ÑĞ´ĞµĞ¹. ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ°Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾-Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ´Ğ¸ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Empowering Real-Time AI Guidance with Streaming Video Insights",
                    "desc": "This paper presents a framework for creating real-time conversational AI that can guide users through tasks using live video feeds. It introduces a new data curation pipeline that generates a large synthetic dialogue dataset from annotated egocentric videos, which helps in training the AI. The authors also propose automatic evaluation metrics that have been validated through human studies to assess the system's performance. Finally, they develop an end-to-end model that effectively processes streaming video inputs to provide timely and relevant responses, addressing challenges like data imbalance and long video durations."
                },
                "zh": {
                    "title": "å®æ—¶ä¸»åŠ¨å¯¹è¯AIä»»åŠ¡æŒ‡å¯¼çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ¡†æ¶ï¼Œç”¨äºè‡ªåŠ¨åŒ–æ•°æ®åˆæˆã€è¯„ä¼°æŒ‡æ ‡å’Œç«¯åˆ°ç«¯æ¨¡å‹ï¼Œä»¥å®ç°åŸºäºæµåª’ä½“è§†é¢‘è¾“å…¥çš„å®æ—¶ä¸»åŠ¨å¯¹è¯AIä»»åŠ¡æŒ‡å¯¼ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒè´¡çŒ®åŒ…æ‹¬ï¼šé¦–å…ˆï¼Œå¼€å‘äº†ä¸€ç§æ–°é¢–çš„æ•°æ®ç­–åˆ’ç®¡é“ï¼Œä»æ ‡æ³¨çš„è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘ä¸­åˆæˆå¯¹è¯ï¼Œç”Ÿæˆäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„åˆæˆå¯¹è¯æ•°æ®é›†ã€‚å…¶æ¬¡ï¼Œæå‡ºäº†ä¸€å¥—è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶é€šè¿‡å¹¿æ³›çš„äººç±»ç ”ç©¶è¿›è¡Œäº†éªŒè¯ã€‚æœ€åï¼Œæ„å»ºäº†ä¸€ä¸ªå¤„ç†æµåª’ä½“è§†é¢‘è¾“å…¥çš„ç«¯åˆ°ç«¯æ¨¡å‹ï¼Œèƒ½å¤Ÿç”Ÿæˆä¸Šä¸‹æ–‡é€‚å½“çš„å“åº”ï¼Œè§£å†³äº†æ•°æ®ä¸å¹³è¡¡å’Œé•¿æ—¶è§†é¢‘å¤„ç†çš„é—®é¢˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23473",
            "title": "EVOREFUSE: Evolutionary Prompt Optimization for Evaluation and\n  Mitigation of LLM Over-Refusal to Pseudo-Malicious Instructions",
            "url": "https://huggingface.co/papers/2505.23473",
            "abstract": "EVOREFUSE, an evolutionary algorithm, generates diverse pseudo-malicious instructions to optimize LLM refusal training, improving user experience without compromising safety.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) frequently refuse to respond to pseudo-malicious instructions: semantically harmless input queries triggering unnecessary LLM refusals due to conservative safety alignment, significantly impairing user experience. Collecting such instructions is crucial for evaluating and mitigating over-refusals, but existing instruction curation methods, like manual creation or instruction rewriting, either lack scalability or fail to produce sufficiently diverse and effective refusal-inducing prompts. To address these limitations, we introduce EVOREFUSE, a prompt optimization approach that generates diverse pseudo-malicious instructions consistently eliciting confident refusals across LLMs. EVOREFUSE employs an evolutionary algorithm exploring the instruction space in more diverse directions than existing methods via mutation strategies and recombination, and iteratively evolves seed instructions to maximize evidence lower bound on LLM refusal probability. Using EVOREFUSE, we create two novel datasets: EVOREFUSE-TEST, a benchmark of 582 pseudo-malicious instructions that outperforms the next-best benchmark with 140.41% higher average refusal triggering rate across 9 LLMs, 34.86% greater lexical diversity, and 40.03% improved LLM response confidence scores; and EVOREFUSE-ALIGN, which provides 3,000 pseudo-malicious instructions with responses for supervised and preference-based alignment training. LLAMA3.1-8B-INSTRUCT supervisedly fine-tuned on EVOREFUSE-ALIGN achieves up to 14.31% fewer over-refusals than models trained on the second-best alignment dataset, without compromising safety. Our analysis with EVOREFUSE-TEST reveals models trigger over-refusals by overly focusing on sensitive keywords while ignoring broader context.",
            "score": 0,
            "issue_id": 4212,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 Ğ¼Ğ°Ñ",
                "en": "May 29",
                "zh": "5æœˆ29æ—¥"
            },
            "hash": "dc6dd3911616cbf5",
            "authors": [
                "Xiaorui Wu",
                "Xiaofeng Mao",
                "Xin Zhang",
                "Fei Li",
                "Chong Teng",
                "Yuxiang Peng",
                "Li Zheng",
                "Donghong Ji",
                "Zhuang Li"
            ],
            "affiliations": [
                "Ant Group",
                "Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, China",
                "School of Computing Technologies, Royal Melbourne Institute of Technology, Australia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23473.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#optimization",
                    "#alignment",
                    "#dataset",
                    "#training",
                    "#data"
                ],
                "emoji": "ğŸ§¬",
                "ru": {
                    "title": "Ğ­Ğ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ¼Ğ½Ñ‹Ñ… Ğ¾Ñ‚ĞºĞ°Ğ·Ğ¾Ğ² Ğ˜Ğ˜",
                    "desc": "EVOREFUSE - ÑÑ‚Ğ¾ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚ĞºĞ°Ğ·Ğ°Ğ¼ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… EVOREFUSE-TEST Ğ¸ EVOREFUSE-ALIGN, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ° Ğ¾Ñ‚ĞºĞ°Ğ·Ğ¾Ğ². ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ÑÑ‚Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ LLM ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¾Ñ‚ĞºĞ°Ğ·Ğ¾Ğ² Ğ±ĞµĞ· ÑƒÑ‰ĞµÑ€Ğ±Ğ° Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¾Ñ‚ĞºĞ°Ğ·Ñ‹, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑÑŒ Ğ½Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ»Ğ¾Ğ²Ğ°Ñ… Ğ¸ Ğ¸Ğ³Ğ½Ğ¾Ñ€Ğ¸Ñ€ÑƒÑ Ğ±Ğ¾Ğ»ĞµĞµ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚."
                },
                "en": {
                    "title": "EVOREFUSE: Enhancing LLM Refusal Training with Evolutionary Algorithms",
                    "desc": "EVOREFUSE is an innovative evolutionary algorithm designed to enhance the training of large language models (LLMs) by generating a variety of pseudo-malicious instructions. These instructions help to optimize refusal training, allowing LLMs to respond more effectively to potentially harmful queries without sacrificing user safety. By employing mutation strategies and recombination, EVOREFUSE explores the instruction space more thoroughly than traditional methods, resulting in a significant increase in the diversity and effectiveness of refusal-inducing prompts. The approach has led to the creation of two new datasets that improve LLM performance, reducing unnecessary refusals while maintaining safety standards."
                },
                "zh": {
                    "title": "EVOREFUSEï¼šä¼˜åŒ–LLMæ‹’ç»è®­ç»ƒçš„è¿›åŒ–ç®—æ³•",
                    "desc": "EVOREFUSEæ˜¯ä¸€ç§è¿›åŒ–ç®—æ³•ï¼Œæ—¨åœ¨ç”Ÿæˆå¤šæ ·åŒ–çš„ä¼ªæ¶æ„æŒ‡ä»¤ï¼Œä»¥ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ‹’ç»è®­ç»ƒï¼Œä»è€Œæå‡ç”¨æˆ·ä½“éªŒè€Œä¸å½±å“å®‰å…¨æ€§ã€‚ç°æœ‰çš„æŒ‡ä»¤ç­–åˆ’æ–¹æ³•å¦‚æ‰‹åŠ¨åˆ›å»ºæˆ–é‡å†™æŒ‡ä»¤ï¼Œç¼ºä¹å¯æ‰©å±•æ€§æˆ–æ— æ³•äº§ç”Ÿè¶³å¤Ÿå¤šæ ·å’Œæœ‰æ•ˆçš„æ‹’ç»è¯±å¯¼æç¤ºã€‚EVOREFUSEé€šè¿‡å˜å¼‚ç­–ç•¥å’Œé‡ç»„æ¢ç´¢æŒ‡ä»¤ç©ºé—´ï¼Œè¿­ä»£æ¼”åŒ–ç§å­æŒ‡ä»¤ï¼Œä»¥æœ€å¤§åŒ–LLMæ‹’ç»æ¦‚ç‡çš„è¯æ®ä¸‹ç•Œã€‚ä½¿ç”¨EVOREFUSEï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸¤ä¸ªæ–°æ•°æ®é›†ï¼Œæ˜¾è‘—æé«˜äº†æ‹’ç»è§¦å‘ç‡å’Œå“åº”ä¿¡å¿ƒåˆ†æ•°ï¼Œå‡å°‘äº†è¿‡åº¦æ‹’ç»çš„æƒ…å†µã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-06-09.html",
    "link_next": "2025-06-11.html",
    "link_month": "2025-06.html",
    "short_date_prev": {
        "ru": "09.06",
        "en": "06/09",
        "zh": "6æœˆ9æ—¥"
    },
    "short_date_next": {
        "ru": "11.06",
        "en": "06/11",
        "zh": "6æœˆ11æ—¥"
    },
    "categories": {
        "#dataset": 14,
        "#data": 7,
        "#benchmark": 10,
        "#agents": 3,
        "#cv": 6,
        "#rl": 7,
        "#rlhf": 4,
        "#rag": 0,
        "#plp": 0,
        "#inference": 5,
        "#3d": 2,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 10,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 3,
        "#healthcare": 1,
        "#training": 19,
        "#robotics": 2,
        "#agi": 1,
        "#games": 5,
        "#interpretability": 4,
        "#reasoning": 10,
        "#transfer_learning": 2,
        "#graphs": 2,
        "#ethics": 0,
        "#security": 2,
        "#optimization": 16,
        "#survey": 2,
        "#diffusion": 1,
        "#alignment": 4,
        "#story_generation": 0,
        "#hallucinations": 4,
        "#long_context": 5,
        "#synthetic": 6,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 11,
        "#small_models": 2,
        "#science": 2,
        "#low_resource": 1,
        "#medical": 1
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§å«åšå¼ºåŒ–é¢„è®­ç»ƒï¼ˆRPTï¼‰çš„æ–°æ–¹æ³•ã€‚å®ƒæŠŠä¸‹ä¸€ä¸ªè¯é¢„æµ‹å½“ä½œå¼ºåŒ–å­¦ä¹ ä»»åŠ¡æ¥è®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚è¿™æ ·å¯ä»¥æé«˜é¢„æµ‹çš„å‡†ç¡®æ€§ï¼Œå¹¶ä¸ºè¿›ä¸€æ­¥çš„å¾®è°ƒæä¾›åšå®åŸºç¡€ã€‚RPTåˆ©ç”¨å¤§é‡æ–‡æœ¬æ•°æ®ï¼Œä¸ä¾èµ–ç‰¹å®šé¢†åŸŸçš„æ ‡æ³¨ç­”æ¡ˆã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¢åŠ è®­ç»ƒè®¡ç®—é‡å¯ä»¥æé«˜é¢„æµ‹å‡†ç¡®æ€§ã€‚",
        "title": "Reinforcement Pre-Training",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§å«åšå¼ºåŒ–é¢„è®­ç»ƒï¼ˆRPTï¼‰çš„æ–°æ–¹æ³•ã€‚\nZhÃ¨ piÄn wÃ©nzhÄng jiÃ¨shÃ o le yÄ« zhÇ’ng jiÃ ozuÃ² qiÃ¡ng huÃ  yÃ¹ xÃ¹nliÃ n (RPT) de xÄ«n fÄngfÇ.\n\nå®ƒæŠŠä¸‹ä¸€ä¸ªè¯é¢„æµ‹å½“ä½œå¼ºåŒ–å­¦ä¹ ä»»åŠ¡æ¥è®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚\nTÄ bÇ xiÃ  yÄ« gÃ¨ cÃ­ yÃ¹cÃ¨ dÄngzuÃ² qiÃ¡ng huÃ  xuÃ©xÃ­ rÃ¨nwÃ¹ lÃ¡i xÃ¹nliÃ n yÇ”yÃ¡n mÃ³xÃ­ng.\n\nè¿™æ ·å¯ä»¥æé«˜é¢„æµ‹çš„å‡†ç¡®æ€§ï¼Œå¹¶ä¸ºè¿›ä¸€æ­¥çš„å¾®è°ƒæä¾›åšå®åŸºç¡€ã€‚\nZhÃ¨yÃ ng kÄ›yÇ tÃ­gÄo yÃ¹cÃ¨ de zhÇ”nquÃ¨xÃ¬ng, bÃ¬ng wÃ¨i jÃ¬n yÄ« bÃ¹ de wÄ“i tiÃ¡o tÃ­gÅng jiÄnshÃ­ jÄ«chÇ”.\n\nRPTåˆ©ç”¨å¤§é‡æ–‡æœ¬æ•°æ®ï¼Œä¸ä¾èµ–ç‰¹å®šé¢†åŸŸçš„æ ‡æ³¨ç­”æ¡ˆã€‚\nRPT lÃ¬yÃ²ng dÃ liÃ ng wÃ©nbÄ›n shÃ¹jÃ¹, bÃ¹ yÄ«lÃ i tÃ¨dÃ¬ng lÇngyÃ¹ de biÄozhÃ¹ dÃ¡'Ã n.\n\nå®éªŒç»“æœæ˜¾ç¤ºï¼Œå¢åŠ è®­ç»ƒè®¡ç®—é‡å¯ä»¥æé«˜é¢„æµ‹å‡†ç¡®æ€§ã€‚\nShÃ­yÃ n jiÃ©guÇ’ xiÇnshÃ¬, zÄ“ngjiÄ xÃ¹nliÃ n jÃ¬suÃ n liÃ ng kÄ›yÇ tÃ­gÄo yÃ¹cÃ¨ zhÇ”nquÃ¨xÃ¬ng.",
        "vocab": "[\n    {\"word\": \"å¼ºåŒ–\", \"pinyin\": \"qiÃ¡ng huÃ \", \"trans\": \"reinforcement\"},\n    {\"word\": \"é¢„è®­ç»ƒ\", \"pinyin\": \"yÃ¹ xÃ¹n liÃ n\", \"trans\": \"pre-training\"},\n    {\"word\": \"æ–¹æ³•\", \"pinyin\": \"fÄng fÇ\", \"trans\": \"method\"},\n    {\"word\": \"é¢„æµ‹\", \"pinyin\": \"yÃ¹ cÃ¨\", \"trans\": \"prediction\"},\n    {\"word\": \"ä»»åŠ¡\", \"pinyin\": \"rÃ¨n wu\", \"trans\": \"task\"},\n    {\"word\": \"æ¨¡å‹\", \"pinyin\": \"mÃ³ xÃ­ng\", \"trans\": \"model\"},\n    {\"word\": \"å‡†ç¡®æ€§\", \"pinyin\": \"zhÇ”n quÃ¨ xÃ¬ng\", \"trans\": \"accuracy\"},\n    {\"word\": \"å¾®è°ƒ\", \"pinyin\": \"wÄ“i tiÃ¡o\", \"trans\": \"fine-tuning\"},\n    {\"word\": \"åšå®\", \"pinyin\": \"jiÄn shÃ­\", \"trans\": \"solid\"},\n    {\"word\": \"åŸºç¡€\", \"pinyin\": \"jÄ« chÇ”\", \"trans\": \"foundation\"},\n    {\"word\": \"åˆ©ç”¨\", \"pinyin\": \"lÃ¬ yÃ²ng\", \"trans\": \"utilize\"},\n    {\"word\": \"æ–‡æœ¬\", \"pinyin\": \"wÃ©n bÄ›n\", \"trans\": \"text\"},\n    {\"word\": \"æ•°æ®\", \"pinyin\": \"shÃ¹ jÃ¹\", \"trans\": \"data\"},\n    {\"word\": \"ä¾èµ–\", \"pinyin\": \"yÄ« lÃ i\", \"trans\": \"depend on\"},\n    {\"word\": \"ç‰¹å®š\", \"pinyin\": \"tÃ¨ dÃ¬ng\", \"trans\": \"specific\"},\n    {\"word\": \"é¢†åŸŸ\", \"pinyin\": \"lÇng yÃ¹\", \"trans\": \"field\"},\n    {\"word\": \"æ ‡æ³¨\", \"pinyin\": \"biÄo zhÃ¹\", \"trans\": \"annotation\"},\n    {\"word\": \"ç­”æ¡ˆ\", \"pinyin\": \"dÃ¡ Ã n\", \"trans\": \"answer\"},\n    {\"word\": \"å®éªŒ\", \"pinyin\": \"shÃ­ yÃ n\", \"trans\": \"experiment\"},\n    {\"word\": \"ç»“æœ\", \"pinyin\": \"jiÃ© guÇ’\", \"trans\": \"result\"},\n    {\"word\": \"æ˜¾ç¤º\", \"pinyin\": \"xiÇn shÃ¬\", \"trans\": \"show\"},\n    {\"word\": \"å¢åŠ \", \"pinyin\": \"zÄ“ng jiÄ\", \"trans\": \"increase\"},\n    {\"word\": \"è®¡ç®—é‡\", \"pinyin\": \"jÃ¬ suÃ n liÃ ng\", \"trans\": \"computational amount\"}\n]",
        "trans": "This article introduces a new method called Reinforced Pre-Training (RPT). It trains language models by treating next-word prediction as a reinforcement learning task. This approach improves the accuracy of predictions and provides a solid foundation for further fine-tuning. RPT leverages a large amount of text data and does not rely on domain-specific annotated answers. Experimental results show that increasing the amount of training computation can enhance prediction accuracy.",
        "update_ts": "2025-06-10 09:13"
    }
}