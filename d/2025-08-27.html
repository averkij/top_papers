
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 12 papers. August 27.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">27 августа</span> | <span id="title-articles-count">12 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-08-26.html">⬅️ <span id="prev-date">26.08</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-08-28.html">➡️ <span id="next-date">28.08</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-08.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '27 августа', 'en': 'August 27', 'zh': '8月27日'};
        let feedDateNext = {'ru': '28.08', 'en': '08/28', 'zh': '8月28日'};
        let feedDatePrev = {'ru': '26.08', 'en': '08/26', 'zh': '8月26日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2508.17661', 'title': 'Spacer: Towards Engineered Scientific Inspiration', 'url': 'https://huggingface.co/papers/2508.17661', 'abstract': "Spacer, a scientific discovery system, uses deliberate decontextualization to generate creative and factually grounded scientific concepts from keyword sets, achieving high accuracy and similarity to leading publications.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in LLMs have made automated scientific research the next frontline in the path to artificial superintelligence. However, these systems are bound either to tasks of narrow scope or the limited creative capabilities of LLMs. We propose Spacer, a scientific discovery system that develops creative and factually grounded concepts without external intervention. Spacer attempts to achieve this via 'deliberate decontextualization,' an approach that disassembles information into atomic units - keywords - and draws creativity from unexplored connections between them. Spacer consists of (i) Nuri, an inspiration engine that builds keyword sets, and (ii) the Manifesting Pipeline that refines these sets into elaborate scientific statements. Nuri extracts novel, high-potential keyword sets from a keyword graph built with 180,000 academic publications in biological fields. The Manifesting Pipeline finds links between keywords, analyzes their logical structure, validates their plausibility, and ultimately drafts original scientific concepts. According to our experiments, the evaluation metric of Nuri accurately classifies high-impact publications with an AUROC score of 0.737. Our Manifesting Pipeline also successfully reconstructs core concepts from the latest top-journal articles solely from their keyword sets. An LLM-based scoring system estimates that this reconstruction was sound for over 85% of the cases. Finally, our embedding space analysis shows that outputs from Spacer are significantly more similar to leading publications compared with those from SOTA LLMs.", 'score': 17, 'issue_id': 5563, 'pub_date': '2025-08-25', 'pub_date_card': {'ru': '25 августа', 'en': 'August 25', 'zh': '8月25日'}, 'hash': '5ecb3211ea5e7d8d', 'pdf_title_img': 'assets/pdf/title_img/2508.17661.jpg', 'data': {'categories': ['#science', '#data', '#multimodal', '#dataset'], 'emoji': '🧬', 'ru': {'title': 'Spacer: Революция в автоматизированных научных открытиях', 'desc': 'Spacer - это система научных открытий, использующая намеренную деконтекстуализацию для генерации креативных и фактически обоснованных научных концепций из наборов ключевых слов. Система состоит из двух компонентов: Nuri, создающего наборы ключевых слов, и Manifesting Pipeline, преобразующего эти наборы в развернутые научные утверждения. Эксперименты показали высокую точность классификации высокоимпактных публикаций и успешную реконструкцию ключевых концепций из статей ведущих журналов. Анализ векторного пространства выявил, что результаты Spacer значительно ближе к ведущим публикациям по сравнению с современными языковыми моделями.'}, 'en': {'title': 'Spacer: Unleashing Creativity in Scientific Discovery', 'desc': "Spacer is a scientific discovery system that generates innovative and accurate scientific concepts by using a method called deliberate decontextualization. This process breaks down information into basic units, or keywords, and explores new connections between them to foster creativity. The system includes Nuri, which creates keyword sets from a vast database of academic publications, and the Manifesting Pipeline, which refines these sets into coherent scientific statements. Experimental results show that Spacer's outputs are highly similar to top-tier publications, outperforming existing language models in terms of relevance and originality."}, 'zh': {'title': 'Spacer：创新科学概念的发现系统', 'desc': 'Spacer是一个科学发现系统，通过故意去上下文化的方法，从关键词集合中生成创造性且事实基础的科学概念。该系统将信息拆解为原子单位——关键词，并从它们之间未被探索的联系中汲取创造力。Spacer包括两个部分：Nuri，一个灵感引擎，用于构建关键词集合；以及Manifesting Pipeline，用于将这些集合精炼成详细的科学陈述。实验结果表明，Nuri能够准确分类高影响力的出版物，而Manifesting Pipeline能够成功重建最新顶级期刊文章的核心概念。'}}}, {'id': 'https://huggingface.co/papers/2508.19205', 'title': 'VibeVoice Technical Report', 'url': 'https://huggingface.co/papers/2508.19205', 'abstract': "VibeVoice synthesizes long-form multi-speaker speech using next-token diffusion and a highly efficient continuous speech tokenizer, achieving superior performance and fidelity.  \t\t\t\t\tAI-generated summary \t\t\t\t This report presents VibeVoice, a novel model designed to synthesize long-form speech with multiple speakers by employing next-token diffusion, which is a unified method for modeling continuous data by autoregressively generating latent vectors via diffusion. To enable this, we introduce a novel continuous speech tokenizer that, when compared to the popular Encodec model, improves data compression by 80 times while maintaining comparable performance. The tokenizer effectively preserves audio fidelity while significantly boosting computational efficiency for processing long sequences. Thus, VibeVoice can synthesize long-form speech for up to 90 minutes (in a 64K context window length) with a maximum of 4 speakers, capturing the authentic conversational ``vibe'' and surpassing open-source and proprietary dialogue models.", 'score': 15, 'issue_id': 5562, 'pub_date': '2025-08-26', 'pub_date_card': {'ru': '26 августа', 'en': 'August 26', 'zh': '8月26日'}, 'hash': 'fc5dc3d2ea656b66', 'authors': ['Zhiliang Peng', 'Jianwei Yu', 'Wenhui Wang', 'Yaoyao Chang', 'Yutao Sun', 'Li Dong', 'Yi Zhu', 'Weijiang Xu', 'Hangbo Bao', 'Zehua Wang', 'Shaohan Huang', 'Yan Xia', 'Furu Wei'], 'affiliations': ['Microsoft Research'], 'pdf_title_img': 'assets/pdf/title_img/2508.19205.jpg', 'data': {'categories': ['#open_source', '#long_context', '#diffusion', '#audio'], 'emoji': '🗣️', 'ru': {'title': 'VibeVoice: революция в синтезе длительной многоголосой речи', 'desc': 'VibeVoice - это новая модель для синтеза длительной многоголосой речи, использующая диффузию следующего токена и эффективный непрерывный токенизатор речи. Модель способна генерировать до 90 минут речи с участием до 4 говорящих, сохраняя естественность разговора. По сравнению с популярной моделью Encodec, токенизатор VibeVoice улучшает сжатие данных в 80 раз при сопоставимом качестве. VibeVoice превосходит как открытые, так и проприетарные модели диалогов по качеству и достоверности синтезированной речи.'}, 'en': {'title': 'VibeVoice: Revolutionizing Multi-Speaker Speech Synthesis', 'desc': 'VibeVoice is a new model that creates long speeches with multiple speakers using a technique called next-token diffusion. This method generates speech by predicting the next part of the audio in a smart way, making it efficient for continuous data. The model also introduces a special speech tokenizer that compresses data much better than existing models, allowing it to maintain high audio quality while being faster. With VibeVoice, users can generate up to 90 minutes of realistic multi-speaker conversations, outperforming other dialogue systems.'}, 'zh': {'title': 'VibeVoice：高效合成多说话人长语音的创新模型', 'desc': 'VibeVoice是一种新型模型，旨在合成长时间的多说话人语音。它采用了下一步扩散的方法，通过自回归生成潜在向量来建模连续数据。为了实现这一点，我们引入了一种新型的连续语音标记器，与流行的Encodec模型相比，数据压缩提高了80倍，同时保持了相似的性能。VibeVoice能够合成最长可达90分钟的语音，捕捉真实的对话氛围，超越了开源和专有的对话模型。'}}}, {'id': 'https://huggingface.co/papers/2508.19209', 'title': 'OmniHuman-1.5: Instilling an Active Mind in Avatars via Cognitive\n  Simulation', 'url': 'https://huggingface.co/papers/2508.19209', 'abstract': "A framework using Multimodal Large Language Models and a specialized Multimodal DiT architecture generates semantically coherent and expressive character animations from multimodal inputs.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing video avatar models can produce fluid human animations, yet they struggle to move beyond mere physical likeness to capture a character's authentic essence. Their motions typically synchronize with low-level cues like audio rhythm, lacking a deeper semantic understanding of emotion, intent, or context. To bridge this gap, we propose a framework designed to generate character animations that are not only physically plausible but also semantically coherent and expressive. Our model, OmniHuman-1.5, is built upon two key technical contributions. First, we leverage Multimodal Large Language Models to synthesize a structured textual representation of conditions that provides high-level semantic guidance. This guidance steers our motion generator beyond simplistic rhythmic synchronization, enabling the production of actions that are contextually and emotionally resonant. Second, to ensure the effective fusion of these multimodal inputs and mitigate inter-modality conflicts, we introduce a specialized Multimodal DiT architecture with a novel Pseudo Last Frame design. The synergy of these components allows our model to accurately interpret the joint semantics of audio, images, and text, thereby generating motions that are deeply coherent with the character, scene, and linguistic content. Extensive experiments demonstrate that our model achieves leading performance across a comprehensive set of metrics, including lip-sync accuracy, video quality, motion naturalness and semantic consistency with textual prompts. Furthermore, our approach shows remarkable extensibility to complex scenarios, such as those involving multi-person and non-human subjects. Homepage: https://omnihuman-lab.github.io/v1_5/", 'score': 10, 'issue_id': 5563, 'pub_date': '2025-08-26', 'pub_date_card': {'ru': '26 августа', 'en': 'August 26', 'zh': '8月26日'}, 'hash': 'a23b1da7bec45638', 'authors': ['Jianwen Jiang', 'Weihong Zeng', 'Zerong Zheng', 'Jiaqi Yang', 'Chao Liang', 'Wang Liao', 'Han Liang', 'Yuan Zhang', 'Mingyuan Gao'], 'affiliations': ['Intelligent Creation Lab, ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2508.19209.jpg', 'data': {'categories': ['#architecture', '#interpretability', '#optimization', '#games', '#multimodal', '#video'], 'emoji': '🎭', 'ru': {'title': 'Семантически осмысленная анимация персонажей с помощью мультимодального ИИ', 'desc': 'Эта статья представляет новую модель OmniHuman-1.5 для генерации анимации персонажей, которая использует мультимодальные языковые модели (Multimodal LLM) для создания семантически связных движений. Авторы предлагают специализированную архитектуру Multimodal DiT с дизайном Pseudo Last Frame для эффективного объединения мультимодальных входных данных. Модель способна интерпретировать совместную семантику аудио, изображений и текста, создавая движения, согласованные с характером персонажа, сценой и лингвистическим содержанием. Эксперименты показывают превосходную производительность модели по различным метрикам, включая точность синхронизации губ и семантическую согласованность с текстовыми подсказками.'}, 'en': {'title': 'Animating Characters with Emotion and Context', 'desc': "This paper presents a framework called OmniHuman-1.5 that enhances character animation by integrating Multimodal Large Language Models with a specialized Multimodal DiT architecture. Unlike traditional models that focus on physical likeness, this framework aims to create animations that reflect deeper emotional and contextual understanding. By synthesizing structured textual representations, the model guides motion generation to produce actions that resonate with the character's intent and the surrounding context. The results show significant improvements in lip-sync accuracy, video quality, and overall motion naturalness, making it adaptable for complex scenarios involving multiple characters or non-human entities."}, 'zh': {'title': '生成富有表现力的角色动画', 'desc': '本论文提出了一种框架，利用多模态大型语言模型和专门的多模态DiT架构，从多模态输入生成语义连贯且富有表现力的角色动画。现有的视频头像模型虽然能够生成流畅的人类动画，但往往无法捕捉角色的真实本质，动作多依赖于低级线索如音频节奏。我们的模型OmniHuman-1.5通过合成结构化的文本表示，提供高层次的语义指导，使得动作生成超越简单的节奏同步，能够产生与上下文和情感相符的动作。实验结果表明，该模型在唇同步精度、视频质量、动作自然性和与文本提示的语义一致性等多个指标上表现优异，且在复杂场景中具有良好的扩展性。'}}}, {'id': 'https://huggingface.co/papers/2508.18756', 'title': 'UltraMemV2: Memory Networks Scaling to 120B Parameters with Superior\n  Long-Context Learning', 'url': 'https://huggingface.co/papers/2508.18756', 'abstract': 'UltraMemV2, a redesigned memory-layer architecture, achieves performance parity with 8-expert MoE models while significantly reducing memory access costs.  \t\t\t\t\tAI-generated summary \t\t\t\t While Mixture of Experts (MoE) models achieve remarkable efficiency by activating only subsets of parameters, they suffer from high memory access costs during inference. Memory-layer architectures offer an appealing alternative with very few memory access, but previous attempts like UltraMem have only matched the performance of 2-expert MoE models, falling significantly short of state-of-the-art 8-expert configurations. We present UltraMemV2, a redesigned memory-layer architecture that closes this performance gap. Our approach introduces five key improvements: integrating memory layers into every transformer block, simplifying value expansion with single linear projections, adopting FFN-based value processing from PEER, implementing principled parameter initialization, and rebalancing memory-to-FFN computation ratios. Through extensive evaluation, we demonstrate that UltraMemV2 achieves performance parity with 8-expert MoE models under same computation and parameters but significantly low memory access. Notably, UltraMemV2 shows superior performance on memory-intensive tasks, with improvements of +1.6 points on long-context memorization, +6.2 points on multi-round memorization, and +7.9 points on in-context learning. We validate our approach at scale with models up to 2.5B activated parameters from 120B total parameters, and establish that activation density has greater impact on performance than total sparse parameter count. Our work brings memory-layer architectures to performance parity with state-of-the-art MoE models, presenting a compelling alternative for efficient sparse computation.', 'score': 8, 'issue_id': 5563, 'pub_date': '2025-08-26', 'pub_date_card': {'ru': '26 августа', 'en': 'August 26', 'zh': '8月26日'}, 'hash': 'baef9d7d7f92b5b0', 'pdf_title_img': 'assets/pdf/title_img/2508.18756.jpg', 'data': {'categories': ['#architecture', '#long_context', '#optimization', '#training'], 'emoji': '🧠', 'ru': {'title': 'UltraMemV2: Эффективность MoE без высоких затрат на память', 'desc': 'UltraMemV2 - это новая архитектура слоя памяти для нейронных сетей, которая достигает производительности 8-экспертных моделей Mixture of Experts (MoE) при значительно меньших затратах на доступ к памяти. Ключевые улучшения включают интеграцию слоев памяти в каждый блок трансформера, упрощение расширения значений и принципиальную инициализацию параметров. UltraMemV2 показывает превосходную производительность на задачах, требующих интенсивного использования памяти, таких как запоминание длинного контекста и обучение в контексте. Эта архитектура представляет собой перспективную альтернативу моделям MoE для эффективных разреженных вычислений.'}, 'en': {'title': 'UltraMemV2: Bridging Memory Efficiency and Expert Performance', 'desc': 'UltraMemV2 is a new memory-layer architecture designed to improve the efficiency of machine learning models by reducing memory access costs. It achieves performance similar to advanced 8-expert Mixture of Experts (MoE) models while using fewer resources. The architecture incorporates several enhancements, such as integrating memory layers into transformer blocks and optimizing value processing. This results in better performance on tasks that require extensive memory, making UltraMemV2 a strong contender for efficient sparse computation.'}, 'zh': {'title': 'UltraMemV2：高效内存层架构的突破', 'desc': 'UltraMemV2是一种重新设计的内存层架构，能够在显著降低内存访问成本的同时，实现与8专家混合专家（MoE）模型的性能相当。该架构通过将内存层集成到每个变换器块中、简化值扩展、采用基于前馈神经网络的值处理等五个关键改进，成功缩小了与高性能MoE模型之间的差距。经过广泛评估，UltraMemV2在相同计算和参数下，显著降低了内存访问，同时在内存密集型任务上表现优越。我们的研究表明，激活密度对性能的影响大于稀疏参数的总数，展示了内存层架构在高效稀疏计算中的潜力。'}}}, {'id': 'https://huggingface.co/papers/2508.19247', 'title': 'VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D\n  Space', 'url': 'https://huggingface.co/papers/2508.19247', 'abstract': 'VoxHammer is a training-free method that performs precise and coherent 3D editing in latent space, ensuring consistency in preserved regions and high-quality overall results.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D local editing of specified regions is crucial for game industry and robot interaction. Recent methods typically edit rendered multi-view images and then reconstruct 3D models, but they face challenges in precisely preserving unedited regions and overall coherence. Inspired by structured 3D generative models, we propose VoxHammer, a novel training-free approach that performs precise and coherent editing in 3D latent space. Given a 3D model, VoxHammer first predicts its inversion trajectory and obtains its inverted latents and key-value tokens at each timestep. Subsequently, in the denoising and editing phase, we replace the denoising features of preserved regions with the corresponding inverted latents and cached key-value tokens. By retaining these contextual features, this approach ensures consistent reconstruction of preserved areas and coherent integration of edited parts. To evaluate the consistency of preserved regions, we constructed Edit3D-Bench, a human-annotated dataset comprising hundreds of samples, each with carefully labeled 3D editing regions. Experiments demonstrate that VoxHammer significantly outperforms existing methods in terms of both 3D consistency of preserved regions and overall quality. Our method holds promise for synthesizing high-quality edited paired data, thereby laying the data foundation for in-context 3D generation. See our project page at https://huanngzh.github.io/VoxHammer-Page/.', 'score': 4, 'issue_id': 5562, 'pub_date': '2025-08-26', 'pub_date_card': {'ru': '26 августа', 'en': 'August 26', 'zh': '8月26日'}, 'hash': '1b1c0dd833778383', 'authors': ['Lin Li', 'Zehuan Huang', 'Haoran Feng', 'Gengxiong Zhuang', 'Rui Chen', 'Chunchao Guo', 'Lu Sheng'], 'affiliations': ['Beihang University', 'Renmin University of China', 'Tencent Hunyuan', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2508.19247.jpg', 'data': {'categories': ['#games', '#dataset', '#synthetic', '#3d'], 'emoji': '🔨', 'ru': {'title': 'Точное 3D-редактирование без компромиссов', 'desc': 'VoxHammer - это метод редактирования 3D-моделей в латентном пространстве без дополнительного обучения. Он обеспечивает точное и согласованное редактирование указанных областей, сохраняя при этом неизмененные части модели. VoxHammer предсказывает траекторию инверсии 3D-модели и использует инвертированные латентные представления и токены для сохранения контекстуальных особенностей. Эксперименты показывают, что метод превосходит существующие подходы по качеству и согласованности результатов редактирования.'}, 'en': {'title': 'VoxHammer: Precision and Coherence in 3D Latent Space Editing', 'desc': 'VoxHammer is a novel method for editing 3D models in latent space without the need for training. It focuses on maintaining the consistency of unedited regions while ensuring high-quality results in the edited areas. By predicting an inversion trajectory and utilizing key-value tokens, VoxHammer effectively integrates changes while preserving contextual features. This approach outperforms existing techniques, making it valuable for applications in the gaming industry and robotics.'}, 'zh': {'title': 'VoxHammer：无训练的精确3D编辑新方法', 'desc': 'VoxHammer是一种无需训练的方法，能够在潜在空间中进行精确且连贯的3D编辑。该方法确保了保留区域的一致性和整体结果的高质量。与传统方法不同，VoxHammer直接在3D潜在空间中进行编辑，避免了多视图图像渲染和重建模型的复杂性。实验结果表明，VoxHammer在保留区域的3D一致性和整体质量方面显著优于现有方法。'}}}, {'id': 'https://huggingface.co/papers/2508.17437', 'title': 'Pixie: Fast and Generalizable Supervised Learning of 3D Physics from\n  Pixels', 'url': 'https://huggingface.co/papers/2508.17437', 'abstract': 'PIXIE, a neural network method, predicts physical properties of 3D scenes from visual features, enabling fast and realistic physics simulation using supervised learning and pretrained visual features.  \t\t\t\t\tAI-generated summary \t\t\t\t Inferring the physical properties of 3D scenes from visual information is a critical yet challenging task for creating interactive and realistic virtual worlds. While humans intuitively grasp material characteristics such as elasticity or stiffness, existing methods often rely on slow, per-scene optimization, limiting their generalizability and application. To address this problem, we introduce PIXIE, a novel method that trains a generalizable neural network to predict physical properties across multiple scenes from 3D visual features purely using supervised losses. Once trained, our feed-forward network can perform fast inference of plausible material fields, which coupled with a learned static scene representation like Gaussian Splatting enables realistic physics simulation under external forces. To facilitate this research, we also collected PIXIEVERSE, one of the largest known datasets of paired 3D assets and physic material annotations. Extensive evaluations demonstrate that PIXIE is about 1.46-4.39x better and orders of magnitude faster than test-time optimization methods. By leveraging pretrained visual features like CLIP, our method can also zero-shot generalize to real-world scenes despite only ever been trained on synthetic data. https://pixie-3d.github.io/', 'score': 3, 'issue_id': 5563, 'pub_date': '2025-08-20', 'pub_date_card': {'ru': '20 августа', 'en': 'August 20', 'zh': '8月20日'}, 'hash': '113a80a24546f00c', 'authors': ['Long Le', 'Ryan Lucas', 'Chen Wang', 'Chuhao Chen', 'Dinesh Jayaraman', 'Eric Eaton', 'Lingjie Liu'], 'affiliations': ['Massachusetts Institute of Technology', 'University of Pennsylvania'], 'pdf_title_img': 'assets/pdf/title_img/2508.17437.jpg', 'data': {'categories': ['#optimization', '#dataset', '#inference', '#synthetic', '#3d'], 'emoji': '🧠', 'ru': {'title': 'Быстрое предсказание физических свойств 3D-сцен с помощью нейронных сетей', 'desc': 'PIXIE - это нейросетевой метод, который предсказывает физические свойства 3D-сцен на основе визуальных признаков. Он использует обучение с учителем и предобученные визуальные признаки для быстрого и реалистичного физического моделирования. PIXIE превосходит методы оптимизации во время теста по точности и скорости работы. Метод способен к обобщению на реальные сцены, несмотря на обучение только на синтетических данных.'}, 'en': {'title': 'Fast and Realistic Physics Simulation with PIXIE', 'desc': 'PIXIE is a neural network approach designed to predict the physical properties of 3D scenes from visual features, which enhances the speed and realism of physics simulations. Unlike traditional methods that require slow optimization for each scene, PIXIE uses supervised learning to create a generalizable model that can infer material characteristics across various environments. The model is trained on a large dataset called PIXIEVERSE, which includes 3D assets and their corresponding physical property annotations. By utilizing pretrained visual features, PIXIE can also apply its knowledge to real-world scenes, achieving significant improvements in performance and efficiency compared to existing methods.'}, 'zh': {'title': 'PIXIE：快速预测三维场景物理属性的神经网络', 'desc': 'PIXIE是一种神经网络方法，可以从视觉特征中预测三维场景的物理属性，从而实现快速而真实的物理模拟。该方法通过监督学习和预训练的视觉特征，克服了传统方法在每个场景上优化的慢速限制。PIXIE训练出一个可泛化的神经网络，能够在多个场景中预测物理属性，并且在推理时速度极快。通过结合学习到的静态场景表示，PIXIE能够在外力作用下进行真实的物理模拟。'}}}, {'id': 'https://huggingface.co/papers/2508.18773', 'title': 'ThinkDial: An Open Recipe for Controlling Reasoning Effort in Large\n  Language Models', 'url': 'https://huggingface.co/papers/2508.18773', 'abstract': "ThinkDial is an open-source framework that implements controllable reasoning in large language models through discrete operational modes, achieving performance while reducing computational effort.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) with chain-of-thought reasoning have demonstrated remarkable problem-solving capabilities, but controlling their computational effort remains a significant challenge for practical deployment. Recent proprietary systems like OpenAI's gpt-oss series have introduced discrete operational modes for intuitive reasoning control, but the open-source community has largely failed to achieve such capabilities. In this paper, we introduce ThinkDial, the first open-recipe end-to-end framework that successfully implements gpt-oss-style controllable reasoning through discrete operational modes. Our system enables seamless switching between three distinct reasoning regimes: High mode (full reasoning capability), Medium mode (50 percent token reduction with <10 percent performance degradation), and Low mode (75 percent token reduction with <15 percent performance degradation). We achieve this through an end-to-end training paradigm that integrates budget-mode control throughout the entire pipeline: budget-mode supervised fine-tuning that embeds controllable reasoning capabilities directly into the learning process, and two-phase budget-aware reinforcement learning with adaptive reward shaping. Extensive experiments demonstrate that ThinkDial achieves target compression-performance trade-offs with clear response length reductions while maintaining performance thresholds. The framework also exhibits strong generalization capabilities on out-of-distribution tasks.", 'score': 1, 'issue_id': 5563, 'pub_date': '2025-08-26', 'pub_date_card': {'ru': '26 августа', 'en': 'August 26', 'zh': '8月26日'}, 'hash': '2f5e648d646cae5b', 'authors': ['Qianyu He', 'Siyu Yuan', 'Xuefeng Li', 'Mingxuan Wang', 'Jiangjie Chen'], 'affiliations': ['ByteDance Seed', 'Fudan University', 'SIA-Lab of Tsinghua AIR', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2508.18773.jpg', 'data': {'categories': ['#architecture', '#open_source', '#training', '#optimization', '#reasoning', '#agi', '#rl'], 'emoji': '🧠', 'ru': {'title': 'ThinkDial: контролируемое рассуждение в больших языковых моделях', 'desc': 'ThinkDial - это фреймворк с открытым исходным кодом, который реализует контролируемое рассуждение в больших языковых моделях через дискретные режимы работы. Система позволяет переключаться между тремя режимами рассуждения: полным, средним (50% сокращение токенов) и низким (75% сокращение). Это достигается с помощью сквозной парадигмы обучения, включающей контролируемую тонкую настройку и обучение с подкреплением. ThinkDial демонстрирует хорошие результаты по соотношению сжатия и производительности, а также обобщающую способность на новых задачах.'}, 'en': {'title': "ThinkDial: Control Your AI's Thinking Power!", 'desc': "ThinkDial is an innovative open-source framework designed to enhance large language models (LLMs) by enabling controllable reasoning through discrete operational modes. It allows users to switch between three reasoning modes: High, Medium, and Low, which vary in computational effort and performance. This framework incorporates budget-mode control during training, ensuring that reasoning capabilities are embedded into the model's learning process. Extensive testing shows that ThinkDial effectively balances performance and efficiency, making it a valuable tool for practical applications of LLMs."}, 'zh': {'title': 'ThinkDial：可控推理的新开源框架', 'desc': 'ThinkDial是一个开源框架，旨在通过离散操作模式实现大型语言模型的可控推理。该系统允许在三种不同的推理模式之间无缝切换：高模式（完全推理能力）、中模式（减少50%的令牌，性能下降不到10%）和低模式（减少75%的令牌，性能下降不到15%）。通过端到端的训练方法，ThinkDial在整个流程中集成了预算模式控制，确保了推理能力的可控性。实验结果表明，ThinkDial在压缩性能权衡方面表现出色，同时在处理超出分布的任务时也展现了强大的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2508.18621', 'title': 'Wan-S2V: Audio-Driven Cinematic Video Generation', 'url': 'https://huggingface.co/papers/2508.18621', 'abstract': 'Wan-S2V, an audio-driven model built on Wan, enhances expressiveness and fidelity in cinematic character animation compared to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Current state-of-the-art (SOTA) methods for audio-driven character animation demonstrate promising performance for scenarios primarily involving speech and singing. However, they often fall short in more complex film and television productions, which demand sophisticated elements such as nuanced character interactions, realistic body movements, and dynamic camera work. To address this long-standing challenge of achieving film-level character animation, we propose an audio-driven model, which we refere to as Wan-S2V, built upon Wan. Our model achieves significantly enhanced expressiveness and fidelity in cinematic contexts compared to existing approaches. We conducted extensive experiments, benchmarking our method against cutting-edge models such as Hunyuan-Avatar and Omnihuman. The experimental results consistently demonstrate that our approach significantly outperforms these existing solutions. Additionally, we explore the versatility of our method through its applications in long-form video generation and precise video lip-sync editing.', 'score': 1, 'issue_id': 5563, 'pub_date': '2025-08-26', 'pub_date_card': {'ru': '26 августа', 'en': 'August 26', 'zh': '8月26日'}, 'hash': '0e23abc799529ace', 'authors': ['Xin Gao', 'Li Hu', 'Siqi Hu', 'Mingyang Huang', 'Chaonan Ji', 'Dechao Meng', 'Jinwei Qi', 'Penchong Qiao', 'Zhen Shen', 'Yafei Song', 'Ke Sun', 'Linrui Tian', 'Guangyuan Wang', 'Qi Wang', 'Zhongjian Wang', 'Jiayu Xiao', 'Sheng Xu', 'Bang Zhang', 'Peng Zhang', 'Xindi Zhang', 'Zhe Zhang', 'Jingren Zhou', 'Lian Zhuo'], 'affiliations': ['Tongyi Lab, Alibaba'], 'pdf_title_img': 'assets/pdf/title_img/2508.18621.jpg', 'data': {'categories': ['#story_generation', '#audio', '#games', '#benchmark', '#video'], 'emoji': '🎭', 'ru': {'title': 'Wan-S2V: Революция в анимации кинематографических персонажей на основе аудио', 'desc': 'Модель Wan-S2V, основанная на аудио, улучшает выразительность и точность анимации кинематографических персонажей по сравнению с существующими методами. Она преодолевает ограничения современных подходов в сложных сценариях фильмов и телепередач, требующих нюансированного взаимодействия персонажей и реалистичных движений тела. Экспериментальные результаты показывают, что Wan-S2V значительно превосходит передовые модели, такие как Hunyuan-Avatar и Omnihuman. Модель также демонстрирует универсальность в генерации длинных видео и точном редактировании синхронизации губ.'}, 'en': {'title': 'Elevating Cinematic Animation with Wan-S2V', 'desc': "The paper introduces Wan-S2V, an advanced audio-driven model designed to improve character animation in cinematic contexts. Unlike current state-of-the-art methods that excel mainly in speech and singing, Wan-S2V addresses the complexities of film and television, including intricate character interactions and realistic movements. Through rigorous benchmarking against leading models like Hunyuan-Avatar and Omnihuman, the results show that Wan-S2V significantly enhances expressiveness and fidelity. Furthermore, the model's versatility is highlighted through its applications in long-form video generation and accurate lip-sync editing."}, 'zh': {'title': '提升电影角色动画的音频驱动模型', 'desc': 'Wan-S2V是一种基于音频驱动的模型，旨在提升电影角色动画的表现力和真实感。与现有方法相比，它在复杂的影视制作中表现更佳，能够处理细腻的角色互动、真实的身体动作和动态的镜头运作。我们通过与最先进的模型进行广泛实验，证明了Wan-S2V在动画效果上的显著优势。此外，该模型还具有在长视频生成和精确视频口型同步编辑中的应用潜力。'}}}, {'id': 'https://huggingface.co/papers/2508.18370', 'title': 'Training Language Model Agents to Find Vulnerabilities with CTF-Dojo', 'url': 'https://huggingface.co/papers/2508.18370', 'abstract': 'CTF-Dojo, a large-scale executable runtime with 658 CTF challenges, enables rapid training of LLM-based agents with verifiable feedback, achieving state-of-the-art performance in competitive benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have demonstrated exceptional capabilities when trained within executable runtime environments, notably excelling at software engineering tasks through verified feedback loops. Yet, scalable and generalizable execution-grounded environments remain scarce, limiting progress in training more capable ML agents. We introduce CTF-Dojo, the first large-scale executable runtime tailored for training LLMs with verifiable feedback, featuring 658 fully functional Capture-The-Flag (CTF)-style challenges containerized in Docker with guaranteed reproducibility. To enable rapid scaling without manual intervention, we develop CTF-Forge, an automated pipeline that transforms publicly available artifacts into ready-to-use execution environments in minutes, eliminating weeks of expert configuration traditionally required. We trained LLM-based agents on just 486 high-quality, execution-verified trajectories from CTF-Dojo, achieving up to 11.6% absolute gains over strong baselines across three competitive benchmarks: InterCode-CTF, NYU CTF Bench, and Cybench. Our best-performing 32B model reaches 31.9% Pass@1, establishing a new open-weight state-of-the-art that rivals frontier models like DeepSeek-V3-0324 and Gemini-2.5-Flash. By framing CTF-style tasks as a benchmark for executable-agent learning, CTF-Dojo demonstrates that execution-grounded training signals are not only effective but pivotal in advancing high-performance ML agents without dependence on costly proprietary systems.', 'score': 1, 'issue_id': 5563, 'pub_date': '2025-08-25', 'pub_date_card': {'ru': '25 августа', 'en': 'August 25', 'zh': '8月25日'}, 'hash': 'cd60ce12b233220d', 'authors': ['Terry Yue Zhuo', 'Dingmin Wang', 'Hantian Ding', 'Varun Kumar', 'Zijian Wang'], 'affiliations': ['Amazon', 'Monash University'], 'pdf_title_img': 'assets/pdf/title_img/2508.18370.jpg', 'data': {'categories': ['#open_source', '#training', '#dataset', '#games', '#benchmark', '#agents'], 'emoji': '🏆', 'ru': {'title': 'CTF-Dojo: революция в обучении ИИ-агентов через исполняемую среду', 'desc': 'CTF-Dojo - это крупномасштабная исполняемая среда с 658 задачами CTF, позволяющая быстро обучать агентов на основе больших языковых моделей с проверяемой обратной связью. Разработанный авторами конвейер CTF-Forge автоматизирует процесс создания сред выполнения из общедоступных артефактов. Обучение на всего 486 высококачественных траекториях из CTF-Dojo позволило достичь значительных улучшений по сравнению с сильными базовыми моделями в нескольких конкурентных бенчмарках. Лучшая 32-миллиардная модель авторов достигла нового открытого state-of-the-art результата, соперничая с передовыми моделями.'}, 'en': {'title': 'CTF-Dojo: Revolutionizing LLM Training with Executable Challenges', 'desc': 'CTF-Dojo is a large-scale platform designed to train large language models (LLMs) using executable runtime environments. It features 658 Capture-The-Flag (CTF) challenges that provide verifiable feedback, which is crucial for improving the performance of machine learning agents. The introduction of CTF-Forge allows for the rapid creation of these training environments from publicly available resources, significantly reducing setup time. As a result, LLM-based agents trained on CTF-Dojo have achieved state-of-the-art performance on competitive benchmarks, showcasing the effectiveness of execution-grounded training methods.'}, 'zh': {'title': 'CTF-Dojo：高效训练智能体的新平台', 'desc': 'CTF-Dojo是一个大型可执行运行环境，包含658个CTF挑战，旨在快速训练基于大语言模型（LLM）的智能体，并提供可验证的反馈。该平台通过自动化管道CTF-Forge，将公开可用的资源转化为可用的执行环境，显著减少了配置时间。研究表明，使用CTF-Dojo训练的智能体在多个基准测试中表现优异，取得了最高11.6%的绝对提升。CTF-Dojo展示了执行基础训练信号在提升高性能机器学习智能体方面的重要性，且不依赖于昂贵的专有系统。'}}}, {'id': 'https://huggingface.co/papers/2508.15774', 'title': 'CineScale: Free Lunch in High-Resolution Cinematic Visual Generation', 'url': 'https://huggingface.co/papers/2508.15774', 'abstract': 'CineScale is a novel inference paradigm that enables high-resolution visual generation for both images and videos without extensive fine-tuning, addressing issues of repetitive patterns and high-frequency information accumulation.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual diffusion models achieve remarkable progress, yet they are typically trained at limited resolutions due to the lack of high-resolution data and constrained computation resources, hampering their ability to generate high-fidelity images or videos at higher resolutions. Recent efforts have explored tuning-free strategies to exhibit the untapped potential higher-resolution visual generation of pre-trained models. However, these methods are still prone to producing low-quality visual content with repetitive patterns. The key obstacle lies in the inevitable increase in high-frequency information when the model generates visual content exceeding its training resolution, leading to undesirable repetitive patterns deriving from the accumulated errors. In this work, we propose CineScale, a novel inference paradigm to enable higher-resolution visual generation. To tackle the various issues introduced by the two types of video generation architectures, we propose dedicated variants tailored to each. Unlike existing baseline methods that are confined to high-resolution T2I and T2V generation, CineScale broadens the scope by enabling high-resolution I2V and V2V synthesis, built atop state-of-the-art open-source video generation frameworks. Extensive experiments validate the superiority of our paradigm in extending the capabilities of higher-resolution visual generation for both image and video models. Remarkably, our approach enables 8k image generation without any fine-tuning, and achieves 4k video generation with only minimal LoRA fine-tuning. Generated video samples are available at our website: https://eyeline-labs.github.io/CineScale/.', 'score': 1, 'issue_id': 5563, 'pub_date': '2025-08-21', 'pub_date_card': {'ru': '21 августа', 'en': 'August 21', 'zh': '8月21日'}, 'hash': '520a6e202fa3979c', 'pdf_title_img': 'assets/pdf/title_img/2508.15774.jpg', 'data': {'categories': ['#open_source', '#diffusion', '#inference', '#cv', '#video'], 'emoji': '🎬', 'ru': {'title': 'CineScale: Прорыв в генерации высококачественного визуального контента', 'desc': 'CineScale - это новая парадигма вывода, позволяющая генерировать изображения и видео высокого разрешения без обширной дополнительной настройки. Она решает проблемы повторяющихся паттернов и накопления высокочастотной информации, характерные для существующих методов. CineScale предлагает специализированные варианты для различных архитектур генерации видео, расширяя возможности создания визуального контента высокого разрешения. Эксперименты подтверждают превосходство этого подхода, позволяя генерировать 8K изображения без дополнительной настройки и 4K видео с минимальной настройкой LoRA.'}, 'en': {'title': 'CineScale: Elevating Visual Generation to New Resolutions', 'desc': 'CineScale is a new method for generating high-resolution images and videos without needing extensive adjustments to existing models. It addresses common problems like repetitive patterns and the buildup of high-frequency details that can occur when generating visuals at resolutions higher than those used during training. By introducing specialized versions of the model for different types of video generation, CineScale enhances the capabilities of pre-trained models, allowing for high-quality outputs. The results show that it can produce 8k images and 4k videos with minimal fine-tuning, significantly improving visual fidelity.'}, 'zh': {'title': 'CineScale：高分辨率视觉生成的新范式', 'desc': 'CineScale是一种新颖的推理范式，能够在不进行大量微调的情况下实现高分辨率的图像和视频生成。该方法解决了生成内容中重复模式和高频信息积累的问题，提升了视觉生成的质量。通过专门设计的变体，CineScale扩展了高分辨率图像到视频的合成能力，超越了现有的基线方法。实验结果表明，CineScale在高分辨率视觉生成方面具有显著优势，能够生成8k图像和4k视频。'}}}, {'id': 'https://huggingface.co/papers/2508.19188', 'title': 'FastMesh:Efficient Artistic Mesh Generation via Component Decoupling', 'url': 'https://huggingface.co/papers/2508.19188', 'abstract': 'A framework for efficient artistic mesh generation reduces redundancy by separating vertex and face generation, using an autoregressive model for vertices and a bidirectional transformer for faces, and includes a fidelity enhancer and post-processing to improve quality and speed.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent mesh generation approaches typically tokenize triangle meshes into sequences of tokens and train autoregressive models to generate these tokens sequentially. Despite substantial progress, such token sequences inevitably reuse vertices multiple times to fully represent manifold meshes, as each vertex is shared by multiple faces. This redundancy leads to excessively long token sequences and inefficient generation processes. In this paper, we propose an efficient framework that generates artistic meshes by treating vertices and faces separately, significantly reducing redundancy. We employ an autoregressive model solely for vertex generation, decreasing the token count to approximately 23\\% of that required by the most compact existing tokenizer. Next, we leverage a bidirectional transformer to complete the mesh in a single step by capturing inter-vertex relationships and constructing the adjacency matrix that defines the mesh faces. To further improve the generation quality, we introduce a fidelity enhancer to refine vertex positioning into more natural arrangements and propose a post-processing framework to remove undesirable edge connections. Experimental results show that our method achieves more than 8times faster speed on mesh generation compared to state-of-the-art approaches, while producing higher mesh quality.', 'score': 0, 'issue_id': 5563, 'pub_date': '2025-08-26', 'pub_date_card': {'ru': '26 августа', 'en': 'August 26', 'zh': '8月26日'}, 'hash': '31ec4c8e0ef6b3e8', 'authors': ['Jeonghwan Kim', 'Yushi Lan', 'Armando Fortes', 'Yongwei Chen', 'Xingang Pan'], 'affiliations': ['S-Lab, Nanyang Technological University'], 'pdf_title_img': 'assets/pdf/title_img/2508.19188.jpg', 'data': {'categories': ['#games', '#optimization', '#3d'], 'emoji': '🎨', 'ru': {'title': 'Эффективная генерация 3D-моделей: разделяй и властвуй', 'desc': 'Предложена эффективная система генерации художественных 3D-моделей, разделяющая процессы создания вершин и граней. Для вершин используется авторегрессионная модель, а для граней - двунаправленный трансформер, что значительно сокращает избыточность. Введены улучшатель точности для оптимизации расположения вершин и постобработка для устранения нежелательных соединений рёбер. Эксперименты показали 8-кратное ускорение генерации и повышение качества моделей по сравнению с современными методами.'}, 'en': {'title': 'Efficient Artistic Mesh Generation: Less Redundancy, More Quality!', 'desc': 'This paper presents a new framework for generating artistic meshes that improves efficiency by separating the processes of vertex and face generation. It uses an autoregressive model to generate vertices, which reduces the number of tokens needed by about 77% compared to traditional methods. For face generation, a bidirectional transformer is employed to quickly construct the mesh by understanding the relationships between vertices. Additionally, a fidelity enhancer and post-processing techniques are introduced to enhance the quality and speed of the generated meshes.'}, 'zh': {'title': '高效艺术网格生成的新框架', 'desc': '本文提出了一种高效的艺术网格生成框架，通过将顶点和面生成分开，减少了冗余。我们使用自回归模型生成顶点，显著降低了所需的令牌数量。接着，利用双向变换器一次性完成网格构建，捕捉顶点间的关系。最后，通过引入保真度增强器和后处理框架，进一步提高生成质量和速度。'}}}, {'id': 'https://huggingface.co/papers/2508.18192', 'title': 'Unraveling the cognitive patterns of Large Language Models through\n  module communities', 'url': 'https://huggingface.co/papers/2508.18192', 'abstract': 'A network-based framework links cognitive skills, LLM architectures, and datasets, revealing unique emergent skill patterns in LLMs that benefit from dynamic, cross-regional interactions.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have reshaped our world with significant advancements in science, engineering, and society through applications ranging from scientific discoveries and medical diagnostics to Chatbots. Despite their ubiquity and utility, the underlying mechanisms of LLM remain concealed within billions of parameters and complex structures, making their inner architecture and cognitive processes challenging to comprehend. We address this gap by adopting approaches to understanding emerging cognition in biology and developing a network-based framework that links cognitive skills, LLM architectures, and datasets, ushering in a paradigm shift in foundation model analysis. The skill distribution in the module communities demonstrates that while LLMs do not strictly parallel the focalized specialization observed in specific biological systems, they exhibit unique communities of modules whose emergent skill patterns partially mirror the distributed yet interconnected cognitive organization seen in avian and small mammalian brains. Our numerical results highlight a key divergence from biological systems to LLMs, where skill acquisition benefits substantially from dynamic, cross-regional interactions and neural plasticity. By integrating cognitive science principles with machine learning, our framework provides new insights into LLM interpretability and suggests that effective fine-tuning strategies should leverage distributed learning dynamics rather than rigid modular interventions.', 'score': 0, 'issue_id': 5563, 'pub_date': '2025-08-25', 'pub_date_card': {'ru': '25 августа', 'en': 'August 25', 'zh': '8月25日'}, 'hash': '6ce511ea51abb060', 'authors': ['Kushal Raj Bhandari', 'Pin-Yu Chen', 'Jianxi Gao'], 'affiliations': ['IBM Research', 'Rensselaer Polytechnic Institute'], 'pdf_title_img': 'assets/pdf/title_img/2508.18192.jpg', 'data': {'categories': ['#architecture', '#science', '#training', '#interpretability', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Сетевой анализ раскрывает когнитивные паттерны в языковых моделях', 'desc': 'Данная статья представляет сетевую модель, связывающую когнитивные навыки, архитектуры языковых моделей (LLM) и наборы данных. Исследование выявляет уникальные паттерны появления навыков в LLM, которые выигрывают от динамических взаимодействий между различными областями модели. Авторы обнаружили, что распределение навыков в модулях LLM частично отражает распределенную, но взаимосвязанную когнитивную организацию, наблюдаемую в мозге птиц и мелких млекопитающих. Результаты подчеркивают важность пластичности и межрегиональных взаимодействий в процессе обучения LLM, что отличает их от биологических систем.'}, 'en': {'title': 'Unlocking LLMs: A Networked Approach to Cognitive Skills and Architecture', 'desc': 'This paper presents a network-based framework that connects cognitive skills with the architectures of Large Language Models (LLMs) and the datasets they are trained on. It reveals that LLMs exhibit unique emergent skill patterns that are influenced by dynamic interactions across different regions of their architecture, similar to cognitive processes in biological systems. The study highlights that while LLMs do not strictly mimic the specialization found in biological brains, they show a distributed and interconnected organization of skills. The findings suggest that understanding these emergent skills can improve LLM interpretability and inform better fine-tuning strategies that utilize flexible learning dynamics.'}, 'zh': {'title': '揭示大型语言模型的认知技能与架构的联系', 'desc': '这篇论文提出了一个基于网络的框架，连接了认知技能、语言模型架构和数据集，揭示了大型语言模型（LLMs）中独特的技能模式。这些模型在动态的跨区域交互中受益，展现出与生物系统不同的技能获取方式。研究表明，LLMs的模块社区虽然不完全与特定生物系统的专业化相似，但它们的技能模式部分反映了鸟类和小型哺乳动物大脑的分布式认知组织。通过将认知科学原理与机器学习结合，该框架为LLMs的可解释性提供了新见解，并建议有效的微调策略应利用分布式学习动态，而非僵化的模块干预。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (3)', '#agents (1)', '#agi (1)', '#alignment', '#architecture (4)', '#audio (2)', '#benchmark (2)', '#cv (1)', '#data (1)', '#dataset (5)', '#diffusion (2)', '#ethics', '#games (5)', '#graphs', '#hallucinations', '#healthcare', '#inference (2)', '#interpretability (2)', '#leakage', '#long_context (2)', '#low_resource', '#machine_translation', '#math', '#multilingual', '#multimodal (2)', '#open_source (4)', '#optimization (5)', '#plp', '#rag', '#reasoning (1)', '#rl (1)', '#rlhf', '#robotics', '#science (2)', '#security', '#small_models', '#story_generation (1)', '#survey', '#synthetic (2)', '#training (4)', '#transfer_learning', '#video (3)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-08-27 03:30',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-08-27 03:30')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-08-27 03:30')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    