{
    "date": {
        "ru": "18 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
        "en": "February 18",
        "zh": "2æœˆ18æ—¥"
    },
    "time_utc": "2025-02-18 03:13",
    "weekday": 1,
    "issue_id": 2263,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2502.11275",
            "title": "Cuckoo: An IE Free Rider Hatched by Massive Nutrition in LLM's Nest",
            "url": "https://huggingface.co/papers/2502.11275",
            "abstract": "Massive high-quality data, both pre-training raw texts and post-training annotations, have been carefully prepared to incubate advanced large language models (LLMs). In contrast, for information extraction (IE), pre-training data, such as BIO-tagged sequences, are hard to scale up. We show that IE models can act as free riders on LLM resources by reframing next-token prediction into extraction for tokens already present in the context. Specifically, our proposed next tokens extraction (NTE) paradigm learns a versatile IE model, Cuckoo, with 102.6M extractive data converted from LLM's pre-training and post-training data. Under the few-shot setting, Cuckoo adapts effectively to traditional and complex instruction-following IE with better performance than existing pre-trained IE models. As a free rider, Cuckoo can naturally evolve with the ongoing advancements in LLM data preparation, benefiting from improvements in LLM training pipelines without additional manual effort.",
            "score": 1,
            "issue_id": 2263,
            "pub_date": "2025-02-16",
            "pub_date_card": {
                "ru": "16 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 16",
                "zh": "2æœˆ16æ—¥"
            },
            "hash": "6444052efad6f8be",
            "authors": [
                "Letian Peng",
                "Zilong Wang",
                "Feng Yao",
                "Jingbo Shang"
            ],
            "affiliations": [
                "University of California, San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.11275.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#dataset",
                    "#data",
                    "#transfer_learning"
                ],
                "emoji": "ğŸ£",
                "ru": {
                    "title": "Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¿Ğ»ĞµÑ‡Ğ°Ñ… Ğ³Ğ¸Ğ³Ğ°Ğ½Ñ‚Ğ¾Ğ²: ĞºĞ°Ğº IE Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµÑÑƒÑ€ÑÑ‹ LLM",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ (IE) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 'Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²' (NTE) Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑƒĞ¶Ğµ Ğ¿Ñ€Ğ¸ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Cuckoo, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° 102,6 Ğ¼Ğ»Ğ½ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¼Ğ°Ğ»Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ IE Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ IE Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ²Ğ¼ĞµÑÑ‚Ğµ Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ² Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ LLM Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ÑƒÑ‡Ğ½Ñ‹Ñ… ÑƒÑĞ¸Ğ»Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Leveraging LLMs for Enhanced Information Extraction",
                    "desc": "This paper introduces a new approach for information extraction (IE) using large language models (LLMs) as a resource. The authors propose a method called next tokens extraction (NTE), which allows IE models to leverage existing LLM data for training. They present a model named Cuckoo, which is trained on 102.6 million extractive data points derived from LLMs, showing superior performance in few-shot scenarios. Cuckoo's design enables it to adapt to various IE tasks while benefiting from ongoing improvements in LLM training without requiring extra manual data preparation."
                },
                "zh": {
                    "title": "åˆ©ç”¨LLMæå‡ä¿¡æ¯æå–æ¨¡å‹çš„æ€§èƒ½",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥æå‡ä¿¡æ¯æå–ï¼ˆIEï¼‰æ¨¡å‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æå–æ–¹æ³•ï¼Œç§°ä¸ºä¸‹ä¸€æ ‡è®°æå–ï¼ˆNTEï¼‰ï¼Œé€šè¿‡å°†ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹è½¬åŒ–ä¸ºå¯¹ä¸Šä¸‹æ–‡ä¸­å·²å­˜åœ¨æ ‡è®°çš„æå–ï¼Œä»è€Œä½¿IEæ¨¡å‹èƒ½å¤Ÿåˆ©ç”¨LLMçš„èµ„æºã€‚æˆ‘ä»¬å¼€å‘çš„Cuckooæ¨¡å‹åœ¨å°‘é‡æ ·æœ¬çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆé€‚åº”ä¼ ç»Ÿå’Œå¤æ‚çš„æŒ‡ä»¤è·ŸéšIEä»»åŠ¡ï¼Œå¹¶ä¸”è¡¨ç°ä¼˜äºç°æœ‰çš„é¢„è®­ç»ƒIEæ¨¡å‹ã€‚Cuckooä½œä¸ºä¸€ä¸ªâ€œæ­ä¾¿è½¦è€…â€ï¼Œèƒ½å¤Ÿéšç€LLMæ•°æ®å‡†å¤‡çš„è¿›æ­¥è€Œè‡ªç„¶æ¼”å˜ï¼Œæ— éœ€é¢å¤–çš„äººå·¥åŠªåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.11901",
            "title": "Building A Proof-Oriented Programmer That Is 64% Better Than GPT-4o Under Data Scarsity",
            "url": "https://huggingface.co/papers/2502.11901",
            "abstract": "Existing LMs struggle with proof-oriented programming due to data scarcity, which manifest in two key ways: (1) a lack of sufficient corpora for proof-oriented programming languages such as F*, and (2) the absence of large-scale, project-level proof-oriented implementations that can teach the model the intricate reasoning process when performing proof-oriented programming. We present the first on synthetic data augmentation for project level proof oriented programming for both generation and repair. Our method addresses data scarcity by synthesizing basic proof-oriented programming problems for proficiency in that language; incorporating diverse coding data for reasoning capability elicitation and creating new proofs and repair data within existing repositories. This approach enables language models to both synthesize and repair proofs for function- and repository-level code. We show that our fine-tuned 14B parameter model, PoPilot, can exceed the performance of the models that outperforms GPT-4o in project-level proof-oriented programming by 64% relative margin, and can improve GPT-4o's performance by 54% by repairing its outputs over GPT-4o's self-repair.",
            "score": 1,
            "issue_id": 2263,
            "pub_date": "2025-02-17",
            "pub_date_card": {
                "ru": "17 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 17",
                "zh": "2æœˆ17æ—¥"
            },
            "hash": "9451c99877c67e4d",
            "authors": [
                "Dylan Zhang",
                "Justin Wang",
                "Tianran Sun"
            ],
            "affiliations": [
                "Shanghai Jiaotong University",
                "University of Chicago",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.11901.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#dataset",
                    "#data",
                    "#plp",
                    "#transfer_learning",
                    "#synthetic"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ñ‹ Ğ² Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ½Ğ° Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµÑ…Ğ²Ğ°Ñ‚ĞºĞ¸ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ¾Ğ² Ğ½Ğ° ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ PoPilot, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ GPT-4 Ğ½Ğ° 64% Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ GPT-4 Ğ½Ğ° 54% Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞµĞ³Ğ¾ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Enhancing Proof-Oriented Programming with Synthetic Data Augmentation",
                    "desc": "This paper addresses the challenges faced by language models (LMs) in proof-oriented programming due to limited data availability. It introduces a novel approach of synthetic data augmentation to enhance the training of LMs for generating and repairing proofs in programming languages like F*. The method involves creating basic proof-oriented programming problems and utilizing diverse coding data to improve reasoning capabilities. The results demonstrate that the fine-tuned 14B parameter model, PoPilot, significantly outperforms existing models, including GPT-4o, in project-level proof-oriented programming tasks."
                },
                "zh": {
                    "title": "åˆæˆæ•°æ®å¢å¼ºï¼Œæå‡è¯æ˜ç¼–ç¨‹èƒ½åŠ›ï¼",
                    "desc": "ç°æœ‰çš„è¯­è¨€æ¨¡å‹åœ¨é¢å‘è¯æ˜çš„ç¼–ç¨‹ä¸­é¢ä¸´æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œä¸»è¦ä½“ç°åœ¨ä¸¤ä¸ªæ–¹é¢ï¼šç¼ºä¹è¶³å¤Ÿçš„é¢å‘è¯æ˜ç¼–ç¨‹è¯­è¨€ï¼ˆå¦‚F*ï¼‰çš„è¯­æ–™åº“ï¼Œä»¥åŠç¼ºå°‘å¤§è§„æ¨¡çš„é¡¹ç›®çº§è¯æ˜å®ç°ï¼Œæ— æ³•æ•™ä¼šæ¨¡å‹å¤æ‚çš„æ¨ç†è¿‡ç¨‹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºåˆæˆæ•°æ®å¢å¼ºçš„æ–¹æ³•ï¼Œä¸“æ³¨äºé¡¹ç›®çº§çš„é¢å‘è¯æ˜ç¼–ç¨‹ï¼Œæ—¢ç”¨äºç”Ÿæˆä¹Ÿç”¨äºä¿®å¤ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆæˆåŸºæœ¬çš„é¢å‘è¯æ˜ç¼–ç¨‹é—®é¢˜æ¥è§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œå¹¶ç»“åˆå¤šæ ·åŒ–çš„ç¼–ç æ•°æ®ä»¥æé«˜æ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶åœ¨ç°æœ‰ä»£ç åº“ä¸­åˆ›å»ºæ–°çš„è¯æ˜å’Œä¿®å¤æ•°æ®ã€‚æˆ‘ä»¬çš„14Bå‚æ•°æ¨¡å‹PoPilotç»è¿‡å¾®è°ƒåï¼Œåœ¨é¡¹ç›®çº§é¢å‘è¯æ˜ç¼–ç¨‹ä¸­è¶…è¶Šäº†GPT-4oæ¨¡å‹64%çš„æ€§èƒ½ï¼Œå¹¶é€šè¿‡ä¿®å¤å…¶è¾“å‡ºæé«˜äº†54%çš„æ€§èƒ½ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-02-17.html",
    "link_next": "2025-02-19.html",
    "link_month": "2025-02.html",
    "short_date_prev": {
        "ru": "17.02",
        "en": "02/17",
        "zh": "2æœˆ17æ—¥"
    },
    "short_date_next": {
        "ru": "19.02",
        "en": "02/19",
        "zh": "2æœˆ19æ—¥"
    },
    "categories": {
        "#dataset": 2,
        "#data": 2,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 1,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 2,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨å„ç§é¢†åŸŸçš„ç”Ÿæˆä»»åŠ¡ä¸­æˆä¸ºé¦–é€‰ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä¾èµ–å¤šæ¬¡é¡ºåºå‰å‘ä¼ é€’ï¼Œæ˜¾è‘—é™åˆ¶äº†å®æ—¶æ€§èƒ½ã€‚ä»¥å‰çš„åŠ é€Ÿæ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å‡å°‘é‡‡æ ·æ­¥éª¤æˆ–é‡ç”¨ä¸­é—´ç»“æœï¼Œæœªèƒ½åˆ©ç”¨å›¾åƒå†…éƒ¨ç©ºé—´åŒºåŸŸçš„å˜åŒ–ã€‚é€šè¿‡åˆ©ç”¨æ‰©æ•£å˜å‹å™¨ï¼ˆDiTsï¼‰å¤„ç†å¯å˜æ•°é‡çš„æ ‡è®°çš„çµæ´»æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†RASï¼Œä¸€ç§æ–°çš„æ— éœ€è®­ç»ƒçš„é‡‡æ ·ç­–ç•¥ï¼Œæ ¹æ®DiTæ¨¡å‹çš„å…³æ³¨ç‚¹åŠ¨æ€åˆ†é…å›¾åƒå†…ä¸åŒåŒºåŸŸçš„é‡‡æ ·æ¯”ç‡ã€‚æˆ‘ä»¬çš„å…³é”®è§‚å¯Ÿæ˜¯ï¼Œåœ¨æ¯ä¸ªé‡‡æ ·æ­¥éª¤ä¸­ï¼Œæ¨¡å‹é›†ä¸­åœ¨è¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„åŒºåŸŸï¼Œè¿™äº›å…³æ³¨åŒºåŸŸåœ¨è¿ç»­æ­¥éª¤ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„è¿ç»­æ€§ã€‚åˆ©ç”¨è¿™ä¸€æ´å¯Ÿï¼ŒRASä»…æ›´æ–°å½“å‰å…³æ³¨çš„åŒºåŸŸï¼Œè€Œå…¶ä»–åŒºåŸŸä½¿ç”¨ä¸Šä¸€æ­¥çš„ç¼“å­˜å™ªå£°æ›´æ–°ã€‚æ¨¡å‹çš„å…³æ³¨ç‚¹æ ¹æ®å‰ä¸€æ­¥çš„è¾“å‡ºç¡®å®šï¼Œåˆ©ç”¨äº†æˆ‘ä»¬è§‚å¯Ÿåˆ°çš„æ—¶é—´ä¸€è‡´æ€§ã€‚æˆ‘ä»¬åœ¨Stable Diffusion 3å’ŒLumina-Next-T2Iä¸Šè¯„ä¼°RASï¼Œåˆ†åˆ«å®ç°äº†æœ€é«˜2.36å€å’Œ2.51å€çš„åŠ é€Ÿï¼Œç”Ÿæˆè´¨é‡ä»…è½»å¾®ä¸‹é™ã€‚æ­¤å¤–ï¼Œç”¨æˆ·ç ”ç©¶è¡¨æ˜ï¼ŒRASåœ¨äººç±»è¯„ä¼°ä¸‹æä¾›äº†ç›¸ä¼¼çš„è´¨é‡ï¼ŒåŒæ—¶å®ç°äº†1.6å€çš„åŠ é€Ÿã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ›´é«˜æ•ˆçš„æ‰©æ•£å˜å‹å™¨æ–¹é¢å–å¾—äº†é‡è¦è¿›å±•ï¼Œå¢å¼ºäº†å®ƒä»¬åœ¨å®æ—¶åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚",
        "title": "Region-Adaptive Sampling for Diffusion Transformers",
        "pinyin": "æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨å„ç§é¢†åŸŸçš„ç”Ÿæˆä»»åŠ¡ä¸­æˆä¸ºé¦–é€‰ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä¾èµ–å¤šæ¬¡é¡ºåºå‰å‘ä¼ é€’ï¼Œæ˜¾è‘—é™åˆ¶äº†å®æ—¶æ€§èƒ½ã€‚ä»¥å‰çš„åŠ é€Ÿæ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å‡å°‘é‡‡æ ·æ­¥éª¤æˆ–é‡ç”¨ä¸­é—´ç»“æœï¼Œæœªèƒ½åˆ©ç”¨å›¾åƒå†…éƒ¨ç©ºé—´åŒºåŸŸçš„å˜åŒ–ã€‚é€šè¿‡åˆ©ç”¨æ‰©æ•£å˜å‹å™¨ï¼ˆDiTsï¼‰å¤„ç†å¯å˜æ•°é‡çš„æ ‡è®°çš„çµæ´»æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†RASï¼Œä¸€ç§æ–°çš„æ— éœ€è®­ç»ƒçš„é‡‡æ ·ç­–ç•¥ï¼Œæ ¹æ®DiTæ¨¡å‹çš„å…³æ³¨ç‚¹åŠ¨æ€åˆ†é…å›¾åƒå†…ä¸åŒåŒºåŸŸçš„é‡‡æ ·æ¯”ç‡ã€‚æˆ‘ä»¬çš„å…³é”®è§‚å¯Ÿæ˜¯ï¼Œåœ¨æ¯ä¸ªé‡‡æ ·æ­¥éª¤ä¸­ï¼Œæ¨¡å‹é›†ä¸­åœ¨è¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„åŒºåŸŸï¼Œè¿™äº›å…³æ³¨åŒºåŸŸåœ¨è¿ç»­æ­¥éª¤ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„è¿ç»­æ€§ã€‚åˆ©ç”¨è¿™ä¸€æ´å¯Ÿï¼ŒRASä»…æ›´æ–°å½“å‰å…³æ³¨çš„åŒºåŸŸï¼Œè€Œå…¶ä»–åŒºåŸŸä½¿ç”¨ä¸Šä¸€æ­¥çš„ç¼“å­˜å™ªå£°æ›´æ–°ã€‚æ¨¡å‹çš„å…³æ³¨ç‚¹æ ¹æ®å‰ä¸€æ­¥çš„è¾“å‡ºç¡®å®šï¼Œåˆ©ç”¨äº†æˆ‘ä»¬è§‚å¯Ÿåˆ°çš„æ—¶é—´ä¸€è‡´æ€§ã€‚æˆ‘ä»¬åœ¨Stable Diffusion 3å’ŒLumina-Next-T2Iä¸Šè¯„ä¼°RASï¼Œåˆ†åˆ«å®ç°äº†æœ€é«˜2.36å€å’Œ2.51å€çš„åŠ é€Ÿï¼Œç”Ÿæˆè´¨é‡ä»…è½»å¾®ä¸‹é™ã€‚æ­¤å¤–ï¼Œç”¨æˆ·ç ”ç©¶è¡¨æ˜ï¼ŒRASåœ¨äººç±»è¯„ä¼°ä¸‹æä¾›äº†ç›¸ä¼¼çš„è´¨é‡ï¼ŒåŒæ—¶å®ç°äº†1.6å€çš„åŠ é€Ÿã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ›´é«˜æ•ˆçš„æ‰©æ•£å˜å‹å™¨æ–¹é¢å–å¾—äº†é‡è¦è¿›å±•ï¼Œå¢å¼ºäº†å®ƒä»¬åœ¨å®æ—¶åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚\n\nkuÃ² sÃ n mÃ³ xÃ­ng (DMs) zÃ i gÃ¨ zhÇ’ng lÇng yÃ¹ de shÄ“ng chÃ©ng rÃ¨n wÃ¹ zhÅng chÃ©ng wÃ©i shÇ’u xuÇn. rÃ¡n Ã©r, tÄ men yÄ« lÃ i duÅ cÃ¬ shÃ¹n xÃ¹ qiÃ¡n xiÄng chuÃ¡n dÃ¬, xiÇn zhÃ¹ xiÃ n zhÃ¬ le shÃ­ shÃ­ xÃ¬ng nÃ©ng. yÇ qiÃ¡n de jiÄ sÃ¹ fÄng fÇ zhÇ” yÃ o jÄ« zhÅng zÃ i jiÇn shÇo cÇi yÃ ng bÃ¹ zhÃ²u huÃ² chÃ³ng yÃ²ng zhÅng jiÄn jiÃ© guÇ’, wÃ¨i nÃ©ng lÃ¬ yÃ²ng tÃº xiÃ ng nÃ¨i bÃ¹ kÅng jiÄn qÅ« yÃ¹ de biÃ n huÃ . tÅng guÃ² lÃ¬ yÃ²ng kuÃ² sÃ n biÃ n shÅ« zhÇ” (DiTs) chÇ” lÇ kÄ› biÃ n shÃ¹ liÃ ng de biÄo jÃ¬ de lÃ­ng huÃ³ xÃ¬ng, wÇ’ men yÇn rÃ¹ le RAS, yÄ« zhÇ’ng xÄ«n de wÃº xÅ« xÃ¹n liÃ n de cÇi yÃ ng cÃ¨ lÃ¼Ã¨, gÄ“n jÃ¹ DiT mÃ³ xÃ­ng de guÄn zhÃ¹ diÇn dÃ²ng tÃ i fÄ“n pÃ¨i tÃº xiÃ ng nÃ¨i bÃ¹ tÅng qÅ« yÃ¹ de cÇi yÃ ng bÇ lÇœ. wÇ’ men de guÇn jiÃ n guÄn chÃ¡ shÃ¬, zÃ i mÄ›i gÃ¨ cÇi yÃ ng bÃ¹ zhÃ²u zhÅng, mÃ³ xÃ­ng jÃ­ zhÅng zÃ i yÇ” yÃ¬ shÃ ng yÇ’u yÃ¬ yÃ¬ de qÅ« yÃ¹, zhÃ¨ xiÄ“ guÄn zhÃ¹ qÅ« yÃ¹ zÃ i liÃ¡n xÃ¹ bÃ¹ zhÃ²u zhÅng biÇo xiÃ n chÅ« qiÃ¡ng dÃ  de liÃ¡n xÃ¹ xÃ¬ng. lÃ¬ yÃ²ng zhÃ¨ yÄ« dÃ²ng chÃ¡, RAS jÇn gÄ“ng xÄ«n shÇ dÄng qiÃ¡n guÄn zhÃ¹ de qÅ« yÃ¹, Ã©r qÃ­ tÄ qÅ« yÃ¹ shÇ yÃ²ng shÃ ng yÄ« bÃ¹ de huÇn cÃ¹n zÃ o shÄ“ng gÄ“ng xÄ«n. mÃ³ xÃ­ng de guÄn zhÃ¹ diÇn gÄ“n jÃ¹ qiÃ¡n yÄ« bÃ¹ de shÅ« chÅ« quÃ¨ dÃ¬ng, lÃ¬ yÃ²ng le wÇ’ men guÄn chÃ¡ dÃ o de shÃ­ jiÄn yÄ« zhÃ¬ xÃ¬ng. wÇ’ men zÃ i Stable Diffusion 3 hÃ© Lumina-Next-T2I shÃ ng pÃ­ng guÇ RAS, fÄ“n biÃ© shÃ­ xiÃ n le zuÃ¬ gÄo 2.36 bÃ¨i hÃ© 2.51 bÃ¨i de jiÄ sÃ¹, shÄ“ng chÃ©ng zhÃ¬ liÃ ng jÇn qÄ«ng wÄ“i xiÃ  jiÃ ng. cÇ wÃ i, yÃ²ng hÃ¹ yÃ¡n jiÅ« biÇo mÃ­ng, RAS zÃ i rÃ©n lÃ¨i pÃ­ng jiÃ  xiÃ  tÃ­ gÅng le xiÄng sÃ¬ de zhÃ¬ liÃ ng, tÃ³ng shÃ­ shÃ­ xiÃ n le 1.6 bÃ¨i de jiÄ sÃ¹. wÇ’ men de fÄng fÇ zÃ i gÃ¨ng gÄo xiÃ o de kuÃ² sÃ n biÃ n shÅ« zhÇ” fÄng miÃ n zhÇ” dÃ© dÃ o le zhÃ²ng yÃ o jÃ¬n zhÇn, zÄ“ng qiÃ¡ng le tÄ men zÃ i shÃ­ shÃ­ yÃ¬ng yÃ²ng zhÅng de qiÃ¡n lÃ¬.",
        "vocab": "[\n    {\"word\": \"æ‰©æ•£æ¨¡å‹\", \"pinyin\": \"kuÃ² sÃ n mÃ³ xÃ­ng\", \"trans\": \"diffusion model\"},\n    {\"word\": \"é¦–é€‰\", \"pinyin\": \"shÇ’u xuÇn\", \"trans\": \"preferred choice\"},\n    {\"word\": \"ä¾èµ–\", \"pinyin\": \"yÄ« lÃ i\", \"trans\": \"depend on\"},\n    {\"word\": \"é¡ºåº\", \"pinyin\": \"shÃ¹n xÃ¹\", \"trans\": \"sequential\"},\n    {\"word\": \"å‰å‘ä¼ é€’\", \"pinyin\": \"qiÃ¡n xiÃ ng chuÃ¡n dÃ¬\", \"trans\": \"forward pass\"},\n    {\"word\": \"æ˜¾è‘—\", \"pinyin\": \"xiÇn zhÃ¹\", \"trans\": \"significant\"},\n    {\"word\": \"é™åˆ¶\", \"pinyin\": \"xiÃ n zhÃ¬\", \"trans\": \"limit\"},\n    {\"word\": \"å®æ—¶æ€§èƒ½\", \"pinyin\": \"shÃ­ shÃ­ xÃ¬ng nÃ©ng\", \"trans\": \"real-time performance\"},\n    {\"word\": \"åŠ é€Ÿ\", \"pinyin\": \"jiÄ sÃ¹\", \"trans\": \"accelerate\"},\n    {\"word\": \"æ–¹æ³•\", \"pinyin\": \"fÄng fÇ\", \"trans\": \"method\"},\n    {\"word\": \"é›†ä¸­\", \"pinyin\": \"jÃ­ zhÅng\", \"trans\": \"focus on\"},\n    {\"word\": \"å‡å°‘\", \"pinyin\": \"jiÇn shÇo\", \"trans\": \"reduce\"},\n    {\"word\": \"é‡‡æ ·æ­¥éª¤\", \"pinyin\": \"cÇi yÃ ng bÃ¹ zhÃ²u\", \"trans\": \"sampling steps\"},\n    {\"word\": \"é‡ç”¨\", \"pinyin\": \"chÃ³ng yÃ²ng\", \"trans\": \"reuse\"},\n    {\"word\": \"ä¸­é—´ç»“æœ\", \"pinyin\": \"zhÅng jiÄn jiÃ© guÇ’\", \"trans\": \"intermediate results\"},\n    {\"word\": \"åˆ©ç”¨\", \"pinyin\": \"lÃ¬ yÃ²ng\", \"trans\": \"utilize\"},\n    {\"word\": \"å›¾åƒ\", \"pinyin\": \"tÃº xiÃ ng\", \"trans\": \"image\"},\n    {\"word\": \"å†…éƒ¨ç©ºé—´åŒºåŸŸ\", \"pinyin\": \"nÃ¨i bÃ¹ kÅng jiÄn qÅ« yÃ¹\", \"trans\": \"internal spatial regions\"},\n    {\"word\": \"å˜åŒ–\", \"pinyin\": \"biÃ n huÃ \", \"trans\": \"change\"},\n    {\"word\": \"æ‰©æ•£å˜å‹å™¨\", \"pinyin\": \"kuÃ² sÃ n biÃ n yÄ qÃ¬\", \"trans\": \"diffusion transformer\"},\n    {\"word\": \"çµæ´»æ€§\", \"pinyin\": \"lÃ­ng huÃ³ xÃ¬ng\", \"trans\": \"flexibility\"},\n    {\"word\": \"å¼•å…¥\", \"pinyin\": \"yÇn rÃ¹\", \"trans\": \"introduce\"},\n    {\"word\": \"RAS\", \"pinyin\": \"RAS\", \"trans\": \"RAS\"},\n    {\"word\": \"é‡‡æ ·ç­–ç•¥\", \"pinyin\": \"cÇi yÃ ng cÃ¨ lÃ¼Ã¨\", \"trans\": \"sampling strategy\"},\n    {\"word\": \"åŠ¨æ€åˆ†é…\", \"pinyin\": \"dÃ²ng tÃ i fÄ“n pÃ¨i\", \"trans\": \"dynamic allocation\"},\n    {\"word\": \"å…³æ³¨ç‚¹\", \"pinyin\": \"guÄn zhÃ¹ diÇn\", \"trans\": \"focus points\"},\n    {\"word\": \"å…³é”®è§‚å¯Ÿ\", \"pinyin\": \"guÇn jiÃ n guÄn chÃ¡\", \"trans\": \"key observation\"},\n    {\"word\": \"è¯­ä¹‰\", \"pinyin\": \"yÇ” yÃ¬\", \"trans\": \"semantic\"},\n    {\"word\": \"æœ‰æ„ä¹‰\", \"pinyin\": \"yÇ’u yÃ¬ yÃ¬\", \"trans\": \"meaningful\"},\n    {\"word\": \"è¿ç»­æ­¥éª¤\", \"pinyin\": \"liÃ¡n xÃ¹ bÃ¹ zhÃ²u\", \"trans\": \"continuous steps\"},\n    {\"word\": \"è¡¨ç°\", \"pinyin\": \"biÇo xiÃ n\", \"trans\": \"performance\"},\n    {\"word\": \"è¿ç»­æ€§\", \"pinyin\": \"liÃ¡n xÃ¹ xÃ¬ng\", \"trans\": \"continuity\"},\n    {\"word\": \"æ´å¯Ÿ\", \"pinyin\": \"dÃ²ng chÃ¡\", \"trans\": \"insight\"},\n    {\"word\": \"æ›´æ–°\", \"pinyin\": \"gÄ“ng xÄ«n\", \"trans\": \"update\"},\n    {\"word\": \"ç¼“å­˜å™ªå£°\", \"pinyin\": \"huÇn cÃºn zÃ o shÄ“ng\", \"trans\": \"cached noise\"},\n    {\"word\": \"ç¡®å®š\", \"pinyin\": \"quÃ¨ dÃ¬ng\", \"trans\": \"determine\"},\n    {\"word\": \"æ—¶é—´ä¸€è‡´æ€§\", \"pinyin\": \"shÃ­ jiÄn yÄ« zhÃ¬ xÃ¬ng\", \"trans\": \"temporal consistency\"},\n    {\"word\": \"è¯„ä¼°\", \"pinyin\": \"pÃ­ng gÅ«\", \"trans\": \"evaluate\"},\n    {\"word\": \"Stable Diffusion 3\", \"pinyin\": \"Stable Diffusion 3\", \"trans\": \"Stable Diffusion 3\"},\n    {\"word\": \"Lumina-Next-T2I\", \"pinyin\": \"Lumina-Next-T2I\", \"trans\": \"Lumina-Next-T2I\"},\n    {\"word\": \"å®ç°\", \"pinyin\": \"shÃ­ xiÃ n\", \"trans\": \"achieve\"},\n    {\"word\": \"åŠ é€Ÿ\", \"pinyin\": \"jiÄ sÃ¹\", \"trans\": \"acceleration\"},\n    {\"word\": \"ç”Ÿæˆè´¨é‡\", \"pinyin\": \"shÄ“ng chÃ©ng zhÃ¬ liÃ ng\", \"trans\": \"generation quality\"},\n    {\"word\": \"è½»å¾®ä¸‹é™\", \"pinyin\": \"qÄ«ng wÄ“i xiÃ  jiÃ ng\", \"trans\": \"slight decrease\"},\n    {\"word\": \"ç”¨æˆ·ç ”ç©¶\", \"pinyin\": \"yÃ²ng hÃ¹ yÃ¡n jiÅ«\", \"trans\": \"user study\"},\n    {\"word\": \"äººç±»è¯„ä¼°\", \"pinyin\": \"rÃ©n lÃ¨i pÃ­ng gÅ«\", \"trans\": \"human evaluation\"},\n    {\"word\": \"ç›¸ä¼¼\", \"pinyin\": \"xiÄng sÃ¬\", \"trans\": \"similar\"},\n    {\"word\": \"æ½œåŠ›\", \"pinyin\": \"qiÃ¡n lÃ¬\", \"trans\": \"potential\"},\n    {\"word\": \"é‡è¦è¿›å±•\", \"pinyin\": \"zhÃ²ng yÃ o jÃ¬n zhÇn\", \"trans\": \"significant progress\"}\n]",
        "trans": "Diffusion models (DMs) have become the preferred choice for generative tasks in various fields. However, they rely on multiple sequential forward passes, significantly limiting real-time performance. Previous acceleration methods have primarily focused on reducing sampling steps or reusing intermediate results, failing to leverage variations in spatial regions within images. By exploiting the flexibility of diffusion transformers (DiTs) in handling a variable number of tokens, we introduce RAS, a new training-free sampling strategy that dynamically allocates sampling ratios to different regions within an image based on the attention focus of the DiT model. Our key observation is that, at each sampling step, the model concentrates on semantically meaningful regions, and these attention regions exhibit strong continuity across consecutive steps. Leveraging this insight, RAS updates only the currently attended regions, while other regions are updated using cached noise from the previous step. The model's attention focus is determined based on the output from the previous step, utilizing the temporal consistency we observed. We evaluate RAS on Stable Diffusion 3 and Lumina-Next-T2I, achieving up to 2.36x and 2.51x speedup, respectively, with only a slight decrease in generation quality. Additionally, user studies indicate that RAS provides similar quality under human evaluation while achieving a 1.6x speedup. Our method represents a significant advancement in more efficient diffusion transformers, enhancing their potential for real-time applications.",
        "update_ts": "2025-02-17 09:12"
    }
}