{
    "date": {
        "ru": "2 –¥–µ–∫–∞–±—Ä—è",
        "en": "December 2",
        "zh": "12Êúà2Êó•"
    },
    "time_utc": "2024-12-02 07:11",
    "weekday": 0,
    "issue_id": 888,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.19930",
            "title": "On Domain-Specific Post-Training for Multimodal Large Language Models",
            "url": "https://huggingface.co/papers/2411.19930",
            "abstract": "Recent years have witnessed the rapid development of general multimodal large language models (MLLMs). However, adapting general MLLMs to specific domains, such as scientific fields and industrial applications, remains less explored. This paper systematically investigates domain adaptation of MLLMs through post-training, focusing on data synthesis, training pipelines, and task evaluation. (1) Data Synthesis: Using open-source models, we develop a visual instruction synthesizer that effectively generates diverse visual instruction tasks from domain-specific image-caption pairs. Our synthetic tasks surpass those generated by manual rules, GPT-4, and GPT-4V in enhancing the domain-specific performance of MLLMs. (2) Training Pipeline: While the two-stage training--initially on image-caption pairs followed by visual instruction tasks--is commonly adopted for developing general MLLMs, we apply a single-stage training pipeline to enhance task diversity for domain-specific post-training. (3) Task Evaluation: We conduct experiments in two domains, biomedicine and food, by post-training MLLMs of different sources and scales (e.g., Qwen2-VL-2B, LLaVA-v1.6-8B, Llama-3.2-11B), and then evaluating MLLM performance on various domain-specific tasks. To support further research in MLLM domain adaptation, we will open-source our implementations.",
            "score": 11,
            "issue_id": 885,
            "pub_date": "2024-11-29",
            "pub_date_card": {
                "ru": "29 –Ω–æ—è–±—Ä—è",
                "en": "November 29",
                "zh": "11Êúà29Êó•"
            },
            "hash": "5d14749b38f15e60",
            "authors": [
                "Daixuan Cheng",
                "Shaohan Huang",
                "Ziyu Zhu",
                "Xintong Zhang",
                "Wayne Xin Zhao",
                "Zhongzhi Luan",
                "Bo Dai",
                "Zhenliang Zhang"
            ],
            "affiliations": [
                "Beihang University",
                "Beijing Institute of Technology",
                "Renmin University of China",
                "State Key Laboratory of General Artificial Intelligence, BIGAI",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.19930.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#dataset",
                    "#training",
                    "#open_source",
                    "#multimodal",
                    "#synthetic"
                ],
                "emoji": "üî¨",
                "ru": {
                    "title": "–ê–¥–∞–ø—Ç–∞—Ü–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò –∫ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –æ–±–ª–∞—Å—Ç—è–º",
                    "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –∞–¥–∞–ø—Ç–∞—Ü–∏—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –∫ —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–º –¥–æ–º–µ–Ω–∞–º. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ —Å–∏–Ω—Ç–µ–∑–∞—Ç–æ—Ä –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –¥–æ–º–µ–Ω–Ω—ã–µ –ø–∞—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ-–ø–æ–¥–ø–∏—Å—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á. –û–Ω–∏ –ø—Ä–∏–º–µ–Ω–∏–ª–∏ –æ–¥–Ω–æ—ç—Ç–∞–ø–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –∑–∞–¥–∞—á –ø—Ä–∏ –¥–æ–º–µ–Ω–Ω–æ–π –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–µ. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø—Ä–æ–≤–æ–¥–∏–ª–∏—Å—å –≤ –±–∏–æ–º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –∏ –ø–∏—â–µ–≤–æ–π –æ–±–ª–∞—Å—Ç—è—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã—Ö MLLM, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞."
                },
                "en": {
                    "title": "Enhancing MLLMs for Specific Domains through Innovative Adaptation Techniques",
                    "desc": "This paper explores how to adapt general multimodal large language models (MLLMs) for specific fields like biomedicine and food. It introduces a method for data synthesis that creates diverse visual instruction tasks from image-caption pairs, outperforming previous methods. The authors propose a single-stage training pipeline to improve task diversity during post-training, rather than the usual two-stage approach. Finally, they evaluate the performance of various MLLMs on domain-specific tasks and plan to share their findings to aid future research in this area."
                },
                "zh": {
                    "title": "ÊèêÂçáÂ§öÊ®°ÊÄÅÊ®°ÂûãÁöÑÈ¢ÜÂüüÈÄÇÂ∫îÊÄß",
                    "desc": "ËøëÂπ¥Êù•ÔºåÈÄöÁî®Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâËøÖÈÄüÂèëÂ±ï„ÄÇÁÑ∂ËÄåÔºåÂ∞ÜÈÄöÁî®MLLMsÈÄÇÂ∫î‰∫éÁâπÂÆöÈ¢ÜÂüüÔºåÂ¶ÇÁßëÂ≠¶ÂíåÂ∑•‰∏öÂ∫îÁî®Ôºå‰ªçÁÑ∂ËæÉÂ∞ëË¢´Êé¢Á¥¢„ÄÇÊú¨ÊñáÁ≥ªÁªüÁ†îÁ©∂‰∫ÜMLLMsÁöÑÈ¢ÜÂüüÈÄÇÂ∫îÊÄßÔºåÈáçÁÇπÂú®‰∫éÊï∞ÊçÆÂêàÊàê„ÄÅËÆ≠ÁªÉÊµÅÁ®ãÂíå‰ªªÂä°ËØÑ‰º∞„ÄÇÊàë‰ª¨ÂºÄÂèë‰∫Ü‰∏ÄÁßçËßÜËßâÊåá‰ª§ÂêàÊàêÂô®ÔºåËÉΩÂ§üÊúâÊïàÁîüÊàêÂ§öÊ†∑ÂåñÁöÑËßÜËßâÊåá‰ª§‰ªªÂä°Ôºå‰ªéËÄåÊèêÂçáMLLMsÂú®ÁâπÂÆöÈ¢ÜÂüüÁöÑË°®Áé∞„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.19460",
            "title": "Look Every Frame All at Once: Video-Ma$^2$mba for Efficient Long-form Video Understanding with Multi-Axis Gradient Checkpointing",
            "url": "https://huggingface.co/papers/2411.19460",
            "abstract": "With the growing scale and complexity of video data, efficiently processing long video sequences poses significant challenges due to the quadratic increase in memory and computational demands associated with existing transformer-based Large Multi-modal Models (LMMs). To address these issues, we introduce Video-Ma^2mba, a novel architecture that incorporates State Space Models (SSMs) within the Mamba-2 framework, replacing the attention mechanisms. This allows the LMMs to scale linearly in terms of time and memory requirements, making it feasible to handle long-duration video content. Furthermore, we enhance the memory efficiency introducing the Multi-Axis Gradient Checkpointing (MA-GC) method, which strategically manages memory by retaining only essential activations across multiple computational axes. Our approach significantly reduces the memory footprint compared to standard gradient checkpointing. Empirical analyses show that Video-Ma^2mba can process extensive video sequences-equivalent to millions of tokens or over two hours of continuous sequences at 1 FPS-on a single GPU. By maintaining a detailed capture of temporal dynamics, our model improves the accuracy and relevance of responses in long video understanding tasks, demonstrating substantial advantages over existing frameworks.",
            "score": 4,
            "issue_id": 886,
            "pub_date": "2024-11-29",
            "pub_date_card": {
                "ru": "29 –Ω–æ—è–±—Ä—è",
                "en": "November 29",
                "zh": "11Êúà29Êó•"
            },
            "hash": "b96751a3db484750",
            "authors": [
                "Hosu Lee",
                "Junho Kim",
                "Hyunjun Kim",
                "Yong Man Ro"
            ],
            "affiliations": [
                "Integrated Vision and Language Lab, KAIST, South Korea"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.19460.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#long_context",
                    "#video",
                    "#optimization"
                ],
                "emoji": "üé•",
                "ru": {
                    "title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é Video-Ma^2mba",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Video-Ma^2mba - –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π, –∏—Å–ø–æ–ª—å–∑—É—é—â—É—é –º–æ–¥–µ–ª–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ —Å–æ—Å—Ç–æ—è–Ω–∏–π –≤–º–µ—Å—Ç–æ –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ª–∏–Ω–µ–π–Ω–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å –±–æ–ª—å—à–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ (LMM) –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –∏ –ø–∞–º—è—Ç–∏. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –≤–≤–æ–¥—è—Ç –º–µ—Ç–æ–¥ –º—É–ª—å—Ç–∏–æ—Å–µ–≤–æ–≥–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —á–µ–∫–ø–æ–π–Ω—Ç–∏–Ω–≥–∞ (MA-GC) –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏. –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ Video-Ma^2mba –º–æ–∂–µ—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–µ –≤–∏–¥–µ–æ–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –æ–¥–Ω–æ–º GPU, —É–ª—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–æ–≤ –≤ –∑–∞–¥–∞—á–∞—Ö –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ."
                },
                "en": {
                    "title": "Revolutionizing Long Video Processing with Linear Scalability",
                    "desc": "The paper presents Video-Ma^2mba, a new architecture designed to efficiently process long video sequences by integrating State Space Models (SSMs) into the Mamba-2 framework. This innovative approach replaces traditional attention mechanisms, allowing the model to scale linearly in memory and computational requirements, which is crucial for handling extensive video data. Additionally, the introduction of Multi-Axis Gradient Checkpointing (MA-GC) optimizes memory usage by retaining only necessary activations, significantly reducing the memory footprint. Empirical results indicate that Video-Ma^2mba can effectively manage video sequences equivalent to millions of tokens, enhancing the accuracy of long video understanding tasks compared to existing models."
                },
                "zh": {
                    "title": "È´òÊïàÂ§ÑÁêÜÈïøËßÜÈ¢ëÂ∫èÂàóÁöÑÊñ∞ÊñπÊ≥ï",
                    "desc": "ÈöèÁùÄËßÜÈ¢ëÊï∞ÊçÆËßÑÊ®°ÂíåÂ§çÊùÇÊÄßÁöÑÂ¢ûÂä†ÔºåÂ§ÑÁêÜÈïøËßÜÈ¢ëÂ∫èÂàóÈù¢‰∏¥ÁùÄÊòæËëóÁöÑÊåëÊàò„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞Êû∂ÊûÑVideo-Ma^2mbaÔºåÂÆÉÂú®Mamba-2Ê°ÜÊû∂‰∏≠ÂºïÂÖ•‰∫ÜÁä∂ÊÄÅÁ©∫Èó¥Ê®°ÂûãÔºàSSMsÔºâÔºåÊõø‰ª£‰∫Ü‰º†ÁªüÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂Ôºå‰ªéËÄå‰ΩøÂæóÂ§ßËßÑÊ®°Â§öÊ®°ÊÄÅÊ®°ÂûãÔºàLMMsÔºâÂú®Êó∂Èó¥ÂíåÂÜÖÂ≠òÈúÄÊ±Ç‰∏äÂÆûÁé∞Á∫øÊÄßÊâ©Â±ï„ÄÇÊàë‰ª¨ËøòÂºïÂÖ•‰∫ÜÂ§öËΩ¥Ê¢ØÂ∫¶Ê£ÄÊü•ÁÇπÔºàMA-GCÔºâÊñπÊ≥ïÔºå‰ºòÂåñÂÜÖÂ≠òÁÆ°ÁêÜÔºå‰ªÖ‰øùÁïôÂøÖË¶ÅÁöÑÊøÄÊ¥ª‰ø°ÊÅØÔºåÊòæËëóÈôç‰Ωé‰∫ÜÂÜÖÂ≠òÂç†Áî®„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåVideo-Ma^2mbaËÉΩÂ§üÂú®Âçï‰∏™GPU‰∏äÂ§ÑÁêÜÁõ∏ÂΩì‰∫éÊï∞Áôæ‰∏á‰∏™Ê†áËÆ∞ÊàñË∂ÖËøá‰∏§Â∞èÊó∂ÁöÑËøûÁª≠ËßÜÈ¢ëÂ∫èÂàóÔºåÊèêÂçá‰∫ÜÈïøËßÜÈ¢ëÁêÜËß£‰ªªÂä°ÁöÑÂáÜÁ°ÆÊÄßÂíåÁõ∏ÂÖ≥ÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.19108",
            "title": "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model",
            "url": "https://huggingface.co/papers/2411.19108",
            "abstract": "As a fundamental backbone for video generation, diffusion models are challenged by low inference speed due to the sequential nature of denoising. Previous methods speed up the models by caching and reusing model outputs at uniformly selected timesteps. However, such a strategy neglects the fact that differences among model outputs are not uniform across timesteps, which hinders selecting the appropriate model outputs to cache, leading to a poor balance between inference efficiency and visual quality. In this study, we introduce Timestep Embedding Aware Cache (TeaCache), a training-free caching approach that estimates and leverages the fluctuating differences among model outputs across timesteps. Rather than directly using the time-consuming model outputs, TeaCache focuses on model inputs, which have a strong correlation with the modeloutputs while incurring negligible computational cost. TeaCache first modulates the noisy inputs using the timestep embeddings to ensure their differences better approximating those of model outputs. TeaCache then introduces a rescaling strategy to refine the estimated differences and utilizes them to indicate output caching. Experiments show that TeaCache achieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07% Vbench score) degradation of visual quality.",
            "score": 4,
            "issue_id": 886,
            "pub_date": "2024-11-28",
            "pub_date_card": {
                "ru": "28 –Ω–æ—è–±—Ä—è",
                "en": "November 28",
                "zh": "11Êúà28Êó•"
            },
            "hash": "02a6c2edf156e9d3",
            "authors": [
                "Feng Liu",
                "Shiwei Zhang",
                "Xiaofeng Wang",
                "Yujie Wei",
                "Haonan Qiu",
                "Yuzhong Zhao",
                "Yingya Zhang",
                "Qixiang Ye",
                "Fang Wan"
            ],
            "affiliations": [
                "Alibaba Group",
                "Fudan University",
                "Institute of Automation, Chinese Academy of Sciences",
                "Nanyang Technological University",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.19108.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#optimization",
                    "#video",
                    "#inference"
                ],
                "emoji": "‚è±Ô∏è",
                "ru": {
                    "title": "–£–º–Ω–æ–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É—Å–∫–æ—Ä–µ–Ω–∏—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º TeaCache. –ú–µ—Ç–æ–¥ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–∏—è –º–µ–∂–¥—É –≤—ã—Ö–æ–¥–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —à–∞–≥–∞—Ö, –∏—Å–ø–æ–ª—å–∑—É—è –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ –≤—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —à–∞–≥–æ–≤. TeaCache –ø—Ä–∏–º–µ–Ω—è–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è —É—Ç–æ—á–Ω–µ–Ω–∏—è –æ—Ü–µ–Ω–æ–∫ —Ä–∞–∑–ª–∏—á–∏–π –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏—Ö –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ TeaCache –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —É—Å–∫–æ—Ä–µ–Ω–∏—è –¥–æ 4,41 —Ä–∞–∑–∞ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å Open-Sora-Plan –ø—Ä–∏ –Ω–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–º —Å–Ω–∏–∂–µ–Ω–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏."
                },
                "en": {
                    "title": "Accelerating Video Generation with Smart Caching",
                    "desc": "This paper presents TeaCache, a novel caching method designed to enhance the efficiency of diffusion models in video generation. Traditional approaches cache model outputs at fixed timesteps, which can lead to suboptimal performance due to the uneven differences in outputs across timesteps. TeaCache addresses this by focusing on modulating noisy inputs with timestep embeddings, allowing for a more accurate estimation of output differences without the heavy computational cost. The results demonstrate that TeaCache significantly accelerates inference speed while maintaining high visual quality, achieving a notable performance improvement over existing methods."
                },
                "zh": {
                    "title": "ÊèêÂçáËßÜÈ¢ëÁîüÊàêÈÄüÂ∫¶ÁöÑÊñ∞ÊñπÊ≥ïÔºöTeaCache",
                    "desc": "Êú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÁºìÂ≠òÊñπÊ≥ïÔºåÁß∞‰∏∫Êó∂Èó¥Ê≠•ÂµåÂÖ•ÊÑüÁü•ÁºìÂ≠òÔºàTeaCacheÔºâÔºåÊó®Âú®ÊèêÈ´òËßÜÈ¢ëÁîüÊàê‰∏≠ÁöÑÊâ©Êï£Ê®°ÂûãÁöÑÊé®ÁêÜÈÄüÂ∫¶„ÄÇ‰º†ÁªüÊñπÊ≥ïÈÄöËøáÂú®ÂùáÂåÄÈÄâÊã©ÁöÑÊó∂Èó¥Ê≠•ÁºìÂ≠òÊ®°ÂûãËæìÂá∫Ôºå‰ΩÜÂøΩÁï•‰∫Ü‰∏çÂêåÊó∂Èó¥Ê≠•‰πãÈó¥ËæìÂá∫Â∑ÆÂºÇÁöÑ‰∏çÂùáÂåÄÊÄß„ÄÇTeaCacheÈÄöËøáË∞ÉËäÇÂô™Â£∞ËæìÂÖ•ÔºåÂà©Áî®Êó∂Èó¥Ê≠•ÂµåÂÖ•Êù•Êõ¥Â•ΩÂú∞Ëøë‰ººÊ®°ÂûãËæìÂá∫ÁöÑÂ∑ÆÂºÇÔºå‰ªéËÄå‰ºòÂåñÁºìÂ≠òÈÄâÊã©„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåTeaCacheÂú®‰øùÊåÅËßÜËßâË¥®ÈáèÁöÑÂêåÊó∂ÔºåÊé®ÁêÜÈÄüÂ∫¶ÊèêÈ´ò‰∫Ü4.41ÂÄç„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.19146",
            "title": "Puzzle: Distillation-Based NAS for Inference-Optimized LLMs",
            "url": "https://huggingface.co/papers/2411.19146",
            "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities, but their adoption is limited by high computational costs during inference. While increasing parameter counts enhances accuracy, it also widens the gap between state-of-the-art capabilities and practical deployability. We present Puzzle, a framework to accelerate LLM inference on specific hardware while preserving their capabilities. Through an innovative application of neural architecture search (NAS) at an unprecedented scale, Puzzle systematically optimizes models with tens of billions of parameters under hardware constraints. Our approach utilizes blockwise local knowledge distillation (BLD) for parallel architecture exploration and employs mixed-integer programming for precise constraint optimization.   We demonstrate the real-world impact of our framework through Llama-3.1-Nemotron-51B-Instruct (Nemotron-51B), a publicly available model derived from Llama-3.1-70B-Instruct. Nemotron-51B achieves a 2.17x inference throughput speedup, fitting on a single NVIDIA H100 GPU while preserving 98.4% of the original model's capabilities. Nemotron-51B currently stands as the most accurate language model capable of inference on a single GPU with large batch sizes. Remarkably, this transformation required just 45B training tokens, compared to over 15T tokens used for the 70B model it was derived from. This establishes a new paradigm where powerful models can be optimized for efficient deployment with only negligible compromise of their capabilities, demonstrating that inference performance, not parameter count alone, should guide model selection. With the release of Nemotron-51B and the presentation of the Puzzle framework, we provide practitioners immediate access to state-of-the-art language modeling capabilities at significantly reduced computational costs.",
            "score": 3,
            "issue_id": 886,
            "pub_date": "2024-11-28",
            "pub_date_card": {
                "ru": "28 –Ω–æ—è–±—Ä—è",
                "en": "November 28",
                "zh": "11Êúà28Êó•"
            },
            "hash": "b33bb17742a81e99",
            "authors": [
                "Akhiad Bercovich",
                "Tomer Ronen",
                "Talor Abramovich",
                "Nir Ailon",
                "Nave Assaf",
                "Mohammad Dabbah",
                "Ido Galil",
                "Amnon Geifman",
                "Yonatan Geifman",
                "Izhak Golan",
                "Netanel Haber",
                "Ehud Karpas",
                "Itay Levy",
                "Shahar Mor",
                "Zach Moshe",
                "Najeeb Nabwani",
                "Omri Puny",
                "Ran Rubin",
                "Itamar Schen",
                "Ido Shahaf",
                "Oren Tropp",
                "Omer Ullman Argov",
                "Ran Zilberstein",
                "Ran El-Yaniv"
            ],
            "affiliations": [
                "NVIDIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.19146.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#training",
                    "#inference"
                ],
                "emoji": "üß©",
                "ru": {
                    "title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ LLM –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ò–ò",
                    "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ Puzzle –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–∏ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∏—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π. –ò—Å–ø–æ–ª—å–∑—É—è –Ω–µ–π—Ä–æ–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–π –ø–æ–∏—Å–∫ (NAS) –∏ –±–ª–æ—á–Ω—É—é –ª–æ–∫–∞–ª—å–Ω—É—é –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—é –∑–Ω–∞–Ω–∏–π (BLD), Puzzle –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª–∏ —Å –¥–µ—Å—è—Ç–∫–∞–º–∏ –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ–¥ –∞–ø–ø–∞—Ä–∞—Ç–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è. –ù–∞ –ø—Ä–∏–º–µ—Ä–µ –º–æ–¥–µ–ª–∏ Nemotron-51B, –ø–æ–ª—É—á–µ–Ω–Ω–æ–π –∏–∑ Llama-3.1-70B-Instruct, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç—Å—è 2.17-–∫—Ä–∞—Ç–Ω–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ 98.4% –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –∏—Å—Ö–æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏. –≠—Ç–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞, –∞ –Ω–µ —Ç–æ–ª—å–∫–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –¥–æ–ª–∂–Ω–∞ –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å –≤—ã–±–æ—Ä –º–æ–¥–µ–ª–∏."
                },
                "en": {
                    "title": "Optimizing Large Language Models for Efficient Inference",
                    "desc": "This paper introduces Puzzle, a framework designed to enhance the inference speed of large language models (LLMs) while maintaining their performance. It employs neural architecture search (NAS) to optimize models with billions of parameters specifically for certain hardware, addressing the challenge of high computational costs. The framework utilizes blockwise local knowledge distillation (BLD) and mixed-integer programming to efficiently explore and optimize model architectures. The resulting model, Nemotron-51B, achieves a significant speedup in inference on a single GPU while retaining most of the original model's capabilities, showcasing a new approach to deploying powerful LLMs more efficiently."
                },
                "zh": {
                    "title": "È´òÊïàÊé®ÁêÜÔºåÂº∫Â§ßÊ®°ÂûãÁöÑÊñ∞ËåÉÂºè",
                    "desc": "Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Êé®ÁêÜÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÈ´òËÆ°ÁÆóÊàêÊú¨ÈôêÂà∂‰∫ÜÂÆÉ‰ª¨ÁöÑÂ∫îÁî®„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜPuzzleÊ°ÜÊû∂ÔºåÈÄöËøáÁ•ûÁªèÊû∂ÊûÑÊêúÁ¥¢ÔºàNASÔºâÂú®ÁâπÂÆöÁ°¨‰ª∂‰∏äÂä†ÈÄüLLMÊé®ÁêÜÔºåÂêåÊó∂‰øùÊåÅÂÖ∂ËÉΩÂäõ„ÄÇËØ•Ê°ÜÊû∂Âà©Áî®ÂùóÁä∂Â±ÄÈÉ®Áü•ËØÜËí∏È¶èÔºàBLDÔºâËøõË°åÂπ∂Ë°åÊû∂ÊûÑÊé¢Á¥¢ÔºåÂπ∂ÈááÁî®Ê∑∑ÂêàÊï¥Êï∞ËßÑÂàíËøõË°åÁ≤æÁ°ÆÁ∫¶Êùü‰ºòÂåñ„ÄÇÈÄöËøáNemotron-51BÊ®°ÂûãÔºåÊàë‰ª¨Â±ïÁ§∫‰∫ÜÂú®Âçï‰∏™NVIDIA H100 GPU‰∏äÂÆûÁé∞2.17ÂÄçÊé®ÁêÜÂêûÂêêÈáèÊèêÂçáÁöÑÂÆûÈôÖÊïàÊûúÔºåÂêåÊó∂‰øùÁïô‰∫Ü98.4%ÁöÑÂéüÂßãÊ®°ÂûãËÉΩÂäõ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.19324",
            "title": "Trajectory Attention for Fine-grained Video Motion Control",
            "url": "https://huggingface.co/papers/2411.19324",
            "abstract": "Recent advancements in video generation have been greatly driven by video diffusion models, with camera motion control emerging as a crucial challenge in creating view-customized visual content. This paper introduces trajectory attention, a novel approach that performs attention along available pixel trajectories for fine-grained camera motion control. Unlike existing methods that often yield imprecise outputs or neglect temporal correlations, our approach possesses a stronger inductive bias that seamlessly injects trajectory information into the video generation process. Importantly, our approach models trajectory attention as an auxiliary branch alongside traditional temporal attention. This design enables the original temporal attention and the trajectory attention to work in synergy, ensuring both precise motion control and new content generation capability, which is critical when the trajectory is only partially available. Experiments on camera motion control for images and videos demonstrate significant improvements in precision and long-range consistency while maintaining high-quality generation. Furthermore, we show that our approach can be extended to other video motion control tasks, such as first-frame-guided video editing, where it excels in maintaining content consistency over large spatial and temporal ranges.",
            "score": 3,
            "issue_id": 885,
            "pub_date": "2024-11-28",
            "pub_date_card": {
                "ru": "28 –Ω–æ—è–±—Ä—è",
                "en": "November 28",
                "zh": "11Êúà28Êó•"
            },
            "hash": "02a266f597ae69e7",
            "authors": [
                "Zeqi Xiao",
                "Wenqi Ouyang",
                "Yifan Zhou",
                "Shuai Yang",
                "Lei Yang",
                "Jianlou Si",
                "Xingang Pan"
            ],
            "affiliations": [
                "S-Lab, Nanyang Technological University",
                "Sensetime Research",
                "Wangxuan Institute of Computer Technology, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.19324.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#video"
                ],
                "emoji": "üé•",
                "ru": {
                    "title": "–¢–æ—á–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å –¥–≤–∏–∂–µ–Ω–∏—è –∫–∞–º–µ—Ä—ã –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –≤–∏–¥–µ–æ–º–æ–¥–µ–ª—è—Ö —Å –ø–æ–º–æ—â—å—é trajectory attention",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º 'trajectory attention' –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è –¥–≤–∏–∂–µ–Ω–∏—è –∫–∞–º–µ—Ä—ã –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –≤–∏–¥–µ–æ–º–æ–¥–µ–ª—è—Ö. –ú–µ—Ç–æ–¥ –≤—ã–ø–æ–ª–Ω—è–µ—Ç –≤–Ω–∏–º–∞–Ω–∏–µ –≤–¥–æ–ª—å –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –ø–∏–∫—Å–µ–ª–µ–π, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ –≤–Ω–µ–¥—Ä—è—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –≤ –ø—Ä–æ—Ü–µ—Å—Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ. Trajectory attention —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–∞–∫ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è –≤–µ—Ç–≤—å –Ω–∞—Ä—è–¥—É —Å —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–º –≤—Ä–µ–º–µ–Ω–Ω—ã–º –≤–Ω–∏–º–∞–Ω–∏–µ–º, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –∫–∞–∫ —Ç–æ—á–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å –¥–≤–∏–∂–µ–Ω–∏—è, —Ç–∞–∫ –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–æ–≤–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –≤ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –Ω–∞ –±–æ–ª—å—à–∏—Ö —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è—Ö –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏."
                },
                "en": {
                    "title": "Enhancing Video Generation with Trajectory Attention",
                    "desc": "This paper presents a new method called trajectory attention for improving video generation, particularly in controlling camera motion. It enhances the traditional temporal attention mechanism by incorporating pixel trajectories, allowing for more precise and consistent video outputs. The method effectively combines trajectory information with temporal attention, addressing challenges in generating view-customized content. Experiments show that this approach not only improves motion control but also maintains high-quality video generation across various tasks."
                },
                "zh": {
                    "title": "ËΩ®ËøπÊ≥®ÊÑèÂäõÔºöÁ≤æÁ°ÆÊéßÂà∂ËßÜÈ¢ëÁîüÊàê‰∏≠ÁöÑÁõ∏Êú∫ËøêÂä®",
                    "desc": "Êú¨ËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑËΩ®ËøπÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåÁî®‰∫éËßÜÈ¢ëÁîüÊàê‰∏≠ÁöÑÁõ∏Êú∫ËøêÂä®ÊéßÂà∂„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïËÉΩÂ§üÊõ¥Á≤æÁ°ÆÂú∞Â§ÑÁêÜËøêÂä®ÊéßÂà∂ÔºåÂπ∂ÊúâÊïàÂú∞ÁªìÂêà‰∫ÜËΩ®Ëøπ‰ø°ÊÅØ„ÄÇÈÄöËøáÂ∞ÜËΩ®ËøπÊ≥®ÊÑèÂäõ‰Ωú‰∏∫ËæÖÂä©ÂàÜÊîØ‰∏é‰º†ÁªüÊó∂Èó¥Ê≥®ÊÑèÂäõÁªìÂêàÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®ÁîüÊàêÊñ∞ÂÜÖÂÆπÁöÑÂêåÊó∂ÔºåÁ°Æ‰øù‰∫ÜËøêÂä®ÊéßÂà∂ÁöÑÁ≤æÁ°ÆÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®ÂõæÂÉèÂíåËßÜÈ¢ëÁöÑÁõ∏Êú∫ËøêÂä®ÊéßÂà∂‰∏≠ÊòæËëóÊèêÈ´ò‰∫ÜÁ≤æÂ∫¶ÂíåÈïøË∑ùÁ¶ª‰∏ÄËá¥ÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.18673",
            "title": "AC3D: Analyzing and Improving 3D Camera Control in Video Diffusion Transformers",
            "url": "https://huggingface.co/papers/2411.18673",
            "abstract": "Numerous works have recently integrated 3D camera control into foundational text-to-video models, but the resulting camera control is often imprecise, and video generation quality suffers. In this work, we analyze camera motion from a first principles perspective, uncovering insights that enable precise 3D camera manipulation without compromising synthesis quality. First, we determine that motion induced by camera movements in videos is low-frequency in nature. This motivates us to adjust train and test pose conditioning schedules, accelerating training convergence while improving visual and motion quality. Then, by probing the representations of an unconditional video diffusion transformer, we observe that they implicitly perform camera pose estimation under the hood, and only a sub-portion of their layers contain the camera information. This suggested us to limit the injection of camera conditioning to a subset of the architecture to prevent interference with other video features, leading to 4x reduction of training parameters, improved training speed and 10% higher visual quality. Finally, we complement the typical dataset for camera control learning with a curated dataset of 20K diverse dynamic videos with stationary cameras. This helps the model disambiguate the difference between camera and scene motion, and improves the dynamics of generated pose-conditioned videos. We compound these findings to design the Advanced 3D Camera Control (AC3D) architecture, the new state-of-the-art model for generative video modeling with camera control.",
            "score": 2,
            "issue_id": 886,
            "pub_date": "2024-11-27",
            "pub_date_card": {
                "ru": "27 –Ω–æ—è–±—Ä—è",
                "en": "November 27",
                "zh": "11Êúà27Êó•"
            },
            "hash": "1ea35d3552a278a3",
            "authors": [
                "Sherwin Bahmani",
                "Ivan Skorokhodov",
                "Guocheng Qian",
                "Aliaksandr Siarohin",
                "Willi Menapace",
                "Andrea Tagliasacchi",
                "David B. Lindell",
                "Sergey Tulyakov"
            ],
            "affiliations": [
                "SFU",
                "Snap Inc.",
                "University of Toronto",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.18673.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#architecture",
                    "#diffusion",
                    "#games",
                    "#3d",
                    "#training",
                    "#optimization",
                    "#video"
                ],
                "emoji": "üé•",
                "ru": {
                    "title": "–ü—Ä–µ—Ü–∏–∑–∏–æ–Ω–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ 3D-–∫–∞–º–µ—Ä–æ–π –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –≤–∏–¥–µ–æ–º–æ–¥–µ–ª—è—Ö",
                    "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—é 3D-–∫–∞–º–µ—Ä–æ–π –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –≤–∏–¥–µ–æ–º–æ–¥–µ–ª—è—Ö. –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –¥–≤–∏–∂–µ–Ω–∏–µ –∫–∞–º–µ—Ä—ã —Å —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–æ–π —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —É–ª—É—á—à–µ–Ω–∏–π, –≤–∫–ª—é—á–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –≥—Ä–∞—Ñ–∏–∫–æ–≤ –æ–±—É—á–µ–Ω–∏—è –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –≤–Ω–µ–¥—Ä–µ–Ω–∏—è —É—Å–ª–æ–≤–∏–π –∫–∞–º–µ—Ä—ã –≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ —Å–ª–æ–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –ø–æ–¥–æ–±—Ä–∞–Ω–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. –†–µ–∑—É–ª—å—Ç–∞—Ç–æ–º —è–≤–ª—è–µ—Ç—Å—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Advanced 3D Camera Control (AC3D), –∫–æ—Ç–æ—Ä–∞—è –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–∞–º–µ—Ä–æ–π –±–µ–∑ —É—â–µ—Ä–±–∞ –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–∞ —Å–∏–Ω—Ç–µ–∑–∞ –≤–∏–¥–µ–æ. –ú–æ–¥–µ–ª—å AC3D –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –Ω–æ–≤–æ–≥–æ —É—Ä–æ–≤–Ω—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –≤–∏–¥–µ–æ–º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è —Å —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –∫–∞–º–µ—Ä–æ–π."
                },
                "en": {
                    "title": "Precision in 3D Camera Control for Enhanced Video Generation",
                    "desc": "This paper presents a novel approach to improve 3D camera control in text-to-video models, addressing issues of imprecision and video quality. By analyzing camera motion, the authors discover that it is primarily low-frequency, which leads to adjustments in training and testing schedules that enhance convergence and visual fidelity. They also find that only certain layers of a video diffusion transformer contain relevant camera information, allowing for a more efficient architecture that reduces training parameters while boosting quality. Finally, the introduction of a curated dataset of dynamic videos aids the model in distinguishing between camera and scene motion, culminating in the development of the Advanced 3D Camera Control (AC3D) model, which sets a new standard in generative video modeling."
                },
                "zh": {
                    "title": "ÂÖàËøõÁöÑ3DÁõ∏Êú∫ÊéßÂà∂ÔºåÊèêÂçáËßÜÈ¢ëÁîüÊàêË¥®Èáè",
                    "desc": "Êú¨Á†îÁ©∂ÂàÜÊûê‰∫Ü3DÁõ∏Êú∫ÊéßÂà∂Âú®ÊñáÊú¨Âà∞ËßÜÈ¢ëÊ®°Âûã‰∏≠ÁöÑÂ∫îÁî®ÔºåÂèëÁé∞Áõ∏Êú∫ËøêÂä®ÂØπËßÜÈ¢ëÁîüÊàêË¥®ÈáèÊúâÊòæËëóÂΩ±Âìç„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËÆ≠ÁªÉÂíåÊµãËØïÂßøÊÄÅË∞ÉËäÇÁ≠ñÁï•Ôºå‰ª•ÊèêÈ´òËÆ≠ÁªÉÊî∂ÊïõÈÄüÂ∫¶ÂíåËßÜÈ¢ëÁöÑËßÜËßâË¥®Èáè„ÄÇÈÄöËøáÂØπÊó†Êù°‰ª∂ËßÜÈ¢ëÊâ©Êï£ÂèòÊç¢Âô®ÁöÑË°®Á§∫ËøõË°åÊé¢ÊµãÔºåÊàë‰ª¨ÂèëÁé∞Áõ∏Êú∫ÂßøÊÄÅ‰º∞ËÆ°Âú®Ê®°ÂûãÂÜÖÈÉ®ÈöêÂºèÊâßË°åÔºåÂõ†Ê≠§Êàë‰ª¨ÈôêÂà∂‰∫ÜÁõ∏Êú∫Êù°‰ª∂ÁöÑÊ≥®ÂÖ•Ôºå‰ª•ÂáèÂ∞ëÂØπÂÖ∂‰ªñËßÜÈ¢ëÁâπÂæÅÁöÑÂπ≤Êâ∞„ÄÇÊúÄÁªàÔºåÊàë‰ª¨ËÆæËÆ°‰∫ÜÂÖàËøõÁöÑ3DÁõ∏Êú∫ÊéßÂà∂Êû∂ÊûÑÔºàAC3DÔºâÔºåÊàê‰∏∫ÂÖ∑ÊúâÁõ∏Êú∫ÊéßÂà∂ÁöÑÁîüÊàêËßÜÈ¢ëÂª∫Ê®°ÁöÑÊñ∞‰∏Ä‰ª£Ê®°Âûã„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.19950",
            "title": "AlphaTablets: A Generic Plane Representation for 3D Planar Reconstruction from Monocular Videos",
            "url": "https://huggingface.co/papers/2411.19950",
            "abstract": "We introduce AlphaTablets, a novel and generic representation of 3D planes that features continuous 3D surface and precise boundary delineation. By representing 3D planes as rectangles with alpha channels, AlphaTablets combine the advantages of current 2D and 3D plane representations, enabling accurate, consistent and flexible modeling of 3D planes. We derive differentiable rasterization on top of AlphaTablets to efficiently render 3D planes into images, and propose a novel bottom-up pipeline for 3D planar reconstruction from monocular videos. Starting with 2D superpixels and geometric cues from pre-trained models, we initialize 3D planes as AlphaTablets and optimize them via differentiable rendering. An effective merging scheme is introduced to facilitate the growth and refinement of AlphaTablets. Through iterative optimization and merging, we reconstruct complete and accurate 3D planes with solid surfaces and clear boundaries. Extensive experiments on the ScanNet dataset demonstrate state-of-the-art performance in 3D planar reconstruction, underscoring the great potential of AlphaTablets as a generic 3D plane representation for various applications. Project page is available at: https://hyzcluster.github.io/alphatablets",
            "score": 1,
            "issue_id": 888,
            "pub_date": "2024-11-29",
            "pub_date_card": {
                "ru": "29 –Ω–æ—è–±—Ä—è",
                "en": "November 29",
                "zh": "11Êúà29Êó•"
            },
            "hash": "9f7d2daec9cb311d",
            "authors": [
                "Yuze He",
                "Wang Zhao",
                "Shaohui Liu",
                "Yubin Hu",
                "Yushi Bai",
                "Yu-Hui Wen",
                "Yong-Jin Liu"
            ],
            "affiliations": [
                "Beijing Jiaotong University",
                "ETH Zurich",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.19950.jpg",
            "data": {
                "categories": [
                    "#3d"
                ],
                "emoji": "üìê",
                "ru": {
                    "title": "AlphaTablets: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–∏ 3D –ø–ª–æ—Å–∫–æ—Å—Ç–µ–π",
                    "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω AlphaTablets - –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—é —Ç—Ä–µ—Ö–º–µ—Ä–Ω—ã—Ö –ø–ª–æ—Å–∫–æ—Å—Ç–µ–π –≤ –≤–∏–¥–µ –ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω–∏–∫–æ–≤ —Å –∞–ª—å—Ñ–∞-–∫–∞–Ω–∞–ª–∞–º–∏. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ—á–µ—Ç–∞—Ç—å –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö 2D –∏ 3D –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è —Ç–æ—á–Ω–æ–µ –∏ –≥–∏–±–∫–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ 3D –ø–ª–æ—Å–∫–æ—Å—Ç–µ–π. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º—É—é —Ä–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ 3D –ø–ª–æ—Å–∫–æ—Å—Ç–µ–π –∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º 3D —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–ª–æ—Å–∫–æ—Å—Ç–µ–π –∏–∑ –º–æ–Ω–æ–∫—É–ª—è—Ä–Ω—ã—Ö –≤–∏–¥–µ–æ. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö ScanNet –ø–æ–∫–∞–∑–∞–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –∑–∞–¥–∞—á–µ 3D —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–ª–æ—Å–∫–æ—Å—Ç–µ–π."
                },
                "en": {
                    "title": "AlphaTablets: Revolutionizing 3D Plane Representation",
                    "desc": "AlphaTablets is a new way to represent 3D planes that combines the benefits of both 2D and 3D models. It uses rectangles with alpha channels to create smooth surfaces and clear edges for accurate modeling. The paper introduces a method for rendering these planes efficiently and a pipeline for reconstructing 3D planes from single videos. By optimizing the representation through merging and iterative processes, AlphaTablets achieves high-quality 3D reconstructions, as shown in tests on the ScanNet dataset."
                },
                "zh": {
                    "title": "AlphaTabletsÔºö3DÂπ≥Èù¢ÈáçÂª∫ÁöÑÊñ∞ÊñπÊ≥ï",
                    "desc": "Êàë‰ª¨‰ªãÁªç‰∫ÜAlphaTabletsÔºåËøôÊòØ‰∏ÄÁßçÊñ∞È¢ñ‰∏îÈÄöÁî®ÁöÑ3DÂπ≥Èù¢Ë°®Á§∫ÊñπÊ≥ïÔºåÂÖ∑ÊúâËøûÁª≠ÁöÑ3DË°®Èù¢ÂíåÁ≤æÁ°ÆÁöÑËæπÁïåÂàíÂàÜ„ÄÇÈÄöËøáÂ∞Ü3DÂπ≥Èù¢Ë°®Á§∫‰∏∫Â∏¶ÊúâalphaÈÄöÈÅìÁöÑÁü©ÂΩ¢ÔºåAlphaTabletsÁªìÂêà‰∫ÜÂΩìÂâç2DÂíå3DÂπ≥Èù¢Ë°®Á§∫ÁöÑ‰ºòÁÇπÔºåÂÆûÁé∞‰∫Ü3DÂπ≥Èù¢ÁöÑÂáÜÁ°Æ„ÄÅ‰∏ÄËá¥ÂíåÁÅµÊ¥ªÂª∫Ê®°„ÄÇÊàë‰ª¨Âú®AlphaTabletsÁöÑÂü∫Á°Ä‰∏äÊé®ÂØºÂá∫ÂèØÂæÆÂàÜÂÖâÊ†ÖÂåñÊäÄÊúØÔºå‰ª•È´òÊïàÂú∞Â∞Ü3DÂπ≥Èù¢Ê∏≤Êüì‰∏∫ÂõæÂÉèÔºåÂπ∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑËá™‰∏ãËÄå‰∏äÁöÑÂçïÁõÆËßÜÈ¢ë3DÂπ≥Èù¢ÈáçÂª∫ÁÆ°ÈÅì„ÄÇÈÄöËøáËø≠‰ª£‰ºòÂåñÂíåÂêàÂπ∂ÔºåÊàë‰ª¨ËÉΩÂ§üÈáçÂª∫Âá∫ÂÆåÊï¥‰∏îÂáÜÁ°ÆÁöÑ3DÂπ≥Èù¢ÔºåÂÖ∑ÊúâÂùöÂÆûÁöÑË°®Èù¢ÂíåÊ∏ÖÊô∞ÁöÑËæπÁïå„ÄÇ"
                }
            }
        }
    ],
    "link_prev": "2024-11-29.html",
    "link_next": "2024-12-03.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "29.11",
        "en": "11/29",
        "zh": "11Êúà29Êó•"
    },
    "short_date_next": {
        "ru": "03.12",
        "en": "12/03",
        "zh": "12Êúà3Êó•"
    },
    "categories": {
        "#dataset": 2,
        "#data": 0,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 2,
        "#audio": 0,
        "#video": 4,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 3,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 4,
        "#survey": 0,
        "#diffusion": 3,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "ËøôÁØáÊñáÁ´†‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÊ°ÜÊû∂ÔºåÁß∞‰∏∫Critic-VÔºåÊó®Âú®ÊèêÈ´òÂ§öÊ®°ÊÄÅÊé®ÁêÜ‰ªªÂä°ÁöÑËÉΩÂäõ„ÄÇËØ•Ê°ÜÊû∂Áî±‰∏§‰∏™Áã¨Á´ãÁªÑ‰ª∂ÁªÑÊàêÔºöÊé®ÁêÜÂô®ÂíåËØÑËÆ∫Âô®„ÄÇÊé®ÁêÜÂô®Ê†πÊçÆËßÜËßâÂíåÊñáÊú¨ËæìÂÖ•ÁîüÊàêÊé®ÁêÜË∑ØÂæÑÔºåËÄåËØÑËÆ∫Âô®Êèê‰æõÂª∫ËÆæÊÄßÁöÑÊâπËØÑÊù•ÁªÜÂåñËøô‰∫õË∑ØÂæÑ„ÄÇËØÑËÆ∫Âô®‰ΩøÁî®Áõ¥Êé•ÂÅèÂ•Ω‰ºòÂåñÔºàDPOÔºâËøõË°åËÆ≠ÁªÉÔºå‰ª•Â¢ûÂº∫ÂÖ∂ËØÑËÆ∫ËÉΩÂäõ„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåCritic-VÊ°ÜÊû∂Âú®5‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÁâπÂà´ÊòØÂú®Êé®ÁêÜÂáÜÁ°ÆÊÄßÂíåÊïàÁéáÊñπÈù¢„ÄÇ",
        "title": "Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning",
        "pinyin": "ËøôÁØáÊñáÁ´†‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÊ°ÜÊû∂ÔºåÁß∞‰∏∫Critic-VÔºåÊó®Âú®ÊèêÈ´òÂ§öÊ®°ÊÄÅÊé®ÁêÜ‰ªªÂä°ÁöÑËÉΩÂäõ„ÄÇËØ•Ê°ÜÊû∂Áî±‰∏§‰∏™Áã¨Á´ãÁªÑ‰ª∂ÁªÑÊàêÔºöÊé®ÁêÜÂô®ÂíåËØÑËÆ∫Âô®„ÄÇÊé®ÁêÜÂô®Ê†πÊçÆËßÜËßâÂíåÊñáÊú¨ËæìÂÖ•ÁîüÊàêÊé®ÁêÜË∑ØÂæÑÔºåËÄåËØÑËÆ∫Âô®Êèê‰æõÂª∫ËÆæÊÄßÁöÑÊâπËØÑÊù•ÁªÜÂåñËøô‰∫õË∑ØÂæÑ„ÄÇËØÑËÆ∫Âô®‰ΩøÁî®Áõ¥Êé•ÂÅèÂ•Ω‰ºòÂåñÔºàDPOÔºâËøõË°åËÆ≠ÁªÉÔºå‰ª•Â¢ûÂº∫ÂÖ∂ËØÑËÆ∫ËÉΩÂäõ„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåCritic-VÊ°ÜÊû∂Âú®5‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÁâπÂà´ÊòØÂú®Êé®ÁêÜÂáÜÁ°ÆÊÄßÂíåÊïàÁéáÊñπÈù¢„ÄÇ\n\nZh√® piƒÅn w√©nzhƒÅng ji√®sh√†o le yƒ´ zh«íng xƒ´n de sh√¨ju√©-y«îy√°n m√≥x√≠ng ku√†ngji√†, chƒìngw√©i Critic-V, zh«êy«ên t√≠gƒÅo du≈ç m√≥sh√¨ tuƒ´l«ê r√®nw√π de n√©ngl√¨. G«éi ku√†ngji√† y«íu li«éng g√® d√∫l√¨ z«îji√†n z«îch√©ng: tuƒ´l«êq√¨ h√© p√≠ngl√πnq√¨. Tuƒ´l«êq√¨ gƒìnj√π sh√¨ju√© h√© w√©nbƒõn sh≈´r√π shƒìngch√©ng tuƒ´l«ê l√πj√¨ng, √©r p√≠ngl√πnq√¨ t√≠g≈çng ji√†nsh√®x√¨ng de pƒ´p√≠ng l√°i x√¨hu√† zh√®xiƒì l√πj√¨ng. P√≠ngl√πnq√¨ sh«êy√≤ng zh√≠jiƒì piƒÅnhu√† y≈çuhu√† (DPO) j√¨nx√≠ng x√πnli√†n, y«ê zƒìngqi√°ng q√≠ p√≠ngl√πn n√©ngl√¨. Sh√≠y√†n ji√©gu«í xi«énsh√¨, Critic-V ku√†ngji√† z√†i 5 g√® jƒ´zh«în c√®sh√¨ zh≈çng xi«énzh√π y≈çu xi√†nz√†i fƒÅngf«é, t√®bi√© sh√¨ z√†i tuƒ´l«ê zh«înqu√®x√¨ng h√© xi√†ol«ú fƒÅngmi√†n.",
        "vocab": "[{'word': 'ËßÜËßâ', 'pinyin': 'sh√¨ju√©', 'trans': 'vision'},\n{'word': 'ËØ≠Ë®Ä', 'pinyin': 'y«îy√°n', 'trans': 'language'},\n{'word': 'Ê®°Âûã', 'pinyin': 'm√≥x√≠ng', 'trans': 'model'},\n{'word': 'Ê°ÜÊû∂', 'pinyin': 'ku√†ngji√†', 'trans': 'framework'},\n{'word': 'Áß∞‰∏∫', 'pinyin': 'chƒìngw√©i', 'trans': 'called'},\n{'word': 'Êó®Âú®', 'pinyin': 'zh«êz√†i', 'trans': 'aim to'},\n{'word': 'ÊèêÈ´ò', 'pinyin': 't√≠gƒÅo', 'trans': 'improve'},\n{'word': 'Â§öÊ®°ÊÄÅ', 'pinyin': 'du≈çm√≥shu√†i', 'trans': 'multimodal'},\n{'word': 'Êé®ÁêÜ', 'pinyin': 'tuƒ´l«ê', 'trans': 'reasoning'},\n{'word': '‰ªªÂä°', 'pinyin': 'r√®nw√π', 'trans': 'task'},\n{'word': 'ËÉΩÂäõ', 'pinyin': 'n√©ngl√¨', 'trans': 'ability'},\n{'word': 'Áã¨Á´ã', 'pinyin': 'd√∫l√¨', 'trans': 'independent'},\n{'word': 'ÁªÑ‰ª∂', 'pinyin': 'z«îji√†n', 'trans': 'component'},\n{'word': 'ÁªÑÊàê', 'pinyin': 'zh«îch√©ng', 'trans': 'composed of'},\n{'word': 'Êé®ÁêÜÂô®', 'pinyin': 'tuƒ´l«êq√¨', 'trans': 'reasoner'},\n{'word': 'ËØÑËÆ∫Âô®', 'pinyin': 'p√≠ngl√πnq√¨', 'trans': 'critic'},\n{'word': 'Ê†πÊçÆ', 'pinyin': 'gƒìnj√π', 'trans': 'based on'},\n{'word': 'ËæìÂÖ•', 'pinyin': 'sh≈´r√π', 'trans': 'input'},\n{'word': 'ÁîüÊàê', 'pinyin': 'shƒìngch√©ng', 'trans': 'generate'},\n{'word': 'Ë∑ØÂæÑ', 'pinyin': 'l√πj√¨ng', 'trans': 'path'},\n{'word': 'Âª∫ËÆæÊÄß', 'pinyin': 'ji√†nsh√®x√¨ng', 'trans': 'constructive'},\n{'word': 'ÊâπËØÑ', 'pinyin': 'pƒ´p√≠ng', 'trans': 'criticism'},\n{'word': 'ÁªÜÂåñ', 'pinyin': 'x√¨hu√†', 'trans': 'refine'},\n{'word': 'Áõ¥Êé•', 'pinyin': 'zh√≠jiƒì', 'trans': 'direct'},\n{'word': 'ÂÅèÂ•Ω', 'pinyin': 'piƒÅnh√†o', 'trans': 'preference'},\n{'word': '‰ºòÂåñ', 'pinyin': 'y≈çuhu√†', 'trans': 'optimization'},\n{'word': 'ËøõË°å', 'pinyin': 'j√¨nx√≠ng', 'trans': 'conduct'},\n{'word': 'ËÆ≠ÁªÉ', 'pinyin': 'x√πnli√†n', 'trans': 'training'},\n{'word': 'Â¢ûÂº∫', 'pinyin': 'zƒìngqi√°ng', 'trans': 'enhance'},\n{'word': 'ËØÑËÆ∫', 'pinyin': 'p√≠ngl√πn', 'trans': 'comment'},\n{'word': 'ËÉΩÂäõ', 'pinyin': 'n√©ngl√¨', 'trans': 'ability'},\n{'word': 'ÂÆûÈ™å', 'pinyin': 'sh√≠y√†n', 'trans': 'experiment'},\n{'word': 'ÁªìÊûú', 'pinyin': 'ji√©gu«í', 'trans': 'result'},\n{'word': 'ÊòæÁ§∫', 'pinyin': 'xi«énsh√¨', 'trans': 'show'},\n{'word': 'ÊòæËëó', 'pinyin': 'xi«énzh√π', 'trans': 'significant'},\n{'word': '‰ºò‰∫é', 'pinyin': 'y≈çuy√∫', 'trans': 'superior to'},\n{'word': 'Áé∞Êúâ', 'pinyin': 'xi√†ny«íu', 'trans': 'existing'},\n{'word': 'ÊñπÊ≥ï', 'pinyin': 'fƒÅngf«é', 'trans': 'method'},\n{'word': 'ÁâπÂà´', 'pinyin': 't√®bi√©', 'trans': 'especially'},\n{'word': 'ÂáÜÁ°ÆÊÄß', 'pinyin': 'zh«înqu√®x√¨ng', 'trans': 'accuracy'},\n{'word': 'ÊïàÁéá', 'pinyin': 'xi√†ol«ú', 'trans': 'efficiency'}]",
        "trans": "This article introduces a new vision-language model framework called Critic-V, aimed at enhancing the capabilities of multimodal reasoning tasks. The framework consists of two independent components: a reasoner and a critic. The reasoner generates reasoning paths based on visual and textual inputs, while the critic provides constructive criticism to refine these paths. The critic is trained using direct preference optimization (DPO) to enhance its critiquing ability. Experimental results demonstrate that the Critic-V framework significantly outperforms existing methods on five benchmark tests, particularly in terms of reasoning accuracy and efficiency.",
        "update_ts": "2024-12-01 12:42"
    }
}