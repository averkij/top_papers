{
    "date": {
        "ru": "2 –¥–µ–∫–∞–±—Ä—è",
        "en": "December 2",
        "zh": "12Êúà2Êó•"
    },
    "time_utc": "2024-12-02 04:13",
    "weekday": 0,
    "issue_id": 885,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.19930",
            "title": "On Domain-Specific Post-Training for Multimodal Large Language Models",
            "url": "https://huggingface.co/papers/2411.19930",
            "abstract": "Recent years have witnessed the rapid development of general multimodal large language models (MLLMs). However, adapting general MLLMs to specific domains, such as scientific fields and industrial applications, remains less explored. This paper systematically investigates domain adaptation of MLLMs through post-training, focusing on data synthesis, training pipelines, and task evaluation. (1) Data Synthesis: Using open-source models, we develop a visual instruction synthesizer that effectively generates diverse visual instruction tasks from domain-specific image-caption pairs. Our synthetic tasks surpass those generated by manual rules, GPT-4, and GPT-4V in enhancing the domain-specific performance of MLLMs. (2) Training Pipeline: While the two-stage training--initially on image-caption pairs followed by visual instruction tasks--is commonly adopted for developing general MLLMs, we apply a single-stage training pipeline to enhance task diversity for domain-specific post-training. (3) Task Evaluation: We conduct experiments in two domains, biomedicine and food, by post-training MLLMs of different sources and scales (e.g., Qwen2-VL-2B, LLaVA-v1.6-8B, Llama-3.2-11B), and then evaluating MLLM performance on various domain-specific tasks. To support further research in MLLM domain adaptation, we will open-source our implementations.",
            "score": 1,
            "issue_id": 885,
            "pub_date": "2024-11-29",
            "pub_date_card": {
                "ru": "29 –Ω–æ—è–±—Ä—è",
                "en": "November 29",
                "zh": "11Êúà29Êó•"
            },
            "hash": "5d14749b38f15e60",
            "authors": [
                "Daixuan Cheng",
                "Shaohan Huang",
                "Ziyu Zhu",
                "Xintong Zhang",
                "Wayne Xin Zhao",
                "Zhongzhi Luan",
                "Bo Dai",
                "Zhenliang Zhang"
            ],
            "affiliations": [
                "Beihang University",
                "Beijing Institute of Technology",
                "Renmin University of China",
                "State Key Laboratory of General Artificial Intelligence, BIGAI",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.19930.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#dataset",
                    "#training",
                    "#open_source",
                    "#multimodal",
                    "#synthetic"
                ],
                "emoji": "üî¨",
                "ru": {
                    "title": "–ê–¥–∞–ø—Ç–∞—Ü–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò –∫ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –æ–±–ª–∞—Å—Ç—è–º",
                    "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –∞–¥–∞–ø—Ç–∞—Ü–∏—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –∫ —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–º –¥–æ–º–µ–Ω–∞–º. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ —Å–∏–Ω—Ç–µ–∑–∞—Ç–æ—Ä –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –¥–æ–º–µ–Ω–Ω—ã–µ –ø–∞—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ-–ø–æ–¥–ø–∏—Å—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á. –û–Ω–∏ –ø—Ä–∏–º–µ–Ω–∏–ª–∏ –æ–¥–Ω–æ—ç—Ç–∞–ø–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –∑–∞–¥–∞—á –ø—Ä–∏ –¥–æ–º–µ–Ω–Ω–æ–π –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–µ. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø—Ä–æ–≤–æ–¥–∏–ª–∏—Å—å –≤ –±–∏–æ–º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –∏ –ø–∏—â–µ–≤–æ–π –æ–±–ª–∞—Å—Ç—è—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã—Ö MLLM, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞."
                },
                "en": {
                    "title": "Enhancing MLLMs for Specific Domains through Innovative Adaptation Techniques",
                    "desc": "This paper explores how to adapt general multimodal large language models (MLLMs) for specific fields like biomedicine and food. It introduces a method for data synthesis that creates diverse visual instruction tasks from image-caption pairs, outperforming previous methods. The authors propose a single-stage training pipeline to improve task diversity during post-training, rather than the usual two-stage approach. Finally, they evaluate the performance of various MLLMs on domain-specific tasks and plan to share their findings to aid future research in this area."
                },
                "zh": {
                    "title": "ÊèêÂçáÂ§öÊ®°ÊÄÅÊ®°ÂûãÁöÑÈ¢ÜÂüüÈÄÇÂ∫îÊÄß",
                    "desc": "ËøëÂπ¥Êù•ÔºåÈÄöÁî®Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâËøÖÈÄüÂèëÂ±ï„ÄÇÁÑ∂ËÄåÔºåÂ∞ÜÈÄöÁî®MLLMsÈÄÇÂ∫î‰∫éÁâπÂÆöÈ¢ÜÂüüÔºåÂ¶ÇÁßëÂ≠¶ÂíåÂ∑•‰∏öÂ∫îÁî®Ôºå‰ªçÁÑ∂ËæÉÂ∞ëË¢´Êé¢Á¥¢„ÄÇÊú¨ÊñáÁ≥ªÁªüÁ†îÁ©∂‰∫ÜMLLMsÁöÑÈ¢ÜÂüüÈÄÇÂ∫îÊÄßÔºåÈáçÁÇπÂú®‰∫éÊï∞ÊçÆÂêàÊàê„ÄÅËÆ≠ÁªÉÊµÅÁ®ãÂíå‰ªªÂä°ËØÑ‰º∞„ÄÇÊàë‰ª¨ÂºÄÂèë‰∫Ü‰∏ÄÁßçËßÜËßâÊåá‰ª§ÂêàÊàêÂô®ÔºåËÉΩÂ§üÊúâÊïàÁîüÊàêÂ§öÊ†∑ÂåñÁöÑËßÜËßâÊåá‰ª§‰ªªÂä°Ôºå‰ªéËÄåÊèêÂçáMLLMsÂú®ÁâπÂÆöÈ¢ÜÂüüÁöÑË°®Áé∞„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.19324",
            "title": "Trajectory Attention for Fine-grained Video Motion Control",
            "url": "https://huggingface.co/papers/2411.19324",
            "abstract": "Recent advancements in video generation have been greatly driven by video diffusion models, with camera motion control emerging as a crucial challenge in creating view-customized visual content. This paper introduces trajectory attention, a novel approach that performs attention along available pixel trajectories for fine-grained camera motion control. Unlike existing methods that often yield imprecise outputs or neglect temporal correlations, our approach possesses a stronger inductive bias that seamlessly injects trajectory information into the video generation process. Importantly, our approach models trajectory attention as an auxiliary branch alongside traditional temporal attention. This design enables the original temporal attention and the trajectory attention to work in synergy, ensuring both precise motion control and new content generation capability, which is critical when the trajectory is only partially available. Experiments on camera motion control for images and videos demonstrate significant improvements in precision and long-range consistency while maintaining high-quality generation. Furthermore, we show that our approach can be extended to other video motion control tasks, such as first-frame-guided video editing, where it excels in maintaining content consistency over large spatial and temporal ranges.",
            "score": 1,
            "issue_id": 885,
            "pub_date": "2024-11-28",
            "pub_date_card": {
                "ru": "28 –Ω–æ—è–±—Ä—è",
                "en": "November 28",
                "zh": "11Êúà28Êó•"
            },
            "hash": "02a266f597ae69e7",
            "authors": [
                "Zeqi Xiao",
                "Wenqi Ouyang",
                "Yifan Zhou",
                "Shuai Yang",
                "Lei Yang",
                "Jianlou Si",
                "Xingang Pan"
            ],
            "affiliations": [
                "S-Lab, Nanyang Technological University",
                "Sensetime Research",
                "Wangxuan Institute of Computer Technology, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.19324.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#video"
                ],
                "emoji": "üé•",
                "ru": {
                    "title": "–¢–æ—á–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å –¥–≤–∏–∂–µ–Ω–∏—è –∫–∞–º–µ—Ä—ã –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –≤–∏–¥–µ–æ–º–æ–¥–µ–ª—è—Ö —Å –ø–æ–º–æ—â—å—é trajectory attention",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º 'trajectory attention' –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è –¥–≤–∏–∂–µ–Ω–∏—è –∫–∞–º–µ—Ä—ã –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –≤–∏–¥–µ–æ–º–æ–¥–µ–ª—è—Ö. –ú–µ—Ç–æ–¥ –≤—ã–ø–æ–ª–Ω—è–µ—Ç –≤–Ω–∏–º–∞–Ω–∏–µ –≤–¥–æ–ª—å –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –ø–∏–∫—Å–µ–ª–µ–π, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ –≤–Ω–µ–¥—Ä—è—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –≤ –ø—Ä–æ—Ü–µ—Å—Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ. Trajectory attention —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–∞–∫ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è –≤–µ—Ç–≤—å –Ω–∞—Ä—è–¥—É —Å —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–º –≤—Ä–µ–º–µ–Ω–Ω—ã–º –≤–Ω–∏–º–∞–Ω–∏–µ–º, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –∫–∞–∫ —Ç–æ—á–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å –¥–≤–∏–∂–µ–Ω–∏—è, —Ç–∞–∫ –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–æ–≤–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –≤ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –Ω–∞ –±–æ–ª—å—à–∏—Ö —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è—Ö –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏."
                },
                "en": {
                    "title": "Enhancing Video Generation with Trajectory Attention",
                    "desc": "This paper presents a new method called trajectory attention for improving video generation, particularly in controlling camera motion. It enhances the traditional temporal attention mechanism by incorporating pixel trajectories, allowing for more precise and consistent video outputs. The method effectively combines trajectory information with temporal attention, addressing challenges in generating view-customized content. Experiments show that this approach not only improves motion control but also maintains high-quality video generation across various tasks."
                },
                "zh": {
                    "title": "ËΩ®ËøπÊ≥®ÊÑèÂäõÔºöÁ≤æÁ°ÆÊéßÂà∂ËßÜÈ¢ëÁîüÊàê‰∏≠ÁöÑÁõ∏Êú∫ËøêÂä®",
                    "desc": "Êú¨ËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑËΩ®ËøπÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåÁî®‰∫éËßÜÈ¢ëÁîüÊàê‰∏≠ÁöÑÁõ∏Êú∫ËøêÂä®ÊéßÂà∂„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïËÉΩÂ§üÊõ¥Á≤æÁ°ÆÂú∞Â§ÑÁêÜËøêÂä®ÊéßÂà∂ÔºåÂπ∂ÊúâÊïàÂú∞ÁªìÂêà‰∫ÜËΩ®Ëøπ‰ø°ÊÅØ„ÄÇÈÄöËøáÂ∞ÜËΩ®ËøπÊ≥®ÊÑèÂäõ‰Ωú‰∏∫ËæÖÂä©ÂàÜÊîØ‰∏é‰º†ÁªüÊó∂Èó¥Ê≥®ÊÑèÂäõÁªìÂêàÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®ÁîüÊàêÊñ∞ÂÜÖÂÆπÁöÑÂêåÊó∂ÔºåÁ°Æ‰øù‰∫ÜËøêÂä®ÊéßÂà∂ÁöÑÁ≤æÁ°ÆÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®ÂõæÂÉèÂíåËßÜÈ¢ëÁöÑÁõ∏Êú∫ËøêÂä®ÊéßÂà∂‰∏≠ÊòæËëóÊèêÈ´ò‰∫ÜÁ≤æÂ∫¶ÂíåÈïøË∑ùÁ¶ª‰∏ÄËá¥ÊÄß„ÄÇ"
                }
            }
        }
    ],
    "link_prev": "2024-11-29.html",
    "link_next": "2024-12-03.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "29.11",
        "en": "11/29",
        "zh": "11Êúà29Êó•"
    },
    "short_date_next": {
        "ru": "03.12",
        "en": "12/03",
        "zh": "12Êúà3Êó•"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "ËøôÁØáÊñáÁ´†‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÊ°ÜÊû∂ÔºåÁß∞‰∏∫Critic-VÔºåÊó®Âú®ÊèêÈ´òÂ§öÊ®°ÊÄÅÊé®ÁêÜ‰ªªÂä°ÁöÑËÉΩÂäõ„ÄÇËØ•Ê°ÜÊû∂Áî±‰∏§‰∏™Áã¨Á´ãÁªÑ‰ª∂ÁªÑÊàêÔºöÊé®ÁêÜÂô®ÂíåËØÑËÆ∫Âô®„ÄÇÊé®ÁêÜÂô®Ê†πÊçÆËßÜËßâÂíåÊñáÊú¨ËæìÂÖ•ÁîüÊàêÊé®ÁêÜË∑ØÂæÑÔºåËÄåËØÑËÆ∫Âô®Êèê‰æõÂª∫ËÆæÊÄßÁöÑÊâπËØÑÊù•ÁªÜÂåñËøô‰∫õË∑ØÂæÑ„ÄÇËØÑËÆ∫Âô®‰ΩøÁî®Áõ¥Êé•ÂÅèÂ•Ω‰ºòÂåñÔºàDPOÔºâËøõË°åËÆ≠ÁªÉÔºå‰ª•Â¢ûÂº∫ÂÖ∂ËØÑËÆ∫ËÉΩÂäõ„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåCritic-VÊ°ÜÊû∂Âú®5‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÁâπÂà´ÊòØÂú®Êé®ÁêÜÂáÜÁ°ÆÊÄßÂíåÊïàÁéáÊñπÈù¢„ÄÇ",
        "title": "Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning",
        "pinyin": "ËøôÁØáÊñáÁ´†‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÊ°ÜÊû∂ÔºåÁß∞‰∏∫Critic-VÔºåÊó®Âú®ÊèêÈ´òÂ§öÊ®°ÊÄÅÊé®ÁêÜ‰ªªÂä°ÁöÑËÉΩÂäõ„ÄÇËØ•Ê°ÜÊû∂Áî±‰∏§‰∏™Áã¨Á´ãÁªÑ‰ª∂ÁªÑÊàêÔºöÊé®ÁêÜÂô®ÂíåËØÑËÆ∫Âô®„ÄÇÊé®ÁêÜÂô®Ê†πÊçÆËßÜËßâÂíåÊñáÊú¨ËæìÂÖ•ÁîüÊàêÊé®ÁêÜË∑ØÂæÑÔºåËÄåËØÑËÆ∫Âô®Êèê‰æõÂª∫ËÆæÊÄßÁöÑÊâπËØÑÊù•ÁªÜÂåñËøô‰∫õË∑ØÂæÑ„ÄÇËØÑËÆ∫Âô®‰ΩøÁî®Áõ¥Êé•ÂÅèÂ•Ω‰ºòÂåñÔºàDPOÔºâËøõË°åËÆ≠ÁªÉÔºå‰ª•Â¢ûÂº∫ÂÖ∂ËØÑËÆ∫ËÉΩÂäõ„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåCritic-VÊ°ÜÊû∂Âú®5‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÁâπÂà´ÊòØÂú®Êé®ÁêÜÂáÜÁ°ÆÊÄßÂíåÊïàÁéáÊñπÈù¢„ÄÇ\n\nZh√® piƒÅn w√©nzhƒÅng ji√®sh√†o le yƒ´ zh«íng xƒ´n de sh√¨ju√©-y«îy√°n m√≥x√≠ng ku√†ngji√†, chƒìngw√©i Critic-V, zh«êy«ên t√≠gƒÅo du≈ç m√≥sh√¨ tuƒ´l«ê r√®nw√π de n√©ngl√¨. G«éi ku√†ngji√† y«íu li«éng g√® d√∫l√¨ z«îji√†n z«îch√©ng: tuƒ´l«êq√¨ h√© p√≠ngl√πnq√¨. Tuƒ´l«êq√¨ gƒìnj√π sh√¨ju√© h√© w√©nbƒõn sh≈´r√π shƒìngch√©ng tuƒ´l«ê l√πj√¨ng, √©r p√≠ngl√πnq√¨ t√≠g≈çng ji√†nsh√®x√¨ng de pƒ´p√≠ng l√°i x√¨hu√† zh√®xiƒì l√πj√¨ng. P√≠ngl√πnq√¨ sh«êy√≤ng zh√≠jiƒì piƒÅnhu√† y≈çuhu√† (DPO) j√¨nx√≠ng x√πnli√†n, y«ê zƒìngqi√°ng q√≠ p√≠ngl√πn n√©ngl√¨. Sh√≠y√†n ji√©gu«í xi«énsh√¨, Critic-V ku√†ngji√† z√†i 5 g√® jƒ´zh«în c√®sh√¨ zh≈çng xi«énzh√π y≈çu xi√†nz√†i fƒÅngf«é, t√®bi√© sh√¨ z√†i tuƒ´l«ê zh«înqu√®x√¨ng h√© xi√†ol«ú fƒÅngmi√†n.",
        "vocab": "[{'word': 'ËßÜËßâ', 'pinyin': 'sh√¨ju√©', 'trans': 'vision'},\n{'word': 'ËØ≠Ë®Ä', 'pinyin': 'y«îy√°n', 'trans': 'language'},\n{'word': 'Ê®°Âûã', 'pinyin': 'm√≥x√≠ng', 'trans': 'model'},\n{'word': 'Ê°ÜÊû∂', 'pinyin': 'ku√†ngji√†', 'trans': 'framework'},\n{'word': 'Áß∞‰∏∫', 'pinyin': 'chƒìngw√©i', 'trans': 'called'},\n{'word': 'Êó®Âú®', 'pinyin': 'zh«êz√†i', 'trans': 'aim to'},\n{'word': 'ÊèêÈ´ò', 'pinyin': 't√≠gƒÅo', 'trans': 'improve'},\n{'word': 'Â§öÊ®°ÊÄÅ', 'pinyin': 'du≈çm√≥shu√†i', 'trans': 'multimodal'},\n{'word': 'Êé®ÁêÜ', 'pinyin': 'tuƒ´l«ê', 'trans': 'reasoning'},\n{'word': '‰ªªÂä°', 'pinyin': 'r√®nw√π', 'trans': 'task'},\n{'word': 'ËÉΩÂäõ', 'pinyin': 'n√©ngl√¨', 'trans': 'ability'},\n{'word': 'Áã¨Á´ã', 'pinyin': 'd√∫l√¨', 'trans': 'independent'},\n{'word': 'ÁªÑ‰ª∂', 'pinyin': 'z«îji√†n', 'trans': 'component'},\n{'word': 'ÁªÑÊàê', 'pinyin': 'zh«îch√©ng', 'trans': 'composed of'},\n{'word': 'Êé®ÁêÜÂô®', 'pinyin': 'tuƒ´l«êq√¨', 'trans': 'reasoner'},\n{'word': 'ËØÑËÆ∫Âô®', 'pinyin': 'p√≠ngl√πnq√¨', 'trans': 'critic'},\n{'word': 'Ê†πÊçÆ', 'pinyin': 'gƒìnj√π', 'trans': 'based on'},\n{'word': 'ËæìÂÖ•', 'pinyin': 'sh≈´r√π', 'trans': 'input'},\n{'word': 'ÁîüÊàê', 'pinyin': 'shƒìngch√©ng', 'trans': 'generate'},\n{'word': 'Ë∑ØÂæÑ', 'pinyin': 'l√πj√¨ng', 'trans': 'path'},\n{'word': 'Âª∫ËÆæÊÄß', 'pinyin': 'ji√†nsh√®x√¨ng', 'trans': 'constructive'},\n{'word': 'ÊâπËØÑ', 'pinyin': 'pƒ´p√≠ng', 'trans': 'criticism'},\n{'word': 'ÁªÜÂåñ', 'pinyin': 'x√¨hu√†', 'trans': 'refine'},\n{'word': 'Áõ¥Êé•', 'pinyin': 'zh√≠jiƒì', 'trans': 'direct'},\n{'word': 'ÂÅèÂ•Ω', 'pinyin': 'piƒÅnh√†o', 'trans': 'preference'},\n{'word': '‰ºòÂåñ', 'pinyin': 'y≈çuhu√†', 'trans': 'optimization'},\n{'word': 'ËøõË°å', 'pinyin': 'j√¨nx√≠ng', 'trans': 'conduct'},\n{'word': 'ËÆ≠ÁªÉ', 'pinyin': 'x√πnli√†n', 'trans': 'training'},\n{'word': 'Â¢ûÂº∫', 'pinyin': 'zƒìngqi√°ng', 'trans': 'enhance'},\n{'word': 'ËØÑËÆ∫', 'pinyin': 'p√≠ngl√πn', 'trans': 'comment'},\n{'word': 'ËÉΩÂäõ', 'pinyin': 'n√©ngl√¨', 'trans': 'ability'},\n{'word': 'ÂÆûÈ™å', 'pinyin': 'sh√≠y√†n', 'trans': 'experiment'},\n{'word': 'ÁªìÊûú', 'pinyin': 'ji√©gu«í', 'trans': 'result'},\n{'word': 'ÊòæÁ§∫', 'pinyin': 'xi«énsh√¨', 'trans': 'show'},\n{'word': 'ÊòæËëó', 'pinyin': 'xi«énzh√π', 'trans': 'significant'},\n{'word': '‰ºò‰∫é', 'pinyin': 'y≈çuy√∫', 'trans': 'superior to'},\n{'word': 'Áé∞Êúâ', 'pinyin': 'xi√†ny«íu', 'trans': 'existing'},\n{'word': 'ÊñπÊ≥ï', 'pinyin': 'fƒÅngf«é', 'trans': 'method'},\n{'word': 'ÁâπÂà´', 'pinyin': 't√®bi√©', 'trans': 'especially'},\n{'word': 'ÂáÜÁ°ÆÊÄß', 'pinyin': 'zh«înqu√®x√¨ng', 'trans': 'accuracy'},\n{'word': 'ÊïàÁéá', 'pinyin': 'xi√†ol«ú', 'trans': 'efficiency'}]",
        "trans": "This article introduces a new vision-language model framework called Critic-V, aimed at enhancing the capabilities of multimodal reasoning tasks. The framework consists of two independent components: a reasoner and a critic. The reasoner generates reasoning paths based on visual and textual inputs, while the critic provides constructive criticism to refine these paths. The critic is trained using direct preference optimization (DPO) to enhance its critiquing ability. Experimental results demonstrate that the Critic-V framework significantly outperforms existing methods on five benchmark tests, particularly in terms of reasoning accuracy and efficiency.",
        "update_ts": "2024-12-01 12:42"
    }
}