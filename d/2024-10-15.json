{
    "date": {
        "ru": "15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
        "en": "October 15",
        "zh": "10æœˆ15æ—¥"
    },
    "time_utc": "2024-10-16 20:13",
    "issue_id": 131,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2406.15786",
            "title": "What Matters in Transformers? Not All Attention is Needed",
            "url": "https://huggingface.co/papers/2406.15786",
            "abstract": "While scaling Transformer-based large language models (LLMs) has demonstrated promising performance across various tasks, it also introduces redundant architectures, posing efficiency challenges for real-world deployment. Despite some recognition of redundancy in LLMs, the variability of redundancy across different architectures in transformers, such as MLP and Attention layers, is under-explored. In this work, we investigate redundancy across different modules within Transformers, including Blocks, MLP, and Attention layers, using a similarity-based metric. Surprisingly, despite the critical role of attention layers in distinguishing transformers from other architectures, we found that a large portion of these layers exhibit excessively high similarity and can be pruned without degrading performance. For instance, Llama-2-70B achieved a 48.4\\% speedup with only a 2.4\\% performance drop by pruning half of the attention layers. Furthermore, by tracing model checkpoints throughout the training process, we observed that attention layer redundancy is inherent and consistent across training stages. Additionally, we further propose a method that jointly drops Attention and MLP layers, allowing us to more aggressively drop additional layers. For instance, when dropping 31 layers (Attention + MLP), Llama-2-13B still retains 90\\% of the performance on the MMLU task. Our work provides valuable insights for future network architecture design. The code is released at: https://github.com/Shwai-He/LLM-Drop.",
            "score": 18,
            "issue_id": 121,
            "pub_date": "2024-06-22",
            "pub_date_ru": "22 Ğ¸ÑĞ½Ñ",
            "data": {
                "categories": [
                    "#architecture",
                    "#inference"
                ],
                "emoji": "âœ‚ï¸",
                "ru": {
                    "title": "ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ LLM: Ğ¼ĞµĞ½ÑŒÑˆĞµ ÑĞ»Ğ¾ĞµĞ², Ñ‚Ğ° Ğ¶Ğµ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚ÑŒ",
                    "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ñ‡Ğ°ÑÑ‚ÑŒ ÑĞ»Ğ¾ĞµĞ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ (attention layers) Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑÑ…Ğ¾Ğ¶ĞµÑÑ‚ÑŒÑ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ° Ğ±ĞµĞ· ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Llama-2-70B Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ½Ğ° 48.4% Ğ¿Ñ€Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²ÑĞµĞ³Ğ¾ Ğ½Ğ° 2.4% Ğ¿Ğ¾ÑĞ»Ğµ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¾Ğ²Ğ¸Ğ½Ñ‹ ÑĞ»Ğ¾ĞµĞ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾ĞµĞ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ MLP, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞµÑ‰Ğµ Ğ±Ğ¾Ğ»ĞµĞµ Ğ°Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°Ñ‚ÑŒ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Prune to Perform: Streamlining Transformers for Efficiency",
                    "desc": "This paper explores the redundancy in Transformer-based large language models, focusing on the MLP and Attention layers. The authors found that many attention layers are highly similar and can be pruned without significantly affecting performance, as demonstrated by a 48.4% speedup in Llama-2-70B with minimal performance loss. They also propose a method to jointly prune Attention and MLP layers, achieving substantial efficiency gains while maintaining most of the model's performance. This research offers insights into optimizing Transformer architectures for more efficient real-world applications."
                },
                "zh": {
                    "title": "ä¼˜åŒ–Transformerï¼šå‡å°‘å†—ä½™ï¼Œæå‡æ•ˆç‡",
                    "desc": "è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†åœ¨Transformeræ¨¡å‹ä¸­ä¸åŒæ¨¡å—çš„å†—ä½™é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯æ³¨æ„åŠ›å±‚å’ŒMLPå±‚ã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡æ³¨æ„åŠ›å±‚åœ¨Transformerä¸­éå¸¸é‡è¦ï¼Œä½†è®¸å¤šå±‚çš„ç›¸ä¼¼åº¦è¿‡é«˜ï¼Œå¯ä»¥è¢«å‰ªæè€Œä¸å½±å“æ€§èƒ½ã€‚é€šè¿‡å‰ªææ³¨æ„åŠ›å±‚ï¼Œæ¨¡å‹çš„é€Ÿåº¦å¯ä»¥æ˜¾è‘—æé«˜ï¼Œè€Œæ€§èƒ½æŸå¤±å¾ˆå°ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†ä¸€ç§åŒæ—¶å‰ªææ³¨æ„åŠ›å±‚å’ŒMLPå±‚çš„æ–¹æ³•ï¼Œè¿›ä¸€æ­¥æé«˜äº†æ¨¡å‹çš„æ•ˆç‡ã€‚"
                }
            },
            "hash": "fdf872015ec393aa",
            "pub_date_card": {
                "ru": "22 Ğ¸ÑĞ½Ñ",
                "en": "June 22",
                "zh": "6æœˆ22æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.11779",
            "title": "MLLM can see? Dynamic Correction Decoding for Hallucination Mitigation",
            "url": "https://huggingface.co/papers/2410.11779",
            "abstract": "Multimodal Large Language Models (MLLMs) frequently exhibit hallucination phenomena, but the underlying reasons remain poorly understood. In this paper, we present an empirical analysis and find that, although MLLMs incorrectly generate the objects in the final output, they are actually able to recognize visual objects in the preceding layers. We speculate that this may be due to the strong knowledge priors of the language model suppressing the visual information, leading to hallucinations. Motivated by this, we propose a novel dynamic correction decoding method for MLLMs (DeCo), which adaptively selects the appropriate preceding layers and proportionally integrates knowledge into the final layer to adjust the output logits. Note that DeCo is model agnostic and can be seamlessly incorporated with various classic decoding strategies and applied to different MLLMs. We evaluate DeCo on widely-used benchmarks, demonstrating that it can reduce hallucination rates by a large margin compared to baselines, highlighting its potential to mitigate hallucinations. Code is available at https://github.com/zjunlp/DeCo.",
            "score": 18,
            "issue_id": 121,
            "pub_date": "2024-10-15",
            "pub_date_ru": "15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#cv",
                    "#hallucinations",
                    "#multimodal"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ‘Ğ¾Ñ€ÑŒĞ±Ğ° Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (MLLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ MLLM ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ² Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑĞ»Ğ¾ÑÑ…, Ğ½Ğ¾ Ğ² Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ğ¼ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¸Ñ… Ğ½ĞµĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ. ĞŸÑ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾ ÑĞ²ÑĞ·Ğ°Ğ½Ğ¾ Ñ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ DeCo, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… ÑĞ»Ğ¾ĞµĞ² Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ»Ğ¾Ğ¹."
                },
                "en": {
                    "title": "DeCo: Correcting Hallucinations in Multimodal Models",
                    "desc": "This paper investigates why Multimodal Large Language Models (MLLMs) often produce hallucinations, where they generate incorrect outputs. The authors discover that while MLLMs can recognize visual objects in earlier layers, the language model's strong knowledge priors may suppress this visual information, causing hallucinations. To address this, they introduce a new method called dynamic correction decoding (DeCo), which adjusts the integration of knowledge in the final output layer. DeCo is versatile and can be used with different models and decoding strategies, significantly reducing hallucination rates in tests."
                },
                "zh": {
                    "title": "åŠ¨æ€æ ¡æ­£è§£ç ï¼šå‡å°‘å¤šæ¨¡æ€æ¨¡å‹å¹»è§‰çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å¸¸å¸¸ä¼šå‡ºç°å¹»è§‰ç°è±¡ï¼Œä½†å…¶åŸå› å°šä¸æ˜ç¡®ã€‚æœ¬æ–‡é€šè¿‡å®è¯åˆ†æå‘ç°ï¼Œå°½ç®¡MLLMsåœ¨æœ€ç»ˆè¾“å‡ºä¸­é”™è¯¯ç”Ÿæˆå¯¹è±¡ï¼Œä½†å®ƒä»¬å®é™…ä¸Šèƒ½å¤Ÿåœ¨å‰é¢çš„å±‚ä¸­è¯†åˆ«è§†è§‰å¯¹è±¡ã€‚æˆ‘ä»¬æ¨æµ‹è¿™å¯èƒ½æ˜¯ç”±äºè¯­è¨€æ¨¡å‹çš„å¼ºçŸ¥è¯†å…ˆéªŒæŠ‘åˆ¶äº†è§†è§‰ä¿¡æ¯ï¼Œå¯¼è‡´å¹»è§‰ç°è±¡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åŠ¨æ€æ ¡æ­£è§£ç æ–¹æ³•ï¼ˆDeCoï¼‰ï¼Œå¯ä»¥è‡ªé€‚åº”åœ°é€‰æ‹©åˆé€‚çš„å‰ç½®å±‚ï¼Œå¹¶å°†çŸ¥è¯†æŒ‰æ¯”ä¾‹æ•´åˆåˆ°æœ€ç»ˆå±‚ä¸­ï¼Œä»¥è°ƒæ•´è¾“å‡ºã€‚"
                }
            },
            "hash": "25a9176442c65ae3",
            "pub_date_card": {
                "ru": "15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 15",
                "zh": "10æœˆ15æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.10814",
            "title": "Your Mixture-of-Experts LLM Is Secretly an Embedding Model For Free",
            "url": "https://huggingface.co/papers/2410.10814",
            "abstract": "While large language models (LLMs) excel on generation tasks, their decoder-only architecture often limits their potential as embedding models if no further representation finetuning is applied. Does this contradict their claim of generalists? To answer the question, we take a closer look at Mixture-of-Experts (MoE) LLMs. Our study shows that the expert routers in MoE LLMs can serve as an off-the-shelf embedding model with promising performance on a diverse class of embedding-focused tasks, without requiring any finetuning. Moreover, our extensive analysis shows that the MoE routing weights (RW) is complementary to the hidden state (HS) of LLMs, a widely-used embedding. Compared to HS, we find that RW is more robust to the choice of prompts and focuses on high-level semantics. Motivated by the analysis, we propose MoEE combining RW and HS, which achieves better performance than using either separately. Our exploration of their combination and prompting strategy shed several novel insights, e.g., a weighted sum of RW and HS similarities outperforms the similarity on their concatenation. Our experiments are conducted on 6 embedding tasks with 20 datasets from the Massive Text Embedding Benchmark (MTEB). The results demonstrate the significant improvement brought by MoEE to LLM-based embedding without further finetuning.",
            "score": 17,
            "issue_id": 124,
            "pub_date": "2024-10-14",
            "pub_date_ru": "14 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "MoE LLM: Ğ¡ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Mixture-of-Experts (MoE) LLM Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ÑŒ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±ĞµÑ‰Ğ°ÑÑ‰ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²ĞµÑĞ° Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ MoE (RW) Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‚ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ (HS) LLM, ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ğ¾Ğµ Ğ´Ğ»Ñ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ MoEE, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ RW Ğ¸ HS, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, Ñ‡ĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¸ÑÑŒ Ğ½Ğ° 6 Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ° Ñ 20 Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Massive Text Embedding Benchmark (MTEB)."
                },
                "en": {
                    "title": "Unlocking the Embedding Power of MoE LLMs: No Finetuning Needed!",
                    "desc": "The paper investigates the potential of Mixture-of-Experts (MoE) large language models (LLMs) as effective embedding models without additional finetuning. It reveals that the expert routers in MoE LLMs can be used directly for embedding tasks, showing strong performance across various tasks. The study finds that the routing weights (RW) in MoE models complement the hidden states (HS) of LLMs, offering robustness to prompt variations and focusing on high-level semantics. By combining RW and HS, the proposed MoEE model achieves superior results, as demonstrated on multiple datasets from the Massive Text Embedding Benchmark (MTEB)."
                },
                "zh": {
                    "title": "ä¸“å®¶æ··åˆï¼šæå‡è¯­è¨€æ¨¡å‹åµŒå…¥èƒ½åŠ›çš„æ–°è·¯å¾„",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†å…¶ä»…è§£ç æ¶æ„é™åˆ¶äº†å…¶ä½œä¸ºåµŒå…¥æ¨¡å‹çš„æ½œåŠ›ã€‚æˆ‘ä»¬ç ”ç©¶äº†ä¸“å®¶æ··åˆï¼ˆMoEï¼‰LLMï¼Œå‘ç°å…¶ä¸“å®¶è·¯ç”±å™¨æ— éœ€å¾®è°ƒå³å¯åœ¨å¤šç§åµŒå…¥ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ã€‚MoEçš„è·¯ç”±æƒé‡ï¼ˆRWï¼‰ä¸éšè—çŠ¶æ€ï¼ˆHSï¼‰äº’è¡¥ï¼Œä¸”å¯¹æç¤ºé€‰æ‹©æ›´å…·é²æ£’æ€§ã€‚æˆ‘ä»¬æå‡ºçš„MoEEç»“åˆäº†RWå’ŒHSï¼Œæ˜¾è‘—æå‡äº†åµŒå…¥ä»»åŠ¡çš„æ€§èƒ½ã€‚"
                }
            },
            "hash": "c2ddb801d2228a78",
            "pub_date_card": {
                "ru": "14 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 14",
                "zh": "10æœˆ14æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.11710",
            "title": "MTU-Bench: A Multi-granularity Tool-Use Benchmark for Large Language Models",
            "url": "https://huggingface.co/papers/2410.11710",
            "abstract": "Large Language Models (LLMs) have displayed massive improvements in reasoning and decision-making skills and can hold natural conversations with users. Recently, many tool-use benchmark datasets have been proposed. However, existing datasets have the following limitations: (1). Insufficient evaluation scenarios (e.g., only cover limited tool-use scenes). (2). Extensive evaluation costs (e.g., GPT API costs). To address these limitations, in this work, we propose a multi-granularity tool-use benchmark for large language models called MTU-Bench. For the \"multi-granularity\" property, our MTU-Bench covers five tool usage scenes (i.e., single-turn and single-tool, single-turn and multiple-tool, multiple-turn and single-tool, multiple-turn and multiple-tool, and out-of-distribution tasks). Besides, all evaluation metrics of our MTU-Bench are based on the prediction results and the ground truth without using any GPT or human evaluation metrics. Moreover, our MTU-Bench is collected by transforming existing high-quality datasets to simulate real-world tool usage scenarios, and we also propose an instruction dataset called MTU-Instruct data to enhance the tool-use abilities of existing LLMs. Comprehensive experimental results demonstrate the effectiveness of our MTU-Bench. Code and data will be released at https: //github.com/MTU-Bench-Team/MTU-Bench.git.",
            "score": 16,
            "issue_id": 121,
            "pub_date": "2024-10-15",
            "pub_date_ru": "15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#rlhf"
                ],
                "emoji": "ğŸ› ï¸",
                "ru": {
                    "title": "ĞœĞ½Ğ¾Ğ³Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² LLM Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº MTU-Bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LLM). MTU-Bench Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ÑÑ‚ÑŒ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ´Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¸ Ğ½ĞµÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞÑ†ĞµĞ½ĞºĞ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ API GPT Ğ¸Ğ»Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¸Ğ·Ñ‹, Ñ‡Ñ‚Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… MTU-Instruct Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ LLM Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "MTU-Bench: Elevating LLM Tool-Use Evaluation",
                    "desc": "The paper introduces MTU-Bench, a new benchmark designed to evaluate large language models (LLMs) on their ability to use tools effectively. MTU-Bench addresses the limitations of existing datasets by providing a multi-granularity approach, covering various tool-use scenarios such as single or multiple tools and turns, as well as out-of-distribution tasks. Unlike previous benchmarks, MTU-Bench relies solely on prediction results and ground truth for evaluation, avoiding costly GPT or human assessments. Additionally, the authors present MTU-Instruct, a dataset aimed at enhancing LLMs' tool-use capabilities, demonstrating its effectiveness through comprehensive experiments."
                },
                "zh": {
                    "title": "MTU-Benchï¼šæå‡è¯­è¨€æ¨¡å‹å·¥å…·ä½¿ç”¨èƒ½åŠ›çš„æ–°åŸºå‡†",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨ç†å’Œå†³ç­–èƒ½åŠ›ä¸Šæœ‰äº†æ˜¾è‘—æå‡ï¼Œå¹¶èƒ½ä¸ç”¨æˆ·è¿›è¡Œè‡ªç„¶å¯¹è¯ã€‚ç°æœ‰çš„å·¥å…·ä½¿ç”¨åŸºå‡†æ•°æ®é›†å­˜åœ¨è¯„ä¼°åœºæ™¯ä¸è¶³å’Œè¯„ä¼°æˆæœ¬é«˜çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå¤šç²’åº¦å·¥å…·ä½¿ç”¨åŸºå‡†MTU-Benchï¼Œæ¶µç›–äº”ç§å·¥å…·ä½¿ç”¨åœºæ™¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMTU-Benchåœ¨æå‡è¯­è¨€æ¨¡å‹å·¥å…·ä½¿ç”¨èƒ½åŠ›æ–¹é¢æ•ˆæœæ˜¾è‘—ã€‚"
                }
            },
            "hash": "44b55db6ab50c3d4",
            "pub_date_card": {
                "ru": "15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 15",
                "zh": "10æœˆ15æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.09342",
            "title": "LLM$\\times$MapReduce: Simplified Long-Sequence Processing using Large Language Models",
            "url": "https://huggingface.co/papers/2410.09342",
            "abstract": "Enlarging the context window of large language models (LLMs) has become a crucial research area, particularly for applications involving extremely long texts. In this work, we propose a novel training-free framework for processing long texts, utilizing a divide-and-conquer strategy to achieve comprehensive document understanding. The proposed LLMtimesMapReduce framework splits the entire document into several chunks for LLMs to read and then aggregates the intermediate answers to produce the final output. The main challenge for divide-and-conquer long text processing frameworks lies in the risk of losing essential long-range information when splitting the document, which can lead the model to produce incomplete or incorrect answers based on the segmented texts. Disrupted long-range information can be classified into two categories: inter-chunk dependency and inter-chunk conflict. We design a structured information protocol to better cope with inter-chunk dependency and an in-context confidence calibration mechanism to resolve inter-chunk conflicts. Experimental results demonstrate that LLMtimesMapReduce can outperform representative open-source and commercial long-context LLMs, and is applicable to several different models.",
            "score": 15,
            "issue_id": 125,
            "pub_date": "2024-10-12",
            "pub_date_ru": "12 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#benchmark",
                    "#long_context",
                    "#rag"
                ],
                "emoji": "ğŸ“„",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº LLMÃ—MapReduce, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚ Ğ½Ğ° Ñ‡Ğ°ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ñ‹ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¸ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LLMÃ—MapReduce Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ LLM."
                },
                "en": {
                    "title": "Divide and Conquer: Mastering Long Texts with LLMtimesMapReduce",
                    "desc": "The paper introduces a new method called LLMtimesMapReduce to help large language models (LLMs) understand very long texts without needing extra training. It uses a divide-and-conquer approach, breaking the text into smaller parts for the LLMs to process, and then combines the results to form a complete understanding. The main challenge is ensuring that important information isn't lost when the text is split, which the authors address with a structured information protocol and a confidence calibration mechanism. Tests show that this method works better than other similar tools and can be used with different LLMs."
                },
                "zh": {
                    "title": "åˆ†è€Œæ²»ä¹‹ï¼šæ— è®­ç»ƒæ¡†æ¶å¤„ç†è¶…é•¿æ–‡æœ¬",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ— è®­ç»ƒæ¡†æ¶ï¼Œç§°ä¸ºLLMtimesMapReduceï¼Œç”¨äºå¤„ç†è¶…é•¿æ–‡æœ¬ã€‚è¯¥æ¡†æ¶é€šè¿‡åˆ†è€Œæ²»ä¹‹çš„æ–¹æ³•ï¼Œå°†æ–‡æ¡£åˆ†æˆå¤šä¸ªéƒ¨åˆ†ä¾›å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é˜…è¯»ï¼Œç„¶åæ±‡æ€»ä¸­é—´ç­”æ¡ˆä»¥ç”Ÿæˆæœ€ç»ˆè¾“å‡ºã€‚ä¸»è¦æŒ‘æˆ˜åœ¨äºåˆ†å‰²æ–‡æ¡£æ—¶å¯èƒ½ä¸¢å¤±é‡è¦çš„é•¿è·ç¦»ä¿¡æ¯ï¼Œå¯¼è‡´æ¨¡å‹åŸºäºåˆ†æ®µæ–‡æœ¬äº§ç”Ÿä¸å®Œæ•´æˆ–é”™è¯¯çš„ç­”æ¡ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMtimesMapReduceåœ¨å¤„ç†é•¿æ–‡æœ¬æ–¹é¢ä¼˜äºå…¶ä»–å¼€æºå’Œå•†ä¸šé•¿ä¸Šä¸‹æ–‡LLMã€‚"
                }
            },
            "hash": "214a55e4bcf12eb6",
            "pub_date_card": {
                "ru": "12 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 12",
                "zh": "10æœˆ12æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.10626",
            "title": "Efficiently Democratizing Medical LLMs for 50 Languages via a Mixture of Language Family Experts",
            "url": "https://huggingface.co/papers/2410.10626",
            "abstract": "Adapting medical Large Language Models to local languages can reduce barriers to accessing healthcare services, but data scarcity remains a significant challenge, particularly for low-resource languages. To address this, we first construct a high-quality medical dataset and conduct analysis to ensure its quality. In order to leverage the generalization capability of multilingual LLMs to efficiently scale to more resource-constrained languages, we explore the internal information flow of LLMs from a multilingual perspective using Mixture of Experts (MoE) modularity. Technically, we propose a novel MoE routing method that employs language-specific experts and cross-lingual routing. Inspired by circuit theory, our routing analysis revealed a Spread Out in the End information flow mechanism: while earlier layers concentrate cross-lingual information flow, the later layers exhibit language-specific divergence. This insight directly led to the development of the Post-MoE architecture, which applies sparse routing only in the later layers while maintaining dense others. Experimental results demonstrate that this approach enhances the generalization of multilingual models to other languages while preserving interpretability. Finally, to efficiently scale the model to 50 languages, we introduce the concept of language family experts, drawing on linguistic priors, which enables scaling the number of languages without adding additional parameters.",
            "score": 13,
            "issue_id": 121,
            "pub_date": "2024-10-14",
            "pub_date_ru": "14 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#architecture",
                    "#dataset",
                    "#medicine",
                    "#multilingual"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "ĞœĞ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğµ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğµ LLM: Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ±Ğ°Ñ€ÑŒĞµÑ€Ğ¾Ğ² Ğ² Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ñ†ĞµĞ»ĞµĞ¹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… LLM Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Mixture of Experts (MoE). ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ MoE Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ ĞºÑ€Ğ¾ÑÑ-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Post-MoE, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‰Ğ°Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ² Ğ¿Ğ¾Ğ·Ğ´Ğ½Ğ¸Ñ… ÑĞ»Ğ¾ÑÑ… Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Breaking Language Barriers in Medical AI",
                    "desc": "The paper addresses the challenge of adapting medical Large Language Models (LLMs) to low-resource languages by constructing a high-quality medical dataset and analyzing its quality. It introduces a novel Mixture of Experts (MoE) routing method that uses language-specific experts and cross-lingual routing to enhance the model's generalization capabilities. Inspired by circuit theory, the authors propose the Post-MoE architecture, which applies sparse routing in later layers to improve language-specific performance while maintaining interpretability. The approach allows the model to scale efficiently to 50 languages by using language family experts, leveraging linguistic priors without increasing parameters."
                },
                "zh": {
                    "title": "è·¨è¯­è¨€åŒ»ç–—æ¨¡å‹ï¼šçªç ´è¯­è¨€éšœç¢çš„åˆ›æ–°ä¹‹è·¯",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¦‚ä½•å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€‚åº”æœ¬åœ°è¯­è¨€ï¼Œä»¥å‡å°‘è·å–åŒ»ç–—æœåŠ¡çš„éšœç¢ã€‚ç ”ç©¶ä¸­ï¼Œä½œè€…é¦–å…ˆæ„å»ºäº†ä¸€ä¸ªé«˜è´¨é‡çš„åŒ»ç–—æ•°æ®é›†ï¼Œå¹¶è¿›è¡Œäº†åˆ†æä»¥ç¡®ä¿å…¶è´¨é‡ã€‚é€šè¿‡ä½¿ç”¨ä¸“å®¶æ··åˆï¼ˆMoEï¼‰æ¨¡å—åŒ–æ–¹æ³•ï¼Œç ”ç©¶äº†å¤šè¯­è¨€LLMçš„å†…éƒ¨ä¿¡æ¯æµï¼Œæå‡ºäº†ä¸€ç§æ–°çš„MoEè·¯ç”±æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•æé«˜äº†å¤šè¯­è¨€æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒäº†è§£é‡Šæ€§ã€‚"
                }
            },
            "hash": "e8c698bf76856366",
            "pub_date_card": {
                "ru": "14 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 14",
                "zh": "10æœˆ14æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.11096",
            "title": "SecCodePLT: A Unified Platform for Evaluating the Security of Code GenAI",
            "url": "https://huggingface.co/papers/2410.11096",
            "abstract": "Existing works have established multiple benchmarks to highlight the security risks associated with Code GenAI. These risks are primarily reflected in two areas: a model potential to generate insecure code (insecure coding) and its utility in cyberattacks (cyberattack helpfulness). While these benchmarks have made significant strides, there remain opportunities for further improvement. For instance, many current benchmarks tend to focus more on a model ability to provide attack suggestions rather than its capacity to generate executable attacks. Additionally, most benchmarks rely heavily on static evaluation metrics, which may not be as precise as dynamic metrics such as passing test cases. Conversely, expert-verified benchmarks, while offering high-quality data, often operate at a smaller scale. To address these gaps, we develop SecCodePLT, a unified and comprehensive evaluation platform for code GenAIs' risks. For insecure code, we introduce a new methodology for data creation that combines experts with automatic generation. Our methodology ensures the data quality while enabling large-scale generation. We also associate samples with test cases to conduct code-related dynamic evaluation. For cyberattack helpfulness, we set up a real environment and construct samples to prompt a model to generate actual attacks, along with dynamic metrics in our environment. We conduct extensive experiments and show that SecCodePLT outperforms the state-of-the-art (SOTA) benchmark CyberSecEval in security relevance. Furthermore, it better identifies the security risks of SOTA models in insecure coding and cyberattack helpfulness. Finally, we apply SecCodePLT to the SOTA code agent, Cursor, and, for the first time, identify non-trivial security risks in this advanced coding agent.",
            "score": 12,
            "issue_id": 121,
            "pub_date": "2024-10-14",
            "pub_date_ru": "14 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#security"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "SecCodePLT: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ AI-ĞºĞ¾Ğ´Ğ¾Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SecCodePLT - Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñƒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ¸ÑĞºĞ¾Ğ² Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ AI Ğ´Ğ»Ñ ĞºĞ¾Ğ´Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ÑƒÑ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ½ĞµĞ´Ñ€Ğ¸Ğ»Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. SecCodePLT Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ğº Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµĞ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ´, Ñ‚Ğ°Ğº Ğ¸ Ğ¸Ñ… Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ ĞºĞ¸Ğ±ĞµÑ€Ğ°Ñ‚Ğ°Ğº. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ SecCodePLT Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ğ¼Ğ¸ Ğ² Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ¸ÑĞºĞ¾Ğ² Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ´Ğ¾Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "SecCodePLT: Elevating Code GenAI Security Evaluation",
                    "desc": "The paper introduces SecCodePLT, a new platform designed to evaluate the security risks of Code GenAI models more effectively. It addresses the limitations of existing benchmarks by combining expert input with automatic data generation to ensure high-quality, large-scale data for insecure code evaluation. The platform also uses dynamic evaluation metrics, such as test cases, to assess the models' ability to generate executable attacks in a real environment. Extensive experiments demonstrate that SecCodePLT surpasses current benchmarks in identifying security risks, revealing significant vulnerabilities in advanced coding agents like Cursor."
                },
                "zh": {
                    "title": "SecCodePLTï¼šæå‡ä»£ç ç”ŸæˆAIå®‰å…¨è¯„ä¼°çš„æ–°å¹³å°",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºSecCodePLTçš„æ–°å¹³å°ï¼Œç”¨äºè¯„ä¼°ä»£ç ç”ŸæˆAIçš„å®‰å…¨é£é™©ã€‚SecCodePLTé€šè¿‡ç»“åˆä¸“å®¶å’Œè‡ªåŠ¨ç”Ÿæˆçš„æ–¹æ³•ï¼Œåˆ›å»ºé«˜è´¨é‡ä¸”å¤§è§„æ¨¡çš„æ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨åŠ¨æ€è¯„ä¼°æ–¹æ³•æ¥æ£€æµ‹ä¸å®‰å…¨ä»£ç å’Œç½‘ç»œæ”»å‡»çš„ç”Ÿæˆèƒ½åŠ›ã€‚ä¸ç°æœ‰çš„åŸºå‡†ç›¸æ¯”ï¼ŒSecCodePLTåœ¨å®‰å…¨ç›¸å…³æ€§ä¸Šè¡¨ç°æ›´å¥½ï¼Œå¹¶èƒ½æ›´å‡†ç¡®åœ°è¯†åˆ«æœ€å…ˆè¿›æ¨¡å‹çš„å®‰å…¨é£é™©ã€‚ç ”ç©¶è¿˜é¦–æ¬¡åœ¨å…ˆè¿›çš„ä»£ç ä»£ç†Cursorä¸­å‘ç°äº†æ˜¾è‘—çš„å®‰å…¨é£é™©ã€‚"
                }
            },
            "hash": "f1b69c45d56f76d8",
            "pub_date_card": {
                "ru": "14 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 14",
                "zh": "10æœˆ14æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.10816",
            "title": "LVD-2M: A Long-take Video Dataset with Temporally Dense Captions",
            "url": "https://huggingface.co/papers/2410.10816",
            "abstract": "The efficacy of video generation models heavily depends on the quality of their training datasets. Most previous video generation models are trained on short video clips, while recently there has been increasing interest in training long video generation models directly on longer videos. However, the lack of such high-quality long videos impedes the advancement of long video generation. To promote research in long video generation, we desire a new dataset with four key features essential for training long video generation models: (1) long videos covering at least 10 seconds, (2) long-take videos without cuts, (3) large motion and diverse contents, and (4) temporally dense captions. To achieve this, we introduce a new pipeline for selecting high-quality long-take videos and generating temporally dense captions. Specifically, we define a set of metrics to quantitatively assess video quality including scene cuts, dynamic degrees, and semantic-level quality, enabling us to filter high-quality long-take videos from a large amount of source videos. Subsequently, we develop a hierarchical video captioning pipeline to annotate long videos with temporally-dense captions. With this pipeline, we curate the first long-take video dataset, LVD-2M, comprising 2 million long-take videos, each covering more than 10 seconds and annotated with temporally dense captions. We further validate the effectiveness of LVD-2M by fine-tuning video generation models to generate long videos with dynamic motions. We believe our work will significantly contribute to future research in long video generation.",
            "score": 11,
            "issue_id": 123,
            "pub_date": "2024-10-14",
            "pub_date_ru": "14 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#long_context",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "LVD-2M: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ LVD-2M Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 2 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ±Ğ¾Ğ»ĞµĞµ 10 ÑĞµĞºÑƒĞ½Ğ´, ÑĞ½ÑÑ‚Ñ‹Ñ… Ğ¾Ğ´Ğ½Ğ¸Ğ¼ ĞºĞ°Ğ´Ñ€Ğ¾Ğ¼ Ğ¸ ÑĞ¾Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ¶Ğ´Ğ°ĞµĞ¼Ñ‹Ñ… Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºÑƒ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹. Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ° ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "\"Unlocking the Future of Long Video Generation\"",
                    "desc": "The paper discusses the importance of high-quality datasets for training video generation models, especially for long videos. It introduces a new dataset, LVD-2M, which includes 2 million long-take videos, each over 10 seconds, with diverse content and detailed captions. The authors developed a pipeline to select these videos based on specific quality metrics and to generate temporally dense captions. This dataset aims to advance research in generating long videos with dynamic motions."
                },
                "zh": {
                    "title": "æ¨åŠ¨é•¿è§†é¢‘ç”Ÿæˆç ”ç©¶çš„æ–°æ•°æ®é›†",
                    "desc": "è¿™ç¯‡è®ºæ–‡è®¨è®ºäº†è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„è®­ç»ƒæ•°æ®é›†è´¨é‡å¯¹æ¨¡å‹æ•ˆæœçš„é‡è¦æ€§ã€‚ä½œè€…æŒ‡å‡ºï¼Œç°æœ‰çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹å¤§å¤šåŸºäºçŸ­è§†é¢‘ç‰‡æ®µï¼Œè€Œé•¿è§†é¢‘ç”Ÿæˆçš„ç ”ç©¶å—åˆ°é«˜è´¨é‡é•¿è§†é¢‘ç¼ºä¹çš„é™åˆ¶ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®é›†é€‰æ‹©æµç¨‹ï¼Œä¸“æ³¨äºé•¿è§†é¢‘çš„è´¨é‡è¯„ä¼°å’Œå¯†é›†å­—å¹•ç”Ÿæˆã€‚æœ€ç»ˆï¼Œä»–ä»¬åˆ›å»ºäº†ä¸€ä¸ªåä¸ºLVD-2Mçš„æ•°æ®é›†ï¼ŒåŒ…å«200ä¸‡æ®µé•¿è§†é¢‘ï¼Œå¹¶éªŒè¯äº†å…¶åœ¨é•¿è§†é¢‘ç”Ÿæˆä¸­çš„æœ‰æ•ˆæ€§ã€‚"
                }
            },
            "hash": "b7f807e5ff9e1614",
            "pub_date_card": {
                "ru": "14 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 14",
                "zh": "10æœˆ14æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.09704",
            "title": "EchoPrime: A Multi-Video View-Informed Vision-Language Model for Comprehensive Echocardiography Interpretation",
            "url": "https://huggingface.co/papers/2410.09704",
            "abstract": "Echocardiography is the most widely used cardiac imaging modality, capturing ultrasound video data to assess cardiac structure and function. Artificial intelligence (AI) in echocardiography has the potential to streamline manual tasks and improve reproducibility and precision. However, most echocardiography AI models are single-view, single-task systems that do not synthesize complementary information from multiple views captured during a full exam, and thus lead to limited performance and scope of applications. To address this problem, we introduce EchoPrime, a multi-view, view-informed, video-based vision-language foundation model trained on over 12 million video-report pairs. EchoPrime uses contrastive learning to train a unified embedding model for all standard views in a comprehensive echocardiogram study with representation of both rare and common diseases and diagnoses. EchoPrime then utilizes view-classification and a view-informed anatomic attention model to weight video-specific interpretations that accurately maps the relationship between echocardiographic views and anatomical structures. With retrieval-augmented interpretation, EchoPrime integrates information from all echocardiogram videos in a comprehensive study and performs holistic comprehensive clinical echocardiography interpretation. In datasets from two independent healthcare systems, EchoPrime achieves state-of-the art performance on 23 diverse benchmarks of cardiac form and function, surpassing the performance of both task-specific approaches and prior foundation models. Following rigorous clinical evaluation, EchoPrime can assist physicians in the automated preliminary assessment of comprehensive echocardiography.",
            "score": 10,
            "issue_id": 127,
            "pub_date": "2024-10-13",
            "pub_date_ru": "13 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#benchmark",
                    "#cv",
                    "#dataset",
                    "#medicine",
                    "#multimodal",
                    "#rag"
                ],
                "emoji": "â¤ï¸",
                "ru": {
                    "title": "EchoPrime: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ ÑÑ…Ğ¾ĞºĞ°Ñ€Ğ´Ğ¸Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼",
                    "desc": "EchoPrime - ÑÑ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ÑÑ…Ğ¾ĞºĞ°Ñ€Ğ´Ğ¸Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 12 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ğ¿Ğ°Ñ€ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ¾Ğ². ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ğ¹ Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ¼ ÑÑ…Ğ¾ĞºĞ°Ñ€Ğ´Ğ¸Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸. EchoPrime Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ğ¹ Ğ¸ Ğ°Ğ½Ğ°Ñ‚Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ…Ğ¾ĞºĞ°Ñ€Ğ´Ğ¸Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ°Ğ½Ğ°Ñ‚Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ² 23 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ ÑĞµÑ€Ğ´Ñ†Ğ°, Ñ‡Ñ‚Ğ¾ Ğ±Ñ‹Ğ»Ğ¾ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¾ Ğ² Ğ´Ğ²ÑƒÑ… Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…."
                },
                "en": {
                    "title": "EchoPrime: Revolutionizing Heart Imaging with Multi-View AI",
                    "desc": "EchoPrime is a new AI model designed to improve echocardiography by integrating information from multiple ultrasound views, unlike traditional single-view models. It uses contrastive learning to create a unified representation of echocardiogram data, capturing both common and rare cardiac conditions. The model employs view-classification and anatomic attention to accurately interpret the relationship between different views and heart structures. EchoPrime has demonstrated superior performance across various benchmarks, offering a more comprehensive and precise tool for cardiac assessment."
                },
                "zh": {
                    "title": "EchoPrimeï¼šå¤šè§†è§’å¿ƒè„è¶…å£°AIçš„é©æ–°",
                    "desc": "EchoPrime æ˜¯ä¸€ä¸ªå¤šè§†è§’ã€è§†å›¾çŸ¥æƒ…çš„è§†é¢‘è¯­è¨€åŸºç¡€æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿå•è§†è§’AIæ¨¡å‹çš„å±€é™æ€§ã€‚å®ƒé€šè¿‡å¯¹æ¯”å­¦ä¹ è®­ç»ƒä¸€ä¸ªç»Ÿä¸€çš„åµŒå…¥æ¨¡å‹ï¼Œèƒ½å¤Ÿå¤„ç†æ ‡å‡†å¿ƒè„è¶…å£°æ£€æŸ¥ä¸­çš„æ‰€æœ‰è§†å›¾ã€‚EchoPrime åˆ©ç”¨è§†å›¾åˆ†ç±»å’Œè§£å‰–æ³¨æ„æ¨¡å‹ï¼Œå‡†ç¡®æ˜ å°„è¶…å£°è§†å›¾ä¸è§£å‰–ç»“æ„ä¹‹é—´çš„å…³ç³»ã€‚ç»è¿‡ä¸¥æ ¼çš„ä¸´åºŠè¯„ä¼°ï¼ŒEchoPrime åœ¨å¿ƒè„å½¢æ€å’ŒåŠŸèƒ½çš„23ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œèƒ½å¤Ÿå¸®åŠ©åŒ»ç”Ÿè¿›è¡Œè‡ªåŠ¨åŒ–çš„åˆæ­¥è¯„ä¼°ã€‚"
                }
            },
            "hash": "5bf065e3ea6b9f7a",
            "pub_date_card": {
                "ru": "13 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 13",
                "zh": "10æœˆ13æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.11795",
            "title": "Efficient Diffusion Models: A Comprehensive Survey from Principles to Practices",
            "url": "https://huggingface.co/papers/2410.11795",
            "abstract": "As one of the most popular and sought-after generative models in the recent years, diffusion models have sparked the interests of many researchers and steadily shown excellent advantage in various generative tasks such as image synthesis, video generation, molecule design, 3D scene rendering and multimodal generation, relying on their dense theoretical principles and reliable application practices. The remarkable success of these recent efforts on diffusion models comes largely from progressive design principles and efficient architecture, training, inference, and deployment methodologies. However, there has not been a comprehensive and in-depth review to summarize these principles and practices to help the rapid understanding and application of diffusion models. In this survey, we provide a new efficiency-oriented perspective on these existing efforts, which mainly focuses on the profound principles and efficient practices in architecture designs, model training, fast inference and reliable deployment, to guide further theoretical research, algorithm migration and model application for new scenarios in a reader-friendly way. https://github.com/ponyzym/Efficient-DMs-Survey",
            "score": 10,
            "issue_id": 124,
            "pub_date": "2024-10-15",
            "pub_date_ru": "15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#architecture",
                    "#cv",
                    "#diffusion",
                    "#inference",
                    "#multimodal",
                    "#survey",
                    "#video"
                ],
                "emoji": "ğŸŒ€",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: Ğ¾Ñ‚ Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ Ğº Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞµ",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ· ÑĞ°Ğ¼Ñ‹Ñ… Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ¸ Ğ²Ğ¾ÑÑ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ñ… Ğ»ĞµÑ‚. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¾Ğ² Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ğº Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ğ¹ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½. ĞÑĞ¾Ğ±Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑƒĞ´ĞµĞ»ÑĞµÑ‚ÑÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¼Ñƒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ñƒ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€Ğ¸Ğ·Ğ²Ğ°Ğ½Ğ° Ğ¿Ğ¾Ğ¼Ğ¾Ñ‡ÑŒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞ°Ğ¼ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾ Ğ¿Ğ¾Ğ½ÑÑ‚ÑŒ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ½Ğ¾Ğ²Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Unlocking the Power of Diffusion Models: Efficiency in Generative Tasks",
                    "desc": "Diffusion models have become a popular choice for generative tasks like image and video creation due to their strong theoretical foundations and practical applications. This paper reviews the design principles and methodologies that make diffusion models efficient and effective. It aims to provide a comprehensive understanding of these models to facilitate their application in new scenarios. The survey focuses on architecture design, model training, fast inference, and reliable deployment to guide future research and development."
                },
                "zh": {
                    "title": "æ‰©æ•£æ¨¡å‹ï¼šé«˜æ•ˆç”Ÿæˆçš„æœªæ¥",
                    "desc": "æ‰©æ•£æ¨¡å‹æ˜¯è¿‘å¹´æ¥éå¸¸å—æ¬¢è¿çš„ç”Ÿæˆæ¨¡å‹ï¼Œå¹¿æ³›åº”ç”¨äºå›¾åƒåˆæˆã€è§†é¢‘ç”Ÿæˆç­‰ä»»åŠ¡ã€‚å®ƒä»¬çš„æˆåŠŸä¸»è¦å½’åŠŸäºæ¸è¿›çš„è®¾è®¡åŸåˆ™å’Œé«˜æ•ˆçš„æ¶æ„ã€è®­ç»ƒã€æ¨ç†åŠéƒ¨ç½²æ–¹æ³•ã€‚æœ¬æ–‡ç»¼è¿°äº†è¿™äº›åŸåˆ™å’Œå®è·µï¼Œæä¾›äº†ä¸€ä¸ªä»¥æ•ˆç‡ä¸ºå¯¼å‘çš„æ–°è§†è§’ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¸®åŠ©è¯»è€…æ›´å¥½åœ°ç†è§£å’Œåº”ç”¨æ‰©æ•£æ¨¡å‹ã€‚"
                }
            },
            "hash": "e1a8ba0c4193aa99",
            "pub_date_card": {
                "ru": "15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 15",
                "zh": "10æœˆ15æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.11805",
            "title": "NesTools: A Dataset for Evaluating Nested Tool Learning Abilities of Large Language Models",
            "url": "https://huggingface.co/papers/2410.11805",
            "abstract": "Large language models (LLMs) combined with tool learning have gained impressive results in real-world applications. During tool learning, LLMs may call multiple tools in nested orders, where the latter tool call may take the former response as its input parameters. However, current research on the nested tool learning capabilities is still under-explored, since the existing benchmarks lack of relevant data instances. To address this problem, we introduce NesTools to bridge the current gap in comprehensive nested tool learning evaluations. NesTools comprises a novel automatic data generation method to construct large-scale nested tool calls with different nesting structures. With manual review and refinement, the dataset is in high quality and closely aligned with real-world scenarios. Therefore, NesTools can serve as a new benchmark to evaluate the nested tool learning abilities of LLMs. We conduct extensive experiments on 22 LLMs, and provide in-depth analyses with NesTools, which shows that current LLMs still suffer from the complex nested tool learning task.",
            "score": 9,
            "issue_id": 127,
            "pub_date": "2024-10-15",
            "pub_date_ru": "15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "ğŸ› ï¸",
                "ru": {
                    "title": "NesTools: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ² LLM",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ NesTools Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². NesTools Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ² Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸ Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¿Ñ€Ğ¾ÑˆĞµĞ» Ñ€ÑƒÑ‡Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºÑƒ Ğ¸ Ğ´Ğ¾Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ĞµĞ³Ğ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° 22 LLM Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ NesTools Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²ÑĞµ ĞµÑ‰Ğµ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼."
                },
                "en": {
                    "title": "NesTools: Elevating LLMs' Nested Tool Learning Evaluation",
                    "desc": "The paper introduces NesTools, a new benchmark designed to evaluate the nested tool learning capabilities of large language models (LLMs). NesTools uses an innovative automatic data generation method to create complex nested tool call scenarios, which are then manually reviewed to ensure high quality and real-world relevance. The study highlights that current LLMs struggle with these complex tasks, as demonstrated through extensive experiments on 22 different models. This work aims to fill the gap in existing research by providing a comprehensive dataset for assessing LLMs' ability to handle nested tool learning."
                },
                "zh": {
                    "title": "NesToolsï¼šæå‡åµŒå¥—å·¥å…·å­¦ä¹ çš„å…¨æ–°åŸºå‡†",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºNesToolsçš„æ–°æ–¹æ³•ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„åµŒå¥—å·¥å…·å­¦ä¹ èƒ½åŠ›ã€‚NesToolsé€šè¿‡è‡ªåŠ¨ç”Ÿæˆå¤§é‡ä¸åŒåµŒå¥—ç»“æ„çš„å·¥å…·è°ƒç”¨æ•°æ®ï¼Œå¡«è¡¥äº†ç°æœ‰åŸºå‡†ä¸­ç¼ºä¹ç›¸å…³æ•°æ®å®ä¾‹çš„ç©ºç™½ã€‚ç»è¿‡äººå·¥å®¡æ ¸å’Œæ”¹è¿›ï¼Œæ•°æ®é›†è´¨é‡é«˜ä¸”ä¸ç°å®åœºæ™¯ç´§å¯†ç»“åˆã€‚å®éªŒè¡¨æ˜ï¼Œå½“å‰çš„LLMsåœ¨å¤„ç†å¤æ‚çš„åµŒå¥—å·¥å…·å­¦ä¹ ä»»åŠ¡æ—¶ä»ç„¶å­˜åœ¨å›°éš¾ã€‚"
                }
            },
            "hash": "9823a7c3dbc76e44",
            "pub_date_card": {
                "ru": "15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 15",
                "zh": "10æœˆ15æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.11419",
            "title": "GS^3: Efficient Relighting with Triple Gaussian Splatting",
            "url": "https://huggingface.co/papers/2410.11419",
            "abstract": "We present a spatial and angular Gaussian based representation and a triple splatting process, for real-time, high-quality novel lighting-and-view synthesis from multi-view point-lit input images. To describe complex appearance, we employ a Lambertian plus a mixture of angular Gaussians as an effective reflectance function for each spatial Gaussian. To generate self-shadow, we splat all spatial Gaussians towards the light source to obtain shadow values, which are further refined by a small multi-layer perceptron. To compensate for other effects like global illumination, another network is trained to compute and add a per-spatial-Gaussian RGB tuple. The effectiveness of our representation is demonstrated on 30 samples with a wide variation in geometry (from solid to fluffy) and appearance (from translucent to anisotropic), as well as using different forms of input data, including rendered images of synthetic/reconstructed objects, photographs captured with a handheld camera and a flash, or from a professional lightstage. We achieve a training time of 40-70 minutes and a rendering speed of 90 fps on a single commodity GPU. Our results compare favorably with state-of-the-art techniques in terms of quality/performance. Our code and data are publicly available at https://GSrelight.github.io/.",
            "score": 8,
            "issue_id": 121,
            "pub_date": "2024-10-15",
            "pub_date_ru": "15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#3d",
                    "#cv"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³ Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€Ğ°ĞºÑƒÑ€ÑĞ° Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ ÑƒĞ³Ğ»Ğ¾Ğ²Ñ‹Ñ… Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ¾Ğ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ‚Ñ€Ğ¾Ğ¹Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¿Ğ»Ğ°Ñ‚Ñ‚Ğ¸Ğ½Ğ³Ğ°. Ğ”Ğ»Ñ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ„Ñ„ĞµĞºÑ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¾Ñ‚Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, ÑĞ¾ÑÑ‚Ğ¾ÑÑ‰Ğ°Ñ Ğ¸Ğ· Ğ»Ğ°Ğ¼Ğ±ĞµÑ€Ñ‚Ğ¾Ğ²ÑĞºĞ¾Ğ¹ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‰ĞµĞ¹ Ğ¸ ÑĞ¼ĞµÑĞ¸ ÑƒĞ³Ğ»Ğ¾Ğ²Ñ‹Ñ… Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ñ… Ğ¿Ñ€Ğ¸ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğµ."
                },
                "en": {
                    "title": "\"Illuminate and Transform: Real-Time View Synthesis with Gaussian Magic\"",
                    "desc": "This paper introduces a novel method for creating high-quality images with new lighting and viewpoints using a spatial and angular Gaussian representation combined with a triple splatting process. The approach models complex appearances by using a Lambertian and angular Gaussian mixture reflectance function for each spatial Gaussian. To handle self-shadowing, spatial Gaussians are projected towards the light source, with shadow values refined by a small neural network, while another network adjusts for global illumination effects. The method is tested on diverse samples and achieves fast training and rendering times, outperforming existing techniques in quality and performance."
                },
                "zh": {
                    "title": "å®æ—¶é«˜è´¨é‡å…‰ç…§åˆæˆçš„æ–°æ–¹æ³•",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åŸºäºç©ºé—´å’Œè§’åº¦é«˜æ–¯çš„è¡¨ç¤ºæ–¹æ³•ï¼Œä»¥åŠä¸‰é‡å–·æº…è¿‡ç¨‹ï¼Œç”¨äºä»å¤šè§†ç‚¹å…‰ç…§è¾“å…¥å›¾åƒä¸­å®æ—¶ç”Ÿæˆé«˜è´¨é‡çš„æ–°å…‰ç…§å’Œè§†å›¾åˆæˆã€‚ä¸ºäº†æè¿°å¤æ‚çš„å¤–è§‚ï¼Œä½œè€…ä½¿ç”¨äº†æœ—ä¼¯åå°„åŠ ä¸Šè§’åº¦é«˜æ–¯æ··åˆä½œä¸ºæ¯ä¸ªç©ºé—´é«˜æ–¯çš„æœ‰æ•ˆåå°„å‡½æ•°ã€‚ä¸ºäº†ç”Ÿæˆè‡ªé˜´å½±ï¼Œæ‰€æœ‰ç©ºé—´é«˜æ–¯è¢«å–·æº…åˆ°å…‰æºæ–¹å‘ä»¥è·å¾—é˜´å½±å€¼ï¼Œå¹¶é€šè¿‡ä¸€ä¸ªå°å‹å¤šå±‚æ„ŸçŸ¥å™¨è¿›ä¸€æ­¥ä¼˜åŒ–ã€‚ä¸ºäº†è¡¥å¿å…¨å±€å…‰ç…§ç­‰å…¶ä»–æ•ˆæœï¼Œå¦ä¸€ä¸ªç½‘ç»œè¢«è®­ç»ƒæ¥è®¡ç®—å¹¶æ·»åŠ æ¯ä¸ªç©ºé—´é«˜æ–¯çš„RGBå€¼ã€‚"
                }
            },
            "hash": "8aa8e4a555a54086",
            "pub_date_card": {
                "ru": "15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 15",
                "zh": "10æœˆ15æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.10934",
            "title": "Agent-as-a-Judge: Evaluate Agents with Agents",
            "url": "https://huggingface.co/papers/2410.10934",
            "abstract": "Contemporary evaluation techniques are inadequate for agentic systems. These approaches either focus exclusively on final outcomes -- ignoring the step-by-step nature of agentic systems, or require excessive manual labour. To address this, we introduce the Agent-as-a-Judge framework, wherein agentic systems are used to evaluate agentic systems. This is an organic extension of the LLM-as-a-Judge framework, incorporating agentic features that enable intermediate feedback for the entire task-solving process. We apply the Agent-as-a-Judge to the task of code generation. To overcome issues with existing benchmarks and provide a proof-of-concept testbed for Agent-as-a-Judge, we present DevAI, a new benchmark of 55 realistic automated AI development tasks. It includes rich manual annotations, like a total of 365 hierarchical user requirements. We benchmark three of the popular agentic systems using Agent-as-a-Judge and find it dramatically outperforms LLM-as-a-Judge and is as reliable as our human evaluation baseline. Altogether, we believe that Agent-as-a-Judge marks a concrete step forward for modern agentic systems -- by providing rich and reliable reward signals necessary for dynamic and scalable self-improvement.",
            "score": 6,
            "issue_id": 127,
            "pub_date": "2024-10-14",
            "pub_date_ru": "14 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#agents",
                    "#benchmark",
                    "#rlhf"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ³ĞµĞ½Ñ‚ ÑÑƒĞ´Ğ¸Ñ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ˜Ğ˜-ÑĞ¸ÑÑ‚ĞµĞ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° - Agent-as-a-Judge. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ‚ÑĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ²ÑĞµĞ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Agent-as-a-Judge Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº DevAI, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 55 Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ˜Ğ˜. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Agent-as-a-Judge Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ LLM-as-a-Judge Ğ¸ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼ Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¾Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°."
                },
                "en": {
                    "title": "\"Agent-as-a-Judge: Revolutionizing Evaluation for Agentic Systems\"",
                    "desc": "The paper introduces the Agent-as-a-Judge framework, which uses agentic systems to evaluate other agentic systems, providing intermediate feedback throughout the task-solving process. This approach is applied to code generation and is tested using a new benchmark called DevAI, which includes 55 realistic AI development tasks with detailed annotations. The framework is shown to outperform existing evaluation methods like LLM-as-a-Judge and matches human evaluation reliability. Overall, Agent-as-a-Judge offers a significant advancement in providing dynamic and scalable feedback for agentic systems."
                },
                "zh": {
                    "title": "ä»£ç†å³è¯„åˆ¤è€…ï¼šç°ä»£ä»£ç†ç³»ç»Ÿè¯„ä¼°çš„æ–°æ–¹å‘",
                    "desc": "ä¼ ç»Ÿçš„è¯„ä¼°æ–¹æ³•å¯¹ä»£ç†ç³»ç»Ÿæ¥è¯´ä¸å¤Ÿå®Œå–„ï¼Œå› ä¸ºå®ƒä»¬è¦ä¹ˆåªå…³æ³¨æœ€ç»ˆç»“æœè€Œå¿½ç•¥äº†ä»£ç†ç³»ç»Ÿçš„é€æ­¥ç‰¹æ€§ï¼Œè¦ä¹ˆéœ€è¦å¤§é‡çš„äººå·¥åŠ³åŠ¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†â€œä»£ç†å³è¯„åˆ¤è€…â€æ¡†æ¶ï¼Œåˆ©ç”¨ä»£ç†ç³»ç»Ÿæ¥è¯„ä¼°ä»£ç†ç³»ç»Ÿã€‚è¿™ç§æ–¹æ³•æ˜¯â€œLLMå³è¯„åˆ¤è€…â€æ¡†æ¶çš„è‡ªç„¶æ‰©å±•ï¼ŒåŠ å…¥äº†ä»£ç†ç‰¹æ€§ï¼Œèƒ½å¤Ÿåœ¨æ•´ä¸ªä»»åŠ¡è§£å†³è¿‡ç¨‹ä¸­æä¾›ä¸­é—´åé¦ˆã€‚æˆ‘ä»¬åº”ç”¨â€œä»£ç†å³è¯„åˆ¤è€…â€äºä»£ç ç”Ÿæˆä»»åŠ¡ï¼Œå¹¶å¼•å…¥äº†æ–°çš„åŸºå‡†DevAIï¼Œè¯æ˜å…¶åœ¨è¯„ä¼°ä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚"
                }
            },
            "hash": "a25e89cb7be0bed4",
            "pub_date_card": {
                "ru": "14 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 14",
                "zh": "10æœˆ14æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.09754",
            "title": "SimBa: Simplicity Bias for Scaling Up Parameters in Deep Reinforcement Learning",
            "url": "https://huggingface.co/papers/2410.09754",
            "abstract": "Recent advances in CV and NLP have been largely driven by scaling up the number of network parameters, despite traditional theories suggesting that larger networks are prone to overfitting. These large networks avoid overfitting by integrating components that induce a simplicity bias, guiding models toward simple and generalizable solutions. However, in deep RL, designing and scaling up networks have been less explored. Motivated by this opportunity, we present SimBa, an architecture designed to scale up parameters in deep RL by injecting a simplicity bias. SimBa consists of three components: (i) an observation normalization layer that standardizes inputs with running statistics, (ii) a residual feedforward block to provide a linear pathway from the input to output, and (iii) a layer normalization to control feature magnitudes. By scaling up parameters with SimBa, the sample efficiency of various deep RL algorithms-including off-policy, on-policy, and unsupervised methods-is consistently improved. Moreover, solely by integrating SimBa architecture into SAC, it matches or surpasses state-of-the-art deep RL methods with high computational efficiency across DMC, MyoSuite, and HumanoidBench. These results demonstrate SimBa's broad applicability and effectiveness across diverse RL algorithms and environments.",
            "score": 6,
            "issue_id": 125,
            "pub_date": "2024-10-13",
            "pub_date_ru": "13 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#architecture",
                    "#rl"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "SimBa: ĞŸÑ€Ğ¾ÑÑ‚Ğ¾Ñ‚Ğ° Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ SimBa Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ² Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL). SimBa Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹, Ñ€ĞµĞ·Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±Ğ»Ğ¾Ğº Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¸ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ»Ğ¾ĞµĞ² Ğ´Ğ»Ñ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğº Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ñ‚Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SimBa ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² RL, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ off-policy, on-policy Ğ¸ unsupervised Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹. Ğ˜Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ SimBa Ğ² Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ SAC Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ñ‚Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ RL Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ… Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ."
                },
                "en": {
                    "title": "\"SimBa: Scaling Simplicity in Deep Reinforcement Learning\"",
                    "desc": "The paper introduces SimBa, a new architecture for deep reinforcement learning (RL) that scales up network parameters while maintaining simplicity to avoid overfitting. SimBa includes an observation normalization layer, a residual feedforward block, and a layer normalization to enhance model performance. By using SimBa, various deep RL algorithms show improved sample efficiency and computational effectiveness. The architecture proves its versatility by matching or surpassing state-of-the-art methods across different environments and tasks."
                },
                "zh": {
                    "title": "SimBaï¼šé€šè¿‡ç®€å•æ€§åå·®æå‡æ·±åº¦å¼ºåŒ–å­¦ä¹ ",
                    "desc": "è¿‘å¹´æ¥ï¼Œè®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸé€šè¿‡å¢åŠ ç½‘ç»œå‚æ•°æ•°é‡å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå°½ç®¡ä¼ ç»Ÿç†è®ºè®¤ä¸ºæ›´å¤§çš„ç½‘ç»œå®¹æ˜“è¿‡æ‹Ÿåˆã€‚è¿™äº›å¤§å‹ç½‘ç»œé€šè¿‡å¼•å…¥ç®€å•æ€§åå·®çš„ç»„ä»¶æ¥é¿å…è¿‡æ‹Ÿåˆï¼ŒæŒ‡å¯¼æ¨¡å‹æœå‘ç®€å•ä¸”å¯æ¨å¹¿çš„è§£å†³æ–¹æ¡ˆã€‚åœ¨æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œç½‘ç»œè®¾è®¡å’Œæ‰©å±•çš„ç ”ç©¶è¾ƒå°‘ã€‚SimBaæ¶æ„é€šè¿‡æ³¨å…¥ç®€å•æ€§åå·®æ¥æ‰©å±•æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸­çš„å‚æ•°ï¼Œæé«˜äº†å¤šç§æ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„æ ·æœ¬æ•ˆç‡ã€‚"
                }
            },
            "hash": "04b27b8dbf829651",
            "pub_date_card": {
                "ru": "13 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 13",
                "zh": "10æœˆ13æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.08001",
            "title": "Towards Synergistic, Generalized, and Efficient Dual-System for Robotic Manipulation",
            "url": "https://huggingface.co/papers/2410.08001",
            "abstract": "The increasing demand for versatile robotic systems to operate in diverse and dynamic environments has emphasized the importance of a generalist policy, which leverages a large cross-embodiment data corpus to facilitate broad adaptability and high-level reasoning. However, the generalist would struggle with inefficient inference and cost-expensive training. The specialist policy, instead, is curated for specific domain data and excels at task-level precision with efficiency. Yet, it lacks the generalization capacity for a wide range of applications. Inspired by these observations, we introduce RoboDual, a synergistic dual-system that supplements the merits of both generalist and specialist policy. A diffusion transformer-based specialist is devised for multi-step action rollouts, exquisitely conditioned on the high-level task understanding and discretized action output of a vision-language-action (VLA) based generalist. Compared to OpenVLA, RoboDual achieves 26.7% improvement in real-world setting and 12% gain on CALVIN by introducing a specialist policy with merely 20M trainable parameters. It maintains strong performance with 5% of demonstration data only, and enables a 3.8 times higher control frequency in real-world deployment. Code would be made publicly available. Our project page is hosted at: https://opendrivelab.com/RoboDual/",
            "score": 3,
            "issue_id": 122,
            "pub_date": "2024-10-10",
            "pub_date_ru": "10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#cv",
                    "#multimodal",
                    "#rl",
                    "#robotics"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "RoboDual: ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²",
                    "desc": "RoboDual - ÑÑ‚Ğ¾ ÑĞ¸Ğ½ĞµÑ€Ğ³ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ². ĞĞ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ½Ğ°Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ vision-language-action Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ¾ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. RoboDual Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ OpenVLA ĞºĞ°Ğº Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ² ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²ÑĞµĞ³Ğ¾ 5% Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ñƒ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ."
                },
                "en": {
                    "title": "RoboDual: Merging Generalist Flexibility with Specialist Precision",
                    "desc": "The paper introduces RoboDual, a dual-system approach that combines the strengths of both generalist and specialist policies for robotic systems. The generalist policy uses a vision-language-action model to provide high-level task understanding, while the specialist policy, based on a diffusion transformer, focuses on efficient multi-step action rollouts. RoboDual significantly improves performance in real-world settings and specific benchmarks by using a specialist policy with a small number of trainable parameters. This system achieves high efficiency and adaptability, even with limited demonstration data, and enhances control frequency in practical applications."
                },
                "zh": {
                    "title": "RoboDualï¼šé€šç”¨ä¸ä¸“ä¸šç­–ç•¥çš„å®Œç¾ç»“åˆ",
                    "desc": "RoboDual æ˜¯ä¸€ä¸ªç»“åˆé€šç”¨ç­–ç•¥å’Œä¸“ä¸šç­–ç•¥çš„åŒç³»ç»Ÿï¼Œæ—¨åœ¨æé«˜æœºå™¨äººåœ¨å¤šå˜ç¯å¢ƒä¸­çš„é€‚åº”æ€§ã€‚é€šç”¨ç­–ç•¥åˆ©ç”¨å¤§è§„æ¨¡è·¨ä½“æ•°æ®ï¼Œæä¾›é«˜å±‚æ¬¡çš„ä»»åŠ¡ç†è§£ï¼Œè€Œä¸“ä¸šç­–ç•¥åˆ™ä¸“æ³¨äºç‰¹å®šä»»åŠ¡çš„é«˜æ•ˆæ‰§è¡Œã€‚RoboDual é€šè¿‡å¼•å…¥åŸºäºæ‰©æ•£å˜å‹å™¨çš„ä¸“ä¸šç­–ç•¥ï¼Œåœ¨çœŸå®ç¯å¢ƒä¸­å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚è¯¥ç³»ç»Ÿåœ¨ä»…ä½¿ç”¨å°‘é‡ç¤ºèŒƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œä»èƒ½ä¿æŒé«˜æ•ˆçš„æ§åˆ¶é¢‘ç‡ã€‚"
                }
            },
            "hash": "f9a57a673dfbc6f6",
            "pub_date_card": {
                "ru": "10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 10",
                "zh": "10æœˆ10æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.09745",
            "title": "Empirical Study of Mutual Reinforcement Effect and Application in Few-shot Text Classification Tasks via Prompt",
            "url": "https://huggingface.co/papers/2410.09745",
            "abstract": "The Mutual Reinforcement Effect (MRE) investigates the synergistic relationship between word-level and text-level classifications in text classification tasks. It posits that the performance of both classification levels can be mutually enhanced. However, this mechanism has not been adequately demonstrated or explained in prior research. To address this gap, we employ empirical experiment to observe and substantiate the MRE theory. Our experiments on 21 MRE mix datasets revealed the presence of MRE in the model and its impact. Specifically, we conducted compare experiments use fine-tune. The results of findings from comparison experiments corroborates the existence of MRE. Furthermore, we extended the application of MRE to prompt learning, utilizing word-level information as a verbalizer to bolster the model's prediction of text-level classification labels. In our final experiment, the F1-score significantly surpassed the baseline in 18 out of 21 MRE Mix datasets, further validating the notion that word-level information enhances the language model's comprehension of the text as a whole.",
            "score": 2,
            "issue_id": 125,
            "pub_date": "2024-10-13",
            "pub_date_ru": "13 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#classification",
                    "#rl"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ’Ğ·Ğ°Ğ¸Ğ¼Ğ½Ğ¾Ğµ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ² Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚ (MRE) Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸ĞµĞ¹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞ»Ğ¾Ğ² Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° 21 Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… MRE Mix Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ MRE. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ MRE. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ MRE Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ°Ğ¼Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞ»Ğ¾Ğ² Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ²ĞµÑ€Ğ±Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ°, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ»Ğ¾ F1-Ğ¼ĞµÑ€Ñƒ Ğ½Ğ° 18 Ğ¸Ğ· 21 Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Boosting Text Understanding: The Power of Word-Level Insights",
                    "desc": "The paper explores the Mutual Reinforcement Effect (MRE), which suggests that word-level and text-level classifications can improve each other's performance in text classification tasks. Through empirical experiments on 21 datasets, the study demonstrates the presence and impact of MRE, showing that word-level information can enhance text-level predictions. The researchers also apply MRE to prompt learning, using word-level data to improve the model's text-level classification accuracy. The experiments show a significant improvement in F1-scores, confirming that word-level insights can enhance overall text comprehension by the model."
                },
                "zh": {
                    "title": "è¯çº§ä¸æ–‡æœ¬çº§åˆ†ç±»çš„ç›¸äº’å¢å¼ºæ•ˆåº”",
                    "desc": "è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†è¯çº§å’Œæ–‡æœ¬çº§åˆ†ç±»åœ¨æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­çš„ç›¸äº’å¢å¼ºæ•ˆåº”ã€‚é€šè¿‡å®éªŒè¯æ˜äº†è¿™ç§æ•ˆåº”çš„å­˜åœ¨ï¼Œå¹¶å±•ç¤ºäº†å…¶å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚ç ”ç©¶è¿˜å°†è¿™ç§æ•ˆåº”åº”ç”¨äºæç¤ºå­¦ä¹ ï¼Œåˆ©ç”¨è¯çº§ä¿¡æ¯æ¥å¢å¼ºæ–‡æœ¬çº§åˆ†ç±»æ ‡ç­¾çš„é¢„æµ‹ã€‚æœ€ç»ˆå®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨21ä¸ªæ•°æ®é›†ä¸­çš„18ä¸ªï¼ŒF1åˆ†æ•°æ˜¾è‘—è¶…è¿‡åŸºçº¿ï¼ŒéªŒè¯äº†è¯çº§ä¿¡æ¯å¯¹è¯­è¨€æ¨¡å‹ç†è§£æ–‡æœ¬æ•´ä½“çš„æå‡ä½œç”¨ã€‚"
                }
            },
            "hash": "9e1731e2bbd0b081",
            "pub_date_card": {
                "ru": "13 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 13",
                "zh": "10æœˆ13æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.06593",
            "title": "Towards Natural Image Matting in the Wild via Real-Scenario Prior",
            "url": "https://huggingface.co/papers/2410.06593",
            "abstract": "Recent approaches attempt to adapt powerful interactive segmentation models, such as SAM, to interactive matting and fine-tune the models based on synthetic matting datasets. However, models trained on synthetic data fail to generalize to complex and occlusion scenes. We address this challenge by proposing a new matting dataset based on the COCO dataset, namely COCO-Matting. Specifically, the construction of our COCO-Matting includes accessory fusion and mask-to-matte, which selects real-world complex images from COCO and converts semantic segmentation masks to matting labels. The built COCO-Matting comprises an extensive collection of 38,251 human instance-level alpha mattes in complex natural scenarios. Furthermore, existing SAM-based matting methods extract intermediate features and masks from a frozen SAM and only train a lightweight matting decoder by end-to-end matting losses, which do not fully exploit the potential of the pre-trained SAM. Thus, we propose SEMat which revamps the network architecture and training objectives. For network architecture, the proposed feature-aligned transformer learns to extract fine-grained edge and transparency features. The proposed matte-aligned decoder aims to segment matting-specific objects and convert coarse masks into high-precision mattes. For training objectives, the proposed regularization and trimap loss aim to retain the prior from the pre-trained model and push the matting logits extracted from the mask decoder to contain trimap-based semantic information. Extensive experiments across seven diverse datasets demonstrate the superior performance of our method, proving its efficacy in interactive natural image matting. We open-source our code, models, and dataset at https://github.com/XiaRho/SEMat.",
            "score": 1,
            "issue_id": 126,
            "pub_date": "2024-10-09",
            "pub_date_ru": "9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#architecture",
                    "#cv",
                    "#dataset"
                ],
                "emoji": "âœ‚ï¸",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¼Ğ°Ñ‚Ñ‚Ğ¸Ğ½Ğ³Ğµ: Ğ¾Ñ‚ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸ĞºĞ¸ Ğº Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ COCO-Matting Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°Ñ‚Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ… Ğ¸Ğ· COCO. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ SEMat, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ SAM, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ feature-aligned transformer Ğ¸ matte-aligned decoder. Ğ’Ğ²ĞµĞ´ĞµĞ½Ñ‹ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ trimap loss. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ÑĞµĞ¼Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°Ñ‚Ñ‚Ğ¸Ğ½Ğ³Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Revolutionizing Image Matting with Real-World Data and Advanced Architectures",
                    "desc": "The paper introduces a new dataset called COCO-Matting, which enhances the generalization of matting models to complex real-world scenes by using real images from the COCO dataset. It addresses the limitations of models trained on synthetic data by converting semantic segmentation masks into matting labels. The authors propose a novel model, SEMat, which includes a feature-aligned transformer and a matte-aligned decoder to improve edge and transparency feature extraction. Their approach, validated through extensive experiments, shows superior performance in interactive image matting, leveraging both new training objectives and architecture improvements."
                },
                "zh": {
                    "title": "SEMatï¼šæå‡äº¤äº’å¼æŠ å›¾çš„ç²¾åº¦ä¸æ³›åŒ–èƒ½åŠ›",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æŠ å›¾æ•°æ®é›†COCO-Mattingï¼ŒåŸºäºCOCOæ•°æ®é›†æ„å»ºï¼Œæ—¨åœ¨è§£å†³åˆæˆæ•°æ®è®­ç»ƒæ¨¡å‹åœ¨å¤æ‚åœºæ™¯ä¸­æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚COCO-Mattingé€šè¿‡ä»COCOä¸­é€‰æ‹©çœŸå®ä¸–ç•Œçš„å¤æ‚å›¾åƒï¼Œå¹¶å°†è¯­ä¹‰åˆ†å‰²æ©ç è½¬æ¢ä¸ºæŠ å›¾æ ‡ç­¾ï¼ŒåŒ…å«äº†38,251ä¸ªäººç‰©å®ä¾‹çº§åˆ«çš„alpha mattesã€‚ä¸ºäº†å……åˆ†åˆ©ç”¨é¢„è®­ç»ƒçš„SAMæ¨¡å‹ï¼Œä½œè€…æå‡ºäº†SEMatï¼Œé€šè¿‡ç‰¹å¾å¯¹é½çš„transformerå’ŒæŠ å›¾å¯¹é½çš„è§£ç å™¨æ¥æå–ç»†ç²’åº¦çš„è¾¹ç¼˜å’Œé€æ˜åº¦ç‰¹å¾ï¼Œå¹¶å°†ç²—ç•¥çš„æ©ç è½¬æ¢ä¸ºé«˜ç²¾åº¦çš„æŠ å›¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSEMatåœ¨ä¸ƒä¸ªä¸åŒçš„æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¯æ˜äº†å…¶åœ¨äº¤äº’å¼è‡ªç„¶å›¾åƒæŠ å›¾ä¸­çš„æœ‰æ•ˆæ€§ã€‚"
                }
            },
            "hash": "656804d454a782ca",
            "pub_date_card": {
                "ru": "9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 9",
                "zh": "10æœˆ9æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.11619",
            "title": "MultiVENT 2.0: A Massive Multilingual Benchmark for Event-Centric Video Retrieval",
            "url": "https://huggingface.co/papers/2410.11619",
            "abstract": "Efficiently retrieving and synthesizing information from large-scale multimodal collections has become a critical challenge. However, existing video retrieval datasets suffer from scope limitations, primarily focusing on matching descriptive but vague queries with small collections of professionally edited, English-centric videos. To address this gap, we introduce MultiVENT 2.0, a large-scale, multilingual event-centric video retrieval benchmark featuring a collection of more than 218,000 news videos and 3,906 queries targeting specific world events. These queries specifically target information found in the visual content, audio, embedded text, and text metadata of the videos, requiring systems leverage all these sources to succeed at the task. Preliminary results show that state-of-the-art vision-language models struggle significantly with this task, and while alternative approaches show promise, they are still insufficient to adequately address this problem. These findings underscore the need for more robust multimodal retrieval systems, as effective video retrieval is a crucial step towards multimodal content understanding and generation tasks.",
            "score": 0,
            "issue_id": 130,
            "pub_date": "2024-10-15",
            "pub_date_ru": "15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#multilingual",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "MultiVENT 2.0: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ² Ğ¼Ğ¸Ñ€Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "MultiVENT 2.0 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 218 000 Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ 3 906 Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ğ½Ğ°Ñ†ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ° ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ñ‹Ğµ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¾Ñ‚ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°, Ğ°ÑƒĞ´Ğ¸Ğ¾, Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. ĞŸÑ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑÑ‚ÑƒĞ¿Ğ°ÑÑ‚ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ, Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°Ñ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞ°."
                },
                "en": {
                    "title": "Unlocking the Power of Multimodal Video Retrieval",
                    "desc": "The paper introduces MultiVENT 2.0, a new benchmark for video retrieval that focuses on multilingual and event-centric content, addressing the limitations of existing datasets. It includes over 218,000 news videos and 3,906 queries that require systems to utilize visual, audio, and text data effectively. Initial tests reveal that current vision-language models struggle with this complex task, highlighting the need for more advanced multimodal retrieval systems. This research emphasizes the importance of improving video retrieval to enhance multimodal content understanding and generation."
                },
                "zh": {
                    "title": "çªç ´å¤šæ¨¡æ€è§†é¢‘æ£€ç´¢çš„ç“¶é¢ˆ",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºMultiVENT 2.0çš„å¤§è§„æ¨¡å¤šè¯­è¨€äº‹ä»¶ä¸­å¿ƒè§†é¢‘æ£€ç´¢åŸºå‡†ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è§†é¢‘æ£€ç´¢æ•°æ®é›†èŒƒå›´æœ‰é™çš„é—®é¢˜ã€‚è¯¥åŸºå‡†åŒ…å«è¶…è¿‡218,000ä¸ªæ–°é—»è§†é¢‘å’Œ3,906ä¸ªé’ˆå¯¹ç‰¹å®šä¸–ç•Œäº‹ä»¶çš„æŸ¥è¯¢ï¼Œè¦æ±‚ç³»ç»Ÿåˆ©ç”¨è§†é¢‘çš„è§†è§‰å†…å®¹ã€éŸ³é¢‘ã€åµŒå…¥æ–‡æœ¬å’Œæ–‡æœ¬å…ƒæ•°æ®æ¥å®Œæˆä»»åŠ¡ã€‚åˆæ­¥ç»“æœæ˜¾ç¤ºï¼Œæœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è¿™ä¸ªä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ï¼Œè€Œå…¶ä»–æ–¹æ³•è™½ç„¶æœ‰æ½œåŠ›ï¼Œä½†ä»ä¸è¶³ä»¥è§£å†³é—®é¢˜ã€‚è¿™è¡¨æ˜éœ€è¦æ›´å¼ºå¤§çš„å¤šæ¨¡æ€æ£€ç´¢ç³»ç»Ÿï¼Œä»¥å®ç°å¯¹å¤šæ¨¡æ€å†…å®¹çš„ç†è§£å’Œç”Ÿæˆã€‚"
                }
            },
            "hash": "dd603cadf5349b0c",
            "pub_date_card": {
                "ru": "15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 15",
                "zh": "10æœˆ15æ—¥"
            }
        }
    ],
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å¸¸è§çš„å¹»è§‰ç°è±¡åŠå…¶åŸå› ã€‚ç ”ç©¶å‘ç°ï¼Œè™½ç„¶MLLMsåœ¨æœ€ç»ˆè¾“å‡ºä¸­é”™è¯¯ç”Ÿæˆå¯¹è±¡ï¼Œä½†å®ƒä»¬åœ¨å‰é¢çš„å±‚ä¸­å®é™…ä¸Šèƒ½å¤Ÿè¯†åˆ«è§†è§‰å¯¹è±¡ã€‚ä½œè€…æ¨æµ‹ï¼Œè¿™å¯èƒ½æ˜¯ç”±äºè¯­è¨€æ¨¡å‹çš„å¼ºçŸ¥è¯†å…ˆéªŒæŠ‘åˆ¶äº†è§†è§‰ä¿¡æ¯ï¼Œå¯¼è‡´å¹»è§‰ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§æ–°çš„åŠ¨æ€çº æ­£è§£ç æ–¹æ³•ï¼ˆDeCoï¼‰ï¼Œèƒ½å¤Ÿé€‚åº”æ€§åœ°é€‰æ‹©åˆé€‚çš„å‰é¢å±‚å¹¶å°†çŸ¥è¯†æ•´åˆåˆ°æœ€ç»ˆå±‚ä»¥è°ƒæ•´è¾“å‡ºã€‚DeCoå¯ä»¥ä¸å„ç§ç»å…¸è§£ç ç­–ç•¥æ— ç¼ç»“åˆï¼Œå¹¶åº”ç”¨äºä¸åŒçš„MLLMsã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDeCoèƒ½å¤Ÿæ˜¾è‘—é™ä½å¹»è§‰ç‡ã€‚",
        "title": "MLLM can see? Dynamic Correction Decoding for Hallucination Mitigation",
        "pinyin": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å¸¸è§çš„å¹»è§‰ç°è±¡åŠå…¶åŸå› ã€‚ç ”ç©¶å‘ç°ï¼Œè™½ç„¶MLLMsåœ¨æœ€ç»ˆè¾“å‡ºä¸­é”™è¯¯ç”Ÿæˆå¯¹è±¡ï¼Œä½†å®ƒä»¬åœ¨å‰é¢çš„å±‚ä¸­å®é™…ä¸Šèƒ½å¤Ÿè¯†åˆ«è§†è§‰å¯¹è±¡ã€‚ä½œè€…æ¨æµ‹ï¼Œè¿™å¯èƒ½æ˜¯ç”±äºè¯­è¨€æ¨¡å‹çš„å¼ºçŸ¥è¯†å…ˆéªŒæŠ‘åˆ¶äº†è§†è§‰ä¿¡æ¯ï¼Œå¯¼è‡´å¹»è§‰ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§æ–°çš„åŠ¨æ€çº æ­£è§£ç æ–¹æ³•ï¼ˆDeCoï¼‰ï¼Œèƒ½å¤Ÿé€‚åº”æ€§åœ°é€‰æ‹©åˆé€‚çš„å‰é¢å±‚å¹¶å°†çŸ¥è¯†æ•´åˆåˆ°æœ€ç»ˆå±‚ä»¥è°ƒæ•´è¾“å‡ºã€‚DeCoå¯ä»¥ä¸å„ç§ç»å…¸è§£ç ç­–ç•¥æ— ç¼ç»“åˆï¼Œå¹¶åº”ç”¨äºä¸åŒçš„MLLMsã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDeCoèƒ½å¤Ÿæ˜¾è‘—é™ä½å¹»è§‰ç‡ã€‚\n\nzhÃ¨ piÄn wÃ©n zhÄng tÇo lÃ¹n le duÅ mÃ³ tÃ i dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng (MLLMs) chÃ¡ng jiÃ n de huÃ n juÃ© xiÃ n xiÃ ng jÃ­ qÃ­ yuÃ¡n yÄ«n. yÃ¡n jiÅ« fÄ xiÃ n, suÄ« rÃ¡n MLLMs zÃ i zuÃ¬ zhÃ²ng chÅ« lÃ¡i zhÅng cuÃ² wÃ¹ shÄ“ng chÃ©ng duÃ¬ xiÃ ng, dÃ n tÄ men zÃ i qiÃ¡n miÃ n de cÃ©ng zhÅng shÃ­ jÃ¬ shÃ­ nÃ©n shÃ­ biÃ© shÃ¬ wÃ¹. zuÃ² zhÄ› tuÄ« cÃ¨, zhÃ¨ kÄ› nÃ©ng shÃ¬ yÃ³u yÇ yÇ” yÃ¡n mÃ³ xÃ­ng de qiÃ¡ng zhÄ« shi xiÄn yÇn yÃ¬ zhÃ¬ le shÃ¬ jÃ¹e xÃ¬n xÄ«, dÇo zhÃ¬ huÃ n juÃ©. wÃ¨i cÇ, zuÃ² zhÄ› tÃ­ chÅ« le yÄ« zhÇ’ng xÄ«n de dÃ²ng tÃ i jiÇo zhÃ¨ng jiÄ› mÇ fÄng fÇ (DeCo), nÃ©ng gÃ²u shÃ¬ yÃ¬ng de xuÇn zÃ© hÃ© shÃ¬ de qiÃ¡n miÃ n cÃ©ng bÃ¬ng jiÄng zhÄ« shi zhÄ›ng hÃ© dÃ o zuÃ¬ zhÃ²ng cÃ©ng yÇ tiÃ¡o zhÄ›ng chÅ« lÃ¡i. DeCo kÄ› yÇ yÇ” gÃ¨ zhÇ’ng jÄ«ng diÇn jiÄ› mÇ cÃ¨ lÃ¼Ã¨ jiÃ© hÃ©, bÃ¬ng yÃ¬ng yÃ²ng yÃº bÃ¹ tÃ³ng de MLLMs. shÃ­ yÃ n jiÃ© guÇ’ xiÇn shÃ¬, DeCo nÃ©ng gÃ²u xiÇn zhÃ¹ jiÃ ng dÄ« huÃ n juÃ© lÇœ.",
        "vocab": "[\n    {\"word\": \"å¤šæ¨¡æ€\", \"pinyin\": \"duÅ mÃ³ shuÃ i\", \"trans\": \"multimodal\"},\n    {\"word\": \"å¤§è¯­è¨€æ¨¡å‹\", \"pinyin\": \"dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng\", \"trans\": \"large language model\"},\n    {\"word\": \"å¹»è§‰\", \"pinyin\": \"huÃ n juÃ©\", \"trans\": \"hallucination\"},\n    {\"word\": \"ç°è±¡\", \"pinyin\": \"xiÃ n xiÃ ng\", \"trans\": \"phenomenon\"},\n    {\"word\": \"åŸå› \", \"pinyin\": \"yuÃ¡n yÄ«n\", \"trans\": \"reason\"},\n    {\"word\": \"é”™è¯¯\", \"pinyin\": \"cuÃ² wÃ¹\", \"trans\": \"error\"},\n    {\"word\": \"ç”Ÿæˆ\", \"pinyin\": \"shÄ“ng chÃ©ng\", \"trans\": \"generate\"},\n    {\"word\": \"å¯¹è±¡\", \"pinyin\": \"duÃ¬ xiÃ ng\", \"trans\": \"object\"},\n    {\"word\": \"è¯†åˆ«\", \"pinyin\": \"shÃ­ biÃ©\", \"trans\": \"recognize\"},\n    {\"word\": \"è§†è§‰\", \"pinyin\": \"shÃ¬ juÃ©\", \"trans\": \"visual\"},\n    {\"word\": \"å¼º\", \"pinyin\": \"qiÃ¡ng\", \"trans\": \"strong\"},\n    {\"word\": \"çŸ¥è¯†\", \"pinyin\": \"zhÄ« shi\", \"trans\": \"knowledge\"},\n    {\"word\": \"å…ˆéªŒ\", \"pinyin\": \"xiÄn yÃ n\", \"trans\": \"prior\"},\n    {\"word\": \"æŠ‘åˆ¶\", \"pinyin\": \"yÃ¬ zhÃ¬\", \"trans\": \"suppress\"},\n    {\"word\": \"ä¿¡æ¯\", \"pinyin\": \"xÃ¬n xÄ«\", \"trans\": \"information\"},\n    {\"word\": \"åŠ¨æ€\", \"pinyin\": \"dÃ²ng tÃ i\", \"trans\": \"dynamic\"},\n    {\"word\": \"çº æ­£\", \"pinyin\": \"jiÅ« zhÃ¨ng\", \"trans\": \"correct\"},\n    {\"word\": \"è§£ç \", \"pinyin\": \"jiÄ› mÇ\", \"trans\": \"decode\"},\n    {\"word\": \"æ–¹æ³•\", \"pinyin\": \"fÄng fÇ\", \"trans\": \"method\"},\n    {\"word\": \"é€‚åº”æ€§\", \"pinyin\": \"shÃ¬ yÃ¬ng xÃ¬ng\", \"trans\": \"adaptive\"},\n    {\"word\": \"é€‰æ‹©\", \"pinyin\": \"xuÇn zÃ©\", \"trans\": \"select\"},\n    {\"word\": \"åˆé€‚\", \"pinyin\": \"hÃ© shÃ¬\", \"trans\": \"suitable\"},\n    {\"word\": \"æ•´åˆ\", \"pinyin\": \"zhÄ›ng hÃ©\", \"trans\": \"integrate\"},\n    {\"word\": \"è°ƒæ•´\", \"pinyin\": \"tiÃ¡o zhÄ›ng\", \"trans\": \"adjust\"},\n    {\"word\": \"ç»å…¸\", \"pinyin\": \"jÄ«ng diÇn\", \"trans\": \"classical\"},\n    {\"word\": \"ç­–ç•¥\", \"pinyin\": \"cÃ¨ lÃ¼Ã¨\", \"trans\": \"strategy\"},\n    {\"word\": \"æ— ç¼\", \"pinyin\": \"wÃº fÄ“ng\", \"trans\": \"seamless\"},\n    {\"word\": \"ç»“åˆ\", \"pinyin\": \"jiÃ© hÃ©\", \"trans\": \"combine\"},\n    {\"word\": \"åº”ç”¨\", \"pinyin\": \"yÃ¬ng yÃ²ng\", \"trans\": \"apply\"},\n    {\"word\": \"ä¸åŒ\", \"pinyin\": \"bÃ¹ tÃ³ng\", \"trans\": \"different\"},\n    {\"word\": \"å®éªŒ\", \"pinyin\": \"shÃ­ yÃ n\", \"trans\": \"experiment\"},\n    {\"word\": \"ç»“æœ\", \"pinyin\": \"jiÃ© guÇ’\", \"trans\": \"result\"},\n    {\"word\": \"æ˜¾è‘—\", \"pinyin\": \"xiÇn zhÃ¹\", \"trans\": \"significant\"},\n    {\"word\": \"é™ä½\", \"pinyin\": \"jiÃ ng dÄ«\", \"trans\": \"reduce\"},\n    {\"word\": \"ç‡\", \"pinyin\": \"lÇœ\", \"trans\": \"rate\"}\n]",
        "trans": "This article discusses the common phenomenon of hallucinations in multimodal large language models (MLLMs) and their causes. Research has found that while MLLMs incorrectly generate objects in their final output, they are actually able to recognize visual objects in their earlier layers. The authors speculate that this may be due to the strong knowledge priors of the language model suppressing visual information, leading to hallucinations. To address this, the authors propose a new dynamic correction decoding method (DeCo), which can adaptively select appropriate earlier layers and integrate knowledge into the final layer to adjust the output. DeCo can be seamlessly combined with various classical decoding strategies and applied to different MLLMs. Experimental results show that DeCo can significantly reduce the hallucination rate.",
        "update_ts": "2024-10-16 04:59"
    },
    "weekday": 1,
    "link_prev": "2024-10-14.html",
    "link_next": "2024-10-16.html",
    "date_en": "15 October",
    "date_prev": "14 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
    "date_next": "16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
    "short_date_prev": {
        "ru": "14.10",
        "en": "10/14",
        "zh": "10æœˆ14æ—¥"
    },
    "short_date_next": {
        "ru": "16.10",
        "en": "10/16",
        "zh": "10æœˆ16æ—¥"
    },
    "categories": {
        "#dataset": 9,
        "#data": 0,
        "#benchmark": 9,
        "#agents": 1,
        "#cv": 6,
        "#rl": 3,
        "#rlhf": 2,
        "#rag": 2,
        "#plp": 0,
        "#inference": 2,
        "#3d": 1,
        "#audio": 0,
        "#video": 3,
        "#multimodal": 6,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 5,
        "#medicine": 2,
        "#training": 5,
        "#robotics": 1,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#quantum": 0,
        "#edge_computing": 0,
        "#optimization": 0,
        "#survey": 1,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#classification": 0
    }
}