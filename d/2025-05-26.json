{
    "date": {
        "ru": "26 Ğ¼Ğ°Ñ",
        "en": "May 26",
        "zh": "5æœˆ26æ—¥"
    },
    "time_utc": "2025-05-26 04:18",
    "weekday": 0,
    "issue_id": 3947,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2505.17612",
            "title": "Distilling LLM Agent into Small Models with Retrieval and Code Tools",
            "url": "https://huggingface.co/papers/2505.17612",
            "abstract": "Agent Distillation transfers reasoning and task-solving capabilities from large language models to smaller models using enhanced prompts and self-consistent actions, matching performance of larger models on various reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) excel at complex reasoning tasks but remain computationally expensive, limiting their practical deployment. To address this, recent works have focused on distilling reasoning capabilities into smaller language models (sLMs) using chain-of-thought (CoT) traces from teacher LLMs. However, this approach struggles in scenarios requiring rare factual knowledge or precise computation, where sLMs often hallucinate due to limited capability. In this work, we propose Agent Distillation, a framework for transferring not only reasoning capability but full task-solving behavior from LLM-based agents into sLMs with retrieval and code tools. We improve agent distillation along two complementary axes: (1) we introduce a prompting method called first-thought prefix to enhance the quality of teacher-generated trajectories; and (2) we propose a self-consistent action generation for improving test-time robustness of small agents. We evaluate our method on eight reasoning tasks across factual and mathematical domains, covering both in-domain and out-of-domain generalization. Our results show that sLMs as small as 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier larger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the potential of agent distillation for building practical, tool-using small agents. Our code is available at https://github.com/Nardien/agent-distillation.",
            "score": 21,
            "issue_id": 3945,
            "pub_date": "2025-05-23",
            "pub_date_card": {
                "ru": "23 Ğ¼Ğ°Ñ",
                "en": "May 23",
                "zh": "5æœˆ23æ—¥"
            },
            "hash": "258cfb9a5b51fa42",
            "authors": [
                "Minki Kang",
                "Jongwon Jeong",
                "Seanie Lee",
                "Jaewoong Cho",
                "Sung Ju Hwang"
            ],
            "affiliations": [
                "DeepAuto.ai",
                "KAIST",
                "KRAFTON"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17612.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#small_models",
                    "#agents",
                    "#transfer_learning",
                    "#training",
                    "#hallucinations",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ° Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ°Ğ³ĞµĞ½Ñ‚Ğ°: Ğ¾Ñ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¼Ğ°Ğ»Ñ‹Ğ¼",
                    "desc": "ĞœĞµÑ‚Ğ¾Ğ´ Agent Distillation Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¾Ñ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ Ğ¸ ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. Agent Distillation Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ°Ğ»ĞµĞ½ÑŒĞºĞ¸Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ¹ Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ±Ñ‹Ğ» Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ²Ğ¾ÑÑŒĞ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ² Ñ„Ğ°ĞºÑ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…."
                },
                "en": {
                    "title": "Empowering Small Models with Big Model Intelligence",
                    "desc": "Agent Distillation is a method that helps smaller language models (sLMs) learn reasoning and task-solving skills from larger language models (LLMs). It uses improved prompts and self-consistent actions to enhance the performance of sLMs on reasoning tasks, making them competitive with larger models. The approach addresses challenges like hallucination in sLMs when faced with rare facts or complex computations. By evaluating on various reasoning tasks, the study shows that even small models can perform well, paving the way for more efficient AI applications."
                },
                "zh": {
                    "title": "ä»£ç†è’¸é¦ï¼šå°å‹æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æå‡",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºä»£ç†è’¸é¦çš„æ¡†æ¶ï¼Œæ—¨åœ¨å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†å’Œä»»åŠ¡è§£å†³èƒ½åŠ›è½¬ç§»åˆ°è¾ƒå°çš„è¯­è¨€æ¨¡å‹ï¼ˆsLMï¼‰ä¸­ã€‚é€šè¿‡ä½¿ç”¨å¢å¼ºçš„æç¤ºå’Œè‡ªä¸€è‡´æ€§åŠ¨ä½œï¼Œä»£ç†è’¸é¦èƒ½å¤Ÿåœ¨å¤šä¸ªæ¨ç†ä»»åŠ¡ä¸Šå®ç°ä¸å¤§å‹æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬å¼•å…¥ä¸€ç§æ–°çš„æç¤ºæ–¹æ³•å’Œæ”¹è¿›å°å‹ä»£ç†åœ¨æµ‹è¯•æ—¶çš„é²æ£’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå‚æ•°é‡ä¸º0.5Båˆ°3Bçš„å°å‹æ¨¡å‹å¯ä»¥åœ¨äº‹å®å’Œæ•°å­¦é¢†åŸŸçš„æ¨ç†ä»»åŠ¡ä¸­ä¸æ›´å¤§çš„æ¨¡å‹ç«äº‰ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.17941",
            "title": "VeriThinker: Learning to Verify Makes Reasoning Model Efficient",
            "url": "https://huggingface.co/papers/2505.17941",
            "abstract": "VeriThinker reduces the length of complex reasoning chains in Large Reasoning Models (LRMs) by fine-tuning them on a verification task, thereby decreasing inference costs without significantly sacrificing accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Reasoning Models (LRMs) excel at complex tasks using Chain-of-Thought (CoT) reasoning. However, their tendency to overthinking leads to unnecessarily lengthy reasoning chains, dramatically increasing inference costs. To mitigate this issue, we introduce VeriThinker, a novel approach for CoT compression. Unlike conventional methods that fine-tune LRMs directly on the original reasoning task using synthetic concise CoT data, we innovatively fine-tune the model solely through an auxiliary verification task. By training LRMs to accurately verify the correctness of CoT solutions, the LRMs inherently become more discerning about the necessity of subsequent self-reflection steps, thereby effectively suppressing overthinking. Extensive experiments validate that VeriThinker substantially reduces reasoning chain lengths while maintaining or even slightly improving accuracy. When applied to DeepSeek-R1-Distill-Qwen-7B, our approach reduces reasoning tokens on MATH500 from 3790 to 2125 while improving accuracy by 0.8% (94.0% to 94.8%), and on AIME25, tokens decrease from 14321 to 10287 with a 2.1% accuracy gain (38.7% to 40.8%). Additionally, our experiments demonstrate that VeriThinker can also be zero-shot generalized to speculative reasoning. Code is available at https://github.com/czg1225/VeriThinker",
            "score": 12,
            "issue_id": 3945,
            "pub_date": "2025-05-23",
            "pub_date_card": {
                "ru": "23 Ğ¼Ğ°Ñ",
                "en": "May 23",
                "zh": "5æœˆ23æ—¥"
            },
            "hash": "cfc0e5dae345ea81",
            "authors": [
                "Zigeng Chen",
                "Xinyin Ma",
                "Gongfan Fang",
                "Ruonan Yu",
                "Xinchao Wang"
            ],
            "affiliations": [
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17941.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#optimization",
                    "#inference",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "VeriThinker - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (LRM). ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LRM Ğ½Ğ° Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² ÑĞ°Ğ¼Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ VeriThinker Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ´Ğ»Ğ¸Ğ½Ñƒ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¸Ğ»Ğ¸ Ğ´Ğ°Ğ¶Ğµ Ğ½ĞµĞ¼Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑĞ¿ĞµĞºÑƒĞ»ÑÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Streamlining Reasoning with VeriThinker",
                    "desc": "VeriThinker is a method designed to enhance Large Reasoning Models (LRMs) by reducing the length of their reasoning chains. It achieves this by fine-tuning the models on a verification task instead of directly on the original reasoning tasks. This approach helps the models become more efficient by minimizing unnecessary steps in their reasoning process, which lowers inference costs. Experimental results show that VeriThinker not only shortens reasoning chains but also improves accuracy in various tasks, demonstrating its effectiveness in optimizing LRM performance."
                },
                "zh": {
                    "title": "VeriThinkerï¼šä¼˜åŒ–æ¨ç†é“¾ï¼Œæå‡æ•ˆç‡ä¸å‡†ç¡®æ€§",
                    "desc": "VeriThinkeræ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡åœ¨éªŒè¯ä»»åŠ¡ä¸Šå¾®è°ƒå¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰ï¼Œå‡å°‘å¤æ‚æ¨ç†é“¾çš„é•¿åº¦ï¼Œä»è€Œé™ä½æ¨ç†æˆæœ¬è€Œä¸æ˜¾è‘—ç‰ºç‰²å‡†ç¡®æ€§ã€‚ä¼ ç»Ÿæ–¹æ³•ç›´æ¥åœ¨åŸå§‹æ¨ç†ä»»åŠ¡ä¸Šå¾®è°ƒæ¨¡å‹ï¼Œè€ŒVeriThinkeråˆ™åˆ›æ–°æ€§åœ°ä»…é€šè¿‡è¾…åŠ©éªŒè¯ä»»åŠ¡è¿›è¡Œå¾®è°ƒã€‚é€šè¿‡è®­ç»ƒLRMså‡†ç¡®éªŒè¯æ¨ç†è§£å†³æ–¹æ¡ˆçš„æ­£ç¡®æ€§ï¼Œæ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°åˆ¤æ–­åç»­è‡ªæˆ‘åæ€æ­¥éª¤çš„å¿…è¦æ€§ï¼Œæœ‰æ•ˆæŠ‘åˆ¶è¿‡åº¦æ€è€ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVeriThinkeræ˜¾è‘—å‡å°‘äº†æ¨ç†é“¾çš„é•¿åº¦ï¼ŒåŒæ—¶ä¿æŒæˆ–ç•¥å¾®æé«˜äº†å‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16211",
            "title": "AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large\n  Language Models",
            "url": "https://huggingface.co/papers/2505.16211",
            "abstract": "AudioTrust evaluates the trustworthiness of Audio Large Language Models across multifaceted dimensions, using a comprehensive dataset and specific metrics to assess their performance in real-world audio scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid advancement and expanding applications of Audio Large Language Models (ALLMs) demand a rigorous understanding of their trustworthiness. However, systematic research on evaluating these models, particularly concerning risks unique to the audio modality, remains largely unexplored. Existing evaluation frameworks primarily focus on the text modality or address only a restricted set of safety dimensions, failing to adequately account for the unique characteristics and application scenarios inherent to the audio modality. We introduce AudioTrust-the first multifaceted trustworthiness evaluation framework and benchmark specifically designed for ALLMs. AudioTrust facilitates assessments across six key dimensions: fairness, hallucination, safety, privacy, robustness, and authentication. To comprehensively evaluate these dimensions, AudioTrust is structured around 18 distinct experimental setups. Its core is a meticulously constructed dataset of over 4,420 audio/text samples, drawn from real-world scenarios (e.g., daily conversations, emergency calls, voice assistant interactions), specifically designed to probe the multifaceted trustworthiness of ALLMs. For assessment, the benchmark carefully designs 9 audio-specific evaluation metrics, and we employ a large-scale automated pipeline for objective and scalable scoring of model outputs. Experimental results reveal the trustworthiness boundaries and limitations of current state-of-the-art open-source and closed-source ALLMs when confronted with various high-risk audio scenarios, offering valuable insights for the secure and trustworthy deployment of future audio models. Our platform and benchmark are available at https://github.com/JusperLee/AudioTrust.",
            "score": 10,
            "issue_id": 3946,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 Ğ¼Ğ°Ñ",
                "en": "May 22",
                "zh": "5æœˆ22æ—¥"
            },
            "hash": "1849951d3588375e",
            "authors": [
                "Kai Li",
                "Can Shen",
                "Yile Liu",
                "Jirui Han",
                "Kelong Zheng",
                "Xuechao Zou",
                "Zhe Wang",
                "Xingjian Du",
                "Shun Zhang",
                "Hanjun Luo",
                "Yingbin Jin",
                "Xinxin Xing",
                "Ziyang Ma",
                "Yue Liu",
                "Xiaojun Jia",
                "Yifan Zhang",
                "Junfeng Fang",
                "Kun Wang",
                "Yibo Yan",
                "Haoyang Li",
                "Yiming Li",
                "Xiaobin Zhuang",
                "Yang Liu",
                "Haibo Hu",
                "Zhuo Chen",
                "Zhizheng Wu",
                "Xiaolin Hu",
                "Eng-Siong Chng",
                "XiaoFeng Wang",
                "Wenyuan Xu",
                "Wei Dong",
                "Xinfeng Li"
            ],
            "affiliations": [
                "ACM Member",
                "BJTU",
                "BNBU",
                "Bytedance",
                "CAS",
                "HUST",
                "Hong Kong Polytechnic University",
                "Hong Kong University of Science and Technology (Guangzhou)",
                "Independent Researcher",
                "Nanyang Technological University",
                "National University of Singapore",
                "QHU",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong (Shenzhen)",
                "Tsinghua University",
                "University of Rochester",
                "Waseda University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16211.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#benchmark",
                    "#security",
                    "#ethics",
                    "#open_source",
                    "#dataset",
                    "#audio"
                ],
                "emoji": "ğŸ™ï¸",
                "ru": {
                    "title": "AudioTrust: ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ˜Ğ˜",
                    "desc": "AudioTrust - ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ°Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ĞÑƒĞ´Ğ¸Ğ¾ Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¯Ğ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… ĞœĞ¾Ğ´ĞµĞ»ĞµĞ¹ (ĞĞ‘Ğ›Ğœ). ĞĞ½Ğ° Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ĞĞ‘Ğ›Ğœ Ğ¿Ğ¾ ÑˆĞµÑÑ‚Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼: ÑĞ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ, Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸, Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ, ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ°ÑƒÑ‚ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 4420 Ğ°ÑƒĞ´Ğ¸Ğ¾/Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ¸Ğ· Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ² Ğ¸ 9 ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ĞĞ‘Ğ›Ğœ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ€Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ñ… Ğ°ÑƒĞ´Ğ¸Ğ¾ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Evaluating Trust in Audio Large Language Models with AudioTrust",
                    "desc": "AudioTrust is a novel framework designed to evaluate the trustworthiness of Audio Large Language Models (ALLMs) across multiple dimensions. It addresses the unique challenges and risks associated with audio data, which are often overlooked in existing evaluation methods that focus primarily on text. The framework includes a comprehensive dataset of over 4,420 audio/text samples and employs 18 experimental setups to assess six key dimensions: fairness, hallucination, safety, privacy, robustness, and authentication. By utilizing nine audio-specific metrics and an automated scoring pipeline, AudioTrust provides insights into the limitations of current ALLMs, guiding their secure deployment in real-world applications."
                },
                "zh": {
                    "title": "éŸ³é¢‘æ¨¡å‹ä¿¡ä»»è¯„ä¼°æ–°æ ‡å‡†",
                    "desc": "AudioTrustæ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºéŸ³é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆALLMsï¼‰è®¾è®¡çš„å¤šç»´ä¿¡ä»»è¯„ä¼°æ¡†æ¶ã€‚å®ƒé€šè¿‡ä¸€ä¸ªåŒ…å«4420å¤šä¸ªéŸ³é¢‘/æ–‡æœ¬æ ·æœ¬çš„æ•°æ®é›†ï¼Œè¯„ä¼°æ¨¡å‹åœ¨å…¬å¹³æ€§ã€å¹»è§‰ã€å®‰å…¨æ€§ã€éšç§ã€é²æ£’æ€§å’Œè®¤è¯ç­‰å…­ä¸ªå…³é”®ç»´åº¦çš„è¡¨ç°ã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†18ç§ä¸åŒçš„å®éªŒè®¾ç½®å’Œ9ä¸ªéŸ³é¢‘ç‰¹å®šçš„è¯„ä¼°æŒ‡æ ‡ï¼Œä»¥ç¡®ä¿å…¨é¢è¯„ä¼°ALLMsçš„ä¿¡ä»»worthinessã€‚å®éªŒç»“æœæ­ç¤ºäº†å½“å‰æœ€å…ˆè¿›çš„å¼€æºå’Œé—­æºALLMsåœ¨é«˜é£é™©éŸ³é¢‘åœºæ™¯ä¸‹çš„ä¿¡ä»»è¾¹ç•Œå’Œå±€é™æ€§ï¼Œä¸ºæœªæ¥éŸ³é¢‘æ¨¡å‹çš„å®‰å…¨å’Œå¯ä¿¡éƒ¨ç½²æä¾›äº†é‡è¦è§è§£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.18129",
            "title": "One RL to See Them All: Visual Triple Unified Reinforcement Learning",
            "url": "https://huggingface.co/papers/2505.18129",
            "abstract": "A unified reinforcement learning system, V-Triune, combines visual reasoning and perception tasks in vision-language models through a single training pipeline, achieving significant improvements across various tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) has significantly advanced the reasoning capabilities of vision-language models (VLMs). However, the use of RL beyond reasoning tasks remains largely unexplored, especially for perceptionintensive tasks like object detection and grounding. We propose V-Triune, a Visual Triple Unified Reinforcement Learning system that enables VLMs to jointly learn visual reasoning and perception tasks within a single training pipeline. V-Triune comprises triple complementary components: Sample-Level Data Formatting (to unify diverse task inputs), Verifier-Level Reward Computation (to deliver custom rewards via specialized verifiers) , and Source-Level Metric Monitoring (to diagnose problems at the data-source level). We further introduce a novel Dynamic IoU reward, which provides adaptive, progressive, and definite feedback for perception tasks handled by V-Triune. Our approach is instantiated within off-the-shelf RL training framework using open-source 7B and 32B backbone models. The resulting model, dubbed Orsta (One RL to See Them All), demonstrates consistent improvements across both reasoning and perception tasks. This broad capability is significantly shaped by its training on a diverse dataset, constructed around four representative visual reasoning tasks (Math, Puzzle, Chart, and Science) and four visual perception tasks (Grounding, Detection, Counting, and OCR). Subsequently, Orsta achieves substantial gains on MEGA-Bench Core, with improvements ranging from +2.1 to an impressive +14.1 across its various 7B and 32B model variants, with performance benefits extending to a wide range of downstream tasks. These results highlight the effectiveness and scalability of our unified RL approach for VLMs. The V-Triune system, along with the Orsta models, is publicly available at https://github.com/MiniMax-AI.",
            "score": 9,
            "issue_id": 3945,
            "pub_date": "2025-05-23",
            "pub_date_card": {
                "ru": "23 Ğ¼Ğ°Ñ",
                "en": "May 23",
                "zh": "5æœˆ23æ—¥"
            },
            "hash": "e690c5668b7f4cd0",
            "authors": [
                "Yan Ma",
                "Linge Du",
                "Xuyang Shen",
                "Shaoxiang Chen",
                "Pengfei Li",
                "Qibing Ren",
                "Lizhuang Ma",
                "Yuchao Dai",
                "Pengfei Liu",
                "Junjie Yan"
            ],
            "affiliations": [
                "Google DeepMind",
                "MiniMax-AI",
                "OpenAI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.18129.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#dataset",
                    "#multimodal",
                    "#optimization",
                    "#training",
                    "#open_source",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜",
                    "desc": "V-Triune - ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‡ĞµÑ€ĞµĞ· ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸, Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ° Ğ¸ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ°. Ğ’Ğ²ĞµĞ´ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ° IoU Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Orsta Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ğº Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°Ğº Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ."
                },
                "en": {
                    "title": "Unifying Visual Reasoning and Perception in One RL System",
                    "desc": "The paper introduces V-Triune, a unified reinforcement learning system designed to enhance vision-language models (VLMs) by integrating visual reasoning and perception tasks into a single training framework. It features three key components: Sample-Level Data Formatting for input unification, Verifier-Level Reward Computation for tailored reward systems, and Source-Level Metric Monitoring for data diagnostics. A novel Dynamic IoU reward mechanism is also proposed, providing adaptive feedback for perception tasks. The resulting model, Orsta, shows significant performance improvements across various reasoning and perception benchmarks, demonstrating the effectiveness of this unified approach."
                },
                "zh": {
                    "title": "ç»Ÿä¸€å¼ºåŒ–å­¦ä¹ ï¼Œæå‡è§†è§‰æ¨ç†ä¸æ„ŸçŸ¥èƒ½åŠ›",
                    "desc": "V-Triuneæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿï¼Œæ—¨åœ¨é€šè¿‡å•ä¸€çš„è®­ç»ƒæµç¨‹ç»“åˆè§†è§‰æ¨ç†å’Œæ„ŸçŸ¥ä»»åŠ¡ã€‚è¯¥ç³»ç»ŸåŒ…å«ä¸‰ä¸ªäº’è¡¥çš„ç»„ä»¶ï¼Œåˆ†åˆ«æ˜¯æ ·æœ¬çº§æ•°æ®æ ¼å¼åŒ–ã€éªŒè¯å™¨çº§å¥–åŠ±è®¡ç®—å’Œæºçº§æŒ‡æ ‡ç›‘æ§ï¼Œä»¥æ”¯æŒå¤šæ ·åŒ–çš„ä»»åŠ¡è¾“å…¥å’Œå®šåˆ¶åŒ–çš„å¥–åŠ±åé¦ˆã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ–°çš„åŠ¨æ€IoUå¥–åŠ±ï¼Œä¸ºæ„ŸçŸ¥ä»»åŠ¡æä¾›é€‚åº”æ€§å’Œæ¸è¿›æ€§çš„åé¦ˆã€‚é€šè¿‡åœ¨å¤šæ ·åŒ–æ•°æ®é›†ä¸Šè®­ç»ƒï¼ŒV-Triuneæ˜¾è‘—æå‡äº†è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ¨ç†å’Œæ„ŸçŸ¥ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.15692",
            "title": "Thought-Augmented Policy Optimization: Bridging External Guidance and\n  Internal Capabilities",
            "url": "https://huggingface.co/papers/2505.15692",
            "abstract": "A novel RL framework, TAPO, integrates external guidance to enhance model performance and exploration compared to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) has emerged as an effective method for training reasoning models. However, existing RL approaches typically bias the model's output distribution toward reward-maximizing paths without introducing external knowledge. This limits their exploration capacity and results in a narrower reasoning capability boundary compared to base models. To address this limitation, we propose TAPO (Thought-Augmented Policy Optimization), a novel framework that augments RL by incorporating external high-level guidance (\"thought patterns\"). By adaptively integrating structured thoughts during training, TAPO effectively balances model-internal exploration and external guidance exploitation. Extensive experiments show that our approach significantly outperforms GRPO by 99% on AIME, 41% on AMC, and 17% on Minerva Math. Notably, these high-level thought patterns, abstracted from only 500 prior samples, generalize effectively across various tasks and models. This highlights TAPO's potential for broader applications across multiple tasks and domains. Our further analysis reveals that introducing external guidance produces powerful reasoning models with superior explainability of inference behavior and enhanced output readability.",
            "score": 9,
            "issue_id": 3946,
            "pub_date": "2025-05-21",
            "pub_date_card": {
                "ru": "21 Ğ¼Ğ°Ñ",
                "en": "May 21",
                "zh": "5æœˆ21æ—¥"
            },
            "hash": "5b1bedaf6be49ffa",
            "authors": [
                "Jinyang Wu",
                "Chonghua Liao",
                "Mingkuan Feng",
                "Shuai Zhang",
                "Zhengqi Wen",
                "Pengpeng Shao",
                "Huazhe Xu",
                "Jianhua Tao"
            ],
            "affiliations": [
                "Beijing National Research Center for Information Science and Technology",
                "Department of Automation, Tsinghua University",
                "Institution for Interdisciplinary Information Sciences, Tsinghua University",
                "Shanghai AI Lab",
                "Shanghai Qi Zhi Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.15692.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#interpretability",
                    "#rl",
                    "#rlhf"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "TAPO: Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¼Ğ¸ Ğ¼Ñ‹ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°Ğ¼Ğ¸",
                    "desc": "TAPO - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², TAPO Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ñ‹ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğ¼ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ TAPO Ğ½Ğ°Ğ´ GRPO Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ», Ñ‡Ñ‚Ğ¾ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ±ÑŠÑÑĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ñ‡Ğ¸Ñ‚Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒÑ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing Reinforcement Learning with Thought Patterns",
                    "desc": "The paper introduces TAPO (Thought-Augmented Policy Optimization), a new reinforcement learning framework that enhances model performance by integrating external guidance. Traditional RL methods often limit exploration by focusing solely on reward-maximizing paths, which restricts the reasoning capabilities of the models. TAPO addresses this issue by incorporating structured thought patterns during training, allowing for a better balance between internal exploration and external guidance. Experimental results demonstrate that TAPO significantly outperforms existing methods, leading to improved reasoning models that are more explainable and readable."
                },
                "zh": {
                    "title": "TAPOï¼šå¢å¼ºæ¢ç´¢ä¸æ¨ç†çš„æ–°æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶TAPOï¼Œé€šè¿‡æ•´åˆå¤–éƒ¨æŒ‡å¯¼æ¥æå‡æ¨¡å‹æ€§èƒ½å’Œæ¢ç´¢èƒ½åŠ›ã€‚ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•å¾€å¾€åªå…³æ³¨æœ€å¤§åŒ–å¥–åŠ±è·¯å¾„ï¼Œç¼ºä¹å¤–éƒ¨çŸ¥è¯†çš„å¼•å…¥ï¼Œé™åˆ¶äº†æ¨¡å‹çš„æ¢ç´¢èƒ½åŠ›ã€‚TAPOé€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€‚åº”æ€§åœ°æ•´åˆç»“æ„åŒ–æ€ç»´ï¼Œå¹³è¡¡äº†æ¨¡å‹å†…éƒ¨çš„æ¢ç´¢ä¸å¤–éƒ¨æŒ‡å¯¼çš„åˆ©ç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTAPOåœ¨å¤šä¸ªä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨å¹¿æ³›åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.17558",
            "title": "Teaching with Lies: Curriculum DPO on Synthetic Negatives for\n  Hallucination Detection",
            "url": "https://huggingface.co/papers/2505.17558",
            "abstract": "The use of carefully crafted hallucinations in a curriculum learning approach within the DPO alignment procedure significantly enhances LLMs' hallucination detection abilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Aligning large language models (LLMs) to accurately detect hallucinations remains a significant challenge due to the sophisticated nature of hallucinated text. Recognizing that hallucinated samples typically exhibit higher deceptive quality than traditional negative samples, we use these carefully engineered hallucinations as negative examples in the DPO alignment procedure. Our method incorporates a curriculum learning strategy, gradually transitioning the training from easier samples, identified based on the greatest reduction in probability scores from independent fact checking models, to progressively harder ones. This structured difficulty scaling ensures stable and incremental learning. Experimental evaluation demonstrates that our HaluCheck models, trained with curriculum DPO approach and high quality negative samples, significantly improves model performance across various metrics, achieving improvements of upto 24% on difficult benchmarks like MedHallu and HaluEval. Additionally, HaluCheck models demonstrate robustness in zero-shot settings, significantly outperforming larger state-of-the-art models across various benchmarks.",
            "score": 7,
            "issue_id": 3945,
            "pub_date": "2025-05-23",
            "pub_date_card": {
                "ru": "23 Ğ¼Ğ°Ñ",
                "en": "May 23",
                "zh": "5æœˆ23æ—¥"
            },
            "hash": "9faa21418742a88c",
            "authors": [
                "Shrey Pandit",
                "Ashwin Vinod",
                "Liu Leqi",
                "Ying Ding"
            ],
            "affiliations": [
                "The University of Texas at Austin"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17558.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#benchmark",
                    "#rlhf",
                    "#training",
                    "#hallucinations"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LLM Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ°Ğ¼Ğ¸Ñ… Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğµ DPO-Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡ĞµĞ±Ğ½Ñ‹Ğ¼ Ğ¿Ğ»Ğ°Ğ½Ğ¾Ğ¼, Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ñ Ğ¾Ñ‚ Ğ»ĞµĞ³ĞºĞ¸Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğº Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ HaluCheck, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ ÑÑ‚Ğ¸Ğ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Enhancing Hallucination Detection in LLMs through Curriculum Learning",
                    "desc": "This paper presents a novel approach to improve large language models' (LLMs) ability to detect hallucinations by using specially designed negative examples in a curriculum learning framework. The authors recognize that hallucinated texts are often more deceptive than standard negative samples, and they leverage this insight in the DPO alignment procedure. By gradually increasing the difficulty of training samples, the method ensures that LLMs learn to identify hallucinations more effectively over time. Experimental results show that the proposed HaluCheck models achieve significant performance gains, particularly on challenging benchmarks, and demonstrate strong performance even in zero-shot scenarios."
                },
                "zh": {
                    "title": "åˆ©ç”¨è¯¾ç¨‹å­¦ä¹ æå‡å¹»è§‰æ£€æµ‹èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åœ¨DPOå¯¹é½è¿‡ç¨‹ä¸­ä½¿ç”¨ç²¾å¿ƒè®¾è®¡çš„å¹»è§‰æ ·æœ¬çš„è¯¾ç¨‹å­¦ä¹ æ–¹æ³•ï¼Œä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹å¹»è§‰çš„æ£€æµ‹èƒ½åŠ›ã€‚æˆ‘ä»¬è®¤è¯†åˆ°ï¼Œå¹»è§‰æ ·æœ¬é€šå¸¸æ¯”ä¼ ç»Ÿçš„è´Ÿæ ·æœ¬å…·æœ‰æ›´é«˜çš„æ¬ºéª—æ€§ï¼Œå› æ­¤å°†è¿™äº›å¹»è§‰æ ·æœ¬ä½œä¸ºè´Ÿä¾‹ä½¿ç”¨ã€‚é€šè¿‡é€æ­¥å¼•å…¥æ›´éš¾çš„æ ·æœ¬ï¼Œæˆ‘ä»¬çš„è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ç¡®ä¿äº†ç¨³å®šçš„å¢é‡å­¦ä¹ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨è¯¾ç¨‹DPOæ–¹æ³•å’Œé«˜è´¨é‡è´Ÿæ ·æœ¬è®­ç»ƒçš„HaluCheckæ¨¡å‹åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šæ˜¾è‘—æé«˜äº†æ¨¡å‹æ€§èƒ½ï¼Œå°¤å…¶åœ¨MedHalluå’ŒHaluEvalç­‰å›°éš¾åŸºå‡†ä¸Šæå‡äº†å¤šè¾¾24%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16483",
            "title": "Teaching Large Language Models to Maintain Contextual Faithfulness via\n  Synthetic Tasks and Reinforcement Learning",
            "url": "https://huggingface.co/papers/2505.16483",
            "abstract": "CANOE improves LLM faithfulness in generation tasks using synthetic QA data and Dual-GRPO reinforcement learning without human annotations.  \t\t\t\t\tAI-generated summary \t\t\t\t Teaching large language models (LLMs) to be faithful in the provided context is crucial for building reliable information-seeking systems. Therefore, we propose a systematic framework, CANOE, to improve the faithfulness of LLMs in both short-form and long-form generation tasks without human annotations. Specifically, we first synthesize short-form question-answering (QA) data with four diverse tasks to construct high-quality and easily verifiable training data without human annotation. Also, we propose Dual-GRPO, a rule-based reinforcement learning method that includes three tailored rule-based rewards derived from synthesized short-form QA data, while simultaneously optimizing both short-form and long-form response generation. Notably, Dual-GRPO eliminates the need to manually label preference data to train reward models and avoids over-optimizing short-form generation when relying only on the synthesized short-form QA data. Experimental results show that CANOE greatly improves the faithfulness of LLMs across 11 different downstream tasks, even outperforming the most advanced LLMs, e.g., GPT-4o and OpenAI o1.",
            "score": 6,
            "issue_id": 3945,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 Ğ¼Ğ°Ñ",
                "en": "May 22",
                "zh": "5æœˆ22æ—¥"
            },
            "hash": "3bd78fedbb109d9a",
            "authors": [
                "Shuzheng Si",
                "Haozhe Zhao",
                "Cheng Gao",
                "Yuzhuo Bai",
                "Zhitong Wang",
                "Bofei Gao",
                "Kangyang Luo",
                "Wenhao Li",
                "Yufei Huang",
                "Gang Chen",
                "Fanchao Qi",
                "Minjia Zhang",
                "Baobao Chang",
                "Maosong Sun"
            ],
            "affiliations": [
                "DeepLang AI",
                "Peking University",
                "Tsinghua University",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16483.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#dataset",
                    "#rlhf",
                    "#optimization",
                    "#training",
                    "#synthetic"
                ],
                "emoji": "ğŸ›¶",
                "ru": {
                    "title": "Ğ”Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±ĞµĞ· Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸: CANOE ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸",
                    "desc": "CANOE - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ñ… Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñƒ Dual-GRPO. CANOE Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ‚Ñ€Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ° Ğ´Ğ»Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ CANOE Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° 11 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ´Ğ°Ğ¶Ğµ ÑĞ°Ğ¼Ñ‹Ğµ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ñ€Ğ¾Ğ´Ğµ GPT-4."
                },
                "en": {
                    "title": "Enhancing LLM Faithfulness with CANOE and Synthetic Data",
                    "desc": "The paper presents CANOE, a framework designed to enhance the faithfulness of large language models (LLMs) in generating text. It achieves this by creating synthetic question-answering (QA) data, which serves as high-quality training material without requiring human annotations. The framework employs a novel reinforcement learning approach called Dual-GRPO, which uses rule-based rewards to optimize both short-form and long-form text generation. Experimental results demonstrate that CANOE significantly improves LLM performance across various tasks, surpassing even state-of-the-art models like GPT-4o."
                },
                "zh": {
                    "title": "CANOEï¼šæå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„å¯ä¿¡åº¦",
                    "desc": "CANOEæ˜¯ä¸€ä¸ªç³»ç»Ÿæ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç”Ÿæˆä»»åŠ¡ä¸­çš„å¯ä¿¡åº¦ï¼Œè€Œæ— éœ€äººå·¥æ ‡æ³¨ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆæˆçŸ­å½¢å¼é—®ç­”ï¼ˆQAï¼‰æ•°æ®ï¼Œæ„å»ºé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ï¼Œå¹¶ä½¿ç”¨åŒé‡GRPOå¼ºåŒ–å­¦ä¹ æ–¹æ³•æ¥ä¼˜åŒ–ç”Ÿæˆè¿‡ç¨‹ã€‚åŒé‡GRPOç»“åˆäº†åŸºäºè§„åˆ™çš„å¥–åŠ±æœºåˆ¶ï¼Œç¡®ä¿åœ¨çŸ­å½¢å¼å’Œé•¿å½¢å¼ç”Ÿæˆä»»åŠ¡ä¸­éƒ½èƒ½æœ‰æ•ˆæå‡æ¨¡å‹çš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCANOEåœ¨11ä¸ªä¸åŒçš„ä¸‹æ¸¸ä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†LLMsçš„å¯ä¿¡åº¦ï¼Œç”šè‡³è¶…è¶Šäº†æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œå¦‚GPT-4oå’ŒOpenAI o1ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.15389",
            "title": "Are Vision-Language Models Safe in the Wild? A Meme-Based Benchmark\n  Study",
            "url": "https://huggingface.co/papers/2505.15389",
            "abstract": "VLMs are more vulnerable to harmful meme-based prompts than to synthetic images, and while multi-turn interactions offer some protection, significant vulnerabilities remain.  \t\t\t\t\tAI-generated summary \t\t\t\t Rapid deployment of vision-language models (VLMs) magnifies safety risks, yet most evaluations rely on artificial images. This study asks: How safe are current VLMs when confronted with meme images that ordinary users share? To investigate this question, we introduce MemeSafetyBench, a 50,430-instance benchmark pairing real meme images with both harmful and benign instructions. Using a comprehensive safety taxonomy and LLM-based instruction generation, we assess multiple VLMs across single and multi-turn interactions. We investigate how real-world memes influence harmful outputs, the mitigating effects of conversational context, and the relationship between model scale and safety metrics. Our findings demonstrate that VLMs show greater vulnerability to meme-based harmful prompts than to synthetic or typographic images. Memes significantly increase harmful responses and decrease refusals compared to text-only inputs. Though multi-turn interactions provide partial mitigation, elevated vulnerability persists. These results highlight the need for ecologically valid evaluations and stronger safety mechanisms.",
            "score": 6,
            "issue_id": 3945,
            "pub_date": "2025-05-21",
            "pub_date_card": {
                "ru": "21 Ğ¼Ğ°Ñ",
                "en": "May 21",
                "zh": "5æœˆ21æ—¥"
            },
            "hash": "102a2cdaf1ccf7d5",
            "authors": [
                "DongGeon Lee",
                "Joonwon Jang",
                "Jihae Jeong",
                "Hwanjo Yu"
            ],
            "affiliations": [
                "LG AI Research",
                "POSTECH"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.15389.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#ethics",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "ĞœĞµĞ¼Ñ‹ vs Ğ˜Ğ˜: Ğ½ĞµĞ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ½Ğ°Ñ ÑƒĞ³Ñ€Ğ¾Ğ·Ğ° Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (VLM) Ğ±Ğ¾Ğ»ĞµĞµ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ñ‹ Ğº Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ğ¼ Ğ¼ĞµĞ¼Ğ°Ğ¼, Ñ‡ĞµĞ¼ Ğº ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ MemeSafetyBench - Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 50 430 Ğ¼ĞµĞ¼Ğ¾Ğ² Ñ Ğ²Ñ€ĞµĞ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ±Ğ¸Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ VLM. ĞœĞ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ÑÑ‚ Ñ€Ğ¸ÑĞºĞ¸, Ğ½Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ÑÑ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑĞºĞ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ñ‹Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ¸ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ VLM."
                },
                "en": {
                    "title": "Meme Vulnerability: A Call for Safer VLMs",
                    "desc": "This paper investigates the safety of vision-language models (VLMs) when exposed to real-world meme images, which are often shared by users. The authors introduce a benchmark called MemeSafetyBench, consisting of over 50,000 instances of meme images paired with harmful and benign instructions. The study finds that VLMs are more susceptible to harmful prompts from memes compared to synthetic images, and while multi-turn interactions can offer some protection, vulnerabilities remain significant. The results emphasize the importance of realistic evaluations and the need for improved safety measures in VLMs."
                },
                "zh": {
                    "title": "æ¶æå›¾åƒå¯¹è§†è§‰è¯­è¨€æ¨¡å‹çš„å®‰å…¨å¨èƒ",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨é¢å¯¹ç”¨æˆ·åˆ†äº«çš„æ¶æå›¾åƒæ—¶çš„å®‰å…¨æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†MemeSafetyBenchï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«50,430ä¸ªå®ä¾‹çš„åŸºå‡†ï¼Œç»“åˆäº†çœŸå®çš„æ¶æå›¾åƒå’Œæœ‰å®³ä¸æ— å®³çš„æŒ‡ä»¤ã€‚ç ”ç©¶å‘ç°ï¼ŒVLMså¯¹æ¶æå›¾åƒçš„æœ‰å®³æç¤ºæ¯”å¯¹åˆæˆå›¾åƒæ›´è„†å¼±ï¼Œä¸”å¤šè½®å¯¹è¯è™½ç„¶æä¾›äº†ä¸€å®šçš„ä¿æŠ¤ï¼Œä½†ä»ç„¶å­˜åœ¨æ˜¾è‘—çš„è„†å¼±æ€§ã€‚æˆ‘ä»¬çš„ç»“æœå¼ºè°ƒäº†éœ€è¦è¿›è¡Œç”Ÿæ€æœ‰æ•ˆçš„è¯„ä¼°å’Œæ›´å¼ºçš„å®‰å…¨æœºåˆ¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.17826",
            "title": "Trinity-RFT: A General-Purpose and Unified Framework for Reinforcement\n  Fine-Tuning of Large Language Models",
            "url": "https://huggingface.co/papers/2505.17826",
            "abstract": "Trinity-RFT is a flexible and scalable framework for reinforcement fine-tuning of large language models, supporting various interaction modes and data pipelines.  \t\t\t\t\tAI-generated summary \t\t\t\t Trinity-RFT is a general-purpose, flexible and scalable framework designed for reinforcement fine-tuning (RFT) of large language models. It is built with a decoupled design, consisting of (1) an RFT-core that unifies and generalizes synchronous/asynchronous, on-policy/off-policy, and online/offline modes of RFT, (2) seamless integration for agent-environment interaction with high efficiency and robustness, and (3) systematic data pipelines optimized for RFT. Trinity-RFT can be easily adapted for diverse application scenarios, and serves as a unified platform for exploring advanced reinforcement learning paradigms. This technical report outlines the vision, features, design and implementations of Trinity-RFT, accompanied by extensive examples demonstrating the utility and user-friendliness of the proposed framework.",
            "score": 5,
            "issue_id": 3945,
            "pub_date": "2025-05-23",
            "pub_date_card": {
                "ru": "23 Ğ¼Ğ°Ñ",
                "en": "May 23",
                "zh": "5æœˆ23æ—¥"
            },
            "hash": "6f6fdf1b20859c44",
            "authors": [
                "Xuchen Pan",
                "Yanxi Chen",
                "Yushuo Chen",
                "Yuchang Sun",
                "Daoyuan Chen",
                "Wenhao Zhang",
                "Yuexiang Xie",
                "Yilun Huang",
                "Yilei Zhang",
                "Dawei Gao",
                "Yaliang Li",
                "Bolin Ding",
                "Jingren Zhou"
            ],
            "affiliations": [
                "alibaba-inc.com"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17826.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#rlhf",
                    "#agi",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Trinity-RFT: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ´Ğ»Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Trinity-RFT - ÑÑ‚Ğ¾ Ğ³Ğ¸Ğ±ĞºĞ°Ñ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ´Ğ»Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ½Ğ° Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ€ĞµĞ¶Ğ¸Ğ¼Ñ‹ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ/Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ, on-policy/off-policy Ğ¸ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½/Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Trinity-RFT ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ´Ñ€Ğ° RFT, Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞŸĞ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ»ĞµĞ³ĞºĞ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ¿Ğ¾Ğ´ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼."
                },
                "en": {
                    "title": "Empowering Language Models with Flexible Reinforcement Fine-Tuning",
                    "desc": "Trinity-RFT is a versatile framework designed for reinforcement fine-tuning (RFT) of large language models. It features a decoupled architecture that supports various modes of RFT, including synchronous and asynchronous, as well as on-policy and off-policy approaches. The framework ensures efficient and robust interactions between agents and environments, while also providing optimized data pipelines for RFT tasks. This makes Trinity-RFT adaptable to a wide range of applications, serving as a comprehensive platform for exploring advanced reinforcement learning techniques."
                },
                "zh": {
                    "title": "Trinity-RFTï¼šçµæ´»çš„å¼ºåŒ–å¾®è°ƒæ¡†æ¶",
                    "desc": "Trinity-RFTæ˜¯ä¸€ä¸ªçµæ´»ä¸”å¯æ‰©å±•çš„æ¡†æ¶ï¼Œä¸“é—¨ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¼ºåŒ–å¾®è°ƒã€‚å®ƒé‡‡ç”¨è§£è€¦è®¾è®¡ï¼ŒåŒ…å«ä¸€ä¸ªRFTæ ¸å¿ƒï¼Œèƒ½å¤Ÿç»Ÿä¸€å’Œæ¦‚æ‹¬åŒæ­¥/å¼‚æ­¥ã€åœ¨çº¿/ç¦»çº¿ç­‰å¤šç§å¼ºåŒ–å¾®è°ƒæ¨¡å¼ã€‚è¯¥æ¡†æ¶æ”¯æŒé«˜æ•ˆä¸”ç¨³å¥çš„æ™ºèƒ½ä½“ä¸ç¯å¢ƒçš„äº¤äº’ï¼Œå¹¶ä¼˜åŒ–äº†æ•°æ®ç®¡é“ä»¥é€‚åº”å¼ºåŒ–å¾®è°ƒçš„éœ€æ±‚ã€‚Trinity-RFTæ˜“äºé€‚åº”ä¸åŒçš„åº”ç”¨åœºæ™¯ï¼Œæ˜¯æ¢ç´¢å…ˆè¿›å¼ºåŒ–å­¦ä¹ èŒƒå¼çš„ç»Ÿä¸€å¹³å°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.17225",
            "title": "Reasoning Model is Stubborn: Diagnosing Instruction Overriding in\n  Reasoning Models",
            "url": "https://huggingface.co/papers/2505.17225",
            "abstract": "A diagnostic set examines and categorizes reasoning rigidity in large language models, identifying patterns where models ignore instructions and default to familiar reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models have demonstrated remarkable proficiency in long and complex reasoning tasks. However, they frequently exhibit a problematic reliance on familiar reasoning patterns, a phenomenon we term reasoning rigidity. Despite explicit instructions from users, these models often override clearly stated conditions and default to habitual reasoning trajectories, leading to incorrect conclusions. This behavior presents significant challenges, particularly in domains such as mathematics and logic puzzle, where precise adherence to specified constraints is critical. To systematically investigate reasoning rigidity, a behavior largely unexplored in prior work, we introduce a expert-curated diagnostic set, . Our dataset includes specially modified variants of existing mathematical benchmarks, namely AIME and MATH500, as well as well-known puzzles deliberately redesigned to require deviation from familiar reasoning strategies. Using this dataset, we identify recurring contamination patterns that occur when models default to ingrained reasoning. Specifically, we categorize this contamination into three distinctive modes: (i) Interpretation Overload, (ii) Input Distrust, and (iii) Partial Instruction Attention, each causing models to ignore or distort provided instructions. We publicly release our diagnostic set to facilitate future research on mitigating reasoning rigidity in language models.",
            "score": 5,
            "issue_id": 3945,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 Ğ¼Ğ°Ñ",
                "en": "May 22",
                "zh": "5æœˆ22æ—¥"
            },
            "hash": "41036303d3b75082",
            "authors": [
                "Doohyuk Jang",
                "Yoonjeon Kim",
                "Chanjae Park",
                "Hyun Ryu",
                "Eunho Yang"
            ],
            "affiliations": [
                "AITRICS",
                "KAIST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17225.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#dataset",
                    "#data",
                    "#interpretability",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ¶ĞµÑÑ‚ĞºĞ¾ÑÑ‚Ğ¸ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ² Ğ˜Ğ˜: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¶ĞµÑÑ‚ĞºĞ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ñ‚ĞµĞ½Ğ´ĞµĞ½Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ³Ğ½Ğ¾Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ·Ğ½Ğ°ĞºĞ¾Ğ¼Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ‘Ñ‹Ğ»Ğ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ñ‹ Ñ‚Ñ€Ğ¸ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ° ĞºĞ¾Ğ½Ñ‚Ğ°Ğ¼Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸: Ğ¿ĞµÑ€ĞµĞ³Ñ€ÑƒĞ·ĞºĞ° Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸, Ğ½ĞµĞ´Ğ¾Ğ²ĞµÑ€Ğ¸Ğµ Ğº Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğº Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼. ĞĞ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ñ‹ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ¸ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğ»Ğ¾Ğ¼Ğ¾Ğº, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ Ğ¿Ñ€Ğ¸Ğ²Ñ‹Ñ‡Ğ½Ñ‹Ñ… ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Unraveling Reasoning Rigidity in Language Models",
                    "desc": "This paper investigates a phenomenon called reasoning rigidity in large language models, where these models often ignore user instructions and revert to familiar reasoning patterns. The authors introduce a diagnostic set designed to identify and categorize this behavior, which can lead to incorrect conclusions in tasks requiring precise adherence to instructions. They highlight three specific modes of contamination: Interpretation Overload, Input Distrust, and Partial Instruction Attention, which describe how models distort or overlook given instructions. By releasing this diagnostic set, the authors aim to support further research aimed at reducing reasoning rigidity in language models."
                },
                "zh": {
                    "title": "æ­ç¤ºè¯­è¨€æ¨¡å‹çš„æ¨ç†åƒµåŒ–ç°è±¡",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æ¨ç†åƒµåŒ–ç°è±¡ï¼Œå³æ¨¡å‹åœ¨é¢å¯¹æ˜ç¡®æŒ‡ä»¤æ—¶ï¼Œä»ç„¶å€¾å‘äºä½¿ç”¨ç†Ÿæ‚‰çš„æ¨ç†æ¨¡å¼ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªä¸“å®¶ç­–åˆ’çš„è¯Šæ–­é›†ï¼Œä»¥ç³»ç»Ÿåœ°ç ”ç©¶è¿™ä¸€è¡Œä¸ºï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°å­¦å’Œé€»è¾‘éš¾é¢˜ç­‰é¢†åŸŸã€‚è¯¥æ•°æ®é›†åŒ…å«ç»è¿‡ä¿®æ”¹çš„æ•°å­¦åŸºå‡†å’Œé‡æ–°è®¾è®¡çš„éš¾é¢˜ï¼Œæ—¨åœ¨ä¿ƒä½¿æ¨¡å‹åç¦»å¸¸è§„æ¨ç†ç­–ç•¥ã€‚é€šè¿‡åˆ†æï¼Œæˆ‘ä»¬è¯†åˆ«å‡ºä¸‰ç§ä¸»è¦çš„æ¨ç†åƒµåŒ–æ¨¡å¼ï¼Œå¸®åŠ©æœªæ¥çš„ç ”ç©¶æ›´å¥½åœ°è§£å†³è¿™ä¸€é—®é¢˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.17508",
            "title": "On the Design of KL-Regularized Policy Gradient Algorithms for LLM\n  Reasoning",
            "url": "https://huggingface.co/papers/2505.17508",
            "abstract": "A regularized policy gradient framework is introduced to explore KL divergence formulations for enhancing the reasoning capabilities of LLMs in online reinforcement learning, demonstrating improved training stability and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Policy gradient algorithms have been successfully applied to enhance the reasoning capabilities of large language models (LLMs). Despite the widespread use of Kullback-Leibler (KL) regularization in policy gradient algorithms to stabilize training, the systematic exploration of how different KL divergence formulations can be estimated and integrated into surrogate loss functions for online reinforcement learning (RL) presents a nuanced and systematically explorable design space. In this paper, we propose regularized policy gradient (RPG), a systematic framework for deriving and analyzing KL-regularized policy gradient methods in the online RL setting. We derive policy gradients and corresponding surrogate loss functions for objectives regularized by both forward and reverse KL divergences, considering both normalized and unnormalized policy distributions. Furthermore, we present derivations for fully differentiable loss functions as well as REINFORCE-style gradient estimators, accommodating diverse algorithmic needs. We conduct extensive experiments on RL for LLM reasoning using these methods, showing improved or competitive results in terms of training stability and performance compared to strong baselines such as GRPO, REINFORCE++, and DAPO. The code is available at https://github.com/complex-reasoning/RPG.",
            "score": 4,
            "issue_id": 3945,
            "pub_date": "2025-05-23",
            "pub_date_card": {
                "ru": "23 Ğ¼Ğ°Ñ",
                "en": "May 23",
                "zh": "5æœˆ23æ—¥"
            },
            "hash": "6ae63edcb7127847",
            "authors": [
                "Yifan Zhang",
                "Yifeng Liu",
                "Huizhuo Yuan",
                "Yang Yuan",
                "Quanquan Gu",
                "Andrew C Yao"
            ],
            "affiliations": [
                "IIIS, Tsinghua University",
                "Shanghai Qi Zhi Institute",
                "University of California, Los Angeles"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17508.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#rlhf",
                    "#optimization",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ° Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ RPG (regularized policy gradient) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ KL-Ğ´Ğ¸Ğ²ĞµÑ€Ğ³ĞµĞ½Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ° Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ°Ğ¼Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº RPG Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ KL-Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ° Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸."
                },
                "en": {
                    "title": "Enhancing LLM Reasoning with Regularized Policy Gradients",
                    "desc": "This paper introduces a regularized policy gradient framework that utilizes Kullback-Leibler (KL) divergence to improve the reasoning abilities of large language models (LLMs) in online reinforcement learning (RL). It systematically explores various KL divergence formulations to enhance training stability and performance through surrogate loss functions. The authors derive policy gradients for both forward and reverse KL divergences, accommodating different types of policy distributions. Extensive experiments demonstrate that their proposed methods achieve better or comparable results against established baselines in RL tasks involving LLMs."
                },
                "zh": {
                    "title": "æ­£åˆ™åŒ–ç­–ç•¥æ¢¯åº¦ï¼šæå‡LLMæ¨ç†èƒ½åŠ›çš„å…³é”®",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ­£åˆ™åŒ–ç­–ç•¥æ¢¯åº¦æ¡†æ¶ï¼Œç”¨äºæ¢ç´¢KLæ•£åº¦çš„ä¸åŒå½¢å¼ï¼Œä»¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åœ¨çº¿å¼ºåŒ–å­¦ä¹ ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°åˆ†æäº†å¦‚ä½•å°†ä¸åŒçš„KLæ•£åº¦ä¼°è®¡æ•´åˆåˆ°æ›¿ä»£æŸå¤±å‡½æ•°ä¸­ï¼Œä»è€Œæé«˜è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚é€šè¿‡å¯¹æ­£å‘å’Œåå‘KLæ•£åº¦çš„æ­£åˆ™åŒ–ç›®æ ‡ï¼Œæˆ‘ä»¬æ¨å¯¼äº†ç›¸åº”çš„ç­–ç•¥æ¢¯åº¦å’ŒæŸå¤±å‡½æ•°ï¼Œå¹¶è€ƒè™‘äº†æ ‡å‡†åŒ–å’Œéæ ‡å‡†åŒ–çš„ç­–ç•¥åˆ†å¸ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰çš„å¼ºåŸºçº¿ç®—æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è®­ç»ƒç¨³å®šæ€§å’Œæ€§èƒ½ä¸Šéƒ½æœ‰æ˜¾è‘—æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.17417",
            "title": "Speechless: Speech Instruction Training Without Speech for Low Resource\n  Languages",
            "url": "https://huggingface.co/papers/2505.17417",
            "abstract": "The rapid growth of voice assistants powered by large language models (LLM) has highlighted a need for speech instruction data to train these systems. Despite the abundance of speech recognition data, there is a notable scarcity of speech instruction data, which is essential for fine-tuning models to understand and execute spoken commands. Generating high-quality synthetic speech requires a good text-to-speech (TTS) model, which may not be available to low resource languages. Our novel approach addresses this challenge by halting synthesis at the semantic representation level, bypassing the need for TTS. We achieve this by aligning synthetic semantic representations with the pre-trained Whisper encoder, enabling an LLM to be fine-tuned on text instructions while maintaining the ability to understand spoken instructions during inference. This simplified training process is a promising approach to building voice assistant for low-resource languages.",
            "score": 4,
            "issue_id": 3945,
            "pub_date": "2025-05-23",
            "pub_date_card": {
                "ru": "23 Ğ¼Ğ°Ñ",
                "en": "May 23",
                "zh": "5æœˆ23æ—¥"
            },
            "hash": "9d72b20aca0789ee",
            "authors": [
                "Alan Dao",
                "Dinh Bach Vu",
                "Huy Hoang Ha",
                "Tuan Le Duc Anh",
                "Shreyas Gopal",
                "Yue Heng Yeo",
                "Warren Keng Hoong Low",
                "Eng Siong Chng",
                "Jia Qi Yip"
            ],
            "affiliations": [
                "CCDS, Nanyang Technological University, Singapore",
                "Menlo Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17417.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#dataset",
                    "#low_resource",
                    "#data",
                    "#synthetic",
                    "#training",
                    "#audio"
                ],
                "emoji": "ğŸ—£ï¸",
                "ru": {
                    "title": "Ğ“Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ñ‹Ğµ Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ñ€ĞµĞ´ĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ· TTS",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ñ‹Ñ… Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ² Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±Ğ¾Ğ¹Ñ‚Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ text-to-speech, Ğ¾ÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ¼ Whisper. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑÑ…, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ ÑƒÑÑ‚Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹ Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞµ."
                },
                "en": {
                    "title": "Empowering Voice Assistants for Low-Resource Languages",
                    "desc": "This paper addresses the challenge of training voice assistants in low-resource languages, where there is a lack of speech instruction data. It proposes a novel method that generates synthetic speech by stopping at the semantic representation level, eliminating the need for a text-to-speech (TTS) model. By aligning these semantic representations with the pre-trained Whisper encoder, the approach allows for fine-tuning large language models (LLMs) on text instructions while still being able to process spoken commands. This method simplifies the training process and enhances the development of voice assistants for languages with limited resources."
                },
                "zh": {
                    "title": "ä¸ºä½èµ„æºè¯­è¨€æ„å»ºè¯­éŸ³åŠ©æ‰‹çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†ä¸ºè¯­éŸ³åŠ©æ‰‹è®­ç»ƒæ‰€éœ€çš„è¯­éŸ³æŒ‡ä»¤æ•°æ®çš„ä¸è¶³é—®é¢˜ã€‚å°½ç®¡è¯­éŸ³è¯†åˆ«æ•°æ®ä¸°å¯Œï¼Œä½†è¯­éŸ³æŒ‡ä»¤æ•°æ®å´ç›¸å¯¹ç¨€ç¼ºï¼Œè¿™å¯¹æ¨¡å‹ç†è§£å’Œæ‰§è¡Œå£å¤´å‘½ä»¤è‡³å…³é‡è¦ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡åœ¨è¯­ä¹‰è¡¨ç¤ºå±‚é¢åœæ­¢åˆæˆï¼Œé¿å…äº†å¯¹æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹çš„ä¾èµ–ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†åˆæˆçš„è¯­ä¹‰è¡¨ç¤ºä¸é¢„è®­ç»ƒçš„Whisperç¼–ç å™¨å¯¹é½ï¼Œä½¿å¾—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿåœ¨æ–‡æœ¬æŒ‡ä»¤ä¸Šè¿›è¡Œå¾®è°ƒï¼ŒåŒæ—¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­ç†è§£å£å¤´æŒ‡ä»¤ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.17561",
            "title": "Model Already Knows the Best Noise: Bayesian Active Noise Selection via\n  Attention in Video Diffusion Model",
            "url": "https://huggingface.co/papers/2505.17561",
            "abstract": "ANSE enhances video diffusion models by selecting noise seeds based on model confidence, improving video quality and temporal coherence with minimal increase in inference time.  \t\t\t\t\tAI-generated summary \t\t\t\t The choice of initial noise significantly affects the quality and prompt alignment of video diffusion models, where different noise seeds for the same prompt can lead to drastically different generations. While recent methods rely on externally designed priors such as frequency filters or inter-frame smoothing, they often overlook internal model signals that indicate which noise seeds are inherently preferable. To address this, we propose ANSE (Active Noise Selection for Generation), a model-aware framework that selects high-quality noise seeds by quantifying attention-based uncertainty. At its core is BANSA (Bayesian Active Noise Selection via Attention), an acquisition function that measures entropy disagreement across multiple stochastic attention samples to estimate model confidence and consistency. For efficient inference-time deployment, we introduce a Bernoulli-masked approximation of BANSA that enables score estimation using a single diffusion step and a subset of attention layers. Experiments on CogVideoX-2B and 5B demonstrate that ANSE improves video quality and temporal coherence with only an 8% and 13% increase in inference time, respectively, providing a principled and generalizable approach to noise selection in video diffusion. See our project page: https://anse-project.github.io/anse-project/",
            "score": 3,
            "issue_id": 3945,
            "pub_date": "2025-05-23",
            "pub_date_card": {
                "ru": "23 Ğ¼Ğ°Ñ",
                "en": "May 23",
                "zh": "5æœˆ23æ—¥"
            },
            "hash": "224b15e182587a84",
            "authors": [
                "Kwanyoung Kim",
                "Sanghyun Kim"
            ],
            "affiliations": [
                "Samsung Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17561.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#inference",
                    "#diffusion",
                    "#optimization"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ ÑˆÑƒĞ¼Ğ° Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ANSE - Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑˆÑƒĞ¼Ğ¾Ğ²Ñ‹Ñ… ÑĞ¸Ğ´Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ’ Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ»ĞµĞ¶Ğ¸Ñ‚ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ BANSA, Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‰Ğ°Ñ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ»Ğ°ÑĞ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ°Ğ¼Ğ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. Ğ”Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ°Ğ¿Ğ¿Ñ€Ğ¾ĞºÑĞ¸Ğ¼Ğ°Ñ†Ğ¸Ñ BANSA Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ Ğ‘ĞµÑ€Ğ½ÑƒĞ»Ğ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°."
                },
                "en": {
                    "title": "Smart Noise Selection for Better Video Generation",
                    "desc": "The paper introduces ANSE, a method that enhances video diffusion models by intelligently selecting noise seeds based on the model's confidence. It highlights the importance of initial noise in generating high-quality videos, as different seeds can lead to varying results. ANSE utilizes an acquisition function called BANSA, which measures uncertainty through attention-based entropy to identify the best noise seeds. This approach improves video quality and temporal coherence while only slightly increasing the time needed for inference."
                },
                "zh": {
                    "title": "ä¸»åŠ¨é€‰æ‹©å™ªå£°ï¼Œæå‡è§†é¢‘ç”Ÿæˆè´¨é‡",
                    "desc": "ANSEï¼ˆä¸»åŠ¨å™ªå£°é€‰æ‹©ç”Ÿæˆï¼‰é€šè¿‡åŸºäºæ¨¡å‹ä¿¡å¿ƒé€‰æ‹©å™ªå£°ç§å­ï¼Œå¢å¼ºäº†è§†é¢‘æ‰©æ•£æ¨¡å‹çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶é‡åŒ–ä¸ç¡®å®šæ€§ï¼Œä»è€Œé€‰æ‹©é«˜è´¨é‡çš„å™ªå£°ç§å­ï¼Œæ˜¾è‘—æé«˜è§†é¢‘è´¨é‡å’Œæ—¶é—´ä¸€è‡´æ€§ã€‚æ ¸å¿ƒç®—æ³•BANSAï¼ˆåŸºäºæ³¨æ„åŠ›çš„è´å¶æ–¯ä¸»åŠ¨å™ªå£°é€‰æ‹©ï¼‰é€šè¿‡æµ‹é‡å¤šä¸ªéšæœºæ³¨æ„åŠ›æ ·æœ¬ä¹‹é—´çš„ç†µä¸ä¸€è‡´æ€§æ¥ä¼°è®¡æ¨¡å‹çš„ä¿¡å¿ƒå’Œä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒANSEåœ¨æ¨ç†æ—¶é—´ä»…å¢åŠ 8%å’Œ13%çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—æ”¹å–„äº†è§†é¢‘ç”Ÿæˆçš„è´¨é‡å’Œä¸€è‡´æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16270",
            "title": "Transformer Copilot: Learning from The Mistake Log in LLM Fine-tuning",
            "url": "https://huggingface.co/papers/2505.16270",
            "abstract": "The Transformer Copilot framework enhances large language model performance through a Copilot model that refines the Pilot's logits based on a Mistake Log, leading to consistent performance improvements across various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models are typically adapted to downstream tasks through supervised fine-tuning on domain-specific data. While standard fine-tuning focuses on minimizing generation loss to optimize model parameters, we take a deeper step by retaining and leveraging the model's own learning signals, analogous to how human learners reflect on past mistakes to improve future performance. We first introduce the concept of Mistake Log to systematically track the model's learning behavior and recurring errors throughout fine-tuning. Treating the original transformer-based model as the Pilot, we correspondingly design a Copilot model to refine the Pilot's inference performance via logits rectification. We name the overall Pilot-Copilot framework the Transformer Copilot, which introduces (i) a novel Copilot model design, (ii) a joint training paradigm where the Copilot continuously learns from the evolving Mistake Log alongside the Pilot, and (iii) a fused inference paradigm where the Copilot rectifies the Pilot's logits for enhanced generation. We provide both theoretical and empirical analyses on our new learning framework. Experiments on 12 benchmarks spanning commonsense, arithmetic, and recommendation tasks demonstrate that Transformer Copilot consistently improves performance by up to 34.5%, while introducing marginal computational overhead to Pilot models and exhibiting strong scalability and transferability.",
            "score": 3,
            "issue_id": 3945,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 Ğ¼Ğ°Ñ",
                "en": "May 22",
                "zh": "5æœˆ22æ—¥"
            },
            "hash": "242b4420fd3d9c4f",
            "authors": [
                "Jiaru Zou",
                "Yikun Ban",
                "Zihao Li",
                "Yunzhe Qi",
                "Ruizhong Qiu",
                "Ling Yang",
                "Jingrui He"
            ],
            "affiliations": [
                "Princeton University",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16270.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#optimization",
                    "#architecture",
                    "#transfer_learning",
                    "#training"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Transformer Copilot: Ğ£Ñ‡Ğ¸Ğ¼ÑÑ Ğ½Ğ° Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ñ… Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Transformer Copilot, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸-Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ³Ğ¾ Ğ¿Ğ¸Ğ»Ğ¾Ñ‚Ğ° (Copilot), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»Ğ¾Ğ³Ğ¸Ñ‚Ñ‹ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (Pilot) Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¶ÑƒÑ€Ğ½Ğ°Ğ»Ğ° Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº (Mistake Log). Ğ­Ñ‚Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒÑ‡Ğ¸Ñ‚ÑŒÑÑ Ğ½Ğ° ÑĞ²Ğ¾Ğ¸Ñ… Ğ¿Ñ€Ğ¾ÑˆĞ»Ñ‹Ñ… Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ñ…, Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ğ¾ Ñ‚Ğ¾Ğ¼Ñƒ, ĞºĞ°Ğº ÑƒÑ‡Ğ°Ñ‚ÑÑ Ğ»ÑĞ´Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° 12 Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¾ 34.5% Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "Enhancing Language Models with Reflective Learning",
                    "desc": "The Transformer Copilot framework improves the performance of large language models by using a Copilot model that refines the Pilot's outputs based on a Mistake Log. This Mistake Log tracks the model's errors during fine-tuning, allowing the Copilot to learn from these mistakes, similar to how humans learn. The framework includes a novel design for the Copilot, a joint training approach where both models learn together, and a fused inference method that enhances the Pilot's predictions. Experiments show that this approach can boost performance by up to 34.5% across various tasks with minimal additional computational cost."
                },
                "zh": {
                    "title": "æå‡è¯­è¨€æ¨¡å‹æ€§èƒ½çš„å‰¯é©¾é©¶æ¡†æ¶",
                    "desc": "Transformer Copilotæ¡†æ¶é€šè¿‡ä¸€ä¸ªå‰¯é©¾é©¶æ¨¡å‹æ¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚è¿™ä¸ªå‰¯é©¾é©¶æ¨¡å‹æ ¹æ®é”™è¯¯æ—¥å¿—æ¥ä¼˜åŒ–ä¸»æ¨¡å‹çš„è¾“å‡ºï¼Œä»è€Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æŒç»­çš„æ€§èƒ½æå‡ã€‚æˆ‘ä»¬å¼•å…¥äº†é”™è¯¯æ—¥å¿—çš„æ¦‚å¿µï¼Œä»¥ç³»ç»Ÿåœ°è·Ÿè¸ªæ¨¡å‹çš„å­¦ä¹ è¡Œä¸ºå’Œé‡å¤é”™è¯¯ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå‰¯é©¾é©¶æ¨¡å‹èƒ½å¤Ÿåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸æ–­å­¦ä¹ ï¼Œä»è€Œæé«˜ç”Ÿæˆçš„å‡†ç¡®æ€§å’Œè´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.17091",
            "title": "Large Language Models Implicitly Learn to See and Hear Just By Reading",
            "url": "https://huggingface.co/papers/2505.17091",
            "abstract": "Auto-regressive text LLMs trained on text can develop internal capabilities for understanding images and audio, enabling them to perform classification tasks across different modalities without fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper presents a fascinating find: By training an auto-regressive LLM model on text tokens, the text model inherently develops internally an ability to understand images and audio, thereby developing the ability to see and hear just by reading. Popular audio and visual LLM models fine-tune text LLM models to give text output conditioned on images and audio embeddings. On the other hand, our architecture takes in patches of images, audio waveforms or tokens as input. It gives us the embeddings or category labels typical of a classification pipeline. We show the generality of text weights in aiding audio classification for datasets FSD-50K and GTZAN. Further, we show this working for image classification on CIFAR-10 and Fashion-MNIST, as well on image patches. This pushes the notion of text-LLMs learning powerful internal circuits that can be utilized by activating necessary connections for various applications rather than training models from scratch every single time.",
            "score": 3,
            "issue_id": 3945,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ",
                "en": "May 20",
                "zh": "5æœˆ20æ—¥"
            },
            "hash": "7bb07d5dfb6680e6",
            "authors": [
                "Prateek Verma",
                "Mert Pilanci"
            ],
            "affiliations": [
                "Department of Electrical Engineering Stanford University Stanford, CA, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17091.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#multimodal",
                    "#optimization",
                    "#architecture",
                    "#transfer_learning",
                    "#audio"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¯Ğ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ±Ñ€ĞµÑ‚Ğ°ÑÑ‚ Ğ·Ñ€ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ»ÑƒÑ… Ñ‡ĞµÑ€ĞµĞ· Ñ‡Ñ‚ĞµĞ½Ğ¸Ğµ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ¼ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ… Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ²ĞµÑĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ´Ğ»Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…. Ğ”Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ñ… Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… ÑÑ…ĞµĞ¼Ğ°Ñ…, Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¸ Ğ¸Ñ… Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ½ÑƒĞ»Ñ."
                },
                "en": {
                    "title": "Unlocking Multi-Modal Understanding with Text LLMs",
                    "desc": "This paper explores how auto-regressive language models (LLMs) trained solely on text can develop the ability to understand and classify images and audio without needing additional fine-tuning. The authors demonstrate that these text-based models can process inputs like image patches and audio waveforms, producing embeddings or category labels similar to those used in traditional classification tasks. They validate their findings by applying the model to audio classification tasks on datasets like FSD-50K and GTZAN, as well as image classification on CIFAR-10 and Fashion-MNIST. This research highlights the potential of leveraging text LLMs' internal capabilities for multi-modal applications, reducing the need for training separate models for each modality."
                },
                "zh": {
                    "title": "æ–‡æœ¬æ¨¡å‹çš„è·¨æ¨¡æ€ç†è§£èƒ½åŠ›",
                    "desc": "è¿™ç¯‡è®ºæ–‡å±•ç¤ºäº†ä¸€ä¸ªæœ‰è¶£çš„å‘ç°ï¼šé€šè¿‡å¯¹æ–‡æœ¬è¿›è¡Œè®­ç»ƒçš„è‡ªå›å½’è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿå†…åœ¨åœ°å‘å±•å‡ºç†è§£å›¾åƒå’ŒéŸ³é¢‘çš„èƒ½åŠ›ã€‚è¿™æ ·ï¼Œæ¨¡å‹åœ¨æ²¡æœ‰å¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œå°±èƒ½è¿›è¡Œè·¨æ¨¡æ€çš„åˆ†ç±»ä»»åŠ¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡è¾“å…¥å›¾åƒå—ã€éŸ³é¢‘æ³¢å½¢æˆ–æ ‡è®°ï¼Œç”Ÿæˆå…¸å‹çš„åˆ†ç±»ç®¡é“æ‰€éœ€çš„åµŒå…¥æˆ–ç±»åˆ«æ ‡ç­¾ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ–‡æœ¬æ¨¡å‹çš„æƒé‡åœ¨éŸ³é¢‘å’Œå›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­å…·æœ‰å¹¿æ³›çš„é€‚ç”¨æ€§ï¼Œæ¨åŠ¨äº†æ–‡æœ¬è¯­è¨€æ¨¡å‹å­¦ä¹ å¼ºå¤§å†…éƒ¨ç”µè·¯çš„æ¦‚å¿µã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.17063",
            "title": "Synthetic Data RL: Task Definition Is All You Need",
            "url": "https://huggingface.co/papers/2505.17063",
            "abstract": "Synthetic Data RL enhances foundation models through reinforcement learning using only synthetic data, achieving performance comparable to models trained with full human-labeled data.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) is a powerful way to adapt foundation models to specialized tasks, but its reliance on large-scale human-labeled data limits broad adoption. We introduce Synthetic Data RL, a simple and general framework that reinforcement fine-tunes models using only synthetic data generated from a task definition. Our method first generates question and answer pairs from the task definition and retrieved documents, then adapts the difficulty of the question based on model solvability, and selects questions using the average pass rate of the model across samples for RL training. On Qwen-2.5-7B, our method achieves a 29.2% absolute improvement over the base model on GSM8K (+2.9 pp vs. instruction-tuned, +6.6 pp vs. Self-Instruct), 8.7% on MATH, 13.1% on GPQA (+7.0 pp vs. SynthLLM), 8.9% on MedQA, 17.7% on CQA (law) and 13.7% on CFA (finance). It surpasses supervised fine-tuning under the same data budget and nearly matches RL with full human data across datasets (e.g., +17.2 pp on GSM8K). Adding 100 human demonstrations improves the performance of GSM8K only by 0.4 pp, showing a limited added value. By reducing human data annotation, Synthetic Data RL enables scalable and efficient RL-based model adaptation. Code and demos are available at https://github.com/gydpku/Data_Synthesis_RL/.",
            "score": 1,
            "issue_id": 3947,
            "pub_date": "2025-05-18",
            "pub_date_card": {
                "ru": "18 Ğ¼Ğ°Ñ",
                "en": "May 18",
                "zh": "5æœˆ18æ—¥"
            },
            "hash": "a46b0456dc458ebe",
            "authors": [
                "Yiduo Guo",
                "Zhen Guo",
                "Chuanwei Huang",
                "Zi-Ang Wang",
                "Zekai Zhang",
                "Haofei Yu",
                "Huishuai Zhang",
                "Yikang Shen"
            ],
            "affiliations": [
                "MIT",
                "MIT-IBM",
                "Peking University",
                "UIUC"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17063.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#rl",
                    "#optimization",
                    "#training",
                    "#synthetic"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Synthetic Data RL, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ°Ñ€Ñ‹ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ¸Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. Synthetic Data RL Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¾Ğ±ÑŠĞµĞ¼Ğ°Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Reinforcement Learning with Synthetic Data: A Game Changer for Model Training!",
                    "desc": "Synthetic Data RL is a novel approach that enhances foundation models using reinforcement learning (RL) without the need for extensive human-labeled data. It generates synthetic question and answer pairs based on task definitions, allowing for effective model fine-tuning. The method adapts question difficulty according to the model's performance, optimizing the training process. Results show significant performance improvements on various benchmarks, demonstrating that this approach can achieve results comparable to traditional RL methods that rely on human data."
                },
                "zh": {
                    "title": "åˆæˆæ•°æ®å¼ºåŒ–å­¦ä¹ ï¼šé«˜æ•ˆæå‡æ¨¡å‹æ€§èƒ½çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºåˆæˆæ•°æ®å¼ºåŒ–å­¦ä¹ ï¼ˆSynthetic Data RLï¼‰çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡ä½¿ç”¨åˆæˆæ•°æ®æ¥å¢å¼ºåŸºç¡€æ¨¡å‹çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•ç”Ÿæˆä¸ä»»åŠ¡å®šä¹‰ç›¸å…³çš„é—®é¢˜å’Œç­”æ¡ˆå¯¹ï¼Œå¹¶æ ¹æ®æ¨¡å‹çš„è§£å†³èƒ½åŠ›è°ƒæ•´é—®é¢˜çš„éš¾åº¦ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒSynthetic Data RLåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç”šè‡³æ¥è¿‘äºä½¿ç”¨å…¨äººç±»æ ‡æ³¨æ•°æ®çš„å¼ºåŒ–å­¦ä¹ æ¨¡å‹ã€‚æ­¤æ–¹æ³•å‡å°‘äº†å¯¹äººç±»æ•°æ®æ ‡æ³¨çš„ä¾èµ–ï¼Œä½¿å¾—æ¨¡å‹é€‚åº”å˜å¾—æ›´åŠ é«˜æ•ˆå’Œå¯æ‰©å±•ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-05-23.html",
    "link_next": "2025-05-27.html",
    "link_month": "2025-05.html",
    "short_date_prev": {
        "ru": "23.05",
        "en": "05/23",
        "zh": "5æœˆ23æ—¥"
    },
    "short_date_next": {
        "ru": "27.05",
        "en": "05/27",
        "zh": "5æœˆ27æ—¥"
    },
    "categories": {
        "#dataset": 5,
        "#data": 2,
        "#benchmark": 4,
        "#agents": 1,
        "#cv": 1,
        "#rl": 6,
        "#rlhf": 6,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 0,
        "#audio": 3,
        "#video": 1,
        "#multimodal": 3,
        "#math": 3,
        "#multilingual": 1,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 11,
        "#robotics": 0,
        "#agi": 1,
        "#games": 0,
        "#interpretability": 2,
        "#reasoning": 6,
        "#transfer_learning": 3,
        "#graphs": 0,
        "#ethics": 2,
        "#security": 2,
        "#optimization": 9,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 3,
        "#long_context": 0,
        "#synthetic": 3,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 1
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰å¦‚ä½•åŠ é€Ÿç§‘å­¦ç ”ç©¶èŒƒå¼çš„è½¬å˜ï¼Œæé«˜ç ”ç©¶æ•ˆç‡å¹¶æ¨åŠ¨åˆ›æ–°ã€‚ä½œè€…æå‡ºäº†NovelSeekï¼Œä¸€ä¸ªç»Ÿä¸€çš„é—­ç¯å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œç”¨äºåœ¨å„ç§ç§‘å­¦ç ”ç©¶é¢†åŸŸä¸­è¿›è¡Œè‡ªä¸»ç§‘å­¦ç ”ç©¶ï¼ˆASRï¼‰ã€‚NovelSeekå…·æœ‰ä¸‰ä¸ªå…³é”®ä¼˜åŠ¿ï¼šå¯æ‰©å±•æ€§ã€äº’åŠ¨æ€§å’Œé«˜æ•ˆæ€§ã€‚ä¾‹å¦‚ï¼Œåœ¨ååº”æ”¶ç‡é¢„æµ‹ä¸­ï¼Œå®ƒåœ¨12å°æ—¶å†…æé«˜äº†7.8%ï¼›åœ¨å¢å¼ºæ´»æ€§é¢„æµ‹ä¸­ï¼Œå‡†ç¡®æ€§åœ¨4å°æ—¶å†…æé«˜äº†0.27ï¼›åœ¨2Dè¯­ä¹‰åˆ†å‰²ä¸­ï¼Œç²¾åº¦åœ¨30å°æ—¶å†…æé«˜äº†2.2%ã€‚",
        "title": "NovelSeek: When Agent Becomes the Scientist -- Building Closed-Loop\n  System from Hypothesis to Verification",
        "pinyin": "ZhÃ¨ piÄn wÃ©nzhÄng jiÃ¨shÃ o le rÃ©ngÅng zhÃ¬nÃ©ng (AI) rÃºhÃ© jiÄsÃ¹ kÄ“xuÃ© yÃ¡njiÅ« fÃ nshÃ¬ de zhuÇnbiÃ n, tÃ­gÄo yÃ¡njiÅ« xiÃ olÇœ bÃ¬ng tuÄ«dÃ²ng chuÃ ngxÄ«n. ZuÃ²zhÄ› tÃ­chÅ« le NovelSeek, yÄ«gÃ¨ tÇ’ngyÄ« de bÃ¬huÃ¡n duÅ zhÃ¬nÃ©ngtÇ kuÃ ngjiÃ , yÃ²ngyÃº zÃ i gÃ¨zhÇ’ng kÄ“xuÃ© yÃ¡njiÅ« lÇngyÃ¹ zhÅng jÃ¬nxÃ­ng zÃ¬zhÇ” kÄ“xuÃ© yÃ¡njiÅ« (ASR). NovelSeek jÃ¹yÇ’u sÄn gÃ¨ guÇnjiÃ n yÅushÃ¬: kÄ› kuÃ²zhÇn xÃ¬ng, hÃ¹dÃ²ng xÃ¬ng hÃ© gÄoxiÃ o xÃ¬ng. LÃ¬rÃº, zÃ i fÇnyÃ¬ng shÅulÇœ yÃ¹cÃ¨ zhÅng, tÄ zÃ i 12 xiÇoshÃ­ nÃ¨i tÃ­gÄo le 7.8%; zÃ i zÄ“ngqiÃ¡ng huÃ³xÃ¬ng yÃ¹cÃ¨ zhÅng, zhÇ”nquÃ¨ xÃ¬ng zÃ i 4 xiÇoshÃ­ nÃ¨i tÃ­gÄo le 0.27; zÃ i 2D yÇ”yÃ¡n fÄ“n'gÃ© zhÅng, jÄ«ngdÃ¹ zÃ i 30 xiÇoshÃ­ nÃ¨i tÃ­gÄo le 2.2%.",
        "vocab": "[\n    {\"word\": \"äººå·¥æ™ºèƒ½\", \"pinyin\": \"rÃ©ngÅng zhÃ¬nÃ©ng\", \"trans\": \"artificial intelligence\"},\n    {\"word\": \"èŒƒå¼\", \"pinyin\": \"fÃ nshÃ¬\", \"trans\": \"paradigm\"},\n    {\"word\": \"è½¬å˜\", \"pinyin\": \"zhuÇnbiÃ n\", \"trans\": \"transformation\"},\n    {\"word\": \"æé«˜\", \"pinyin\": \"tÃ­gÄo\", \"trans\": \"improve\"},\n    {\"word\": \"æ•ˆç‡\", \"pinyin\": \"xiÃ olÇœ\", \"trans\": \"efficiency\"},\n    {\"word\": \"æ¨åŠ¨\", \"pinyin\": \"tuÄ«dÃ²ng\", \"trans\": \"promote\"},\n    {\"word\": \"åˆ›æ–°\", \"pinyin\": \"chuÃ ngxÄ«n\", \"trans\": \"innovation\"},\n    {\"word\": \"ç»Ÿä¸€\", \"pinyin\": \"tÇ’ngyÄ«\", \"trans\": \"unified\"},\n    {\"word\": \"é—­ç¯\", \"pinyin\": \"bÃ¬huÃ¡n\", \"trans\": \"closed-loop\"},\n    {\"word\": \"å¤šæ™ºèƒ½ä½“\", \"pinyin\": \"duÅ zhÃ¬nÃ©ngtÇ\", \"trans\": \"multi-agent\"},\n    {\"word\": \"æ¡†æ¶\", \"pinyin\": \"kuÃ ngjiÃ \", \"trans\": \"framework\"},\n    {\"word\": \"è‡ªä¸»\", \"pinyin\": \"zÃ¬zhÇ”\", \"trans\": \"autonomous\"},\n    {\"word\": \"é¢†åŸŸ\", \"pinyin\": \"lÇngyÃ¹\", \"trans\": \"field\"},\n    {\"word\": \"å…³é”®\", \"pinyin\": \"guÇnjiÃ n\", \"trans\": \"key\"},\n    {\"word\": \"ä¼˜åŠ¿\", \"pinyin\": \"yÅushÃ¬\", \"trans\": \"advantage\"},\n    {\"word\": \"å¯æ‰©å±•æ€§\", \"pinyin\": \"kÄ› kuÃ²zhÄn xÃ¬ng\", \"trans\": \"scalability\"},\n    {\"word\": \"äº’åŠ¨æ€§\", \"pinyin\": \"hÃ¹dÃ²ng xÃ¬ng\", \"trans\": \"interactivity\"},\n    {\"word\": \"é«˜æ•ˆæ€§\", \"pinyin\": \"gÄoxiÃ o xÃ¬ng\", \"trans\": \"efficiency\"},\n    {\"word\": \"ååº”\", \"pinyin\": \"fÇnyÃ¬ng\", \"trans\": \"reaction\"},\n    {\"word\": \"æ”¶ç‡\", \"pinyin\": \"shÅulÇœ\", \"trans\": \"yield\"},\n    {\"word\": \"é¢„æµ‹\", \"pinyin\": \"yÃ¹cÃ¨\", \"trans\": \"prediction\"},\n    {\"word\": \"å¢å¼º\", \"pinyin\": \"zÄ“ngqiÃ¡ng\", \"trans\": \"enhance\"},\n    {\"word\": \"æ´»æ€§\", \"pinyin\": \"huÃ³xÃ¬ng\", \"trans\": \"activity\"},\n    {\"word\": \"å‡†ç¡®æ€§\", \"pinyin\": \"zhÇ”nquÃ¨ xÃ¬ng\", \"trans\": \"accuracy\"},\n    {\"word\": \"2D\", \"pinyin\": \"Ã¨r wÃ©i\", \"trans\": \"2D\"},\n    {\"word\": \"è¯­ä¹‰\", \"pinyin\": \"yÇ”yÃ¬\", \"trans\": \"semantic\"},\n    {\"word\": \"åˆ†å‰²\", \"pinyin\": \"fÄ“ngÄ“\", \"trans\": \"segmentation\"},\n    {\"word\": \"ç²¾åº¦\", \"pinyin\": \"jÄ«ngdÃ¹\", \"trans\": \"precision\"}\n]",
        "trans": "This article discusses how artificial intelligence (AI) is accelerating the transformation of scientific research paradigms, enhancing research efficiency, and driving innovation. The authors introduce NovelSeek, a unified closed-loop multi-agent framework for autonomous scientific research (ASR) across various scientific domains. NovelSeek offers three key advantages: scalability, interactivity, and efficiency. For instance, in reaction yield prediction, it improved accuracy by 7.8% within 12 hours; in enhanced activity prediction, accuracy increased by 0.27 within 4 hours; and in 2D semantic segmentation, precision improved by 2.2% within 30 hours.",
        "update_ts": "2025-05-25 12:45"
    }
}