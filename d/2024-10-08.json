{
    "date": "8 –æ–∫—Ç—è–±—Ä—è",
    "issue_id": 34,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2410.04717",
            "title": "$\\textbf{Only-IF}$:Revealing the Decisive Effect of Instruction Diversity on Generalization",
            "url": "https://huggingface.co/papers/2410.04717",
            "abstract": "Understanding and accurately following instructions is critical for large language models (LLMs) to be effective across diverse tasks. In this work, we rigorously examine the key factors that enable models to generalize to unseen instructions, providing insights to guide the collection of data for instruction-tuning. Through controlled experiments, inspired by the Turing-complete Markov algorithm, we demonstrate that such generalization only emerges when training data is diversified enough across semantic domains. Our findings also reveal that merely diversifying within limited domains fails to ensure robust generalization. In contrast, cross-domain data diversification, even under constrained data budgets, significantly enhances a model's adaptability. We further extend our analysis to real-world scenarios, including fine-tuning of $textbf{specialist} and textbf{generalist}$ models. In both cases, we demonstrate that 1) better performance can be achieved by increasing the diversity of an established dataset while keeping the data size constant, and 2) when scaling up the data, diversifying the semantics of instructions is more effective than simply increasing the quantity of similar data. Our research provides important insights for dataset collation, particularly when optimizing model performance by expanding training data for both specialist and generalist scenarios. We show that careful consideration of data diversification is key: training specialist models with data extending beyond their core domain leads to significant performance improvements, while generalist models benefit from diverse data mixtures that enhance their overall instruction-following capabilities across a wide range of applications. Our results highlight the critical role of strategic diversification and offer clear guidelines for improving data quality.",
            "score": 13,
            "issue_id": 25,
            "pub_date": "2024-10-07",
            "pub_date_ru": "7 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —Ñ–∞–∫—Ç–æ—Ä—ã, –ø–æ–∑–≤–æ–ª—è—é—â–∏–µ —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º –æ–±–æ–±—â–∞—Ç—å –Ω–∞–≤—ã–∫–∏ –Ω–∞ –Ω–æ–≤—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–æ–¥—è—Ç —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—â–∏–µ, —á—Ç–æ —Ç–∞–∫–æ–µ –æ–±–æ–±—â–µ–Ω–∏–µ –≤–æ–∑–Ω–∏–∫–∞–µ—Ç —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–∏ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –ø–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º –¥–æ–º–µ–Ω–∞–º. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –¥–∏–≤–µ—Ä—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –º–µ–∂–¥—É –¥–æ–º–µ–Ω–∞–º–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç –∞–¥–∞–ø—Ç–∏–≤–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–∏–º–µ–Ω–∏–º—ã –∫–∞–∫ –¥–ª—è —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö, —Ç–∞–∫ –∏ –¥–ª—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–æ–π –¥–∏–≤–µ—Ä—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏.",
                "tags": [
                    "#instruction-tuning",
                    "#cross-domain-generalization",
                    "#data-diversity"
                ],
                "emoji": "üß†",
                "title": "–†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –¥–∞–Ω–Ω—ã—Ö - –∫–ª—é—á –∫ –æ–±–æ–±—â–µ–Ω–∏—é –Ω–∞–≤—ã–∫–æ–≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.04199",
            "title": "LongGenBench: Long-context Generation Benchmark",
            "url": "https://huggingface.co/papers/2410.04199",
            "abstract": "Current long-context benchmarks primarily focus on retrieval-based tests, requiring Large Language Models (LLMs) to locate specific information within extensive input contexts, such as the needle-in-a-haystack (NIAH) benchmark. Long-context generation refers to the ability of a language model to generate coherent and contextually accurate text that spans across lengthy passages or documents. While recent studies show strong performance on NIAH and other retrieval-based long-context benchmarks, there is a significant lack of benchmarks for evaluating long-context generation capabilities. To bridge this gap and offer a comprehensive assessment, we introduce a synthetic benchmark, LongGenBench, which allows for flexible configurations of customized generation context lengths. LongGenBench advances beyond traditional benchmarks by redesigning the format of questions and necessitating that LLMs respond with a single, cohesive long-context answer. Upon extensive evaluation using LongGenBench, we observe that: (1) both API accessed and open source models exhibit performance degradation in long-context generation scenarios, ranging from 1.2% to 47.1%; (2) different series of LLMs exhibit varying trends of performance degradation, with the Gemini-1.5-Flash model showing the least degradation among API accessed models, and the Qwen2 series exhibiting the least degradation in LongGenBench among open source models.",
            "score": 10,
            "issue_id": 25,
            "pub_date": "2024-10-05",
            "pub_date_ru": "5 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "LongGenBench - —ç—Ç–æ –Ω–æ–≤—ã–π —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ç–µ—Å—Ç–æ–≤, —Ñ–æ–∫—É—Å–∏—Ä—É—é—â–∏—Ö—Å—è –Ω–∞ –ø–æ–∏—Å–∫–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, LongGenBench —Ç—Ä–µ–±—É–µ—Ç –æ—Ç –º–æ–¥–µ–ª–µ–π —Å–æ–∑–¥–∞–Ω–∏—è —Ü–µ–ª–æ—Å—Ç–Ω—ã—Ö –¥–ª–∏–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑–∞–ª–æ, —á—Ç–æ –≤—Å–µ –º–æ–¥–µ–ª–∏, –∫–∞–∫ API-–¥–æ—Å—Ç—É–ø–Ω—ã–µ, —Ç–∞–∫ –∏ —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–¥–æ–º, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —Å–Ω–∏–∂–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –¥–ª–∏–Ω–Ω—ã–º–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞–º–∏. –ú–æ–¥–µ–ª–∏ Gemini-1.5-Flash –∏ Qwen2 –ø–æ–∫–∞–∑–∞–ª–∏ –Ω–∞–∏–º–µ–Ω—å—à—É—é –¥–µ–≥—Ä–∞–¥–∞—Ü–∏—é —Å—Ä–µ–¥–∏ API-–º–æ–¥–µ–ª–µ–π –∏ –º–æ–¥–µ–ª–µ–π —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –∫–æ–¥–æ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ.",
                "tags": [
                    "#LongGenBench",
                    "#–î–ª–∏–Ω–Ω–æ–∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∞—è–ì–µ–Ω–µ—Ä–∞—Ü–∏—è",
                    "#–û—Ü–µ–Ω–∫–∞–Ø–∑—ã–∫–æ–≤—ã—Ö–ú–æ–¥–µ–ª–µ–π"
                ],
                "emoji": "üìè",
                "title": "LongGenBench: –Ω–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –æ—Ü–µ–Ω–∫–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.01912",
            "title": "A Spark of Vision-Language Intelligence: 2-Dimensional Autoregressive Transformer for Efficient Finegrained Image Generation",
            "url": "https://huggingface.co/papers/2410.01912",
            "abstract": "This work tackles the information loss bottleneck of vector-quantization (VQ) autoregressive image generation by introducing a novel model architecture called the 2-Dimensional Autoregression (DnD) Transformer. The DnD-Transformer predicts more codes for an image by introducing a new autoregression direction, model depth, along with the sequence length direction. Compared to traditional 1D autoregression and previous work utilizing similar 2D image decomposition such as RQ-Transformer, the DnD-Transformer is an end-to-end model that can generate higher quality images with the same backbone model size and sequence length, opening a new optimization perspective for autoregressive image generation. Furthermore, our experiments reveal that the DnD-Transformer's potential extends beyond generating natural images. It can even generate images with rich text and graphical elements in a self-supervised manner, demonstrating an understanding of these combined modalities. This has not been previously demonstrated for popular vision generative models such as diffusion models, showing a spark of vision-language intelligence when trained solely on images. Code, datasets and models are open at https://github.com/chenllliang/DnD-Transformer.",
            "score": 8,
            "issue_id": 28,
            "pub_date": "2024-10-02",
            "pub_date_ru": "2 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –º–æ–¥–µ–ª–∏ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º 2-Dimensional Autoregression (DnD) Transformer –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. DnD-Transformer —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –ø–æ—Ç–µ—Ä–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø—Ä–∏ –≤–µ–∫—Ç–æ—Ä–Ω–æ–º –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–∏, –≤–≤–æ–¥—è –Ω–æ–≤–æ–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–∏ - –≥–ª—É–±–∏–Ω—É –º–æ–¥–µ–ª–∏. –ü–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏, DnD-Transformer –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–∏ —Ç–æ–π –∂–µ –¥–ª–∏–Ω–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –ú–æ–¥–µ–ª—å —Ç–∞–∫–∂–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å —Ç–µ–∫—Å—Ç–æ–º –∏ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–º–∏ —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏, –ø–æ–∫–∞–∑—ã–≤–∞—è –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π.",
                "tags": [
                    "#–≤–µ–∫—Ç–æ—Ä–Ω–æ–µ-–∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ",
                    "#–∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏—è",
                    "#–º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—å"
                ],
                "emoji": "üñºÔ∏è",
                "title": "DnD-Transformer: –ù–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.05193",
            "title": "RevisEval: Improving LLM-as-a-Judge via Response-Adapted References",
            "url": "https://huggingface.co/papers/2410.05193",
            "abstract": "With significant efforts in recent studies, LLM-as-a-Judge has become a cost-effective alternative to human evaluation for assessing the text generation quality in a wide range of tasks. However, there still remains a reliability gap between LLM-as-a-Judge and human evaluation. One important reason is the lack of guided oracles in the evaluation process. Motivated by the role of reference pervasively used in classic text evaluation, we introduce RevisEval, a novel text generation evaluation paradigm via the response-adapted references. RevisEval is driven by the key observation that an ideal reference should maintain the necessary relevance to the response to be evaluated. Specifically, RevisEval leverages the text revision capabilities of large language models (LLMs) to adaptively revise the response, then treat the revised text as the reference (response-adapted reference) for the subsequent evaluation. Extensive experiments demonstrate that RevisEval outperforms traditional reference-free and reference-based evaluation paradigms that use LLM-as-a-Judge across NLG tasks and open-ended instruction-following tasks. More importantly, our response-adapted references can further boost the classical text metrics, e.g., BLEU and BERTScore, compared to traditional references and even rival the LLM-as-a-Judge. A detailed analysis is also conducted to confirm RevisEval's effectiveness in bias reduction, the impact of inference cost, and reference relevance.",
            "score": 7,
            "issue_id": 25,
            "pub_date": "2024-10-07",
            "pub_date_ru": "7 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "RevisEval - —ç—Ç–æ –Ω–æ–≤–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ –æ—Ü–µ–Ω–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ–¥ –æ—Ç–≤–µ—Ç —ç—Ç–∞–ª–æ–Ω—ã. –ú–µ—Ç–æ–¥ –∑–∞–¥–µ–π—Å—Ç–≤—É–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø–µ—Ä–µ—Å–º–æ—Ç—Ä–∞ –æ—Ç–≤–µ—Ç–∞ –∏ —Å–æ–∑–¥–∞–Ω–∏—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ —ç—Ç–∞–ª–æ–Ω–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ RevisEval –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã –æ—Ü–µ–Ω–∫–∏ –±–µ–∑ —ç—Ç–∞–ª–æ–Ω–∞ –∏ —Å —ç—Ç–∞–ª–æ–Ω–æ–º, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–µ LLM –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Å—É–¥—å–∏. –ë–æ–ª–µ–µ —Ç–æ–≥–æ, –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —ç—Ç–∞–ª–æ–Ω—ã —É–ª—É—á—à–∞—é—Ç –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏ —Ç–µ–∫—Å—Ç–∞, —Ç–∞–∫–∏–µ –∫–∞–∫ BLEU –∏ BERTScore.",
                "tags": [
                    "#RevisEval",
                    "#TextGeneration",
                    "#EvaluationMetrics"
                ],
                "emoji": "üîÑ",
                "title": "RevisEval: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ—Ü–µ–Ω–∫–µ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.03290",
            "title": "Grounded-VideoLLM: Sharpening Fine-grained Temporal Grounding in Video Large Language Models",
            "url": "https://huggingface.co/papers/2410.03290",
            "abstract": "Video Large Language Models (Video-LLMs) have demonstrated remarkable capabilities in coarse-grained video understanding, however, they struggle with fine-grained temporal grounding. In this paper, we introduce Grounded-VideoLLM, a novel Video-LLM adept at perceiving and reasoning over specific video moments in a fine-grained manner. We identify that current Video-LLMs have limitations for fine-grained video understanding since they lack effective temporal modeling and timestamp representation. In light of this, we sharpen our model by incorporating (1) an additional temporal stream to encode the relationships between frames and (2) discrete temporal tokens enriched with specific time knowledge to represent timestamps. To optimize the training of Grounded-VideoLLM, we employ a multi-stage training scheme, beginning with simple video-captioning tasks and progressively introducing video temporal grounding tasks of increasing complexity. To further enhance Grounded-VideoLLM's temporal reasoning capability, we also curate a grounded VideoQA dataset by an automatic annotation pipeline. Extensive experiments demonstrate that Grounded-VideoLLM not only excels in fine-grained grounding tasks such as temporal sentence grounding, dense video captioning, and grounded VideoQA, but also shows great potential as a versatile video assistant for general video understanding.",
            "score": 5,
            "issue_id": 29,
            "pub_date": "2024-10-04",
            "pub_date_ru": "4 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "Grounded-VideoLLM - —ç—Ç–æ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å Video-LLM, —Å–ø–æ—Å–æ–±–Ω–∞—è –∫ –¥–µ—Ç–∞–ª—å–Ω–æ–º—É –ø–æ–Ω–∏–º–∞–Ω–∏—é –≤–∏–¥–µ–æ –∏ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø–æ—Ç–æ–∫ –¥–ª—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è —Å–≤—è–∑–µ–π –º–µ–∂–¥—É –∫–∞–¥—Ä–∞–º–∏ –∏ –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –¥–ª—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫. –û–±—É—á–µ–Ω–∏–µ –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è –ø–æ—ç—Ç–∞–ø–Ω–æ, –Ω–∞—á–∏–Ω–∞—è —Å –ø—Ä–æ—Å—Ç—ã—Ö –∑–∞–¥–∞—á –æ–ø–∏—Å–∞–Ω–∏—è –≤–∏–¥–µ–æ –∏ –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —É—Å–ª–æ–∂–Ω—è—è—Å—å –¥–æ –∑–∞–¥–∞—á –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å Grounded-VideoLLM –≤ –∑–∞–¥–∞—á–∞—Ö –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, –ø–ª–æ—Ç–Ω–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è –≤–∏–¥–µ–æ –∏ –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã —Å –ø—Ä–∏–≤—è–∑–∫–æ–π –∫–æ –≤—Ä–µ–º–µ–Ω–∏.",
                "tags": [
                    "#VideoLLM",
                    "#TemporalGrounding",
                    "#FinegrainedVideoUnderstanding"
                ],
                "emoji": "üé¨",
                "title": "–ì—Ä–∞–Ω—É–ª—è—Ü–∏—è –≤—Ä–µ–º–µ–Ω–∏: –Ω–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤–∏–¥–µ–æ –¥–ª—è –ò–ò"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.03864",
            "title": "DOTS: Learning to Reason Dynamically in LLMs via Optimal Reasoning Trajectories Search",
            "url": "https://huggingface.co/papers/2410.03864",
            "abstract": "Enhancing the capability of large language models (LLMs) in reasoning has gained significant attention in recent years. Previous studies have demonstrated the effectiveness of various prompting strategies in aiding LLMs in reasoning (called \"reasoning actions\"), such as step-by-step thinking, reflecting before answering, solving with programs, and their combinations. However, these approaches often applied static, predefined reasoning actions uniformly to all questions, without considering the specific characteristics of each question or the capability of the task-solving LLM. In this paper, we propose DOTS, an approach enabling LLMs to reason dynamically via optimal reasoning trajectory search, tailored to the specific characteristics of each question and the inherent capability of the task-solving LLM. Our approach involves three key steps: i) defining atomic reasoning action modules that can be composed into various reasoning action trajectories; ii) searching for the optimal action trajectory for each training question through iterative exploration and evaluation for the specific task-solving LLM; and iii) using the collected optimal trajectories to train an LLM to plan for the reasoning trajectories of unseen questions. In particular, we propose two learning paradigms, i.e., fine-tuning an external LLM as a planner to guide the task-solving LLM, or directly fine-tuning the task-solving LLM with an internalized capability for reasoning actions planning. Our experiments across eight reasoning tasks show that our method consistently outperforms static reasoning techniques and the vanilla instruction tuning approach. Further analysis reveals that our method enables LLMs to adjust their computation based on problem complexity, allocating deeper thinking and reasoning to harder problems.",
            "score": 4,
            "issue_id": 34,
            "pub_date": "2024-10-04",
            "pub_date_ru": "4 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç DOTS - –ø–æ–¥—Ö–æ–¥, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ —Ä–∞—Å—Å—É–∂–¥–∞—Ç—å –ø—É—Ç–µ–º –ø–æ–∏—Å–∫–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –ú–µ—Ç–æ–¥ –≤–∫–ª—é—á–∞–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∞—Ç–æ–º–∞—Ä–Ω—ã—Ö –º–æ–¥—É–ª–µ–π –¥–µ–π—Å—Ç–≤–∏–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –ø–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –¥–µ–π—Å—Ç–≤–∏–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞ –∏ –æ–±—É—á–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—é —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è –Ω–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ DOTS –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø–æ–¥—Ö–æ–¥ —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏. –ê–Ω–∞–ª–∏–∑ –≤—ã—è–≤–ª—è–µ—Ç, —á—Ç–æ –º–µ—Ç–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∑–∞–¥–∞—á–∏.",
                "tags": [
                    "#–¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ–†–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ",
                    "#–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–¢—Ä–∞–µ–∫—Ç–æ—Ä–∏–π",
                    "#–∞–¥–∞–ø—Ç–∏–≤–Ω–æ–µ–ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ"
                ],
                "emoji": "üß†",
                "title": "DOTS: –î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02705",
            "title": "ControlAR: Controllable Image Generation with Autoregressive Models",
            "url": "https://huggingface.co/papers/2410.02705",
            "abstract": "Autoregressive (AR) models have reformulated image generation as next-token prediction, demonstrating remarkable potential and emerging as strong competitors to diffusion models. However, control-to-image generation, akin to ControlNet, remains largely unexplored within AR models. Although a natural approach, inspired by advancements in Large Language Models, is to tokenize control images into tokens and prefill them into the autoregressive model before decoding image tokens, it still falls short in generation quality compared to ControlNet and suffers from inefficiency. To this end, we introduce ControlAR, an efficient and effective framework for integrating spatial controls into autoregressive image generation models. Firstly, we explore control encoding for AR models and propose a lightweight control encoder to transform spatial inputs (e.g., canny edges or depth maps) into control tokens. Then ControlAR exploits the conditional decoding method to generate the next image token conditioned on the per-token fusion between control and image tokens, similar to positional encodings. Compared to prefilling tokens, using conditional decoding significantly strengthens the control capability of AR models but also maintains the model's efficiency. Furthermore, the proposed ControlAR surprisingly empowers AR models with arbitrary-resolution image generation via conditional decoding and specific controls. Extensive experiments can demonstrate the controllability of the proposed ControlAR for the autoregressive control-to-image generation across diverse inputs, including edges, depths, and segmentation masks. Furthermore, both quantitative and qualitative results indicate that ControlAR surpasses previous state-of-the-art controllable diffusion models, e.g., ControlNet++. Code, models, and demo will soon be available at https://github.com/hustvl/ControlAR.",
            "score": 4,
            "issue_id": 28,
            "pub_date": "2024-10-03",
            "pub_date_ru": "3 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "ControlAR - —ç—Ç–æ –Ω–æ–≤—ã–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è –≤ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ª–µ–≥–∫–æ–≤–µ—Å–Ω—ã–π —ç–Ω–∫–æ–¥–µ—Ä –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã—Ö –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã. ControlAR –ø—Ä–∏–º–µ–Ω—è–µ—Ç —É—Å–ª–æ–≤–Ω–æ–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ—Å–ª–æ–π–Ω–æ–≥–æ —Å–ª–∏—è–Ω–∏—è –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã—Ö –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω—á–µ—Å–∫–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –º–æ–¥–µ–ª–∏ —É–ø—Ä–∞–≤–ª—è–µ–º–æ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏.",
                "tags": [
                    "#–∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–∞—è_–≥–µ–Ω–µ—Ä–∞—Ü–∏—è",
                    "#–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π_–∫–æ–Ω—Ç—Ä–æ–ª—å",
                    "#—É—Å–ª–æ–≤–Ω–æ–µ_–¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ"
                ],
                "emoji": "üé®",
                "title": "ControlAR: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ —É–ø—Ä–∞–≤–ª—è–µ–º–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.04422",
            "title": "Hyper-multi-step: The Truth Behind Difficult Long-context Tasks",
            "url": "https://huggingface.co/papers/2410.04422",
            "abstract": "Long-context language models (LCLM), characterized by their extensive context window, is becoming increasingly popular. Meanwhile, many long-context benchmarks present challenging tasks that even the most advanced LCLMs struggle to complete. However, the underlying sources of various challenging long-context tasks have seldom been studied. To bridge this gap, we conduct experiments to indicate their difficulty stems primarily from two basic issues: \"multi-matching retrieval,\" which requires the simultaneous retrieval of multiple items, and \"logic-based retrieval,\" which necessitates logical judgment within retrieval criteria. These two problems, while seemingly straightforward, actually exceed the capabilities of LCLMs because they are proven to be hyper-multi-step (demanding numerous steps to solve) in nature. This finding could explain why LLMs struggle with more advanced long-context tasks, providing a more accurate perspective for rethinking solutions for them.",
            "score": 4,
            "issue_id": 28,
            "pub_date": "2024-10-06",
            "pub_date_ru": "6 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º (LCLM) –∏ –∏—Ö —Ç—Ä—É–¥–Ω–æ—Å—Ç—è–º –≤ —Ä–µ—à–µ–Ω–∏–∏ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á. –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–∏–ª–∏ –¥–≤–µ –æ—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã: –º–Ω–æ–≥–æ–∫—Ä–∞—Ç–Ω–æ–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ –ª–æ–≥–∏—á–µ—Å–∫–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö. –≠—Ç–∏ –∑–∞–¥–∞—á–∏ –æ–∫–∞–∑—ã–≤–∞—é—Ç—Å—è –≥–∏–ø–µ—Ä–º–Ω–æ–≥–æ—Å—Ç—É–ø–µ–Ω—á–∞—Ç—ã–º–∏, —á—Ç–æ –ø—Ä–µ–≤—ã—à–∞–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö LCLM. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –æ–±—ä—è—Å–Ω—è—é—Ç, –ø–æ—á–µ–º—É —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –∏—Å–ø—ã—Ç—ã–≤–∞—é—Ç —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º–∏ –∑–∞–¥–∞—á–∞–º–∏ –Ω–∞ –¥–ª–∏–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ.",
                "tags": [
                    "#LCLM",
                    "#–º–Ω–æ–≥–æ—Å—Ç—É–ø–µ–Ω—á–∞—Ç—ã–µ–ó–∞–¥–∞—á–∏",
                    "#–∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"
                ],
                "emoji": "üß†",
                "title": "–†–∞—Å–∫—Ä—ã—Ç–∏–µ –∫–æ—Ä–Ω–µ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏: –ø–æ—á–µ–º—É —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —Å–ø–æ—Ç—ã–∫–∞—é—Ç—Å—è –Ω–∞ –¥–ª–∏–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02743",
            "title": "MA-RLHF: Reinforcement Learning from Human Feedback with Macro Actions",
            "url": "https://huggingface.co/papers/2410.02743",
            "abstract": "Reinforcement learning from human feedback (RLHF) has demonstrated effectiveness in aligning large language models (LLMs) with human preferences. However, token-level RLHF suffers from the credit assignment problem over long sequences, where delayed rewards make it challenging for the model to discern which actions contributed to successful outcomes. This hinders learning efficiency and slows convergence. In this paper, we propose MA-RLHF, a simple yet effective RLHF framework that incorporates macro actions -- sequences of tokens or higher-level language constructs -- into the learning process. By operating at this higher level of abstraction, our approach reduces the temporal distance between actions and rewards, facilitating faster and more accurate credit assignment. This results in more stable policy gradient estimates and enhances learning efficiency within each episode, all without increasing computational complexity during training or inference. We validate our approach through extensive experiments across various model sizes and tasks, including text summarization, dialogue generation, question answering, and program synthesis. Our method achieves substantial performance improvements over standard RLHF, with performance gains of up to 30% in text summarization and code generation, 18% in dialogue, and 8% in question answering tasks. Notably, our approach reaches parity with vanilla RLHF 1.7x to 2x faster in terms of training time and continues to outperform it with further training. We will make our code and data publicly available at https://github.com/ernie-research/MA-RLHF .",
            "score": 4,
            "issue_id": 26,
            "pub_date": "2024-10-03",
            "pub_date_ru": "3 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –æ—Ç —á–µ–ª–æ–≤–µ–∫–∞ (RLHF) –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ MA-RLHF –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–∞–∫—Ä–æ–¥–µ–π—Å—Ç–≤–∏—è - –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ç–æ–∫–µ–Ω–æ–≤ –∏–ª–∏ –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ - –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Ä–µ—à–∏—Ç—å –ø—Ä–æ–±–ª–µ–º—É –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è –∫—Ä–µ–¥–∏—Ç–∞ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –∏ –ø–æ–≤—ã—Å–∏—Ç—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º RLHF –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞.",
                "tags": [
                    "#RLHF",
                    "#–º–∞–∫—Ä–æ–¥–µ–π—Å—Ç–≤–∏—è",
                    "#–∫—Ä–µ–¥–∏—Ç–Ω–æ–µ–Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ"
                ],
                "emoji": "üöÄ",
                "title": "–£—Å–∫–æ—Ä–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é –º–∞–∫—Ä–æ–¥–µ–π—Å—Ç–≤–∏–π"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.05076",
            "title": "TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention",
            "url": "https://huggingface.co/papers/2410.05076",
            "abstract": "Large language models (LLMs) have driven significant advancements across diverse NLP tasks, with long-context models gaining prominence for handling extended inputs. However, the expanding key-value (KV) cache size required by Transformer architectures intensifies the memory constraints, particularly during the decoding phase, creating a significant bottleneck. Existing sparse attention mechanisms designed to address this bottleneck have two limitations: (1) they often fail to reliably identify the most relevant tokens for attention, and (2) they overlook the spatial coherence of token selection across consecutive Transformer layers, which can lead to performance degradation and substantial overhead in token selection. This paper introduces TidalDecode, a simple yet effective algorithm and system for fast and accurate LLM decoding through position persistent sparse attention. TidalDecode leverages the spatial coherence of tokens selected by existing sparse attention methods and introduces a few token selection layers that perform full attention to identify the tokens with the highest attention scores, while all other layers perform sparse attention with the pre-selected tokens. This design enables TidalDecode to substantially reduce the overhead of token selection for sparse attention without sacrificing the quality of the generated results. Evaluation on a diverse set of LLMs and tasks shows that TidalDecode closely matches the generative performance of full attention methods while reducing the LLM decoding latency by up to 2.1x.",
            "score": 2,
            "issue_id": 33,
            "pub_date": "2024-10-07",
            "pub_date_ru": "7 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç TidalDecode - –∞–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –∏ —Ç–æ—á–Ω–æ–≥–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Å –ø–æ–º–æ—â—å—é —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø–æ–∑–∏—Ü–∏–∏. TidalDecode –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—É—é –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å —Ç–æ–∫–µ–Ω–æ–≤, –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è, –∏ –≤–≤–æ–¥–∏—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–ª–æ–µ–≤ –≤—ã–±–æ—Ä–∞ —Ç–æ–∫–µ–Ω–æ–≤ —Å –ø–æ–ª–Ω—ã–º –≤–Ω–∏–º–∞–Ω–∏–µ–º. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∑–∏—Ç—å –Ω–∞–∫–ª–∞–¥–Ω—ã–µ —Ä–∞—Å—Ö–æ–¥—ã –Ω–∞ –≤—ã–±–æ—Ä —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –±–µ–∑ —É—â–µ—Ä–±–∞ –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö LLM –∏ –∑–∞–¥–∞—á–∞—Ö –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ TidalDecode —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–µ—Ç–æ–¥–æ–≤ —Å –ø–æ–ª–Ω—ã–º –≤–Ω–∏–º–∞–Ω–∏–µ–º, —É–º–µ–Ω—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫—É –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è LLM –¥–æ 2,1 —Ä–∞–∑–∞.",
                "tags": [
                    "#—Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–µ_–≤–Ω–∏–º–∞–Ω–∏–µ",
                    "#–¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ_LLM",
                    "#–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è_–ø–∞–º—è—Ç–∏"
                ],
                "emoji": "üåä",
                "title": "TidalDecode: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ LLM —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø–æ–∑–∏—Ü–∏–∏ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.03399",
            "title": "EBES: Easy Benchmarking for Event Sequences",
            "url": "https://huggingface.co/papers/2410.03399",
            "abstract": "Event sequences, characterized by irregular sampling intervals and a mix of categorical and numerical features, are common data structures in various real-world domains such as healthcare, finance, and user interaction logs. Despite advances in temporal data modeling techniques, there is no standardized benchmarks for evaluating their performance on event sequences. This complicates result comparison across different papers due to varying evaluation protocols, potentially misleading progress in this field. We introduce EBES, a comprehensive benchmarking tool with standardized evaluation scenarios and protocols, focusing on regression and classification problems with sequence-level targets. Our library simplifies benchmarking, dataset addition, and method integration through a unified interface. It includes a novel synthetic dataset and provides preprocessed real-world datasets, including the largest publicly available banking dataset. Our results provide an in-depth analysis of datasets, identifying some as unsuitable for model comparison. We investigate the importance of modeling temporal and sequential components, as well as the robustness and scaling properties of the models. These findings highlight potential directions for future research. Our benchmark aim is to facilitate reproducible research, expediting progress and increasing real-world impacts.",
            "score": 2,
            "issue_id": 30,
            "pub_date": "2024-10-04",
            "pub_date_ru": "4 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç EBES - –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –±–µ–Ω—á–º–∞—Ä–∫–∏–Ω–≥–∞ –º–æ–¥–µ–ª–µ–π, —Ä–∞–±–æ—Ç–∞—é—â–∏—Ö —Å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è–º–∏ —Å–æ–±—ã—Ç–∏–π. EBES —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –∑–∞–¥–∞—á–∞—Ö —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å —Ü–µ–ª–µ–≤—ã–º–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –≤–∫–ª—é—á–∞–µ—Ç —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, –≤ —Ç–æ–º —á–∏—Å–ª–µ –∫—Ä—É–ø–Ω–µ–π—à–∏–π –ø—É–±–ª–∏—á–Ω–æ –¥–æ—Å—Ç—É–ø–Ω—ã–π –±–∞–Ω–∫–æ–≤—Å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç. –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤, –∞ —Ç–∞–∫–∂–µ –∏—Å—Å–ª–µ–¥—É—é—Ç —Ä–æ–±–∞—Å—Ç–Ω–æ—Å—Ç—å –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π.",
                "tags": [
                    "#EventSequenceModeling",
                    "#BenchmarkingTool",
                    "#TemporalDataAnalysis"
                ],
                "emoji": "üìä",
                "title": "EBES: –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π —Å–æ–±—ã—Ç–∏–π"
            }
        }
    ],
    "weekday": 1,
    "link_prev": "2024-10-07.html",
    "link_next": "2024-10-09.html",
    "time_utc": "2024-10-08 20:26"
}