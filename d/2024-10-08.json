{
    "date": "8 октября",
    "issue_id": 34,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2410.04717",
            "title": "$\\textbf{Only-IF}$:Revealing the Decisive Effect of Instruction Diversity on Generalization",
            "url": "https://huggingface.co/papers/2410.04717",
            "abstract": "Understanding and accurately following instructions is critical for large language models (LLMs) to be effective across diverse tasks. In this work, we rigorously examine the key factors that enable models to generalize to unseen instructions, providing insights to guide the collection of data for instruction-tuning. Through controlled experiments, inspired by the Turing-complete Markov algorithm, we demonstrate that such generalization only emerges when training data is diversified enough across semantic domains. Our findings also reveal that merely diversifying within limited domains fails to ensure robust generalization. In contrast, cross-domain data diversification, even under constrained data budgets, significantly enhances a model's adaptability. We further extend our analysis to real-world scenarios, including fine-tuning of $textbf{specialist} and textbf{generalist}$ models. In both cases, we demonstrate that 1) better performance can be achieved by increasing the diversity of an established dataset while keeping the data size constant, and 2) when scaling up the data, diversifying the semantics of instructions is more effective than simply increasing the quantity of similar data. Our research provides important insights for dataset collation, particularly when optimizing model performance by expanding training data for both specialist and generalist scenarios. We show that careful consideration of data diversification is key: training specialist models with data extending beyond their core domain leads to significant performance improvements, while generalist models benefit from diverse data mixtures that enhance their overall instruction-following capabilities across a wide range of applications. Our results highlight the critical role of strategic diversification and offer clear guidelines for improving data quality.",
            "score": 13,
            "issue_id": 25,
            "pub_date": "2024-10-07",
            "pub_date_ru": "7 октября",
            "data": {
                "categories": [
                    "#dataset",
                    "#data"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Разнообразие данных - ключ к обобщению навыков языковых моделей",
                    "desc": "Статья исследует факторы, позволяющие языковым моделям обобщать навыки на новые инструкции. Авторы проводят эксперименты, демонстрирующие, что такое обобщение возникает только при достаточном разнообразии обучающих данных по семантическим доменам. Исследование показывает, что диверсификация данных между доменами значительно повышает адаптивность модели. Результаты применимы как для специализированных, так и для универсальных моделей, подчеркивая важность стратегической диверсификации данных при обучении."
                },
                "en": {
                    "title": "\"Diversity Drives Understanding: Enhancing LLMs with Cross-Domain Data\"",
                    "desc": "This paper explores how large language models (LLMs) can better understand and follow instructions by examining the factors that help them generalize to new tasks. The study shows that models perform better when trained on data that is diverse across different semantic domains, rather than just within a single domain. By diversifying the types of instructions in the training data, even with limited data, models become more adaptable and effective. The research provides guidelines for improving model performance by strategically diversifying training data, benefiting both specialist and generalist models."
                },
                "zh": {
                    "title": "数据多样化：提升模型泛化能力的关键",
                    "desc": "这篇论文探讨了如何让大型语言模型更好地理解和执行未见过的指令。研究表明，只有在训练数据在语义领域上足够多样化时，模型才能实现良好的泛化。通过实验，作者发现跨领域的数据多样化，即使在数据量有限的情况下，也能显著提高模型的适应性。论文还指出，增加数据的语义多样性比单纯增加相似数据的数量更有效。"
                }
            },
            "hash": "30a0e92854b70969"
        },
        {
            "id": "https://huggingface.co/papers/2410.04199",
            "title": "LongGenBench: Long-context Generation Benchmark",
            "url": "https://huggingface.co/papers/2410.04199",
            "abstract": "Current long-context benchmarks primarily focus on retrieval-based tests, requiring Large Language Models (LLMs) to locate specific information within extensive input contexts, such as the needle-in-a-haystack (NIAH) benchmark. Long-context generation refers to the ability of a language model to generate coherent and contextually accurate text that spans across lengthy passages or documents. While recent studies show strong performance on NIAH and other retrieval-based long-context benchmarks, there is a significant lack of benchmarks for evaluating long-context generation capabilities. To bridge this gap and offer a comprehensive assessment, we introduce a synthetic benchmark, LongGenBench, which allows for flexible configurations of customized generation context lengths. LongGenBench advances beyond traditional benchmarks by redesigning the format of questions and necessitating that LLMs respond with a single, cohesive long-context answer. Upon extensive evaluation using LongGenBench, we observe that: (1) both API accessed and open source models exhibit performance degradation in long-context generation scenarios, ranging from 1.2% to 47.1%; (2) different series of LLMs exhibit varying trends of performance degradation, with the Gemini-1.5-Flash model showing the least degradation among API accessed models, and the Qwen2 series exhibiting the least degradation in LongGenBench among open source models.",
            "score": 10,
            "issue_id": 25,
            "pub_date": "2024-10-05",
            "pub_date_ru": "5 октября",
            "data": {
                "categories": [
                    "#benchmark"
                ],
                "emoji": "📏",
                "ru": {
                    "title": "LongGenBench: новый рубеж в оценке генерации длинных текстов",
                    "desc": "LongGenBench - это новый синтетический бенчмарк для оценки способности языковых моделей к генерации длинных контекстов. В отличие от существующих тестов, фокусирующихся на поиске информации, LongGenBench требует от моделей создания целостных длинных ответов. Исследование показало, что все модели, как API-доступные, так и с открытым исходным кодом, демонстрируют снижение производительности при работе с длинными контекстами. Модели Gemini-1.5-Flash и Qwen2 показали наименьшую деградацию среди API-моделей и моделей с открытым кодом соответственно."
                },
                "en": {
                    "title": "\"LongGenBench: Elevating LLMs to New Contextual Heights\"",
                    "desc": "The paper introduces LongGenBench, a new benchmark designed to evaluate the long-context generation capabilities of Large Language Models (LLMs). Unlike existing benchmarks that focus on retrieval tasks, LongGenBench assesses how well LLMs can generate coherent and contextually accurate text over long passages. The study finds that many models, both API-based and open source, show significant performance drops in long-context generation, with some models like Gemini-1.5-Flash and Qwen2 series performing better than others. This benchmark aims to fill the gap in evaluating LLMs' ability to handle extended text generation, providing a more comprehensive understanding of their capabilities."
                },
                "zh": {
                    "title": "突破长文本生成的评估瓶颈",
                    "desc": "这篇论文介绍了一种新的基准测试，名为LongGenBench，用于评估大型语言模型在长文本生成中的表现。传统的基准测试主要关注信息检索能力，而LongGenBench则专注于生成连贯且上下文准确的长文本。研究发现，不同的模型在长文本生成中表现不同，其中Gemini-1.5-Flash和Qwen2系列模型的性能下降最小。通过这种新的测试方法，可以更全面地评估模型的长文本生成能力。"
                }
            },
            "hash": "12ea9c1effd189a8"
        },
        {
            "id": "https://huggingface.co/papers/2410.01912",
            "title": "A Spark of Vision-Language Intelligence: 2-Dimensional Autoregressive Transformer for Efficient Finegrained Image Generation",
            "url": "https://huggingface.co/papers/2410.01912",
            "abstract": "This work tackles the information loss bottleneck of vector-quantization (VQ) autoregressive image generation by introducing a novel model architecture called the 2-Dimensional Autoregression (DnD) Transformer. The DnD-Transformer predicts more codes for an image by introducing a new autoregression direction, model depth, along with the sequence length direction. Compared to traditional 1D autoregression and previous work utilizing similar 2D image decomposition such as RQ-Transformer, the DnD-Transformer is an end-to-end model that can generate higher quality images with the same backbone model size and sequence length, opening a new optimization perspective for autoregressive image generation. Furthermore, our experiments reveal that the DnD-Transformer's potential extends beyond generating natural images. It can even generate images with rich text and graphical elements in a self-supervised manner, demonstrating an understanding of these combined modalities. This has not been previously demonstrated for popular vision generative models such as diffusion models, showing a spark of vision-language intelligence when trained solely on images. Code, datasets and models are open at https://github.com/chenllliang/DnD-Transformer.",
            "score": 8,
            "issue_id": 28,
            "pub_date": "2024-10-02",
            "pub_date_ru": "2 октября",
            "data": {
                "categories": [
                    "#architecture",
                    "#cv",
                    "#multimodal"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "DnD-Transformer: Новый взгляд на авторегрессивную генерацию изображений",
                    "desc": "Статья представляет новую архитектуру модели под названием 2-Dimensional Autoregression (DnD) Transformer для генерации изображений. DnD-Transformer решает проблему потери информации при векторном квантовании, вводя новое направление авторегрессии - глубину модели. По сравнению с традиционными методами, DnD-Transformer генерирует изображения более высокого качества при той же длине последовательности. Модель также демонстрирует способность генерировать изображения с текстом и графическими элементами, показывая понимание комбинированных модальностей."
                },
                "en": {
                    "title": "\"DnD-Transformer: Elevating Image Generation with Depth and Direction\"",
                    "desc": "The paper introduces the DnD-Transformer, a new model architecture for autoregressive image generation that addresses the information loss bottleneck of vector-quantization. By adding a new autoregression direction, the model predicts more codes for an image, improving image quality without increasing model size. Unlike previous models, the DnD-Transformer can generate images with complex text and graphical elements, showcasing a unique understanding of combined modalities. This advancement suggests a new level of vision-language intelligence in image generation models."
                },
                "zh": {
                    "title": "DnD-Transformer：突破图像生成的自回归新视角",
                    "desc": "这篇论文提出了一种新的模型架构，称为二维自回归（DnD）Transformer，以解决矢量量化自回归图像生成中的信息损失瓶颈。DnD-Transformer通过引入新的自回归方向和模型深度，预测图像的更多编码。与传统的一维自回归和类似的二维图像分解方法相比，DnD-Transformer能够在相同的模型大小和序列长度下生成更高质量的图像。实验表明，DnD-Transformer不仅能生成自然图像，还能在自监督的情况下生成包含丰富文本和图形元素的图像，展示了对这些组合模态的理解。"
                }
            },
            "hash": "63855d0547d5d08f"
        },
        {
            "id": "https://huggingface.co/papers/2410.05193",
            "title": "RevisEval: Improving LLM-as-a-Judge via Response-Adapted References",
            "url": "https://huggingface.co/papers/2410.05193",
            "abstract": "With significant efforts in recent studies, LLM-as-a-Judge has become a cost-effective alternative to human evaluation for assessing the text generation quality in a wide range of tasks. However, there still remains a reliability gap between LLM-as-a-Judge and human evaluation. One important reason is the lack of guided oracles in the evaluation process. Motivated by the role of reference pervasively used in classic text evaluation, we introduce RevisEval, a novel text generation evaluation paradigm via the response-adapted references. RevisEval is driven by the key observation that an ideal reference should maintain the necessary relevance to the response to be evaluated. Specifically, RevisEval leverages the text revision capabilities of large language models (LLMs) to adaptively revise the response, then treat the revised text as the reference (response-adapted reference) for the subsequent evaluation. Extensive experiments demonstrate that RevisEval outperforms traditional reference-free and reference-based evaluation paradigms that use LLM-as-a-Judge across NLG tasks and open-ended instruction-following tasks. More importantly, our response-adapted references can further boost the classical text metrics, e.g., BLEU and BERTScore, compared to traditional references and even rival the LLM-as-a-Judge. A detailed analysis is also conducted to confirm RevisEval's effectiveness in bias reduction, the impact of inference cost, and reference relevance.",
            "score": 7,
            "issue_id": 25,
            "pub_date": "2024-10-07",
            "pub_date_ru": "7 октября",
            "data": {
                "categories": [
                    "#benchmark",
                    "#interpretability"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "RevisEval: Революция в оценке качества генерации текста",
                    "desc": "RevisEval - это новая парадигма оценки генерации текста, использующая адаптированные под ответ эталоны. Метод задействует возможности больших языковых моделей для пересмотра ответа и создания релевантного эталона. Эксперименты показывают, что RevisEval превосходит традиционные методы оценки без эталона и с эталоном, использующие LLM в качестве судьи. Более того, адаптированные эталоны улучшают классические метрики текста, такие как BLEU и BERTScore."
                },
                "en": {
                    "title": "RevisEval: Revolutionizing Text Evaluation with Adaptive References",
                    "desc": "The paper introduces RevisEval, a new method for evaluating text generation by using response-adapted references. This approach leverages large language models to revise responses, creating more relevant references for evaluation. RevisEval has been shown to outperform traditional evaluation methods, improving metrics like BLEU and BERTScore. The study also highlights RevisEval's ability to reduce bias and enhance evaluation reliability."
                },
                "zh": {
                    "title": "RevisEval：通过响应适应的参考提升文本评估",
                    "desc": "这篇论文介绍了一种新的文本生成评估方法，称为RevisEval，它通过响应适应的参考来提高评估的准确性。RevisEval利用大型语言模型的文本修订能力，将生成的文本进行适应性修订，然后将修订后的文本作为参考进行评估。实验表明，RevisEval在自然语言生成任务中优于传统的无参考和基于参考的评估方法。更重要的是，RevisEval的参考可以提升经典文本指标的表现，如BLEU和BERTScore，并且在某些情况下甚至可以媲美LLM-as-a-Judge。"
                }
            },
            "hash": "dcb3bbd44f2e2a94"
        },
        {
            "id": "https://huggingface.co/papers/2410.03290",
            "title": "Grounded-VideoLLM: Sharpening Fine-grained Temporal Grounding in Video Large Language Models",
            "url": "https://huggingface.co/papers/2410.03290",
            "abstract": "Video Large Language Models (Video-LLMs) have demonstrated remarkable capabilities in coarse-grained video understanding, however, they struggle with fine-grained temporal grounding. In this paper, we introduce Grounded-VideoLLM, a novel Video-LLM adept at perceiving and reasoning over specific video moments in a fine-grained manner. We identify that current Video-LLMs have limitations for fine-grained video understanding since they lack effective temporal modeling and timestamp representation. In light of this, we sharpen our model by incorporating (1) an additional temporal stream to encode the relationships between frames and (2) discrete temporal tokens enriched with specific time knowledge to represent timestamps. To optimize the training of Grounded-VideoLLM, we employ a multi-stage training scheme, beginning with simple video-captioning tasks and progressively introducing video temporal grounding tasks of increasing complexity. To further enhance Grounded-VideoLLM's temporal reasoning capability, we also curate a grounded VideoQA dataset by an automatic annotation pipeline. Extensive experiments demonstrate that Grounded-VideoLLM not only excels in fine-grained grounding tasks such as temporal sentence grounding, dense video captioning, and grounded VideoQA, but also shows great potential as a versatile video assistant for general video understanding.",
            "score": 5,
            "issue_id": 29,
            "pub_date": "2024-10-04",
            "pub_date_ru": "4 октября",
            "data": {
                "categories": [
                    "#video",
                    "#cv",
                    "#dataset"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Грануляция времени: новый уровень понимания видео для ИИ",
                    "desc": "Grounded-VideoLLM - это новая модель Video-LLM, способная к детальному пониманию видео и временной локализации. Модель использует дополнительный временной поток для кодирования связей между кадрами и дискретные временные токены для представления временных меток. Обучение проводится поэтапно, начиная с простых задач описания видео и постепенно усложняясь до задач временной локализации. Эксперименты показывают эффективность Grounded-VideoLLM в задачах временной локализации предложений, плотного описания видео и ответов на вопросы с привязкой ко времени."
                },
                "en": {
                    "title": "Mastering Time: Grounded-VideoLLM's Leap in Video Understanding",
                    "desc": "The paper introduces Grounded-VideoLLM, a new model designed to improve fine-grained temporal understanding in videos. It addresses the limitations of existing Video-LLMs by adding a temporal stream to better capture frame relationships and using discrete temporal tokens for precise timestamp representation. The model is trained using a multi-stage approach, starting with simple tasks and gradually tackling more complex temporal grounding challenges. Additionally, a specialized dataset is created to enhance the model's temporal reasoning, resulting in superior performance in tasks like temporal sentence grounding and dense video captioning."
                },
                "zh": {
                    "title": "细粒度视频理解的新突破",
                    "desc": "现有的视频大语言模型在理解视频的细节时存在困难，因为它们缺乏有效的时间建模和时间戳表示。为了解决这个问题，我们提出了一种新模型，Grounded-VideoLLM，通过增加时间流和离散时间标记来增强时间感知能力。我们采用多阶段训练策略，从简单的视频字幕任务开始，逐步引入复杂的视频时间定位任务。实验表明，Grounded-VideoLLM在细粒度任务中表现出色，并有潜力成为通用的视频助手。"
                }
            },
            "hash": "6bcaebbfbd863fb6"
        },
        {
            "id": "https://huggingface.co/papers/2410.03864",
            "title": "DOTS: Learning to Reason Dynamically in LLMs via Optimal Reasoning Trajectories Search",
            "url": "https://huggingface.co/papers/2410.03864",
            "abstract": "Enhancing the capability of large language models (LLMs) in reasoning has gained significant attention in recent years. Previous studies have demonstrated the effectiveness of various prompting strategies in aiding LLMs in reasoning (called \"reasoning actions\"), such as step-by-step thinking, reflecting before answering, solving with programs, and their combinations. However, these approaches often applied static, predefined reasoning actions uniformly to all questions, without considering the specific characteristics of each question or the capability of the task-solving LLM. In this paper, we propose DOTS, an approach enabling LLMs to reason dynamically via optimal reasoning trajectory search, tailored to the specific characteristics of each question and the inherent capability of the task-solving LLM. Our approach involves three key steps: i) defining atomic reasoning action modules that can be composed into various reasoning action trajectories; ii) searching for the optimal action trajectory for each training question through iterative exploration and evaluation for the specific task-solving LLM; and iii) using the collected optimal trajectories to train an LLM to plan for the reasoning trajectories of unseen questions. In particular, we propose two learning paradigms, i.e., fine-tuning an external LLM as a planner to guide the task-solving LLM, or directly fine-tuning the task-solving LLM with an internalized capability for reasoning actions planning. Our experiments across eight reasoning tasks show that our method consistently outperforms static reasoning techniques and the vanilla instruction tuning approach. Further analysis reveals that our method enables LLMs to adjust their computation based on problem complexity, allocating deeper thinking and reasoning to harder problems.",
            "score": 4,
            "issue_id": 34,
            "pub_date": "2024-10-04",
            "pub_date_ru": "4 октября",
            "data": {
                "categories": [
                    "#reasoning",
                    "#alignment"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "DOTS: Динамическое рассуждение для языковых моделей через оптимальный поиск траекторий",
                    "desc": "Статья представляет DOTS - подход, позволяющий языковым моделям динамически рассуждать путем поиска оптимальной траектории рассуждений. Метод включает определение атомарных модулей действий рассуждения, поиск оптимальной траектории действий для каждого вопроса и обучение языковой модели планированию траекторий рассуждений для новых вопросов. Эксперименты показывают, что DOTS превосходит статические методы рассуждения и стандартный подход с инструкциями. Анализ выявляет, что метод позволяет языковым моделям адаптировать вычисления в зависимости от сложности задачи."
                },
                "en": {
                    "title": "Dynamic Reasoning: Tailoring AI's Thought Process for Better Problem Solving",
                    "desc": "This paper introduces DOTS, a method to enhance large language models' reasoning by dynamically tailoring reasoning strategies to each question. Unlike previous static approaches, DOTS searches for the best reasoning path for each question, using atomic reasoning modules. The method involves training a language model to plan reasoning paths for new questions, either by using an external planner or by enhancing the model's internal planning capabilities. Experiments show that DOTS improves performance across various reasoning tasks by allowing models to allocate more resources to complex problems."
                },
                "zh": {
                    "title": "DOTS：动态推理路径搜索，提升语言模型推理能力",
                    "desc": "这篇论文提出了一种名为DOTS的方法，旨在提升大型语言模型（LLM）的推理能力。与以往使用静态推理策略不同，DOTS通过动态搜索最佳推理路径，根据每个问题的特性和LLM的能力进行调整。该方法包括定义基本推理动作模块、为每个问题寻找最佳推理路径，以及利用这些路径训练LLM以应对新问题。实验结果表明，DOTS在多个推理任务中表现优于传统方法，能够根据问题难度调整计算深度。"
                }
            },
            "hash": "c9a263348ca9157d"
        },
        {
            "id": "https://huggingface.co/papers/2410.02705",
            "title": "ControlAR: Controllable Image Generation with Autoregressive Models",
            "url": "https://huggingface.co/papers/2410.02705",
            "abstract": "Autoregressive (AR) models have reformulated image generation as next-token prediction, demonstrating remarkable potential and emerging as strong competitors to diffusion models. However, control-to-image generation, akin to ControlNet, remains largely unexplored within AR models. Although a natural approach, inspired by advancements in Large Language Models, is to tokenize control images into tokens and prefill them into the autoregressive model before decoding image tokens, it still falls short in generation quality compared to ControlNet and suffers from inefficiency. To this end, we introduce ControlAR, an efficient and effective framework for integrating spatial controls into autoregressive image generation models. Firstly, we explore control encoding for AR models and propose a lightweight control encoder to transform spatial inputs (e.g., canny edges or depth maps) into control tokens. Then ControlAR exploits the conditional decoding method to generate the next image token conditioned on the per-token fusion between control and image tokens, similar to positional encodings. Compared to prefilling tokens, using conditional decoding significantly strengthens the control capability of AR models but also maintains the model's efficiency. Furthermore, the proposed ControlAR surprisingly empowers AR models with arbitrary-resolution image generation via conditional decoding and specific controls. Extensive experiments can demonstrate the controllability of the proposed ControlAR for the autoregressive control-to-image generation across diverse inputs, including edges, depths, and segmentation masks. Furthermore, both quantitative and qualitative results indicate that ControlAR surpasses previous state-of-the-art controllable diffusion models, e.g., ControlNet++. Code, models, and demo will soon be available at https://github.com/hustvl/ControlAR.",
            "score": 4,
            "issue_id": 28,
            "pub_date": "2024-10-03",
            "pub_date_ru": "3 октября",
            "data": {
                "categories": [
                    "#cv",
                    "#architecture"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "ControlAR: Революция в управляемой генерации изображений",
                    "desc": "ControlAR - это новый эффективный метод для интеграции пространственного контроля в авторегрессионные модели генерации изображений. Он использует легковесный энкодер для преобразования пространственных входных данных в контрольные токены. ControlAR применяет условное декодирование для генерации следующего токена изображения на основе послойного слияния контрольных и изображенческих токенов. Этот подход позволяет генерировать изображения произвольного разрешения и превосходит предыдущие модели управляемой диффузии по качеству и эффективности."
                },
                "en": {
                    "title": "ControlAR: Elevating Image Generation with Precision and Efficiency",
                    "desc": "The paper introduces ControlAR, a new framework for autoregressive (AR) models that enhances control-to-image generation by integrating spatial controls. ControlAR uses a lightweight control encoder to convert spatial inputs like edges or depth maps into control tokens, which are then used in conditional decoding to improve image generation quality. This method allows AR models to generate images with better control and efficiency compared to previous methods like ControlNet. Experiments show that ControlAR not only improves controllability but also supports arbitrary-resolution image generation, outperforming existing diffusion models."
                },
                "zh": {
                    "title": "ControlAR：自回归图像生成的新控制力",
                    "desc": "这篇论文介绍了一种名为ControlAR的新框架，用于在自回归图像生成模型中集成空间控制。通过引入轻量级控制编码器，将空间输入转换为控制标记，并使用条件解码方法生成图像标记。与预填充标记的方法相比，条件解码显著增强了自回归模型的控制能力，同时保持了模型的效率。实验表明，ControlAR在多种输入条件下的可控性优于现有的可控扩散模型。"
                }
            },
            "hash": "455b112b5c885449"
        },
        {
            "id": "https://huggingface.co/papers/2410.04422",
            "title": "Hyper-multi-step: The Truth Behind Difficult Long-context Tasks",
            "url": "https://huggingface.co/papers/2410.04422",
            "abstract": "Long-context language models (LCLM), characterized by their extensive context window, is becoming increasingly popular. Meanwhile, many long-context benchmarks present challenging tasks that even the most advanced LCLMs struggle to complete. However, the underlying sources of various challenging long-context tasks have seldom been studied. To bridge this gap, we conduct experiments to indicate their difficulty stems primarily from two basic issues: \"multi-matching retrieval,\" which requires the simultaneous retrieval of multiple items, and \"logic-based retrieval,\" which necessitates logical judgment within retrieval criteria. These two problems, while seemingly straightforward, actually exceed the capabilities of LCLMs because they are proven to be hyper-multi-step (demanding numerous steps to solve) in nature. This finding could explain why LLMs struggle with more advanced long-context tasks, providing a more accurate perspective for rethinking solutions for them.",
            "score": 4,
            "issue_id": 28,
            "pub_date": "2024-10-06",
            "pub_date_ru": "6 октября",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#interpretability"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Раскрытие корней сложности: почему языковые модели спотыкаются на длинном контексте",
                    "desc": "Исследование посвящено языковым моделям с длинным контекстом (LCLM) и их трудностям в решении сложных задач. Авторы выявили две основные проблемы: многократное сопоставление при извлечении информации и логическое извлечение данных. Эти задачи оказываются гипермногоступенчатыми, что превышает возможности современных LCLM. Результаты исследования объясняют, почему языковые модели испытывают трудности с продвинутыми задачами на длинном контексте."
                },
                "en": {
                    "title": "Unraveling the Complexity of Long-Context Language Models",
                    "desc": "The paper explores why long-context language models (LCLMs) struggle with certain complex tasks. It identifies two main challenges: 'multi-matching retrieval,' which involves retrieving multiple items at once, and 'logic-based retrieval,' which requires logical reasoning. These tasks are hyper-multi-step, meaning they need many steps to solve, which exceeds the current capabilities of LCLMs. Understanding these challenges helps in rethinking how to improve LCLMs for better performance on advanced tasks."
                },
                "zh": {
                    "title": "揭示长上下文任务的隐藏挑战",
                    "desc": "这篇论文研究了长上下文语言模型（LCLM）在处理复杂任务时遇到的困难。研究发现，这些困难主要来自于两个基本问题：多匹配检索和基于逻辑的检索。尽管这些问题看似简单，但实际上需要超多步骤才能解决，超出了LCLM的能力范围。这一发现为重新思考解决方案提供了更准确的视角。"
                }
            },
            "hash": "6077c1d0003654bc"
        },
        {
            "id": "https://huggingface.co/papers/2410.02743",
            "title": "MA-RLHF: Reinforcement Learning from Human Feedback with Macro Actions",
            "url": "https://huggingface.co/papers/2410.02743",
            "abstract": "Reinforcement learning from human feedback (RLHF) has demonstrated effectiveness in aligning large language models (LLMs) with human preferences. However, token-level RLHF suffers from the credit assignment problem over long sequences, where delayed rewards make it challenging for the model to discern which actions contributed to successful outcomes. This hinders learning efficiency and slows convergence. In this paper, we propose MA-RLHF, a simple yet effective RLHF framework that incorporates macro actions -- sequences of tokens or higher-level language constructs -- into the learning process. By operating at this higher level of abstraction, our approach reduces the temporal distance between actions and rewards, facilitating faster and more accurate credit assignment. This results in more stable policy gradient estimates and enhances learning efficiency within each episode, all without increasing computational complexity during training or inference. We validate our approach through extensive experiments across various model sizes and tasks, including text summarization, dialogue generation, question answering, and program synthesis. Our method achieves substantial performance improvements over standard RLHF, with performance gains of up to 30% in text summarization and code generation, 18% in dialogue, and 8% in question answering tasks. Notably, our approach reaches parity with vanilla RLHF 1.7x to 2x faster in terms of training time and continues to outperform it with further training. We will make our code and data publicly available at https://github.com/ernie-research/MA-RLHF .",
            "score": 4,
            "issue_id": 26,
            "pub_date": "2024-10-03",
            "pub_date_ru": "3 октября",
            "data": {
                "categories": [
                    "#rlhf"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Ускоренное обучение языковых моделей с помощью макродействий",
                    "desc": "Статья представляет новый подход к обучению с подкреплением на основе обратной связи от человека (RLHF) для больших языковых моделей. Предложенный метод MA-RLHF использует макродействия - последовательности токенов или высокоуровневые языковые конструкции - для улучшения процесса обучения. Это позволяет решить проблему назначения кредита для длинных последовательностей и повысить эффективность обучения. Эксперименты показывают значительное улучшение производительности по сравнению со стандартным RLHF на различных задачах обработки естественного языка."
                },
                "en": {
                    "title": "\"Boosting Learning Efficiency with Macro Actions in RLHF\"",
                    "desc": "The paper introduces MA-RLHF, a new framework for reinforcement learning from human feedback that uses macro actions to improve learning efficiency. By focusing on sequences of tokens or higher-level constructs, the approach addresses the credit assignment problem, making it easier to link actions to rewards. This method enhances the stability of policy gradient estimates and speeds up training without adding computational complexity. Experiments show significant performance improvements in tasks like text summarization and dialogue generation compared to traditional RLHF methods."
                },
                "zh": {
                    "title": "通过宏观动作提升强化学习效率",
                    "desc": "这篇论文介绍了一种新的强化学习方法，称为MA-RLHF，通过引入宏观动作来提高学习效率。宏观动作是指一系列的词或更高层次的语言结构，这样可以缩短动作与奖励之间的时间距离。通过这种方法，模型能够更快、更准确地进行信用分配，从而提高学习效率。实验结果表明，这种方法在文本摘要、对话生成、问答和程序合成等任务中表现优异，训练时间也大大缩短。"
                }
            },
            "hash": "2db1c25c0ecd97fd"
        },
        {
            "id": "https://huggingface.co/papers/2410.05076",
            "title": "TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention",
            "url": "https://huggingface.co/papers/2410.05076",
            "abstract": "Large language models (LLMs) have driven significant advancements across diverse NLP tasks, with long-context models gaining prominence for handling extended inputs. However, the expanding key-value (KV) cache size required by Transformer architectures intensifies the memory constraints, particularly during the decoding phase, creating a significant bottleneck. Existing sparse attention mechanisms designed to address this bottleneck have two limitations: (1) they often fail to reliably identify the most relevant tokens for attention, and (2) they overlook the spatial coherence of token selection across consecutive Transformer layers, which can lead to performance degradation and substantial overhead in token selection. This paper introduces TidalDecode, a simple yet effective algorithm and system for fast and accurate LLM decoding through position persistent sparse attention. TidalDecode leverages the spatial coherence of tokens selected by existing sparse attention methods and introduces a few token selection layers that perform full attention to identify the tokens with the highest attention scores, while all other layers perform sparse attention with the pre-selected tokens. This design enables TidalDecode to substantially reduce the overhead of token selection for sparse attention without sacrificing the quality of the generated results. Evaluation on a diverse set of LLMs and tasks shows that TidalDecode closely matches the generative performance of full attention methods while reducing the LLM decoding latency by up to 2.1x.",
            "score": 2,
            "issue_id": 33,
            "pub_date": "2024-10-07",
            "pub_date_ru": "7 октября",
            "data": {
                "categories": [
                    "#inference",
                    "#optimization",
                    "#architecture"
                ],
                "emoji": "🌊",
                "ru": {
                    "title": "TidalDecode: Эффективное декодирование LLM с сохранением позиции разреженного внимания",
                    "desc": "Статья представляет TidalDecode - алгоритм для быстрого и точного декодирования больших языковых моделей (LLM) с помощью разреженного внимания с сохранением позиции. TidalDecode использует пространственную когерентность токенов, выбранных существующими методами разреженного внимания, и вводит несколько слоев выбора токенов с полным вниманием. Это позволяет значительно снизить накладные расходы на выбор токенов для разреженного внимания без ущерба для качества генерируемых результатов. Оценка на различных LLM и задачах показывает, что TidalDecode соответствует производительности методов с полным вниманием, уменьшая задержку декодирования LLM до 2,1 раза."
                },
                "en": {
                    "title": "\"TidalDecode: Streamlining LLM Decoding with Smart Sparse Attention\"",
                    "desc": "The paper addresses the memory constraints in large language models (LLMs) caused by the expanding key-value cache size during the decoding phase. It critiques existing sparse attention mechanisms for not effectively identifying relevant tokens and ignoring spatial coherence across Transformer layers. The authors propose TidalDecode, an algorithm that uses position persistent sparse attention to improve token selection efficiency without compromising performance. TidalDecode significantly reduces decoding latency while maintaining high-quality results, as demonstrated in various LLM tasks."
                },
                "zh": {
                    "title": "TidalDecode：提升大语言模型解码效率的新方法",
                    "desc": "这篇论文介绍了一种名为TidalDecode的新算法，用于提高大语言模型的解码效率。TidalDecode通过位置持久稀疏注意力机制，解决了现有稀疏注意力方法在选择相关词元时的不足。它在少数层中使用全注意力来识别高注意力分数的词元，而其他层则使用预选词元进行稀疏注意力。实验表明，TidalDecode在保持生成质量的同时，将解码延迟减少了最多2.1倍。"
                }
            },
            "hash": "299609c59ee293c9"
        },
        {
            "id": "https://huggingface.co/papers/2410.03399",
            "title": "EBES: Easy Benchmarking for Event Sequences",
            "url": "https://huggingface.co/papers/2410.03399",
            "abstract": "Event sequences, characterized by irregular sampling intervals and a mix of categorical and numerical features, are common data structures in various real-world domains such as healthcare, finance, and user interaction logs. Despite advances in temporal data modeling techniques, there is no standardized benchmarks for evaluating their performance on event sequences. This complicates result comparison across different papers due to varying evaluation protocols, potentially misleading progress in this field. We introduce EBES, a comprehensive benchmarking tool with standardized evaluation scenarios and protocols, focusing on regression and classification problems with sequence-level targets. Our library simplifies benchmarking, dataset addition, and method integration through a unified interface. It includes a novel synthetic dataset and provides preprocessed real-world datasets, including the largest publicly available banking dataset. Our results provide an in-depth analysis of datasets, identifying some as unsuitable for model comparison. We investigate the importance of modeling temporal and sequential components, as well as the robustness and scaling properties of the models. These findings highlight potential directions for future research. Our benchmark aim is to facilitate reproducible research, expediting progress and increasing real-world impacts.",
            "score": 2,
            "issue_id": 30,
            "pub_date": "2024-10-04",
            "pub_date_ru": "4 октября",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#data"
                ],
                "emoji": "📊",
                "ru": {
                    "title": "EBES: Стандартизация оценки моделей для последовательностей событий",
                    "desc": "Статья представляет EBES - инструмент для стандартизированного бенчмаркинга моделей, работающих с последовательностями событий. EBES фокусируется на задачах регрессии и классификации с целевыми переменными на уровне последовательности. Инструмент включает синтетический датасет и предобработанные реальные данные, в том числе крупнейший публично доступный банковский датасет. Авторы анализируют важность моделирования временных и последовательных компонентов, а также исследуют робастность и масштабируемость моделей."
                },
                "en": {
                    "title": "Standardizing Event Sequence Evaluation with EBES",
                    "desc": "The paper introduces EBES, a benchmarking tool designed to standardize the evaluation of machine learning models on event sequences, which are common in fields like healthcare and finance. EBES provides a unified interface for benchmarking, adding datasets, and integrating methods, making it easier to compare results across different studies. It includes both synthetic and real-world datasets, offering insights into which datasets are suitable for model comparison. The tool aims to promote reproducible research and accelerate progress in understanding the temporal and sequential aspects of event sequence data."
                },
                "zh": {
                    "title": "EBES：事件序列数据评估的标准化工具",
                    "desc": "这篇论文介绍了一种名为EBES的工具，用于标准化事件序列数据的评估。事件序列数据在医疗、金融等领域很常见，但缺乏统一的评估标准。EBES提供了一个统一的接口，简化了基准测试和方法集成。研究结果强调了时间和序列建模的重要性，并为未来研究指明了方向。"
                }
            },
            "hash": "c88e217d2ab5a78c"
        }
    ],
    "weekday": 1,
    "link_prev": "2024-10-07.html",
    "link_next": "2024-10-09.html",
    "time_utc": "2024-10-08 20:26",
    "date_en": "8 October",
    "date_prev": "7 октября",
    "date_next": "9 октября",
    "short_date_prev": "07.10",
    "short_date_next": "09.10",
    "categories": {
        "#dataset": 3,
        "#data": 2,
        "#benchmark": 4,
        "#agents": 0,
        "#cv": 3,
        "#rl": 0,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 3,
        "#medicine": 0,
        "#training": 3,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 2,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#quantum": 0,
        "#edge_computing": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0
    }
}