{
    "date": "8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
    "issue_id": 34,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2410.04717",
            "title": "$\\textbf{Only-IF}$:Revealing the Decisive Effect of Instruction Diversity on Generalization",
            "url": "https://huggingface.co/papers/2410.04717",
            "abstract": "Understanding and accurately following instructions is critical for large language models (LLMs) to be effective across diverse tasks. In this work, we rigorously examine the key factors that enable models to generalize to unseen instructions, providing insights to guide the collection of data for instruction-tuning. Through controlled experiments, inspired by the Turing-complete Markov algorithm, we demonstrate that such generalization only emerges when training data is diversified enough across semantic domains. Our findings also reveal that merely diversifying within limited domains fails to ensure robust generalization. In contrast, cross-domain data diversification, even under constrained data budgets, significantly enhances a model's adaptability. We further extend our analysis to real-world scenarios, including fine-tuning of $textbf{specialist} and textbf{generalist}$ models. In both cases, we demonstrate that 1) better performance can be achieved by increasing the diversity of an established dataset while keeping the data size constant, and 2) when scaling up the data, diversifying the semantics of instructions is more effective than simply increasing the quantity of similar data. Our research provides important insights for dataset collation, particularly when optimizing model performance by expanding training data for both specialist and generalist scenarios. We show that careful consideration of data diversification is key: training specialist models with data extending beyond their core domain leads to significant performance improvements, while generalist models benefit from diverse data mixtures that enhance their overall instruction-following capabilities across a wide range of applications. Our results highlight the critical role of strategic diversification and offer clear guidelines for improving data quality.",
            "score": 13,
            "issue_id": 25,
            "pub_date": "2024-10-07",
            "pub_date_ru": "7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#dataset",
                    "#data"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… - ĞºĞ»ÑÑ‡ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ñ‹, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°Ñ‚ÑŒ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ñ€Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¼ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ğ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¸Ğ²ĞµÑ€ÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ñ‹ ĞºĞ°Ğº Ğ´Ğ»Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°Ñ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ´Ğ¸Ğ²ĞµÑ€ÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸."
                },
                "en": {
                    "title": "\"Diversity Drives Understanding: Enhancing LLMs with Cross-Domain Data\"",
                    "desc": "This paper explores how large language models (LLMs) can better understand and follow instructions by examining the factors that help them generalize to new tasks. The study shows that models perform better when trained on data that is diverse across different semantic domains, rather than just within a single domain. By diversifying the types of instructions in the training data, even with limited data, models become more adaptable and effective. The research provides guidelines for improving model performance by strategically diversifying training data, benefiting both specialist and generalist models."
                },
                "zh": {
                    "title": "æ•°æ®å¤šæ ·åŒ–ï¼šæå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›çš„å…³é”®",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¦‚ä½•è®©å¤§å‹è¯­è¨€æ¨¡å‹æ›´å¥½åœ°ç†è§£å’Œæ‰§è¡Œæœªè§è¿‡çš„æŒ‡ä»¤ã€‚ç ”ç©¶è¡¨æ˜ï¼Œåªæœ‰åœ¨è®­ç»ƒæ•°æ®åœ¨è¯­ä¹‰é¢†åŸŸä¸Šè¶³å¤Ÿå¤šæ ·åŒ–æ—¶ï¼Œæ¨¡å‹æ‰èƒ½å®ç°è‰¯å¥½çš„æ³›åŒ–ã€‚é€šè¿‡å®éªŒï¼Œä½œè€…å‘ç°è·¨é¢†åŸŸçš„æ•°æ®å¤šæ ·åŒ–ï¼Œå³ä½¿åœ¨æ•°æ®é‡æœ‰é™çš„æƒ…å†µä¸‹ï¼Œä¹Ÿèƒ½æ˜¾è‘—æé«˜æ¨¡å‹çš„é€‚åº”æ€§ã€‚è®ºæ–‡è¿˜æŒ‡å‡ºï¼Œå¢åŠ æ•°æ®çš„è¯­ä¹‰å¤šæ ·æ€§æ¯”å•çº¯å¢åŠ ç›¸ä¼¼æ•°æ®çš„æ•°é‡æ›´æœ‰æ•ˆã€‚"
                }
            },
            "hash": "30a0e92854b70969"
        },
        {
            "id": "https://huggingface.co/papers/2410.04199",
            "title": "LongGenBench: Long-context Generation Benchmark",
            "url": "https://huggingface.co/papers/2410.04199",
            "abstract": "Current long-context benchmarks primarily focus on retrieval-based tests, requiring Large Language Models (LLMs) to locate specific information within extensive input contexts, such as the needle-in-a-haystack (NIAH) benchmark. Long-context generation refers to the ability of a language model to generate coherent and contextually accurate text that spans across lengthy passages or documents. While recent studies show strong performance on NIAH and other retrieval-based long-context benchmarks, there is a significant lack of benchmarks for evaluating long-context generation capabilities. To bridge this gap and offer a comprehensive assessment, we introduce a synthetic benchmark, LongGenBench, which allows for flexible configurations of customized generation context lengths. LongGenBench advances beyond traditional benchmarks by redesigning the format of questions and necessitating that LLMs respond with a single, cohesive long-context answer. Upon extensive evaluation using LongGenBench, we observe that: (1) both API accessed and open source models exhibit performance degradation in long-context generation scenarios, ranging from 1.2% to 47.1%; (2) different series of LLMs exhibit varying trends of performance degradation, with the Gemini-1.5-Flash model showing the least degradation among API accessed models, and the Qwen2 series exhibiting the least degradation in LongGenBench among open source models.",
            "score": 10,
            "issue_id": 25,
            "pub_date": "2024-10-05",
            "pub_date_ru": "5 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#benchmark"
                ],
                "emoji": "ğŸ“",
                "ru": {
                    "title": "LongGenBench: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²",
                    "desc": "LongGenBench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ². Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ², Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ…ÑÑ Ğ½Ğ° Ğ¿Ğ¾Ğ¸ÑĞºĞµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, LongGenBench Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¾Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ñ‹Ñ… Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ²ÑĞµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ĞºĞ°Ğº API-Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Gemini-1.5-Flash Ğ¸ Qwen2 Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ¸Ğ¼ĞµĞ½ÑŒÑˆÑƒÑ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ñ ÑÑ€ĞµĞ´Ğ¸ API-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾."
                },
                "en": {
                    "title": "\"LongGenBench: Elevating LLMs to New Contextual Heights\"",
                    "desc": "The paper introduces LongGenBench, a new benchmark designed to evaluate the long-context generation capabilities of Large Language Models (LLMs). Unlike existing benchmarks that focus on retrieval tasks, LongGenBench assesses how well LLMs can generate coherent and contextually accurate text over long passages. The study finds that many models, both API-based and open source, show significant performance drops in long-context generation, with some models like Gemini-1.5-Flash and Qwen2 series performing better than others. This benchmark aims to fill the gap in evaluating LLMs' ability to handle extended text generation, providing a more comprehensive understanding of their capabilities."
                },
                "zh": {
                    "title": "çªç ´é•¿æ–‡æœ¬ç”Ÿæˆçš„è¯„ä¼°ç“¶é¢ˆ",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„åŸºå‡†æµ‹è¯•ï¼Œåä¸ºLongGenBenchï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é•¿æ–‡æœ¬ç”Ÿæˆä¸­çš„è¡¨ç°ã€‚ä¼ ç»Ÿçš„åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨ä¿¡æ¯æ£€ç´¢èƒ½åŠ›ï¼Œè€ŒLongGenBenchåˆ™ä¸“æ³¨äºç”Ÿæˆè¿è´¯ä¸”ä¸Šä¸‹æ–‡å‡†ç¡®çš„é•¿æ–‡æœ¬ã€‚ç ”ç©¶å‘ç°ï¼Œä¸åŒçš„æ¨¡å‹åœ¨é•¿æ–‡æœ¬ç”Ÿæˆä¸­è¡¨ç°ä¸åŒï¼Œå…¶ä¸­Gemini-1.5-Flashå’ŒQwen2ç³»åˆ—æ¨¡å‹çš„æ€§èƒ½ä¸‹é™æœ€å°ã€‚é€šè¿‡è¿™ç§æ–°çš„æµ‹è¯•æ–¹æ³•ï¼Œå¯ä»¥æ›´å…¨é¢åœ°è¯„ä¼°æ¨¡å‹çš„é•¿æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ã€‚"
                }
            },
            "hash": "12ea9c1effd189a8"
        },
        {
            "id": "https://huggingface.co/papers/2410.01912",
            "title": "A Spark of Vision-Language Intelligence: 2-Dimensional Autoregressive Transformer for Efficient Finegrained Image Generation",
            "url": "https://huggingface.co/papers/2410.01912",
            "abstract": "This work tackles the information loss bottleneck of vector-quantization (VQ) autoregressive image generation by introducing a novel model architecture called the 2-Dimensional Autoregression (DnD) Transformer. The DnD-Transformer predicts more codes for an image by introducing a new autoregression direction, model depth, along with the sequence length direction. Compared to traditional 1D autoregression and previous work utilizing similar 2D image decomposition such as RQ-Transformer, the DnD-Transformer is an end-to-end model that can generate higher quality images with the same backbone model size and sequence length, opening a new optimization perspective for autoregressive image generation. Furthermore, our experiments reveal that the DnD-Transformer's potential extends beyond generating natural images. It can even generate images with rich text and graphical elements in a self-supervised manner, demonstrating an understanding of these combined modalities. This has not been previously demonstrated for popular vision generative models such as diffusion models, showing a spark of vision-language intelligence when trained solely on images. Code, datasets and models are open at https://github.com/chenllliang/DnD-Transformer.",
            "score": 8,
            "issue_id": 28,
            "pub_date": "2024-10-02",
            "pub_date_ru": "2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#architecture",
                    "#cv",
                    "#multimodal"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "DnD-Transformer: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 2-Dimensional Autoregression (DnD) Transformer Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. DnD-Transformer Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¼ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸, Ğ²Ğ²Ğ¾Ğ´Ñ Ğ½Ğ¾Ğ²Ğ¾Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¸ - Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞŸĞ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸, DnD-Transformer Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¸ Ñ‚Ğ¾Ğ¹ Ğ¶Ğµ Ğ´Ğ»Ğ¸Ğ½Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹."
                },
                "en": {
                    "title": "\"DnD-Transformer: Elevating Image Generation with Depth and Direction\"",
                    "desc": "The paper introduces the DnD-Transformer, a new model architecture for autoregressive image generation that addresses the information loss bottleneck of vector-quantization. By adding a new autoregression direction, the model predicts more codes for an image, improving image quality without increasing model size. Unlike previous models, the DnD-Transformer can generate images with complex text and graphical elements, showcasing a unique understanding of combined modalities. This advancement suggests a new level of vision-language intelligence in image generation models."
                },
                "zh": {
                    "title": "DnD-Transformerï¼šçªç ´å›¾åƒç”Ÿæˆçš„è‡ªå›å½’æ–°è§†è§’",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¨¡å‹æ¶æ„ï¼Œç§°ä¸ºäºŒç»´è‡ªå›å½’ï¼ˆDnDï¼‰Transformerï¼Œä»¥è§£å†³çŸ¢é‡é‡åŒ–è‡ªå›å½’å›¾åƒç”Ÿæˆä¸­çš„ä¿¡æ¯æŸå¤±ç“¶é¢ˆã€‚DnD-Transformeré€šè¿‡å¼•å…¥æ–°çš„è‡ªå›å½’æ–¹å‘å’Œæ¨¡å‹æ·±åº¦ï¼Œé¢„æµ‹å›¾åƒçš„æ›´å¤šç¼–ç ã€‚ä¸ä¼ ç»Ÿçš„ä¸€ç»´è‡ªå›å½’å’Œç±»ä¼¼çš„äºŒç»´å›¾åƒåˆ†è§£æ–¹æ³•ç›¸æ¯”ï¼ŒDnD-Transformerèƒ½å¤Ÿåœ¨ç›¸åŒçš„æ¨¡å‹å¤§å°å’Œåºåˆ—é•¿åº¦ä¸‹ç”Ÿæˆæ›´é«˜è´¨é‡çš„å›¾åƒã€‚å®éªŒè¡¨æ˜ï¼ŒDnD-Transformerä¸ä»…èƒ½ç”Ÿæˆè‡ªç„¶å›¾åƒï¼Œè¿˜èƒ½åœ¨è‡ªç›‘ç£çš„æƒ…å†µä¸‹ç”ŸæˆåŒ…å«ä¸°å¯Œæ–‡æœ¬å’Œå›¾å½¢å…ƒç´ çš„å›¾åƒï¼Œå±•ç¤ºäº†å¯¹è¿™äº›ç»„åˆæ¨¡æ€çš„ç†è§£ã€‚"
                }
            },
            "hash": "63855d0547d5d08f"
        },
        {
            "id": "https://huggingface.co/papers/2410.05193",
            "title": "RevisEval: Improving LLM-as-a-Judge via Response-Adapted References",
            "url": "https://huggingface.co/papers/2410.05193",
            "abstract": "With significant efforts in recent studies, LLM-as-a-Judge has become a cost-effective alternative to human evaluation for assessing the text generation quality in a wide range of tasks. However, there still remains a reliability gap between LLM-as-a-Judge and human evaluation. One important reason is the lack of guided oracles in the evaluation process. Motivated by the role of reference pervasively used in classic text evaluation, we introduce RevisEval, a novel text generation evaluation paradigm via the response-adapted references. RevisEval is driven by the key observation that an ideal reference should maintain the necessary relevance to the response to be evaluated. Specifically, RevisEval leverages the text revision capabilities of large language models (LLMs) to adaptively revise the response, then treat the revised text as the reference (response-adapted reference) for the subsequent evaluation. Extensive experiments demonstrate that RevisEval outperforms traditional reference-free and reference-based evaluation paradigms that use LLM-as-a-Judge across NLG tasks and open-ended instruction-following tasks. More importantly, our response-adapted references can further boost the classical text metrics, e.g., BLEU and BERTScore, compared to traditional references and even rival the LLM-as-a-Judge. A detailed analysis is also conducted to confirm RevisEval's effectiveness in bias reduction, the impact of inference cost, and reference relevance.",
            "score": 7,
            "issue_id": 25,
            "pub_date": "2024-10-07",
            "pub_date_ru": "7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#benchmark",
                    "#interpretability"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "RevisEval: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°",
                    "desc": "RevisEval - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´ Ğ¾Ñ‚Ğ²ĞµÑ‚ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ñ‹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ·Ğ°Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ğ° Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ RevisEval Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±ĞµĞ· ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ° Ğ¸ Ñ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ¾Ğ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğµ LLM Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑÑƒĞ´ÑŒĞ¸. Ğ‘Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ñ‹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ñ‚ĞµĞºÑÑ‚Ğ°, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº BLEU Ğ¸ BERTScore."
                },
                "en": {
                    "title": "RevisEval: Revolutionizing Text Evaluation with Adaptive References",
                    "desc": "The paper introduces RevisEval, a new method for evaluating text generation by using response-adapted references. This approach leverages large language models to revise responses, creating more relevant references for evaluation. RevisEval has been shown to outperform traditional evaluation methods, improving metrics like BLEU and BERTScore. The study also highlights RevisEval's ability to reduce bias and enhance evaluation reliability."
                },
                "zh": {
                    "title": "RevisEvalï¼šé€šè¿‡å“åº”é€‚åº”çš„å‚è€ƒæå‡æ–‡æœ¬è¯„ä¼°",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ–‡æœ¬ç”Ÿæˆè¯„ä¼°æ–¹æ³•ï¼Œç§°ä¸ºRevisEvalï¼Œå®ƒé€šè¿‡å“åº”é€‚åº”çš„å‚è€ƒæ¥æé«˜è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚RevisEvalåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æ–‡æœ¬ä¿®è®¢èƒ½åŠ›ï¼Œå°†ç”Ÿæˆçš„æ–‡æœ¬è¿›è¡Œé€‚åº”æ€§ä¿®è®¢ï¼Œç„¶åå°†ä¿®è®¢åçš„æ–‡æœ¬ä½œä¸ºå‚è€ƒè¿›è¡Œè¯„ä¼°ã€‚å®éªŒè¡¨æ˜ï¼ŒRevisEvalåœ¨è‡ªç„¶è¯­è¨€ç”Ÿæˆä»»åŠ¡ä¸­ä¼˜äºä¼ ç»Ÿçš„æ— å‚è€ƒå’ŒåŸºäºå‚è€ƒçš„è¯„ä¼°æ–¹æ³•ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒRevisEvalçš„å‚è€ƒå¯ä»¥æå‡ç»å…¸æ–‡æœ¬æŒ‡æ ‡çš„è¡¨ç°ï¼Œå¦‚BLEUå’ŒBERTScoreï¼Œå¹¶ä¸”åœ¨æŸäº›æƒ…å†µä¸‹ç”šè‡³å¯ä»¥åª²ç¾LLM-as-a-Judgeã€‚"
                }
            },
            "hash": "dcb3bbd44f2e2a94"
        },
        {
            "id": "https://huggingface.co/papers/2410.03290",
            "title": "Grounded-VideoLLM: Sharpening Fine-grained Temporal Grounding in Video Large Language Models",
            "url": "https://huggingface.co/papers/2410.03290",
            "abstract": "Video Large Language Models (Video-LLMs) have demonstrated remarkable capabilities in coarse-grained video understanding, however, they struggle with fine-grained temporal grounding. In this paper, we introduce Grounded-VideoLLM, a novel Video-LLM adept at perceiving and reasoning over specific video moments in a fine-grained manner. We identify that current Video-LLMs have limitations for fine-grained video understanding since they lack effective temporal modeling and timestamp representation. In light of this, we sharpen our model by incorporating (1) an additional temporal stream to encode the relationships between frames and (2) discrete temporal tokens enriched with specific time knowledge to represent timestamps. To optimize the training of Grounded-VideoLLM, we employ a multi-stage training scheme, beginning with simple video-captioning tasks and progressively introducing video temporal grounding tasks of increasing complexity. To further enhance Grounded-VideoLLM's temporal reasoning capability, we also curate a grounded VideoQA dataset by an automatic annotation pipeline. Extensive experiments demonstrate that Grounded-VideoLLM not only excels in fine-grained grounding tasks such as temporal sentence grounding, dense video captioning, and grounded VideoQA, but also shows great potential as a versatile video assistant for general video understanding.",
            "score": 5,
            "issue_id": 29,
            "pub_date": "2024-10-04",
            "pub_date_ru": "4 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#video",
                    "#cv",
                    "#dataset"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ“Ñ€Ğ°Ğ½ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ğ˜Ğ˜",
                    "desc": "Grounded-VideoLLM - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Video-LLM, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ Ğº Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ñ‚Ğ¾Ğº Ğ´Ğ»Ñ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ²ÑĞ·ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸ Ğ¸ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğº. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ¿Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾, Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°Ñ Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾ ÑƒÑĞ»Ğ¾Ğ¶Ğ½ÑÑÑÑŒ Ğ´Ğ¾ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Grounded-VideoLLM Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ñ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¾Ğ¹ ĞºĞ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸."
                },
                "en": {
                    "title": "Mastering Time: Grounded-VideoLLM's Leap in Video Understanding",
                    "desc": "The paper introduces Grounded-VideoLLM, a new model designed to improve fine-grained temporal understanding in videos. It addresses the limitations of existing Video-LLMs by adding a temporal stream to better capture frame relationships and using discrete temporal tokens for precise timestamp representation. The model is trained using a multi-stage approach, starting with simple tasks and gradually tackling more complex temporal grounding challenges. Additionally, a specialized dataset is created to enhance the model's temporal reasoning, resulting in superior performance in tasks like temporal sentence grounding and dense video captioning."
                },
                "zh": {
                    "title": "ç»†ç²’åº¦è§†é¢‘ç†è§£çš„æ–°çªç ´",
                    "desc": "ç°æœ‰çš„è§†é¢‘å¤§è¯­è¨€æ¨¡å‹åœ¨ç†è§£è§†é¢‘çš„ç»†èŠ‚æ—¶å­˜åœ¨å›°éš¾ï¼Œå› ä¸ºå®ƒä»¬ç¼ºä¹æœ‰æ•ˆçš„æ—¶é—´å»ºæ¨¡å’Œæ—¶é—´æˆ³è¡¨ç¤ºã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ¨¡å‹ï¼ŒGrounded-VideoLLMï¼Œé€šè¿‡å¢åŠ æ—¶é—´æµå’Œç¦»æ•£æ—¶é—´æ ‡è®°æ¥å¢å¼ºæ—¶é—´æ„ŸçŸ¥èƒ½åŠ›ã€‚æˆ‘ä»¬é‡‡ç”¨å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œä»ç®€å•çš„è§†é¢‘å­—å¹•ä»»åŠ¡å¼€å§‹ï¼Œé€æ­¥å¼•å…¥å¤æ‚çš„è§†é¢‘æ—¶é—´å®šä½ä»»åŠ¡ã€‚å®éªŒè¡¨æ˜ï¼ŒGrounded-VideoLLMåœ¨ç»†ç²’åº¦ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶æœ‰æ½œåŠ›æˆä¸ºé€šç”¨çš„è§†é¢‘åŠ©æ‰‹ã€‚"
                }
            },
            "hash": "6bcaebbfbd863fb6"
        },
        {
            "id": "https://huggingface.co/papers/2410.03864",
            "title": "DOTS: Learning to Reason Dynamically in LLMs via Optimal Reasoning Trajectories Search",
            "url": "https://huggingface.co/papers/2410.03864",
            "abstract": "Enhancing the capability of large language models (LLMs) in reasoning has gained significant attention in recent years. Previous studies have demonstrated the effectiveness of various prompting strategies in aiding LLMs in reasoning (called \"reasoning actions\"), such as step-by-step thinking, reflecting before answering, solving with programs, and their combinations. However, these approaches often applied static, predefined reasoning actions uniformly to all questions, without considering the specific characteristics of each question or the capability of the task-solving LLM. In this paper, we propose DOTS, an approach enabling LLMs to reason dynamically via optimal reasoning trajectory search, tailored to the specific characteristics of each question and the inherent capability of the task-solving LLM. Our approach involves three key steps: i) defining atomic reasoning action modules that can be composed into various reasoning action trajectories; ii) searching for the optimal action trajectory for each training question through iterative exploration and evaluation for the specific task-solving LLM; and iii) using the collected optimal trajectories to train an LLM to plan for the reasoning trajectories of unseen questions. In particular, we propose two learning paradigms, i.e., fine-tuning an external LLM as a planner to guide the task-solving LLM, or directly fine-tuning the task-solving LLM with an internalized capability for reasoning actions planning. Our experiments across eight reasoning tasks show that our method consistently outperforms static reasoning techniques and the vanilla instruction tuning approach. Further analysis reveals that our method enables LLMs to adjust their computation based on problem complexity, allocating deeper thinking and reasoning to harder problems.",
            "score": 4,
            "issue_id": 34,
            "pub_date": "2024-10-04",
            "pub_date_ru": "4 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#reasoning",
                    "#alignment"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "DOTS: Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ DOTS - Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ°Ñ‚Ğ¾Ğ¼Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¹ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ¸ÑĞº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ° Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ DOTS Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸."
                },
                "en": {
                    "title": "Dynamic Reasoning: Tailoring AI's Thought Process for Better Problem Solving",
                    "desc": "This paper introduces DOTS, a method to enhance large language models' reasoning by dynamically tailoring reasoning strategies to each question. Unlike previous static approaches, DOTS searches for the best reasoning path for each question, using atomic reasoning modules. The method involves training a language model to plan reasoning paths for new questions, either by using an external planner or by enhancing the model's internal planning capabilities. Experiments show that DOTS improves performance across various reasoning tasks by allowing models to allocate more resources to complex problems."
                },
                "zh": {
                    "title": "DOTSï¼šåŠ¨æ€æ¨ç†è·¯å¾„æœç´¢ï¼Œæå‡è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºDOTSçš„æ–¹æ³•ï¼Œæ—¨åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ä¸ä»¥å¾€ä½¿ç”¨é™æ€æ¨ç†ç­–ç•¥ä¸åŒï¼ŒDOTSé€šè¿‡åŠ¨æ€æœç´¢æœ€ä½³æ¨ç†è·¯å¾„ï¼Œæ ¹æ®æ¯ä¸ªé—®é¢˜çš„ç‰¹æ€§å’ŒLLMçš„èƒ½åŠ›è¿›è¡Œè°ƒæ•´ã€‚è¯¥æ–¹æ³•åŒ…æ‹¬å®šä¹‰åŸºæœ¬æ¨ç†åŠ¨ä½œæ¨¡å—ã€ä¸ºæ¯ä¸ªé—®é¢˜å¯»æ‰¾æœ€ä½³æ¨ç†è·¯å¾„ï¼Œä»¥åŠåˆ©ç”¨è¿™äº›è·¯å¾„è®­ç»ƒLLMä»¥åº”å¯¹æ–°é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDOTSåœ¨å¤šä¸ªæ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œèƒ½å¤Ÿæ ¹æ®é—®é¢˜éš¾åº¦è°ƒæ•´è®¡ç®—æ·±åº¦ã€‚"
                }
            },
            "hash": "c9a263348ca9157d"
        },
        {
            "id": "https://huggingface.co/papers/2410.02705",
            "title": "ControlAR: Controllable Image Generation with Autoregressive Models",
            "url": "https://huggingface.co/papers/2410.02705",
            "abstract": "Autoregressive (AR) models have reformulated image generation as next-token prediction, demonstrating remarkable potential and emerging as strong competitors to diffusion models. However, control-to-image generation, akin to ControlNet, remains largely unexplored within AR models. Although a natural approach, inspired by advancements in Large Language Models, is to tokenize control images into tokens and prefill them into the autoregressive model before decoding image tokens, it still falls short in generation quality compared to ControlNet and suffers from inefficiency. To this end, we introduce ControlAR, an efficient and effective framework for integrating spatial controls into autoregressive image generation models. Firstly, we explore control encoding for AR models and propose a lightweight control encoder to transform spatial inputs (e.g., canny edges or depth maps) into control tokens. Then ControlAR exploits the conditional decoding method to generate the next image token conditioned on the per-token fusion between control and image tokens, similar to positional encodings. Compared to prefilling tokens, using conditional decoding significantly strengthens the control capability of AR models but also maintains the model's efficiency. Furthermore, the proposed ControlAR surprisingly empowers AR models with arbitrary-resolution image generation via conditional decoding and specific controls. Extensive experiments can demonstrate the controllability of the proposed ControlAR for the autoregressive control-to-image generation across diverse inputs, including edges, depths, and segmentation masks. Furthermore, both quantitative and qualitative results indicate that ControlAR surpasses previous state-of-the-art controllable diffusion models, e.g., ControlNet++. Code, models, and demo will soon be available at https://github.com/hustvl/ControlAR.",
            "score": 4,
            "issue_id": 28,
            "pub_date": "2024-10-03",
            "pub_date_ru": "3 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#cv",
                    "#architecture"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "ControlAR: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "ControlAR - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹. ControlAR Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "ControlAR: Elevating Image Generation with Precision and Efficiency",
                    "desc": "The paper introduces ControlAR, a new framework for autoregressive (AR) models that enhances control-to-image generation by integrating spatial controls. ControlAR uses a lightweight control encoder to convert spatial inputs like edges or depth maps into control tokens, which are then used in conditional decoding to improve image generation quality. This method allows AR models to generate images with better control and efficiency compared to previous methods like ControlNet. Experiments show that ControlAR not only improves controllability but also supports arbitrary-resolution image generation, outperforming existing diffusion models."
                },
                "zh": {
                    "title": "ControlARï¼šè‡ªå›å½’å›¾åƒç”Ÿæˆçš„æ–°æ§åˆ¶åŠ›",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºControlARçš„æ–°æ¡†æ¶ï¼Œç”¨äºåœ¨è‡ªå›å½’å›¾åƒç”Ÿæˆæ¨¡å‹ä¸­é›†æˆç©ºé—´æ§åˆ¶ã€‚é€šè¿‡å¼•å…¥è½»é‡çº§æ§åˆ¶ç¼–ç å™¨ï¼Œå°†ç©ºé—´è¾“å…¥è½¬æ¢ä¸ºæ§åˆ¶æ ‡è®°ï¼Œå¹¶ä½¿ç”¨æ¡ä»¶è§£ç æ–¹æ³•ç”Ÿæˆå›¾åƒæ ‡è®°ã€‚ä¸é¢„å¡«å……æ ‡è®°çš„æ–¹æ³•ç›¸æ¯”ï¼Œæ¡ä»¶è§£ç æ˜¾è‘—å¢å¼ºäº†è‡ªå›å½’æ¨¡å‹çš„æ§åˆ¶èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒäº†æ¨¡å‹çš„æ•ˆç‡ã€‚å®éªŒè¡¨æ˜ï¼ŒControlARåœ¨å¤šç§è¾“å…¥æ¡ä»¶ä¸‹çš„å¯æ§æ€§ä¼˜äºç°æœ‰çš„å¯æ§æ‰©æ•£æ¨¡å‹ã€‚"
                }
            },
            "hash": "455b112b5c885449"
        },
        {
            "id": "https://huggingface.co/papers/2410.04422",
            "title": "Hyper-multi-step: The Truth Behind Difficult Long-context Tasks",
            "url": "https://huggingface.co/papers/2410.04422",
            "abstract": "Long-context language models (LCLM), characterized by their extensive context window, is becoming increasingly popular. Meanwhile, many long-context benchmarks present challenging tasks that even the most advanced LCLMs struggle to complete. However, the underlying sources of various challenging long-context tasks have seldom been studied. To bridge this gap, we conduct experiments to indicate their difficulty stems primarily from two basic issues: \"multi-matching retrieval,\" which requires the simultaneous retrieval of multiple items, and \"logic-based retrieval,\" which necessitates logical judgment within retrieval criteria. These two problems, while seemingly straightforward, actually exceed the capabilities of LCLMs because they are proven to be hyper-multi-step (demanding numerous steps to solve) in nature. This finding could explain why LLMs struggle with more advanced long-context tasks, providing a more accurate perspective for rethinking solutions for them.",
            "score": 4,
            "issue_id": 28,
            "pub_date": "2024-10-06",
            "pub_date_ru": "6 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#interpretability"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ ĞºĞ¾Ñ€Ğ½ĞµĞ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸: Ğ¿Ğ¾Ñ‡ĞµĞ¼Ñƒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¿Ğ¾Ñ‚Ñ‹ĞºĞ°ÑÑ‚ÑÑ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ (LCLM) Ğ¸ Ğ¸Ñ… Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚ÑĞ¼ Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ´Ğ²Ğµ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹: Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­Ñ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ÑÑ Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ñ‹Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LCLM. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑŠÑÑĞ½ÑÑÑ‚, Ğ¿Ğ¾Ñ‡ĞµĞ¼Ñƒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ."
                },
                "en": {
                    "title": "Unraveling the Complexity of Long-Context Language Models",
                    "desc": "The paper explores why long-context language models (LCLMs) struggle with certain complex tasks. It identifies two main challenges: 'multi-matching retrieval,' which involves retrieving multiple items at once, and 'logic-based retrieval,' which requires logical reasoning. These tasks are hyper-multi-step, meaning they need many steps to solve, which exceeds the current capabilities of LCLMs. Understanding these challenges helps in rethinking how to improve LCLMs for better performance on advanced tasks."
                },
                "zh": {
                    "title": "æ­ç¤ºé•¿ä¸Šä¸‹æ–‡ä»»åŠ¡çš„éšè—æŒ‘æˆ˜",
                    "desc": "è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†é•¿ä¸Šä¸‹æ–‡è¯­è¨€æ¨¡å‹ï¼ˆLCLMï¼‰åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶é‡åˆ°çš„å›°éš¾ã€‚ç ”ç©¶å‘ç°ï¼Œè¿™äº›å›°éš¾ä¸»è¦æ¥è‡ªäºä¸¤ä¸ªåŸºæœ¬é—®é¢˜ï¼šå¤šåŒ¹é…æ£€ç´¢å’ŒåŸºäºé€»è¾‘çš„æ£€ç´¢ã€‚å°½ç®¡è¿™äº›é—®é¢˜çœ‹ä¼¼ç®€å•ï¼Œä½†å®é™…ä¸Šéœ€è¦è¶…å¤šæ­¥éª¤æ‰èƒ½è§£å†³ï¼Œè¶…å‡ºäº†LCLMçš„èƒ½åŠ›èŒƒå›´ã€‚è¿™ä¸€å‘ç°ä¸ºé‡æ–°æ€è€ƒè§£å†³æ–¹æ¡ˆæä¾›äº†æ›´å‡†ç¡®çš„è§†è§’ã€‚"
                }
            },
            "hash": "6077c1d0003654bc"
        },
        {
            "id": "https://huggingface.co/papers/2410.02743",
            "title": "MA-RLHF: Reinforcement Learning from Human Feedback with Macro Actions",
            "url": "https://huggingface.co/papers/2410.02743",
            "abstract": "Reinforcement learning from human feedback (RLHF) has demonstrated effectiveness in aligning large language models (LLMs) with human preferences. However, token-level RLHF suffers from the credit assignment problem over long sequences, where delayed rewards make it challenging for the model to discern which actions contributed to successful outcomes. This hinders learning efficiency and slows convergence. In this paper, we propose MA-RLHF, a simple yet effective RLHF framework that incorporates macro actions -- sequences of tokens or higher-level language constructs -- into the learning process. By operating at this higher level of abstraction, our approach reduces the temporal distance between actions and rewards, facilitating faster and more accurate credit assignment. This results in more stable policy gradient estimates and enhances learning efficiency within each episode, all without increasing computational complexity during training or inference. We validate our approach through extensive experiments across various model sizes and tasks, including text summarization, dialogue generation, question answering, and program synthesis. Our method achieves substantial performance improvements over standard RLHF, with performance gains of up to 30% in text summarization and code generation, 18% in dialogue, and 8% in question answering tasks. Notably, our approach reaches parity with vanilla RLHF 1.7x to 2x faster in terms of training time and continues to outperform it with further training. We will make our code and data publicly available at https://github.com/ernie-research/MA-RLHF .",
            "score": 4,
            "issue_id": 26,
            "pub_date": "2024-10-03",
            "pub_date_ru": "3 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#rlhf"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ°ĞºÑ€Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¾Ñ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° (RLHF) Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ MA-RLHF Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ°ĞºÑ€Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ - Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸Ğ»Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ - Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€ĞµÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ ĞºÑ€ĞµĞ´Ğ¸Ñ‚Ğ° Ğ´Ğ»Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ñ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼ RLHF Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°."
                },
                "en": {
                    "title": "\"Boosting Learning Efficiency with Macro Actions in RLHF\"",
                    "desc": "The paper introduces MA-RLHF, a new framework for reinforcement learning from human feedback that uses macro actions to improve learning efficiency. By focusing on sequences of tokens or higher-level constructs, the approach addresses the credit assignment problem, making it easier to link actions to rewards. This method enhances the stability of policy gradient estimates and speeds up training without adding computational complexity. Experiments show significant performance improvements in tasks like text summarization and dialogue generation compared to traditional RLHF methods."
                },
                "zh": {
                    "title": "é€šè¿‡å®è§‚åŠ¨ä½œæå‡å¼ºåŒ–å­¦ä¹ æ•ˆç‡",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œç§°ä¸ºMA-RLHFï¼Œé€šè¿‡å¼•å…¥å®è§‚åŠ¨ä½œæ¥æé«˜å­¦ä¹ æ•ˆç‡ã€‚å®è§‚åŠ¨ä½œæ˜¯æŒ‡ä¸€ç³»åˆ—çš„è¯æˆ–æ›´é«˜å±‚æ¬¡çš„è¯­è¨€ç»“æ„ï¼Œè¿™æ ·å¯ä»¥ç¼©çŸ­åŠ¨ä½œä¸å¥–åŠ±ä¹‹é—´çš„æ—¶é—´è·ç¦»ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œæ¨¡å‹èƒ½å¤Ÿæ›´å¿«ã€æ›´å‡†ç¡®åœ°è¿›è¡Œä¿¡ç”¨åˆ†é…ï¼Œä»è€Œæé«˜å­¦ä¹ æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•åœ¨æ–‡æœ¬æ‘˜è¦ã€å¯¹è¯ç”Ÿæˆã€é—®ç­”å’Œç¨‹åºåˆæˆç­‰ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè®­ç»ƒæ—¶é—´ä¹Ÿå¤§å¤§ç¼©çŸ­ã€‚"
                }
            },
            "hash": "2db1c25c0ecd97fd"
        },
        {
            "id": "https://huggingface.co/papers/2410.05076",
            "title": "TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention",
            "url": "https://huggingface.co/papers/2410.05076",
            "abstract": "Large language models (LLMs) have driven significant advancements across diverse NLP tasks, with long-context models gaining prominence for handling extended inputs. However, the expanding key-value (KV) cache size required by Transformer architectures intensifies the memory constraints, particularly during the decoding phase, creating a significant bottleneck. Existing sparse attention mechanisms designed to address this bottleneck have two limitations: (1) they often fail to reliably identify the most relevant tokens for attention, and (2) they overlook the spatial coherence of token selection across consecutive Transformer layers, which can lead to performance degradation and substantial overhead in token selection. This paper introduces TidalDecode, a simple yet effective algorithm and system for fast and accurate LLM decoding through position persistent sparse attention. TidalDecode leverages the spatial coherence of tokens selected by existing sparse attention methods and introduces a few token selection layers that perform full attention to identify the tokens with the highest attention scores, while all other layers perform sparse attention with the pre-selected tokens. This design enables TidalDecode to substantially reduce the overhead of token selection for sparse attention without sacrificing the quality of the generated results. Evaluation on a diverse set of LLMs and tasks shows that TidalDecode closely matches the generative performance of full attention methods while reducing the LLM decoding latency by up to 2.1x.",
            "score": 2,
            "issue_id": 33,
            "pub_date": "2024-10-07",
            "pub_date_ru": "7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#inference",
                    "#optimization",
                    "#architecture"
                ],
                "emoji": "ğŸŒŠ",
                "ru": {
                    "title": "TidalDecode: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ LLM Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ TidalDecode - Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸. TidalDecode Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ ĞºĞ¾Ğ³ĞµÑ€ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ¸ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑĞ»Ğ¾ĞµĞ² Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ğ½Ğ°ĞºĞ»Ğ°Ğ´Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑ…Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ· ÑƒÑ‰ĞµÑ€Ğ±Ğ° Ğ´Ğ»Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². ĞÑ†ĞµĞ½ĞºĞ° Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… LLM Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ TidalDecode ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼, ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°Ñ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºÑƒ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ LLM Ğ´Ğ¾ 2,1 Ñ€Ğ°Ğ·Ğ°."
                },
                "en": {
                    "title": "\"TidalDecode: Streamlining LLM Decoding with Smart Sparse Attention\"",
                    "desc": "The paper addresses the memory constraints in large language models (LLMs) caused by the expanding key-value cache size during the decoding phase. It critiques existing sparse attention mechanisms for not effectively identifying relevant tokens and ignoring spatial coherence across Transformer layers. The authors propose TidalDecode, an algorithm that uses position persistent sparse attention to improve token selection efficiency without compromising performance. TidalDecode significantly reduces decoding latency while maintaining high-quality results, as demonstrated in various LLM tasks."
                },
                "zh": {
                    "title": "TidalDecodeï¼šæå‡å¤§è¯­è¨€æ¨¡å‹è§£ç æ•ˆç‡çš„æ–°æ–¹æ³•",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºTidalDecodeçš„æ–°ç®—æ³•ï¼Œç”¨äºæé«˜å¤§è¯­è¨€æ¨¡å‹çš„è§£ç æ•ˆç‡ã€‚TidalDecodeé€šè¿‡ä½ç½®æŒä¹…ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ï¼Œè§£å†³äº†ç°æœ‰ç¨€ç–æ³¨æ„åŠ›æ–¹æ³•åœ¨é€‰æ‹©ç›¸å…³è¯å…ƒæ—¶çš„ä¸è¶³ã€‚å®ƒåœ¨å°‘æ•°å±‚ä¸­ä½¿ç”¨å…¨æ³¨æ„åŠ›æ¥è¯†åˆ«é«˜æ³¨æ„åŠ›åˆ†æ•°çš„è¯å…ƒï¼Œè€Œå…¶ä»–å±‚åˆ™ä½¿ç”¨é¢„é€‰è¯å…ƒè¿›è¡Œç¨€ç–æ³¨æ„åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒTidalDecodeåœ¨ä¿æŒç”Ÿæˆè´¨é‡çš„åŒæ—¶ï¼Œå°†è§£ç å»¶è¿Ÿå‡å°‘äº†æœ€å¤š2.1å€ã€‚"
                }
            },
            "hash": "299609c59ee293c9"
        },
        {
            "id": "https://huggingface.co/papers/2410.03399",
            "title": "EBES: Easy Benchmarking for Event Sequences",
            "url": "https://huggingface.co/papers/2410.03399",
            "abstract": "Event sequences, characterized by irregular sampling intervals and a mix of categorical and numerical features, are common data structures in various real-world domains such as healthcare, finance, and user interaction logs. Despite advances in temporal data modeling techniques, there is no standardized benchmarks for evaluating their performance on event sequences. This complicates result comparison across different papers due to varying evaluation protocols, potentially misleading progress in this field. We introduce EBES, a comprehensive benchmarking tool with standardized evaluation scenarios and protocols, focusing on regression and classification problems with sequence-level targets. Our library simplifies benchmarking, dataset addition, and method integration through a unified interface. It includes a novel synthetic dataset and provides preprocessed real-world datasets, including the largest publicly available banking dataset. Our results provide an in-depth analysis of datasets, identifying some as unsuitable for model comparison. We investigate the importance of modeling temporal and sequential components, as well as the robustness and scaling properties of the models. These findings highlight potential directions for future research. Our benchmark aim is to facilitate reproducible research, expediting progress and increasing real-world impacts.",
            "score": 2,
            "issue_id": 30,
            "pub_date": "2024-10-04",
            "pub_date_ru": "4 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#data"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "EBES: Ğ¡Ñ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ EBES - Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸Ğ½Ğ³Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ñ… Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹. EBES Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¸ Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ñ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğ¼Ğ¸ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ˜Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ² Ñ‚Ğ¾Ğ¼ Ñ‡Ğ¸ÑĞ»Ğµ ĞºÑ€ÑƒĞ¿Ğ½ĞµĞ¹ÑˆĞ¸Ğ¹ Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ğ¾ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğ¹ Ğ±Ğ°Ğ½ĞºĞ¾Ğ²ÑĞºĞ¸Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Standardizing Event Sequence Evaluation with EBES",
                    "desc": "The paper introduces EBES, a benchmarking tool designed to standardize the evaluation of machine learning models on event sequences, which are common in fields like healthcare and finance. EBES provides a unified interface for benchmarking, adding datasets, and integrating methods, making it easier to compare results across different studies. It includes both synthetic and real-world datasets, offering insights into which datasets are suitable for model comparison. The tool aims to promote reproducible research and accelerate progress in understanding the temporal and sequential aspects of event sequence data."
                },
                "zh": {
                    "title": "EBESï¼šäº‹ä»¶åºåˆ—æ•°æ®è¯„ä¼°çš„æ ‡å‡†åŒ–å·¥å…·",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºEBESçš„å·¥å…·ï¼Œç”¨äºæ ‡å‡†åŒ–äº‹ä»¶åºåˆ—æ•°æ®çš„è¯„ä¼°ã€‚äº‹ä»¶åºåˆ—æ•°æ®åœ¨åŒ»ç–—ã€é‡‘èç­‰é¢†åŸŸå¾ˆå¸¸è§ï¼Œä½†ç¼ºä¹ç»Ÿä¸€çš„è¯„ä¼°æ ‡å‡†ã€‚EBESæä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„æ¥å£ï¼Œç®€åŒ–äº†åŸºå‡†æµ‹è¯•å’Œæ–¹æ³•é›†æˆã€‚ç ”ç©¶ç»“æœå¼ºè°ƒäº†æ—¶é—´å’Œåºåˆ—å»ºæ¨¡çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºæœªæ¥ç ”ç©¶æŒ‡æ˜äº†æ–¹å‘ã€‚"
                }
            },
            "hash": "c88e217d2ab5a78c"
        }
    ],
    "weekday": 1,
    "link_prev": "2024-10-07.html",
    "link_next": "2024-10-09.html",
    "time_utc": "2024-10-08 20:26",
    "date_en": "8 October",
    "date_prev": "7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
    "date_next": "9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
    "short_date_prev": "07.10",
    "short_date_next": "09.10",
    "categories": {
        "#dataset": 3,
        "#data": 2,
        "#benchmark": 4,
        "#agents": 0,
        "#cv": 3,
        "#rl": 0,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 3,
        "#medicine": 0,
        "#training": 3,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 2,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#quantum": 0,
        "#edge_computing": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0
    }
}