
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 21 papers. June 25.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["–º–∏–Ω—É—Ç—É", "–º–∏–Ω—É—Ç—ã", "–º–∏–Ω—É—Ç"],
                hour: ["—á–∞—Å", "—á–∞—Å–∞", "—á–∞—Å–æ–≤"],
                day: ["–¥–µ–Ω—å", "–¥–Ω—è", "–¥–Ω–µ–π"],
                justNow: "—Ç–æ–ª—å–∫–æ —á—Ç–æ",
                ago: "–Ω–∞–∑–∞–¥"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["ÂàÜÈíü", "ÂàÜÈíü", "ÂàÜÈíü"],
                hour: ["Â∞èÊó∂", "Â∞èÊó∂", "Â∞èÊó∂"],
                day: ["Â§©", "Â§©", "Â§©"],
                justNow: "ÂàöÂàö",
                ago: "Ââç"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "—Å—Ç–∞—Ç–µ–π";
            } else if (lastDigit === 1) {
                word = "—Å—Ç–∞—Ç—å—è";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "—Å—Ç–∞—Ç—å–∏";
            } else {
                word = "—Å—Ç–∞—Ç–µ–π";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ÁØáËÆ∫Êñá"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">üî∫</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">25 –∏—é–Ω—è</span> | <span id="title-articles-count">21 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-06-24.html">‚¨ÖÔ∏è <span id="prev-date">24.06</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-06-26.html">‚û°Ô∏è <span id="next-date">26.06</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-06.html">üìà <span id='top-month-label'>–ú–µ—Å—è—Ü</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">üîÄ <span id="sort-label-text">–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">—Ä–µ–π—Ç–∏–Ω–≥—É</option>
                    <option value="pub_date">–¥–∞—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏</option>
                    <option value="issue_id">–¥–æ–±–∞–≤–ª–µ–Ω–∏—é –Ω–∞ HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">üè∑Ô∏è –§–∏–ª—å—Ç—Ä</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A‚à™B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A‚à©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">üßπ</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ‚úñÔ∏è <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '25 –∏—é–Ω—è', 'en': 'June 25', 'zh': '6Êúà25Êó•'};
        let feedDateNext = {'ru': '26.06', 'en': '06/26', 'zh': '6Êúà26Êó•'};
        let feedDatePrev = {'ru': '24.06', 'en': '06/24', 'zh': '6Êúà24Êó•'};
        let filterLabel = {'ru': '–§–∏–ª—å—Ç—Ä', 'en': 'Topics', 'zh': '‰∏ªÈ¢òÁ≠õÈÄâ'}
        let publishedLabel = {'ru': '—Å—Ç–∞—Ç—å—è –æ—Ç ', 'en': 'published on ', 'zh': 'ÂèëË°®‰∫é'}
        let sortLabel = {'ru': '–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ', 'en': 'Sort by', 'zh': 'ÊéíÂ∫èÊñπÂºè'}
        let paperLabel = {'ru': '–°—Ç–∞—Ç—å—è', 'en': 'Paper', 'zh': 'ËÆ∫Êñá'}
        let topMonthLabel = {'ru': '–ú–µ—Å—è—Ü', 'en': 'Month', 'zh': 'ÊúàÂ∫¶ËÆ∫Êñá'}
        let topDayLabel = {'ru': '–î–µ–Ω—å', 'en': 'Day', 'zh': 'Êó•Â∫¶ËÆ∫Êñá'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2506.19851', 'title': 'AnimaX: Animating the Inanimate in 3D with Joint Video-Pose Diffusion\n  Models', 'url': 'https://huggingface.co/papers/2506.19851', 'abstract': 'AnimaX creates multi-skeleton 3D animations by blending video diffusion model priors with skeleton-based control, using joint video-pose diffusion and shared positional encodings.  \t\t\t\t\tAI-generated summary \t\t\t\t We present AnimaX, a feed-forward 3D animation framework that bridges the motion priors of video diffusion models with the controllable structure of skeleton-based animation. Traditional motion synthesis methods are either restricted to fixed skeletal topologies or require costly optimization in high-dimensional deformation spaces. In contrast, AnimaX effectively transfers video-based motion knowledge to the 3D domain, supporting diverse articulated meshes with arbitrary skeletons. Our method represents 3D motion as multi-view, multi-frame 2D pose maps, and enables joint video-pose diffusion conditioned on template renderings and a textual motion prompt. We introduce shared positional encodings and modality-aware embeddings to ensure spatial-temporal alignment between video and pose sequences, effectively transferring video priors to motion generation task. The resulting multi-view pose sequences are triangulated into 3D joint positions and converted into mesh animation via inverse kinematics. Trained on a newly curated dataset of 160,000 rigged sequences, AnimaX achieves state-of-the-art results on VBench in generalization, motion fidelity, and efficiency, offering a scalable solution for category-agnostic 3D animation. Project page: https://anima-x.github.io/{https://anima-x.github.io/}.', 'score': 39, 'issue_id': 4470, 'pub_date': '2025-06-24', 'pub_date_card': {'ru': '24 –∏—é–Ω—è', 'en': 'June 24', 'zh': '6Êúà24Êó•'}, 'hash': 'ce9a811b1a9e7d9d', 'authors': ['Zehuan Huang', 'Haoran Feng', 'Yangtian Sun', 'Yuanchen Guo', 'Yanpei Cao', 'Lu Sheng'], 'affiliations': ['Beihang University, China', 'The University of Hong Kong, China', 'Tsinghua University, China', 'VAST, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.19851.jpg', 'data': {'categories': ['#optimization', '#3d', '#transfer_learning', '#diffusion', '#dataset'], 'emoji': 'ü¶æ', 'ru': {'title': 'AnimaX: —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è 3D-–∞–Ω–∏–º–∞—Ü–∏—è —Å –ø–æ–º–æ—â—å—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤–∏–¥–µ–æ', 'desc': 'AnimaX - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è 3D-–∞–Ω–∏–º–∞—Ü–∏–∏, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤–∏–¥–µ–æ —Å –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π —Å–∫–µ–ª–µ—Ç–Ω–æ–π –∞–Ω–∏–º–∞—Ü–∏–∏. –ú–µ—Ç–æ–¥ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç 3D-–¥–≤–∏–∂–µ–Ω–∏–µ –≤ –≤–∏–¥–µ –º–Ω–æ–≥–æ—Ä–∞–∫—É—Ä—Å–Ω—ã—Ö 2D-–∫–∞—Ä—Ç –ø–æ–∑ –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤—ã–ø–æ–ª–Ω—è—Ç—å —Å–æ–≤–º–µ—Å—Ç–Ω—É—é –¥–∏—Ñ—Ñ—É–∑–∏—é –≤–∏–¥–µ–æ –∏ –ø–æ–∑ –Ω–∞ –æ—Å–Ω–æ–≤–µ —à–∞–±–ª–æ–Ω–Ω—ã—Ö —Ä–µ–Ω–¥–µ—Ä–æ–≤ –∏ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏—è. AnimaX –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±—â–∏–µ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ –∫–æ–¥–∏—Ä–æ–≤–∫–∏ –∏ –º–æ–¥–∞–ª—å–Ω–æ-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –º–µ–∂–¥—É –≤–∏–¥–µ–æ –∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è–º–∏ –ø–æ–∑. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –ø–µ—Ä–µ–¥–æ–≤—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ –æ–±–æ–±—â–µ–Ω–∏–∏, —Ç–æ—á–Ω–æ—Å—Ç–∏ –¥–≤–∏–∂–µ–Ω–∏–π –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –¥–ª—è –∞–Ω–∏–º–∞—Ü–∏–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö 3D-–º–æ–¥–µ–ª–µ–π.'}, 'en': {'title': 'Bridging Video Motion and 3D Animation with AnimaX', 'desc': 'AnimaX is a novel framework for creating 3D animations that combines the strengths of video diffusion models with skeleton-based control. It allows for the generation of animations without being limited to fixed skeletal structures, overcoming the challenges of high-dimensional optimization. By using multi-view, multi-frame 2D pose maps and shared positional encodings, AnimaX ensures that video motion knowledge is effectively transferred to 3D motion generation. This method has been trained on a large dataset and demonstrates superior performance in terms of generalization, motion fidelity, and efficiency for diverse animated characters.'}, 'zh': {'title': 'AnimaXÔºöÊó†Á±ªÂà´ 3D Âä®ÁîªÁöÑÂàõÊñ∞Ëß£ÂÜ≥ÊñπÊ°à', 'desc': 'AnimaX ÊòØ‰∏Ä‰∏™ÂâçÈ¶àÂºèÁöÑ 3D Âä®ÁîªÊ°ÜÊû∂ÔºåÂÆÉÁªìÂêà‰∫ÜËßÜÈ¢ëÊâ©Êï£Ê®°ÂûãÁöÑËøêÂä®ÂÖàÈ™åÂíåÂü∫‰∫éÈ™®È™ºÁöÑÂèØÊéßÁªìÊûÑ„ÄÇ‰∏é‰º†ÁªüÁöÑËøêÂä®ÂêàÊàêÊñπÊ≥ï‰∏çÂêåÔºåAnimaX ÊîØÊåÅ‰ªªÊÑèÈ™®È™ºÁöÑÂ§öÊ†∑ÂåñÂÖ≥ËäÇÁΩëÊ†ºÔºåÂπ∂ÊúâÊïàÂú∞Â∞ÜËßÜÈ¢ë‰∏≠ÁöÑËøêÂä®Áü•ËØÜËΩ¨ÁßªÂà∞ 3D È¢ÜÂüü„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂ§öËßÜËßí„ÄÅÂ§öÂ∏ßÁöÑ 2D ÂßøÊÄÅÂõæË°®Á§∫ 3D ËøêÂä®ÔºåÂπ∂Âà©Áî®ÂÖ±‰∫´‰ΩçÁΩÆÁºñÁ†ÅÂíåÊ®°ÊÄÅÊÑüÁü•ÂµåÂÖ•Á°Æ‰øùËßÜÈ¢ëÂíåÂßøÊÄÅÂ∫èÂàó‰πãÈó¥ÁöÑÊó∂Á©∫ÂØπÈΩê„ÄÇÁªèËøáÂú®‰∏Ä‰∏™ÂåÖÂê´ 160,000 ‰∏™ÁªëÂÆöÂ∫èÂàóÁöÑÊñ∞Êï∞ÊçÆÈõÜ‰∏äËÆ≠ÁªÉÔºåAnimaX Âú® VBench ‰∏äÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÁªìÊûúÔºåÊèê‰æõ‰∫Ü‰∏ÄÁßçÂèØÊâ©Â±ïÁöÑÊó†Á±ªÂà´ 3D Âä®ÁîªËß£ÂÜ≥ÊñπÊ°à„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2506.18701', 'title': 'Matrix-Game: Interactive World Foundation Model', 'url': 'https://huggingface.co/papers/2506.18701', 'abstract': 'Matrix-Game, a controllable game world generation model trained in a two-stage process, outperforms existing models by producing high-quality, action-controllable, and physically consistent Minecraft world videos.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Matrix-Game, an interactive world foundation model for controllable game world generation. Matrix-Game is trained using a two-stage pipeline that first performs large-scale unlabeled pretraining for environment understanding, followed by action-labeled training for interactive video generation. To support this, we curate Matrix-Game-MC, a comprehensive Minecraft dataset comprising over 2,700 hours of unlabeled gameplay video clips and over 1,000 hours of high-quality labeled clips with fine-grained keyboard and mouse action annotations. Our model adopts a controllable image-to-world generation paradigm, conditioned on a reference image, motion context, and user actions. With over 17 billion parameters, Matrix-Game enables precise control over character actions and camera movements, while maintaining high visual quality and temporal coherence. To evaluate performance, we develop GameWorld Score, a unified benchmark measuring visual quality, temporal quality, action controllability, and physical rule understanding for Minecraft world generation. Extensive experiments show that Matrix-Game consistently outperforms prior open-source Minecraft world models (including Oasis and MineWorld) across all metrics, with particularly strong gains in controllability and physical consistency. Double-blind human evaluations further confirm the superiority of Matrix-Game, highlighting its ability to generate perceptually realistic and precisely controllable videos across diverse game scenarios. To facilitate future research on interactive image-to-world generation, we will open-source the Matrix-Game model weights and the GameWorld Score benchmark at https://github.com/SkyworkAI/Matrix-Game.', 'score': 37, 'issue_id': 4478, 'pub_date': '2025-06-23', 'pub_date_card': {'ru': '23 –∏—é–Ω—è', 'en': 'June 23', 'zh': '6Êúà23Êó•'}, 'hash': 'a6267b86f005d608', 'authors': ['Yifan Zhang', 'Chunli Peng', 'Boyang Wang', 'Puyi Wang', 'Qingcheng Zhu', 'Fei Kang', 'Biao Jiang', 'Zedong Gao', 'Eric Li', 'Yang Liu', 'Yahui Zhou'], 'affiliations': ['Interactive World Foundation', 'Skywork AI'], 'pdf_title_img': 'assets/pdf/title_img/2506.18701.jpg', 'data': {'categories': ['#open_source', '#dataset', '#video', '#benchmark', '#data', '#games'], 'emoji': 'üéÆ', 'ru': {'title': '–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–≥—Ä–æ–≤—ã—Ö –º–∏—Ä–æ–≤: Matrix-Game —Å–æ–∑–¥–∞–µ—Ç —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –∏ —É–ø—Ä–∞–≤–ª—è–µ–º—ã–µ –≤–∏–¥–µ–æ Minecraft', 'desc': 'Matrix-Game - —ç—Ç–æ –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã—Ö –∏–≥—Ä–æ–≤—ã—Ö –º–∏—Ä–æ–≤, –æ–±—É—á–µ–Ω–Ω–∞—è –ø–æ –¥–≤—É—Ö—ç—Ç–∞–ø–Ω–æ–º—É –ø—Ä–æ—Ü–µ—Å—Å—É. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –±–æ–ª—å—à–æ–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö Minecraft, –≤–∫–ª—é—á–∞—é—â–∏–π –±–æ–ª–µ–µ 2700 —á–∞—Å–æ–≤ –Ω–µ–º–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏ 1000 —á–∞—Å–æ–≤ –º–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ–∫–ª–∏–ø–æ–≤ –∏–≥—Ä–æ–≤–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞. –ú–æ–¥–µ–ª—å —Å 17 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Ç–æ—á–Ω–æ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä–æ–≤–∞—Ç—å –¥–µ–π—Å—Ç–≤–∏—è –ø–µ—Ä—Å–æ–Ω–∞–∂–∞ –∏ –¥–≤–∏–∂–µ–Ω–∏—è –∫–∞–º–µ—Ä—ã, —Å–æ—Ö—Ä–∞–Ω—è—è –≤—ã—Å–æ–∫–æ–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∏ –≤—Ä–µ–º–µ–Ω–Ω—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ Matrix-Game –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–æ–¥–µ–ª–∏ –ø–æ –≤—Å–µ–º –º–µ—Ç—Ä–∏–∫–∞–º, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ —É–ø—Ä–∞–≤–ª—è–µ–º–æ—Å—Ç–∏ –∏ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º–æ–≥–æ –º–∏—Ä–∞.'}, 'en': {'title': 'Matrix-Game: Revolutionizing Game World Generation with Control and Quality', 'desc': "Matrix-Game is a novel model designed for generating interactive game worlds, specifically in Minecraft, using a two-stage training process. The first stage involves unsupervised pretraining on a large dataset of gameplay videos to understand the environment, followed by a second stage of supervised training with action-labeled data for generating videos based on user interactions. With over 17 billion parameters, the model excels in producing high-quality, controllable videos that maintain physical consistency and visual coherence. The introduction of the GameWorld Score benchmark allows for comprehensive evaluation of the model's performance in terms of visual quality, action controllability, and adherence to physical rules, demonstrating its superiority over existing models."}, 'zh': {'title': 'Matrix-GameÔºöÂèØÊéßÁöÑÊ∏∏Êàè‰∏ñÁïåÁîüÊàêÊñ∞Ê†áÂáÜ', 'desc': 'Matrix-GameÊòØ‰∏ÄÁßçÂèØÊéßÁöÑÊ∏∏Êàè‰∏ñÁïåÁîüÊàêÊ®°ÂûãÔºåÈÄöËøá‰∏§Èò∂ÊÆµÁöÑËÆ≠ÁªÉËøáÁ®ãÊù•ÊèêÈ´òÁîüÊàêË¥®Èáè„ÄÇÈ¶ñÂÖàËøõË°åÂ§ßËßÑÊ®°ÁöÑÊó†Ê†áÁ≠æÈ¢ÑËÆ≠ÁªÉÔºå‰ª•ÁêÜËß£ÁéØÂ¢ÉÔºåÁÑ∂ÂêéËøõË°åÊúâÊ†áÁ≠æÁöÑËÆ≠ÁªÉ‰ª•ÁîüÊàê‰∫íÂä®ËßÜÈ¢ë„ÄÇËØ•Ê®°Âûã‰ΩøÁî®Ë∂ÖËøá170‰∫ø‰∏™ÂèÇÊï∞ÔºåËÉΩÂ§üÁ≤æÁ°ÆÊéßÂà∂ËßíËâ≤Âä®‰ΩúÂíåÁõ∏Êú∫ÁßªÂä®ÔºåÂêåÊó∂‰øùÊåÅÈ´òËßÜËßâË¥®ÈáèÂíåÊó∂Èó¥‰∏ÄËá¥ÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMatrix-GameÂú®ÂèØÊéßÊÄßÂíåÁâ©ÁêÜ‰∏ÄËá¥ÊÄßÊñπÈù¢ÊòæËëó‰ºò‰∫éÁé∞ÊúâÁöÑMinecraft‰∏ñÁïåÊ®°Âûã„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2506.19290', 'title': 'Skywork-SWE: Unveiling Data Scaling Laws for Software Engineering in\n  LLMs', 'url': 'https://huggingface.co/papers/2506.19290', 'abstract': "An automated data-curation pipeline for software engineering improves large language model performance on SWE tasks, achieving state-of-the-art results with and without test-time scaling techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Software engineering (SWE) has recently emerged as a crucial testbed for next-generation LLM agents, demanding inherent capabilities in two critical dimensions: sustained iterative problem-solving (e.g., >50 interaction rounds) and long-context dependency resolution (e.g., >32k tokens). However, the data curation process in SWE remains notoriously time-consuming, as it heavily relies on manual annotation for code file filtering and the setup of dedicated runtime environments to execute and validate unit tests. Consequently, most existing datasets are limited to only a few thousand GitHub-sourced instances. To this end, we propose an incremental, automated data-curation pipeline that systematically scales both the volume and diversity of SWE datasets. Our dataset comprises 10,169 real-world Python task instances from 2,531 distinct GitHub repositories, each accompanied by a task specified in natural language and a dedicated runtime-environment image for automated unit-test validation. We have carefully curated over 8,000 successfully runtime-validated training trajectories from our proposed SWE dataset. When fine-tuning the Skywork-SWE model on these trajectories, we uncover a striking data scaling phenomenon: the trained model's performance for software engineering capabilities in LLMs continues to improve as the data size increases, showing no signs of saturation. Notably, our Skywork-SWE model achieves 38.0% pass@1 accuracy on the SWE-bench Verified benchmark without using verifiers or multiple rollouts, establishing a new state-of-the-art (SOTA) among the Qwen2.5-Coder-32B-based LLMs built on the OpenHands agent framework. Furthermore, with the incorporation of test-time scaling techniques, the performance further improves to 47.0% accuracy, surpassing the previous SOTA results for sub-32B parameter models. We release the Skywork-SWE-32B model checkpoint to accelerate future research.", 'score': 27, 'issue_id': 4471, 'pub_date': '2025-06-24', 'pub_date_card': {'ru': '24 –∏—é–Ω—è', 'en': 'June 24', 'zh': '6Êúà24Êó•'}, 'hash': '7c26a879c3db6096', 'authors': ['Liang Zeng', 'Yongcong Li', 'Yuzhen Xiao', 'Changshi Li', 'Chris Yuhao Liu', 'Rui Yan', 'Tianwen Wei', 'Jujie He', 'Xuchen Song', 'Yang Liu', 'Yahui Zhou'], 'affiliations': ['Skywork AI, Kunlun Inc'], 'pdf_title_img': 'assets/pdf/title_img/2506.19290.jpg', 'data': {'categories': ['#open_source', '#dataset', '#data', '#long_context', '#benchmark', '#training'], 'emoji': 'üöÄ', 'ru': {'title': '–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è –∫—É—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –ø–æ–¥–Ω–∏–º–∞–µ—Ç LLM –Ω–∞ –Ω–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –≤ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–π –∏–Ω–∂–µ–Ω–µ—Ä–∏–∏', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –¥–ª—è –∫—É—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –≤ –æ–±–ª–∞—Å—Ç–∏ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–π –∏–Ω–∂–µ–Ω–µ—Ä–∏–∏, —á—Ç–æ –ø–æ–∑–≤–æ–ª–∏–ª–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∏—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –Ω–∞ –∑–∞–¥–∞—á–∞—Ö —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è. –ù–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª–∏–ª —Å–æ–∑–¥–∞—Ç—å –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ –±–æ–ª–µ–µ —á–µ–º 10 000 —Ä–µ–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á Python —Å –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π —á–µ—Ä–µ–∑ –º–æ–¥—É–ª—å–Ω—ã–µ —Ç–µ—Å—Ç—ã. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ Skywork-SWE –Ω–∞ —ç—Ç–∏—Ö –¥–∞–Ω–Ω—ã—Ö –ø–æ–∫–∞–∑–∞–ª–æ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å —É–≤–µ–ª–∏—á–µ–Ω–∏–µ–º –æ–±—ä–µ–º–∞ –¥–∞–Ω–Ω—ã—Ö. –ú–æ–¥–µ–ª—å –¥–æ—Å—Ç–∏–≥–ª–∞ –Ω–∞–∏–ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å—Ä–µ–¥–∏ LLM –Ω–∞ –±–∞–∑–µ Qwen2.5-Coder-32B, –∫–∞–∫ —Å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ–º —Ç–µ—Ö–Ω–∏–∫ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, —Ç–∞–∫ –∏ –±–µ–∑ –Ω–∏—Ö.'}, 'en': {'title': 'Automating Data Curation to Boost LLMs in Software Engineering', 'desc': "This paper presents an automated data-curation pipeline designed to enhance the performance of large language models (LLMs) in software engineering (SWE) tasks. The pipeline addresses the challenges of manual data annotation and environment setup by providing a diverse dataset of over 10,000 real-world Python task instances from GitHub. The authors demonstrate that increasing the dataset size leads to improved model performance, with their Skywork-SWE model achieving state-of-the-art accuracy on the SWE-bench Verified benchmark. Additionally, the incorporation of test-time scaling techniques further boosts the model's performance, establishing new benchmarks for LLMs in SWE."}, 'zh': {'title': 'Ëá™Âä®ÂåñÊï∞ÊçÆÊï¥ÁêÜÔºåÊèêÂçáËΩØ‰ª∂Â∑•Á®ãÊ®°ÂûãË°®Áé∞', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçËá™Âä®ÂåñÊï∞ÊçÆÊï¥ÁêÜÁÆ°ÈÅìÔºåÊó®Âú®ÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÂú®ËΩØ‰ª∂Â∑•Á®ãÔºàSWEÔºâ‰ªªÂä°‰∏äÁöÑË°®Áé∞„ÄÇËØ•ÁÆ°ÈÅìÈÄöËøáÁ≥ªÁªüÊÄßÂú∞Êâ©Â±ïÊï∞ÊçÆÈõÜÁöÑËßÑÊ®°ÂíåÂ§öÊ†∑ÊÄßÔºåËß£ÂÜ≥‰∫Ü‰º†ÁªüÊï∞ÊçÆÊï¥ÁêÜËøáÁ®ã‰∏≠ÁöÑÊó∂Èó¥Ê∂àËÄóÈóÆÈ¢ò„ÄÇÊàë‰ª¨ÊûÑÂª∫‰∫Ü‰∏Ä‰∏™ÂåÖÂê´10,169‰∏™ÁúüÂÆûPython‰ªªÂä°ÂÆû‰æãÁöÑÊï∞ÊçÆÈõÜÔºåÂπ∂ÊàêÂäüÈ™åËØÅ‰∫ÜË∂ÖËøá8,000‰∏™ËÆ≠ÁªÉËΩ®Ëøπ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÈöèÁùÄÊï∞ÊçÆÈáèÁöÑÂ¢ûÂä†ÔºåÊ®°ÂûãÂú®ËΩØ‰ª∂Â∑•Á®ãËÉΩÂäõ‰∏äÁöÑË°®Áé∞ÊåÅÁª≠ÊèêÂçáÔºåÊúÄÁªàÂú®SWE-bench VerifiedÂü∫ÂáÜÊµãËØï‰∏≠ËææÂà∞‰∫Ü38.0%ÁöÑÂáÜÁ°ÆÁéáÔºåÂàõÈÄ†‰∫ÜÊñ∞ÁöÑÊúÄÂÖàËøõÊ∞¥Âπ≥„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2506.16141', 'title': 'GRPO-CARE: Consistency-Aware Reinforcement Learning for Multimodal\n  Reasoning', 'url': 'https://huggingface.co/papers/2506.16141', 'abstract': "GRPO-CARE, a reinforcement learning framework optimizing for consistency and correctness, outperforms standard GRPO on a new video understanding benchmark, SEED-Bench-R1, improving both performance and logical coherence in multimodal large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent reinforcement learning approaches, such as outcome-supervised GRPO, have advanced Chain-of-Thought reasoning in large language models (LLMs), yet their adaptation to multimodal LLMs (MLLMs) is unexplored. To address the lack of rigorous evaluation for MLLM post-training methods, we introduce SEED-Bench-R1, a benchmark with complex real-world videos requiring balanced perception and reasoning. It offers a large training set and evaluates generalization across three escalating challenges: in-distribution, cross-environment, and cross-environment-task scenarios. Using SEED-Bench-R1, we find that standard GRPO, while improving answer accuracy, often reduces logical coherence between reasoning steps and answers, with only a 57.9% consistency rate. This stems from reward signals focusing solely on final answers, encouraging shortcuts, and strict KL penalties limiting exploration.To address this, we propose GRPO-CARE, a consistency-aware RL framework optimizing both answer correctness and reasoning coherence without explicit supervision. GRPO-CARE introduces a two-tiered reward: (1) a base reward for answer correctness, and (2) an adaptive consistency bonus, computed by comparing the model's reasoning-to-answer likelihood (via a slowly-evolving reference model) against group peers.This dual mechanism amplifies rewards for reasoning paths that are both correct and logically consistent. Replacing KL penalties with this adaptive bonus, GRPO-CARE outperforms standard GRPO on SEED-Bench-R1, achieving a 6.7% performance gain on the hardest evaluation level and a 24.5% improvement in consistency. It also shows strong transferability, improving model performance across diverse video understanding benchmarks. Our work contributes a systematically designed benchmark and a generalizable post-training framework, advancing the development of more interpretable and robust MLLMs.", 'score': 24, 'issue_id': 4471, 'pub_date': '2025-06-19', 'pub_date_card': {'ru': '19 –∏—é–Ω—è', 'en': 'June 19', 'zh': '6Êúà19Êó•'}, 'hash': '0d8fc795754c4210', 'authors': ['Yi Chen', 'Yuying Ge', 'Rui Wang', 'Yixiao Ge', 'Junhao Cheng', 'Ying Shan', 'Xihui Liu'], 'affiliations': ['ARC Lab, Tencent PCG', 'The Chinese University of Hong Kong', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2506.16141.jpg', 'data': {'categories': ['#transfer_learning', '#rl', '#interpretability', '#multimodal', '#benchmark', '#training', '#optimization', '#video', '#reasoning'], 'emoji': 'ü§ñ', 'ru': {'title': 'GRPO-CARE: –°–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò', 'desc': 'GRPO-CARE - —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –∫–∞–∫ —Ç–æ—á–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–æ–≤, —Ç–∞–∫ –∏ –ª–æ–≥–∏—á–µ—Å–∫—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –±–µ–∑ —è–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è. GRPO-CARE –≤–≤–æ–¥–∏—Ç –¥–≤—É—Ö—É—Ä–æ–≤–Ω–µ–≤—É—é —Å–∏—Å—Ç–µ–º—É –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π: –±–∞–∑–æ–≤—É—é –∑–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–∞ –∏ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π –±–æ–Ω—É—Å –∑–∞ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å. –§—Ä–µ–π–º–≤–æ—Ä–∫ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π GRPO –Ω–∞ –Ω–æ–≤–æ–º –±–µ–Ω—á–º–∞—Ä–∫–µ SEED-Bench-R1 –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤–∏–¥–µ–æ, —É–ª—É—á—à–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ –ª–æ–≥–∏—á–µ—Å–∫—É—é —Å–≤—è–∑–Ω–æ—Å—Ç—å.'}, 'en': {'title': 'Enhancing Consistency and Correctness in Multimodal Learning', 'desc': 'The paper introduces GRPO-CARE, a new reinforcement learning framework that enhances the performance and logical coherence of multimodal large language models (MLLMs) in video understanding tasks. It addresses the limitations of standard GRPO, which often sacrifices reasoning consistency for accuracy, by implementing a two-tiered reward system that promotes both correct answers and coherent reasoning. The authors present SEED-Bench-R1, a benchmark designed to rigorously evaluate MLLMs on complex video tasks, revealing that GRPO-CARE significantly outperforms GRPO with a notable increase in consistency and performance. This work not only proposes a novel framework but also contributes a valuable benchmark for future research in MLLM development.'}, 'zh': {'title': 'ÊèêÂçá‰∏ÄËá¥ÊÄß‰∏éÊ≠£Á°ÆÊÄßÁöÑÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂', 'desc': 'GRPO-CAREÊòØ‰∏ÄÁßçÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂ÔºåÊó®Âú®‰ºòÂåñ‰∏ÄËá¥ÊÄßÂíåÊ≠£Á°ÆÊÄßÔºåË∂ÖË∂ä‰∫ÜÊ†áÂáÜÁöÑGRPOÊñπÊ≥ï„ÄÇÂÆÉÂú®Êñ∞ÁöÑËßÜÈ¢ëÁêÜËß£Âü∫ÂáÜSEED-Bench-R1‰∏äË°®Áé∞Âá∫Ëâ≤ÔºåÊèêÂçá‰∫ÜÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÊÄßËÉΩÂíåÈÄªËæëËøûË¥ØÊÄß„ÄÇÈÄöËøáÂºïÂÖ•ÂèåÈáçÂ•ñÂä±Êú∫Âà∂ÔºåGRPO-CARE‰∏ç‰ªÖÂÖ≥Ê≥®Á≠îÊ°àÁöÑÊ≠£Á°ÆÊÄßÔºåËøòÈºìÂä±Êé®ÁêÜËøáÁ®ãÁöÑÈÄªËæë‰∏ÄËá¥ÊÄß„ÄÇËØ•Ê°ÜÊû∂Âú®Â§çÊùÇÁöÑÁúüÂÆû‰∏ñÁïåËßÜÈ¢ë‰ªªÂä°‰∏≠Â±ïÁé∞‰∫ÜÂº∫Â§ßÁöÑËøÅÁßªËÉΩÂäõÔºåÊé®Âä®‰∫ÜÊõ¥ÂÖ∑ÂèØËß£ÈáäÊÄßÂíåÈ≤ÅÊ£íÊÄßÁöÑÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÂèëÂ±ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2506.19848', 'title': 'ScaleCap: Inference-Time Scalable Image Captioning via Dual-Modality\n  Debiasing', 'url': 'https://huggingface.co/papers/2506.19848', 'abstract': 'ScaleCap enhances image captioning by iteratively enriching and calibrating captions using heuristic question answering and contrastive sentence rating, addressing multimodal and linguistic biases to improve accuracy, balance, and informativeness.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper presents ScaleCap, an inference-time scalable image captioning strategy that generates comprehensive and detailed image captions. The key challenges of high-quality image captioning lie in the inherent biases of LVLMs: multimodal bias resulting in imbalanced descriptive granularity, offering detailed accounts of some elements while merely skimming over others; linguistic bias leading to hallucinated descriptions of non-existent objects. To address these issues, we propose a scalable debiased captioning strategy, which continuously enriches and calibrates the caption with increased inference budget. Specifically, we propose two novel components: heuristic question answering and contrastive sentence rating. The former generates content-specific questions based on the image and answers them to progressively inject relevant information into the caption. The latter employs sentence-level offline contrastive decoding to effectively identify and eliminate hallucinations caused by linguistic biases. With increased inference cost, more heuristic questions are raised by ScaleCap to progressively capture additional visual details, generating captions that are more accurate, balanced, and informative. Extensive modality alignment experiments demonstrate the effectiveness of ScaleCap. Annotating 450K images with ScaleCap and using them for LVLM pretraining leads to consistent performance gains across 11 widely used benchmarks. Furthermore, ScaleCap showcases superb richness and fidelity of generated captions with two additional tasks: replacing images with captions in VQA task, and reconstructing images from captions to assess semantic coverage. Code is available at https://github.com/Cooperx521/ScaleCap.', 'score': 23, 'issue_id': 4473, 'pub_date': '2025-06-24', 'pub_date_card': {'ru': '24 –∏—é–Ω—è', 'en': 'June 24', 'zh': '6Êúà24Êó•'}, 'hash': '0017ef0507a6b74d', 'authors': ['Long Xing', 'Qidong Huang', 'Xiaoyi Dong', 'Pan Zhang', 'Yuhang Zang', 'Yuhang Cao', 'Jinsong Li', 'Shuangrui Ding', 'Weiming Zhang', 'Nenghai Yu', 'Jiaqi Wang', 'Feng Wu', 'Dahua Lin'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory', 'The Chinese University of Hong Kong', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2506.19848.jpg', 'data': {'categories': ['#cv', '#benchmark', '#hallucinations', '#interpretability', '#multimodal', '#inference'], 'emoji': 'üñºÔ∏è', 'ru': {'title': 'ScaleCap: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ–¥–ø–∏—Å–µ–π –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º', 'desc': 'ScaleCap - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ–¥–ø–∏—Å–µ–π –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ –æ–±–æ–≥–∞—â–µ–Ω–∏–µ –∏ –∫–∞–ª–∏–±—Ä–æ–≤–∫—É –ø–æ–¥–ø–∏—Å–µ–π —Å –ø–æ–º–æ—â—å—é —ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω–æ–≥–æ –º–µ—Ö–∞–Ω–∏–∑–º–∞ –∏ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π. ScaleCap –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω –Ω–∞ —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∏ –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö —Å–º–µ—â–µ–Ω–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –∑—Ä–µ–Ω–∏—è. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ–∑–¥–∞–≤–∞—Ç—å –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã–µ, —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –ø–æ–¥–ø–∏—Å–∏ –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º.'}, 'en': {'title': 'Enhancing Image Captions with ScaleCap: Accurate, Balanced, and Informative!', 'desc': 'ScaleCap is a novel approach to image captioning that improves the quality of generated captions by addressing biases in large vision-language models (LVLMs). It uses heuristic question answering to generate specific questions about the image, which helps to enrich the captions with relevant details. Additionally, it employs contrastive sentence rating to identify and remove inaccuracies or hallucinations in the descriptions. By iteratively refining captions with these techniques, ScaleCap produces more accurate, balanced, and informative image descriptions.'}, 'zh': {'title': 'ScaleCapÔºöÊèêÂçáÂõæÂÉèÊèèËø∞ÁöÑÊô∫ËÉΩÁ≠ñÁï•', 'desc': 'ScaleCapÊòØ‰∏ÄÁßçÂ¢ûÂº∫ÂõæÂÉèÊèèËø∞ÁîüÊàêÁöÑÁ≠ñÁï•ÔºåÈÄöËøáËø≠‰ª£‰∏∞ÂØåÂíåÊ†°ÂáÜÊèèËø∞ÔºåËß£ÂÜ≥Â§öÊ®°ÊÄÅÂíåËØ≠Ë®ÄÂÅèËßÅÈóÆÈ¢òÔºå‰ªéËÄåÊèêÈ´òÂáÜÁ°ÆÊÄßÂíå‰ø°ÊÅØÈáè„ÄÇËØ•ÊñπÊ≥ïÂºïÂÖ•‰∫ÜÂêØÂèëÂºèÈóÆÁ≠îÂíåÂØπÊØîÂè•Â≠êËØÑÂàÜ‰∏§‰∏™Êñ∞ÁªÑ‰ª∂ÔºåÂâçËÄÖÊ†πÊçÆÂõæÂÉèÁîüÊàêÁâπÂÆöÈóÆÈ¢òÂπ∂ÂõûÁ≠îÔºå‰ª•ÈÄêÊ≠•Ê≥®ÂÖ•Áõ∏ÂÖ≥‰ø°ÊÅØ„ÄÇÂêéËÄÖÈÄöËøáÂè•Â≠êÁ∫ßÁöÑÂØπÊØîËß£Á†ÅÔºåÊúâÊïàËØÜÂà´Âπ∂Ê∂àÈô§Áî±‰∫éËØ≠Ë®ÄÂÅèËßÅÂØºËá¥ÁöÑËôöÂÅáÊèèËø∞„ÄÇÈÄöËøáÂ¢ûÂä†Êé®ÁêÜÈ¢ÑÁÆóÔºåScaleCapËÉΩÂ§üÁîüÊàêÊõ¥ÂáÜÁ°Æ„ÄÅÂπ≥Ë°°Âíå‰ø°ÊÅØ‰∏∞ÂØåÁöÑÂõæÂÉèÊèèËø∞„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2506.17612', 'title': 'JarvisArt: Liberating Human Artistic Creativity via an Intelligent Photo\n  Retouching Agent', 'url': 'https://huggingface.co/papers/2506.17612', 'abstract': 'JarvisArt, an MLLM-driven agent, achieves superior photo retouching by understanding user intent and coordinating multiple retouching tools in Lightroom, outperforming GPT-4o on a novel benchmark.  \t\t\t\t\tAI-generated summary \t\t\t\t Photo retouching has become integral to contemporary visual storytelling, enabling users to capture aesthetics and express creativity. While professional tools such as Adobe Lightroom offer powerful capabilities, they demand substantial expertise and manual effort. In contrast, existing AI-based solutions provide automation but often suffer from limited adjustability and poor generalization, failing to meet diverse and personalized editing needs. To bridge this gap, we introduce JarvisArt, a multi-modal large language model (MLLM)-driven agent that understands user intent, mimics the reasoning process of professional artists, and intelligently coordinates over 200 retouching tools within Lightroom. JarvisArt undergoes a two-stage training process: an initial Chain-of-Thought supervised fine-tuning to establish basic reasoning and tool-use skills, followed by Group Relative Policy Optimization for Retouching (GRPO-R) to further enhance its decision-making and tool proficiency. We also propose the Agent-to-Lightroom Protocol to facilitate seamless integration with Lightroom. To evaluate performance, we develop MMArt-Bench, a novel benchmark constructed from real-world user edits. JarvisArt demonstrates user-friendly interaction, superior generalization, and fine-grained control over both global and local adjustments, paving a new avenue for intelligent photo retouching. Notably, it outperforms GPT-4o with a 60% improvement in average pixel-level metrics on MMArt-Bench for content fidelity, while maintaining comparable instruction-following capabilities. Project Page: https://jarvisart.vercel.app/.', 'score': 17, 'issue_id': 4472, 'pub_date': '2025-06-21', 'pub_date_card': {'ru': '21 –∏—é–Ω—è', 'en': 'June 21', 'zh': '6Êúà21Êó•'}, 'hash': '2504aa6b996e0739', 'authors': ['Yunlong Lin', 'Zixu Lin', 'Kunjie Lin', 'Jinbin Bai', 'Panwang Pan', 'Chenxin Li', 'Haoyu Chen', 'Zhongdao Wang', 'Xinghao Ding', 'Wenbo Li', 'Shuicheng Yan'], 'affiliations': ['Bytedance', 'Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, Xiamen, Fujian, China', 'National University of Singapore', 'The Chinese University of Hong Kong', 'The Hong Kong University of Science and Technology (Guangzhou)', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2506.17612.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#games', '#reasoning', '#optimization', '#agents', '#cv', '#training'], 'emoji': 'üñºÔ∏è', 'ru': {'title': 'JarvisArt: –ò–ò-–ø–æ–º–æ—â–Ω–∏–∫ –¥–ª—è –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–π —Ä–µ—Ç—É—à–∏ —Ñ–æ—Ç–æ', 'desc': 'JarvisArt - —ç—Ç–æ –∞–≥–µ–Ω—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –±–æ–ª—å—à–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ (–úLLM), –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —Ä–µ—Ç—É—à—å —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π, –ø–æ–Ω–∏–º–∞—è –Ω–∞–º–µ—Ä–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ –∫–æ–æ—Ä–¥–∏–Ω–∏—Ä—É—è –º–Ω–æ–∂–µ—Å—Ç–≤–æ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –≤ Lightroom. –°–∏—Å—Ç–µ–º–∞ –ø—Ä–æ—Ö–æ–¥–∏—Ç –¥–≤—É—Ö—ç—Ç–∞–ø–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ: —Å–Ω–∞—á–∞–ª–∞ fine-tuning —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –∑–∞—Ç–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é —Å –ø–æ–º–æ—â—å—é Group Relative Policy Optimization for Retouching (GRPO-R). JarvisArt –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç GPT-4o –Ω–∞ –Ω–æ–≤–æ–º –±–µ–Ω—á–º–∞—Ä–∫–µ MMArt-Bench, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è —É–ª—É—á—à–µ–Ω–∏–µ –Ω–∞ 60% –≤ –º–µ—Ç—Ä–∏–∫–∞—Ö –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø–∏–∫—Å–µ–ª–µ–π –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π.'}, 'en': {'title': 'Revolutionizing Photo Retouching with Intelligent AI', 'desc': 'JarvisArt is an advanced multi-modal large language model (MLLM) agent designed for photo retouching, which excels in understanding user intent and effectively coordinating over 200 tools in Adobe Lightroom. Unlike traditional AI solutions that lack flexibility, JarvisArt mimics the reasoning of professional artists, allowing for personalized and nuanced editing. It employs a two-stage training process that enhances its decision-making abilities and tool proficiency, ensuring high-quality results. The performance of JarvisArt is validated through a new benchmark, MMArt-Bench, where it significantly outperforms existing models like GPT-4o in pixel-level metrics for content fidelity.'}, 'zh': {'title': 'Êô∫ËÉΩ‰øÆÈ•∞ÔºåË∂ÖË∂ä‰º†ÁªüÔºÅ', 'desc': 'JarvisArt ÊòØ‰∏Ä‰∏™Âü∫‰∫éÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMÔºâÁöÑÊô∫ËÉΩ‰ª£ÁêÜÔºå‰∏ìÊ≥®‰∫éÁÖßÁâá‰øÆÈ•∞„ÄÇÂÆÉËÉΩÂ§üÁêÜËß£Áî®Êà∑ÊÑèÂõæÔºåÂπ∂ÂçèË∞É Lightroom ‰∏≠ÁöÑ200Â§öÁßç‰øÆÈ•∞Â∑•ÂÖ∑ÔºåÊèê‰æõÊõ¥‰ºòË¥®ÁöÑ‰øÆÈ•∞ÊïàÊûú„ÄÇÈÄöËøá‰∏§Èò∂ÊÆµÁöÑËÆ≠ÁªÉËøáÁ®ãÔºåJarvisArt ÊèêÂçá‰∫ÜÂÜ≥Á≠ñËÉΩÂäõÂíåÂ∑•ÂÖ∑‰ΩøÁî®ÁÜüÁªÉÂ∫¶ÔºåËÉΩÂ§üÊª°Ë∂≥Áî®Êà∑‰∏™ÊÄßÂåñÁöÑÁºñËæëÈúÄÊ±Ç„ÄÇ‰∏éÁé∞ÊúâÁöÑ AI Ëß£ÂÜ≥ÊñπÊ°àÁõ∏ÊØîÔºåJarvisArt Âú®ÂÜÖÂÆπ‰øùÁúüÂ∫¶‰∏äË°®Áé∞Âá∫Ëâ≤ÔºåÂπ≥ÂùáÂÉèÁ¥†Á∫ßÊåáÊ†áÊèêÈ´ò‰∫Ü60%„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2506.19467', 'title': 'Can Large Language Models Capture Human Annotator Disagreements?', 'url': 'https://huggingface.co/papers/2506.19467', 'abstract': 'LLMs struggle to predict human annotation disagreements, contrary to their performance in predicting majority labels, and RLVR-style reasoning exacerbates this issue.  \t\t\t\t\tAI-generated summary \t\t\t\t Human annotation variation (i.e., annotation disagreements) is common in NLP and often reflects important information such as task subjectivity and sample ambiguity. While Large Language Models (LLMs) are increasingly used for automatic annotation to reduce human effort, their evaluation often focuses on predicting the majority-voted "ground truth" labels. It is still unclear, however, whether these models also capture informative human annotation variation. Our work addresses this gap by extensively evaluating LLMs\' ability to predict annotation disagreements without access to repeated human labels. Our results show that LLMs struggle with modeling disagreements, which can be overlooked by majority label-based evaluations. Notably, while RLVR-style (Reinforcement learning with verifiable rewards) reasoning generally boosts LLM performance, it degrades performance in disagreement prediction. Our findings highlight the critical need for evaluating and improving LLM annotators in disagreement modeling. Code and data at https://github.com/EdisonNi-hku/Disagreement_Prediction.', 'score': 14, 'issue_id': 4480, 'pub_date': '2025-06-24', 'pub_date_card': {'ru': '24 –∏—é–Ω—è', 'en': 'June 24', 'zh': '6Êúà24Êó•'}, 'hash': 'bdc388a5dcc6fc40', 'authors': ['Jingwei Ni', 'Yu Fan', 'Vil√©m Zouhar', 'Donya Rooein', 'Alexander Hoyle', 'Mrinmaya Sachan', 'Markus Leippold', 'Dirk Hovy', 'Elliott Ash'], 'affiliations': ['Bocconi University', 'ETH Z√ºrich', 'University of Z√ºrich'], 'pdf_title_img': 'assets/pdf/title_img/2506.19467.jpg', 'data': {'categories': ['#rl', '#interpretability', '#reasoning', '#rlhf', '#alignment', '#data', '#multilingual'], 'emoji': 'ü§î', 'ru': {'title': 'LLM –Ω–µ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ–º —Ä–∞–∑–Ω–æ–≥–ª–∞—Å–∏–π –≤ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è—Ö', 'desc': '–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å —Ä–∞–∑–Ω–æ–≥–ª–∞—Å–∏—è –≤ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è—Ö, —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –ª—é–¥—å–º–∏. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ LLM –∏—Å–ø—ã—Ç—ã–≤–∞—é—Ç —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ —Å –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ–º —Ä–∞–∑–Ω–æ–≥–ª–∞—Å–∏–π, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –∏—Ö —Ö–æ—Ä–æ—à—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏ –º–µ—Ç–æ–∫ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞. –ò–Ω—Ç–µ—Ä–µ—Å–Ω–æ, —á—Ç–æ –º–µ—Ç–æ–¥—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ —Å—Ç–∏–ª–µ RLVR (–æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —Å –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã–º–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è–º–∏) —É—Ö—É–¥—à–∞—é—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å —Ä–∞–∑–Ω–æ–≥–ª–∞—Å–∏—è. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –æ—Ü–µ–Ω–∫–∏ –∏ —É–ª—É—á—à–µ–Ω–∏—è LLM –≤ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–∏ —Ä–∞–∑–Ω–æ–≥–ª–∞—Å–∏–π –ø—Ä–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏.'}, 'en': {'title': 'Bridging the Gap: Enhancing LLMs in Predicting Human Annotation Disagreements', 'desc': "This paper investigates how well Large Language Models (LLMs) can predict disagreements among human annotations, which often indicate important nuances in tasks. While LLMs perform well in predicting majority labels, they struggle to capture the variation in human opinions, which is crucial for understanding task subjectivity. The study reveals that using RLVR-style reasoning, which typically enhances performance, actually worsens the models' ability to predict these disagreements. The authors emphasize the importance of developing better evaluation methods for LLMs to address this gap in understanding human annotation variation."}, 'zh': {'title': 'ÊèêÂçáLLMÂú®Ê†áÊ≥®‰∏ç‰∏ÄËá¥ÊÄßÈ¢ÑÊµã‰∏≠ÁöÑËÉΩÂäõ', 'desc': 'Êú¨Á†îÁ©∂Êé¢ËÆ®‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®È¢ÑÊµã‰∫∫Á±ªÊ†áÊ≥®‰∏ç‰∏ÄËá¥ÊÄßÊñπÈù¢ÁöÑË°®Áé∞„ÄÇÂ∞ΩÁÆ°LLMsÂú®È¢ÑÊµãÂ§öÊï∞Ê†áÁ≠æÊó∂Ë°®Áé∞ËâØÂ•ΩÔºå‰ΩÜÂÆÉ‰ª¨Âú®ÊçïÊçâ‰∫∫Á±ªÊ†áÊ≥®ÁöÑÂèòÂºÇÊÄßÊó∂Âç¥Â≠òÂú®Âõ∞Èöæ„ÄÇÊàë‰ª¨ÂèëÁé∞Ôºå‰ΩøÁî®Âº∫ÂåñÂ≠¶‰π†‰∏éÂèØÈ™åËØÅÂ•ñÂä±ÔºàRLVRÔºâÁöÑÊñπÊ≥ïËôΩÁÑ∂ËÉΩÊèêÂçáLLMsÁöÑÊï¥‰ΩìÊÄßËÉΩÔºå‰ΩÜÂç¥‰ºöÈôç‰ΩéÂÖ∂Âú®È¢ÑÊµãÊ†áÊ≥®‰∏ç‰∏ÄËá¥ÊÄßÊñπÈù¢ÁöÑËÉΩÂäõ„ÄÇÊàë‰ª¨ÁöÑÁªìÊûúÂº∫Ë∞É‰∫ÜÂú®‰∏ç‰∏ÄËá¥ÊÄßÂª∫Ê®°‰∏≠ËØÑ‰º∞ÂíåÊîπËøõLLMÊ†áÊ≥®Âô®ÁöÑÈáçË¶ÅÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2506.18951', 'title': 'SWE-SQL: Illuminating LLM Pathways to Solve User SQL Issues in\n  Real-World Applications', 'url': 'https://huggingface.co/papers/2506.18951', 'abstract': "A new benchmark and training environment for debugging SQL issues using advanced open-source models significantly improves their performance compared to proprietary solutions.  \t\t\t\t\tAI-generated summary \t\t\t\t Resolution of complex SQL issues persists as a significant bottleneck in real-world database applications. Current Large Language Models (LLMs), while adept at text-to-SQL translation, have not been rigorously evaluated on the more challenging task of debugging SQL issues. To address this gap, we introduce BIRD-CRITIC, a new SQL issue debugging benchmark comprising 530 PostgreSQL tasks (BIRD-CRITIC-PG) and 570 multi-dialect tasks (BIRD-CRITIC-Multi), distilled from authentic user issues and replayed within new environments to facilitate rigorous evaluation. Baseline evaluations underscore the task's complexity, with the leading reasoning model O3-Mini achieving only 38.87% success rate on BIRD-CRITIC-PG and 33.33% on BIRD-CRITIC-Multi. Meanwhile, advancing open-source models for database tasks is crucial for empowering local development while safeguarding data privacy. Therefore, we present Six-Gym (Sql-fIX-Gym), a training environment for elevating open-source model capabilities for SQL issue debugging. This environment leverages SQL-Rewind strategy, which automatically generates executable issue-solution datasets by reverse-engineering issues from verified SQLs. However, popular trajectory-based fine-tuning methods do not explore substantial supervisory signals. We further propose f-Plan Boosting, which extracts high-level debugging plans from SQL solutions, enabling teacher LLMs to produce 73.7% more successful trajectories for training. We integrate these components into an open-source agent, Bird-Fixer. Based on Qwen-2.5-Coder-14B, Bird-Fixer achieves 38.11% success rate on BIRD-CRITIC-PG and 29.65% on BIRD-CRITIC-Multi, surpassing leading proprietary models such as Claude-3.7-Sonnet and GPT-4.1, marking a significant step toward democratizing sophisticated SQL-debugging capabilities. The leaderboard and source code are available: https://bird-critic.github.io/", 'score': 14, 'issue_id': 4476, 'pub_date': '2025-06-23', 'pub_date_card': {'ru': '23 –∏—é–Ω—è', 'en': 'June 23', 'zh': '6Êúà23Êó•'}, 'hash': 'b6a4ef33fdb5cc7e', 'authors': ['Jinyang Li', 'Xiaolong Li', 'Ge Qu', 'Per Jacobsson', 'Bowen Qin', 'Binyuan Hui', 'Shuzheng Si', 'Nan Huo', 'Xiaohan Xu', 'Yue Zhang', 'Ziwei Tang', 'Yuanshuai Li', 'Florensia Widjaja', 'Xintong Zhu', 'Feige Zhou', 'Yongfeng Huang', 'Yannis Papakonstantinou', 'Fatma Ozcan', 'Chenhao Ma', 'Reynold Cheng'], 'affiliations': ['CUHK', 'CUHKSZ', 'Google Cloud', 'HKU STAR Lab', 'THU', 'The BIRD Team'], 'pdf_title_img': 'assets/pdf/title_img/2506.18951.jpg', 'data': {'categories': ['#optimization', '#training', '#open_source', '#reasoning', '#benchmark', '#agents'], 'emoji': 'üê¶', 'ru': {'title': '–û—Ç–∫—Ä—ã—Ç—ã–π –ò–ò –ø–æ–±–µ–∂–¥–∞–µ—Ç –ø—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è –≤ –æ—Ç–ª–∞–¥–∫–µ SQL', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ BIRD-CRITIC –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ SQL-–∑–∞–ø—Ä–æ—Å–æ–≤, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –∑–∞–¥–∞—á–∏ –¥–ª—è PostgreSQL –∏ –º—É–ª—å—Ç–∏–¥–∏–∞–ª–µ–∫—Ç–Ω—ã–µ –∑–∞–¥–∞—á–∏. –û–Ω–∏ —Ç–∞–∫–∂–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ —Å—Ä–µ–¥—É –æ–±—É—á–µ–Ω–∏—è Six-Gym –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –æ—Ç–∫—Ä—ã—Ç—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –æ—Ç–ª–∞–¥–∫–µ SQL. –ò—Å–ø–æ–ª—å–∑—É—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—é SQL-Rewind –∏ –º–µ—Ç–æ–¥ f-Plan Boosting, –æ–Ω–∏ —Å–æ–∑–¥–∞–ª–∏ –∞–≥–µ–Ω—Ç–∞ Bird-Fixer –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–æ–¥–µ–ª–∏ Qwen-2.5-Coder-14B. Bird-Fixer –ø—Ä–µ–≤–∑–æ—à–µ–ª –≤–µ–¥—É—â–∏–µ –ø—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤ —Ä–µ—à–µ–Ω–∏–∏ –∑–∞–¥–∞—á –æ—Ç–ª–∞–¥–∫–∏ SQL, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å –≤ –¥–µ–º–æ–∫—Ä–∞—Ç–∏–∑–∞—Ü–∏–∏ —ç—Ç–∏—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π.'}, 'en': {'title': 'Empowering SQL Debugging with Open-Source Innovations', 'desc': 'This paper introduces BIRD-CRITIC, a new benchmark for evaluating SQL debugging capabilities of machine learning models, featuring 530 PostgreSQL and 570 multi-dialect tasks derived from real user issues. It highlights the limitations of current Large Language Models (LLMs) in effectively debugging SQL problems, with the best existing model achieving only modest success rates. To enhance model performance, the authors present Six-Gym, a training environment that utilizes the SQL-Rewind strategy to create executable datasets for training. Additionally, they propose f-Plan Boosting to improve the learning process by extracting high-level debugging plans, resulting in better training outcomes for open-source models like Bird-Fixer, which outperforms some proprietary solutions.'}, 'zh': {'title': 'ÊèêÂçáÂºÄÊ∫êÊ®°ÂûãÔºåÁ†¥Ëß£SQLË∞ÉËØïÈöæÈ¢ò', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂü∫ÂáÜÊµãËØïÂíåËÆ≠ÁªÉÁéØÂ¢ÉÔºåÊó®Âú®ÈÄöËøáÂÖàËøõÁöÑÂºÄÊ∫êÊ®°ÂûãÊù•Ë∞ÉËØïSQLÈóÆÈ¢òÔºå‰ªéËÄåÊòæËëóÊèêÈ´òÊÄßËÉΩ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜBIRD-CRITICÔºåËøôÊòØ‰∏Ä‰∏™ÂåÖÂê´530‰∏™PostgreSQL‰ªªÂä°Âíå570‰∏™Â§öÊñπË®Ä‰ªªÂä°ÁöÑSQLË∞ÉËØïÂü∫ÂáÜÔºåÊó®Âú®ËØÑ‰º∞ÂΩìÂâçÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®Ë∞ÉËØïSQLÈóÆÈ¢ò‰∏äÁöÑËÉΩÂäõ„ÄÇ‰∏∫‰∫ÜÊèêÂçáÂºÄÊ∫êÊ®°ÂûãÁöÑË∞ÉËØïËÉΩÂäõÔºåÊàë‰ª¨ÂºÄÂèë‰∫ÜSix-GymËÆ≠ÁªÉÁéØÂ¢ÉÔºåÂπ∂ÂºïÂÖ•‰∫ÜSQL-RewindÁ≠ñÁï•Âíåf-Plan BoostingÊñπÊ≥ïÔºå‰ª•ÊèêÈ´òÊ®°ÂûãÁöÑÊàêÂäüÁéá„ÄÇÊúÄÁªàÔºåÂü∫‰∫éQwen-2.5-Coder-14BÁöÑBird-FixerÂú®BIRD-CRITICÂü∫ÂáÜ‰∏äÂèñÂæó‰∫Ü‰ºò‰∫éÈ¢ÜÂÖà‰∏ìÊúâÊ®°ÂûãÁöÑÊàêÁª©ÔºåÊ†áÂøóÁùÄSQLË∞ÉËØïËÉΩÂäõÁöÑÊ∞ë‰∏ªÂåñËøàÂá∫‰∫ÜÈáçË¶Å‰∏ÄÊ≠•„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2506.19850', 'title': 'Unified Vision-Language-Action Model', 'url': 'https://huggingface.co/papers/2506.19850', 'abstract': "UniVLA is a multimodal VLA model that autoregressively processes vision, language, and action as token sequences, incorporating world modeling for effective long-horizon policy learning and achieving state-of-the-art results across simulation and real-world benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-language-action models (VLAs) have garnered significant attention for their potential in advancing robotic manipulation. However, previous approaches predominantly rely on the general comprehension capabilities of vision-language models (VLMs) to generate action signals, often overlooking the rich temporal and causal structure embedded in visual observations. In this paper, we present UniVLA, a unified and native multimodal VLA model that autoregressively models vision, language, and action signals as discrete token sequences. This formulation enables flexible multimodal tasks learning, particularly from large-scale video data. By incorporating world modeling during post-training, UniVLA captures causal dynamics from videos, facilitating effective transfer to downstream policy learning--especially for long-horizon tasks. Our approach sets new state-of-the-art results across several widely used simulation benchmarks, including CALVIN, LIBERO, and Simplenv-Bridge, significantly surpassing previous methods. For example, UniVLA achieves 95.5% average success rate on LIBERO benchmark, surpassing pi0-FAST's 85.5%. We further demonstrate its broad applicability on real-world ALOHA manipulation and autonomous driving.", 'score': 9, 'issue_id': 4476, 'pub_date': '2025-06-24', 'pub_date_card': {'ru': '24 –∏—é–Ω—è', 'en': 'June 24', 'zh': '6Êúà24Êó•'}, 'hash': '72bc18c81e5ac0e4', 'authors': ['Yuqi Wang', 'Xinghang Li', 'Wenxuan Wang', 'Junbo Zhang', 'Yingyan Li', 'Yuntao Chen', 'Xinlong Wang', 'Zhaoxiang Zhang'], 'affiliations': ['BAAI', 'CASIA', 'HKISI', 'THU'], 'pdf_title_img': 'assets/pdf/title_img/2506.19850.jpg', 'data': {'categories': ['#multimodal', '#optimization', '#transfer_learning', '#video', '#benchmark', '#agents'], 'emoji': 'ü§ñ', 'ru': {'title': 'UniVLA: –ï–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å –∑—Ä–µ–Ω–∏—è, —è–∑—ã–∫–∞ –∏ –¥–µ–π—Å—Ç–≤–∏—è –¥–ª—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∏', 'desc': 'UniVLA - —ç—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—â–∞—è –∑—Ä–µ–Ω–∏–µ, —è–∑—ã–∫ –∏ –¥–µ–π—Å—Ç–≤–∏—è –∫–∞–∫ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ç–æ–∫–µ–Ω–æ–≤. –ú–æ–¥–µ–ª—å –≤–∫–ª—é—á–∞–µ—Ç –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –º–∏—Ä–∞ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –ø–æ–ª–∏—Ç–∏–∫–∏ –Ω–∞ –¥–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞—Ö. UniVLA –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —Å–∏–º—É–ª—è—Ü–∏–æ–Ω–Ω—ã—Ö –∏ —Ä–µ–∞–ª—å–Ω—ã—Ö —Ç–µ—Å—Ç–∞—Ö –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏. –ú–æ–¥–µ–ª—å –æ—Å–æ–±–µ–Ω–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞ –¥–ª—è –∑–∞–¥–∞—á —Å –¥–ª–∏—Ç–µ–ª—å–Ω—ã–º –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ–º –±–ª–∞–≥–æ–¥–∞—Ä—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —É–ª–∞–≤–ª–∏–≤–∞—Ç—å –ø—Ä–∏—á–∏–Ω–Ω–æ-—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å–≤—è–∑–∏ –∏–∑ –≤–∏–¥–µ–æ–¥–∞–Ω–Ω—ã—Ö.'}, 'en': {'title': 'UniVLA: Advancing Robotic Manipulation with Multimodal Learning', 'desc': 'UniVLA is a new multimodal vision-language-action (VLA) model that processes visual, linguistic, and action data as sequences of tokens. It improves upon previous models by incorporating world modeling, which helps it understand the causal relationships in video data. This allows UniVLA to learn effective policies for long-horizon tasks, achieving top performance on various benchmarks. The model has shown significant success in both simulated environments and real-world applications, outperforming existing methods.'}, 'zh': {'title': 'UniVLAÔºöÂ§öÊ®°ÊÄÅÂ≠¶‰π†ÁöÑÊñ∞Á™ÅÁ†¥', 'desc': 'UniVLAÊòØ‰∏ÄÁßçÂ§öÊ®°ÊÄÅËßÜËßâ-ËØ≠Ë®Ä-Âä®‰Ωú(VLA)Ê®°ÂûãÔºåÂÆÉ‰ª•Ëá™ÂõûÂΩíÁöÑÊñπÂºèÂ§ÑÁêÜËßÜËßâ„ÄÅËØ≠Ë®ÄÂíåÂä®‰Ωú‰ø°Âè∑ÔºåÂ∞ÜÂÖ∂‰Ωú‰∏∫Á¶ªÊï£ÁöÑÊ†áËÆ∞Â∫èÂàóËøõË°åÂª∫Ê®°„ÄÇËØ•Ê®°ÂûãÈÄöËøá‰∏ñÁïåÂª∫Ê®°Êù•ÊçïÊçâËßÜÈ¢ë‰∏≠ÁöÑÂõ†ÊûúÂä®ÊÄÅÔºå‰ªéËÄåÊúâÊïàÂú∞ËøõË°åÈïøÊúüÁ≠ñÁï•Â≠¶‰π†„ÄÇUniVLAÂú®Â§ö‰∏™ÂπøÊ≥õ‰ΩøÁî®ÁöÑ‰ªøÁúüÂü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫ÜÊñ∞ÁöÑÊúÄÂÖàËøõÁªìÊûúÔºåÊòæËëóË∂ÖË∂ä‰∫Ü‰πãÂâçÁöÑÊñπÊ≥ï„ÄÇÂÆÉÂú®LIBEROÂü∫ÂáÜÊµãËØï‰∏≠ËææÂà∞‰∫Ü95.5%ÁöÑÂπ≥ÂùáÊàêÂäüÁéáÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®Áé∞ÂÆû‰∏ñÁïå‰∏≠ÁöÑÂπøÊ≥õÂ∫îÁî®ÊΩúÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2506.19767', 'title': 'SRFT: A Single-Stage Method with Supervised and Reinforcement\n  Fine-Tuning for Reasoning', 'url': 'https://huggingface.co/papers/2506.19767', 'abstract': 'Supervised Reinforcement Fine-Tuning (SRFT) integrates Supervised Fine-Tuning and Reinforcement Learning through entropy-aware weighting to achieve high accuracy in language model optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have achieved remarkable progress in reasoning tasks, yet the optimal integration of Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) remains a fundamental challenge. Through comprehensive analysis of token distributions, learning dynamics, and integration mechanisms from entropy-based perspectives, we reveal key differences between these paradigms: SFT induces coarse-grained global changes to LLM policy distributions, while RL performs fine-grained selective optimizations, with entropy serving as a critical indicator of training effectiveness. Building on these observations, we propose Supervised Reinforcement Fine-Tuning (SRFT), a single-stage method that unifies both fine-tuning paradigms through entropy-aware weighting mechanisms. Our approach simultaneously applies SFT and RL to directly optimize the LLM using demonstrations and self-exploration rollouts rather than through two-stage sequential methods. Extensive experiments show that SRFT achieves 59.1% average accuracy, outperforming zero-RL methods by 9.0% on five mathematical reasoning benchmarks and 10.9% on three out-of-distribution benchmarks.', 'score': 9, 'issue_id': 4471, 'pub_date': '2025-06-24', 'pub_date_card': {'ru': '24 –∏—é–Ω—è', 'en': 'June 24', 'zh': '6Êúà24Êó•'}, 'hash': 'dfdf02be41523939', 'authors': ['Yuqian Fu', 'Tinghong Chen', 'Jiajun Chai', 'Xihuai Wang', 'Songjun Tu', 'Guojun Yin', 'Wei Lin', 'Qichao Zhang', 'Yuanheng Zhu', 'Dongbin Zhao'], 'affiliations': ['Institute of Automation, Chinese Academy of Sciences', 'Meituan', 'School of Artificial Intelligence, University of Chinese Academy of Sciences', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2506.19767.jpg', 'data': {'categories': ['#rl', '#rlhf', '#training', '#optimization', '#reasoning'], 'emoji': 'üß†', 'ru': {'title': '–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ supervised –∏ reinforcement learning –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π - Supervised Reinforcement Fine-Tuning (SRFT). –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç Supervised Fine-Tuning –∏ Reinforcement Learning, –∏—Å–ø–æ–ª—å–∑—É—è –º–µ—Ö–∞–Ω–∏–∑–º –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç–Ω—Ç—Ä–æ–ø–∏–∏. SRFT –ø—Ä–∏–º–µ–Ω—è–µ—Ç –æ–±–∞ –º–µ—Ç–æ–¥–∞ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ, –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É—è —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å –Ω–∞–ø—Ä—è–º—É—é —Å –ø–æ–º–æ—â—å—é –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–π –∏ —Å–∞–º–æ–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ SRFT –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –¥—Ä—É–≥–∏–µ –º–µ—Ç–æ–¥—ã –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –≤–Ω–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö.'}, 'en': {'title': 'Unifying Fine-Tuning for Superior Language Model Performance', 'desc': 'Supervised Reinforcement Fine-Tuning (SRFT) combines Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to enhance the performance of language models. It uses entropy-aware weighting to balance the strengths of both methods, allowing for better optimization of model policies. The paper highlights how SFT makes broad changes to model behavior, while RL focuses on specific improvements, with entropy being a key measure of success. SRFT has been shown to significantly improve accuracy on various reasoning tasks compared to traditional methods.'}, 'zh': {'title': 'ÁõëÁù£Âº∫ÂåñÂæÆË∞ÉÔºö‰ºòÂåñËØ≠Ë®ÄÊ®°ÂûãÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'ÁõëÁù£Âº∫ÂåñÂæÆË∞ÉÔºàSRFTÔºâÁªìÂêà‰∫ÜÁõëÁù£ÂæÆË∞ÉÂíåÂº∫ÂåñÂ≠¶‰π†ÔºåÈÄöËøáÁÜµÊÑüÁü•Âä†ÊùÉÂÆûÁé∞ËØ≠Ë®ÄÊ®°Âûã‰ºòÂåñÁöÑÈ´òÂáÜÁ°ÆÊÄß„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÁõëÁù£ÂæÆË∞É‰ºöÂØπËØ≠Ë®ÄÊ®°ÂûãÁöÑÁ≠ñÁï•ÂàÜÂ∏É‰∫ßÁîüÁ≤óÁ≤íÂ∫¶ÁöÑÂÖ®Â±ÄÂèòÂåñÔºåËÄåÂº∫ÂåñÂ≠¶‰π†ÂàôËøõË°åÁªÜÁ≤íÂ∫¶ÁöÑÈÄâÊã©ÊÄß‰ºòÂåñÔºåÁÜµÊòØËÆ≠ÁªÉÊïàÊûúÁöÑÈáçË¶ÅÊåáÊ†á„ÄÇSRFTÊñπÊ≥ïÈÄöËøáÁÜµÊÑüÁü•Âä†ÊùÉÊú∫Âà∂ÔºåÂ∞ÜËøô‰∏§ÁßçÂæÆË∞ÉËåÉÂºèÁªü‰∏Ä‰∏∫ÂçïÈò∂ÊÆµÊñπÊ≥ïÔºåÁõ¥Êé•‰ºòÂåñËØ≠Ë®ÄÊ®°Âûã„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåSRFTÂú®‰∫î‰∏™Êï∞Â≠¶Êé®ÁêÜÂü∫ÂáÜ‰∏äÂπ≥ÂùáÂáÜÁ°ÆÁéáËææÂà∞59.1%ÔºåÊØîÈõ∂Âº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÊèêÈ´ò‰∫Ü9.0%„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2506.14012', 'title': 'Lost in the Mix: Evaluating LLM Understanding of Code-Switched Text', 'url': 'https://huggingface.co/papers/2506.14012', 'abstract': "LLMs' comprehension and reasoning skills are evaluated under code-switching conditions, revealing that embedding English into other languages can improve understanding, while prompts and fine-tuning affect degradation mitigation differently.  \t\t\t\t\tAI-generated summary \t\t\t\t Code-switching (CSW) is the act of alternating between two or more languages within a single discourse. This phenomenon is widespread in multilingual communities, and increasingly prevalent in online content, where users naturally mix languages in everyday communication. As a result, Large Language Models (LLMs), now central to content processing and generation, are frequently exposed to code-switched inputs. Given their widespread use, it is crucial to understand how LLMs process and reason about such mixed-language text. This paper presents a systematic evaluation of LLM comprehension under code-switching by generating CSW variants of established reasoning and comprehension benchmarks. While degradation is evident when foreign tokens disrupt English textx2013even under linguistic constraintsx2013embedding English into other languages often improves comprehension. Though prompting yields mixed results, fine-tuning offers a more stable path to degradation mitigation.", 'score': 9, 'issue_id': 4474, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 –∏—é–Ω—è', 'en': 'June 16', 'zh': '6Êúà16Êó•'}, 'hash': 'd8a7231ee69205ba', 'authors': ['Amr Mohamed', 'Yang Zhang', 'Michalis Vazirgiannis', 'Guokan Shang'], 'affiliations': ['Ecole Polytechnique', 'MBZUAI'], 'pdf_title_img': 'assets/pdf/title_img/2506.14012.jpg', 'data': {'categories': ['#reasoning', '#low_resource', '#multilingual', '#long_context'], 'emoji': 'üîÄ', 'ru': {'title': '–Ø–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –≤ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–º –º–∏—Ä–µ: –ø–æ–Ω–∏–º–∞–Ω–∏–µ —á–µ—Ä–µ–∑ —Å–º–µ—à–µ–Ω–∏–µ', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –Ω–∞–≤—ã–∫–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ —É—Å–ª–æ–≤–∏—è—Ö –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è –∫–æ–¥–æ–≤. –í—ã—è–≤–ª–µ–Ω–æ, —á—Ç–æ –≤—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏–µ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ –≤ –¥—Ä—É–≥–∏–µ —è–∑—ã–∫–∏ –º–æ–∂–µ—Ç —É–ª—É—á—à–∏—Ç—å –ø–æ–Ω–∏–º–∞–Ω–∏–µ. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è –≤–ª–∏—è–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–æ–≤ –∏ —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –Ω–∞ —Å–Ω–∏–∂–µ–Ω–∏–µ –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Ç–æ–Ω–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω—ã–π –ø—É—Ç—å –∫ —Å–º—è–≥—á–µ–Ω–∏—é –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö —ç—Ñ—Ñ–µ–∫—Ç–æ–≤ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è –∫–æ–¥–æ–≤.'}, 'en': {'title': 'Enhancing LLM Comprehension through Code-Switching', 'desc': "This paper investigates how Large Language Models (LLMs) understand and reason when faced with code-switching, which is the mixing of languages in communication. The study finds that including English words within other languages can enhance the models' comprehension, despite some degradation when foreign words disrupt English text. It also examines the effects of different prompting techniques and highlights that fine-tuning the models provides a more reliable way to reduce comprehension issues. Overall, the research emphasizes the importance of understanding LLM performance in multilingual contexts, especially as code-switching becomes more common in digital communication."}, 'zh': {'title': '‰ª£Á†ÅÂàáÊç¢ÊèêÂçáÁêÜËß£ËÉΩÂäõÁöÑÁ†îÁ©∂', 'desc': 'Êú¨ÊñáÁ†îÁ©∂‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®‰ª£Á†ÅÂàáÊç¢Êù°‰ª∂‰∏ãÁöÑÁêÜËß£ÂíåÊé®ÁêÜËÉΩÂäõ„ÄÇ‰ª£Á†ÅÂàáÊç¢ÊòØÊåáÂú®Âêå‰∏Ä‰∫§ÊµÅ‰∏≠‰∫§Êõø‰ΩøÁî®‰∏§ÁßçÊàñÂ§öÁßçËØ≠Ë®ÄÔºåËøôÂú®Â§öËØ≠Ë®ÄÁ§æÂå∫‰∏≠ÈùûÂ∏∏ÊôÆÈÅç„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÂ∞ÜËã±ËØ≠ÂµåÂÖ•ÂÖ∂‰ªñËØ≠Ë®Ä‰∏≠ÂèØ‰ª•ÊèêÈ´òÁêÜËß£ËÉΩÂäõÔºåËÄåÊèêÁ§∫ÂíåÂæÆË∞ÉÂØπÂáèËΩªÊÄßËÉΩ‰∏ãÈôçÁöÑÂΩ±ÂìçÂàôÊúâÊâÄ‰∏çÂêå„ÄÇÊÄª‰ΩìËÄåË®ÄÔºåÂæÆË∞ÉÊèê‰æõ‰∫Ü‰∏ÄÁßçÊõ¥Á®≥ÂÆöÁöÑÊñπÂºèÊù•Â∫îÂØπËØ≠Ë®ÄÊ∑∑ÂêàÂ∏¶Êù•ÁöÑÊåëÊàò„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2506.19838', 'title': 'SimpleGVR: A Simple Baseline for Latent-Cascaded Video Super-Resolution', 'url': 'https://huggingface.co/papers/2506.19838', 'abstract': 'Researchers propose design principles for cascaded video super-resolution models to improve high-resolution video generation by introduces degradation strategies, timestep sampling, noise augmentation, and interleaving temporal units with sparse local attention.  \t\t\t\t\tAI-generated summary \t\t\t\t Latent diffusion models have emerged as a leading paradigm for efficient video generation. However, as user expectations shift toward higher-resolution outputs, relying solely on latent computation becomes inadequate. A promising approach involves decoupling the process into two stages: semantic content generation and detail synthesis. The former employs a computationally intensive base model at lower resolutions, while the latter leverages a lightweight cascaded video super-resolution (VSR) model to achieve high-resolution output. In this work, we focus on studying key design principles for latter cascaded VSR models, which are underexplored currently. First, we propose two degradation strategies to generate training pairs that better mimic the output characteristics of the base model, ensuring alignment between the VSR model and its upstream generator. Second, we provide critical insights into VSR model behavior through systematic analysis of (1) timestep sampling strategies, (2) noise augmentation effects on low-resolution (LR) inputs. These findings directly inform our architectural and training innovations. Finally, we introduce interleaving temporal unit and sparse local attention to achieve efficient training and inference, drastically reducing computational overhead. Extensive experiments demonstrate the superiority of our framework over existing methods, with ablation studies confirming the efficacy of each design choice. Our work establishes a simple yet effective baseline for cascaded video super-resolution generation, offering practical insights to guide future advancements in efficient cascaded synthesis systems.', 'score': 8, 'issue_id': 4470, 'pub_date': '2025-06-24', 'pub_date_card': {'ru': '24 –∏—é–Ω—è', 'en': 'June 24', 'zh': '6Êúà24Êó•'}, 'hash': 'a00bfa00a7f0f869', 'authors': ['Liangbin Xie', 'Yu Li', 'Shian Du', 'Menghan Xia', 'Xintao Wang', 'Fanghua Yu', 'Ziyan Chen', 'Pengfei Wan', 'Jiantao Zhou', 'Chao Dong'], 'affiliations': ['Kuaishou Technology', 'Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences', 'Shenzhen University of Advanced Technology', 'State Key Laboratory of Internet of Things for Smart City, University of Macau', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2506.19838.jpg', 'data': {'categories': ['#optimization', '#training', '#video', '#diffusion', '#architecture'], 'emoji': 'üé•', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Å–≤–µ—Ä—Ö—Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –∫–∞—Å–∫–∞–¥–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –ø—Ä–∏–Ω—Ü–∏–ø—ã –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∫–∞—Å–∫–∞–¥–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å–≤–µ—Ä—Ö—Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –≤–∏–¥–µ–æ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è. –û–Ω–∏ –≤–≤–æ–¥—è—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–∏, –≤—ã–±–æ—Ä–∫—É –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —à–∞–≥–æ–≤, –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—é —à—É–º–∞ –∏ —á–µ—Ä–µ–¥–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –±–ª–æ–∫–æ–≤ —Å —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–º –ª–æ–∫–∞–ª—å–Ω—ã–º –≤–Ω–∏–º–∞–Ω–∏–µ–º. –≠—Ç–∏ –º–µ—Ç–æ–¥—ã –ø–æ–∑–≤–æ–ª—è—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–±—É—á–∞—Ç—å –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª–∏, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∂–∞—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –Ω–∞–¥ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏.'}, 'en': {'title': 'Enhancing Video Quality with Smart Cascaded Super-Resolution', 'desc': "This paper presents new design principles for improving cascaded video super-resolution (VSR) models, which are essential for generating high-resolution videos. The authors introduce innovative degradation strategies and timestep sampling methods to enhance the training process, ensuring that the VSR model aligns well with the base model's output. They also explore the impact of noise augmentation on low-resolution inputs and propose techniques like interleaving temporal units and sparse local attention to optimize training efficiency. The results show that their framework outperforms existing methods, providing a solid foundation for future developments in video super-resolution."}, 'zh': {'title': 'ÊèêÂçáËßÜÈ¢ëË∂ÖÂàÜËæ®ÁéáÁîüÊàêÁöÑËÆæËÆ°ÂéüÂàô', 'desc': 'Êú¨Á†îÁ©∂ÊèêÂá∫‰∫ÜÁ∫ßËÅîËßÜÈ¢ëË∂ÖÂàÜËæ®ÁéáÊ®°ÂûãÁöÑËÆæËÆ°ÂéüÂàôÔºå‰ª•ÊèêÈ´òÈ´òÂàÜËæ®ÁéáËßÜÈ¢ëÁîüÊàêÁöÑÊïàÊûú„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫ÜÈÄÄÂåñÁ≠ñÁï•„ÄÅÊó∂Èó¥Ê≠•ÈááÊ†∑„ÄÅÂô™Â£∞Â¢ûÂº∫ÂíåÁ®ÄÁñèÂ±ÄÈÉ®Ê≥®ÊÑèÂäõÁ≠âÊñπÊ≥ïÔºåÊù•‰ºòÂåñÊ®°ÂûãÁöÑËÆ≠ÁªÉÂíåÊé®ÁêÜËøáÁ®ã„ÄÇÈÄöËøáÁ≥ªÁªüÂàÜÊûêÔºåÊàë‰ª¨Êè≠Á§∫‰∫ÜÊó∂Èó¥Ê≠•ÈááÊ†∑Á≠ñÁï•ÂíåÂô™Â£∞Â¢ûÂº∫ÂØπ‰ΩéÂàÜËæ®ÁéáËæìÂÖ•ÁöÑÂΩ±ÂìçÔºå‰ªéËÄåÊåáÂØºÊ®°ÂûãÊû∂ÊûÑÂíåËÆ≠ÁªÉÂàõÊñ∞„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®Áé∞ÊúâÊäÄÊúØ‰∏≠ÂÖ∑ÊúâÊòæËëó‰ºòÂäøÔºå‰∏∫Êú™Êù•È´òÊïàÁ∫ßËÅîÂêàÊàêÁ≥ªÁªüÁöÑÂèëÂ±ïÊèê‰æõ‰∫ÜÂÆûÁî®ÁöÑËßÅËß£„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2506.19794', 'title': 'Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic\n  Empirical Study', 'url': 'https://huggingface.co/papers/2506.19794', 'abstract': "Enhancements to open-source large language models' data analysis capabilities through strategic planning, interaction design, and data quality improvements were identified and applied.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) hold promise in automating data analysis tasks, yet open-source models face significant limitations in these kinds of reasoning-intensive scenarios. In this work, we investigate strategies to enhance the data analysis capabilities of open-source LLMs. By curating a seed dataset of diverse, realistic scenarios, we evaluate models across three dimensions: data understanding, code generation, and strategic planning. Our analysis reveals three key findings: (1) Strategic planning quality serves as the primary determinant of model performance; (2) Interaction design and task complexity significantly influence reasoning capabilities; (3) Data quality demonstrates a greater impact than diversity in achieving optimal performance. We leverage these insights to develop a data synthesis methodology, demonstrating significant improvements in open-source LLMs' analytical reasoning capabilities.", 'score': 8, 'issue_id': 4472, 'pub_date': '2025-06-24', 'pub_date_card': {'ru': '24 –∏—é–Ω—è', 'en': 'June 24', 'zh': '6Êúà24Êó•'}, 'hash': '590e2169fc8a24c8', 'authors': ['Yuqi Zhu', 'Yi Zhong', 'Jintian Zhang', 'Ziheng Zhang', 'Shuofei Qiao', 'Yujie Luo', 'Lun Du', 'Da Zheng', 'Huajun Chen', 'Ningyu Zhang'], 'affiliations': ['Ant Group', 'Zhejiang University', 'Zhejiang University - Ant Group Joint Laboratory of Knowledge Graph'], 'pdf_title_img': 'assets/pdf/title_img/2506.19794.jpg', 'data': {'categories': ['#synthetic', '#multimodal', '#benchmark', '#reasoning', '#open_source', '#dataset', '#data'], 'emoji': 'üìä', 'ru': {'title': '–°—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ - –∫–ª—é—á –∫ —É–ª—É—á—à–µ–Ω–∏—é –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö —É –æ—Ç–∫—Ä—ã—Ç—ã—Ö LLM', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ —É–ª—É—á—à–µ–Ω–∏—é –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö —É –æ—Ç–∫—Ä—ã—Ç—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä—ã –æ—Ü–µ–Ω–∏–≤–∞–ª–∏ –º–æ–¥–µ–ª–∏ –ø–æ —Ç—Ä–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º: –ø–æ–Ω–∏–º–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö, –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞ –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ. –ë—ã–ª–æ –≤—ã—è–≤–ª–µ–Ω–æ, —á—Ç–æ –∫–∞—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–æ–≥–æ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —è–≤–ª—è–µ—Ç—Å—è –∫–ª—é—á–µ–≤—ã–º —Ñ–∞–∫—Ç–æ—Ä–æ–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏. –†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è —Å–∏–Ω—Ç–µ–∑–∞ –¥–∞–Ω–Ω—ã—Ö –ø–æ–∑–≤–æ–ª–∏–ª–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∏—Ç—å –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –æ—Ç–∫—Ä—ã—Ç—ã—Ö LLM.'}, 'en': {'title': 'Boosting Open-Source LLMs for Better Data Analysis', 'desc': 'This paper explores how to improve the data analysis abilities of open-source Large Language Models (LLMs). It identifies that strategic planning is crucial for enhancing model performance in reasoning tasks. The study also highlights the importance of interaction design and task complexity, as well as the role of data quality over diversity in achieving better results. By applying these findings, the authors propose a new data synthesis method that significantly boosts the analytical reasoning skills of LLMs.'}, 'zh': {'title': 'ÊèêÂçáÂºÄÊ∫êLLMsÁöÑÊï∞ÊçÆÂàÜÊûêËÉΩÂäõ', 'desc': 'Êú¨Á†îÁ©∂Êé¢ËÆ®‰∫ÜÂ¶Ç‰ΩïÊèêÂçáÂºÄÊ∫êÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Êï∞ÊçÆÂàÜÊûê‰ªªÂä°‰∏≠ÁöÑËÉΩÂäõ„ÄÇÊàë‰ª¨ÈÄöËøáÁ≠ñÂàíÂ§öÊ†∑ÂåñÁöÑÁúüÂÆûÂú∫ÊôØÊï∞ÊçÆÈõÜÔºåËØÑ‰º∞Ê®°ÂûãÂú®Êï∞ÊçÆÁêÜËß£„ÄÅ‰ª£Á†ÅÁîüÊàêÂíåÊàòÁï•ËßÑÂàíÁ≠â‰∏â‰∏™Áª¥Â∫¶ÁöÑË°®Áé∞„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÊàòÁï•ËßÑÂàíÁöÑË¥®ÈáèÊòØÊ®°ÂûãÊÄßËÉΩÁöÑ‰∏ªË¶ÅÂÜ≥ÂÆöÂõ†Á¥†Ôºå‰∫§‰∫íËÆæËÆ°Âíå‰ªªÂä°Â§çÊùÇÊÄß‰πüÊòæËëóÂΩ±ÂìçÊé®ÁêÜËÉΩÂäõ„ÄÇÊ≠§Â§ñÔºåÊï∞ÊçÆË¥®ÈáèÂØπÂÆûÁé∞ÊúÄ‰Ω≥ÊÄßËÉΩÁöÑÂΩ±ÂìçÂ§ß‰∫éÊï∞ÊçÆÁöÑÂ§öÊ†∑ÊÄß„ÄÇÂü∫‰∫éËøô‰∫õÂèëÁé∞ÔºåÊàë‰ª¨ÂºÄÂèë‰∫Ü‰∏ÄÁßçÊï∞ÊçÆÂêàÊàêÊñπÊ≥ïÔºåÊòæËëóÊèêÂçá‰∫ÜÂºÄÊ∫êLLMsÁöÑÂàÜÊûêÊé®ÁêÜËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2506.19713', 'title': 'Guidance in the Frequency Domain Enables High-Fidelity Sampling at Low\n  CFG Scales', 'url': 'https://huggingface.co/papers/2506.19713', 'abstract': 'Frequency-decoupled guidance (FDG) enhances image quality and diversity by separately controlling low- and high-frequency guidance components in diffusion models, outperforming standard classifier-free guidance.  \t\t\t\t\tAI-generated summary \t\t\t\t Classifier-free guidance (CFG) has become an essential component of modern conditional diffusion models. Although highly effective in practice, the underlying mechanisms by which CFG enhances quality, detail, and prompt alignment are not fully understood. We present a novel perspective on CFG by analyzing its effects in the frequency domain, showing that low and high frequencies have distinct impacts on generation quality. Specifically, low-frequency guidance governs global structure and condition alignment, while high-frequency guidance mainly enhances visual fidelity. However, applying a uniform scale across all frequencies -- as is done in standard CFG -- leads to oversaturation and reduced diversity at high scales and degraded visual quality at low scales. Based on these insights, we propose frequency-decoupled guidance (FDG), an effective approach that decomposes CFG into low- and high-frequency components and applies separate guidance strengths to each component. FDG improves image quality at low guidance scales and avoids the drawbacks of high CFG scales by design. Through extensive experiments across multiple datasets and models, we demonstrate that FDG consistently enhances sample fidelity while preserving diversity, leading to improved FID and recall compared to CFG, establishing our method as a plug-and-play alternative to standard classifier-free guidance.', 'score': 8, 'issue_id': 4475, 'pub_date': '2025-06-24', 'pub_date_card': {'ru': '24 –∏—é–Ω—è', 'en': 'June 24', 'zh': '6Êúà24Êó•'}, 'hash': '5fb28071fbad67d8', 'authors': ['Seyedmorteza Sadat', 'Tobias Vontobel', 'Farnood Salehi', 'Romann M. Weber'], 'affiliations': ['Disney Research Studios', 'ETH Z√ºrich'], 'pdf_title_img': 'assets/pdf/title_img/2506.19713.jpg', 'data': {'categories': ['#training', '#diffusion', '#cv', '#optimization', '#dataset'], 'emoji': 'üé®', 'ru': {'title': '–ß–∞—Å—Ç–æ—Ç–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π', 'desc': '–ê–≤—Ç–æ—Ä—ã —Å—Ç–∞—Ç—å–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏, –Ω–∞–∑—ã–≤–∞–µ–º—ã–π —á–∞—Å—Ç–æ—Ç–Ω–æ-—Ä–∞–∑–≤—è–∑–∞–Ω–Ω—ã–º —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ–º (FDG). FDG —Ä–∞–∑–¥–µ–ª—è–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –Ω–∏–∑–∫–æ- –∏ –≤—ã—Å–æ–∫–æ—á–∞—Å—Ç–æ—Ç–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã, –ø—Ä–∏–º–µ–Ω—è—è –∫ –Ω–∏–º —Ä–∞–∑–ª–∏—á–Ω—É—é —Å–∏–ª—É —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç —É–ª—É—á—à–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º –±–µ–∑–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–Ω—ã–º —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ–º (CFG). –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ FDG –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å —Å—ç–º–ø–ª–æ–≤, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ —É–ª—É—á—à–µ–Ω–∏—é –º–µ—Ç—Ä–∏–∫ FID –∏ recall.'}, 'en': {'title': 'Decoupling Frequencies for Superior Image Generation', 'desc': 'This paper introduces Frequency-Decoupled Guidance (FDG), a new method for improving image generation in diffusion models. FDG separates the guidance into low-frequency and high-frequency components, allowing for tailored control over each aspect of image quality. The low-frequency guidance focuses on the overall structure and alignment with conditions, while high-frequency guidance enhances fine details and visual fidelity. By adjusting the strengths of these components independently, FDG avoids the common pitfalls of standard classifier-free guidance, resulting in higher quality images with better diversity.'}, 'zh': {'title': 'È¢ëÁéáËß£ËÄ¶ÂºïÂØºÔºöÊèêÂçáÂõæÂÉèË¥®Èáè‰∏éÂ§öÊ†∑ÊÄßÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'È¢ëÁéáËß£ËÄ¶ÂºïÂØºÔºàFDGÔºâÈÄöËøáÂàÜÂà´ÊéßÂà∂Êâ©Êï£Ê®°Âûã‰∏≠ÁöÑ‰ΩéÈ¢ëÂíåÈ´òÈ¢ëÂºïÂØºÊàêÂàÜÔºåÊèêÂçá‰∫ÜÂõæÂÉèÁöÑË¥®ÈáèÂíåÂ§öÊ†∑ÊÄßÔºåË∂ÖË∂ä‰∫ÜÊ†áÂáÜÁöÑÊó†ÂàÜÁ±ªÂô®ÂºïÂØºÔºàCFGÔºâ„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºå‰ΩéÈ¢ëÂºïÂØº‰∏ªË¶ÅÂΩ±ÂìçÂÖ®Â±ÄÁªìÊûÑÂíåÊù°‰ª∂ÂØπÈΩêÔºåËÄåÈ´òÈ¢ëÂºïÂØºÂàôÂ¢ûÂº∫ËßÜËßâÁªÜËäÇ„ÄÇ‰º†ÁªüÁöÑCFGÂú®ÊâÄÊúâÈ¢ëÁéá‰∏ä‰ΩøÁî®Áªü‰∏ÄÁöÑÊØî‰æãÔºåÂØºËá¥È´òÈ¢ëÊó∂ÁöÑËøáÈ•±ÂíåÂíåÂ§öÊ†∑ÊÄßÈôç‰ΩéÔºå‰ΩéÈ¢ëÊó∂ÁöÑËßÜËßâË¥®Èáè‰∏ãÈôç„ÄÇFDGÈÄöËøáÂ∞ÜCFGÂàÜËß£‰∏∫‰ΩéÈ¢ëÂíåÈ´òÈ¢ëÊàêÂàÜÔºåÂπ∂ÂØπÊØè‰∏™ÊàêÂàÜÂ∫îÁî®‰∏çÂêåÁöÑÂºïÂØºÂº∫Â∫¶ÔºåÊúâÊïàÊîπÂñÑ‰∫ÜÂõæÂÉèË¥®ÈáèÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜÊ†∑Êú¨ÁöÑÂ§öÊ†∑ÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2506.18843', 'title': 'USAD: Universal Speech and Audio Representation via Distillation', 'url': 'https://huggingface.co/papers/2506.18843', 'abstract': 'USAD integrates diverse audio types using efficient layer-to-layer distillation from domain-specific models, achieving competitive performance across various benchmarks with a single encoder.  \t\t\t\t\tAI-generated summary \t\t\t\t Self-supervised learning (SSL) has revolutionized audio representations, yet models often remain domain-specific, focusing on either speech or non-speech tasks. In this work, we present Universal Speech and Audio Distillation (USAD), a unified approach to audio representation learning that integrates diverse audio types - speech, sound, and music - into a single model. USAD employs efficient layer-to-layer distillation from domain-specific SSL models to train a student on a comprehensive audio dataset. USAD offers competitive performance across various benchmarks and datasets, including frame and instance-level speech processing tasks, audio tagging, and sound classification, achieving near state-of-the-art results with a single encoder on SUPERB and HEAR benchmarks.', 'score': 8, 'issue_id': 4472, 'pub_date': '2025-06-23', 'pub_date_card': {'ru': '23 –∏—é–Ω—è', 'en': 'June 23', 'zh': '6Êúà23Êó•'}, 'hash': 'c7855e42be638a70', 'authors': ['Heng-Jui Chang', 'Saurabhchand Bhati', 'James Glass', 'Alexander H. Liu'], 'affiliations': ['MIT CSAIL Cambridge, MA, USA'], 'pdf_title_img': 'assets/pdf/title_img/2506.18843.jpg', 'data': {'categories': ['#benchmark', '#audio'], 'emoji': 'üéß', 'ru': {'title': '–ï–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≤—Å–µ—Ö —Ç–∏–ø–æ–≤ –∞—É–¥–∏–æ', 'desc': 'USAD - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã—Ö –∞—É–¥–∏–æ–ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π —Ä–µ—á—å, –∑–≤—É–∫–∏ –∏ –º—É–∑—ã–∫—É –≤ –µ–¥–∏–Ω–æ–π –º–æ–¥–µ–ª–∏. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—é –æ—Ç —Å–ª–æ—è –∫ —Å–ª–æ—é –∏–∑ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å—Ç—É–¥–µ–Ω—Ç–∞ –Ω–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω–æ–º –∞—É–¥–∏–æ–¥–∞—Ç–∞—Å–µ—Ç–µ. USAD –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, –≤–∫–ª—é—á–∞—è –∑–∞–¥–∞—á–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–µ—á–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ –∫–∞–¥—Ä–æ–≤ –∏ —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤, —Ç–µ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∞—É–¥–∏–æ –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –∑–≤—É–∫–æ–≤. –ú–æ–¥–µ–ª—å –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –±–ª–∏–∑–∫–∏—Ö –∫ state-of-the-art —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –µ–¥–∏–Ω–æ–≥–æ —ç–Ω–∫–æ–¥–µ—Ä–∞ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö SUPERB –∏ HEAR.'}, 'en': {'title': 'One Model, Many Sounds: USAD Unifies Audio Learning', 'desc': 'The paper introduces Universal Speech and Audio Distillation (USAD), a novel method for learning audio representations that combines different audio types such as speech, sound, and music into one model. It utilizes layer-to-layer distillation from specialized self-supervised learning (SSL) models, allowing a student model to learn from a wide-ranging audio dataset. This approach enables USAD to perform well on various tasks, including speech processing, audio tagging, and sound classification. The results demonstrate that USAD achieves competitive performance on multiple benchmarks with just a single encoder, showcasing its efficiency and versatility in audio representation learning.'}, 'zh': {'title': 'Áªü‰∏ÄÈü≥È¢ëË°®Á§∫ÔºåÊèêÂçáÂ§ö‰ªªÂä°ÊÄßËÉΩ', 'desc': 'USADÊòØ‰∏ÄÁßçÁªü‰∏ÄÁöÑÈü≥È¢ëË°®Á§∫Â≠¶‰π†ÊñπÊ≥ïÔºåËÉΩÂ§üÊï¥ÂêàÂ§öÁßçÈü≥È¢ëÁ±ªÂûãÔºåÂåÖÊã¨ËØ≠Èü≥„ÄÅÂ£∞Èü≥ÂíåÈü≥‰πê„ÄÇÂÆÉÈÄöËøá‰ªéÁâπÂÆöÈ¢ÜÂüüÁöÑËá™ÁõëÁù£Â≠¶‰π†Ê®°Âûã‰∏≠ËøõË°åÈ´òÊïàÁöÑÂ±ÇÈó¥Ëí∏È¶èÔºåËÆ≠ÁªÉ‰∏Ä‰∏™Â≠¶ÁîüÊ®°ÂûãÔºå‰ª•‰æøÂú®‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑÈü≥È¢ëÊï∞ÊçÆÈõÜ‰∏äËøõË°åÂ≠¶‰π†„ÄÇUSADÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂåÖÊã¨ËØ≠Èü≥Â§ÑÁêÜ„ÄÅÈü≥È¢ëÊ†áËÆ∞ÂíåÂ£∞Èü≥ÂàÜÁ±ª‰ªªÂä°„ÄÇËØ•ÊñπÊ≥ï‰ΩøÁî®Âçï‰∏ÄÁºñÁ†ÅÂô®ÔºåËææÂà∞‰∫ÜÊé•ËøëÊúÄÂÖàËøõÁöÑÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2506.19830', 'title': 'Scaling Speculative Decoding with Lookahead Reasoning', 'url': 'https://huggingface.co/papers/2506.19830', 'abstract': 'Lookahead Reasoning enhances the speed of speculative decoding by introducing step-level parallelism, improving speedup over token-level decoding while maintaining answer quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Reasoning models excel by generating long chain-of-thoughts, but decoding the resulting thousands of tokens is slow. Token-level speculative decoding (SD) helps, but its benefit is capped, because the chance that an entire gamma-token guess is correct falls exponentially as gamma grows. This means allocating more compute for longer token drafts faces an algorithmic ceiling -- making the speedup modest and hardware-agnostic. We raise this ceiling with Lookahead Reasoning, which exploits a second, step-level layer of parallelism. Our key insight is that reasoning models generate step-by-step, and each step needs only to be semantically correct, not exact token matching. In Lookahead Reasoning, a lightweight draft model proposes several future steps; the target model expands each proposal in one batched pass, and a verifier keeps semantically correct steps while letting the target regenerate any that fail. Token-level SD still operates within each reasoning step, so the two layers of parallelism multiply. We show Lookahead Reasoning lifts the peak speedup of SD both theoretically and empirically. Across GSM8K, AIME, and other benchmarks, Lookahead Reasoning improves the speedup of SD from 1.4x to 2.1x while preserving answer quality, and its speedup scales better with additional GPU throughput. Our code is available at https://github.com/hao-ai-lab/LookaheadReasoning', 'score': 5, 'issue_id': 4482, 'pub_date': '2025-06-24', 'pub_date_card': {'ru': '24 –∏—é–Ω—è', 'en': 'June 24', 'zh': '6Êúà24Êó•'}, 'hash': '1310e53c2a30fcac', 'authors': ['Yichao Fu', 'Rui Ge', 'Zelei Shao', 'Zhijie Deng', 'Hao Zhang'], 'affiliations': ['Shanghai Jiao Tong University', 'UCSD', 'UIUC'], 'pdf_title_img': 'assets/pdf/title_img/2506.19830.jpg', 'data': {'categories': ['#training', '#optimization', '#reasoning', '#benchmark'], 'emoji': 'üöÄ', 'ru': {'title': '–£—Å–∫–æ—Ä–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ Lookahead Reasoning, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —Å–∫–æ—Ä–æ—Å—Ç—å —Å–ø–µ–∫—É–ª—è—Ç–∏–≤–Ω–æ–≥–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –≤–≤–æ–¥–∏—Ç –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º –Ω–∞ —É—Ä–æ–≤–Ω–µ —à–∞–≥–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–µ–æ–¥–æ–ª–µ—Ç—å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Ç–æ–∫–µ–Ω-—É—Ä–æ–≤–Ω–µ–≤–æ–≥–æ —Å–ø–µ–∫—É–ª—è—Ç–∏–≤–Ω–æ–≥–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è. Lookahead Reasoning –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ª–µ–≥–∫–æ–≤–µ—Å–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –±—É–¥—É—â–∏—Ö —à–∞–≥–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –∑–∞—Ç–µ–º –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è —Ü–µ–ª–µ–≤–æ–π –º–æ–¥–µ–ª—å—é –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ. –ú–µ—Ç–æ–¥ –ø–æ–∫–∞–∑–∞–ª —É–≤–µ–ª–∏—á–µ–Ω–∏–µ —É—Å–∫–æ—Ä–µ–Ω–∏—è —Å 1.4x –¥–æ 2.1x –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–æ–≤.'}, 'en': {'title': 'Boosting Decoding Speed with Lookahead Reasoning', 'desc': 'This paper introduces Lookahead Reasoning, a method that enhances the efficiency of speculative decoding in machine learning models. By implementing step-level parallelism, it allows for faster processing of reasoning tasks while maintaining the quality of the generated answers. The approach involves a lightweight draft model that proposes future steps, which are then expanded by a target model, ensuring semantic correctness without requiring exact token matches. The results show a significant improvement in speedup from 1.4x to 2.1x over traditional token-level decoding methods, especially when utilizing more GPU resources.'}, 'zh': {'title': 'ÂâçÁûªÊé®ÁêÜÔºöÊèêÂçáÊé®ÊµãËß£Á†ÅÈÄüÂ∫¶ÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫ÂâçÁûªÊé®ÁêÜÔºàLookahead ReasoningÔºâÁöÑÊñπÊ≥ïÔºåÊó®Âú®ÊèêÈ´òÊé®ÊµãËß£Á†ÅÁöÑÈÄüÂ∫¶„ÄÇÈÄöËøáÂºïÂÖ•Ê≠•È™§Á∫ßÂπ∂Ë°åÊÄßÔºåÂâçÁûªÊé®ÁêÜÂú®‰øùÊåÅÁ≠îÊ°àË¥®ÈáèÁöÑÂêåÊó∂ÔºåÊòæËëóÊèêÂçá‰∫ÜÈÄüÂ∫¶„ÄÇ‰º†ÁªüÁöÑÂü∫‰∫é‰ª§ÁâåÁöÑÊé®ÊµãËß£Á†ÅÂú®Â§ÑÁêÜÈïøÈìæÊé®ÁêÜÊó∂Èù¢‰∏¥ÁÆóÊ≥ïÁì∂È¢àÔºåËÄåÂâçÁûªÊé®ÁêÜÈÄöËøáËΩªÈáèÁ∫ßËçâÁ®øÊ®°ÂûãÂíåÁõÆÊ†áÊ®°ÂûãÁöÑÊâπÈáèÊâ©Â±ïÔºåÂÖãÊúç‰∫ÜËøô‰∏ÄÈôêÂà∂„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂâçÁûªÊé®ÁêÜÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Â∞ÜÊé®ÊµãËß£Á†ÅÁöÑÈÄüÂ∫¶ÊèêÂçá‰ªé1.4ÂÄçÊèêÈ´òÂà∞2.1ÂÄç„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2506.19807', 'title': 'KnowRL: Exploring Knowledgeable Reinforcement Learning for Factuality', 'url': 'https://huggingface.co/papers/2506.19807', 'abstract': 'KnowRL, a knowledge-enhanced reinforcement learning approach, reduces hallucinations in slow-thinking large language models by incorporating factuality rewards based on knowledge verification during training.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs), particularly slow-thinking models, often exhibit severe hallucination, outputting incorrect content due to an inability to accurately recognize knowledge boundaries during reasoning. While Reinforcement Learning (RL) can enhance complex reasoning abilities, its outcome-oriented reward mechanism often lacks factual supervision over the thinking process, further exacerbating the hallucination problem. To address the high hallucination in slow-thinking models, we propose Knowledge-enhanced RL, KnowRL. KnowRL guides models to perform fact-based slow thinking by integrating a factuality reward, based on knowledge verification, into the RL training process, helping them recognize their knowledge boundaries. KnowRL guides models to perform fact-based slow thinking by integrating a factuality reward, based on knowledge verification, into the RL training process, helping them recognize their knowledge boundaries. This targeted factual input during RL training enables the model to learn and internalize fact-based reasoning strategies. By directly rewarding adherence to facts within the reasoning steps, KnowRL fosters a more reliable thinking process. Experimental results on three hallucination evaluation datasets and two reasoning evaluation datasets demonstrate that KnowRL effectively mitigates hallucinations in slow-thinking models while maintaining their original strong reasoning capabilities. Our code is available at https://github.com/zjunlp/KnowRL.', 'score': 5, 'issue_id': 4472, 'pub_date': '2025-06-24', 'pub_date_card': {'ru': '24 –∏—é–Ω—è', 'en': 'June 24', 'zh': '6Êúà24Êó•'}, 'hash': '2f1016b014bf6ee5', 'authors': ['Baochang Ren', 'Shuofei Qiao', 'Wenhao Yu', 'Huajun Chen', 'Ningyu Zhang'], 'affiliations': ['Tencent AI Seattle Lab', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.19807.jpg', 'data': {'categories': ['#hallucinations', '#rl', '#rlhf', '#reasoning', '#training'], 'emoji': 'üß†', 'ru': {'title': '–§–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –¥–ª—è –ò–ò: KnowRL –ø—Ä–æ—Ç–∏–≤ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π', 'desc': 'KnowRL - —ç—Ç–æ –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, —É–ª—É—á—à–µ–Ω–Ω—ã–π –∑–Ω–∞–Ω–∏—è–º–∏, –∫–æ—Ç–æ—Ä—ã–π —Å–Ω–∏–∂–∞–µ—Ç –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏ –≤ –º–µ–¥–ª–µ–Ω–Ω–æ –º—ã—Å–ª—è—â–∏—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –∑–∞ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ –ø—Ä–æ–≤–µ—Ä–∫–µ –∑–Ω–∞–Ω–∏–π –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è. KnowRL –Ω–∞–ø—Ä–∞–≤–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ –Ω–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –º–µ–¥–ª–µ–Ω–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è, –æ—Å–Ω–æ–≤–∞–Ω–Ω–æ–≥–æ –Ω–∞ —Ñ–∞–∫—Ç–∞—Ö, –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É—è –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ –∑–∞ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –≤ –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ KnowRL —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏ –≤ –º–µ–¥–ª–µ–Ω–Ω–æ –º—ã—Å–ª—è—â–∏—Ö –º–æ–¥–µ–ª—è—Ö, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º –∏—Ö –∏—Å—Ö–æ–¥–Ω—ã–µ —Å–∏–ª—å–Ω—ã–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é.'}, 'en': {'title': 'Enhancing Truthfulness in AI with KnowRL', 'desc': "KnowRL is a novel approach that enhances reinforcement learning by incorporating factuality rewards to reduce hallucinations in slow-thinking large language models. These models often generate incorrect information due to their inability to recognize the limits of their knowledge. By integrating knowledge verification into the training process, KnowRL encourages models to engage in fact-based reasoning. Experimental results show that this method effectively decreases hallucinations while preserving the models' reasoning abilities."}, 'zh': {'title': 'Áü•ËØÜÂ¢ûÂº∫ÁöÑÂº∫ÂåñÂ≠¶‰π†ÔºåÂáèÂ∞ëÂπªËßâÁé∞Ë±°', 'desc': 'KnowRLÊòØ‰∏ÄÁßçÂ¢ûÂº∫Áü•ËØÜÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÔºåÈÄöËøáÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÂºïÂÖ•Âü∫‰∫éÁü•ËØÜÈ™åËØÅÁöÑ‰∫ãÂÆûÂ•ñÂä±ÔºåÂáèÂ∞ë‰∫ÜÊÖ¢ÊÄùÁª¥Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÂπªËßâÁé∞Ë±°„ÄÇÊÖ¢ÊÄùÁª¥Ê®°ÂûãÂ∏∏Â∏∏Âõ†‰∏∫Êó†Ê≥ïÂáÜÁ°ÆËØÜÂà´Áü•ËØÜËæπÁïåËÄåËæìÂá∫ÈîôËØØÂÜÖÂÆπÔºåÂØºËá¥‰∏•ÈáçÁöÑÂπªËßâÈóÆÈ¢ò„ÄÇKnowRLÈÄöËøáÂ∞Ü‰∫ãÂÆûÂ•ñÂä±Êï¥ÂêàÂà∞Âº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉ‰∏≠ÔºåÊåáÂØºÊ®°ÂûãËøõË°åÂü∫‰∫é‰∫ãÂÆûÁöÑÊÖ¢ÊÄùÁª¥Ôºå‰ªéËÄåÂ∏ÆÂä©ÂÆÉ‰ª¨ËØÜÂà´Áü•ËØÜËæπÁïå„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåKnowRLÂú®ÂáèËΩªÂπªËßâÁöÑÂêåÊó∂Ôºå‰øùÊåÅ‰∫ÜÊ®°ÂûãÁöÑÂº∫Â§ßÊé®ÁêÜËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2506.19839', 'title': 'Improving Progressive Generation with Decomposable Flow Matching', 'url': 'https://huggingface.co/papers/2506.19839', 'abstract': 'Decomposable Flow Matching (DFM) framework enhances visual generation and video quality by applying Flow Matching at multiple scales without requiring complex multi-stage architectures.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating high-dimensional visual modalities is a computationally intensive task. A common solution is progressive generation, where the outputs are synthesized in a coarse-to-fine spectral autoregressive manner. While diffusion models benefit from the coarse-to-fine nature of denoising, explicit multi-stage architectures are rarely adopted. These architectures have increased the complexity of the overall approach, introducing the need for a custom diffusion formulation, decomposition-dependent stage transitions, add-hoc samplers, or a model cascade. Our contribution, Decomposable Flow Matching (DFM), is a simple and effective framework for the progressive generation of visual media. DFM applies Flow Matching independently at each level of a user-defined multi-scale representation (such as Laplacian pyramid). As shown by our experiments, our approach improves visual quality for both images and videos, featuring superior results compared to prior multistage frameworks. On Imagenet-1k 512px, DFM achieves 35.2% improvements in FDD scores over the base architecture and 26.4% over the best-performing baseline, under the same training compute. When applied to finetuning of large models, such as FLUX, DFM shows faster convergence speed to the training distribution. Crucially, all these advantages are achieved with a single model, architectural simplicity, and minimal modifications to existing training pipelines.', 'score': 4, 'issue_id': 4484, 'pub_date': '2025-06-24', 'pub_date_card': {'ru': '24 –∏—é–Ω—è', 'en': 'June 24', 'zh': '6Êúà24Êó•'}, 'hash': '02798e339aace939', 'authors': ['Moayed Haji-Ali', 'Willi Menapace', 'Ivan Skorokhodov', 'Arpit Sahni', 'Sergey Tulyakov', 'Vicente Ordonez', 'Aliaksandr Siarohin'], 'affiliations': ['Rice University', 'Snap Inc'], 'pdf_title_img': 'assets/pdf/title_img/2506.19839.jpg', 'data': {'categories': ['#video', '#diffusion', '#optimization', '#architecture', '#training', '#cv'], 'emoji': 'üñºÔ∏è', 'ru': {'title': '–ú–Ω–æ–≥–æ–º–∞—Å—à—Ç–∞–±–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ —É—Å–ª–æ–∂–Ω–µ–Ω–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã', 'desc': '–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –Ω–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Decomposable Flow Matching (DFM) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞. DFM –ø—Ä–∏–º–µ–Ω—è–µ—Ç –º–µ—Ç–æ–¥ Flow Matching –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–∞—Å—à—Ç–∞–±–∞—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –Ω–µ —Ç—Ä–µ–±—É—è —Å–ª–æ–∂–Ω—ã—Ö –º–Ω–æ–≥–æ—Å—Ç—É–ø–µ–Ω—á–∞—Ç—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ DFM –ø–æ–≤—ã—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≤–∏–¥–µ–æ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –ø—Ä–æ—Å—Ç–æ—Ç–æ–π –∏ –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º–∏ –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–∞—Ö –æ–±—É—á–µ–Ω–∏—è.'}, 'en': {'title': 'Simplifying Visual Generation with Decomposable Flow Matching', 'desc': 'The Decomposable Flow Matching (DFM) framework simplifies the process of generating high-dimensional visual content by applying Flow Matching at various scales. Unlike traditional methods that rely on complex multi-stage architectures, DFM operates effectively with a single model, enhancing both image and video quality. It utilizes a user-defined multi-scale representation, such as a Laplacian pyramid, to progressively generate visuals, leading to significant improvements in fidelity and convergence speed. Experimental results demonstrate that DFM outperforms existing methods, achieving notable gains in FDD scores while maintaining architectural simplicity.'}, 'zh': {'title': 'ÁÆÄÂåñËßÜËßâÁîüÊàêÁöÑÂèØÂàÜËß£ÊµÅÂåπÈÖçÊ°ÜÊû∂', 'desc': 'ÂèØÂàÜËß£ÊµÅÂåπÈÖçÔºàDFMÔºâÊ°ÜÊû∂ÈÄöËøáÂú®Â§ö‰∏™Â∞∫Â∫¶‰∏äÂ∫îÁî®ÊµÅÂåπÈÖçÔºåÂ¢ûÂº∫‰∫ÜËßÜËßâÁîüÊàêÂíåËßÜÈ¢ëË¥®ÈáèÔºåËÄåÊó†ÈúÄÂ§çÊùÇÁöÑÂ§öÈò∂ÊÆµÊû∂ÊûÑ„ÄÇDFMÂú®Áî®Êà∑ÂÆö‰πâÁöÑÂ§öÂ∞∫Â∫¶Ë°®Á§∫ÁöÑÊØè‰∏™Â±ÇÊ¨°‰∏äÁã¨Á´ãÂ∫îÁî®ÊµÅÂåπÈÖçÔºå‰ªéËÄåÁÆÄÂåñ‰∫ÜÁîüÊàêËøáÁ®ã„ÄÇÂÆûÈ™åË°®ÊòéÔºåDFMÂú®ÂõæÂÉèÂíåËßÜÈ¢ëÁöÑËßÜËßâË¥®Èáè‰∏äÂùá‰ºò‰∫é‰ª•ÂæÄÁöÑÂ§öÈò∂ÊÆµÊ°ÜÊû∂ÔºåÂ∞§ÂÖ∂Âú®Imagenet-1k 512pxÊï∞ÊçÆÈõÜ‰∏äÔºåFDDÂæóÂàÜÊèêÈ´ò‰∫Ü35.2%„ÄÇÊ≠§Â§ñÔºåDFMÂú®ÂæÆË∞ÉÂ§ßÂûãÊ®°ÂûãÊó∂ÊòæÁ§∫Âá∫Êõ¥Âø´ÁöÑÊî∂ÊïõÈÄüÂ∫¶ÔºåÊâÄÊúâËøô‰∫õ‰ºòÂäøÈÉΩÂú®Âçï‰∏ÄÊ®°ÂûãÂíåÁÆÄÂçïÊû∂ÊûÑ‰∏ãÂÆûÁé∞„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2506.16095', 'title': 'Intelligent Operation and Maintenance and Prediction Model Optimization\n  for Improving Wind Power Generation Efficiency', 'url': 'https://huggingface.co/papers/2506.16095', 'abstract': 'This study explores the effectiveness of predictive maintenance models and the optimization of intelligent Operation and Maintenance (O&M) systems in improving wind power generation efficiency. Through qualitative research, structured interviews were conducted with five wind farm engineers and maintenance managers, each with extensive experience in turbine operations. Using thematic analysis, the study revealed that while predictive maintenance models effectively reduce downtime by identifying major faults, they often struggle with detecting smaller, gradual failures. Key challenges identified include false positives, sensor malfunctions, and difficulties in integrating new models with older turbine systems. Advanced technologies such as digital twins, SCADA systems, and condition monitoring have significantly enhanced turbine maintenance practices. However, these technologies still require improvements, particularly in AI refinement and real-time data integration. The findings emphasize the need for continuous development to fully optimize wind turbine performance and support the broader adoption of renewable energy.', 'score': 4, 'issue_id': 4479, 'pub_date': '2025-06-19', 'pub_date_card': {'ru': '19 –∏—é–Ω—è', 'en': 'June 19', 'zh': '6Êúà19Êó•'}, 'hash': 'b6bac9ae1a6539ff', 'authors': ['Xun Liu', 'Xiaobin Wu', 'Jiaqi He', 'Rajan Das Gupta'], 'affiliations': ['Department of Computer Science and Engineering Jahangirnagar University Dhaka, Bangladesh', 'Faculty of Education Shinawatra University Bangkok, Thailand', 'School of Automotive Engineering Chengdu Industry And Trade College Chengdu, China', 'Tilburg School of Social and Behavioral Sciences Tilburg University Tilburg, The Netherlands'], 'pdf_title_img': 'assets/pdf/title_img/2506.16095.jpg', 'data': {'categories': ['#data', '#healthcare', '#training', '#optimization', '#science'], 'emoji': 'üå¨Ô∏è', 'ru': {'title': '–ü—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω–æ–µ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ –≤–µ—Ç—Ä—è–Ω—ã—Ö —Ç—É—Ä–±–∏–Ω: –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –∏ –≤—ã–∑–æ–≤—ã', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –ø—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏—è –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã—Ö —Å–∏—Å—Ç–µ–º —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏–∏ –∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏—è –≤ –≤–µ—Ç—Ä–æ—ç–Ω–µ—Ä–≥–µ—Ç–∏–∫–µ. –ê–Ω–∞–ª–∏–∑ –∏–Ω—Ç–µ—Ä–≤—å—é —Å –∏–Ω–∂–µ–Ω–µ—Ä–∞–º–∏ –≤–µ—Ç—Ä—è–Ω—ã—Ö —ç–ª–µ–∫—Ç—Ä–æ—Å—Ç–∞–Ω—Ü–∏–π –ø–æ–∫–∞–∑–∞–ª, —á—Ç–æ –º–æ–¥–µ–ª–∏ —Ö–æ—Ä–æ—à–æ –≤—ã—è–≤–ª—è—é—Ç –∫—Ä—É–ø–Ω—ã–µ –Ω–µ–∏—Å–ø—Ä–∞–≤–Ω–æ—Å—Ç–∏, –Ω–æ –º–µ–Ω–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã –≤ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–∏ –º–µ–ª–∫–∏—Ö –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω—ã—Ö –æ—Ç–∫–∞–∑–æ–≤. –û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –≤–∫–ª—é—á–∞—é—Ç –ª–æ–∂–Ω—ã–µ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è, –Ω–µ–∏—Å–ø—Ä–∞–≤–Ω–æ—Å—Ç–∏ –¥–∞—Ç—á–∏–∫–æ–≤ –∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –Ω–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å–æ —Å—Ç–∞—Ä—ã–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏ —Ç—É—Ä–±–∏–Ω. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ —Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏—è —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π, —Ç–∞–∫–∏—Ö –∫–∞–∫ —Ü–∏—Ñ—Ä–æ–≤—ã–µ –¥–≤–æ–π–Ω–∏–∫–∏ –∏ —Å–∏—Å—Ç–µ–º—ã –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è, –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤–µ—Ç—Ä—è–Ω—ã—Ö —Ç—É—Ä–±–∏–Ω.'}, 'en': {'title': 'Optimizing Wind Power with Predictive Maintenance and Advanced Technologies', 'desc': 'This paper investigates how predictive maintenance models can enhance the efficiency of wind power generation. It highlights the importance of intelligent Operation and Maintenance (O&M) systems in minimizing downtime by identifying major faults in turbines. However, the study points out that these models often fail to detect smaller, gradual failures and face challenges like false positives and sensor issues. The research suggests that while advanced technologies like digital twins and SCADA systems improve maintenance, further advancements in AI and real-time data integration are necessary for optimal turbine performance.'}, 'zh': {'title': '‰ºòÂåñÈ£éÁîµÁª¥Êä§ÔºåÊèêÂçáÂèØÂÜçÁîüËÉΩÊ∫êÊïàÁéá', 'desc': 'Êú¨Á†îÁ©∂Êé¢ËÆ®‰∫ÜÈ¢ÑÊµãÊÄßÁª¥Êä§Ê®°ÂûãÂú®ÊèêÈ´òÈ£éÂäõÂèëÁîµÊïàÁéáÊñπÈù¢ÁöÑÊúâÊïàÊÄßÔºå‰ª•ÂèäÊô∫ËÉΩËøêÁª¥Á≥ªÁªüÁöÑ‰ºòÂåñ„ÄÇÈÄöËøáÂØπ‰∫î‰ΩçÈ£éÁîµÂú∫Â∑•Á®ãÂ∏àÂíåÁª¥Êä§ÁªèÁêÜÁöÑÁªìÊûÑÂåñËÆøË∞àÔºåÁ†îÁ©∂ÂèëÁé∞È¢ÑÊµãÊÄßÁª¥Êä§Ê®°ÂûãËÉΩÂ§üÊúâÊïàÂáèÂ∞ëÂÅúÊú∫Êó∂Èó¥Ôºå‰ΩÜÂú®Ê£ÄÊµãÂ∞èÂûãÊ∏êËøõÊÄßÊïÖÈöúÊñπÈù¢Â≠òÂú®Âõ∞Èöæ„ÄÇÁ†îÁ©∂ËøòÊåáÂá∫‰∫ÜÂÅáÈò≥ÊÄß„ÄÅ‰º†ÊÑüÂô®ÊïÖÈöúÂíåÊñ∞Ê®°Âûã‰∏éÊóßÊ∂°ËΩÆÁ≥ªÁªüÈõÜÊàêÁöÑÊåëÊàò„ÄÇÂ∞ΩÁÆ°Êï∞Â≠óÂèåËÉûËÉé„ÄÅSCADAÁ≥ªÁªüÂíåÁä∂ÊÄÅÁõëÊµãÁ≠âÂÖàËøõÊäÄÊúØÊòæËëóÊèêÂçá‰∫ÜÊ∂°ËΩÆÁª¥Êä§ÂÆûË∑µÔºå‰ΩÜÂú®‰∫∫Â∑•Êô∫ËÉΩ‰ºòÂåñÂíåÂÆûÊó∂Êï∞ÊçÆÈõÜÊàêÊñπÈù¢‰ªçÈúÄÊîπËøõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2506.19433', 'title': 'Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments\n  with a Hierarchical Spatial-Cognition Long-Short Memory System', 'url': 'https://huggingface.co/papers/2506.19433', 'abstract': 'Mem4Nav enhances Vision-and-Language Navigation by integrating a hierarchical spatial-cognition system with dual-memory modules using a reversible Transformer for improved task completion, speed, and detour detection.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-and-Language Navigation (VLN) in large-scale urban environments requires embodied agents to ground linguistic instructions in complex scenes and recall relevant experiences over extended time horizons. Prior modular pipelines offer interpretability but lack unified memory, while end-to-end (M)LLM agents excel at fusing vision and language yet remain constrained by fixed context windows and implicit spatial reasoning. We introduce Mem4Nav, a hierarchical spatial-cognition long-short memory system that can augment any VLN backbone. Mem4Nav fuses a sparse octree for fine-grained voxel indexing with a semantic topology graph for high-level landmark connectivity, storing both in trainable memory tokens embedded via a reversible Transformer. Long-term memory (LTM) compresses and retains historical observations at both octree and graph nodes, while short-term memory (STM) caches recent multimodal entries in relative coordinates for real-time obstacle avoidance and local planning. At each step, STM retrieval sharply prunes dynamic context, and, when deeper history is needed, LTM tokens are decoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and Map2Seq across three backbones (modular, state-of-the-art VLN with prompt-based LLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13 pp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW improvement. Ablations confirm the indispensability of both the hierarchical map and dual memory modules. Our codes are open-sourced via https://github.com/tsinghua-fib-lab/Mem4Nav.', 'score': 2, 'issue_id': 4479, 'pub_date': '2025-06-24', 'pub_date_card': {'ru': '24 –∏—é–Ω—è', 'en': 'June 24', 'zh': '6Êúà24Êó•'}, 'hash': 'b905012a9184cad4', 'authors': ['Lixuan He', 'Haoyu Dong', 'Zhenxing Chen', 'Yangcheng Yu', 'Jie Feng', 'Yong Li'], 'affiliations': ['Department of Electronic Engineering, BRNist, Tsinghua University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.19433.jpg', 'data': {'categories': ['#long_context', '#games', '#agents', '#multimodal', '#open_source', '#architecture', '#training'], 'emoji': 'üß≠', 'ru': {'title': 'Mem4Nav: –£–º–Ω–∞—è –Ω–∞–≤–∏–≥–∞—Ü–∏—è —Å –¥–≤–æ–π–Ω–æ–π –ø–∞–º—è—Ç—å—é –¥–ª—è –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤', 'desc': 'Mem4Nav - —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ –¥–ª—è –∑–∞–¥–∞—á Vision-and-Language Navigation, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫—É—é –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-–∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—É—é —Å–∏—Å—Ç–µ–º—É —Å –¥–≤—É–º—è –º–æ–¥—É–ª—è–º–∏ –ø–∞–º—è—Ç–∏. –û–Ω–∞ –ø—Ä–∏–º–µ–Ω—è–µ—Ç –æ–±—Ä–∞—Ç–∏–º—ã–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á, —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –æ–±—ä–µ–∑–¥–æ–≤. –°–∏—Å—Ç–µ–º–∞ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–µ –æ–∫—Ç–æ–¥–µ—Ä–µ–≤–æ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –≤–æ–∫—Å–µ–ª–µ–π –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π —Ç–æ–ø–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –≥—Ä–∞—Ñ –¥–ª—è —Å–≤—è–∑–Ω–æ—Å—Ç–∏ –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤. Mem4Nav –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ –∑–∞–¥–∞—á –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏.'}, 'en': {'title': 'Enhancing Navigation with Smart Memory Systems', 'desc': 'Mem4Nav is a novel approach to Vision-and-Language Navigation (VLN) that enhances task performance by integrating a hierarchical spatial-cognition system with dual-memory modules. It utilizes a reversible Transformer to manage long-term and short-term memory, allowing agents to effectively recall and utilize past experiences in complex environments. The system combines a sparse octree for detailed spatial indexing with a semantic topology graph for understanding landmark connections, improving navigation efficiency and obstacle avoidance. Evaluations show significant improvements in task completion rates and navigation accuracy across various VLN models, demonstrating the effectiveness of its memory architecture.'}, 'zh': {'title': 'Mem4NavÔºöÊèêÂçáËßÜËßâ‰∏éËØ≠Ë®ÄÂØºËà™ÁöÑÊô∫ËÉΩËÆ∞ÂøÜÁ≥ªÁªü', 'desc': 'Mem4NavÊòØ‰∏ÄÁßçÂ¢ûÂº∫ËßÜËßâ‰∏éËØ≠Ë®ÄÂØºËà™ÁöÑÁ≥ªÁªüÔºåÂÆÉÁªìÂêà‰∫ÜÂàÜÂ±ÇÁ©∫Èó¥ËÆ§Áü•Á≥ªÁªüÂíåÂèåÈáçËÆ∞ÂøÜÊ®°ÂùóÔºå‰ΩøÁî®ÂèØÈÄÜÂèòÊç¢Âô®Êù•ÊèêÈ´ò‰ªªÂä°ÂÆåÊàêÁéá„ÄÅÈÄüÂ∫¶ÂíåÁªïË°åÊ£ÄÊµãËÉΩÂäõ„ÄÇËØ•Á≥ªÁªüËÉΩÂ§üÂú®Â§çÊùÇÁöÑÂüéÂ∏ÇÁéØÂ¢É‰∏≠Â∏ÆÂä©‰ª£ÁêÜ‰∫∫ÁêÜËß£ËØ≠Ë®ÄÊåá‰ª§ÔºåÂπ∂Âú®ËæÉÈïøÊó∂Èó¥ÂÜÖÂõûÂøÜÁõ∏ÂÖ≥ÁªèÈ™å„ÄÇ‰∏é‰º†ÁªüÁöÑÊ®°ÂùóÂåñÁÆ°ÈÅìÁõ∏ÊØîÔºåMem4NavÊèê‰æõ‰∫ÜÁªü‰∏ÄÁöÑËÆ∞ÂøÜÁªìÊûÑÔºåÂÖãÊúç‰∫ÜÂõ∫ÂÆö‰∏ä‰∏ãÊñáÁ™óÂè£ÂíåÈöêÂºèÁ©∫Èó¥Êé®ÁêÜÁöÑÈôêÂà∂„ÄÇÈÄöËøáÈïøÁü≠ÊúüËÆ∞ÂøÜÁöÑÁªìÂêàÔºåMem4NavÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÊòæËëóÊèêÈ´ò‰∫Ü‰ªªÂä°ÂÆåÊàêÁéáÂíåÂÆûÊó∂ËßÑÂàíËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2506.19847', 'title': 'Orthogonal Finetuning Made Scalable', 'url': 'https://huggingface.co/papers/2506.19847', 'abstract': 'OFTv2 optimizes orthogonal fine-tuning by shifting from matrix-matrix to matrix-vector multiplications and introducing efficient Cayley-Neumann parameterization, enhancing speed, memory usage, and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Orthogonal finetuning (OFT) offers highly parameter-efficient adaptation while preventing catastrophic forgetting, but its high runtime and memory demands limit practical deployment. We identify the core computational bottleneck in OFT as its weight-centric implementation, which relies on costly matrix-matrix multiplications with cubic complexity. To overcome this, we propose OFTv2, an input-centric reformulation that instead uses matrix-vector multiplications (i.e., matrix-free computation), reducing the computational cost to quadratic. We further introduce the Cayley-Neumann parameterization, an efficient orthogonal parameterization that approximates the matrix inversion in Cayley transform via a truncated Neumann series. These modifications allow OFTv2 to achieve up to 10x faster training and 3x lower GPU memory usage without compromising performance. In addition, we extend OFTv2 to support finetuning quantized foundation models and show that it outperforms the popular QLoRA in training stability, efficiency, and memory usage.', 'score': 1, 'issue_id': 4483, 'pub_date': '2025-06-24', 'pub_date_card': {'ru': '24 –∏—é–Ω—è', 'en': 'June 24', 'zh': '6Êúà24Êó•'}, 'hash': 'c6e0936200cd15de', 'authors': ['Zeju Qiu', 'Weiyang Liu', 'Adrian Weller', 'Bernhard Sch√∂lkopf'], 'affiliations': ['Max Planck Institute for Intelligent Systems', 'The Alan Turing Institute', 'The Chinese University of Hong Kong', 'University of Cambridge'], 'pdf_title_img': 'assets/pdf/title_img/2506.19847.jpg', 'data': {'categories': ['#optimization', '#training', '#inference'], 'emoji': 'üöÄ', 'ru': {'title': 'OFTv2: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –¥–æ–æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π', 'desc': 'OFTv2 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω–æ–≥–æ –¥–æ–æ–±—É—á–µ–Ω–∏—è (OFT), –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—É—é –Ω–∞ –ø–æ–≤—ã—à–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –û—Å–Ω–æ–≤–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ –ø–µ—Ä–µ—Ö–æ–¥–µ –æ—Ç –º–∞—Ç—Ä–∏—á–Ω–æ-–º–∞—Ç—Ä–∏—á–Ω—ã—Ö –∫ –º–∞—Ç—Ä–∏—á–Ω–æ-–≤–µ–∫—Ç–æ—Ä–Ω—ã–º —É–º–Ω–æ–∂–µ–Ω–∏—è–º, —á—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—É—é —Å–ª–æ–∂–Ω–æ—Å—Ç—å. –ú–µ—Ç–æ–¥ —Ç–∞–∫–∂–µ –≤–≤–æ–¥–∏—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é –ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–∞—Ü–∏—é –ö—ç–π–ª–∏-–ù–µ–π–º–∞–Ω–∞ –¥–ª—è –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏–∏ –æ–±—Ä–∞—â–µ–Ω–∏—è –º–∞—Ç—Ä–∏—Ü—ã. OFTv2 –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è –∏ —Å–Ω–∏–∂–µ–Ω–∏–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è –ø–∞–º—è—Ç–∏ GPU, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å.'}, 'en': {'title': 'OFTv2: Speeding Up Orthogonal Fine-Tuning with Matrix-Vector Magic!', 'desc': 'OFTv2 is an advanced method for orthogonal fine-tuning that improves the efficiency of adapting machine learning models. It shifts from using matrix-matrix multiplications, which are computationally expensive, to matrix-vector multiplications, significantly reducing the time and memory required for training. The introduction of the Cayley-Neumann parameterization allows for a more efficient way to handle orthogonal parameters, further enhancing performance. Overall, OFTv2 achieves faster training speeds and lower memory usage while maintaining high performance, making it suitable for fine-tuning quantized models.'}, 'zh': {'title': 'OFTv2ÔºöÈ´òÊïàÁöÑÊ≠£‰∫§ÂæÆË∞ÉÊñ∞ÊñπÊ≥ï', 'desc': 'OFTv2ÈÄöËøáÂ∞ÜÁü©Èòµ-Áü©Èòµ‰πòÊ≥ïËΩ¨Âèò‰∏∫Áü©Èòµ-ÂêëÈáè‰πòÊ≥ïÔºå‰ºòÂåñ‰∫ÜÊ≠£‰∫§ÂæÆË∞ÉÁöÑËøáÁ®ãÔºå‰ªéËÄåÊèêÈ´ò‰∫ÜÈÄüÂ∫¶ÂíåÂÜÖÂ≠ò‰ΩøÁî®ÊïàÁéá„ÄÇËØ•ÊñπÊ≥ïÂºïÂÖ•‰∫ÜÈ´òÊïàÁöÑCayley-NeumannÂèÇÊï∞ÂåñÔºåËÉΩÂ§üÂú®‰∏çÁâ∫Áâ≤ÊÄßËÉΩÁöÑÊÉÖÂÜµ‰∏ãÔºåÊòæËëóÈôç‰ΩéËÆ°ÁÆóÊàêÊú¨„ÄÇOFTv2ÁöÑËÆ°ÁÆóÂ§çÊùÇÂ∫¶‰ªéÁ´ãÊñπÁ∫ßÂà´Èôç‰ΩéÂà∞Âπ≥ÊñπÁ∫ßÂà´Ôºå‰ΩøÂæóËÆ≠ÁªÉÈÄüÂ∫¶ÊèêÈ´ò‰∫Ü10ÂÄçÔºåGPUÂÜÖÂ≠ò‰ΩøÁî®Èôç‰Ωé‰∫Ü3ÂÄç„ÄÇËØ•ÊñπÊ≥ïËøòÊâ©Â±ï‰∫ÜÂØπÈáèÂåñÂü∫Á°ÄÊ®°ÂûãÁöÑÂæÆË∞ÉÊîØÊåÅÔºåÊòæÁ§∫Âá∫Âú®ËÆ≠ÁªÉÁ®≥ÂÆöÊÄß„ÄÅÊïàÁéáÂíåÂÜÖÂ≠ò‰ΩøÁî®ÊñπÈù¢‰ºò‰∫éÊµÅË°åÁöÑQLoRA„ÄÇ'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents (4)', '#agi', '#alignment (1)', '#architecture (3)', '#audio (1)', '#benchmark (10)', '#cv (4)', '#data (5)', '#dataset (5)', '#diffusion (4)', '#ethics', '#games (3)', '#graphs', '#hallucinations (2)', '#healthcare (1)', '#inference (2)', '#interpretability (3)', '#leakage', '#long_context (3)', '#low_resource (1)', '#machine_translation', '#math', '#multilingual (2)', '#multimodal (6)', '#open_source (5)', '#optimization (12)', '#plp', '#rag', '#reasoning (9)', '#rl (4)', '#rlhf (3)', '#robotics', '#science (1)', '#security', '#small_models', '#story_generation', '#survey', '#synthetic (1)', '#training (13)', '#transfer_learning (3)', '#video (5)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `üè∑Ô∏è ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `üè∑Ô∏è ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            üî∫ ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'üîÑ ' + getTimeDiff('2025-06-25 20:13',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "—Ä–µ–π—Ç–∏–Ω–≥—É",
                    pub_date: "–¥–∞—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏",
                    issue_id: "–¥–æ–±–∞–≤–ª–µ–Ω–∏—é –Ω–∞ HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "ËØÑÂàÜ",
                    pub_date: "ÂèëÂ∏ÉÊó•Êúü",
                    issue_id: "HF‰∏ä‰º†Êó•Êúü"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-06-25 20:13')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-06-25 20:13')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    