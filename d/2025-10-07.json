{
    "date": {
        "ru": "7 октября",
        "en": "October 7",
        "zh": "10月7日"
    },
    "time_utc": "2025-10-07 02:16",
    "weekday": 1,
    "issue_id": 6275,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2510.00263",
            "title": "Judging with Confidence: Calibrating Autoraters to Preference\n  Distributions",
            "url": "https://huggingface.co/papers/2510.00263",
            "abstract": "A framework for calibrating probabilistic autoraters to preference distributions using supervised fine-tuning and reinforcement learning improves alignment with human values and reduces bias.  \t\t\t\t\tAI-generated summary \t\t\t\t The alignment of large language models (LLMs) with human values increasingly relies on using other LLMs as automated judges, or ``autoraters''. However, their reliability is limited by a foundational issue: they are trained on discrete preference labels, forcing a single ground truth onto tasks that are often subjective, ambiguous, or nuanced. We argue that a reliable autorater must learn to model the full distribution of preferences defined by a target population. In this paper, we propose a general framework for calibrating probabilistic autoraters to any given preference distribution. We formalize the problem and present two learning methods tailored to different data conditions: 1) a direct supervised fine-tuning for dense, probabilistic labels, and 2) a reinforcement learning approach for sparse, binary labels. Our empirical results show that finetuning autoraters with a distribution-matching objective leads to verbalized probability predictions that are better aligned with the target preference distribution, with improved calibration and significantly lower positional bias, all while preserving performance on objective tasks.",
            "score": 5,
            "issue_id": 6275,
            "pub_date": "2025-09-30",
            "pub_date_card": {
                "ru": "30 сентября",
                "en": "September 30",
                "zh": "9月30日"
            },
            "hash": "96cee62eae60ad82",
            "authors": [
                "Zhuohang Li",
                "Xiaowei Li",
                "Chengyu Huang",
                "Guowang Li",
                "Katayoon Goshvadi",
                "Bo Dai",
                "Dale Schuurmans",
                "Paul Zhou",
                "Hamid Palangi",
                "Yiwen Song",
                "Palash Goyal",
                "Murat Kantarcioglu",
                "Bradley A. Malin",
                "Yuan Xue"
            ],
            "affiliations": [
                "Cornell University",
                "Google",
                "Google DeepMind",
                "Scale AI",
                "University of Alberta",
                "Vanderbilt University",
                "Virginia Tech"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.00263.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#training",
                    "#ethics",
                    "#rlhf"
                ],
                "emoji": "⚖️",
                "ru": {
                    "title": "Автооценщики, настроенные на распределение предпочтений людей",
                    "desc": "Статья предлагает framework для калибровки вероятностных автооценщиков (autoraters) - LLM, которые автоматически оценивают ответы других моделей. Вместо обучения на дискретных метках авторы учат модели предсказывать полное распределение предпочтений целевой аудитории людей. Для этого используются два подхода: supervised fine-tuning для плотных вероятностных меток и reinforcement learning для разреженных бинарных меток. Результаты показывают улучшенную калибровку, снижение позиционного bias и лучшее alignment с человеческими ценностями при сохранении качества на объективных задачах."
                },
                "en": {
                    "title": "Aligning Autoraters with Human Preferences through Advanced Calibration",
                    "desc": "This paper presents a framework for improving the accuracy of automated judges, known as autoraters, which evaluate preferences in a way that aligns better with human values. The authors highlight the limitations of traditional autoraters that rely on fixed preference labels, which can oversimplify complex human judgments. They propose two methods for training these autoraters: one using supervised fine-tuning for detailed preference data and another using reinforcement learning for simpler binary data. The results demonstrate that their approach enhances the alignment of predictions with actual human preferences, reduces bias, and maintains performance on objective tasks."
                },
                "zh": {
                    "title": "校准自动评分器以对齐人类价值观",
                    "desc": "本文提出了一种框架，用于通过监督微调和强化学习来校准概率自动评分器，以更好地与人类价值观对齐并减少偏见。我们认为，可靠的自动评分器必须学习建模目标人群定义的完整偏好分布，而不是仅依赖于离散的偏好标签。我们提出了两种学习方法，分别适用于不同的数据条件：一种是针对密集概率标签的直接监督微调，另一种是针对稀疏二元标签的强化学习方法。实验证明，使用分布匹配目标微调自动评分器可以提高其预测的概率与目标偏好分布的对齐程度，同时降低位置偏见。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.03264",
            "title": "Front-Loading Reasoning: The Synergy between Pretraining and\n  Post-Training Data",
            "url": "https://huggingface.co/papers/2510.03264",
            "abstract": "Introducing reasoning data during pretraining significantly enhances LLM performance compared to post-training, with pretraining benefiting more from diverse data patterns while SFT benefits more from high-quality data.  \t\t\t\t\tAI-generated summary \t\t\t\t The prevailing paradigm for enhancing the reasoning abilities of LLMs revolves around post-training on high-quality, reasoning-intensive data. While emerging literature suggests that reasoning data is increasingly incorporated also during the mid-training stage-a practice that is relatively more proprietary and less openly characterized-the role of such data in pretraining remains unclear. In particular, due to the opaqueness of pretraining corpora in most frontier models, the effect of reasoning data introduced at different phases of pre- and/or post-training is relatively less reported in the scientific literature. This raises several important questions: Is adding reasoning data earlier during pretraining any better than introducing it during post-training? Could earlier inclusion risk overfitting and harm generalization, or instead establish durable foundations that later fine-tuning cannot recover? We conduct the first systematic study of how reasoning data-varying in scale, diversity, and quality-affects LLM performance when introduced at different stages of training. We find that front-loading reasoning data into pretraining is critical (19% avg gain), establishing foundational capabilities that cannot be fully replicated by later-stage SFT, even with more data. We uncover an asymmetric principle for optimal data allocation: pretraining benefits most from broad diversity in reasoning patterns (11% avg gain), while SFT is more sensitive to data quality (15% avg gain). We show that high-quality pretraining data has latent effects, activated only after SFT, and that naively scaling SFT data can be detrimental, washing away the benefits of early reasoning injection. Our results challenge the conventional separation of language modeling and reasoning, providing a principled guide for strategically allocating data across the entire training pipeline to build more capable models.",
            "score": 2,
            "issue_id": 6275,
            "pub_date": "2025-09-26",
            "pub_date_card": {
                "ru": "26 сентября",
                "en": "September 26",
                "zh": "9月26日"
            },
            "hash": "4ab12dcfe1afbbf7",
            "authors": [
                "Syeda Nahida Akter",
                "Shrimai Prabhumoye",
                "Eric Nyberg",
                "Mostofa Patwary",
                "Mohammad Shoeybi",
                "Yejin Choi",
                "Bryan Catanzaro"
            ],
            "affiliations": [
                "Boston University",
                "Carnegie Mellon University",
                "NVIDIA",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.03264.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#optimization",
                    "#data"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Учить рассуждать нужно с самого начала",
                    "desc": "Исследование показывает, что добавление данных для обучения рассуждениям на этапе pretraining значительно эффективнее (прирост 19%), чем только на этапе post-training, создавая фундаментальные способности, которые невозможно полностью восстановить последующим fine-tuning. Обнаружен асимметричный принцип: pretraining больше выигрывает от разнообразия паттернов рассуждений (прирост 11%), тогда как supervised fine-tuning более чувствителен к качеству данных (прирост 15%). Высококачественные данные на этапе pretraining имеют латентный эффект, активирующийся только после SFT, а избыточное масштабирование SFT-данных может быть вредным. Результаты бросают вызов традиционному разделению языкового моделирования и обучения рассуждениям, предлагая стратегический подход к распределению данных на всех этапах обучения LLM."
                },
                "en": {
                    "title": "Front-Load Reasoning for Stronger LLMs!",
                    "desc": "This paper investigates the impact of introducing reasoning data during the pretraining phase of large language models (LLMs) compared to post-training. The authors find that incorporating diverse reasoning data early in pretraining leads to significant performance improvements, establishing foundational reasoning capabilities that are not fully recoverable through later fine-tuning. They highlight that pretraining benefits from a variety of reasoning patterns, while fine-tuning is more effective with high-quality data. The study challenges traditional views on language modeling and reasoning, offering insights on optimal data allocation throughout the training process."
                },
                "zh": {
                    "title": "提前引入推理数据，提升模型性能！",
                    "desc": "本研究探讨了在预训练阶段引入推理数据对大型语言模型（LLM）性能的影响。研究发现，提前在预训练中加入推理数据可以显著提高模型性能，平均提升19%。此外，预训练阶段更依赖于推理模式的多样性，而微调阶段则更注重数据的质量。我们的结果挑战了语言建模与推理的传统分离，为数据在整个训练过程中的合理分配提供了指导。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.04399",
            "title": "Utility-Learning Tension in Self-Modifying Agents",
            "url": "https://huggingface.co/papers/2510.04399",
            "abstract": "Self-improving systems face a utility-learning tension that can degrade their ability to learn and generalize, requiring capacity bounds to ensure safe self-modification.  \t\t\t\t\tAI-generated summary \t\t\t\t As systems trend toward superintelligence, a natural modeling premise is that agents can self-improve along every facet of their own design. We formalize this with a five-axis decomposition and a decision layer, separating incentives from learning behavior and analyzing axes in isolation. Our central result identifies and introduces a sharp utility--learning tension, the structural conflict in self-modifying systems whereby utility-driven changes that improve immediate or expected performance can also erode the statistical preconditions for reliable learning and generalization. Our findings show that distribution-free guarantees are preserved iff the policy-reachable model family is uniformly capacity-bounded; when capacity can grow without limit, utility-rational self-changes can render learnable tasks unlearnable. Under standard assumptions common in practice, these axes reduce to the same capacity criterion, yielding a single boundary for safe self-modification. Numerical experiments across several axes validate the theory by comparing destructive utility policies against our proposed two-gate policies that preserve learnability.",
            "score": 1,
            "issue_id": 6275,
            "pub_date": "2025-10-05",
            "pub_date_card": {
                "ru": "5 октября",
                "en": "October 5",
                "zh": "10月5日"
            },
            "hash": "9fa188fee82ece5c",
            "authors": [
                "Charles L. Wang",
                "Keir Dorchen",
                "Peter Jin"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "DeepMind",
                "ETH Zurich",
                "Google Brain",
                "IDSIA (Istituto Dalle Molle di Studi sull’Intelligenza Artificiale)",
                "Max Planck Institute for Intelligent Systems",
                "New York University",
                "SingularityNET",
                "Stanford University",
                "Technische Universität München",
                "University of Amsterdam",
                "University of Bath",
                "University of California, Berkeley",
                "University of Cambridge",
                "University of Edinburgh",
                "University of Freiburg",
                "University of Montreal (MILA)",
                "University of Oxford",
                "University of Toronto",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.04399.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#alignment",
                    "#agi",
                    "#rl"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "Парадокс самосовершенствования: как AI может разучиться учиться",
                    "desc": "Исследователи формализовали проблему самомодифицирующихся AI-систем, стремящихся к сверхинтеллекту. Они обнаружили фундаментальное противоречие: изменения, улучшающие текущую производительность системы, могут разрушить её способность к обучению и генерализации в будущем. Математически доказано, что безопасная самомодификация возможна только при ограничении ёмкости (capacity) модели - без таких ограничений система может сделать обучаемые задачи необучаемыми. Авторы предложили политики с двойным контролем, которые сохраняют способность к обучению при самомодификации системы."
                },
                "en": {
                    "title": "Balancing Improvement and Learning in Self-Modifying AI Systems",
                    "desc": "This paper discusses the challenges faced by self-improving AI systems, particularly the conflict between improving performance (utility) and maintaining the ability to learn effectively. It introduces a framework that separates different aspects of self-modification, allowing for a clearer analysis of how changes can impact learning. The authors identify a critical tension where beneficial changes can lead to a decline in the system's ability to generalize from data. They propose that to ensure safe self-modification, the system's capacity for change must be limited, and they validate their findings through numerical experiments comparing different modification strategies."
                },
                "zh": {
                    "title": "自我改进系统的效用与学习的平衡",
                    "desc": "自我改进系统面临效用学习的紧张关系，这可能会降低其学习和泛化能力。本文通过五个维度的分解和决策层的形式化，分析了激励与学习行为的分离。我们的主要结果揭示了效用与学习之间的结构性冲突，表明效用驱动的变化可能会破坏可靠学习和泛化的统计前提。研究表明，当模型的容量无限增长时，效用理性的自我变化可能使可学习的任务变得不可学习。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.03561",
            "title": "Reactive Transformer (RxT) -- Stateful Real-Time Processing for\n  Event-Driven Reactive Language Models",
            "url": "https://huggingface.co/papers/2510.03561",
            "abstract": "The Reactive Transformer (RxT) addresses the limitations of stateless Transformers in conversational AI by using an event-driven paradigm with a fixed-size Short-Term Memory (STM) system, achieving linear scaling and low latency.  \t\t\t\t\tAI-generated summary \t\t\t\t The Transformer architecture has become the de facto standard for Large Language Models (LLMs), demonstrating remarkable capabilities in language understanding and generation. However, its application in conversational AI is fundamentally constrained by its stateless nature and the quadratic computational complexity (O(L^2)) with respect to sequence length L. Current models emulate memory by reprocessing an ever-expanding conversation history with each turn, leading to prohibitive costs and latency in long dialogues. This paper introduces the Reactive Transformer (RxT), a novel architecture designed to overcome these limitations by shifting from a data-driven to an event-driven paradigm. RxT processes each conversational turn as a discrete event in real-time, maintaining context in an integrated, fixed-size Short-Term Memory (STM) system. The architecture features a distinct operational cycle where a generator-decoder produces a response based on the current query and the previous memory state, after which a memory-encoder and a dedicated Memory Attention network asynchronously update the STM with a representation of the complete interaction. This design fundamentally alters the scaling dynamics, reducing the total user-facing cost of a conversation from quadratic (O(N^2 cdot T)) to linear (O(N cdot T)) with respect to the number of interactions N. By decoupling response generation from memory updates, RxT achieves low latency, enabling truly real-time, stateful, and economically viable long-form conversations. We validated our architecture with a series of proof-of-concept experiments on synthetic data, demonstrating superior performance and constant-time inference latency compared to a baseline stateless model of comparable size.",
            "score": 1,
            "issue_id": 6275,
            "pub_date": "2025-10-03",
            "pub_date_card": {
                "ru": "3 октября",
                "en": "October 3",
                "zh": "10月3日"
            },
            "hash": "b213f271f5c52cec",
            "authors": [
                "Adam Filipek"
            ],
            "affiliations": [
                "Reactive AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.03561.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#training",
                    "#architecture",
                    "#synthetic"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "Реактивный Transformer: постоянная память для экономичных диалогов",
                    "desc": "Авторы предлагают архитектуру Reactive Transformer (RxT), которая решает проблему обработки длинных диалогов в conversational AI. В отличие от обычных Transformer моделей, которые обрабатывают всю историю разговора заново на каждом шаге, RxT использует event-driven подход с фиксированной кратковременной памятью (STM). Это снижает вычислительную сложность с квадратичной O(N²·T) до линейной O(N·T) относительно числа взаимодействий, обеспечивая низкую задержку и экономичность. Архитектура разделяет генерацию ответа и асинхронное обновление памяти, что позволяет вести долгие диалоги в реальном времени с постоянными затратами на каждый шаг."
                },
                "en": {
                    "title": "Revolutionizing Conversational AI with Reactive Transformers",
                    "desc": "The Reactive Transformer (RxT) is a new architecture designed to improve conversational AI by addressing the limitations of traditional stateless Transformers. It uses an event-driven approach combined with a fixed-size Short-Term Memory (STM) system, which allows for linear scaling and reduced latency during interactions. By processing each conversational turn as a discrete event, RxT maintains context efficiently and updates memory asynchronously, leading to faster response times. Experimental results show that RxT outperforms stateless models in terms of performance and inference speed, making it suitable for real-time, long-form conversations."
                },
                "zh": {
                    "title": "反应式变换器：实现实时对话的创新架构",
                    "desc": "反应式变换器（RxT）通过使用事件驱动的范式和固定大小的短期记忆（STM）系统，解决了无状态变换器在对话AI中的局限性。与传统模型相比，RxT能够以线性方式扩展，并显著降低延迟。该架构将每个对话轮次视为实时的离散事件，保持上下文的同时，优化了内存更新过程。通过将响应生成与内存更新解耦，RxT实现了真正的实时对话，适用于长时间的交互。"
                }
            }
        }
    ],
    "link_prev": "2025-10-06.html",
    "link_next": "2025-10-08.html",
    "link_month": "2025-10.html",
    "short_date_prev": {
        "ru": "06.10",
        "en": "10/06",
        "zh": "10月6日"
    },
    "short_date_next": {
        "ru": "08.10",
        "en": "10/08",
        "zh": "10月8日"
    },
    "categories": {
        "#dataset": 0,
        "#data": 1,
        "#benchmark": 0,
        "#agents": 1,
        "#cv": 0,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 3,
        "#robotics": 0,
        "#agi": 1,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}