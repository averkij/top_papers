{
    "date": {
        "ru": "24 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
        "en": "September 24",
        "zh": "9æœˆ24æ—¥"
    },
    "time_utc": "2025-09-24 03:26",
    "weekday": 2,
    "issue_id": 6053,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2509.19249",
            "title": "Reinforcement Learning on Pre-Training Data",
            "url": "https://huggingface.co/papers/2509.19249",
            "abstract": "Reinforcement Learning on Pre-Training data (RLPT) optimizes large language models by autonomously exploring meaningful trajectories in pre-training data, improving generalizable reasoning skills without human annotation.  \t\t\t\t\tAI-generated summary \t\t\t\t The growing disparity between the exponential scaling of computational resources and the finite growth of high-quality text data now constrains conventional scaling approaches for large language models (LLMs). To address this challenge, we introduce Reinforcement Learning on Pre-Training data (RLPT), a new training-time scaling paradigm for optimizing LLMs. In contrast to prior approaches that scale training primarily through supervised learning, RLPT enables the policy to autonomously explore meaningful trajectories to learn from pre-training data and improve its capability through reinforcement learning (RL). While existing RL strategies such as reinforcement learning from human feedback (RLHF) and reinforcement learning with verifiable rewards (RLVR) rely on human annotation for reward construction, RLPT eliminates this dependency by deriving reward signals directly from pre-training data. Specifically, it adopts a next-segment reasoning objective, rewarding the policy for accurately predicting subsequent text segments conditioned on the preceding context. This formulation allows RL to be scaled on pre-training data, encouraging the exploration of richer trajectories across broader contexts and thereby fostering more generalizable reasoning skills. Extensive experiments on both general-domain and mathematical reasoning benchmarks across multiple models validate the effectiveness of RLPT. For example, when applied to Qwen3-4B-Base, RLPT yields absolute improvements of 3.0, 5.1, 8.1, 6.0, 6.6, and 5.3 on MMLU, MMLU-Pro, GPQA-Diamond, KOR-Bench, AIME24, and AIME25, respectively. The results further demonstrate favorable scaling behavior, suggesting strong potential for continued gains with more compute. In addition, RLPT provides a solid foundation, extending the reasoning boundaries of LLMs and enhancing RLVR performance.",
            "score": 6,
            "issue_id": 6052,
            "pub_date": "2025-09-23",
            "pub_date_card": {
                "ru": "23 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 23",
                "zh": "9æœˆ23æ—¥"
            },
            "hash": "7ee5ca9be200b064",
            "authors": [
                "Siheng Li",
                "Kejiao Li",
                "Zenan Xu",
                "Guanhua Huang",
                "Evander Yang",
                "Kun Li",
                "Haoyuan Wu",
                "Jiajia Wu",
                "Zihao Zheng",
                "Chenchen Zhang",
                "Kun Shi",
                "Kyrierl Deng",
                "Qi Yi",
                "Ruibin Xiong",
                "Tingqiang Xu",
                "Yuhao Jiang",
                "Jianfeng Yan",
                "Yuyuan Zeng",
                "Guanghui Xu",
                "Jinbao Xue",
                "Zhijiang Xu",
                "Zheng Fang",
                "Shuai Li",
                "Qibin Liu",
                "Xiaoxue Li",
                "Zhuoyu Li",
                "Yangyu Tao",
                "Fei Gao",
                "Cheng Jiang",
                "Bo Chao Wang",
                "Kai Liu",
                "Jianchen Zhu",
                "Wai Lam",
                "Wayyt Wang",
                "Bo Zhou",
                "Di Wang"
            ],
            "affiliations": [
                "HunYuan Infra Team",
                "LLM Department, Tencent",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.19249.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#rl",
                    "#reasoning",
                    "#benchmark",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "RLPT: Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… (RLPT) Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¸Ğ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ² Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. RLPT Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ†ĞµĞ»ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¾ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ¼ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğµ, Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ·Ğ° Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰ĞµĞ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒÑ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ñ‹Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Autonomous Learning for Enhanced Reasoning in Language Models",
                    "desc": "Reinforcement Learning on Pre-Training data (RLPT) is a novel approach that enhances large language models (LLMs) by allowing them to learn from pre-training data without needing human annotations. It uses reinforcement learning to autonomously explore meaningful data trajectories, which helps improve the model's reasoning abilities. Unlike traditional methods that rely on supervised learning, RLPT derives reward signals directly from the data, focusing on predicting subsequent text segments based on prior context. This method not only boosts performance on various reasoning tasks but also shows promise for scaling with increased computational resources."
                },
                "zh": {
                    "title": "è‡ªä¸»æ¢ç´¢ï¼Œæå‡æ¨ç†èƒ½åŠ›çš„å¼ºåŒ–å­¦ä¹ æ–°æ–¹æ³•",
                    "desc": "å¼ºåŒ–å­¦ä¹ åœ¨é¢„è®­ç»ƒæ•°æ®ä¸Šçš„åº”ç”¨ï¼ˆRLPTï¼‰é€šè¿‡è‡ªä¸»æ¢ç´¢é¢„è®­ç»ƒæ•°æ®ä¸­çš„æœ‰æ„ä¹‰è½¨è¿¹ï¼Œä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæå‡å…¶é€šç”¨æ¨ç†èƒ½åŠ›ï¼Œè€Œæ— éœ€äººå·¥æ ‡æ³¨ã€‚ä¸ä¼ ç»Ÿçš„ç›‘ç£å­¦ä¹ æ–¹æ³•ä¸åŒï¼ŒRLPTå…è®¸ç­–ç•¥ä»é¢„è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ ï¼Œå¹¶é€šè¿‡å¼ºåŒ–å­¦ä¹ æé«˜èƒ½åŠ›ã€‚è¯¥æ–¹æ³•é€šè¿‡é¢„æµ‹åç»­æ–‡æœ¬æ®µè½æ¥æ„å»ºå¥–åŠ±ä¿¡å·ï¼Œé¼“åŠ±åœ¨æ›´å¹¿æ³›çš„ä¸Šä¸‹æ–‡ä¸­æ¢ç´¢æ›´ä¸°å¯Œçš„è½¨è¿¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRLPTåœ¨å¤šä¸ªæ¨¡å‹ä¸Šæ˜¾è‘—æå‡äº†æ¨ç†æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¼˜åŒ–ä¸­çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.18154",
            "title": "MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and\n  Training Recipe",
            "url": "https://huggingface.co/papers/2509.18154",
            "abstract": "MiniCPM-V 4.5, a 8B parameter multimodal large language model, achieves high performance and efficiency through a unified 3D-Resampler architecture, a unified learning paradigm, and a hybrid reinforcement learning strategy.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) are undergoing rapid progress and represent the frontier of AI development. However, their training and inference efficiency have emerged as a core bottleneck in making MLLMs more accessible and scalable. To address the challenges, we present MiniCPM-V 4.5, an 8B parameter model designed for high efficiency and strong performance. We introduce three core improvements in model architecture, data strategy and training method: a unified 3D-Resampler model architecture for highly compact encoding over images and videos, a unified learning paradigm for document knowledge and text recognition without heavy data engineering, and a hybrid reinforcement learning strategy for proficiency in both short and long reasoning modes. Comprehensive experimental results in OpenCompass evaluation show that MiniCPM-V 4.5 surpasses widely used proprietary models such as GPT-4o-latest, and significantly larger open-source models such as Qwen2.5-VL 72B. Notably, the strong performance is achieved with remarkable efficiency. For example, on the widely adopted VideoMME benchmark, MiniCPM-V 4.5 achieves state-of-the-art performance among models under 30B size, using just 46.7\\% GPU memory cost and 8.7\\% inference time of Qwen2.5-VL 7B.",
            "score": 6,
            "issue_id": 6052,
            "pub_date": "2025-09-16",
            "pub_date_card": {
                "ru": "16 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 16",
                "zh": "9æœˆ16æ—¥"
            },
            "hash": "e46263baf17f8869",
            "authors": [
                "Tianyu Yu",
                "Zefan Wang",
                "Chongyi Wang",
                "Fuwei Huang",
                "Wenshuo Ma",
                "Zhihui He",
                "Tianchi Cai",
                "Weize Chen",
                "Yuxiang Huang",
                "Yuanqian Zhao",
                "Bokai Xu",
                "Junbo Cui",
                "Yingjing Xu",
                "Liqing Ruan",
                "Luoyuan Zhang",
                "Hanyu Liu",
                "Jingkun Tang",
                "Hongyuan Liu",
                "Qining Guo",
                "Wenhao Hu",
                "Bingxiang He",
                "Jie Zhou",
                "Jie Cai",
                "Ji Qi",
                "Zonghao Guo",
                "Chi Chen",
                "Guoyang Zeng",
                "Yuxuan Li",
                "Ganqu Cui",
                "Ning Ding",
                "Xu Han",
                "Yuan Yao",
                "Zhiyuan Liu",
                "Maosong Sun"
            ],
            "affiliations": [
                "MiniCPM-V Team, OpenBMB"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.18154.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#rl",
                    "#agi",
                    "#benchmark",
                    "#optimization",
                    "#multimodal",
                    "#training"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "MiniCPM-V 4.5: ĞšĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "MiniCPM-V 4.5 - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 8 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ 3D-Resampler, ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. MiniCPM-V 4.5 Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº GPT-4 Ğ¸ Qwen2.5-VL 72B, Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ… Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ VideoMME ÑÑ€ĞµĞ´Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¾ 30 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "Efficiency Meets Performance in Multimodal AI",
                    "desc": "MiniCPM-V 4.5 is an advanced multimodal large language model with 8 billion parameters, designed to enhance both performance and efficiency. It utilizes a novel 3D-Resampler architecture that allows for compact encoding of images and videos, streamlining the processing of multimodal data. The model also incorporates a unified learning paradigm that simplifies document knowledge and text recognition, reducing the need for extensive data engineering. Additionally, a hybrid reinforcement learning strategy enables the model to excel in both short and long reasoning tasks, achieving superior results while using significantly less computational resources compared to larger models."
                },
                "zh": {
                    "title": "é«˜æ•ˆå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„æœªæ¥",
                    "desc": "MiniCPM-V 4.5 æ˜¯ä¸€ä¸ªæ‹¥æœ‰ 80 äº¿å‚æ•°çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé‡‡ç”¨ç»Ÿä¸€çš„ 3D-é‡é‡‡æ ·æ¶æ„ï¼Œæ—¨åœ¨æé«˜æ•ˆç‡å’Œæ€§èƒ½ã€‚è¯¥æ¨¡å‹é€šè¿‡ç»Ÿä¸€å­¦ä¹ èŒƒå¼å’Œæ··åˆå¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œè§£å†³äº†å¤šæ¨¡æ€æ¨¡å‹åœ¨è®­ç»ƒå’Œæ¨ç†ä¸­çš„æ•ˆç‡ç“¶é¢ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMiniCPM-V 4.5 åœ¨ OpenCompass è¯„ä¼°ä¸­è¶…è¶Šäº†è®¸å¤šçŸ¥åæ¨¡å‹ï¼Œå±•ç°å‡ºå“è¶Šçš„æ€§èƒ½å’Œæ•ˆç‡ã€‚å°¤å…¶æ˜¯åœ¨ VideoMME åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ¨¡å‹åœ¨ 30B ä»¥ä¸‹çš„æ¨¡å‹ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾è‘—é™ä½äº† GPU å†…å­˜å’Œæ¨ç†æ—¶é—´ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.19296",
            "title": "Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model\n  Self-Distillation",
            "url": "https://huggingface.co/papers/2509.19296",
            "abstract": "A self-distillation framework converts implicit 3D knowledge from video diffusion models into an explicit 3D Gaussian Splatting representation, enabling 3D scene generation from text or images.  \t\t\t\t\tAI-generated summary \t\t\t\t The ability to generate virtual environments is crucial for applications ranging from gaming to physical AI domains such as robotics, autonomous driving, and industrial AI. Current learning-based 3D reconstruction methods rely on the availability of captured real-world multi-view data, which is not always readily available. Recent advancements in video diffusion models have shown remarkable imagination capabilities, yet their 2D nature limits the applications to simulation where a robot needs to navigate and interact with the environment. In this paper, we propose a self-distillation framework that aims to distill the implicit 3D knowledge in the video diffusion models into an explicit 3D Gaussian Splatting (3DGS) representation, eliminating the need for multi-view training data. Specifically, we augment the typical RGB decoder with a 3DGS decoder, which is supervised by the output of the RGB decoder. In this approach, the 3DGS decoder can be purely trained with synthetic data generated by video diffusion models. At inference time, our model can synthesize 3D scenes from either a text prompt or a single image for real-time rendering. Our framework further extends to dynamic 3D scene generation from a monocular input video. Experimental results show that our framework achieves state-of-the-art performance in static and dynamic 3D scene generation.",
            "score": 4,
            "issue_id": 6052,
            "pub_date": "2025-09-23",
            "pub_date_card": {
                "ru": "23 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 23",
                "zh": "9æœˆ23æ—¥"
            },
            "hash": "763d3ecf06625fdf",
            "authors": [
                "Sherwin Bahmani",
                "Tianchang Shen",
                "Jiawei Ren",
                "Jiahui Huang",
                "Yifeng Jiang",
                "Haithem Turki",
                "Andrea Tagliasacchi",
                "David B. Lindell",
                "Zan Gojcic",
                "Sanja Fidler",
                "Huan Ling",
                "Jun Gao",
                "Xuanchi Ren"
            ],
            "affiliations": [
                "NVIDIA",
                "Simon Fraser University",
                "University of Toronto",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.19296.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#robotics",
                    "#games",
                    "#synthetic",
                    "#video"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "3D-Ğ¼Ğ¸Ñ€Ñ‹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ€ĞµĞ´",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑĞ°Ğ¼Ğ¾Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ½ĞµÑĞ²Ğ½Ñ‹Ğµ 3D-Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ· Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ² ÑĞ²Ğ½Ğ¾Ğµ 3D-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Gaussian Splatting. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸Ğ»Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ñ… Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ RGB-Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ Ğ¸ 3DGS-Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… 3D-ÑÑ†ĞµĞ½ Ğ¸Ğ· Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Transforming 2D Imagination into 3D Reality",
                    "desc": "This paper introduces a self-distillation framework that transforms implicit 3D knowledge from video diffusion models into an explicit 3D Gaussian Splatting (3DGS) representation. This method allows for the generation of 3D scenes from text or images without needing extensive multi-view training data. By enhancing the RGB decoder with a 3DGS decoder, the framework can be trained solely on synthetic data produced by video diffusion models. The results demonstrate superior performance in generating both static and dynamic 3D scenes, making it valuable for applications in gaming and robotics."
                },
                "zh": {
                    "title": "è‡ªè’¸é¦æ¡†æ¶ï¼šä»è§†é¢‘ç”Ÿæˆ3Dåœºæ™¯çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªè’¸é¦æ¡†æ¶ï¼Œå°†è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­çš„éšå¼3DçŸ¥è¯†è½¬åŒ–ä¸ºæ˜¾å¼çš„3Dé«˜æ–¯ç‚¹äº‘è¡¨ç¤ºã€‚è¿™ç§æ–¹æ³•ä½¿å¾—ä»æ–‡æœ¬æˆ–å›¾åƒç”Ÿæˆ3Dåœºæ™¯æˆä¸ºå¯èƒ½ï¼Œé¿å…äº†å¯¹å¤šè§†è§’è®­ç»ƒæ•°æ®çš„ä¾èµ–ã€‚æˆ‘ä»¬é€šè¿‡å¢å¼ºå…¸å‹çš„RGBè§£ç å™¨ï¼ŒåŠ å…¥3Dé«˜æ–¯ç‚¹äº‘è§£ç å™¨ï¼Œå¹¶åˆ©ç”¨RGBè§£ç å™¨çš„è¾“å‡ºè¿›è¡Œç›‘ç£è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨é™æ€å’ŒåŠ¨æ€3Dåœºæ™¯ç”Ÿæˆæ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.19284",
            "title": "What Characterizes Effective Reasoning? Revisiting Length, Review, and\n  Structure of CoT",
            "url": "https://huggingface.co/papers/2509.19284",
            "abstract": "Effective chain-of-thoughts in large reasoning models are characterized by fewer failed steps and better structural quality, not necessarily by length or review.  \t\t\t\t\tAI-generated summary \t\t\t\t Large reasoning models (LRMs) spend substantial test-time compute on long chain-of-thought (CoT) traces, but what *characterizes* an effective CoT remains unclear. While prior work reports gains from lengthening CoTs and increasing review (revisiting earlier steps) via appended *wait* tokens, recent studies suggest that shorter thinking can outperform longer traces. We therefore conduct a systematic evaluation across ten LRMs on math and scientific reasoning. Contrary to the \"longer-is-better\" narrative, we find that both naive CoT lengthening and increased review are associated with *lower* accuracy.   As CoT unfolds step by step, token-level metrics can conflate verbosity with process quality. We introduce a graph view of CoT to extract structure and identify a single statistic-the *Failed-Step Fraction (FSF)*, the fraction of steps in abandoned branches-that consistently outpredicts length and review ratio for correctness across models. To probe causality, we design two interventions. First, we rank candidate CoTs by each metric at test time, where FSF yields the largest pass@1 gains; second, we edit CoTs to remove failed branches, which significantly improves accuracy, indicating that failed branches bias subsequent reasoning. Taken together, these results characterize effective CoTs as those that *fail less* and support *structure-aware* test-time scaling over indiscriminately generating long CoT.",
            "score": 4,
            "issue_id": 6052,
            "pub_date": "2025-09-23",
            "pub_date_card": {
                "ru": "23 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 23",
                "zh": "9æœˆ23æ—¥"
            },
            "hash": "4e8ddb8ed978283e",
            "authors": [
                "Yunzhen Feng",
                "Julia Kempe",
                "Cheng Zhang",
                "Parag Jain",
                "Anthony Hartshorn"
            ],
            "affiliations": [
                "Meta Superintelligence Labs",
                "New York University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.19284.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#rl",
                    "#reasoning",
                    "#interpretability",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞšĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾, Ğ° Ğ½Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾: ĞºĞ»ÑÑ‡ Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ°Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ˜Ğ˜",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (Chain-of-Thought, CoT) Ğ² ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (Large Reasoning Models, LRM) Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ÑÑ Ğ½Ğµ Ğ¸Ñ… Ğ´Ğ»Ğ¸Ğ½Ğ¾Ğ¹ Ğ¸Ğ»Ğ¸ ÑÑ‚ĞµĞ¿ĞµĞ½ÑŒÑ Ğ¿ĞµÑ€ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ğ°, Ğ° Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ½ĞµÑƒĞ´Ğ°Ñ‡Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ğ¸ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¼ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼. Ğ’Ğ²ĞµĞ´ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒ - Ğ´Ğ¾Ğ»Ñ Ğ½ĞµÑƒĞ´Ğ°Ñ‡Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² (Failed-Step Fraction, FSF), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ‡ĞµĞ¼ Ğ´Ğ»Ğ¸Ğ½Ğ° CoT. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ CoT Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ğ½ĞµÑƒĞ´Ğ°Ñ‡Ğ½Ñ‹Ñ… Ğ²ĞµÑ‚Ğ²ĞµĞ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğº Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ CoT Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¸Ñ… Ğ´Ğ»Ğ¸Ğ½Ñ‹."
                },
                "en": {
                    "title": "Less Failure, More Structure: The Key to Effective Reasoning in AI",
                    "desc": "This paper investigates what makes chain-of-thought (CoT) reasoning effective in large reasoning models (LRMs). It challenges the idea that longer CoTs are always better, showing that both longer CoTs and increased review can lead to lower accuracy. The authors introduce a new metric called the Failed-Step Fraction (FSF), which measures the proportion of steps in abandoned reasoning paths, and find it to be a better predictor of correctness than length or review. Their findings suggest that effective CoTs are characterized by fewer failures and emphasize the importance of structure in reasoning processes."
                },
                "zh": {
                    "title": "æœ‰æ•ˆæ€ç»´é“¾ï¼šå‡å°‘å¤±è´¥æ­¥éª¤ï¼Œæå‡æ¨ç†è´¨é‡",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰ä¸­æœ‰æ•ˆçš„æ€ç»´é“¾ï¼ˆCoTï¼‰çš„ç‰¹å¾ã€‚ç ”ç©¶å‘ç°ï¼Œæ€ç»´é“¾çš„æœ‰æ•ˆæ€§ä¸å¤±è´¥æ­¥éª¤çš„æ•°é‡å’Œç»“æ„è´¨é‡æœ‰å…³ï¼Œè€Œä¸æ˜¯ç®€å•çš„é•¿åº¦æˆ–å¤å®¡æ¬¡æ•°ã€‚é€šè¿‡å¯¹åä¸ªLRMsè¿›è¡Œç³»ç»Ÿè¯„ä¼°ï¼Œç»“æœè¡¨æ˜ï¼Œç®€å•åœ°å»¶é•¿æ€ç»´é“¾æˆ–å¢åŠ å¤å®¡ä¼šå¯¼è‡´å‡†ç¡®ç‡é™ä½ã€‚è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„å›¾å½¢è§†è§’æ¥æå–æ€ç»´é“¾çš„ç»“æ„ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªç»Ÿè®¡é‡â€”â€”å¤±è´¥æ­¥éª¤æ¯”ä¾‹ï¼ˆFSFï¼‰ï¼Œè¯¥æ¯”ä¾‹èƒ½å¤Ÿæ›´å¥½åœ°é¢„æµ‹æ¨¡å‹çš„æ­£ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.18824",
            "title": "Hyper-Bagel: A Unified Acceleration Framework for Multimodal\n  Understanding and Generation",
            "url": "https://huggingface.co/papers/2509.18824",
            "abstract": "Hyper-Bagel accelerates multimodal understanding and generation tasks using speculative decoding and multi-stage distillation, achieving significant speedups while maintaining high-quality outputs.  \t\t\t\t\tAI-generated summary \t\t\t\t Unified multimodal models have recently attracted considerable attention for their remarkable abilities in jointly understanding and generating diverse content. However, as contexts integrate increasingly numerous interleaved multimodal tokens, the iterative processes of diffusion denoising and autoregressive decoding impose significant computational overhead. To address this, we propose Hyper-Bagel, a unified acceleration framework designed to simultaneously speed up both multimodal understanding and generation tasks. Our approach uses a divide-and-conquer strategy, employing speculative decoding for next-token prediction and a multi-stage distillation process for diffusion denoising. The framework delivers substantial performance gains, achieving over a 2x speedup in multimodal understanding. For generative tasks, our resulting lossless 6-NFE model yields a 16.67x speedup in text-to-image generation and a 22x speedup in image editing, all while preserving the high-quality output of the original model. We further develop a highly efficient 1-NFE model that enables near real-time interactive editing and generation. By combining advanced adversarial distillation with human feedback learning, this model achieves ultimate cost-effectiveness and responsiveness, making complex multimodal interactions seamless and instantaneous.",
            "score": 4,
            "issue_id": 6052,
            "pub_date": "2025-09-23",
            "pub_date_card": {
                "ru": "23 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 23",
                "zh": "9æœˆ23æ—¥"
            },
            "hash": "6de809558bad8c90",
            "authors": [
                "Yanzuo Lu",
                "Xin Xia",
                "Manlin Zhang",
                "Huafeng Kuang",
                "Jianbin Zheng",
                "Yuxi Ren",
                "Xuefeng Xiao"
            ],
            "affiliations": [
                "ByteDance AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.18824.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#multimodal",
                    "#training"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Hyper-Bagel: Ğ¡Ğ²ĞµÑ€Ñ…Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ˜Ğ˜ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Hyper-Bagel - ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµĞºÑƒĞ»ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ´Ğ»Ñ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. Hyper-Bagel Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ: Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ² 2 Ñ€Ğ°Ğ·Ğ° Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ´Ğ¾ 22 Ñ€Ğ°Ğ· Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ 1-NFE Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¼Ğ³Ğ½Ğ¾Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°."
                },
                "en": {
                    "title": "Hyper-Bagel: Speeding Up Multimodal Tasks with Smart Techniques",
                    "desc": "Hyper-Bagel is a new framework that speeds up tasks involving multiple types of data, like text and images, by using advanced techniques. It combines speculative decoding, which predicts the next piece of data quickly, with a multi-stage distillation process to reduce noise in the data. This approach allows for more than double the speed in understanding multimodal content and significantly faster generation of images from text. The framework also includes a model that can edit and generate content in real-time, making it efficient and responsive while maintaining high-quality results."
                },
                "zh": {
                    "title": "Hyper-Bagelï¼šåŠ é€Ÿå¤šæ¨¡æ€ä»»åŠ¡çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "Hyper-Bagel æ˜¯ä¸€ä¸ªåŠ é€Ÿå¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆä»»åŠ¡çš„æ¡†æ¶ï¼Œé‡‡ç”¨äº†æ¨æµ‹è§£ç å’Œå¤šé˜¶æ®µè’¸é¦çš„æ–¹æ³•ã€‚å®ƒé€šè¿‡åˆ†è€Œæ²»ä¹‹çš„ç­–ç•¥ï¼Œæ˜¾è‘—å‡å°‘äº†è®¡ç®—å¼€é”€ï¼ŒåŒæ—¶ä¿æŒäº†é«˜è´¨é‡çš„è¾“å‡ºã€‚è¯¥æ¡†æ¶åœ¨å¤šæ¨¡æ€ç†è§£ä»»åŠ¡ä¸­å®ç°äº†è¶…è¿‡2å€çš„åŠ é€Ÿï¼Œè€Œåœ¨ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„é€Ÿåº¦æå‡è¾¾åˆ°äº†16.67å€ï¼Œå›¾åƒç¼–è¾‘çš„é€Ÿåº¦æå‡è¾¾åˆ°äº†22å€ã€‚é€šè¿‡ç»“åˆå¯¹æŠ—è’¸é¦å’Œäººç±»åé¦ˆå­¦ä¹ ï¼ŒHyper-Bagel ä½¿å¾—å¤æ‚çš„å¤šæ¨¡æ€äº¤äº’å˜å¾—æ— ç¼ä¸”å³æ—¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.19297",
            "title": "VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with\n  Voxel-Aligned Prediction",
            "url": "https://huggingface.co/papers/2509.19297",
            "abstract": "VolSplat, a voxel-aligned Gaussian prediction method, improves novel view synthesis by overcoming pixel alignment limitations and enhancing 3D reconstruction quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Feed-forward 3D Gaussian Splatting (3DGS) has emerged as a highly effective solution for novel view synthesis. Existing methods predominantly rely on a pixel-aligned Gaussian prediction paradigm, where each 2D pixel is mapped to a 3D Gaussian. We rethink this widely adopted formulation and identify several inherent limitations: it renders the reconstructed 3D models heavily dependent on the number of input views, leads to view-biased density distributions, and introduces alignment errors, particularly when source views contain occlusions or low texture. To address these challenges, we introduce VolSplat, a new multi-view feed-forward paradigm that replaces pixel alignment with voxel-aligned Gaussians. By directly predicting Gaussians from a predicted 3D voxel grid, it overcomes pixel alignment's reliance on error-prone 2D feature matching, ensuring robust multi-view consistency. Furthermore, it enables adaptive control over Gaussian density based on 3D scene complexity, yielding more faithful Gaussian point clouds, improved geometric consistency, and enhanced novel-view rendering quality. Experiments on widely used benchmarks including RealEstate10K and ScanNet demonstrate that VolSplat achieves state-of-the-art performance while producing more plausible and view-consistent Gaussian reconstructions. In addition to superior results, our approach establishes a more scalable framework for feed-forward 3D reconstruction with denser and more robust representations, paving the way for further research in wider communities. The video results, code and trained models are available on our project page: https://lhmd.top/volsplat.",
            "score": 2,
            "issue_id": 6053,
            "pub_date": "2025-09-23",
            "pub_date_card": {
                "ru": "23 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 23",
                "zh": "9æœˆ23æ—¥"
            },
            "hash": "708db1d702c58d65",
            "authors": [
                "Weijie Wang",
                "Yeqing Chen",
                "Zeyu Zhang",
                "Hengyu Liu",
                "Haoxiao Wang",
                "Zhiyuan Feng",
                "Wenkang Qin",
                "Zheng Zhu",
                "Donny Y. Chen",
                "Bohan Zhuang"
            ],
            "affiliations": [
                "GigaAI",
                "Monash University",
                "The Chinese University of Hong Kong",
                "Tsinghua University",
                "University of Electronic Science and Technology of China",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.19297.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#open_source",
                    "#benchmark",
                    "#3d",
                    "#games"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "VolSplat: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¾ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ¾Ğ²",
                    "desc": "VolSplat - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ²Ğ¾ĞºÑĞµĞ»ÑŒĞ½Ğ¾-Ğ²Ñ‹Ñ€Ğ¾Ğ²Ğ½ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ¾Ğ² Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ¾Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾. ĞĞ½ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°ĞºÑƒÑ€ÑĞ°Ğ¼Ğ¸. VolSplat Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ 3D-ÑÑ†ĞµĞ½Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ğµ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing 3D Reconstruction with Voxel-Aligned Gaussians",
                    "desc": "VolSplat is a novel method for synthesizing new views in 3D reconstruction by using voxel-aligned Gaussian predictions instead of the traditional pixel-aligned approach. This new technique addresses limitations such as dependency on input views, view-biased density distributions, and alignment errors caused by occlusions or low texture in source views. By predicting Gaussians directly from a 3D voxel grid, VolSplat enhances multi-view consistency and adapts Gaussian density based on scene complexity. Experiments show that it outperforms existing methods, providing more accurate and consistent 3D reconstructions, making it a significant advancement in the field."
                },
                "zh": {
                    "title": "ä½“ç´ å¯¹é½ï¼Œé‡å¡‘3Dé‡å»ºçš„æœªæ¥",
                    "desc": "VolSplatæ˜¯ä¸€ç§åŸºäºä½“ç´ å¯¹é½çš„é«˜æ–¯é¢„æµ‹æ–¹æ³•ï¼Œæ—¨åœ¨æ”¹å–„æ–°è§†è§’åˆæˆï¼Œå…‹æœåƒç´ å¯¹é½çš„å±€é™æ€§ï¼Œå¹¶æå‡3Dé‡å»ºè´¨é‡ã€‚ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºåƒç´ å¯¹é½çš„é«˜æ–¯é¢„æµ‹ï¼Œè¿™å¯¼è‡´é‡å»ºçš„3Dæ¨¡å‹å¯¹è¾“å…¥è§†å›¾æ•°é‡é«˜åº¦ä¾èµ–ï¼Œå¹¶ä¸”åœ¨è§†å›¾åå·®å’Œé®æŒ¡æƒ…å†µä¸‹å®¹æ˜“å‡ºç°å¯¹é½é”™è¯¯ã€‚VolSplaté€šè¿‡ç›´æ¥ä»é¢„æµ‹çš„3Dä½“ç´ ç½‘æ ¼ä¸­é¢„æµ‹é«˜æ–¯ï¼Œé¿å…äº†å¯¹é”™è¯¯æ˜“æ„Ÿçš„2Dç‰¹å¾åŒ¹é…ï¼Œä»è€Œç¡®ä¿äº†å¤šè§†å›¾çš„ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVolSplatåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œæä¾›äº†æ›´çœŸå®å’Œä¸€è‡´çš„é«˜æ–¯é‡å»ºã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.19087",
            "title": "Zero-Shot Multi-Spectral Learning: Reimagining a Generalist Multimodal\n  Gemini 2.5 Model for Remote Sensing Applications",
            "url": "https://huggingface.co/papers/2509.19087",
            "abstract": "A training-free method enables generalist multimodal models to process multi-spectral imagery in a zero-shot manner, enhancing performance on remote sensing tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-spectral imagery plays a crucial role in diverse Remote Sensing applications including land-use classification, environmental monitoring and urban planning. These images are widely adopted because their additional spectral bands correlate strongly with physical materials on the ground, such as ice, water, and vegetation. This allows for more accurate identification, and their public availability from missions, such as Sentinel-2 and Landsat, only adds to their value. Currently, the automatic analysis of such data is predominantly managed through machine learning models specifically trained for multi-spectral input, which are costly to train and support. Furthermore, although providing a lot of utility for Remote Sensing, such additional inputs cannot be used with powerful generalist large multimodal models, which are capable of solving many visual problems, but are not able to understand specialized multi-spectral signals.   To address this, we propose a training-free approach which introduces new multi-spectral data in a Zero-Shot-only mode, as inputs to generalist multimodal models, trained on RGB-only inputs. Our approach leverages the multimodal models' understanding of the visual space, and proposes to adapt to inputs to that space, and to inject domain-specific information as instructions into the model. We exemplify this idea with the Gemini2.5 model and observe strong Zero-Shot performance gains of the approach on popular Remote Sensing benchmarks for land cover and land use classification and demonstrate the easy adaptability of Gemini2.5 to new inputs. These results highlight the potential for geospatial professionals, working with non-standard specialized inputs, to easily leverage powerful multimodal models, such as Gemini2.5, to accelerate their work, benefiting from their rich reasoning and contextual capabilities, grounded in the specialized sensor data.",
            "score": 1,
            "issue_id": 6052,
            "pub_date": "2025-09-23",
            "pub_date_card": {
                "ru": "23 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 23",
                "zh": "9æœˆ23æ—¥"
            },
            "hash": "fbe226390b6ea231",
            "authors": [
                "Ganesh Mallya",
                "Yotam Gigi",
                "Dahun Kim",
                "Maxim Neumann",
                "Genady Beryozkin",
                "Tomer Shekel",
                "Anelia Angelova"
            ],
            "affiliations": [
                "Google DeepMind",
                "Google Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.19087.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#dataset",
                    "#benchmark",
                    "#transfer_learning",
                    "#optimization",
                    "#multimodal"
                ],
                "emoji": "ğŸ›°ï¸",
                "ru": {
                    "title": "ĞœÑƒĞ»ÑŒÑ‚Ğ¸ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ·Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ˜Ğ˜ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ zero-shot, Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° RGB-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ±Ñ‹Ğ» Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Gemini2.5 Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ´Ğ¸ÑÑ‚Ğ°Ğ½Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ¾Ğ½Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ·ĞµĞ¼Ğ»ĞµĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸ÑÑ‚Ğ¾Ğ² Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ³ĞµĞ¾Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ ÑĞ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞµĞ½ÑĞ¾Ñ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Unlocking Multimodal Models for Multi-Spectral Imagery Without Training",
                    "desc": "This paper presents a novel training-free method that allows generalist multimodal models to process multi-spectral imagery without prior training, enhancing their performance in remote sensing tasks. Multi-spectral images, which contain additional spectral bands, are crucial for accurately identifying physical materials on the ground. The proposed approach enables these models, typically trained only on RGB images, to adapt to multi-spectral data in a zero-shot manner by injecting domain-specific instructions. The results demonstrate significant performance improvements on remote sensing benchmarks, showcasing the potential for geospatial professionals to utilize advanced multimodal models effectively."
                },
                "zh": {
                    "title": "æ— è®­ç»ƒçš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œè½»æ¾å¤„ç†å¤šå…‰è°±å›¾åƒ",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ— è®­ç»ƒçš„æ–¹æ³•ï¼Œä½¿é€šç”¨å¤šæ¨¡æ€æ¨¡å‹èƒ½å¤Ÿä»¥é›¶æ ·æœ¬çš„æ–¹å¼å¤„ç†å¤šå…‰è°±å›¾åƒï¼Œä»è€Œæé«˜é¥æ„Ÿä»»åŠ¡çš„æ€§èƒ½ã€‚å¤šå…‰è°±å›¾åƒåœ¨åœŸåœ°åˆ©ç”¨åˆ†ç±»ã€ç¯å¢ƒç›‘æµ‹å’ŒåŸå¸‚è§„åˆ’ç­‰é¥æ„Ÿåº”ç”¨ä¸­å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚ä¼ ç»Ÿä¸Šï¼Œè¿™äº›å›¾åƒéœ€è¦ä¸“é—¨è®­ç»ƒçš„æœºå™¨å­¦ä¹ æ¨¡å‹è¿›è¡Œè‡ªåŠ¨åˆ†æï¼Œä½†è¿™ç§æ–¹æ³•æˆæœ¬é«˜ä¸”ä¸å¤Ÿçµæ´»ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨é€šç”¨å¤šæ¨¡æ€æ¨¡å‹çš„è§†è§‰ç†è§£èƒ½åŠ›ï¼Œèƒ½å¤Ÿè½»æ¾é€‚åº”æ–°çš„å¤šå…‰è°±è¾“å…¥ï¼Œå±•ç¤ºäº†åœ¨é¥æ„ŸåŸºå‡†æµ‹è¯•ä¸­çš„æ˜¾è‘—æ€§èƒ½æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.17083",
            "title": "HyRF: Hybrid Radiance Fields for Memory-efficient and High-quality Novel\n  View Synthesis",
            "url": "https://huggingface.co/papers/2509.17083",
            "abstract": "Hybrid Radiance Fields combine explicit Gaussians and neural fields to achieve high-quality rendering with reduced memory usage and real-time performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, 3D Gaussian Splatting (3DGS) has emerged as a powerful alternative to NeRF-based approaches, enabling real-time, high-quality novel view synthesis through explicit, optimizable 3D Gaussians. However, 3DGS suffers from significant memory overhead due to its reliance on per-Gaussian parameters to model view-dependent effects and anisotropic shapes. While recent works propose compressing 3DGS with neural fields, these methods struggle to capture high-frequency spatial variations in Gaussian properties, leading to degraded reconstruction of fine details. We present Hybrid Radiance Fields (HyRF), a novel scene representation that combines the strengths of explicit Gaussians and neural fields. HyRF decomposes the scene into (1) a compact set of explicit Gaussians storing only critical high-frequency parameters and (2) grid-based neural fields that predict remaining properties. To enhance representational capacity, we introduce a decoupled neural field architecture, separately modeling geometry (scale, opacity, rotation) and view-dependent color. Additionally, we propose a hybrid rendering scheme that composites Gaussian splatting with a neural field-predicted background, addressing limitations in distant scene representation. Experiments demonstrate that HyRF achieves state-of-the-art rendering quality while reducing model size by over 20 times compared to 3DGS and maintaining real-time performance. Our project page is available at https://wzpscott.github.io/hyrf/.",
            "score": 1,
            "issue_id": 6053,
            "pub_date": "2025-09-21",
            "pub_date_card": {
                "ru": "21 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 21",
                "zh": "9æœˆ21æ—¥"
            },
            "hash": "4a06acbb1d75ae4a",
            "authors": [
                "Zipeng Wang",
                "Dan Xu"
            ],
            "affiliations": [
                "Hong Kong University of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.17083.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#3d"
                ],
                "emoji": "ğŸŒˆ",
                "ru": {
                    "title": "Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ´Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»Ñ: Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² 3D Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğµ",
                    "desc": "Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ´Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»Ñ (HyRF) Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‚ ÑĞ²Ğ½Ñ‹Ğµ Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ñ‹ Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»Ñ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ° Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ†ĞµĞ½Ñƒ Ğ½Ğ° ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ ÑĞ²Ğ½Ñ‹Ñ… Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ¾Ğ², Ñ…Ñ€Ğ°Ğ½ÑÑ‰Ğ¸Ñ… Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹, Ğ¸ ÑĞµÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»Ñ, Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğµ Ğ¾ÑÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°. HyRF Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑŒĞ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ»Ñ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¸ Ñ†Ğ²ĞµÑ‚Ğ°, Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‰ĞµĞ³Ğ¾ Ğ¾Ñ‚ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ HyRF Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ°, ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ² 20 Ñ€Ğ°Ğ· Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ 3DGS, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing 3D Rendering with Hybrid Radiance Fields",
                    "desc": "Hybrid Radiance Fields (HyRF) introduce a new way to represent 3D scenes by merging explicit Gaussians with neural fields. This approach allows for high-quality rendering while significantly reducing memory usage and enabling real-time performance. HyRF uses a compact set of explicit Gaussians to capture essential high-frequency details and employs grid-based neural fields for other properties. The innovative architecture separates geometry and color modeling, leading to improved scene representation and rendering quality compared to previous methods."
                },
                "zh": {
                    "title": "æ··åˆè¾å°„åœºï¼šé«˜æ•ˆæ¸²æŸ“çš„æ–°æ–¹æ³•",
                    "desc": "æ··åˆè¾å°„åœºï¼ˆHyRFï¼‰æ˜¯ä¸€ç§æ–°é¢–çš„åœºæ™¯è¡¨ç¤ºæ–¹æ³•ï¼Œç»“åˆäº†æ˜¾å¼é«˜æ–¯å’Œç¥ç»åœºçš„ä¼˜ç‚¹ï¼Œä»¥å®ç°é«˜è´¨é‡æ¸²æŸ“ï¼ŒåŒæ—¶å‡å°‘å†…å­˜ä½¿ç”¨å¹¶ä¿æŒå®æ—¶æ€§èƒ½ã€‚HyRFå°†åœºæ™¯åˆ†è§£ä¸ºä¸€ç»„ç´§å‡‘çš„æ˜¾å¼é«˜æ–¯ï¼Œä»…å­˜å‚¨å…³é”®çš„é«˜é¢‘å‚æ•°ï¼Œä»¥åŠåŸºäºç½‘æ ¼çš„ç¥ç»åœºï¼Œç”¨äºé¢„æµ‹å…¶ä½™å±æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è§£è€¦çš„ç¥ç»åœºæ¶æ„ï¼Œåˆ†åˆ«å»ºæ¨¡å‡ ä½•å½¢çŠ¶ï¼ˆå°ºåº¦ã€ä¸é€æ˜åº¦ã€æ—‹è½¬ï¼‰å’Œè§†è§’ä¾èµ–çš„é¢œè‰²ã€‚å®éªŒè¡¨æ˜ï¼ŒHyRFåœ¨æ¸²æŸ“è´¨é‡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ï¼ŒåŒæ—¶æ¨¡å‹å¤§å°æ¯”3Dé«˜æ–¯ç‚¹äº‘å‡å°‘äº†20å€ä»¥ä¸Šã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.18849",
            "title": "MAPO: Mixed Advantage Policy Optimization",
            "url": "https://huggingface.co/papers/2509.18849",
            "abstract": "Mixed Advantage Policy Optimization (MAPO) dynamically reweights the advantage function to improve trajectory ranking in reinforcement learning for foundation models.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in reinforcement learning for foundation models, such as Group Relative Policy Optimization (GRPO), have significantly improved the performance of foundation models on reasoning tasks. Notably, the advantage function serves as a central mechanism in GRPO for ranking the trajectory importance. However, existing explorations encounter both advantage reversion and advantage mirror problems, which hinder the reasonable advantage allocation across different query samples. In this work, we propose an easy but effective GRPO strategy, Mixed Advantage Policy Optimization (MAPO). We reveal that the trajectory appears with different certainty and propose the advantage percent deviation for samples with high-certainty trajectories. Furthermore, we dynamically reweight the advantage function for samples with varying trajectory certainty, thereby adaptively configuring the advantage function to account for sample-specific characteristics. Comparison with related state-of-the-art methods, along with ablation studies on different advantage variants, validates the effectiveness of our approach.",
            "score": 0,
            "issue_id": 6053,
            "pub_date": "2025-09-23",
            "pub_date_card": {
                "ru": "23 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 23",
                "zh": "9æœˆ23æ—¥"
            },
            "hash": "9416ec88d3b85956",
            "authors": [
                "Wenke Huang",
                "Quan Zhang",
                "Yiyang Fang",
                "Jian Liang",
                "Xuankun Rong",
                "Huanjin Yao",
                "Guancheng Wan",
                "Ke Liang",
                "Wenwen He",
                "Mingjun Li",
                "Leszek Rutkowski",
                "Mang Ye",
                "Bo Du",
                "Dacheng Tao"
            ],
            "affiliations": [
                "ByteDance",
                "Nanyang Technological University",
                "National University of Defense Technology",
                "The AGH University of Krakow",
                "Wuhan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.18849.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#rl",
                    "#rlhf",
                    "#training"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "MAPO - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ° Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿ĞµÑ€ĞµĞ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°ĞµÑ‚ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹. MAPO Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ€ĞµĞ²ĞµÑ€ÑĞ¸Ğ¸ Ğ¸ Ğ·ĞµÑ€ĞºĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ°, Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‰Ğ¸Ğµ Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Dynamic Advantage Reweighting for Enhanced Trajectory Ranking",
                    "desc": "Mixed Advantage Policy Optimization (MAPO) enhances reinforcement learning by dynamically adjusting the advantage function to better rank trajectories in foundation models. It addresses issues like advantage reversion and advantage mirror problems that affect how advantages are distributed among different query samples. By introducing the concept of advantage percent deviation, MAPO focuses on samples with high-certainty trajectories, allowing for a more tailored advantage allocation. The effectiveness of MAPO is demonstrated through comparisons with existing methods and detailed ablation studies."
                },
                "zh": {
                    "title": "åŠ¨æ€è°ƒæ•´ä¼˜åŠ¿ï¼Œæå‡å¼ºåŒ–å­¦ä¹ æ•ˆæœ",
                    "desc": "æ··åˆä¼˜åŠ¿ç­–ç•¥ä¼˜åŒ–ï¼ˆMAPOï¼‰æ˜¯ä¸€ç§åœ¨å¼ºåŒ–å­¦ä¹ ä¸­åŠ¨æ€è°ƒæ•´ä¼˜åŠ¿å‡½æ•°çš„æ–¹æ³•ï¼Œæ—¨åœ¨æ”¹å–„åŸºç¡€æ¨¡å‹çš„è½¨è¿¹æ’åã€‚è¯¥æ–¹æ³•è§£å†³äº†ç°æœ‰æŠ€æœ¯ä¸­å­˜åœ¨çš„ä¼˜åŠ¿åè½¬å’Œä¼˜åŠ¿é•œåƒé—®é¢˜ï¼Œä»è€Œå®ç°æ›´åˆç†çš„ä¼˜åŠ¿åˆ†é…ã€‚MAPOé€šè¿‡å¼•å…¥é«˜ç¡®å®šæ€§è½¨è¿¹çš„ä¼˜åŠ¿ç™¾åˆ†æ¯”åå·®ï¼Œæ¥é€‚åº”ä¸åŒæ ·æœ¬çš„ç‰¹æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMAPOåœ¨ä¸å…¶ä»–å…ˆè¿›æ–¹æ³•çš„æ¯”è¾ƒä¸­è¡¨ç°å‡ºè‰²ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-09-23.html",
    "link_next": "2025-09-25.html",
    "link_month": "2025-09.html",
    "short_date_prev": {
        "ru": "23.09",
        "en": "09/23",
        "zh": "9æœˆ23æ—¥"
    },
    "short_date_next": {
        "ru": "25.09",
        "en": "09/25",
        "zh": "9æœˆ25æ—¥"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 4,
        "#agents": 0,
        "#cv": 0,
        "#rl": 4,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 3,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 3,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 5,
        "#robotics": 1,
        "#agi": 1,
        "#games": 2,
        "#interpretability": 1,
        "#reasoning": 3,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 7,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    }
}