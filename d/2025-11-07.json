{
    "date": {
        "ru": "7 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
        "en": "November 7",
        "zh": "11æœˆ7æ—¥"
    },
    "time_utc": "2025-11-07 09:00",
    "weekday": 4,
    "issue_id": 1,
    "home_page_url": "https://huggingface.co/papers?date=2025-11-07",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2511.04570",
            "title": "Thinking with Video: Video Generation as a Promising Multimodal\n  Reasoning Paradigm",
            "url": "https://huggingface.co/papers/2511.04570",
            "abstract": "The \"Thinking with Video\" paradigm enhances multimodal reasoning by integrating video generation models, demonstrated through the Video Thinking Benchmark and improved performance on both vision and text tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t \"Thinking with Text\" and \"Thinking with Images\" paradigm significantly improve the reasoning ability of large language models (LLMs) and Vision Language Models (VLMs). However, these paradigms have inherent limitations. (1) Images capture only single moments and fail to represent dynamic processes or continuous changes, and (2) The separation of text and vision as distinct modalities, hindering unified multimodal understanding and generation. To overcome these limitations, we introduce \"Thinking with Video\", a new paradigm that leverages video generation models, such as Sora-2, to bridge visual and textual reasoning in a unified temporal framework. To support this exploration, we developed the Video Thinking Benchmark (VideoThinkBench). VideoThinkBench encompasses two task categories: (1) vision-centric tasks (e.g., Eyeballing Puzzles), and (2) text-centric tasks (e.g., subsets of GSM8K, MMMU). Our evaluation establishes Sora-2 as a capable reasoner. On vision-centric tasks, Sora-2 is generally comparable to state-of-the-art (SOTA) VLMs, and even surpasses VLMs on several tasks, such as Eyeballing Games. On text-centric tasks, Sora-2 achieves 92% accuracy on MATH, and 75.53% accuracy on MMMU. Furthermore, we systematically analyse the source of these abilities. We also find that self-consistency and in-context learning can improve Sora-2's performance. In summary, our findings demonstrate that the video generation model is the potential unified multimodal understanding and generation model, positions \"thinking with video\" as a unified multimodal reasoning paradigm.",
            "score": 208,
            "issue_id": 1,
            "pub_date": "2025-11-06",
            "pub_date_card": {
                "ru": "6 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 6",
                "zh": "11æœˆ6æ—¥"
            },
            "hash": "da0cf16fafb0f708",
            "authors": [
                "Jingqi Tong",
                "Yurong Mou",
                "Hangcheng Li",
                "Mingzhe Li",
                "Yongzhuo Yang",
                "Ming Zhang",
                "Qiguang Chen",
                "Tianyi Liang",
                "Xiaomeng Hu",
                "Yining Zheng",
                "Xinchi Chen",
                "Jun Zhao",
                "Xuanjing Huang",
                "Xipeng Qiu"
            ],
            "affiliations": [
                "Fudan University",
                "Harbin Institute of Technology",
                "Shanghai Innovation Institute",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.04570.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#games",
                    "#video",
                    "#multimodal",
                    "#reasoning"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ’Ğ¸Ğ´ĞµĞ¾ ĞºĞ°Ğº ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Â«ĞœÑ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Â», ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ VideoThinkBench â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ°Ğº Ğ½Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Ñ‚ĞµÑÑ‚Ñ‹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Sora-2 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ğµ Ñ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¼Ğ¸ Vision Language Models, Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… (92% Ğ½Ğ° MATH Ğ¸ 75.53% Ğ½Ğ° MMMU). Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ÑŒ ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑ Ğ²Ğ¸Ğ´ĞµĞ¾ ĞºĞ°Ğº Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Revolutionizing Reasoning with Video Integration",
                    "desc": "The paper introduces the \"Thinking with Video\" paradigm, which enhances multimodal reasoning by integrating video generation models into the reasoning process. This approach addresses the limitations of previous paradigms that relied solely on text and images, which could not effectively represent dynamic changes. The authors developed the Video Thinking Benchmark (VideoThinkBench) to evaluate the performance of their model, Sora-2, on both vision-centric and text-centric tasks. Results show that Sora-2 performs comparably to state-of-the-art vision language models and achieves high accuracy on various reasoning tasks, highlighting its potential for unified multimodal understanding."
                },
                "zh": {
                    "title": "è§†é¢‘æ€ç»´ï¼šç»Ÿä¸€å¤šæ¨¡æ€æ¨ç†çš„æ–°èŒƒå¼",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†â€œè§†é¢‘æ€ç»´â€è¿™ä¸€æ–°èŒƒå¼ï¼Œé€šè¿‡æ•´åˆè§†é¢‘ç”Ÿæˆæ¨¡å‹æ¥å¢å¼ºå¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚ä¸ä¼ ç»Ÿçš„â€œæ–‡æœ¬æ€ç»´â€å’Œâ€œå›¾åƒæ€ç»´â€ç›¸æ¯”ï¼Œè§†é¢‘èƒ½å¤Ÿæ›´å¥½åœ°è¡¨ç¤ºåŠ¨æ€è¿‡ç¨‹å’Œè¿ç»­å˜åŒ–ï¼Œä»è€Œå…‹æœäº†å›¾åƒå’Œæ–‡æœ¬åˆ†ç¦»çš„å±€é™æ€§ã€‚æˆ‘ä»¬å¼€å‘äº†è§†é¢‘æ€ç»´åŸºå‡†ï¼ˆVideoThinkBenchï¼‰ï¼ŒåŒ…æ‹¬è§†è§‰ä¸­å¿ƒå’Œæ–‡æœ¬ä¸­å¿ƒçš„ä»»åŠ¡ï¼Œä»¥è¯„ä¼°è§†é¢‘ç”Ÿæˆæ¨¡å‹Sora-2çš„æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSora-2åœ¨è§†è§‰ä»»åŠ¡ä¸Šä¸æœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹ç›¸å½“ï¼Œå¹¶åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¶…è¶Šäº†å®ƒä»¬ï¼Œå±•ç¤ºäº†è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨ç»Ÿä¸€å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆä¸­çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.04460",
            "title": "V-Thinker: Interactive Thinking with Images",
            "url": "https://huggingface.co/papers/2511.04460",
            "abstract": "V-Thinker, a multimodal reasoning assistant using reinforcement learning, enhances image-interactive thinking by synthesizing datasets and aligning perception for improved performance in vision-centric tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Empowering Large Multimodal Models (LMMs) to deeply integrate image interaction with long-horizon reasoning capabilities remains a long-standing challenge in this field. Recent advances in vision-centric reasoning explore a promising \"Thinking with Images\" paradigm for LMMs, marking a shift from image-assisted reasoning to image-interactive thinking. While this milestone enables models to focus on fine-grained image regions, progress remains constrained by limited visual tool spaces and task-specific workflow designs. To bridge this gap, we present V-Thinker, a general-purpose multimodal reasoning assistant that enables interactive, vision-centric thinking through end-to-end reinforcement learning. V-Thinker comprises two key components: (1) a Data Evolution Flywheel that automatically synthesizes, evolves, and verifies interactive reasoning datasets across three dimensions-diversity, quality, and difficulty; and (2) a Visual Progressive Training Curriculum that first aligns perception via point-level supervision, then integrates interactive reasoning through a two-stage reinforcement learning framework. Furthermore, we introduce VTBench, an expert-verified benchmark targeting vision-centric interactive reasoning tasks. Extensive experiments demonstrate that V-Thinker consistently outperforms strong LMM-based baselines in both general and interactive reasoning scenarios, providing valuable insights for advancing image-interactive reasoning applications.",
            "score": 96,
            "issue_id": 1,
            "pub_date": "2025-11-06",
            "pub_date_card": {
                "ru": "6 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 6",
                "zh": "11æœˆ6æ—¥"
            },
            "hash": "40e8658b7a62ece7",
            "authors": [
                "Runqi Qiao",
                "Qiuna Tan",
                "Minghan Yang",
                "Guanting Dong",
                "Peiqing Yang",
                "Shiqiang Lang",
                "Enhui Wan",
                "Xiaowan Wang",
                "Yida Xu",
                "Lan Yang",
                "Chong Sun",
                "Chen Li",
                "Honggang Zhang"
            ],
            "affiliations": [
                "Beijing University of Posts and Telecommunications",
                "WeChat Vision, Tencent Inc."
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.04460.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#optimization",
                    "#multimodal",
                    "#training",
                    "#dataset",
                    "#rl",
                    "#reasoning",
                    "#synthetic"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸",
                    "desc": "V-Thinker â€” ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ°Ñ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ²Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½ÑƒÑ ÑƒÑ‡ĞµĞ±Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñƒ Ñ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°ÑÑ‰ÑƒÑÑÑ Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ VTBench â€” ÑĞºÑĞ¿ĞµÑ€Ñ‚-Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ V-Thinker Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Empowering Interactive Thinking with V-Thinker",
                    "desc": "V-Thinker is a multimodal reasoning assistant that uses reinforcement learning to improve how models think interactively with images. It addresses the challenge of integrating image interaction with long-term reasoning by introducing a new paradigm called 'Thinking with Images'. The system features a Data Evolution Flywheel that creates and refines datasets for better reasoning, and a Visual Progressive Training Curriculum that enhances perception and reasoning capabilities. Experimental results show that V-Thinker outperforms existing models in tasks that require vision-centric interactive reasoning."
                },
                "zh": {
                    "title": "V-Thinkerï¼šæå‡å›¾åƒäº¤äº’æ€ç»´çš„å¤šæ¨¡æ€æ¨ç†åŠ©æ‰‹",
                    "desc": "V-Thinkeræ˜¯ä¸€ç§å¤šæ¨¡æ€æ¨ç†åŠ©æ‰‹ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ æå‡å›¾åƒäº¤äº’æ€ç»´èƒ½åŠ›ã€‚å®ƒé€šè¿‡åˆæˆæ•°æ®é›†å’Œå¯¹é½æ„ŸçŸ¥ï¼Œæ”¹å–„è§†è§‰ä¸­å¿ƒä»»åŠ¡çš„è¡¨ç°ã€‚V-ThinkeråŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šæ•°æ®æ¼”åŒ–é£è½®å’Œè§†è§‰æ¸è¿›è®­ç»ƒè¯¾ç¨‹ï¼Œå‰è€…è‡ªåŠ¨ç”Ÿæˆå’ŒéªŒè¯æ¨ç†æ•°æ®é›†ï¼Œåè€…é€šè¿‡ä¸¤é˜¶æ®µå¼ºåŒ–å­¦ä¹ æ•´åˆäº¤äº’æ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒV-Thinkeråœ¨ä¸€èˆ¬å’Œäº¤äº’æ¨ç†åœºæ™¯ä¸­å‡ä¼˜äºç°æœ‰çš„å¤šæ¨¡æ€æ¨¡å‹åŸºçº¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.03773",
            "title": "Scaling Agent Learning via Experience Synthesis",
            "url": "https://huggingface.co/papers/2511.03773",
            "abstract": "DreamGym is a unified framework that synthesizes diverse experiences for scalable online RL training, improving agent performance and reducing real-world interactions.  \t\t\t\t\tAI-generated summary \t\t\t\t While reinforcement learning (RL) can empower large language model (LLM) agents by enabling self-improvement through interaction, its practical adoption remains challenging due to costly rollouts, limited task diversity, unreliable reward signals, and infrastructure complexity, all of which obstruct the collection of scalable experience data. To address these challenges, we introduce DreamGym, the first unified framework designed to synthesize diverse experiences with scalability in mind to enable effective online RL training for autonomous agents. Rather than relying on expensive real-environment rollouts, DreamGym distills environment dynamics into a reasoning-based experience model that derives consistent state transitions and feedback signals through step-by-step reasoning, enabling scalable agent rollout collection for RL. To improve the stability and quality of transitions, DreamGym leverages an experience replay buffer initialized with offline real-world data and continuously enriched with fresh interactions to actively support agent training. To improve knowledge acquisition, DreamGym adaptively generates new tasks that challenge the current agent policy, enabling more effective online curriculum learning. Experiments across diverse environments and agent backbones demonstrate that DreamGym substantially improves RL training, both in fully synthetic settings and in sim-to-real transfer scenarios. On non-RL-ready tasks like WebArena, DreamGym outperforms all baselines by over 30%. And in RL-ready but costly settings, it matches GRPO and PPO performance using only synthetic interactions. When transferring a policy trained purely on synthetic experiences to real-environment RL, DreamGym yields significant additional performance gains while requiring far fewer real-world interactions, providing a scalable warm-start strategy for general-purpose RL.",
            "score": 80,
            "issue_id": 1,
            "pub_date": "2025-11-05",
            "pub_date_card": {
                "ru": "5 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 5",
                "zh": "11æœˆ5æ—¥"
            },
            "hash": "bbedd76432556a37",
            "authors": [
                "Zhaorun Chen",
                "Zhuokai Zhao",
                "Kai Zhang",
                "Bo Liu",
                "Qi Qi",
                "Yifan Wu",
                "Tarun Kalluri",
                "Sara Cao",
                "Yuanhao Xiong",
                "Haibo Tong",
                "Huaxiu Yao",
                "Hengduo Li",
                "Jiacheng Zhu",
                "Xian Li",
                "Dawn Song",
                "Bo Li",
                "Jason Weston",
                "Dat Huynh"
            ],
            "affiliations": [
                "FAIR at Meta",
                "Meta Superintelligence Labs",
                "UC Berkeley",
                "UNC",
                "University of Chicago"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.03773.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#agents",
                    "#training",
                    "#rl",
                    "#reasoning",
                    "#synthetic"
                ],
                "emoji": "ğŸ‹ï¸",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ÑĞ¿Ğ¾Ñ€Ñ‚Ğ·Ğ°Ğ» Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ",
                    "desc": "DreamGym â€” ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL). Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğ¾Ğ¹, Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ¿Ñ‹Ñ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¾Ğ² ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ¸ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ±ÑƒÑ„ĞµÑ€ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¾Ğ¿Ñ‹Ñ‚Ğ°, Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾ Ğ¿Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ñ‹Ğ¹ Ğ½Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„ĞµÑ€ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ¸Ğ· ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑÑ€ĞµĞ´Ñ‹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ."
                },
                "en": {
                    "title": "DreamGym: Scalable Experience Synthesis for Enhanced RL Training",
                    "desc": "DreamGym is a novel framework that enhances online reinforcement learning (RL) by synthesizing diverse experiences, which helps improve agent performance while minimizing the need for real-world interactions. It addresses common challenges in RL, such as high costs and limited task diversity, by creating a reasoning-based experience model that simulates environment dynamics. This model allows for consistent state transitions and feedback, facilitating scalable agent training through an experience replay buffer that combines offline and online data. Additionally, DreamGym generates new tasks to promote adaptive learning, leading to significant performance improvements in both synthetic and real-world scenarios."
                },
                "zh": {
                    "title": "DreamGymï¼šå¯æ‰©å±•çš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ æ–°æ¡†æ¶",
                    "desc": "DreamGymæ˜¯ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨åˆæˆå¤šæ ·åŒ–çš„ç»éªŒï¼Œä»¥å®ç°å¯æ‰©å±•çš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒï¼Œä»è€Œæé«˜æ™ºèƒ½ä½“çš„æ€§èƒ½å¹¶å‡å°‘ä¸çœŸå®ä¸–ç•Œçš„äº¤äº’ã€‚è¯¥æ¡†æ¶é€šè¿‡æ¨ç†åŸºç¡€çš„ç»éªŒæ¨¡å‹ï¼Œæç‚¼ç¯å¢ƒåŠ¨æ€ï¼Œç”Ÿæˆä¸€è‡´çš„çŠ¶æ€è½¬ç§»å’Œåé¦ˆä¿¡å·ï¼Œé¿å…äº†æ˜‚è´µçš„çœŸå®ç¯å¢ƒå›åˆã€‚DreamGymè¿˜åˆ©ç”¨ç»éªŒé‡æ”¾ç¼“å†²åŒºï¼Œç»“åˆç¦»çº¿çœŸå®æ•°æ®å’Œæ–°äº¤äº’ï¼Œæå‡è¿‡æ¸¡çš„ç¨³å®šæ€§å’Œè´¨é‡ã€‚é€šè¿‡è‡ªé€‚åº”ç”Ÿæˆæ–°ä»»åŠ¡ï¼ŒDreamGymæœ‰æ•ˆæ”¯æŒåœ¨çº¿è¯¾ç¨‹å­¦ä¹ ï¼Œæ˜¾è‘—æé«˜äº†RLè®­ç»ƒçš„æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.04670",
            "title": "Cambrian-S: Towards Spatial Supersensing in Video",
            "url": "https://huggingface.co/papers/2511.04670",
            "abstract": "Progress in multimodal intelligence requires a shift to supersensing, including semantic perception, event cognition, spatial cognition, and predictive modeling, demonstrated through VSI-SUPER benchmarks and a self-supervised predictive sensing approach.  \t\t\t\t\tAI-generated summary \t\t\t\t We argue that progress in true multimodal intelligence calls for a shift from reactive, task-driven systems and brute-force long context towards a broader paradigm of supersensing. We frame spatial supersensing as four stages beyond linguistic-only understanding: semantic perception (naming what is seen), streaming event cognition (maintaining memory across continuous experiences), implicit 3D spatial cognition (inferring the world behind pixels), and predictive world modeling (creating internal models that filter and organize information). Current benchmarks largely test only the early stages, offering narrow coverage of spatial cognition and rarely challenging models in ways that require true world modeling. To drive progress in spatial supersensing, we present VSI-SUPER, a two-part benchmark: VSR (long-horizon visual spatial recall) and VSC (continual visual spatial counting). These tasks require arbitrarily long video inputs yet are resistant to brute-force context expansion. We then test data scaling limits by curating VSI-590K and training Cambrian-S, achieving +30% absolute improvement on VSI-Bench without sacrificing general capabilities. Yet performance on VSI-SUPER remains limited, indicating that scale alone is insufficient for spatial supersensing. We propose predictive sensing as a path forward, presenting a proof-of-concept in which a self-supervised next-latent-frame predictor leverages surprise (prediction error) to drive memory and event segmentation. On VSI-SUPER, this approach substantially outperforms leading proprietary baselines, showing that spatial supersensing requires models that not only see but also anticipate, select, and organize experience.",
            "score": 36,
            "issue_id": 1,
            "pub_date": "2025-11-06",
            "pub_date_card": {
                "ru": "6 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 6",
                "zh": "11æœˆ6æ—¥"
            },
            "hash": "1562e300c0eb5c05",
            "authors": [
                "Shusheng Yang",
                "Jihan Yang",
                "Pinzhi Huang",
                "Ellis Brown",
                "Zihao Yang",
                "Yue Yu",
                "Shengbang Tong",
                "Zihan Zheng",
                "Yifan Xu",
                "Muhan Wang",
                "Daohan Lu",
                "Rob Fergus",
                "Yann LeCun",
                "Li Fei-Fei",
                "Saining Xie"
            ],
            "affiliations": [
                "New York University",
                "Stanford University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.04670.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#open_source",
                    "#video",
                    "#long_context",
                    "#multimodal",
                    "#training",
                    "#dataset",
                    "#synthetic"
                ],
                "emoji": "ğŸ”®",
                "ru": {
                    "title": "ĞÑ‚ Ñ€ĞµĞ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğº Ğ¿Ñ€ĞµĞ´Ğ¸ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ ÑÑƒĞ¿ĞµÑ€ÑĞµĞ½ÑĞ¸Ğ½Ğ³Ñƒ: Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ³Ğ¾ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğµ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ğ°Ñ 'ÑÑƒĞ¿ĞµÑ€ÑĞµĞ½ÑĞ¸Ğ½Ğ³', ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ: ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ, ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ğ² Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¸ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¸Ñ€Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº VSI-SUPER Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑÑƒĞ¿ĞµÑ€ÑĞµĞ½ÑĞ¸Ğ½Ğ³Ğ°, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ´Ğ¾Ğ»Ğ³Ğ¾Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑÑ‡Ñ‘Ñ‚Ğ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¾ 590K Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Cambrian-S Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 30%, Ğ¾Ğ´Ğ½Ğ°ĞºĞ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´Ğ¸ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞµĞ½ÑĞ¸Ğ½Ğ³Ğ° Ñ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾ÑˆĞ¸Ğ±ĞºÑƒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ VSI-SUPER."
                },
                "en": {
                    "title": "Advancing Multimodal Intelligence through Supersensing",
                    "desc": "This paper discusses the need for advancements in multimodal intelligence through a concept called supersensing, which includes understanding semantics, events, spatial awareness, and predictive modeling. It introduces the VSI-SUPER benchmarks to evaluate these capabilities, emphasizing that current tests focus too much on basic understanding rather than true world modeling. The authors present a new approach called predictive sensing, which uses self-supervised learning to improve memory and event segmentation by predicting future frames based on past experiences. Their findings show that simply increasing data size is not enough; models must also be able to anticipate and organize information effectively to achieve true spatial supersensing."
                },
                "zh": {
                    "title": "è¶…æ„ŸçŸ¥ï¼šå¤šæ¨¡æ€æ™ºèƒ½çš„æ–°æ–¹å‘",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤šæ¨¡æ€æ™ºèƒ½çš„è¿›å±•ï¼Œæå‡ºéœ€è¦è½¬å‘è¶…æ„ŸçŸ¥çš„æ¦‚å¿µï¼ŒåŒ…æ‹¬è¯­ä¹‰æ„ŸçŸ¥ã€äº‹ä»¶è®¤çŸ¥ã€ç©ºé—´è®¤çŸ¥å’Œé¢„æµ‹å»ºæ¨¡ã€‚ä½œè€…ä»‹ç»äº†VSI-SUPERåŸºå‡†æµ‹è¯•ï¼Œå¼ºè°ƒäº†å½“å‰åŸºå‡†ä¸»è¦æµ‹è¯•æ—©æœŸé˜¶æ®µï¼Œç¼ºä¹å¯¹ç©ºé—´è®¤çŸ¥çš„å…¨é¢æŒ‘æˆ˜ã€‚é€šè¿‡è‡ªç›‘ç£çš„é¢„æµ‹æ„ŸçŸ¥æ–¹æ³•ï¼Œè®ºæ–‡å±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨é¢„æµ‹è¯¯å·®æ¥æ¨åŠ¨è®°å¿†å’Œäº‹ä»¶åˆ†å‰²ï¼Œä»è€Œæå‡æ¨¡å‹çš„æ€§èƒ½ã€‚æœ€ç»ˆï¼Œç ”ç©¶è¡¨æ˜ï¼Œç©ºé—´è¶…æ„ŸçŸ¥ä¸ä»…éœ€è¦æ¨¡å‹èƒ½å¤Ÿè§‚å¯Ÿï¼Œè¿˜éœ€è¦å…¶å…·å¤‡é¢„æµ‹ã€é€‰æ‹©å’Œç»„ç»‡ç»éªŒçš„èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.03929",
            "title": "NVIDIA Nemotron Nano V2 VL",
            "url": "https://huggingface.co/papers/2511.03929",
            "abstract": "Nemotron Nano V2 VL, a hybrid Mamba-Transformer LLM, improves document and video understanding through enhanced architecture and token reduction techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Nemotron Nano V2 VL, the latest model of the Nemotron vision-language series designed for strong real-world document understanding, long video comprehension, and reasoning tasks. Nemotron Nano V2 VL delivers significant improvements over our previous model, Llama-3.1-Nemotron-Nano-VL-8B, across all vision and text domains through major enhancements in model architecture, datasets, and training recipes. Nemotron Nano V2 VL builds on Nemotron Nano V2, a hybrid Mamba-Transformer LLM, and innovative token reduction techniques to achieve higher inference throughput in long document and video scenarios. We are releasing model checkpoints in BF16, FP8, and FP4 formats and sharing large parts of our datasets, recipes and training code.",
            "score": 26,
            "issue_id": 1,
            "pub_date": "2025-11-06",
            "pub_date_card": {
                "ru": "6 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 6",
                "zh": "11æœˆ6æ—¥"
            },
            "hash": "fba6c98589033970",
            "authors": [
                "NVIDIA",
                ":",
                "Amala Sanjay Deshmukh",
                "Kateryna Chumachenko",
                "Tuomas Rintamaki",
                "Matthieu Le",
                "Tyler Poon",
                "Danial Mohseni Taheri",
                "Ilia Karmanov",
                "Guilin Liu",
                "Jarno Seppanen",
                "Guo Chen",
                "Karan Sapra",
                "Zhiding Yu",
                "Adi Renduchintala",
                "Charles Wang",
                "Peter Jin",
                "Arushi Goel",
                "Mike Ranzinger",
                "Lukas Voegtle",
                "Philipp Fischer",
                "Timo Roman",
                "Wei Ping",
                "Boxin Wang",
                "Zhuolin Yang",
                "Nayeon Lee",
                "Shaokun Zhang",
                "Fuxiao Liu",
                "Zhiqi Li",
                "Di Zhang",
                "Greg Heinrich",
                "Hongxu Yin",
                "Song Han",
                "Pavlo Molchanov",
                "Parth Mannan",
                "Yao Xu",
                "Jane Polak Scowcroft",
                "Tom Balough",
                "Subhashree Radhakrishnan",
                "Paris Zhang",
                "Sean Cha",
                "Ratnesh Kumar",
                "Zaid Pervaiz Bhat",
                "Jian Zhang",
                "Darragh Hanley",
                "Pritam Biswas",
                "Jesse Oliver",
                "Kevin Vasques",
                "Roger Waleffe",
                "Duncan Riach",
                "Oluwatobi Olabiyi",
                "Ameya Sunil Mahabaleshwarkar",
                "Bilal Kartal",
                "Pritam Gundecha",
                "Khanh Nguyen",
                "Alexandre Milesi",
                "Eugene Khvedchenia",
                "Ran Zilberstein",
                "Ofri Masad",
                "Natan Bagrov",
                "Nave Assaf",
                "Tomer Asida",
                "Daniel Afrimi",
                "Amit Zuker",
                "Netanel Haber",
                "Zhiyu Cheng",
                "Jingyu Xin",
                "Di Wu",
                "Nik Spirin",
                "Maryam Moosaei",
                "Roman Ageev",
                "Vanshil Atul Shah",
                "Yuting Wu",
                "Daniel Korzekwa",
                "Unnikrishnan Kizhakkemadam Sreekumar",
                "Wanli Jiang",
                "Padmavathy Subramanian",
                "Alejandra Rico",
                "Sandip Bhaskar",
                "Saeid Motiian",
                "Kedi Wu",
                "Annie Surla",
                "Chia-Chih Chen",
                "Hayden Wolff",
                "Matthew Feinberg",
                "Melissa Corpuz",
                "Marek Wawrzos",
                "Eileen Long",
                "Aastha Jhunjhunwala",
                "Paul Hendricks",
                "Farzan Memarian",
                "Benika Hall",
                "Xin-Yu Wang",
                "David Mosallanezhad",
                "Soumye Singhal",
                "Luis Vega",
                "Katherine Cheung",
                "Krzysztof Pawelec",
                "Michael Evans",
                "Katherine Luna",
                "Jie Lou",
                "Erick Galinkin",
                "Akshay Hazare",
                "Kaustubh Purandare",
                "Ann Guan",
                "Anna Warno",
                "Chen Cui",
                "Yoshi Suhara",
                "Shibani Likhite",
                "Seph Mard",
                "Meredith Price",
                "Laya Sleiman",
                "Saori Kaji",
                "Udi Karpas",
                "Kari Briski",
                "Joey Conway",
                "Michael Lightstone",
                "Jan Kautz",
                "Mohammad Shoeybi",
                "Mostofa Patwary",
                "Jonathen Cohen",
                "Oleksii Kuchaiev",
                "Andrew Tao",
                "Bryan Catanzaro"
            ],
            "affiliations": [
                "NVIDIA"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.03929.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#inference",
                    "#open_source",
                    "#architecture",
                    "#long_context",
                    "#multimodal",
                    "#training",
                    "#dataset"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¸ Ñ€ĞµĞ´ÑƒĞºÑ†Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²",
                    "desc": "Nemotron Nano V2 VL â€” ÑÑ‚Ğ¾ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Mamba Ğ¸ Transformer Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ñ€ĞµĞ´ÑƒĞºÑ†Ğ¸Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ—Ğ° ÑÑ‡ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹ Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ, Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰ĞµĞ¹ Ğ²ĞµÑ€ÑĞ¸ĞµĞ¹ Ğ½Ğ° Ğ²ÑĞµÑ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºÑƒÑÑ‚ Ğ²ĞµÑĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ°Ñ… Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ´ĞµĞ»ÑÑ‚ÑÑ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ğ¼Ğ¸, Ñ€ĞµÑ†ĞµĞ¿Ñ‚Ğ°Ğ¼Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ĞºĞ¾Ğ´Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Revolutionizing Document and Video Comprehension with Nemotron Nano V2 VL",
                    "desc": "The Nemotron Nano V2 VL is a new hybrid Mamba-Transformer large language model (LLM) that enhances the understanding of documents and videos. It outperforms its predecessor, Llama-3.1-Nemotron-Nano-VL-8B, by improving the model architecture and utilizing advanced datasets and training methods. The model incorporates innovative token reduction techniques, allowing for faster processing of long documents and videos. Additionally, the release includes model checkpoints in various formats and access to datasets and training code for further research."
                },
                "zh": {
                    "title": "æå‡æ–‡æ¡£ä¸è§†é¢‘ç†è§£çš„æ··åˆæ¨¡å‹",
                    "desc": "Nemotron Nano V2 VL æ˜¯ä¸€ç§æ··åˆçš„ Mamba-Transformer å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨æå‡æ–‡æ¡£å’Œè§†é¢‘çš„ç†è§£èƒ½åŠ›ã€‚é€šè¿‡æ”¹è¿›çš„æ¨¡å‹æ¶æ„å’Œä»¤ç‰Œå‡å°‘æŠ€æœ¯ï¼Œè¯¥æ¨¡å‹åœ¨å¤„ç†é•¿æ–‡æ¡£å’Œè§†é¢‘æ—¶å®ç°äº†æ›´é«˜çš„æ¨ç†æ•ˆç‡ã€‚ä¸ä¹‹å‰çš„ Llama-3.1-Nemotron-Nano-VL-8B æ¨¡å‹ç›¸æ¯”ï¼ŒNemotron Nano V2 VL åœ¨è§†è§‰å’Œæ–‡æœ¬é¢†åŸŸéƒ½å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚æˆ‘ä»¬è¿˜å°†å‘å¸ƒ BF16ã€FP8 å’Œ FP4 æ ¼å¼çš„æ¨¡å‹æ£€æŸ¥ç‚¹ï¼Œå¹¶åˆ†äº«å¤§é‡æ•°æ®é›†ã€è®­ç»ƒé…æ–¹å’Œä»£ç ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.04217",
            "title": "The Strong Lottery Ticket Hypothesis for Multi-Head Attention Mechanisms",
            "url": "https://huggingface.co/papers/2511.04217",
            "abstract": "Theoretical analysis proves the existence of strong lottery tickets within multi-head attention mechanisms and extends the strong lottery ticket hypothesis to transformers without normalization layers.  \t\t\t\t\tAI-generated summary \t\t\t\t The strong lottery ticket hypothesis (SLTH) conjectures that high-performing subnetworks, called strong lottery tickets (SLTs), are hidden in randomly initialized neural networks. Although recent theoretical studies have established the SLTH across various neural architectures, the SLTH for transformer architectures still lacks theoretical understanding. In particular, the current theory of the SLTH does not yet account for the multi-head attention (MHA) mechanism, a core component of transformers. To address this gap, we introduce a theoretical analysis of the existence of SLTs within MHAs. We prove that, if a randomly initialized MHA of H heads and input dimension d has the hidden dimension O(dlog(Hd^{3/2})) for the key and value, it contains an SLT that approximates an arbitrary MHA with the same input dimension with high probability. Furthermore, by leveraging this theory for MHAs, we extend the SLTH to transformers without normalization layers. We empirically validate our theoretical findings, demonstrating that the approximation error between the SLT within a source model (MHA and transformer) and an approximate target counterpart decreases exponentially by increasing the hidden dimension of the source model.",
            "score": 15,
            "issue_id": 1,
            "pub_date": "2025-11-06",
            "pub_date_card": {
                "ru": "6 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 6",
                "zh": "11æœˆ6æ—¥"
            },
            "hash": "6c0c89f3ba0b4082",
            "authors": [
                "Hikari Otsuka",
                "Daiki Chijiwa",
                "Yasuyuki Okoshi",
                "Daichi Fujiki",
                "Susumu Takeuchi",
                "Masato Motomura"
            ],
            "affiliations": [
                "Institute of Science Tokyo",
                "NTT, Inc."
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.04217.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#math"
                ],
                "emoji": "ğŸŸï¸",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ»Ğ¾Ñ‚ĞµÑ€ĞµĞ¹Ğ½Ñ‹Ğµ Ğ±Ğ¸Ğ»ĞµÑ‚Ñ‹ ÑĞºÑ€Ñ‹Ñ‚Ñ‹ Ğ² Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ… Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ»Ğ¾Ñ‚ĞµÑ€ĞµĞ¹Ğ½Ñ‹Ñ… Ğ±Ğ¸Ğ»ĞµÑ‚Ğ¾Ğ² (subnetworks Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸) Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğ¹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ (multi-head attention), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ¼ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ². ĞĞ½Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ MHA Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğ¼ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ñ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ»Ğ¾Ñ‚ĞµÑ€ĞµĞ¹Ğ½Ñ‹Ğ¹ Ğ±Ğ¸Ğ»ĞµÑ‚, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ°Ğ¿Ğ¿Ñ€Ğ¾ĞºÑĞ¸Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ğ¹ MHA Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ¹ Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑÑ‚ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·Ñƒ Ğ¾ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ»Ğ¾Ñ‚ĞµÑ€ĞµĞ¹Ğ½Ñ‹Ñ… Ğ±Ğ¸Ğ»ĞµÑ‚Ğ°Ñ… Ğ½Ğ° Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ğ±ĞµĞ· ÑĞ»Ğ¾ĞµĞ² Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑĞºÑĞ¿Ğ¾Ğ½ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ°Ğ¿Ğ¿Ñ€Ğ¾ĞºÑĞ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ñ."
                },
                "en": {
                    "title": "Unlocking Strong Lottery Tickets in Transformers",
                    "desc": "This paper explores the strong lottery ticket hypothesis (SLTH) in the context of multi-head attention (MHA) mechanisms used in transformers. It demonstrates that within a randomly initialized MHA, there exist high-performing subnetworks, known as strong lottery tickets, that can effectively approximate the performance of the full model. The authors provide a theoretical framework showing that if certain conditions on the hidden dimensions are met, these SLTs can be found with high probability. Additionally, they extend the SLTH to transformers that do not utilize normalization layers, supporting their claims with empirical evidence of reduced approximation error as the hidden dimensions increase."
                },
                "zh": {
                    "title": "å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„å¼ºå½©ç¥¨ç¥¨æ®å­˜åœ¨æ€§åˆ†æ",
                    "desc": "æœ¬æ–‡æå‡ºäº†å¼ºå½©ç¥¨å‡è®¾åœ¨å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„å­˜åœ¨æ€§åˆ†æï¼Œå¹¶å°†å…¶æ‰©å±•åˆ°æ²¡æœ‰å½’ä¸€åŒ–å±‚çš„å˜æ¢å™¨æ¶æ„ã€‚å¼ºå½©ç¥¨å‡è®¾è®¤ä¸ºï¼Œåœ¨éšæœºåˆå§‹åŒ–çš„ç¥ç»ç½‘ç»œä¸­ï¼Œå­˜åœ¨é«˜æ€§èƒ½çš„å­ç½‘ç»œï¼Œç§°ä¸ºå¼ºå½©ç¥¨ç¥¨æ®ã€‚æˆ‘ä»¬è¯æ˜äº†ï¼Œå¦‚æœä¸€ä¸ªéšæœºåˆå§‹åŒ–çš„å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶å…·æœ‰ç‰¹å®šçš„éšè—ç»´åº¦ï¼Œå®ƒå°±åŒ…å«ä¸€ä¸ªå¯ä»¥é«˜æ¦‚ç‡è¿‘ä¼¼ä»»æ„å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶çš„å¼ºå½©ç¥¨ç¥¨æ®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„å®éªŒè¯æ˜äº†æºæ¨¡å‹ä¸­çš„å¼ºå½©ç¥¨ç¥¨æ®ä¸ç›®æ ‡æ¨¡å‹ä¹‹é—´çš„è¿‘ä¼¼è¯¯å·®éšç€éšè—ç»´åº¦çš„å¢åŠ è€ŒæŒ‡æ•°ä¸‹é™ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.04307",
            "title": "GUI-360: A Comprehensive Dataset and Benchmark for Computer-Using Agents",
            "url": "https://huggingface.co/papers/2511.04307",
            "abstract": "GUI-360Â° is a large-scale dataset and benchmark suite for computer-using agents, addressing gaps in real-world tasks, automated data collection, and unified evaluation of GUI grounding, screen parsing, and action prediction.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce GUI-360^circ, a large-scale, comprehensive dataset and benchmark suite designed to advance computer-using agents (CUAs). CUAs present unique challenges and is constrained by three persistent gaps: a scarcity of real-world CUA tasks, the lack of automated collection-and-annotation pipelines for multi-modal trajectories, and the absence of a unified benchmark that jointly evaluates GUI grounding, screen parsing, and action prediction.   GUI-360^circ addresses these gaps with an LLM-augmented, largely automated pipeline for query sourcing, environment-template construction, task instantiation, batched execution, and LLM-driven quality filtering. The released corpus contains over 1.2M executed action steps across thousands of trajectories in popular Windows office applications, and includes full-resolution screenshots, accessibility metadata when available, instantiated goals, intermediate reasoning traces, and both successful and failed action trajectories. The dataset supports three canonical tasks, GUI grounding, screen parsing, and action prediction, and a hybrid GUI+API action space that reflects modern agent designs. Benchmarking state-of-the-art vision--language models on GUI-360^circ reveals substantial out-of-the-box shortcomings in grounding and action prediction; supervised fine-tuning and reinforcement learning yield significant gains but do not close the gap to human-level reliability. We release GUI-360^circ and accompanying code to facilitate reproducible research and accelerate progress on robust desktop CUAs.   The full dataset has been made public on https://huggingface.co/datasets/vyokky/GUI-360.",
            "score": 14,
            "issue_id": 1,
            "pub_date": "2025-11-06",
            "pub_date_card": {
                "ru": "6 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 6",
                "zh": "11æœˆ6æ—¥"
            },
            "hash": "aa241d25d4b42cb3",
            "authors": [
                "Jian Mu",
                "Chaoyun Zhang",
                "Chiming Ni",
                "Lu Wang",
                "Bo Qiao",
                "Kartik Mathur",
                "Qianhui Wu",
                "Yuhang Xie",
                "Xiaojun Ma",
                "Mengyu Zhou",
                "Si Qin",
                "Liqun Li",
                "Yu Kang",
                "Minghua Ma",
                "Qingwei Lin",
                "Saravan Rajmohan",
                "Dongmei Zhang"
            ],
            "affiliations": [
                "Microsoft",
                "Nanjing University",
                "Peking University",
                "ZJU-UIUC"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.04307.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#agents",
                    "#multimodal",
                    "#training",
                    "#dataset"
                ],
                "emoji": "ğŸ–¥ï¸",
                "ru": {
                    "title": "ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğ¼ ÑÑ‚Ğ¾Ğ»Ğ¾Ğ¼",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ Ğ±Ğ°Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… GUI-360Â°, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ°Ñ Ğ±Ğ¾Ğ»ĞµĞµ 1.2 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°Ğ¼Ğ¸. ĞĞ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¾Ğ±Ñ€Ğ°Ğ½ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞºÑ€Ğ¸Ğ½ÑˆĞ¾Ñ‚Ñ‹, Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ² Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ… Windows. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸: Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ° ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑĞºÑ€Ğ°Ğ½Ğ° Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ ÑÑ‚Ğ¸Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ². Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑÑÑ‚ÑÑ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ½Ğ¾ Ğ¾ÑÑ‚Ğ°Ñ‘Ñ‚ÑÑ Ñ€Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ´Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°."
                },
                "en": {
                    "title": "Empowering Computer-Using Agents with GUI-360Â° Dataset",
                    "desc": "The paper introduces GUI-360Â°, a large-scale dataset aimed at improving computer-using agents (CUAs) by addressing key challenges in real-world applications. It provides a comprehensive benchmark that evaluates GUI grounding, screen parsing, and action prediction, which are essential for CUAs to interact effectively with graphical user interfaces. The dataset includes over 1.2 million action steps from various Windows applications, along with detailed metadata and reasoning traces, enabling researchers to train and test their models. Initial evaluations of state-of-the-art models on this dataset show significant performance gaps, highlighting the need for further advancements in grounding and action prediction to achieve human-level performance."
                },
                "zh": {
                    "title": "æ¨åŠ¨è®¡ç®—æœºä½¿ç”¨ä»£ç†çš„é©å‘½æ€§æ•°æ®é›†",
                    "desc": "GUI-360Â°æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„æ•°æ®é›†å’ŒåŸºå‡†å¥—ä»¶ï¼Œæ—¨åœ¨æ¨åŠ¨è®¡ç®—æœºä½¿ç”¨ä»£ç†ï¼ˆCUAï¼‰çš„å‘å±•ã€‚è¯¥æ•°æ®é›†è§£å†³äº†CUAé¢ä¸´çš„ä¸‰ä¸ªä¸»è¦é—®é¢˜ï¼šç¼ºä¹çœŸå®ä¸–ç•Œçš„ä»»åŠ¡ã€ç¼ºå°‘è‡ªåŠ¨åŒ–çš„æ•°æ®æ”¶é›†å’Œæ³¨é‡Šæµç¨‹ï¼Œä»¥åŠç¼ºä¹ç»Ÿä¸€çš„è¯„ä¼°åŸºå‡†ã€‚GUI-360Â°æä¾›äº†ä¸€ä¸ªè‡ªåŠ¨åŒ–çš„ç®¡é“ï¼Œç”¨äºæŸ¥è¯¢æ¥æºã€ç¯å¢ƒæ¨¡æ¿æ„å»ºã€ä»»åŠ¡å®ä¾‹åŒ–å’Œè´¨é‡è¿‡æ»¤ï¼ŒåŒ…å«è¶…è¿‡120ä¸‡æ¡æ‰§è¡Œçš„åŠ¨ä½œæ­¥éª¤ã€‚é€šè¿‡å¯¹æœ€å…ˆè¿›çš„è§†è§‰-è¯­è¨€æ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œå‘ç°å…¶åœ¨GUIå®šä½å’ŒåŠ¨ä½œé¢„æµ‹æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œå°½ç®¡ç»è¿‡ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ åæœ‰æ‰€æ”¹å–„ï¼Œä½†ä»æœªè¾¾åˆ°äººç±»æ°´å¹³çš„å¯é æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.03774",
            "title": "Contamination Detection for VLMs using Multi-Modal Semantic Perturbation",
            "url": "https://huggingface.co/papers/2511.03774",
            "abstract": "A novel detection method based on multi-modal semantic perturbation is proposed to identify contaminated Vision-Language Models, demonstrating robustness across various contamination strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Vision-Language Models (VLMs) have achieved state-of-the-art performance on numerous benchmark tasks. However, the use of internet-scale, often proprietary, pretraining corpora raises a critical concern for both practitioners and users: inflated performance due to test-set leakage. While prior works have proposed mitigation strategies such as decontamination of pretraining data and benchmark redesign for LLMs, the complementary direction of developing detection methods for contaminated VLMs remains underexplored. To address this gap, we deliberately contaminate open-source VLMs on popular benchmarks and show that existing detection approaches either fail outright or exhibit inconsistent behavior. We then propose a novel simple yet effective detection method based on multi-modal semantic perturbation, demonstrating that contaminated models fail to generalize under controlled perturbations. Finally, we validate our approach across multiple realistic contamination strategies, confirming its robustness and effectiveness. The code and perturbed dataset will be released publicly.",
            "score": 12,
            "issue_id": 1,
            "pub_date": "2025-11-05",
            "pub_date_card": {
                "ru": "5 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 5",
                "zh": "11æœˆ5æ—¥"
            },
            "hash": "fb9226e5719b12da",
            "authors": [
                "Jaden Park",
                "Mu Cai",
                "Feng Yao",
                "Jingbo Shang",
                "Soochahn Lee",
                "Yong Jae Lee"
            ],
            "affiliations": [
                "Kookmin University",
                "University of California, San Diego",
                "University of Wisconsin-Madison"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.03774.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multimodal",
                    "#leakage",
                    "#dataset",
                    "#security"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ’Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ ÑƒÑ‚ĞµÑ‡ĞµĞº Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Vision-Language Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‡ĞµÑ€ĞµĞ· Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ ÑƒÑ‚ĞµÑ‡ĞºĞ¸ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Vision-Language (VLM) Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸Ñ… ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğº ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸ÑĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹ Ğ¸Ğ»Ğ¸ Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚Ğ°Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ·Ğ°Ğ³Ñ€ÑĞ·Ğ½ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‚ÑÑ Ğ¿Ğ¾Ğ´ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… ĞºĞ¾Ğ½Ñ‚Ğ°Ğ¼Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "Detecting Contamination in Vision-Language Models with Robust Perturbation Techniques",
                    "desc": "This paper introduces a new method for detecting contaminated Vision-Language Models (VLMs) using multi-modal semantic perturbation. The authors highlight the issue of inflated performance in VLMs due to test-set leakage from pretraining data. They demonstrate that existing detection methods are inadequate and often inconsistent when faced with contaminated models. The proposed method shows that these contaminated models struggle to generalize when subjected to specific perturbations, proving its robustness across various contamination strategies."
                },
                "zh": {
                    "title": "æ–°æ–¹æ³•è¯†åˆ«æ±¡æŸ“çš„è§†è§‰-è¯­è¨€æ¨¡å‹",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤šæ¨¡æ€è¯­ä¹‰æ‰°åŠ¨çš„æ–°å‹æ£€æµ‹æ–¹æ³•ï¼Œç”¨äºè¯†åˆ«è¢«æ±¡æŸ“çš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç°æœ‰çš„æ£€æµ‹æ–¹æ³•åœ¨é¢å¯¹ä¸åŒçš„æ±¡æŸ“ç­–ç•¥æ—¶ï¼Œå¾€å¾€æ— æ³•æœ‰æ•ˆè¯†åˆ«æˆ–è¡¨ç°ä¸ä¸€è‡´ã€‚é€šè¿‡æ•…æ„æ±¡æŸ“å¼€æºVLMså¹¶è¿›è¡Œå®éªŒï¼ŒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ§åˆ¶æ‰°åŠ¨ä¸‹çš„æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬å°†åœ¨å¤šä¸ªçœŸå®æ±¡æŸ“ç­–ç•¥ä¸‹éªŒè¯è¯¥æ–¹æ³•ï¼Œå¹¶è®¡åˆ’å…¬å¼€å‘å¸ƒä»£ç å’Œæ‰°åŠ¨æ•°æ®é›†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.04655",
            "title": "Benchmark Designers Should \"Train on the Test Set\" to Expose Exploitable\n  Non-Visual Shortcuts",
            "url": "https://huggingface.co/papers/2511.04655",
            "abstract": "A framework for diagnosing and debiasing multimodal benchmarks reveals and mitigates non-visual biases, improving the robustness of Multimodal Large Language Models.  \t\t\t\t\tAI-generated summary \t\t\t\t Robust benchmarks are crucial for evaluating Multimodal Large Language Models (MLLMs). Yet we find that models can ace many multimodal benchmarks without strong visual understanding, instead exploiting biases, linguistic priors, and superficial patterns. This is especially problematic for vision-centric benchmarks that are meant to require visual inputs. We adopt a diagnostic principle for benchmark design: if a benchmark can be gamed, it will be. Designers should therefore try to ``game'' their own benchmarks first, using diagnostic and debiasing procedures to systematically identify and mitigate non-visual biases. Effective diagnosis requires directly ``training on the test set'' -- probing the released test set for its intrinsic, exploitable patterns.   We operationalize this standard with two components. First, we diagnose benchmark susceptibility using a ``Test-set Stress-Test'' (TsT) methodology. Our primary diagnostic tool involves fine-tuning a powerful Large Language Model via k-fold cross-validation on exclusively the non-visual, textual inputs of the test set to reveal shortcut performance and assign each sample a bias score s(x). We complement this with a lightweight Random Forest-based diagnostic operating on hand-crafted features for fast, interpretable auditing. Second, we debias benchmarks by filtering high-bias samples using an ``Iterative Bias Pruning'' (IBP) procedure. Applying this framework to four benchmarks -- VSI-Bench, CV-Bench, MMMU, and VideoMME -- we uncover pervasive non-visual biases. As a case study, we apply our full framework to create VSI-Bench-Debiased, demonstrating reduced non-visual solvability and a wider vision-blind performance gap than the original.",
            "score": 7,
            "issue_id": 1,
            "pub_date": "2025-11-06",
            "pub_date_card": {
                "ru": "6 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 6",
                "zh": "11æœˆ6æ—¥"
            },
            "hash": "2e166587761671a4",
            "authors": [
                "Ellis Brown",
                "Jihan Yang",
                "Shusheng Yang",
                "Rob Fergus",
                "Saining Xie"
            ],
            "affiliations": [],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.04655.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#ethics",
                    "#multimodal",
                    "#interpretability"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ”Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ° Ğ¸ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ¾Ñ‚ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚ĞµĞ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ Multimodal Large Language Models. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒÑÑ Ñ Ñ‚ĞµÑÑ‚Ğ°Ğ¼Ğ¸, Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑÑŒ Ğ½Ğ° Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ¾Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´: Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ° ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Fine-tuning ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ¸ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ±Ğ¸Ğ°ÑĞ½Ñ‹Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ². ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºĞ¸ Ğº Ñ‡ĞµÑ‚Ñ‹Ñ€Ñ‘Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ğ¼ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ½ĞµĞ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Diagnosing and Debiasing for Robust Multimodal Models",
                    "desc": "This paper presents a framework aimed at diagnosing and reducing non-visual biases in multimodal benchmarks for Multimodal Large Language Models (MLLMs). The authors highlight that many models can perform well on these benchmarks without truly understanding visual content, instead relying on biases and superficial patterns. To address this, they propose a diagnostic approach that includes a 'Test-set Stress-Test' methodology to identify exploitable patterns and a 'Iterative Bias Pruning' procedure to filter out high-bias samples. Their findings reveal significant non-visual biases across multiple benchmarks, leading to the creation of a debiased version that shows improved robustness in evaluating visual understanding."
                },
                "zh": {
                    "title": "æ­ç¤ºä¸å»åè§ï¼šæå‡å¤šæ¨¡æ€æ¨¡å‹çš„é²æ£’æ€§",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ¡†æ¶ï¼Œç”¨äºè¯Šæ–­å’Œå»åè§å¤šæ¨¡æ€åŸºå‡†ï¼Œä»¥æé«˜å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„é²æ£’æ€§ã€‚ç ”ç©¶å‘ç°ï¼Œæ¨¡å‹åœ¨è®¸å¤šå¤šæ¨¡æ€åŸºå‡†ä¸Šè¡¨ç°è‰¯å¥½ï¼Œå®é™…ä¸Šæ˜¯åˆ©ç”¨äº†éè§†è§‰åè§å’Œè¡¨é¢æ¨¡å¼ï¼Œè€Œä¸æ˜¯å¼ºå¤§çš„è§†è§‰ç†è§£ã€‚ä¸ºäº†è®¾è®¡æœ‰æ•ˆçš„åŸºå‡†ï¼Œä½œè€…å»ºè®®è®¾è®¡è€…é¦–å…ˆå°è¯•â€œæ¸¸æˆâ€è‡ªå·±çš„åŸºå‡†ï¼Œé€šè¿‡è¯Šæ–­å’Œå»åè§ç¨‹åºç³»ç»Ÿåœ°è¯†åˆ«å’Œå‡è½»éè§†è§‰åè§ã€‚é€šè¿‡å¯¹å››ä¸ªåŸºå‡†çš„åº”ç”¨ï¼Œç ”ç©¶æ­ç¤ºäº†æ™®éå­˜åœ¨çš„éè§†è§‰åè§ï¼Œå¹¶å±•ç¤ºäº†å»åè§åçš„åŸºå‡†åœ¨è§†è§‰ç›²æ€§èƒ½å·®è·ä¸Šçš„æ”¹å–„ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.27656",
            "title": "RDMA Point-to-Point Communication for LLM Systems",
            "url": "https://huggingface.co/papers/2510.27656",
            "abstract": "TransferEngine provides a uniform interface for flexible point-to-point communication in large language models, supporting disaggregated inference, reinforcement learning, and Mixture-of-Experts routing across different hardware.  \t\t\t\t\tAI-generated summary \t\t\t\t Emerging Large Language Model (LLM) system patterns, such as disaggregated inference, Mixture-of-Experts (MoE) routing, and asynchronous reinforcement fine-tuning, require flexible point-to-point communication beyond simple collectives. Existing implementations are locked to specific Network Interface Controllers (NICs), hindering integration into inference engines and portability across hardware providers. We present TransferEngine, which bridges the functionality of common NICs to expose a uniform interface. TransferEngine exposes one-sided WriteImm operations with a ImmCounter primitive for completion notification, without ordering assumptions of network transport, transparently managing multiple NICs per GPU. We demonstrate peak throughput of 400 Gbps on both NVIDIA ConnectX-7 and AWS Elastic Fabric Adapter (EFA). We showcase TransferEngine through three production systems: (1) KvCache transfer for disaggregated inference with dynamic scaling, (2) RL weight updates achieving 1.3 seconds for trillion-parameter models, and (3) MoE dispatch/combine implementation exceeding DeepEP decode latency on ConnectX-7, with the first viable latencies on EFA. We demonstrate that our portable point-to-point communication complements collectives while avoiding lock-in.",
            "score": 6,
            "issue_id": 1,
            "pub_date": "2025-10-31",
            "pub_date_card": {
                "ru": "31 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 31",
                "zh": "10æœˆ31æ—¥"
            },
            "hash": "fd30ddff99639d4d",
            "authors": [
                "Nandor Licker",
                "Kevin Hu",
                "Vladimir Zaytsev",
                "Lequn Chen"
            ],
            "affiliations": [
                "Perplexity AI"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2510.27656.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#optimization",
                    "#transfer_learning",
                    "#architecture",
                    "#training",
                    "#inference",
                    "#rl"
                ],
                "emoji": "ğŸŒ‰",
                "ru": {
                    "title": "ĞŸĞ¾Ñ€Ñ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ±ĞµĞ· Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸ Ğº Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "TransferEngine â€” ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ Ğ´Ğ»Ñ Ğ³Ğ¸Ğ±ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡ĞºĞ°-Ğº-Ñ‚Ğ¾Ñ‡ĞºĞµ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ğ¹ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ ÑĞµÑ‚ĞµĞ²Ñ‹Ğ¼ Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ„Ñ‘Ñ€ĞµĞ½Ñ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ¼ĞµÑĞµĞ¹ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ… Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ»ĞµÑ€Ğ¾Ğ². Ğ ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½Ğ¾Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ 400 Ğ“Ğ±Ğ¸Ñ‚/Ñ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ‚Ñ€Ñ‘Ñ… production-ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…: Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğµ KV-ĞºÑÑˆĞ°, Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ²ĞµÑĞ¾Ğ² Ğ² RL Ğ´Ğ»Ñ Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½-Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ MoE Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹. Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¸Ğ·Ğ±ĞµĞ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸ Ğº ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ ĞºĞ¾Ğ»Ğ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ°Ñ… Ğ´Ğ»Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Seamless Communication for Advanced AI Models",
                    "desc": "TransferEngine is a novel framework designed to enhance communication in large language models (LLMs) by providing a consistent interface for point-to-point communication. It supports advanced techniques like disaggregated inference, reinforcement learning, and Mixture-of-Experts (MoE) routing, which require more flexible communication than traditional methods. By bridging the functionality of various Network Interface Controllers (NICs), TransferEngine allows for seamless integration and portability across different hardware platforms. The framework achieves impressive performance, demonstrating peak throughput of 400 Gbps and significantly reducing latency in production systems for large-scale AI models."
                },
                "zh": {
                    "title": "TransferEngineï¼šçµæ´»çš„ç‚¹å¯¹ç‚¹é€šä¿¡è§£å†³æ–¹æ¡ˆ",
                    "desc": "TransferEngine æ˜¯ä¸€ä¸ªä¸ºå¤§å‹è¯­è¨€æ¨¡å‹æä¾›ç»Ÿä¸€æ¥å£çš„å·¥å…·ï¼Œæ”¯æŒçµæ´»çš„ç‚¹å¯¹ç‚¹é€šä¿¡ã€‚å®ƒèƒ½å¤Ÿå¤„ç†åˆ†æ•£æ¨ç†ã€å¼ºåŒ–å­¦ä¹ å’Œä¸“å®¶æ··åˆè·¯ç”±ç­‰å¤æ‚ä»»åŠ¡ï¼Œè¶…è¶Šäº†ç®€å•çš„é›†ä½“é€šä¿¡ã€‚ç°æœ‰çš„å®ç°é€šå¸¸ä¾èµ–ç‰¹å®šçš„ç½‘ç»œæ¥å£æ§åˆ¶å™¨ï¼Œé™åˆ¶äº†ä¸æ¨ç†å¼•æ“çš„é›†æˆå’Œè·¨ç¡¬ä»¶çš„å¯ç§»æ¤æ€§ã€‚TransferEngine é€šè¿‡ç®¡ç†å¤šä¸ªç½‘ç»œæ¥å£ï¼Œæä¾›é«˜è¾¾ 400 Gbps çš„å³°å€¼ååé‡ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…ç³»ç»Ÿä¸­çš„åº”ç”¨æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.04668",
            "title": "SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding",
            "url": "https://huggingface.co/papers/2511.04668",
            "abstract": "A data-generation framework using 3D simulators improves spatial reasoning in multimodal language models with efficient training on simulated data.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite impressive high-level video comprehension, multimodal language models struggle with spatial reasoning across time and space. While current spatial training approaches rely on real-world video data, obtaining diverse footage with precise spatial annotations remains a bottleneck. To alleviate this bottleneck, we present SIMS-V -- a systematic data-generation framework that leverages the privileged information of 3D simulators to create spatially-rich video training data for multimodal language models. Using this framework, we investigate which properties of simulated data drive effective real-world transfer through systematic ablations of question types, mixes, and scales. We identify a minimal set of three question categories (metric measurement, perspective-dependent reasoning, and temporal tracking) that prove most effective for developing transferable spatial intelligence, outperforming comprehensive coverage despite using fewer question types. These insights enable highly efficient training: our 7B-parameter video LLM fine-tuned on just 25K simulated examples outperforms the larger 72B baseline and achieves competitive performance with proprietary models on rigorous real-world spatial reasoning benchmarks. Our approach demonstrates robust generalization, maintaining performance on general video understanding while showing substantial improvements on embodied and real-world spatial tasks.",
            "score": 4,
            "issue_id": 1,
            "pub_date": "2025-11-06",
            "pub_date_card": {
                "ru": "6 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 6",
                "zh": "11æœˆ6æ—¥"
            },
            "hash": "b6084aefb5cf622d",
            "authors": [
                "Ellis Brown",
                "Arijit Ray",
                "Ranjay Krishna",
                "Ross Girshick",
                "Rob Fergus",
                "Saining Xie"
            ],
            "affiliations": [
                "AllenAI",
                "Boston University",
                "New York University",
                "Vercept"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.04668.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#small_models",
                    "#3d",
                    "#video",
                    "#transfer_learning",
                    "#data",
                    "#multimodal",
                    "#training",
                    "#dataset",
                    "#reasoning",
                    "#synthetic"
                ],
                "emoji": "ğŸ®",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-LLM",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° SIMS-V â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 3D-ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ² multimodal language models. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚, ĞºĞ°ĞºĞ¸Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµĞ½Ğ¾Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼Ğ¸Ñ€, Ğ¸ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ· Ñ‚Ñ€Ñ‘Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² (Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹, Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹), Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. ĞĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ 7-Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ½Ğ°Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-LLM, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ²ÑĞµĞ³Ğ¾ Ğ½Ğ° 25 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ñ… ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ 72-Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²ÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ½Ğ° Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ."
                },
                "en": {
                    "title": "Enhancing Spatial Reasoning with Simulated Data",
                    "desc": "This paper introduces SIMS-V, a data-generation framework that uses 3D simulators to create training data for multimodal language models, specifically targeting spatial reasoning. Traditional methods rely on real-world video data, which is often limited by the availability of diverse footage and accurate spatial annotations. The authors identify three key question categories that enhance the model's ability to transfer learned spatial reasoning skills to real-world scenarios. Their findings show that a smaller dataset of simulated examples can outperform larger models, demonstrating efficient training and strong generalization capabilities in spatial tasks."
                },
                "zh": {
                    "title": "åˆ©ç”¨3Dæ¨¡æ‹Ÿå™¨æå‡å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºSIMS-Vçš„æ•°æ®ç”Ÿæˆæ¡†æ¶ï¼Œåˆ©ç”¨3Dæ¨¡æ‹Ÿå™¨ç”Ÿæˆä¸°å¯Œçš„ç©ºé—´è§†é¢‘è®­ç»ƒæ•°æ®ï¼Œä»¥æé«˜å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚å½“å‰çš„ç©ºé—´è®­ç»ƒæ–¹æ³•ä¾èµ–äºçœŸå®è§†é¢‘æ•°æ®ï¼Œä½†è·å–å¤šæ ·åŒ–ä¸”ç²¾ç¡®æ ‡æ³¨çš„ç´ æå­˜åœ¨ç“¶é¢ˆã€‚é€šè¿‡ç³»ç»Ÿæ€§åœ°åˆ†æä¸åŒé—®é¢˜ç±»å‹å’Œç»„åˆï¼Œç ”ç©¶å‘ç°ä¸‰ç§é—®é¢˜ç±»åˆ«ï¼ˆåº¦é‡æµ‹é‡ã€è§†è§’ä¾èµ–æ¨ç†å’Œæ—¶é—´è·Ÿè¸ªï¼‰å¯¹æå‡å¯è½¬ç§»çš„ç©ºé—´æ™ºèƒ½æœ€ä¸ºæœ‰æ•ˆã€‚æœ€ç»ˆï¼Œç»è¿‡25Kä¸ªæ¨¡æ‹Ÿç¤ºä¾‹çš„å¾®è°ƒï¼Œæˆ‘ä»¬çš„7Bå‚æ•°è§†é¢‘è¯­è¨€æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œç©ºé—´æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äº72BåŸºçº¿æ¨¡å‹ï¼Œæ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.00956",
            "title": "EVTAR: End-to-End Try on with Additional Unpaired Visual Reference",
            "url": "https://huggingface.co/papers/2511.00956",
            "abstract": "EVTAR is an end-to-end virtual try-on model that enhances accuracy by using reference images, simplifying the inference process and improving garment texture and detail preservation.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose EVTAR, an End-to-End Virtual Try-on model with Additional Reference, that directly fits the target garment onto the person image while incorporating reference images to enhance try-on accuracy. Most existing virtual try-on approaches rely on complex inputs such as agnostic person images, human pose, densepose, or body keypoints, making them labor-intensive and impractical for real-world applications. In contrast, EVTAR adopts a two-stage training strategy, enabling simple inference with only the source image and the target garment inputs. Our model generates try-on results without masks, densepose, or segmentation maps. Moreover, EVTAR leverages additional reference images of different individuals wearing the same clothes to preserve garment texture and fine-grained details better. This mechanism is analogous to how humans consider reference models when choosing outfits, thereby simulating a more realistic and high-quality dressing effect. We enrich the training data with supplementary references and unpaired person images to support these capabilities. We evaluate EVTAR on two widely used benchmarks and diverse tasks, and the results consistently validate the effectiveness of our approach.",
            "score": 4,
            "issue_id": 1,
            "pub_date": "2025-11-02",
            "pub_date_card": {
                "ru": "2 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 2",
                "zh": "11æœˆ2æ—¥"
            },
            "hash": "4c04c63e861c45a6",
            "authors": [
                "Liuzhuozheng Li",
                "Yue Gong",
                "Shanyuan Liu",
                "Bo Cheng",
                "Yuhang Ma",
                "Liebucha Wu",
                "Dengyang Jiang",
                "Zanyi Wang",
                "Dawei Leng",
                "Yuhui Yin"
            ],
            "affiliations": [
                "360 AI Research",
                "Hong Kong University of Science and Technology",
                "The University of Tokyo",
                "University of California San Diego"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.00956.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#cv"
                ],
                "emoji": "ğŸ‘—",
                "ru": {
                    "title": "ĞŸÑ€Ğ¸Ğ¼ĞµÑ€ĞºĞ° Ğ±ĞµĞ· ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹: ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ñ‹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ĞºÑƒ",
                    "desc": "EVTAR â€” ÑÑ‚Ğ¾ ÑĞºĞ²Ğ¾Ğ·Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ĞºĞ¸ Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ¿Ñ€Ğ¾Ñ‰Ğ°ĞµÑ‚ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ñ‚Ñ€ĞµĞ±ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚ Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ±ĞµĞ· Ğ¼Ğ°ÑĞ¾Ğº, densepose Ğ¸ ĞºĞ°Ñ€Ñ‚ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞµÑ‘ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ â€” Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ»ÑĞ´ĞµĞ¹ Ğ² Ñ‚Ğ¾Ğ¹ Ğ¶Ğµ Ğ¾Ğ´ĞµĞ¶Ğ´Ğµ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ñ‹ Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ± Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ½Ğ°Ñ€ÑĞ´Ğ¾Ğ² Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ…."
                },
                "en": {
                    "title": "EVTAR: Simplifying Virtual Try-Ons with Reference Images",
                    "desc": "EVTAR is a novel virtual try-on model that improves the accuracy of garment fitting by using reference images. Unlike traditional methods that require complex inputs like body keypoints or segmentation maps, EVTAR simplifies the process by only needing the source image and target garment. It employs a two-stage training strategy to enhance the quality of the try-on results, ensuring better preservation of garment textures and details. By mimicking how humans use reference models for outfit selection, EVTAR achieves a more realistic dressing effect in its outputs."
                },
                "zh": {
                    "title": "EVTARï¼šç®€åŒ–è™šæ‹Ÿè¯•è¡£çš„é«˜æ•ˆæ¨¡å‹",
                    "desc": "EVTARæ˜¯ä¸€ç§ç«¯åˆ°ç«¯çš„è™šæ‹Ÿè¯•è¡£æ¨¡å‹ï¼Œé€šè¿‡ä½¿ç”¨å‚è€ƒå›¾åƒæ¥æé«˜å‡†ç¡®æ€§ï¼Œç®€åŒ–æ¨ç†è¿‡ç¨‹ï¼Œå¹¶æ”¹å–„æœè£…çº¹ç†å’Œç»†èŠ‚çš„ä¿ç•™ã€‚ä¸ç°æœ‰çš„è™šæ‹Ÿè¯•è¡£æ–¹æ³•ä¸åŒï¼ŒEVTARåªéœ€æºå›¾åƒå’Œç›®æ ‡æœè£…è¾“å…¥ï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œé¿å…äº†å¤æ‚çš„è¾“å…¥è¦æ±‚ã€‚è¯¥æ¨¡å‹æ— éœ€ä½¿ç”¨é®ç½©ã€å¯†é›†å§¿æ€æˆ–åˆ†å‰²å›¾ï¼Œç›´æ¥å°†ç›®æ ‡æœè£…é€‚é…åˆ°äººç‰©å›¾åƒä¸Šã€‚é€šè¿‡åˆ©ç”¨ä¸åŒä¸ªä½“ç©¿ç€ç›¸åŒæœè£…çš„å‚è€ƒå›¾åƒï¼ŒEVTARèƒ½å¤Ÿæ›´å¥½åœ°ä¿ç•™æœè£…çš„çº¹ç†å’Œç»†èŠ‚ï¼Œæ¨¡æ‹Ÿæ›´çœŸå®çš„ç©¿è¡£æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.03996",
            "title": "Learning Vision-Driven Reactive Soccer Skills for Humanoid Robots",
            "url": "https://huggingface.co/papers/2511.03996",
            "abstract": "A unified reinforcement learning controller integrates visual perception and motion control for humanoid robots in soccer, using Adversarial Motion Priors and an encoder-decoder architecture to achieve reactive and coherent behaviors.  \t\t\t\t\tAI-generated summary \t\t\t\t Humanoid soccer poses a representative challenge for embodied intelligence, requiring robots to operate within a tightly coupled perception-action loop. However, existing systems typically rely on decoupled modules, resulting in delayed responses and incoherent behaviors in dynamic environments, while real-world perceptual limitations further exacerbate these issues. In this work, we present a unified reinforcement learning-based controller that enables humanoid robots to acquire reactive soccer skills through the direct integration of visual perception and motion control. Our approach extends Adversarial Motion Priors to perceptual settings in real-world dynamic environments, bridging motion imitation and visually grounded dynamic control. We introduce an encoder-decoder architecture combined with a virtual perception system that models real-world visual characteristics, allowing the policy to recover privileged states from imperfect observations and establish active coordination between perception and action. The resulting controller demonstrates strong reactivity, consistently executing coherent and robust soccer behaviors across various scenarios, including real RoboCup matches.",
            "score": 3,
            "issue_id": 1,
            "pub_date": "2025-11-06",
            "pub_date_card": {
                "ru": "6 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 6",
                "zh": "11æœˆ6æ—¥"
            },
            "hash": "f9d9b36823f3fe2e",
            "authors": [
                "Yushi Wang",
                "Changsheng Luo",
                "Penghui Chen",
                "Jianran Liu",
                "Weijian Sun",
                "Tong Guo",
                "Kechang Yang",
                "Biao Hu",
                "Yangang Zhang",
                "Mingguo Zhao"
            ],
            "affiliations": [
                "ByteDance Seed",
                "China Agricultural University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.03996.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#optimization",
                    "#architecture",
                    "#cv",
                    "#multimodal",
                    "#robotics",
                    "#rl"
                ],
                "emoji": "âš½",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ´Ğ»Ñ Ñ„ÑƒÑ‚Ğ±Ğ¾Ğ»Ğ° Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ»ĞµÑ€ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ³ÑƒĞ¼Ğ°Ğ½Ğ¾Ğ¸Ğ´Ğ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ², Ğ¸Ğ³Ñ€Ğ°ÑÑ‰Ğ¸Ñ… Ğ² Ñ„ÑƒÑ‚Ğ±Ğ¾Ğ». ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑˆĞ¸Ñ€Ğ¸Ğ»Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Adversarial Motion Priors Ğ½Ğ° ÑĞ»ÑƒÑ‡Ğ°Ğ¹ Ğ¿ĞµÑ€Ñ†ĞµĞ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½Ğ¸Ğ² Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° encoder-decoder Ñ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ¾Ğ¹ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞµ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¸Ğ· Ğ½ĞµÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ»ĞµÑ€ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ€ĞµĞ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ„ÑƒÑ‚Ğ±Ğ¾Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚Ñ‡Ğ°Ñ… RoboCup."
                },
                "en": {
                    "title": "Unified Learning for Reactive Humanoid Soccer Robots",
                    "desc": "This paper presents a unified reinforcement learning controller designed for humanoid robots playing soccer, integrating visual perception with motion control. The approach utilizes Adversarial Motion Priors and an encoder-decoder architecture to enhance the robots' ability to react and behave coherently in dynamic environments. By bridging the gap between motion imitation and visually grounded control, the system allows robots to effectively respond to real-world visual inputs. The results show that this controller can perform robustly in various scenarios, including actual RoboCup matches, demonstrating improved reactivity and coordination between perception and action."
                },
                "zh": {
                    "title": "ç»Ÿä¸€å¼ºåŒ–å­¦ä¹ æ§åˆ¶å™¨ï¼šæå‡äººå½¢æœºå™¨äººè¶³çƒæŠ€èƒ½çš„å…³é”®",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å¼ºåŒ–å­¦ä¹ æ§åˆ¶å™¨ï¼Œæ—¨åœ¨å°†è§†è§‰æ„ŸçŸ¥ä¸è¿åŠ¨æ§åˆ¶æ•´åˆï¼Œä»¥æé«˜äººå½¢æœºå™¨äººåœ¨è¶³çƒæ¯”èµ›ä¸­çš„è¡¨ç°ã€‚é€šè¿‡ä½¿ç”¨å¯¹æŠ—è¿åŠ¨å…ˆéªŒå’Œç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œè¯¥æ§åˆ¶å™¨èƒ½å¤Ÿåœ¨åŠ¨æ€ç¯å¢ƒä¸­å®ç°å¿«é€Ÿååº”å’Œä¸€è‡´çš„è¡Œä¸ºã€‚æˆ‘ä»¬çš„æ–¹æ³•å…‹æœäº†ä¼ ç»Ÿç³»ç»Ÿä¸­æ¨¡å—è§£è€¦å¯¼è‡´çš„å»¶è¿Ÿå“åº”é—®é¢˜ï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿåœ¨çœŸå®ä¸–ç•Œä¸­æ›´å¥½åœ°é€‚åº”å¤æ‚çš„è§†è§‰ä¿¡æ¯ã€‚æœ€ç»ˆï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ§åˆ¶å™¨åœ¨å„ç§åœºæ™¯ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„ååº”èƒ½åŠ›ï¼Œèƒ½å¤Ÿç¨³å®šåœ°æ‰§è¡Œè¶³çƒç›¸å…³çš„å¤æ‚åŠ¨ä½œã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.03295",
            "title": "How to Evaluate Speech Translation with Source-Aware Neural MT Metrics",
            "url": "https://huggingface.co/papers/2511.03295",
            "abstract": "Source-aware metrics using ASR transcripts and back-translations improve speech-to-text evaluation by addressing alignment issues and incorporating source information.  \t\t\t\t\tAI-generated summary \t\t\t\t Automatic evaluation of speech-to-text translation (ST) systems is typically performed by comparing translation hypotheses with one or more reference translations. While effective to some extent, this approach inherits the limitation of reference-based evaluation that ignores valuable information from the source input. In machine translation (MT), recent progress has shown that neural metrics incorporating the source text achieve stronger correlation with human judgments. Extending this idea to ST, however, is not trivial because the source is audio rather than text, and reliable transcripts or alignments between source and references are often unavailable. In this work, we conduct the first systematic study of source-aware metrics for ST, with a particular focus on real-world operating conditions where source transcripts are not available. We explore two complementary strategies for generating textual proxies of the input audio, automatic speech recognition (ASR) transcripts, and back-translations of the reference translation, and introduce a novel two-step cross-lingual re-segmentation algorithm to address the alignment mismatch between synthetic sources and reference translations. Our experiments, carried out on two ST benchmarks covering 79 language pairs and six ST systems with diverse architectures and performance levels, show that ASR transcripts constitute a more reliable synthetic source than back-translations when word error rate is below 20%, while back-translations always represent a computationally cheaper but still effective alternative. Furthermore, our cross-lingual re-segmentation algorithm enables robust use of source-aware MT metrics in ST evaluation, paving the way toward more accurate and principled evaluation methodologies for speech translation.",
            "score": 3,
            "issue_id": 1,
            "pub_date": "2025-11-05",
            "pub_date_card": {
                "ru": "5 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 5",
                "zh": "11æœˆ5æ—¥"
            },
            "hash": "93fe21aa583cbc7e",
            "authors": [
                "Mauro Cettolo",
                "Marco Gaido",
                "Matteo Negri",
                "Sara Papi",
                "Luisa Bentivogli"
            ],
            "affiliations": [
                "Fondazione Bruno Kessler"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.03295.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#audio",
                    "#multilingual",
                    "#translation",
                    "#science"
                ],
                "emoji": "ğŸ™ï¸",
                "ru": {
                    "title": "ĞÑ†ĞµĞ½ĞºĞ° speech-to-text Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ°ÑƒĞ´Ğ¸Ğ¾ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ° Ñ‡ĞµÑ€ĞµĞ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾ĞºÑĞ¸",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¸ÑÑ‚ĞµĞ¼ speech-to-text Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ Ğ°ÑƒĞ´Ğ¸Ğ¾ÑĞ¸Ğ³Ğ½Ğ°Ğ» Ñ‡ĞµÑ€ĞµĞ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾ĞºÑĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ Ğ´Ğ²Ğ° Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°: Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ¾Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ¾Ğ² ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ¾Ğ². Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµĞ²Ñ‹Ñ€Ğ°Ğ²Ğ½ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ¸ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ°Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ ĞºÑ€Ğ¾ÑÑ-Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµÑƒĞ¿Ğ¾Ñ€ÑĞ´Ğ¾Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° 79 ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ñ‹ ASR Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹, Ñ‡ĞµĞ¼ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ñ‹Ğµ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ñ‹, ĞºĞ¾Ğ³Ğ´Ğ° ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº ÑĞ»Ğ¾Ğ² Ğ½Ğ¸Ğ¶Ğµ 20%."
                },
                "en": {
                    "title": "Enhancing Speech-to-Text Evaluation with Source-Aware Metrics",
                    "desc": "This paper presents a novel approach to improve the evaluation of speech-to-text (ST) systems by using source-aware metrics that incorporate information from the original audio input. The authors propose generating textual proxies from audio using automatic speech recognition (ASR) transcripts and back-translations, addressing the challenge of alignment between these synthetic sources and reference translations. They introduce a two-step cross-lingual re-segmentation algorithm to enhance the evaluation process, particularly in scenarios where reliable transcripts are unavailable. Experimental results demonstrate that ASR transcripts provide a more reliable source than back-translations under certain conditions, leading to more accurate evaluations of ST systems."
                },
                "zh": {
                    "title": "æºæ„ŸçŸ¥æŒ‡æ ‡æå‡è¯­éŸ³è½¬æ–‡æœ¬è¯„ä¼°å‡†ç¡®æ€§",
                    "desc": "æœ¬æ–‡ç ”ç©¶äº†æºæ„ŸçŸ¥æŒ‡æ ‡åœ¨è¯­éŸ³è½¬æ–‡æœ¬ï¼ˆSTï¼‰è¯„ä¼°ä¸­çš„åº”ç”¨ï¼Œæ—¨åœ¨è§£å†³å¯¹é½é—®é¢˜å¹¶æ•´åˆæºä¿¡æ¯ã€‚ä¼ ç»Ÿçš„è¯„ä¼°æ–¹æ³•ä¸»è¦ä¾èµ–äºå‚è€ƒç¿»è¯‘ï¼Œå¿½è§†äº†æºè¾“å…¥ä¸­çš„é‡è¦ä¿¡æ¯ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ç§ç”Ÿæˆæ–‡æœ¬ä»£ç†çš„æ–¹æ³•ï¼šè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰è½¬å½•å’Œå‚è€ƒç¿»è¯‘çš„å›è¯‘ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°çš„è·¨è¯­è¨€é‡æ–°åˆ†æ®µç®—æ³•æ¥è§£å†³åˆæˆæºä¸å‚è€ƒç¿»è¯‘ä¹‹é—´çš„å¯¹é½ä¸åŒ¹é…ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå½“å­—é”™è¯¯ç‡ä½äº20%æ—¶ï¼ŒASRè½¬å½•æ¯”å›è¯‘æ›´å¯é ï¼Œè€Œå›è¯‘åˆ™æ˜¯è®¡ç®—ä¸Šæ›´ä¾¿å®œä½†ä»æœ‰æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.02280",
            "title": "SAIL-RL: Guiding MLLMs in When and How to Think via Dual-Reward RL Tuning",
            "url": "https://huggingface.co/papers/2511.02280",
            "abstract": "SAIL-RL enhances multimodal large language models' reasoning capabilities using a dual reward system that improves factual grounding, logical coherence, and adaptability.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce SAIL-RL, a reinforcement learning (RL) post-training framework that enhances the reasoning capabilities of multimodal large language models (MLLMs) by teaching them when and how to think. Existing approaches are limited by outcome-only supervision, which rewards correct answers without ensuring sound reasoning, and by uniform thinking strategies, which often lead to overthinking on simple tasks and underthinking on complex ones. SAIL-RL addresses these challenges with a dual reward system: the Thinking Reward, which evaluates reasoning quality through factual grounding, logical coherence, and answer consistency, and the Judging Reward, which adaptively determines whether deep reasoning or direct answering is appropriate. Experiments on the state-of-the-art SAIL-VL2 show that SAIL-RL improves reasoning and multimodal understanding benchmarks at both 4B and 8B scales, achieving competitive performance against commercial closed-source models such as GPT-4o, and substantially reduces hallucinations, establishing it as a principled framework for building more reliable and adaptive MLLMs. The code will be available at https://github.com/BytedanceDouyinContent/SAIL-RL.",
            "score": 3,
            "issue_id": 1,
            "pub_date": "2025-11-04",
            "pub_date_card": {
                "ru": "4 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 4",
                "zh": "11æœˆ4æ—¥"
            },
            "hash": "388cff3b05f165a8",
            "authors": [
                "Fangxun Shu",
                "Yongjie Ye",
                "Yue Liao",
                "Zijian Kang",
                "Weijie Yin",
                "Jiacong Wang",
                "Xiao Liang",
                "Shuicheng Yan",
                "Chao Feng"
            ],
            "affiliations": [
                "Douyin SAIL Team",
                "National University of Singapore"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.02280.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#alignment",
                    "#multimodal",
                    "#training",
                    "#rlhf",
                    "#hallucinations",
                    "#rl",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ°ÑƒÑ‡Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´ÑƒĞ¼Ğ°Ñ‚ÑŒ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾: Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ°Ñ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ° Ğ´Ğ»Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹",
                    "desc": "SAIL-RL â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒÑ‡Ğ¸Ñ‚ Ğ¸Ñ… Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğ¾Ğ¹Ğ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹: Thinking Reward Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ‡ĞµÑ€ĞµĞ· Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ° Judging Reward Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚, Ğ½ÑƒĞ¶Ğ½Ñ‹ Ğ»Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğµ Ñ€Ğ°Ğ·Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡ĞµĞ½ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´Ğ°ĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ±ĞµĞ· Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ğ¸ Ğ·Ğ²ÑƒĞºĞ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ ĞºĞ¾ Ğ²ÑĞµĞ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ‚Ğ¸Ğ¿Ğ° GPT-4o."
                },
                "en": {
                    "title": "Enhancing Reasoning in MLLMs with Dual Rewards",
                    "desc": "SAIL-RL is a reinforcement learning framework designed to improve the reasoning abilities of multimodal large language models (MLLMs). It introduces a dual reward system that includes a Thinking Reward for assessing the quality of reasoning and a Judging Reward that helps the model decide when to use deep reasoning versus direct answers. This approach addresses limitations of previous methods that relied solely on correct outcomes and uniform thinking strategies, which could lead to inconsistent reasoning. Experiments show that SAIL-RL enhances performance on reasoning and multimodal tasks while reducing errors known as hallucinations, making MLLMs more reliable and adaptable."
                },
                "zh": {
                    "title": "SAIL-RLï¼šæå‡å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„åŒé‡å¥–åŠ±ç³»ç»Ÿ",
                    "desc": "SAIL-RLæ˜¯ä¸€ç§å¢å¼ºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„å¼ºåŒ–å­¦ä¹ åè®­ç»ƒæ¡†æ¶ã€‚å®ƒé€šè¿‡åŒé‡å¥–åŠ±ç³»ç»Ÿæ¥è§£å†³ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ï¼Œç¡®ä¿æ¨¡å‹ä¸ä»…èƒ½ç»™å‡ºæ­£ç¡®ç­”æ¡ˆï¼Œè¿˜èƒ½è¿›è¡Œåˆç†æ¨ç†ã€‚è¯¥ç³»ç»ŸåŒ…æ‹¬æ€ç»´å¥–åŠ±å’Œåˆ¤æ–­å¥–åŠ±ï¼Œå‰è€…è¯„ä¼°æ¨ç†è´¨é‡ï¼Œåè€…æ ¹æ®ä»»åŠ¡å¤æ‚æ€§å†³å®šæ˜¯æ·±åº¦æ¨ç†è¿˜æ˜¯ç›´æ¥å›ç­”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSAIL-RLåœ¨æ¨ç†å’Œå¤šæ¨¡æ€ç†è§£åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—å‡å°‘äº†å¹»è§‰ç°è±¡ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-11-06.html",
    "link_next": "2025-11-10.html",
    "link_month": "2025-11.html",
    "short_date_prev": {
        "ru": "06.11",
        "en": "11/06",
        "zh": "11æœˆ6æ—¥"
    },
    "short_date_next": {
        "ru": "10.11",
        "en": "11/10",
        "zh": "11æœˆ10æ—¥"
    },
    "categories": {
        "#dataset": 7,
        "#data": 1,
        "#benchmark": 9,
        "#agents": 2,
        "#cv": 2,
        "#rl": 5,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 1,
        "#audio": 1,
        "#video": 3,
        "#multimodal": 10,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 4,
        "#healthcare": 0,
        "#training": 8,
        "#robotics": 1,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 1,
        "#reasoning": 5,
        "#transfer_learning": 3,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 1,
        "#optimization": 4,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 2,
        "#synthetic": 4,
        "#machine_translation": 0,
        "#leakage": 1,
        "#open_source": 4,
        "#small_models": 2,
        "#science": 1,
        "#low_resource": 0,
        "#translation": 1
    }
}