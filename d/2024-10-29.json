{
    "date": {
        "ru": "29 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
        "en": "October 29",
        "zh": "10æœˆ29æ—¥"
    },
    "time_utc": "2024-10-29 02:48",
    "weekday": 1,
    "issue_id": 321,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2410.21252",
            "title": "LongReward: Improving Long-context Large Language Models with AI Feedback",
            "url": "https://huggingface.co/papers/2410.21252",
            "abstract": "Though significant advancements have been achieved in developing long-context large language models (LLMs), the compromised quality of LLM-synthesized data for supervised fine-tuning (SFT) often affects the long-context performance of SFT models and leads to inherent limitations. In principle, reinforcement learning (RL) with appropriate reward signals can further enhance models' capacities. However, how to obtain reliable rewards in long-context scenarios remains unexplored. To this end, we propose LongReward, a novel method that utilizes an off-the-shelf LLM to provide rewards for long-context model responses from four human-valued dimensions: helpfulness, logicality, faithfulness, and completeness, each with a carefully designed assessment pipeline. By combining LongReward and offline RL algorithm DPO, we are able to effectively improve long-context SFT models. Our experiments indicate that LongReward not only significantly improves models' long-context performance but also enhances their ability to follow short instructions. We also find that long-context DPO with LongReward and conventional short-context DPO can be used together without hurting either one's performance.",
            "score": 5,
            "issue_id": 321,
            "pub_date": "2024-10-28",
            "pub_date_card": {
                "ru": "28 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 28",
                "zh": "10æœˆ28æ—¥"
            },
            "hash": "0ff5d39896cdfbbe",
            "data": {
                "categories": [
                    "#rl",
                    "#rlhf",
                    "#long_context",
                    "#training"
                ],
                "emoji": "ğŸ“",
                "ru": {
                    "title": "LongReward: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ LongReward Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ğ¾Ñ‚Ğ¾Ğ²ÑƒÑ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¿Ğ¾ Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµĞ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼: Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ğ°. LongReward Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ²Ğ¼ĞµÑÑ‚Ğµ Ñ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ DPO Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¸ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Long-Context Performance with LongReward",
                    "desc": "This paper addresses the challenges faced by long-context large language models (LLMs) in generating high-quality data for supervised fine-tuning (SFT). It introduces LongReward, a method that leverages an existing LLM to provide rewards based on four key dimensions: helpfulness, logicality, faithfulness, and completeness. By integrating LongReward with the offline reinforcement learning algorithm DPO, the authors demonstrate significant improvements in the long-context performance of SFT models. The findings suggest that LongReward enhances both long-context and short instruction-following capabilities without compromising performance across different contexts."
                },
                "zh": {
                    "title": "æå‡é•¿ä¸Šä¸‹æ–‡æ¨¡å‹æ€§èƒ½çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºLongRewardçš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜é•¿ä¸Šä¸‹æ–‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ€§èƒ½ã€‚é€šè¿‡åˆ©ç”¨ç°æˆçš„LLMï¼Œä»å››ä¸ªç»´åº¦ï¼ˆæœ‰ç”¨æ€§ã€é€»è¾‘æ€§ã€å¯ä¿¡æ€§å’Œå®Œæ•´æ€§ï¼‰ä¸ºé•¿ä¸Šä¸‹æ–‡æ¨¡å‹çš„å“åº”æä¾›å¥–åŠ±ä¿¡å·ã€‚ç»“åˆLongRewardå’Œç¦»çº¿å¼ºåŒ–å­¦ä¹ ç®—æ³•DPOï¼Œæˆ‘ä»¬èƒ½å¤Ÿæœ‰æ•ˆæå‡é•¿ä¸Šä¸‹æ–‡çš„ç›‘ç£å¾®è°ƒæ¨¡å‹çš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLongRewardä¸ä»…æ˜¾è‘—æ”¹å–„äº†æ¨¡å‹çš„é•¿ä¸Šä¸‹æ–‡æ€§èƒ½ï¼Œè¿˜å¢å¼ºäº†å…¶æ‰§è¡ŒçŸ­æŒ‡ä»¤çš„èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.21220",
            "title": "Vision Search Assistant: Empower Vision-Language Models as Multimodal Search Engines",
            "url": "https://huggingface.co/papers/2410.21220",
            "abstract": "Search engines enable the retrieval of unknown information with texts. However, traditional methods fall short when it comes to understanding unfamiliar visual content, such as identifying an object that the model has never seen before. This challenge is particularly pronounced for large vision-language models (VLMs): if the model has not been exposed to the object depicted in an image, it struggles to generate reliable answers to the user's question regarding that image. Moreover, as new objects and events continuously emerge, frequently updating VLMs is impractical due to heavy computational burdens. To address this limitation, we propose Vision Search Assistant, a novel framework that facilitates collaboration between VLMs and web agents. This approach leverages VLMs' visual understanding capabilities and web agents' real-time information access to perform open-world Retrieval-Augmented Generation via the web. By integrating visual and textual representations through this collaboration, the model can provide informed responses even when the image is novel to the system. Extensive experiments conducted on both open-set and closed-set QA benchmarks demonstrate that the Vision Search Assistant significantly outperforms the other models and can be widely applied to existing VLMs.",
            "score": 1,
            "issue_id": 321,
            "pub_date": "2024-10-28",
            "pub_date_card": {
                "ru": "28 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 28",
                "zh": "10æœˆ28æ—¥"
            },
            "hash": "bdb8b2a5fbb4c663",
            "data": {
                "categories": [
                    "#rag",
                    "#agents",
                    "#cv",
                    "#benchmark"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ—Ñ€ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜: Ğ¾Ñ‚ Ğ½ĞµĞ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Vision Search Assistant - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞĞ½ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM) Ğ¸ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾ Ğ½ĞµĞ·Ğ½Ğ°ĞºĞ¾Ğ¼Ñ‹Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ· Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğ° (Retrieval-Augmented Generation), Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ°ĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Empowering Vision-Language Models with Real-Time Web Collaboration",
                    "desc": "This paper introduces the Vision Search Assistant, a new framework that enhances the capabilities of vision-language models (VLMs) by enabling them to collaborate with web agents. Traditional VLMs struggle with unfamiliar visual content, especially when they encounter objects they have never seen before, leading to unreliable responses. The proposed framework allows VLMs to access real-time information from the web, facilitating open-world Retrieval-Augmented Generation. Experimental results show that the Vision Search Assistant significantly improves performance on both open-set and closed-set question-answering tasks, making it a valuable addition to existing VLMs."
                },
                "zh": {
                    "title": "è§†è§‰æœç´¢åŠ©æ‰‹ï¼šæ‰“ç ´æœªçŸ¥è§†è§‰å†…å®¹çš„å£å’",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œç§°ä¸ºè§†è§‰æœç´¢åŠ©æ‰‹ï¼ˆVision Search Assistantï¼‰ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤„ç†æœªçŸ¥è§†è§‰å†…å®¹æ—¶çš„å±€é™æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡ç»“åˆVLMsçš„è§†è§‰ç†è§£èƒ½åŠ›å’Œç½‘ç»œä»£ç†çš„å®æ—¶ä¿¡æ¯è®¿é—®ï¼Œå®ç°äº†å¼€æ”¾ä¸–ç•Œçš„æ£€ç´¢å¢å¼ºç”Ÿæˆã€‚è¿™æ ·ï¼Œå³ä½¿æ¨¡å‹ä»æœªè§è¿‡æŸä¸ªå›¾åƒä¸­çš„å¯¹è±¡ï¼Œä¹Ÿèƒ½æä¾›å‡†ç¡®çš„å›ç­”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè§†è§‰æœç´¢åŠ©æ‰‹åœ¨å¼€æ”¾é›†å’Œå°é—­é›†çš„é—®ç­”åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.20011",
            "title": "A Survey of Small Language Models",
            "url": "https://huggingface.co/papers/2410.20011",
            "abstract": "Small Language Models (SLMs) have become increasingly important due to their efficiency and performance to perform various language tasks with minimal computational resources, making them ideal for various settings including on-device, mobile, edge devices, among many others. In this article, we present a comprehensive survey on SLMs, focusing on their architectures, training techniques, and model compression techniques. We propose a novel taxonomy for categorizing the methods used to optimize SLMs, including model compression, pruning, and quantization techniques. We summarize the benchmark datasets that are useful for benchmarking SLMs along with the evaluation metrics commonly used. Additionally, we highlight key open challenges that remain to be addressed. Our survey aims to serve as a valuable resource for researchers and practitioners interested in developing and deploying small yet efficient language models.",
            "score": 1,
            "issue_id": 321,
            "pub_date": "2024-10-25",
            "pub_date_card": {
                "ru": "25 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 25",
                "zh": "10æœˆ25æ—¥"
            },
            "hash": "bde2fa0e4317a316",
            "data": {
                "categories": [
                    "#survey",
                    "#architecture",
                    "#training",
                    "#inference",
                    "#benchmark",
                    "#edge_computing"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞœĞ°Ğ»Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ²ÑĞµÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½Ğ¸Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¼Ğ°Ğ»Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (SLM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ÑÑ‚ÑÑ Ğ²ÑĞµ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·-Ğ·Ğ° Ğ¸Ñ… ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ SLM, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ñ€ÑƒĞ½Ğ¸Ğ½Ğ³ Ğ¸ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‚ÑÑ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸Ğ½Ğ³Ğ° SLM Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾ÑĞ²ĞµÑ‰Ğ°ĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ½ĞµÑ€ĞµÑˆĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼Ğ°Ğ»Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Optimizing Small Language Models for Efficiency and Performance",
                    "desc": "This paper provides a detailed overview of Small Language Models (SLMs), which are designed to perform language tasks efficiently with low computational requirements. It introduces a new classification system for the various optimization methods used in SLMs, such as model compression, pruning, and quantization. The authors also compile important benchmark datasets and evaluation metrics that are essential for assessing the performance of SLMs. Furthermore, the paper discusses ongoing challenges in the field, aiming to assist researchers and practitioners in advancing the development of efficient language models."
                },
                "zh": {
                    "title": "å°å‹è¯­è¨€æ¨¡å‹ï¼šé«˜æ•ˆè¯­è¨€å¤„ç†çš„æœªæ¥",
                    "desc": "å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰å› å…¶é«˜æ•ˆæ€§å’Œæ€§èƒ½è€Œå˜å¾—è¶Šæ¥è¶Šé‡è¦ï¼Œèƒ½å¤Ÿåœ¨èµ„æºæœ‰é™çš„æƒ…å†µä¸‹æ‰§è¡Œå„ç§è¯­è¨€ä»»åŠ¡ï¼Œéå¸¸é€‚åˆåœ¨è®¾å¤‡ã€ç§»åŠ¨å’Œè¾¹ç¼˜è®¾å¤‡ç­‰ç¯å¢ƒä¸­ä½¿ç”¨ã€‚æœ¬æ–‡å¯¹SLMsè¿›è¡Œäº†å…¨é¢çš„è°ƒæŸ¥ï¼Œé‡ç‚¹ä»‹ç»äº†å®ƒä»¬çš„æ¶æ„ã€è®­ç»ƒæŠ€æœ¯å’Œæ¨¡å‹å‹ç¼©æŠ€æœ¯ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åˆ†ç±»æ³•ï¼Œç”¨äºå¯¹ä¼˜åŒ–SLMsçš„æ–¹æ³•è¿›è¡Œåˆ†ç±»ï¼ŒåŒ…æ‹¬æ¨¡å‹å‹ç¼©ã€å‰ªæå’Œé‡åŒ–æŠ€æœ¯ã€‚æˆ‘ä»¬æ€»ç»“äº†å¯¹SLMsè¿›è¡ŒåŸºå‡†æµ‹è¯•çš„æœ‰ç”¨æ•°æ®é›†ä»¥åŠå¸¸ç”¨çš„è¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶å¼ºè°ƒäº†ä»éœ€è§£å†³çš„å…³é”®å¼€æ”¾æŒ‘æˆ˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.21264",
            "title": "LARP: Tokenizing Videos with a Learned Autoregressive Generative Prior",
            "url": "https://huggingface.co/papers/2410.21264",
            "abstract": "We present LARP, a novel video tokenizer designed to overcome limitations in current video tokenization methods for autoregressive (AR) generative models. Unlike traditional patchwise tokenizers that directly encode local visual patches into discrete tokens, LARP introduces a holistic tokenization scheme that gathers information from the visual content using a set of learned holistic queries. This design allows LARP to capture more global and semantic representations, rather than being limited to local patch-level information. Furthermore, it offers flexibility by supporting an arbitrary number of discrete tokens, enabling adaptive and efficient tokenization based on the specific requirements of the task. To align the discrete token space with downstream AR generation tasks, LARP integrates a lightweight AR transformer as a training-time prior model that predicts the next token on its discrete latent space. By incorporating the prior model during training, LARP learns a latent space that is not only optimized for video reconstruction but is also structured in a way that is more conducive to autoregressive generation. Moreover, this process defines a sequential order for the discrete tokens, progressively pushing them toward an optimal configuration during training, ensuring smoother and more accurate AR generation at inference time. Comprehensive experiments demonstrate LARP's strong performance, achieving state-of-the-art FVD on the UCF101 class-conditional video generation benchmark. LARP enhances the compatibility of AR models with videos and opens up the potential to build unified high-fidelity multimodal large language models (MLLMs).",
            "score": 1,
            "issue_id": 321,
            "pub_date": "2024-10-28",
            "pub_date_card": {
                "ru": "28 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 28",
                "zh": "10æœˆ28æ—¥"
            },
            "hash": "20438897f3e9bc41",
            "data": {
                "categories": [
                    "#video",
                    "#multimodal",
                    "#benchmark",
                    "#architecture"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "LARP: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "LARP - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ², LARP Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ…Ğ¾Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ´Ğ»Ñ ÑĞ±Ğ¾Ñ€Ğ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. LARP Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ±Ğ»ĞµĞ³Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¹ AR-Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LARP Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ UCF101 Ğ¿Ğ¾ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "LARP: Revolutionizing Video Tokenization for Better Generative Models",
                    "desc": "LARP is a new video tokenizer that improves how videos are processed for autoregressive generative models. Instead of just breaking videos into small patches, LARP uses learned holistic queries to capture broader and more meaningful visual information. This method allows for flexible tokenization, adapting the number of tokens based on the task's needs. By integrating a lightweight autoregressive transformer during training, LARP optimizes the token space for better video generation, achieving top performance in benchmarks."
                },
                "zh": {
                    "title": "LARPï¼šè§†é¢‘ç”Ÿæˆçš„æ–°çªç ´",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„è§†é¢‘æ ‡è®°å™¨LARPï¼Œæ—¨åœ¨å…‹æœå½“å‰è‡ªå›å½’ç”Ÿæˆæ¨¡å‹åœ¨è§†é¢‘æ ‡è®°æ–¹é¢çš„å±€é™æ€§ã€‚ä¸ä¼ ç»Ÿçš„å±€éƒ¨è¡¥ä¸æ ‡è®°å™¨ä¸åŒï¼ŒLARPé‡‡ç”¨æ•´ä½“æ ‡è®°æ–¹æ¡ˆï¼Œé€šè¿‡å­¦ä¹ çš„æ•´ä½“æŸ¥è¯¢æ”¶é›†è§†è§‰å†…å®¹çš„ä¿¡æ¯ï¼Œä»è€Œæ•æ‰æ›´å…¨çƒå’Œè¯­ä¹‰åŒ–çš„è¡¨ç¤ºã€‚LARPæ”¯æŒä»»æ„æ•°é‡çš„ç¦»æ•£æ ‡è®°ï¼Œèƒ½å¤Ÿæ ¹æ®ä»»åŠ¡çš„å…·ä½“éœ€æ±‚è¿›è¡Œè‡ªé€‚åº”å’Œé«˜æ•ˆçš„æ ‡è®°ã€‚é€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ•´åˆè½»é‡çº§çš„è‡ªå›å½’å˜æ¢å™¨ï¼ŒLARPä¼˜åŒ–äº†è§†é¢‘é‡å»ºå’Œè‡ªå›å½’ç”Ÿæˆçš„æ½œåœ¨ç©ºé—´ï¼Œç¡®ä¿åœ¨æ¨ç†æ—¶å®ç°æ›´å¹³æ»‘å’Œå‡†ç¡®çš„ç”Ÿæˆã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-10-28.html",
    "link_next": "2024-10-30.html",
    "short_date_prev": {
        "ru": "28.10",
        "en": "10/28",
        "zh": "10æœˆ28æ—¥"
    },
    "short_date_next": {
        "ru": "30.10",
        "en": "10/30",
        "zh": "10æœˆ30æ—¥"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 3,
        "#agents": 1,
        "#cv": 1,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 1,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#medicine": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#quantum": 0,
        "#edge_computing": 1,
        "#optimization": 0,
        "#survey": 1,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¼€æ”¾ç¯å¢ƒä¸­çš„å†³ç­–èƒ½åŠ›ã€‚VLMsåœ¨å¤šæ¨¡å¼ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¼€æ”¾ç¯å¢ƒä¸­åº”ç”¨æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚ä¸»è¦é—®é¢˜æ˜¯å°†ä½å±‚æ¬¡è§‚å¯Ÿä¸­çš„å•ä¸ªå®ä½“ä¸è§„åˆ’æ‰€éœ€çš„æŠ½è±¡æ¦‚å¿µè¿æ¥èµ·æ¥ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½œè€…æå‡ºäº†è§†è§‰æ—¶é—´ä¸Šä¸‹æ–‡æç¤ºï¼Œä¸€ç§VLMsä¸ç­–ç•¥æ¨¡å‹ä¹‹é—´çš„æ–°é€šä¿¡åè®®ã€‚è¯¥æ–¹æ³•åˆ©ç”¨è¿‡å»å’Œç°åœ¨çš„è§‚å¯Ÿè¿›è¡Œå¯¹è±¡åˆ†å‰²ï¼ŒæŒ‡å¯¼ç­–ç•¥ä¸ç¯å¢ƒçš„äº¤äº’ã€‚å®éªŒè¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•ä½¿å¾—ä»£ç†èƒ½å¤Ÿå®Œæˆä»¥å‰æ— æ³•å®ç°çš„ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯é‚£äº›ä¾èµ–ç©ºé—´ç†è§£çš„å¤æ‚ä»»åŠ¡ã€‚",
        "title": "ROCKET-1: Master Open-World Interaction with Visual-Temporal Context Prompting",
        "pinyin": "ZhÃ¨ piÄn wÃ©nzhÄng tÇolÃ¹n le shÃ¬juÃ© yÇ”yÃ¡n mÃ³xÃ­ng (VLMs) zÃ i kÄifÃ ng huÃ¡njÃ¬ng zhÅng de juÃ©cÃ¨ nÃ©nglÃ¬. VLMs zÃ i duÅ mÃ³shÃ¬ rÃ¨nwÃ¹ zhÅng biÇoxiÇn chÅ«sÃ¨, dÃ n zÃ i kÄifÃ ng huÃ¡njÃ¬ng zhÅng yÃ¬ngyÃ²ng shÃ­ miÃ nlÃ­n tiÇozhÃ n. ZhÇ”yÃ o wÃ¨nti shÃ¬ jiÄng dÄ« cÃ©ng guÄnchÃ¡ zhÅng de dÄn gÃ¨ shÃ­jÃ¬ yÇ” guÄ«huÃ  suÇ’xÅ« de chÅuxiÃ ng gÃ iniÃ n liÃ¡njiÄ“ qÇlÃ¡i. WÃ¨i jiÄ›juÃ© zhÃ¨ yÄ« wÃ¨nti, zuÃ²zhÄ› tÃ­chÅ«le shÃ¬juÃ© shÃ­jiÄn shÃ ngxiÃ tÃ¨ng tÃ­shÃ¬, yÄ«zhÇ’ng VLMs yÇ” cÃ¨lÃ¼Ã¨ mÃ³xÃ­ng zhÄ«jiÄn de xÄ«n tÅngxÃ¬n xiÃ©yÃ¬. GÄi fÄngfÇ lÃ¬yÃ²ng guÃ²qÃ¹ hÃ© xiÃ nzÃ i de guÄnchÃ¡ zuÃ²wÃ©i duÃ¬xiÃ ng fÄ“ngÃ©, zhÇdÇo cÃ¨lÃ¼Ã¨ yÇ” huÃ¡njÃ¬ng de jiÄohÃ¹. ShÃ­yÃ n biÇomÃ­ng, zhÃ¨ zhÇ’ng fÄngfÇ shÇdÃ© dÃ ilÇ nÃ©nggÃ²u wÃ¡nchÃ©ng yÇqiÃ¡n wÃºfÇ shÃ­xiÃ n de rÃ¨nwÃ¹, tÃ¨biÃ© shÃ¬ nÃ xiÄ“ yÄ«lÇi kÅngjiÄn lÇjiÄ› de fÃ¹zÃ¡ rÃ¨nwÃ¹.",
        "vocab": "[\n    {\"word\": \"è§†è§‰è¯­è¨€æ¨¡å‹\", \"pinyin\": \"shÃ¬juÃ© yÇ”yÃ¡n mÃ³xÃ­ng\", \"trans\": \"visual language models\"},\n    {\"word\": \"å¼€æ”¾ç¯å¢ƒ\", \"pinyin\": \"kÄifÃ ng huÃ¡njÃ¬ng\", \"trans\": \"open environments\"},\n    {\"word\": \"å†³ç­–èƒ½åŠ›\", \"pinyin\": \"juÃ©cÃ¨ nÃ©nglÃ¬\", \"trans\": \"decision-making ability\"},\n    {\"word\": \"å¤šæ¨¡å¼ä»»åŠ¡\", \"pinyin\": \"duÅ mÃ³shÃ¬ rÃ¨nwÃ¹\", \"trans\": \"multimodal tasks\"},\n    {\"word\": \"è¡¨ç°å‡ºè‰²\", \"pinyin\": \"biÇoxiÃ n chÅ«sÃ¨\", \"trans\": \"perform well\"},\n    {\"word\": \"é¢ä¸´æŒ‘æˆ˜\", \"pinyin\": \"miÃ nlÃ­n tiÇozhÃ n\", \"trans\": \"face challenges\"},\n    {\"word\": \"ä½å±‚æ¬¡è§‚å¯Ÿ\", \"pinyin\": \"dÄ« cÃ©ngcÃ¬ guÄnchÃ¡\", \"trans\": \"low-level observations\"},\n    {\"word\": \"å•ä¸ªå®ä½“\", \"pinyin\": \"dÄn gÃ¨ shÃ­tÇ\", \"trans\": \"individual entities\"},\n    {\"word\": \"è§„åˆ’æ‰€éœ€çš„æŠ½è±¡æ¦‚å¿µ\", \"pinyin\": \"guÄ«huÃ  suÇ’xÅ« de chÅuxiÃ ng gÃ iniÃ n\", \"trans\": \"abstract concepts required for planning\"},\n    {\"word\": \"è§†è§‰æ—¶é—´ä¸Šä¸‹æ–‡æç¤º\", \"pinyin\": \"shÃ¬juÃ© shÃ­jiÄn shÃ ngxiÃ wÃ©n tÃ­shÃ¬\", \"trans\": \"visual temporal context prompts\"},\n    {\"word\": \"æ–°é€šä¿¡åè®®\", \"pinyin\": \"xÄ«n tÅngxÃ¬n xiÃ©yÃ¬\", \"trans\": \"new communication protocol\"},\n    {\"word\": \"è¿‡å»å’Œç°åœ¨çš„è§‚å¯Ÿ\", \"pinyin\": \"guÃ²qÃ¹ hÃ© xiÃ nzÃ i de guÄnchÃ¡\", \"trans\": \"past and present observations\"},\n    {\"word\": \"å¯¹è±¡åˆ†å‰²\", \"pinyin\": \"duÃ¬xiÃ ng fÄ“ngÃ©\", \"trans\": \"object segmentation\"},\n    {\"word\": \"æŒ‡å¯¼ç­–ç•¥ä¸ç¯å¢ƒçš„äº¤äº’\", \"pinyin\": \"zhÇdÇo cÃ¨lÃ¼Ã¨ yÇ” huÃ¡njÃ¬ng de jiÄohÃ¹\", \"trans\": \"guide the interaction of strategies with the environment\"},\n    {\"word\": \"ä»£ç†\", \"pinyin\": \"dÃ ilÇ\", \"trans\": \"agent\"},\n    {\"word\": \"å®Œæˆä»¥å‰æ— æ³•å®ç°çš„ä»»åŠ¡\", \"pinyin\": \"wÃ¡nchÃ©ng yÇqiÃ¡n wÃºfÇ shÃ­xiÃ n de rÃ¨nwÃ¹\", \"trans\": \"complete tasks that were previously impossible\"},\n    {\"word\": \"ä¾èµ–ç©ºé—´ç†è§£çš„å¤æ‚ä»»åŠ¡\", \"pinyin\": \"yÄ«lÃ i kÅngjiÄn lÇjiÄ› de fÃ¹zÃ¡ rÃ¨nwÃ¹\", \"trans\": \"complex tasks that rely on spatial understanding\"}\n]",
        "trans": "This article discusses the decision-making capabilities of Vision-Language Models (VLMs) in open environments. VLMs perform well in multimodal tasks but face challenges when applied in open environments. The main issue is connecting individual entities from low-level observations with the abstract concepts required for planning. To address this problem, the authors propose visual temporal context prompting, a new communication protocol between VLMs and policy models. This method utilizes past and present observations for object segmentation, guiding the policy's interaction with the environment. Experiments show that this approach enables agents to complete tasks that were previously unachievable, particularly those that rely on spatial understanding.",
        "update_ts": "2024-10-28 09:26"
    }
}