{
    "date": {
        "ru": "29 октября",
        "en": "October 29",
        "zh": "10月29日"
    },
    "time_utc": "2025-10-29 02:31",
    "weekday": 2,
    "issue_id": 6667,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2510.24320",
            "title": "Critique-RL: Training Language Models for Critiquing through Two-Stage\n  Reinforcement Learning",
            "url": "https://huggingface.co/papers/2510.24320",
            "abstract": "Critique-RL is an online reinforcement learning approach for developing critiquing language models without strong supervision, using a two-stage optimization strategy to improve both the critic's discriminability and helpfulness.  \t\t\t\t\tAI-generated summary \t\t\t\t Training critiquing language models to assess and provide feedback on model outputs is a promising way to improve LLMs for complex reasoning tasks. However, existing approaches typically rely on stronger supervisors for annotating critique data. To address this, we propose Critique-RL, an online RL approach for developing critiquing language models without stronger supervision. Our approach operates on a two-player paradigm: the actor generates a response, the critic provides feedback, and the actor refines the response accordingly. We first reveal that relying solely on indirect reward signals from the actor's outputs for RL optimization often leads to unsatisfactory critics: while their helpfulness (i.e., providing constructive feedback) improves, the discriminability (i.e., determining whether a response is high-quality or not) remains poor, resulting in marginal performance gains. To overcome this, Critique-RL adopts a two-stage optimization strategy. In stage I, it reinforces the discriminability of the critic with direct rule-based reward signals; in stage II, it introduces indirect rewards based on actor refinement to improve the critic's helpfulness, while maintaining its discriminability via appropriate regularization. Extensive experiments across various tasks and models show that Critique-RL delivers substantial performance improvements. For example, it achieves a 9.02% gain on in-domain tasks and a 5.70% gain on out-of-domain tasks for Qwen2.5-7B, highlighting its potential.",
            "score": 5,
            "issue_id": 6667,
            "pub_date": "2025-10-28",
            "pub_date_card": {
                "ru": "28 октября",
                "en": "October 28",
                "zh": "10月28日"
            },
            "hash": "0c26fc5b04f76bff",
            "authors": [
                "Zhiheng Xi",
                "Jixuan Huang",
                "Xin Guo",
                "Boyang Hong",
                "Dingwen Yang",
                "Xiaoran Fan",
                "Shuo Li",
                "Zehui Chen",
                "Junjie Ye",
                "Siyu Yuan",
                "Zhengyin Du",
                "Xuesong Yao",
                "Yufei Xu",
                "Jiecao Chen",
                "Rui Zheng",
                "Tao Gui",
                "Qi Zhang",
                "Xuanjing Huang"
            ],
            "affiliations": [
                "ByteDance Seed",
                "Fudan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.24320.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#reasoning",
                    "#optimization",
                    "#training",
                    "#rlhf"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Critique-RL: Улучшение языковых моделей без сильного надзора",
                    "desc": "Critique-RL — это метод обучения с подкреплением для создания языковых моделей, которые могут критиковать без сильного надзора. Он использует двухэтапную стратегию оптимизации, чтобы улучшить как способность критика различать, так и его полезность. В первом этапе усиливается способность критика различать с помощью прямых сигналов вознаграждения, а во втором этапе вводятся косвенные вознаграждения для улучшения полезности критика. Эксперименты показывают, что Critique-RL значительно улучшает производительность моделей на различных задачах."
                },
                "en": {
                    "title": "Empowering Language Models with Self-Supervised Critique",
                    "desc": "Critique-RL is a novel online reinforcement learning method designed to enhance critiquing language models without the need for strong supervision. It employs a two-stage optimization strategy where an actor generates responses and a critic evaluates them, allowing for iterative refinement. The first stage focuses on improving the critic's ability to distinguish high-quality responses using direct reward signals, while the second stage enhances the critic's helpfulness through indirect rewards based on the actor's improvements. Experimental results demonstrate that Critique-RL significantly boosts performance, achieving notable gains in both in-domain and out-of-domain tasks."
                },
                "zh": {
                    "title": "Critique-RL：无强监督的评估语言模型优化方法",
                    "desc": "Critique-RL是一种在线强化学习方法，用于在没有强监督的情况下开发评估语言模型。该方法采用两阶段优化策略，旨在提高评估者的区分能力和有用性。在第一阶段，通过直接的基于规则的奖励信号来增强评估者的区分能力；在第二阶段，基于生成者的反馈引入间接奖励，以提高评估者的有用性，同时通过适当的正则化保持其区分能力。实验结果表明，Critique-RL在多个任务和模型上显著提升了性能，展示了其潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.24701",
            "title": "Tongyi DeepResearch Technical Report",
            "url": "https://huggingface.co/papers/2510.24701",
            "abstract": "Tongyi DeepResearch, a large language model with agentic capabilities, achieves top performance in various deep research tasks through an end-to-end training framework and automated data synthesis.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Tongyi DeepResearch, an agentic large language model, which is specifically designed for long-horizon, deep information-seeking research tasks. To incentivize autonomous deep research agency, Tongyi DeepResearch is developed through an end-to-end training framework that combines agentic mid-training and agentic post-training, enabling scalable reasoning and information seeking across complex tasks. We design a highly scalable data synthesis pipeline that is fully automatic, without relying on costly human annotation, and empowers all training stages. By constructing customized environments for each stage, our system enables stable and consistent interactions throughout. Tongyi DeepResearch, featuring 30.5 billion total parameters, with only 3.3 billion activated per token, achieves state-of-the-art performance across a range of agentic deep research benchmarks, including Humanity's Last Exam, BrowseComp, BrowseComp-ZH, WebWalkerQA, xbench-DeepSearch, FRAMES and xbench-DeepSearch-2510. We open-source the model, framework, and complete solutions to empower the community.",
            "score": 4,
            "issue_id": 6667,
            "pub_date": "2025-10-28",
            "pub_date_card": {
                "ru": "28 октября",
                "en": "October 28",
                "zh": "10月28日"
            },
            "hash": "a6f467c756fc29b7",
            "authors": [
                "Tongyi DeepResearch Team",
                "Baixuan Li",
                "Bo Zhang",
                "Dingchu Zhang",
                "Fei Huang",
                "Guangyu Li",
                "Guoxin Chen",
                "Huifeng Yin",
                "Jialong Wu",
                "Jingren Zhou",
                "Kuan Li",
                "Liangcai Su",
                "Litu Ou",
                "Liwen Zhang",
                "Pengjun Xie",
                "Rui Ye",
                "Wenbiao Yin",
                "Xinmiao Yu",
                "Xinyu Wang",
                "Xixi Wu",
                "Xuanzhong Chen",
                "Yida Zhao",
                "Zhen Zhang",
                "Zhengwei Tao",
                "Zhongwang Zhang",
                "Zile Qiao",
                "Chenxi Wang",
                "Donglei Yu",
                "Gang Fu",
                "Haiyang Shen",
                "Jiayin Yang",
                "Jun Lin",
                "Junkai Zhang",
                "Kui Zeng",
                "Li Yang",
                "Hailong Yin",
                "Maojia Song",
                "Ming Yan",
                "Peng Xia",
                "Qian Xiao",
                "Rui Min",
                "Ruixue Ding",
                "Runnan Fang",
                "Shaowei Chen",
                "Shen Huang",
                "Shihang Wang",
                "Shihao Cai",
                "Weizhou Shen",
                "Xiaobin Wang",
                "Xin Guan",
                "Xinyu Geng",
                "Yingcheng Shi",
                "Yuning Wu",
                "Zhuo Chen",
                "Zijian Li",
                "Yong Jiang"
            ],
            "affiliations": [
                "Tongyi Lab, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.24701.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#synthetic",
                    "#dataset",
                    "#benchmark",
                    "#training",
                    "#open_source",
                    "#agi",
                    "#agents"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Автономный AI-агент для глубоких исследований",
                    "desc": "Tongyi DeepResearch - это большая языковая модель с агентными способностями, специализирующаяся на долгосрочных исследовательских задачах с глубоким поиском информации. Модель обучается через end-to-end фреймворк, включающий агентное mid-training и post-training, что позволяет масштабируемо рассуждать и искать информацию в сложных задачах. Ключевая особенность - полностью автоматический пайплайн синтеза данных без дорогостоящей человеческой разметки. При 30.5 миллиардах параметров, из которых активируется только 3.3 миллиарда на токен, модель достигает state-of-the-art результатов на множестве бенчмарков для агентных исследований."
                },
                "en": {
                    "title": "Empowering Autonomous Deep Research with Tongyi DeepResearch",
                    "desc": "Tongyi DeepResearch is a large language model designed for complex research tasks that require deep information-seeking capabilities. It utilizes an end-to-end training framework that includes both mid-training and post-training phases to enhance its autonomous research abilities. The model features a fully automated data synthesis pipeline, eliminating the need for expensive human annotations, which allows for scalable training across various tasks. With 30.5 billion parameters, Tongyi DeepResearch achieves top performance on multiple benchmarks, demonstrating its effectiveness in deep research applications."
                },
                "zh": {
                    "title": "自主深度研究的未来",
                    "desc": "Tongyi DeepResearch 是一种具有自主能力的大型语言模型，专门用于长时间的信息检索研究任务。它通过端到端的训练框架和自动化的数据合成，能够在复杂任务中实现可扩展的推理和信息获取。该模型具有 305 亿个参数，且每个令牌仅激活 33 亿个参数，表现出色，达到了多项深度研究基准的最先进水平。我们将模型、框架和完整解决方案开源，以支持社区的发展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.22099",
            "title": "Generalization or Memorization: Dynamic Decoding for Mode Steering",
            "url": "https://huggingface.co/papers/2510.22099",
            "abstract": "A framework using the Information Bottleneck principle and Dynamic Mode Steering algorithm improves the reliability of Large Language Models by balancing generalization and memorization.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) exhibit a troubling duality, capable of both remarkable generalization and brittle, verbatim memorization of their training data. This unpredictability undermines their reliability in high-stakes applications. In this work, we propose a unified framework to understand, identify, and control these distinct reasoning modes. First, we introduce a theoretical model based on the Information Bottleneck (IB) principle, formalizing generalization as the learning of a compressed, task-relevant representation and memorization as a failure to compress. Building on this theory, we develop Dynamic Mode Steering (DMS), a novel inference-time algorithm which comprises two components: (1) a lightweight, causally-grounded linear probe that identifies the model's instantaneous reliance on memorization, and (2) a dynamic activation steering mechanism that nudges the model's computation towards pre-identified generalization circuits. We frame DMS as a form of adaptive, self-contrastive decoding. Experiments on reasoning and faithfulness tasks demonstrate that DMS significantly improves logical consistency and factual accuracy, thereby offering a principled approach to enhancing LLM reliability.",
            "score": 1,
            "issue_id": 6667,
            "pub_date": "2025-10-25",
            "pub_date_card": {
                "ru": "25 октября",
                "en": "October 25",
                "zh": "10月25日"
            },
            "hash": "7a4136db7043277f",
            "authors": [
                "Xuanming Zhang"
            ],
            "affiliations": [
                "Department of Computer Science, University of Wisconsin-Madison, Madison, USA",
                "Stanford University, Palo Alto, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.22099.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#interpretability",
                    "#inference",
                    "#training"
                ],
                "emoji": "🧭",
                "ru": {
                    "title": "Управление режимами мышления: как научить LLM обобщать, а не заучивать",
                    "desc": "Исследователи предлагают новый подход к проблеме, когда большие языковые модели то блестяще обобщают знания, то просто воспроизводят заученные фразы из обучающих данных. На основе принципа Information Bottleneck они создали теоретическую модель, где обобщение понимается как сжатие информации до важных для задачи элементов, а механическое запоминание — как неспособность к такому сжатию. Разработанный алгоритм Dynamic Mode Steering использует легковесный линейный классификатор для определения момента, когда модель полагается на заученные данные, и динамически корректирует её вычисления в сторону обобщающих паттернов. Эксперименты показали, что метод значительно улучшает логическую последовательность и фактическую точность ответов LLM."
                },
                "en": {
                    "title": "Balancing Generalization and Memorization for Reliable LLMs",
                    "desc": "This paper presents a framework that enhances the reliability of Large Language Models (LLMs) by addressing their tendency to both generalize and memorize training data. It introduces the Information Bottleneck principle to differentiate between effective generalization, which compresses information, and problematic memorization, which retains too much detail. The Dynamic Mode Steering algorithm is developed to dynamically adjust the model's focus during inference, promoting generalization while minimizing reliance on memorization. Experimental results show that this approach improves logical consistency and factual accuracy in LLM outputs, making them more reliable for critical applications."
                },
                "zh": {
                    "title": "提升大型语言模型可靠性的创新框架",
                    "desc": "本文提出了一种新的框架，利用信息瓶颈原理和动态模式引导算法来提高大型语言模型的可靠性。该框架旨在平衡模型的泛化能力和记忆能力，解决模型在高风险应用中的不可靠性问题。通过理论模型，泛化被定义为学习压缩的、与任务相关的表示，而记忆则被视为未能进行压缩。实验结果表明，动态模式引导算法显著提高了模型的逻辑一致性和事实准确性，提供了一种增强大型语言模型可靠性的原则性方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.21323",
            "title": "VL-SAE: Interpreting and Enhancing Vision-Language Alignment with a\n  Unified Concept Set",
            "url": "https://huggingface.co/papers/2510.21323",
            "abstract": "VL-SAE, a sparse autoencoder, enhances vision-language alignment by correlating neurons to unified concepts, improving interpretability and performance in tasks like zero-shot image classification and hallucination elimination.  \t\t\t\t\tAI-generated summary \t\t\t\t The alignment of vision-language representations endows current Vision-Language Models (VLMs) with strong multi-modal reasoning capabilities. However, the interpretability of the alignment component remains uninvestigated due to the difficulty in mapping the semantics of multi-modal representations into a unified concept set. To address this problem, we propose VL-SAE, a sparse autoencoder that encodes vision-language representations into its hidden activations. Each neuron in its hidden layer correlates to a concept represented by semantically similar images and texts, thereby interpreting these representations with a unified concept set. To establish the neuron-concept correlation, we encourage semantically similar representations to exhibit consistent neuron activations during self-supervised training. First, to measure the semantic similarity of multi-modal representations, we perform their alignment in an explicit form based on cosine similarity. Second, we construct the VL-SAE with a distance-based encoder and two modality-specific decoders to ensure the activation consistency of semantically similar representations. Experiments across multiple VLMs (e.g., CLIP, LLaVA) demonstrate the superior capability of VL-SAE in interpreting and enhancing the vision-language alignment. For interpretation, the alignment between vision and language representations can be understood by comparing their semantics with concepts. For enhancement, the alignment can be strengthened by aligning vision-language representations at the concept level, contributing to performance improvements in downstream tasks, including zero-shot image classification and hallucination elimination. Codes are available at https://github.com/ssfgunner/VL-SAE.",
            "score": 1,
            "issue_id": 6667,
            "pub_date": "2025-10-24",
            "pub_date_card": {
                "ru": "24 октября",
                "en": "October 24",
                "zh": "10月24日"
            },
            "hash": "41279ff25d4d0ecb",
            "authors": [
                "Shufan Shen",
                "Junshu Sun",
                "Qingming Huang",
                "Shuhui Wang"
            ],
            "affiliations": [
                "Key Lab of Intell. Info. Process., Inst. of Comput. Tech., CAS",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.21323.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#multimodal",
                    "#interpretability",
                    "#cv",
                    "#hallucinations",
                    "#training"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Разреженный автоэнкодер для понимания и усиления связи между зрением и языком",
                    "desc": "VL-SAE — это разреженный автоэнкодер, который повышает интерпретируемость визуально-языкового выравнивания в VLM моделях путём сопоставления нейронов с единым набором концептов. Каждый нейрон скрытого слоя коррелирует с концептом, представленным семантически похожими изображениями и текстами. Модель обучается самообучением, обеспечивая согласованную активацию нейронов для семантически близких представлений разных модальностей. Эксперименты показывают улучшение производительности в задачах zero-shot классификации изображений и устранения галлюцинаций в моделях типа CLIP и LLaVA."
                },
                "en": {
                    "title": "Enhancing Vision-Language Alignment with VL-SAE",
                    "desc": "The paper introduces VL-SAE, a sparse autoencoder designed to improve the alignment between vision and language representations in multi-modal models. By correlating neurons in its hidden layer to unified concepts derived from semantically similar images and texts, VL-SAE enhances both interpretability and performance. The model employs self-supervised training to ensure consistent neuron activations for similar representations, using cosine similarity for semantic alignment. Experiments show that VL-SAE significantly boosts the effectiveness of vision-language models in tasks like zero-shot image classification and reducing hallucinations."
                },
                "zh": {
                    "title": "VL-SAE：提升视觉-语言对齐的稀疏自编码器",
                    "desc": "VL-SAE是一种稀疏自编码器，通过将视觉和语言表示关联到统一的概念，增强了视觉-语言对齐的能力。它的隐藏层中的每个神经元与语义相似的图像和文本所代表的概念相关联，从而提高了模型的可解释性和性能。通过自监督训练，VL-SAE鼓励语义相似的表示在神经元激活上保持一致。实验表明，VL-SAE在零样本图像分类和消除幻觉等任务中表现出色，显著提升了视觉-语言模型的对齐能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.24591",
            "title": "ReplicationBench: Can AI Agents Replicate Astrophysics Research Papers?",
            "url": "https://huggingface.co/papers/2510.24591",
            "abstract": "ReplicationBench evaluates AI agents' ability to replicate astrophysics research papers, providing insights into their faithfulness and correctness in scientific research tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Frontier AI agents show increasing promise as scientific research assistants, and may eventually be useful for extended, open-ended research workflows. However, in order to use agents for novel research, we must first assess the underlying faithfulness and correctness of their work. To evaluate agents as research assistants, we introduce ReplicationBench, an evaluation framework that tests whether agents can replicate entire research papers drawn from the astrophysics literature. Astrophysics, where research relies heavily on archival data and computational study while requiring little real-world experimentation, is a particularly useful testbed for AI agents in scientific research. We split each paper into tasks which require agents to replicate the paper's core contributions, including the experimental setup, derivations, data analysis, and codebase. Each task is co-developed with the original paper authors and targets a key scientific result, enabling objective evaluation of both faithfulness (adherence to original methods) and correctness (technical accuracy of results). ReplicationBench is extremely challenging for current frontier language models: even the best-performing language models score under 20%. We analyze ReplicationBench trajectories in collaboration with domain experts and find a rich, diverse set of failure modes for agents in scientific research. ReplicationBench establishes the first benchmark of paper-scale, expert-validated astrophysics research tasks, reveals insights about agent performance generalizable to other domains of data-driven science, and provides a scalable framework for measuring AI agents' reliability in scientific research.",
            "score": 0,
            "issue_id": 6667,
            "pub_date": "2025-10-28",
            "pub_date_card": {
                "ru": "28 октября",
                "en": "October 28",
                "zh": "10月28日"
            },
            "hash": "d655b0884e7b15c0",
            "authors": [
                "Christine Ye",
                "Sihan Yuan",
                "Suchetha Cooray",
                "Steven Dillmann",
                "Ian L. V. Roque",
                "Dalya Baron",
                "Philipp Frank",
                "Sergio Martin-Alvarez",
                "Nolan Koblischke",
                "Frank J Qu",
                "Diyi Yang",
                "Risa Wechsler",
                "Ioana Ciuca"
            ],
            "affiliations": [
                "Stanford University",
                "University of Toronto"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.24591.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#science",
                    "#agents"
                ],
                "emoji": "🔭",
                "ru": {
                    "title": "Проверка AI-агентов на репликацию научных исследований",
                    "desc": "Исследователи представили ReplicationBench — бенчмарк для оценки способности AI-агентов воспроизводить научные работы по астрофизике. Каждая задача включает проверку верности оригинальным методам и корректности технических результатов, при этом все задачи разработаны совместно с авторами оригинальных статей. Даже лучшие современные LLM показывают результат ниже 20%, демонстрируя множество разнообразных ошибок при работе с научными исследованиями. Этот бенчмарк предоставляет первую масштабируемую систему для измерения надежности AI-агентов в научных исследованиях и может быть применен в других областях науки, основанных на данных."
                },
                "en": {
                    "title": "Evaluating AI Agents in Astrophysics Research Replication",
                    "desc": "ReplicationBench is a framework designed to evaluate AI agents' ability to replicate research papers in astrophysics, focusing on their faithfulness and correctness. It breaks down each paper into specific tasks that require the agents to reproduce key contributions, such as experimental setups and data analyses, in collaboration with the original authors. The framework reveals that even advanced language models struggle with these tasks, scoring below 20%, highlighting various failure modes in their performance. This benchmark not only assesses AI reliability in astrophysics but also offers insights applicable to other scientific fields."
                },
                "zh": {
                    "title": "评估 AI 代理在科学研究中的可靠性",
                    "desc": "ReplicationBench 是一个评估 AI 代理在复制天体物理学研究论文能力的框架。它通过将论文分解为多个任务，测试代理是否能够准确复现论文的核心贡献，包括实验设置、推导、数据分析和代码库。该框架与原论文作者共同开发，确保评估的客观性，关注代理的忠实性和正确性。尽管当前的前沿语言模型在此任务中表现不佳，ReplicationBench 仍为科学研究中的 AI 代理提供了一个可扩展的可靠性测量框架。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.24563",
            "title": "OSWorld-MCP: Benchmarking MCP Tool Invocation In Computer-Use Agents",
            "url": "https://huggingface.co/papers/2510.24563",
            "abstract": "OSWorld-MCP is a benchmark that evaluates multimodal agents' tool invocation, GUI operation, and decision-making abilities, highlighting the importance of assessing tool usage in real-world scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t With advances in decision-making and reasoning capabilities, multimodal agents show strong potential in computer application scenarios. Past evaluations have mainly assessed GUI interaction skills, while tool invocation abilities, such as those enabled by the Model Context Protocol (MCP), have been largely overlooked. Comparing agents with integrated tool invocation to those evaluated only on GUI interaction is inherently unfair. We present OSWorld-MCP, the first comprehensive and fair benchmark for assessing computer-use agents' tool invocation, GUI operation, and decision-making abilities in a real-world environment. We design a novel automated code-generation pipeline to create tools and combine them with a curated selection from existing tools. Rigorous manual validation yields 158 high-quality tools (covering 7 common applications), each verified for correct functionality, practical applicability, and versatility. Extensive evaluations of state-of-the-art multimodal agents on OSWorld-MCP show that MCP tools generally improve task success rates (e.g., from 8.3% to 20.4% for OpenAI o3 at 15 steps, from 40.1% to 43.3% for Claude 4 Sonnet at 50 steps), underscoring the importance of assessing tool invocation capabilities. However, even the strongest models have relatively low tool invocation rates, Only 36.3%, indicating room for improvement and highlighting the benchmark's challenge. By explicitly measuring MCP tool usage skills, OSWorld-MCP deepens understanding of multimodal agents and sets a new standard for evaluating performance in complex, tool-assisted environments. Our code, environment, and data are publicly available at https://osworld-mcp.github.io.",
            "score": 0,
            "issue_id": 6667,
            "pub_date": "2025-10-28",
            "pub_date_card": {
                "ru": "28 октября",
                "en": "October 28",
                "zh": "10月28日"
            },
            "hash": "31a5246f81b64759",
            "authors": [
                "Hongrui Jia",
                "Jitong Liao",
                "Xi Zhang",
                "Haiyang Xu",
                "Tianbao Xie",
                "Chaoya Jiang",
                "Ming Yan",
                "Si Liu",
                "Wei Ye",
                "Fei Huang"
            ],
            "affiliations": [
                "Beijing Zhongguancun Academy",
                "Peking University",
                "Tongyi Lab, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.24563.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#reasoning",
                    "#benchmark",
                    "#optimization",
                    "#open_source",
                    "#agents"
                ],
                "emoji": "🛠️",
                "ru": {
                    "title": "Новый стандарт оценки AI-агентов: не только GUI, но и умение пользоваться инструментами",
                    "desc": "OSWorld-MCP — это первый комплексный бенчмарк для оценки мультимодальных агентов в реальных условиях, который измеряет способности к вызову инструментов, работе с графическим интерфейсом и принятию решений. Исследователи создали 158 высококачественных инструментов для 7 популярных приложений с помощью автоматизированной генерации кода и ручной проверки. Эксперименты показали, что использование MCP-инструментов улучшает показатели успеха задач для современных моделей, например, с 8.3% до 20.4% для OpenAI o3. Однако даже лучшие модели используют инструменты только в 36.3% случаев, что указывает на значительный потенциал для улучшения."
                },
                "en": {
                    "title": "Revolutionizing Multimodal Agent Evaluation with OSWorld-MCP",
                    "desc": "OSWorld-MCP is a new benchmark designed to evaluate multimodal agents on their ability to invoke tools, operate GUIs, and make decisions in real-world scenarios. It addresses the gap in previous assessments that primarily focused on GUI interactions, providing a fair comparison by including tool invocation capabilities. The benchmark features a unique automated code-generation pipeline that creates and validates 158 high-quality tools for common applications. Results show that while integrating MCP tools significantly improves task success rates, there is still a need for enhancement in tool invocation rates among leading models, emphasizing the benchmark's importance in advancing multimodal agent evaluation."
                },
                "zh": {
                    "title": "评估多模态智能体的新标准",
                    "desc": "OSWorld-MCP是一个基准测试，旨在评估多模态智能体在工具调用、图形用户界面（GUI）操作和决策能力方面的表现。该研究强调了在真实场景中评估工具使用的重要性，尤其是通过模型上下文协议（MCP）实现的工具调用能力。通过设计自动化代码生成管道，研究团队创建了158个高质量工具，并对其功能和适用性进行了严格验证。评估结果表明，使用MCP工具的智能体在任务成功率上有显著提升，显示出工具调用能力的关键性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.24448",
            "title": "Rethinking Visual Intelligence: Insights from Video Pretraining",
            "url": "https://huggingface.co/papers/2510.24448",
            "abstract": "Video Diffusion Models (VDMs) show higher data efficiency than large language models across various visual tasks, suggesting video pretraining can enhance visual foundation models.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have demonstrated that large-scale pretraining enables systems to adapt rapidly to new problems with little supervision in the language domain. This success, however, has not translated as effectively to the visual domain, where models, including LLMs, continue to struggle with compositional understanding, sample efficiency, and general-purpose problem-solving. We investigate Video Diffusion Models (VDMs) as a promising direction for bridging this gap. Pretraining on spatiotemporal data endows these models with strong inductive biases for structure and dynamics, which we hypothesize can support broad task adaptability. To test this, we design a controlled evaluation in which both a pretrained LLM and a pretrained VDM are equipped with lightweight adapters and presented with tasks in their natural modalities. Across benchmarks including ARC-AGI, ConceptARC, visual games, route planning, and cellular automata, VDMs demonstrate higher data efficiency than their language counterparts. Taken together, our results indicate that video pretraining offers inductive biases that support progress toward visual foundation models.",
            "score": 0,
            "issue_id": 6667,
            "pub_date": "2025-10-28",
            "pub_date_card": {
                "ru": "28 октября",
                "en": "October 28",
                "zh": "10月28日"
            },
            "hash": "cb963e7271205da4",
            "authors": [
                "Pablo Acuaviva",
                "Aram Davtyan",
                "Mariam Hassan",
                "Sebastian Stapf",
                "Ahmad Rahimi",
                "Alexandre Alahi",
                "Paolo Favaro"
            ],
            "affiliations": [
                "Computer Vision Group University of Bern Bern, Switzerland",
                "VITA Lab, EPFL Lausanne, Switzerland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.24448.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#transfer_learning",
                    "#games",
                    "#diffusion",
                    "#benchmark",
                    "#video"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Видео-диффузия побеждает языковые модели в визуальных задачах",
                    "desc": "Исследователи сравнили Video Diffusion Models (VDMs) и большие языковые модели (LLM) в визуальных задачах. VDMs, предобученные на видеоданных, показали более высокую эффективность обучения на малых данных благодаря лучшему пониманию пространственно-временной структуры. Модели тестировались на различных бенчмарках, включая ARC-AGI, ConceptARC, визуальные игры и планирование маршрутов. Результаты показывают, что предобучение на видео дает важные индуктивные смещения для создания визуальных foundation models."
                },
                "en": {
                    "title": "Unlocking Visual Potential with Video Diffusion Models",
                    "desc": "This paper explores the effectiveness of Video Diffusion Models (VDMs) in improving visual tasks through video pretraining. Unlike large language models (LLMs), which excel in language tasks, VDMs leverage spatiotemporal data to enhance their understanding of structure and dynamics. The authors conducted experiments comparing pretrained VDMs and LLMs, finding that VDMs showed superior data efficiency across various benchmarks. The results suggest that video pretraining can significantly advance the development of visual foundation models."
                },
                "zh": {
                    "title": "视频预训练提升视觉模型效率",
                    "desc": "视频扩散模型（VDMs）在各种视觉任务中显示出比大型语言模型更高的数据效率，表明视频预训练可以增强视觉基础模型的能力。尽管大型语言模型在语言领域的预训练取得了成功，但在视觉领域，模型仍然面临组合理解和样本效率等挑战。我们研究VDMs作为弥合这一差距的有前景的方向，认为其在时空数据上的预训练赋予了模型强大的结构和动态的归纳偏置。我们的实验结果表明，VDMs在多个基准测试中表现出比语言模型更高的数据效率，支持了视频预训练对视觉基础模型的进展。"
                }
            }
        }
    ],
    "link_prev": "2025-10-28.html",
    "link_next": "2025-10-30.html",
    "link_month": "2025-10.html",
    "short_date_prev": {
        "ru": "28.10",
        "en": "10/28",
        "zh": "10月28日"
    },
    "short_date_next": {
        "ru": "30.10",
        "en": "10/30",
        "zh": "10月30日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 4,
        "#agents": 3,
        "#cv": 1,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 0,
        "#agi": 1,
        "#games": 1,
        "#interpretability": 2,
        "#reasoning": 3,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    }
}