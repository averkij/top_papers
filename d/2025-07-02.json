{
    "date": {
        "ru": "2 июля",
        "en": "July 2",
        "zh": "7月2日"
    },
    "time_utc": "2025-07-02 03:46",
    "weekday": 2,
    "issue_id": 4593,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2506.23115",
            "title": "MoCa: Modality-aware Continual Pre-training Makes Better Bidirectional\n  Multimodal Embeddings",
            "url": "https://huggingface.co/papers/2506.23115",
            "abstract": "MoCa, a two-stage framework, enhances pre-trained causal vision-language models for multimodal embedding by introducing bidirectional attention, scaling with unlabeled data, and diverse training objectives.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal embedding models, built upon causal Vision Language Models (VLMs), have shown promise in various tasks. However, current approaches face three key limitations: the use of causal attention in VLM backbones is suboptimal for embedding tasks; scalability issues due to reliance on high-quality labeled paired data for contrastive learning; and limited diversity in training objectives and data. To address these issues, we propose MoCa, a two-stage framework for transforming pre-trained VLMs into effective bidirectional multimodal embedding models. The first stage, Modality-aware Continual Pre-training, introduces a joint reconstruction objective that simultaneously denoises interleaved text and image inputs, enhancing bidirectional context-aware reasoning. The second stage, Heterogeneous Contrastive Fine-tuning, leverages diverse, semantically rich multimodal data beyond simple image-caption pairs to enhance generalization and alignment. Our method addresses the stated limitations by introducing bidirectional attention through continual pre-training, scaling effectively with massive unlabeled datasets via joint reconstruction objectives, and utilizing diverse multimodal data for enhanced representation robustness. Experiments demonstrate that MoCa consistently improves performance across MMEB and ViDoRe-v2 benchmarks, achieving new state-of-the-art results, and exhibits strong scalability with both model size and training data on MMEB.",
            "score": 19,
            "issue_id": 4592,
            "pub_date": "2025-06-29",
            "pub_date_card": {
                "ru": "29 июня",
                "en": "June 29",
                "zh": "6月29日"
            },
            "hash": "d7fecdae218ccf8e",
            "authors": [
                "Haonan Chen",
                "Hong Liu",
                "Yuping Luo",
                "Liang Wang",
                "Nan Yang",
                "Furu Wei",
                "Zhicheng Dou"
            ],
            "affiliations": [
                "Gaoling School of Artificial Intelligence, Renmin University of China",
                "Microsoft Corporation",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.23115.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#training",
                    "#optimization",
                    "#multimodal",
                    "#alignment",
                    "#architecture"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "MoCa: революция в мультимодальном встраивании",
                    "desc": "MoCa - это двухэтапная система для улучшения предобученных причинно-следственных визуально-языковых моделей для мультимодального встраивания. Она вводит двунаправленное внимание, масштабирование с помощью неразмеченных данных и разнообразные цели обучения. Первый этап включает совместную реконструкцию для улучшения двунаправленных рассуждений с учетом контекста. Второй этап использует разнообразные семантически богатые мультимодальные данные для улучшения обобщения и выравнивания."
                },
                "en": {
                    "title": "MoCa: Enhancing Multimodal Embedding with Bidirectional Attention",
                    "desc": "MoCa is a two-stage framework designed to improve pre-trained causal vision-language models for better multimodal embedding. It addresses limitations in current models, such as the inefficiency of causal attention and the need for high-quality labeled data. The first stage focuses on modality-aware continual pre-training, which enhances understanding by denoising both text and image inputs. The second stage employs heterogeneous contrastive fine-tuning, using diverse multimodal data to improve model generalization and alignment, leading to state-of-the-art performance in various benchmarks."
                },
                "zh": {
                    "title": "MoCa：双向多模态嵌入的创新框架",
                    "desc": "MoCa是一个两阶段框架，旨在增强预训练的因果视觉语言模型在多模态嵌入中的表现。它通过引入双向注意力机制、利用未标记数据进行扩展以及多样化的训练目标来解决现有方法的局限性。第一阶段通过联合重建目标来提高文本和图像输入的去噪能力，增强双向上下文感知推理。第二阶段则利用丰富的多模态数据进行异构对比微调，从而提高模型的泛化能力和对齐效果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.01001",
            "title": "SciArena: An Open Evaluation Platform for Foundation Models in\n  Scientific Literature Tasks",
            "url": "https://huggingface.co/papers/2507.01001",
            "abstract": "SciArena is a community-driven platform for evaluating foundation models on scientific literature tasks, using collective voter judgments to rank models and address the need for reliable automated evaluation.  \t\t\t\t\tAI-generated summary \t\t\t\t We present SciArena, an open and collaborative platform for evaluating foundation models on scientific literature tasks. Unlike traditional benchmarks for scientific literature understanding and synthesis, SciArena engages the research community directly, following the Chatbot Arena evaluation approach of community voting on model comparisons. By leveraging collective intelligence, SciArena offers a community-driven evaluation of model performance on open-ended scientific tasks that demand literature-grounded, long-form responses. The platform currently supports 23 open-source and proprietary foundation models and has collected over 13,000 votes from trusted researchers across diverse scientific domains. We analyze the data collected so far and confirm that the submitted questions are diverse, aligned with real-world literature needs, and that participating researchers demonstrate strong self-consistency and inter-annotator agreement in their evaluations. We discuss the results and insights based on the model ranking leaderboard. To further promote research in building model-based automated evaluation systems for literature tasks, we release SciArena-Eval, a meta-evaluation benchmark based on our collected preference data. The benchmark measures the accuracy of models in judging answer quality by comparing their pairwise assessments with human votes. Our experiments highlight the benchmark's challenges and emphasize the need for more reliable automated evaluation methods.",
            "score": 17,
            "issue_id": 4593,
            "pub_date": "2025-07-01",
            "pub_date_card": {
                "ru": "1 июля",
                "en": "July 1",
                "zh": "7月1日"
            },
            "hash": "f3c20682e2dcf410",
            "authors": [
                "Yilun Zhao",
                "Kaiyan Zhang",
                "Tiansheng Hu",
                "Sihong Wu",
                "Ronan Le Bras",
                "Taira Anderson",
                "Jonathan Bragg",
                "Joseph Chee Chang",
                "Jesse Dodge",
                "Matt Latzke",
                "Yixin Liu",
                "Charles McGrady",
                "Xiangru Tang",
                "Zihang Wang",
                "Chen Zhao",
                "Hannaneh Hajishirzi",
                "Doug Downey",
                "Arman Cohan"
            ],
            "affiliations": [
                "Allen Institute for AI",
                "New York University",
                "Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.01001.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#science",
                    "#benchmark",
                    "#survey",
                    "#dataset"
                ],
                "emoji": "🧪",
                "ru": {
                    "title": "Коллективный разум в оценке ИИ для научной литературы",
                    "desc": "SciArena - это платформа для оценки фундаментальных моделей в задачах работы с научной литературой, использующая коллективные суждения оценщиков для ранжирования моделей. Платформа поддерживает 23 модели с открытым исходным кодом и проприетарные модели, собрав более 13 000 голосов от доверенных исследователей из различных научных областей. Анализ собранных данных подтверждает разнообразие вопросов, их соответствие реальным потребностям литературы и высокую согласованность оценок участвующих исследователей. На основе собранных данных о предпочтениях авторы также выпустили бенчмарк SciArena-Eval для оценки точности моделей в определении качества ответов."
                },
                "en": {
                    "title": "Empowering Scientific Model Evaluation through Community Collaboration",
                    "desc": "SciArena is a collaborative platform designed to evaluate foundation models specifically for tasks related to scientific literature. It utilizes community voting to rank models, moving away from traditional benchmarks and fostering direct engagement from researchers. The platform supports a variety of models and has gathered extensive voting data, demonstrating strong agreement among participants in their evaluations. Additionally, SciArena introduces a meta-evaluation benchmark, SciArena-Eval, to enhance automated evaluation systems by comparing model assessments with human judgments."
                },
                "zh": {
                    "title": "SciArena：科学文献任务的社区评估平台",
                    "desc": "SciArena是一个社区驱动的平台，用于评估基础模型在科学文献任务上的表现。与传统的科学文献理解基准不同，SciArena通过社区投票的方式直接参与研究者，利用集体智慧对模型性能进行评估。该平台支持23个开源和专有的基础模型，并收集了来自不同科学领域的研究者的超过13,000个投票。我们还推出了SciArena-Eval，一个基于收集的偏好数据的元评估基准，旨在促进文献任务的自动评估系统的研究。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.00432",
            "title": "Does Math Reasoning Improve General LLM Capabilities? Understanding\n  Transferability of LLM Reasoning",
            "url": "https://huggingface.co/papers/2507.00432",
            "abstract": "Reinforcement learning-tuned models outperform supervised fine-tuned models in generalizing mathematical problem-solving abilities to other domains, indicating a need to re-evaluate training methods for reasoning models.  \t\t\t\t\tAI-generated summary \t\t\t\t Math reasoning has become the poster child of progress in large language models (LLMs), with new models rapidly surpassing human-level performance on benchmarks like MATH and AIME. But as math leaderboards improve week by week, it is worth asking: do these gains reflect broader problem-solving ability or just narrow overfitting? To answer this question, we evaluate over 20 open-weight reasoning-tuned models across a broad suite of tasks, including math, scientific QA, agent planning, coding, and standard instruction-following. We surprisingly find that most models that succeed in math fail to transfer their gains to other domains. To rigorously study this phenomenon, we conduct controlled experiments on Qwen3-14B models using math-only data but different tuning methods. We find that reinforcement learning (RL)-tuned models generalize well across domains, while supervised fine-tuning (SFT)-tuned models often forget general capabilities. Latent-space representation and token-space distribution shift analyses reveal that SFT induces substantial representation and output drift, while RL preserves general-domain structure. Our results suggest a need to rethink standard post-training recipes, particularly the reliance on SFT-distilled data for advancing reasoning models.",
            "score": 13,
            "issue_id": 4592,
            "pub_date": "2025-07-01",
            "pub_date_card": {
                "ru": "1 июля",
                "en": "July 1",
                "zh": "7月1日"
            },
            "hash": "c4a7e4dd11865858",
            "authors": [
                "Maggie Huan",
                "Yuetai Li",
                "Tuney Zheng",
                "Xiaoyu Xu",
                "Seungone Kim",
                "Minxin Du",
                "Radha Poovendran",
                "Graham Neubig",
                "Xiang Yue"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "M-A-P",
                "The Hong Kong Polytechnic University",
                "University of Pennsylvania",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.00432.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#rl",
                    "#transfer_learning",
                    "#optimization",
                    "#math"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Обучение с подкреплением превосходит обучение с учителем в обобщении навыков решения задач",
                    "desc": "Исследование показывает, что модели, обученные с помощью обучения с подкреплением (RL), лучше обобщают навыки решения математических задач на другие области, чем модели, настроенные с помощью обучения с учителем (SFT). Анализ латентного пространства и распределения токенов выявил, что SFT вызывает значительный сдвиг в представлении и выводе, в то время как RL сохраняет общую структуру. Эксперименты проводились на моделях Qwen3-14B с использованием только математических данных, но разных методов обучения. Результаты указывают на необходимость пересмотра стандартных подходов к обучению моделей рассуждения, особенно в отношении использования данных, полученных с помощью SFT."
                },
                "en": {
                    "title": "Rethinking Training: Reinforcement Learning for Better Generalization",
                    "desc": "This paper investigates the effectiveness of different training methods for reasoning models, particularly in the context of mathematical problem-solving. It finds that models trained with reinforcement learning (RL) outperform those fine-tuned with supervised learning (SFT) when applied to a variety of tasks beyond mathematics. The study reveals that while SFT models excel in math, they struggle to generalize their skills to other domains due to significant representation drift. In contrast, RL-tuned models maintain their general problem-solving abilities, suggesting a need to reconsider current training approaches for reasoning models."
                },
                "zh": {
                    "title": "重新思考推理模型的训练方法",
                    "desc": "这篇论文探讨了强化学习（RL）调优模型在数学问题解决能力上的表现，发现其在其他领域的泛化能力优于监督微调（SFT）模型。研究表明，虽然许多模型在数学任务上表现出色，但它们在其他任务上的迁移能力却较差。通过对Qwen3-14B模型的实验，发现RL调优模型能够在多个领域中保持良好的泛化能力，而SFT模型则容易遗忘其通用能力。结果提示我们需要重新审视现有的训练方法，尤其是对SFT数据的依赖。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.21277",
            "title": "HumanOmniV2: From Understanding to Omni-Modal Reasoning with Context",
            "url": "https://huggingface.co/papers/2506.21277",
            "abstract": "A reinforcement learning-based approach enhances multimodal reasoning by addressing context understanding and shortcut problems, using context, format, accuracy, and logical rewards, and achieving superior performance on the IntentBench benchmark.  \t\t\t\t\tAI-generated summary \t\t\t\t With the rapid evolution of multimodal large language models, the capacity to deeply understand and interpret human intentions has emerged as a critical capability, which demands detailed and thoughtful reasoning. In recent studies, Reinforcement Learning (RL) has demonstrated potential in enhancing the reasoning capabilities of Large Language Models (LLMs). Nonetheless, the challenges associated with adapting RL to multimodal data and formats remain largely unaddressed. In this paper, we identify two issues in existing multimodal reasoning models: insufficient global context understanding and shortcut problems. Insufficient context understanding can happen when a model misinterprets multimodal context, resulting in incorrect answers. The shortcut problem occurs when the model overlooks crucial clues in multimodal inputs, directly addressing the query without considering the multimodal information. To tackle these issues, we emphasize the necessity for the model to reason with a clear understanding of the global context within multimodal inputs. This global context understanding can effectively prevent the model from overlooking key multimodal cues and ensure a thorough reasoning process. To ensure the accurate interpretation of multimodal context information, we implement a context reward judged by a large language model, alongside format and accuracy rewards. Additionally, to improve complex reasoning capability, we employ the LLM to assess the logical reward, determining whether the reasoning process successfully integrates multimodal information with logical methods. We also introduce a reasoning omni-modal benchmark, IntentBench, aimed at evaluating models in understanding complex human intentions and emotions. Our proposed method demonstrates advanced performance across multiple omni-modal benchmarks compared to other open-source omni-modal models.",
            "score": 5,
            "issue_id": 4592,
            "pub_date": "2025-06-26",
            "pub_date_card": {
                "ru": "26 июня",
                "en": "June 26",
                "zh": "6月26日"
            },
            "hash": "15a38ef84e7820fa",
            "authors": [
                "Qize Yang",
                "Shimin Yao",
                "Weixuan Chen",
                "Shenghao Fu",
                "Detao Bai",
                "Jiaxing Zhao",
                "Boyuan Sun",
                "Bowen Yin",
                "Xihan Wei",
                "Jingren Zhou"
            ],
            "affiliations": [
                "Tongyi Lab, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.21277.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#games",
                    "#rl",
                    "#multimodal",
                    "#survey"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Усиление мультимодального рассуждения с помощью обучения с подкреплением",
                    "desc": "Предложен подход на основе обучения с подкреплением для улучшения мультимодального рассуждения в больших языковых моделях. Метод решает проблемы понимания контекста и упрощения, используя контекстные, форматные, точностные и логические награды. Авторы вводят новый бенчмарк IntentBench для оценки понимания сложных человеческих намерений и эмоций. Предложенный метод показывает превосходную производительность на IntentBench по сравнению с другими открытыми мультимодальными моделями."
                },
                "en": {
                    "title": "Enhancing Multimodal Reasoning with Reinforcement Learning",
                    "desc": "This paper presents a reinforcement learning approach to improve multimodal reasoning in large language models. It addresses two main challenges: insufficient understanding of global context and the shortcut problem, where models fail to consider important multimodal cues. By implementing context, format, accuracy, and logical rewards, the model enhances its reasoning capabilities and interprets multimodal inputs more effectively. The proposed method outperforms existing models on the IntentBench benchmark, showcasing its ability to understand complex human intentions and emotions."
                },
                "zh": {
                    "title": "强化学习提升多模态推理能力",
                    "desc": "本论文提出了一种基于强化学习的方法，以增强多模态推理能力，解决上下文理解和捷径问题。我们发现现有多模态推理模型存在全球上下文理解不足和捷径问题，这会导致模型错误解读多模态信息。为了解决这些问题，我们强调模型需要在多模态输入中清晰理解全球上下文，并通过上下文奖励、格式奖励和准确性奖励来确保对多模态信息的准确解读。我们的研究在IntentBench基准测试中表现优异，展示了在理解复杂人类意图和情感方面的先进性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.00951",
            "title": "Thinking Beyond Tokens: From Brain-Inspired Intelligence to Cognitive\n  Foundations for Artificial General Intelligence and its Societal Impact",
            "url": "https://huggingface.co/papers/2507.00951",
            "abstract": "The paper synthesizes the interdisciplinary approach to achieving Artificial General Intelligence, emphasizing modular reasoning, memory, multi-agent coordination, and the integration of neurosymbolic systems and reinforcement learning to overcome current model limitations.  \t\t\t\t\tAI-generated summary \t\t\t\t Can machines truly think, reason and act in domains like humans? This enduring question continues to shape the pursuit of Artificial General Intelligence (AGI). Despite the growing capabilities of models such as GPT-4.5, DeepSeek, Claude 3.5 Sonnet, Phi-4, and Grok 3, which exhibit multimodal fluency and partial reasoning, these systems remain fundamentally limited by their reliance on token-level prediction and lack of grounded agency. This paper offers a cross-disciplinary synthesis of AGI development, spanning artificial intelligence, cognitive neuroscience, psychology, generative models, and agent-based systems. We analyze the architectural and cognitive foundations of general intelligence, highlighting the role of modular reasoning, persistent memory, and multi-agent coordination. In particular, we emphasize the rise of Agentic RAG frameworks that combine retrieval, planning, and dynamic tool use to enable more adaptive behavior. We discuss generalization strategies, including information compression, test-time adaptation, and training-free methods, as critical pathways toward flexible, domain-agnostic intelligence. Vision-Language Models (VLMs) are reexamined not just as perception modules but as evolving interfaces for embodied understanding and collaborative task completion. We also argue that true intelligence arises not from scale alone but from the integration of memory and reasoning: an orchestration of modular, interactive, and self-improving components where compression enables adaptive behavior. Drawing on advances in neurosymbolic systems, reinforcement learning, and cognitive scaffolding, we explore how recent architectures begin to bridge the gap between statistical learning and goal-directed cognition. Finally, we identify key scientific, technical, and ethical challenges on the path to AGI.",
            "score": 3,
            "issue_id": 4593,
            "pub_date": "2025-07-01",
            "pub_date_card": {
                "ru": "1 июля",
                "en": "July 1",
                "zh": "7月1日"
            },
            "hash": "056b6a5007ee5fc2",
            "authors": [
                "Rizwan Qureshi",
                "Ranjan Sapkota",
                "Abbas Shah",
                "Amgad Muneer",
                "Anas Zafar",
                "Ashmal Vayani",
                "Maged Shoman",
                "Abdelrahman B. M. Eldaly",
                "Kai Zhang",
                "Ferhat Sadak",
                "Shaina Raza",
                "Xinqi Fan",
                "Ravid Shwartz-Ziv",
                "Hong Yan",
                "Vinjia Jain",
                "Aman Chadha",
                "Manoj Karkee",
                "Jia Wu",
                "Philip Torr",
                "Seyedali Mirjalili"
            ],
            "affiliations": [
                "Amazon Research (Work done outside Amazon)",
                "Center for Data Science, New York University, NYU, NY, USA",
                "Center for research in Computer Vision, University of Central Florida, Orlando, FL, USA",
                "Centre for Artificial Intelligence Research and Optimization, Torrens University Australia, Fortitude Valley, Brisbane, QLD 4006, Australia",
                "Cornell University, Department of Biological and Environmental Engineering, Ithaca, NY 14853, USA",
                "Department of Electrical Engineering, City University of Hong Kong, SAR China",
                "Department of Electronics Engineering, Mehran University of Engineering & Technology, Jamshoro, Sindh, Pakistan",
                "Department of Engineering Science, University of Oxford, UK",
                "Department of Imaging Physics, The University of Texas MD Anderson Cancer Center, Houston, TX, USA",
                "Department of Mechanical Engineering, Bartin University, Bartin Turkey",
                "Intelligent Transportation Systems, University of Tennessee, Oakridge, TN, USA",
                "Manchester Metropolitan University, Manchester, UK",
                "Meta Research (Work done outside Meta)",
                "University Research and Innovation Center, Obuda University, 1034 Budapest, Hungary",
                "Vector Institute, Toronto Canada"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.00951.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#agi",
                    "#rl",
                    "#architecture",
                    "#multimodal",
                    "#rag",
                    "#ethics",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Путь к AGI: объединяя модульность, память и мультиагентность",
                    "desc": "Статья представляет междисциплинарный подход к достижению искусственного общего интеллекта (AGI). Авторы подчеркивают важность модульного рассуждения, памяти и координации между несколькими агентами. Особое внимание уделяется интеграции нейросимволических систем и обучения с подкреплением для преодоления ограничений современных моделей. В работе также рассматриваются стратегии генерализации и роль мультимодальных моделей в развитии AGI."
                },
                "en": {
                    "title": "Towards True Intelligence: Integrating Memory, Reasoning, and Adaptation for AGI",
                    "desc": "This paper explores the interdisciplinary approach to achieving Artificial General Intelligence (AGI) by integrating concepts from various fields such as cognitive neuroscience and psychology. It emphasizes the importance of modular reasoning, persistent memory, and multi-agent coordination in developing intelligent systems. The authors propose that true intelligence is not just about scaling models but involves the orchestration of memory and reasoning capabilities. They also highlight the potential of neurosymbolic systems and reinforcement learning to create more adaptive and flexible AI agents."
                },
                "zh": {
                    "title": "跨学科推动人工通用智能的实现",
                    "desc": "这篇论文探讨了实现人工通用智能（AGI）的跨学科方法，强调了模块化推理、持久记忆和多智能体协调的重要性。论文分析了通用智能的架构和认知基础，提出了结合检索、规划和动态工具使用的Agentic RAG框架，以实现更灵活的行为。我们还讨论了信息压缩、测试时适应和无训练方法等泛化策略，作为实现灵活、领域无关智能的关键路径。最后，论文指出了在实现AGI过程中面临的科学、技术和伦理挑战。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.20639",
            "title": "DiffuCoder: Understanding and Improving Masked Diffusion Models for Code\n  Generation",
            "url": "https://huggingface.co/papers/2506.20639",
            "abstract": "Diffusion large language models are applied to code generation, revealing their unique denoising processes and benefiting from a novel reinforcement learning sampling scheme.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion large language models (dLLMs) are compelling alternatives to autoregressive (AR) models because their denoising models operate over the entire sequence. The global planning and iterative refinement features of dLLMs are particularly useful for code generation. However, current training and inference mechanisms for dLLMs in coding are still under-explored. To demystify the decoding behavior of dLLMs and unlock their potential for coding, we systematically investigate their denoising processes and reinforcement learning (RL) methods. We train a 7B dLLM, DiffuCoder, on 130B tokens of code. Using this model as a testbed, we analyze its decoding behavior, revealing how it differs from that of AR models: (1) dLLMs can decide how causal their generation should be without relying on semi-AR decoding, and (2) increasing the sampling temperature diversifies not only token choices but also their generation order. This diversity creates a rich search space for RL rollouts. For RL training, to reduce the variance of token log-likelihood estimates and maintain training efficiency, we propose coupled-GRPO, a novel sampling scheme that constructs complementary mask noise for completions used in training. In our experiments, coupled-GRPO significantly improves DiffuCoder's performance on code generation benchmarks (+4.4\\% on EvalPlus) and reduces reliance on AR causal during decoding. Our work provides deeper insight into the machinery of dLLM generation and offers an effective, diffusion-native RL training framework. https://github.com/apple/ml-diffucoder.",
            "score": 3,
            "issue_id": 4593,
            "pub_date": "2025-06-25",
            "pub_date_card": {
                "ru": "25 июня",
                "en": "June 25",
                "zh": "6月25日"
            },
            "hash": "20d886d0a4cd5bb6",
            "authors": [
                "Shansan Gong",
                "Ruixiang Zhang",
                "Huangjie Zheng",
                "Jiatao Gu",
                "Navdeep Jaitly",
                "Lingpeng Kong",
                "Yizhe Zhang"
            ],
            "affiliations": [
                "Apple",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.20639.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#architecture",
                    "#optimization",
                    "#dataset",
                    "#diffusion"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Раскрытие потенциала диффузионных языковых моделей для генерации кода",
                    "desc": "Статья исследует применение диффузионных языковых моделей (dLLM) для генерации кода. Авторы анализируют уникальные процессы шумоподавления в dLLM и их отличия от авторегрессивных моделей. Они предлагают новый метод обучения с подкреплением под названием coupled-GRPO для улучшения производительности. Эксперименты показывают значительное повышение эффективности генерации кода с использованием предложенного подхода."
                },
                "en": {
                    "title": "Unlocking Code Generation with Diffusion Models",
                    "desc": "This paper explores the use of diffusion large language models (dLLMs) for code generation, highlighting their unique denoising processes compared to traditional autoregressive models. The authors introduce a novel reinforcement learning sampling scheme called coupled-GRPO, which enhances the training efficiency and performance of the dLLM named DiffuCoder. By analyzing the decoding behavior of DiffuCoder, the study reveals that dLLMs can flexibly adjust their generation strategies and improve diversity in output. The findings demonstrate that dLLMs have significant potential for coding tasks, providing a new framework for effective code generation."
                },
                "zh": {
                    "title": "扩散大语言模型：代码生成的新选择",
                    "desc": "本文探讨了扩散大语言模型（dLLMs）在代码生成中的应用，揭示了其独特的去噪过程。dLLMs与自回归模型相比，能够在整个序列上进行去噪，具有全球规划和迭代优化的特点，特别适合代码生成。我们提出了一种新颖的强化学习采样方案coupled-GRPO，以提高训练效率并减少标记对数估计的方差。实验结果表明，coupled-GRPO显著提升了DiffuCoder在代码生成基准上的表现，并减少了对自回归因果解码的依赖。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.22960",
            "title": "Peccavi: Visual Paraphrase Attack Safe and Distortion Free Image\n  Watermarking Technique for AI-Generated Images",
            "url": "https://huggingface.co/papers/2506.22960",
            "abstract": "PECCAVI is a robust image watermarking technique that is resistant to visual paraphrase attacks and distortions, utilizing NMPs and multi-channel frequency domain watermarking.  \t\t\t\t\tAI-generated summary \t\t\t\t A report by the European Union Law Enforcement Agency predicts that by 2026, up to 90 percent of online content could be synthetically generated, raising concerns among policymakers, who cautioned that \"Generative AI could act as a force multiplier for political disinformation. The combined effect of generative text, images, videos, and audio may surpass the influence of any single modality.\" In response, California's Bill AB 3211 mandates the watermarking of AI-generated images, videos, and audio. However, concerns remain regarding the vulnerability of invisible watermarking techniques to tampering and the potential for malicious actors to bypass them entirely. Generative AI-powered de-watermarking attacks, especially the newly introduced visual paraphrase attack, have shown an ability to fully remove watermarks, resulting in a paraphrase of the original image. This paper introduces PECCAVI, the first visual paraphrase attack-safe and distortion-free image watermarking technique. In visual paraphrase attacks, an image is altered while preserving its core semantic regions, termed Non-Melting Points (NMPs). PECCAVI strategically embeds watermarks within these NMPs and employs multi-channel frequency domain watermarking. It also incorporates noisy burnishing to counter reverse-engineering efforts aimed at locating NMPs to disrupt the embedded watermark, thereby enhancing durability. PECCAVI is model-agnostic. All relevant resources and codes will be open-sourced.",
            "score": 1,
            "issue_id": 4593,
            "pub_date": "2025-06-28",
            "pub_date_card": {
                "ru": "28 июня",
                "en": "June 28",
                "zh": "6月28日"
            },
            "hash": "c946e3ac9bf6133a",
            "authors": [
                "Shreyas Dixit",
                "Ashhar Aziz",
                "Shashwat Bajpai",
                "Vasu Sharma",
                "Aman Chadha",
                "Vinija Jain",
                "Amitava Das"
            ],
            "affiliations": [
                "AI Institute, University of South Carolina, USA",
                "Amazon GenAI, USA",
                "BITS Pilani Hyderabad, India",
                "IIIT Delhi, India",
                "Meta AI, USA",
                "Stanford University, USA",
                "VIIT Pune, India"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.22960.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#synthetic",
                    "#data",
                    "#open_source",
                    "#multimodal",
                    "#cv"
                ],
                "emoji": "🔐",
                "ru": {
                    "title": "Непобедимые водяные знаки для эпохи генеративного ИИ",
                    "desc": "PECCAVI - это устойчивая техника водяных знаков для изображений, которая противостоит атакам визуального перефразирования и искажениям. Она использует неизменяемые точки (NMP) и многоканальное встраивание водяных знаков в частотной области. PECCAVI стратегически размещает водяные знаки в NMP и применяет шумовую полировку для противодействия обратному инжинирингу. Эта техника является моделенезависимой и обещает быть открытым исходным кодом."
                },
                "en": {
                    "title": "PECCAVI: Watermarking Resilience Against Visual Paraphrase Attacks",
                    "desc": "PECCAVI is a novel image watermarking technique designed to withstand visual paraphrase attacks and various distortions. It utilizes Non-Melting Points (NMPs) to strategically embed watermarks, ensuring that the essential features of the image remain intact. The method employs multi-channel frequency domain watermarking and incorporates noisy burnishing to protect against reverse-engineering attempts. This approach is model-agnostic, making it applicable across different systems, and all resources will be made available to the public."
                },
                "zh": {
                    "title": "PECCAVI：抵御视觉改写的水印新技术",
                    "desc": "PECCAVI是一种强大的图像水印技术，能够抵御视觉改写攻击和失真。它利用非熔化点（NMPs）和多通道频域水印技术，将水印嵌入图像的核心语义区域。该技术还采用了噪声烧灼方法，以防止逆向工程攻击，增强水印的耐久性。PECCAVI不依赖于特定模型，所有相关资源和代码将开源。"
                }
            }
        }
    ],
    "link_prev": "2025-07-01.html",
    "link_next": "2025-07-03.html",
    "link_month": "2025-07.html",
    "short_date_prev": {
        "ru": "01.07",
        "en": "07/01",
        "zh": "7月1日"
    },
    "short_date_next": {
        "ru": "03.07",
        "en": "07/03",
        "zh": "7月3日"
    },
    "categories": {
        "#dataset": 3,
        "#data": 1,
        "#benchmark": 3,
        "#agents": 1,
        "#cv": 1,
        "#rl": 4,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 4,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 3,
        "#robotics": 0,
        "#agi": 1,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 3,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 1,
        "#optimization": 3,
        "#survey": 2,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    }
}