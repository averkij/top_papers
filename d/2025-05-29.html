
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 47 papers. May 29.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">29 мая</span> | <span id="title-articles-count">47 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-05-28.html">⬅️ <span id="prev-date">28.05</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-05-30.html">➡️ <span id="next-date">30.05</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-05.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'};
        let feedDateNext = {'ru': '30.05', 'en': '05/30', 'zh': '5月30日'};
        let feedDatePrev = {'ru': '28.05', 'en': '05/28', 'zh': '5月28日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2505.22617', 'title': 'The Entropy Mechanism of Reinforcement Learning for Reasoning Language\n  Models', 'url': 'https://huggingface.co/papers/2505.22617', 'abstract': 'This paper aims to overcome a major obstacle in scaling RL for reasoning with LLMs, namely the collapse of policy entropy. Such phenomenon is consistently observed across vast RL runs without entropy intervention, where the policy entropy dropped sharply at the early training stage, this diminished exploratory ability is always accompanied with the saturation of policy performance. In practice, we establish a transformation equation R=-a*e^H+b between entropy H and downstream performance R. This empirical law strongly indicates that, the policy performance is traded from policy entropy, thus bottlenecked by its exhaustion, and the ceiling is fully predictable H=0, R=-a+b. Our finding necessitates entropy management for continuous exploration toward scaling compute for RL. To this end, we investigate entropy dynamics both theoretically and empirically. Our derivation highlights that, the change in policy entropy is driven by the covariance between action probability and the change in logits, which is proportional to its advantage when using Policy Gradient-like algorithms. Empirical study shows that, the values of covariance term and entropy differences matched exactly, supporting the theoretical conclusion. Moreover, the covariance term stays mostly positive throughout training, further explaining why policy entropy would decrease monotonically. Through understanding the mechanism behind entropy dynamics, we motivate to control entropy by restricting the update of high-covariance tokens. Specifically, we propose two simple yet effective techniques, namely Clip-Cov and KL-Cov, which clip and apply KL penalty to tokens with high covariances respectively. Experiments show that these methods encourage exploration, thus helping policy escape entropy collapse and achieve better downstream performance.', 'score': 79, 'issue_id': 4014, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 мая', 'en': 'May 28', 'zh': '5月28日'}, 'hash': '7313a363374c80e6', 'authors': ['Ganqu Cui', 'Yuchen Zhang', 'Jiacheng Chen', 'Lifan Yuan', 'Zhi Wang', 'Yuxin Zuo', 'Haozhan Li', 'Yuchen Fan', 'Huayu Chen', 'Weize Chen', 'Zhiyuan Liu', 'Hao Peng', 'Lei Bai', 'Wanli Ouyang', 'Yu Cheng', 'Bowen Zhou', 'Ning Ding'], 'affiliations': ['CUHK', 'Nanjing University', 'Peking University', 'Shanghai AI Laboratory', 'Tsinghua University', 'UIUC'], 'pdf_title_img': 'assets/pdf/title_img/2505.22617.jpg', 'data': {'categories': ['#training', '#rl', '#optimization', '#reasoning'], 'emoji': '🔬', 'ru': {'title': 'Управление энтропией для масштабирования обучения с подкреплением в больших языковых моделях', 'desc': 'Статья рассматривает проблему снижения энтропии политики при масштабировании обучения с подкреплением для рассуждений с использованием больших языковых моделей. Авторы устанавливают эмпирическую зависимость между энтропией и производительностью модели, подчеркивая необходимость управления энтропией для непрерывного исследования. Исследуется динамика энтропии теоретически и эмпирически, выявляя механизмы, стоящие за ее изменением. Предлагаются два метода - Clip-Cov и KL-Cov - для контроля энтропии путем ограничения обновления токенов с высокой ковариацией, что способствует улучшению исследования и производительности модели.'}, 'en': {'title': 'Managing Entropy for Enhanced Exploration in RL', 'desc': 'This paper addresses the issue of policy entropy collapse in reinforcement learning (RL) when applied to large language models (LLMs). It identifies that as training progresses, the policy entropy decreases significantly, leading to reduced exploration and stagnation in policy performance. The authors establish a relationship between entropy and performance, suggesting that managing entropy is crucial for effective exploration in RL. They propose two techniques, Clip-Cov and KL-Cov, to control high-covariance tokens, which help maintain policy entropy and improve overall performance.'}, 'zh': {'title': '控制熵以促进探索，提升强化学习性能', 'desc': '本文旨在克服在大规模强化学习（RL）中与大型语言模型（LLM）推理相关的一个主要障碍，即策略熵的崩溃。我们观察到，在没有熵干预的情况下，策略熵在训练初期急剧下降，导致探索能力减弱，并伴随策略性能的饱和。我们建立了熵与下游性能之间的变换方程，表明策略性能与策略熵之间存在权衡关系，因此熵的耗尽成为瓶颈。为了解决这一问题，我们提出了两种简单有效的技术，Clip-Cov和KL-Cov，旨在通过限制高协方差标记的更新来控制熵，从而鼓励探索并改善策略性能。'}}}, {'id': 'https://huggingface.co/papers/2505.21600', 'title': 'R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large\n  Model Token Routing', 'url': 'https://huggingface.co/papers/2505.21600', 'abstract': "Roads to Rome (R2R) selectively utilizes large language models for critical reasoning tasks to enhance efficiency and performance in lightweight models.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) achieve impressive reasoning capabilities at the cost of substantial inference overhead, posing substantial deployment challenges. Although distilled Small Language Models (SLMs) significantly enhance efficiency, their performance suffers as they fail to follow LLMs' reasoning paths. Luckily, we reveal that only a small fraction of tokens genuinely diverge reasoning paths between LLMs and SLMs. Most generated tokens are either identical or exhibit neutral differences, such as minor variations in abbreviations or expressions. Leveraging this insight, we introduce **Roads to Rome (R2R)**, a neural token routing method that selectively utilizes LLMs only for these critical, path-divergent tokens, while leaving the majority of token generation to the SLM. We also develop an automatic data generation pipeline that identifies divergent tokens and generates token-level routing labels to train the lightweight router. We apply R2R to combine R1-1.5B and R1-32B models from the DeepSeek family, and evaluate on challenging math, coding, and QA benchmarks. With an average activated parameter size of 5.6B, R2R surpasses the average accuracy of R1-7B by 1.6x, outperforming even the R1-14B model. Compared to R1-32B, it delivers a 2.8x wall-clock speedup with comparable performance, advancing the Pareto frontier of test-time scaling efficiency. Our code is available at https://github.com/thu-nics/R2R.", 'score': 53, 'issue_id': 4013, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': '54850077437e9524', 'authors': ['Tianyu Fu', 'Yi Ge', 'Yichen You', 'Enshu Liu', 'Zhihang Yuan', 'Guohao Dai', 'Shengen Yan', 'Huazhong Yang', 'Yu Wang'], 'affiliations': ['Infinigence AI', 'Shanghai Jiao Tong University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.21600.jpg', 'data': {'categories': ['#small_models', '#architecture', '#optimization', '#reasoning', '#benchmark', '#math', '#training'], 'emoji': '🛣️', 'ru': {'title': 'Эффективное сочетание больших и малых языковых моделей для улучшения рассуждений', 'desc': 'Статья представляет метод Roads to Rome (R2R), который селективно использует большие языковые модели (LLM) для критических задач рассуждения, улучшая эффективность и производительность легких моделей. R2R применяет нейронную маршрутизацию токенов, задействуя LLM только для ключевых, расходящихся токенов, оставляя большую часть генерации малым языковым моделям (SLM). Авторы разработали автоматический конвейер генерации данных для идентификации расходящихся токенов и создания меток маршрутизации. Метод R2R показал значительное улучшение точности и скорости по сравнению с базовыми моделями на сложных задачах математики, программирования и вопросно-ответных систем.'}, 'en': {'title': 'Efficient Reasoning with Selective Token Routing', 'desc': 'The paper introduces Roads to Rome (R2R), a method that enhances the efficiency of lightweight models by selectively using large language models (LLMs) for critical reasoning tasks. It identifies that only a small number of tokens significantly affect the reasoning paths between LLMs and distilled small language models (SLMs). By focusing on these path-divergent tokens, R2R allows SLMs to handle the majority of token generation, improving overall performance without the heavy computational cost of LLMs. The results show that R2R achieves higher accuracy and faster processing times compared to existing models, pushing the boundaries of efficiency in machine learning applications.'}, 'zh': {'title': '高效推理的新路径：R2R方法', 'desc': '本文介绍了一种名为Roads to Rome (R2R)的方法，该方法通过选择性地利用大型语言模型（LLMs）来处理关键推理任务，从而提高轻量级模型的效率和性能。研究发现，LLMs和小型语言模型（SLMs）之间的推理路径仅在少数标记上存在显著差异，大多数生成的标记要么相同，要么差异微小。基于这一发现，R2R方法仅在关键的路径分歧标记上使用LLMs，而将大部分标记生成留给SLMs，从而实现了高效的推理。通过在多个数学、编码和问答基准上进行评估，R2R在保持性能的同时显著提高了计算速度和准确性。'}}}, {'id': 'https://huggingface.co/papers/2505.20411', 'title': 'SWE-rebench: An Automated Pipeline for Task Collection and\n  Decontaminated Evaluation of Software Engineering Agents', 'url': 'https://huggingface.co/papers/2505.20411', 'abstract': 'A novel pipeline extracts real-world, interactive software engineering tasks from GitHub to create SWE-rebench, improving the evaluation of reinforcement learning models in SWE.  \t\t\t\t\tAI-generated summary \t\t\t\t LLM-based agents have shown promising capabilities in a growing range of software engineering (SWE) tasks. However, advancing this field faces two critical challenges. First, high-quality training data is scarce, especially data that reflects real-world SWE scenarios, where agents must interact with development environments, execute code and adapt behavior based on the outcomes of their actions. Existing datasets are either limited to one-shot code generation or comprise small, manually curated collections of interactive tasks, lacking both scale and diversity. Second, the lack of fresh interactive SWE tasks affects evaluation of rapidly improving models, as static benchmarks quickly become outdated due to contamination issues. To address these limitations, we introduce a novel, automated, and scalable pipeline to continuously extract real-world interactive SWE tasks from diverse GitHub repositories. Using this pipeline, we construct SWE-rebench, a public dataset comprising over 21,000 interactive Python-based SWE tasks, suitable for reinforcement learning of SWE agents at scale. Additionally, we use continuous supply of fresh tasks collected using SWE-rebench methodology to build a contamination-free benchmark for agentic software engineering. We compare results of various LLMs on this benchmark to results on SWE-bench Verified and show that performance of some language models might be inflated due to contamination issues.', 'score': 49, 'issue_id': 4020, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 мая', 'en': 'May 26', 'zh': '5月26日'}, 'hash': '223845bb2e40cbe5', 'authors': ['Ibragim Badertdinov', 'Alexander Golubev', 'Maksim Nekrashevich', 'Anton Shevtsov', 'Simon Karasik', 'Andrei Andriushchenko', 'Maria Trofimova', 'Daria Litvintseva', 'Boris Yangel'], 'affiliations': ['Nebius'], 'pdf_title_img': 'assets/pdf/title_img/2505.20411.jpg', 'data': {'categories': ['#benchmark', '#transfer_learning', '#data', '#open_source', '#games', '#rl', '#dataset'], 'emoji': '🤖', 'ru': {'title': 'Автоматизированное создание датасетов для обучения ИИ-ассистентов в разработке ПО', 'desc': 'Статья представляет новый подход к созданию датасета для обучения моделей машинного обучения в области разработки программного обеспечения. Авторы разработали автоматизированный конвейер для извлечения реальных интерактивных задач из репозиториев GitHub. Результатом стал датасет SWE-rebench, содержащий более 21 000 интерактивных задач на Python, пригодных для обучения с подкреплением. Данный подход также позволяет создавать незагрязненные бенчмарки для оценки языковых моделей в задачах разработки ПО.'}, 'en': {'title': 'Revolutionizing Software Engineering Evaluation with SWE-rebench', 'desc': 'This paper presents a new method for extracting real-world software engineering tasks from GitHub, creating a dataset called SWE-rebench. This dataset contains over 21,000 interactive Python tasks, which are essential for training reinforcement learning models in software engineering. The authors highlight the importance of having diverse and up-to-date tasks to evaluate the performance of these models accurately. By addressing the challenges of data scarcity and contamination in benchmarks, this work aims to enhance the development and assessment of AI agents in software engineering.'}, 'zh': {'title': '构建真实交互任务，提升软件工程模型评估', 'desc': '本文提出了一种新颖的管道，从GitHub提取真实世界的交互式软件工程任务，以创建SWE-rebench，从而改善强化学习模型在软件工程中的评估。当前，软件工程任务的高质量训练数据稀缺，尤其是能够反映真实开发环境的交互式任务。通过自动化和可扩展的方式，我们构建了一个包含超过21,000个交互式Python软件工程任务的公共数据集，适合大规模强化学习。我们还利用SWE-rebench方法收集的新任务，建立了一个无污染的基准，以便更准确地评估软件工程代理的性能。'}}}, {'id': 'https://huggingface.co/papers/2505.22651', 'title': 'Sherlock: Self-Correcting Reasoning in Vision-Language Models', 'url': 'https://huggingface.co/papers/2505.22651', 'abstract': "Sherlock, a self-correction and self-improvement framework for reasoning vision-language models, enhances accuracy across benchmarks using limited annotated data.  \t\t\t\t\tAI-generated summary \t\t\t\t Reasoning Vision-Language Models (VLMs) have shown promising performance on complex multimodal tasks. However, they still face significant challenges: they are highly sensitive to reasoning errors, require large volumes of annotated data or accurate verifiers, and struggle to generalize beyond specific domains. To address these limitations, we explore self-correction as a strategy to enhance reasoning VLMs. We first conduct an in-depth analysis of reasoning VLMs' self-correction abilities and identify key gaps. Based on our findings, we introduce Sherlock, a self-correction and self-improvement training framework. Sherlock introduces a trajectory-level self-correction objective, a preference data construction method based on visual perturbation, and a dynamic beta for preference tuning. Once the model acquires self-correction capabilities using only 20k randomly sampled annotated data, it continues to self-improve without external supervision. Built on the Llama3.2-Vision-11B model, Sherlock achieves remarkable results across eight benchmarks, reaching an average accuracy of 64.1 with direct generation and 65.4 after self-correction. It outperforms LLaVA-CoT (63.2), Mulberry (63.9), and LlamaV-o1 (63.4) while using less than 20% of the annotated data.", 'score': 43, 'issue_id': 4019, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 мая', 'en': 'May 28', 'zh': '5月28日'}, 'hash': '39e5a06798255d74', 'authors': ['Yi Ding', 'Ruqi Zhang'], 'affiliations': ['Department of Computer Science, Purdue University, USA'], 'pdf_title_img': 'assets/pdf/title_img/2505.22651.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#training', '#multimodal', '#reasoning'], 'emoji': '🕵️', 'ru': {'title': 'Самокоррекция VLM: меньше данных, выше точность', 'desc': 'Статья представляет Sherlock - фреймворк для самокоррекции и самосовершенствования моделей зрительно-языкового рассуждения (VLM). Sherlock использует траекторную самокоррекцию, визуальные возмущения для создания предпочтительных данных и динамическую бета-настройку. Модель, обученная на всего 20 тысячах аннотированных примеров, демонстрирует впечатляющие результаты на восьми бенчмарках. Sherlock превосходит другие современные модели, используя значительно меньше размеченных данных.'}, 'en': {'title': 'Sherlock: Self-Correction for Smarter Vision-Language Models', 'desc': 'The paper presents Sherlock, a framework designed to enhance reasoning in vision-language models (VLMs) through self-correction and self-improvement. It addresses the challenges of high sensitivity to reasoning errors and the need for extensive annotated data by introducing a trajectory-level self-correction objective and a method for constructing preference data. Sherlock allows the model to improve its reasoning capabilities using only a small amount of annotated data, achieving significant accuracy improvements across multiple benchmarks. The results show that Sherlock outperforms existing models while utilizing less than 20% of the required annotated data, demonstrating its efficiency and effectiveness in enhancing VLM performance.'}, 'zh': {'title': 'Sherlock：推理模型的自我修正与提升', 'desc': '本论文提出了Sherlock，一个用于推理视觉语言模型的自我修正和自我改进框架。Sherlock通过引入轨迹级自我修正目标和基于视觉扰动的偏好数据构建方法，显著提高了模型的准确性。该框架在仅使用2万条随机采样的标注数据的情况下，能够实现自我改进，且无需外部监督。实验结果表明，Sherlock在多个基准测试中表现优异，平均准确率达到64.1，且在自我修正后提升至65.4，超越了其他模型。'}}}, {'id': 'https://huggingface.co/papers/2505.22312', 'title': 'Skywork Open Reasoner 1 Technical Report', 'url': 'https://huggingface.co/papers/2505.22312', 'abstract': 'The success of DeepSeek-R1 underscores the significant role of reinforcement learning (RL) in enhancing the reasoning capabilities of large language models (LLMs). In this work, we present Skywork-OR1, an effective and scalable RL implementation for long Chain-of-Thought (CoT) models. Building on the DeepSeek-R1-Distill model series, our RL approach achieves notable performance gains, increasing average accuracy across AIME24, AIME25, and LiveCodeBench from 57.8% to 72.8% (+15.0%) for the 32B model and from 43.6% to 57.5% (+13.9%) for the 7B model. Our Skywork-OR1-32B model surpasses both DeepSeek-R1 and Qwen3-32B on the AIME24 and AIME25 benchmarks, while achieving comparable results on LiveCodeBench. The Skywork-OR1-7B and Skywork-OR1-Math-7B models demonstrate competitive reasoning capabilities among models of similar size. We perform comprehensive ablation studies on the core components of our training pipeline to validate their effectiveness. Additionally, we thoroughly investigate the phenomenon of entropy collapse, identify key factors affecting entropy dynamics, and demonstrate that mitigating premature entropy collapse is critical for improved test performance. To support community research, we fully open-source our model weights, training code, and training datasets.', 'score': 43, 'issue_id': 4013, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 мая', 'en': 'May 28', 'zh': '5月28日'}, 'hash': '712fb8daefb54030', 'authors': ['Jujie He', 'Jiacai Liu', 'Chris Yuhao Liu', 'Rui Yan', 'Chaojie Wang', 'Peng Cheng', 'Xiaoyu Zhang', 'Fuxiang Zhang', 'Jiacheng Xu', 'Wei Shen', 'Siyuan Li', 'Liang Zeng', 'Tianwen Wei', 'Cheng Cheng', 'Bo An', 'Yang Liu', 'Yahui Zhou'], 'affiliations': ['Skywork AI, Kunlun Inc'], 'pdf_title_img': 'assets/pdf/title_img/2505.22312.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#rl', '#benchmark', '#open_source', '#training'], 'emoji': '🧠', 'ru': {'title': 'Усиление рассуждений языковых моделей с помощью обучения с подкреплением', 'desc': 'Статья представляет Skywork-OR1, эффективную реализацию обучения с подкреплением для улучшения способностей рассуждения больших языковых моделей. Авторы демонстрируют значительное повышение точности на нескольких бенчмарках по сравнению с базовыми моделями. Исследование включает подробный анализ компонентов обучающего пайплайна и феномена коллапса энтропии. Авторы открыто публикуют веса моделей, код и наборы данных для поддержки исследовательского сообщества.'}, 'en': {'title': 'Boosting Reasoning in Language Models with Reinforcement Learning', 'desc': 'This paper introduces Skywork-OR1, a reinforcement learning (RL) framework designed to improve the reasoning abilities of large language models (LLMs) through long Chain-of-Thought (CoT) processes. By building on the previous DeepSeek-R1-Distill models, Skywork-OR1 achieves significant accuracy improvements on various benchmarks, with the 32B model increasing from 57.8% to 72.8%. The study also addresses the issue of entropy collapse, highlighting its impact on model performance and the importance of managing it during training. The authors provide open access to their model weights and training resources to foster further research in the community.'}, 'zh': {'title': '强化学习提升语言模型推理能力的突破', 'desc': 'DeepSeek-R1的成功表明强化学习在提升大型语言模型的推理能力方面的重要性。我们提出了Skywork-OR1，这是一个有效且可扩展的强化学习实现，专为长链思维模型设计。通过对DeepSeek-R1-Distill模型系列的改进，我们的强化学习方法在多个基准测试中显著提高了准确率。我们还进行了全面的消融研究，验证了训练流程中核心组件的有效性，并探讨了熵崩溃现象及其对测试性能的影响。'}}}, {'id': 'https://huggingface.co/papers/2505.22453', 'title': 'Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO', 'url': 'https://huggingface.co/papers/2505.22453', 'abstract': 'Improving Multi-modal Large Language Models (MLLMs) in the post-training stage typically relies on supervised fine-tuning (SFT) or reinforcement learning (RL). However, these supervised methods require expensive and manually annotated multi-modal data--an ultimately unsustainable resource. While recent efforts have explored unsupervised post-training, their methods are complex and difficult to iterate. In this work, we are the first to investigate the use of GRPO, a stable and scalable online RL algorithm, for enabling continual self-improvement without any external supervision. We propose MM-UPT, a simple yet effective framework for unsupervised post-training of MLLMs. MM-UPT builds upon GRPO, replacing traditional reward signals with a self-rewarding mechanism based on majority voting over multiple sampled responses. Our experiments demonstrate that MM-UPT significantly improves the reasoning ability of Qwen2.5-VL-7B (e.g., 66.3 %rightarrow72.9 % on MathVista, 62.9 %rightarrow68.7 % on We-Math), using standard dataset without ground truth labels. MM-UPT also outperforms prior unsupervised baselines and even approaches the results of supervised GRPO. Furthermore, we show that incorporating synthetic questions, generated solely by MLLM itself, can boost performance as well, highlighting a promising approach for scalable self-improvement. Overall, MM-UPT offers a new paradigm for continual, autonomous enhancement of MLLMs in the absence of external supervision. Our code is available at https://github.com/waltonfuture/MM-UPT.', 'score': 34, 'issue_id': 4013, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 мая', 'en': 'May 28', 'zh': '5月28日'}, 'hash': '96b8c222cef41b46', 'authors': ['Lai Wei', 'Yuting Li', 'Chen Wang', 'Yue Wang', 'Linghe Kong', 'Weiran Huang', 'Lichao Sun'], 'affiliations': ['Lehigh University', 'School of Computer Science, Shanghai Jiao Tong University', 'Shanghai Innovation Institute', 'Zhongguancun Academy'], 'pdf_title_img': 'assets/pdf/title_img/2505.22453.jpg', 'data': {'categories': ['#reasoning', '#rl', '#rlhf', '#multimodal', '#training', '#synthetic'], 'emoji': '🤖', 'ru': {'title': 'Самосовершенствование ИИ без учителя', 'desc': 'Статья представляет новый метод MM-UPT для улучшения мультимодальных больших языковых моделей (MLLM) без использования размеченных данных. Авторы применяют алгоритм обучения с подкреплением GRPO, заменяя традиционные сигналы вознаграждения механизмом самовознаграждения на основе голосования. Эксперименты показывают значительное улучшение способности рассуждать у модели Qwen2.5-VL-7B на различных задачах. MM-UPT превосходит предыдущие неконтролируемые методы и приближается к результатам контролируемого GRPO, открывая новую парадигму для автономного улучшения MLLM.'}, 'en': {'title': 'Autonomous Self-Improvement for MLLMs with MM-UPT', 'desc': 'This paper introduces MM-UPT, a novel framework for improving Multi-modal Large Language Models (MLLMs) without the need for expensive supervised fine-tuning. It leverages a stable online reinforcement learning algorithm called GRPO, which allows for continual self-improvement through a self-rewarding mechanism based on majority voting of responses. The experiments show that MM-UPT enhances the reasoning capabilities of the Qwen2.5-VL-7B model significantly, even outperforming previous unsupervised methods. Additionally, the incorporation of synthetic questions generated by the MLLM itself further boosts performance, suggesting a scalable approach for autonomous model enhancement.'}, 'zh': {'title': '无监督自我改进的多模态语言模型新框架', 'desc': '本研究提出了一种新的框架MM-UPT，用于多模态大型语言模型（MLLMs）的无监督后训练。与传统的监督微调方法不同，MM-UPT利用了一种基于多数投票的自我奖励机制，避免了对昂贵标注数据的依赖。通过实验，我们发现MM-UPT显著提高了模型的推理能力，并且在无监督的情况下表现优于之前的方法。该方法还展示了通过生成合成问题来进一步提升性能的潜力，为MLLMs的持续自我改进提供了新的思路。'}}}, {'id': 'https://huggingface.co/papers/2505.21136', 'title': 'SageAttention2++: A More Efficient Implementation of SageAttention2', 'url': 'https://huggingface.co/papers/2505.21136', 'abstract': 'The efficiency of attention is critical because its time complexity grows quadratically with sequence length. SageAttention2 addresses this by utilizing quantization to accelerate matrix multiplications (Matmul) in attention. To further accelerate SageAttention2, we propose to utilize the faster instruction of FP8 Matmul accumulated in FP16. The instruction is 2x faster than the FP8 Matmul used in SageAttention2. Our experiments show that SageAttention2++ achieves a 3.9x speedup over FlashAttention while maintaining the same attention accuracy as SageAttention2. This means SageAttention2++ effectively accelerates various models, including those for language, image, and video generation, with negligible end-to-end metrics loss. The code will be available at https://github.com/thu-ml/SageAttention.', 'score': 29, 'issue_id': 4014, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': 'd26c2e844cce91c6', 'authors': ['Jintao Zhang', 'Xiaoming Xu', 'Jia Wei', 'Haofeng Huang', 'Pengle Zhang', 'Chendong Xiang', 'Jun Zhu', 'Jianfei Chen'], 'affiliations': ['Department of Computer Science, Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.21136.jpg', 'data': {'categories': ['#architecture', '#inference'], 'emoji': '🚀', 'ru': {'title': 'Ускорение механизма внимания без потери точности', 'desc': 'SageAttention2++ - это улучшенная версия SageAttention2, которая использует квантизацию для ускорения матричных умножений в механизме внимания. Основное нововведение заключается в использовании более быстрой инструкции FP8 Matmul с накоплением в FP16, что в два раза быстрее, чем FP8 Matmul в SageAttention2. Эксперименты показывают, что SageAttention2++ достигает 3.9-кратного ускорения по сравнению с FlashAttention, сохраняя при этом точность внимания на уровне SageAttention2. Это позволяет эффективно ускорять различные модели для обработки языка, изображений и видео с минимальными потерями в метриках.'}, 'en': {'title': 'Accelerating Attention with SageAttention2++', 'desc': 'This paper introduces SageAttention2++, an improved version of the SageAttention2 model that enhances the efficiency of attention mechanisms in machine learning. It achieves this by implementing quantization techniques to speed up matrix multiplications, which are crucial for attention calculations. Additionally, it leverages a faster FP8 Matmul instruction that is twice as fast as the previous FP8 Matmul used in SageAttention2. The results demonstrate that SageAttention2++ provides a significant speedup while preserving the accuracy of attention, making it suitable for various applications in language, image, and video generation.'}, 'zh': {'title': '加速注意力机制，提升计算效率！', 'desc': '本文提出了一种新的注意力机制SageAttention2++，旨在提高计算效率。通过量化技术加速矩阵乘法，SageAttention2++在保持相同注意力准确度的同时，实现了3.9倍的速度提升。我们还利用FP8矩阵乘法的更快指令，进一步加速了计算过程。该方法适用于语言、图像和视频生成等多种模型，且几乎没有损失端到端的性能指标。'}}}, {'id': 'https://huggingface.co/papers/2505.22457', 'title': 'Fostering Video Reasoning via Next-Event Prediction', 'url': 'https://huggingface.co/papers/2505.22457', 'abstract': 'Next-event prediction (NEP) is proposed as a learning task to enable MLLMs to reason temporally over video inputs, using future video segments as a self-supervised signal.  \t\t\t\t\tAI-generated summary \t\t\t\t Next-token prediction serves as the foundational learning task enabling reasoning in LLMs. But what should the learning task be when aiming to equip MLLMs with temporal reasoning capabilities over video inputs? Existing tasks such as video question answering often rely on annotations from humans or much stronger MLLMs, while video captioning tends to entangle temporal reasoning with spatial information. To address this gap, we propose next-event prediction (NEP), a learning task that harnesses future video segments as a rich, self-supervised signal to foster temporal reasoning. We segment each video into past and future frames: the MLLM takes the past frames as input and predicts a summary of events derived from the future frames, thereby encouraging the model to reason temporally in order to complete the task. To support this task, we curate V1-33K, a dataset comprising 33,000 automatically extracted video segments spanning diverse real-world scenarios. We further explore a range of video instruction-tuning strategies to study their effects on temporal reasoning. To evaluate progress, we introduce FutureBench to assess coherence in predicting unseen future events. Experiments validate that NEP offers a scalable and effective training paradigm for fostering temporal reasoning in MLLMs.', 'score': 25, 'issue_id': 4019, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 мая', 'en': 'May 28', 'zh': '5月28日'}, 'hash': '1ec4fe9e4580308e', 'authors': ['Haonan Wang', 'Hongfu Liu', 'Xiangyan Liu', 'Chao Du', 'Kenji Kawaguchi', 'Ye Wang', 'Tianyu Pang'], 'affiliations': ['National University of Singapore', 'SSea AI Lab, Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2505.22457.jpg', 'data': {'categories': ['#benchmark', '#training', '#video', '#multimodal', '#dataset', '#reasoning'], 'emoji': '🎬', 'ru': {'title': 'Предсказание будущего: новый подход к обучению ИИ временному мышлению', 'desc': 'Статья предлагает новую задачу обучения для мультимодальных языковых моделей (MLLM) - предсказание следующего события (NEP) в видео. Эта задача использует будущие сегменты видео в качестве самоконтролируемого сигнала для развития темпорального рассуждения. Авторы создали датасет V1-33K из 33 000 видеосегментов и исследовали различные стратегии инструктирования моделей на видео. Для оценки прогресса был разработан бенчмарк FutureBench, оценивающий согласованность в предсказании будущих событий.'}, 'en': {'title': 'Empowering MLLMs with Temporal Reasoning through Next-Event Prediction', 'desc': "This paper introduces Next-event prediction (NEP) as a new learning task designed to enhance the temporal reasoning abilities of Multi-Modal Language Models (MLLMs) when processing video data. NEP utilizes future video segments as a self-supervised signal, allowing the model to predict events based on past frames. The authors present a dataset called V1-33K, which contains 33,000 video segments to support this task, and they also propose FutureBench for evaluating the model's performance in predicting future events. The experiments demonstrate that NEP is an effective and scalable approach for improving temporal reasoning in MLLMs."}, 'zh': {'title': '下一事件预测：提升视频时间推理能力的创新任务', 'desc': '本文提出了下一事件预测（NEP）作为一种学习任务，旨在使多模态大语言模型（MLLMs）能够对视频输入进行时间推理。通过利用未来视频片段作为自监督信号，NEP鼓励模型在处理过去帧时预测未来事件的摘要，从而增强时间推理能力。我们还创建了V1-33K数据集，包含33,000个自动提取的视频片段，涵盖多种真实场景，以支持这一任务。实验结果表明，NEP为促进MLLMs的时间推理提供了一种可扩展且有效的训练范式。'}}}, {'id': 'https://huggingface.co/papers/2505.22334', 'title': 'Advancing Multimodal Reasoning via Reinforcement Learning with Cold\n  Start', 'url': 'https://huggingface.co/papers/2505.22334', 'abstract': 'Recent advancements in large language models (LLMs) have demonstrated impressive chain-of-thought reasoning capabilities, with reinforcement learning (RL) playing a crucial role in this progress. While "aha moment" patterns--where models exhibit self-correction through reflection--are often attributed to emergent properties from RL, we first demonstrate that these patterns exist in multimodal LLMs (MLLMs) prior to RL training but may not necessarily correlate with improved reasoning performance. Building on these insights, we present a comprehensive study on enhancing multimodal reasoning through a two-stage approach: (1) supervised fine-tuning (SFT) as a cold start with structured chain-of-thought reasoning patterns, followed by (2) reinforcement learning via GRPO to further refine these capabilities. Our extensive experiments show that this combined approach consistently outperforms both SFT-only and RL-only methods across challenging multimodal reasoning benchmarks. The resulting models achieve state-of-the-art performance among open-source MLLMs at both 3B and 7B scales, with our 7B model showing substantial improvements over base models (e.g., 66.3 %rightarrow73.4 % on MathVista, 62.9 %rightarrow70.4 % on We-Math) and our 3B model achieving performance competitive with several 7B models. Overall, this work provides practical guidance for building advanced multimodal reasoning models. Our code is available at https://github.com/waltonfuture/RL-with-Cold-Start.', 'score': 25, 'issue_id': 4013, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 мая', 'en': 'May 28', 'zh': '5月28日'}, 'hash': '129b6f3da36369f9', 'authors': ['Lai Wei', 'Yuting Li', 'Kaipeng Zheng', 'Chen Wang', 'Yue Wang', 'Linghe Kong', 'Lichao Sun', 'Weiran Huang'], 'affiliations': ['Lehigh University', 'School of Computer Science, Shanghai Jiao Tong University', 'Shanghai Innovation Institute', 'Zhongguancun Academy'], 'pdf_title_img': 'assets/pdf/title_img/2505.22334.jpg', 'data': {'categories': ['#reasoning', '#rl', '#multimodal', '#benchmark', '#open_source', '#training'], 'emoji': '🧠', 'ru': {'title': 'Двухэтапное обучение для прорыва в мультимодальном рассуждении', 'desc': 'Статья исследует улучшение мультимодального рассуждения в больших языковых моделях. Авторы предлагают двухэтапный подход: сначала обучение с учителем для структурированных цепочек рассуждений, затем обучение с подкреплением. Эксперименты показывают, что этот комбинированный метод превосходит другие подходы на сложных бенчмарках мультимодального рассуждения. Результаты демонстрируют значительное улучшение производительности моделей разных размеров в задачах визуального и математического рассуждения.'}, 'en': {'title': 'Enhancing Multimodal Reasoning with a Two-Stage Approach', 'desc': "This paper explores the reasoning abilities of large language models (LLMs) and their improvement through reinforcement learning (RL). It reveals that 'aha moment' patterns, which indicate self-correction, are present in multimodal LLMs even before RL training, although they do not always lead to better reasoning. The authors propose a two-stage enhancement method: first, using supervised fine-tuning (SFT) to establish structured reasoning, and then applying RL to refine these skills. Their experiments show that this combined approach significantly outperforms traditional methods, achieving state-of-the-art results in multimodal reasoning tasks."}, 'zh': {'title': '提升多模态推理的双阶段方法', 'desc': '最近大型语言模型（LLMs）的进展显示出令人印象深刻的思维链推理能力，而强化学习（RL）在这一进展中起着关键作用。我们首次证明了“顿悟时刻”模式在多模态大型语言模型（MLLMs）中存在，这些模式在RL训练之前就已存在，但不一定与推理性能的提升相关。基于这些见解，我们提出了一种通过两阶段方法增强多模态推理的全面研究：首先进行监督微调（SFT），然后通过GRPO进行强化学习以进一步提升能力。我们的实验表明，这种结合的方法在多模态推理基准测试中始终优于仅使用SFT或RL的方法。'}}}, {'id': 'https://huggingface.co/papers/2505.21925', 'title': 'RenderFormer: Transformer-based Neural Rendering of Triangle Meshes with\n  Global Illumination', 'url': 'https://huggingface.co/papers/2505.21925', 'abstract': 'We present RenderFormer, a neural rendering pipeline that directly renders an image from a triangle-based representation of a scene with full global illumination effects and that does not require per-scene training or fine-tuning. Instead of taking a physics-centric approach to rendering, we formulate rendering as a sequence-to-sequence transformation where a sequence of tokens representing triangles with reflectance properties is converted to a sequence of output tokens representing small patches of pixels. RenderFormer follows a two stage pipeline: a view-independent stage that models triangle-to-triangle light transport, and a view-dependent stage that transforms a token representing a bundle of rays to the corresponding pixel values guided by the triangle-sequence from the view-independent stage. Both stages are based on the transformer architecture and are learned with minimal prior constraints. We demonstrate and evaluate RenderFormer on scenes with varying complexity in shape and light transport.', 'score': 23, 'issue_id': 4013, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 мая', 'en': 'May 28', 'zh': '5月28日'}, 'hash': 'a6275b50c7372a2c', 'authors': ['Chong Zeng', 'Yue Dong', 'Pieter Peers', 'Hongzhi Wu', 'Xin Tong'], 'affiliations': ['College of William & Mary', 'Microsoft Research Asia', 'State Key Lab of CAD & CG, Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.21925.jpg', 'data': {'categories': ['#architecture', '#3d'], 'emoji': '🎨', 'ru': {'title': 'Нейронный рендеринг без физики: от треугольников к пикселям', 'desc': 'RenderFormer - это нейронная система рендеринга, которая напрямую создает изображение из треугольного представления сцены с полными эффектами глобального освещения без необходимости обучения для каждой сцены. Система использует подход преобразования последовательности в последовательность, где токены, представляющие треугольники с свойствами отражения, преобразуются в токены, представляющие небольшие участки пикселей. RenderFormer состоит из двух этапов: независимого от вида этапа, моделирующего перенос света между треугольниками, и зависимого от вида этапа, преобразующего токен, представляющий пучок лучей, в соответствующие значения пикселей. Оба этапа основаны на архитектуре трансформера и обучаются с минимальными предварительными ограничениями.'}, 'en': {'title': 'Transforming Triangles to Pixels with RenderFormer', 'desc': 'RenderFormer is a novel neural rendering pipeline that generates images directly from a triangle-based scene representation, incorporating full global illumination effects without needing specific training for each scene. It approaches rendering as a sequence-to-sequence transformation, where input tokens representing triangles are converted into output tokens for pixel patches. The pipeline consists of two stages: a view-independent stage that models light transport between triangles, and a view-dependent stage that converts ray bundles into pixel values. Both stages utilize the transformer architecture and are designed to learn with minimal prior constraints, showcasing effectiveness across various scene complexities.'}, 'zh': {'title': 'RenderFormer：无需训练的高效神经渲染', 'desc': 'RenderFormer是一种神经渲染管道，可以直接从基于三角形的场景表示中渲染图像，并实现全局光照效果，而无需针对每个场景进行训练或微调。该方法将渲染视为一种序列到序列的转换，将表示三角形及其反射特性的令牌序列转换为表示小像素块的输出令牌序列。RenderFormer采用两阶段管道：第一阶段是视图无关的三角形光传输建模，第二阶段则是将光束的令牌转换为相应的像素值。两个阶段都基于变换器架构，并在最小的先验约束下进行学习。'}}}, {'id': 'https://huggingface.co/papers/2505.19253', 'title': 'DeepResearchGym: A Free, Transparent, and Reproducible Evaluation\n  Sandbox for Deep Research', 'url': 'https://huggingface.co/papers/2505.19253', 'abstract': "DeepResearchGym provides an open-source evaluation framework for deep research systems using a reproducible search API and LLM-as-a-judge assessments.  \t\t\t\t\tAI-generated summary \t\t\t\t Deep research systems represent an emerging class of agentic information retrieval methods that generate comprehensive and well-supported reports to complex queries. However, most existing frameworks rely on dynamic commercial search APIs, which pose reproducibility and transparency challenges in addition to their cost. To address these limitations, we introduce DeepResearchGym, an open-source sandbox that combines a reproducible search API with a rigorous evaluation protocol for benchmarking deep research systems. The API indexes large-scale public web corpora, namely ClueWeb22 and FineWeb, using a state-of-the-art dense retriever and approximate nearest neighbor search via DiskANN. It achieves lower latency than popular commercial APIs while ensuring stable document rankings across runs, and is freely available for research use. To evaluate deep research systems' outputs, we extend the Researchy Questions benchmark with automatic metrics through LLM-as-a-judge assessments to measure alignment with users' information needs, retrieval faithfulness, and report quality. Experimental results show that systems integrated with DeepResearchGym achieve performance comparable to those using commercial APIs, with performance rankings remaining consistent across evaluation metrics. A human evaluation study further confirms that our automatic protocol aligns with human preferences, validating the framework's ability to help support controlled assessment of deep research systems. Our code and API documentation are available at https://www.deepresearchgym.ai.", 'score': 20, 'issue_id': 4013, 'pub_date': '2025-05-25', 'pub_date_card': {'ru': '25 мая', 'en': 'May 25', 'zh': '5月25日'}, 'hash': 'fabfc1ec7f6826eb', 'authors': ['João Coelho', 'Jingjie Ning', 'Jingyuan He', 'Kangrui Mao', 'Abhijay Paladugu', 'Pranav Setlur', 'Jiahe Jin', 'Jamie Callan', 'João Magalhães', 'Bruno Martins', 'Chenyan Xiong'], 'affiliations': ['Carnegie Mellon University', 'IST and INESC-ID', 'NOVA LINCS'], 'pdf_title_img': 'assets/pdf/title_img/2505.19253.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#alignment', '#rag'], 'emoji': '🔬', 'ru': {'title': 'Открытая платформа для оценки систем глубокого исследования', 'desc': 'DeepResearchGym - это открытая система оценки для систем глубокого исследования, использующая воспроизводимый поисковый API и оценки с помощью языковых моделей в роли судей. Она индексирует крупномасштабные веб-корпусы с помощью современного плотного ретривера и приближенного поиска ближайших соседей. Система расширяет бенчмарк Researchy Questions автоматическими метриками для оценки соответствия информационным потребностям пользователей, точности поиска и качества отчетов. Эксперименты показывают, что системы, интегрированные с DeepResearchGym, достигают производительности, сопоставимой с коммерческими API.'}, 'en': {'title': 'Revolutionizing Research Evaluation with DeepResearchGym', 'desc': "DeepResearchGym is an open-source framework designed to evaluate deep research systems, which are advanced methods for retrieving and generating detailed reports from complex queries. It addresses issues of reproducibility and transparency found in existing commercial search APIs by providing a stable and cost-free alternative. The framework utilizes a state-of-the-art dense retriever and approximate nearest neighbor search to index large public web corpora, ensuring lower latency and consistent document rankings. Additionally, it incorporates LLM-as-a-judge assessments to automatically evaluate the quality of outputs, aligning them with user needs and confirming the framework's effectiveness through both automatic and human evaluations."}, 'zh': {'title': '深度研究系统的开源评估框架', 'desc': 'DeepResearchGym是一个开源评估框架，旨在为深度研究系统提供可重复的搜索API和LLM作为评估者的评估方法。该框架解决了现有商业搜索API在可重复性和透明性方面的挑战，同时降低了成本。它使用先进的密集检索器和DiskANN进行近似最近邻搜索，能够在较低延迟下索引大规模公共网络语料库。通过扩展Researchy Questions基准，DeepResearchGym能够自动评估深度研究系统的输出，确保与用户信息需求的一致性和报告质量。'}}}, {'id': 'https://huggingface.co/papers/2505.18600', 'title': 'Chain-of-Zoom: Extreme Super-Resolution via Scale Autoregression and\n  Preference Alignment', 'url': 'https://huggingface.co/papers/2505.18600', 'abstract': 'Chain-of-Zoom (CoZ) enhances single-image super-resolution models by using an autoregressive chain of intermediate scale-states and multi-scale-aware prompts to achieve extreme magnifications with high quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern single-image super-resolution (SISR) models deliver photo-realistic results at the scale factors on which they are trained, but collapse when asked to magnify far beyond that regime. We address this scalability bottleneck with Chain-of-Zoom (CoZ), a model-agnostic framework that factorizes SISR into an autoregressive chain of intermediate scale-states with multi-scale-aware prompts. CoZ repeatedly re-uses a backbone SR model, decomposing the conditional probability into tractable sub-problems to achieve extreme resolutions without additional training. Because visual cues diminish at high magnifications, we augment each zoom step with multi-scale-aware text prompts generated by a vision-language model (VLM). The prompt extractor itself is fine-tuned using Generalized Reward Policy Optimization (GRPO) with a critic VLM, aligning text guidance towards human preference. Experiments show that a standard 4x diffusion SR model wrapped in CoZ attains beyond 256x enlargement with high perceptual quality and fidelity. Project Page: https://bryanswkim.github.io/chain-of-zoom/ .', 'score': 19, 'issue_id': 4018, 'pub_date': '2025-05-24', 'pub_date_card': {'ru': '24 мая', 'en': 'May 24', 'zh': '5月24日'}, 'hash': '4c38f564fd997a1c', 'authors': ['Bryan Sangwoo Kim', 'Jeongsol Kim', 'Jong Chul Ye'], 'affiliations': ['KAIST AI'], 'pdf_title_img': 'assets/pdf/title_img/2505.18600.jpg', 'data': {'categories': ['#alignment', '#cv', '#optimization', '#diffusion', '#rlhf', '#rag'], 'emoji': '🔍', 'ru': {'title': 'Революция в сверхразрешении изображений: от пикселей к деталям', 'desc': 'Статья представляет Chain-of-Zoom (CoZ) - фреймворк для улучшения моделей сверхразрешения одиночных изображений. CoZ использует авторегрессивную цепочку промежуточных масштабных состояний и мультимасштабные подсказки для достижения экстремальных увеличений с высоким качеством. Метод разбивает задачу на более простые подзадачи, позволяя многократно использовать базовую модель сверхразрешения. Для улучшения результатов применяются текстовые подсказки, генерируемые языковой моделью компьютерного зрения.'}, 'en': {'title': 'Zooming into High-Quality Super-Resolution with CoZ', 'desc': 'Chain-of-Zoom (CoZ) is a novel framework that improves single-image super-resolution (SISR) by breaking down the process into an autoregressive sequence of intermediate scale-states. This approach allows the model to achieve extreme magnifications while maintaining high image quality, even beyond the typical training scale. CoZ utilizes multi-scale-aware prompts generated by a vision-language model to enhance the visual cues at high magnifications. By employing Generalized Reward Policy Optimization (GRPO) for fine-tuning, CoZ aligns the prompts with human preferences, resulting in superior perceptual quality in the generated images.'}, 'zh': {'title': '超分辨率的新突破：Chain-of-Zoom', 'desc': 'Chain-of-Zoom (CoZ) 是一种增强单图像超分辨率模型的方法。它通过使用自回归的中间尺度状态链和多尺度感知提示，来实现高质量的极端放大。CoZ 将超分辨率任务分解为可处理的子问题，允许在不额外训练的情况下重复使用基础超分辨率模型。实验表明，使用 CoZ 的标准 4 倍扩散超分辨率模型可以实现超过 256 倍的放大，且保持高感知质量和保真度。'}}}, {'id': 'https://huggingface.co/papers/2505.22232', 'title': 'Judging Quality Across Languages: A Multilingual Approach to Pretraining\n  Data Filtering with Language Models', 'url': 'https://huggingface.co/papers/2505.22232', 'abstract': "JQL systematically curates high-quality multilingual training data using pretrained multilingual embeddings, outperforming heuristic methods and improving downstream model training across diverse languages.  \t\t\t\t\tAI-generated summary \t\t\t\t High-quality multilingual training data is essential for effectively pretraining large language models (LLMs). Yet, the availability of suitable open-source multilingual datasets remains limited. Existing state-of-the-art datasets mostly rely on heuristic filtering methods, restricting both their cross-lingual transferability and scalability. Here, we introduce JQL, a systematic approach that efficiently curates diverse and high-quality multilingual data at scale while significantly reducing computational demands. JQL distills LLMs' annotation capabilities into lightweight annotators based on pretrained multilingual embeddings. These models exhibit robust multilingual and cross-lingual performance, even for languages and scripts unseen during training. Evaluated empirically across 35 languages, the resulting annotation pipeline substantially outperforms current heuristic filtering methods like Fineweb2. JQL notably enhances downstream model training quality and increases data retention rates. Our research provides practical insights and valuable resources for multilingual data curation, raising the standards of multilingual dataset development.", 'score': 14, 'issue_id': 4020, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 мая', 'en': 'May 28', 'zh': '5月28日'}, 'hash': '77da366f396f2728', 'authors': ['Mehdi Ali', 'Manuel Brack', 'Max Lübbering', 'Elias Wendt', 'Abbas Goher Khan', 'Richard Rutmann', 'Alex Jude', 'Maurice Kraus', 'Alexander Arno Weber', 'Felix Stollenwerk', 'David Kaczér', 'Florian Mai', 'Lucie Flek', 'Rafet Sifa', 'Nicolas Flores-Herr', 'Joachim Köhler', 'Patrick Schramowski', 'Michael Fromm', 'Kristian Kersting'], 'affiliations': ['AI Sweden', 'Computer Science Department, TU Darmstadt', 'DFKI SAINT', 'Fraunhofer IAIS', 'Hessian AI', 'Lamarr Institute'], 'pdf_title_img': 'assets/pdf/title_img/2505.22232.jpg', 'data': {'categories': ['#multilingual', '#data', '#transfer_learning', '#open_source', '#low_resource', '#dataset'], 'emoji': '🌍', 'ru': {'title': 'JQL: качественные многоязычные данные для обучения языковых моделей', 'desc': 'JQL - это новый подход к созданию многоязычных обучающих данных высокого качества для языковых моделей. Он использует предобученные многоязычные эмбеддинги для эффективной фильтрации и аннотирования текстов на разных языках. По сравнению с эвристическими методами, JQL показывает лучшие результаты при оценке на 35 языках. Этот метод позволяет улучшить качество обучения моделей и увеличить объем сохраняемых данных.'}, 'en': {'title': 'JQL: Revolutionizing Multilingual Data Curation for Better AI Models', 'desc': 'This paper presents JQL, a novel method for curating high-quality multilingual training data using pretrained multilingual embeddings. JQL improves upon traditional heuristic methods by systematically selecting diverse datasets, which enhances the performance of downstream language models. The approach reduces computational costs while maintaining robust performance across various languages, including those not seen during training. Empirical evaluations show that JQL significantly outperforms existing methods, leading to better model training and higher data retention rates.'}, 'zh': {'title': 'JQL：高效策划多语言训练数据的系统化方法', 'desc': 'JQL是一种系统化的方法，用于高效地策划多语言训练数据，利用预训练的多语言嵌入，显著优于传统的启发式方法。该方法能够在大规模下减少计算需求，同时提高下游模型训练的质量。JQL通过轻量级的注释器提炼大型语言模型的注释能力，展现出强大的多语言和跨语言性能。我们的研究为多语言数据策划提供了实用的见解和宝贵的资源，提升了多语言数据集开发的标准。'}}}, {'id': 'https://huggingface.co/papers/2505.21887', 'title': 'SVRPBench: A Realistic Benchmark for Stochastic Vehicle Routing Problem', 'url': 'https://huggingface.co/papers/2505.21887', 'abstract': 'SVRPBench introduces a new benchmark for vehicle routing under uncertainty, simulating realistic urban conditions and highlighting the limitations of state-of-the-art RL solvers.  \t\t\t\t\tAI-generated summary \t\t\t\t Robust routing under uncertainty is central to real-world logistics, yet most benchmarks assume static, idealized settings. We present SVRPBench, the first open benchmark to capture high-fidelity stochastic dynamics in vehicle routing at urban scale. Spanning more than 500 instances with up to 1000 customers, it simulates realistic delivery conditions: time-dependent congestion, log-normal delays, probabilistic accidents, and empirically grounded time windows for residential and commercial clients. Our pipeline generates diverse, constraint-rich scenarios, including multi-depot and multi-vehicle setups. Benchmarking reveals that state-of-the-art RL solvers like POMO and AM degrade by over 20% under distributional shift, while classical and metaheuristic methods remain robust. To enable reproducible research, we release the dataset and evaluation suite. SVRPBench challenges the community to design solvers that generalize beyond synthetic assumptions and adapt to real-world uncertainty.', 'score': 14, 'issue_id': 4016, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 мая', 'en': 'May 28', 'zh': '5月28日'}, 'hash': 'ea6c5f0bb081e773', 'authors': ['Ahmed Heakl', 'Yahia Salaheldin Shaaban', 'Martin Takac', 'Salem Lahlou', 'Zangir Iklassov'], 'affiliations': ['MBZUAI, Abu Dhabi, UAE'], 'pdf_title_img': 'assets/pdf/title_img/2505.21887.jpg', 'data': {'categories': ['#open_source', '#optimization', '#dataset', '#rl', '#benchmark'], 'emoji': '🚚', 'ru': {'title': 'Реалистичный бенчмарк для маршрутизации в условиях городской неопределенности', 'desc': 'SVRPBench представляет новый бенчмарк для маршрутизации транспортных средств в условиях неопределенности, моделируя реалистичные городские условия. Он включает более 500 сценариев с количеством клиентов до 1000, симулируя реальные условия доставки, такие как зависящие от времени пробки и вероятностные аварии. Бенчмарк выявил, что современные решатели на основе обучения с подкреплением, такие как POMO и AM, ухудшают производительность более чем на 20% при смещении распределения. SVRPBench призывает сообщество разрабатывать решатели, которые обобщаются за пределы синтетических предположений и адаптируются к реальной неопределенности.'}, 'en': {'title': 'Revolutionizing Vehicle Routing: Benchmarking Under Real-World Uncertainty', 'desc': 'SVRPBench is a new benchmark designed to evaluate vehicle routing algorithms under uncertain urban conditions. It simulates realistic scenarios with factors like traffic congestion, delays, and accidents, providing over 500 instances with up to 1000 customers. The study shows that advanced reinforcement learning (RL) solvers struggle with over 20% performance degradation when faced with real-world uncertainties, while traditional methods perform better. By releasing this dataset and evaluation suite, SVRPBench encourages researchers to develop more robust algorithms that can handle unpredictable environments.'}, 'zh': {'title': '应对不确定性的车辆调度新基准', 'desc': 'SVRPBench是一个新的基准测试，专注于不确定条件下的车辆调度，模拟现实城市环境，并突显当前最先进的强化学习求解器的局限性。该基准涵盖超过500个实例，最多可容纳1000个客户，模拟了时间依赖的拥堵、对数延迟、概率性事故等真实交付条件。通过基准测试发现，像POMO和AM这样的最先进的强化学习求解器在分布转移下性能下降超过20%，而经典和元启发式方法则保持稳健。SVRPBench旨在推动研究社区设计能够超越合成假设并适应现实世界不确定性的求解器。'}}}, {'id': 'https://huggingface.co/papers/2505.19075', 'title': 'Universal Reasoner: A Single, Composable Plug-and-Play Reasoner for\n  Frozen LLMs', 'url': 'https://huggingface.co/papers/2505.19075', 'abstract': 'UniR, a lightweight reasoning module, enhances Large Language Models with specialized reasoning abilities through modular composition, improving performance and generalization at lower computational costs.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have demonstrated remarkable general capabilities, but enhancing skills such as reasoning often demands substantial computational resources and may compromise their generalization. While Parameter-Efficient Fine-Tuning (PEFT) methods offer a more resource-conscious alternative, they typically requires retraining for each LLM backbone due to architectural dependencies. To address these challenges, here we propose Universal Reasoner (UniR) - a single, lightweight, composable, and plug-and-play reasoning module that can be used with any frozen LLM to endow it with specialized reasoning capabilities. Specifically, UniR decomposes the reward into a standalone reasoning module that is trained independently using predefined rewards, effectively translating trajectory-level signals into token-level guidance. Once trained, UniR can be combined with any frozen LLM at inference time by simply adding its output logits to those of the LLM backbone. This additive structure naturally enables modular composition: multiple UniR modules trained for different tasks can be jointly applied by summing their logits, enabling complex reasoning via composition. Experimental results on mathematical reasoning and machine translation tasks show that UniR significantly outperforms existing baseline fine-tuning methods using the Llama3.2 model. Furthermore, UniR demonstrates strong weak-to-strong generalization: reasoning modules trained on smaller models effectively guide much larger LLMs. This makes UniR a cost-efficient, adaptable, and robust solution for enhancing reasoning in LLMs without compromising their core capabilities. Code is open-sourced at https://github.com/hangeol/UniR', 'score': 14, 'issue_id': 4018, 'pub_date': '2025-05-25', 'pub_date_card': {'ru': '25 мая', 'en': 'May 25', 'zh': '5月25日'}, 'hash': 'afa75bef28fdbfdb', 'authors': ['Jaemin Kim', 'Hangeol Chang', 'Hyunmin Hwang', 'Choonghan Kim', 'Jong Chul Ye'], 'affiliations': ['Korea Advanced Institute of Science and Technology (KAIST)'], 'pdf_title_img': 'assets/pdf/title_img/2505.19075.jpg', 'data': {'categories': ['#open_source', '#training', '#architecture', '#reasoning', '#rlhf'], 'emoji': '🧠', 'ru': {'title': 'UniR: Универсальный модуль рассуждений для усиления больших языковых моделей', 'desc': 'UniR - это универсальный модуль рассуждений для больших языковых моделей (LLM). Он позволяет улучшить способности LLM к рассуждениям без полной переподготовки модели, что экономит вычислительные ресурсы. UniR можно комбинировать с любой замороженной LLM, добавляя его выходные логиты к логитам основной модели. Эксперименты показали, что UniR значительно превосходит существующие методы дообучения на задачах математических рассуждений и машинного перевода.'}, 'en': {'title': 'Enhancing LLMs with Modular Reasoning: The Power of UniR', 'desc': 'The paper introduces UniR, a lightweight reasoning module designed to enhance Large Language Models (LLMs) with specialized reasoning skills while minimizing computational costs. Unlike traditional methods that require extensive retraining for each LLM, UniR operates as a plug-and-play module that can be added to any frozen LLM. It achieves this by independently training a reasoning module that translates high-level rewards into actionable guidance for the LLM. Experimental results demonstrate that UniR not only improves performance on tasks like mathematical reasoning and machine translation but also exhibits strong generalization capabilities across different model sizes.'}, 'zh': {'title': 'UniR：轻量级推理模块，提升LLM推理能力', 'desc': 'UniR是一个轻量级的推理模块，通过模块化组合增强大型语言模型（LLM）的推理能力，提升性能和泛化能力，同时降低计算成本。它将奖励分解为独立的推理模块，使用预定义的奖励进行独立训练，从而有效地将轨迹级信号转化为标记级指导。UniR可以与任何冻结的LLM结合使用，只需在推理时将其输出与LLM的输出相加，支持模块化组合。实验结果表明，UniR在数学推理和机器翻译任务上显著优于现有的微调方法，且在小模型上训练的推理模块能够有效指导更大的LLM。'}}}, {'id': 'https://huggingface.co/papers/2505.22129', 'title': 'What Makes for Text to 360-degree Panorama Generation with Stable\n  Diffusion?', 'url': 'https://huggingface.co/papers/2505.22129', 'abstract': 'Analysis of fine-tuning diffusion models for panoramic image generation reveals distinct roles of attention module matrices and introduces UniPano, a memory-efficient and speed-enhanced baseline framework.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent prosperity of text-to-image diffusion models, e.g. Stable Diffusion, has stimulated research to adapt them to 360-degree panorama generation. Prior work has demonstrated the feasibility of using conventional low-rank adaptation techniques on pre-trained diffusion models to generate panoramic images. However, the substantial domain gap between perspective and panoramic images raises questions about the underlying mechanisms enabling this empirical success. We hypothesize and examine that the trainable counterparts exhibit distinct behaviors when fine-tuned on panoramic data, and such an adaptation conceals some intrinsic mechanism to leverage the prior knowledge within the pre-trained diffusion models. Our analysis reveals the following: 1) the query and key matrices in the attention modules are responsible for common information that can be shared between the panoramic and perspective domains, thus are less relevant to panorama generation; and 2) the value and output weight matrices specialize in adapting pre-trained knowledge to the panoramic domain, playing a more critical role during fine-tuning for panorama generation. We empirically verify these insights by introducing a simple framework called UniPano, with the objective of establishing an elegant baseline for future research. UniPano not only outperforms existing methods but also significantly reduces memory usage and training time compared to prior dual-branch approaches, making it scalable for end-to-end panorama generation with higher resolution. The code will be released.', 'score': 13, 'issue_id': 4015, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 мая', 'en': 'May 28', 'zh': '5月28日'}, 'hash': '1148d530d0e60180', 'authors': ['Jinhong Ni', 'Chang-Bin Zhang', 'Qiang Zhang', 'Jing Zhang'], 'affiliations': ['Australian National University', 'Beijing Innovation Center of Humanoid Robotics', 'Hong Kong University of Science and Technology (Guangzhou)', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.22129.jpg', 'data': {'categories': ['#optimization', '#dataset', '#training', '#diffusion', '#cv'], 'emoji': '�panorama', 'ru': {'title': 'Раскрытие механизмов адаптации диффузионных моделей к панорамной генерации', 'desc': 'Исследование процесса тонкой настройки диффузионных моделей для генерации панорамных изображений выявило различные роли матриц модуля внимания. Анализ показал, что матрицы запросов и ключей отвечают за общую информацию, применимую как к панорамным, так и к перспективным изображениям. Матрицы значений и выходных весов специализируются на адаптации предобученных знаний к панорамной области. На основе этих выводов авторы представили фреймворк UniPano, который превосходит существующие методы и значительно снижает использование памяти и время обучения.'}, 'en': {'title': 'UniPano: Efficient Panoramic Image Generation with Diffusion Models', 'desc': 'This paper investigates how to improve the generation of panoramic images using diffusion models, which are typically used for standard images. It identifies the specific roles of different components in the attention mechanism of these models, particularly how they adapt to the unique characteristics of panoramic data. The authors introduce a new framework called UniPano, which enhances efficiency by reducing memory usage and training time while achieving better performance than existing methods. This work aims to provide a solid foundation for future advancements in panoramic image generation.'}, 'zh': {'title': 'UniPano：高效全景图像生成的新基线', 'desc': '本文分析了微调扩散模型在全景图像生成中的作用，揭示了注意力模块矩阵的不同角色，并引入了UniPano，一个内存高效且速度增强的基线框架。研究表明，查询和键矩阵在全景和透视领域之间共享信息，而值和输出权重矩阵则专注于将预训练知识适应于全景领域。通过这些发现，UniPano在全景图像生成中表现优于现有方法，并显著减少了内存使用和训练时间。该框架为未来的研究提供了一个优雅的基线，并将发布相关代码。'}}}, {'id': 'https://huggingface.co/papers/2505.22648', 'title': 'WebDancer: Towards Autonomous Information Seeking Agency', 'url': 'https://huggingface.co/papers/2505.22648', 'abstract': 'Addressing intricate real-world problems necessitates in-depth information seeking and multi-step reasoning. Recent progress in agentic systems, exemplified by Deep Research, underscores the potential for autonomous multi-step research. In this work, we present a cohesive paradigm for building end-to-end agentic information seeking agents from a data-centric and training-stage perspective. Our approach consists of four key stages: (1) browsing data construction, (2) trajectories sampling, (3) supervised fine-tuning for effective cold start, and (4) reinforcement learning for enhanced generalisation. We instantiate this framework in a web agent based on the ReAct, WebDancer. Empirical evaluations on the challenging information seeking benchmarks, GAIA and WebWalkerQA, demonstrate the strong performance of WebDancer, achieving considerable results and highlighting the efficacy of our training paradigm. Further analysis of agent training provides valuable insights and actionable, systematic pathways for developing more capable agentic models. The codes and demo will be released in https://github.com/Alibaba-NLP/WebAgent.', 'score': 12, 'issue_id': 4013, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 мая', 'en': 'May 28', 'zh': '5月28日'}, 'hash': '906f55ecff56c6ba', 'authors': ['Jialong Wu', 'Baixuan Li', 'Runnan Fang', 'Wenbiao Yin', 'Liwen Zhang', 'Zhengwei Tao', 'Dingchu Zhang', 'Zekun Xi', 'Yong Jiang', 'Pengjun Xie', 'Fei Huang', 'Jingren Zhou'], 'affiliations': ['Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2505.22648.jpg', 'data': {'categories': ['#agi', '#reasoning', '#agents', '#benchmark', '#training'], 'emoji': '🕸️', 'ru': {'title': 'Новая парадигма обучения агентов для автономного поиска информации в сети', 'desc': 'Эта статья представляет новый подход к созданию агентов для автономного поиска информации. Авторы предлагают четырехэтапную парадигму, включающую построение данных для просмотра, выборку траекторий, обучение с учителем и обучение с подкреплением. На основе этой парадигмы они создали веб-агента WebDancer, использующего архитектуру ReAct. Эмпирические оценки на сложных тестах GAIA и WebWalkerQA показали высокую эффективность WebDancer и предложенного подхода к обучению.'}, 'en': {'title': 'Empowering Agents for Autonomous Information Seeking', 'desc': "This paper introduces a new framework for creating intelligent agents that can autonomously seek information and perform multi-step reasoning. The proposed method involves four stages: constructing a dataset for browsing, sampling trajectories for learning, fine-tuning the model to improve initial performance, and applying reinforcement learning to enhance the agent's ability to generalize. The authors demonstrate their approach through a web agent called WebDancer, which shows strong performance on challenging benchmarks like GAIA and WebWalkerQA. The findings provide insights into training strategies that can lead to the development of more advanced agentic systems."}, 'zh': {'title': '构建智能信息检索代理的全新框架', 'desc': '本论文探讨了如何构建自主的信息检索代理系统，以解决复杂的现实问题。我们提出了一个端到端的代理信息检索框架，包含数据构建、轨迹采样、监督微调和强化学习四个关键阶段。通过在WebDancer上实施该框架，我们在GAIA和WebWalkerQA等信息检索基准上取得了显著的性能。我们的研究为开发更强大的代理模型提供了系统化的路径和有价值的见解。'}}}, {'id': 'https://huggingface.co/papers/2505.19187', 'title': 'LIMOPro: Reasoning Refinement for Efficient and Effective Test-time\n  Scaling', 'url': 'https://huggingface.co/papers/2505.19187', 'abstract': 'A framework called PIR refines the importance of reasoning steps in large language models by pruning low-importance functional elements, leading to more concise reasoning chains with improved accuracy and reduced computational demands.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have demonstrated remarkable reasoning capabilities through test-time scaling approaches, particularly when fine-tuned with chain-of-thought (CoT) data distilled from more powerful large reasoning models (LRMs). However, these reasoning chains often contain verbose elements that mirror human problem-solving, categorized as progressive reasoning (the essential solution development path) and functional elements (verification processes, alternative solution approaches, and error corrections). While progressive reasoning is crucial, the functional elements significantly increase computational demands during test-time inference. We introduce PIR (Perplexity-based Importance Refinement), a principled framework that quantitatively evaluates the importance of each reasoning step based on its impact on answer prediction confidence. PIR systematically identifies and selectively prunes only low-importance functional steps while preserving progressive reasoning components, creating optimized training data that maintains the integrity of the core solution path while reducing verbosity. Models fine-tuned on PIR-optimized data exhibit superior test-time scaling properties, generating more concise reasoning chains while achieving improved accuracy (+0.9\\% to +6.6\\%) with significantly reduced token usage (-3\\% to -41\\%) across challenging reasoning benchmarks (AIME, AMC, and GPQA Diamond). Our approach demonstrates strong generalizability across different model sizes, data sources, and token budgets, offering a practical solution for deploying reasoning-capable LLMs in scenarios where efficient test-time scaling, response time, and computational efficiency are valuable constraints.', 'score': 11, 'issue_id': 4013, 'pub_date': '2025-05-25', 'pub_date_card': {'ru': '25 мая', 'en': 'May 25', 'zh': '5月25日'}, 'hash': '072196a147db8410', 'authors': ['Yang Xiao', 'Jiashuo Wang', 'Ruifeng Yuan', 'Chunpu Xu', 'Kaishuai Xu', 'Wenjie Li', 'Pengfei Liu'], 'affiliations': ['Shanghai Jiao Tong University', 'The Hong Kong Polytechnic University'], 'pdf_title_img': 'assets/pdf/title_img/2505.19187.jpg', 'data': {'categories': ['#optimization', '#inference', '#reasoning', '#benchmark', '#training'], 'emoji': '✂️', 'ru': {'title': 'PIR: Оптимизация рассуждений в ИИ через умное сокращение', 'desc': 'PIR - это фреймворк, который оптимизирует важность шагов рассуждения в больших языковых моделях путем удаления малозначимых функциональных элементов. Это приводит к более кратким цепочкам рассуждений с улучшенной точностью и сниженными вычислительными требованиями. Модели, дообученные на данных, оптимизированных с помощью PIR, демонстрируют улучшенные свойства масштабирования во время тестирования. Подход показывает хорошую обобщаемость для разных размеров моделей, источников данных и ограничений на количество токенов.'}, 'en': {'title': 'Streamlining Reasoning for Efficient AI Performance', 'desc': 'The paper introduces a framework called PIR (Perplexity-based Importance Refinement) that enhances the reasoning capabilities of large language models (LLMs) by focusing on the importance of reasoning steps. It identifies and prunes low-importance functional elements from reasoning chains, which helps in reducing computational demands while maintaining essential progressive reasoning. By optimizing training data, models fine-tuned with PIR show improved accuracy and efficiency, achieving better performance on reasoning benchmarks. This approach allows for more concise reasoning outputs, making LLMs more practical for real-world applications where speed and resource usage are critical.'}, 'zh': {'title': '优化推理链，提升模型效率', 'desc': '本文提出了一种名为PIR（基于困惑度的重要性精炼）的框架，旨在优化大型语言模型中的推理步骤。通过评估每个推理步骤对答案预测信心的影响，PIR能够识别并剪除低重要性的功能性元素，从而简化推理链。该方法在保持核心解决路径完整性的同时，减少了冗余信息，显著提高了模型的准确性和计算效率。经过PIR优化的数据训练的模型在多个推理基准测试中表现出更优的性能，减少了计算资源的消耗。'}}}, {'id': 'https://huggingface.co/papers/2505.17663', 'title': 'Towards Dynamic Theory of Mind: Evaluating LLM Adaptation to Temporal\n  Evolution of Human States', 'url': 'https://huggingface.co/papers/2505.17663', 'abstract': "The DynToM benchmark evaluates LLMs' ability to track and understand the temporal progression of mental states, revealing significant gaps compared to human performance.  \t\t\t\t\tAI-generated summary \t\t\t\t As Large Language Models (LLMs) increasingly participate in human-AI interactions, evaluating their Theory of Mind (ToM) capabilities - particularly their ability to track dynamic mental states - becomes crucial. While existing benchmarks assess basic ToM abilities, they predominantly focus on static snapshots of mental states, overlooking the temporal evolution that characterizes real-world social interactions. We present DynToM, a novel benchmark specifically designed to evaluate LLMs' ability to understand and track the temporal progression of mental states across interconnected scenarios. Through a systematic four-step framework, we generate 1,100 social contexts encompassing 5,500 scenarios and 78,100 questions, each validated for realism and quality. Our comprehensive evaluation of ten state-of-the-art LLMs reveals that their average performance underperforms humans by 44.7\\%, with performance degrading significantly when tracking and reasoning about the shift of mental states. This performance gap highlights fundamental limitations in current LLMs' ability to model the dynamic nature of human mental states.", 'score': 11, 'issue_id': 4013, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 мая', 'en': 'May 23', 'zh': '5月23日'}, 'hash': '01d30f0f90863bd8', 'authors': ['Yang Xiao', 'Jiashuo Wang', 'Qiancheng Xu', 'Changhe Song', 'Chunpu Xu', 'Yi Cheng', 'Wenjie Li', 'Pengfei Liu'], 'affiliations': ['Shanghai Jiao Tong University', 'The Hong Kong Polytechnic University'], 'pdf_title_img': 'assets/pdf/title_img/2505.17663.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#alignment'], 'emoji': '🧠', 'ru': {'title': 'Большие языковые модели отстают от людей в понимании динамики психических состояний', 'desc': 'Представлен новый бенчмарк DynToM для оценки способности больших языковых моделей (LLM) отслеживать изменение психических состояний во времени. Бенчмарк включает 1100 социальных контекстов с 5500 сценариями и 78100 вопросами. Тестирование 10 современных LLM показало, что их средняя производительность на 44.7% ниже человеческой. Выявлены существенные ограничения LLM в моделировании динамической природы человеческих психических состояний.'}, 'en': {'title': "Evaluating LLMs' Understanding of Dynamic Mental States", 'desc': "The DynToM benchmark assesses how well Large Language Models (LLMs) can understand and track changes in mental states over time, which is essential for effective human-AI interaction. Unlike previous benchmarks that only evaluate static mental states, DynToM focuses on the dynamic progression of these states in social contexts. The study involved creating a large dataset of scenarios and questions to rigorously test LLMs' Theory of Mind capabilities. Results showed that LLMs lag behind human performance by 44.7%, particularly struggling with the complexities of shifting mental states."}, 'zh': {'title': '评估LLMs的动态心理状态理解能力', 'desc': 'DynToM基准测试评估大型语言模型（LLMs）在跟踪和理解心理状态的时间进展方面的能力，显示出与人类表现之间的显著差距。现有的基准主要关注静态心理状态的快照，忽视了真实社交互动中心理状态的动态演变。我们提出了DynToM，这是一个专门设计的基准，旨在评估LLMs在相互关联场景中理解和跟踪心理状态时间进展的能力。通过系统的四步框架，我们生成了1100个社交背景，涵盖5500个场景和78100个问题，验证了其现实性和质量。'}}}, {'id': 'https://huggingface.co/papers/2505.22202', 'title': "Let's Predict Sentence by Sentence", 'url': 'https://huggingface.co/papers/2505.22202', 'abstract': 'Pretrained language models can adapt to operate in sentence space, reason over structured semantic units, and achieve competitive performance with Chain-of-Thought while reducing inference-time FLOPs.  \t\t\t\t\tAI-generated summary \t\t\t\t Autoregressive language models (LMs) generate one token at a time, yet human reasoning operates over higher-level abstractions - sentences, propositions, and concepts. This contrast raises a central question- Can LMs likewise learn to reason over structured semantic units rather than raw token sequences? In this work, we investigate whether pretrained LMs can be lifted into such abstract reasoning spaces by building on their learned representations. We present a framework that adapts a pretrained token-level LM to operate in sentence space by autoregressively predicting continuous embeddings of next sentences. We explore two embedding paradigms inspired by classical representation learning: 1) semantic embeddings, learned via autoencoding to preserve surface meaning; and 2) contextual embeddings, trained via next-sentence prediction to encode anticipatory structure. We evaluate both under two inference regimes: Discretized, which decodes each predicted embedding into text before re-encoding; and Continuous, which reasons entirely in embedding space for improved efficiency. Across four domains - mathematics, logic, commonsense, and planning - contextual embeddings under continuous inference show competitive performance with Chain-of-Thought (CoT) while reducing inference-time FLOPs on average by half. We also present early signs of scalability and modular adaptation. Finally, to visualize latent trajectories, we introduce SentenceLens, a diagnostic tool that decodes intermediate model states into interpretable sentences. Together, our results indicate that pretrained LMs can effectively transition to abstract, structured reasoning within latent embedding spaces.', 'score': 10, 'issue_id': 4017, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 мая', 'en': 'May 28', 'zh': '5月28日'}, 'hash': '2d6858513b9b0682', 'authors': ['Hyeonbin Hwang', 'Byeongguk Jeon', 'Seungone Kim', 'Jiyeon Kim', 'Hoyeon Chang', 'Sohee Yang', 'Seungpil Won', 'Dohaeng Lee', 'Youbin Ahn', 'Minjoon Seo'], 'affiliations': ['Carnegie Mellon University', 'KAIST', 'LG AI Research', 'University College London'], 'pdf_title_img': 'assets/pdf/title_img/2505.22202.jpg', 'data': {'categories': ['#architecture', '#inference', '#interpretability', '#data', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Языковые модели учатся мыслить предложениями', 'desc': 'Исследование показывает, что предобученные языковые модели могут быть адаптированы для работы в пространстве предложений, а не отдельных токенов. Авторы представляют фреймворк, который позволяет модели предсказывать непрерывные эмбеддинги следующих предложений. Эксперименты демонстрируют, что такой подход позволяет достичь результатов, сопоставимых с методом Chain-of-Thought, при этом снижая вычислительные затраты вдвое. Результаты указывают на возможность эффективного перехода предобученных языковых моделей к абстрактным рассуждениям в латентных пространствах эмбеддингов.'}, 'en': {'title': 'Empowering Language Models for Abstract Reasoning in Sentence Space', 'desc': 'This paper explores how pretrained language models (LMs) can be adapted to reason over higher-level abstractions like sentences instead of just raw tokens. The authors propose a framework that allows LMs to predict continuous embeddings of sentences, enhancing their reasoning capabilities. They investigate two types of embeddings: semantic embeddings that focus on meaning and contextual embeddings that capture anticipatory structures. The results show that contextual embeddings can achieve competitive performance with Chain-of-Thought reasoning while significantly reducing computational costs during inference.'}, 'zh': {'title': '预训练语言模型的抽象推理能力', 'desc': '这篇论文探讨了预训练语言模型（LM）如何在句子空间中进行推理，而不仅仅是处理原始的标记序列。研究者们提出了一种框架，使得预训练的标记级LM能够通过自回归预测下一个句子的连续嵌入来适应句子空间。论文中介绍了两种嵌入范式：语义嵌入和上下文嵌入，并在数学、逻辑、常识和规划等四个领域进行了评估。结果表明，在连续推理下，上下文嵌入的性能与链式思维（CoT）相当，同时推理时间的FLOPs平均减少了一半。'}}}, {'id': 'https://huggingface.co/papers/2505.22019', 'title': 'VRAG-RL: Empower Vision-Perception-Based RAG for Visually Rich\n  Information Understanding via Iterative Reasoning with Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.22019', 'abstract': "VRAG-RL, a reinforcement learning framework, enhances reasoning and visual information handling in RAG methods by integrating visual perception tokens and employing specialized action spaces and rewards.  \t\t\t\t\tAI-generated summary \t\t\t\t Effectively retrieving, reasoning and understanding visually rich information remains a challenge for RAG methods. Traditional text-based methods cannot handle visual-related information. On the other hand, current vision-based RAG approaches are often limited by fixed pipelines and frequently struggle to reason effectively due to the insufficient activation of the fundamental capabilities of models. As RL has been proven to be beneficial for model reasoning, we introduce VRAG-RL, a novel RL framework tailored for complex reasoning across visually rich information. With this framework, VLMs interact with search engines, autonomously sampling single-turn or multi-turn reasoning trajectories with the help of visual perception tokens and undergoing continual optimization based on these samples. Our approach highlights key limitations of RL in RAG domains: (i) Prior Multi-modal RAG approaches tend to merely incorporate images into the context, leading to insufficient reasoning token allocation and neglecting visual-specific perception; and (ii) When models interact with search engines, their queries often fail to retrieve relevant information due to the inability to articulate requirements, thereby leading to suboptimal performance. To address these challenges, we define an action space tailored for visually rich inputs, with actions including cropping and scaling, allowing the model to gather information from a coarse-to-fine perspective. Furthermore, to bridge the gap between users' original inquiries and the retriever, we employ a simple yet effective reward that integrates query rewriting and retrieval performance with a model-based reward. Our VRAG-RL optimizes VLMs for RAG tasks using specially designed RL strategies, aligning the model with real-world applications. The code is available at https://github.com/Alibaba-NLP/VRAG{https://github.com/Alibaba-NLP/VRAG}.", 'score': 9, 'issue_id': 4017, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 мая', 'en': 'May 28', 'zh': '5月28日'}, 'hash': '913f87ab16364e21', 'authors': ['Qiuchen Wang', 'Ruixue Ding', 'Yu Zeng', 'Zehui Chen', 'Lin Chen', 'Shihang Wang', 'Pengjun Xie', 'Fei Huang', 'Feng Zhao'], 'affiliations': ['MoE Key Laboratory of Brain-inspired Intelligent Perception and Cognition, USTC', 'Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2505.22019.jpg', 'data': {'categories': ['#rl', '#rag', '#multimodal', '#reasoning', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'VRAG-RL: Улучшение визуальных рассуждений в RAG с помощью обучения с подкреплением', 'desc': 'VRAG-RL - это новая система обучения с подкреплением для улучшения рассуждений и обработки визуальной информации в методах RAG. Она интегрирует токены визуального восприятия и использует специализированные пространства действий и вознаграждения. VRAG-RL позволяет моделям взаимодействовать с поисковыми системами, автономно выбирая траектории рассуждений с помощью визуальных токенов. Система оптимизирует мультимодальные языковые модели для задач RAG, используя специально разработанные стратегии обучения с подкреплением.'}, 'en': {'title': 'Enhancing Visual Reasoning in RAG with VRAG-RL', 'desc': "VRAG-RL is a reinforcement learning framework designed to improve the reasoning and visual information processing capabilities of Retrieval-Augmented Generation (RAG) methods. It addresses the limitations of traditional text-based approaches and current vision-based RAG methods by integrating visual perception tokens and creating specialized action spaces for better interaction with search engines. The framework allows Vision-Language Models (VLMs) to autonomously sample reasoning trajectories and optimize their performance through a tailored reward system that enhances query effectiveness. By focusing on visually rich inputs and refining the model's ability to articulate information needs, VRAG-RL aims to bridge the gap between user queries and relevant data retrieval."}, 'zh': {'title': '提升视觉推理能力的VRAG-RL框架', 'desc': 'VRAG-RL是一种强化学习框架，旨在提升RAG方法在推理和视觉信息处理方面的能力。它通过整合视觉感知标记和采用专门的动作空间与奖励机制，解决了传统文本方法无法处理视觉信息的问题。该框架允许视觉语言模型与搜索引擎互动，自主采样推理轨迹，并基于这些样本进行持续优化。VRAG-RL通过定义适合视觉丰富输入的动作空间和有效的奖励机制，优化了RAG任务中的模型表现。'}}}, {'id': 'https://huggingface.co/papers/2505.20779', 'title': 'CHIMERA: A Knowledge Base of Idea Recombination in Scientific Literature', 'url': 'https://huggingface.co/papers/2505.20779', 'abstract': 'A large-scale knowledge base of recombination examples is built from scientific paper abstracts using an LLM-based extraction model to analyze and inspire new creative directions in AI.  \t\t\t\t\tAI-generated summary \t\t\t\t A hallmark of human innovation is the process of recombination -- creating original ideas by integrating elements of existing mechanisms and concepts. In this work, we automatically mine the scientific literature and build CHIMERA: a large-scale knowledge base (KB) of recombination examples. CHIMERA can be used to empirically explore at scale how scientists recombine concepts and take inspiration from different areas, or to train supervised machine learning models that learn to predict new creative cross-domain directions. To build this KB, we present a novel information extraction task of extracting recombination from scientific paper abstracts, collect a high-quality corpus of hundreds of manually annotated abstracts, and use it to train an LLM-based extraction model. The model is applied to a large corpus of papers in the AI domain, yielding a KB of over 28K recombination examples. We analyze CHIMERA to explore the properties of recombination in different subareas of AI. Finally, we train a scientific hypothesis generation model using the KB, which predicts new recombination directions that real-world researchers find inspiring. Our data and code are available at https://github.cs.huji.ac.il/tomhope-lab/CHIMERA', 'score': 8, 'issue_id': 4023, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': 'f73a43d6ad307bdc', 'authors': ['Noy Sternlicht', 'Tom Hope'], 'affiliations': ['School of Computer Science and Engineering, The Hebrew University of Jerusalem', 'The Allen Institute for AI (AI2)'], 'pdf_title_img': 'assets/pdf/title_img/2505.20779.jpg', 'data': {'categories': ['#science', '#multimodal', '#data', '#dataset', '#transfer_learning'], 'emoji': '🧬', 'ru': {'title': 'CHIMERA: вдохновение для инноваций через анализ научной литературы', 'desc': 'Исследователи создали CHIMERA - крупномасштабную базу знаний примеров рекомбинации, извлеченных из научных статей с помощью модели на основе больших языковых моделей (LLM). База знаний содержит более 28 тысяч примеров рекомбинации в области искусственного интеллекта. Анализ CHIMERA позволяет изучить свойства рекомбинации в различных подобластях ИИ. На основе этой базы знаний была обучена модель генерации научных гипотез, предсказывающая новые направления рекомбинации.'}, 'en': {'title': 'CHIMERA: Unleashing Innovation through Recombination in AI', 'desc': 'This paper presents CHIMERA, a large-scale knowledge base that captures examples of recombination from scientific paper abstracts using a large language model (LLM) for information extraction. The process of recombination is essential for innovation, as it involves merging existing ideas to create new concepts. By analyzing a vast corpus of AI literature, the authors built a dataset of over 28,000 recombination examples, which can be used to understand how ideas are integrated across different domains. Additionally, they developed a model that generates scientific hypotheses based on this knowledge base, helping researchers identify novel directions for exploration.'}, 'zh': {'title': '重组创新：从科学文献中提取灵感', 'desc': '本研究构建了一个名为CHIMERA的大规模知识库，专注于从科学论文摘要中提取重组示例。通过使用基于大语言模型的提取模型，我们能够自动挖掘科学文献，分析科学家如何将不同领域的概念进行重组。该知识库包含超过28,000个重组示例，可以用于训练监督学习模型，预测新的跨领域创意方向。最终，我们还利用该知识库训练了一个科学假设生成模型，以激发研究人员的灵感。'}}}, {'id': 'https://huggingface.co/papers/2505.22613', 'title': 'RICO: Improving Accuracy and Completeness in Image Recaptioning via\n  Visual Reconstruction', 'url': 'https://huggingface.co/papers/2505.22613', 'abstract': 'A novel iterative framework, RICO, improves image caption accuracy by using visual reconstruction and a text-to-image model to refine discrepancies, while RICO-Flash enhances efficiency using DPO.  \t\t\t\t\tAI-generated summary \t\t\t\t Image recaptioning is widely used to generate training datasets with enhanced quality for various multimodal tasks. Existing recaptioning methods typically rely on powerful multimodal large language models (MLLMs) to enhance textual descriptions, but often suffer from inaccuracies due to hallucinations and incompleteness caused by missing fine-grained details. To address these limitations, we propose RICO, a novel framework that refines captions through visual reconstruction. Specifically, we leverage a text-to-image model to reconstruct a caption into a reference image, and prompt an MLLM to identify discrepancies between the original and reconstructed images to refine the caption. This process is performed iteratively, further progressively promoting the generation of more faithful and comprehensive descriptions. To mitigate the additional computational cost induced by the iterative process, we introduce RICO-Flash, which learns to generate captions like RICO using DPO. Extensive experiments demonstrate that our approach significantly improves caption accuracy and completeness, outperforms most baselines by approximately 10% on both CapsBench and CompreCap. Code released at https://github.com/wangyuchi369/RICO.', 'score': 6, 'issue_id': 4015, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 мая', 'en': 'May 28', 'zh': '5月28日'}, 'hash': '0f01107a0f4b4350', 'authors': ['Yuchi Wang', 'Yishuo Cai', 'Shuhuai Ren', 'Sihan Yang', 'Linli Yao', 'Yuanxin Liu', 'Yuanxing Zhang', 'Pengfei Wan', 'Xu Sun'], 'affiliations': ['Central South University', 'Kuaishou Technology', 'National Key Laboratory for Multimedia Information Processing, Peking University', 'Xian JiaoTong University'], 'pdf_title_img': 'assets/pdf/title_img/2505.22613.jpg', 'data': {'categories': ['#optimization', '#dataset', '#training', '#open_source', '#hallucinations', '#rlhf', '#multimodal'], 'emoji': '🖼️', 'ru': {'title': 'Точные подписи к изображениям через визуальную реконструкцию', 'desc': 'RICO - это новая итеративная система для улучшения точности подписей к изображениям. Она использует визуальную реконструкцию и модель преобразования текста в изображение для уточнения несоответствий. RICO-Flash повышает эффективность процесса с помощью DPO (Direct Preference Optimization). Эксперименты показывают, что подход значительно улучшает точность и полноту подписей, превосходя большинство базовых методов примерно на 10% в тестах CapsBench и CompreCap.'}, 'en': {'title': 'RICO: Refining Image Captions with Visual Reconstruction', 'desc': 'The paper introduces RICO, an innovative framework designed to enhance the accuracy of image captions by utilizing visual reconstruction techniques. It employs a text-to-image model to create a reference image from a caption, allowing a multimodal large language model (MLLM) to identify and correct discrepancies between the original and reconstructed images. This iterative process helps generate more accurate and detailed captions by progressively refining them. To improve efficiency, the authors present RICO-Flash, which uses DPO to streamline the caption generation process while maintaining high quality.'}, 'zh': {'title': 'RICO：提升图像描述准确性的创新框架', 'desc': '本文提出了一种新的迭代框架RICO，通过视觉重建和文本到图像模型来提高图像描述的准确性。RICO利用文本到图像模型将描述重建为参考图像，并通过多模态大语言模型（MLLM）识别原始图像与重建图像之间的差异，从而逐步改进描述。为了提高效率，本文还引入了RICO-Flash，利用DPO技术来生成描述，减少计算成本。实验结果表明，该方法在CapsBench和CompreCap数据集上显著提高了描述的准确性和完整性，超越了大多数基线方法。'}}}, {'id': 'https://huggingface.co/papers/2505.22525', 'title': 'Thinking with Generated Images', 'url': 'https://huggingface.co/papers/2505.22525', 'abstract': 'Thinking with Generated Images allows large multimodal models to generate and critique intermediate visual steps, enhancing visual reasoning capabilities and achieving significant improvements in complex scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Thinking with Generated Images, a novel paradigm that fundamentally transforms how large multimodal models (LMMs) engage with visual reasoning by enabling them to natively think across text and vision modalities through spontaneous generation of intermediate visual thinking steps. Current visual reasoning with LMMs is constrained to either processing fixed user-provided images or reasoning solely through text-based chain-of-thought (CoT). Thinking with Generated Images unlocks a new dimension of cognitive capability where models can actively construct intermediate visual thoughts, critique their own visual hypotheses, and refine them as integral components of their reasoning process. We demonstrate the effectiveness of our approach through two complementary mechanisms: (1) vision generation with intermediate visual subgoals, where models decompose complex visual tasks into manageable components that are generated and integrated progressively, and (2) vision generation with self-critique, where models generate an initial visual hypothesis, analyze its shortcomings through textual reasoning, and produce refined outputs based on their own critiques. Our experiments on vision generation benchmarks show substantial improvements over baseline approaches, with our models achieving up to 50% (from 38% to 57%) relative improvement in handling complex multi-object scenarios. From biochemists exploring novel protein structures, and architects iterating on spatial designs, to forensic analysts reconstructing crime scenes, and basketball players envisioning strategic plays, our approach enables AI models to engage in the kind of visual imagination and iterative refinement that characterizes human creative, analytical, and strategic thinking. We release our open-source suite at https://github.com/GAIR-NLP/thinking-with-generated-images.', 'score': 6, 'issue_id': 4015, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 мая', 'en': 'May 28', 'zh': '5月28日'}, 'hash': '0d852a3cf6a78a01', 'authors': ['Ethan Chern', 'Zhulin Hu', 'Steffi Chern', 'Siqi Kou', 'Jiadi Su', 'Yan Ma', 'Zhijie Deng', 'Pengfei Liu'], 'affiliations': ['Fudan University', 'Generative AI Research Lab (GAIR)', 'SII', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2505.22525.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#cv', '#reasoning', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'Визуальное воображение ИИ: новый уровень мышления мультимодальных моделей', 'desc': "Статья представляет новую парадигму 'Мышление с генерируемыми изображениями', которая позволяет крупным мультимодальным моделям (LMM) генерировать и критиковать промежуточные визуальные шаги в процессе рассуждений. Этот подход существенно улучшает способности моделей к визуальному мышлению, позволяя им создавать промежуточные визуальные гипотезы и уточнять их. Эксперименты показали значительное улучшение результатов в сложных сценариях с несколькими объектами по сравнению с базовыми подходами. Авторы демонстрируют потенциал метода в различных областях, от биохимии до спортивной аналитики."}, 'en': {'title': 'Empowering AI with Visual Imagination and Self-Critique', 'desc': "The paper introduces 'Thinking with Generated Images', a new approach that enhances large multimodal models (LMMs) by allowing them to generate and evaluate intermediate visual steps during reasoning. This method enables models to create visual representations spontaneously, rather than relying solely on fixed images or text-based reasoning. By breaking down complex visual tasks into smaller, manageable components and allowing for self-critique, the models can refine their outputs iteratively. The results show significant improvements in performance on visual reasoning tasks, demonstrating the potential for AI to mimic human-like visual thinking and creativity."}, 'zh': {'title': '生成图像思维：提升视觉推理能力的创新方法', 'desc': '本文介绍了一种新颖的思维方式——生成图像思维，旨在提升大型多模态模型（LMMs）在视觉推理方面的能力。通过生成中间视觉步骤，模型能够在文本和视觉之间自发地进行思考，从而克服了传统方法的局限。该方法包括两个机制：一是通过中间视觉子目标分解复杂任务，二是通过自我批判分析初步视觉假设的不足。实验结果表明，该方法在处理复杂多对象场景时，相较于基线方法有显著提升，最高可达50%的相对改善。'}}}, {'id': 'https://huggingface.co/papers/2505.21876', 'title': 'EPiC: Efficient Video Camera Control Learning with Precise Anchor-Video\n  Guidance', 'url': 'https://huggingface.co/papers/2505.21876', 'abstract': 'EPiC is a framework for efficient 3D camera control in video diffusion models that constructs high-quality anchor videos through first-frame visibility masking, integrates them using a lightweight ControlNet module, and achieves state-of-the-art performance on I2V tasks with minimal resources.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent approaches on 3D camera control in video diffusion models (VDMs) often create anchor videos to guide diffusion models as a structured prior by rendering from estimated point clouds following annotated camera trajectories. However, errors inherent in point cloud estimation often lead to inaccurate anchor videos. Moreover, the requirement for extensive camera trajectory annotations further increases resource demands. To address these limitations, we introduce EPiC, an efficient and precise camera control learning framework that automatically constructs high-quality anchor videos without expensive camera trajectory annotations. Concretely, we create highly precise anchor videos for training by masking source videos based on first-frame visibility. This approach ensures high alignment, eliminates the need for camera trajectory annotations, and thus can be readily applied to any in-the-wild video to generate image-to-video (I2V) training pairs. Furthermore, we introduce Anchor-ControlNet, a lightweight conditioning module that integrates anchor video guidance in visible regions to pretrained VDMs, with less than 1% of backbone model parameters. By combining the proposed anchor video data and ControlNet module, EPiC achieves efficient training with substantially fewer parameters, training steps, and less data, without requiring modifications to the diffusion model backbone typically needed to mitigate rendering misalignments. Although being trained on masking-based anchor videos, our method generalizes robustly to anchor videos made with point clouds during inference, enabling precise 3D-informed camera control. EPiC achieves SOTA performance on RealEstate10K and MiraData for I2V camera control task, demonstrating precise and robust camera control ability both quantitatively and qualitatively. Notably, EPiC also exhibits strong zero-shot generalization to video-to-video scenarios.', 'score': 6, 'issue_id': 4015, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 мая', 'en': 'May 28', 'zh': '5月28日'}, 'hash': '2085924b7cd22768', 'authors': ['Zun Wang', 'Jaemin Cho', 'Jialu Li', 'Han Lin', 'Jaehong Yoon', 'Yue Zhang', 'Mohit Bansal'], 'affiliations': ['UNC Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2505.21876.jpg', 'data': {'categories': ['#optimization', '#video', '#training', '#transfer_learning', '#diffusion', '#3d', '#multimodal'], 'emoji': '🎥', 'ru': {'title': 'Эффективный 3D-контроль камеры без сложных аннотаций', 'desc': 'EPiC - это фреймворк для эффективного 3D-контроля камеры в моделях видеодиффузии. Он создает высококачественные опорные видео с помощью маскирования видимости первого кадра и интегрирует их с использованием облегченного модуля ControlNet. EPiC достигает наилучших результатов в задачах преобразования изображения в видео (I2V) при минимальных ресурсах. Этот подход не требует дорогостоящих аннотаций траектории камеры и может применяться к любому видео для создания обучающих пар I2V.'}, 'en': {'title': 'Efficient 3D Camera Control with EPiC', 'desc': 'EPiC is a novel framework designed to enhance 3D camera control in video diffusion models (VDMs) by creating high-quality anchor videos without the need for extensive camera trajectory annotations. It utilizes first-frame visibility masking to ensure that the generated anchor videos are accurately aligned with the source videos, thus improving the training process. The framework incorporates a lightweight ControlNet module that integrates these anchor videos into existing VDMs, achieving state-of-the-art performance on image-to-video (I2V) tasks while minimizing resource requirements. Additionally, EPiC demonstrates strong generalization capabilities, performing well even in zero-shot video-to-video scenarios, making it a versatile tool for efficient camera control.'}, 'zh': {'title': 'EPiC：高效的3D相机控制新框架', 'desc': 'EPiC是一个高效的3D相机控制框架，专为视频扩散模型设计。它通过第一帧可见性掩蔽自动构建高质量的锚视频，避免了昂贵的相机轨迹注释需求。该框架结合了轻量级的ControlNet模块，能够在资源有限的情况下实现图像到视频（I2V）任务的最先进性能。EPiC在RealEstate10K和MiraData数据集上表现出色，展现了其精确和稳健的相机控制能力。'}}}, {'id': 'https://huggingface.co/papers/2505.22523', 'title': 'PrismLayers: Open Data for High-Quality Multi-Layer Transparent Image\n  Generative Models', 'url': 'https://huggingface.co/papers/2505.22523', 'abstract': 'Generating high-quality, multi-layer transparent images from text prompts can unlock a new level of creative control, allowing users to edit each layer as effortlessly as editing text outputs from LLMs. However, the development of multi-layer generative models lags behind that of conventional text-to-image models due to the absence of a large, high-quality corpus of multi-layer transparent data. In this paper, we address this fundamental challenge by: (i) releasing the first open, ultra-high-fidelity PrismLayers (PrismLayersPro) dataset of 200K (20K) multilayer transparent images with accurate alpha mattes, (ii) introducing a trainingfree synthesis pipeline that generates such data on demand using off-the-shelf diffusion models, and (iii) delivering a strong, open-source multi-layer generation model, ART+, which matches the aesthetics of modern text-to-image generation models. The key technical contributions include: LayerFLUX, which excels at generating high-quality single transparent layers with accurate alpha mattes, and MultiLayerFLUX, which composes multiple LayerFLUX outputs into complete images, guided by human-annotated semantic layout. To ensure higher quality, we apply a rigorous filtering stage to remove artifacts and semantic mismatches, followed by human selection. Fine-tuning the state-of-the-art ART model on our synthetic PrismLayersPro yields ART+, which outperforms the original ART in 60% of head-to-head user study comparisons and even matches the visual quality of images generated by the FLUX.1-[dev] model. We anticipate that our work will establish a solid dataset foundation for the multi-layer transparent image generation task, enabling research and applications that require precise, editable, and visually compelling layered imagery.', 'score': 5, 'issue_id': 4013, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 мая', 'en': 'May 28', 'zh': '5月28日'}, 'hash': '2f373283a5bfede3', 'authors': ['Junwen Chen', 'Heyang Jiang', 'Yanbin Wang', 'Keming Wu', 'Ji Li', 'Chao Zhang', 'Keiji Yanai', 'Dong Chen', 'Yuhui Yuan'], 'affiliations': ['Microsoft Research Asia'], 'pdf_title_img': 'assets/pdf/title_img/2505.22523.jpg', 'data': {'categories': ['#data', '#cv', '#diffusion', '#dataset', '#open_source', '#synthetic'], 'emoji': '🎨', 'ru': {'title': 'Прорыв в создании редактируемых многослойных изображений с помощью ИИ', 'desc': 'Данная статья представляет новый подход к генерации многослойных прозрачных изображений на основе текстовых запросов. Авторы создали первый открытый набор данных PrismLayers высокого качества, содержащий 200 тысяч многослойных прозрачных изображений с точными альфа-масками. Они также разработали метод синтеза таких данных по запросу, используя существующие диффузионные модели. В результате была создана модель ART+, превосходящая предыдущие аналоги в генерации многослойных изображений.'}, 'en': {'title': 'Unlocking Creative Control with Multi-Layer Transparent Image Generation', 'desc': 'This paper presents a novel approach to generating high-quality, multi-layer transparent images from text prompts, enhancing creative control for users. The authors introduce the PrismLayersPro dataset, which contains 200,000 multilayer transparent images with accurate alpha mattes, addressing the lack of quality data in this area. They also propose a training-free synthesis pipeline that utilizes existing diffusion models to create these images on demand. Additionally, the ART+ model is developed, which outperforms previous models in user studies and provides a foundation for future research in editable layered imagery.'}, 'zh': {'title': '开启多层透明图像生成的新篇章', 'desc': '本文提出了一种生成高质量多层透明图像的方法，允许用户像编辑文本一样轻松编辑每一层。我们发布了首个开放的超高保真PrismLayers数据集，包含20万张带有准确alpha通道的多层透明图像。我们还引入了一种无训练合成管道，利用现成的扩散模型按需生成数据，并提供了开源的多层生成模型ART+，其美学与现代文本到图像生成模型相匹配。通过严格的过滤和人工选择，我们确保生成的图像质量更高，ART+在用户研究中表现优于原始ART模型。'}}}, {'id': 'https://huggingface.co/papers/2505.22338', 'title': 'Text2Grad: Reinforcement Learning from Natural Language Feedback', 'url': 'https://huggingface.co/papers/2505.22338', 'abstract': "Traditional RLHF optimizes language models with coarse, scalar rewards that mask the fine-grained reasons behind success or failure, leading to slow and opaque learning. Recent work augments RL with textual critiques through prompting or reflection, improving interpretability but leaving model parameters untouched. We introduce Text2Grad, a reinforcement-learning paradigm that turns free-form textual feedback into span-level gradients. Given human (or programmatic) critiques, Text2Grad aligns each feedback phrase with the relevant token spans, converts these alignments into differentiable reward signals, and performs gradient updates that directly refine the offending portions of the model's policy. This yields precise, feedback-conditioned adjustments instead of global nudges. Text2Grad is realized through three components: (1) a high-quality feedback-annotation pipeline that pairs critiques with token spans; (2) a fine-grained reward model that predicts span-level reward on answer while generating explanatory critiques; and (3) a span-level policy optimizer that back-propagates natural-language gradients. Across summarization, code generation, and question answering, Text2Grad consistently surpasses scalar-reward RL and prompt-only baselines, providing both higher task metrics and richer interpretability. Our results demonstrate that natural-language feedback, when converted to gradients, is a powerful signal for fine-grained policy optimization. The code for our method is available at https://github.com/microsoft/Text2Grad", 'score': 5, 'issue_id': 4013, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 мая', 'en': 'May 28', 'zh': '5月28日'}, 'hash': '6a628958ae1efdcc', 'authors': ['Hanyang Wang', 'Lu Wang', 'Chaoyun Zhang', 'Tianjun Mao', 'Si Qin', 'Qingwei Lin', 'Saravan Rajmohan', 'Dongmei Zhang'], 'affiliations': ['Fudan University', 'Microsoft', 'University of Chicago'], 'pdf_title_img': 'assets/pdf/title_img/2505.22338.jpg', 'data': {'categories': ['#optimization', '#training', '#interpretability', '#rlhf'], 'emoji': '🔍', 'ru': {'title': 'Text2Grad: Точная настройка языковых моделей с помощью текстовых градиентов', 'desc': 'Статья представляет новый подход к обучению языковых моделей под названием Text2Grad. Этот метод преобразует текстовые отзывы в градиенты на уровне отдельных фрагментов текста, что позволяет более точно настраивать модель. Text2Grad состоит из трех компонентов: системы аннотирования отзывов, модели вознаграждения на уровне фрагментов и оптимизатора политики. Эксперименты показали, что Text2Grad превосходит традиционные методы обучения с подкреплением в задачах суммаризации, генерации кода и ответов на вопросы.'}, 'en': {'title': 'Transforming Textual Feedback into Targeted Learning Signals', 'desc': "This paper presents Text2Grad, a novel approach in reinforcement learning from human feedback (RLHF) that enhances language model training by using detailed textual critiques. Unlike traditional methods that rely on simple scalar rewards, Text2Grad translates free-form feedback into specific gradient updates for model parameters, allowing for more precise adjustments. The method involves a feedback-annotation pipeline, a fine-grained reward model, and a span-level policy optimizer to ensure that critiques are effectively aligned with the relevant parts of the model's output. The results show that Text2Grad outperforms existing RLHF techniques in various tasks, providing better performance metrics and improved interpretability of the model's learning process."}, 'zh': {'title': 'Text2Grad：将文本反馈转化为梯度的强化学习新方法', 'desc': '传统的强化学习人类反馈（RLHF）使用粗略的标量奖励来优化语言模型，这种方法掩盖了成功或失败的细微原因，导致学习过程缓慢且不透明。最近的研究通过提示或反思增强了RL与文本批评的结合，提高了可解释性，但未能改变模型参数。我们提出了Text2Grad，这是一种强化学习范式，将自由形式的文本反馈转化为跨度级梯度。Text2Grad通过将人类或程序化的批评与相关的标记跨度对齐，生成可微分的奖励信号，从而实现对模型策略的精确调整。'}}}, {'id': 'https://huggingface.co/papers/2505.18227', 'title': 'Token Reduction Should Go Beyond Efficiency in Generative Models -- From\n  Vision, Language to Multimodality', 'url': 'https://huggingface.co/papers/2505.18227', 'abstract': 'In Transformer architectures, tokens\\textemdash discrete units derived from raw data\\textemdash are formed by segmenting inputs into fixed-length chunks. Each token is then mapped to an embedding, enabling parallel attention computations while preserving the input\'s essential information. Due to the quadratic computational complexity of transformer self-attention mechanisms, token reduction has primarily been used as an efficiency strategy. This is especially true in single vision and language domains, where it helps balance computational costs, memory usage, and inference latency. Despite these advances, this paper argues that token reduction should transcend its traditional efficiency-oriented role in the era of large generative models. Instead, we position it as a fundamental principle in generative modeling, critically influencing both model architecture and broader applications. Specifically, we contend that across vision, language, and multimodal systems, token reduction can: (i) facilitate deeper multimodal integration and alignment, (ii) mitigate "overthinking" and hallucinations, (iii) maintain coherence over long inputs, and (iv) enhance training stability, etc. We reframe token reduction as more than an efficiency measure. By doing so, we outline promising future directions, including algorithm design, reinforcement learning-guided token reduction, token optimization for in-context learning, and broader ML and scientific domains. We highlight its potential to drive new model architectures and learning strategies that improve robustness, increase interpretability, and better align with the objectives of generative modeling.', 'score': 5, 'issue_id': 4024, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 мая', 'en': 'May 23', 'zh': '5月23日'}, 'hash': '404502beaef00992', 'authors': ['Zhenglun Kong', 'Yize Li', 'Fanhu Zeng', 'Lei Xin', 'Shvat Messica', 'Xue Lin', 'Pu Zhao', 'Manolis Kellis', 'Hao Tang', 'Marinka Zitnik'], 'affiliations': ['Chinese Academy of Sciences', 'Harvard University', 'Massachusetts Institute of Technology', 'Northeastern University', 'Peking University', 'Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2505.18227.jpg', 'data': {'categories': ['#rl', '#optimization', '#multimodal', '#interpretability', '#long_context', '#architecture', '#hallucinations', '#alignment', '#training'], 'emoji': '🔬', 'ru': {'title': 'Уменьшение токенов: от эффективности к новым горизонтам генеративного моделирования', 'desc': 'Статья исследует роль уменьшения токенов в архитектурах трансформеров, выходя за рамки традиционного подхода к эффективности. Авторы утверждают, что уменьшение токенов может улучшить мультимодальную интеграцию, снизить галлюцинации и повысить стабильность обучения в генеративных моделях. Предлагается рассматривать уменьшение токенов как фундаментальный принцип в генеративном моделировании, влияющий на архитектуру модели и ее применения. Статья намечает перспективные направления исследований, включая дизайн алгоритмов и оптимизацию токенов для обучения в контексте.'}, 'en': {'title': 'Token Reduction: A Key to Enhanced Generative Modeling', 'desc': 'This paper discusses the role of token reduction in Transformer architectures, which are commonly used in machine learning for processing data. Traditionally, token reduction has been seen as a way to improve efficiency by reducing computational costs and memory usage. However, the authors argue that token reduction should be viewed as a fundamental principle that can enhance generative modeling across various domains, including vision and language. They propose that effective token reduction can lead to better model integration, reduce errors, and improve the overall stability and coherence of models.'}, 'zh': {'title': '令牌减少：生成建模的新原则', 'desc': '在Transformer架构中，令牌是从原始数据中提取的离散单元，通过将输入分割成固定长度的块来形成。每个令牌被映射到一个嵌入，使得在保持输入关键信息的同时能够进行并行注意力计算。尽管令牌减少主要被视为提高效率的策略，但本文认为在大型生成模型时代，令牌减少应超越传统的效率导向角色，成为生成建模的基本原则。我们提出，令牌减少可以促进多模态整合、减轻“过度思考”和幻觉、保持长输入的一致性，并增强训练稳定性等。'}}}, {'id': 'https://huggingface.co/papers/2505.22203', 'title': 'Pitfalls of Rule- and Model-based Verifiers -- A Case Study on\n  Mathematical Reasoning', 'url': 'https://huggingface.co/papers/2505.22203', 'abstract': 'The study examines the effectiveness and reliability of rule-based and model-based verifiers in reinforcement learning with verifiable reward, highlighting limitations and vulnerabilities in their use for mathematical reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Trustworthy verifiers are essential for the success of reinforcement learning with verifiable reward (RLVR), which is the core methodology behind various large reasoning models such as DeepSeek-R1. In complex domains like mathematical reasoning, rule-based verifiers have been widely adopted in previous works to train strong reasoning models. However, the reliability of these verifiers and their impact on the RL training process remain poorly understood. In this work, we take mathematical reasoning as a case study and conduct a comprehensive analysis of various verifiers in both static evaluation and RL training scenarios. First, we find that current open-source rule-based verifiers often fail to recognize equivalent answers presented in different formats across multiple commonly used mathematical datasets, resulting in non-negligible false negative rates. This limitation adversely affects RL training performance and becomes more pronounced as the policy model gets stronger. Subsequently, we investigate model-based verifiers as a potential solution to address these limitations. While the static evaluation shows that model-based verifiers achieve significantly higher verification accuracy, further analysis and RL training results imply that they are highly susceptible to hacking, where they misclassify certain patterns in responses as correct (i.e., false positives). This vulnerability is exploited during policy model optimization, leading to artificially inflated rewards. Our findings underscore the unique risks inherent to both rule-based and model-based verifiers, aiming to offer valuable insights to develop more robust reward systems in reinforcement learning.', 'score': 4, 'issue_id': 4015, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 мая', 'en': 'May 28', 'zh': '5月28日'}, 'hash': 'bb47fbc50ee1bc3f', 'authors': ['Yuzhen Huang', 'Weihao Zeng', 'Xingshan Zeng', 'Qi Zhu', 'Junxian He'], 'affiliations': ['The Chinese University of Hong Kong', 'The Hong Kong University of Science and Technology', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.22203.jpg', 'data': {'categories': ['#optimization', '#training', '#math', '#rl', '#security', '#reasoning'], 'emoji': '🔍', 'ru': {'title': 'Ограничения верификаторов в RLVR: необходимость более надежных систем', 'desc': 'Исследование анализирует эффективность и надежность верификаторов на основе правил и моделей в обучении с подкреплением с проверяемым вознаграждением (RLVR). Авторы обнаружили, что верификаторы на основе правил часто не распознают эквивалентные ответы в разных форматах, что негативно влияет на процесс обучения. Модельные верификаторы показывают более высокую точность, но подвержены уязвимостям и могут быть обмануты во время оптимизации политики. Результаты подчеркивают риски обоих типов верификаторов и необходимость разработки более надежных систем вознаграждения в обучении с подкреплением.'}, 'en': {'title': 'Enhancing Trust in Reinforcement Learning Verifiers', 'desc': 'This paper investigates the effectiveness of rule-based and model-based verifiers in reinforcement learning with verifiable rewards (RLVR), particularly in the context of mathematical reasoning tasks. It highlights that rule-based verifiers often fail to recognize equivalent answers in different formats, leading to false negatives that hinder the training of reinforcement learning models. The study also reveals that while model-based verifiers show higher accuracy in static evaluations, they are vulnerable to misclassifying incorrect patterns as correct, resulting in false positives that can inflate rewards during training. Overall, the research emphasizes the need for more reliable verification methods to enhance the robustness of reward systems in RLVR applications.'}, 'zh': {'title': '强化学习中的验证器风险与挑战', 'desc': '本研究探讨了在可验证奖励的强化学习中，基于规则和基于模型的验证器的有效性和可靠性。我们发现，当前的基于规则的验证器在识别不同格式的等效答案时存在显著的假阴性率，这对强化学习的训练性能产生了负面影响。虽然基于模型的验证器在静态评估中表现出更高的验证准确性，但它们在强化学习训练中容易受到攻击，导致假阳性现象。我们的研究揭示了这两种验证器的独特风险，为开发更强大的强化学习奖励系统提供了重要见解。'}}}, {'id': 'https://huggingface.co/papers/2505.18700', 'title': 'GRE Suite: Geo-localization Inference via Fine-Tuned Vision-Language\n  Models and Enhanced Reasoning Chains', 'url': 'https://huggingface.co/papers/2505.18700', 'abstract': 'Recent advances in Visual Language Models (VLMs) have demonstrated exceptional performance in visual reasoning tasks. However, geo-localization presents unique challenges, requiring the extraction of multigranular visual cues from images and their integration with external world knowledge for systematic reasoning. Current approaches to geo-localization tasks often lack robust reasoning mechanisms and explainability, limiting their effectiveness. To address these limitations, we propose the Geo Reason Enhancement (GRE) Suite, a novel framework that augments VLMs with structured reasoning chains for accurate and interpretable location inference. The GRE Suite is systematically developed across three key dimensions: dataset, model, and benchmark. First, we introduce GRE30K, a high-quality geo-localization reasoning dataset designed to facilitate fine-grained visual and contextual analysis. Next, we present the GRE model, which employs a multi-stage reasoning strategy to progressively infer scene attributes, local details, and semantic features, thereby narrowing down potential geographic regions with enhanced precision. Finally, we construct the Geo Reason Evaluation Benchmark (GREval-Bench), a comprehensive evaluation framework that assesses VLMs across diverse urban, natural, and landmark scenes to measure both coarse-grained (e.g., country, continent) and fine-grained (e.g., city, street) localization performance. Experimental results demonstrate that GRE significantly outperforms existing methods across all granularities of geo-localization tasks, underscoring the efficacy of reasoning-augmented VLMs in complex geographic inference. Code and data will be released at https://github.com/Thorin215/GRE.', 'score': 4, 'issue_id': 4014, 'pub_date': '2025-05-24', 'pub_date_card': {'ru': '24 мая', 'en': 'May 24', 'zh': '5月24日'}, 'hash': '6e364db427c53c05', 'authors': ['Chun Wang', 'Xiaoran Pan', 'Zihao Pan', 'Haofan Wang', 'Yiren Song'], 'affiliations': ['LibLib.ai', 'NUS', 'Sun Yat-sen University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.18700.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#cv', '#interpretability', '#reasoning'], 'emoji': '🌍', 'ru': {'title': 'Умное определение местоположения: VLM с усиленным географическим рассуждением', 'desc': 'Статья представляет набор инструментов Geo Reason Enhancement (GRE) Suite для улучшения геолокализации с помощью визуальных языковых моделей (VLM). GRE Suite включает в себя новый датасет GRE30K, модель GRE с многоступенчатой стратегией рассуждений, и бенчмарк GREval-Bench для оценки производительности. Подход основан на структурированных цепочках рассуждений для точного и интерпретируемого определения местоположения. Экспериментальные результаты показывают значительное превосходство GRE над существующими методами в задачах геолокализации различной гранулярности.'}, 'en': {'title': 'Enhancing Geo-Localization with Structured Reasoning', 'desc': 'This paper introduces the Geo Reason Enhancement (GRE) Suite, a framework designed to improve visual language models (VLMs) for geo-localization tasks. It addresses the challenges of extracting detailed visual cues and integrating them with external knowledge for better reasoning. The GRE Suite includes a new dataset called GRE30K, a model that uses multi-stage reasoning to enhance location inference, and a benchmark for evaluating performance across various geographic contexts. Experimental results show that the GRE Suite significantly outperforms existing methods, highlighting the importance of structured reasoning in visual localization.'}, 'zh': {'title': '增强推理，精准定位！', 'desc': '最近，视觉语言模型（VLMs）在视觉推理任务中表现出色。然而，地理定位面临独特挑战，需要从图像中提取多层次的视觉线索，并将其与外部世界知识结合进行系统推理。目前的地理定位方法往往缺乏稳健的推理机制和可解释性，限制了其有效性。为了解决这些问题，我们提出了Geo Reason Enhancement（GRE）套件，这是一种新框架，通过结构化推理链增强VLMs，以实现准确且可解释的定位推断。'}}}, {'id': 'https://huggingface.co/papers/2505.17870', 'title': 'Just as Humans Need Vaccines, So Do Models: Model Immunization to Combat\n  Falsehoods', 'url': 'https://huggingface.co/papers/2505.17870', 'abstract': 'A generative AI model is fine-tuned with labeled falsehoods to reduce misinformation generation, analogous to biological immunization.  \t\t\t\t\tAI-generated summary \t\t\t\t Generative AI models often learn and reproduce false information present in their training corpora. This position paper argues that, analogous to biological immunization, where controlled exposure to a weakened pathogen builds immunity, AI models should be fine tuned on small, quarantined sets of explicitly labeled falsehoods as a "vaccine" against misinformation. These curated false examples are periodically injected during finetuning, strengthening the model ability to recognize and reject misleading claims while preserving accuracy on truthful inputs. An illustrative case study shows that immunized models generate substantially less misinformation than baselines. To our knowledge, this is the first training framework that treats fact checked falsehoods themselves as a supervised vaccine, rather than relying on input perturbations or generic human feedback signals, to harden models against future misinformation. We also outline ethical safeguards and governance controls to ensure the safe use of false data. Model immunization offers a proactive paradigm for aligning AI systems with factuality.', 'score': 4, 'issue_id': 4017, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 мая', 'en': 'May 23', 'zh': '5月23日'}, 'hash': '434efd8595252228', 'authors': ['Shaina Raza', 'Rizwan Qureshi', 'Marcelo Lotif', 'Aman Chadha', 'Deval Pandya', 'Christos Emmanouilidis'], 'affiliations': ['Amazon Web Services', 'University of Central Florida, Orlando, USA', 'University of Groningen, Netherlands', 'Vector Institute, Toronto, Canada'], 'pdf_title_img': 'assets/pdf/title_img/2505.17870.jpg', 'data': {'categories': ['#training', '#rlhf', '#alignment', '#ethics', '#hallucinations'], 'emoji': '💉', 'ru': {'title': 'Вакцинация ИИ против дезинформации', 'desc': 'Статья предлагает новый метод обучения генеративных моделей ИИ для снижения генерации дезинформации. Метод основан на аналогии с биологической иммунизацией и включает дообучение модели на небольших наборах размеченных ложных данных. Это позволяет модели лучше распознавать и отвергать ложные утверждения, сохраняя при этом точность на правдивых данных. Авторы проводят тематическое исследование, демонстрирующее эффективность метода, и обсуждают этические аспекты использования ложных данных в обучении.'}, 'en': {'title': 'Immunizing AI Against Misinformation: A Proactive Approach', 'desc': "This paper presents a novel approach to combat misinformation in generative AI models by fine-tuning them with labeled falsehoods, similar to how vaccines work in biology. By exposing the model to controlled examples of misinformation, it learns to identify and reject misleading claims while maintaining its ability to generate accurate information. The study demonstrates that models trained with this 'immunization' technique produce significantly less misinformation compared to traditional methods. Additionally, the authors propose ethical guidelines to ensure the responsible use of false data in this training process."}, 'zh': {'title': '用虚假信息“免疫”AI模型，抵御错误信息', 'desc': '这篇论文提出了一种新的方法，通过对生成性人工智能模型进行微调，使用标记的虚假信息来减少错误信息的生成。这种方法类似于生物免疫，通过控制暴露于减弱病原体来建立免疫力。研究表明，定期注入这些经过筛选的虚假示例可以增强模型识别和拒绝误导性声明的能力，同时保持对真实输入的准确性。该框架首次将经过事实检查的虚假信息视为监督疫苗，以增强模型抵御未来错误信息的能力。'}}}, {'id': 'https://huggingface.co/papers/2505.21960', 'title': 'One-Way Ticket:Time-Independent Unified Encoder for Distilling\n  Text-to-Image Diffusion Models', 'url': 'https://huggingface.co/papers/2505.21960', 'abstract': 'Time-independent Unified Encoder TiUE reduces inference time and improves diversity and quality in Text-to-Image diffusion models by sharing encoder features across decoder time steps.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-Image (T2I) diffusion models have made remarkable advancements in generative modeling; however, they face a trade-off between inference speed and image quality, posing challenges for efficient deployment. Existing distilled T2I models can generate high-fidelity images with fewer sampling steps, but often struggle with diversity and quality, especially in one-step models. From our analysis, we observe redundant computations in the UNet encoders. Our findings suggest that, for T2I diffusion models, decoders are more adept at capturing richer and more explicit semantic information, while encoders can be effectively shared across decoders from diverse time steps. Based on these observations, we introduce the first Time-independent Unified Encoder TiUE for the student model UNet architecture, which is a loop-free image generation approach for distilling T2I diffusion models. Using a one-pass scheme, TiUE shares encoder features across multiple decoder time steps, enabling parallel sampling and significantly reducing inference time complexity. In addition, we incorporate a KL divergence term to regularize noise prediction, which enhances the perceptual realism and diversity of the generated images. Experimental results demonstrate that TiUE outperforms state-of-the-art methods, including LCM, SD-Turbo, and SwiftBrushv2, producing more diverse and realistic results while maintaining the computational efficiency.', 'score': 3, 'issue_id': 4015, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 мая', 'en': 'May 28', 'zh': '5月28日'}, 'hash': 'fff7f01ccce8324f', 'authors': ['Senmao Li', 'Lei Wang', 'Kai Wang', 'Tao Liu', 'Jiehang Xie', 'Joost van de Weijer', 'Fahad Shahbaz Khan', 'Shiqi Yang', 'Yaxing Wang', 'Jian Yang'], 'affiliations': ['Computer Vision Center, Universitat Aut`onoma de Barcelona', 'Linkoping University', 'Mohamed bin Zayed University of AI', 'Nankai International Advanced Research Institute (Shenzhen Futian), Nankai University', 'SB Intuitions, SoftBank', 'School of Big Data and Computer Science, Guizhou Normal University', 'VCIP, CS, Nankai University'], 'pdf_title_img': 'assets/pdf/title_img/2505.21960.jpg', 'data': {'categories': ['#architecture', '#optimization', '#diffusion', '#cv', '#inference'], 'emoji': '🖼️', 'ru': {'title': 'Ускорение генерации изображений без потери качества', 'desc': 'Статья представляет Time-independent Unified Encoder (TiUE) - новый подход к дистилляции диффузионных моделей для генерации изображений по тексту. TiUE использует единый энкодер для разных временных шагов декодера, что значительно сокращает время вывода. Авторы также вводят регуляризацию KL-дивергенцией для улучшения качества и разнообразия генерируемых изображений. Эксперименты показывают, что TiUE превосходит современные методы по качеству результатов при сохранении вычислительной эффективности.'}, 'en': {'title': 'Accelerating Image Generation with Unified Encoding', 'desc': 'The paper introduces the Time-independent Unified Encoder (TiUE), a novel approach to enhance Text-to-Image (T2I) diffusion models. By sharing encoder features across different decoder time steps, TiUE reduces the inference time while improving the diversity and quality of generated images. The study highlights that decoders are better at capturing semantic information, allowing for a more efficient use of encoder resources. Experimental results show that TiUE surpasses existing models in generating high-fidelity images with greater diversity and lower computational costs.'}, 'zh': {'title': '时间独立统一编码器TiUE：提升推理速度与图像质量的创新方案', 'desc': '本文提出了一种新的时间独立统一编码器TiUE，旨在提高文本到图像扩散模型的推理速度和图像质量。通过在解码器的多个时间步骤之间共享编码器特征，TiUE显著减少了推理时间的复杂性。研究表明，解码器在捕捉丰富的语义信息方面更为有效，而编码器可以在不同时间步骤之间有效共享。实验结果显示，TiUE在生成多样性和真实感方面优于现有的最先进方法，同时保持了计算效率。'}}}, {'id': 'https://huggingface.co/papers/2505.20298', 'title': 'MangaVQA and MangaLMM: A Benchmark and Specialized Model for Multimodal\n  Manga Understanding', 'url': 'https://huggingface.co/papers/2505.20298', 'abstract': 'Manga, or Japanese comics, is a richly multimodal narrative form that blends images and text in complex ways. Teaching large multimodal models (LMMs) to understand such narratives at a human-like level could help manga creators reflect on and refine their stories. To this end, we introduce two benchmarks for multimodal manga understanding: MangaOCR, which targets in-page text recognition, and MangaVQA, a novel benchmark designed to evaluate contextual understanding through visual question answering. MangaVQA consists of 526 high-quality, manually constructed question-answer pairs, enabling reliable evaluation across diverse narrative and visual scenarios. Building on these benchmarks, we develop MangaLMM, a manga-specialized model finetuned from the open-source LMM Qwen2.5-VL to jointly handle both tasks. Through extensive experiments, including comparisons with proprietary models such as GPT-4o and Gemini 2.5, we assess how well LMMs understand manga. Our benchmark and model provide a comprehensive foundation for evaluating and advancing LMMs in the richly narrative domain of manga.', 'score': 3, 'issue_id': 4022, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 мая', 'en': 'May 26', 'zh': '5月26日'}, 'hash': '52a09929449c8817', 'authors': ['Jeonghun Baek', 'Kazuki Egashira', 'Shota Onohara', 'Atsuyuki Miyai', 'Yuki Imajuku', 'Hikaru Ikuta', 'Kiyoharu Aizawa'], 'affiliations': ['The University of Tokyo'], 'pdf_title_img': 'assets/pdf/title_img/2505.20298.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#multimodal', '#training', '#games'], 'emoji': '📚', 'ru': {'title': 'Новые бенчмарки и модель для глубокого понимания манги искусственным интеллектом', 'desc': 'Статья представляет два новых бенчмарка для оценки понимания манги мультимодальными моделями: MangaOCR для распознавания текста и MangaVQA для контекстуального понимания через визуальные вопросы и ответы. Авторы разработали специализированную модель MangaLMM на основе Qwen2.5-VL для решения обеих задач. Проведены эксперименты по сравнению MangaLMM с проприетарными моделями вроде GPT-4 и Gemini 2.5. Бенчмарки и модель создают основу для оценки и улучшения больших мультимодальных моделей в сфере понимания нарративных элементов манги.'}, 'en': {'title': 'Enhancing Manga Understanding with Multimodal Models', 'desc': 'This paper presents a study on improving large multimodal models (LMMs) to understand manga, which combines images and text. It introduces two benchmarks: MangaOCR for recognizing text within manga pages and MangaVQA for assessing contextual understanding through visual question answering. The authors develop MangaLMM, a specialized model fine-tuned from Qwen2.5-VL, to effectively perform both tasks. Through experiments comparing it with advanced models like GPT-4o and Gemini 2.5, the paper aims to enhance the evaluation and capabilities of LMMs in the unique narrative style of manga.'}, 'zh': {'title': '提升漫画理解的多模态模型', 'desc': '本论文介绍了漫画（日本漫画）作为一种多模态叙事形式，结合了图像和文本。我们提出了两个基准测试，MangaOCR用于识别页面内的文本，MangaVQA用于通过视觉问答评估上下文理解。MangaVQA包含526对高质量的手动构建问答对，能够在多样的叙事和视觉场景中进行可靠评估。基于这些基准，我们开发了MangaLMM，一个专门针对漫画的模型，经过微调以同时处理这两个任务。'}}}, {'id': 'https://huggingface.co/papers/2505.17507', 'title': 'Benchmarking Recommendation, Classification, and Tracing Based on\n  Hugging Face Knowledge Graph', 'url': 'https://huggingface.co/papers/2505.17507', 'abstract': 'HuggingKG, a large-scale knowledge graph, enhances open source ML resource management by enabling advanced queries and analyses via HuggingBench.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid growth of open source machine learning (ML) resources, such as models and datasets, has accelerated IR research. However, existing platforms like Hugging Face do not explicitly utilize structured representations, limiting advanced queries and analyses such as tracing model evolution and recommending relevant datasets. To fill the gap, we construct HuggingKG, the first large-scale knowledge graph built from the Hugging Face community for ML resource management. With 2.6 million nodes and 6.2 million edges, HuggingKG captures domain-specific relations and rich textual attributes. It enables us to further present HuggingBench, a multi-task benchmark with three novel test collections for IR tasks including resource recommendation, classification, and tracing. Our experiments reveal unique characteristics of HuggingKG and the derived tasks. Both resources are publicly available, expected to advance research in open source resource sharing and management.', 'score': 3, 'issue_id': 4013, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 мая', 'en': 'May 23', 'zh': '5月23日'}, 'hash': '5f55eec50d967abe', 'authors': ['Qiaosheng Chen', 'Kaijia Huang', 'Xiao Zhou', 'Weiqing Luo', 'Yuanning Cui', 'Gong Cheng'], 'affiliations': ['Nanjing University State Key Laboratory for Novel Software Technology Nanjing, Jiangsu, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.17507.jpg', 'data': {'categories': ['#dataset', '#open_source', '#benchmark', '#graphs'], 'emoji': '🧠', 'ru': {'title': 'HuggingKG: Структурированное представление ресурсов ML для продвинутого анализа', 'desc': 'HuggingKG - это первый крупномасштабный граф знаний, созданный на основе сообщества Hugging Face для управления ресурсами машинного обучения. Он содержит 2,6 миллиона узлов и 6,2 миллиона связей, отражающих отношения и атрибуты в области ML. HuggingKG позволяет создать HuggingBench - многозадачный бенчмарк для задач информационного поиска, включая рекомендацию ресурсов, классификацию и отслеживание. Эти ресурсы доступны публично и призваны способствовать исследованиям в области обмена и управления ресурсами с открытым исходным кодом.'}, 'en': {'title': 'Empowering ML Resource Management with HuggingKG', 'desc': 'HuggingKG is a large-scale knowledge graph designed to improve the management of open source machine learning resources. It addresses the limitations of existing platforms by providing structured representations that allow for advanced queries and analyses. With millions of nodes and edges, HuggingKG captures important relationships and attributes within the ML community. Additionally, it introduces HuggingBench, a benchmark for evaluating tasks like resource recommendation and classification, facilitating better resource sharing and management in the field.'}, 'zh': {'title': 'HuggingKG：开源机器学习资源管理的新视野', 'desc': 'HuggingKG是一个大型知识图谱，旨在改善开源机器学习资源的管理。它通过结构化表示，支持高级查询和分析，帮助追踪模型演变和推荐相关数据集。HuggingKG包含260万个节点和620万个边，捕捉了领域特定的关系和丰富的文本属性。该图谱与HuggingBench基准测试结合，推动了信息检索任务的研究，包括资源推荐和分类等。'}}}, {'id': 'https://huggingface.co/papers/2505.15813', 'title': 'Meta-Learning an In-Context Transformer Model of Human Higher Visual\n  Cortex', 'url': 'https://huggingface.co/papers/2505.15813', 'abstract': 'BraInCoRL employs a transformer-based in-context learning approach to model higher visual cortex neural responses with few-shot examples, demonstrating superior performance and generalizability across new subjects, stimuli, and datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Understanding functional representations within higher visual cortex is a fundamental question in computational neuroscience. While artificial neural networks pretrained on large-scale datasets exhibit striking representational alignment with human neural responses, learning image-computable models of visual cortex relies on individual-level, large-scale fMRI datasets. The necessity for expensive, time-intensive, and often impractical data acquisition limits the generalizability of encoders to new subjects and stimuli. BraInCoRL uses in-context learning to predict voxelwise neural responses from few-shot examples without any additional finetuning for novel subjects and stimuli. We leverage a transformer architecture that can flexibly condition on a variable number of in-context image stimuli, learning an inductive bias over multiple subjects. During training, we explicitly optimize the model for in-context learning. By jointly conditioning on image features and voxel activations, our model learns to directly generate better performing voxelwise models of higher visual cortex. We demonstrate that BraInCoRL consistently outperforms existing voxelwise encoder designs in a low-data regime when evaluated on entirely novel images, while also exhibiting strong test-time scaling behavior. The model also generalizes to an entirely new visual fMRI dataset, which uses different subjects and fMRI data acquisition parameters. Further, BraInCoRL facilitates better interpretability of neural signals in higher visual cortex by attending to semantically relevant stimuli. Finally, we show that our framework enables interpretable mappings from natural language queries to voxel selectivity.', 'score': 3, 'issue_id': 4019, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 мая', 'en': 'May 21', 'zh': '5月21日'}, 'hash': '1dc1d35baba9a901', 'authors': ['Muquan Yu', 'Mu Nan', 'Hossein Adeli', 'Jacob S. Prince', 'John A. Pyles', 'Leila Wehbe', 'Margaret M. Henderson', 'Michael J. Tarr', 'Andrew F. Luo'], 'affiliations': ['Carnegie Mellon University', 'Chinese University of Hong Kong', 'Columbia University', 'Harvard University', 'University of Hong Kong', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2505.15813.jpg', 'data': {'categories': ['#architecture', '#interpretability', '#transfer_learning', '#training', '#multimodal', '#cv', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Революция в моделировании зрительной коры: обучение на малых данных с BraInCoRL', 'desc': 'BraInCoRL - это новый подход к моделированию нейронных ответов высших отделов зрительной коры, использующий трансформерную архитектуру и обучение в контексте. Модель демонстрирует превосходную производительность и обобщаемость на новых испытуемых, стимулах и наборах данных без дополнительной настройки. BraInCoRL оптимизирован для обучения на малом количестве примеров и позволяет генерировать более эффективные воксельные модели высших отделов зрительной коры. Подход также обеспечивает лучшую интерпретируемость нейронных сигналов в высших отделах зрительной коры.'}, 'en': {'title': 'Transforming Neural Insights with Few-Shot Learning', 'desc': 'BraInCoRL is a machine learning model that uses a transformer architecture to predict neural responses in the higher visual cortex from a few examples, without needing extensive retraining. It addresses the challenge of generalizing to new subjects and stimuli by employing in-context learning, which allows the model to adapt based on variable input data. The model is trained to optimize performance in low-data scenarios, demonstrating superior accuracy compared to traditional voxelwise encoders. Additionally, BraInCoRL enhances the interpretability of neural signals by linking them to relevant visual stimuli and natural language queries.'}, 'zh': {'title': '少样本学习，解锁视觉皮层的奥秘', 'desc': 'BraInCoRL是一种基于变换器的上下文学习方法，旨在通过少量示例建模高视觉皮层的神经反应。该方法在新受试者、刺激和数据集上表现出优越的性能和泛化能力。通过在训练过程中优化上下文学习，BraInCoRL能够在没有额外微调的情况下，直接预测体素级的神经反应。该模型还提高了对高视觉皮层神经信号的可解释性，能够将自然语言查询与体素选择性进行可解释的映射。'}}}, {'id': 'https://huggingface.co/papers/2505.12667', 'title': 'Safe-Sora: Safe Text-to-Video Generation via Graphical Watermarking', 'url': 'https://huggingface.co/papers/2505.12667', 'abstract': 'Safe-Sora embeds invisible watermarks into AI-generated videos using a hierarchical adaptive matching mechanism and a 3D wavelet transform-enhanced Mamba architecture, achieving top performance in video quality, watermark fidelity, and robustness.  \t\t\t\t\tAI-generated summary \t\t\t\t The explosive growth of generative video models has amplified the demand for reliable copyright preservation of AI-generated content. Despite its popularity in image synthesis, invisible generative watermarking remains largely underexplored in video generation. To address this gap, we propose Safe-Sora, the first framework to embed graphical watermarks directly into the video generation process. Motivated by the observation that watermarking performance is closely tied to the visual similarity between the watermark and cover content, we introduce a hierarchical coarse-to-fine adaptive matching mechanism. Specifically, the watermark image is divided into patches, each assigned to the most visually similar video frame, and further localized to the optimal spatial region for seamless embedding. To enable spatiotemporal fusion of watermark patches across video frames, we develop a 3D wavelet transform-enhanced Mamba architecture with a novel spatiotemporal local scanning strategy, effectively modeling long-range dependencies during watermark embedding and retrieval. To the best of our knowledge, this is the first attempt to apply state space models to watermarking, opening new avenues for efficient and robust watermark protection. Extensive experiments demonstrate that Safe-Sora achieves state-of-the-art performance in terms of video quality, watermark fidelity, and robustness, which is largely attributed to our proposals. We will release our code upon publication.', 'score': 3, 'issue_id': 4014, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 мая', 'en': 'May 19', 'zh': '5月19日'}, 'hash': '975482063e194827', 'authors': ['Zihan Su', 'Xuerui Qiu', 'Hongbin Xu', 'Tangyu Jiang', 'Junhao Zhuang', 'Chun Yuan', 'Ming Li', 'Shengfeng He', 'Fei Richard Yu'], 'affiliations': ['Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ)', 'Institute of Automation, Chinese Academy of Sciences', 'Singapore Management University', 'South China University of Technology', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.12667.jpg', 'data': {'categories': ['#video', '#3d', '#architecture', '#security'], 'emoji': '🎥', 'ru': {'title': 'Защита авторских прав на AI-видео с помощью невидимых водяных знаков', 'desc': 'Safe-Sora - это новая система для внедрения невидимых водяных знаков в видео, генерируемые искусственным интеллектом. Она использует иерархический механизм адаптивного сопоставления и улучшенную архитектуру Mamba с 3D вейвлет-преобразованием. Система достигает высоких показателей качества видео, точности водяных знаков и устойчивости к атакам. Safe-Sora применяет модели состояния пространства для эффективного внедрения и извлечения водяных знаков в видеоконтенте.'}, 'en': {'title': 'Revolutionizing Copyright Protection in AI-Generated Videos with Safe-Sora', 'desc': 'Safe-Sora is a novel framework designed to embed invisible watermarks into AI-generated videos, addressing the need for copyright protection in the growing field of generative video models. It utilizes a hierarchical adaptive matching mechanism to ensure that the watermark is seamlessly integrated into the video by matching it with visually similar frames. The framework employs a 3D wavelet transform-enhanced Mamba architecture to effectively manage the spatial and temporal aspects of watermark embedding, allowing for robust watermark retrieval. Experimental results show that Safe-Sora achieves superior video quality and watermark fidelity compared to existing methods, marking a significant advancement in the field of generative watermarking.'}, 'zh': {'title': 'Safe-Sora：保护 AI 视频版权的新方法', 'desc': 'Safe-Sora 是一种新颖的框架，能够在 AI 生成的视频中嵌入不可见水印，以保护版权。该方法采用分层自适应匹配机制，将水印图像分割成多个小块，并将其嵌入到与之视觉相似的视频帧中。通过增强的 3D 小波变换 Mamba 架构，Safe-Sora 实现了水印在时空上的融合，确保了水印的鲁棒性和高保真度。实验结果表明，Safe-Sora 在视频质量和水印保护方面达到了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2505.22645', 'title': 'Characterizing Bias: Benchmarking Large Language Models in Simplified\n  versus Traditional Chinese', 'url': 'https://huggingface.co/papers/2505.22645', 'abstract': 'Research examines LLM performance biases between Simplified and Traditional Chinese in regional term and name choice tasks, attributing differences to training data and tokenization.  \t\t\t\t\tAI-generated summary \t\t\t\t While the capabilities of Large Language Models (LLMs) have been studied in both Simplified and Traditional Chinese, it is yet unclear whether LLMs exhibit differential performance when prompted in these two variants of written Chinese. This understanding is critical, as disparities in the quality of LLM responses can perpetuate representational harms by ignoring the different cultural contexts underlying Simplified versus Traditional Chinese, and can exacerbate downstream harms in LLM-facilitated decision-making in domains such as education or hiring. To investigate potential LLM performance disparities, we design two benchmark tasks that reflect real-world scenarios: regional term choice (prompting the LLM to name a described item which is referred to differently in Mainland China and Taiwan), and regional name choice (prompting the LLM to choose who to hire from a list of names in both Simplified and Traditional Chinese). For both tasks, we audit the performance of 11 leading commercial LLM services and open-sourced models -- spanning those primarily trained on English, Simplified Chinese, or Traditional Chinese. Our analyses indicate that biases in LLM responses are dependent on both the task and prompting language: while most LLMs disproportionately favored Simplified Chinese responses in the regional term choice task, they surprisingly favored Traditional Chinese names in the regional name choice task. We find that these disparities may arise from differences in training data representation, written character preferences, and tokenization of Simplified and Traditional Chinese. These findings highlight the need for further analysis of LLM biases; as such, we provide an open-sourced benchmark dataset to foster reproducible evaluations of future LLM behavior across Chinese language variants (https://github.com/brucelyu17/SC-TC-Bench).', 'score': 2, 'issue_id': 4015, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 мая', 'en': 'May 28', 'zh': '5月28日'}, 'hash': 'fa422e70671c24d0', 'authors': ['Hanjia Lyu', 'Jiebo Luo', 'Jian Kang', 'Allison Koenecke'], 'affiliations': ['Cornell University', 'University of Rochester'], 'pdf_title_img': 'assets/pdf/title_img/2505.22645.jpg', 'data': {'categories': ['#ethics', '#low_resource', '#multilingual', '#dataset', '#benchmark', '#open_source'], 'emoji': '🇨🇳', 'ru': {'title': 'Скрытые предубеждения LLM в китайском языке: упрощенный vs традиционный', 'desc': 'Исследование анализирует различия в производительности больших языковых моделей (LLM) между упрощенным и традиционным китайским языком в задачах выбора региональных терминов и имен. Результаты показывают, что большинство LLM отдают предпочтение упрощенному китайскому в выборе терминов, но традиционному китайскому в выборе имен. Эти различия могут быть обусловлены особенностями обучающих данных, предпочтениями в написании символов и токенизацией. Исследование подчеркивает необходимость дальнейшего анализа предвзятостей LLM в отношении вариантов китайского языка.'}, 'en': {'title': 'Unveiling Biases: LLM Performance in Simplified vs. Traditional Chinese', 'desc': 'This research investigates how Large Language Models (LLMs) perform differently when using Simplified versus Traditional Chinese, focusing on regional term and name choice tasks. The study reveals that LLMs often show biases based on the language variant used, which can lead to unequal representation and potential harms in decision-making processes. By analyzing 11 different LLMs, the authors found that while Simplified Chinese was favored in naming items, Traditional Chinese names were preferred in hiring scenarios. The paper emphasizes the importance of understanding these biases and provides a benchmark dataset for future evaluations of LLM performance across Chinese language variants.'}, 'zh': {'title': '大型语言模型的中文表现偏差研究', 'desc': '本研究探讨了大型语言模型（LLM）在简体中文和繁体中文之间的表现偏差，特别是在地区术语和名称选择任务中。研究发现，LLM的表现差异与训练数据和分词方式有关，这可能导致对不同文化背景的忽视。通过设计两个基准任务，研究审计了11个主要的商业LLM服务和开源模型的表现。结果显示，LLM在地区术语选择任务中偏向简体中文，而在名称选择任务中则意外偏向繁体中文，强调了对LLM偏见的进一步分析的必要性。'}}}, {'id': 'https://huggingface.co/papers/2505.21191', 'title': "Unveiling Instruction-Specific Neurons & Experts: An Analytical\n  Framework for LLM's Instruction-Following Capabilities", 'url': 'https://huggingface.co/papers/2505.21191', 'abstract': 'The finetuning of Large Language Models (LLMs) has significantly advanced their instruction-following capabilities, yet the underlying computational mechanisms driving these improvements remain poorly understood. This study systematically examines how fine-tuning reconfigures LLM computations by isolating and analyzing instruction-specific sparse components, i.e., neurons in dense models and both neurons and experts in Mixture-of-Experts (MoE) architectures. In particular, we introduce HexaInst, a carefully curated and balanced instructional dataset spanning six distinct categories, and propose SPARCOM, a novel analytical framework comprising three key contributions: (1) a method for identifying these sparse components, (2) an evaluation of their functional generality and uniqueness, and (3) a systematic comparison of their alterations. Through experiments, we demonstrate functional generality, uniqueness, and the critical role of these components in instruction execution. By elucidating the relationship between fine-tuning-induced adaptations and sparse computational substrates, this work provides deeper insights into how LLMs internalize instruction-following behavior for the trustworthy LLM community.', 'score': 2, 'issue_id': 4016, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': 'd2c1aefe13ff525b', 'authors': ['Junyan Zhang', 'Yubo Gao', 'Yibo Yan', 'Jungang Li', 'Zhaorui Hou', 'Sicheng Tao', 'Shuliang Liu', 'Song Dai', 'Yonghua Hei', 'Junzhuo Li', 'Xuming Hu'], 'affiliations': ['The Hong Kong University of Science and Technology', 'The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2505.21191.jpg', 'data': {'categories': ['#training', '#optimization', '#interpretability', '#dataset', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Раскрывая тайны дообучения: как LLM учатся выполнять инструкции', 'desc': 'Исследование анализирует, как дообучение изменяет вычислительные механизмы больших языковых моделей (LLM) для выполнения инструкций. Авторы представляют HexaInst - набор данных с инструкциями, и SPARCOM - аналитическую структуру для изучения разреженных компонентов LLM. Эксперименты показывают функциональную общность и уникальность этих компонентов, а также их ключевую роль в выполнении инструкций. Работа углубляет понимание того, как LLM усваивают способность следовать инструкциям.'}, 'en': {'title': 'Unlocking the Secrets of Instruction-Following in LLMs', 'desc': "This paper explores how fine-tuning Large Language Models (LLMs) enhances their ability to follow instructions, focusing on the computational changes that occur during this process. The authors introduce HexaInst, a diverse dataset designed to analyze instruction-specific sparse components in LLMs, including neurons and experts in Mixture-of-Experts architectures. They present SPARCOM, a framework that identifies these components, evaluates their generality and uniqueness, and compares their changes during fine-tuning. The findings reveal the importance of these sparse components in executing instructions, offering valuable insights into the mechanisms behind LLMs' instruction-following capabilities."}, 'zh': {'title': '揭示微调与稀疏计算的关系', 'desc': '本研究探讨了大语言模型（LLMs）微调如何影响其指令执行能力。我们引入了HexaInst，一个涵盖六个不同类别的平衡指令数据集，并提出了SPARCOM分析框架。该框架包括识别稀疏组件的方法、评估其功能的通用性和独特性，以及系统比较其变化。通过实验，我们展示了这些稀疏组件在指令执行中的重要性，揭示了微调与稀疏计算基础之间的关系。'}}}, {'id': 'https://huggingface.co/papers/2505.21582', 'title': 'AITEE -- Agentic Tutor for Electrical Engineering', 'url': 'https://huggingface.co/papers/2505.21582', 'abstract': "An agent-based tutoring system for electrical engineering enhances learning through natural circuit interaction, context-retrieving generation, and guided questioning, demonstrating superior performance compared to baseline methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Intelligent tutoring systems combined with large language models offer a promising approach to address students' diverse needs and promote self-efficacious learning. While large language models possess good foundational knowledge of electrical engineering basics, they remain insufficiently capable of addressing specific questions about electrical circuits. In this paper, we present AITEE, an agent-based tutoring system for electrical engineering designed to accompany students throughout their learning process, offer individualized support, and promote self-directed learning. AITEE supports both hand-drawn and digital circuits through an adapted circuit reconstruction process, enabling natural interaction with students. Our novel graph-based similarity measure identifies relevant context from lecture materials through a retrieval augmented generation approach, while parallel Spice simulation further enhances accuracy in applying solution methodologies. The system implements a Socratic dialogue to foster learner autonomy through guided questioning. Experimental evaluations demonstrate that AITEE significantly outperforms baseline approaches in domain-specific knowledge application, with even medium-sized LLM models showing acceptable performance. Our results highlight the potential of agentic tutors to deliver scalable, personalized, and effective learning environments for electrical engineering education.", 'score': 2, 'issue_id': 4019, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': '30e929164ff65b3c', 'authors': ['Christopher Knievel', 'Alexander Bernhardt', 'Christian Bernhardt'], 'affiliations': ['Department of Electrical Engineering and Information Technology, HTWG Hochschule Konstanz, University of Applied Sciences, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2505.21582.jpg', 'data': {'categories': ['#graphs', '#interpretability', '#healthcare', '#science', '#rag', '#games', '#agents', '#multimodal', '#reasoning'], 'emoji': '⚡', 'ru': {'title': 'ИИ-репетитор по электротехнике: персонализированное обучение через интерактивное взаимодействие', 'desc': 'AITEE - это система репетиторства по электротехнике на основе искусственного интеллекта. Она поддерживает работу как с рукописными, так и с цифровыми схемами, используя адаптированный процесс реконструкции цепей. Система применяет метод извлечения релевантного контекста из учебных материалов на основе графового сходства и дополнительно использует параллельное моделирование в Spice. AITEE реализует сократический диалог для развития самостоятельности обучающихся через наводящие вопросы.'}, 'en': {'title': 'Empowering Electrical Engineering Learning with AITEE', 'desc': 'This paper introduces AITEE, an agent-based tutoring system specifically designed for electrical engineering education. AITEE enhances learning by allowing students to interact naturally with both hand-drawn and digital circuit designs, while also providing personalized support through guided questioning. The system utilizes a graph-based similarity measure to retrieve relevant context from lecture materials, improving the accuracy of responses through a retrieval augmented generation approach. Experimental results indicate that AITEE significantly outperforms traditional methods, showcasing its effectiveness in promoting self-directed learning and domain-specific knowledge application.'}, 'zh': {'title': '智能辅导，提升电气工程学习效果', 'desc': '本文介绍了一种名为AITEE的基于代理的电气工程辅导系统，旨在通过自然电路交互、上下文检索生成和引导提问来增强学习效果。AITEE能够支持手绘和数字电路，促进学生的自主学习，并提供个性化的支持。该系统采用图形相似度测量和Spice仿真技术，提高了解决方案方法的准确性。实验结果表明，AITEE在领域特定知识应用方面显著优于基线方法，展示了代理辅导员在电气工程教育中的潜力。'}}}, {'id': 'https://huggingface.co/papers/2505.20715', 'title': 'MUSEG: Reinforcing Video Temporal Understanding via Timestamp-Aware\n  Multi-Segment Grounding', 'url': 'https://huggingface.co/papers/2505.20715', 'abstract': 'MUSEG, an RL-based method with timestamp-aware multi-segment grounding, significantly enhances the temporal understanding of large language models by improving alignment with video segments and demonstrating superior performance in temporal reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Video temporal understanding is crucial for multimodal large language models (MLLMs) to reason over events in videos. Despite recent advances in general video understanding, current MLLMs still struggle with fine-grained temporal reasoning. While reinforcement learning (RL) has been explored to address this issue recently, existing RL approaches remain limited in effectiveness. In this work, we propose MUSEG, a novel RL-based method that enhances temporal understanding by introducing timestamp-aware multi-segment grounding. MUSEG enables MLLMs to align queries with multiple relevant video segments, promoting more comprehensive temporal reasoning. To facilitate effective learning, we design a customized RL training recipe with phased rewards that progressively guides the model toward temporally grounded reasoning. Extensive experiments on temporal grounding and time-sensitive video QA tasks demonstrate that MUSEG significantly outperforms existing methods and generalizes well across diverse temporal understanding scenarios. View our project at https://github.com/THUNLP-MT/MUSEG.', 'score': 2, 'issue_id': 4017, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': '7d2c1d63ea6e59e3', 'authors': ['Fuwen Luo', 'Shengfeng Lou', 'Chi Chen', 'Ziyue Wang', 'Chenliang Li', 'Weizhou Shen', 'Jiyue Guo', 'Peng Li', 'Ming Yan', 'Ji Zhang', 'Fei Huang', 'Yang Liu'], 'affiliations': ['Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China', 'Institute for AI Industry Research (AIR), Tsinghua University, Beijing, China', 'School of Computer Science and Technology (School of Artificial Intelligence), Zhejiang Sci-Tech University, China', 'Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2505.20715.jpg', 'data': {'categories': ['#training', '#rl', '#multimodal', '#alignment', '#video', '#reasoning'], 'emoji': '⏱️', 'ru': {'title': 'MUSEG: Прорыв во временном понимании видео для больших языковых моделей', 'desc': 'MUSEG - это новый метод, основанный на обучении с подкреплением, который улучшает временное понимание видео большими языковыми моделями. Он вводит привязку к нескольким сегментам видео с учетом временных меток, что позволяет моделям более точно анализировать события во времени. MUSEG использует специальную схему обучения с поэтапным вознаграждением для постепенного улучшения временного рассуждения. Эксперименты показывают значительное превосходство MUSEG над существующими методами в задачах временной привязки и ответов на вопросы по видео с учетом времени.'}, 'en': {'title': 'Enhancing Temporal Reasoning in MLLMs with MUSEG', 'desc': "MUSEG is a novel reinforcement learning (RL) method designed to improve the temporal understanding of large language models (MLLMs) when analyzing video content. It introduces timestamp-aware multi-segment grounding, which allows the model to better align its queries with relevant segments of a video, enhancing its ability to reason about events over time. The method employs a customized RL training strategy that uses phased rewards to progressively develop the model's temporal reasoning capabilities. Experimental results show that MUSEG outperforms existing approaches in tasks related to temporal grounding and time-sensitive video question answering, demonstrating its effectiveness across various scenarios."}, 'zh': {'title': 'MUSEG：提升视频时间理解的强化学习方法', 'desc': 'MUSEG是一种基于强化学习的方法，旨在提高大型语言模型对视频的时间理解能力。通过引入时间戳感知的多段落对齐，MUSEG能够更好地将查询与多个相关视频片段对齐，从而促进更全面的时间推理。我们设计了一种定制的强化学习训练方案，采用分阶段奖励，逐步引导模型实现时间基础的推理。实验结果表明，MUSEG在时间对齐和时间敏感的视频问答任务中显著优于现有方法，并在多种时间理解场景中表现良好。'}}}, {'id': 'https://huggingface.co/papers/2505.20589', 'title': 'Prot2Token: A Unified Framework for Protein Modeling via Next-Token\n  Prediction', 'url': 'https://huggingface.co/papers/2505.20589', 'abstract': 'Prot2Token unifies protein prediction tasks using an autoregressive decoder with task tokens, improving efficiency and accuracy across different benchmarks compared to specialized models.  \t\t\t\t\tAI-generated summary \t\t\t\t The diverse nature of protein prediction tasks has traditionally necessitated specialized models, hindering the development of broadly applicable and computationally efficient Protein Language Models (PLMs). In this work, we introduce Prot2Token, a unified framework that overcomes these challenges by converting a wide spectrum of protein-related predictions, from sequence-level properties and residue-specific attributes to complex inter-protein interactions, into a standardized next-token prediction format. At its core, Prot2Token employs an autoregressive decoder, conditioned on embeddings from pre-trained protein encoders and guided by learnable task tokens, to perform diverse predictions. This architecture uniquely facilitates multi-task learning, enabling a single model to master numerous tasks with improved efficiency. We present extensive experimental validation across a variety of benchmarks, demonstrating Prot2Tokens strong predictive power in different types of protein-prediction tasks. Key results include significant speedups (e.g., near 1000x over AlphaFold2 with MSA) and performance often matching or exceeding specialized approaches. Beyond that, we introduce an auxiliary self-supervised decoder pre-training approach to improve spatially sensitive task performance. Prot2Token thus offers a significant step towards a versatile, high-throughput paradigm for protein modeling, promising to accelerate biological discovery and the development of novel therapeutics. The code is available at https://github.com/mahdip72/prot2token .', 'score': 2, 'issue_id': 4027, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 мая', 'en': 'May 26', 'zh': '5月26日'}, 'hash': '57600073f21ca2c1', 'authors': ['Mahdi Pourmirzaei', 'Farzaneh Esmaili', 'Salhuldin Alqarghuli', 'Mohammadreza Pourmirzaei', 'Ye Han', 'Kai Chen', 'Mohsen Rezaei', 'Duolin Wang', 'Dong Xu'], 'affiliations': ['Politecnico di Milano, Milan, Italy', 'ProGene', 'University of Missouri'], 'pdf_title_img': 'assets/pdf/title_img/2505.20589.jpg', 'data': {'categories': ['#architecture', '#science', '#open_source', '#training', '#dataset', '#optimization'], 'emoji': '🧬', 'ru': {'title': 'Единая модель для всех задач предсказания свойств белков', 'desc': 'Prot2Token - это унифицированный фреймворк для различных задач предсказания свойств белков, использующий автореrрессивный декодер с токенами задач. Он преобразует разнообразные задачи прогнозирования белков в стандартизированный формат предсказания следующего токена. Prot2Token демонстрирует высокую эффективность и точность на различных бенчмарках по сравнению со специализированными моделями. Этот подход позволяет создать универсальную модель для множества задач, ускоряя биологические исследования и разработку новых терапевтических средств.'}, 'en': {'title': 'Unified Protein Predictions with Prot2Token', 'desc': 'Prot2Token is a novel framework that simplifies protein prediction tasks by using an autoregressive decoder with task tokens. This approach allows the model to handle various protein-related predictions in a unified manner, improving both efficiency and accuracy. By leveraging pre-trained protein encoders and enabling multi-task learning, Prot2Token can perform multiple tasks simultaneously, often outperforming specialized models. The results show significant speed improvements and strong predictive capabilities, making it a promising tool for advancing protein modeling and biological research.'}, 'zh': {'title': 'Prot2Token：高效统一的蛋白质预测框架', 'desc': 'Prot2Token 是一个统一的框架，用于解决多种蛋白质预测任务，采用自回归解码器和任务标记，提升了效率和准确性。该模型将蛋白质相关的预测转化为标准的下一个标记预测格式，支持从序列特性到复杂的蛋白质间相互作用的多样化预测。通过使用预训练的蛋白质编码器的嵌入和可学习的任务标记，Prot2Token 实现了多任务学习，使得单一模型能够高效地掌握多种任务。实验结果显示，Prot2Token 在不同的基准测试中表现出色，预测能力强，速度显著提升，推动了生物发现和新疗法的开发。'}}}, {'id': 'https://huggingface.co/papers/2505.19051', 'title': 'Efficient Data Selection at Scale via Influence Distillation', 'url': 'https://huggingface.co/papers/2505.19051', 'abstract': 'Effective data selection is critical for efficient training of modern Large Language Models (LLMs). This paper introduces Influence Distillation, a novel, mathematically-justified framework for data selection that employs second-order information to optimally weight training samples. By distilling each sample\'s influence on a target distribution, our method assigns model-specific weights that are used to select training data for LLM fine-tuning, guiding it toward strong performance on the target domain. We derive these optimal weights for both Gradient Descent and Adam optimizers. To ensure scalability and reduce computational cost, we propose a landmark-based approximation: influence is precisely computed for a small subset of "landmark" samples and then efficiently propagated to all other samples to determine their weights. We validate Influence Distillation by applying it to instruction tuning on the Tulu V2 dataset, targeting a range of tasks including GSM8k, SQuAD, and MMLU, across several models from the Llama and Qwen families. Experiments show that Influence Distillation matches or outperforms state-of-the-art performance while achieving up to 3.5times faster selection.', 'score': 2, 'issue_id': 4021, 'pub_date': '2025-05-25', 'pub_date_card': {'ru': '25 мая', 'en': 'May 25', 'zh': '5月25日'}, 'hash': 'cbb8b90722e6d09c', 'authors': ['Mahdi Nikdan', 'Vincent Cohen-Addad', 'Dan Alistarh', 'Vahab Mirrokni'], 'affiliations': ['Google Research', 'ISTA', 'Red Hat AI'], 'pdf_title_img': 'assets/pdf/title_img/2505.19051.jpg', 'data': {'categories': ['#training', '#data', '#optimization'], 'emoji': '🎯', 'ru': {'title': 'Эффективный отбор данных для точной настройки языковых моделей', 'desc': 'Статья представляет новый метод отбора данных для обучения больших языковых моделей (LLM), называемый Influence Distillation. Этот подход использует информацию второго порядка для оптимального взвешивания обучающих примеров, основываясь на их влиянии на целевое распределение. Авторы предлагают аппроксимацию на основе ориентиров для обеспечения масштабируемости и снижения вычислительных затрат. Эксперименты показывают, что Influence Distillation соответствует или превосходит современные методы, обеспечивая при этом до 3,5 раз более быстрый отбор данных.'}, 'en': {'title': 'Optimizing Data Selection for Superior Model Performance', 'desc': "This paper presents Influence Distillation, a new method for selecting training data for Large Language Models (LLMs) that uses second-order information to determine the importance of each training sample. By calculating the influence of each sample on the model's performance, the method assigns specific weights to guide the selection of data for fine-tuning. The approach is designed to be efficient, using a landmark-based approximation to reduce computational costs while maintaining accuracy. Experiments demonstrate that this method not only matches but can also exceed the performance of existing state-of-the-art techniques, significantly speeding up the data selection process."}, 'zh': {'title': '影响蒸馏：优化大型语言模型训练数据选择的新方法', 'desc': '有效的数据选择对于现代大型语言模型（LLMs）的高效训练至关重要。本文提出了一种名为影响蒸馏的新框架，通过使用二阶信息来优化训练样本的权重。该方法通过提取每个样本对目标分布的影响，分配特定于模型的权重，以选择用于LLM微调的训练数据，从而提高在目标领域的表现。我们在多个模型上验证了影响蒸馏的有效性，实验结果表明其在性能上与最先进的方法相当或更优，同时选择速度提高了3.5倍。'}}}, {'id': 'https://huggingface.co/papers/2505.22664', 'title': 'Zero-Shot Vision Encoder Grafting via LLM Surrogates', 'url': 'https://huggingface.co/papers/2505.22664', 'abstract': 'The approach of training vision encoders with small surrogate models before transferring them to large language models reduces training costs and enhances performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision language models (VLMs) typically pair a modestly sized vision encoder with a large language model (LLM), e.g., Llama-70B, making the decoder the primary computational burden during training. To reduce costs, a potential promising strategy is to first train the vision encoder using a small language model before transferring it to the large one. We construct small "surrogate models" that share the same embedding space and representation language as the large target LLM by directly inheriting its shallow layers. Vision encoders trained on the surrogate can then be directly transferred to the larger model, a process we call zero-shot grafting -- when plugged directly into the full-size target LLM, the grafted pair surpasses the encoder-surrogate pair and, on some benchmarks, even performs on par with full decoder training with the target LLM. Furthermore, our surrogate training approach reduces overall VLM training costs by ~45% when using Llama-70B as the decoder.', 'score': 1, 'issue_id': 4024, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 мая', 'en': 'May 28', 'zh': '5月28日'}, 'hash': 'be8516f11be3656f', 'authors': ['Kaiyu Yue', 'Vasu Singla', 'Menglin Jia', 'John Kirchenbauer', 'Rifaa Qadri', 'Zikui Cai', 'Abhinav Bhatele', 'Furong Huang', 'Tom Goldstein'], 'affiliations': ['Meta'], 'pdf_title_img': 'assets/pdf/title_img/2505.22664.jpg', 'data': {'categories': ['#optimization', '#small_models', '#transfer_learning', '#training', '#cv'], 'emoji': '🔀', 'ru': {'title': 'Эффективное обучение зрительно-языковых моделей через суррогатные модели', 'desc': "Статья представляет новый подход к обучению мультимодальных моделей, сочетающих зрение и язык. Авторы предлагают сначала обучать энкодер изображений с помощью небольшой языковой модели-суррогата, а затем переносить его в большую целевую модель. Этот метод, названный 'zero-shot grafting', позволяет значительно снизить вычислительные затраты при обучении. Результаты показывают, что такой подход не только экономит ресурсы, но и может давать производительность на уровне полного обучения с большой языковой моделью."}, 'en': {'title': 'Efficient Vision Encoder Training with Surrogate Models', 'desc': "This paper presents a method for training vision encoders using smaller surrogate models before integrating them with larger language models. By training the vision encoder on a compact model that shares the same embedding space as the larger model, the authors demonstrate a significant reduction in training costs and improved performance. The technique, termed 'zero-shot grafting', allows the trained vision encoder to be directly transferred to the larger language model, achieving results comparable to full decoder training. Overall, this approach can lower the training expenses of vision language models by approximately 45%."}, 'zh': {'title': '小模型助力大模型，降低成本提升性能', 'desc': '本文提出了一种通过先用小型替代模型训练视觉编码器，再将其转移到大型语言模型的方法。这种方法可以降低训练成本并提高性能。我们构建的小型替代模型与大型目标语言模型共享相同的嵌入空间和表示语言，从而实现了零-shot嫁接。实验表明，经过这种训练的视觉编码器在与大型语言模型结合时，性能超过了直接训练的编码器-替代模型组合，并且在某些基准测试中表现与完整解码器训练相当。'}}}, {'id': 'https://huggingface.co/papers/2505.21060', 'title': 'Styl3R: Instant 3D Stylized Reconstruction for Arbitrary Scenes and\n  Styles', 'url': 'https://huggingface.co/papers/2505.21060', 'abstract': 'A novel feed-forward model achieves fast 3D stylization using sparse view images, maintaining multi-view consistency and high-quality style transfer while retaining reconstruction accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Stylizing 3D scenes instantly while maintaining multi-view consistency and faithfully resembling a style image remains a significant challenge. Current state-of-the-art 3D stylization methods typically involve computationally intensive test-time optimization to transfer artistic features into a pretrained 3D representation, often requiring dense posed input images. In contrast, leveraging recent advances in feed-forward reconstruction models, we demonstrate a novel approach to achieve direct 3D stylization in less than a second using unposed sparse-view scene images and an arbitrary style image. To address the inherent decoupling between reconstruction and stylization, we introduce a branched architecture that separates structure modeling and appearance shading, effectively preventing stylistic transfer from distorting the underlying 3D scene structure. Furthermore, we adapt an identity loss to facilitate pre-training our stylization model through the novel view synthesis task. This strategy also allows our model to retain its original reconstruction capabilities while being fine-tuned for stylization. Comprehensive evaluations, using both in-domain and out-of-domain datasets, demonstrate that our approach produces high-quality stylized 3D content that achieve a superior blend of style and scene appearance, while also outperforming existing methods in terms of multi-view consistency and efficiency.', 'score': 1, 'issue_id': 4020, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': 'cfa395c8cc947534', 'authors': ['Peng Wang', 'Xiang Liu', 'Peidong Liu'], 'affiliations': ['Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.21060.jpg', 'data': {'categories': ['#training', '#optimization', '#architecture', '#3d'], 'emoji': '🎨', 'ru': {'title': 'Мгновенная 3D-стилизация с сохранением структуры сцены', 'desc': 'Статья представляет новую модель прямой подачи для быстрой 3D-стилизации с использованием изображений с разреженными видами. Модель поддерживает согласованность между несколькими ракурсами и высококачественный перенос стиля, сохраняя при этом точность реконструкции. Архитектура модели разделяет моделирование структуры и затенение внешнего вида, что предотвращает искажение базовой 3D-структуры сцены при стилизации. Предобучение модели с использованием задачи синтеза новых ракурсов позволяет сохранить исходные возможности реконструкции при точной настройке для стилизации.'}, 'en': {'title': 'Fast and Consistent 3D Stylization with Sparse Views', 'desc': "This paper presents a new feed-forward model for fast 3D stylization using sparse view images, which allows for quick artistic transformations of 3D scenes. Unlike traditional methods that require extensive computational resources and dense input images, this approach achieves stylization in under a second while ensuring multi-view consistency. The model features a branched architecture that separates the tasks of structure modeling and appearance shading, preventing style transfer from altering the 3D scene's structure. Additionally, an identity loss is utilized to pre-train the model, enabling it to maintain reconstruction accuracy while being fine-tuned for stylization tasks."}, 'zh': {'title': '快速3D风格化，保持一致性与高质量', 'desc': '本文提出了一种新颖的前馈模型，能够快速实现3D风格化，使用稀疏视图图像，同时保持多视图一致性和高质量的风格转移。与传统的3D风格化方法相比，该方法避免了计算密集的测试时间优化，能够在不到一秒的时间内完成风格化。我们引入了一种分支架构，将结构建模和外观着色分开，有效防止风格转移扭曲3D场景的结构。通过适应身份损失，我们的模型在保持重建能力的同时，能够进行风格化的微调。'}}}, {'id': 'https://huggingface.co/papers/2505.20444', 'title': 'HoPE: Hybrid of Position Embedding for Length Generalization in\n  Vision-Language Models', 'url': 'https://huggingface.co/papers/2505.20444', 'abstract': 'Vision-Language Models (VLMs) have made significant progress in multimodal tasks. However, their performance often deteriorates in long-context scenarios, particularly long videos. While Rotary Position Embedding (RoPE) has been widely adopted for length generalization in Large Language Models (LLMs), extending vanilla RoPE to capture the intricate spatial-temporal dependencies in videos remains an unsolved challenge. Existing methods typically allocate different frequencies within RoPE to encode 3D positional information. However, these allocation strategies mainly rely on heuristics, lacking in-depth theoretical analysis. In this paper, we first study how different allocation strategies impact the long-context capabilities of VLMs. Our analysis reveals that current multimodal RoPEs fail to reliably capture semantic similarities over extended contexts. To address this issue, we propose HoPE, a Hybrid of Position Embedding designed to improve the long-context capabilities of VLMs. HoPE introduces a hybrid frequency allocation strategy for reliable semantic modeling over arbitrarily long context, and a dynamic temporal scaling mechanism to facilitate robust learning and flexible inference across diverse context lengths. Extensive experiments across four video benchmarks on long video understanding and retrieval tasks demonstrate that HoPE consistently outperforms existing methods, confirming its effectiveness. Code is available at https://github.com/hrlics/HoPE.', 'score': 1, 'issue_id': 4024, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 мая', 'en': 'May 26', 'zh': '5月26日'}, 'hash': 'b461185334c2de42', 'authors': ['Haoran Li', 'Yingjie Qin', 'Baoyuan Ou', 'Lai Xu', 'Ruiwen Xu'], 'affiliations': ['Carnegie Mellon University', 'Xiaohongshu Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2505.20444.jpg', 'data': {'categories': ['#video', '#benchmark', '#multimodal', '#long_context', '#architecture'], 'emoji': '🎬', 'ru': {'title': 'HoPE: Улучшение работы мультимодальных моделей с длинными видео', 'desc': 'Статья представляет новый метод позиционного кодирования для мультимодальных моделей, названный HoPE (Hybrid of Position Embedding). Авторы анализируют проблемы существующих методов при работе с длинными видео и предлагают гибридную стратегию распределения частот для надежного семантического моделирования. HoPE включает динамический механизм временного масштабирования для обучения и вывода с различной длиной контекста. Эксперименты на четырех видео-бенчмарках показывают превосходство HoPE над существующими методами в задачах понимания и поиска длинных видео.'}, 'en': {'title': 'Enhancing Long-Context Understanding in VLMs with HoPE', 'desc': 'This paper addresses the limitations of Vision-Language Models (VLMs) in handling long-context scenarios, especially with long videos. It critiques existing Rotary Position Embedding (RoPE) methods for their inadequate ability to capture complex spatial-temporal dependencies. The authors introduce HoPE, a new Hybrid of Position Embedding that employs a novel frequency allocation strategy and a dynamic temporal scaling mechanism. Through extensive experiments, HoPE shows significant improvements in long video understanding and retrieval tasks compared to current methods.'}, 'zh': {'title': '提升视觉-语言模型的长上下文能力', 'desc': '视觉-语言模型（VLMs）在多模态任务中取得了显著进展，但在长视频等长上下文场景中的表现往往下降。本文提出了一种新的位置嵌入方法HoPE，旨在改善VLMs在长上下文中的能力。HoPE采用混合频率分配策略，以可靠地建模语义，并引入动态时间缩放机制，以支持不同上下文长度的灵活推理。通过在四个视频基准上的广泛实验，HoPE的表现优于现有方法，验证了其有效性。'}}}, {'id': 'https://huggingface.co/papers/2505.18366', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'url': 'https://huggingface.co/papers/2505.18366', 'abstract': "Enterprise search systems often struggle to retrieve accurate, domain-specific information due to semantic mismatches and overlapping terminologies. These issues can degrade the performance of downstream applications such as knowledge management, customer support, and retrieval-augmented generation agents. To address this challenge, we propose a scalable hard-negative mining framework tailored specifically for domain-specific enterprise data. Our approach dynamically selects semantically challenging but contextually irrelevant documents to enhance deployed re-ranking models.   Our method integrates diverse embedding models, performs dimensionality reduction, and uniquely selects hard negatives, ensuring computational efficiency and semantic precision. Evaluation on our proprietary enterprise corpus (cloud services domain) demonstrates substantial improvements of 15\\% in MRR@3 and 19\\% in MRR@10 compared to state-of-the-art baselines and other negative sampling techniques. Further validation on public domain-specific datasets (FiQA, Climate Fever, TechQA) confirms our method's generalizability and readiness for real-world applications.", 'score': 0, 'issue_id': 4027, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 мая', 'en': 'May 23', 'zh': '5月23日'}, 'hash': 'b9fbad227b517357', 'authors': ['Hansa Meghwani', 'Amit Agarwal', 'Priyaranjan Pattnayak', 'Hitesh Laxmichand Patel', 'Srikant Panda'], 'affiliations': ['Oracle AI'], 'pdf_title_img': 'assets/pdf/title_img/2505.18366.jpg', 'data': {'categories': ['#rag', '#optimization', '#dataset', '#benchmark'], 'emoji': '🔍', 'ru': {'title': 'Повышение точности корпоративного поиска с помощью интеллектуального отбора негативных примеров', 'desc': 'Статья представляет новый подход к улучшению корпоративных поисковых систем с использованием метода hard-negative mining. Авторы предлагают масштабируемую систему, которая динамически выбирает семантически сложные, но контекстуально нерелевантные документы для улучшения моделей ранжирования. Метод интегрирует различные модели эмбеддингов, выполняет снижение размерности и уникальным образом выбирает сложные негативные примеры. Эксперименты показали значительное улучшение метрик MRR@3 и MRR@10 по сравнению с современными базовыми моделями.'}, 'en': {'title': 'Enhancing Enterprise Search with Hard-Negative Mining', 'desc': "This paper addresses the challenges faced by enterprise search systems in retrieving accurate information due to semantic mismatches and overlapping terminologies. It introduces a scalable hard-negative mining framework that focuses on domain-specific data to improve the performance of re-ranking models. The method involves selecting semantically difficult but contextually irrelevant documents to enhance the model's ability to distinguish relevant information. Evaluation results show significant improvements in mean reciprocal rank (MRR) metrics, indicating the effectiveness and generalizability of the proposed approach across various datasets."}, 'zh': {'title': '提升企业搜索的语义精度', 'desc': '本论文提出了一种针对企业特定数据的可扩展硬负样本挖掘框架，以解决企业搜索系统在检索准确的领域特定信息时面临的语义不匹配和术语重叠问题。该方法动态选择语义上具有挑战性但上下文无关的文档，以增强已部署的重排序模型。我们的方法结合了多种嵌入模型，进行降维处理，并独特地选择硬负样本，确保计算效率和语义精度。通过在专有企业语料库上的评估，我们的方法在MRR@3和MRR@10上分别比现有最先进的基线提高了15%和19%。'}}}, {'id': 'https://huggingface.co/papers/2505.18149', 'title': 'First Finish Search: Efficient Test-Time Scaling in Large Language\n  Models', 'url': 'https://huggingface.co/papers/2505.18149', 'abstract': "First Finish Search improves accuracy in large language models by stopping inference at the first completed sample, significantly outperforming other decoding strategies in reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Test-time scaling (TTS), which involves dynamic allocation of compute during inference, offers a promising way to improve reasoning in large language models. While existing TTS methods work well, they often rely on long decoding paths or require a large number of samples to be generated, increasing the token usage and inference latency. We observe the surprising fact that for reasoning tasks, shorter traces are much more likely to be correct than longer ones. Motivated by this, we introduce First Finish Search (FFS), a training-free parallel decoding strategy that launches n independent samples and returns as soon as any one completes. We evaluate FFS alongside simple decoding, beam search, majority voting, and budget forcing on four reasoning models (DeepSeek-R1, R1-Distill-Qwen-32B, QwQ-32B and Phi-4-Reasoning-Plus) and across four datasets (AIME24, AIME25-I, AIME25-II and GPQA Diamond). With DeepSeek-R1, FFS achieves 82.23% accuracy on the AIME datasets, a 15% improvement over DeepSeek-R1's standalone accuracy, nearly matching OpenAI's o4-mini performance. Our theoretical analysis explains why stopping at the shortest trace is likely to yield a correct answer and identifies the conditions under which early stopping may be suboptimal. The elegance and simplicity of FFS demonstrate that straightforward TTS strategies can perform remarkably well, revealing the untapped potential of simple approaches at inference time.", 'score': 0, 'issue_id': 4019, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 мая', 'en': 'May 23', 'zh': '5月23日'}, 'hash': 'd05921967a27b012', 'authors': ['Aradhye Agarwal', 'Ayan Sengupta', 'Tanmoy Chakraborty'], 'affiliations': ['Indian Institute of Technology Delhi'], 'pdf_title_img': 'assets/pdf/title_img/2505.18149.jpg', 'data': {'categories': ['#training', '#inference', '#optimization', '#reasoning'], 'emoji': '🏁', 'ru': {'title': 'Первый финиширует - первый побеждает: революция в декодировании языковых моделей', 'desc': 'Метод First Finish Search (FFS) улучшает точность больших языковых моделей, останавливая вывод на первом завершенном образце. FFS значительно превосходит другие стратегии декодирования в задачах рассуждения. Этот метод особенно эффективен, так как более короткие выводы с большей вероятностью оказываются правильными. FFS достигает 82.23% точности на наборах данных AIME, что на 15% лучше базовой точности модели DeepSeek-R1.'}, 'en': {'title': 'First Finish Search: Quick Answers, Better Accuracy!', 'desc': 'First Finish Search (FFS) is a novel decoding strategy that enhances the accuracy of large language models by terminating inference as soon as the first complete sample is produced. This method contrasts with traditional approaches that often generate multiple samples or rely on lengthy decoding paths, which can lead to increased latency and token usage. FFS has been shown to significantly improve performance on reasoning tasks, achieving a notable accuracy boost compared to existing methods. The findings suggest that shorter inference traces are more likely to yield correct answers, highlighting the effectiveness of simpler, more efficient strategies in machine learning inference.'}, 'zh': {'title': '简化推理，提升准确性！', 'desc': '本文提出了一种名为“First Finish Search”（FFS）的新解码策略，旨在提高大型语言模型在推理任务中的准确性。FFS通过并行生成多个独立样本，并在第一个样本完成时立即返回结果，从而显著减少了推理时间和计算资源的使用。研究表明，对于推理任务，较短的解码路径比较长的路径更可能产生正确答案。通过与其他解码方法的比较，FFS在多个数据集上表现出色，显示了简单解码策略在推理时的巨大潜力。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (4)', '#agents (2)', '#agi (1)', '#alignment (6)', '#architecture (13)', '#audio', '#benchmark (18)', '#cv (8)', '#data (6)', '#dataset (15)', '#diffusion (5)', '#ethics (2)', '#games (3)', '#graphs (2)', '#hallucinations (3)', '#healthcare (1)', '#inference (5)', '#interpretability (7)', '#leakage', '#long_context (2)', '#low_resource (2)', '#machine_translation', '#math (2)', '#multilingual (2)', '#multimodal (15)', '#open_source (14)', '#optimization (22)', '#plp', '#rag (5)', '#reasoning (19)', '#rl (10)', '#rlhf (6)', '#robotics', '#science (3)', '#security (2)', '#small_models (2)', '#story_generation', '#survey', '#synthetic (2)', '#training (26)', '#transfer_learning (6)', '#video (5)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-05-29 17:10',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-05-29 17:10')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-05-29 17:10')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    