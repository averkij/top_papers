
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 23 papers. May 29.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">29 мая</span> | <span id="title-articles-count">23 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-05-28.html">⬅️ <span id="prev-date">28.05</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-05-30.html">➡️ <span id="next-date">30.05</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-05.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'};
        let feedDateNext = {'ru': '30.05', 'en': '05/30', 'zh': '5月30日'};
        let feedDatePrev = {'ru': '28.05', 'en': '05/28', 'zh': '5月28日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2505.22617', 'title': 'The Entropy Mechanism of Reinforcement Learning for Reasoning Language\n  Models', 'url': 'https://huggingface.co/papers/2505.22617', 'abstract': 'This paper aims to overcome a major obstacle in scaling RL for reasoning with LLMs, namely the collapse of policy entropy. Such phenomenon is consistently observed across vast RL runs without entropy intervention, where the policy entropy dropped sharply at the early training stage, this diminished exploratory ability is always accompanied with the saturation of policy performance. In practice, we establish a transformation equation R=-a*e^H+b between entropy H and downstream performance R. This empirical law strongly indicates that, the policy performance is traded from policy entropy, thus bottlenecked by its exhaustion, and the ceiling is fully predictable H=0, R=-a+b. Our finding necessitates entropy management for continuous exploration toward scaling compute for RL. To this end, we investigate entropy dynamics both theoretically and empirically. Our derivation highlights that, the change in policy entropy is driven by the covariance between action probability and the change in logits, which is proportional to its advantage when using Policy Gradient-like algorithms. Empirical study shows that, the values of covariance term and entropy differences matched exactly, supporting the theoretical conclusion. Moreover, the covariance term stays mostly positive throughout training, further explaining why policy entropy would decrease monotonically. Through understanding the mechanism behind entropy dynamics, we motivate to control entropy by restricting the update of high-covariance tokens. Specifically, we propose two simple yet effective techniques, namely Clip-Cov and KL-Cov, which clip and apply KL penalty to tokens with high covariances respectively. Experiments show that these methods encourage exploration, thus helping policy escape entropy collapse and achieve better downstream performance.', 'score': 22, 'issue_id': 4014, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 мая', 'en': 'May 28', 'zh': '5月28日'}, 'hash': '7313a363374c80e6', 'authors': ['Ganqu Cui', 'Yuchen Zhang', 'Jiacheng Chen', 'Lifan Yuan', 'Zhi Wang', 'Yuxin Zuo', 'Haozhan Li', 'Yuchen Fan', 'Huayu Chen', 'Weize Chen', 'Zhiyuan Liu', 'Hao Peng', 'Lei Bai', 'Wanli Ouyang', 'Yu Cheng', 'Bowen Zhou', 'Ning Ding'], 'affiliations': ['CUHK', 'Nanjing University', 'Peking University', 'Shanghai AI Laboratory', 'Tsinghua University', 'UIUC'], 'pdf_title_img': 'assets/pdf/title_img/2505.22617.jpg', 'data': {'categories': ['#training', '#rl', '#optimization', '#reasoning'], 'emoji': '🔬', 'ru': {'title': 'Управление энтропией для масштабирования обучения с подкреплением в больших языковых моделях', 'desc': 'Статья рассматривает проблему снижения энтропии политики при масштабировании обучения с подкреплением для рассуждений с использованием больших языковых моделей. Авторы устанавливают эмпирическую зависимость между энтропией и производительностью модели, подчеркивая необходимость управления энтропией для непрерывного исследования. Исследуется динамика энтропии теоретически и эмпирически, выявляя механизмы, стоящие за ее изменением. Предлагаются два метода - Clip-Cov и KL-Cov - для контроля энтропии путем ограничения обновления токенов с высокой ковариацией, что способствует улучшению исследования и производительности модели.'}, 'en': {'title': 'Managing Entropy for Enhanced Exploration in RL', 'desc': 'This paper addresses the issue of policy entropy collapse in reinforcement learning (RL) when applied to large language models (LLMs). It identifies that as training progresses, the policy entropy decreases significantly, leading to reduced exploration and stagnation in policy performance. The authors establish a relationship between entropy and performance, suggesting that managing entropy is crucial for effective exploration in RL. They propose two techniques, Clip-Cov and KL-Cov, to control high-covariance tokens, which help maintain policy entropy and improve overall performance.'}, 'zh': {'title': '控制熵以促进探索，提升强化学习性能', 'desc': '本文旨在克服在大规模强化学习（RL）中与大型语言模型（LLM）推理相关的一个主要障碍，即策略熵的崩溃。我们观察到，在没有熵干预的情况下，策略熵在训练初期急剧下降，导致探索能力减弱，并伴随策略性能的饱和。我们建立了熵与下游性能之间的变换方程，表明策略性能与策略熵之间存在权衡关系，因此熵的耗尽成为瓶颈。为了解决这一问题，我们提出了两种简单有效的技术，Clip-Cov和KL-Cov，旨在通过限制高协方差标记的更新来控制熵，从而鼓励探索并改善策略性能。'}}}, {'id': 'https://huggingface.co/papers/2505.22453', 'title': 'Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO', 'url': 'https://huggingface.co/papers/2505.22453', 'abstract': 'Improving Multi-modal Large Language Models (MLLMs) in the post-training stage typically relies on supervised fine-tuning (SFT) or reinforcement learning (RL). However, these supervised methods require expensive and manually annotated multi-modal data--an ultimately unsustainable resource. While recent efforts have explored unsupervised post-training, their methods are complex and difficult to iterate. In this work, we are the first to investigate the use of GRPO, a stable and scalable online RL algorithm, for enabling continual self-improvement without any external supervision. We propose MM-UPT, a simple yet effective framework for unsupervised post-training of MLLMs. MM-UPT builds upon GRPO, replacing traditional reward signals with a self-rewarding mechanism based on majority voting over multiple sampled responses. Our experiments demonstrate that MM-UPT significantly improves the reasoning ability of Qwen2.5-VL-7B (e.g., 66.3 %rightarrow72.9 % on MathVista, 62.9 %rightarrow68.7 % on We-Math), using standard dataset without ground truth labels. MM-UPT also outperforms prior unsupervised baselines and even approaches the results of supervised GRPO. Furthermore, we show that incorporating synthetic questions, generated solely by MLLM itself, can boost performance as well, highlighting a promising approach for scalable self-improvement. Overall, MM-UPT offers a new paradigm for continual, autonomous enhancement of MLLMs in the absence of external supervision. Our code is available at https://github.com/waltonfuture/MM-UPT.', 'score': 21, 'issue_id': 4013, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 мая', 'en': 'May 28', 'zh': '5月28日'}, 'hash': '96b8c222cef41b46', 'authors': ['Lai Wei', 'Yuting Li', 'Chen Wang', 'Yue Wang', 'Linghe Kong', 'Weiran Huang', 'Lichao Sun'], 'affiliations': ['Lehigh University', 'School of Computer Science, Shanghai Jiao Tong University', 'Shanghai Innovation Institute', 'Zhongguancun Academy'], 'pdf_title_img': 'assets/pdf/title_img/2505.22453.jpg', 'data': {'categories': ['#reasoning', '#rl', '#rlhf', '#multimodal', '#training', '#synthetic'], 'emoji': '🤖', 'ru': {'title': 'Самосовершенствование ИИ без учителя', 'desc': 'Статья представляет новый метод MM-UPT для улучшения мультимодальных больших языковых моделей (MLLM) без использования размеченных данных. Авторы применяют алгоритм обучения с подкреплением GRPO, заменяя традиционные сигналы вознаграждения механизмом самовознаграждения на основе голосования. Эксперименты показывают значительное улучшение способности рассуждать у модели Qwen2.5-VL-7B на различных задачах. MM-UPT превосходит предыдущие неконтролируемые методы и приближается к результатам контролируемого GRPO, открывая новую парадигму для автономного улучшения MLLM.'}, 'en': {'title': 'Autonomous Self-Improvement for MLLMs with MM-UPT', 'desc': 'This paper introduces MM-UPT, a novel framework for improving Multi-modal Large Language Models (MLLMs) without the need for expensive supervised fine-tuning. It leverages a stable online reinforcement learning algorithm called GRPO, which allows for continual self-improvement through a self-rewarding mechanism based on majority voting of responses. The experiments show that MM-UPT enhances the reasoning capabilities of the Qwen2.5-VL-7B model significantly, even outperforming previous unsupervised methods. Additionally, the incorporation of synthetic questions generated by the MLLM itself further boosts performance, suggesting a scalable approach for autonomous model enhancement.'}, 'zh': {'title': '无监督自我改进的多模态语言模型新框架', 'desc': '本研究提出了一种新的框架MM-UPT，用于多模态大型语言模型（MLLMs）的无监督后训练。与传统的监督微调方法不同，MM-UPT利用了一种基于多数投票的自我奖励机制，避免了对昂贵标注数据的依赖。通过实验，我们发现MM-UPT显著提高了模型的推理能力，并且在无监督的情况下表现优于之前的方法。该方法还展示了通过生成合成问题来进一步提升性能的潜力，为MLLMs的持续自我改进提供了新的思路。'}}}, {'id': 'https://huggingface.co/papers/2505.21136', 'title': 'SageAttention2++: A More Efficient Implementation of SageAttention2', 'url': 'https://huggingface.co/papers/2505.21136', 'abstract': 'The efficiency of attention is critical because its time complexity grows quadratically with sequence length. SageAttention2 addresses this by utilizing quantization to accelerate matrix multiplications (Matmul) in attention. To further accelerate SageAttention2, we propose to utilize the faster instruction of FP8 Matmul accumulated in FP16. The instruction is 2x faster than the FP8 Matmul used in SageAttention2. Our experiments show that SageAttention2++ achieves a 3.9x speedup over FlashAttention while maintaining the same attention accuracy as SageAttention2. This means SageAttention2++ effectively accelerates various models, including those for language, image, and video generation, with negligible end-to-end metrics loss. The code will be available at https://github.com/thu-ml/SageAttention.', 'score': 21, 'issue_id': 4014, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': 'd26c2e844cce91c6', 'authors': ['Jintao Zhang', 'Xiaoming Xu', 'Jia Wei', 'Haofeng Huang', 'Pengle Zhang', 'Chendong Xiang', 'Jun Zhu', 'Jianfei Chen'], 'affiliations': ['Department of Computer Science, Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.21136.jpg', 'data': {'categories': ['#architecture', '#inference'], 'emoji': '🚀', 'ru': {'title': 'Ускорение механизма внимания без потери точности', 'desc': 'SageAttention2++ - это улучшенная версия SageAttention2, которая использует квантизацию для ускорения матричных умножений в механизме внимания. Основное нововведение заключается в использовании более быстрой инструкции FP8 Matmul с накоплением в FP16, что в два раза быстрее, чем FP8 Matmul в SageAttention2. Эксперименты показывают, что SageAttention2++ достигает 3.9-кратного ускорения по сравнению с FlashAttention, сохраняя при этом точность внимания на уровне SageAttention2. Это позволяет эффективно ускорять различные модели для обработки языка, изображений и видео с минимальными потерями в метриках.'}, 'en': {'title': 'Accelerating Attention with SageAttention2++', 'desc': 'This paper introduces SageAttention2++, an improved version of the SageAttention2 model that enhances the efficiency of attention mechanisms in machine learning. It achieves this by implementing quantization techniques to speed up matrix multiplications, which are crucial for attention calculations. Additionally, it leverages a faster FP8 Matmul instruction that is twice as fast as the previous FP8 Matmul used in SageAttention2. The results demonstrate that SageAttention2++ provides a significant speedup while preserving the accuracy of attention, making it suitable for various applications in language, image, and video generation.'}, 'zh': {'title': '加速注意力机制，提升计算效率！', 'desc': '本文提出了一种新的注意力机制SageAttention2++，旨在提高计算效率。通过量化技术加速矩阵乘法，SageAttention2++在保持相同注意力准确度的同时，实现了3.9倍的速度提升。我们还利用FP8矩阵乘法的更快指令，进一步加速了计算过程。该方法适用于语言、图像和视频生成等多种模型，且几乎没有损失端到端的性能指标。'}}}, {'id': 'https://huggingface.co/papers/2505.21600', 'title': 'R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large\n  Model Token Routing', 'url': 'https://huggingface.co/papers/2505.21600', 'abstract': "Roads to Rome (R2R) selectively utilizes large language models for critical reasoning tasks to enhance efficiency and performance in lightweight models.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) achieve impressive reasoning capabilities at the cost of substantial inference overhead, posing substantial deployment challenges. Although distilled Small Language Models (SLMs) significantly enhance efficiency, their performance suffers as they fail to follow LLMs' reasoning paths. Luckily, we reveal that only a small fraction of tokens genuinely diverge reasoning paths between LLMs and SLMs. Most generated tokens are either identical or exhibit neutral differences, such as minor variations in abbreviations or expressions. Leveraging this insight, we introduce **Roads to Rome (R2R)**, a neural token routing method that selectively utilizes LLMs only for these critical, path-divergent tokens, while leaving the majority of token generation to the SLM. We also develop an automatic data generation pipeline that identifies divergent tokens and generates token-level routing labels to train the lightweight router. We apply R2R to combine R1-1.5B and R1-32B models from the DeepSeek family, and evaluate on challenging math, coding, and QA benchmarks. With an average activated parameter size of 5.6B, R2R surpasses the average accuracy of R1-7B by 1.6x, outperforming even the R1-14B model. Compared to R1-32B, it delivers a 2.8x wall-clock speedup with comparable performance, advancing the Pareto frontier of test-time scaling efficiency. Our code is available at https://github.com/thu-nics/R2R.", 'score': 20, 'issue_id': 4013, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': '54850077437e9524', 'authors': ['Tianyu Fu', 'Yi Ge', 'Yichen You', 'Enshu Liu', 'Zhihang Yuan', 'Guohao Dai', 'Shengen Yan', 'Huazhong Yang', 'Yu Wang'], 'affiliations': ['Infinigence AI', 'Shanghai Jiao Tong University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.21600.jpg', 'data': {'categories': ['#small_models', '#architecture', '#optimization', '#reasoning', '#benchmark', '#math', '#training'], 'emoji': '🛣️', 'ru': {'title': 'Эффективное сочетание больших и малых языковых моделей для улучшения рассуждений', 'desc': 'Статья представляет метод Roads to Rome (R2R), который селективно использует большие языковые модели (LLM) для критических задач рассуждения, улучшая эффективность и производительность легких моделей. R2R применяет нейронную маршрутизацию токенов, задействуя LLM только для ключевых, расходящихся токенов, оставляя большую часть генерации малым языковым моделям (SLM). Авторы разработали автоматический конвейер генерации данных для идентификации расходящихся токенов и создания меток маршрутизации. Метод R2R показал значительное улучшение точности и скорости по сравнению с базовыми моделями на сложных задачах математики, программирования и вопросно-ответных систем.'}, 'en': {'title': 'Efficient Reasoning with Selective Token Routing', 'desc': 'The paper introduces Roads to Rome (R2R), a method that enhances the efficiency of lightweight models by selectively using large language models (LLMs) for critical reasoning tasks. It identifies that only a small number of tokens significantly affect the reasoning paths between LLMs and distilled small language models (SLMs). By focusing on these path-divergent tokens, R2R allows SLMs to handle the majority of token generation, improving overall performance without the heavy computational cost of LLMs. The results show that R2R achieves higher accuracy and faster processing times compared to existing models, pushing the boundaries of efficiency in machine learning applications.'}, 'zh': {'title': '高效推理的新路径：R2R方法', 'desc': '本文介绍了一种名为Roads to Rome (R2R)的方法，该方法通过选择性地利用大型语言模型（LLMs）来处理关键推理任务，从而提高轻量级模型的效率和性能。研究发现，LLMs和小型语言模型（SLMs）之间的推理路径仅在少数标记上存在显著差异，大多数生成的标记要么相同，要么差异微小。基于这一发现，R2R方法仅在关键的路径分歧标记上使用LLMs，而将大部分标记生成留给SLMs，从而实现了高效的推理。通过在多个数学、编码和问答基准上进行评估，R2R在保持性能的同时显著提高了计算速度和准确性。'}}}, {'id': 'https://huggingface.co/papers/2505.22334', 'title': 'Advancing Multimodal Reasoning via Reinforcement Learning with Cold\n  Start', 'url': 'https://huggingface.co/papers/2505.22334', 'abstract': 'Recent advancements in large language models (LLMs) have demonstrated impressive chain-of-thought reasoning capabilities, with reinforcement learning (RL) playing a crucial role in this progress. While "aha moment" patterns--where models exhibit self-correction through reflection--are often attributed to emergent properties from RL, we first demonstrate that these patterns exist in multimodal LLMs (MLLMs) prior to RL training but may not necessarily correlate with improved reasoning performance. Building on these insights, we present a comprehensive study on enhancing multimodal reasoning through a two-stage approach: (1) supervised fine-tuning (SFT) as a cold start with structured chain-of-thought reasoning patterns, followed by (2) reinforcement learning via GRPO to further refine these capabilities. Our extensive experiments show that this combined approach consistently outperforms both SFT-only and RL-only methods across challenging multimodal reasoning benchmarks. The resulting models achieve state-of-the-art performance among open-source MLLMs at both 3B and 7B scales, with our 7B model showing substantial improvements over base models (e.g., 66.3 %rightarrow73.4 % on MathVista, 62.9 %rightarrow70.4 % on We-Math) and our 3B model achieving performance competitive with several 7B models. Overall, this work provides practical guidance for building advanced multimodal reasoning models. Our code is available at https://github.com/waltonfuture/RL-with-Cold-Start.', 'score': 16, 'issue_id': 4013, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 мая', 'en': 'May 28', 'zh': '5月28日'}, 'hash': '129b6f3da36369f9', 'authors': ['Lai Wei', 'Yuting Li', 'Kaipeng Zheng', 'Chen Wang', 'Yue Wang', 'Linghe Kong', 'Lichao Sun', 'Weiran Huang'], 'affiliations': ['Lehigh University', 'School of Computer Science, Shanghai Jiao Tong University', 'Shanghai Innovation Institute', 'Zhongguancun Academy'], 'pdf_title_img': 'assets/pdf/title_img/2505.22334.jpg', 'data': {'categories': ['#reasoning', '#rl', '#multimodal', '#benchmark', '#open_source', '#training'], 'emoji': '🧠', 'ru': {'title': 'Двухэтапное обучение для прорыва в мультимодальном рассуждении', 'desc': 'Статья исследует улучшение мультимодального рассуждения в больших языковых моделях. Авторы предлагают двухэтапный подход: сначала обучение с учителем для структурированных цепочек рассуждений, затем обучение с подкреплением. Эксперименты показывают, что этот комбинированный метод превосходит другие подходы на сложных бенчмарках мультимодального рассуждения. Результаты демонстрируют значительное улучшение производительности моделей разных размеров в задачах визуального и математического рассуждения.'}, 'en': {'title': 'Enhancing Multimodal Reasoning with a Two-Stage Approach', 'desc': "This paper explores the reasoning abilities of large language models (LLMs) and their improvement through reinforcement learning (RL). It reveals that 'aha moment' patterns, which indicate self-correction, are present in multimodal LLMs even before RL training, although they do not always lead to better reasoning. The authors propose a two-stage enhancement method: first, using supervised fine-tuning (SFT) to establish structured reasoning, and then applying RL to refine these skills. Their experiments show that this combined approach significantly outperforms traditional methods, achieving state-of-the-art results in multimodal reasoning tasks."}, 'zh': {'title': '提升多模态推理的双阶段方法', 'desc': '最近大型语言模型（LLMs）的进展显示出令人印象深刻的思维链推理能力，而强化学习（RL）在这一进展中起着关键作用。我们首次证明了“顿悟时刻”模式在多模态大型语言模型（MLLMs）中存在，这些模式在RL训练之前就已存在，但不一定与推理性能的提升相关。基于这些见解，我们提出了一种通过两阶段方法增强多模态推理的全面研究：首先进行监督微调（SFT），然后通过GRPO进行强化学习以进一步提升能力。我们的实验表明，这种结合的方法在多模态推理基准测试中始终优于仅使用SFT或RL的方法。'}}}, {'id': 'https://huggingface.co/papers/2505.22312', 'title': 'Skywork Open Reasoner 1 Technical Report', 'url': 'https://huggingface.co/papers/2505.22312', 'abstract': 'The success of DeepSeek-R1 underscores the significant role of reinforcement learning (RL) in enhancing the reasoning capabilities of large language models (LLMs). In this work, we present Skywork-OR1, an effective and scalable RL implementation for long Chain-of-Thought (CoT) models. Building on the DeepSeek-R1-Distill model series, our RL approach achieves notable performance gains, increasing average accuracy across AIME24, AIME25, and LiveCodeBench from 57.8% to 72.8% (+15.0%) for the 32B model and from 43.6% to 57.5% (+13.9%) for the 7B model. Our Skywork-OR1-32B model surpasses both DeepSeek-R1 and Qwen3-32B on the AIME24 and AIME25 benchmarks, while achieving comparable results on LiveCodeBench. The Skywork-OR1-7B and Skywork-OR1-Math-7B models demonstrate competitive reasoning capabilities among models of similar size. We perform comprehensive ablation studies on the core components of our training pipeline to validate their effectiveness. Additionally, we thoroughly investigate the phenomenon of entropy collapse, identify key factors affecting entropy dynamics, and demonstrate that mitigating premature entropy collapse is critical for improved test performance. To support community research, we fully open-source our model weights, training code, and training datasets.', 'score': 14, 'issue_id': 4013, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 мая', 'en': 'May 28', 'zh': '5月28日'}, 'hash': '712fb8daefb54030', 'authors': ['Jujie He', 'Jiacai Liu', 'Chris Yuhao Liu', 'Rui Yan', 'Chaojie Wang', 'Peng Cheng', 'Xiaoyu Zhang', 'Fuxiang Zhang', 'Jiacheng Xu', 'Wei Shen', 'Siyuan Li', 'Liang Zeng', 'Tianwen Wei', 'Cheng Cheng', 'Bo An', 'Yang Liu', 'Yahui Zhou'], 'affiliations': ['Skywork AI, Kunlun Inc'], 'pdf_title_img': 'assets/pdf/title_img/2505.22312.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#rl', '#benchmark', '#open_source', '#training'], 'emoji': '🧠', 'ru': {'title': 'Усиление рассуждений языковых моделей с помощью обучения с подкреплением', 'desc': 'Статья представляет Skywork-OR1, эффективную реализацию обучения с подкреплением для улучшения способностей рассуждения больших языковых моделей. Авторы демонстрируют значительное повышение точности на нескольких бенчмарках по сравнению с базовыми моделями. Исследование включает подробный анализ компонентов обучающего пайплайна и феномена коллапса энтропии. Авторы открыто публикуют веса моделей, код и наборы данных для поддержки исследовательского сообщества.'}, 'en': {'title': 'Boosting Reasoning in Language Models with Reinforcement Learning', 'desc': 'This paper introduces Skywork-OR1, a reinforcement learning (RL) framework designed to improve the reasoning abilities of large language models (LLMs) through long Chain-of-Thought (CoT) processes. By building on the previous DeepSeek-R1-Distill models, Skywork-OR1 achieves significant accuracy improvements on various benchmarks, with the 32B model increasing from 57.8% to 72.8%. The study also addresses the issue of entropy collapse, highlighting its impact on model performance and the importance of managing it during training. The authors provide open access to their model weights and training resources to foster further research in the community.'}, 'zh': {'title': '强化学习提升语言模型推理能力的突破', 'desc': 'DeepSeek-R1的成功表明强化学习在提升大型语言模型的推理能力方面的重要性。我们提出了Skywork-OR1，这是一个有效且可扩展的强化学习实现，专为长链思维模型设计。通过对DeepSeek-R1-Distill模型系列的改进，我们的强化学习方法在多个基准测试中显著提高了准确率。我们还进行了全面的消融研究，验证了训练流程中核心组件的有效性，并探讨了熵崩溃现象及其对测试性能的影响。'}}}, {'id': 'https://huggingface.co/papers/2505.19253', 'title': 'DeepResearchGym: A Free, Transparent, and Reproducible Evaluation\n  Sandbox for Deep Research', 'url': 'https://huggingface.co/papers/2505.19253', 'abstract': "DeepResearchGym provides an open-source evaluation framework for deep research systems using a reproducible search API and LLM-as-a-judge assessments.  \t\t\t\t\tAI-generated summary \t\t\t\t Deep research systems represent an emerging class of agentic information retrieval methods that generate comprehensive and well-supported reports to complex queries. However, most existing frameworks rely on dynamic commercial search APIs, which pose reproducibility and transparency challenges in addition to their cost. To address these limitations, we introduce DeepResearchGym, an open-source sandbox that combines a reproducible search API with a rigorous evaluation protocol for benchmarking deep research systems. The API indexes large-scale public web corpora, namely ClueWeb22 and FineWeb, using a state-of-the-art dense retriever and approximate nearest neighbor search via DiskANN. It achieves lower latency than popular commercial APIs while ensuring stable document rankings across runs, and is freely available for research use. To evaluate deep research systems' outputs, we extend the Researchy Questions benchmark with automatic metrics through LLM-as-a-judge assessments to measure alignment with users' information needs, retrieval faithfulness, and report quality. Experimental results show that systems integrated with DeepResearchGym achieve performance comparable to those using commercial APIs, with performance rankings remaining consistent across evaluation metrics. A human evaluation study further confirms that our automatic protocol aligns with human preferences, validating the framework's ability to help support controlled assessment of deep research systems. Our code and API documentation are available at https://www.deepresearchgym.ai.", 'score': 14, 'issue_id': 4013, 'pub_date': '2025-05-25', 'pub_date_card': {'ru': '25 мая', 'en': 'May 25', 'zh': '5月25日'}, 'hash': 'fabfc1ec7f6826eb', 'authors': ['João Coelho', 'Jingjie Ning', 'Jingyuan He', 'Kangrui Mao', 'Abhijay Paladugu', 'Pranav Setlur', 'Jiahe Jin', 'Jamie Callan', 'João Magalhães', 'Bruno Martins', 'Chenyan Xiong'], 'affiliations': ['Carnegie Mellon University', 'IST and INESC-ID', 'NOVA LINCS'], 'pdf_title_img': 'assets/pdf/title_img/2505.19253.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#alignment', '#rag'], 'emoji': '🔬', 'ru': {'title': 'Открытая платформа для оценки систем глубокого исследования', 'desc': 'DeepResearchGym - это открытая система оценки для систем глубокого исследования, использующая воспроизводимый поисковый API и оценки с помощью языковых моделей в роли судей. Она индексирует крупномасштабные веб-корпусы с помощью современного плотного ретривера и приближенного поиска ближайших соседей. Система расширяет бенчмарк Researchy Questions автоматическими метриками для оценки соответствия информационным потребностям пользователей, точности поиска и качества отчетов. Эксперименты показывают, что системы, интегрированные с DeepResearchGym, достигают производительности, сопоставимой с коммерческими API.'}, 'en': {'title': 'Revolutionizing Research Evaluation with DeepResearchGym', 'desc': "DeepResearchGym is an open-source framework designed to evaluate deep research systems, which are advanced methods for retrieving and generating detailed reports from complex queries. It addresses issues of reproducibility and transparency found in existing commercial search APIs by providing a stable and cost-free alternative. The framework utilizes a state-of-the-art dense retriever and approximate nearest neighbor search to index large public web corpora, ensuring lower latency and consistent document rankings. Additionally, it incorporates LLM-as-a-judge assessments to automatically evaluate the quality of outputs, aligning them with user needs and confirming the framework's effectiveness through both automatic and human evaluations."}, 'zh': {'title': '深度研究系统的开源评估框架', 'desc': 'DeepResearchGym是一个开源评估框架，旨在为深度研究系统提供可重复的搜索API和LLM作为评估者的评估方法。该框架解决了现有商业搜索API在可重复性和透明性方面的挑战，同时降低了成本。它使用先进的密集检索器和DiskANN进行近似最近邻搜索，能够在较低延迟下索引大规模公共网络语料库。通过扩展Researchy Questions基准，DeepResearchGym能够自动评估深度研究系统的输出，确保与用户信息需求的一致性和报告质量。'}}}, {'id': 'https://huggingface.co/papers/2505.19187', 'title': 'LIMOPro: Reasoning Refinement for Efficient and Effective Test-time\n  Scaling', 'url': 'https://huggingface.co/papers/2505.19187', 'abstract': 'A framework called PIR refines the importance of reasoning steps in large language models by pruning low-importance functional elements, leading to more concise reasoning chains with improved accuracy and reduced computational demands.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have demonstrated remarkable reasoning capabilities through test-time scaling approaches, particularly when fine-tuned with chain-of-thought (CoT) data distilled from more powerful large reasoning models (LRMs). However, these reasoning chains often contain verbose elements that mirror human problem-solving, categorized as progressive reasoning (the essential solution development path) and functional elements (verification processes, alternative solution approaches, and error corrections). While progressive reasoning is crucial, the functional elements significantly increase computational demands during test-time inference. We introduce PIR (Perplexity-based Importance Refinement), a principled framework that quantitatively evaluates the importance of each reasoning step based on its impact on answer prediction confidence. PIR systematically identifies and selectively prunes only low-importance functional steps while preserving progressive reasoning components, creating optimized training data that maintains the integrity of the core solution path while reducing verbosity. Models fine-tuned on PIR-optimized data exhibit superior test-time scaling properties, generating more concise reasoning chains while achieving improved accuracy (+0.9\\% to +6.6\\%) with significantly reduced token usage (-3\\% to -41\\%) across challenging reasoning benchmarks (AIME, AMC, and GPQA Diamond). Our approach demonstrates strong generalizability across different model sizes, data sources, and token budgets, offering a practical solution for deploying reasoning-capable LLMs in scenarios where efficient test-time scaling, response time, and computational efficiency are valuable constraints.', 'score': 9, 'issue_id': 4013, 'pub_date': '2025-05-25', 'pub_date_card': {'ru': '25 мая', 'en': 'May 25', 'zh': '5月25日'}, 'hash': '072196a147db8410', 'authors': ['Yang Xiao', 'Jiashuo Wang', 'Ruifeng Yuan', 'Chunpu Xu', 'Kaishuai Xu', 'Wenjie Li', 'Pengfei Liu'], 'affiliations': ['Shanghai Jiao Tong University', 'The Hong Kong Polytechnic University'], 'pdf_title_img': 'assets/pdf/title_img/2505.19187.jpg', 'data': {'categories': ['#optimization', '#inference', '#reasoning', '#benchmark', '#training'], 'emoji': '✂️', 'ru': {'title': 'PIR: Оптимизация рассуждений в ИИ через умное сокращение', 'desc': 'PIR - это фреймворк, который оптимизирует важность шагов рассуждения в больших языковых моделях путем удаления малозначимых функциональных элементов. Это приводит к более кратким цепочкам рассуждений с улучшенной точностью и сниженными вычислительными требованиями. Модели, дообученные на данных, оптимизированных с помощью PIR, демонстрируют улучшенные свойства масштабирования во время тестирования. Подход показывает хорошую обобщаемость для разных размеров моделей, источников данных и ограничений на количество токенов.'}, 'en': {'title': 'Streamlining Reasoning for Efficient AI Performance', 'desc': 'The paper introduces a framework called PIR (Perplexity-based Importance Refinement) that enhances the reasoning capabilities of large language models (LLMs) by focusing on the importance of reasoning steps. It identifies and prunes low-importance functional elements from reasoning chains, which helps in reducing computational demands while maintaining essential progressive reasoning. By optimizing training data, models fine-tuned with PIR show improved accuracy and efficiency, achieving better performance on reasoning benchmarks. This approach allows for more concise reasoning outputs, making LLMs more practical for real-world applications where speed and resource usage are critical.'}, 'zh': {'title': '优化推理链，提升模型效率', 'desc': '本文提出了一种名为PIR（基于困惑度的重要性精炼）的框架，旨在优化大型语言模型中的推理步骤。通过评估每个推理步骤对答案预测信心的影响，PIR能够识别并剪除低重要性的功能性元素，从而简化推理链。该方法在保持核心解决路径完整性的同时，减少了冗余信息，显著提高了模型的准确性和计算效率。经过PIR优化的数据训练的模型在多个推理基准测试中表现出更优的性能，减少了计算资源的消耗。'}}}, {'id': 'https://huggingface.co/papers/2505.17663', 'title': 'Towards Dynamic Theory of Mind: Evaluating LLM Adaptation to Temporal\n  Evolution of Human States', 'url': 'https://huggingface.co/papers/2505.17663', 'abstract': "The DynToM benchmark evaluates LLMs' ability to track and understand the temporal progression of mental states, revealing significant gaps compared to human performance.  \t\t\t\t\tAI-generated summary \t\t\t\t As Large Language Models (LLMs) increasingly participate in human-AI interactions, evaluating their Theory of Mind (ToM) capabilities - particularly their ability to track dynamic mental states - becomes crucial. While existing benchmarks assess basic ToM abilities, they predominantly focus on static snapshots of mental states, overlooking the temporal evolution that characterizes real-world social interactions. We present DynToM, a novel benchmark specifically designed to evaluate LLMs' ability to understand and track the temporal progression of mental states across interconnected scenarios. Through a systematic four-step framework, we generate 1,100 social contexts encompassing 5,500 scenarios and 78,100 questions, each validated for realism and quality. Our comprehensive evaluation of ten state-of-the-art LLMs reveals that their average performance underperforms humans by 44.7\\%, with performance degrading significantly when tracking and reasoning about the shift of mental states. This performance gap highlights fundamental limitations in current LLMs' ability to model the dynamic nature of human mental states.", 'score': 9, 'issue_id': 4013, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 мая', 'en': 'May 23', 'zh': '5月23日'}, 'hash': '01d30f0f90863bd8', 'authors': ['Yang Xiao', 'Jiashuo Wang', 'Qiancheng Xu', 'Changhe Song', 'Chunpu Xu', 'Yi Cheng', 'Wenjie Li', 'Pengfei Liu'], 'affiliations': ['Shanghai Jiao Tong University', 'The Hong Kong Polytechnic University'], 'pdf_title_img': 'assets/pdf/title_img/2505.17663.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#alignment'], 'emoji': '🧠', 'ru': {'title': 'Большие языковые модели отстают от людей в понимании динамики психических состояний', 'desc': 'Представлен новый бенчмарк DynToM для оценки способности больших языковых моделей (LLM) отслеживать изменение психических состояний во времени. Бенчмарк включает 1100 социальных контекстов с 5500 сценариями и 78100 вопросами. Тестирование 10 современных LLM показало, что их средняя производительность на 44.7% ниже человеческой. Выявлены существенные ограничения LLM в моделировании динамической природы человеческих психических состояний.'}, 'en': {'title': "Evaluating LLMs' Understanding of Dynamic Mental States", 'desc': "The DynToM benchmark assesses how well Large Language Models (LLMs) can understand and track changes in mental states over time, which is essential for effective human-AI interaction. Unlike previous benchmarks that only evaluate static mental states, DynToM focuses on the dynamic progression of these states in social contexts. The study involved creating a large dataset of scenarios and questions to rigorously test LLMs' Theory of Mind capabilities. Results showed that LLMs lag behind human performance by 44.7%, particularly struggling with the complexities of shifting mental states."}, 'zh': {'title': '评估LLMs的动态心理状态理解能力', 'desc': 'DynToM基准测试评估大型语言模型（LLMs）在跟踪和理解心理状态的时间进展方面的能力，显示出与人类表现之间的显著差距。现有的基准主要关注静态心理状态的快照，忽视了真实社交互动中心理状态的动态演变。我们提出了DynToM，这是一个专门设计的基准，旨在评估LLMs在相互关联场景中理解和跟踪心理状态时间进展的能力。通过系统的四步框架，我们生成了1100个社交背景，涵盖5500个场景和78100个问题，验证了其现实性和质量。'}}}, {'id': 'https://huggingface.co/papers/2505.22648', 'title': 'WebDancer: Towards Autonomous Information Seeking Agency', 'url': 'https://huggingface.co/papers/2505.22648', 'abstract': 'Addressing intricate real-world problems necessitates in-depth information seeking and multi-step reasoning. Recent progress in agentic systems, exemplified by Deep Research, underscores the potential for autonomous multi-step research. In this work, we present a cohesive paradigm for building end-to-end agentic information seeking agents from a data-centric and training-stage perspective. Our approach consists of four key stages: (1) browsing data construction, (2) trajectories sampling, (3) supervised fine-tuning for effective cold start, and (4) reinforcement learning for enhanced generalisation. We instantiate this framework in a web agent based on the ReAct, WebDancer. Empirical evaluations on the challenging information seeking benchmarks, GAIA and WebWalkerQA, demonstrate the strong performance of WebDancer, achieving considerable results and highlighting the efficacy of our training paradigm. Further analysis of agent training provides valuable insights and actionable, systematic pathways for developing more capable agentic models. The codes and demo will be released in https://github.com/Alibaba-NLP/WebAgent.', 'score': 6, 'issue_id': 4013, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 мая', 'en': 'May 28', 'zh': '5月28日'}, 'hash': '906f55ecff56c6ba', 'authors': ['Jialong Wu', 'Baixuan Li', 'Runnan Fang', 'Wenbiao Yin', 'Liwen Zhang', 'Zhengwei Tao', 'Dingchu Zhang', 'Zekun Xi', 'Yong Jiang', 'Pengjun Xie', 'Fei Huang', 'Jingren Zhou'], 'affiliations': ['Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2505.22648.jpg', 'data': {'categories': ['#agi', '#reasoning', '#agents', '#benchmark', '#training'], 'emoji': '🕸️', 'ru': {'title': 'Новая парадигма обучения агентов для автономного поиска информации в сети', 'desc': 'Эта статья представляет новый подход к созданию агентов для автономного поиска информации. Авторы предлагают четырехэтапную парадигму, включающую построение данных для просмотра, выборку траекторий, обучение с учителем и обучение с подкреплением. На основе этой парадигмы они создали веб-агента WebDancer, использующего архитектуру ReAct. Эмпирические оценки на сложных тестах GAIA и WebWalkerQA показали высокую эффективность WebDancer и предложенного подхода к обучению.'}, 'en': {'title': 'Empowering Agents for Autonomous Information Seeking', 'desc': "This paper introduces a new framework for creating intelligent agents that can autonomously seek information and perform multi-step reasoning. The proposed method involves four stages: constructing a dataset for browsing, sampling trajectories for learning, fine-tuning the model to improve initial performance, and applying reinforcement learning to enhance the agent's ability to generalize. The authors demonstrate their approach through a web agent called WebDancer, which shows strong performance on challenging benchmarks like GAIA and WebWalkerQA. The findings provide insights into training strategies that can lead to the development of more advanced agentic systems."}, 'zh': {'title': '构建智能信息检索代理的全新框架', 'desc': '本论文探讨了如何构建自主的信息检索代理系统，以解决复杂的现实问题。我们提出了一个端到端的代理信息检索框架，包含数据构建、轨迹采样、监督微调和强化学习四个关键阶段。通过在WebDancer上实施该框架，我们在GAIA和WebWalkerQA等信息检索基准上取得了显著的性能。我们的研究为开发更强大的代理模型提供了系统化的路径和有价值的见解。'}}}, {'id': 'https://huggingface.co/papers/2505.22129', 'title': 'What Makes for Text to 360-degree Panorama Generation with Stable\n  Diffusion?', 'url': 'https://huggingface.co/papers/2505.22129', 'abstract': 'Analysis of fine-tuning diffusion models for panoramic image generation reveals distinct roles of attention module matrices and introduces UniPano, a memory-efficient and speed-enhanced baseline framework.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent prosperity of text-to-image diffusion models, e.g. Stable Diffusion, has stimulated research to adapt them to 360-degree panorama generation. Prior work has demonstrated the feasibility of using conventional low-rank adaptation techniques on pre-trained diffusion models to generate panoramic images. However, the substantial domain gap between perspective and panoramic images raises questions about the underlying mechanisms enabling this empirical success. We hypothesize and examine that the trainable counterparts exhibit distinct behaviors when fine-tuned on panoramic data, and such an adaptation conceals some intrinsic mechanism to leverage the prior knowledge within the pre-trained diffusion models. Our analysis reveals the following: 1) the query and key matrices in the attention modules are responsible for common information that can be shared between the panoramic and perspective domains, thus are less relevant to panorama generation; and 2) the value and output weight matrices specialize in adapting pre-trained knowledge to the panoramic domain, playing a more critical role during fine-tuning for panorama generation. We empirically verify these insights by introducing a simple framework called UniPano, with the objective of establishing an elegant baseline for future research. UniPano not only outperforms existing methods but also significantly reduces memory usage and training time compared to prior dual-branch approaches, making it scalable for end-to-end panorama generation with higher resolution. The code will be released.', 'score': 6, 'issue_id': 4015, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 мая', 'en': 'May 28', 'zh': '5月28日'}, 'hash': '1148d530d0e60180', 'authors': ['Jinhong Ni', 'Chang-Bin Zhang', 'Qiang Zhang', 'Jing Zhang'], 'affiliations': ['Australian National University', 'Beijing Innovation Center of Humanoid Robotics', 'Hong Kong University of Science and Technology (Guangzhou)', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.22129.jpg', 'data': {'categories': ['#optimization', '#dataset', '#training', '#diffusion', '#cv'], 'emoji': '�panorama', 'ru': {'title': 'Раскрытие механизмов адаптации диффузионных моделей к панорамной генерации', 'desc': 'Исследование процесса тонкой настройки диффузионных моделей для генерации панорамных изображений выявило различные роли матриц модуля внимания. Анализ показал, что матрицы запросов и ключей отвечают за общую информацию, применимую как к панорамным, так и к перспективным изображениям. Матрицы значений и выходных весов специализируются на адаптации предобученных знаний к панорамной области. На основе этих выводов авторы представили фреймворк UniPano, который превосходит существующие методы и значительно снижает использование памяти и время обучения.'}, 'en': {'title': 'UniPano: Efficient Panoramic Image Generation with Diffusion Models', 'desc': 'This paper investigates how to improve the generation of panoramic images using diffusion models, which are typically used for standard images. It identifies the specific roles of different components in the attention mechanism of these models, particularly how they adapt to the unique characteristics of panoramic data. The authors introduce a new framework called UniPano, which enhances efficiency by reducing memory usage and training time while achieving better performance than existing methods. This work aims to provide a solid foundation for future advancements in panoramic image generation.'}, 'zh': {'title': 'UniPano：高效全景图像生成的新基线', 'desc': '本文分析了微调扩散模型在全景图像生成中的作用，揭示了注意力模块矩阵的不同角色，并引入了UniPano，一个内存高效且速度增强的基线框架。研究表明，查询和键矩阵在全景和透视领域之间共享信息，而值和输出权重矩阵则专注于将预训练知识适应于全景领域。通过这些发现，UniPano在全景图像生成中表现优于现有方法，并显著减少了内存使用和训练时间。该框架为未来的研究提供了一个优雅的基线，并将发布相关代码。'}}}, {'id': 'https://huggingface.co/papers/2505.22523', 'title': 'PrismLayers: Open Data for High-Quality Multi-Layer Transparent Image\n  Generative Models', 'url': 'https://huggingface.co/papers/2505.22523', 'abstract': 'Generating high-quality, multi-layer transparent images from text prompts can unlock a new level of creative control, allowing users to edit each layer as effortlessly as editing text outputs from LLMs. However, the development of multi-layer generative models lags behind that of conventional text-to-image models due to the absence of a large, high-quality corpus of multi-layer transparent data. In this paper, we address this fundamental challenge by: (i) releasing the first open, ultra-high-fidelity PrismLayers (PrismLayersPro) dataset of 200K (20K) multilayer transparent images with accurate alpha mattes, (ii) introducing a trainingfree synthesis pipeline that generates such data on demand using off-the-shelf diffusion models, and (iii) delivering a strong, open-source multi-layer generation model, ART+, which matches the aesthetics of modern text-to-image generation models. The key technical contributions include: LayerFLUX, which excels at generating high-quality single transparent layers with accurate alpha mattes, and MultiLayerFLUX, which composes multiple LayerFLUX outputs into complete images, guided by human-annotated semantic layout. To ensure higher quality, we apply a rigorous filtering stage to remove artifacts and semantic mismatches, followed by human selection. Fine-tuning the state-of-the-art ART model on our synthetic PrismLayersPro yields ART+, which outperforms the original ART in 60% of head-to-head user study comparisons and even matches the visual quality of images generated by the FLUX.1-[dev] model. We anticipate that our work will establish a solid dataset foundation for the multi-layer transparent image generation task, enabling research and applications that require precise, editable, and visually compelling layered imagery.', 'score': 4, 'issue_id': 4013, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 мая', 'en': 'May 28', 'zh': '5月28日'}, 'hash': '2f373283a5bfede3', 'authors': ['Junwen Chen', 'Heyang Jiang', 'Yanbin Wang', 'Keming Wu', 'Ji Li', 'Chao Zhang', 'Keiji Yanai', 'Dong Chen', 'Yuhui Yuan'], 'affiliations': ['Microsoft Research Asia'], 'pdf_title_img': 'assets/pdf/title_img/2505.22523.jpg', 'data': {'categories': ['#data', '#cv', '#diffusion', '#dataset', '#open_source', '#synthetic'], 'emoji': '🎨', 'ru': {'title': 'Прорыв в создании редактируемых многослойных изображений с помощью ИИ', 'desc': 'Данная статья представляет новый подход к генерации многослойных прозрачных изображений на основе текстовых запросов. Авторы создали первый открытый набор данных PrismLayers высокого качества, содержащий 200 тысяч многослойных прозрачных изображений с точными альфа-масками. Они также разработали метод синтеза таких данных по запросу, используя существующие диффузионные модели. В результате была создана модель ART+, превосходящая предыдущие аналоги в генерации многослойных изображений.'}, 'en': {'title': 'Unlocking Creative Control with Multi-Layer Transparent Image Generation', 'desc': 'This paper presents a novel approach to generating high-quality, multi-layer transparent images from text prompts, enhancing creative control for users. The authors introduce the PrismLayersPro dataset, which contains 200,000 multilayer transparent images with accurate alpha mattes, addressing the lack of quality data in this area. They also propose a training-free synthesis pipeline that utilizes existing diffusion models to create these images on demand. Additionally, the ART+ model is developed, which outperforms previous models in user studies and provides a foundation for future research in editable layered imagery.'}, 'zh': {'title': '开启多层透明图像生成的新篇章', 'desc': '本文提出了一种生成高质量多层透明图像的方法，允许用户像编辑文本一样轻松编辑每一层。我们发布了首个开放的超高保真PrismLayers数据集，包含20万张带有准确alpha通道的多层透明图像。我们还引入了一种无训练合成管道，利用现成的扩散模型按需生成数据，并提供了开源的多层生成模型ART+，其美学与现代文本到图像生成模型相匹配。通过严格的过滤和人工选择，我们确保生成的图像质量更高，ART+在用户研究中表现优于原始ART模型。'}}}, {'id': 'https://huggingface.co/papers/2505.22338', 'title': 'Text2Grad: Reinforcement Learning from Natural Language Feedback', 'url': 'https://huggingface.co/papers/2505.22338', 'abstract': "Traditional RLHF optimizes language models with coarse, scalar rewards that mask the fine-grained reasons behind success or failure, leading to slow and opaque learning. Recent work augments RL with textual critiques through prompting or reflection, improving interpretability but leaving model parameters untouched. We introduce Text2Grad, a reinforcement-learning paradigm that turns free-form textual feedback into span-level gradients. Given human (or programmatic) critiques, Text2Grad aligns each feedback phrase with the relevant token spans, converts these alignments into differentiable reward signals, and performs gradient updates that directly refine the offending portions of the model's policy. This yields precise, feedback-conditioned adjustments instead of global nudges. Text2Grad is realized through three components: (1) a high-quality feedback-annotation pipeline that pairs critiques with token spans; (2) a fine-grained reward model that predicts span-level reward on answer while generating explanatory critiques; and (3) a span-level policy optimizer that back-propagates natural-language gradients. Across summarization, code generation, and question answering, Text2Grad consistently surpasses scalar-reward RL and prompt-only baselines, providing both higher task metrics and richer interpretability. Our results demonstrate that natural-language feedback, when converted to gradients, is a powerful signal for fine-grained policy optimization. The code for our method is available at https://github.com/microsoft/Text2Grad", 'score': 4, 'issue_id': 4013, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 мая', 'en': 'May 28', 'zh': '5月28日'}, 'hash': '6a628958ae1efdcc', 'authors': ['Hanyang Wang', 'Lu Wang', 'Chaoyun Zhang', 'Tianjun Mao', 'Si Qin', 'Qingwei Lin', 'Saravan Rajmohan', 'Dongmei Zhang'], 'affiliations': ['Fudan University', 'Microsoft', 'University of Chicago'], 'pdf_title_img': 'assets/pdf/title_img/2505.22338.jpg', 'data': {'categories': ['#optimization', '#training', '#interpretability', '#rlhf'], 'emoji': '🔍', 'ru': {'title': 'Text2Grad: Точная настройка языковых моделей с помощью текстовых градиентов', 'desc': 'Статья представляет новый подход к обучению языковых моделей под названием Text2Grad. Этот метод преобразует текстовые отзывы в градиенты на уровне отдельных фрагментов текста, что позволяет более точно настраивать модель. Text2Grad состоит из трех компонентов: системы аннотирования отзывов, модели вознаграждения на уровне фрагментов и оптимизатора политики. Эксперименты показали, что Text2Grad превосходит традиционные методы обучения с подкреплением в задачах суммаризации, генерации кода и ответов на вопросы.'}, 'en': {'title': 'Transforming Textual Feedback into Targeted Learning Signals', 'desc': "This paper presents Text2Grad, a novel approach in reinforcement learning from human feedback (RLHF) that enhances language model training by using detailed textual critiques. Unlike traditional methods that rely on simple scalar rewards, Text2Grad translates free-form feedback into specific gradient updates for model parameters, allowing for more precise adjustments. The method involves a feedback-annotation pipeline, a fine-grained reward model, and a span-level policy optimizer to ensure that critiques are effectively aligned with the relevant parts of the model's output. The results show that Text2Grad outperforms existing RLHF techniques in various tasks, providing better performance metrics and improved interpretability of the model's learning process."}, 'zh': {'title': 'Text2Grad：将文本反馈转化为梯度的强化学习新方法', 'desc': '传统的强化学习人类反馈（RLHF）使用粗略的标量奖励来优化语言模型，这种方法掩盖了成功或失败的细微原因，导致学习过程缓慢且不透明。最近的研究通过提示或反思增强了RL与文本批评的结合，提高了可解释性，但未能改变模型参数。我们提出了Text2Grad，这是一种强化学习范式，将自由形式的文本反馈转化为跨度级梯度。Text2Grad通过将人类或程序化的批评与相关的标记跨度对齐，生成可微分的奖励信号，从而实现对模型策略的精确调整。'}}}, {'id': 'https://huggingface.co/papers/2505.21925', 'title': 'RenderFormer: Transformer-based Neural Rendering of Triangle Meshes with\n  Global Illumination', 'url': 'https://huggingface.co/papers/2505.21925', 'abstract': 'We present RenderFormer, a neural rendering pipeline that directly renders an image from a triangle-based representation of a scene with full global illumination effects and that does not require per-scene training or fine-tuning. Instead of taking a physics-centric approach to rendering, we formulate rendering as a sequence-to-sequence transformation where a sequence of tokens representing triangles with reflectance properties is converted to a sequence of output tokens representing small patches of pixels. RenderFormer follows a two stage pipeline: a view-independent stage that models triangle-to-triangle light transport, and a view-dependent stage that transforms a token representing a bundle of rays to the corresponding pixel values guided by the triangle-sequence from the view-independent stage. Both stages are based on the transformer architecture and are learned with minimal prior constraints. We demonstrate and evaluate RenderFormer on scenes with varying complexity in shape and light transport.', 'score': 4, 'issue_id': 4013, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 мая', 'en': 'May 28', 'zh': '5月28日'}, 'hash': 'a6275b50c7372a2c', 'authors': ['Chong Zeng', 'Yue Dong', 'Pieter Peers', 'Hongzhi Wu', 'Xin Tong'], 'affiliations': ['College of William & Mary', 'Microsoft Research Asia', 'State Key Lab of CAD & CG, Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.21925.jpg', 'data': {'categories': ['#architecture', '#3d'], 'emoji': '🎨', 'ru': {'title': 'Нейронный рендеринг без физики: от треугольников к пикселям', 'desc': 'RenderFormer - это нейронная система рендеринга, которая напрямую создает изображение из треугольного представления сцены с полными эффектами глобального освещения без необходимости обучения для каждой сцены. Система использует подход преобразования последовательности в последовательность, где токены, представляющие треугольники с свойствами отражения, преобразуются в токены, представляющие небольшие участки пикселей. RenderFormer состоит из двух этапов: независимого от вида этапа, моделирующего перенос света между треугольниками, и зависимого от вида этапа, преобразующего токен, представляющий пучок лучей, в соответствующие значения пикселей. Оба этапа основаны на архитектуре трансформера и обучаются с минимальными предварительными ограничениями.'}, 'en': {'title': 'Transforming Triangles to Pixels with RenderFormer', 'desc': 'RenderFormer is a novel neural rendering pipeline that generates images directly from a triangle-based scene representation, incorporating full global illumination effects without needing specific training for each scene. It approaches rendering as a sequence-to-sequence transformation, where input tokens representing triangles are converted into output tokens for pixel patches. The pipeline consists of two stages: a view-independent stage that models light transport between triangles, and a view-dependent stage that converts ray bundles into pixel values. Both stages utilize the transformer architecture and are designed to learn with minimal prior constraints, showcasing effectiveness across various scene complexities.'}, 'zh': {'title': 'RenderFormer：无需训练的高效神经渲染', 'desc': 'RenderFormer是一种神经渲染管道，可以直接从基于三角形的场景表示中渲染图像，并实现全局光照效果，而无需针对每个场景进行训练或微调。该方法将渲染视为一种序列到序列的转换，将表示三角形及其反射特性的令牌序列转换为表示小像素块的输出令牌序列。RenderFormer采用两阶段管道：第一阶段是视图无关的三角形光传输建模，第二阶段则是将光束的令牌转换为相应的像素值。两个阶段都基于变换器架构，并在最小的先验约束下进行学习。'}}}, {'id': 'https://huggingface.co/papers/2505.22203', 'title': 'Pitfalls of Rule- and Model-based Verifiers -- A Case Study on\n  Mathematical Reasoning', 'url': 'https://huggingface.co/papers/2505.22203', 'abstract': 'The study examines the effectiveness and reliability of rule-based and model-based verifiers in reinforcement learning with verifiable reward, highlighting limitations and vulnerabilities in their use for mathematical reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Trustworthy verifiers are essential for the success of reinforcement learning with verifiable reward (RLVR), which is the core methodology behind various large reasoning models such as DeepSeek-R1. In complex domains like mathematical reasoning, rule-based verifiers have been widely adopted in previous works to train strong reasoning models. However, the reliability of these verifiers and their impact on the RL training process remain poorly understood. In this work, we take mathematical reasoning as a case study and conduct a comprehensive analysis of various verifiers in both static evaluation and RL training scenarios. First, we find that current open-source rule-based verifiers often fail to recognize equivalent answers presented in different formats across multiple commonly used mathematical datasets, resulting in non-negligible false negative rates. This limitation adversely affects RL training performance and becomes more pronounced as the policy model gets stronger. Subsequently, we investigate model-based verifiers as a potential solution to address these limitations. While the static evaluation shows that model-based verifiers achieve significantly higher verification accuracy, further analysis and RL training results imply that they are highly susceptible to hacking, where they misclassify certain patterns in responses as correct (i.e., false positives). This vulnerability is exploited during policy model optimization, leading to artificially inflated rewards. Our findings underscore the unique risks inherent to both rule-based and model-based verifiers, aiming to offer valuable insights to develop more robust reward systems in reinforcement learning.', 'score': 3, 'issue_id': 4015, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 мая', 'en': 'May 28', 'zh': '5月28日'}, 'hash': 'bb47fbc50ee1bc3f', 'authors': ['Yuzhen Huang', 'Weihao Zeng', 'Xingshan Zeng', 'Qi Zhu', 'Junxian He'], 'affiliations': ['The Chinese University of Hong Kong', 'The Hong Kong University of Science and Technology', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.22203.jpg', 'data': {'categories': ['#optimization', '#training', '#math', '#rl', '#security', '#reasoning'], 'emoji': '🔍', 'ru': {'title': 'Ограничения верификаторов в RLVR: необходимость более надежных систем', 'desc': 'Исследование анализирует эффективность и надежность верификаторов на основе правил и моделей в обучении с подкреплением с проверяемым вознаграждением (RLVR). Авторы обнаружили, что верификаторы на основе правил часто не распознают эквивалентные ответы в разных форматах, что негативно влияет на процесс обучения. Модельные верификаторы показывают более высокую точность, но подвержены уязвимостям и могут быть обмануты во время оптимизации политики. Результаты подчеркивают риски обоих типов верификаторов и необходимость разработки более надежных систем вознаграждения в обучении с подкреплением.'}, 'en': {'title': 'Enhancing Trust in Reinforcement Learning Verifiers', 'desc': 'This paper investigates the effectiveness of rule-based and model-based verifiers in reinforcement learning with verifiable rewards (RLVR), particularly in the context of mathematical reasoning tasks. It highlights that rule-based verifiers often fail to recognize equivalent answers in different formats, leading to false negatives that hinder the training of reinforcement learning models. The study also reveals that while model-based verifiers show higher accuracy in static evaluations, they are vulnerable to misclassifying incorrect patterns as correct, resulting in false positives that can inflate rewards during training. Overall, the research emphasizes the need for more reliable verification methods to enhance the robustness of reward systems in RLVR applications.'}, 'zh': {'title': '强化学习中的验证器风险与挑战', 'desc': '本研究探讨了在可验证奖励的强化学习中，基于规则和基于模型的验证器的有效性和可靠性。我们发现，当前的基于规则的验证器在识别不同格式的等效答案时存在显著的假阴性率，这对强化学习的训练性能产生了负面影响。虽然基于模型的验证器在静态评估中表现出更高的验证准确性，但它们在强化学习训练中容易受到攻击，导致假阳性现象。我们的研究揭示了这两种验证器的独特风险，为开发更强大的强化学习奖励系统提供了重要见解。'}}}, {'id': 'https://huggingface.co/papers/2505.21876', 'title': 'EPiC: Efficient Video Camera Control Learning with Precise Anchor-Video\n  Guidance', 'url': 'https://huggingface.co/papers/2505.21876', 'abstract': 'EPiC is a framework for efficient 3D camera control in video diffusion models that constructs high-quality anchor videos through first-frame visibility masking, integrates them using a lightweight ControlNet module, and achieves state-of-the-art performance on I2V tasks with minimal resources.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent approaches on 3D camera control in video diffusion models (VDMs) often create anchor videos to guide diffusion models as a structured prior by rendering from estimated point clouds following annotated camera trajectories. However, errors inherent in point cloud estimation often lead to inaccurate anchor videos. Moreover, the requirement for extensive camera trajectory annotations further increases resource demands. To address these limitations, we introduce EPiC, an efficient and precise camera control learning framework that automatically constructs high-quality anchor videos without expensive camera trajectory annotations. Concretely, we create highly precise anchor videos for training by masking source videos based on first-frame visibility. This approach ensures high alignment, eliminates the need for camera trajectory annotations, and thus can be readily applied to any in-the-wild video to generate image-to-video (I2V) training pairs. Furthermore, we introduce Anchor-ControlNet, a lightweight conditioning module that integrates anchor video guidance in visible regions to pretrained VDMs, with less than 1% of backbone model parameters. By combining the proposed anchor video data and ControlNet module, EPiC achieves efficient training with substantially fewer parameters, training steps, and less data, without requiring modifications to the diffusion model backbone typically needed to mitigate rendering misalignments. Although being trained on masking-based anchor videos, our method generalizes robustly to anchor videos made with point clouds during inference, enabling precise 3D-informed camera control. EPiC achieves SOTA performance on RealEstate10K and MiraData for I2V camera control task, demonstrating precise and robust camera control ability both quantitatively and qualitatively. Notably, EPiC also exhibits strong zero-shot generalization to video-to-video scenarios.', 'score': 3, 'issue_id': 4015, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 мая', 'en': 'May 28', 'zh': '5月28日'}, 'hash': '2085924b7cd22768', 'authors': ['Zun Wang', 'Jaemin Cho', 'Jialu Li', 'Han Lin', 'Jaehong Yoon', 'Yue Zhang', 'Mohit Bansal'], 'affiliations': ['UNC Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2505.21876.jpg', 'data': {'categories': ['#optimization', '#video', '#training', '#transfer_learning', '#diffusion', '#3d', '#multimodal'], 'emoji': '🎥', 'ru': {'title': 'Эффективный 3D-контроль камеры без сложных аннотаций', 'desc': 'EPiC - это фреймворк для эффективного 3D-контроля камеры в моделях видеодиффузии. Он создает высококачественные опорные видео с помощью маскирования видимости первого кадра и интегрирует их с использованием облегченного модуля ControlNet. EPiC достигает наилучших результатов в задачах преобразования изображения в видео (I2V) при минимальных ресурсах. Этот подход не требует дорогостоящих аннотаций траектории камеры и может применяться к любому видео для создания обучающих пар I2V.'}, 'en': {'title': 'Efficient 3D Camera Control with EPiC', 'desc': 'EPiC is a novel framework designed to enhance 3D camera control in video diffusion models (VDMs) by creating high-quality anchor videos without the need for extensive camera trajectory annotations. It utilizes first-frame visibility masking to ensure that the generated anchor videos are accurately aligned with the source videos, thus improving the training process. The framework incorporates a lightweight ControlNet module that integrates these anchor videos into existing VDMs, achieving state-of-the-art performance on image-to-video (I2V) tasks while minimizing resource requirements. Additionally, EPiC demonstrates strong generalization capabilities, performing well even in zero-shot video-to-video scenarios, making it a versatile tool for efficient camera control.'}, 'zh': {'title': 'EPiC：高效的3D相机控制新框架', 'desc': 'EPiC是一个高效的3D相机控制框架，专为视频扩散模型设计。它通过第一帧可见性掩蔽自动构建高质量的锚视频，避免了昂贵的相机轨迹注释需求。该框架结合了轻量级的ControlNet模块，能够在资源有限的情况下实现图像到视频（I2V）任务的最先进性能。EPiC在RealEstate10K和MiraData数据集上表现出色，展现了其精确和稳健的相机控制能力。'}}}, {'id': 'https://huggingface.co/papers/2505.18700', 'title': 'GRE Suite: Geo-localization Inference via Fine-Tuned Vision-Language\n  Models and Enhanced Reasoning Chains', 'url': 'https://huggingface.co/papers/2505.18700', 'abstract': 'Recent advances in Visual Language Models (VLMs) have demonstrated exceptional performance in visual reasoning tasks. However, geo-localization presents unique challenges, requiring the extraction of multigranular visual cues from images and their integration with external world knowledge for systematic reasoning. Current approaches to geo-localization tasks often lack robust reasoning mechanisms and explainability, limiting their effectiveness. To address these limitations, we propose the Geo Reason Enhancement (GRE) Suite, a novel framework that augments VLMs with structured reasoning chains for accurate and interpretable location inference. The GRE Suite is systematically developed across three key dimensions: dataset, model, and benchmark. First, we introduce GRE30K, a high-quality geo-localization reasoning dataset designed to facilitate fine-grained visual and contextual analysis. Next, we present the GRE model, which employs a multi-stage reasoning strategy to progressively infer scene attributes, local details, and semantic features, thereby narrowing down potential geographic regions with enhanced precision. Finally, we construct the Geo Reason Evaluation Benchmark (GREval-Bench), a comprehensive evaluation framework that assesses VLMs across diverse urban, natural, and landmark scenes to measure both coarse-grained (e.g., country, continent) and fine-grained (e.g., city, street) localization performance. Experimental results demonstrate that GRE significantly outperforms existing methods across all granularities of geo-localization tasks, underscoring the efficacy of reasoning-augmented VLMs in complex geographic inference. Code and data will be released at https://github.com/Thorin215/GRE.', 'score': 3, 'issue_id': 4014, 'pub_date': '2025-05-24', 'pub_date_card': {'ru': '24 мая', 'en': 'May 24', 'zh': '5月24日'}, 'hash': '6e364db427c53c05', 'authors': ['Chun Wang', 'Xiaoran Pan', 'Zihao Pan', 'Haofan Wang', 'Yiren Song'], 'affiliations': ['LibLib.ai', 'NUS', 'Sun Yat-sen University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.18700.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#cv', '#interpretability', '#reasoning'], 'emoji': '🌍', 'ru': {'title': 'Умное определение местоположения: VLM с усиленным географическим рассуждением', 'desc': 'Статья представляет набор инструментов Geo Reason Enhancement (GRE) Suite для улучшения геолокализации с помощью визуальных языковых моделей (VLM). GRE Suite включает в себя новый датасет GRE30K, модель GRE с многоступенчатой стратегией рассуждений, и бенчмарк GREval-Bench для оценки производительности. Подход основан на структурированных цепочках рассуждений для точного и интерпретируемого определения местоположения. Экспериментальные результаты показывают значительное превосходство GRE над существующими методами в задачах геолокализации различной гранулярности.'}, 'en': {'title': 'Enhancing Geo-Localization with Structured Reasoning', 'desc': 'This paper introduces the Geo Reason Enhancement (GRE) Suite, a framework designed to improve visual language models (VLMs) for geo-localization tasks. It addresses the challenges of extracting detailed visual cues and integrating them with external knowledge for better reasoning. The GRE Suite includes a new dataset called GRE30K, a model that uses multi-stage reasoning to enhance location inference, and a benchmark for evaluating performance across various geographic contexts. Experimental results show that the GRE Suite significantly outperforms existing methods, highlighting the importance of structured reasoning in visual localization.'}, 'zh': {'title': '增强推理，精准定位！', 'desc': '最近，视觉语言模型（VLMs）在视觉推理任务中表现出色。然而，地理定位面临独特挑战，需要从图像中提取多层次的视觉线索，并将其与外部世界知识结合进行系统推理。目前的地理定位方法往往缺乏稳健的推理机制和可解释性，限制了其有效性。为了解决这些问题，我们提出了Geo Reason Enhancement（GRE）套件，这是一种新框架，通过结构化推理链增强VLMs，以实现准确且可解释的定位推断。'}}}, {'id': 'https://huggingface.co/papers/2505.22613', 'title': 'RICO: Improving Accuracy and Completeness in Image Recaptioning via\n  Visual Reconstruction', 'url': 'https://huggingface.co/papers/2505.22613', 'abstract': 'A novel iterative framework, RICO, improves image caption accuracy by using visual reconstruction and a text-to-image model to refine discrepancies, while RICO-Flash enhances efficiency using DPO.  \t\t\t\t\tAI-generated summary \t\t\t\t Image recaptioning is widely used to generate training datasets with enhanced quality for various multimodal tasks. Existing recaptioning methods typically rely on powerful multimodal large language models (MLLMs) to enhance textual descriptions, but often suffer from inaccuracies due to hallucinations and incompleteness caused by missing fine-grained details. To address these limitations, we propose RICO, a novel framework that refines captions through visual reconstruction. Specifically, we leverage a text-to-image model to reconstruct a caption into a reference image, and prompt an MLLM to identify discrepancies between the original and reconstructed images to refine the caption. This process is performed iteratively, further progressively promoting the generation of more faithful and comprehensive descriptions. To mitigate the additional computational cost induced by the iterative process, we introduce RICO-Flash, which learns to generate captions like RICO using DPO. Extensive experiments demonstrate that our approach significantly improves caption accuracy and completeness, outperforms most baselines by approximately 10% on both CapsBench and CompreCap. Code released at https://github.com/wangyuchi369/RICO.', 'score': 2, 'issue_id': 4015, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 мая', 'en': 'May 28', 'zh': '5月28日'}, 'hash': '0f01107a0f4b4350', 'authors': ['Yuchi Wang', 'Yishuo Cai', 'Shuhuai Ren', 'Sihan Yang', 'Linli Yao', 'Yuanxin Liu', 'Yuanxing Zhang', 'Pengfei Wan', 'Xu Sun'], 'affiliations': ['Central South University', 'Kuaishou Technology', 'National Key Laboratory for Multimedia Information Processing, Peking University', 'Xian JiaoTong University'], 'pdf_title_img': 'assets/pdf/title_img/2505.22613.jpg', 'data': {'categories': ['#optimization', '#dataset', '#training', '#open_source', '#hallucinations', '#rlhf', '#multimodal'], 'emoji': '🖼️', 'ru': {'title': 'Точные подписи к изображениям через визуальную реконструкцию', 'desc': 'RICO - это новая итеративная система для улучшения точности подписей к изображениям. Она использует визуальную реконструкцию и модель преобразования текста в изображение для уточнения несоответствий. RICO-Flash повышает эффективность процесса с помощью DPO (Direct Preference Optimization). Эксперименты показывают, что подход значительно улучшает точность и полноту подписей, превосходя большинство базовых методов примерно на 10% в тестах CapsBench и CompreCap.'}, 'en': {'title': 'RICO: Refining Image Captions with Visual Reconstruction', 'desc': 'The paper introduces RICO, an innovative framework designed to enhance the accuracy of image captions by utilizing visual reconstruction techniques. It employs a text-to-image model to create a reference image from a caption, allowing a multimodal large language model (MLLM) to identify and correct discrepancies between the original and reconstructed images. This iterative process helps generate more accurate and detailed captions by progressively refining them. To improve efficiency, the authors present RICO-Flash, which uses DPO to streamline the caption generation process while maintaining high quality.'}, 'zh': {'title': 'RICO：提升图像描述准确性的创新框架', 'desc': '本文提出了一种新的迭代框架RICO，通过视觉重建和文本到图像模型来提高图像描述的准确性。RICO利用文本到图像模型将描述重建为参考图像，并通过多模态大语言模型（MLLM）识别原始图像与重建图像之间的差异，从而逐步改进描述。为了提高效率，本文还引入了RICO-Flash，利用DPO技术来生成描述，减少计算成本。实验结果表明，该方法在CapsBench和CompreCap数据集上显著提高了描述的准确性和完整性，超越了大多数基线方法。'}}}, {'id': 'https://huggingface.co/papers/2505.17507', 'title': 'Benchmarking Recommendation, Classification, and Tracing Based on\n  Hugging Face Knowledge Graph', 'url': 'https://huggingface.co/papers/2505.17507', 'abstract': 'HuggingKG, a large-scale knowledge graph, enhances open source ML resource management by enabling advanced queries and analyses via HuggingBench.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid growth of open source machine learning (ML) resources, such as models and datasets, has accelerated IR research. However, existing platforms like Hugging Face do not explicitly utilize structured representations, limiting advanced queries and analyses such as tracing model evolution and recommending relevant datasets. To fill the gap, we construct HuggingKG, the first large-scale knowledge graph built from the Hugging Face community for ML resource management. With 2.6 million nodes and 6.2 million edges, HuggingKG captures domain-specific relations and rich textual attributes. It enables us to further present HuggingBench, a multi-task benchmark with three novel test collections for IR tasks including resource recommendation, classification, and tracing. Our experiments reveal unique characteristics of HuggingKG and the derived tasks. Both resources are publicly available, expected to advance research in open source resource sharing and management.', 'score': 2, 'issue_id': 4013, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 мая', 'en': 'May 23', 'zh': '5月23日'}, 'hash': '5f55eec50d967abe', 'authors': ['Qiaosheng Chen', 'Kaijia Huang', 'Xiao Zhou', 'Weiqing Luo', 'Yuanning Cui', 'Gong Cheng'], 'affiliations': ['Nanjing University State Key Laboratory for Novel Software Technology Nanjing, Jiangsu, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.17507.jpg', 'data': {'categories': ['#dataset', '#open_source', '#benchmark', '#graphs'], 'emoji': '🧠', 'ru': {'title': 'HuggingKG: Структурированное представление ресурсов ML для продвинутого анализа', 'desc': 'HuggingKG - это первый крупномасштабный граф знаний, созданный на основе сообщества Hugging Face для управления ресурсами машинного обучения. Он содержит 2,6 миллиона узлов и 6,2 миллиона связей, отражающих отношения и атрибуты в области ML. HuggingKG позволяет создать HuggingBench - многозадачный бенчмарк для задач информационного поиска, включая рекомендацию ресурсов, классификацию и отслеживание. Эти ресурсы доступны публично и призваны способствовать исследованиям в области обмена и управления ресурсами с открытым исходным кодом.'}, 'en': {'title': 'Empowering ML Resource Management with HuggingKG', 'desc': 'HuggingKG is a large-scale knowledge graph designed to improve the management of open source machine learning resources. It addresses the limitations of existing platforms by providing structured representations that allow for advanced queries and analyses. With millions of nodes and edges, HuggingKG captures important relationships and attributes within the ML community. Additionally, it introduces HuggingBench, a benchmark for evaluating tasks like resource recommendation and classification, facilitating better resource sharing and management in the field.'}, 'zh': {'title': 'HuggingKG：开源机器学习资源管理的新视野', 'desc': 'HuggingKG是一个大型知识图谱，旨在改善开源机器学习资源的管理。它通过结构化表示，支持高级查询和分析，帮助追踪模型演变和推荐相关数据集。HuggingKG包含260万个节点和620万个边，捕捉了领域特定的关系和丰富的文本属性。该图谱与HuggingBench基准测试结合，推动了信息检索任务的研究，包括资源推荐和分类等。'}}}, {'id': 'https://huggingface.co/papers/2505.12667', 'title': 'Safe-Sora: Safe Text-to-Video Generation via Graphical Watermarking', 'url': 'https://huggingface.co/papers/2505.12667', 'abstract': 'Safe-Sora embeds invisible watermarks into AI-generated videos using a hierarchical adaptive matching mechanism and a 3D wavelet transform-enhanced Mamba architecture, achieving top performance in video quality, watermark fidelity, and robustness.  \t\t\t\t\tAI-generated summary \t\t\t\t The explosive growth of generative video models has amplified the demand for reliable copyright preservation of AI-generated content. Despite its popularity in image synthesis, invisible generative watermarking remains largely underexplored in video generation. To address this gap, we propose Safe-Sora, the first framework to embed graphical watermarks directly into the video generation process. Motivated by the observation that watermarking performance is closely tied to the visual similarity between the watermark and cover content, we introduce a hierarchical coarse-to-fine adaptive matching mechanism. Specifically, the watermark image is divided into patches, each assigned to the most visually similar video frame, and further localized to the optimal spatial region for seamless embedding. To enable spatiotemporal fusion of watermark patches across video frames, we develop a 3D wavelet transform-enhanced Mamba architecture with a novel spatiotemporal local scanning strategy, effectively modeling long-range dependencies during watermark embedding and retrieval. To the best of our knowledge, this is the first attempt to apply state space models to watermarking, opening new avenues for efficient and robust watermark protection. Extensive experiments demonstrate that Safe-Sora achieves state-of-the-art performance in terms of video quality, watermark fidelity, and robustness, which is largely attributed to our proposals. We will release our code upon publication.', 'score': 2, 'issue_id': 4014, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 мая', 'en': 'May 19', 'zh': '5月19日'}, 'hash': '975482063e194827', 'authors': ['Zihan Su', 'Xuerui Qiu', 'Hongbin Xu', 'Tangyu Jiang', 'Junhao Zhuang', 'Chun Yuan', 'Ming Li', 'Shengfeng He', 'Fei Richard Yu'], 'affiliations': ['Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ)', 'Institute of Automation, Chinese Academy of Sciences', 'Singapore Management University', 'South China University of Technology', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.12667.jpg', 'data': {'categories': ['#video', '#3d', '#architecture', '#security'], 'emoji': '🎥', 'ru': {'title': 'Защита авторских прав на AI-видео с помощью невидимых водяных знаков', 'desc': 'Safe-Sora - это новая система для внедрения невидимых водяных знаков в видео, генерируемые искусственным интеллектом. Она использует иерархический механизм адаптивного сопоставления и улучшенную архитектуру Mamba с 3D вейвлет-преобразованием. Система достигает высоких показателей качества видео, точности водяных знаков и устойчивости к атакам. Safe-Sora применяет модели состояния пространства для эффективного внедрения и извлечения водяных знаков в видеоконтенте.'}, 'en': {'title': 'Revolutionizing Copyright Protection in AI-Generated Videos with Safe-Sora', 'desc': 'Safe-Sora is a novel framework designed to embed invisible watermarks into AI-generated videos, addressing the need for copyright protection in the growing field of generative video models. It utilizes a hierarchical adaptive matching mechanism to ensure that the watermark is seamlessly integrated into the video by matching it with visually similar frames. The framework employs a 3D wavelet transform-enhanced Mamba architecture to effectively manage the spatial and temporal aspects of watermark embedding, allowing for robust watermark retrieval. Experimental results show that Safe-Sora achieves superior video quality and watermark fidelity compared to existing methods, marking a significant advancement in the field of generative watermarking.'}, 'zh': {'title': 'Safe-Sora：保护 AI 视频版权的新方法', 'desc': 'Safe-Sora 是一种新颖的框架，能够在 AI 生成的视频中嵌入不可见水印，以保护版权。该方法采用分层自适应匹配机制，将水印图像分割成多个小块，并将其嵌入到与之视觉相似的视频帧中。通过增强的 3D 小波变换 Mamba 架构，Safe-Sora 实现了水印在时空上的融合，确保了水印的鲁棒性和高保真度。实验结果表明，Safe-Sora 在视频质量和水印保护方面达到了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2505.22645', 'title': 'Characterizing Bias: Benchmarking Large Language Models in Simplified\n  versus Traditional Chinese', 'url': 'https://huggingface.co/papers/2505.22645', 'abstract': 'Research examines LLM performance biases between Simplified and Traditional Chinese in regional term and name choice tasks, attributing differences to training data and tokenization.  \t\t\t\t\tAI-generated summary \t\t\t\t While the capabilities of Large Language Models (LLMs) have been studied in both Simplified and Traditional Chinese, it is yet unclear whether LLMs exhibit differential performance when prompted in these two variants of written Chinese. This understanding is critical, as disparities in the quality of LLM responses can perpetuate representational harms by ignoring the different cultural contexts underlying Simplified versus Traditional Chinese, and can exacerbate downstream harms in LLM-facilitated decision-making in domains such as education or hiring. To investigate potential LLM performance disparities, we design two benchmark tasks that reflect real-world scenarios: regional term choice (prompting the LLM to name a described item which is referred to differently in Mainland China and Taiwan), and regional name choice (prompting the LLM to choose who to hire from a list of names in both Simplified and Traditional Chinese). For both tasks, we audit the performance of 11 leading commercial LLM services and open-sourced models -- spanning those primarily trained on English, Simplified Chinese, or Traditional Chinese. Our analyses indicate that biases in LLM responses are dependent on both the task and prompting language: while most LLMs disproportionately favored Simplified Chinese responses in the regional term choice task, they surprisingly favored Traditional Chinese names in the regional name choice task. We find that these disparities may arise from differences in training data representation, written character preferences, and tokenization of Simplified and Traditional Chinese. These findings highlight the need for further analysis of LLM biases; as such, we provide an open-sourced benchmark dataset to foster reproducible evaluations of future LLM behavior across Chinese language variants (https://github.com/brucelyu17/SC-TC-Bench).', 'score': 1, 'issue_id': 4015, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 мая', 'en': 'May 28', 'zh': '5月28日'}, 'hash': 'fa422e70671c24d0', 'authors': ['Hanjia Lyu', 'Jiebo Luo', 'Jian Kang', 'Allison Koenecke'], 'affiliations': ['Cornell University', 'University of Rochester'], 'pdf_title_img': 'assets/pdf/title_img/2505.22645.jpg', 'data': {'categories': ['#ethics', '#low_resource', '#multilingual', '#dataset', '#benchmark', '#open_source'], 'emoji': '🇨🇳', 'ru': {'title': 'Скрытые предубеждения LLM в китайском языке: упрощенный vs традиционный', 'desc': 'Исследование анализирует различия в производительности больших языковых моделей (LLM) между упрощенным и традиционным китайским языком в задачах выбора региональных терминов и имен. Результаты показывают, что большинство LLM отдают предпочтение упрощенному китайскому в выборе терминов, но традиционному китайскому в выборе имен. Эти различия могут быть обусловлены особенностями обучающих данных, предпочтениями в написании символов и токенизацией. Исследование подчеркивает необходимость дальнейшего анализа предвзятостей LLM в отношении вариантов китайского языка.'}, 'en': {'title': 'Unveiling Biases: LLM Performance in Simplified vs. Traditional Chinese', 'desc': 'This research investigates how Large Language Models (LLMs) perform differently when using Simplified versus Traditional Chinese, focusing on regional term and name choice tasks. The study reveals that LLMs often show biases based on the language variant used, which can lead to unequal representation and potential harms in decision-making processes. By analyzing 11 different LLMs, the authors found that while Simplified Chinese was favored in naming items, Traditional Chinese names were preferred in hiring scenarios. The paper emphasizes the importance of understanding these biases and provides a benchmark dataset for future evaluations of LLM performance across Chinese language variants.'}, 'zh': {'title': '大型语言模型的中文表现偏差研究', 'desc': '本研究探讨了大型语言模型（LLM）在简体中文和繁体中文之间的表现偏差，特别是在地区术语和名称选择任务中。研究发现，LLM的表现差异与训练数据和分词方式有关，这可能导致对不同文化背景的忽视。通过设计两个基准任务，研究审计了11个主要的商业LLM服务和开源模型的表现。结果显示，LLM在地区术语选择任务中偏向简体中文，而在名称选择任务中则意外偏向繁体中文，强调了对LLM偏见的进一步分析的必要性。'}}}, {'id': 'https://huggingface.co/papers/2505.22525', 'title': 'Thinking with Generated Images', 'url': 'https://huggingface.co/papers/2505.22525', 'abstract': 'Thinking with Generated Images allows large multimodal models to generate and critique intermediate visual steps, enhancing visual reasoning capabilities and achieving significant improvements in complex scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Thinking with Generated Images, a novel paradigm that fundamentally transforms how large multimodal models (LMMs) engage with visual reasoning by enabling them to natively think across text and vision modalities through spontaneous generation of intermediate visual thinking steps. Current visual reasoning with LMMs is constrained to either processing fixed user-provided images or reasoning solely through text-based chain-of-thought (CoT). Thinking with Generated Images unlocks a new dimension of cognitive capability where models can actively construct intermediate visual thoughts, critique their own visual hypotheses, and refine them as integral components of their reasoning process. We demonstrate the effectiveness of our approach through two complementary mechanisms: (1) vision generation with intermediate visual subgoals, where models decompose complex visual tasks into manageable components that are generated and integrated progressively, and (2) vision generation with self-critique, where models generate an initial visual hypothesis, analyze its shortcomings through textual reasoning, and produce refined outputs based on their own critiques. Our experiments on vision generation benchmarks show substantial improvements over baseline approaches, with our models achieving up to 50% (from 38% to 57%) relative improvement in handling complex multi-object scenarios. From biochemists exploring novel protein structures, and architects iterating on spatial designs, to forensic analysts reconstructing crime scenes, and basketball players envisioning strategic plays, our approach enables AI models to engage in the kind of visual imagination and iterative refinement that characterizes human creative, analytical, and strategic thinking. We release our open-source suite at https://github.com/GAIR-NLP/thinking-with-generated-images.', 'score': 1, 'issue_id': 4015, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 мая', 'en': 'May 28', 'zh': '5月28日'}, 'hash': '0d852a3cf6a78a01', 'authors': ['Ethan Chern', 'Zhulin Hu', 'Steffi Chern', 'Siqi Kou', 'Jiadi Su', 'Yan Ma', 'Zhijie Deng', 'Pengfei Liu'], 'affiliations': ['Fudan University', 'Generative AI Research Lab (GAIR)', 'SII', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2505.22525.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#cv', '#reasoning', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'Визуальное воображение ИИ: новый уровень мышления мультимодальных моделей', 'desc': "Статья представляет новую парадигму 'Мышление с генерируемыми изображениями', которая позволяет крупным мультимодальным моделям (LMM) генерировать и критиковать промежуточные визуальные шаги в процессе рассуждений. Этот подход существенно улучшает способности моделей к визуальному мышлению, позволяя им создавать промежуточные визуальные гипотезы и уточнять их. Эксперименты показали значительное улучшение результатов в сложных сценариях с несколькими объектами по сравнению с базовыми подходами. Авторы демонстрируют потенциал метода в различных областях, от биохимии до спортивной аналитики."}, 'en': {'title': 'Empowering AI with Visual Imagination and Self-Critique', 'desc': "The paper introduces 'Thinking with Generated Images', a new approach that enhances large multimodal models (LMMs) by allowing them to generate and evaluate intermediate visual steps during reasoning. This method enables models to create visual representations spontaneously, rather than relying solely on fixed images or text-based reasoning. By breaking down complex visual tasks into smaller, manageable components and allowing for self-critique, the models can refine their outputs iteratively. The results show significant improvements in performance on visual reasoning tasks, demonstrating the potential for AI to mimic human-like visual thinking and creativity."}, 'zh': {'title': '生成图像思维：提升视觉推理能力的创新方法', 'desc': '本文介绍了一种新颖的思维方式——生成图像思维，旨在提升大型多模态模型（LMMs）在视觉推理方面的能力。通过生成中间视觉步骤，模型能够在文本和视觉之间自发地进行思考，从而克服了传统方法的局限。该方法包括两个机制：一是通过中间视觉子目标分解复杂任务，二是通过自我批判分析初步视觉假设的不足。实验结果表明，该方法在处理复杂多对象场景时，相较于基线方法有显著提升，最高可达50%的相对改善。'}}}, {'id': 'https://huggingface.co/papers/2505.21960', 'title': 'One-Way Ticket:Time-Independent Unified Encoder for Distilling\n  Text-to-Image Diffusion Models', 'url': 'https://huggingface.co/papers/2505.21960', 'abstract': 'Time-independent Unified Encoder TiUE reduces inference time and improves diversity and quality in Text-to-Image diffusion models by sharing encoder features across decoder time steps.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-Image (T2I) diffusion models have made remarkable advancements in generative modeling; however, they face a trade-off between inference speed and image quality, posing challenges for efficient deployment. Existing distilled T2I models can generate high-fidelity images with fewer sampling steps, but often struggle with diversity and quality, especially in one-step models. From our analysis, we observe redundant computations in the UNet encoders. Our findings suggest that, for T2I diffusion models, decoders are more adept at capturing richer and more explicit semantic information, while encoders can be effectively shared across decoders from diverse time steps. Based on these observations, we introduce the first Time-independent Unified Encoder TiUE for the student model UNet architecture, which is a loop-free image generation approach for distilling T2I diffusion models. Using a one-pass scheme, TiUE shares encoder features across multiple decoder time steps, enabling parallel sampling and significantly reducing inference time complexity. In addition, we incorporate a KL divergence term to regularize noise prediction, which enhances the perceptual realism and diversity of the generated images. Experimental results demonstrate that TiUE outperforms state-of-the-art methods, including LCM, SD-Turbo, and SwiftBrushv2, producing more diverse and realistic results while maintaining the computational efficiency.', 'score': 1, 'issue_id': 4015, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 мая', 'en': 'May 28', 'zh': '5月28日'}, 'hash': 'fff7f01ccce8324f', 'authors': ['Senmao Li', 'Lei Wang', 'Kai Wang', 'Tao Liu', 'Jiehang Xie', 'Joost van de Weijer', 'Fahad Shahbaz Khan', 'Shiqi Yang', 'Yaxing Wang', 'Jian Yang'], 'affiliations': ['Computer Vision Center, Universitat Aut`onoma de Barcelona', 'Linkoping University', 'Mohamed bin Zayed University of AI', 'Nankai International Advanced Research Institute (Shenzhen Futian), Nankai University', 'SB Intuitions, SoftBank', 'School of Big Data and Computer Science, Guizhou Normal University', 'VCIP, CS, Nankai University'], 'pdf_title_img': 'assets/pdf/title_img/2505.21960.jpg', 'data': {'categories': ['#architecture', '#optimization', '#diffusion', '#cv', '#inference'], 'emoji': '🖼️', 'ru': {'title': 'Ускорение генерации изображений без потери качества', 'desc': 'Статья представляет Time-independent Unified Encoder (TiUE) - новый подход к дистилляции диффузионных моделей для генерации изображений по тексту. TiUE использует единый энкодер для разных временных шагов декодера, что значительно сокращает время вывода. Авторы также вводят регуляризацию KL-дивергенцией для улучшения качества и разнообразия генерируемых изображений. Эксперименты показывают, что TiUE превосходит современные методы по качеству результатов при сохранении вычислительной эффективности.'}, 'en': {'title': 'Accelerating Image Generation with Unified Encoding', 'desc': 'The paper introduces the Time-independent Unified Encoder (TiUE), a novel approach to enhance Text-to-Image (T2I) diffusion models. By sharing encoder features across different decoder time steps, TiUE reduces the inference time while improving the diversity and quality of generated images. The study highlights that decoders are better at capturing semantic information, allowing for a more efficient use of encoder resources. Experimental results show that TiUE surpasses existing models in generating high-fidelity images with greater diversity and lower computational costs.'}, 'zh': {'title': '时间独立统一编码器TiUE：提升推理速度与图像质量的创新方案', 'desc': '本文提出了一种新的时间独立统一编码器TiUE，旨在提高文本到图像扩散模型的推理速度和图像质量。通过在解码器的多个时间步骤之间共享编码器特征，TiUE显著减少了推理时间的复杂性。研究表明，解码器在捕捉丰富的语义信息方面更为有效，而编码器可以在不同时间步骤之间有效共享。实验结果显示，TiUE在生成多样性和真实感方面优于现有的最先进方法，同时保持了计算效率。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (3)', '#agents (1)', '#agi (1)', '#alignment (2)', '#architecture (5)', '#audio', '#benchmark (11)', '#cv (5)', '#data (1)', '#dataset (6)', '#diffusion (4)', '#ethics (1)', '#games', '#graphs (1)', '#hallucinations (1)', '#healthcare', '#inference (3)', '#interpretability (2)', '#leakage', '#long_context', '#low_resource (1)', '#machine_translation', '#math (2)', '#multilingual (1)', '#multimodal (5)', '#open_source (8)', '#optimization (10)', '#plp', '#rag (1)', '#reasoning (11)', '#rl (5)', '#rlhf (3)', '#robotics', '#science', '#security (2)', '#small_models (1)', '#story_generation', '#survey', '#synthetic (2)', '#training (12)', '#transfer_learning (1)', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-05-29 04:17',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-05-29 04:17')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-05-29 04:17')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    