{
    "date": {
        "ru": "14 Ð¼Ð°Ñ€Ñ‚Ð°",
        "en": "March 14",
        "zh": "3æœˆ14æ—¥"
    },
    "time_utc": "2025-03-14 17:09",
    "weekday": 4,
    "issue_id": 2714,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2503.10613",
            "title": "CoSTAast: Cost-Sensitive Toolpath Agent for Multi-turn Image Editing",
            "url": "https://huggingface.co/papers/2503.10613",
            "abstract": "Text-to-image models like stable diffusion and DALLE-3 still struggle with multi-turn image editing. We decompose such a task as an agentic workflow (path) of tool use that addresses a sequence of subtasks by AI tools of varying costs. Conventional search algorithms require expensive exploration to find tool paths. While large language models (LLMs) possess prior knowledge of subtask planning, they may lack accurate estimations of capabilities and costs of tools to determine which to apply in each subtask. Can we combine the strengths of both LLMs and graph search to find cost-efficient tool paths? We propose a three-stage approach \"CoSTA*\" that leverages LLMs to create a subtask tree, which helps prune a graph of AI tools for the given task, and then conducts A* search on the small subgraph to find a tool path. To better balance the total cost and quality, CoSTA* combines both metrics of each tool on every subtask to guide the A* search. Each subtask's output is then evaluated by a vision-language model (VLM), where a failure will trigger an update of the tool's cost and quality on the subtask. Hence, the A* search can recover from failures quickly to explore other paths. Moreover, CoSTA* can automatically switch between modalities across subtasks for a better cost-quality trade-off. We build a novel benchmark of challenging multi-turn image editing, on which CoSTA* outperforms state-of-the-art image-editing models or agents in terms of both cost and quality, and performs versatile trade-offs upon user preference.",
            "score": 38,
            "issue_id": 2700,
            "pub_date": "2025-03-13",
            "pub_date_card": {
                "ru": "13 Ð¼Ð°Ñ€Ñ‚Ð°",
                "en": "March 13",
                "zh": "3æœˆ13æ—¥"
            },
            "hash": "7f2d9ee971af97a8",
            "authors": [
                "Advait Gupta",
                "NandaKiran Velaga",
                "Dang Nguyen",
                "Tianyi Zhou"
            ],
            "affiliations": [
                "University of Maryland, College Park"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.10613.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#benchmark",
                    "#multimodal",
                    "#agents",
                    "#optimization"
                ],
                "emoji": "ðŸŽ¨",
                "ru": {
                    "title": "CoSTA*: Ð£Ð¼Ð½Ð¾Ðµ Ñ€ÐµÐ´Ð°ÐºÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ Ð˜Ð˜ Ð¸ Ð¿Ð¾Ð¸ÑÐºÐ°",
                    "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð½Ð¾Ð²Ñ‹Ð¹ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ CoSTA* Ð´Ð»Ñ Ð¼Ð½Ð¾Ð³Ð¾ÑÑ‚Ð°Ð¿Ð½Ð¾Ð³Ð¾ Ñ€ÐµÐ´Ð°ÐºÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹. ÐœÐµÑ‚Ð¾Ð´ Ð¾Ð±ÑŠÐµÐ´Ð¸Ð½ÑÐµÑ‚ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ðµ ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ (LLM) Ð´Ð»Ñ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ Ð´ÐµÑ€ÐµÐ²Ð° Ð¿Ð¾Ð´Ð·Ð°Ð´Ð°Ñ‡ Ð¸ Ð°Ð»Ð³Ð¾Ñ€Ð¸Ñ‚Ð¼ Ð¿Ð¾Ð¸ÑÐºÐ° A* Ð´Ð»Ñ Ð½Ð°Ñ…Ð¾Ð¶Ð´ÐµÐ½Ð¸Ñ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð¿ÑƒÑ‚Ð¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð² Ð˜Ð˜. CoSTA* Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½ÑƒÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð² ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ ÑÑ‚Ð°Ð¿Ð° Ð¸ Ð¼Ð¾Ð¶ÐµÑ‚ Ð°Ð´Ð°Ð¿Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒÑÑ Ðº Ð½ÐµÑƒÐ´Ð°Ñ‡Ð°Ð¼. Ð­ÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ñ‹ Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÑŽÑ‚ Ð¿Ñ€ÐµÐ²Ð¾ÑÑ…Ð¾Ð´ÑÑ‚Ð²Ð¾ CoSTA* Ð½Ð°Ð´ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ð¼Ð¸ Ð¼Ð¾Ð´ÐµÐ»ÑÐ¼Ð¸ Ð¸ Ð°Ð³ÐµÐ½Ñ‚Ð°Ð¼Ð¸ Ð² ÑÐ¾Ð¾Ñ‚Ð½Ð¾ÑˆÐµÐ½Ð¸Ð¸ ÑÑ‚Ð¾Ð¸Ð¼Ð¾ÑÑ‚Ð¸ Ð¸ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð° Ñ€ÐµÐ´Ð°ÐºÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹."
                },
                "en": {
                    "title": "Optimizing Multi-Turn Image Editing with CoSTA*",
                    "desc": "This paper addresses the challenges faced by text-to-image models in multi-turn image editing by introducing a new approach called CoSTA*. It combines large language models (LLMs) with graph search techniques to efficiently plan and execute a sequence of subtasks using AI tools. CoSTA* creates a subtask tree to streamline the selection of tools based on their costs and capabilities, and employs A* search to find optimal tool paths. The method also adapts to failures by updating tool metrics, allowing for quick recovery and improved cost-quality trade-offs in image editing tasks."
                },
                "zh": {
                    "title": "é«˜æ•ˆçš„å¤šè½®å›¾åƒç¼–è¾‘å·¥å…·è·¯å¾„ä¼˜åŒ–",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºCoSTA*çš„ä¸‰é˜¶æ®µæ–¹æ³•ï¼Œç”¨äºŽè§£å†³æ–‡æœ¬åˆ°å›¾åƒæ¨¡åž‹åœ¨å¤šè½®å›¾åƒç¼–è¾‘ä¸­çš„æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•ç»“åˆäº†å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰å’Œå›¾æœç´¢çš„ä¼˜ç‚¹ï¼Œé€šè¿‡åˆ›å»ºå­ä»»åŠ¡æ ‘æ¥ä¼˜åŒ–AIå·¥å…·çš„ä½¿ç”¨è·¯å¾„ã€‚CoSTA*åœ¨æ¯ä¸ªå­ä»»åŠ¡ä¸­ç»¼åˆè€ƒè™‘å·¥å…·çš„æˆæœ¬å’Œè´¨é‡ï¼Œä»¥æŒ‡å¯¼A*æœç´¢ï¼Œä»Žè€Œæ‰¾åˆ°é«˜æ•ˆçš„å·¥å…·è·¯å¾„ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒCoSTA*åœ¨å¤šè½®å›¾åƒç¼–è¾‘ä»»åŠ¡ä¸­è¶…è¶Šäº†çŽ°æœ‰çš„æœ€å…ˆè¿›æ¨¡åž‹ï¼Œèƒ½å¤Ÿæ ¹æ®ç”¨æˆ·åå¥½è¿›è¡Œçµæ´»çš„æˆæœ¬ä¸Žè´¨é‡æƒè¡¡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.10622",
            "title": "Transformers without Normalization",
            "url": "https://huggingface.co/papers/2503.10622",
            "abstract": "Normalization layers are ubiquitous in modern neural networks and have long been considered essential. This work demonstrates that Transformers without normalization can achieve the same or better performance using a remarkably simple technique. We introduce Dynamic Tanh (DyT), an element-wise operation DyT(x) = tanh(alpha x), as a drop-in replacement for normalization layers in Transformers. DyT is inspired by the observation that layer normalization in Transformers often produces tanh-like, S-shaped input-output mappings. By incorporating DyT, Transformers without normalization can match or exceed the performance of their normalized counterparts, mostly without hyperparameter tuning. We validate the effectiveness of Transformers with DyT across diverse settings, ranging from recognition to generation, supervised to self-supervised learning, and computer vision to language models. These findings challenge the conventional understanding that normalization layers are indispensable in modern neural networks, and offer new insights into their role in deep networks.",
            "score": 29,
            "issue_id": 2702,
            "pub_date": "2025-03-13",
            "pub_date_card": {
                "ru": "13 Ð¼Ð°Ñ€Ñ‚Ð°",
                "en": "March 13",
                "zh": "3æœˆ13æ—¥"
            },
            "hash": "d790a15c7d75d15e",
            "authors": [
                "Jiachen Zhu",
                "Xinlei Chen",
                "Kaiming He",
                "Yann LeCun",
                "Zhuang Liu"
            ],
            "affiliations": [
                "FAIR, Meta",
                "MIT",
                "New York University",
                "Princeton University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.10622.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#architecture",
                    "#multimodal",
                    "#cv"
                ],
                "emoji": "ðŸ”„",
                "ru": {
                    "title": "ÐŸÑ€Ð¾Ñ‰Ð°Ð¹, Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ: Dynamic Tanh Ð¼ÐµÐ½ÑÐµÑ‚ Ð¿Ñ€Ð°Ð²Ð¸Ð»Ð° Ð¸Ð³Ñ€Ñ‹ Ð² Ð°Ñ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ðµ Ñ‚Ñ€Ð°Ð½ÑÑ„Ð¾Ñ€Ð¼ÐµÑ€Ð¾Ð²",
                    "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»Ð¸ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð¸Ð»Ð¸ Ð°Ð»ÑŒÑ‚ÐµÑ€Ð½Ð°Ñ‚Ð¸Ð²Ñƒ Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¾Ð½Ð½Ñ‹Ð¼ ÑÐ»Ð¾ÑÐ¼ Ð² Ñ‚Ñ€Ð°Ð½ÑÑ„Ð¾Ñ€Ð¼ÐµÑ€Ð°Ñ… - Dynamic Tanh (DyT). DyT - ÑÑ‚Ð¾ Ð¿Ñ€Ð¾ÑÑ‚Ð°Ñ Ð¿Ð¾ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð½Ð°Ñ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ñ, ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ Ð¼Ð¾Ð¶ÐµÑ‚ Ð·Ð°Ð¼ÐµÐ½Ð¸Ñ‚ÑŒ Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸ÑŽ Ð±ÐµÐ· Ð¿Ð¾Ñ‚ÐµÑ€Ð¸ Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸. Ð­ÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ñ‹ Ð¿Ð¾ÐºÐ°Ð·Ð°Ð»Ð¸, Ñ‡Ñ‚Ð¾ Ñ‚Ñ€Ð°Ð½ÑÑ„Ð¾Ñ€Ð¼ÐµÑ€Ñ‹ Ñ DyT Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÑŽÑ‚ Ð½Ðµ Ñ…ÑƒÐ¶Ðµ Ð¸Ð»Ð¸ Ð»ÑƒÑ‡ÑˆÐµ ÑÑ‚Ð°Ð½Ð´Ð°Ñ€Ñ‚Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð² Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ñ‹Ñ… Ð·Ð°Ð´Ð°Ñ‡Ð°Ñ… ÐºÐ¾Ð¼Ð¿ÑŒÑŽÑ‚ÐµÑ€Ð½Ð¾Ð³Ð¾ Ð·Ñ€ÐµÐ½Ð¸Ñ Ð¸ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ ÐµÑÑ‚ÐµÑÑ‚Ð²ÐµÐ½Ð½Ð¾Ð³Ð¾ ÑÐ·Ñ‹ÐºÐ°. Ð­Ñ‚Ð¾ Ð¾Ñ‚ÐºÑ€Ñ‹Ñ‚Ð¸Ðµ ÑÑ‚Ð°Ð²Ð¸Ñ‚ Ð¿Ð¾Ð´ ÑÐ¾Ð¼Ð½ÐµÐ½Ð¸Ðµ Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ð¾ÑÑ‚ÑŒ Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¾Ð½Ð½Ñ‹Ñ… ÑÐ»Ð¾ÐµÐ² Ð² ÑÐ¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ñ… Ð½ÐµÐ¹Ñ€Ð¾Ð½Ð½Ñ‹Ñ… ÑÐµÑ‚ÑÑ…."
                },
                "en": {
                    "title": "Transformers Thrive Without Normalization: Introducing Dynamic Tanh",
                    "desc": "This paper explores the role of normalization layers in Transformers, showing that they may not be as essential as previously thought. The authors introduce Dynamic Tanh (DyT), a simple element-wise operation that can replace normalization layers while maintaining or improving performance. DyT is based on the observation that layer normalization often results in S-shaped mappings similar to the tanh function. The study demonstrates that Transformers using DyT can perform well across various tasks, suggesting a reevaluation of the necessity of normalization in deep learning architectures."
                },
                "zh": {
                    "title": "åŠ¨æ€åŒæ›²æ­£åˆ‡ï¼šè¶…è¶Šå½’ä¸€åŒ–çš„å˜æ¢å™¨",
                    "desc": "æœ¬ç ”ç©¶å±•ç¤ºäº†åœ¨çŽ°ä»£ç¥žç»ç½‘ç»œä¸­ï¼Œå½’ä¸€åŒ–å±‚å¹¶éžå¿…ä¸å¯å°‘ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºåŠ¨æ€åŒæ›²æ­£åˆ‡ï¼ˆDynamic Tanh, DyTï¼‰çš„ç®€å•æŠ€æœ¯ï¼Œå¯ä»¥æ›¿ä»£å˜æ¢å™¨ä¸­çš„å½’ä¸€åŒ–å±‚ã€‚DyTé€šè¿‡è§‚å¯Ÿå˜æ¢å™¨ä¸­çš„å±‚å½’ä¸€åŒ–é€šå¸¸äº§ç”Ÿç±»ä¼¼tanhçš„Så½¢è¾“å…¥è¾“å‡ºæ˜ å°„è€Œå¾—å‡ºã€‚é€šè¿‡å¼•å…¥DyTï¼Œæœªä½¿ç”¨å½’ä¸€åŒ–çš„å˜æ¢å™¨å¯ä»¥åœ¨å¤šç§ä»»åŠ¡ä¸­è¾¾åˆ°æˆ–è¶…è¿‡ä½¿ç”¨å½’ä¸€åŒ–çš„å˜æ¢å™¨çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.10480",
            "title": "World Modeling Makes a Better Planner: Dual Preference Optimization for\n  Embodied Task Planning",
            "url": "https://huggingface.co/papers/2503.10480",
            "abstract": "Recent advances in large vision-language models (LVLMs) have shown promise for embodied task planning, yet they struggle with fundamental challenges like dependency constraints and efficiency. Existing approaches either solely optimize action selection or leverage world models during inference, overlooking the benefits of learning to model the world as a way to enhance planning capabilities. We propose Dual Preference Optimization (D^2PO), a new learning framework that jointly optimizes state prediction and action selection through preference learning, enabling LVLMs to understand environment dynamics for better planning. To automatically collect trajectories and stepwise preference data without human annotation, we introduce a tree search mechanism for extensive exploration via trial-and-error. Extensive experiments on VoTa-Bench demonstrate that our D^2PO-based method significantly outperforms existing methods and GPT-4o when applied to Qwen2-VL (7B), LLaVA-1.6 (7B), and LLaMA-3.2 (11B), achieving superior task success rates with more efficient execution paths.",
            "score": 29,
            "issue_id": 2700,
            "pub_date": "2025-03-13",
            "pub_date_card": {
                "ru": "13 Ð¼Ð°Ñ€Ñ‚Ð°",
                "en": "March 13",
                "zh": "3æœˆ13æ—¥"
            },
            "hash": "2d3e6b8c84c51e69",
            "authors": [
                "Siyin Wang",
                "Zhaoye Fei",
                "Qinyuan Cheng",
                "Shiduo Zhang",
                "Panpan Cai",
                "Jinlan Fu",
                "Xipeng Qiu"
            ],
            "affiliations": [
                "Fudan University",
                "National University of Singapore",
                "Shanghai Innovation Institute",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.10480.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#multimodal",
                    "#agents",
                    "#optimization",
                    "#rl",
                    "#training"
                ],
                "emoji": "ðŸ¤–",
                "ru": {
                    "title": "Ð£Ð»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ðµ Ð¿Ð»Ð°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ð¹ Ñƒ LVLM Ñ‡ÐµÑ€ÐµÐ· ÑÐ¾Ð²Ð¼ÐµÑÑ‚Ð½ÑƒÑŽ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸ÑŽ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ð¹ Ð¸ Ð²Ñ‹Ð±Ð¾Ñ€Ð°",
                    "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð½Ð¾Ð²Ñ‹Ð¹ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ Ðº Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸ÑŽ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… Ð²Ð¸Ð·ÑƒÐ°Ð»ÑŒÐ½Ð¾-ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ (LVLM) Ð´Ð»Ñ Ð¿Ð»Ð°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ð¹ Ð² Ð²Ð¾Ð¿Ð»Ð¾Ñ‰ÐµÐ½Ð½Ñ‹Ñ… Ð·Ð°Ð´Ð°Ñ‡Ð°Ñ…. ÐŸÑ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð½Ñ‹Ð¹ Ð¼ÐµÑ‚Ð¾Ð´ Dual Preference Optimization (D^2PO) ÑÐ¾Ð²Ð¼ÐµÑÑ‚Ð½Ð¾ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€ÑƒÐµÑ‚ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ðµ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ð¹ Ð¸ Ð²Ñ‹Ð±Ð¾Ñ€ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ð¹ Ñ‡ÐµÑ€ÐµÐ· Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¿Ñ€ÐµÐ´Ð¿Ð¾Ñ‡Ñ‚ÐµÐ½Ð¸ÑÐ¼. Ð”Ð»Ñ ÑÐ±Ð¾Ñ€Ð° Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð±ÐµÐ· ÑƒÑ‡Ð°ÑÑ‚Ð¸Ñ Ñ‡ÐµÐ»Ð¾Ð²ÐµÐºÐ° Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ Ð¼ÐµÑ…Ð°Ð½Ð¸Ð·Ð¼ Ð¿Ð¾Ð¸ÑÐºÐ° Ð¿Ð¾ Ð´ÐµÑ€ÐµÐ²Ñƒ. Ð­ÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ñ‹ Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÑŽÑ‚, Ñ‡Ñ‚Ð¾ D^2PO Ð·Ð½Ð°Ñ‡Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ð¿Ñ€ÐµÐ²Ð¾ÑÑ…Ð¾Ð´Ð¸Ñ‚ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ðµ Ð¼ÐµÑ‚Ð¾Ð´Ñ‹ Ð¿Ð¾ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ÑÑ‚Ð¸ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ Ð·Ð°Ð´Ð°Ñ‡ Ð¸ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾ÑÑ‚Ð¸ Ñ‚Ñ€Ð°ÐµÐºÑ‚Ð¾Ñ€Ð¸Ð¹."
                },
                "en": {
                    "title": "Enhancing Planning in LVLMs with Dual Preference Optimization",
                    "desc": "This paper introduces Dual Preference Optimization (D^2PO), a novel framework designed to improve the planning capabilities of large vision-language models (LVLMs) by jointly optimizing state prediction and action selection. The approach addresses key challenges in embodied task planning, such as dependency constraints and efficiency, which have been inadequately handled by existing methods. By employing a tree search mechanism, D^2PO enables the automatic collection of trajectories and preference data, facilitating extensive exploration through trial-and-error without the need for human annotation. Experimental results on VoTa-Bench show that D^2PO significantly enhances task success rates and execution efficiency compared to previous methods and GPT-4o across various LVLMs."
                },
                "zh": {
                    "title": "åŒé‡åå¥½ä¼˜åŒ–ï¼šæå‡ä»»åŠ¡è§„åˆ’èƒ½åŠ›çš„å…³é”®",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å­¦ä¹ æ¡†æž¶ï¼Œç§°ä¸ºåŒé‡åå¥½ä¼˜åŒ–ï¼ˆD^2POï¼‰ï¼Œæ—¨åœ¨æé«˜å¤§åž‹è§†è§‰è¯­è¨€æ¨¡åž‹ï¼ˆLVLMsï¼‰åœ¨ä»»åŠ¡è§„åˆ’ä¸­çš„èƒ½åŠ›ã€‚è¯¥æ¡†æž¶é€šè¿‡åå¥½å­¦ä¹ åŒæ—¶ä¼˜åŒ–çŠ¶æ€é¢„æµ‹å’ŒåŠ¨ä½œé€‰æ‹©ï¼Œä½¿æ¨¡åž‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£çŽ¯å¢ƒåŠ¨æ€ã€‚ä¸ºäº†è‡ªåŠ¨æ”¶é›†è½¨è¿¹å’Œé€æ­¥åå¥½æ•°æ®ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ç§æ ‘æœç´¢æœºåˆ¶ï¼Œä»¥ä¾¿é€šè¿‡è¯•é”™è¿›è¡Œå¹¿æ³›æŽ¢ç´¢ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒåŸºäºŽD^2POçš„æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºŽçŽ°æœ‰æ–¹æ³•å’ŒGPT-4oï¼Œè¾¾åˆ°äº†æ›´é«˜çš„ä»»åŠ¡æˆåŠŸçŽ‡å’Œæ›´é«˜æ•ˆçš„æ‰§è¡Œè·¯å¾„ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.10633",
            "title": "Charting and Navigating Hugging Face's Model Atlas",
            "url": "https://huggingface.co/papers/2503.10633",
            "abstract": "As there are now millions of publicly available neural networks, searching and analyzing large model repositories becomes increasingly important. Navigating so many models requires an atlas, but as most models are poorly documented charting such an atlas is challenging. To explore the hidden potential of model repositories, we chart a preliminary atlas representing the documented fraction of Hugging Face. It provides stunning visualizations of the model landscape and evolution. We demonstrate several applications of this atlas including predicting model attributes (e.g., accuracy), and analyzing trends in computer vision models. However, as the current atlas remains incomplete, we propose a method for charting undocumented regions. Specifically, we identify high-confidence structural priors based on dominant real-world model training practices. Leveraging these priors, our approach enables accurate mapping of previously undocumented areas of the atlas. We publicly release our datasets, code, and interactive atlas.",
            "score": 28,
            "issue_id": 2703,
            "pub_date": "2025-03-13",
            "pub_date_card": {
                "ru": "13 Ð¼Ð°Ñ€Ñ‚Ð°",
                "en": "March 13",
                "zh": "3æœˆ13æ—¥"
            },
            "hash": "128e9e97a54b8404",
            "authors": [
                "Eliahu Horwitz",
                "Nitzan Kurer",
                "Jonathan Kahana",
                "Liel Amar",
                "Yedid Hoshen"
            ],
            "affiliations": [
                "School of Computer Science and Engineering The Hebrew University of Jerusalem, Israel"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.10633.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#benchmark",
                    "#cv",
                    "#data",
                    "#survey",
                    "#dataset"
                ],
                "emoji": "ðŸ—ºï¸",
                "ru": {
                    "title": "Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð°Ñ‚Ð»Ð°ÑÐ° Ð½ÐµÐ¹Ñ€Ð¾ÑÐµÑ‚ÐµÐ¹: Ð½Ð°Ð²Ð¸Ð³Ð°Ñ†Ð¸Ñ Ð² Ð¾ÐºÐµÐ°Ð½Ðµ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð¼Ð°ÑˆÐ¸Ð½Ð½Ð¾Ð³Ð¾ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ",
                    "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¾Ð¿Ð¸ÑÑ‹Ð²Ð°ÐµÑ‚ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð°Ñ‚Ð»Ð°ÑÐ° Ð´Ð»Ñ Ð½Ð°Ð²Ð¸Ð³Ð°Ñ†Ð¸Ð¸ Ð¿Ð¾ Ð¼Ð¸Ð»Ð»Ð¸Ð¾Ð½Ð°Ð¼ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ñ… Ð½ÐµÐ¹Ñ€Ð¾Ð½Ð½Ñ‹Ñ… ÑÐµÑ‚ÐµÐ¹. ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ Ð²Ð¸Ð·ÑƒÐ°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÑŽÑ‚ Ð»Ð°Ð½Ð´ÑˆÐ°Ñ„Ñ‚ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ð¹ Ñ‡Ð°ÑÑ‚Ð¸ Ñ€ÐµÐ¿Ð¾Ð·Ð¸Ñ‚Ð¾Ñ€Ð¸Ñ Hugging Face. ÐžÐ½Ð¸ Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð¸Ñ€ÑƒÑŽÑ‚ Ð¿Ñ€Ð¸Ð¼ÐµÐ½ÐµÐ½Ð¸Ðµ Ð°Ñ‚Ð»Ð°ÑÐ° Ð´Ð»Ñ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ñ Ð°Ñ‚Ñ€Ð¸Ð±ÑƒÑ‚Ð¾Ð² Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð¸ Ð°Ð½Ð°Ð»Ð¸Ð·Ð° Ñ‚Ñ€ÐµÐ½Ð´Ð¾Ð² Ð² ÐºÐ¾Ð¼Ð¿ÑŒÑŽÑ‚ÐµÑ€Ð½Ð¾Ð¼ Ð·Ñ€ÐµÐ½Ð¸Ð¸. ÐŸÑ€ÐµÐ´Ð»Ð°Ð³Ð°ÐµÑ‚ÑÑ Ð¼ÐµÑ‚Ð¾Ð´ Ð´Ð»Ñ ÐºÐ°Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð½ÐµÐ´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ñ… Ð¾Ð±Ð»Ð°ÑÑ‚ÐµÐ¹ Ð°Ñ‚Ð»Ð°ÑÐ° Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð½Ñ‹Ñ… Ð°Ð¿Ñ€Ð¸Ð¾Ñ€Ð½Ñ‹Ñ… Ð·Ð½Ð°Ð½Ð¸Ð¹."
                },
                "en": {
                    "title": "Mapping the Neural Network Landscape: An Interactive Atlas",
                    "desc": "This paper addresses the challenge of navigating the vast number of publicly available neural networks by creating a visual atlas of documented models from Hugging Face. The atlas not only visualizes the model landscape but also tracks the evolution of these models over time. It includes applications such as predicting model attributes like accuracy and analyzing trends in computer vision. To improve the atlas, the authors propose a method to chart undocumented regions by using high-confidence structural priors based on common training practices."
                },
                "zh": {
                    "title": "æŽ¢ç´¢ç¥žç»ç½‘ç»œæ¨¡åž‹çš„åœ°å›¾",
                    "desc": "éšç€å…¬å¼€ç¥žç»ç½‘ç»œæ•°é‡çš„æ¿€å¢žï¼Œæœç´¢å’Œåˆ†æžå¤§åž‹æ¨¡åž‹åº“å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚ç”±äºŽå¤§å¤šæ•°æ¨¡åž‹æ–‡æ¡£ä¸å…¨ï¼Œç»˜åˆ¶æ¨¡åž‹çš„åœ°å›¾å˜å¾—å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬ç»˜åˆ¶äº†ä¸€ä¸ªåˆæ­¥çš„åœ°å›¾ï¼Œå±•ç¤ºäº†Hugging Faceä¸Šå·²è®°å½•æ¨¡åž‹çš„åˆ†å¸ƒå’Œæ¼”å˜ï¼Œå¹¶å±•ç¤ºäº†å¤šç§åº”ç”¨ï¼ŒåŒ…æ‹¬é¢„æµ‹æ¨¡åž‹å±žæ€§å’Œåˆ†æžè®¡ç®—æœºè§†è§‰æ¨¡åž‹çš„è¶‹åŠ¿ã€‚ä¸ºäº†å¡«è¡¥å½“å‰åœ°å›¾çš„ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œé€šè¿‡è¯†åˆ«åŸºäºŽä¸»æµè®­ç»ƒå®žè·µçš„é«˜ç½®ä¿¡åº¦ç»“æž„å…ˆéªŒï¼Œå‡†ç¡®ç»˜åˆ¶æœªè®°å½•åŒºåŸŸã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.09669",
            "title": "Silent Branding Attack: Trigger-free Data Poisoning Attack on\n  Text-to-Image Diffusion Models",
            "url": "https://huggingface.co/papers/2503.09669",
            "abstract": "Text-to-image diffusion models have achieved remarkable success in generating high-quality contents from text prompts. However, their reliance on publicly available data and the growing trend of data sharing for fine-tuning make these models particularly vulnerable to data poisoning attacks. In this work, we introduce the Silent Branding Attack, a novel data poisoning method that manipulates text-to-image diffusion models to generate images containing specific brand logos or symbols without any text triggers. We find that when certain visual patterns are repeatedly in the training data, the model learns to reproduce them naturally in its outputs, even without prompt mentions. Leveraging this, we develop an automated data poisoning algorithm that unobtrusively injects logos into original images, ensuring they blend naturally and remain undetected. Models trained on this poisoned dataset generate images containing logos without degrading image quality or text alignment. We experimentally validate our silent branding attack across two realistic settings on large-scale high-quality image datasets and style personalization datasets, achieving high success rates even without a specific text trigger. Human evaluation and quantitative metrics including logo detection show that our method can stealthily embed logos.",
            "score": 28,
            "issue_id": 2701,
            "pub_date": "2025-03-12",
            "pub_date_card": {
                "ru": "12 Ð¼Ð°Ñ€Ñ‚Ð°",
                "en": "March 12",
                "zh": "3æœˆ12æ—¥"
            },
            "hash": "f54f510a4cf48d22",
            "authors": [
                "Sangwon Jang",
                "June Suk Choi",
                "Jaehyeong Jo",
                "Kimin Lee",
                "Sung Ju Hwang"
            ],
            "affiliations": [
                "DeepAuto.ai",
                "KAIST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.09669.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#security",
                    "#diffusion",
                    "#data"
                ],
                "emoji": "ðŸ•µï¸",
                "ru": {
                    "title": "ÐÐµÐ²Ð¸Ð´Ð¸Ð¼Ñ‹Ðµ Ð»Ð¾Ð³Ð¾Ñ‚Ð¸Ð¿Ñ‹: ÑÐºÑ€Ñ‹Ñ‚Ð¾Ðµ Ð²Ð½ÐµÐ´Ñ€ÐµÐ½Ð¸Ðµ Ð±Ñ€ÐµÐ½Ð´Ð¾Ð² Ð² Ð³ÐµÐ½ÐµÑ€Ð°Ñ‚Ð¸Ð²Ð½Ñ‹Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸",
                    "desc": "Ð­Ñ‚Ð¾ Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð½Ð¾Ð²Ñ‹Ð¹ Ð¼ÐµÑ‚Ð¾Ð´ Ð°Ñ‚Ð°ÐºÐ¸ Ð½Ð° Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ð¸ Ñ‚ÐµÐºÑÑ‚-Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ Ð¿ÑƒÑ‚ÐµÐ¼ Ð¾Ñ‚Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ñ Ð´Ð°Ð½Ð½Ñ‹Ñ…. ÐœÐµÑ‚Ð¾Ð´, Ð½Ð°Ð·Ð²Ð°Ð½Ð½Ñ‹Ð¹ 'Ð¢Ð¸Ñ…Ð°Ñ Ð±Ñ€ÐµÐ½Ð´Ð¸Ð½Ð³Ð¾Ð²Ð°Ñ Ð°Ñ‚Ð°ÐºÐ°', Ð·Ð°ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ Ñ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð½Ñ‹Ð¼Ð¸ Ð»Ð¾Ð³Ð¾Ñ‚Ð¸Ð¿Ð°Ð¼Ð¸ Ð±ÐµÐ· ÑÐ²Ð½Ñ‹Ñ… Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ñ… Ñ‚Ñ€Ð¸Ð³Ð³ÐµÑ€Ð¾Ð². ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð»Ð¸ Ð°Ð»Ð³Ð¾Ñ€Ð¸Ñ‚Ð¼, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð½ÐµÐ·Ð°Ð¼ÐµÑ‚Ð½Ð¾ Ð²Ð½ÐµÐ´Ñ€ÑÐµÑ‚ Ð»Ð¾Ð³Ð¾Ñ‚Ð¸Ð¿Ñ‹ Ð² Ð¾Ñ€Ð¸Ð³Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ Ð´Ð»Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸. Ð­ÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ñ‹ Ð¿Ð¾ÐºÐ°Ð·Ð°Ð»Ð¸ Ð²Ñ‹ÑÐ¾ÐºÑƒÑŽ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾ÑÑ‚ÑŒ Ð¼ÐµÑ‚Ð¾Ð´Ð° Ð½Ð° ÐºÑ€ÑƒÐ¿Ð½Ð¾Ð¼Ð°ÑÑˆÑ‚Ð°Ð±Ð½Ñ‹Ñ… Ð½Ð°Ð±Ð¾Ñ€Ð°Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…, Ñ‡Ñ‚Ð¾ Ð¿Ð¾Ð´Ñ‚Ð²ÐµÑ€Ð¶Ð´ÐµÐ½Ð¾ ÐºÐ°Ðº Ñ‡ÐµÐ»Ð¾Ð²ÐµÑ‡ÐµÑÐºÐ¾Ð¹ Ð¾Ñ†ÐµÐ½ÐºÐ¾Ð¹, Ñ‚Ð°Ðº Ð¸ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ð¼Ð¸ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ°Ð¼Ð¸."
                },
                "en": {
                    "title": "Stealthy Logo Injection in Image Generation",
                    "desc": "This paper presents a new method called the Silent Branding Attack, which targets text-to-image diffusion models by subtly injecting brand logos into training data. The attack exploits the model's tendency to reproduce visual patterns it has seen, allowing logos to appear in generated images without any explicit text prompts. The authors developed an automated algorithm that seamlessly integrates these logos into original images, making them difficult to detect. Their experiments demonstrate that models trained on this manipulated data can produce high-quality images containing logos, achieving significant success rates in various settings."
                },
                "zh": {
                    "title": "é™é»˜å“ç‰Œæ”»å‡»ï¼šéšç§˜æ¤å…¥å“ç‰Œæ ‡å¿—çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡åž‹åœ¨æ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆé«˜è´¨é‡å†…å®¹æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆåŠŸã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡åž‹ä¾èµ–äºŽå…¬å¼€æ•°æ®ï¼Œå¹¶ä¸”æ•°æ®å…±äº«çš„è¶‹åŠ¿ä½¿å…¶ç‰¹åˆ«å®¹æ˜“å—åˆ°æ•°æ®ä¸­æ¯’æ”»å‡»ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®ä¸­æ¯’æ–¹æ³•â€”â€”é™é»˜å“ç‰Œæ”»å‡»ï¼Œèƒ½å¤Ÿæ“æŽ§æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡åž‹ç”ŸæˆåŒ…å«ç‰¹å®šå“ç‰Œæ ‡å¿—æˆ–ç¬¦å·çš„å›¾åƒï¼Œè€Œæ— éœ€ä»»ä½•æ–‡æœ¬è§¦å‘ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§è‡ªåŠ¨åŒ–çš„æ•°æ®ä¸­æ¯’ç®—æ³•ï¼Œå¯ä»¥åœ¨åŽŸå§‹å›¾åƒä¸­æ‚„æ— å£°æ¯åœ°æ³¨å…¥æ ‡å¿—ï¼Œç¡®ä¿å®ƒä»¬è‡ªç„¶èžåˆä¸”ä¸è¢«æ£€æµ‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.09662",
            "title": "CoRe^2: Collect, Reflect and Refine to Generate Better and Faster",
            "url": "https://huggingface.co/papers/2503.09662",
            "abstract": "Making text-to-image (T2I) generative model sample both fast and well represents a promising research direction. Previous studies have typically focused on either enhancing the visual quality of synthesized images at the expense of sampling efficiency or dramatically accelerating sampling without improving the base model's generative capacity. Moreover, nearly all inference methods have not been able to ensure stable performance simultaneously on both diffusion models (DMs) and visual autoregressive models (ARMs). In this paper, we introduce a novel plug-and-play inference paradigm, CoRe^2, which comprises three subprocesses: Collect, Reflect, and Refine. CoRe^2 first collects classifier-free guidance (CFG) trajectories, and then use collected data to train a weak model that reflects the easy-to-learn contents while reducing number of function evaluations during inference by half. Subsequently, CoRe^2 employs weak-to-strong guidance to refine the conditional output, thereby improving the model's capacity to generate high-frequency and realistic content, which is difficult for the base model to capture. To the best of our knowledge, CoRe^2 is the first to demonstrate both efficiency and effectiveness across a wide range of DMs, including SDXL, SD3.5, and FLUX, as well as ARMs like LlamaGen. It has exhibited significant performance improvements on HPD v2, Pick-of-Pic, Drawbench, GenEval, and T2I-Compbench. Furthermore, CoRe^2 can be seamlessly integrated with the state-of-the-art Z-Sampling, outperforming it by 0.3 and 0.16 on PickScore and AES, while achieving 5.64s time saving using SD3.5.Code is released at https://github.com/xie-lab-ml/CoRe/tree/main.",
            "score": 25,
            "issue_id": 2705,
            "pub_date": "2025-03-12",
            "pub_date_card": {
                "ru": "12 Ð¼Ð°Ñ€Ñ‚Ð°",
                "en": "March 12",
                "zh": "3æœˆ12æ—¥"
            },
            "hash": "a6fdb00ac6a8ebca",
            "authors": [
                "Shitong Shao",
                "Zikai Zhou",
                "Dian Xie",
                "Yuetong Fang",
                "Tian Ye",
                "Lichen Bai",
                "Zeke Xie"
            ],
            "affiliations": [
                "The Hong Kong University of Science and Technology (GuangZhou)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.09662.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#benchmark",
                    "#inference",
                    "#diffusion"
                ],
                "emoji": "ðŸŽ¨",
                "ru": {
                    "title": "CoRe^2: Ð ÐµÐ²Ð¾Ð»ÑŽÑ†Ð¸Ð¾Ð½Ð½Ñ‹Ð¹ Ð¼ÐµÑ‚Ð¾Ð´ Ð´Ð»Ñ Ð±Ñ‹ÑÑ‚Ñ€Ð¾Ð¹ Ð¸ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²ÐµÐ½Ð½Ð¾Ð¹ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹ Ð¿Ð¾ Ñ‚ÐµÐºÑÑ‚Ñƒ",
                    "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð½Ð¾Ð²Ñ‹Ð¹ Ð¼ÐµÑ‚Ð¾Ð´ CoRe^2 Ð´Ð»Ñ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ‚Ð¸Ð²Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ñ‚ÐµÐºÑÑ‚-Ð²-Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ. ÐœÐµÑ‚Ð¾Ð´ ÑÐ¾ÑÑ‚Ð¾Ð¸Ñ‚ Ð¸Ð· Ñ‚Ñ€ÐµÑ… ÑÑ‚Ð°Ð¿Ð¾Ð²: ÑÐ±Ð¾Ñ€ Ñ‚Ñ€Ð°ÐµÐºÑ‚Ð¾Ñ€Ð¸Ð¹ Ð±ÐµÐ·ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ð¾Ð½Ð½Ð¾Ð³Ð¾ Ñ€ÑƒÐºÐ¾Ð²Ð¾Ð´ÑÑ‚Ð²Ð°, Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ ÑÐ»Ð°Ð±Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð¸ ÑƒÑ‚Ð¾Ñ‡Ð½ÐµÐ½Ð¸Ðµ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð² Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ Ñ€ÑƒÐºÐ¾Ð²Ð¾Ð´ÑÑ‚Ð²Ð° Ð¾Ñ‚ ÑÐ»Ð°Ð±Ð¾Ð¹ Ðº ÑÐ¸Ð»ÑŒÐ½Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸. CoRe^2 Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÑ‚ Ð²Ñ‹ÑÐ¾ÐºÑƒÑŽ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾ÑÑ‚ÑŒ Ð¸ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð¾ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð² Ð´Ð»Ñ Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ñ‹Ñ… Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ð¾Ð½Ð½Ñ‹Ñ… Ð¸ Ð°Ð²Ñ‚Ð¾Ñ€ÐµÐ³Ñ€ÐµÑÑÐ¸Ð¾Ð½Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹. ÐœÐµÑ‚Ð¾Ð´ Ð¿Ñ€ÐµÐ²Ð¾ÑÑ…Ð¾Ð´Ð¸Ñ‚ ÑÐ¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ Ð¿Ð¾Ð´Ñ…Ð¾Ð´Ñ‹ Ð¿Ð¾ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ°Ð¼ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð° Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹ Ð¸ ÑÐºÐ¾Ñ€Ð¾ÑÑ‚Ð¸ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸."
                },
                "en": {
                    "title": "CoRe^2: Fast and High-Quality Text-to-Image Generation",
                    "desc": "This paper presents CoRe^2, a new method for improving the speed and quality of text-to-image (T2I) generative models. It introduces a three-step process: Collect, Reflect, and Refine, which enhances sampling efficiency while maintaining high visual fidelity. By using classifier-free guidance trajectories, CoRe^2 trains a weak model that simplifies the learning process and reduces function evaluations during inference. The method shows significant performance gains across various diffusion models and autoregressive models, making it a notable advancement in the field of generative modeling."
                },
                "zh": {
                    "title": "é«˜æ•ˆç”Ÿæˆï¼ŒçœŸå®žå›¾åƒï¼",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡åž‹æŽ¨ç†èŒƒå¼ï¼Œç§°ä¸ºCoRe^2ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸‰ä¸ªå­è¿‡ç¨‹ï¼šæ”¶é›†ã€åå°„å’Œç²¾ç‚¼ï¼Œæ¥æé«˜ç”Ÿæˆæ•ˆçŽ‡å’Œæ•ˆæžœã€‚CoRe^2é¦–å…ˆæ”¶é›†æ— åˆ†ç±»å™¨å¼•å¯¼è½¨è¿¹ï¼Œç„¶åŽè®­ç»ƒä¸€ä¸ªå¼±æ¨¡åž‹ä»¥å‡å°‘æŽ¨ç†è¿‡ç¨‹ä¸­çš„å‡½æ•°è¯„ä¼°æ¬¡æ•°ã€‚æœ€åŽï¼Œé€šè¿‡å¼±åˆ°å¼ºçš„å¼•å¯¼æ¥ç²¾ç‚¼æ¡ä»¶è¾“å‡ºï¼Œä»Žè€Œç”Ÿæˆæ›´é«˜é¢‘çŽ‡å’Œæ›´çœŸå®žçš„å†…å®¹ï¼Œæ˜¾è‘—æå‡äº†å¤šç§æ‰©æ•£æ¨¡åž‹å’Œè‡ªå›žå½’æ¨¡åž‹çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.10639",
            "title": "GoT: Unleashing Reasoning Capability of Multimodal Large Language Model\n  for Visual Generation and Editing",
            "url": "https://huggingface.co/papers/2503.10639",
            "abstract": "Current image generation and editing methods primarily process textual prompts as direct inputs without reasoning about visual composition and explicit operations. We present Generation Chain-of-Thought (GoT), a novel paradigm that enables generation and editing through an explicit language reasoning process before outputting images. This approach transforms conventional text-to-image generation and editing into a reasoning-guided framework that analyzes semantic relationships and spatial arrangements. We define the formulation of GoT and construct large-scale GoT datasets containing over 9M samples with detailed reasoning chains capturing semantic-spatial relationships. To leverage the advantages of GoT, we implement a unified framework that integrates Qwen2.5-VL for reasoning chain generation with an end-to-end diffusion model enhanced by our novel Semantic-Spatial Guidance Module. Experiments show our GoT framework achieves excellent performance on both generation and editing tasks, with significant improvements over baselines. Additionally, our approach enables interactive visual generation, allowing users to explicitly modify reasoning steps for precise image adjustments. GoT pioneers a new direction for reasoning-driven visual generation and editing, producing images that better align with human intent. To facilitate future research, we make our datasets, code, and pretrained models publicly available at https://github.com/rongyaofang/GoT.",
            "score": 22,
            "issue_id": 2701,
            "pub_date": "2025-03-13",
            "pub_date_card": {
                "ru": "13 Ð¼Ð°Ñ€Ñ‚Ð°",
                "en": "March 13",
                "zh": "3æœˆ13æ—¥"
            },
            "hash": "70c9d741eedd181c",
            "authors": [
                "Rongyao Fang",
                "Chengqi Duan",
                "Kun Wang",
                "Linjiang Huang",
                "Hao Li",
                "Shilin Yan",
                "Hao Tian",
                "Xingyu Zeng",
                "Rui Zhao",
                "Jifeng Dai",
                "Xihui Liu",
                "Hongsheng Li"
            ],
            "affiliations": [
                "BUAA",
                "CUHK MMLab",
                "HKU",
                "SenseTime",
                "Shanghai AI Laboratory",
                "THU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.10639.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#cv",
                    "#dataset",
                    "#reasoning",
                    "#diffusion",
                    "#open_source"
                ],
                "emoji": "ðŸ§ ",
                "ru": {
                    "title": "Ð Ð°Ð·ÑƒÐ¼Ð½Ð°Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹: Ð¾Ñ‚ Ñ‚ÐµÐºÑÑ‚Ð° Ðº Ð²Ð¸Ð·ÑƒÐ°Ð»ÑŒÐ½Ð¾Ð¼Ñƒ Ð¼Ñ‹ÑˆÐ»ÐµÐ½Ð¸ÑŽ",
                    "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð½Ð¾Ð²ÑƒÑŽ Ð¿Ð°Ñ€Ð°Ð´Ð¸Ð³Ð¼Ñƒ Generation Chain-of-Thought (GoT) Ð´Ð»Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¸ Ñ€ÐµÐ´Ð°ÐºÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹. GoT Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ ÑÐ²Ð½Ñ‹Ð¹ Ð¿Ñ€Ð¾Ñ†ÐµÑÑ ÑÐ·Ñ‹ÐºÐ¾Ð²Ð¾Ð³Ð¾ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ñ Ð¿ÐµÑ€ÐµÐ´ Ð²Ñ‹Ð²Ð¾Ð´Ð¾Ð¼ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹, Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÑ ÑÐµÐ¼Ð°Ð½Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ð¾Ñ‚Ð½Ð¾ÑˆÐµÐ½Ð¸Ñ Ð¸ Ð¿Ñ€Ð¾ÑÑ‚Ñ€Ð°Ð½ÑÑ‚Ð²ÐµÐ½Ð½Ð¾Ðµ Ñ€Ð°ÑÐ¿Ð¾Ð»Ð¾Ð¶ÐµÐ½Ð¸Ðµ. ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ ÑÐ¾Ð·Ð´Ð°Ð»Ð¸ Ð¼Ð°ÑÑˆÑ‚Ð°Ð±Ð½Ñ‹Ðµ Ð½Ð°Ð±Ð¾Ñ€Ñ‹ Ð´Ð°Ð½Ð½Ñ‹Ñ… GoT Ð¸ Ñ€ÐµÐ°Ð»Ð¸Ð·Ð¾Ð²Ð°Ð»Ð¸ ÑƒÐ½Ð¸Ñ„Ð¸Ñ†Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½ÑƒÑŽ Ð°Ñ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ñƒ, Ð¾Ð±ÑŠÐµÐ´Ð¸Ð½ÑÑŽÑ‰ÑƒÑŽ Qwen2.5-VL Ð´Ð»Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ñ†ÐµÐ¿Ð¾Ñ‡ÐµÐº Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹ Ñ Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ð¾Ð½Ð½Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»ÑŒÑŽ. Ð­ÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ñ‹ Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÑŽÑ‚ Ð·Ð½Ð°Ñ‡Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ðµ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ðµ Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸ Ð¿Ð¾ ÑÑ€Ð°Ð²Ð½ÐµÐ½Ð¸ÑŽ Ñ Ð±Ð°Ð·Ð¾Ð²Ñ‹Ð¼Ð¸ Ð¼ÐµÑ‚Ð¾Ð´Ð°Ð¼Ð¸ Ð¸ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾ÑÑ‚ÑŒ Ð¸Ð½Ñ‚ÐµÑ€Ð°ÐºÑ‚Ð¸Ð²Ð½Ð¾Ð¹ Ð²Ð¸Ð·ÑƒÐ°Ð»ÑŒÐ½Ð¾Ð¹ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸."
                },
                "en": {
                    "title": "Revolutionizing Image Generation with Reasoning!",
                    "desc": "The paper introduces Generation Chain-of-Thought (GoT), a new method for image generation and editing that incorporates reasoning about visual composition. Instead of simply processing text prompts, GoT uses a reasoning-guided framework to analyze semantic relationships and spatial arrangements before generating images. It includes a large dataset with over 9 million samples that capture detailed reasoning chains, enhancing the understanding of how different elements relate visually. The results show that GoT significantly improves the quality of generated and edited images, allowing for interactive adjustments based on user-defined reasoning steps."
                },
                "zh": {
                    "title": "æŽ¨ç†é©±åŠ¨çš„å›¾åƒç”Ÿæˆä¸Žç¼–è¾‘æ–°æ–¹å‘",
                    "desc": "å½“å‰çš„å›¾åƒç”Ÿæˆå’Œç¼–è¾‘æ–¹æ³•ä¸»è¦å°†æ–‡æœ¬æç¤ºä½œä¸ºç›´æŽ¥è¾“å…¥ï¼Œè€Œæ²¡æœ‰è€ƒè™‘è§†è§‰æž„å›¾å’Œæ˜Žç¡®çš„æ“ä½œã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç”Ÿæˆæ€ç»´é“¾ï¼ˆGoTï¼‰èŒƒå¼ï¼Œé€šè¿‡åœ¨è¾“å‡ºå›¾åƒä¹‹å‰è¿›è¡Œæ˜Žç¡®çš„è¯­è¨€æŽ¨ç†è¿‡ç¨‹æ¥å®žçŽ°ç”Ÿæˆå’Œç¼–è¾‘ã€‚è¯¥æ–¹æ³•å°†ä¼ ç»Ÿçš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’Œç¼–è¾‘è½¬å˜ä¸ºä¸€ä¸ªåŸºäºŽæŽ¨ç†çš„æ¡†æž¶ï¼Œåˆ†æžè¯­ä¹‰å…³ç³»å’Œç©ºé—´æŽ’åˆ—ã€‚æˆ‘ä»¬çš„GoTæ¡†æž¶åœ¨ç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ä¸Šè¡¨çŽ°å‡ºè‰²ï¼Œæ˜¾è‘—ä¼˜äºŽåŸºçº¿ï¼Œå¹¶å…è®¸ç”¨æˆ·é€šè¿‡æ˜Žç¡®ä¿®æ”¹æŽ¨ç†æ­¥éª¤æ¥è¿›è¡Œäº¤äº’å¼è§†è§‰ç”Ÿæˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.10291",
            "title": "VisualPRM: An Effective Process Reward Model for Multimodal Reasoning",
            "url": "https://huggingface.co/papers/2503.10291",
            "abstract": "We introduce VisualPRM, an advanced multimodal Process Reward Model (PRM) with 8B parameters, which improves the reasoning abilities of existing Multimodal Large Language Models (MLLMs) across different model scales and families with Best-of-N (BoN) evaluation strategies. Specifically, our model improves the reasoning performance of three types of MLLMs and four different model scales. Even when applied to the highly capable InternVL2.5-78B, it achieves a 5.9-point improvement across seven multimodal reasoning benchmarks. Experimental results show that our model exhibits superior performance compared to Outcome Reward Models and Self-Consistency during BoN evaluation. To facilitate the training of multimodal PRMs, we construct a multimodal process supervision dataset VisualPRM400K using an automated data pipeline. For the evaluation of multimodal PRMs, we propose VisualProcessBench, a benchmark with human-annotated step-wise correctness labels, to measure the abilities of PRMs to detect erroneous steps in multimodal reasoning tasks. We hope that our work can inspire more future research and contribute to the development of MLLMs. Our model, data, and benchmark are released in https://internvl.github.io/blog/2025-03-13-VisualPRM/.",
            "score": 17,
            "issue_id": 2705,
            "pub_date": "2025-03-13",
            "pub_date_card": {
                "ru": "13 Ð¼Ð°Ñ€Ñ‚Ð°",
                "en": "March 13",
                "zh": "3æœˆ13æ—¥"
            },
            "hash": "f4734440c38ca048",
            "authors": [
                "Weiyun Wang",
                "Zhangwei Gao",
                "Lianjie Chen",
                "Zhe Chen",
                "Jinguo Zhu",
                "Xiangyu Zhao",
                "Yangzhou Liu",
                "Yue Cao",
                "Shenglong Ye",
                "Xizhou Zhu",
                "Lewei Lu",
                "Haodong Duan",
                "Yu Qiao",
                "Jifeng Dai",
                "Wenhai Wang"
            ],
            "affiliations": [
                "Fudan University",
                "Nanjing University",
                "SenseTime Research",
                "Shanghai AI Laboratory",
                "Shanghai Jiaotong University",
                "The Chinese University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.10291.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#open_source",
                    "#benchmark",
                    "#reasoning",
                    "#multimodal",
                    "#training"
                ],
                "emoji": "ðŸ§ ",
                "ru": {
                    "title": "VisualPRM: ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ðµ Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ Ð¿Ñ€Ð¾Ð´Ð²Ð¸Ð½ÑƒÑ‚Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð²Ð¾Ð·Ð½Ð°Ð³Ñ€Ð°Ð¶Ð´ÐµÐ½Ð¸Ñ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ°",
                    "desc": "VisualPRM - ÑÑ‚Ð¾ ÑƒÑÐ¾Ð²ÐµÑ€ÑˆÐµÐ½ÑÑ‚Ð²Ð¾Ð²Ð°Ð½Ð½Ð°Ñ Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ð°Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð²Ð¾Ð·Ð½Ð°Ð³Ñ€Ð°Ð¶Ð´ÐµÐ½Ð¸Ñ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ° Ñ 8 Ð¼Ð¸Ð»Ð»Ð¸Ð°Ñ€Ð´Ð°Ð¼Ð¸ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð². ÐžÐ½Ð° ÑƒÐ»ÑƒÑ‡ÑˆÐ°ÐµÑ‚ ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑ‚Ð¸ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ñ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ñ… Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ (MLLM) Ñ€Ð°Ð·Ð½Ñ‹Ñ… Ð¼Ð°ÑÑˆÑ‚Ð°Ð±Ð¾Ð² Ð¸ ÑÐµÐ¼ÐµÐ¹ÑÑ‚Ð² Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼ ÑÑ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸Ð¸ Ð¾Ñ†ÐµÐ½ÐºÐ¸ Best-of-N. ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð¸Ñ€ÑƒÐµÑ‚ Ð¿Ñ€ÐµÐ²Ð¾ÑÑ…Ð¾Ð´Ð½ÑƒÑŽ Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ Ð¿Ð¾ ÑÑ€Ð°Ð²Ð½ÐµÐ½Ð¸ÑŽ Ñ Ð¼Ð¾Ð´ÐµÐ»ÑÐ¼Ð¸ Ð²Ð¾Ð·Ð½Ð°Ð³Ñ€Ð°Ð¶Ð´ÐµÐ½Ð¸Ñ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð² Ð¸ ÑÐ°Ð¼Ð¾ÑÐ¾Ð³Ð»Ð°ÑÐ¾Ð²Ð°Ð½Ð½Ð¾ÑÑ‚ÑŒÑŽ Ð¿Ñ€Ð¸ Ð¾Ñ†ÐµÐ½ÐºÐµ BoN. Ð”Ð»Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð¸ Ð¾Ñ†ÐµÐ½ÐºÐ¸ Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ñ‹Ñ… PRM Ð°Ð²Ñ‚Ð¾Ñ€Ñ‹ ÑÐ¾Ð·Ð´Ð°Ð»Ð¸ Ð½Ð°Ð±Ð¾Ñ€ Ð´Ð°Ð½Ð½Ñ‹Ñ… VisualPRM400K Ð¸ Ð±ÐµÐ½Ñ‡Ð¼Ð°Ñ€Ðº VisualProcessBench."
                },
                "en": {
                    "title": "Enhancing Multimodal Reasoning with VisualPRM",
                    "desc": "VisualPRM is a multimodal Process Reward Model designed to enhance the reasoning capabilities of Multimodal Large Language Models (MLLMs) with 8 billion parameters. It demonstrates significant improvements in reasoning performance across various model scales and types, achieving a notable 5.9-point increase on seven multimodal reasoning benchmarks, even with high-capacity models like InternVL2.5-78B. The model outperforms traditional Outcome Reward Models and Self-Consistency methods during Best-of-N evaluations. Additionally, VisualPRM introduces a new dataset, VisualPRM400K, and a benchmark, VisualProcessBench, to support the training and evaluation of multimodal PRMs."
                },
                "zh": {
                    "title": "æå‡å¤šæ¨¡æ€æŽ¨ç†èƒ½åŠ›çš„VisualPRMæ¨¡åž‹",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§å…ˆè¿›çš„å¤šæ¨¡æ€è¿‡ç¨‹å¥–åŠ±æ¨¡åž‹ï¼ˆPRMï¼‰VisualPRMï¼Œå…·æœ‰80äº¿ä¸ªå‚æ•°ï¼Œèƒ½å¤Ÿæå‡çŽ°æœ‰å¤šæ¨¡æ€å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆMLLMsï¼‰çš„æŽ¨ç†èƒ½åŠ›ã€‚è¯¥æ¨¡åž‹åœ¨ä¸‰ç§ç±»åž‹çš„MLLMså’Œå››ç§ä¸åŒçš„æ¨¡åž‹è§„æ¨¡ä¸Šå‡è¡¨çŽ°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯åœ¨å¼ºå¤§çš„InternVL2.5-78Bæ¨¡åž‹ä¸Šï¼ŒæŽ¨ç†æ€§èƒ½æé«˜äº†5.9åˆ†ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒVisualPRMåœ¨æœ€ä½³é€‰æ‹©ï¼ˆBoNï¼‰è¯„ä¼°ä¸­ä¼˜äºŽç»“æžœå¥–åŠ±æ¨¡åž‹å’Œè‡ªä¸€è‡´æ€§æ–¹æ³•ã€‚ä¸ºäº†æ”¯æŒå¤šæ¨¡æ€PRMsçš„è®­ç»ƒï¼Œæˆ‘ä»¬æž„å»ºäº†ä¸€ä¸ªåä¸ºVisualPRM400Kçš„å¤šæ¨¡æ€è¿‡ç¨‹ç›‘ç£æ•°æ®é›†ï¼Œå¹¶æå‡ºäº†VisualProcessBenchåŸºå‡†ï¼Œä»¥è¯„ä¼°PRMsåœ¨å¤šæ¨¡æ€æŽ¨ç†ä»»åŠ¡ä¸­æ£€æµ‹é”™è¯¯æ­¥éª¤çš„èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.10596",
            "title": "GroundingSuite: Measuring Complex Multi-Granular Pixel Grounding",
            "url": "https://huggingface.co/papers/2503.10596",
            "abstract": "Pixel grounding, encompassing tasks such as Referring Expression Segmentation (RES), has garnered considerable attention due to its immense potential for bridging the gap between vision and language modalities. However, advancements in this domain are currently constrained by limitations inherent in existing datasets, including limited object categories, insufficient textual diversity, and a scarcity of high-quality annotations. To mitigate these limitations, we introduce GroundingSuite, which comprises: (1) an automated data annotation framework leveraging multiple Vision-Language Model (VLM) agents; (2) a large-scale training dataset encompassing 9.56 million diverse referring expressions and their corresponding segmentations; and (3) a meticulously curated evaluation benchmark consisting of 3,800 images. The GroundingSuite training dataset facilitates substantial performance improvements, enabling models trained on it to achieve state-of-the-art results. Specifically, a cIoU of 68.9 on gRefCOCO and a gIoU of 55.3 on RefCOCOm. Moreover, the GroundingSuite annotation framework demonstrates superior efficiency compared to the current leading data annotation method, i.e., 4.5 times faster than the GLaMM.",
            "score": 15,
            "issue_id": 2701,
            "pub_date": "2025-03-13",
            "pub_date_card": {
                "ru": "13 Ð¼Ð°Ñ€Ñ‚Ð°",
                "en": "March 13",
                "zh": "3æœˆ13æ—¥"
            },
            "hash": "1b9ea337d198c741",
            "authors": [
                "Rui Hu",
                "Lianghui Zhu",
                "Yuxuan Zhang",
                "Tianheng Cheng",
                "Lei Liu",
                "Heng Liu",
                "Longjin Ran",
                "Xiaoxin Chen",
                "Wenyu Liu",
                "Xinggang Wang"
            ],
            "affiliations": [
                "School of EIC, Huazhong University of Science & Technology",
                "vivo AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.10596.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#data",
                    "#dataset"
                ],
                "emoji": "ðŸ”",
                "ru": {
                    "title": "GroundingSuite: Ñ€ÐµÐ²Ð¾Ð»ÑŽÑ†Ð¸Ñ Ð² Ð¿Ð¸ÐºÑÐµÐ»ÑŒÐ½Ð¾Ð¹ Ð»Ð¾ÐºÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð¾Ð±ÑŠÐµÐºÑ‚Ð¾Ð²",
                    "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ GroundingSuite - Ð½Ð¾Ð²Ñ‹Ð¹ Ð½Ð°Ð±Ð¾Ñ€ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð² Ð´Ð»Ñ Ð·Ð°Ð´Ð°Ñ‡Ð¸ Ð¿Ð¸ÐºÑÐµÐ»ÑŒÐ½Ð¾Ð¹ Ð»Ð¾ÐºÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð¾Ð±ÑŠÐµÐºÑ‚Ð¾Ð² Ð¿Ð¾ Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ð¾Ð¼Ñƒ Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸ÑŽ. ÐžÐ½ Ð²ÐºÐ»ÑŽÑ‡Ð°ÐµÑ‚ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½ÑƒÑŽ ÑÐ¸ÑÑ‚ÐµÐ¼Ñƒ Ð°Ð½Ð½Ð¾Ñ‚Ð°Ñ†Ð¸Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð°Ð³ÐµÐ½Ñ‚Ð½Ð¾Ð³Ð¾ Ð¿Ð¾Ð´Ñ…Ð¾Ð´Ð° Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¸Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð¼Ð°ÑˆÐ¸Ð½Ð½Ð¾Ð³Ð¾ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð´Ð»Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ ÐµÑÑ‚ÐµÑÑ‚Ð²ÐµÐ½Ð½Ð¾Ð³Ð¾ ÑÐ·Ñ‹ÐºÐ° Ð¸ ÐºÐ¾Ð¼Ð¿ÑŒÑŽÑ‚ÐµÑ€Ð½Ð¾Ð³Ð¾ Ð·Ñ€ÐµÐ½Ð¸Ñ. GroundingSuite Ñ‚Ð°ÐºÐ¶Ðµ ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ñ‚ ÐºÑ€ÑƒÐ¿Ð½Ð¾Ð¼Ð°ÑÑˆÑ‚Ð°Ð±Ð½Ñ‹Ð¹ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚ Ð´Ð»Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ñ 9,56 Ð¼Ð¸Ð»Ð»Ð¸Ð¾Ð½Ð°Ð¼Ð¸ Ñ€Ð°Ð·Ð½Ð¾Ð¾Ð±Ñ€Ð°Ð·Ð½Ñ‹Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð²Ñ‹Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹ Ð¸ ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ñ… Ð¸Ð¼ ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ð¹, Ð° Ñ‚Ð°ÐºÐ¶Ðµ Ñ‚Ñ‰Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ð¿Ð¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²Ð»ÐµÐ½Ð½Ñ‹Ð¹ Ð½Ð°Ð±Ð¾Ñ€ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ Ð¸Ð· 3800 Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹. ÐœÐ¾Ð´ÐµÐ»Ð¸, Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð½Ñ‹Ðµ Ð½Ð° ÑÑ‚Ð¾Ð¼ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ðµ, Ð´Ð¾ÑÑ‚Ð¸Ð³Ð°ÑŽÑ‚ Ð½Ð°Ð¸Ð»ÑƒÑ‡ÑˆÐ¸Ñ… Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð² Ð² Ð·Ð°Ð´Ð°Ñ‡Ð°Ñ… ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ð¸ Ð¿Ð¾ Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ð¾Ð¼Ñƒ Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸ÑŽ."
                },
                "en": {
                    "title": "Revolutionizing Pixel Grounding with GroundingSuite",
                    "desc": "This paper introduces GroundingSuite, a new framework designed to enhance pixel grounding tasks like Referring Expression Segmentation (RES) by addressing the limitations of existing datasets. GroundingSuite features an automated data annotation system that utilizes multiple Vision-Language Model (VLM) agents, resulting in a large-scale dataset with 9.56 million diverse referring expressions and their segmentations. The framework not only improves the quality and diversity of training data but also provides a curated evaluation benchmark with 3,800 images. Models trained on this dataset achieve state-of-the-art performance, significantly outperforming previous methods in both efficiency and accuracy."
                },
                "zh": {
                    "title": "GroundingSuiteï¼šæå‡è§†è§‰ä¸Žè¯­è¨€çš„æ¡¥æ¢",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºGroundingSuiteçš„æ¡†æž¶ï¼Œæ—¨åœ¨è§£å†³çŽ°æœ‰æ•°æ®é›†åœ¨åƒç´ å®šä½ä»»åŠ¡ä¸­çš„å±€é™æ€§ã€‚è¯¥æ¡†æž¶åŒ…æ‹¬ä¸€ä¸ªè‡ªåŠ¨åŒ–æ•°æ®æ³¨é‡Šç³»ç»Ÿï¼Œåˆ©ç”¨å¤šä¸ªè§†è§‰-è¯­è¨€æ¨¡åž‹ï¼ˆVLMï¼‰ä»£ç†ç”Ÿæˆæ•°æ®ã€‚GroundingSuiteæä¾›äº†ä¸€ä¸ªåŒ…å«956ä¸‡ç§å¤šæ ·åŒ–æŒ‡ç§°è¡¨è¾¾åŠå…¶å¯¹åº”åˆ†å‰²çš„å¤§è§„æ¨¡è®­ç»ƒæ•°æ®é›†ï¼Œå¹¶å»ºç«‹äº†ä¸€ä¸ªåŒ…å«3800å¼ å›¾åƒçš„è¯„ä¼°åŸºå‡†ã€‚é€šè¿‡ä½¿ç”¨GroundingSuiteè®­ç»ƒçš„æ•°æ®é›†ï¼Œæ¨¡åž‹çš„æ€§èƒ½æ˜¾è‘—æå‡ï¼Œè¾¾åˆ°äº†æœ€æ–°çš„ç ”ç©¶æˆæžœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.10351",
            "title": "New Trends for Modern Machine Translation with Large Reasoning Models",
            "url": "https://huggingface.co/papers/2503.10351",
            "abstract": "Recent advances in Large Reasoning Models (LRMs), particularly those leveraging Chain-of-Thought reasoning (CoT), have opened brand new possibility for Machine Translation (MT). This position paper argues that LRMs substantially transformed traditional neural MT as well as LLMs-based MT paradigms by reframing translation as a dynamic reasoning task that requires contextual, cultural, and linguistic understanding and reasoning. We identify three foundational shifts: 1) contextual coherence, where LRMs resolve ambiguities and preserve discourse structure through explicit reasoning over cross-sentence and complex context or even lack of context; 2) cultural intentionality, enabling models to adapt outputs by inferring speaker intent, audience expectations, and socio-linguistic norms; 3) self-reflection, LRMs can perform self-reflection during the inference time to correct the potential errors in translation especially extremely noisy cases, showing better robustness compared to simply mapping X->Y translation. We explore various scenarios in translation including stylized translation, document-level translation and multimodal translation by showcasing empirical examples that demonstrate the superiority of LRMs in translation. We also identify several interesting phenomenons for LRMs for MT including auto-pivot translation as well as the critical challenges such as over-localisation in translation and inference efficiency. In conclusion, we think that LRMs redefine translation systems not merely as text converters but as multilingual cognitive agents capable of reasoning about meaning beyond the text. This paradigm shift reminds us to think of problems in translation beyond traditional translation scenarios in a much broader context with LRMs - what we can achieve on top of it.",
            "score": 14,
            "issue_id": 2700,
            "pub_date": "2025-03-13",
            "pub_date_card": {
                "ru": "13 Ð¼Ð°Ñ€Ñ‚Ð°",
                "en": "March 13",
                "zh": "3æœˆ13æ—¥"
            },
            "hash": "c0a1b109c05698cc",
            "authors": [
                "Sinuo Liu",
                "Chenyang Lyu",
                "Minghao Wu",
                "Longyue Wang",
                "Weihua Luo",
                "Kaifu Zhang"
            ],
            "affiliations": [
                "MarcoPolo Team, Alibaba International Digital Commerce",
                "University of Edinburgh"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.10351.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#agents",
                    "#multilingual",
                    "#machine_translation"
                ],
                "emoji": "ðŸ§ ",
                "ru": {
                    "title": "LRM: ÐžÑ‚ Ð¿Ñ€Ð¾ÑÑ‚Ð¾Ð³Ð¾ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ð° Ðº Ð¼Ð½Ð¾Ð³Ð¾ÑÐ·Ñ‹Ñ‡Ð½Ñ‹Ð¼ ÐºÐ¾Ð³Ð½Ð¸Ñ‚Ð¸Ð²Ð½Ñ‹Ð¼ Ð°Ð³ÐµÐ½Ñ‚Ð°Ð¼",
                    "desc": "Ð­Ñ‚Ð° ÑÑ‚Ð°Ñ‚ÑŒÑ Ñ€Ð°ÑÑÐ¼Ð°Ñ‚Ñ€Ð¸Ð²Ð°ÐµÑ‚ Ð²Ð»Ð¸ÑÐ½Ð¸Ðµ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹ (LRM) Ð½Ð° Ð¼Ð°ÑˆÐ¸Ð½Ð½Ñ‹Ð¹ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´. ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ ÑƒÑ‚Ð²ÐµÑ€Ð¶Ð´Ð°ÑŽÑ‚, Ñ‡Ñ‚Ð¾ LRM Ñ‚Ñ€Ð°Ð½ÑÑ„Ð¾Ñ€Ð¼Ð¸Ñ€ÑƒÑŽÑ‚ Ñ‚Ñ€Ð°Ð´Ð¸Ñ†Ð¸Ð¾Ð½Ð½Ñ‹Ðµ Ð¿Ð¾Ð´Ñ…Ð¾Ð´Ñ‹, Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÑ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´ ÐºÐ°Ðº Ð´Ð¸Ð½Ð°Ð¼Ð¸Ñ‡ÐµÑÐºÑƒÑŽ Ð·Ð°Ð´Ð°Ñ‡Ñƒ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ñ, Ñ‚Ñ€ÐµÐ±ÑƒÑŽÑ‰ÑƒÑŽ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ÑƒÐ°Ð»ÑŒÐ½Ð¾Ð³Ð¾, ÐºÑƒÐ»ÑŒÑ‚ÑƒÑ€Ð½Ð¾Ð³Ð¾ Ð¸ Ð»Ð¸Ð½Ð³Ð²Ð¸ÑÑ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ Ð¿Ð¾Ð½Ð¸Ð¼Ð°Ð½Ð¸Ñ. Ð’Ñ‹Ð´ÐµÐ»ÑÑŽÑ‚ÑÑ Ñ‚Ñ€Ð¸ ÐºÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ñ… Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ñ: ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ÑƒÐ°Ð»ÑŒÐ½Ð°Ñ ÑÐ¾Ð³Ð»Ð°ÑÐ¾Ð²Ð°Ð½Ð½Ð¾ÑÑ‚ÑŒ, ÐºÑƒÐ»ÑŒÑ‚ÑƒÑ€Ð½Ð°Ñ Ð¸Ð½Ñ‚ÐµÐ½Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾ÑÑ‚ÑŒ Ð¸ ÑÐ°Ð¼Ð¾Ñ€ÐµÑ„Ð»ÐµÐºÑÐ¸Ñ. Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ñ‚Ð°ÐºÐ¶Ðµ Ð¸ÑÑÐ»ÐµÐ´ÑƒÐµÑ‚ Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ñ‹Ðµ ÑÑ†ÐµÐ½Ð°Ñ€Ð¸Ð¸ Ð¿Ñ€Ð¸Ð¼ÐµÐ½ÐµÐ½Ð¸Ñ LRM Ð² Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ðµ Ð¸ Ð¾Ð±ÑÑƒÐ¶Ð´Ð°ÐµÑ‚ Ð²Ð¾Ð·Ð½Ð¸ÐºÐ°ÑŽÑ‰Ð¸Ðµ Ñ„ÐµÐ½Ð¾Ð¼ÐµÐ½Ñ‹ Ð¸ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹."
                },
                "en": {
                    "title": "Transforming Translation: LRMs as Cognitive Agents",
                    "desc": "This paper discusses how Large Reasoning Models (LRMs) are changing the way we think about Machine Translation (MT). It highlights three key shifts: first, LRMs improve contextual coherence by understanding complex contexts and resolving ambiguities; second, they incorporate cultural intentionality, allowing translations to reflect speaker intent and social norms; and third, LRMs can self-reflect during translation to correct errors, making them more robust. The authors provide examples of how LRMs excel in various translation scenarios, suggesting that these models should be seen as cognitive agents that reason about meaning rather than just text converters."
                },
                "zh": {
                    "title": "å¤§åž‹æŽ¨ç†æ¨¡åž‹é‡å¡‘æœºå™¨ç¿»è¯‘çš„æœªæ¥",
                    "desc": "å¤§åž‹æŽ¨ç†æ¨¡åž‹ï¼ˆLRMsï¼‰åœ¨æœºå™¨ç¿»è¯‘ï¼ˆMTï¼‰é¢†åŸŸå¸¦æ¥äº†æ–°çš„å¯èƒ½æ€§ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡é“¾å¼æ€ç»´æŽ¨ç†ï¼ˆCoTï¼‰ã€‚è¿™ç¯‡è®ºæ–‡æŒ‡å‡ºï¼ŒLRMså°†ä¼ ç»Ÿç¥žç»æœºå™¨ç¿»è¯‘è½¬å˜ä¸ºä¸€ç§åŠ¨æ€æŽ¨ç†ä»»åŠ¡ï¼Œå¼ºè°ƒä¸Šä¸‹æ–‡ã€æ–‡åŒ–å’Œè¯­è¨€ç†è§£çš„é‡è¦æ€§ã€‚æˆ‘ä»¬è¯†åˆ«å‡ºä¸‰ä¸ªåŸºç¡€è½¬å˜ï¼šä¸Šä¸‹æ–‡è¿žè´¯æ€§ã€æ–‡åŒ–æ„å›¾æ€§å’Œè‡ªæˆ‘åæ€èƒ½åŠ›ï¼Œä½¿å¾—LRMsåœ¨ç¿»è¯‘ä¸­è¡¨çŽ°å‡ºæ›´å¥½çš„é²æ£’æ€§ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬è®¤ä¸ºLRMsé‡æ–°å®šä¹‰äº†ç¿»è¯‘ç³»ç»Ÿï¼Œä½¿å…¶ä¸ä»…ä»…æ˜¯æ–‡æœ¬è½¬æ¢å™¨ï¼Œè€Œæ˜¯èƒ½å¤Ÿè¶…è¶Šæ–‡æœ¬è¿›è¡Œæ„ä¹‰æŽ¨ç†çš„å¤šè¯­è¨€è®¤çŸ¥ä»£ç†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.04723",
            "title": "Shifting Long-Context LLMs Research from Input to Output",
            "url": "https://huggingface.co/papers/2503.04723",
            "abstract": "Recent advancements in long-context Large Language Models (LLMs) have primarily concentrated on processing extended input contexts, resulting in significant strides in long-context comprehension. However, the equally critical aspect of generating long-form outputs has received comparatively less attention. This paper advocates for a paradigm shift in NLP research toward addressing the challenges of long-output generation. Tasks such as novel writing, long-term planning, and complex reasoning require models to understand extensive contexts and produce coherent, contextually rich, and logically consistent extended text. These demands highlight a critical gap in current LLM capabilities. We underscore the importance of this under-explored domain and call for focused efforts to develop foundational LLMs tailored for generating high-quality, long-form outputs, which hold immense potential for real-world applications.",
            "score": 14,
            "issue_id": 2700,
            "pub_date": "2025-03-06",
            "pub_date_card": {
                "ru": "6 Ð¼Ð°Ñ€Ñ‚Ð°",
                "en": "March 6",
                "zh": "3æœˆ6æ—¥"
            },
            "hash": "009f3e654e927dc3",
            "authors": [
                "Yuhao Wu",
                "Yushi Bai",
                "Zhiqing Hu",
                "Shangqing Tu",
                "Ming Shan Hee",
                "Juanzi Li",
                "Roy Ka-Wei Lee"
            ],
            "affiliations": [
                "Singapore University",
                "Singapore University of Technology and Design",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.04723.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#long_context",
                    "#multimodal"
                ],
                "emoji": "ðŸ“",
                "ru": {
                    "title": "ÐÐ¾Ð²Ñ‹Ð¹ Ð²Ñ‹Ð·Ð¾Ð² Ð´Ð»Ñ Ð˜Ð˜: ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð´Ð»Ð¸Ð½Ð½Ñ‹Ñ… Ñ‚ÐµÐºÑÑ‚Ð¾Ð²",
                    "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¾Ð±ÑÑƒÐ¶Ð´Ð°ÐµÑ‚ Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ð¾ÑÑ‚ÑŒ Ñ€Ð°Ð·Ð²Ð¸Ñ‚Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ ÐµÑÑ‚ÐµÑÑ‚Ð²ÐµÐ½Ð½Ð¾Ð³Ð¾ ÑÐ·Ñ‹ÐºÐ° Ð´Ð»Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð´Ð»Ð¸Ð½Ð½Ñ‹Ñ… Ñ‚ÐµÐºÑÑ‚Ð¾Ð². ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ Ð¾Ñ‚Ð¼ÐµÑ‡Ð°ÑŽÑ‚, Ñ‡Ñ‚Ð¾ ÑÐ¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ñ Ð² Ð¾ÑÐ½Ð¾Ð²Ð½Ð¾Ð¼ ÑÐ¾ÑÑ€ÐµÐ´Ð¾Ñ‚Ð¾Ñ‡ÐµÐ½Ñ‹ Ð½Ð° Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐµ Ð´Ð»Ð¸Ð½Ð½Ñ‹Ñ… Ð²Ñ…Ð¾Ð´Ð½Ñ‹Ñ… ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð¾Ð², Ð½Ð¾ Ð½Ðµ Ð½Ð° ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ð¸ Ð¾Ð±ÑŠÐµÐ¼Ð½Ñ‹Ñ… Ð²Ñ‹Ñ…Ð¾Ð´Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…. ÐŸÐ¾Ð´Ñ‡ÐµÑ€ÐºÐ¸Ð²Ð°ÐµÑ‚ÑÑ Ð²Ð°Ð¶Ð½Ð¾ÑÑ‚ÑŒ Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹, ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ñ‹Ñ… Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ ÑÐ²ÑÐ·Ð½Ñ‹Ðµ Ð¸ Ð»Ð¾Ð³Ð¸Ñ‡ÐµÑÐºÐ¸ Ð¿Ð¾ÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ð´Ð»Ð¸Ð½Ð½Ñ‹Ðµ Ñ‚ÐµÐºÑÑ‚Ñ‹. Ð¢Ð°ÐºÐ¸Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ñ‹ Ð´Ð»Ñ Ñ€ÐµÑˆÐµÐ½Ð¸Ñ Ð·Ð°Ð´Ð°Ñ‡ Ð½Ð°Ð¿Ð¸ÑÐ°Ð½Ð¸Ñ Ñ€Ð¾Ð¼Ð°Ð½Ð¾Ð², Ð´Ð¾Ð»Ð³Ð¾ÑÑ€Ð¾Ñ‡Ð½Ð¾Ð³Ð¾ Ð¿Ð»Ð°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¸ ÑÐ»Ð¾Ð¶Ð½Ñ‹Ñ… Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹."
                },
                "en": {
                    "title": "Bridging the Gap: Enhancing Long-Form Output in LLMs",
                    "desc": "This paper discusses the need for improvements in long-form output generation by Large Language Models (LLMs). While recent advancements have focused on understanding long input contexts, generating coherent and contextually rich long outputs remains underexplored. The authors emphasize that tasks like novel writing and complex reasoning require models to produce logically consistent extended text. They call for more research efforts to develop LLMs that can effectively handle these long-output challenges, which are crucial for various real-world applications."
                },
                "zh": {
                    "title": "æŽ¨åŠ¨é•¿æ–‡æœ¬ç”Ÿæˆçš„ç ”ç©¶è½¬åž‹",
                    "desc": "æœ€è¿‘ï¼Œé•¿ä¸Šä¸‹æ–‡çš„å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†æ‰©å±•è¾“å…¥ä¸Šä¸‹æ–‡æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç”Ÿæˆé•¿æ–‡æœ¬è¾“å‡ºçš„ç ”ç©¶ç›¸å¯¹è¾ƒå°‘ã€‚æœ¬æ–‡æå€¡è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ç ”ç©¶å‘è§£å†³é•¿è¾“å‡ºç”Ÿæˆçš„æŒ‘æˆ˜è½¬å˜ã€‚é•¿ç¯‡å°è¯´å†™ä½œã€é•¿æœŸè§„åˆ’å’Œå¤æ‚æŽ¨ç†ç­‰ä»»åŠ¡éœ€è¦æ¨¡åž‹ç†è§£å¹¿æ³›çš„ä¸Šä¸‹æ–‡ï¼Œå¹¶ç”Ÿæˆè¿žè´¯ã€ä¸°å¯Œä¸”é€»è¾‘ä¸€è‡´çš„æ‰©å±•æ–‡æœ¬ã€‚æˆ‘ä»¬å¼ºè°ƒè¿™ä¸€æœªè¢«å……åˆ†æŽ¢ç´¢é¢†åŸŸçš„é‡è¦æ€§ï¼Œå¹¶å‘¼åé›†ä¸­åŠ›é‡å¼€å‘ä¸“é—¨ç”¨äºŽç”Ÿæˆé«˜è´¨é‡é•¿æ–‡æœ¬è¾“å‡ºçš„åŸºç¡€LLMï¼Œä»¥æ»¡è¶³çŽ°å®žåº”ç”¨çš„å·¨å¤§æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.10437",
            "title": "4D LangSplat: 4D Language Gaussian Splatting via Multimodal Large\n  Language Models",
            "url": "https://huggingface.co/papers/2503.10437",
            "abstract": "Learning 4D language fields to enable time-sensitive, open-ended language queries in dynamic scenes is essential for many real-world applications. While LangSplat successfully grounds CLIP features into 3D Gaussian representations, achieving precision and efficiency in 3D static scenes, it lacks the ability to handle dynamic 4D fields as CLIP, designed for static image-text tasks, cannot capture temporal dynamics in videos. Real-world environments are inherently dynamic, with object semantics evolving over time. Building a precise 4D language field necessitates obtaining pixel-aligned, object-wise video features, which current vision models struggle to achieve. To address these challenges, we propose 4D LangSplat, which learns 4D language fields to handle time-agnostic or time-sensitive open-vocabulary queries in dynamic scenes efficiently. 4D LangSplat bypasses learning the language field from vision features and instead learns directly from text generated from object-wise video captions via Multimodal Large Language Models (MLLMs). Specifically, we propose a multimodal object-wise video prompting method, consisting of visual and text prompts that guide MLLMs to generate detailed, temporally consistent, high-quality captions for objects throughout a video. These captions are encoded using a Large Language Model into high-quality sentence embeddings, which then serve as pixel-aligned, object-specific feature supervision, facilitating open-vocabulary text queries through shared embedding spaces. Recognizing that objects in 4D scenes exhibit smooth transitions across states, we further propose a status deformable network to model these continuous changes over time effectively. Our results across multiple benchmarks demonstrate that 4D LangSplat attains precise and efficient results for both time-sensitive and time-agnostic open-vocabulary queries.",
            "score": 11,
            "issue_id": 2702,
            "pub_date": "2025-03-13",
            "pub_date_card": {
                "ru": "13 Ð¼Ð°Ñ€Ñ‚Ð°",
                "en": "March 13",
                "zh": "3æœˆ13æ—¥"
            },
            "hash": "212017b2c934863c",
            "authors": [
                "Wanhua Li",
                "Renping Zhou",
                "Jiawei Zhou",
                "Yingwei Song",
                "Johannes Herter",
                "Minghan Qin",
                "Gao Huang",
                "Hanspeter Pfister"
            ],
            "affiliations": [
                "Brown University",
                "ETH Zurich",
                "Harvard University",
                "Stony Brook University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.10437.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#video",
                    "#optimization",
                    "#multimodal",
                    "#interpretability",
                    "#reasoning",
                    "#games",
                    "#agi",
                    "#transfer_learning"
                ],
                "emoji": "ðŸŒ",
                "ru": {
                    "title": "4D ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ðµ Ð¿Ð¾Ð»Ñ Ð´Ð»Ñ Ð´Ð¸Ð½Ð°Ð¼Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… ÑÑ†ÐµÐ½: Ð¾Ñ‚ ÑÑ‚Ð°Ñ‚Ð¸ÐºÐ¸ Ðº Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾Ð¹ Ð´Ð¸Ð½Ð°Ð¼Ð¸ÐºÐµ",
                    "desc": "4D LangSplat - ÑÑ‚Ð¾ Ð¼ÐµÑ‚Ð¾Ð´ Ð´Ð»Ñ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ 4D ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¿Ð¾Ð»ÐµÐ¹, Ð¿Ð¾Ð·Ð²Ð¾Ð»ÑÑŽÑ‰Ð¸Ð¹ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÑÑ‚ÑŒ Ð¾Ñ‚ÐºÑ€Ñ‹Ñ‚Ñ‹Ðµ ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ðµ Ð·Ð°Ð¿Ñ€Ð¾ÑÑ‹ Ð² Ð´Ð¸Ð½Ð°Ð¼Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… ÑÑ†ÐµÐ½Ð°Ñ… Ñ ÑƒÑ‡ÐµÑ‚Ð¾Ð¼ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸. ÐžÐ½ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ðµ ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ (MLLM) Ð´Ð»Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¿Ð¾Ð´Ñ€Ð¾Ð±Ð½Ñ‹Ñ… Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ð¹ Ð¾Ð±ÑŠÐµÐºÑ‚Ð¾Ð² Ð² Ð²Ð¸Ð´ÐµÐ¾, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð·Ð°Ñ‚ÐµÐ¼ ÐºÐ¾Ð´Ð¸Ñ€ÑƒÑŽÑ‚ÑÑ Ð² Ð²Ñ‹ÑÐ¾ÐºÐ¾ÐºÐ°Ñ‡ÐµÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ðµ Ð²ÐµÐºÑ‚Ð¾Ñ€Ð½Ñ‹Ðµ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð¸Ñ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ LLM. ÐœÐµÑ‚Ð¾Ð´ Ð²ÐºÐ»ÑŽÑ‡Ð°ÐµÑ‚ ÑÐµÑ‚ÑŒ Ð´ÐµÑ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ð¹ Ð´Ð»Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¿Ð»Ð°Ð²Ð½Ñ‹Ñ… Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ð¹ Ð¾Ð±ÑŠÐµÐºÑ‚Ð¾Ð² Ð²Ð¾ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸. 4D LangSplat Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð¸Ñ€ÑƒÐµÑ‚ Ñ‚Ð¾Ñ‡Ð½Ñ‹Ðµ Ð¸ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ñ‹Ðµ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð´Ð»Ñ Ð¾Ñ‚ÐºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð² ÐºÐ°Ðº Ñ ÑƒÑ‡ÐµÑ‚Ð¾Ð¼ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸, Ñ‚Ð°Ðº Ð¸ Ð±ÐµÐ· Ð½ÐµÐ³Ð¾."
                },
                "en": {
                    "title": "Empowering Dynamic Scene Understanding with 4D Language Fields",
                    "desc": "This paper introduces 4D LangSplat, a novel approach for creating 4D language fields that can handle dynamic scenes in videos. Unlike previous models that focus on static images, 4D LangSplat learns directly from text generated by Multimodal Large Language Models (MLLMs) using object-wise video captions. The method employs a multimodal prompting technique to produce high-quality, temporally consistent captions, which are then transformed into sentence embeddings for effective querying. Additionally, a status deformable network is proposed to accurately model the continuous changes of objects over time, resulting in improved performance for both time-sensitive and time-agnostic queries."
                },
                "zh": {
                    "title": "åŠ¨æ€åœºæ™¯ä¸­çš„4Dè¯­è¨€åœºå­¦ä¹ ",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸º4D LangSplatï¼Œæ—¨åœ¨å¤„ç†åŠ¨æ€åœºæ™¯ä¸­çš„æ—¶é—´æ•æ„Ÿå’Œå¼€æ”¾è¯æ±‡æŸ¥è¯¢ã€‚ä¸ŽçŽ°æœ‰çš„é™æ€å›¾åƒ-æ–‡æœ¬æ¨¡åž‹ä¸åŒï¼Œ4D LangSplatèƒ½å¤Ÿå­¦ä¹ 4Dè¯­è¨€åœºï¼Œç›´æŽ¥ä»Žå¯¹è±¡è§†é¢‘å­—å¹•ç”Ÿæˆçš„æ–‡æœ¬ä¸­èŽ·å–ä¿¡æ¯ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¤šæ¨¡æ€å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆMLLMsï¼‰ç”Ÿæˆé«˜è´¨é‡çš„æ—¶é—´ä¸€è‡´æ€§å­—å¹•ï¼Œå¹¶å°†å…¶ç¼–ç ä¸ºå¥å­åµŒå…¥ï¼Œä»¥æ”¯æŒåƒç´ å¯¹é½çš„å¯¹è±¡ç‰¹å¾ç›‘ç£ã€‚é€šè¿‡å¼•å…¥çŠ¶æ€å¯å˜å½¢ç½‘ç»œï¼Œ4D LangSplatæœ‰æ•ˆå»ºæ¨¡äº†å¯¹è±¡åœ¨æ—¶é—´ä¸Šçš„è¿žç»­å˜åŒ–ï¼Œå±•ç¤ºäº†åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—çš„ç²¾ç¡®å’Œé«˜æ•ˆçš„æŸ¥è¯¢ç»“æžœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.08677",
            "title": "OmniPaint: Mastering Object-Oriented Editing via Disentangled\n  Insertion-Removal Inpainting",
            "url": "https://huggingface.co/papers/2503.08677",
            "abstract": "Diffusion-based generative models have revolutionized object-oriented image editing, yet their deployment in realistic object removal and insertion remains hampered by challenges such as the intricate interplay of physical effects and insufficient paired training data. In this work, we introduce OmniPaint, a unified framework that re-conceptualizes object removal and insertion as interdependent processes rather than isolated tasks. Leveraging a pre-trained diffusion prior along with a progressive training pipeline comprising initial paired sample optimization and subsequent large-scale unpaired refinement via CycleFlow, OmniPaint achieves precise foreground elimination and seamless object insertion while faithfully preserving scene geometry and intrinsic properties. Furthermore, our novel CFD metric offers a robust, reference-free evaluation of context consistency and object hallucination, establishing a new benchmark for high-fidelity image editing. Project page: https://yeates.github.io/OmniPaint-Page/",
            "score": 11,
            "issue_id": 2712,
            "pub_date": "2025-03-11",
            "pub_date_card": {
                "ru": "11 Ð¼Ð°Ñ€Ñ‚Ð°",
                "en": "March 11",
                "zh": "3æœˆ11æ—¥"
            },
            "hash": "6110f983d46b536a",
            "authors": [
                "Yongsheng Yu",
                "Ziyun Zeng",
                "Haitian Zheng",
                "Jiebo Luo"
            ],
            "affiliations": [
                "Adobe Research",
                "University of Rochester"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.08677.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#dataset",
                    "#diffusion",
                    "#hallucinations",
                    "#benchmark"
                ],
                "emoji": "ðŸŽ¨",
                "ru": {
                    "title": "OmniPaint: Ñ€ÐµÐ²Ð¾Ð»ÑŽÑ†Ð¸Ñ Ð² Ñ€ÐµÐ´Ð°ÐºÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ð¸ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ Ð˜Ð˜",
                    "desc": "OmniPaint - ÑÑ‚Ð¾ ÑƒÐ½Ð¸Ñ„Ð¸Ñ†Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð°Ñ ÑÐ¸ÑÑ‚ÐµÐ¼Ð° Ð´Ð»Ñ Ñ€ÐµÐ´Ð°ÐºÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹, Ð¾ÑÐ½Ð¾Ð²Ð°Ð½Ð½Ð°Ñ Ð½Ð° Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ð¾Ð½Ð½Ñ‹Ñ… Ð³ÐµÐ½ÐµÑ€Ð°Ñ‚Ð¸Ð²Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÑÑ…. ÐžÐ½Ð° Ñ€Ð°ÑÑÐ¼Ð°Ñ‚Ñ€Ð¸Ð²Ð°ÐµÑ‚ ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸Ðµ Ð¸ Ð²ÑÑ‚Ð°Ð²ÐºÑƒ Ð¾Ð±ÑŠÐµÐºÑ‚Ð¾Ð² ÐºÐ°Ðº Ð²Ð·Ð°Ð¸Ð¼Ð¾ÑÐ²ÑÐ·Ð°Ð½Ð½Ñ‹Ðµ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÑ‹, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑ Ð¿Ñ€ÐµÐ´Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð½ÑƒÑŽ Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ð¾Ð½Ð½ÑƒÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð¸ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑÐ¸Ð²Ð½Ñ‹Ð¹ ÐºÐ¾Ð½Ð²ÐµÐ¹ÐµÑ€ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ. OmniPaint Ð´Ð¾ÑÑ‚Ð¸Ð³Ð°ÐµÑ‚ Ñ‚Ð¾Ñ‡Ð½Ð¾Ð³Ð¾ ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸Ñ Ð¿ÐµÑ€ÐµÐ´Ð½ÐµÐ³Ð¾ Ð¿Ð»Ð°Ð½Ð° Ð¸ Ð±ÐµÑÑˆÐ¾Ð²Ð½Ð¾Ð¹ Ð²ÑÑ‚Ð°Ð²ÐºÐ¸ Ð¾Ð±ÑŠÐµÐºÑ‚Ð¾Ð², ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÑ Ð³ÐµÐ¾Ð¼ÐµÑ‚Ñ€Ð¸ÑŽ ÑÑ†ÐµÐ½Ñ‹ Ð¸ Ð²Ð½ÑƒÑ‚Ñ€ÐµÐ½Ð½Ð¸Ðµ ÑÐ²Ð¾Ð¹ÑÑ‚Ð²Ð°. Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð° Ñ‚Ð°ÐºÐ¶Ðµ Ð¿Ñ€ÐµÐ´Ð»Ð°Ð³Ð°ÐµÑ‚ Ð½Ð¾Ð²ÑƒÑŽ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÑƒ CFD Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ ÑÐ¾Ð³Ð»Ð°ÑÐ¾Ð²Ð°Ð½Ð½Ð¾ÑÑ‚Ð¸ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð° Ð¸ Ð³Ð°Ð»Ð»ÑŽÑ†Ð¸Ð½Ð°Ñ†Ð¸Ð¹ Ð¾Ð±ÑŠÐµÐºÑ‚Ð¾Ð²."
                },
                "en": {
                    "title": "OmniPaint: Seamless Object Editing through Interconnected Processes",
                    "desc": "This paper presents OmniPaint, a new framework for object removal and insertion in images, treating these tasks as interconnected rather than separate. It utilizes a pre-trained diffusion model and a two-step training process that starts with paired samples and then refines the model using unpaired data. OmniPaint effectively removes unwanted objects and adds new ones while maintaining the original scene's geometry and characteristics. Additionally, the authors introduce a new evaluation metric called CFD, which measures the consistency and realism of the edited images without needing reference images."
                },
                "zh": {
                    "title": "OmniPaintï¼šç‰©ä½“ç¼–è¾‘çš„æ–°çºªå…ƒ",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºOmniPaintçš„ç»Ÿä¸€æ¡†æž¶ï¼Œæ—¨åœ¨è§£å†³ç‰©ä½“ç§»é™¤å’Œæ’å…¥çš„å¤æ‚é—®é¢˜ã€‚è¯¥æ¡†æž¶å°†ç‰©ä½“ç§»é™¤å’Œæ’å…¥è§†ä¸ºç›¸äº’ä¾èµ–çš„è¿‡ç¨‹ï¼Œè€Œéžå­¤ç«‹çš„ä»»åŠ¡ã€‚é€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡åž‹å’Œé€æ­¥è®­ç»ƒæµç¨‹ï¼ŒOmniPaintèƒ½å¤Ÿå®žçŽ°ç²¾ç¡®çš„å‰æ™¯æ¶ˆé™¤å’Œæ— ç¼çš„ç‰©ä½“æ’å…¥ï¼ŒåŒæ—¶ä¿æŒåœºæ™¯çš„å‡ ä½•å½¢çŠ¶å’Œå†…åœ¨ç‰¹æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºçš„CFDæŒ‡æ ‡ä¸ºä¸Šä¸‹æ–‡ä¸€è‡´æ€§å’Œç‰©ä½“å¹»è§‰æä¾›äº†ä¸€ç§å¼ºå¥çš„æ— å‚è€ƒè¯„ä¼°æ–¹æ³•ï¼Œå»ºç«‹äº†é«˜ä¿çœŸå›¾åƒç¼–è¾‘çš„æ–°åŸºå‡†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.10582",
            "title": "VisualWebInstruct: Scaling up Multimodal Instruction Data through Web\n  Search",
            "url": "https://huggingface.co/papers/2503.10582",
            "abstract": "Vision-Language Models have made significant progress on many perception-focused tasks, however, their progress on reasoning-focused tasks seem to be limited due to the lack of high-quality and diverse training data. In this work, we aim to address the scarcity issue of reasoning-focused multimodal datasets. We propose VisualWebInstruct - a novel approach that leverages search engine to create a diverse, and high-quality dataset spanning multiple disciplines like math, physics, finance, chemistry, etc. Starting with meticulously selected 30,000 seed images, we employ Google Image search to identify websites containing similar images. We collect and process the HTMLs from over 700K unique URL sources. Through a pipeline of content extraction, filtering and synthesis, we build a dataset of approximately 900K question-answer pairs, with 40% being visual QA pairs and the rest as text QA pairs. Models fine-tuned on VisualWebInstruct demonstrate significant performance gains: (1) training from Llava-OV-mid shows 10-20% absolute point gains across benchmarks, (2) training from MAmmoTH-VL shows 5% absoluate gain. Our best model MAmmoTH-VL2 shows state-of-the-art performance within the 10B parameter class on MMMU-Pro-std (40.7%), MathVerse (42.6%), and DynaMath (55.7%). These remarkable results highlight the effectiveness of our dataset in enhancing VLMs' reasoning capabilities for complex multimodal tasks.",
            "score": 10,
            "issue_id": 2700,
            "pub_date": "2025-03-13",
            "pub_date_card": {
                "ru": "13 Ð¼Ð°Ñ€Ñ‚Ð°",
                "en": "March 13",
                "zh": "3æœˆ13æ—¥"
            },
            "hash": "ff1e79c1589f8602",
            "authors": [
                "Yiming Jia",
                "Jiachen Li",
                "Xiang Yue",
                "Bo Li",
                "Ping Nie",
                "Kai Zou",
                "Wenhu Chen"
            ],
            "affiliations": [
                "CMU",
                "Independent",
                "NUS",
                "Netmind.ai",
                "UC Santa Barbara",
                "University of Toronto",
                "University of Waterloo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.10582.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#data",
                    "#reasoning",
                    "#multimodal"
                ],
                "emoji": "ðŸ§ ",
                "ru": {
                    "title": "VisualWebInstruct: Ð¿Ñ€Ð¾Ñ€Ñ‹Ð² Ð² Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ð¸ Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸ÑŽ",
                    "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»Ð¸ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð¸Ð»Ð¸ VisualWebInstruct - Ð½Ð¾Ð²Ñ‹Ð¹ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ Ðº ÑÐ¾Ð·Ð´Ð°Ð½Ð¸ÑŽ Ñ€Ð°Ð·Ð½Ð¾Ð¾Ð±Ñ€Ð°Ð·Ð½Ð¾Ð³Ð¾ Ð¸ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²ÐµÐ½Ð½Ð¾Ð³Ð¾ Ð½Ð°Ð±Ð¾Ñ€Ð° Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð¼Ð°ÑˆÐ¸Ð½Ð½Ð¾Ð³Ð¾ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ. Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑ Ð¿Ð¾Ð¸ÑÐºÐ¾Ð²ÑƒÑŽ ÑÐ¸ÑÑ‚ÐµÐ¼Ñƒ Ð¸ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÑƒ HTML-ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†, Ð¾Ð½Ð¸ ÑÐ¾Ð±Ñ€Ð°Ð»Ð¸ Ð¾ÐºÐ¾Ð»Ð¾ 900 Ñ‚Ñ‹ÑÑÑ‡ Ð¿Ð°Ñ€ Ð²Ð¾Ð¿Ñ€Ð¾Ñ-Ð¾Ñ‚Ð²ÐµÑ‚ Ð¿Ð¾ Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ñ‹Ð¼ Ð´Ð¸ÑÑ†Ð¸Ð¿Ð»Ð¸Ð½Ð°Ð¼. ÐœÐ¾Ð´ÐµÐ»Ð¸, Ð´Ð¾Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð½Ñ‹Ðµ Ð½Ð° ÑÑ‚Ð¾Ð¼ Ð½Ð°Ð±Ð¾Ñ€Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ…, Ð¿Ð¾ÐºÐ°Ð·Ð°Ð»Ð¸ Ð·Ð½Ð°Ñ‡Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ð¹ Ð¿Ñ€Ð¸Ñ€Ð¾ÑÑ‚ Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸ Ð½Ð° Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¸Ñ… Ð±ÐµÐ½Ñ‡Ð¼Ð°Ñ€ÐºÐ°Ñ…. Ð›ÑƒÑ‡ÑˆÐ°Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒ MAmmoTH-VL2 Ð¿Ñ€Ð¾Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð¸Ñ€Ð¾Ð²Ð°Ð»Ð° state-of-the-art Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð² ÑÐ²Ð¾ÐµÐ¼ ÐºÐ»Ð°ÑÑÐµ Ð½Ð° Ð·Ð°Ð´Ð°Ñ‡Ð°Ñ… Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ñ Ð¸ Ñ€ÐµÑˆÐµÐ½Ð¸Ñ Ð·Ð°Ð´Ð°Ñ‡."
                },
                "en": {
                    "title": "Enhancing Reasoning in Vision-Language Models with VisualWebInstruct",
                    "desc": "This paper introduces VisualWebInstruct, a new dataset aimed at improving the reasoning abilities of Vision-Language Models (VLMs). The dataset is created by leveraging search engines to gather diverse and high-quality multimodal data, specifically focusing on reasoning tasks across various disciplines. By processing over 700,000 unique web sources, the authors compile approximately 900,000 question-answer pairs, enhancing the training data available for VLMs. The results show that models trained on this dataset achieve significant performance improvements on reasoning benchmarks, demonstrating its effectiveness in advancing VLM capabilities."
                },
                "zh": {
                    "title": "æå‡è§†è§‰è¯­è¨€æ¨¡åž‹æŽ¨ç†èƒ½åŠ›çš„æ–°æ•°æ®é›†",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•VisualWebInstructï¼Œæ—¨åœ¨è§£å†³æŽ¨ç†å¯¼å‘çš„å¤šæ¨¡æ€æ•°æ®é›†ç¨€ç¼ºé—®é¢˜ã€‚æˆ‘ä»¬åˆ©ç”¨æœç´¢å¼•æ“Žåˆ›å»ºäº†ä¸€ä¸ªå¤šå­¦ç§‘çš„é«˜è´¨é‡æ•°æ®é›†ï¼ŒåŒ…æ‹¬æ•°å­¦ã€ç‰©ç†ã€é‡‘èžå’ŒåŒ–å­¦ç­‰é¢†åŸŸã€‚é€šè¿‡ä»Ž30,000ä¸ªç§å­å›¾åƒå¼€å§‹ï¼Œæ”¶é›†å’Œå¤„ç†è¶…è¿‡70ä¸‡ä¸ªç‹¬ç‰¹ç½‘å€çš„HTMLå†…å®¹ï¼Œæˆ‘ä»¬æž„å»ºäº†çº¦90ä¸‡ä¸ªé—®ç­”å¯¹çš„æ•°æ®é›†ã€‚ç»è¿‡åœ¨VisualWebInstructä¸Šå¾®è°ƒçš„æ¨¡åž‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨çŽ°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¯æ˜Žäº†è¯¥æ•°æ®é›†åœ¨å¢žå¼ºè§†è§‰è¯­è¨€æ¨¡åž‹æŽ¨ç†èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.10618",
            "title": "DiT-Air: Revisiting the Efficiency of Diffusion Model Architecture\n  Design in Text to Image Generation",
            "url": "https://huggingface.co/papers/2503.10618",
            "abstract": "In this work, we empirically study Diffusion Transformers (DiTs) for text-to-image generation, focusing on architectural choices, text-conditioning strategies, and training protocols. We evaluate a range of DiT-based architectures--including PixArt-style and MMDiT variants--and compare them with a standard DiT variant which directly processes concatenated text and noise inputs. Surprisingly, our findings reveal that the performance of standard DiT is comparable with those specialized models, while demonstrating superior parameter-efficiency, especially when scaled up. Leveraging the layer-wise parameter sharing strategy, we achieve a further reduction of 66% in model size compared to an MMDiT architecture, with minimal performance impact. Building on an in-depth analysis of critical components such as text encoders and Variational Auto-Encoders (VAEs), we introduce DiT-Air and DiT-Air-Lite. With supervised and reward fine-tuning, DiT-Air achieves state-of-the-art performance on GenEval and T2I CompBench, while DiT-Air-Lite remains highly competitive, surpassing most existing models despite its compact size.",
            "score": 9,
            "issue_id": 2704,
            "pub_date": "2025-03-13",
            "pub_date_card": {
                "ru": "13 Ð¼Ð°Ñ€Ñ‚Ð°",
                "en": "March 13",
                "zh": "3æœˆ13æ—¥"
            },
            "hash": "3e0a293a0bdb9c8c",
            "authors": [
                "Chen Chen",
                "Rui Qian",
                "Wenze Hu",
                "Tsu-Jui Fu",
                "Lezhi Li",
                "Bowen Zhang",
                "Alex Schwing",
                "Wei Liu",
                "Yinfei Yang"
            ],
            "affiliations": [
                "Apple Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.10618.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#architecture",
                    "#dataset",
                    "#training"
                ],
                "emoji": "ðŸ–¼ï¸",
                "ru": {
                    "title": "Ð­Ñ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ñ‹Ðµ Ð”Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ð¾Ð½Ð½Ñ‹Ðµ Ð¢Ñ€Ð°Ð½ÑÑ„Ð¾Ñ€Ð¼ÐµÑ€Ñ‹ Ð´Ð»Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹",
                    "desc": "Ð’ ÑÑ‚Ð¾Ð¹ Ñ€Ð°Ð±Ð¾Ñ‚Ðµ Ð¸ÑÑÐ»ÐµÐ´ÑƒÑŽÑ‚ÑÑ Ð”Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ð¾Ð½Ð½Ñ‹Ðµ Ð¢Ñ€Ð°Ð½ÑÑ„Ð¾Ñ€Ð¼ÐµÑ€Ñ‹ (DiT) Ð´Ð»Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹ Ð¿Ð¾ Ñ‚ÐµÐºÑÑ‚Ñƒ. ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ Ð¸Ð·ÑƒÑ‡Ð°ÑŽÑ‚ Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ñ‹Ðµ Ð°Ñ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ñ‹ DiT, Ð²ÐºÐ»ÑŽÑ‡Ð°Ñ Ð²Ð°Ñ€Ð¸Ð°Ð½Ñ‚Ñ‹ PixArt Ð¸ MMDiT, Ð¸ ÑÑ€Ð°Ð²Ð½Ð¸Ð²Ð°ÑŽÑ‚ Ð¸Ñ… ÑÐ¾ ÑÑ‚Ð°Ð½Ð´Ð°Ñ€Ñ‚Ð½Ñ‹Ð¼ DiT. Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÑŽÑ‚, Ñ‡Ñ‚Ð¾ ÑÑ‚Ð°Ð½Ð´Ð°Ñ€Ñ‚Ð½Ñ‹Ð¹ DiT ÑÐ¾Ð¿Ð¾ÑÑ‚Ð°Ð²Ð¸Ð¼ Ð¿Ð¾ Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸ ÑÐ¾ ÑÐ¿ÐµÑ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¼Ð¸ Ð¼Ð¾Ð´ÐµÐ»ÑÐ¼Ð¸, Ð½Ð¾ Ð±Ð¾Ð»ÐµÐµ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²ÐµÐ½ Ð¿Ð¾ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð°Ð¼. Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑ ÑÑ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸ÑŽ Ð¿Ð¾ÑÐ»Ð¾Ð¹Ð½Ð¾Ð³Ð¾ Ñ€Ð°Ð·Ð´ÐµÐ»ÐµÐ½Ð¸Ñ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð², Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»Ð¸ Ð´Ð¾Ð±Ð¸Ð²Ð°ÑŽÑ‚ÑÑ Ð´Ð°Ð»ÑŒÐ½ÐµÐ¹ÑˆÐµÐ³Ð¾ ÑƒÐ¼ÐµÐ½ÑŒÑˆÐµÐ½Ð¸Ñ Ñ€Ð°Ð·Ð¼ÐµÑ€Ð° Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð½Ð° 66% Ð¿Ð¾ ÑÑ€Ð°Ð²Ð½ÐµÐ½Ð¸ÑŽ Ñ MMDiT."
                },
                "en": {
                    "title": "Efficient Image Generation with Diffusion Transformers",
                    "desc": "This paper investigates Diffusion Transformers (DiTs) for generating images from text, examining different architectural designs and training methods. The authors compare various DiT models, including specialized versions and a standard model that combines text and noise inputs. They find that the standard DiT performs similarly to more complex models while being more efficient in terms of parameters, especially when scaled. Additionally, they introduce new models, DiT-Air and DiT-Air-Lite, which achieve top performance on benchmark tests while maintaining a smaller size."
                },
                "zh": {
                    "title": "æ‰©æ•£å˜æ¢å™¨ï¼šé«˜æ•ˆçš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ",
                    "desc": "æœ¬æ–‡ç ”ç©¶äº†æ‰©æ•£å˜æ¢å™¨ï¼ˆDiTsï¼‰åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­çš„åº”ç”¨ï¼Œé‡ç‚¹å…³æ³¨æž¶æž„é€‰æ‹©ã€æ–‡æœ¬æ¡ä»¶ç­–ç•¥å’Œè®­ç»ƒåè®®ã€‚æˆ‘ä»¬è¯„ä¼°äº†å¤šç§åŸºäºŽDiTçš„æž¶æž„ï¼ŒåŒ…æ‹¬PixArté£Žæ ¼å’ŒMMDiTå˜ä½“ï¼Œå¹¶ä¸Žç›´æŽ¥å¤„ç†æ–‡æœ¬å’Œå™ªå£°è¾“å…¥çš„æ ‡å‡†DiTå˜ä½“è¿›è¡Œäº†æ¯”è¾ƒã€‚ç ”ç©¶ç»“æžœè¡¨æ˜Žï¼Œæ ‡å‡†DiTçš„æ€§èƒ½ä¸Žè¿™äº›ä¸“ä¸šæ¨¡åž‹ç›¸å½“ï¼ŒåŒæ—¶åœ¨å‚æ•°æ•ˆçŽ‡ä¸Šè¡¨çŽ°æ›´ä½³ï¼Œå°¤å…¶æ˜¯åœ¨æ¨¡åž‹è§„æ¨¡å¢žå¤§æ—¶ã€‚é€šè¿‡å±‚çº§å‚æ•°å…±äº«ç­–ç•¥ï¼Œæˆ‘ä»¬å°†æ¨¡åž‹å¤§å°è¿›ä¸€æ­¥å‡å°‘äº†66%ï¼Œå¯¹æ€§èƒ½å½±å“æžå°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.09642",
            "title": "Open-Sora 2.0: Training a Commercial-Level Video Generation Model in\n  $200k",
            "url": "https://huggingface.co/papers/2503.09642",
            "abstract": "Video generation models have achieved remarkable progress in the past year. The quality of AI video continues to improve, but at the cost of larger model size, increased data quantity, and greater demand for training compute. In this report, we present Open-Sora 2.0, a commercial-level video generation model trained for only $200k. With this model, we demonstrate that the cost of training a top-performing video generation model is highly controllable. We detail all techniques that contribute to this efficiency breakthrough, including data curation, model architecture, training strategy, and system optimization. According to human evaluation results and VBench scores, Open-Sora 2.0 is comparable to global leading video generation models including the open-source HunyuanVideo and the closed-source Runway Gen-3 Alpha. By making Open-Sora 2.0 fully open-source, we aim to democratize access to advanced video generation technology, fostering broader innovation and creativity in content creation. All resources are publicly available at: https://github.com/hpcaitech/Open-Sora.",
            "score": 9,
            "issue_id": 2701,
            "pub_date": "2025-03-12",
            "pub_date_card": {
                "ru": "12 Ð¼Ð°Ñ€Ñ‚Ð°",
                "en": "March 12",
                "zh": "3æœˆ12æ—¥"
            },
            "hash": "8987bc0bbdde3b5a",
            "authors": [
                "Xiangyu Peng",
                "Zangwei Zheng",
                "Chenhui Shen",
                "Tom Young",
                "Xinying Guo",
                "Binluo Wang",
                "Hang Xu",
                "Hongxin Liu",
                "Mingyan Jiang",
                "Wenjun Li",
                "Yuhui Wang",
                "Anbang Ye",
                "Gang Ren",
                "Qianran Ma",
                "Wanying Liang",
                "Xiang Lian",
                "Xiwen Wu",
                "Yuting Zhong",
                "Zhuangyan Li",
                "Chaoyu Gong",
                "Guojun Lei",
                "Leijun Cheng",
                "Limin Zhang",
                "Minghao Li",
                "Ruijie Zhang",
                "Silan Hu",
                "Shijie Huang",
                "Xiaokang Wang",
                "Yuanheng Zhao",
                "Yuqi Wang",
                "Ziang Wei",
                "Yang You"
            ],
            "affiliations": [
                "HPC-AI Tech"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.09642.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#open_source",
                    "#architecture",
                    "#video",
                    "#data"
                ],
                "emoji": "ðŸŽ¬",
                "ru": {
                    "title": "Ð”Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð°Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð²Ð¸Ð´ÐµÐ¾: ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð¼Ð¸Ñ€Ð¾Ð²Ð¾Ð³Ð¾ ÑƒÑ€Ð¾Ð²Ð½Ñ Ð¿Ð¾ Ñ€Ð°Ð·ÑƒÐ¼Ð½Ð¾Ð¹ Ñ†ÐµÐ½Ðµ",
                    "desc": "Open-Sora 2.0 - ÑÑ‚Ð¾ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð²Ð¸Ð´ÐµÐ¾, Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð½Ð°Ñ Ð²ÑÐµÐ³Ð¾ Ð·Ð° $200 Ñ‚Ñ‹Ñ. ÐžÐ½Ð° Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð¸Ñ€ÑƒÐµÑ‚, Ñ‡Ñ‚Ð¾ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð²Ñ‹ÑÐ¾ÐºÐ¾Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð²Ð¸Ð´ÐµÐ¾ Ð¼Ð¾Ð¶ÐµÑ‚ Ð±Ñ‹Ñ‚ÑŒ ÑÐºÐ¾Ð½Ð¾Ð¼Ð¸Ñ‡ÐµÑÐºÐ¸ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ñ‹Ð¼. Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»Ð¸ Ð¿Ñ€Ð¸Ð¼ÐµÐ½Ð¸Ð»Ð¸ Ñ€ÑÐ´ Ñ‚ÐµÑ…Ð½Ð¸Ðº Ð´Ð»Ñ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸, Ð²ÐºÐ»ÑŽÑ‡Ð°Ñ ÐºÑƒÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ…, Ð°Ñ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ñƒ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð¸ ÑÑ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸ÑŽ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ. ÐŸÐ¾ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°Ð¼ Ñ‡ÐµÐ»Ð¾Ð²ÐµÑ‡ÐµÑÐºÐ¾Ð¹ Ð¾Ñ†ÐµÐ½ÐºÐ¸ Ð¸ Ð¿Ð¾ÐºÐ°Ð·Ð°Ñ‚ÐµÐ»ÑÐ¼ VBench, Open-Sora 2.0 ÑÐ¾Ð¿Ð¾ÑÑ‚Ð°Ð²Ð¸Ð¼Ð° Ñ Ð²ÐµÐ´ÑƒÑ‰Ð¸Ð¼Ð¸ Ð¼Ð¾Ð´ÐµÐ»ÑÐ¼Ð¸ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð²Ð¸Ð´ÐµÐ¾."
                },
                "en": {
                    "title": "Affordable Excellence in Video Generation",
                    "desc": "Open-Sora 2.0 is a new video generation model that has been developed to produce high-quality videos while keeping training costs low. It was trained for only $200,000, showcasing that effective video generation can be achieved without massive resources. The model incorporates various techniques such as optimized data curation, innovative model architecture, and efficient training strategies to enhance performance. By making Open-Sora 2.0 open-source, the authors aim to provide wider access to advanced video generation tools, encouraging creativity and innovation in the field."
                },
                "zh": {
                    "title": "Open-Sora 2.0ï¼šé«˜æ•ˆè§†é¢‘ç”Ÿæˆçš„æœªæ¥",
                    "desc": "è§†é¢‘ç”Ÿæˆæ¨¡åž‹åœ¨è¿‡åŽ»ä¸€å¹´å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚è™½ç„¶AIè§†é¢‘çš„è´¨é‡ä¸æ–­æé«˜ï¼Œä½†è¿™ä¹Ÿå¯¼è‡´äº†æ¨¡åž‹è§„æ¨¡å¢žå¤§ã€æ•°æ®é‡å¢žåŠ å’Œè®­ç»ƒè®¡ç®—éœ€æ±‚åŠ å¤§ã€‚æˆ‘ä»¬ä»‹ç»äº†Open-Sora 2.0ï¼Œè¿™æ˜¯ä¸€ä¸ªå•†ä¸šçº§çš„è§†é¢‘ç”Ÿæˆæ¨¡åž‹ï¼Œä»…ç”¨20ä¸‡ç¾Žå…ƒè¿›è¡Œè®­ç»ƒã€‚é€šè¿‡ä¼˜åŒ–æ•°æ®ç®¡ç†ã€æ¨¡åž‹æž¶æž„ã€è®­ç»ƒç­–ç•¥å’Œç³»ç»Ÿæ€§èƒ½ï¼Œæˆ‘ä»¬å±•ç¤ºäº†é«˜æ•ˆè®­ç»ƒé¡¶çº§è§†é¢‘ç”Ÿæˆæ¨¡åž‹çš„å¯æŽ§æ€§ï¼Œå¹¶å¸Œæœ›é€šè¿‡å¼€æºè¿™ä¸€æ¨¡åž‹æ¥ä¿ƒè¿›å†…å®¹åˆ›ä½œçš„åˆ›æ–°ä¸Žå‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.10460",
            "title": "Light-R1: Curriculum SFT, DPO and RL for Long COT from Scratch and\n  Beyond",
            "url": "https://huggingface.co/papers/2503.10460",
            "abstract": "This paper presents our work on the Light-R1 series, with models, data, and code all released.   We first focus on training long COT models from scratch, specifically starting from models initially lacking long COT capabilities. Using a curriculum training recipe consisting of two-stage SFT and semi-on-policy DPO, we train our model Light-R1-32B from Qwen2.5-32B-Instruct, resulting in superior math performance compared to DeepSeek-R1-Distill-Qwen-32B. Despite being trained exclusively on math data, Light-R1-32B shows strong generalization across other domains. In the subsequent phase of this work, we highlight the significant benefit of the 3k dataset constructed for the second SFT stage on enhancing other models. By fine-tuning DeepSeek-R1-Distilled models using this dataset, we obtain new SOTA models in 7B and 14B, while the 32B model, Light-R1-32B-DS performed comparably to QwQ-32B and DeepSeek-R1.   Furthermore, we extend our work by applying reinforcement learning, specifically GRPO, on long-COT models to further improve reasoning performance. We successfully train our final Light-R1-14B-DS with RL, achieving SOTA performance among 14B parameter models in math. With AIME24 & 25 scores of 74.0 and 60.2 respectively, Light-R1-14B-DS surpasses even many 32B models and DeepSeek-R1-Distill-Llama-70B. Its RL training also exhibits well expected behavior, showing simultaneous increase in response length and reward score.   The Light-R1 series of work validates training long-COT models from scratch, showcases the art in SFT data and releases SOTA models from RL.",
            "score": 8,
            "issue_id": 2701,
            "pub_date": "2025-03-13",
            "pub_date_card": {
                "ru": "13 Ð¼Ð°Ñ€Ñ‚Ð°",
                "en": "March 13",
                "zh": "3æœˆ13æ—¥"
            },
            "hash": "503c1d89e949bb41",
            "authors": [
                "Liang Wen",
                "Yunke Cai",
                "Fenrui Xiao",
                "Xin He",
                "Qi An",
                "Zhenyu Duan",
                "Yimin Du",
                "Junchen Liu",
                "Lifu Tang",
                "Xiaowei Lv",
                "Haosheng Zou",
                "Yongchao Deng",
                "Shousheng Jia",
                "Xiangzheng Zhang"
            ],
            "affiliations": [
                "Qiyuan Tech",
                "Renmin University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.10460.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#dataset",
                    "#rl",
                    "#reasoning",
                    "#training",
                    "#optimization",
                    "#long_context",
                    "#open_source"
                ],
                "emoji": "ðŸ§ ",
                "ru": {
                    "title": "ÐŸÑ€Ð¾Ñ€Ñ‹Ð² Ð² Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ð¸ Ð˜Ð˜ Ð´Ð»Ð¸Ð½Ð½Ñ‹Ð¼ Ñ†ÐµÐ¿Ð¾Ñ‡ÐºÐ°Ð¼ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹",
                    "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ ÑÐµÑ€Ð¸ÑŽ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Light-R1, Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð½Ñ‹Ñ… Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÑÑ‚ÑŒ Ð´Ð»Ð¸Ð½Ð½Ñ‹Ðµ Ñ†ÐµÐ¿Ð¾Ñ‡ÐºÐ¸ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹ (COT) Ñ Ð½ÑƒÐ»Ñ. Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑ Ð´Ð²ÑƒÑ…ÑÑ‚Ð°Ð¿Ð½Ð¾Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ñ ÑƒÑ‡Ð¸Ñ‚ÐµÐ»ÐµÐ¼ (SFT) Ð¸ Ð¿Ð¾Ð»Ñƒ-Ð¾Ð½Ð»Ð°Ð¹Ð½Ð¾Ð²Ð¾Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ñ Ð¿Ñ€ÐµÐ´Ð¿Ð¾Ñ‡Ñ‚ÐµÐ½Ð¸ÑÐ¼Ð¸ (DPO), Ð°Ð²Ñ‚Ð¾Ñ€Ñ‹ Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ð»Ð¸ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Light-R1-32B, Ð¿Ñ€ÐµÐ²Ð¾ÑÑ…Ð¾Ð´ÑÑ‰ÑƒÑŽ Ð°Ð½Ð°Ð»Ð¾Ð³Ð¸ Ð² Ð¼Ð°Ñ‚ÐµÐ¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… Ð·Ð°Ð´Ð°Ñ‡Ð°Ñ…. Ð”Ð°Ð»ÑŒÐ½ÐµÐ¹ÑˆÐµÐµ Ð¿Ñ€Ð¸Ð¼ÐµÐ½ÐµÐ½Ð¸Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ñ Ð¿Ð¾Ð´ÐºÑ€ÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼ (RL) Ð¿Ð¾Ð·Ð²Ð¾Ð»Ð¸Ð»Ð¾ ÑÐ¾Ð·Ð´Ð°Ñ‚ÑŒ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Light-R1-14B-DS, Ð´Ð¾ÑÑ‚Ð¸Ð³ÑˆÑƒÑŽ Ð½Ð°Ð¸Ð»ÑƒÑ‡ÑˆÐ¸Ñ… Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð² ÑÑ€ÐµÐ´Ð¸ 14B Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð² Ð¼Ð°Ñ‚ÐµÐ¼Ð°Ñ‚Ð¸ÐºÐµ. Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¿Ð¾Ð´Ñ‚Ð²ÐµÑ€Ð¶Ð´Ð°ÐµÑ‚ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾ÑÑ‚ÑŒ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð´Ð»Ð¸Ð½Ð½Ñ‹Ð¼ COT Ñ Ð½ÑƒÐ»Ñ Ð¸ Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð¸Ñ€ÑƒÐµÑ‚ Ð²Ð°Ð¶Ð½Ð¾ÑÑ‚ÑŒ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ SFT."
                },
                "en": {
                    "title": "Revolutionizing Long COT Models with Light-R1 Series",
                    "desc": "This paper introduces the Light-R1 series, focusing on training long Chain of Thought (COT) models from scratch. The authors employ a two-stage Supervised Fine-Tuning (SFT) and semi-on-policy Direct Preference Optimization (DPO) to enhance the model's math capabilities, resulting in the Light-R1-32B model that outperforms existing models. They also demonstrate that a specially constructed 3k dataset significantly boosts the performance of other models, achieving state-of-the-art (SOTA) results in various parameter sizes. Additionally, the application of reinforcement learning (RL) further improves reasoning performance, with the Light-R1-14B-DS model achieving impressive scores in math tasks, surpassing even larger models."
                },
                "zh": {
                    "title": "ä»Žé›¶å¼€å§‹è®­ç»ƒé•¿é“¾æŽ¨ç†æ¨¡åž‹çš„æˆåŠŸä¹‹è·¯",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†Light-R1ç³»åˆ—çš„ç ”ç©¶å·¥ä½œï¼Œå‘å¸ƒäº†æ¨¡åž‹ã€æ•°æ®å’Œä»£ç ã€‚æˆ‘ä»¬ä¸“æ³¨äºŽä»Žå¤´å¼€å§‹è®­ç»ƒé•¿é“¾æŽ¨ç†ï¼ˆCOTï¼‰æ¨¡åž‹ï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µçš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’ŒåŠåœ¨çº¿ç­–ç•¥ä¼˜åŒ–ï¼ˆDPOï¼‰è¿›è¡Œè¯¾ç¨‹è®­ç»ƒã€‚Light-R1-32Bæ¨¡åž‹åœ¨æ•°å­¦æ€§èƒ½ä¸Šä¼˜äºŽDeepSeek-R1-Distill-Qwen-32Bï¼Œå°½ç®¡ä»…åœ¨æ•°å­¦æ•°æ®ä¸Šè®­ç»ƒï¼Œä½†åœ¨å…¶ä»–é¢†åŸŸä¹Ÿè¡¨çŽ°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›ä¸€æ­¥æå‡æŽ¨ç†æ€§èƒ½ï¼Œæˆ‘ä»¬æˆåŠŸè®­ç»ƒäº†Light-R1-14B-DSï¼Œè¾¾åˆ°äº†14Bå‚æ•°æ¨¡åž‹ä¸­çš„æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.10357",
            "title": "Do I look like a `cat.n.01` to you? A Taxonomy Image Generation\n  Benchmark",
            "url": "https://huggingface.co/papers/2503.10357",
            "abstract": "This paper explores the feasibility of using text-to-image models in a zero-shot setup to generate images for taxonomy concepts. While text-based methods for taxonomy enrichment are well-established, the potential of the visual dimension remains unexplored. To address this, we propose a comprehensive benchmark for Taxonomy Image Generation that assesses models' abilities to understand taxonomy concepts and generate relevant, high-quality images. The benchmark includes common-sense and randomly sampled WordNet concepts, alongside the LLM generated predictions. The 12 models are evaluated using 9 novel taxonomy-related text-to-image metrics and human feedback. Moreover, we pioneer the use of pairwise evaluation with GPT-4 feedback for image generation. Experimental results show that the ranking of models differs significantly from standard T2I tasks. Playground-v2 and FLUX consistently outperform across metrics and subsets and the retrieval-based approach performs poorly. These findings highlight the potential for automating the curation of structured data resources.",
            "score": 8,
            "issue_id": 2705,
            "pub_date": "2025-03-13",
            "pub_date_card": {
                "ru": "13 Ð¼Ð°Ñ€Ñ‚Ð°",
                "en": "March 13",
                "zh": "3æœˆ13æ—¥"
            },
            "hash": "dad8c63e36f6d8c2",
            "authors": [
                "Viktor Moskvoretskii",
                "Alina Lobanova",
                "Ekaterina Neminova",
                "Chris Biemann",
                "Alexander Panchenko",
                "Irina Nikishina"
            ],
            "affiliations": [
                "AIRI",
                "HSE University",
                "Skoltech",
                "University of Hamburg"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.10357.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "ðŸŒ³",
                "ru": {
                    "title": "Ð’Ð¸Ð·ÑƒÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ñ‚Ð°ÐºÑÐ¾Ð½Ð¾Ð¼Ð¸Ð¹: Ð½Ð¾Ð²Ñ‹Ð¹ Ñ€ÑƒÐ±ÐµÐ¶ Ð² Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹",
                    "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¸ÑÑÐ»ÐµÐ´ÑƒÐµÑ‚ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾ÑÑ‚ÑŒ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð¿Ñ€ÐµÐ¾Ð±Ñ€Ð°Ð·Ð¾Ð²Ð°Ð½Ð¸Ñ Ñ‚ÐµÐºÑÑ‚Ð° Ð² Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ Ð´Ð»Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð²Ð¸Ð·ÑƒÐ°Ð»ÑŒÐ½Ð¾Ð³Ð¾ ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚Ð° Ð´Ð»Ñ Ñ‚Ð°ÐºÑÐ¾Ð½Ð¾Ð¼Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… ÐºÐ¾Ð½Ñ†ÐµÐ¿Ñ†Ð¸Ð¹. ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ Ð¿Ñ€ÐµÐ´Ð»Ð°Ð³Ð°ÑŽÑ‚ ÐºÐ¾Ð¼Ð¿Ð»ÐµÐºÑÐ½Ñ‹Ð¹ Ð±ÐµÐ½Ñ‡Ð¼Ð°Ñ€Ðº Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑ‚Ð¸ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð¿Ð¾Ð½Ð¸Ð¼Ð°Ñ‚ÑŒ Ñ‚Ð°ÐºÑÐ¾Ð½Ð¾Ð¼Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ ÐºÐ¾Ð½Ñ†ÐµÐ¿Ñ†Ð¸Ð¸ Ð¸ Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ñ€ÐµÐ»ÐµÐ²Ð°Ð½Ñ‚Ð½Ñ‹Ðµ, ÐºÐ°Ñ‡ÐµÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ðµ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ. Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð²ÐºÐ»ÑŽÑ‡Ð°ÐµÑ‚ Ð¾Ñ†ÐµÐ½ÐºÑƒ 12 Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð¿Ð¾ 9 Ð½Ð¾Ð²Ñ‹Ð¼ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ°Ð¼, ÑÐ²ÑÐ·Ð°Ð½Ð½Ñ‹Ð¼ Ñ Ñ‚Ð°ÐºÑÐ¾Ð½Ð¾Ð¼Ð¸ÐµÐ¹, Ð° Ñ‚Ð°ÐºÐ¶Ðµ Ð¾Ð±Ñ€Ð°Ñ‚Ð½ÑƒÑŽ ÑÐ²ÑÐ·ÑŒ Ð¾Ñ‚ Ð»ÑŽÐ´ÐµÐ¹ Ð¸ GPT-4. Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÑŽÑ‚, Ñ‡Ñ‚Ð¾ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Playground-v2 Ð¸ FLUX Ð¿Ñ€ÐµÐ²Ð¾ÑÑ…Ð¾Ð´ÑÑ‚ Ð´Ñ€ÑƒÐ³Ð¸Ðµ Ð² ÑÑ‚Ð¾Ð¹ Ð·Ð°Ð´Ð°Ñ‡Ðµ, Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð¸Ñ€ÑƒÑ Ð¿Ð¾Ñ‚ÐµÐ½Ñ†Ð¸Ð°Ð» Ð´Ð»Ñ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ð¸ ÐºÑƒÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Unlocking Visual Understanding in Taxonomy with Zero-Shot Image Generation",
                    "desc": "This paper investigates how well text-to-image models can create images based on taxonomy concepts without prior training on those specific concepts, known as a zero-shot setup. It introduces a new benchmark for evaluating these models, focusing on their ability to generate relevant and high-quality images that align with taxonomy terms. The study employs innovative metrics and human feedback to assess the performance of 12 different models, revealing that their effectiveness varies significantly from traditional text-to-image tasks. The results suggest that certain models, like Playground-v2 and FLUX, excel in this context, indicating a promising avenue for automating the enhancement of structured data resources."
                },
                "zh": {
                    "title": "æŽ¢ç´¢é›¶æ ·æœ¬ä¸‹çš„åˆ†ç±»å›¾åƒç”Ÿæˆæ½œåŠ›",
                    "desc": "æœ¬æ–‡æŽ¢è®¨äº†åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹ä½¿ç”¨æ–‡æœ¬åˆ°å›¾åƒæ¨¡åž‹ç”Ÿæˆåˆ†ç±»æ¦‚å¿µå›¾åƒçš„å¯è¡Œæ€§ã€‚è™½ç„¶åŸºäºŽæ–‡æœ¬çš„æ–¹æ³•åœ¨åˆ†ç±»ä¸°å¯Œæ–¹é¢å·²ç»ç›¸å½“æˆç†Ÿï¼Œä½†è§†è§‰ç»´åº¦çš„æ½œåŠ›å°šæœªè¢«å……åˆ†æŒ–æŽ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„åˆ†ç±»å›¾åƒç”ŸæˆåŸºå‡†ï¼Œè¯„ä¼°æ¨¡åž‹ç†è§£åˆ†ç±»æ¦‚å¿µå’Œç”Ÿæˆç›¸å…³é«˜è´¨é‡å›¾åƒçš„èƒ½åŠ›ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼Œæ¨¡åž‹çš„æŽ’åä¸Žæ ‡å‡†çš„æ–‡æœ¬åˆ°å›¾åƒä»»åŠ¡æœ‰æ˜¾è‘—ä¸åŒï¼Œå¼ºè°ƒäº†è‡ªåŠ¨åŒ–ç»“æž„åŒ–æ•°æ®èµ„æºæ•´ç†çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.09641",
            "title": "SANA-Sprint: One-Step Diffusion with Continuous-Time Consistency\n  Distillation",
            "url": "https://huggingface.co/papers/2503.09641",
            "abstract": "This paper presents SANA-Sprint, an efficient diffusion model for ultra-fast text-to-image (T2I) generation. SANA-Sprint is built on a pre-trained foundation model and augmented with hybrid distillation, dramatically reducing inference steps from 20 to 1-4. We introduce three key innovations: (1) We propose a training-free approach that transforms a pre-trained flow-matching model for continuous-time consistency distillation (sCM), eliminating costly training from scratch and achieving high training efficiency. Our hybrid distillation strategy combines sCM with latent adversarial distillation (LADD): sCM ensures alignment with the teacher model, while LADD enhances single-step generation fidelity. (2) SANA-Sprint is a unified step-adaptive model that achieves high-quality generation in 1-4 steps, eliminating step-specific training and improving efficiency. (3) We integrate ControlNet with SANA-Sprint for real-time interactive image generation, enabling instant visual feedback for user interaction. SANA-Sprint establishes a new Pareto frontier in speed-quality tradeoffs, achieving state-of-the-art performance with 7.59 FID and 0.74 GenEval in only 1 step - outperforming FLUX-schnell (7.94 FID / 0.71 GenEval) while being 10x faster (0.1s vs 1.1s on H100). It also achieves 0.1s (T2I) and 0.25s (ControlNet) latency for 1024 x 1024 images on H100, and 0.31s (T2I) on an RTX 4090, showcasing its exceptional efficiency and potential for AI-powered consumer applications (AIPC). Code and pre-trained models will be open-sourced.",
            "score": 7,
            "issue_id": 2702,
            "pub_date": "2025-03-12",
            "pub_date_card": {
                "ru": "12 Ð¼Ð°Ñ€Ñ‚Ð°",
                "en": "March 12",
                "zh": "3æœˆ12æ—¥"
            },
            "hash": "4079b7fe9f67a246",
            "authors": [
                "Junsong Chen",
                "Shuchen Xue",
                "Yuyang Zhao",
                "Jincheng Yu",
                "Sayak Paul",
                "Junyu Chen",
                "Han Cai",
                "Enze Xie",
                "Song Han"
            ],
            "affiliations": [
                "Huggingface",
                "Independent Researcher",
                "MIT",
                "NVIDIA",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.09641.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#training",
                    "#inference",
                    "#cv",
                    "#open_source"
                ],
                "emoji": "ðŸš€",
                "ru": {
                    "title": "ÐœÐ¾Ð»Ð½Ð¸ÐµÐ½Ð¾ÑÐ½Ð°Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹ Ñ SANA-Sprint",
                    "desc": "SANA-Sprint - ÑÑ‚Ð¾ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð°Ñ Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ð¾Ð½Ð½Ð°Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð´Ð»Ñ ÑÐ²ÐµÑ€Ñ…Ð±Ñ‹ÑÑ‚Ñ€Ð¾Ð¹ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹ Ð¿Ð¾ Ñ‚ÐµÐºÑÑ‚Ñƒ. ÐžÐ½Ð° Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ Ð¿Ñ€ÐµÐ´Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð½ÑƒÑŽ Ð±Ð°Ð·Ð¾Ð²ÑƒÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð¸ Ð³Ð¸Ð±Ñ€Ð¸Ð´Ð½ÑƒÑŽ Ð´Ð¸ÑÑ‚Ð¸Ð»Ð»ÑÑ†Ð¸ÑŽ, ÑÐ¾ÐºÑ€Ð°Ñ‰Ð°Ñ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ ÑˆÐ°Ð³Ð¾Ð² Ð²Ñ‹Ð²Ð¾Ð´Ð° Ñ 20 Ð´Ð¾ 1-4. ÐšÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ðµ Ð¸Ð½Ð½Ð¾Ð²Ð°Ñ†Ð¸Ð¸ Ð²ÐºÐ»ÑŽÑ‡Ð°ÑŽÑ‚ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ Ð±ÐµÐ· Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð´Ð»Ñ Ð½ÐµÐ¿Ñ€ÐµÑ€Ñ‹Ð²Ð½Ð¾Ð¹ ÐºÐ¾Ð½ÑÐ¸ÑÑ‚ÐµÐ½Ñ‚Ð½Ð¾Ð¹ Ð´Ð¸ÑÑ‚Ð¸Ð»Ð»ÑÑ†Ð¸Ð¸, ÑƒÐ½Ð¸Ñ„Ð¸Ñ†Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½ÑƒÑŽ Ð°Ð´Ð°Ð¿Ñ‚Ð¸Ð²Ð½ÑƒÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð¸ Ð¸Ð½Ñ‚ÐµÐ³Ñ€Ð°Ñ†Ð¸ÑŽ Ñ ControlNet Ð´Ð»Ñ Ð¸Ð½Ñ‚ÐµÑ€Ð°ÐºÑ‚Ð¸Ð²Ð½Ð¾Ð¹ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸. SANA-Sprint Ð´Ð¾ÑÑ‚Ð¸Ð³Ð°ÐµÑ‚ state-of-the-art Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð² Ð¿Ð¾ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ°Ð¼ FID Ð¸ GenEval Ð·Ð° 1 ÑˆÐ°Ð³, Ð¿Ñ€ÐµÐ²Ð¾ÑÑ…Ð¾Ð´Ñ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ðµ Ñ€ÐµÑˆÐµÐ½Ð¸Ñ Ð¿Ð¾ ÑÐºÐ¾Ñ€Ð¾ÑÑ‚Ð¸ Ð¸ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ñƒ."
                },
                "en": {
                    "title": "SANA-Sprint: Ultra-Fast Text-to-Image Generation Revolutionized!",
                    "desc": "This paper introduces SANA-Sprint, a novel diffusion model designed for rapid text-to-image (T2I) generation. By leveraging a pre-trained foundation model and employing hybrid distillation techniques, SANA-Sprint significantly reduces the number of inference steps required, achieving high-quality image generation in just 1-4 steps. The model integrates a training-free approach and combines continuous-time consistency distillation with latent adversarial distillation to enhance generation fidelity. Additionally, SANA-Sprint incorporates ControlNet for real-time interactive image generation, setting a new standard in the speed-quality tradeoff for T2I models."
                },
                "zh": {
                    "title": "SANA-Sprintï¼šè¶…å¿«é€Ÿæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„é©å‘½æ€§æ¨¡åž‹",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†SANA-Sprintï¼Œä¸€ç§é«˜æ•ˆçš„æ‰©æ•£æ¨¡åž‹ï¼Œç”¨äºŽè¶…å¿«é€Ÿçš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€‚SANA-SprintåŸºäºŽé¢„è®­ç»ƒçš„åŸºç¡€æ¨¡åž‹ï¼Œå¹¶é€šè¿‡æ··åˆè’¸é¦æŠ€æœ¯æ˜¾è‘—å‡å°‘æŽ¨ç†æ­¥éª¤ï¼Œä»Ž20æ­¥å‡å°‘åˆ°1-4æ­¥ã€‚è¯¥æ¨¡åž‹çš„ä¸‰é¡¹å…³é”®åˆ›æ–°åŒ…æ‹¬æ— è®­ç»ƒçš„æµåŒ¹é…æ¨¡åž‹è½¬æ¢ã€ç»Ÿä¸€çš„æ­¥é€‚åº”æ¨¡åž‹ä»¥åŠä¸ŽControlNetçš„é›†æˆï¼Œå®žçŽ°å®žæ—¶äº¤äº’å¼å›¾åƒç”Ÿæˆã€‚SANA-Sprintåœ¨é€Ÿåº¦ä¸Žè´¨é‡çš„æƒè¡¡ä¸­å»ºç«‹äº†æ–°çš„Paretoå‰æ²¿ï¼Œä»¥1æ­¥ç”Ÿæˆè¾¾åˆ°7.59 FIDå’Œ0.74 GenEvalçš„æœ€ä½³æ€§èƒ½ï¼Œä¸”é€Ÿåº¦æ¯”FLUX-schnellå¿«10å€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.10637",
            "title": "Distilling Diversity and Control in Diffusion Models",
            "url": "https://huggingface.co/papers/2503.10637",
            "abstract": "Distilled diffusion models suffer from a critical limitation: reduced sample diversity compared to their base counterparts. In this work, we uncover that despite this diversity loss, distilled models retain the fundamental concept representations of base models. We demonstrate control distillation - where control mechanisms like Concept Sliders and LoRAs trained on base models can be seamlessly transferred to distilled models and vice-versa, effectively distilling control without any retraining. This preservation of representational structure prompted our investigation into the mechanisms of diversity collapse during distillation. To understand how distillation affects diversity, we introduce Diffusion Target (DT) Visualization, an analysis and debugging tool that reveals how models predict final outputs at intermediate steps. Through DT-Visualization, we identify generation artifacts, inconsistencies, and demonstrate that initial diffusion timesteps disproportionately determine output diversity, while later steps primarily refine details. Based on these insights, we introduce diversity distillation - a hybrid inference approach that strategically employs the base model for only the first critical timestep before transitioning to the efficient distilled model. Our experiments demonstrate that this simple modification not only restores the diversity capabilities from base to distilled models but surprisingly exceeds it, while maintaining nearly the computational efficiency of distilled inference, all without requiring additional training or model modifications. Our code and data are available at https://distillation.baulab.info",
            "score": 6,
            "issue_id": 2701,
            "pub_date": "2025-03-13",
            "pub_date_card": {
                "ru": "13 Ð¼Ð°Ñ€Ñ‚Ð°",
                "en": "March 13",
                "zh": "3æœˆ13æ—¥"
            },
            "hash": "5313c06d699a1a7f",
            "authors": [
                "Rohit Gandikota",
                "David Bau"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2503.10637.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#inference",
                    "#training",
                    "#optimization",
                    "#diffusion",
                    "#data"
                ],
                "emoji": "ðŸ”¬",
                "ru": {
                    "title": "Ð’Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ Ñ€Ð°Ð·Ð½Ð¾Ð¾Ð±Ñ€Ð°Ð·Ð¸Ñ Ð² Ð´Ð¸ÑÑ‚Ð¸Ð»Ð»Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ð¾Ð½Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÑÑ…",
                    "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¿Ð¾ÑÐ²ÑÑ‰ÐµÐ½Ð¾ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ðµ ÑƒÐ¼ÐµÐ½ÑŒÑˆÐµÐ½Ð¸Ñ Ñ€Ð°Ð·Ð½Ð¾Ð¾Ð±Ñ€Ð°Ð·Ð¸Ñ ÑÑÐ¼Ð¿Ð»Ð¾Ð² Ð² Ð´Ð¸ÑÑ‚Ð¸Ð»Ð»Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ð¾Ð½Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÑÑ… Ð¿Ð¾ ÑÑ€Ð°Ð²Ð½ÐµÐ½Ð¸ÑŽ Ñ Ð±Ð°Ð·Ð¾Ð²Ñ‹Ð¼Ð¸. ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ Ð¾Ð±Ð½Ð°Ñ€ÑƒÐ¶Ð¸Ð»Ð¸, Ñ‡Ñ‚Ð¾ Ð´Ð¸ÑÑ‚Ð¸Ð»Ð»Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÑŽÑ‚ Ñ„ÑƒÐ½Ð´Ð°Ð¼ÐµÐ½Ñ‚Ð°Ð»ÑŒÐ½Ñ‹Ðµ ÐºÐ¾Ð½Ñ†ÐµÐ¿Ñ‚ÑƒÐ°Ð»ÑŒÐ½Ñ‹Ðµ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð¸Ñ Ð±Ð°Ð·Ð¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹, Ñ‡Ñ‚Ð¾ Ð¿Ð¾Ð·Ð²Ð¾Ð»ÑÐµÑ‚ Ð¿ÐµÑ€ÐµÐ½Ð¾ÑÐ¸Ñ‚ÑŒ Ð¼ÐµÑ…Ð°Ð½Ð¸Ð·Ð¼Ñ‹ ÐºÐ¾Ð½Ñ‚Ñ€Ð¾Ð»Ñ Ð¼ÐµÐ¶Ð´Ñƒ Ð½Ð¸Ð¼Ð¸ Ð±ÐµÐ· Ð¿ÐµÑ€ÐµÐ¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ. Ð”Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð° Ð¿Ñ€Ð¸Ñ‡Ð¸Ð½ Ð¿Ð¾Ñ‚ÐµÑ€Ð¸ Ñ€Ð°Ð·Ð½Ð¾Ð¾Ð±Ñ€Ð°Ð·Ð¸Ñ Ð±Ñ‹Ð»Ð° Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð° Ñ‚ÐµÑ…Ð½Ð¸ÐºÐ° Ð²Ð¸Ð·ÑƒÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Diffusion Target, ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ Ð¿Ð¾ÐºÐ°Ð·Ð°Ð»Ð°, Ñ‡Ñ‚Ð¾ Ð½Ð°Ñ‡Ð°Ð»ÑŒÐ½Ñ‹Ðµ ÑˆÐ°Ð³Ð¸ Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ð¸ Ð½ÐµÐ¿Ñ€Ð¾Ð¿Ð¾Ñ€Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾ Ð²Ð»Ð¸ÑÑŽÑ‚ Ð½Ð° Ñ€Ð°Ð·Ð½Ð¾Ð¾Ð±Ñ€Ð°Ð·Ð¸Ðµ Ð²Ñ‹Ñ…Ð¾Ð´Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…. ÐÐ° Ð¾ÑÐ½Ð¾Ð²Ðµ ÑÑ‚Ð¸Ñ… Ð½Ð°Ð±Ð»ÑŽÐ´ÐµÐ½Ð¸Ð¹ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½ Ð¼ÐµÑ‚Ð¾Ð´ diversity distillation, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑŽÑ‰Ð¸Ð¹ Ð±Ð°Ð·Ð¾Ð²ÑƒÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð»Ñ Ð¿ÐµÑ€Ð²Ð¾Ð³Ð¾ ÐºÑ€Ð¸Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ ÑˆÐ°Ð³Ð°, Ñ‡Ñ‚Ð¾ Ð²Ð¾ÑÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÑ‚ Ð¸ Ð´Ð°Ð¶Ðµ Ð¿Ñ€ÐµÐ²Ð¾ÑÑ…Ð¾Ð´Ð¸Ñ‚ Ñ€Ð°Ð·Ð½Ð¾Ð¾Ð±Ñ€Ð°Ð·Ð¸Ðµ Ð±Ð°Ð·Ð¾Ð²Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð¿Ñ€Ð¸ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ð¸ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾ÑÑ‚Ð¸ Ð´Ð¸ÑÑ‚Ð¸Ð»Ð»Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ð³Ð¾ Ð²Ñ‹Ð²Ð¾Ð´Ð°."
                },
                "en": {
                    "title": "Restoring Diversity in Distilled Diffusion Models",
                    "desc": "This paper addresses the issue of reduced sample diversity in distilled diffusion models compared to their original versions. The authors reveal that distilled models still maintain the essential concept representations of their base models. They introduce a method called control distillation, which allows for the transfer of control mechanisms between models without retraining. Additionally, they present Diffusion Target (DT) Visualization to analyze how diversity is affected during the distillation process, leading to a new approach called diversity distillation that enhances output diversity while keeping computational efficiency intact."
                },
                "zh": {
                    "title": "æ¢å¤è’¸é¦æ¨¡åž‹çš„å¤šæ ·æ€§",
                    "desc": "æœ¬æ–‡æŽ¢è®¨äº†è’¸é¦æ‰©æ•£æ¨¡åž‹çš„ä¸€ä¸ªé‡è¦é™åˆ¶ï¼Œå³ä¸ŽåŸºç¡€æ¨¡åž‹ç›¸æ¯”ï¼Œæ ·æœ¬å¤šæ ·æ€§é™ä½Žã€‚å°½ç®¡å­˜åœ¨è¿™ç§å¤šæ ·æ€§æŸå¤±ï¼Œè’¸é¦æ¨¡åž‹ä»ç„¶ä¿ç•™äº†åŸºç¡€æ¨¡åž‹çš„åŸºæœ¬æ¦‚å¿µè¡¨ç¤ºã€‚æˆ‘ä»¬æå‡ºäº†æŽ§åˆ¶è’¸é¦çš„æ–¹æ³•ï¼Œå¯ä»¥å°†æŽ§åˆ¶æœºåˆ¶æ— ç¼è½¬ç§»åˆ°è’¸é¦æ¨¡åž‹ï¼Œå¹¶ä¸”ä¸éœ€è¦é‡æ–°è®­ç»ƒã€‚é€šè¿‡å¼•å…¥æ‰©æ•£ç›®æ ‡å¯è§†åŒ–å·¥å…·ï¼Œæˆ‘ä»¬åˆ†æžäº†è’¸é¦å¯¹å¤šæ ·æ€§çš„å½±å“ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„æ··åˆæŽ¨ç†æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒè®¡ç®—æ•ˆçŽ‡çš„åŒæ—¶æ¢å¤å¤šæ ·æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.10589",
            "title": "Long Context Tuning for Video Generation",
            "url": "https://huggingface.co/papers/2503.10589",
            "abstract": "Recent advances in video generation can produce realistic, minute-long single-shot videos with scalable diffusion transformers. However, real-world narrative videos require multi-shot scenes with visual and dynamic consistency across shots. In this work, we introduce Long Context Tuning (LCT), a training paradigm that expands the context window of pre-trained single-shot video diffusion models to learn scene-level consistency directly from data. Our method expands full attention mechanisms from individual shots to encompass all shots within a scene, incorporating interleaved 3D position embedding and an asynchronous noise strategy, enabling both joint and auto-regressive shot generation without additional parameters. Models with bidirectional attention after LCT can further be fine-tuned with context-causal attention, facilitating auto-regressive generation with efficient KV-cache. Experiments demonstrate single-shot models after LCT can produce coherent multi-shot scenes and exhibit emerging capabilities, including compositional generation and interactive shot extension, paving the way for more practical visual content creation. See https://guoyww.github.io/projects/long-context-video/ for more details.",
            "score": 6,
            "issue_id": 2702,
            "pub_date": "2025-03-13",
            "pub_date_card": {
                "ru": "13 Ð¼Ð°Ñ€Ñ‚Ð°",
                "en": "March 13",
                "zh": "3æœˆ13æ—¥"
            },
            "hash": "3850b6066f2b7160",
            "authors": [
                "Yuwei Guo",
                "Ceyuan Yang",
                "Ziyan Yang",
                "Zhibei Ma",
                "Zhijie Lin",
                "Zhenheng Yang",
                "Dahua Lin",
                "Lu Jiang"
            ],
            "affiliations": [
                "ByteDance",
                "ByteDance Seed",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.10589.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#3d",
                    "#video",
                    "#diffusion",
                    "#training"
                ],
                "emoji": "ðŸŽ¬",
                "ru": {
                    "title": "Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ ÑÐ¾Ð³Ð»Ð°ÑÐ¾Ð²Ð°Ð½Ð½Ñ‹Ñ… Ð¼Ð½Ð¾Ð³Ð¾ÐºÐ°Ð´Ñ€Ð¾Ð²Ñ‹Ñ… Ð²Ð¸Ð´ÐµÐ¾ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ Long Context Tuning",
                    "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð½Ð¾Ð²Ñ‹Ð¹ Ð¼ÐµÑ‚Ð¾Ð´ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð¿Ð¾Ð´ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸ÐµÐ¼ Long Context Tuning (LCT) Ð´Ð»Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¼Ð½Ð¾Ð³Ð¾ÐºÐ°Ð´Ñ€Ð¾Ð²Ñ‹Ñ… Ð²Ð¸Ð´ÐµÐ¾ Ñ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸ÐµÐ¼ ÑÐ¾Ð³Ð»Ð°ÑÐ¾Ð²Ð°Ð½Ð½Ð¾ÑÑ‚Ð¸ ÑÑ†ÐµÐ½. LCT Ñ€Ð°ÑÑˆÐ¸Ñ€ÑÐµÑ‚ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð½Ð¾Ðµ Ð¾ÐºÐ½Ð¾ Ð¿Ñ€ÐµÐ´Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ð¸ Ð´Ð»Ñ Ð¾Ð´Ð¸Ð½Ð¾Ñ‡Ð½Ñ‹Ñ… ÐºÐ°Ð´Ñ€Ð¾Ð², Ð¿Ð¾Ð·Ð²Ð¾Ð»ÑÑ Ð¸Ð¼ ÑƒÑ‡Ð¸Ñ‚ÑŒÑÑ Ð½ÐµÐ¿Ð¾ÑÑ€ÐµÐ´ÑÑ‚Ð²ÐµÐ½Ð½Ð¾ Ð¸Ð· Ð´Ð°Ð½Ð½Ñ‹Ñ…. ÐœÐµÑ‚Ð¾Ð´ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ Ð¼ÐµÑ…Ð°Ð½Ð¸Ð·Ð¼Ñ‹ Ð¿Ð¾Ð»Ð½Ð¾Ð³Ð¾ Ð²Ð½Ð¸Ð¼Ð°Ð½Ð¸Ñ Ð¸ Ð°ÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð½ÑƒÑŽ ÑÑ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸ÑŽ ÑˆÑƒÐ¼Ð° Ð´Ð»Ñ ÑÐ¾Ð²Ð¼ÐµÑÑ‚Ð½Ð¾Ð¹ Ð¸ Ð°Ð²Ñ‚Ð¾Ñ€ÐµÐ³Ñ€ÐµÑÑÐ¸Ð²Ð½Ð¾Ð¹ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ ÐºÐ°Ð´Ñ€Ð¾Ð². Ð­ÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ñ‹ Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÑŽÑ‚, Ñ‡Ñ‚Ð¾ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð¿Ð¾ÑÐ»Ðµ LCT ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ñ‹ ÑÐ¾Ð·Ð´Ð°Ð²Ð°Ñ‚ÑŒ ÑÐ¾Ð³Ð»Ð°ÑÐ¾Ð²Ð°Ð½Ð½Ñ‹Ðµ Ð¼Ð½Ð¾Ð³Ð¾ÐºÐ°Ð´Ñ€Ð¾Ð²Ñ‹Ðµ ÑÑ†ÐµÐ½Ñ‹ Ð¸ Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð¸Ñ€ÑƒÑŽÑ‚ Ð½Ð¾Ð²Ñ‹Ðµ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾ÑÑ‚Ð¸, Ñ‚Ð°ÐºÐ¸Ðµ ÐºÐ°Ðº ÐºÐ¾Ð¼Ð¿Ð¾Ð·Ð¸Ñ†Ð¸Ð¾Ð½Ð½Ð°Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð¸ Ð¸Ð½Ñ‚ÐµÑ€Ð°ÐºÑ‚Ð¸Ð²Ð½Ð¾Ðµ Ñ€Ð°ÑÑˆÐ¸Ñ€ÐµÐ½Ð¸Ðµ ÐºÐ°Ð´Ñ€Ð¾Ð²."
                },
                "en": {
                    "title": "Enhancing Video Generation with Long Context Tuning",
                    "desc": "This paper presents Long Context Tuning (LCT), a new training approach for video generation models that enhances their ability to create multi-shot scenes with visual and dynamic consistency. By expanding the context window of pre-trained single-shot video diffusion models, LCT allows the model to learn from the entire scene rather than just individual shots. The method utilizes full attention mechanisms and incorporates advanced techniques like interleaved 3D position embedding and an asynchronous noise strategy. As a result, models trained with LCT can generate coherent multi-shot videos and demonstrate new capabilities such as compositional generation and interactive shot extension."
                },
                "zh": {
                    "title": "é•¿ä¸Šä¸‹æ–‡è°ƒä¼˜ï¼šæå‡è§†é¢‘ç”Ÿæˆçš„ä¸€è‡´æ€§ä¸Žè¿žè´¯æ€§",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒèŒƒå¼ï¼Œç§°ä¸ºé•¿ä¸Šä¸‹æ–‡è°ƒä¼˜ï¼ˆLCTï¼‰ï¼Œæ—¨åœ¨æé«˜è§†é¢‘ç”Ÿæˆæ¨¡åž‹åœ¨å¤šé•œå¤´åœºæ™¯ä¸­çš„ä¸€è‡´æ€§ã€‚é€šè¿‡æ‰©å±•é¢„è®­ç»ƒå•é•œå¤´è§†é¢‘æ‰©æ•£æ¨¡åž‹çš„ä¸Šä¸‹æ–‡çª—å£ï¼ŒLCTèƒ½å¤Ÿç›´æŽ¥ä»Žæ•°æ®ä¸­å­¦ä¹ åœºæ™¯çº§åˆ«çš„ä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å…¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œå°†æ³¨æ„åŠ›ä»Žå•ä¸ªé•œå¤´æ‰©å±•åˆ°æ•´ä¸ªåœºæ™¯ï¼Œç»“åˆäº†3Dä½ç½®åµŒå…¥å’Œå¼‚æ­¥å™ªå£°ç­–ç•¥ï¼Œå®žçŽ°äº†æ— é¢å¤–å‚æ•°çš„è”åˆå’Œè‡ªå›žå½’é•œå¤´ç”Ÿæˆã€‚å®žéªŒè¡¨æ˜Žï¼Œç»è¿‡LCTå¤„ç†çš„å•é•œå¤´æ¨¡åž‹èƒ½å¤Ÿç”Ÿæˆè¿žè´¯çš„å¤šé•œå¤´åœºæ™¯ï¼Œå¹¶å±•çŽ°å‡ºç»„åˆç”Ÿæˆå’Œäº¤äº’å¼é•œå¤´æ‰©å±•ç­‰æ–°èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.10630",
            "title": "UniGoal: Towards Universal Zero-shot Goal-oriented Navigation",
            "url": "https://huggingface.co/papers/2503.10630",
            "abstract": "In this paper, we propose a general framework for universal zero-shot goal-oriented navigation. Existing zero-shot methods build inference framework upon large language models (LLM) for specific tasks, which differs a lot in overall pipeline and fails to generalize across different types of goal. Towards the aim of universal zero-shot navigation, we propose a uniform graph representation to unify different goals, including object category, instance image and text description. We also convert the observation of agent into an online maintained scene graph. With this consistent scene and goal representation, we preserve most structural information compared with pure text and are able to leverage LLM for explicit graph-based reasoning. Specifically, we conduct graph matching between the scene graph and goal graph at each time instant and propose different strategies to generate long-term goal of exploration according to different matching states. The agent first iteratively searches subgraph of goal when zero-matched. With partial matching, the agent then utilizes coordinate projection and anchor pair alignment to infer the goal location. Finally scene graph correction and goal verification are applied for perfect matching. We also present a blacklist mechanism to enable robust switch between stages. Extensive experiments on several benchmarks show that our UniGoal achieves state-of-the-art zero-shot performance on three studied navigation tasks with a single model, even outperforming task-specific zero-shot methods and supervised universal methods.",
            "score": 5,
            "issue_id": 2700,
            "pub_date": "2025-03-13",
            "pub_date_card": {
                "ru": "13 Ð¼Ð°Ñ€Ñ‚Ð°",
                "en": "March 13",
                "zh": "3æœˆ13æ—¥"
            },
            "hash": "9e7667a37c699f4c",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#reasoning",
                    "#long_context",
                    "#games",
                    "#graphs",
                    "#benchmark",
                    "#agents",
                    "#rl"
                ],
                "emoji": "ðŸ§­",
                "ru": {
                    "title": "Ð£Ð½Ð¸Ð²ÐµÑ€ÑÐ°Ð»ÑŒÐ½Ð°Ñ Ð½Ð°Ð²Ð¸Ð³Ð°Ñ†Ð¸Ñ Ñ Ð½ÑƒÐ»ÐµÐ²Ñ‹Ð¼ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸ÐµÐ¼ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð³Ñ€Ð°Ñ„Ð¾Ð² Ð¸ ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹",
                    "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ ÑƒÐ½Ð¸Ð²ÐµÑ€ÑÐ°Ð»ÑŒÐ½ÑƒÑŽ ÑÐ¸ÑÑ‚ÐµÐ¼Ñƒ Ð´Ð»Ñ Ð½Ð°Ð²Ð¸Ð³Ð°Ñ†Ð¸Ð¸ Ñ Ð½ÑƒÐ»ÐµÐ²Ñ‹Ð¼ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸ÐµÐ¼ Ðº Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ñ‹Ð¼ Ñ†ÐµÐ»ÑÐ¼. ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ Ð¿Ñ€ÐµÐ´Ð»Ð°Ð³Ð°ÑŽÑ‚ ÐµÐ´Ð¸Ð½Ð¾Ðµ Ð³Ñ€Ð°Ñ„Ð¾Ð²Ð¾Ðµ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð´Ð»Ñ ÑƒÐ½Ð¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ð¸ Ñ€Ð°Ð·Ð½Ñ‹Ñ… Ñ‚Ð¸Ð¿Ð¾Ð² Ñ†ÐµÐ»ÐµÐ¹ Ð¸ Ð½Ð°Ð±Ð»ÑŽÐ´ÐµÐ½Ð¸Ð¹ Ð°Ð³ÐµÐ½Ñ‚Ð°. Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ðµ ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð´Ð»Ñ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð³Ñ€Ð°Ñ„Ð¾Ð², ÑÐ¸ÑÑ‚ÐµÐ¼Ð° Ð¿Ñ€Ð¾Ð²Ð¾Ð´Ð¸Ñ‚ ÑÐ¾Ð¿Ð¾ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð³Ñ€Ð°Ñ„Ð° ÑÑ†ÐµÐ½Ñ‹ Ð¸ Ð³Ñ€Ð°Ñ„Ð° Ñ†ÐµÐ»Ð¸. ÐŸÑ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð½Ñ‹Ð¹ Ð¼ÐµÑ‚Ð¾Ð´ UniGoal Ð´Ð¾ÑÑ‚Ð¸Ð³Ð°ÐµÑ‚ Ð»ÑƒÑ‡ÑˆÐ¸Ñ… Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð² Ð² Ð·Ð°Ð´Ð°Ñ‡Ð°Ñ… Ð½Ð°Ð²Ð¸Ð³Ð°Ñ†Ð¸Ð¸ Ñ Ð½ÑƒÐ»ÐµÐ²Ñ‹Ð¼ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸ÐµÐ¼ Ð¿Ð¾ ÑÑ€Ð°Ð²Ð½ÐµÐ½Ð¸ÑŽ Ñ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ð¼Ð¸ Ð¿Ð¾Ð´Ñ…Ð¾Ð´Ð°Ð¼Ð¸."
                },
                "en": {
                    "title": "Navigating Goals Universally with Graphs!",
                    "desc": "This paper introduces a new framework for universal zero-shot goal-oriented navigation, which allows an agent to navigate towards various goals without prior training on specific tasks. Unlike existing methods that rely heavily on large language models (LLMs) tailored for individual tasks, this approach uses a uniform graph representation to integrate different types of goals, such as object categories and text descriptions. The agent maintains an online scene graph that captures the environment, enabling it to perform graph-based reasoning and match its observations with the goals. The proposed method demonstrates superior performance in navigation tasks, surpassing both task-specific and supervised approaches, by effectively managing goal exploration and verification through innovative strategies."
                },
                "zh": {
                    "title": "é€šç”¨é›¶-shotå¯¼èˆªçš„åˆ›æ–°æ¡†æž¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šç”¨çš„é›¶-shotç›®æ ‡å¯¼å‘å¯¼èˆªæ¡†æž¶ã€‚çŽ°æœ‰çš„é›¶-shotæ–¹æ³•é€šå¸¸ä¾èµ–äºŽå¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMï¼‰è¿›è¡Œç‰¹å®šä»»åŠ¡çš„æŽ¨ç†ï¼Œå¯¼è‡´åœ¨ä¸åŒç›®æ ‡ä¹‹é—´çš„æ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å›¾è¡¨ç¤ºæ–¹æ³•ï¼Œå°†ä¸åŒçš„ç›®æ ‡ï¼ˆå¦‚ç‰©ä½“ç±»åˆ«ã€å®žä¾‹å›¾åƒå’Œæ–‡æœ¬æè¿°ï¼‰æ•´åˆåœ¨ä¸€èµ·ï¼Œå¹¶å°†ä»£ç†çš„è§‚å¯Ÿç»“æžœè½¬æ¢ä¸ºåœ¨çº¿ç»´æŠ¤çš„åœºæ™¯å›¾ã€‚é€šè¿‡è¿™ç§ä¸€è‡´çš„åœºæ™¯å’Œç›®æ ‡è¡¨ç¤ºï¼Œæˆ‘ä»¬èƒ½å¤Ÿåˆ©ç”¨LLMè¿›è¡ŒåŸºäºŽå›¾çš„æŽ¨ç†ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å±•ç¤ºäº†æˆ‘ä»¬çš„UniGoalåœ¨é›¶-shotå¯¼èˆªä»»åŠ¡ä¸Šçš„ä¼˜è¶Šæ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.10615",
            "title": "R1-Onevision: Advancing Generalized Multimodal Reasoning through\n  Cross-Modal Formalization",
            "url": "https://huggingface.co/papers/2503.10615",
            "abstract": "Large Language Models have demonstrated remarkable reasoning capability in complex textual tasks. However, multimodal reasoning, which requires integrating visual and textual information, remains a significant challenge. Existing visual-language models often struggle to effectively analyze and reason visual content, resulting in suboptimal performance on complex reasoning tasks. Moreover, the absence of comprehensive benchmarks hinders the accurate assessment of multimodal reasoning capabilities. In this paper, we introduce R1-Onevision, a multimodal reasoning model designed to bridge the gap between visual perception and deep reasoning. To achieve this, we propose a cross-modal reasoning pipeline that transforms images into formal textural representations, enabling precise language-based reasoning. Leveraging this pipeline, we construct the R1-Onevision dataset which provides detailed, step-by-step multimodal reasoning annotations across diverse domains. We further develop the R1-Onevision model through supervised fine-tuning and reinforcement learning to cultivate advanced reasoning and robust generalization abilities. To comprehensively evaluate multimodal reasoning performance across different grades, we introduce R1-Onevision-Bench, a benchmark aligned with human educational stages, covering exams from junior high school to university and beyond. Experimental results show that R1-Onevision achieves state-of-the-art performance, outperforming models such as GPT-4o and Qwen2.5-VL on multiple challenging multimodal reasoning benchmarks.",
            "score": 5,
            "issue_id": 2701,
            "pub_date": "2025-03-13",
            "pub_date_card": {
                "ru": "13 Ð¼Ð°Ñ€Ñ‚Ð°",
                "en": "March 13",
                "zh": "3æœˆ13æ—¥"
            },
            "hash": "857ef8e4c110da7f",
            "authors": [
                "Yi Yang",
                "Xiaoxuan He",
                "Hongkun Pan",
                "Xiyan Jiang",
                "Yan Deng",
                "Xingtao Yang",
                "Haoyu Lu",
                "Dacheng Yin",
                "Fengyun Rao",
                "Minfeng Zhu",
                "Bo Zhang",
                "Wei Chen"
            ],
            "affiliations": [
                "Renmin University of China",
                "WeChat Vision, Tencent Inc.",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.10615.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multimodal",
                    "#dataset",
                    "#rl",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "ðŸ§ ",
                "ru": {
                    "title": "ÐŸÑ€ÐµÐ¾Ð´Ð¾Ð»ÐµÐ²Ð°Ñ Ð±Ð°Ñ€ÑŒÐµÑ€ Ð¼ÐµÐ¶Ð´Ñƒ Ð·Ñ€ÐµÐ½Ð¸ÐµÐ¼ Ð¸ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸ÐµÐ¼ Ð² Ð˜Ð˜",
                    "desc": "R1-Onevision - ÑÑ‚Ð¾ Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ð°Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹, Ð¿Ñ€ÐµÐ¾Ð´Ð¾Ð»ÐµÐ²Ð°ÑŽÑ‰Ð°Ñ Ñ€Ð°Ð·Ñ€Ñ‹Ð² Ð¼ÐµÐ¶Ð´Ñƒ Ð²Ð¸Ð·ÑƒÐ°Ð»ÑŒÐ½Ñ‹Ð¼ Ð²Ð¾ÑÐ¿Ñ€Ð¸ÑÑ‚Ð¸ÐµÐ¼ Ð¸ Ð³Ð»ÑƒÐ±Ð¾ÐºÐ¸Ð¼ Ð°Ð½Ð°Ð»Ð¸Ð·Ð¾Ð¼. ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ ÐºÐ¾Ð½Ð²ÐµÐ¹ÐµÑ€ ÐºÑ€Ð¾ÑÑ-Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹, Ð¿Ñ€ÐµÐ¾Ð±Ñ€Ð°Ð·ÑƒÑŽÑ‰Ð¸Ð¹ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ Ð² Ñ„Ð¾Ñ€Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ðµ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð¸Ñ Ð´Ð»Ñ Ñ‚Ð¾Ñ‡Ð½Ð¾Ð³Ð¾ ÑÐ·Ñ‹ÐºÐ¾Ð²Ð¾Ð³Ð¾ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°. ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ ÑÐ¾Ð·Ð´Ð°Ð»Ð¸ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚ R1-Onevision Ñ Ð¿Ð¾Ð´Ñ€Ð¾Ð±Ð½Ñ‹Ð¼Ð¸ Ð¿Ð¾ÑˆÐ°Ð³Ð¾Ð²Ñ‹Ð¼Ð¸ Ð°Ð½Ð½Ð¾Ñ‚Ð°Ñ†Ð¸ÑÐ¼Ð¸ Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹ Ð¸Ð· Ñ€Ð°Ð·Ð½Ñ‹Ñ… Ð¾Ð±Ð»Ð°ÑÑ‚ÐµÐ¹. ÐœÐ¾Ð´ÐµÐ»ÑŒ R1-Onevision, Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð½Ð°Ñ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ñ ÑƒÑ‡Ð¸Ñ‚ÐµÐ»ÐµÐ¼ Ð¸ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ñ Ð¿Ð¾Ð´ÐºÑ€ÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼, Ð¿Ñ€ÐµÐ²Ð¾ÑÑ…Ð¾Ð´Ð¸Ñ‚ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð½Ð° Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ñ‹Ñ… ÑÐ»Ð¾Ð¶Ð½Ñ‹Ñ… Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ñ‚ÐµÑÑ‚Ð°Ñ…."
                },
                "en": {
                    "title": "Bridging Visual and Textual Reasoning with R1-Onevision",
                    "desc": "This paper presents R1-Onevision, a new model for multimodal reasoning that combines visual and textual information. It addresses the limitations of existing visual-language models by introducing a cross-modal reasoning pipeline that converts images into structured text representations. The authors also create the R1-Onevision dataset, which includes detailed annotations for multimodal reasoning tasks across various domains. Additionally, they establish R1-Onevision-Bench, a benchmark for evaluating multimodal reasoning abilities at different educational levels, demonstrating that their model outperforms others like GPT-4o and Qwen2.5-VL."
                },
                "zh": {
                    "title": "R1-Onevisionï¼šè§†è§‰ä¸Žè¯­è¨€çš„å®Œç¾Žç»“åˆ",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºR1-Onevisionçš„å¤šæ¨¡æ€æŽ¨ç†æ¨¡åž‹ï¼Œæ—¨åœ¨è§£å†³è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯æ•´åˆçš„æŒ‘æˆ˜ã€‚è¯¥æ¨¡åž‹é€šè¿‡è·¨æ¨¡æ€æŽ¨ç†ç®¡é“ï¼Œå°†å›¾åƒè½¬æ¢ä¸ºæ­£å¼çš„æ–‡æœ¬è¡¨ç¤ºï¼Œä»Žè€Œå®žçŽ°ç²¾ç¡®çš„åŸºäºŽè¯­è¨€çš„æŽ¨ç†ã€‚æˆ‘ä»¬è¿˜æž„å»ºäº†R1-Onevisionæ•°æ®é›†ï¼Œæä¾›äº†è¯¦ç»†çš„å¤šæ¨¡æ€æŽ¨ç†æ³¨é‡Šï¼Œä»¥æ”¯æŒä¸åŒé¢†åŸŸçš„ç ”ç©¶ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒR1-Onevisionåœ¨å¤šä¸ªå¤æ‚çš„å¤šæ¨¡æ€æŽ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨çŽ°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†çŽ°æœ‰çš„å…ˆè¿›æ¨¡åž‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.10391",
            "title": "CINEMA: Coherent Multi-Subject Video Generation via MLLM-Based Guidance",
            "url": "https://huggingface.co/papers/2503.10391",
            "abstract": "Video generation has witnessed remarkable progress with the advent of deep generative models, particularly diffusion models. While existing methods excel in generating high-quality videos from text prompts or single images, personalized multi-subject video generation remains a largely unexplored challenge. This task involves synthesizing videos that incorporate multiple distinct subjects, each defined by separate reference images, while ensuring temporal and spatial consistency. Current approaches primarily rely on mapping subject images to keywords in text prompts, which introduces ambiguity and limits their ability to model subject relationships effectively. In this paper, we propose CINEMA, a novel framework for coherent multi-subject video generation by leveraging Multimodal Large Language Model (MLLM). Our approach eliminates the need for explicit correspondences between subject images and text entities, mitigating ambiguity and reducing annotation effort. By leveraging MLLM to interpret subject relationships, our method facilitates scalability, enabling the use of large and diverse datasets for training. Furthermore, our framework can be conditioned on varying numbers of subjects, offering greater flexibility in personalized content creation. Through extensive evaluations, we demonstrate that our approach significantly improves subject consistency, and overall video coherence, paving the way for advanced applications in storytelling, interactive media, and personalized video generation.",
            "score": 5,
            "issue_id": 2700,
            "pub_date": "2025-03-13",
            "pub_date_card": {
                "ru": "13 Ð¼Ð°Ñ€Ñ‚Ð°",
                "en": "March 13",
                "zh": "3æœˆ13æ—¥"
            },
            "hash": "d7f2c49b29951ace",
            "authors": [
                "Yufan Deng",
                "Xun Guo",
                "Yizhi Wang",
                "Jacob Zhiyuan Fang",
                "Angtian Wang",
                "Shenghai Yuan",
                "Yiding Yang",
                "Bo Liu",
                "Haibin Huang",
                "Chongyang Ma"
            ],
            "affiliations": [
                "ByteDance Intelligent Creation",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.10391.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#multimodal",
                    "#story_generation",
                    "#video",
                    "#diffusion"
                ],
                "emoji": "ðŸŽ¬",
                "ru": {
                    "title": "CINEMA: Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð°Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð²Ð¸Ð´ÐµÐ¾ Ñ Ð¼Ð½Ð¾Ð¶ÐµÑÑ‚Ð²Ð¾Ð¼ ÑÑƒÐ±ÑŠÐµÐºÑ‚Ð¾Ð² Ð¿Ñ€Ð¸ Ð¿Ð¾Ð¼Ð¾Ñ‰Ð¸ MLLM",
                    "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ CINEMA - Ð½Ð¾Ð²ÑƒÑŽ ÑÐ¸ÑÑ‚ÐµÐ¼Ñƒ Ð´Ð»Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð²Ð¸Ð´ÐµÐ¾ Ñ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¸Ð¼Ð¸ Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð¶Ð°Ð¼Ð¸, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑŽÑ‰ÑƒÑŽ Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½ÑƒÑŽ Ð±Ð¾Ð»ÑŒÑˆÑƒÑŽ ÑÐ·Ñ‹ÐºÐ¾Ð²ÑƒÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ (MLLM). CINEMA ÑƒÑÑ‚Ñ€Ð°Ð½ÑÐµÑ‚ Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ð¾ÑÑ‚ÑŒ Ð² ÑÐ²Ð½Ð¾Ð¼ ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²Ð¸Ð¸ Ð¼ÐµÐ¶Ð´Ñƒ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸ÑÐ¼Ð¸ ÑÑƒÐ±ÑŠÐµÐºÑ‚Ð¾Ð² Ð¸ Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ð¼Ð¸ ÑÑƒÑ‰Ð½Ð¾ÑÑ‚ÑÐ¼Ð¸, Ñ‡Ñ‚Ð¾ ÑƒÐ¼ÐµÐ½ÑŒÑˆÐ°ÐµÑ‚ Ð½ÐµÐ¾Ð´Ð½Ð¾Ð·Ð½Ð°Ñ‡Ð½Ð¾ÑÑ‚ÑŒ Ð¸ ÑƒÐ¿Ñ€Ð¾Ñ‰Ð°ÐµÑ‚ Ð°Ð½Ð½Ð¾Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ. Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð° Ð¿Ð¾Ð·Ð²Ð¾Ð»ÑÐµÑ‚ Ð¼Ð°ÑÑˆÑ‚Ð°Ð±Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¿Ñ€Ð¾Ñ†ÐµÑÑ, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ðµ Ð¸ Ñ€Ð°Ð·Ð½Ð¾Ð¾Ð±Ñ€Ð°Ð·Ð½Ñ‹Ðµ Ð½Ð°Ð±Ð¾Ñ€Ñ‹ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ. CINEMA Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð¸Ñ€ÑƒÐµÑ‚ Ð·Ð½Ð°Ñ‡Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ðµ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ðµ ÑÐ¾Ð³Ð»Ð°ÑÐ¾Ð²Ð°Ð½Ð½Ð¾ÑÑ‚Ð¸ Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð¶ÐµÐ¹ Ð¸ Ð¾Ð±Ñ‰ÐµÐ¹ ÑÐ²ÑÐ·Ð½Ð¾ÑÑ‚Ð¸ Ð²Ð¸Ð´ÐµÐ¾ Ð¿Ð¾ ÑÑ€Ð°Ð²Ð½ÐµÐ½Ð¸ÑŽ Ñ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ð¼Ð¸ Ð¼ÐµÑ‚Ð¾Ð´Ð°Ð¼Ð¸."
                },
                "en": {
                    "title": "CINEMA: Coherent Multi-Subject Video Generation Made Easy",
                    "desc": "This paper introduces CINEMA, a new framework for generating videos that feature multiple distinct subjects from separate reference images. Unlike traditional methods that rely on mapping images to text prompts, CINEMA uses a Multimodal Large Language Model (MLLM) to understand and interpret relationships between subjects, which reduces ambiguity. The framework allows for flexible conditioning on different numbers of subjects, making it easier to create personalized content. Extensive evaluations show that CINEMA enhances subject consistency and overall video coherence, opening up new possibilities for applications in storytelling and interactive media."
                },
                "zh": {
                    "title": "ä¸ªæ€§åŒ–å¤šä¸»ä½“è§†é¢‘ç”Ÿæˆçš„æ–°çªç ´",
                    "desc": "è§†é¢‘ç”Ÿæˆåœ¨æ·±åº¦ç”Ÿæˆæ¨¡åž‹ï¼Œç‰¹åˆ«æ˜¯æ‰©æ•£æ¨¡åž‹çš„æŽ¨åŠ¨ä¸‹å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚çŽ°æœ‰æ–¹æ³•åœ¨ä»Žæ–‡æœ¬æç¤ºæˆ–å•å¼ å›¾åƒç”Ÿæˆé«˜è´¨é‡è§†é¢‘æ–¹é¢è¡¨çŽ°å‡ºè‰²ï¼Œä½†ä¸ªæ€§åŒ–çš„å¤šä¸»ä½“è§†é¢‘ç”Ÿæˆä»ç„¶æ˜¯ä¸€ä¸ªæœªè¢«å……åˆ†æŽ¢ç´¢çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†CINEMAæ¡†æž¶ï¼Œé€šè¿‡åˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹ï¼ˆMLLMï¼‰ï¼Œå®žçŽ°äº†ä¸€è‡´çš„å¤šä¸»ä½“è§†é¢‘ç”Ÿæˆï¼Œæ¶ˆé™¤äº†ä¸»ä½“å›¾åƒä¸Žæ–‡æœ¬å®žä½“ä¹‹é—´çš„æ˜Žç¡®å¯¹åº”å…³ç³»ï¼Œä»Žè€Œå‡å°‘äº†æ­§ä¹‰å’Œæ ‡æ³¨å·¥ä½œã€‚æˆ‘ä»¬çš„æ¡†æž¶èƒ½å¤Ÿæ ¹æ®ä¸åŒæ•°é‡çš„ä¸»ä½“è¿›è¡Œæ¡ä»¶ç”Ÿæˆï¼Œæä¾›äº†æ›´å¤§çš„ä¸ªæ€§åŒ–å†…å®¹åˆ›ä½œçµæ´»æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.09905",
            "title": "Quantization for OpenAI's Whisper Models: A Comparative Analysis",
            "url": "https://huggingface.co/papers/2503.09905",
            "abstract": "Automated speech recognition (ASR) models have gained prominence for applications such as captioning, speech translation, and live transcription. This paper studies Whisper and two model variants: one optimized for live speech streaming and another for offline transcription. Notably, these models have been found to generate hallucinated content, reducing transcription reliability. Furthermore, larger model variants exhibit increased latency and pose challenges for deployment on resource-constrained devices. This study analyzes the similarities and differences between three Whisper models, qualitatively examining their distinct capabilities. Next, this study quantifies the impact of model quantization on latency and evaluates its viability for edge deployment. Using the open source LibriSpeech dataset, this paper evaluates the word error rate (WER) along with latency analysis of whispercpp using 3 quantization methods (INT4, INT5, INT8). Results show that quantization reduces latency by 19\\% and model size by 45\\%, while preserving transcription accuracy. These findings provide insights into the optimal use cases of different Whisper models and edge device deployment possibilities. All code, datasets, and implementation details are available in a public GitHub repository: https://github.com/allisonandreyev/WhisperQuantization.git",
            "score": 5,
            "issue_id": 2700,
            "pub_date": "2025-03-12",
            "pub_date_card": {
                "ru": "12 Ð¼Ð°Ñ€Ñ‚Ð°",
                "en": "March 12",
                "zh": "3æœˆ12æ—¥"
            },
            "hash": "8d7b17a97a4cbb6e",
            "authors": [
                "Allison Andreyev"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2503.09905.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#audio",
                    "#open_source",
                    "#optimization",
                    "#hallucinations",
                    "#dataset"
                ],
                "emoji": "ðŸŽ™ï¸",
                "ru": {
                    "title": "ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Whisper Ð´Ð»Ñ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾Ð³Ð¾ Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ð½Ð¸Ñ Ñ€ÐµÑ‡Ð¸ Ð½Ð° Ð¿ÐµÑ€Ð¸Ñ„ÐµÑ€Ð¸Ð¹Ð½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ð¾Ð¹ÑÑ‚Ð²Ð°Ñ…",
                    "desc": "Ð­Ñ‚Ð¾ Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÑ‚ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ð½Ð¸Ñ Ñ€ÐµÑ‡Ð¸ Whisper Ð¸ Ð¸Ñ… Ð²Ð°Ñ€Ð¸Ð°Ð½Ñ‚Ñ‹ Ð´Ð»Ñ Ð¿Ð¾Ñ‚Ð¾ÐºÐ¾Ð²Ð¾Ð¹ Ð¸ Ð¾Ñ„Ð»Ð°Ð¹Ð½-Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸Ð¸. ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ Ð¸Ð·ÑƒÑ‡Ð°ÑŽÑ‚ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñƒ Ð³Ð°Ð»Ð»ÑŽÑ†Ð¸Ð½Ð°Ñ†Ð¸Ð¹ Ð² Ð²Ñ‹Ð²Ð¾Ð´Ðµ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð¸ Ñ‚Ñ€ÑƒÐ´Ð½Ð¾ÑÑ‚Ð¸ Ñ€Ð°Ð·Ð²ÐµÑ€Ñ‚Ñ‹Ð²Ð°Ð½Ð¸Ñ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð½Ð° ÑƒÑÑ‚Ñ€Ð¾Ð¹ÑÑ‚Ð²Ð°Ñ… Ñ Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð½Ñ‹Ð¼Ð¸ Ñ€ÐµÑÑƒÑ€ÑÐ°Ð¼Ð¸. ÐŸÑ€Ð¾Ð²Ð¾Ð´Ð¸Ñ‚ÑÑ Ð¾Ñ†ÐµÐ½ÐºÐ° Ð²Ð»Ð¸ÑÐ½Ð¸Ñ ÐºÐ²Ð°Ð½Ñ‚Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð½Ð° Ð·Ð°Ð´ÐµÑ€Ð¶ÐºÑƒ Ð¸ Ñ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚ÑŒ Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸Ð¸ Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼ Ð½Ð°Ð±Ð¾Ñ€Ð° Ð´Ð°Ð½Ð½Ñ‹Ñ… LibriSpeech. Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÑŽÑ‚, Ñ‡Ñ‚Ð¾ ÐºÐ²Ð°Ð½Ñ‚Ð¾Ð²Ð°Ð½Ð¸Ðµ ÑƒÐ¼ÐµÐ½ÑŒÑˆÐ°ÐµÑ‚ Ð·Ð°Ð´ÐµÑ€Ð¶ÐºÑƒ Ð½Ð° 19% Ð¸ Ñ€Ð°Ð·Ð¼ÐµÑ€ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð½Ð° 45%, ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÑ Ð¿Ñ€Ð¸ ÑÑ‚Ð¾Ð¼ Ñ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚ÑŒ Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸Ð¸."
                },
                "en": {
                    "title": "Optimizing Whisper Models for Efficient Speech Recognition",
                    "desc": "This paper investigates the performance of Whisper, an automated speech recognition (ASR) model, along with its two variants tailored for live streaming and offline transcription. It highlights the issue of hallucinated content in the transcriptions, which can compromise reliability, especially in larger models that also face latency challenges. The study further explores the effects of model quantization on latency and size, demonstrating a significant reduction in both while maintaining transcription accuracy. By analyzing the word error rate (WER) using the LibriSpeech dataset, the research provides valuable insights for deploying Whisper models on resource-constrained devices."
                },
                "zh": {
                    "title": "ä¼˜åŒ–Whisperæ¨¡åž‹ï¼Œæå‡è¯­éŸ³è¯†åˆ«æ•ˆçŽ‡",
                    "desc": "è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡åž‹WhisperåŠå…¶ä¸¤ä¸ªå˜ä½“ï¼Œä¸€ä¸ªé’ˆå¯¹å®žæ—¶è¯­éŸ³æµä¼˜åŒ–ï¼Œå¦ä¸€ä¸ªç”¨äºŽç¦»çº¿è½¬å½•ã€‚ç ”ç©¶å‘çŽ°ï¼Œè¿™äº›æ¨¡åž‹å¯èƒ½ä¼šç”Ÿæˆè™šå‡å†…å®¹ï¼Œä»Žè€Œé™ä½Žè½¬å½•çš„å¯é æ€§ã€‚æ­¤å¤–ï¼Œè¾ƒå¤§çš„æ¨¡åž‹å˜ä½“åœ¨å»¶è¿Ÿæ–¹é¢è¡¨çŽ°è¾ƒå·®ï¼Œç»™èµ„æºå—é™çš„è®¾å¤‡éƒ¨ç½²å¸¦æ¥äº†æŒ‘æˆ˜ã€‚é€šè¿‡å¯¹æ¯”åˆ†æžï¼Œè®ºæ–‡é‡åŒ–äº†æ¨¡åž‹é‡åŒ–å¯¹å»¶è¿Ÿçš„å½±å“ï¼Œå¹¶è¯„ä¼°äº†å…¶åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šçš„å¯è¡Œæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.09799",
            "title": "Communication-Efficient Language Model Training Scales Reliably and\n  Robustly: Scaling Laws for DiLoCo",
            "url": "https://huggingface.co/papers/2503.09799",
            "abstract": "As we scale to more massive machine learning models, the frequent synchronization demands inherent in data-parallel approaches create significant slowdowns, posing a critical challenge to further scaling. Recent work develops an approach (DiLoCo) that relaxes synchronization demands without compromising model quality. However, these works do not carefully analyze how DiLoCo's behavior changes with model size. In this work, we study the scaling law behavior of DiLoCo when training LLMs under a fixed compute budget. We focus on how algorithmic factors, including number of model replicas, hyperparameters, and token budget affect training in ways that can be accurately predicted via scaling laws. We find that DiLoCo scales both predictably and robustly with model size. When well-tuned, DiLoCo scales better than data-parallel training with model size, and can outperform data-parallel training even at small model sizes. Our results showcase a more general set of benefits of DiLoCo than previously documented, including increased optimal batch sizes, improved downstream generalization with scale, and improved evaluation loss for a fixed token budget.",
            "score": 5,
            "issue_id": 2714,
            "pub_date": "2025-03-12",
            "pub_date_card": {
                "ru": "12 Ð¼Ð°Ñ€Ñ‚Ð°",
                "en": "March 12",
                "zh": "3æœˆ12æ—¥"
            },
            "hash": "78f500ddd43c6e65",
            "authors": [
                "Zachary Charles",
                "Gabriel Teston",
                "Lucio Dery",
                "Keith Rush",
                "Nova Fallen",
                "Zachary Garrett",
                "Arthur Szlam",
                "Arthur Douillard"
            ],
            "affiliations": [
                "Google DeepMind",
                "Google Research",
                "Google Search"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.09799.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization"
                ],
                "emoji": "ðŸ“ˆ",
                "ru": {
                    "title": "DiLoCo: Ð­Ñ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾Ðµ Ð¼Ð°ÑÑˆÑ‚Ð°Ð±Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹",
                    "desc": "Ð­Ñ‚Ð° ÑÑ‚Ð°Ñ‚ÑŒÑ Ð¸ÑÑÐ»ÐµÐ´ÑƒÐµÑ‚ Ð¼Ð°ÑÑˆÑ‚Ð°Ð±Ð¸Ñ€ÑƒÐµÐ¼Ð¾ÑÑ‚ÑŒ Ð¼ÐµÑ‚Ð¾Ð´Ð° DiLoCo Ð¿Ñ€Ð¸ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ð¸ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ (LLM) Ñ Ñ„Ð¸ÐºÑÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¼ Ð±ÑŽÐ´Ð¶ÐµÑ‚Ð¾Ð¼ Ð²Ñ‹Ñ‡Ð¸ÑÐ»ÐµÐ½Ð¸Ð¹. ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÑŽÑ‚, ÐºÐ°Ðº Ð°Ð»Ð³Ð¾Ñ€Ð¸Ñ‚Ð¼Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ñ„Ð°ÐºÑ‚Ð¾Ñ€Ñ‹, Ð²ÐºÐ»ÑŽÑ‡Ð°Ñ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ñ€ÐµÐ¿Ð»Ð¸Ðº Ð¼Ð¾Ð´ÐµÐ»Ð¸, Ð³Ð¸Ð¿ÐµÑ€Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ Ð¸ Ð±ÑŽÐ´Ð¶ÐµÑ‚ Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð², Ð²Ð»Ð¸ÑÑŽÑ‚ Ð½Ð° Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ, Ð¸ ÐºÐ°Ðº ÑÑ‚Ð¾ Ð¼Ð¾Ð¶Ð½Ð¾ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ñ‚ÑŒ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ Ð·Ð°ÐºÐ¾Ð½Ð¾Ð² Ð¼Ð°ÑÑˆÑ‚Ð°Ð±Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ. Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÑŽÑ‚, Ñ‡Ñ‚Ð¾ DiLoCo Ð¼Ð°ÑÑˆÑ‚Ð°Ð±Ð¸Ñ€ÑƒÐµÑ‚ÑÑ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·ÑƒÐµÐ¼Ð¾ Ð¸ Ð½Ð°Ð´ÐµÐ¶Ð½Ð¾ Ñ ÑƒÐ²ÐµÐ»Ð¸Ñ‡ÐµÐ½Ð¸ÐµÐ¼ Ñ€Ð°Ð·Ð¼ÐµÑ€Ð° Ð¼Ð¾Ð´ÐµÐ»Ð¸, Ð¿Ñ€ÐµÐ²Ð¾ÑÑ…Ð¾Ð´Ñ Ð¿Ð°Ñ€Ð°Ð»Ð»ÐµÐ»ÑŒÐ½Ð¾Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¿Ð¾ Ð´Ð°Ð½Ð½Ñ‹Ð¼ Ð´Ð°Ð¶Ðµ Ð´Ð»Ñ Ð½ÐµÐ±Ð¾Ð»ÑŒÑˆÐ¸Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹. Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð¸Ñ€ÑƒÐµÑ‚ Ð±Ð¾Ð»ÐµÐµ ÑˆÐ¸Ñ€Ð¾ÐºÐ¸Ð¹ ÑÐ¿ÐµÐºÑ‚Ñ€ Ð¿Ñ€ÐµÐ¸Ð¼ÑƒÑ‰ÐµÑÑ‚Ð² DiLoCo, Ð²ÐºÐ»ÑŽÑ‡Ð°Ñ ÑƒÐ²ÐµÐ»Ð¸Ñ‡ÐµÐ½Ð¸Ðµ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ñ€Ð°Ð·Ð¼ÐµÑ€Ð¾Ð² Ð±Ð°Ñ‚Ñ‡Ð° Ð¸ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ðµ Ð¾Ð±Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ Ð½Ð° downstream Ð·Ð°Ð´Ð°Ñ‡Ð°Ñ…."
                },
                "en": {
                    "title": "DiLoCo: Scaling Up Without the Sync Slowdown!",
                    "desc": "This paper investigates the performance of the DiLoCo approach for training large language models (LLMs) while minimizing synchronization delays. It analyzes how various factors, such as the number of model replicas and hyperparameters, influence the training process under a fixed compute budget. The authors demonstrate that DiLoCo not only scales effectively with model size but also outperforms traditional data-parallel training methods, even for smaller models. Their findings highlight the advantages of DiLoCo, including better batch sizes and enhanced generalization capabilities as model size increases."
                },
                "zh": {
                    "title": "DiLoCoï¼šè¶…è¶Šæ•°æ®å¹¶è¡Œçš„è®­ç»ƒæ–°æ–¹æ³•",
                    "desc": "éšç€æœºå™¨å­¦ä¹ æ¨¡åž‹è§„æ¨¡çš„æ‰©å¤§ï¼Œæ•°æ®å¹¶è¡Œæ–¹æ³•ä¸­é¢‘ç¹çš„åŒæ­¥éœ€æ±‚å¯¼è‡´æ˜¾è‘—çš„æ€§èƒ½ä¸‹é™ï¼Œæˆä¸ºè¿›ä¸€æ­¥æ‰©å±•çš„å…³é”®æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºDiLoCoçš„æ–¹æ³•ï¼Œå®ƒåœ¨ä¸å½±å“æ¨¡åž‹è´¨é‡çš„æƒ…å†µä¸‹ï¼Œæ”¾å®½äº†åŒæ­¥éœ€æ±‚ã€‚æˆ‘ä»¬ç ”ç©¶äº†åœ¨å›ºå®šè®¡ç®—é¢„ç®—ä¸‹ï¼ŒDiLoCoåœ¨è®­ç»ƒå¤§åž‹è¯­è¨€æ¨¡åž‹æ—¶çš„æ‰©å±•è§„å¾‹ï¼Œåˆ†æžäº†æ¨¡åž‹å‰¯æœ¬æ•°é‡ã€è¶…å‚æ•°å’Œä»¤ç‰Œé¢„ç®—ç­‰ç®—æ³•å› ç´ å¯¹è®­ç»ƒçš„å½±å“ã€‚ç ”ç©¶ç»“æžœè¡¨æ˜Žï¼ŒDiLoCoåœ¨æ¨¡åž‹è§„æ¨¡ä¸Šå…·æœ‰å¯é¢„æµ‹å’Œç¨³å¥çš„æ‰©å±•æ€§ï¼Œç»è¿‡è‰¯å¥½è°ƒä¼˜åŽï¼Œå…¶æ‰©å±•æ€§èƒ½ä¼˜äºŽæ•°æ®å¹¶è¡Œè®­ç»ƒï¼Œç”šè‡³åœ¨å°è§„æ¨¡æ¨¡åž‹ä¸­ä¹Ÿèƒ½è¶…è¶Šæ•°æ®å¹¶è¡Œè®­ç»ƒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.10568",
            "title": "Autoregressive Image Generation with Randomized Parallel Decoding",
            "url": "https://huggingface.co/papers/2503.10568",
            "abstract": "We introduce ARPG, a novel visual autoregressive model that enables randomized parallel generation, addressing the inherent limitations of conventional raster-order approaches, which hinder inference efficiency and zero-shot generalization due to their sequential, predefined token generation order. Our key insight is that effective random-order modeling necessitates explicit guidance for determining the position of the next predicted token. To this end, we propose a novel guided decoding framework that decouples positional guidance from content representation, encoding them separately as queries and key-value pairs. By directly incorporating this guidance into the causal attention mechanism, our approach enables fully random-order training and generation, eliminating the need for bidirectional attention. Consequently, ARPG readily generalizes to zero-shot tasks such as image inpainting, outpainting, and resolution expansion. Furthermore, it supports parallel inference by concurrently processing multiple queries using a shared KV cache. On the ImageNet-1K 256 benchmark, our approach attains an FID of 1.94 with only 64 sampling steps, achieving over a 20-fold increase in throughput while reducing memory consumption by over 75% compared to representative recent autoregressive models at a similar scale.",
            "score": 4,
            "issue_id": 2701,
            "pub_date": "2025-03-13",
            "pub_date_card": {
                "ru": "13 Ð¼Ð°Ñ€Ñ‚Ð°",
                "en": "March 13",
                "zh": "3æœˆ13æ—¥"
            },
            "hash": "44a1e551455870ac",
            "authors": [
                "Haopeng Li",
                "Jinyue Yang",
                "Guoqi Li",
                "Huan Wang"
            ],
            "affiliations": [
                "Institute of Automation, Chinese Academy of Sciences",
                "Westlake University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.10568.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#optimization",
                    "#cv",
                    "#architecture"
                ],
                "emoji": "ðŸŽ¨",
                "ru": {
                    "title": "ARPG: Ð ÐµÐ²Ð¾Ð»ÑŽÑ†Ð¸Ñ Ð² Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹ Ñ ÑÐ»ÑƒÑ‡Ð°Ð¹Ð½Ñ‹Ð¼ Ð¿Ð¾Ñ€ÑÐ´ÐºÐ¾Ð¼ Ð¸ Ð¿Ð°Ñ€Ð°Ð»Ð»ÐµÐ»ÑŒÐ½Ñ‹Ð¼ Ð²Ñ‹Ð²Ð¾Ð´Ð¾Ð¼",
                    "desc": "ARPG - ÑÑ‚Ð¾ Ð½Ð¾Ð²Ð°Ñ Ð²Ð¸Ð·ÑƒÐ°Ð»ÑŒÐ½Ð°Ñ Ð°Ð²Ñ‚Ð¾Ñ€ÐµÐ³Ñ€ÐµÑÑÐ¸Ð¾Ð½Ð½Ð°Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒ, Ð¿Ð¾Ð·Ð²Ð¾Ð»ÑÑŽÑ‰Ð°Ñ Ð¾ÑÑƒÑ‰ÐµÑÑ‚Ð²Ð»ÑÑ‚ÑŒ Ñ€Ð°Ð½Ð´Ð¾Ð¼Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½ÑƒÑŽ Ð¿Ð°Ñ€Ð°Ð»Ð»ÐµÐ»ÑŒÐ½ÑƒÑŽ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸ÑŽ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹. ÐžÐ½Ð° Ñ€ÐµÑˆÐ°ÐµÑ‚ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹ Ñ‚Ñ€Ð°Ð´Ð¸Ñ†Ð¸Ð¾Ð½Ð½Ñ‹Ñ… Ð¿Ð¾Ð´Ñ…Ð¾Ð´Ð¾Ð² Ñ Ñ„Ð¸ÐºÑÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¼ Ð¿Ð¾Ñ€ÑÐ´ÐºÐ¾Ð¼ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð², Ð¿Ñ€ÐµÐ´Ð»Ð°Ð³Ð°Ñ Ð¼ÐµÑ…Ð°Ð½Ð¸Ð·Ð¼ ÑƒÐ¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼Ð¾Ð³Ð¾ Ð´ÐµÐºÐ¾Ð´Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ñ€Ð°Ð·Ð´ÐµÐ»ÑÐµÑ‚ Ð¿Ð¾Ð·Ð¸Ñ†Ð¸Ð¾Ð½Ð½Ð¾Ðµ Ñ€ÑƒÐºÐ¾Ð²Ð¾Ð´ÑÑ‚Ð²Ð¾ Ð¸ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚Ð°. ARPG Ð¾Ð±ÑƒÑ‡Ð°ÐµÑ‚ÑÑ Ð¸ Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÑ‚ Ð² Ð¿Ð¾Ð»Ð½Ð¾ÑÑ‚ÑŒÑŽ ÑÐ»ÑƒÑ‡Ð°Ð¹Ð½Ð¾Ð¼ Ð¿Ð¾Ñ€ÑÐ´ÐºÐµ, Ñ‡Ñ‚Ð¾ Ð¿Ð¾Ð·Ð²Ð¾Ð»ÑÐµÑ‚ ÐµÐ¹ Ð»ÐµÐ³ÐºÐ¾ Ð°Ð´Ð°Ð¿Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒÑÑ Ðº Ð·Ð°Ð´Ð°Ñ‡Ð°Ð¼ Ð±ÐµÐ· Ð´Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ð³Ð¾ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ, Ñ‚Ð°ÐºÐ¸Ð¼ ÐºÐ°Ðº Ð·Ð°Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ Ð¸ Ñ€Ð°ÑÑˆÐ¸Ñ€ÐµÐ½Ð¸Ðµ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹. ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð¸Ñ€ÑƒÐµÑ‚ Ð²Ñ‹ÑÐ¾ÐºÑƒÑŽ Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ Ð¸ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾ÑÑ‚ÑŒ Ð¿Ð¾ ÑÑ€Ð°Ð²Ð½ÐµÐ½Ð¸ÑŽ Ñ Ð°Ð½Ð°Ð»Ð¾Ð³Ð°Ð¼Ð¸, Ð´Ð¾ÑÑ‚Ð¸Ð³Ð°Ñ FID 1.94 Ð½Ð° ImageNet-1K 256 Ð²ÑÐµÐ³Ð¾ Ð·Ð° 64 ÑˆÐ°Ð³Ð° ÑÑÐ¼Ð¿Ð»Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ."
                },
                "en": {
                    "title": "Revolutionizing Image Generation with Randomized Parallel Processing",
                    "desc": "The paper presents ARPG, a new visual autoregressive model designed for efficient and flexible image generation. Unlike traditional methods that generate tokens in a fixed order, ARPG allows for randomized parallel generation, improving inference speed and enabling zero-shot generalization. It introduces a guided decoding framework that separates positional guidance from content representation, enhancing the causal attention mechanism. As a result, ARPG excels in tasks like image inpainting and outpainting, achieving significant performance improvements on benchmarks while using less memory."
                },
                "zh": {
                    "title": "ARPGï¼šå®žçŽ°éšæœºå¹¶è¡Œç”Ÿæˆçš„è‡ªå›žå½’æ¨¡åž‹",
                    "desc": "æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„è§†è§‰è‡ªå›žå½’æ¨¡åž‹ARPGï¼Œèƒ½å¤Ÿå®žçŽ°éšæœºå¹¶è¡Œç”Ÿæˆï¼Œè§£å†³äº†ä¼ ç»Ÿå…‰æ …é¡ºåºæ–¹æ³•åœ¨æŽ¨ç†æ•ˆçŽ‡å’Œé›¶-shotæ³›åŒ–æ–¹é¢çš„å›ºæœ‰é™åˆ¶ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯ï¼Œæœ‰æ•ˆçš„éšæœºé¡ºåºå»ºæ¨¡éœ€è¦æ˜Žç¡®çš„æŒ‡å¯¼æ¥ç¡®å®šä¸‹ä¸€ä¸ªé¢„æµ‹æ ‡è®°çš„ä½ç½®ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¼•å¯¼è§£ç æ¡†æž¶ï¼Œå°†ä½ç½®æŒ‡å¯¼ä¸Žå†…å®¹è¡¨ç¤ºåˆ†å¼€ç¼–ç ï¼Œåˆ†åˆ«ä½œä¸ºæŸ¥è¯¢å’Œé”®å€¼å¯¹ã€‚é€šè¿‡å°†è¿™ç§æŒ‡å¯¼ç›´æŽ¥èžå…¥å› æžœæ³¨æ„åŠ›æœºåˆ¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®žçŽ°äº†å®Œå…¨éšæœºé¡ºåºçš„è®­ç»ƒå’Œç”Ÿæˆï¼Œæ¶ˆé™¤äº†å¯¹åŒå‘æ³¨æ„åŠ›çš„éœ€æ±‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.10636",
            "title": "The Curse of Conditions: Analyzing and Improving Optimal Transport for\n  Conditional Flow-Based Generation",
            "url": "https://huggingface.co/papers/2503.10636",
            "abstract": "Minibatch optimal transport coupling straightens paths in unconditional flow matching. This leads to computationally less demanding inference as fewer integration steps and less complex numerical solvers can be employed when numerically solving an ordinary differential equation at test time. However, in the conditional setting, minibatch optimal transport falls short. This is because the default optimal transport mapping disregards conditions, resulting in a conditionally skewed prior distribution during training. In contrast, at test time, we have no access to the skewed prior, and instead sample from the full, unbiased prior distribution. This gap between training and testing leads to a subpar performance. To bridge this gap, we propose conditional optimal transport C^2OT that adds a conditional weighting term in the cost matrix when computing the optimal transport assignment. Experiments demonstrate that this simple fix works with both discrete and continuous conditions in 8gaussians-to-moons, CIFAR-10, ImageNet-32x32, and ImageNet-256x256. Our method performs better overall compared to the existing baselines across different function evaluation budgets. Code is available at https://hkchengrex.github.io/C2OT",
            "score": 3,
            "issue_id": 2700,
            "pub_date": "2025-03-13",
            "pub_date_card": {
                "ru": "13 Ð¼Ð°Ñ€Ñ‚Ð°",
                "en": "March 13",
                "zh": "3æœˆ13æ—¥"
            },
            "hash": "93d6f410c35246be",
            "authors": [
                "Ho Kei Cheng",
                "Alexander Schwing"
            ],
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.10636.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#inference",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ðŸ”€",
                "ru": {
                    "title": "Ð£ÑÐ»Ð¾Ð²Ð½Ñ‹Ð¹ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ñ‚Ñ€Ð°Ð½ÑÐ¿Ð¾Ñ€Ñ‚ Ð´Ð»Ñ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ Ð¿Ð¾Ñ‚Ð¾ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹",
                    "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´Ð»Ð°Ð³Ð°ÐµÑ‚ Ð¼ÐµÑ‚Ð¾Ð´ ÑƒÑÐ»Ð¾Ð²Ð½Ð¾Ð³Ð¾ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ñ‚Ñ€Ð°Ð½ÑÐ¿Ð¾Ñ€Ñ‚Ð° (C^2OT) Ð´Ð»Ñ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð¿Ð¾Ñ‚Ð¾ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð² ÑƒÑÐ»Ð¾Ð²Ð½Ñ‹Ñ… Ð·Ð°Ð´Ð°Ñ‡Ð°Ñ…. ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ Ð²Ñ‹ÑÐ²Ð¸Ð»Ð¸ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñƒ Ð½ÐµÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²Ð¸Ñ Ð¼ÐµÐ¶Ð´Ñƒ Ñ‚Ñ€ÐµÐ½Ð¸Ñ€Ð¾Ð²Ð¾Ñ‡Ð½Ñ‹Ð¼ Ð¸ Ñ‚ÐµÑÑ‚Ð¾Ð²Ñ‹Ð¼ Ñ€Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸ÐµÐ¼ Ð¿Ñ€Ð¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ð¸ Ð¼Ð¸Ð½Ð¸-Ð±Ð°Ñ‚Ñ‡ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ñ‚Ñ€Ð°Ð½ÑÐ¿Ð¾Ñ€Ñ‚Ð° Ð² ÑƒÑÐ»Ð¾Ð²Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÑÑ…. C^2OT Ñ€ÐµÑˆÐ°ÐµÑ‚ ÑÑ‚Ñƒ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñƒ Ð¿ÑƒÑ‚ÐµÐ¼ Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ñ ÑƒÑÐ»Ð¾Ð²Ð½Ð¾Ð³Ð¾ Ð²ÐµÑÐ¾Ð²Ð¾Ð³Ð¾ ÐºÐ¾ÑÑ„Ñ„Ð¸Ñ†Ð¸ÐµÐ½Ñ‚Ð° Ð² Ð¼Ð°Ñ‚Ñ€Ð¸Ñ†Ñƒ ÑÑ‚Ð¾Ð¸Ð¼Ð¾ÑÑ‚Ð¸ Ð¿Ñ€Ð¸ Ð²Ñ‹Ñ‡Ð¸ÑÐ»ÐµÐ½Ð¸Ð¸ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ñ‚Ñ€Ð°Ð½ÑÐ¿Ð¾Ñ€Ñ‚Ð°. Ð­ÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ñ‹ Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÑŽÑ‚, Ñ‡Ñ‚Ð¾ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð½Ñ‹Ð¹ Ð¼ÐµÑ‚Ð¾Ð´ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ Ð»ÑƒÑ‡ÑˆÐµ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ñ… Ð±Ð°Ð·Ð¾Ð²Ñ‹Ñ… Ð»Ð¸Ð½Ð¸Ð¹ Ð½Ð° Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ñ‹Ñ… Ð½Ð°Ð±Ð¾Ñ€Ð°Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¸ Ð¿Ñ€Ð¸ Ñ€Ð°Ð·Ð½Ñ‹Ñ… Ð±ÑŽÐ´Ð¶ÐµÑ‚Ð°Ñ… Ð²Ñ‹Ñ‡Ð¸ÑÐ»ÐµÐ½Ð¸Ð¹."
                },
                "en": {
                    "title": "Bridging the Gap in Conditional Flow Matching with C^2OT",
                    "desc": "This paper introduces a method called conditional optimal transport (C^2OT) to improve the performance of flow matching in machine learning. The authors highlight that while minibatch optimal transport simplifies computations during inference, it fails in conditional settings due to a mismatch between training and testing distributions. By incorporating a conditional weighting term in the cost matrix, C^2OT effectively aligns the training and testing conditions. Experiments show that this approach outperforms existing methods across various datasets, demonstrating its effectiveness in both discrete and continuous scenarios."
                },
                "zh": {
                    "title": "æ¡ä»¶æœ€ä¼˜ä¼ è¾“ï¼šç¼©å°è®­ç»ƒä¸Žæµ‹è¯•çš„æ€§èƒ½å·®è·",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ¡ä»¶æœ€ä¼˜ä¼ è¾“æ–¹æ³•C^2OTï¼Œä»¥è§£å†³åœ¨æ¡ä»¶è®¾ç½®ä¸‹å°æ‰¹é‡æœ€ä¼˜ä¼ è¾“çš„ä¸è¶³ã€‚ä¼ ç»Ÿçš„æœ€ä¼˜ä¼ è¾“æ˜ å°„åœ¨è®­ç»ƒæ—¶å¿½ç•¥äº†æ¡ä»¶ï¼Œå¯¼è‡´è®­ç»ƒæœŸé—´çš„å…ˆéªŒåˆ†å¸ƒåæ–œï¼Œè€Œæµ‹è¯•æ—¶å´æ— æ³•è®¿é—®è¿™ç§åæ–œçš„å…ˆéªŒã€‚é€šè¿‡åœ¨æˆæœ¬çŸ©é˜µä¸­æ·»åŠ æ¡ä»¶åŠ æƒé¡¹ï¼ŒC^2OTèƒ½å¤Ÿæ›´å¥½åœ°è®¡ç®—æœ€ä¼˜ä¼ è¾“åˆ†é…ï¼Œä»Žè€Œç¼©å°è®­ç»ƒä¸Žæµ‹è¯•ä¹‹é—´çš„æ€§èƒ½å·®è·ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨çŽ°ä¼˜äºŽçŽ°æœ‰åŸºçº¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.09046",
            "title": "Discovering Influential Neuron Path in Vision Transformers",
            "url": "https://huggingface.co/papers/2503.09046",
            "abstract": "Vision Transformer models exhibit immense power yet remain opaque to human understanding, posing challenges and risks for practical applications. While prior research has attempted to demystify these models through input attribution and neuron role analysis, there's been a notable gap in considering layer-level information and the holistic path of information flow across layers. In this paper, we investigate the significance of influential neuron paths within vision Transformers, which is a path of neurons from the model input to output that impacts the model inference most significantly. We first propose a joint influence measure to assess the contribution of a set of neurons to the model outcome. And we further provide a layer-progressive neuron locating approach that efficiently selects the most influential neuron at each layer trying to discover the crucial neuron path from input to output within the target model. Our experiments demonstrate the superiority of our method finding the most influential neuron path along which the information flows, over the existing baseline solutions. Additionally, the neuron paths have illustrated that vision Transformers exhibit some specific inner working mechanism for processing the visual information within the same image category. We further analyze the key effects of these neurons on the image classification task, showcasing that the found neuron paths have already preserved the model capability on downstream tasks, which may also shed some lights on real-world applications like model pruning. The project website including implementation code is available at https://foundation-model-research.github.io/NeuronPath/.",
            "score": 3,
            "issue_id": 2712,
            "pub_date": "2025-03-12",
            "pub_date_card": {
                "ru": "12 Ð¼Ð°Ñ€Ñ‚Ð°",
                "en": "March 12",
                "zh": "3æœˆ12æ—¥"
            },
            "hash": "c4d7c64b526c6486",
            "authors": [
                "Yifan Wang",
                "Yifei Liu",
                "Yingdong Shi",
                "Changming Li",
                "Anqi Pang",
                "Sibei Yang",
                "Jingyi Yu",
                "Kan Ren"
            ],
            "affiliations": [
                "ShanghaiTech University",
                "Tencent PCG"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.09046.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#interpretability",
                    "#cv",
                    "#training",
                    "#inference"
                ],
                "emoji": "ðŸ”",
                "ru": {
                    "title": "Ð Ð°ÑÐºÑ€Ñ‹Ñ‚Ð¸Ðµ Ñ‚Ð°Ð¹Ð½ Vision Transformer: Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð²Ð»Ð¸ÑÑ‚ÐµÐ»ÑŒÐ½Ñ‹Ñ… Ð½ÐµÐ¹Ñ€Ð¾Ð½Ð½Ñ‹Ñ… Ð¿ÑƒÑ‚ÐµÐ¹",
                    "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¸ÑÑÐ»ÐµÐ´ÑƒÐµÑ‚ Ð·Ð½Ð°Ñ‡Ð¸Ð¼Ñ‹Ðµ Ð¿ÑƒÑ‚Ð¸ Ð½ÐµÐ¹Ñ€Ð¾Ð½Ð¾Ð² Ð² Ð¼Ð¾Ð´ÐµÐ»ÑÑ… Vision Transformer, Ð¿Ñ€ÐµÐ´Ð»Ð°Ð³Ð°Ñ Ð¼ÐµÑ‚Ð¾Ð´ Ð¾Ñ†ÐµÐ½ÐºÐ¸ Ð²Ð»Ð¸ÑÐ½Ð¸Ñ Ð³Ñ€ÑƒÐ¿Ð¿ Ð½ÐµÐ¹Ñ€Ð¾Ð½Ð¾Ð² Ð½Ð° Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ Ð¼Ð¾Ð´ÐµÐ»Ð¸. ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð»Ð¸ Ð¿Ð¾ÑÐ»Ð¾Ð¹Ð½Ñ‹Ð¹ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ Ð´Ð»Ñ Ð²Ñ‹Ð±Ð¾Ñ€Ð° Ð½Ð°Ð¸Ð±Ð¾Ð»ÐµÐµ Ð²Ð»Ð¸ÑÑ‚ÐµÐ»ÑŒÐ½Ñ‹Ñ… Ð½ÐµÐ¹Ñ€Ð¾Ð½Ð¾Ð², Ð¿Ð¾Ð·Ð²Ð¾Ð»ÑÑŽÑ‰Ð¸Ð¹ Ð¾Ð±Ð½Ð°Ñ€ÑƒÐ¶Ð¸Ñ‚ÑŒ ÐºÐ»ÑŽÑ‡ÐµÐ²Ð¾Ð¹ Ð¿ÑƒÑ‚ÑŒ Ð¿Ñ€Ð¾Ñ…Ð¾Ð¶Ð´ÐµÐ½Ð¸Ñ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸ Ð¾Ñ‚ Ð²Ñ…Ð¾Ð´Ð° Ðº Ð²Ñ‹Ñ…Ð¾Ð´Ñƒ. Ð­ÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ñ‹ Ð¿Ð¾ÐºÐ°Ð·Ð°Ð»Ð¸ Ð¿Ñ€ÐµÐ²Ð¾ÑÑ…Ð¾Ð´ÑÑ‚Ð²Ð¾ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð½Ð¾Ð³Ð¾ Ð¼ÐµÑ‚Ð¾Ð´Ð° Ð½Ð°Ð´ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ð¼Ð¸ Ñ€ÐµÑˆÐµÐ½Ð¸ÑÐ¼Ð¸ Ð² Ð¿Ð¾Ð¸ÑÐºÐµ Ð½Ð°Ð¸Ð±Ð¾Ð»ÐµÐµ Ð·Ð½Ð°Ñ‡Ð¸Ð¼Ñ‹Ñ… Ð½ÐµÐ¹Ñ€Ð¾Ð½Ð½Ñ‹Ñ… Ð¿ÑƒÑ‚ÐµÐ¹. ÐÐ½Ð°Ð»Ð¸Ð· Ð²Ñ‹ÑÐ²Ð»ÐµÐ½Ð½Ñ‹Ñ… Ð¿ÑƒÑ‚ÐµÐ¹ Ð½ÐµÐ¹Ñ€Ð¾Ð½Ð¾Ð² Ð¿Ñ€Ð¾Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð¸Ñ€Ð¾Ð²Ð°Ð» ÑÐ¿ÐµÑ†Ð¸Ñ„Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ð¼ÐµÑ…Ð°Ð½Ð¸Ð·Ð¼Ñ‹ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ Ð²Ð¸Ð·ÑƒÐ°Ð»ÑŒÐ½Ð¾Ð¹ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸ Ð² Vision Transformer Ð¸ Ð¿Ð¾Ñ‚ÐµÐ½Ñ†Ð¸Ð°Ð» Ð´Ð»Ñ Ð¿Ñ€Ð°ÐºÑ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ð¹, Ñ‚Ð°ÐºÐ¸Ñ… ÐºÐ°Ðº pruning Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹."
                },
                "en": {
                    "title": "Unveiling the Neuron Paths in Vision Transformers",
                    "desc": "This paper explores how to better understand Vision Transformer models by focusing on the paths of influential neurons that contribute significantly to model predictions. It introduces a joint influence measure to evaluate the impact of neuron sets on the output and proposes a method to identify the most important neurons layer by layer. The findings reveal that these neuron paths not only clarify the model's decision-making process but also maintain the model's performance on tasks like image classification. This research could lead to practical applications such as model pruning, enhancing the efficiency of Vision Transformers in real-world scenarios."
                },
                "zh": {
                    "title": "æ­ç¤ºè§†è§‰å˜æ¢å™¨ä¸­çš„å…³é”®ç¥žç»å…ƒè·¯å¾„",
                    "desc": "æœ¬æ–‡ç ”ç©¶äº†è§†è§‰å˜æ¢å™¨æ¨¡åž‹ä¸­å½±å“åŠ›ç¥žç»å…ƒè·¯å¾„çš„é‡è¦æ€§ï¼Œè¿™äº›è·¯å¾„ä»Žæ¨¡åž‹è¾“å…¥åˆ°è¾“å‡ºï¼Œå¯¹æ¨¡åž‹æŽ¨ç†æœ‰æ˜¾è‘—å½±å“ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§è”åˆå½±å“åº¦é‡æ–¹æ³•ï¼Œè¯„ä¼°ä¸€ç»„ç¥žç»å…ƒå¯¹æ¨¡åž‹ç»“æžœçš„è´¡çŒ®ï¼Œå¹¶æä¾›äº†ä¸€ç§é€å±‚ç¥žç»å…ƒå®šä½æ–¹æ³•ï¼Œä»¥é«˜æ•ˆé€‰æ‹©æ¯å±‚ä¸­æœ€å…·å½±å“åŠ›çš„ç¥žç»å…ƒã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¯»æ‰¾ä¿¡æ¯æµåŠ¨çš„æœ€å…·å½±å“åŠ›ç¥žç»å…ƒè·¯å¾„æ–¹é¢ä¼˜äºŽçŽ°æœ‰åŸºçº¿è§£å†³æ–¹æ¡ˆã€‚æ­¤å¤–ï¼Œè¿™äº›ç¥žç»å…ƒè·¯å¾„æ­ç¤ºäº†è§†è§‰å˜æ¢å™¨åœ¨å¤„ç†åŒä¸€å›¾åƒç±»åˆ«çš„è§†è§‰ä¿¡æ¯æ—¶çš„ç‰¹å®šå†…éƒ¨å·¥ä½œæœºåˆ¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.10072",
            "title": "\"Silent Is Not Actually Silent\": An Investigation of Toxicity on Bug\n  Report Discussion",
            "url": "https://huggingface.co/papers/2503.10072",
            "abstract": "Toxicity in bug report discussions poses significant challenges to the collaborative dynamics of open-source software development. Bug reports are crucial for identifying and resolving defects, yet their inherently problem-focused nature and emotionally charged context make them susceptible to toxic interactions. This study explores toxicity in GitHub bug reports through a qualitative analysis of 203 bug threads, including 81 toxic ones. Our findings reveal that toxicity frequently arises from misaligned perceptions of bug severity and priority, unresolved frustrations with tools, and lapses in professional communication. These toxic interactions not only derail productive discussions but also reduce the likelihood of actionable outcomes, such as linking issues with pull requests. Our preliminary findings offer actionable recommendations to improve bug resolution by mitigating toxicity.",
            "score": 2,
            "issue_id": 2699,
            "pub_date": "2025-03-13",
            "pub_date_card": {
                "ru": "13 Ð¼Ð°Ñ€Ñ‚Ð°",
                "en": "March 13",
                "zh": "3æœˆ13æ—¥"
            },
            "hash": "a91b6bc8232847a3",
            "authors": [
                "Mia Mohammad Imran",
                "Jaydeb Sarker"
            ],
            "affiliations": [
                "Missouri University of Science and Technology, Rolla, Missouri, USA",
                "University of Nebraska Omaha, Omaha, Nebraska, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.10072.jpg",
            "data": {
                "categories": [
                    "#ethics",
                    "#open_source"
                ],
                "emoji": "ðŸ›",
                "ru": {
                    "title": "Ð¢Ð¾ÐºÑÐ¸Ñ‡Ð½Ð¾ÑÑ‚ÑŒ Ð² Ð¾Ñ‚Ñ‡ÐµÑ‚Ð°Ñ… Ð¾Ð± Ð¾ÑˆÐ¸Ð±ÐºÐ°Ñ…: Ð¿Ñ€ÐµÐ¿ÑÑ‚ÑÑ‚Ð²Ð¸Ðµ Ð´Ð»Ñ Ð¿Ñ€Ð¾Ð´ÑƒÐºÑ‚Ð¸Ð²Ð½Ð¾Ð³Ð¾ Ñ€Ð°Ð·Ñ€ÐµÑˆÐµÐ½Ð¸Ñ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼",
                    "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¿Ð¾ÑÐ²ÑÑ‰ÐµÐ½Ð¾ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ðµ Ñ‚Ð¾ÐºÑÐ¸Ñ‡Ð½Ð¾ÑÑ‚Ð¸ Ð² Ð¾Ð±ÑÑƒÐ¶Ð´ÐµÐ½Ð¸ÑÑ… Ð¾Ñ‚Ñ‡ÐµÑ‚Ð¾Ð² Ð¾Ð± Ð¾ÑˆÐ¸Ð±ÐºÐ°Ñ… Ð² Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð°Ñ… Ñ Ð¾Ñ‚ÐºÑ€Ñ‹Ñ‚Ñ‹Ð¼ Ð¸ÑÑ…Ð¾Ð´Ð½Ñ‹Ð¼ ÐºÐ¾Ð´Ð¾Ð¼. ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ Ð¿Ñ€Ð¾Ð²ÐµÐ»Ð¸ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ð¹ Ð°Ð½Ð°Ð»Ð¸Ð· 203 Ð²ÐµÑ‚Ð¾Ðº Ð¾Ð±ÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ñ Ð¾ÑˆÐ¸Ð±Ð¾Ðº Ð½Ð° GitHub, Ð²ÐºÐ»ÑŽÑ‡Ð°Ñ 81 Ñ‚Ð¾ÐºÑÐ¸Ñ‡Ð½ÑƒÑŽ. Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÑŽÑ‚, Ñ‡Ñ‚Ð¾ Ñ‚Ð¾ÐºÑÐ¸Ñ‡Ð½Ð¾ÑÑ‚ÑŒ Ñ‡Ð°ÑÑ‚Ð¾ Ð²Ð¾Ð·Ð½Ð¸ÐºÐ°ÐµÑ‚ Ð¸Ð·-Ð·Ð° Ñ€Ð°ÑÑ…Ð¾Ð¶Ð´ÐµÐ½Ð¸Ð¹ Ð² Ð²Ð¾ÑÐ¿Ñ€Ð¸ÑÑ‚Ð¸Ð¸ ÑÐµÑ€ÑŒÐµÐ·Ð½Ð¾ÑÑ‚Ð¸ Ð¸ Ð¿Ñ€Ð¸Ð¾Ñ€Ð¸Ñ‚ÐµÑ‚Ð½Ð¾ÑÑ‚Ð¸ Ð¾ÑˆÐ¸Ð±Ð¾Ðº, Ð½ÐµÑ€ÐµÑˆÐµÐ½Ð½Ñ‹Ñ… Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼ Ñ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚Ð°Ð¼Ð¸ Ð¸ Ð½Ð°Ñ€ÑƒÑˆÐµÐ½Ð¸Ð¹ Ð¿Ñ€Ð¾Ñ„ÐµÑÑÐ¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾Ð¹ ÐºÐ¾Ð¼Ð¼ÑƒÐ½Ð¸ÐºÐ°Ñ†Ð¸Ð¸. Ð¢Ð¾ÐºÑÐ¸Ñ‡Ð½Ñ‹Ðµ Ð²Ð·Ð°Ð¸Ð¼Ð¾Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ Ð½Ðµ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð½Ð°Ñ€ÑƒÑˆÐ°ÑŽÑ‚ Ð¿Ñ€Ð¾Ð´ÑƒÐºÑ‚Ð¸Ð²Ð½Ñ‹Ðµ Ð¾Ð±ÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ñ, Ð½Ð¾ Ð¸ ÑÐ½Ð¸Ð¶Ð°ÑŽÑ‚ Ð²ÐµÑ€Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚ÑŒ Ð´Ð¾ÑÑ‚Ð¸Ð¶ÐµÐ½Ð¸Ñ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ñ‹Ñ… Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð², Ñ‚Ð°ÐºÐ¸Ñ… ÐºÐ°Ðº Ð¿Ñ€Ð¸Ð²ÑÐ·ÐºÐ° Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼ Ðº Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°Ð¼ Ð½Ð° ÑÐ»Ð¸ÑÐ½Ð¸Ðµ."
                },
                "en": {
                    "title": "Mitigating Toxicity for Better Bug Resolution in Open Source",
                    "desc": "This paper investigates the issue of toxicity in bug report discussions within open-source software development on GitHub. It analyzes 203 bug threads, identifying 81 as toxic, and highlights that toxicity often stems from differing views on bug severity, frustrations with tools, and poor communication. The study emphasizes that such toxic interactions hinder productive collaboration and decrease the chances of resolving issues effectively. The authors provide recommendations aimed at reducing toxicity to enhance the bug resolution process."
                },
                "zh": {
                    "title": "å‡å°‘æ¯’æ€§ï¼Œæå‡å¼€æºåä½œæ•ˆçŽ‡",
                    "desc": "æœ¬ç ”ç©¶æŽ¢è®¨äº†å¼€æºè½¯ä»¶å¼€å‘ä¸­ï¼ŒGitHub bug æŠ¥å‘Šè®¨è®ºä¸­çš„æ¯’æ€§é—®é¢˜ã€‚é€šè¿‡å¯¹203ä¸ªbugçº¿ç¨‹çš„å®šæ€§åˆ†æžï¼Œå‘çŽ°81ä¸ªçº¿ç¨‹å­˜åœ¨æ¯’æ€§äº’åŠ¨ã€‚ç ”ç©¶è¡¨æ˜Žï¼Œæ¯’æ€§å¾€å¾€æºäºŽå¯¹bugä¸¥é‡æ€§å’Œä¼˜å…ˆçº§çš„è¯¯è§£ã€å¯¹å·¥å…·çš„ä¸æ»¡ä»¥åŠä¸“ä¸šæ²Ÿé€šçš„ç¼ºå¤±ã€‚è¿™äº›æ¯’æ€§äº’åŠ¨ä¸ä»…å½±å“äº†æœ‰æ•ˆè®¨è®ºï¼Œè¿˜é™ä½Žäº†å°†é—®é¢˜ä¸Žæ‹‰å–è¯·æ±‚å…³è”çš„å¯èƒ½æ€§ï¼Œå› æ­¤æå‡ºäº†å‡å°‘æ¯’æ€§ä»¥æ”¹å–„bugè§£å†³çš„å»ºè®®ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.10614",
            "title": "ConsisLoRA: Enhancing Content and Style Consistency for LoRA-based Style\n  Transfer",
            "url": "https://huggingface.co/papers/2503.10614",
            "abstract": "Style transfer involves transferring the style from a reference image to the content of a target image. Recent advancements in LoRA-based (Low-Rank Adaptation) methods have shown promise in effectively capturing the style of a single image. However, these approaches still face significant challenges such as content inconsistency, style misalignment, and content leakage. In this paper, we comprehensively analyze the limitations of the standard diffusion parameterization, which learns to predict noise, in the context of style transfer. To address these issues, we introduce ConsisLoRA, a LoRA-based method that enhances both content and style consistency by optimizing the LoRA weights to predict the original image rather than noise. We also propose a two-step training strategy that decouples the learning of content and style from the reference image. To effectively capture both the global structure and local details of the content image, we introduce a stepwise loss transition strategy. Additionally, we present an inference guidance method that enables continuous control over content and style strengths during inference. Through both qualitative and quantitative evaluations, our method demonstrates significant improvements in content and style consistency while effectively reducing content leakage.",
            "score": 1,
            "issue_id": 2712,
            "pub_date": "2025-03-13",
            "pub_date_card": {
                "ru": "13 Ð¼Ð°Ñ€Ñ‚Ð°",
                "en": "March 13",
                "zh": "3æœˆ13æ—¥"
            },
            "hash": "12aacf8b56fdac9c",
            "authors": [
                "Bolin Chen",
                "Baoquan Zhao",
                "Haoran Xie",
                "Yi Cai",
                "Qing Li",
                "Xudong Mao"
            ],
            "affiliations": [
                "Lingnan University",
                "South China University of Technology",
                "Sun Yat-sen University",
                "The Hong Kong Polytechnic University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.10614.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#cv",
                    "#inference",
                    "#training",
                    "#diffusion",
                    "#leakage",
                    "#optimization"
                ],
                "emoji": "ðŸŽ¨",
                "ru": {
                    "title": "ConsisLoRA: ÐÐ¾Ð²Ñ‹Ð¹ ÑƒÑ€Ð¾Ð²ÐµÐ½ÑŒ ÑÐ¾Ð³Ð»Ð°ÑÐ¾Ð²Ð°Ð½Ð½Ð¾ÑÑ‚Ð¸ Ð² Ð¿ÐµÑ€ÐµÐ½Ð¾ÑÐµ ÑÑ‚Ð¸Ð»Ñ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹",
                    "desc": "Ð­Ñ‚Ð° ÑÑ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð½Ð¾Ð²Ñ‹Ð¹ Ð¼ÐµÑ‚Ð¾Ð´ Ð¿ÐµÑ€ÐµÐ½Ð¾ÑÐ° ÑÑ‚Ð¸Ð»Ñ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹ Ð¿Ð¾Ð´ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸ÐµÐ¼ ConsisLoRA. ÐœÐµÑ‚Ð¾Ð´ Ð¾ÑÐ½Ð¾Ð²Ð°Ð½ Ð½Ð° Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð²ÐµÑÐ¾Ð² LoRA Ð´Ð»Ñ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ñ Ð¾Ñ€Ð¸Ð³Ð¸Ð½Ð°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ Ð²Ð¼ÐµÑÑ‚Ð¾ ÑˆÑƒÐ¼Ð°, Ñ‡Ñ‚Ð¾ ÑƒÐ»ÑƒÑ‡ÑˆÐ°ÐµÑ‚ ÑÐ¾Ð³Ð»Ð°ÑÐ¾Ð²Ð°Ð½Ð½Ð¾ÑÑ‚ÑŒ ÑÐ¾Ð´ÐµÑ€Ð¶Ð°Ð½Ð¸Ñ Ð¸ ÑÑ‚Ð¸Ð»Ñ. ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ Ð¿Ñ€ÐµÐ´Ð»Ð°Ð³Ð°ÑŽÑ‚ Ð´Ð²ÑƒÑ…ÑÑ‚Ð°Ð¿Ð½ÑƒÑŽ ÑÑ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸ÑŽ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð¸ Ð¿Ð¾ÑˆÐ°Ð³Ð¾Ð²Ñ‹Ð¹ Ð¿ÐµÑ€ÐµÑ…Ð¾Ð´ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸ Ð¿Ð¾Ñ‚ÐµÑ€ÑŒ Ð´Ð»Ñ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾Ð³Ð¾ Ð·Ð°Ñ…Ð²Ð°Ñ‚Ð° Ð³Ð»Ð¾Ð±Ð°Ð»ÑŒÐ½Ð¾Ð¹ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñ‹ Ð¸ Ð»Ð¾ÐºÐ°Ð»ÑŒÐ½Ñ‹Ñ… Ð´ÐµÑ‚Ð°Ð»ÐµÐ¹. Ð¢Ð°ÐºÐ¶Ðµ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½ Ð¼ÐµÑ‚Ð¾Ð´ ÑƒÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ñ ÑÐ¸Ð»Ð¾Ð¹ ÑÐ¾Ð´ÐµÑ€Ð¶Ð°Ð½Ð¸Ñ Ð¸ ÑÑ‚Ð¸Ð»Ñ Ð¿Ñ€Ð¸ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹."
                },
                "en": {
                    "title": "Enhancing Style Transfer with ConsisLoRA for Better Consistency",
                    "desc": "This paper focuses on improving style transfer techniques, which blend the style of one image with the content of another. It identifies key challenges in existing methods, such as inconsistencies in content and style alignment. The authors introduce ConsisLoRA, a new approach that optimizes LoRA weights to enhance both content and style consistency by predicting the original image instead of noise. They also propose a two-step training strategy and a stepwise loss transition to better capture the details of the images, resulting in improved performance in style transfer tasks."
                },
                "zh": {
                    "title": "æå‡é£Žæ ¼è¿ç§»çš„ä¸€è‡´æ€§ä¸ŽæŽ§åˆ¶åŠ›",
                    "desc": "é£Žæ ¼è¿ç§»æ˜¯å°†å‚è€ƒå›¾åƒçš„é£Žæ ¼è½¬ç§»åˆ°ç›®æ ‡å›¾åƒå†…å®¹ä¸Šçš„æŠ€æœ¯ã€‚æœ€è¿‘ï¼ŒåŸºäºŽLoRAï¼ˆä½Žç§©é€‚åº”ï¼‰çš„æ–¹æ³•åœ¨æœ‰æ•ˆæ•æ‰å•ä¸€å›¾åƒé£Žæ ¼æ–¹é¢æ˜¾ç¤ºå‡ºè‰¯å¥½å‰æ™¯ï¼Œä½†ä»é¢ä¸´å†…å®¹ä¸ä¸€è‡´ã€é£Žæ ¼é”™ä½å’Œå†…å®¹æ³„æ¼ç­‰æŒ‘æˆ˜ã€‚æœ¬æ–‡åˆ†æžäº†æ ‡å‡†æ‰©æ•£å‚æ•°åŒ–åœ¨é£Žæ ¼è¿ç§»ä¸­çš„å±€é™æ€§ï¼Œå¹¶æå‡ºäº†ConsisLoRAæ–¹æ³•ï¼Œé€šè¿‡ä¼˜åŒ–LoRAæƒé‡æ¥å¢žå¼ºå†…å®¹å’Œé£Žæ ¼çš„ä¸€è‡´æ€§ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸¤æ­¥è®­ç»ƒç­–ç•¥å’Œé€æ­¥æŸå¤±è¿‡æ¸¡ç­–ç•¥ï¼Œä»¥æœ‰æ•ˆæ•æ‰å†…å®¹å›¾åƒçš„å…¨å±€ç»“æž„å’Œå±€éƒ¨ç»†èŠ‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.10365",
            "title": "Piece it Together: Part-Based Concepting with IP-Priors",
            "url": "https://huggingface.co/papers/2503.10365",
            "abstract": "Advanced generative models excel at synthesizing images but often rely on text-based conditioning. Visual designers, however, often work beyond language, directly drawing inspiration from existing visual elements. In many cases, these elements represent only fragments of a potential concept-such as an uniquely structured wing, or a specific hairstyle-serving as inspiration for the artist to explore how they can come together creatively into a coherent whole. Recognizing this need, we introduce a generative framework that seamlessly integrates a partial set of user-provided visual components into a coherent composition while simultaneously sampling the missing parts needed to generate a plausible and complete concept. Our approach builds on a strong and underexplored representation space, extracted from IP-Adapter+, on which we train IP-Prior, a lightweight flow-matching model that synthesizes coherent compositions based on domain-specific priors, enabling diverse and context-aware generations. Additionally, we present a LoRA-based fine-tuning strategy that significantly improves prompt adherence in IP-Adapter+ for a given task, addressing its common trade-off between reconstruction quality and prompt adherence.",
            "score": 1,
            "issue_id": 2713,
            "pub_date": "2025-03-13",
            "pub_date_card": {
                "ru": "13 Ð¼Ð°Ñ€Ñ‚Ð°",
                "en": "March 13",
                "zh": "3æœˆ13æ—¥"
            },
            "hash": "80bcd7657c63561e",
            "authors": [
                "Elad Richardson",
                "Kfir Goldberg",
                "Yuval Alaluf",
                "Daniel Cohen-Or"
            ],
            "affiliations": [
                "Bria AI",
                "Tel Aviv University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.10365.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#cv",
                    "#multimodal",
                    "#synthetic"
                ],
                "emoji": "ðŸŽ¨",
                "ru": {
                    "title": "Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð²Ð¸Ð·ÑƒÐ°Ð»ÑŒÐ½Ñ‹Ñ… ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ð¾Ð²: Ð½Ð¾Ð²Ñ‹Ð¹ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ Ðº Ñ‚Ð²Ð¾Ñ€Ñ‡ÐµÑÐºÐ¾Ð¼Ñƒ ÑÐ¸Ð½Ñ‚ÐµÐ·Ñƒ",
                    "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð½Ð¾Ð²Ñ‹Ð¹ Ð³ÐµÐ½ÐµÑ€Ð°Ñ‚Ð¸Ð²Ð½Ñ‹Ð¹ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº Ð´Ð»Ñ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð¸Ð½Ñ‚ÐµÐ³Ñ€Ð¸Ñ€ÑƒÐµÑ‚ Ð²Ð¸Ð·ÑƒÐ°Ð»ÑŒÐ½Ñ‹Ðµ ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ñ‹, Ð¿Ñ€ÐµÐ´Ð¾ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð½Ñ‹Ðµ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÐµÐ¼, Ð² Ñ†ÐµÐ»Ð¾ÑÑ‚Ð½ÑƒÑŽ ÐºÐ¾Ð¼Ð¿Ð¾Ð·Ð¸Ñ†Ð¸ÑŽ. ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð»Ð¸ Ð¼Ð¾Ð´ÐµÐ»ÑŒ IP-Prior, Ð¾ÑÐ½Ð¾Ð²Ð°Ð½Ð½ÑƒÑŽ Ð½Ð° Ð¿Ñ€Ð¾ÑÑ‚Ñ€Ð°Ð½ÑÑ‚Ð²Ðµ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð¸Ð¹ IP-Adapter+, ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ ÑÐ¸Ð½Ñ‚ÐµÐ·Ð¸Ñ€ÑƒÐµÑ‚ ÑÐ¾Ð³Ð»Ð°ÑÐ¾Ð²Ð°Ð½Ð½Ñ‹Ðµ ÐºÐ¾Ð¼Ð¿Ð¾Ð·Ð¸Ñ†Ð¸Ð¸ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð´Ð¾Ð¼ÐµÐ½Ð½Ð¾-ÑÐ¿ÐµÑ†Ð¸Ñ„Ð¸Ñ‡Ð½Ñ‹Ñ… Ð¿Ñ€Ð¸Ð¾Ñ€Ð¾Ð². ÐŸÑ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð½Ñ‹Ð¹ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ Ð¿Ð¾Ð·Ð²Ð¾Ð»ÑÐµÑ‚ Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ñ€Ð°Ð·Ð½Ð¾Ð¾Ð±Ñ€Ð°Ð·Ð½Ñ‹Ðµ Ð¸ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð½Ð¾-Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ñ‹Ðµ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ. ÐšÑ€Ð¾Ð¼Ðµ Ñ‚Ð¾Ð³Ð¾, Ð°Ð²Ñ‚Ð¾Ñ€Ñ‹ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð¸Ð»Ð¸ ÑÑ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸ÑŽ Ñ‚Ð¾Ð½ÐºÐ¾Ð¹ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ LoRA, ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ Ð·Ð½Ð°Ñ‡Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ ÑƒÐ»ÑƒÑ‡ÑˆÐ°ÐµÑ‚ ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²Ð¸Ðµ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ñƒ Ð² IP-Adapter+."
                },
                "en": {
                    "title": "Bridging Visual Inspiration with Generative Design",
                    "desc": "This paper presents a new generative framework that allows visual designers to create coherent compositions by integrating partial visual elements provided by users. Unlike traditional models that rely heavily on text descriptions, this approach focuses on visual components, enabling artists to explore creative combinations. The framework utilizes a lightweight flow-matching model called IP-Prior, which is trained on a specialized representation space to ensure context-aware and diverse outputs. Additionally, the authors introduce a fine-tuning strategy that enhances the model's ability to adhere to user prompts while maintaining high reconstruction quality."
                },
                "zh": {
                    "title": "æ— ç¼æ•´åˆè§†è§‰å…ƒç´ ï¼Œåˆ›é€ è¿žè´¯æ¦‚å¿µ",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§ç”Ÿæˆæ¡†æž¶ï¼Œèƒ½å¤Ÿå°†ç”¨æˆ·æä¾›çš„éƒ¨åˆ†è§†è§‰å…ƒç´ æ— ç¼æ•´åˆæˆä¸€ä¸ªè¿žè´¯çš„æ•´ä½“ï¼ŒåŒæ—¶ç”Ÿæˆæ‰€éœ€çš„ç¼ºå¤±éƒ¨åˆ†ï¼Œä»¥åˆ›é€ å‡ºåˆç†ä¸”å®Œæ•´çš„æ¦‚å¿µã€‚è¯¥æ–¹æ³•åŸºäºŽä¸€ä¸ªå¼ºå¤§ä¸”æœªè¢«å……åˆ†æŽ¢ç´¢çš„è¡¨ç¤ºç©ºé—´ï¼Œåˆ©ç”¨IP-Adapter+æå–çš„ç‰¹å¾ï¼Œè®­ç»ƒå‡ºè½»é‡çº§çš„æµåŒ¹é…æ¨¡åž‹IP-Priorã€‚æ­¤æ¨¡åž‹èƒ½å¤ŸåŸºäºŽç‰¹å®šé¢†åŸŸçš„å…ˆéªŒçŸ¥è¯†åˆæˆè¿žè´¯çš„ä½œå“ï¼Œå®žçŽ°å¤šæ ·åŒ–å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„ç”Ÿæˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§åŸºäºŽLoRAçš„å¾®è°ƒç­–ç•¥ï¼Œæ˜¾è‘—æé«˜äº†IP-Adapter+åœ¨ç‰¹å®šä»»åŠ¡ä¸­çš„æç¤ºéµå¾ªæ€§ï¼Œè§£å†³äº†é‡å»ºè´¨é‡ä¸Žæç¤ºéµå¾ªæ€§ä¹‹é—´çš„å¸¸è§æƒè¡¡é—®é¢˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.10242",
            "title": "MinorBench: A hand-built benchmark for content-based risks for children",
            "url": "https://huggingface.co/papers/2503.10242",
            "abstract": "Large Language Models (LLMs) are rapidly entering children's lives - through parent-driven adoption, schools, and peer networks - yet current AI ethics and safety research do not adequately address content-related risks specific to minors. In this paper, we highlight these gaps with a real-world case study of an LLM-based chatbot deployed in a middle school setting, revealing how students used and sometimes misused the system. Building on these findings, we propose a new taxonomy of content-based risks for minors and introduce MinorBench, an open-source benchmark designed to evaluate LLMs on their ability to refuse unsafe or inappropriate queries from children. We evaluate six prominent LLMs under different system prompts, demonstrating substantial variability in their child-safety compliance. Our results inform practical steps for more robust, child-focused safety mechanisms and underscore the urgency of tailoring AI systems to safeguard young users.",
            "score": 1,
            "issue_id": 2712,
            "pub_date": "2025-03-13",
            "pub_date_card": {
                "ru": "13 Ð¼Ð°Ñ€Ñ‚Ð°",
                "en": "March 13",
                "zh": "3æœˆ13æ—¥"
            },
            "hash": "28234b0e69863cbe",
            "authors": [
                "Shaun Khoo",
                "Gabriel Chua",
                "Rachel Shong"
            ],
            "affiliations": [
                "Government Technology Agency Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.10242.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#multimodal",
                    "#benchmark",
                    "#ethics"
                ],
                "emoji": "ðŸ§’",
                "ru": {
                    "title": "Ð—Ð°Ñ‰Ð¸Ñ‚Ð° Ð´ÐµÑ‚ÐµÐ¹ Ð¾Ñ‚ Ñ€Ð¸ÑÐºÐ¾Ð² Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹",
                    "desc": "Ð”Ð°Ð½Ð½Ð°Ñ ÑÑ‚Ð°Ñ‚ÑŒÑ Ð¸ÑÑÐ»ÐµÐ´ÑƒÐµÑ‚ Ñ€Ð¸ÑÐºÐ¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ñ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ (LLM) Ð´ÐµÑ‚ÑŒÐ¼Ð¸. ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ Ð¿Ñ€Ð¾Ð²Ð¾Ð´ÑÑ‚ Ð°Ð½Ð°Ð»Ð¸Ð· Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾Ð³Ð¾ ÑÐ»ÑƒÑ‡Ð°Ñ Ð¿Ñ€Ð¸Ð¼ÐµÐ½ÐµÐ½Ð¸Ñ Ñ‡Ð°Ñ‚-Ð±Ð¾Ñ‚Ð° Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ LLM Ð² ÑÑ€ÐµÐ´Ð½ÐµÐ¹ ÑˆÐºÐ¾Ð»Ðµ. ÐÐ° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð½Ñ‹Ñ… Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð² Ð¿Ñ€ÐµÐ´Ð»Ð°Ð³Ð°ÐµÑ‚ÑÑ Ð½Ð¾Ð²Ð°Ñ Ñ‚Ð°ÐºÑÐ¾Ð½Ð¾Ð¼Ð¸Ñ ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚Ð½Ñ‹Ñ… Ñ€Ð¸ÑÐºÐ¾Ð² Ð´Ð»Ñ Ð½ÐµÑÐ¾Ð²ÐµÑ€ÑˆÐµÐ½Ð½Ð¾Ð»ÐµÑ‚Ð½Ð¸Ñ… Ð¸ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ÑÑ MinorBench - Ð¾Ñ‚ÐºÑ€Ñ‹Ñ‚Ñ‹Ð¹ Ð±ÐµÐ½Ñ‡Ð¼Ð°Ñ€Ðº Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑ‚Ð¸ LLM Ð¾Ñ‚ÐºÐ»Ð¾Ð½ÑÑ‚ÑŒ Ð½ÐµÐ±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ñ‹Ðµ Ð¸Ð»Ð¸ Ð½ÐµÑƒÐ¼ÐµÑÑ‚Ð½Ñ‹Ðµ Ð·Ð°Ð¿Ñ€Ð¾ÑÑ‹ Ð¾Ñ‚ Ð´ÐµÑ‚ÐµÐ¹. Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¿Ð¾Ð´Ñ‡ÐµÑ€ÐºÐ¸Ð²Ð°ÐµÑ‚ Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ð¾ÑÑ‚ÑŒ Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ Ð±Ð¾Ð»ÐµÐµ Ð½Ð°Ð´ÐµÐ¶Ð½Ñ‹Ñ… Ð¼ÐµÑ…Ð°Ð½Ð¸Ð·Ð¼Ð¾Ð² Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ÑÑ‚Ð¸, Ð¾Ñ€Ð¸ÐµÐ½Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ñ… Ð½Ð° Ð´ÐµÑ‚ÐµÐ¹."
                },
                "en": {
                    "title": "Ensuring Child Safety in AI: A Call for Better LLM Standards",
                    "desc": "This paper discusses the increasing presence of Large Language Models (LLMs) in the lives of children and the associated risks that are not adequately addressed by current AI ethics. It presents a case study of a chatbot used in a middle school, highlighting both the positive and negative ways students interacted with the system. The authors propose a new classification system for content-related risks specific to minors and introduce MinorBench, a benchmark to assess LLMs on their ability to handle inappropriate queries from children. The evaluation of six LLMs shows significant differences in their compliance with child safety standards, emphasizing the need for improved safety measures tailored for young users."
                },
                "zh": {
                    "title": "ä¿æŠ¤å„¿ç«¥ï¼Œå®‰å…¨ä½¿ç”¨å¤§åž‹è¯­è¨€æ¨¡åž‹",
                    "desc": "æœ¬æ–‡æŽ¢è®¨äº†å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰åœ¨å„¿ç«¥ç”Ÿæ´»ä¸­çš„åº”ç”¨ï¼Œå°¤å…¶æ˜¯åœ¨å­¦æ ¡çŽ¯å¢ƒä¸­çš„ä½¿ç”¨æƒ…å†µã€‚é€šè¿‡æ¡ˆä¾‹ç ”ç©¶ï¼Œæˆ‘ä»¬å‘çŽ°å­¦ç”Ÿåœ¨ä½¿ç”¨èŠå¤©æœºå™¨äººæ—¶å­˜åœ¨å†…å®¹ç›¸å…³çš„é£Žé™©ï¼Œå°¤å…¶æ˜¯å¯¹æœªæˆå¹´äººè€Œè¨€ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æœªæˆå¹´äººå†…å®¹é£Žé™©åˆ†ç±»æ³•ï¼Œå¹¶å¼•å…¥äº†MinorBenchï¼Œä¸€ä¸ªå¼€æºåŸºå‡†ï¼Œç”¨äºŽè¯„ä¼°LLMsæ‹’ç»ä¸å®‰å…¨æˆ–ä¸å½“æŸ¥è¯¢çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æžœæ˜¾ç¤ºï¼Œä¸åŒçš„LLMsåœ¨å„¿ç«¥å®‰å…¨åˆè§„æ€§æ–¹é¢å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œå¼ºè°ƒäº†ä¸ºä¿æŠ¤å¹´è½»ç”¨æˆ·è€Œå®šåˆ¶AIç³»ç»Ÿçš„ç´§è¿«æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.09837",
            "title": "On the Limitations of Vision-Language Models in Understanding Image\n  Transforms",
            "url": "https://huggingface.co/papers/2503.09837",
            "abstract": "Vision Language Models (VLMs) have demonstrated significant potential in various downstream tasks, including Image/Video Generation, Visual Question Answering, Multimodal Chatbots, and Video Understanding. However, these models often struggle with basic image transformations. This paper investigates the image-level understanding of VLMs, specifically CLIP by OpenAI and SigLIP by Google. Our findings reveal that these models lack comprehension of multiple image-level augmentations. To facilitate this study, we created an augmented version of the Flickr8k dataset, pairing each image with a detailed description of the applied transformation. We further explore how this deficiency impacts downstream tasks, particularly in image editing, and evaluate the performance of state-of-the-art Image2Image models on simple transformations.",
            "score": 1,
            "issue_id": 2709,
            "pub_date": "2025-03-12",
            "pub_date_card": {
                "ru": "12 Ð¼Ð°Ñ€Ñ‚Ð°",
                "en": "March 12",
                "zh": "3æœˆ12æ—¥"
            },
            "hash": "cc8e8180cfeef80d",
            "authors": [
                "Ahmad Mustafa Anis",
                "Hasnain Ali",
                "Saquib Sarfraz"
            ],
            "affiliations": [
                "Arbisoft",
                "Cohere for AI Community",
                "Karlsruhe Institute of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.09837.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#cv",
                    "#multimodal",
                    "#games",
                    "#interpretability"
                ],
                "emoji": "ðŸ”",
                "ru": {
                    "title": "ÐžÐ³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð½Ð¾Ðµ Ð¿Ð¾Ð½Ð¸Ð¼Ð°Ð½Ð¸Ðµ Ð¿Ñ€ÐµÐ¾Ð±Ñ€Ð°Ð·Ð¾Ð²Ð°Ð½Ð¸Ð¹ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹ Ð² ÑÐ¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ñ… VLM",
                    "desc": "Ð­Ñ‚Ð¾ Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¿Ð¾ÑÐ²ÑÑ‰ÐµÐ½Ð¾ Ð°Ð½Ð°Ð»Ð¸Ð·Ñƒ ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑ‚Ð¸ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð²Ð¸Ð·ÑƒÐ°Ð»ÑŒÐ½Ð¾Ð³Ð¾ ÑÐ·Ñ‹ÐºÐ° (VLM) Ð¿Ð¾Ð½Ð¸Ð¼Ð°Ñ‚ÑŒ Ð±Ð°Ð·Ð¾Ð²Ñ‹Ðµ Ð¿Ñ€ÐµÐ¾Ð±Ñ€Ð°Ð·Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹. ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ Ð¾Ð±Ð½Ð°Ñ€ÑƒÐ¶Ð¸Ð»Ð¸, Ñ‡Ñ‚Ð¾ Ð¿Ð¾Ð¿ÑƒÐ»ÑÑ€Ð½Ñ‹Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ CLIP Ð¸ SigLIP Ð½Ðµ ÑÐ¿Ñ€Ð°Ð²Ð»ÑÑŽÑ‚ÑÑ Ñ Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ð½Ð¸ÐµÐ¼ Ð¼Ð½Ð¾Ð³Ð¸Ñ… Ð°ÑƒÐ³Ð¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ð¹ Ð½Ð° ÑƒÑ€Ð¾Ð²Ð½Ðµ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹. Ð”Ð»Ñ Ð¿Ñ€Ð¾Ð²ÐµÐ´ÐµÐ½Ð¸Ñ Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ñ Ð±Ñ‹Ð» ÑÐ¾Ð·Ð´Ð°Ð½ Ñ€Ð°ÑÑˆÐ¸Ñ€ÐµÐ½Ð½Ñ‹Ð¹ Ð½Ð°Ð±Ð¾Ñ€ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Flickr8k Ñ Ð¿Ð¾Ð´Ñ€Ð¾Ð±Ð½Ñ‹Ð¼Ð¸ Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸ÑÐ¼Ð¸ Ð¿Ñ€Ð¸Ð¼ÐµÐ½Ñ‘Ð½Ð½Ñ‹Ñ… Ñ‚Ñ€Ð°Ð½ÑÑ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¹. Ð¢Ð°ÐºÐ¶Ðµ Ð¸Ð·ÑƒÑ‡Ð°Ð»Ð¾ÑÑŒ Ð²Ð»Ð¸ÑÐ½Ð¸Ðµ ÑÑ‚Ð¾Ð³Ð¾ Ð½ÐµÐ´Ð¾ÑÑ‚Ð°Ñ‚ÐºÐ° Ð½Ð° Ð·Ð°Ð´Ð°Ñ‡Ð¸ Ñ€ÐµÐ´Ð°ÐºÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹ Ð¸ Ð¾Ñ†ÐµÐ½Ð¸Ð²Ð°Ð»Ð°ÑÑŒ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾ÑÑ‚ÑŒ ÑÐ¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Image2Image Ð¿Ñ€Ð¸ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ð¸ Ð¿Ñ€Ð¾ÑÑ‚Ñ‹Ñ… Ð¿Ñ€ÐµÐ¾Ð±Ñ€Ð°Ð·Ð¾Ð²Ð°Ð½Ð¸Ð¹."
                },
                "en": {
                    "title": "Unlocking Image Transformations in Vision Language Models",
                    "desc": "This paper examines the limitations of Vision Language Models (VLMs) like CLIP and SigLIP in understanding basic image transformations. Despite their success in tasks such as image generation and visual question answering, these models struggle with comprehending various image-level augmentations. The authors created an augmented Flickr8k dataset to analyze how well these models recognize and describe transformations applied to images. The study also assesses the impact of these deficiencies on downstream tasks, particularly in image editing, and evaluates the performance of advanced Image2Image models in handling simple transformations."
                },
                "zh": {
                    "title": "æå‡è§†è§‰è¯­è¨€æ¨¡åž‹çš„å›¾åƒç†è§£èƒ½åŠ›",
                    "desc": "æœ¬æ–‡ç ”ç©¶äº†è§†è§‰è¯­è¨€æ¨¡åž‹ï¼ˆVLMsï¼‰åœ¨å›¾åƒç†è§£æ–¹é¢çš„ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯OpenAIçš„CLIPå’ŒGoogleçš„SigLIPã€‚å°½ç®¡è¿™äº›æ¨¡åž‹åœ¨å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨çŽ°å‡ºè‰²ï¼Œä½†å®ƒä»¬åœ¨åŸºæœ¬å›¾åƒå˜æ¢æ–¹é¢å­˜åœ¨å›°éš¾ã€‚æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªå¢žå¼ºç‰ˆçš„Flickr8kæ•°æ®é›†ï¼Œä¸ºæ¯å¼ å›¾åƒé…ä¸Šè¯¦ç»†çš„å˜æ¢æè¿°ï¼Œä»¥ä¾¿è¿›è¡Œç ”ç©¶ã€‚ç ”ç©¶ç»“æžœè¡¨æ˜Žï¼Œè¿™ç§ç†è§£ç¼ºé™·ä¼šå½±å“ä¸‹æ¸¸ä»»åŠ¡çš„è¡¨çŽ°ï¼Œå°¤å…¶æ˜¯åœ¨å›¾åƒç¼–è¾‘æ–¹é¢ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.09368",
            "title": "PerCoV2: Improved Ultra-Low Bit-Rate Perceptual Image Compression with\n  Implicit Hierarchical Masked Image Modeling",
            "url": "https://huggingface.co/papers/2503.09368",
            "abstract": "We introduce PerCoV2, a novel and open ultra-low bit-rate perceptual image compression system designed for bandwidth- and storage-constrained applications. Building upon prior work by Careil et al., PerCoV2 extends the original formulation to the Stable Diffusion 3 ecosystem and enhances entropy coding efficiency by explicitly modeling the discrete hyper-latent image distribution. To this end, we conduct a comprehensive comparison of recent autoregressive methods (VAR and MaskGIT) for entropy modeling and evaluate our approach on the large-scale MSCOCO-30k benchmark. Compared to previous work, PerCoV2 (i) achieves higher image fidelity at even lower bit-rates while maintaining competitive perceptual quality, (ii) features a hybrid generation mode for further bit-rate savings, and (iii) is built solely on public components. Code and trained models will be released at https://github.com/Nikolai10/PerCoV2.",
            "score": 1,
            "issue_id": 2707,
            "pub_date": "2025-03-12",
            "pub_date_card": {
                "ru": "12 Ð¼Ð°Ñ€Ñ‚Ð°",
                "en": "March 12",
                "zh": "3æœˆ12æ—¥"
            },
            "hash": "7aa613c0b5e4220f",
            "authors": [
                "Nikolai KÃ¶rber",
                "Eduard Kromer",
                "Andreas Siebert",
                "Sascha Hauke",
                "Daniel Mueller-Gritschneder",
                "BjÃ¶rn Schuller"
            ],
            "affiliations": [
                "TU Wien",
                "Technical University of Munich",
                "University of Applied Sciences Landshut"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.09368.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#benchmark",
                    "#dataset",
                    "#architecture"
                ],
                "emoji": "ðŸ—œï¸",
                "ru": {
                    "title": "Ð¡Ð¶Ð¸Ð¼Ð°Ð¹ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½ÐµÐµ, ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÐ¹ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð¾: PerCoV2 Ð½Ð° ÑÑ‚Ñ€Ð°Ð¶Ðµ Ð±Ð¸Ñ‚Ñ€ÐµÐ¹Ñ‚Ð°",
                    "desc": "PerCoV2 - ÑÑ‚Ð¾ Ð½Ð¾Ð²Ð°Ñ ÑÐ¸ÑÑ‚ÐµÐ¼Ð° ÑÐ¶Ð°Ñ‚Ð¸Ñ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹ Ñ ÑƒÐ»ÑŒÑ‚Ñ€Ð°Ð½Ð¸Ð·ÐºÐ¸Ð¼ Ð±Ð¸Ñ‚Ñ€ÐµÐ¹Ñ‚Ð¾Ð¼, Ð¾ÑÐ½Ð¾Ð²Ð°Ð½Ð½Ð°Ñ Ð½Ð° Ð¿ÐµÑ€Ñ†ÐµÐ¿Ñ‚Ð¸Ð²Ð½Ð¾Ð¼ Ð¿Ð¾Ð´Ñ…Ð¾Ð´Ðµ. ÐžÐ½Ð° Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ ÑÐºÐ¾ÑÐ¸ÑÑ‚ÐµÐ¼Ñƒ Stable Diffusion 3 Ð¸ ÑƒÐ»ÑƒÑ‡ÑˆÐ°ÐµÑ‚ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾ÑÑ‚ÑŒ ÑÐ½Ñ‚Ñ€Ð¾Ð¿Ð¸Ð¹Ð½Ð¾Ð³Ð¾ ÐºÐ¾Ð´Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¿ÑƒÑ‚ÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð´Ð¸ÑÐºÑ€ÐµÑ‚Ð½Ð¾Ð³Ð¾ Ð³Ð¸Ð¿ÐµÑ€Ð»Ð°Ñ‚ÐµÐ½Ñ‚Ð½Ð¾Ð³Ð¾ Ñ€Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ñ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹. ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ ÑÑ€Ð°Ð²Ð½Ð¸Ð²Ð°ÑŽÑ‚ Ð°Ð²Ñ‚Ð¾Ñ€ÐµÐ³Ñ€ÐµÑÑÐ¸Ð¾Ð½Ð½Ñ‹Ðµ Ð¼ÐµÑ‚Ð¾Ð´Ñ‹ VAR Ð¸ MaskGIT Ð´Ð»Ñ ÑÐ½Ñ‚Ñ€Ð¾Ð¿Ð¸Ð¹Ð½Ð¾Ð³Ð¾ Ð¼Ð¾Ð´ÐµÐ»Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¸ Ð¾Ñ†ÐµÐ½Ð¸Ð²Ð°ÑŽÑ‚ ÑÐ²Ð¾Ð¹ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ Ð½Ð° Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ðµ MSCOCO-30k. PerCoV2 Ð´Ð¾ÑÑ‚Ð¸Ð³Ð°ÐµÑ‚ Ð±Ð¾Ð»ÐµÐµ Ð²Ñ‹ÑÐ¾ÐºÐ¾Ð¹ Ñ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚Ð¸ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹ Ð¿Ñ€Ð¸ ÐµÑ‰Ðµ Ð±Ð¾Ð»ÐµÐµ Ð½Ð¸Ð·ÐºÐ¸Ñ… Ð±Ð¸Ñ‚Ñ€ÐµÐ¹Ñ‚Ð°Ñ…, ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÑ Ð¿Ñ€Ð¸ ÑÑ‚Ð¾Ð¼ ÐºÐ¾Ð½ÐºÑƒÑ€ÐµÐ½Ñ‚Ð¾ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾Ðµ Ð¿ÐµÑ€Ñ†ÐµÐ¿Ñ‚Ð¸Ð²Ð½Ð¾Ðµ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð¾."
                },
                "en": {
                    "title": "Ultra-Low Bit-Rate Image Compression Redefined",
                    "desc": "PerCoV2 is a new image compression system that uses very few bits to represent images, making it ideal for situations where bandwidth and storage are limited. It improves on earlier methods by better modeling how images are represented in a compressed form, specifically within the Stable Diffusion 3 framework. The system has been tested against other advanced methods and shows better image quality at lower bit rates, while also offering a way to save even more space. Additionally, all components of PerCoV2 are publicly available, allowing others to use and build upon this technology."
                },
                "zh": {
                    "title": "PerCoV2ï¼šè¶…ä½Žæ¯”ç‰¹çŽ‡å›¾åƒåŽ‹ç¼©çš„æ–°çªç ´",
                    "desc": "PerCoV2æ˜¯ä¸€ç§æ–°åž‹çš„è¶…ä½Žæ¯”ç‰¹çŽ‡æ„ŸçŸ¥å›¾åƒåŽ‹ç¼©ç³»ç»Ÿï¼Œä¸“ä¸ºå¸¦å®½å’Œå­˜å‚¨å—é™çš„åº”ç”¨è€Œè®¾è®¡ã€‚è¯¥ç³»ç»Ÿåœ¨Careilç­‰äººçš„å…ˆå‰å·¥ä½œåŸºç¡€ä¸Šè¿›è¡Œäº†æ‰©å±•ï¼Œå¢žå¼ºäº†ç†µç¼–ç æ•ˆçŽ‡ï¼Œé€šè¿‡æ˜Žç¡®å»ºæ¨¡ç¦»æ•£è¶…æ½œå›¾åƒåˆ†å¸ƒæ¥å®žçŽ°ã€‚æˆ‘ä»¬å¯¹æœ€è¿‘çš„è‡ªå›žå½’æ–¹æ³•ï¼ˆVARå’ŒMaskGITï¼‰è¿›è¡Œäº†å…¨é¢æ¯”è¾ƒï¼Œå¹¶åœ¨å¤§è§„æ¨¡çš„MSCOCO-30kåŸºå‡†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚ä¸Žä¹‹å‰çš„å·¥ä½œç›¸æ¯”ï¼ŒPerCoV2åœ¨æ›´ä½Žæ¯”ç‰¹çŽ‡ä¸‹å®žçŽ°äº†æ›´é«˜çš„å›¾åƒä¿çœŸåº¦ï¼ŒåŒæ—¶ä¿æŒäº†ç«žäº‰æ€§çš„æ„ŸçŸ¥è´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.10602",
            "title": "TruthPrInt: Mitigating LVLM Object Hallucination Via Latent\n  Truthful-Guided Pre-Intervention",
            "url": "https://huggingface.co/papers/2503.10602",
            "abstract": "Object Hallucination (OH) has been acknowledged as one of the major trustworthy challenges in Large Vision-Language Models (LVLMs). Recent advancements in Large Language Models (LLMs) indicate that internal states, such as hidden states, encode the \"overall truthfulness\" of generated responses. However, it remains under-explored how internal states in LVLMs function and whether they could serve as \"per-token\" hallucination indicators, which is essential for mitigating OH. In this paper, we first conduct an in-depth exploration of LVLM internal states in relation to OH issues and discover that (1) LVLM internal states are high-specificity per-token indicators of hallucination behaviors. Moreover, (2) different LVLMs encode universal patterns of hallucinations in common latent subspaces, indicating that there exist \"generic truthful directions\" shared by various LVLMs. Based on these discoveries, we propose Truthful-Guided Pre-Intervention (TruthPrInt) that first learns the truthful direction of LVLM decoding and then applies truthful-guided inference-time intervention during LVLM decoding. We further propose ComnHallu to enhance both cross-LVLM and cross-data hallucination detection transferability by constructing and aligning hallucination latent subspaces. We evaluate TruthPrInt in extensive experimental settings, including in-domain and out-of-domain scenarios, over popular LVLMs and OH benchmarks. Experimental results indicate that TruthPrInt significantly outperforms state-of-the-art methods. Codes will be available at https://github.com/jinhaoduan/TruthPrInt.",
            "score": 0,
            "issue_id": 2712,
            "pub_date": "2025-03-13",
            "pub_date_card": {
                "ru": "13 Ð¼Ð°Ñ€Ñ‚Ð°",
                "en": "March 13",
                "zh": "3æœˆ13æ—¥"
            },
            "hash": "9d4ccef966377a11",
            "authors": [
                "Jinhao Duan",
                "Fei Kong",
                "Hao Cheng",
                "James Diffenderfer",
                "Bhavya Kailkhura",
                "Lichao Sun",
                "Xiaofeng Zhu",
                "Xiaoshuang Shi",
                "Kaidi Xu"
            ],
            "affiliations": [
                "Drexel University",
                "Hong Kong University of Science and Technology (Guangzhou)",
                "LLNL",
                "Lehigh University",
                "University of Electronic Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.10602.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#multimodal",
                    "#transfer_learning",
                    "#cv",
                    "#training",
                    "#hallucinations",
                    "#benchmark"
                ],
                "emoji": "ðŸ”",
                "ru": {
                    "title": "Ð‘Ð¾Ñ€ÑŒÐ±Ð° Ñ Ð³Ð°Ð»Ð»ÑŽÑ†Ð¸Ð½Ð°Ñ†Ð¸ÑÐ¼Ð¸ Ð² Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… Ð¼Ð¾Ð´ÐµÐ»ÑÑ…",
                    "desc": "Ð’ ÑÑ‚Ð°Ñ‚ÑŒÐµ Ñ€Ð°ÑÑÐ¼Ð°Ñ‚Ñ€Ð¸Ð²Ð°ÐµÑ‚ÑÑ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ð° Ð³Ð°Ð»Ð»ÑŽÑ†Ð¸Ð½Ð°Ñ†Ð¸Ð¸ Ð¾Ð±ÑŠÐµÐºÑ‚Ð¾Ð² Ð² Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… Ð¼Ð¾Ð´ÐµÐ»ÑÑ…, Ð¾Ð±ÑŠÐµÐ´Ð¸Ð½ÑÑŽÑ‰Ð¸Ñ… Ð·Ñ€ÐµÐ½Ð¸Ðµ Ð¸ ÑÐ·Ñ‹Ðº (LVLMs). Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»Ð¸ Ð¾Ð±Ð½Ð°Ñ€ÑƒÐ¶Ð¸Ð»Ð¸, Ñ‡Ñ‚Ð¾ Ð²Ð½ÑƒÑ‚Ñ€ÐµÐ½Ð½Ð¸Ðµ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ñ ÑÑ‚Ð¸Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð¼Ð¾Ð³ÑƒÑ‚ ÑÐ»ÑƒÐ¶Ð¸Ñ‚ÑŒ Ð¸Ð½Ð´Ð¸ÐºÐ°Ñ‚Ð¾Ñ€Ð°Ð¼Ð¸ Ð³Ð°Ð»Ð»ÑŽÑ†Ð¸Ð½Ð°Ñ†Ð¸Ð¹ Ð½Ð° ÑƒÑ€Ð¾Ð²Ð½Ðµ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ Ñ‚Ð¾ÐºÐµÐ½Ð°. ÐžÐ½Ð¸ Ñ‚Ð°ÐºÐ¶Ðµ Ð²Ñ‹ÑÐ²Ð¸Ð»Ð¸, Ñ‡Ñ‚Ð¾ Ñ€Ð°Ð·Ð½Ñ‹Ðµ LVLMs Ð¸Ð¼ÐµÑŽÑ‚ Ð¾Ð±Ñ‰Ð¸Ðµ Ð¿Ð°Ñ‚Ñ‚ÐµÑ€Ð½Ñ‹ Ð³Ð°Ð»Ð»ÑŽÑ†Ð¸Ð½Ð°Ñ†Ð¸Ð¹ Ð² ÑÐºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ð¿Ð¾Ð´Ð¿Ñ€Ð¾ÑÑ‚Ñ€Ð°Ð½ÑÑ‚Ð²Ð°Ñ…. ÐÐ° Ð¾ÑÐ½Ð¾Ð²Ðµ ÑÑ‚Ð¸Ñ… Ð¾Ñ‚ÐºÑ€Ñ‹Ñ‚Ð¸Ð¹ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½ Ð¼ÐµÑ‚Ð¾Ð´ TruthPrInt, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ ÑƒÐ»ÑƒÑ‡ÑˆÐ°ÐµÑ‚ Ñ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚ÑŒ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð·Ð° ÑÑ‡Ñ‘Ñ‚ Ð½Ð°Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð½Ð¾Ð³Ð¾ Ð²Ð¼ÐµÑˆÐ°Ñ‚ÐµÐ»ÑŒÑÑ‚Ð²Ð° Ð² Ð¿Ñ€Ð¾Ñ†ÐµÑÑ Ð´ÐµÐºÐ¾Ð´Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ."
                },
                "en": {
                    "title": "Enhancing Truthfulness in Vision-Language Models",
                    "desc": "This paper addresses the problem of Object Hallucination (OH) in Large Vision-Language Models (LVLMs), which affects the reliability of their outputs. It investigates how internal states of LVLMs can act as indicators of hallucination on a per-token basis, revealing that these states can signal specific hallucination behaviors. The authors introduce a method called Truthful-Guided Pre-Intervention (TruthPrInt) that learns the 'truthful direction' during decoding to improve the accuracy of generated responses. Additionally, they propose ComnHallu to enhance the detection of hallucinations across different LVLMs and datasets by aligning their latent subspaces, demonstrating significant improvements over existing methods in their experiments."
                },
                "zh": {
                    "title": "æ­ç¤ºLVLMä¸­çš„çœŸå®žæ–¹å‘ï¼Œå‡å°‘å¯¹è±¡å¹»è§‰",
                    "desc": "æœ¬æ–‡æŽ¢è®¨äº†å¤§åž‹è§†è§‰è¯­è¨€æ¨¡åž‹ï¼ˆLVLMsï¼‰ä¸­çš„å¯¹è±¡å¹»è§‰ï¼ˆOHï¼‰é—®é¢˜ï¼Œè®¤ä¸ºå†…éƒ¨çŠ¶æ€å¯ä»¥ä½œä¸ºå¹»è§‰è¡Œä¸ºçš„æŒ‡ç¤ºå™¨ã€‚ç ”ç©¶å‘çŽ°ï¼ŒLVLMçš„å†…éƒ¨çŠ¶æ€èƒ½å¤Ÿé«˜ç‰¹å¼‚æ€§åœ°æŒ‡ç¤ºæ¯ä¸ªtokençš„å¹»è§‰è¡Œä¸ºï¼Œå¹¶ä¸”ä¸åŒçš„LVLMåœ¨æ½œåœ¨å­ç©ºé—´ä¸­ç¼–ç äº†é€šç”¨çš„å¹»è§‰æ¨¡å¼ã€‚åŸºäºŽè¿™äº›å‘çŽ°ï¼Œæå‡ºäº†ä¸€ç§åä¸ºTruthful-Guided Pre-Interventionï¼ˆTruthPrIntï¼‰çš„æ–¹æ³•ï¼Œé€šè¿‡å­¦ä¹ LVLMè§£ç çš„çœŸå®žæ–¹å‘æ¥è¿›è¡Œå¹²é¢„ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒTruthPrIntåœ¨å¤šä¸ªæµè¡Œçš„LVLMå’ŒOHåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºŽçŽ°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-03-13.html",
    "link_next": "2025-03-17.html",
    "link_month": "2025-03.html",
    "short_date_prev": {
        "ru": "13.03",
        "en": "03/13",
        "zh": "3æœˆ13æ—¥"
    },
    "short_date_next": {
        "ru": "17.03",
        "en": "03/17",
        "zh": "3æœˆ17æ—¥"
    },
    "categories": {
        "#dataset": 14,
        "#data": 7,
        "#benchmark": 14,
        "#agents": 4,
        "#cv": 15,
        "#rl": 4,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 7,
        "#3d": 2,
        "#audio": 1,
        "#video": 4,
        "#multimodal": 15,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 6,
        "#healthcare": 0,
        "#training": 16,
        "#robotics": 0,
        "#agi": 1,
        "#games": 4,
        "#interpretability": 4,
        "#reasoning": 9,
        "#transfer_learning": 2,
        "#graphs": 1,
        "#ethics": 2,
        "#security": 1,
        "#optimization": 12,
        "#survey": 1,
        "#diffusion": 10,
        "#alignment": 0,
        "#story_generation": 1,
        "#hallucinations": 3,
        "#long_context": 4,
        "#synthetic": 3,
        "#machine_translation": 1,
        "#leakage": 1,
        "#open_source": 9,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†å¦‚ä½•æ”¹è¿›å¤šè½®æ¬¡å›¾åƒç¼–è¾‘ä»»åŠ¡ã€‚ä¼ ç»Ÿæ–¹æ³•æˆæœ¬é«˜ï¼Œå¤§è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰è™½æœ‰è§„åˆ’èƒ½åŠ›ä½†ç¼ºä¹å·¥å…·æˆæœ¬çš„å‡†ç¡®ä¼°è®¡ã€‚ä½œè€…æå‡ºäº†â€œCoSTA*â€æ–¹æ³•ï¼Œç»“åˆLLMså’Œå›¾æœç´¢ï¼Œæ‰¾åˆ°æˆæœ¬æ•ˆç›Šé«˜çš„å·¥å…·è·¯å¾„ã€‚CoSTA*é€šè¿‡VLMè¯„ä¼°æ¯ä¸ªå­ä»»åŠ¡ï¼Œå¿«é€Ÿæ¢å¤å¤±è´¥ï¼Œå¹¶åœ¨å­ä»»åŠ¡é—´åˆ‡æ¢æ¨¡æ€ä»¥ä¼˜åŒ–æˆæœ¬å’Œè´¨é‡ã€‚å®žéªŒæ˜¾ç¤ºï¼ŒCoSTA*åœ¨æ–°çš„å¤šè½®æ¬¡å›¾åƒç¼–è¾‘åŸºå‡†ä¸Šè¡¨çŽ°ä¼˜å¼‚ã€‚",
        "title": "CoSTAast: Cost-Sensitive Toolpath Agent for Multi-turn Image Editing",
        "pinyin": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†å¦‚ä½•æ”¹è¿›å¤šè½®æ¬¡å›¾åƒç¼–è¾‘ä»»åŠ¡ã€‚ä¼ ç»Ÿæ–¹æ³•æˆæœ¬é«˜ï¼Œå¤§è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰è™½æœ‰è§„åˆ’èƒ½åŠ›ä½†ç¼ºä¹å·¥å…·æˆæœ¬çš„å‡†ç¡®ä¼°è®¡ã€‚ä½œè€…æå‡ºäº†â€œCoSTA*â€æ–¹æ³•ï¼Œç»“åˆLLMså’Œå›¾æœç´¢ï¼Œæ‰¾åˆ°æˆæœ¬æ•ˆç›Šé«˜çš„å·¥å…·è·¯å¾„ã€‚CoSTA*é€šè¿‡VLMè¯„ä¼°æ¯ä¸ªå­ä»»åŠ¡ï¼Œå¿«é€Ÿæ¢å¤å¤±è´¥ï¼Œå¹¶åœ¨å­ä»»åŠ¡é—´åˆ‡æ¢æ¨¡æ€ä»¥ä¼˜åŒ–æˆæœ¬å’Œè´¨é‡ã€‚å®žéªŒæ˜¾ç¤ºï¼ŒCoSTA*åœ¨æ–°çš„å¤šè½®æ¬¡å›¾åƒç¼–è¾‘åŸºå‡†ä¸Šè¡¨çŽ°ä¼˜å¼‚ã€‚\n\nZhÃ¨ piÄn wÃ©nzhÄng tÇŽolÃ¹n le rÃºhÃ© gÇŽijÃ¬n duÅ lÃºn cÃ¬ tÃºxiÃ ng biÄnjÃ­ rÃ¨nwÃ¹. ChuÃ¡ntÇ’ng fÄngfÇŽ chÃ©ngbÄ›n gÄo, dÃ  yÇ”yÃ¡n mÃ³xÃ­ng (LLMs) suÄ« yÇ’u guÄ«huÃ  nÃ©nglÃ¬ dÃ n quÄ“fÃ¡ gÅngjÃ¹ chÃ©ngbÄ›n de zhÇ”nquÃ¨ gÅ«jÃ¬. ZuÃ²zhÄ› tÃ­chÅ« le â€œCoSTA*â€ fÄngfÇŽ, jiÃ©hÃ© LLMs hÃ© tÃº sÅusuÇ’, zhÇŽo dÃ o chÃ©ngbÄ›n xiÃ oyÃ¬ gÄo de gÅngjÃ¹ lÃ¹jÃ¬ng. CoSTA* tÅngguÃ² VLM pÃ­nggÅ« mÄ›i gÃ¨ zÃ¬ rÃ¨nwÃ¹, kuÃ isÃ¹ huÄ«fÃ¹ shÄ«bÃ i, bÃ¬ng zÃ i zÃ¬ rÃ¨nwÃ¹ jiÄn qiÄ“huÃ n mÃ³tÃ i yÇ yÅuhuÃ  chÃ©ngbÄ›n hÃ© zhÃ¬liÃ ng. ShÃ­yÃ n xiÇŽnshÃ¬, CoSTA* zÃ i xÄ«n de duÅ lÃºn cÃ¬ tÃºxiÃ ng biÄnjÃ­ jÄ«zhÇ”n shÃ ng biÇŽoxiÃ n yÅuyÃ¹n.",
        "vocab": "[\n    {\"word\": \"è®¨è®º\", \"pinyin\": \"tÇŽo lÃ¹n\", \"trans\": \"discuss\"},\n    {\"word\": \"æ”¹è¿›\", \"pinyin\": \"gÇŽi jÃ¬n\", \"trans\": \"improve\"},\n    {\"word\": \"å¤šè½®æ¬¡\", \"pinyin\": \"duÅ lÃºn cÃ¬\", \"trans\": \"multi-round\"},\n    {\"word\": \"å›¾åƒ\", \"pinyin\": \"tÃº xiÃ ng\", \"trans\": \"image\"},\n    {\"word\": \"ç¼–è¾‘\", \"pinyin\": \"biÄn jÃ­\", \"trans\": \"edit\"},\n    {\"word\": \"ä»»åŠ¡\", \"pinyin\": \"rÃ¨n wu\", \"trans\": \"task\"},\n    {\"word\": \"ä¼ ç»Ÿ\", \"pinyin\": \"chuÃ¡n tÇ’ng\", \"trans\": \"traditional\"},\n    {\"word\": \"æ–¹æ³•\", \"pinyin\": \"fÄng fÇŽ\", \"trans\": \"method\"},\n    {\"word\": \"æˆæœ¬\", \"pinyin\": \"chÃ©ng bÄ›n\", \"trans\": \"cost\"},\n    {\"word\": \"å¤§è¯­è¨€æ¨¡åž‹\", \"pinyin\": \"dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng\", \"trans\": \"large language model\"},\n    {\"word\": \"è§„åˆ’\", \"pinyin\": \"guÄ« huÃ \", \"trans\": \"plan\"},\n    {\"word\": \"èƒ½åŠ›\", \"pinyin\": \"nÃ©ng lÃ¬\", \"trans\": \"ability\"},\n    {\"word\": \"ç¼ºä¹\", \"pinyin\": \"quÄ“ fÃ¡\", \"trans\": \"lack\"},\n    {\"word\": \"å·¥å…·\", \"pinyin\": \"gÅng jÃ¹\", \"trans\": \"tool\"},\n    {\"word\": \"å‡†ç¡®\", \"pinyin\": \"zhÇ”n quÃ¨\", \"trans\": \"accurate\"},\n    {\"word\": \"ä¼°è®¡\", \"pinyin\": \"gÅ« jÃ¬\", \"trans\": \"estimate\"},\n    {\"word\": \"æå‡º\", \"pinyin\": \"tÃ­ chÅ«\", \"trans\": \"propose\"},\n    {\"word\": \"ç»“åˆ\", \"pinyin\": \"jiÃ© hÃ©\", \"trans\": \"combine\"},\n    {\"word\": \"å›¾æœç´¢\", \"pinyin\": \"tÃº sÅu suÇ’\", \"trans\": \"graph search\"},\n    {\"word\": \"æ•ˆç›Š\", \"pinyin\": \"xiÃ o yÃ¬\", \"trans\": \"benefit\"},\n    {\"word\": \"è·¯å¾„\", \"pinyin\": \"lÃ¹ jÃ¬ng\", \"trans\": \"path\"},\n    {\"word\": \"è¯„ä¼°\", \"pinyin\": \"pÃ­ng gÅ«\", \"trans\": \"evaluate\"},\n    {\"word\": \"å­ä»»åŠ¡\", \"pinyin\": \"zÇ rÃ¨n wu\", \"trans\": \"subtask\"},\n    {\"word\": \"æ¢å¤\", \"pinyin\": \"huÄ« fÃ¹\", \"trans\": \"recover\"},\n    {\"word\": \"å¤±è´¥\", \"pinyin\": \"shÄ« bÃ i\", \"trans\": \"fail\"},\n    {\"word\": \"åˆ‡æ¢\", \"pinyin\": \"qiÄ“ huÃ n\", \"trans\": \"switch\"},\n    {\"word\": \"æ¨¡æ€\", \"pinyin\": \"mÃ³ tÃ i\", \"trans\": \"modality\"},\n    {\"word\": \"ä¼˜åŒ–\", \"pinyin\": \"yÅu huÃ \", \"trans\": \"optimize\"},\n    {\"word\": \"è´¨é‡\", \"pinyin\": \"zhÃ¬ liÃ ng\", \"trans\": \"quality\"},\n    {\"word\": \"å®žéªŒ\", \"pinyin\": \"shÃ­ yÃ n\", \"trans\": \"experiment\"},\n    {\"word\": \"æ˜¾ç¤º\", \"pinyin\": \"xiÇŽn shÃ¬\", \"trans\": \"show\"},\n    {\"word\": \"è¡¨çŽ°\", \"pinyin\": \"biÇŽo xiÃ n\", \"trans\": \"performance\"},\n    {\"word\": \"ä¼˜å¼‚\", \"pinyin\": \"yÅu yÃ¬\", \"trans\": \"excellent\"},\n    {\"word\": \"åŸºå‡†\", \"pinyin\": \"jÄ« zhÇ”n\", \"trans\": \"benchmark\"}\n]",
        "trans": "This article discusses how to improve multi-round image editing tasks. Traditional methods are costly, and while large language models (LLMs) have planning capabilities, they lack accurate estimates of tool costs. The authors propose the \"CoSTA*\" method, which combines LLMs and graph search to find cost-effective tool paths. CoSTA* evaluates each subtask using a vision-language model (VLM), quickly recovers from failures, and switches modalities between subtasks to optimize cost and quality. Experiments show that CoSTA* performs exceptionally well on a new multi-round image editing benchmark.",
        "update_ts": "2025-03-14 09:11"
    }
}