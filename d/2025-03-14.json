{
    "date": {
        "ru": "14 марта",
        "en": "March 14",
        "zh": "3月14日"
    },
    "time_utc": "2025-03-14 03:22",
    "weekday": 4,
    "issue_id": 2700,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2503.10480",
            "title": "World Modeling Makes a Better Planner: Dual Preference Optimization for\n  Embodied Task Planning",
            "url": "https://huggingface.co/papers/2503.10480",
            "abstract": "Recent advances in large vision-language models (LVLMs) have shown promise for embodied task planning, yet they struggle with fundamental challenges like dependency constraints and efficiency. Existing approaches either solely optimize action selection or leverage world models during inference, overlooking the benefits of learning to model the world as a way to enhance planning capabilities. We propose Dual Preference Optimization (D^2PO), a new learning framework that jointly optimizes state prediction and action selection through preference learning, enabling LVLMs to understand environment dynamics for better planning. To automatically collect trajectories and stepwise preference data without human annotation, we introduce a tree search mechanism for extensive exploration via trial-and-error. Extensive experiments on VoTa-Bench demonstrate that our D^2PO-based method significantly outperforms existing methods and GPT-4o when applied to Qwen2-VL (7B), LLaVA-1.6 (7B), and LLaMA-3.2 (11B), achieving superior task success rates with more efficient execution paths.",
            "score": 7,
            "issue_id": 2700,
            "pub_date": "2025-03-13",
            "pub_date_card": {
                "ru": "13 марта",
                "en": "March 13",
                "zh": "3月13日"
            },
            "hash": "2d3e6b8c84c51e69",
            "authors": [
                "Siyin Wang",
                "Zhaoye Fei",
                "Qinyuan Cheng",
                "Shiduo Zhang",
                "Panpan Cai",
                "Jinlan Fu",
                "Xipeng Qiu"
            ],
            "affiliations": [
                "Fudan University",
                "National University of Singapore",
                "Shanghai Innovation Institute",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.10480.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#multimodal",
                    "#agents",
                    "#optimization",
                    "#rl",
                    "#training"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Улучшение планирования действий у LVLM через совместную оптимизацию предсказаний и выбора",
                    "desc": "Статья представляет новый подход к обучению больших визуально-языковых моделей (LVLM) для планирования действий в воплощенных задачах. Предложенный метод Dual Preference Optimization (D^2PO) совместно оптимизирует предсказание состояний и выбор действий через обучение предпочтениям. Для сбора данных без участия человека используется механизм поиска по дереву. Эксперименты показывают, что D^2PO значительно превосходит существующие методы по успешности выполнения задач и эффективности траекторий."
                },
                "en": {
                    "title": "Enhancing Planning in LVLMs with Dual Preference Optimization",
                    "desc": "This paper introduces Dual Preference Optimization (D^2PO), a novel framework designed to improve the planning capabilities of large vision-language models (LVLMs) by jointly optimizing state prediction and action selection. The approach addresses key challenges in embodied task planning, such as dependency constraints and efficiency, which have been inadequately handled by existing methods. By employing a tree search mechanism, D^2PO enables the automatic collection of trajectories and preference data, facilitating extensive exploration through trial-and-error without the need for human annotation. Experimental results on VoTa-Bench show that D^2PO significantly enhances task success rates and execution efficiency compared to previous methods and GPT-4o across various LVLMs."
                },
                "zh": {
                    "title": "双重偏好优化：提升任务规划能力的关键",
                    "desc": "本文提出了一种新的学习框架，称为双重偏好优化（D^2PO），旨在提高大型视觉语言模型（LVLMs）在任务规划中的能力。该框架通过偏好学习同时优化状态预测和动作选择，使模型能够更好地理解环境动态。为了自动收集轨迹和逐步偏好数据，本文引入了一种树搜索机制，以便通过试错进行广泛探索。实验结果表明，基于D^2PO的方法在多个基准测试中显著优于现有方法和GPT-4o，达到了更高的任务成功率和更高效的执行路径。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.10582",
            "title": "VisualWebInstruct: Scaling up Multimodal Instruction Data through Web\n  Search",
            "url": "https://huggingface.co/papers/2503.10582",
            "abstract": "Vision-Language Models have made significant progress on many perception-focused tasks, however, their progress on reasoning-focused tasks seem to be limited due to the lack of high-quality and diverse training data. In this work, we aim to address the scarcity issue of reasoning-focused multimodal datasets. We propose VisualWebInstruct - a novel approach that leverages search engine to create a diverse, and high-quality dataset spanning multiple disciplines like math, physics, finance, chemistry, etc. Starting with meticulously selected 30,000 seed images, we employ Google Image search to identify websites containing similar images. We collect and process the HTMLs from over 700K unique URL sources. Through a pipeline of content extraction, filtering and synthesis, we build a dataset of approximately 900K question-answer pairs, with 40% being visual QA pairs and the rest as text QA pairs. Models fine-tuned on VisualWebInstruct demonstrate significant performance gains: (1) training from Llava-OV-mid shows 10-20% absolute point gains across benchmarks, (2) training from MAmmoTH-VL shows 5% absoluate gain. Our best model MAmmoTH-VL2 shows state-of-the-art performance within the 10B parameter class on MMMU-Pro-std (40.7%), MathVerse (42.6%), and DynaMath (55.7%). These remarkable results highlight the effectiveness of our dataset in enhancing VLMs' reasoning capabilities for complex multimodal tasks.",
            "score": 5,
            "issue_id": 2700,
            "pub_date": "2025-03-13",
            "pub_date_card": {
                "ru": "13 марта",
                "en": "March 13",
                "zh": "3月13日"
            },
            "hash": "ff1e79c1589f8602",
            "authors": [
                "Yiming Jia",
                "Jiachen Li",
                "Xiang Yue",
                "Bo Li",
                "Ping Nie",
                "Kai Zou",
                "Wenhu Chen"
            ],
            "affiliations": [
                "CMU",
                "Independent",
                "NUS",
                "Netmind.ai",
                "UC Santa Barbara",
                "University of Toronto",
                "University of Waterloo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.10582.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#data",
                    "#reasoning",
                    "#multimodal"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "VisualWebInstruct: прорыв в обучении мультимодальных моделей рассуждению",
                    "desc": "Исследователи представили VisualWebInstruct - новый подход к созданию разнообразного и качественного набора данных для обучения мультимодальных моделей машинного обучения. Используя поисковую систему и обработку HTML-страниц, они собрали около 900 тысяч пар вопрос-ответ по различным дисциплинам. Модели, дообученные на этом наборе данных, показали значительный прирост производительности на нескольких бенчмарках. Лучшая модель MAmmoTH-VL2 продемонстрировала state-of-the-art результаты в своем классе на задачах рассуждения и решения задач."
                },
                "en": {
                    "title": "Enhancing Reasoning in Vision-Language Models with VisualWebInstruct",
                    "desc": "This paper introduces VisualWebInstruct, a new dataset aimed at improving the reasoning abilities of Vision-Language Models (VLMs). The dataset is created by leveraging search engines to gather diverse and high-quality multimodal data, specifically focusing on reasoning tasks across various disciplines. By processing over 700,000 unique web sources, the authors compile approximately 900,000 question-answer pairs, enhancing the training data available for VLMs. The results show that models trained on this dataset achieve significant performance improvements on reasoning benchmarks, demonstrating its effectiveness in advancing VLM capabilities."
                },
                "zh": {
                    "title": "提升视觉语言模型推理能力的新数据集",
                    "desc": "本文提出了一种新的方法VisualWebInstruct，旨在解决推理导向的多模态数据集稀缺问题。我们利用搜索引擎创建了一个多学科的高质量数据集，包括数学、物理、金融和化学等领域。通过从30,000个种子图像开始，收集和处理超过70万个独特网址的HTML内容，我们构建了约90万个问答对的数据集。经过在VisualWebInstruct上微调的模型在多个基准测试中表现出显著的性能提升，证明了该数据集在增强视觉语言模型推理能力方面的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.10630",
            "title": "UniGoal: Towards Universal Zero-shot Goal-oriented Navigation",
            "url": "https://huggingface.co/papers/2503.10630",
            "abstract": "In this paper, we propose a general framework for universal zero-shot goal-oriented navigation. Existing zero-shot methods build inference framework upon large language models (LLM) for specific tasks, which differs a lot in overall pipeline and fails to generalize across different types of goal. Towards the aim of universal zero-shot navigation, we propose a uniform graph representation to unify different goals, including object category, instance image and text description. We also convert the observation of agent into an online maintained scene graph. With this consistent scene and goal representation, we preserve most structural information compared with pure text and are able to leverage LLM for explicit graph-based reasoning. Specifically, we conduct graph matching between the scene graph and goal graph at each time instant and propose different strategies to generate long-term goal of exploration according to different matching states. The agent first iteratively searches subgraph of goal when zero-matched. With partial matching, the agent then utilizes coordinate projection and anchor pair alignment to infer the goal location. Finally scene graph correction and goal verification are applied for perfect matching. We also present a blacklist mechanism to enable robust switch between stages. Extensive experiments on several benchmarks show that our UniGoal achieves state-of-the-art zero-shot performance on three studied navigation tasks with a single model, even outperforming task-specific zero-shot methods and supervised universal methods.",
            "score": 2,
            "issue_id": 2700,
            "pub_date": "2025-03-13",
            "pub_date_card": {
                "ru": "13 марта",
                "en": "March 13",
                "zh": "3月13日"
            },
            "hash": "9e7667a37c699f4c",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#reasoning",
                    "#long_context",
                    "#games",
                    "#graphs",
                    "#benchmark",
                    "#agents",
                    "#rl"
                ],
                "emoji": "🧭",
                "ru": {
                    "title": "Универсальная навигация с нулевым обучением на основе графов и языковых моделей",
                    "desc": "Статья представляет универсальную систему для навигации с нулевым обучением к различным целям. Авторы предлагают единое графовое представление для унификации разных типов целей и наблюдений агента. Используя большие языковые модели для рассуждений на основе графов, система проводит сопоставление графа сцены и графа цели. Предложенный метод UniGoal достигает лучших результатов в задачах навигации с нулевым обучением по сравнению с существующими подходами."
                },
                "en": {
                    "title": "Navigating Goals Universally with Graphs!",
                    "desc": "This paper introduces a new framework for universal zero-shot goal-oriented navigation, which allows an agent to navigate towards various goals without prior training on specific tasks. Unlike existing methods that rely heavily on large language models (LLMs) tailored for individual tasks, this approach uses a uniform graph representation to integrate different types of goals, such as object categories and text descriptions. The agent maintains an online scene graph that captures the environment, enabling it to perform graph-based reasoning and match its observations with the goals. The proposed method demonstrates superior performance in navigation tasks, surpassing both task-specific and supervised approaches, by effectively managing goal exploration and verification through innovative strategies."
                },
                "zh": {
                    "title": "通用零-shot导航的创新框架",
                    "desc": "本文提出了一种通用的零-shot目标导向导航框架。现有的零-shot方法通常依赖于大型语言模型（LLM）进行特定任务的推理，导致在不同目标之间的泛化能力不足。我们提出了一种统一的图表示方法，将不同的目标（如物体类别、实例图像和文本描述）整合在一起，并将代理的观察结果转换为在线维护的场景图。通过这种一致的场景和目标表示，我们能够利用LLM进行基于图的推理，并在多个基准测试中展示了我们的UniGoal在零-shot导航任务上的优越性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.10613",
            "title": "CoSTAast: Cost-Sensitive Toolpath Agent for Multi-turn Image Editing",
            "url": "https://huggingface.co/papers/2503.10613",
            "abstract": "Text-to-image models like stable diffusion and DALLE-3 still struggle with multi-turn image editing. We decompose such a task as an agentic workflow (path) of tool use that addresses a sequence of subtasks by AI tools of varying costs. Conventional search algorithms require expensive exploration to find tool paths. While large language models (LLMs) possess prior knowledge of subtask planning, they may lack accurate estimations of capabilities and costs of tools to determine which to apply in each subtask. Can we combine the strengths of both LLMs and graph search to find cost-efficient tool paths? We propose a three-stage approach \"CoSTA*\" that leverages LLMs to create a subtask tree, which helps prune a graph of AI tools for the given task, and then conducts A* search on the small subgraph to find a tool path. To better balance the total cost and quality, CoSTA* combines both metrics of each tool on every subtask to guide the A* search. Each subtask's output is then evaluated by a vision-language model (VLM), where a failure will trigger an update of the tool's cost and quality on the subtask. Hence, the A* search can recover from failures quickly to explore other paths. Moreover, CoSTA* can automatically switch between modalities across subtasks for a better cost-quality trade-off. We build a novel benchmark of challenging multi-turn image editing, on which CoSTA* outperforms state-of-the-art image-editing models or agents in terms of both cost and quality, and performs versatile trade-offs upon user preference.",
            "score": 2,
            "issue_id": 2700,
            "pub_date": "2025-03-13",
            "pub_date_card": {
                "ru": "13 марта",
                "en": "March 13",
                "zh": "3月13日"
            },
            "hash": "7f2d9ee971af97a8",
            "authors": [
                "Advait Gupta",
                "NandaKiran Velaga",
                "Dang Nguyen",
                "Tianyi Zhou"
            ],
            "affiliations": [
                "University of Maryland, College Park"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.10613.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#benchmark",
                    "#multimodal",
                    "#agents",
                    "#optimization"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "CoSTA*: Умное редактирование изображений с помощью ИИ и поиска",
                    "desc": "Статья представляет новый подход CoSTA* для многоэтапного редактирования изображений. Метод объединяет большие языковые модели (LLM) для создания дерева подзадач и алгоритм поиска A* для нахождения оптимального пути использования инструментов ИИ. CoSTA* использует мультимодальную модель для оценки результатов каждого этапа и может адаптироваться к неудачам. Эксперименты показывают превосходство CoSTA* над существующими моделями и агентами в соотношении стоимости и качества редактирования изображений."
                },
                "en": {
                    "title": "Optimizing Multi-Turn Image Editing with CoSTA*",
                    "desc": "This paper addresses the challenges faced by text-to-image models in multi-turn image editing by introducing a new approach called CoSTA*. It combines large language models (LLMs) with graph search techniques to efficiently plan and execute a sequence of subtasks using AI tools. CoSTA* creates a subtask tree to streamline the selection of tools based on their costs and capabilities, and employs A* search to find optimal tool paths. The method also adapts to failures by updating tool metrics, allowing for quick recovery and improved cost-quality trade-offs in image editing tasks."
                },
                "zh": {
                    "title": "高效的多轮图像编辑工具路径优化",
                    "desc": "本文提出了一种名为CoSTA*的三阶段方法，用于解决文本到图像模型在多轮图像编辑中的挑战。该方法结合了大型语言模型（LLMs）和图搜索的优点，通过创建子任务树来优化AI工具的使用路径。CoSTA*在每个子任务中综合考虑工具的成本和质量，以指导A*搜索，从而找到高效的工具路径。实验结果表明，CoSTA*在多轮图像编辑任务中超越了现有的最先进模型，能够根据用户偏好进行灵活的成本与质量权衡。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.10636",
            "title": "The Curse of Conditions: Analyzing and Improving Optimal Transport for\n  Conditional Flow-Based Generation",
            "url": "https://huggingface.co/papers/2503.10636",
            "abstract": "Minibatch optimal transport coupling straightens paths in unconditional flow matching. This leads to computationally less demanding inference as fewer integration steps and less complex numerical solvers can be employed when numerically solving an ordinary differential equation at test time. However, in the conditional setting, minibatch optimal transport falls short. This is because the default optimal transport mapping disregards conditions, resulting in a conditionally skewed prior distribution during training. In contrast, at test time, we have no access to the skewed prior, and instead sample from the full, unbiased prior distribution. This gap between training and testing leads to a subpar performance. To bridge this gap, we propose conditional optimal transport C^2OT that adds a conditional weighting term in the cost matrix when computing the optimal transport assignment. Experiments demonstrate that this simple fix works with both discrete and continuous conditions in 8gaussians-to-moons, CIFAR-10, ImageNet-32x32, and ImageNet-256x256. Our method performs better overall compared to the existing baselines across different function evaluation budgets. Code is available at https://hkchengrex.github.io/C2OT",
            "score": 1,
            "issue_id": 2700,
            "pub_date": "2025-03-13",
            "pub_date_card": {
                "ru": "13 марта",
                "en": "March 13",
                "zh": "3月13日"
            },
            "hash": "93d6f410c35246be",
            "authors": [
                "Ho Kei Cheng",
                "Alexander Schwing"
            ],
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.10636.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#inference",
                    "#optimization",
                    "#training"
                ],
                "emoji": "🔀",
                "ru": {
                    "title": "Условный оптимальный транспорт для улучшения потоковых моделей",
                    "desc": "Статья предлагает метод условного оптимального транспорта (C^2OT) для улучшения обучения потоковых моделей в условных задачах. Авторы выявили проблему несоответствия между тренировочным и тестовым распределением при использовании мини-батч оптимального транспорта в условных моделях. C^2OT решает эту проблему путем добавления условного весового коэффициента в матрицу стоимости при вычислении оптимального транспорта. Эксперименты показывают, что предложенный метод работает лучше существующих базовых линий на различных наборах данных и при разных бюджетах вычислений."
                },
                "en": {
                    "title": "Bridging the Gap in Conditional Flow Matching with C^2OT",
                    "desc": "This paper introduces a method called conditional optimal transport (C^2OT) to improve the performance of flow matching in machine learning. The authors highlight that while minibatch optimal transport simplifies computations during inference, it fails in conditional settings due to a mismatch between training and testing distributions. By incorporating a conditional weighting term in the cost matrix, C^2OT effectively aligns the training and testing conditions. Experiments show that this approach outperforms existing methods across various datasets, demonstrating its effectiveness in both discrete and continuous scenarios."
                },
                "zh": {
                    "title": "条件最优传输：缩小训练与测试的性能差距",
                    "desc": "本文提出了一种条件最优传输方法C^2OT，以解决在条件设置下小批量最优传输的不足。传统的最优传输映射在训练时忽略了条件，导致训练期间的先验分布偏斜，而测试时却无法访问这种偏斜的先验。通过在成本矩阵中添加条件加权项，C^2OT能够更好地计算最优传输分配，从而缩小训练与测试之间的性能差距。实验结果表明，该方法在多个数据集上表现优于现有基线。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.10391",
            "title": "CINEMA: Coherent Multi-Subject Video Generation via MLLM-Based Guidance",
            "url": "https://huggingface.co/papers/2503.10391",
            "abstract": "Video generation has witnessed remarkable progress with the advent of deep generative models, particularly diffusion models. While existing methods excel in generating high-quality videos from text prompts or single images, personalized multi-subject video generation remains a largely unexplored challenge. This task involves synthesizing videos that incorporate multiple distinct subjects, each defined by separate reference images, while ensuring temporal and spatial consistency. Current approaches primarily rely on mapping subject images to keywords in text prompts, which introduces ambiguity and limits their ability to model subject relationships effectively. In this paper, we propose CINEMA, a novel framework for coherent multi-subject video generation by leveraging Multimodal Large Language Model (MLLM). Our approach eliminates the need for explicit correspondences between subject images and text entities, mitigating ambiguity and reducing annotation effort. By leveraging MLLM to interpret subject relationships, our method facilitates scalability, enabling the use of large and diverse datasets for training. Furthermore, our framework can be conditioned on varying numbers of subjects, offering greater flexibility in personalized content creation. Through extensive evaluations, we demonstrate that our approach significantly improves subject consistency, and overall video coherence, paving the way for advanced applications in storytelling, interactive media, and personalized video generation.",
            "score": 1,
            "issue_id": 2700,
            "pub_date": "2025-03-13",
            "pub_date_card": {
                "ru": "13 марта",
                "en": "March 13",
                "zh": "3月13日"
            },
            "hash": "d7f2c49b29951ace",
            "authors": [
                "Yufan Deng",
                "Xun Guo",
                "Yizhi Wang",
                "Jacob Zhiyuan Fang",
                "Angtian Wang",
                "Shenghai Yuan",
                "Yiding Yang",
                "Bo Liu",
                "Haibin Huang",
                "Chongyang Ma"
            ],
            "affiliations": [
                "ByteDance Intelligent Creation",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.10391.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#multimodal",
                    "#story_generation",
                    "#video",
                    "#diffusion"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "CINEMA: персонализированная генерация видео с множеством субъектов при помощи MLLM",
                    "desc": "Статья представляет CINEMA - новую систему для генерации видео с несколькими персонажами, использующую мультимодальную большую языковую модель (MLLM). CINEMA устраняет необходимость в явном соответствии между изображениями субъектов и текстовыми сущностями, что уменьшает неоднозначность и упрощает аннотирование. Система позволяет масштабировать процесс, используя большие и разнообразные наборы данных для обучения. CINEMA демонстрирует значительное улучшение согласованности персонажей и общей связности видео по сравнению с существующими методами."
                },
                "en": {
                    "title": "CINEMA: Coherent Multi-Subject Video Generation Made Easy",
                    "desc": "This paper introduces CINEMA, a new framework for generating videos that feature multiple distinct subjects from separate reference images. Unlike traditional methods that rely on mapping images to text prompts, CINEMA uses a Multimodal Large Language Model (MLLM) to understand and interpret relationships between subjects, which reduces ambiguity. The framework allows for flexible conditioning on different numbers of subjects, making it easier to create personalized content. Extensive evaluations show that CINEMA enhances subject consistency and overall video coherence, opening up new possibilities for applications in storytelling and interactive media."
                },
                "zh": {
                    "title": "个性化多主体视频生成的新突破",
                    "desc": "视频生成在深度生成模型，特别是扩散模型的推动下取得了显著进展。现有方法在从文本提示或单张图像生成高质量视频方面表现出色，但个性化的多主体视频生成仍然是一个未被充分探索的挑战。我们提出了CINEMA框架，通过利用多模态大语言模型（MLLM），实现了一致的多主体视频生成，消除了主体图像与文本实体之间的明确对应关系，从而减少了歧义和标注工作。我们的框架能够根据不同数量的主体进行条件生成，提供了更大的个性化内容创作灵活性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.10351",
            "title": "New Trends for Modern Machine Translation with Large Reasoning Models",
            "url": "https://huggingface.co/papers/2503.10351",
            "abstract": "Recent advances in Large Reasoning Models (LRMs), particularly those leveraging Chain-of-Thought reasoning (CoT), have opened brand new possibility for Machine Translation (MT). This position paper argues that LRMs substantially transformed traditional neural MT as well as LLMs-based MT paradigms by reframing translation as a dynamic reasoning task that requires contextual, cultural, and linguistic understanding and reasoning. We identify three foundational shifts: 1) contextual coherence, where LRMs resolve ambiguities and preserve discourse structure through explicit reasoning over cross-sentence and complex context or even lack of context; 2) cultural intentionality, enabling models to adapt outputs by inferring speaker intent, audience expectations, and socio-linguistic norms; 3) self-reflection, LRMs can perform self-reflection during the inference time to correct the potential errors in translation especially extremely noisy cases, showing better robustness compared to simply mapping X->Y translation. We explore various scenarios in translation including stylized translation, document-level translation and multimodal translation by showcasing empirical examples that demonstrate the superiority of LRMs in translation. We also identify several interesting phenomenons for LRMs for MT including auto-pivot translation as well as the critical challenges such as over-localisation in translation and inference efficiency. In conclusion, we think that LRMs redefine translation systems not merely as text converters but as multilingual cognitive agents capable of reasoning about meaning beyond the text. This paradigm shift reminds us to think of problems in translation beyond traditional translation scenarios in a much broader context with LRMs - what we can achieve on top of it.",
            "score": 1,
            "issue_id": 2700,
            "pub_date": "2025-03-13",
            "pub_date_card": {
                "ru": "13 марта",
                "en": "March 13",
                "zh": "3月13日"
            },
            "hash": "c0a1b109c05698cc",
            "authors": [
                "Sinuo Liu",
                "Chenyang Lyu",
                "Minghao Wu",
                "Longyue Wang",
                "Weihua Luo",
                "Kaifu Zhang"
            ],
            "affiliations": [
                "MarcoPolo Team, Alibaba International Digital Commerce",
                "University of Edinburgh"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.10351.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#agents",
                    "#multilingual",
                    "#translation"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "LRM: От простого перевода к многоязычным когнитивным агентам",
                    "desc": "Эта статья рассматривает влияние больших моделей рассуждений (LRM) на машинный перевод. Авторы утверждают, что LRM трансформируют традиционные подходы, представляя перевод как динамическую задачу рассуждения, требующую контекстуального, культурного и лингвистического понимания. Выделяются три ключевых изменения: контекстуальная согласованность, культурная интенциональность и саморефлексия. Статья также исследует различные сценарии применения LRM в переводе и обсуждает возникающие феномены и проблемы."
                },
                "en": {
                    "title": "Transforming Translation: LRMs as Cognitive Agents",
                    "desc": "This paper discusses how Large Reasoning Models (LRMs) are changing the way we think about Machine Translation (MT). It highlights three key shifts: first, LRMs improve contextual coherence by understanding complex contexts and resolving ambiguities; second, they incorporate cultural intentionality, allowing translations to reflect speaker intent and social norms; and third, LRMs can self-reflect during translation to correct errors, making them more robust. The authors provide examples of how LRMs excel in various translation scenarios, suggesting that these models should be seen as cognitive agents that reason about meaning rather than just text converters."
                },
                "zh": {
                    "title": "大型推理模型重塑机器翻译的未来",
                    "desc": "大型推理模型（LRMs）在机器翻译（MT）领域带来了新的可能性，特别是通过链式思维推理（CoT）。这篇论文指出，LRMs将传统神经机器翻译转变为一种动态推理任务，强调上下文、文化和语言理解的重要性。我们识别出三个基础转变：上下文连贯性、文化意图性和自我反思能力，使得LRMs在翻译中表现出更好的鲁棒性。最终，我们认为LRMs重新定义了翻译系统，使其不仅仅是文本转换器，而是能够超越文本进行意义推理的多语言认知代理。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.10072",
            "title": "\"Silent Is Not Actually Silent\": An Investigation of Toxicity on Bug\n  Report Discussion",
            "url": "https://huggingface.co/papers/2503.10072",
            "abstract": "Toxicity in bug report discussions poses significant challenges to the collaborative dynamics of open-source software development. Bug reports are crucial for identifying and resolving defects, yet their inherently problem-focused nature and emotionally charged context make them susceptible to toxic interactions. This study explores toxicity in GitHub bug reports through a qualitative analysis of 203 bug threads, including 81 toxic ones. Our findings reveal that toxicity frequently arises from misaligned perceptions of bug severity and priority, unresolved frustrations with tools, and lapses in professional communication. These toxic interactions not only derail productive discussions but also reduce the likelihood of actionable outcomes, such as linking issues with pull requests. Our preliminary findings offer actionable recommendations to improve bug resolution by mitigating toxicity.",
            "score": 1,
            "issue_id": 2699,
            "pub_date": "2025-03-13",
            "pub_date_card": {
                "ru": "13 марта",
                "en": "March 13",
                "zh": "3月13日"
            },
            "hash": "a91b6bc8232847a3",
            "authors": [
                "Mia Mohammad Imran",
                "Jaydeb Sarker"
            ],
            "affiliations": [
                "Missouri University of Science and Technology, Rolla, Missouri, USA",
                "University of Nebraska Omaha, Omaha, Nebraska, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.10072.jpg",
            "data": {
                "categories": [
                    "#ethics",
                    "#open_source"
                ],
                "emoji": "🐛",
                "ru": {
                    "title": "Токсичность в отчетах об ошибках: препятствие для продуктивного разрешения проблем",
                    "desc": "Исследование посвящено проблеме токсичности в обсуждениях отчетов об ошибках в проектах с открытым исходным кодом. Авторы провели качественный анализ 203 веток обсуждения ошибок на GitHub, включая 81 токсичную. Результаты показывают, что токсичность часто возникает из-за расхождений в восприятии серьезности и приоритетности ошибок, нерешенных проблем с инструментами и нарушений профессиональной коммуникации. Токсичные взаимодействия не только нарушают продуктивные обсуждения, но и снижают вероятность достижения конкретных результатов, таких как привязка проблем к запросам на слияние."
                },
                "en": {
                    "title": "Mitigating Toxicity for Better Bug Resolution in Open Source",
                    "desc": "This paper investigates the issue of toxicity in bug report discussions within open-source software development on GitHub. It analyzes 203 bug threads, identifying 81 as toxic, and highlights that toxicity often stems from differing views on bug severity, frustrations with tools, and poor communication. The study emphasizes that such toxic interactions hinder productive collaboration and decrease the chances of resolving issues effectively. The authors provide recommendations aimed at reducing toxicity to enhance the bug resolution process."
                },
                "zh": {
                    "title": "减少毒性，提升开源协作效率",
                    "desc": "本研究探讨了开源软件开发中，GitHub bug 报告讨论中的毒性问题。通过对203个bug线程的定性分析，发现81个线程存在毒性互动。研究表明，毒性往往源于对bug严重性和优先级的误解、对工具的不满以及专业沟通的缺失。这些毒性互动不仅影响了有效讨论，还降低了将问题与拉取请求关联的可能性，因此提出了减少毒性以改善bug解决的建议。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.09905",
            "title": "Quantization for OpenAI's Whisper Models: A Comparative Analysis",
            "url": "https://huggingface.co/papers/2503.09905",
            "abstract": "Automated speech recognition (ASR) models have gained prominence for applications such as captioning, speech translation, and live transcription. This paper studies Whisper and two model variants: one optimized for live speech streaming and another for offline transcription. Notably, these models have been found to generate hallucinated content, reducing transcription reliability. Furthermore, larger model variants exhibit increased latency and pose challenges for deployment on resource-constrained devices. This study analyzes the similarities and differences between three Whisper models, qualitatively examining their distinct capabilities. Next, this study quantifies the impact of model quantization on latency and evaluates its viability for edge deployment. Using the open source LibriSpeech dataset, this paper evaluates the word error rate (WER) along with latency analysis of whispercpp using 3 quantization methods (INT4, INT5, INT8). Results show that quantization reduces latency by 19\\% and model size by 45\\%, while preserving transcription accuracy. These findings provide insights into the optimal use cases of different Whisper models and edge device deployment possibilities. All code, datasets, and implementation details are available in a public GitHub repository: https://github.com/allisonandreyev/WhisperQuantization.git",
            "score": 1,
            "issue_id": 2700,
            "pub_date": "2025-03-12",
            "pub_date_card": {
                "ru": "12 марта",
                "en": "March 12",
                "zh": "3月12日"
            },
            "hash": "8d7b17a97a4cbb6e",
            "authors": [
                "Allison Andreyev"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2503.09905.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#audio",
                    "#open_source",
                    "#optimization",
                    "#hallucinations",
                    "#dataset"
                ],
                "emoji": "🎙️",
                "ru": {
                    "title": "Оптимизация моделей Whisper для эффективного распознавания речи на периферийных устройствах",
                    "desc": "Это исследование анализирует модели автоматического распознавания речи Whisper и их варианты для потоковой и офлайн-транскрипции. Авторы изучают проблему галлюцинаций в выводе моделей и трудности развертывания больших моделей на устройствах с ограниченными ресурсами. Проводится оценка влияния квантования модели на задержку и точность транскрипции с использованием набора данных LibriSpeech. Результаты показывают, что квантование уменьшает задержку на 19% и размер модели на 45%, сохраняя при этом точность транскрипции."
                },
                "en": {
                    "title": "Optimizing Whisper Models for Efficient Speech Recognition",
                    "desc": "This paper investigates the performance of Whisper, an automated speech recognition (ASR) model, along with its two variants tailored for live streaming and offline transcription. It highlights the issue of hallucinated content in the transcriptions, which can compromise reliability, especially in larger models that also face latency challenges. The study further explores the effects of model quantization on latency and size, demonstrating a significant reduction in both while maintaining transcription accuracy. By analyzing the word error rate (WER) using the LibriSpeech dataset, the research provides valuable insights for deploying Whisper models on resource-constrained devices."
                },
                "zh": {
                    "title": "优化Whisper模型，提升语音识别效率",
                    "desc": "这篇论文研究了自动语音识别（ASR）模型Whisper及其两个变体，一个针对实时语音流优化，另一个用于离线转录。研究发现，这些模型可能会生成虚假内容，从而降低转录的可靠性。此外，较大的模型变体在延迟方面表现较差，给资源受限的设备部署带来了挑战。通过对比分析，论文量化了模型量化对延迟的影响，并评估了其在边缘设备上的可行性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.04723",
            "title": "Shifting Long-Context LLMs Research from Input to Output",
            "url": "https://huggingface.co/papers/2503.04723",
            "abstract": "Recent advancements in long-context Large Language Models (LLMs) have primarily concentrated on processing extended input contexts, resulting in significant strides in long-context comprehension. However, the equally critical aspect of generating long-form outputs has received comparatively less attention. This paper advocates for a paradigm shift in NLP research toward addressing the challenges of long-output generation. Tasks such as novel writing, long-term planning, and complex reasoning require models to understand extensive contexts and produce coherent, contextually rich, and logically consistent extended text. These demands highlight a critical gap in current LLM capabilities. We underscore the importance of this under-explored domain and call for focused efforts to develop foundational LLMs tailored for generating high-quality, long-form outputs, which hold immense potential for real-world applications.",
            "score": 1,
            "issue_id": 2700,
            "pub_date": "2025-03-06",
            "pub_date_card": {
                "ru": "6 марта",
                "en": "March 6",
                "zh": "3月6日"
            },
            "hash": "009f3e654e927dc3",
            "authors": [
                "Yuhao Wu",
                "Yushi Bai",
                "Zhiqing Hu",
                "Shangqing Tu",
                "Ming Shan Hee",
                "Juanzi Li",
                "Roy Ka-Wei Lee"
            ],
            "affiliations": [
                "Singapore University",
                "Singapore University of Technology and Design",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.04723.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#long_context",
                    "#multimodal"
                ],
                "emoji": "📝",
                "ru": {
                    "title": "Новый вызов для ИИ: создание длинных текстов",
                    "desc": "Статья обсуждает необходимость развития моделей обработки естественного языка для генерации длинных текстов. Авторы отмечают, что современные исследования в основном сосредоточены на обработке длинных входных контекстов, но не на создании объемных выходных данных. Подчеркивается важность разработки языковых моделей, способных генерировать связные и логически последовательные длинные тексты. Такие модели необходимы для решения задач написания романов, долгосрочного планирования и сложных рассуждений."
                },
                "en": {
                    "title": "Bridging the Gap: Enhancing Long-Form Output in LLMs",
                    "desc": "This paper discusses the need for improvements in long-form output generation by Large Language Models (LLMs). While recent advancements have focused on understanding long input contexts, generating coherent and contextually rich long outputs remains underexplored. The authors emphasize that tasks like novel writing and complex reasoning require models to produce logically consistent extended text. They call for more research efforts to develop LLMs that can effectively handle these long-output challenges, which are crucial for various real-world applications."
                },
                "zh": {
                    "title": "推动长文本生成的研究转型",
                    "desc": "最近，长上下文的大型语言模型（LLMs）在处理扩展输入上下文方面取得了显著进展，但生成长文本输出的研究相对较少。本文提倡自然语言处理（NLP）研究向解决长输出生成的挑战转变。长篇小说写作、长期规划和复杂推理等任务需要模型理解广泛的上下文，并生成连贯、丰富且逻辑一致的扩展文本。我们强调这一未被充分探索领域的重要性，并呼吁集中力量开发专门用于生成高质量长文本输出的基础LLM，以满足现实应用的巨大潜力。"
                }
            }
        }
    ],
    "link_prev": "2025-03-13.html",
    "link_next": "2025-03-17.html",
    "link_month": "2025-03.html",
    "short_date_prev": {
        "ru": "13.03",
        "en": "03/13",
        "zh": "3月13日"
    },
    "short_date_next": {
        "ru": "17.03",
        "en": "03/17",
        "zh": "3月17日"
    },
    "categories": {
        "#dataset": 2,
        "#data": 2,
        "#benchmark": 2,
        "#agents": 4,
        "#cv": 1,
        "#rl": 2,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 0,
        "#audio": 1,
        "#video": 1,
        "#multimodal": 5,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 0,
        "#reasoning": 4,
        "#transfer_learning": 0,
        "#graphs": 1,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 4,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 1,
        "#hallucinations": 1,
        "#long_context": 2,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0,
        "#translation": 1
    },
    "zh": {
        "text": "这篇文章讨论了视频扩散模型的发展面临的计算需求挑战。为了缓解这一问题，研究人员提出了TPDiff框架，通过分阶段增加帧率来优化计算效率。该框架只在最后阶段使用全帧率，从而提高训练和推理效率。实验结果显示，这种方法可以减少50%的训练成本，并提高1.5倍的推理效率。",
        "title": "TPDiff: Temporal Pyramid Video Diffusion Model",
        "pinyin": "这篇文章讨论了视频扩散模型的发展面临的计算需求挑战。\nZhè piān wénzhāng tǎolùn le shìpín kuòsàn móxíng de fāzhǎn miànlín de jìsuàn xūqiú tiǎozhàn.\n\n为了缓解这一问题，研究人员提出了TPDiff框架，通过分阶段增加帧率来优化计算效率。\nWèile huǎnjiě zhè yī wèntí, yánjiū rényuán tíchū le TP Diff kuàngjià, tōngguò fēn jiēduàn zēngjiā zhēnlǜ lái yōuhuà jìsuàn xiàolǜ.\n\n该框架只在最后阶段使用全帧率，从而提高训练和推理效率。\nGǎi kuàngjià zhǐ zài zuìhòu jiēduàn shǐyòng quán zhēnlǜ, cóng'ér tígāo xùnliàn hé tuīlǐ xiàolǜ.\n\n实验结果显示，这种方法可以减少50%的训练成本，并提高1.5倍的推理效率。\nShíyàn jiéguǒ xiǎnshì, zhè zhǒng fāngfǎ kěyǐ jiǎnshǎo 50% de xùnliàn chéngběn, bìng tígāo 1.5 bèi de tuīlǐ xiàolǜ.",
        "vocab": "[\n    {\"word\": \"讨论\", \"pinyin\": \"tǎo lùn\", \"trans\": \"discuss\"},\n    {\"word\": \"视频\", \"pinyin\": \"shì pín\", \"trans\": \"video\"},\n    {\"word\": \"扩散\", \"pinyin\": \"kuò sàn\", \"trans\": \"diffusion\"},\n    {\"word\": \"模型\", \"pinyin\": \"mó xíng\", \"trans\": \"model\"},\n    {\"word\": \"发展\", \"pinyin\": \"fā zhǎn\", \"trans\": \"development\"},\n    {\"word\": \"面临\", \"pinyin\": \"miàn lín\", \"trans\": \"face\"},\n    {\"word\": \"计算\", \"pinyin\": \"jì suàn\", \"trans\": \"calculation\"},\n    {\"word\": \"需求\", \"pinyin\": \"xū qiú\", \"trans\": \"demand\"},\n    {\"word\": \"挑战\", \"pinyin\": \"tiǎo zhàn\", \"trans\": \"challenge\"},\n    {\"word\": \"缓解\", \"pinyin\": \"huǎn jiě\", \"trans\": \"alleviate\"},\n    {\"word\": \"提出\", \"pinyin\": \"tí chū\", \"trans\": \"propose\"},\n    {\"word\": \"框架\", \"pinyin\": \"kuàng jià\", \"trans\": \"framework\"},\n    {\"word\": \"通过\", \"pinyin\": \"tōng guò\", \"trans\": \"through\"},\n    {\"word\": \"分阶段\", \"pinyin\": \"fēn jiē duàn\", \"trans\": \"in stages\"},\n    {\"word\": \"增加\", \"pinyin\": \"zēng jiā\", \"trans\": \"increase\"},\n    {\"word\": \"帧率\", \"pinyin\": \"zhèn lǜ\", \"trans\": \"frame rate\"},\n    {\"word\": \"优化\", \"pinyin\": \"yōu huà\", \"trans\": \"optimize\"},\n    {\"word\": \"效率\", \"pinyin\": \"xiào lǜ\", \"trans\": \"efficiency\"},\n    {\"word\": \"阶段\", \"pinyin\": \"jiē duàn\", \"trans\": \"stage\"},\n    {\"word\": \"使用\", \"pinyin\": \"shǐ yòng\", \"trans\": \"use\"},\n    {\"word\": \"全帧率\", \"pinyin\": \"quán zhèn lǜ\", \"trans\": \"full frame rate\"},\n    {\"word\": \"从而\", \"pinyin\": \"cóng ér\", \"trans\": \"thus\"},\n    {\"word\": \"提高\", \"pinyin\": \"tí gāo\", \"trans\": \"improve\"},\n    {\"word\": \"训练\", \"pinyin\": \"xùn liàn\", \"trans\": \"training\"},\n    {\"word\": \"推理\", \"pinyin\": \"tuī lǐ\", \"trans\": \"inference\"},\n    {\"word\": \"实验\", \"pinyin\": \"shí yàn\", \"trans\": \"experiment\"},\n    {\"word\": \"结果\", \"pinyin\": \"jié guǒ\", \"trans\": \"result\"},\n    {\"word\": \"显示\", \"pinyin\": \"xiǎn shì\", \"trans\": \"show\"},\n    {\"word\": \"方法\", \"pinyin\": \"fāng fǎ\", \"trans\": \"method\"},\n    {\"word\": \"减少\", \"pinyin\": \"jiǎn shǎo\", \"trans\": \"reduce\"},\n    {\"word\": \"成本\", \"pinyin\": \"chéng běn\", \"trans\": \"cost\"},\n    {\"word\": \"提高\", \"pinyin\": \"tí gāo\", \"trans\": \"increase\"}\n]",
        "trans": "This article discusses the computational challenges faced in the development of video diffusion models. To mitigate this issue, researchers have proposed the TPDiff framework, which optimizes computational efficiency by incrementally increasing the frame rate in stages. The framework only uses the full frame rate in the final stage, thereby enhancing both training and inference efficiency. Experimental results show that this method can reduce training costs by 50% and improve inference efficiency by 1.5 times.",
        "update_ts": "2025-03-13 09:11"
    }
}