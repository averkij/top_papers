
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 21 papers. March 21.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">21 марта</span> | <span id="title-articles-count">21 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-03-20.html">⬅️ <span id="prev-date">20.03</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-03-24.html">➡️ <span id="next-date">24.03</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-03.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '21 марта', 'en': 'March 21', 'zh': '3月21日'};
        let feedDateNext = {'ru': '24.03', 'en': '03/24', 'zh': '3月24日'};
        let feedDatePrev = {'ru': '20.03', 'en': '03/20', 'zh': '3月20日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2503.16419', 'title': 'Stop Overthinking: A Survey on Efficient Reasoning for Large Language\n  Models', 'url': 'https://huggingface.co/papers/2503.16419', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable capabilities in complex tasks. Recent advancements in Large Reasoning Models (LRMs), such as OpenAI o1 and DeepSeek-R1, have further improved performance in System-2 reasoning domains like mathematics and programming by harnessing supervised fine-tuning (SFT) and reinforcement learning (RL) techniques to enhance the Chain-of-Thought (CoT) reasoning. However, while longer CoT reasoning sequences improve performance, they also introduce significant computational overhead due to verbose and redundant outputs, known as the "overthinking phenomenon". In this paper, we provide the first structured survey to systematically investigate and explore the current progress toward achieving efficient reasoning in LLMs. Overall, relying on the inherent mechanism of LLMs, we categorize existing works into several key directions: (1) model-based efficient reasoning, which considers optimizing full-length reasoning models into more concise reasoning models or directly training efficient reasoning models; (2) reasoning output-based efficient reasoning, which aims to dynamically reduce reasoning steps and length during inference; (3) input prompts-based efficient reasoning, which seeks to enhance reasoning efficiency based on input prompt properties such as difficulty or length control. Additionally, we introduce the use of efficient data for training reasoning models, explore the reasoning capabilities of small language models, and discuss evaluation methods and benchmarking.', 'score': 20, 'issue_id': 2824, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': 'dc9b04a227f74af7', 'authors': ['Yang Sui', 'Yu-Neng Chuang', 'Guanchu Wang', 'Jiamu Zhang', 'Tianyi Zhang', 'Jiayi Yuan', 'Hongyi Liu', 'Andrew Wen', 'Shaochen', 'Zhong', 'Hanjie Chen', 'Xia Hu'], 'affiliations': ['Department of Computer Science, Rice University'], 'pdf_title_img': 'assets/pdf/title_img/2503.16419.jpg', 'data': {'categories': ['#small_models', '#optimization', '#reasoning', '#rl', '#dataset', '#survey', '#training', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Эффективное рассуждение в LLM: преодоление избыточности для оптимальной производительности', 'desc': 'Эта статья представляет собой обзор методов повышения эффективности рассуждений в больших языковых моделях (LLM). Авторы рассматривают три основных направления: оптимизация самих моделей, динамическое сокращение шагов рассуждения при выводе и улучшение входных промптов. Также обсуждается использование эффективных данных для обучения моделей рассуждения и возможности небольших языковых моделей. В работе отмечается, что хотя более длинные цепочки рассуждений улучшают производительность, они также увеличивают вычислительные затраты из-за многословности и избыточности.'}, 'en': {'title': 'Optimizing Reasoning Efficiency in Large Language Models', 'desc': 'This paper surveys the advancements in Large Reasoning Models (LRMs) that enhance reasoning capabilities in complex tasks like mathematics and programming. It highlights the use of supervised fine-tuning and reinforcement learning to improve Chain-of-Thought (CoT) reasoning, while addressing the computational challenges posed by longer reasoning sequences. The authors categorize existing approaches into three main strategies: optimizing reasoning models, reducing reasoning output length, and improving input prompts for efficiency. Additionally, the paper discusses the potential of small language models and introduces methods for evaluating reasoning performance.'}, 'zh': {'title': '高效推理：大型语言模型的新探索', 'desc': '大型语言模型（LLMs）在复杂任务中展现了卓越的能力。最近，大型推理模型（LRMs）的进展进一步提升了在数学和编程等系统-2推理领域的表现，利用监督微调（SFT）和强化学习（RL）技术来增强思维链（CoT）推理。然而，较长的CoT推理序列虽然提高了性能，却也带来了显著的计算开销，这种现象被称为“过度思考现象”。本文首次系统性地调查和探索了实现LLMs高效推理的当前进展，并将现有工作分类为多个关键方向。'}}}, {'id': 'https://huggingface.co/papers/2503.16302', 'title': 'Unleashing Vecset Diffusion Model for Fast Shape Generation', 'url': 'https://huggingface.co/papers/2503.16302', 'abstract': '3D shape generation has greatly flourished through the development of so-called "native" 3D diffusion, particularly through the Vecset Diffusion Model (VDM). While recent advancements have shown promising results in generating high-resolution 3D shapes, VDM still struggles with high-speed generation. Challenges exist because of difficulties not only in accelerating diffusion sampling but also VAE decoding in VDM, areas under-explored in previous works. To address these challenges, we present FlashVDM, a systematic framework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables flexible diffusion sampling with as few as 5 inference steps and comparable quality, which is made possible by stabilizing consistency distillation with our newly introduced Progressive Flow Distillation. For VAE, we introduce a lightning vecset decoder equipped with Adaptive KV Selection, Hierarchical Volume Decoding, and Efficient Network Design. By exploiting the locality of the vecset and the sparsity of shape surface in the volume, our decoder drastically lowers FLOPs, minimizing the overall decoding overhead. We apply FlashVDM to Hunyuan3D-2 to obtain Hunyuan3D-2 Turbo. Through systematic evaluation, we show that our model significantly outperforms existing fast 3D generation methods, achieving comparable performance to the state-of-the-art while reducing inference time by over 45x for reconstruction and 32x for generation. Code and models are available at https://github.com/Tencent/FlashVDM.', 'score': 15, 'issue_id': 2824, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': 'd91e862542d76892', 'authors': ['Zeqiang Lai', 'Yunfei Zhao', 'Zibo Zhao', 'Haolin Liu', 'Fuyun Wang', 'Huiwen Shi', 'Xianghui Yang', 'Qinxiang Lin', 'Jinwei Huang', 'Yuhong Liu', 'Jie Jiang', 'Chunchao Guo', 'Xiangyu Yue'], 'affiliations': ['MMLab, CUHK', 'ShanghaiTech', 'Tencent Hunyuan'], 'pdf_title_img': 'assets/pdf/title_img/2503.16302.jpg', 'data': {'categories': ['#open_source', '#3d', '#diffusion', '#optimization', '#inference'], 'emoji': '🚀', 'ru': {'title': 'Молниеносная генерация 3D-форм с FlashVDM', 'desc': 'Статья представляет FlashVDM - систему для ускорения генерации 3D-форм с использованием векторных диффузионных моделей (VDM). Авторы предлагают улучшения как для процесса диффузии, так и для декодирования VAE, включая метод Progressive Flow Distillation и оптимизированный декодер векторных множеств. FlashVDM позволяет генерировать 3D-формы высокого качества всего за 5 шагов вывода, значительно сокращая время inference. Результаты показывают превосходство FlashVDM над существующими методами быстрой 3D-генерации, уменьшая время вывода в 32-45 раз при сохранении качества на уровне state-of-the-art.'}, 'en': {'title': 'Accelerating 3D Shape Generation with FlashVDM', 'desc': 'This paper introduces FlashVDM, a new framework designed to enhance the speed of 3D shape generation using the Vecset Diffusion Model (VDM). It addresses the slow generation times associated with VDM by improving both the Variational Autoencoder (VAE) and the Diffusion Transformer (DiT) components. FlashVDM achieves faster diffusion sampling with fewer inference steps while maintaining high-quality outputs through a technique called Progressive Flow Distillation. Additionally, it optimizes the VAE with a new decoder that reduces computational load, resulting in significant improvements in inference speed without sacrificing performance.'}, 'zh': {'title': '加速3D形状生成的FlashVDM框架', 'desc': '本论文介绍了一种名为FlashVDM的框架，旨在加速3D形状生成中的VAE和DiT过程。通过引入渐进流蒸馏，FlashVDM实现了灵活的扩散采样，仅需5个推理步骤即可获得与现有方法相当的质量。我们还设计了一种闪电vecset解码器，利用自适应KV选择和分层体积解码，显著降低了计算复杂度。实验结果表明，FlashVDM在重建和生成方面的推理时间分别减少了超过45倍和32倍，显著优于现有的快速3D生成方法。'}}}, {'id': 'https://huggingface.co/papers/2503.16212', 'title': 'MathFusion: Enhancing Mathematic Problem-solving of LLM through\n  Instruction Fusion', 'url': 'https://huggingface.co/papers/2503.16212', 'abstract': 'Large Language Models (LLMs) have shown impressive progress in mathematical reasoning. While data augmentation is promising to enhance mathematical problem-solving ability, current approaches are predominantly limited to instance-level modifications-such as rephrasing or generating syntactic variations-which fail to capture and leverage the intrinsic relational structures inherent in mathematical knowledge. Inspired by human learning processes, where mathematical proficiency develops through systematic exposure to interconnected concepts, we introduce MathFusion, a novel framework that enhances mathematical reasoning through cross-problem instruction synthesis. MathFusion implements this through three fusion strategies: (1) sequential fusion, which chains related problems to model solution dependencies; (2) parallel fusion, which combines analogous problems to reinforce conceptual understanding; and (3) conditional fusion, which creates context-aware selective problems to enhance reasoning flexibility. By applying these strategies, we generate a new dataset, MathFusionQA, followed by fine-tuning models (DeepSeekMath-7B, Mistral-7B, Llama3-8B) on it. Experimental results demonstrate that MathFusion achieves substantial improvements in mathematical reasoning while maintaining high data efficiency, boosting performance by 18.0 points in accuracy across diverse benchmarks while requiring only 45K additional synthetic instructions, representing a substantial improvement over traditional single-instruction approaches. Our datasets, models, and code are publicly available at https://github.com/QizhiPei/mathfusion.', 'score': 12, 'issue_id': 2823, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': 'b5997ebd979f98f0', 'authors': ['Qizhi Pei', 'Lijun Wu', 'Zhuoshi Pan', 'Yu Li', 'Honglin Lin', 'Chenlin Ming', 'Xin Gao', 'Conghui He', 'Rui Yan'], 'affiliations': ['Gaoling School of Artificial Intelligence, Renmin University of China', 'School of Computer Science, Wuhan University', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.16212.jpg', 'data': {'categories': ['#training', '#synthetic', '#open_source', '#math', '#reasoning', '#dataset', '#benchmark', '#data'], 'emoji': '🧮', 'ru': {'title': 'Слияние задач для прокачки математических способностей ИИ', 'desc': 'MathFusion - это новый подход к улучшению математического мышления у больших языковых моделей (LLM) через синтез инструкций на основе связей между задачами. Метод использует три стратегии слияния задач: последовательное, параллельное и условное, что позволяет создавать более сложные и взаимосвязанные обучающие примеры. Авторы создали датасет MathFusionQA и провели эксперименты на нескольких LLM, показав значительное улучшение точности на 18 процентных пунктов при использовании всего 45 тысяч дополнительных синтетических инструкций. Этот подход демонстрирует существенное преимущество над традиционными методами обучения на отдельных задачах.'}, 'en': {'title': 'Enhancing Mathematical Reasoning through Conceptual Fusion', 'desc': 'This paper presents MathFusion, a new framework designed to improve the mathematical reasoning capabilities of Large Language Models (LLMs). Unlike traditional methods that only modify individual problems, MathFusion uses three innovative fusion strategies to connect related mathematical concepts and problems. These strategies include sequential, parallel, and conditional fusion, which help models learn from the relationships between problems rather than in isolation. The results show that MathFusion significantly enhances accuracy in mathematical reasoning tasks while being efficient in data usage, outperforming previous methods with fewer additional instructions.'}, 'zh': {'title': '通过MathFusion提升数学推理能力', 'desc': '大型语言模型在数学推理方面取得了显著进展。现有的数据增强方法主要集中在实例级别的修改，未能有效利用数学知识中固有的关系结构。我们提出了MathFusion框架，通过跨问题的指令合成来提升数学推理能力。该框架采用三种融合策略，显著提高了模型在数学推理任务中的表现。'}}}, {'id': 'https://huggingface.co/papers/2503.15558', 'title': 'Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning', 'url': 'https://huggingface.co/papers/2503.15558', 'abstract': 'Physical AI systems need to perceive, understand, and perform complex actions in the physical world. In this paper, we present the Cosmos-Reason1 models that can understand the physical world and generate appropriate embodied decisions (e.g., next step action) in natural language through long chain-of-thought reasoning processes. We begin by defining key capabilities for Physical AI reasoning, with a focus on physical common sense and embodied reasoning. To represent physical common sense, we use a hierarchical ontology that captures fundamental knowledge about space, time, and physics. For embodied reasoning, we rely on a two-dimensional ontology that generalizes across different physical embodiments. Building on these capabilities, we develop two multimodal large language models, Cosmos-Reason1-8B and Cosmos-Reason1-56B. We curate data and train our models in four stages: vision pre-training, general supervised fine-tuning (SFT), Physical AI SFT, and Physical AI reinforcement learning (RL) as the post-training. To evaluate our models, we build comprehensive benchmarks for physical common sense and embodied reasoning according to our ontologies. Evaluation results show that Physical AI SFT and reinforcement learning bring significant improvements. To facilitate the development of Physical AI, we will make our code and pre-trained models available under the NVIDIA Open Model License at https://github.com/nvidia-cosmos/cosmos-reason1.', 'score': 10, 'issue_id': 2823, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 марта', 'en': 'March 18', 'zh': '3月18日'}, 'hash': '882bcaa2257d8251', 'authors': ['NVIDIA', ':', 'Alisson Azzolini', 'Hannah Brandon', 'Prithvijit Chattopadhyay', 'Huayu Chen', 'Jinju Chu', 'Yin Cui', 'Jenna Diamond', 'Yifan Ding', 'Francesco Ferroni', 'Rama Govindaraju', 'Jinwei Gu', 'Siddharth Gururani', 'Imad El Hanafi', 'Zekun Hao', 'Jacob Huffman', 'Jingyi Jin', 'Brendan Johnson', 'Rizwan Khan', 'George Kurian', 'Elena Lantz', 'Nayeon Lee', 'Zhaoshuo Li', 'Xuan Li', 'Tsung-Yi Lin', 'Yen-Chen Lin', 'Ming-Yu Liu', 'Andrew Mathau', 'Yun Ni', 'Lindsey Pavao', 'Wei Ping', 'David W. Romero', 'Misha Smelyanskiy', 'Shuran Song', 'Lyne Tchapmi', 'Andrew Z. Wang', 'Boxin Wang', 'Haoxiang Wang', 'Fangyin Wei', 'Jiashu Xu', 'Yao Xu', 'Xiaodong Yang', 'Zhuolin Yang', 'Xiaohui Zeng', 'Zhe Zhang'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2503.15558.jpg', 'data': {'categories': ['#training', '#rl', '#benchmark', '#open_source', '#agents', '#reasoning', '#multimodal'], 'emoji': '🤖', 'ru': {'title': 'Физический ИИ: от понимания мира к воплощенным действиям', 'desc': 'В статье представлены модели Cosmos-Reason1, способные понимать физический мир и генерировать решения для воплощенных агентов с помощью цепочек рассуждений на естественном языке. Модели основаны на иерархической онтологии физического здравого смысла и двумерной онтологии для обобщения различных физических воплощений. Обучение проводилось в четыре этапа: предобучение зрения, общая контролируемая доводка, контролируемая доводка для физического ИИ и обучение с подкреплением. Результаты оценки показывают значительные улучшения после этапов доводки и обучения с подкреплением для физического ИИ.'}, 'en': {'title': 'Empowering Physical AI with Cosmos-Reason1 Models', 'desc': 'This paper introduces the Cosmos-Reason1 models designed for Physical AI systems, which need to understand and act in the physical world. The models utilize long chain-of-thought reasoning to generate actions in natural language, focusing on physical common sense and embodied reasoning. A hierarchical ontology is employed to represent physical common sense, while a two-dimensional ontology supports various physical embodiments. The authors train their models through a structured process, including vision pre-training and reinforcement learning, and demonstrate significant performance improvements through comprehensive evaluations.'}, 'zh': {'title': '物理人工智能的推理与决策新突破', 'desc': '本文介绍了Cosmos-Reason1模型，这是一种能够理解物理世界并通过长链推理过程生成适当决策的物理人工智能系统。我们定义了物理人工智能推理的关键能力，重点关注物理常识和具身推理。为了表示物理常识，我们使用了一个层次本体，捕捉空间、时间和物理学的基本知识。通过多模态大语言模型的开发，我们的研究展示了物理人工智能在推理和决策中的显著进步。'}}}, {'id': 'https://huggingface.co/papers/2503.16365', 'title': 'JARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play\n  Visual Games with Keyboards and Mouse', 'url': 'https://huggingface.co/papers/2503.16365', 'abstract': "Recently, action-based decision-making in open-world environments has gained significant attention. Visual Language Action (VLA) models, pretrained on large-scale web datasets, have shown promise in decision-making tasks. However, previous work has primarily focused on action post-training, often neglecting enhancements to the foundational model itself. In response, we introduce a novel approach, Act from Visual Language Post-Training, which refines Visual Language Models (VLMs) through visual and linguistic guidance in a self-supervised manner. This enhancement improves the models' capabilities in world knowledge, visual recognition, and spatial grounding in open-world environments. Following the above post-training paradigms, we obtain the first VLA models in Minecraft that can follow human instructions on over 1k different atomic tasks, including crafting, smelting, cooking, mining, and killing. Our experiments demonstrate that post-training on non-trajectory tasks leads to a significant 40% improvement over the best agent baseline on a diverse set of atomic tasks. Furthermore, we demonstrate that our approach surpasses traditional imitation learning-based policies in Minecraft, achieving state-of-the-art performance. We have open-sourced the code, models, and datasets to foster further research. The project page can be found in https://craftjarvis.github.io/JarvisVLA.", 'score': 8, 'issue_id': 2824, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': '764fc9201b406ec4', 'authors': ['Muyao Li', 'Zihao Wang', 'Kaichen He', 'Xiaojian Ma', 'Yitao Liang'], 'affiliations': ['BIGAI', 'Peking University', 'Team CraftJarvis'], 'pdf_title_img': 'assets/pdf/title_img/2503.16365.jpg', 'data': {'categories': ['#open_source', '#games', '#cv', '#agents', '#training'], 'emoji': '🤖', 'ru': {'title': 'Улучшение VLA моделей для принятия решений в открытых средах', 'desc': 'Статья представляет новый подход к обучению моделей визуального языка и действий (VLA) для принятия решений в открытых средах. Авторы предлагают метод Act from Visual Language Post-Training, который улучшает базовые модели VLM через визуальное и лингвистическое руководство. Этот подход значительно повышает способности моделей в области знаний о мире, визуального распознавания и пространственной привязки. Эксперименты в Minecraft показали 40% улучшение по сравнению с базовыми агентами на разнообразных атомарных задачах.'}, 'en': {'title': 'Enhancing Decision-Making in Open Worlds with Visual Language Models', 'desc': "This paper presents a new method called Act from Visual Language Post-Training, which enhances Visual Language Models (VLMs) for decision-making in open-world environments. The approach uses visual and linguistic guidance in a self-supervised way to improve the models' understanding of world knowledge, visual recognition, and spatial grounding. The authors demonstrate that their VLA models can effectively follow human instructions in Minecraft for over 1,000 different tasks, achieving a 40% performance boost compared to existing agents. Additionally, their method outperforms traditional imitation learning techniques, setting a new standard in the field."}, 'zh': {'title': '提升开放世界决策能力的视觉语言模型', 'desc': '最近，基于动作的决策在开放世界环境中引起了广泛关注。我们提出了一种新方法，称为视觉语言后训练（Act from Visual Language Post-Training），通过视觉和语言指导自我监督地改进视觉语言模型（VLMs）。这种增强提高了模型在世界知识、视觉识别和空间定位方面的能力。我们的实验表明，在Minecraft中，我们的模型能够执行超过1000个不同的原子任务，并在多样化的任务上实现了40%的性能提升。'}}}, {'id': 'https://huggingface.co/papers/2503.16356', 'title': 'CaKE: Circuit-aware Editing Enables Generalizable Knowledge Learners', 'url': 'https://huggingface.co/papers/2503.16356', 'abstract': 'Knowledge Editing (KE) enables the modification of outdated or incorrect information in large language models (LLMs). While existing KE methods can update isolated facts, they struggle to generalize these updates to multi-hop reasoning tasks that depend on the modified knowledge. Through an analysis of reasoning circuits -- the neural pathways LLMs use for knowledge-based inference, we observe that current layer-localized KE approaches, such as MEMIT and WISE, which edit only single or a few model layers, struggle to effectively incorporate updated information into these reasoning pathways. To address this limitation, we propose CaKE (Circuit-aware Knowledge Editing), a novel method that enables more effective integration of updated knowledge in LLMs. CaKE leverages strategically curated data, guided by our circuits-based analysis, that enforces the model to utilize the modified knowledge, stimulating the model to develop appropriate reasoning circuits for newly integrated knowledge. Experimental results show that CaKE enables more accurate and consistent use of updated knowledge across related reasoning tasks, leading to an average of 20% improvement in multi-hop reasoning accuracy on MQuAKE dataset compared to existing KE methods. We release the code and data in https://github.com/zjunlp/CaKE.', 'score': 8, 'issue_id': 2823, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': '8b74fed43c99c37d', 'authors': ['Yunzhi Yao', 'Jizhan Fang', 'Jia-Chen Gu', 'Ningyu Zhang', 'Shumin Deng', 'Huajun Chen', 'Nanyun Peng'], 'affiliations': ['National University of Singapore', 'University of California, Los Angeles', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.16356.jpg', 'data': {'categories': ['#training', '#open_source', '#reasoning', '#dataset', '#data'], 'emoji': '🧠', 'ru': {'title': 'CaKE: улучшение рассуждений в ИИ через умное редактирование знаний', 'desc': 'Статья представляет новый метод редактирования знаний в больших языковых моделях под названием CaKE. Этот метод позволяет более эффективно интегрировать обновленные знания в модели, улучшая их способность к многоступенчатым рассуждениям. CaKE использует стратегически подобранные данные, основанные на анализе нейронных путей, которые модели используют для логических выводов. Эксперименты показывают, что CaKE обеспечивает более точное и последовательное использование обновленных знаний в связанных задачах рассуждения, повышая точность многоступенчатых рассуждений в среднем на 20% по сравнению с существующими методами.'}, 'en': {'title': 'Enhancing Knowledge Integration in Language Models with CaKE', 'desc': 'This paper introduces CaKE (Circuit-aware Knowledge Editing), a new method for updating knowledge in large language models (LLMs). Current methods struggle with multi-hop reasoning tasks that require the integration of modified information. CaKE improves upon these methods by analyzing the neural pathways used for reasoning and ensuring that updated knowledge is effectively incorporated. The results show a significant increase in accuracy for reasoning tasks, demonstrating the effectiveness of this approach.'}, 'zh': {'title': '电路感知知识编辑：提升多跳推理的准确性', 'desc': '知识编辑（KE）使得大型语言模型（LLMs）能够修改过时或不正确的信息。现有的KE方法虽然可以更新孤立的事实，但在多跳推理任务中却难以有效推广这些更新。通过对推理电路的分析，我们发现当前的层局部KE方法在有效整合更新信息方面存在困难。为了解决这个问题，我们提出了CaKE（电路感知知识编辑），它通过精心策划的数据，促进模型利用修改后的知识，从而提高多跳推理的准确性。'}}}, {'id': 'https://huggingface.co/papers/2503.16418', 'title': 'InfiniteYou: Flexible Photo Recrafting While Preserving Your Identity', 'url': 'https://huggingface.co/papers/2503.16418', 'abstract': 'Achieving flexible and high-fidelity identity-preserved image generation remains formidable, particularly with advanced Diffusion Transformers (DiTs) like FLUX. We introduce InfiniteYou (InfU), one of the earliest robust frameworks leveraging DiTs for this task. InfU addresses significant issues of existing methods, such as insufficient identity similarity, poor text-image alignment, and low generation quality and aesthetics. Central to InfU is InfuseNet, a component that injects identity features into the DiT base model via residual connections, enhancing identity similarity while maintaining generation capabilities. A multi-stage training strategy, including pretraining and supervised fine-tuning (SFT) with synthetic single-person-multiple-sample (SPMS) data, further improves text-image alignment, ameliorates image quality, and alleviates face copy-pasting. Extensive experiments demonstrate that InfU achieves state-of-the-art performance, surpassing existing baselines. In addition, the plug-and-play design of InfU ensures compatibility with various existing methods, offering a valuable contribution to the broader community.', 'score': 6, 'issue_id': 2824, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': '71a484a2a785f8be', 'authors': ['Liming Jiang', 'Qing Yan', 'Yumin Jia', 'Zichuan Liu', 'Hao Kang', 'Xin Lu'], 'affiliations': ['ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2503.16418.jpg', 'data': {'categories': ['#cv', '#diffusion', '#architecture', '#synthetic', '#training'], 'emoji': '🖼️', 'ru': {'title': 'InfiniteYou: Новый уровень генерации изображений с сохранением идентичности', 'desc': 'InfiniteYou (InfU) - это новая система для генерации изображений с сохранением идентичности, использующая Диффузионные Трансформеры (DiT). Основной компонент InfU - InfuseNet, который внедряет признаки идентичности в базовую модель DiT через остаточные соединения. Система использует многоэтапную стратегию обучения, включая предобучение и контролируемую тонкую настройку на синтетических данных. Эксперименты показывают, что InfU превосходит существующие методы по качеству генерации и соответствию текста изображению.'}, 'en': {'title': 'InfiniteYou: Revolutionizing Identity-Preserved Image Generation with DiTs', 'desc': 'This paper presents InfiniteYou (InfU), a novel framework that enhances identity-preserved image generation using advanced Diffusion Transformers (DiTs) like FLUX. InfU tackles key challenges in the field, such as improving identity similarity, aligning text with images, and enhancing overall image quality. The core innovation is InfuseNet, which integrates identity features into the DiT model through residual connections, thereby boosting identity preservation. With a multi-stage training approach that includes pretraining and supervised fine-tuning on synthetic data, InfU achieves superior performance compared to existing methods, while also being adaptable to various frameworks.'}, 'zh': {'title': '无限可能的身份保留图像生成', 'desc': '本文介绍了一种名为InfiniteYou（InfU）的新框架，旨在实现高保真度的身份保留图像生成。InfU利用先进的扩散变换器（DiTs），解决了现有方法在身份相似性、文本与图像对齐以及生成质量等方面的不足。其核心组件InfuseNet通过残差连接将身份特征注入到DiT基础模型中，从而增强身份相似性，同时保持生成能力。通过多阶段训练策略，包括预训练和监督微调，InfU显著提高了文本与图像的对齐度和图像质量，实验结果表明其性能超越了现有基准。'}}}, {'id': 'https://huggingface.co/papers/2503.16188', 'title': 'CLS-RL: Image Classification with Rule-Based Reinforcement Learning', 'url': 'https://huggingface.co/papers/2503.16188', 'abstract': "Classification is a core task in machine learning. Recent research has shown that although Multimodal Large Language Models (MLLMs) are initially poor at image classification, fine-tuning them with an adequate amount of data can significantly enhance their performance, making them comparable to SOTA classification models. However, acquiring large-scale labeled data is expensive. In this paper, we explore few-shot MLLM classification fine-tuning. We found that SFT can cause severe overfitting issues and may even degrade performance over the zero-shot approach. To address this challenge, inspired by the recent successes in rule-based reinforcement learning, we propose CLS-RL, which uses verifiable signals as reward to fine-tune MLLMs. We discovered that CLS-RL outperforms SFT in most datasets and has a much higher average accuracy on both base-to-new and few-shot learning setting. Moreover, we observed a free-lunch phenomenon for CLS-RL; when models are fine-tuned on a particular dataset, their performance on other distinct datasets may also improve over zero-shot models, even if those datasets differ in distribution and class names. This suggests that RL-based methods effectively teach models the fundamentals of classification. Lastly, inspired by recent works in inference time thinking, we re-examine the `thinking process' during fine-tuning, a critical aspect of RL-based methods, in the context of visual classification. We question whether such tasks require extensive thinking process during fine-tuning, proposing that this may actually detract from performance. Based on this premise, we introduce the No-Thinking-CLS-RL method, which minimizes thinking processes during training by setting an equality accuracy reward. Our findings indicate that, with much less fine-tuning time, No-Thinking-CLS-RL method achieves superior in-domain performance and generalization capabilities than CLS-RL.", 'score': 5, 'issue_id': 2826, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': '904362034cdf6d16', 'authors': ['Ming Li', 'Shitian Zhao', 'Jike Zhong', 'Yuxiang Lai', 'Kaipeng Zhang'], 'affiliations': ['Emory University', 'Shanghai AI Laboratory', 'University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2503.16188.jpg', 'data': {'categories': ['#rl', '#transfer_learning', '#optimization', '#cv', '#multimodal', '#training'], 'emoji': '🤖', 'ru': {'title': 'Обучение с подкреплением открывает новые горизонты в классификации изображений для мультимодальных ИИ', 'desc': "Статья исследует применение обучения с подкреплением для улучшения классификации изображений мультимодальными большими языковыми моделями (MLLM) в условиях малого количества обучающих данных. Авторы предлагают метод CLS-RL, использующий проверяемые сигналы в качестве награды для дообучения MLLM. Результаты показывают, что CLS-RL превосходит стандартное дообучение и демонстрирует улучшение точности на различных наборах данных. Исследователи также вводят метод No-Thinking-CLS-RL, минимизирующий 'процесс мышления' во время обучения, что приводит к лучшим результатам при меньшем времени дообучения."}, 'en': {'title': 'Reinforcement Learning Revolutionizes Few-Shot Image Classification', 'desc': 'This paper investigates the fine-tuning of Multimodal Large Language Models (MLLMs) for image classification, particularly in few-shot scenarios. It highlights the limitations of standard supervised fine-tuning (SFT), which can lead to overfitting and reduced performance compared to zero-shot methods. To overcome these challenges, the authors propose a novel approach called CLS-RL, which utilizes reinforcement learning with verifiable signals as rewards, resulting in improved accuracy across various datasets. Additionally, they introduce the No-Thinking-CLS-RL method, which minimizes unnecessary cognitive processes during training, further enhancing performance and generalization with less fine-tuning time.'}, 'zh': {'title': '少样本微调，提升分类性能的创新方法', 'desc': '本文探讨了多模态大型语言模型（MLLMs）在图像分类任务中的应用。尽管这些模型在初始阶段的分类性能较差，但通过少量样本的微调，可以显著提升其表现。我们提出了一种新的方法CLS-RL，利用可验证信号作为奖励来微调MLLMs，结果显示其在大多数数据集上优于传统的微调方法。最后，我们还提出了No-Thinking-CLS-RL方法，通过减少微调过程中的思考时间，进一步提高了模型的性能和泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2503.16422', 'title': '1000+ FPS 4D Gaussian Splatting for Dynamic Scene Rendering', 'url': 'https://huggingface.co/papers/2503.16422', 'abstract': '4D Gaussian Splatting (4DGS) has recently gained considerable attention as a method for reconstructing dynamic scenes. Despite achieving superior quality, 4DGS typically requires substantial storage and suffers from slow rendering speed. In this work, we delve into these issues and identify two key sources of temporal redundancy. (Q1) Short-Lifespan Gaussians: 4DGS uses a large portion of Gaussians with short temporal span to represent scene dynamics, leading to an excessive number of Gaussians. (Q2) Inactive Gaussians: When rendering, only a small subset of Gaussians contributes to each frame. Despite this, all Gaussians are processed during rasterization, resulting in redundant computation overhead. To address these redundancies, we present 4DGS-1K, which runs at over 1000 FPS on modern GPUs. For Q1, we introduce the Spatial-Temporal Variation Score, a new pruning criterion that effectively removes short-lifespan Gaussians while encouraging 4DGS to capture scene dynamics using Gaussians with longer temporal spans. For Q2, we store a mask for active Gaussians across consecutive frames, significantly reducing redundant computations in rendering. Compared to vanilla 4DGS, our method achieves a 41times reduction in storage and 9times faster rasterization speed on complex dynamic scenes, while maintaining comparable visual quality. Please see our project page at https://4DGS-1K.github.io.', 'score': 4, 'issue_id': 2823, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': 'a00a6529a3f82f89', 'authors': ['Yuheng Yuan', 'Qiuhong Shen', 'Xingyi Yang', 'Xinchao Wang'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.16422.jpg', 'data': {'categories': ['#3d', '#inference', '#optimization'], 'emoji': '🚀', 'ru': {'title': '4DGS-1K: Сверхбыстрая реконструкция динамических сцен', 'desc': 'Статья представляет метод 4DGS-1K для оптимизации 4D гауссового сплаттинга в реконструкции динамических сцен. Авторы решают проблемы избыточного хранения и медленного рендеринга, вызванные короткоживущими гауссианами и неактивными гауссианами при визуализации. Предложенный подход использует новый критерий отсечения на основе пространственно-временной вариационной оценки и маскирование активных гауссианов. В результате достигается 41-кратное уменьшение объема хранения и 9-кратное ускорение растеризации по сравнению с обычным 4DGS при сохранении сопоставимого визуального качества.'}, 'en': {'title': 'Streamlining Dynamic Scene Reconstruction with 4DGS-1K', 'desc': 'This paper introduces 4DGS-1K, an improved version of 4D Gaussian Splatting that addresses issues of high storage requirements and slow rendering speeds in dynamic scene reconstruction. The authors identify two main sources of inefficiency: the use of short-lifespan Gaussians that clutter the representation and the processing of inactive Gaussians during rendering. To optimize performance, they propose a new pruning criterion called the Spatial-Temporal Variation Score, which helps retain only the most relevant Gaussians. As a result, 4DGS-1K achieves a significant reduction in storage and rendering time while preserving visual quality, making it a more efficient solution for real-time applications.'}, 'zh': {'title': '提升动态场景重建效率的4DGS-1K', 'desc': '4D高斯点云（4DGS）是一种用于重建动态场景的方法，但通常需要大量存储并且渲染速度较慢。本文探讨了导致这些问题的两个主要来源：短生命周期高斯和非活动高斯。我们提出了4DGS-1K，通过引入空间-时间变化评分来有效去除短生命周期高斯，并存储活跃高斯的掩码，从而显著减少冗余计算。与传统4DGS相比，我们的方法在复杂动态场景中实现了41倍的存储减少和9倍的渲染速度提升，同时保持了相似的视觉质量。'}}}, {'id': 'https://huggingface.co/papers/2503.16421', 'title': 'MagicMotion: Controllable Video Generation with Dense-to-Sparse\n  Trajectory Guidance', 'url': 'https://huggingface.co/papers/2503.16421', 'abstract': 'Recent advances in video generation have led to remarkable improvements in visual quality and temporal coherence. Upon this, trajectory-controllable video generation has emerged to enable precise object motion control through explicitly defined spatial paths. However, existing methods struggle with complex object movements and multi-object motion control, resulting in imprecise trajectory adherence, poor object consistency, and compromised visual quality. Furthermore, these methods only support trajectory control in a single format, limiting their applicability in diverse scenarios. Additionally, there is no publicly available dataset or benchmark specifically tailored for trajectory-controllable video generation, hindering robust training and systematic evaluation. To address these challenges, we introduce MagicMotion, a novel image-to-video generation framework that enables trajectory control through three levels of conditions from dense to sparse: masks, bounding boxes, and sparse boxes. Given an input image and trajectories, MagicMotion seamlessly animates objects along defined trajectories while maintaining object consistency and visual quality. Furthermore, we present MagicData, a large-scale trajectory-controlled video dataset, along with an automated pipeline for annotation and filtering. We also introduce MagicBench, a comprehensive benchmark that assesses both video quality and trajectory control accuracy across different numbers of objects. Extensive experiments demonstrate that MagicMotion outperforms previous methods across various metrics. Our project page are publicly available at https://quanhaol.github.io/magicmotion-site.', 'score': 4, 'issue_id': 2823, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': 'ae5c7f7abb796973', 'authors': ['Quanhao Li', 'Zhen Xing', 'Rui Wang', 'Hui Zhang', 'Qi Dai', 'Zuxuan Wu'], 'affiliations': ['Fudan University', 'Microsoft Research Asia'], 'pdf_title_img': 'assets/pdf/title_img/2503.16421.jpg', 'data': {'categories': ['#open_source', '#optimization', '#video', '#games', '#dataset', '#benchmark'], 'emoji': '🎥', 'ru': {'title': 'MagicMotion: точное управление движением объектов в генерируемом видео', 'desc': 'MagicMotion - новая система генерации видео из изображений с контролем траектории движения объектов. Она поддерживает три уровня управления траекторией: маски, ограничивающие рамки и разреженные рамки. Авторы также представили MagicData - крупномасштабный датасет для обучения и MagicBench - бенчмарк для оценки качества видео и точности контроля траектории. Эксперименты показывают, что MagicMotion превосходит существующие методы по различным метрикам.'}, 'en': {'title': 'MagicMotion: Mastering Object Motion in Video Generation', 'desc': 'This paper presents MagicMotion, a new framework for generating videos from images while allowing precise control over object movements along defined paths. It addresses the limitations of existing methods that struggle with complex and multi-object trajectories, which often lead to poor visual quality and object consistency. MagicMotion introduces a tiered approach to trajectory control using masks, bounding boxes, and sparse boxes, enhancing flexibility and applicability. Additionally, the authors provide a new dataset, MagicData, and a benchmark, MagicBench, to facilitate better training and evaluation of trajectory-controllable video generation.'}, 'zh': {'title': 'MagicMotion：精准轨迹控制的视频生成新框架', 'desc': '最近视频生成技术取得了显著进展，提升了视觉质量和时间一致性。针对复杂物体运动和多物体运动控制的问题，本文提出了MagicMotion框架，支持通过不同层次的条件（如掩码、边界框和稀疏框）进行轨迹控制。MagicMotion能够在保持物体一致性和视觉质量的同时，沿着定义的轨迹无缝动画化物体。此外，我们还推出了MagicData数据集和MagicBench基准，促进了轨迹控制视频生成的研究和评估。'}}}, {'id': 'https://huggingface.co/papers/2503.16278', 'title': 'Uni-3DAR: Unified 3D Generation and Understanding via Autoregression on\n  Compressed Spatial Tokens', 'url': 'https://huggingface.co/papers/2503.16278', 'abstract': 'Recent advancements in large language models and their multi-modal extensions have demonstrated the effectiveness of unifying generation and understanding through autoregressive next-token prediction. However, despite the critical role of 3D structural generation and understanding ({3D GU}) in AI for science, these tasks have largely evolved independently, with autoregressive methods remaining underexplored. To bridge this gap, we introduce Uni-3DAR, a unified framework that seamlessly integrates {3D GU} tasks via autoregressive prediction. At its core, Uni-3DAR employs a novel hierarchical tokenization that compresses 3D space using an octree, leveraging the inherent sparsity of 3D structures. It then applies an additional tokenization for fine-grained structural details, capturing key attributes such as atom types and precise spatial coordinates in microscopic 3D structures. We further propose two optimizations to enhance efficiency and effectiveness. The first is a two-level subtree compression strategy, which reduces the octree token sequence by up to 8x. The second is a masked next-token prediction mechanism tailored for dynamically varying token positions, significantly boosting model performance. By combining these strategies, Uni-3DAR successfully unifies diverse {3D GU} tasks within a single autoregressive framework. Extensive experiments across multiple microscopic {3D GU} tasks, including molecules, proteins, polymers, and crystals, validate its effectiveness and versatility. Notably, Uni-3DAR surpasses previous state-of-the-art diffusion models by a substantial margin, achieving up to 256\\% relative improvement while delivering inference speeds up to 21.8x faster. The code is publicly available at https://github.com/dptech-corp/Uni-3DAR.', 'score': 4, 'issue_id': 2822, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': '7012b17f03aeb180', 'authors': ['Shuqi Lu', 'Haowei Lin', 'Lin Yao', 'Zhifeng Gao', 'Xiaohong Ji', 'Weinan E', 'Linfeng Zhang', 'Guolin Ke'], 'affiliations': ['AI for Science Institute, Beijing 100080, China', 'Center for Machine Learning Research, Peking University, Beijing 100084, China', 'DP Technology, Beijing, 100080, China', 'Institute for Artificial Intelligence, Peking University, Beijing 100871, China', 'School of Mathematical Sciences, Peking University, Beijing, 100871, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.16278.jpg', 'data': {'categories': ['#multimodal', '#science', '#training', '#architecture', '#3d', '#optimization', '#inference'], 'emoji': '🧬', 'ru': {'title': 'Uni-3DAR: Единая авторегрессионная модель для 3D генерации и понимания', 'desc': 'Статья представляет Uni-3DAR - унифицированную систему для задач 3D генерации и понимания на основе авторегрессионного предсказания. Ключевой особенностью является иерархическая токенизация с использованием октодерева для сжатия 3D пространства. Предложены оптимизации: двухуровневое сжатие поддеревьев и механизм маскированного предсказания следующего токена. Эксперименты показывают значительное превосходство Uni-3DAR над существующими диффузионными моделями по точности и скорости для различных микроскопических 3D задач.'}, 'en': {'title': 'Unifying 3D Generation and Understanding with Uni-3DAR', 'desc': 'This paper presents Uni-3DAR, a novel framework that integrates 3D structural generation and understanding tasks using autoregressive next-token prediction. It introduces a hierarchical tokenization method that efficiently compresses 3D space with an octree, capturing both the overall structure and fine details like atom types and spatial coordinates. The framework includes optimizations such as a two-level subtree compression strategy and a masked next-token prediction mechanism, enhancing both efficiency and model performance. Experimental results show that Uni-3DAR significantly outperforms existing models, achieving faster inference speeds and improved accuracy across various microscopic 3D tasks.'}, 'zh': {'title': '统一3D结构生成与理解的创新框架', 'desc': '本文介绍了一种名为Uni-3DAR的统一框架，旨在通过自回归预测整合3D结构生成与理解（3D GU）任务。该框架采用新颖的分层标记化方法，利用八叉树压缩3D空间，并为微观3D结构捕捉关键属性如原子类型和精确坐标。Uni-3DAR还提出了两项优化策略，以提高效率和效果，包括两级子树压缩和动态变化标记位置的掩码下一个标记预测机制。通过广泛的实验，Uni-3DAR在多种微观3D GU任务中表现出色，显著超越了之前的扩散模型。'}}}, {'id': 'https://huggingface.co/papers/2503.15567', 'title': 'Towards Unified Latent Space for 3D Molecular Latent Diffusion Modeling', 'url': 'https://huggingface.co/papers/2503.15567', 'abstract': '3D molecule generation is crucial for drug discovery and material science, requiring models to process complex multi-modalities, including atom types, chemical bonds, and 3D coordinates. A key challenge is integrating these modalities of different shapes while maintaining SE(3) equivariance for 3D coordinates. To achieve this, existing approaches typically maintain separate latent spaces for invariant and equivariant modalities, reducing efficiency in both training and sampling. In this work, we propose Unified Variational Auto-Encoder for 3D Molecular Latent Diffusion Modeling (UAE-3D), a multi-modal VAE that compresses 3D molecules into latent sequences from a unified latent space, while maintaining near-zero reconstruction error. This unified latent space eliminates the complexities of handling multi-modality and equivariance when performing latent diffusion modeling. We demonstrate this by employing the Diffusion Transformer--a general-purpose diffusion model without any molecular inductive bias--for latent generation. Extensive experiments on GEOM-Drugs and QM9 datasets demonstrate that our method significantly establishes new benchmarks in both de novo and conditional 3D molecule generation, achieving leading efficiency and quality.', 'score': 4, 'issue_id': 2822, 'pub_date': '2025-03-19', 'pub_date_card': {'ru': '19 марта', 'en': 'March 19', 'zh': '3月19日'}, 'hash': 'd1787a1a75f723a2', 'authors': ['Yanchen Luo', 'Zhiyuan Liu', 'Yi Zhao', 'Sihang Li', 'Kenji Kawaguchi', 'Tat-Seng Chua', 'Xiang Wang'], 'affiliations': ['National University of Singapore', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.15567.jpg', 'data': {'categories': ['#multimodal', '#diffusion', '#benchmark', '#3d', '#optimization', '#dataset'], 'emoji': '🧪', 'ru': {'title': 'Единое латентное пространство для эффективной генерации 3D молекул', 'desc': 'Статья представляет новый подход к генерации трехмерных молекул для разработки лекарств и материаловедения. Авторы предлагают унифицированный вариационный автоэнкодер (UAE-3D) для сжатия 3D молекул в латентные последовательности из единого латентного пространства. Этот метод устраняет сложности обработки мультимодальности и эквивариантности при моделировании латентной диффузии. Эксперименты на наборах данных GEOM-Drugs и QM9 показывают, что предложенный метод значительно улучшает результаты в задачах de novo и условной генерации 3D молекул.'}, 'en': {'title': 'Unified Latent Space for Efficient 3D Molecule Generation', 'desc': 'This paper presents a new approach called Unified Variational Auto-Encoder for 3D Molecular Latent Diffusion Modeling (UAE-3D) aimed at generating 3D molecules for drug discovery and material science. The method addresses the challenge of integrating different types of data, such as atom types and chemical bonds, while ensuring that the 3D coordinates maintain SE(3) equivariance. By using a unified latent space, UAE-3D simplifies the process of handling multi-modalities and improves the efficiency of training and sampling. The results show that this approach outperforms existing methods in generating high-quality 3D molecules, setting new benchmarks in the field.'}, 'zh': {'title': '统一潜在空间，提升3D分子生成效率', 'desc': '3D分子生成对药物发现和材料科学至关重要，需要模型处理复杂的多模态信息，包括原子类型、化学键和3D坐标。一个主要挑战是如何在保持SE(3)等变性的同时，整合不同形状的模态。我们提出了一种统一的变分自编码器（UAE-3D），它将3D分子压缩为来自统一潜在空间的潜在序列，同时保持近乎零的重建误差。通过使用Diffusion Transformer，我们的方法在GEOM-Drugs和QM9数据集上显著提高了新分子生成的效率和质量，建立了新的基准。'}}}, {'id': 'https://huggingface.co/papers/2503.16428', 'title': 'XAttention: Block Sparse Attention with Antidiagonal Scoring', 'url': 'https://huggingface.co/papers/2503.16428', 'abstract': "Long-Context Transformer Models (LCTMs) are vital for real-world applications but suffer high computational costs due to attention's quadratic complexity. Block-sparse attention mitigates this by focusing computation on critical regions, yet existing methods struggle with balancing accuracy and efficiency due to costly block importance measurements. In this paper, we introduce XAttention, a plug-and-play framework that dramatically accelerates long-context inference in Transformers models using sparse attention. XAttention's key innovation is the insight that the sum of antidiagonal values (i.e., from the lower-left to upper-right) in the attention matrix provides a powerful proxy for block importance. This allows for precise identification and pruning of non-essential blocks, resulting in high sparsity and dramatically accelerated inference. Across comprehensive evaluations on demanding long-context benchmarks-including RULER and LongBench for language, VideoMME for video understanding, and VBench for video generation. XAttention achieves accuracy comparable to full attention while delivering substantial computational gains. We demonstrate up to 13.5x acceleration in attention computation. These results underscore XAttention's ability to unlock the practical potential of block sparse attention, paving the way for scalable and efficient deployment of LCTMs in real-world applications. Code is available at https://github.com/mit-han-lab/x-attention.", 'score': 3, 'issue_id': 2823, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': 'befbf726163c510a', 'authors': ['Ruyi Xu', 'Guangxuan Xiao', 'Haofeng Huang', 'Junxian Guo', 'Song Han'], 'affiliations': ['Massachusetts Institute of Technology', 'NVIDIA', 'SJTU', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.16428.jpg', 'data': {'categories': ['#inference', '#architecture', '#long_context', '#optimization', '#video', '#benchmark'], 'emoji': '🚀', 'ru': {'title': 'XAttention: Ускорение трансформеров без потери точности', 'desc': 'XAttention - это новый фреймворк для ускорения обработки длинных контекстов в моделях трансформеров с использованием разреженного внимания. Основная инновация заключается в использовании суммы антидиагональных значений в матрице внимания для определения важности блоков. Это позволяет точно идентифицировать и отсекать несущественные блоки, что приводит к высокой разреженности и значительному ускорению вывода. XAttention показывает точность, сравнимую с полным вниманием, при существенном увеличении вычислительной эффективности на различных задачах обработки длинных контекстов.'}, 'en': {'title': 'Accelerating Long-Context Transformers with XAttention', 'desc': 'This paper presents XAttention, a new framework designed to improve the efficiency of long-context Transformer models by utilizing block-sparse attention. Traditional attention mechanisms are computationally expensive, but XAttention reduces this cost by focusing on important regions of the attention matrix. The key innovation is using the sum of antidiagonal values to identify and prune less important blocks, which maintains accuracy while enhancing speed. Evaluations show that XAttention can achieve up to 13.5 times faster attention computation without sacrificing performance, making it a significant advancement for deploying LCTMs in practical applications.'}, 'zh': {'title': 'XAttention：加速长上下文推理的创新框架', 'desc': '长上下文变换器模型（LCTMs）在实际应用中非常重要，但由于注意力机制的平方复杂度，计算成本很高。块稀疏注意力通过集中计算在关键区域来缓解这一问题，但现有方法在平衡准确性和效率方面面临挑战。本文介绍了XAttention，这是一种即插即用的框架，通过稀疏注意力显著加速变换器模型的长上下文推理。XAttention的关键创新在于利用注意力矩阵中反对角线值的总和作为块重要性的强大代理，从而实现高稀疏性和显著加速推理。'}}}, {'id': 'https://huggingface.co/papers/2503.16413', 'title': 'M3: 3D-Spatial MultiModal Memory', 'url': 'https://huggingface.co/papers/2503.16413', 'abstract': "We present 3D Spatial MultiModal Memory (M3), a multimodal memory system designed to retain information about medium-sized static scenes through video sources for visual perception. By integrating 3D Gaussian Splatting techniques with foundation models, M3 builds a multimodal memory capable of rendering feature representations across granularities, encompassing a wide range of knowledge. In our exploration, we identify two key challenges in previous works on feature splatting: (1) computational constraints in storing high-dimensional features for each Gaussian primitive, and (2) misalignment or information loss between distilled features and foundation model features. To address these challenges, we propose M3 with key components of principal scene components and Gaussian memory attention, enabling efficient training and inference. To validate M3, we conduct comprehensive quantitative evaluations of feature similarity and downstream tasks, as well as qualitative visualizations to highlight the pixel trace of Gaussian memory attention. Our approach encompasses a diverse range of foundation models, including vision-language models (VLMs), perception models, and large multimodal and language models (LMMs/LLMs). Furthermore, to demonstrate real-world applicability, we deploy M3's feature field in indoor scenes on a quadruped robot. Notably, we claim that M3 is the first work to address the core compression challenges in 3D feature distillation.", 'score': 3, 'issue_id': 2823, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': '963439a9c78faf82', 'authors': ['Xueyan Zou', 'Yuchen Song', 'Ri-Zhao Qiu', 'Xuanbin Peng', 'Jianglong Ye', 'Sifei Liu', 'Xiaolong Wang'], 'affiliations': ['NVIDIA', 'UC San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2503.16413.jpg', 'data': {'categories': ['#robotics', '#architecture', '#optimization', '#games', '#3d', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'Эффективная 3D мультимодальная память для визуального восприятия', 'desc': 'Статья представляет 3D Пространственную МультиМодальную Память (M3) - систему, предназначенную для сохранения информации о статичных сценах среднего размера через видеоисточники для визуального восприятия. M3 объединяет методы 3D гауссовского сплаттинга с фундаментальными моделями для создания мультимодальной памяти, способной рендерить представления признаков разной гранулярности. Авторы предлагают решения для преодоления вычислительных ограничений при хранении высокоразмерных признаков и проблем несоответствия между дистиллированными признаками и признаками фундаментальных моделей. Система M3 демонстрирует эффективность в различных задачах и может применяться с разными типами фундаментальных моделей, включая мультимодальные модели и большие языковые модели.'}, 'en': {'title': 'Revolutionizing Scene Memory with M3: Efficient 3D Feature Distillation', 'desc': 'The paper introduces the 3D Spatial MultiModal Memory (M3), a novel memory system that captures and retains information about static scenes using video inputs for enhanced visual perception. M3 combines 3D Gaussian Splatting with foundation models to create a multimodal memory that effectively represents features at various levels of detail. It tackles two significant challenges in previous methods: the difficulty of storing high-dimensional features and the misalignment of distilled features with those from foundation models. The authors validate M3 through extensive evaluations and demonstrate its practical use in real-world scenarios, such as deploying it on a quadruped robot in indoor environments.'}, 'zh': {'title': '3D多模态记忆：解决特征压缩挑战的创新', 'desc': '我们提出了3D空间多模态记忆系统（M3），旨在通过视频源保留中等大小静态场景的信息，以增强视觉感知。M3结合了3D高斯点云技术和基础模型，构建了一个能够跨粒度渲染特征表示的多模态记忆系统，涵盖广泛的知识。我们识别了以往特征点云工作中的两个主要挑战：存储高维特征的计算限制，以及提取特征与基础模型特征之间的错位或信息丢失。为了解决这些问题，我们提出了M3，采用了主要场景组件和高斯记忆注意力的关键组件，实现了高效的训练和推理。'}}}, {'id': 'https://huggingface.co/papers/2503.16322', 'title': 'Ultra-Resolution Adaptation with Ease', 'url': 'https://huggingface.co/papers/2503.16322', 'abstract': 'Text-to-image diffusion models have achieved remarkable progress in recent years. However, training models for high-resolution image generation remains challenging, particularly when training data and computational resources are limited. In this paper, we explore this practical problem from two key perspectives: data and parameter efficiency, and propose a set of key guidelines for ultra-resolution adaptation termed URAE. For data efficiency, we theoretically and empirically demonstrate that synthetic data generated by some teacher models can significantly promote training convergence. For parameter efficiency, we find that tuning minor components of the weight matrices outperforms widely-used low-rank adapters when synthetic data are unavailable, offering substantial performance gains while maintaining efficiency. Additionally, for models leveraging guidance distillation, such as FLUX, we show that disabling classifier-free guidance, i.e., setting the guidance scale to 1 during adaptation, is crucial for satisfactory performance. Extensive experiments validate that URAE achieves comparable 2K-generation performance to state-of-the-art closed-source models like FLUX1.1 [Pro] Ultra with only 3K samples and 2K iterations, while setting new benchmarks for 4K-resolution generation. Codes are available https://github.com/Huage001/URAE{here}.', 'score': 3, 'issue_id': 2824, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': '48ce70d2a4cf0cf7', 'authors': ['Ruonan Yu', 'Songhua Liu', 'Zhenxiong Tan', 'Xinchao Wang'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.16322.jpg', 'data': {'categories': ['#optimization', '#data', '#diffusion', '#cv', '#synthetic', '#training', '#benchmark'], 'emoji': '🖼️', 'ru': {'title': 'Эффективная адаптация диффузионных моделей для сверхвысокого разрешения', 'desc': 'Статья представляет набор рекомендаций под названием URAE для адаптации диффузионных моделей к генерации изображений сверхвысокого разрешения. Авторы исследуют проблему с точки зрения эффективности данных и параметров, демонстрируя преимущества использования синтетических данных от учительских моделей. Они также обнаружили, что настройка небольших компонентов весовых матриц превосходит широко используемые низкоранговые адаптеры при отсутствии синтетических данных. Эксперименты подтверждают, что URAE достигает сопоставимой производительности при генерации 2K-изображений с использованием всего 3000 образцов и 2000 итераций, устанавливая новые ориентиры для генерации изображений с разрешением 4K.'}, 'en': {'title': 'Efficient High-Resolution Image Generation with URAE', 'desc': 'This paper addresses the challenges of training text-to-image diffusion models for high-resolution image generation, especially when data and computational resources are limited. It introduces URAE, a set of guidelines focused on improving data and parameter efficiency. The authors demonstrate that synthetic data from teacher models can enhance training convergence, and that tuning specific components of weight matrices can yield better results than traditional low-rank adapters. Their experiments show that URAE can achieve high-quality 2K and 4K image generation with significantly fewer training samples and iterations compared to existing models.'}, 'zh': {'title': '高效生成高分辨率图像的创新方法', 'desc': '本文探讨了高分辨率图像生成中的训练挑战，尤其是在数据和计算资源有限的情况下。我们提出了一套名为URAE的超分辨率适应指南，旨在提高数据和参数效率。研究表明，某些教师模型生成的合成数据可以显著促进训练收敛，而微调权重矩阵的少量组件在缺乏合成数据时表现优于常用的低秩适配器。通过大量实验验证，URAE在仅使用3000个样本和2000次迭代的情况下，达到了与FLUX1.1等最先进闭源模型相当的2K生成性能，并为4K分辨率生成设定了新基准。'}}}, {'id': 'https://huggingface.co/papers/2503.16252', 'title': 'Fin-R1: A Large Language Model for Financial Reasoning through\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2503.16252', 'abstract': 'Reasoning large language models are rapidly evolving across various domains. However, their capabilities in handling complex financial tasks still require in-depth exploration. In this paper, we introduce Fin-R1, a reasoning large language model specifically designed for the financial sector. Fin-R1 is built using a two-stage architecture, leveraging a financial reasoning dataset distilled and processed based on DeepSeek-R1. Through supervised fine-tuning (SFT) and reinforcement learning (RL) training, it demonstrates performance close to DeepSeek-R1 with a parameter size of 7 billion across a range of financial reasoning tasks. It achieves the state-of-the-art (SOTA) in the FinQA and ConvFinQA tasks between those LLMs in our evaluation, surpassing larger models in other tasks as well. Fin-R1 showcases strong reasoning and decision-making capabilities, providing solutions to various problems encountered in the financial domain. Our code is available at https://github.com/SUFE-AIFLM-Lab/Fin-R1.', 'score': 2, 'issue_id': 2823, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': 'f2a1e8506f9711ee', 'authors': ['Zhaowei Liu', 'Xin Guo', 'Fangqi Lou', 'Lingfeng Zeng', 'Jinyi Niu', 'Zixuan Wang', 'Jiajie Xu', 'Weige Cai', 'Ziwei Yang', 'Xueqian Zhao', 'Chao Li', 'Sheng Xu', 'Dezhi Chen', 'Yun Chen', 'Zuo Bai', 'Liwen Zhang'], 'affiliations': ['FinStep', 'Fudan University', 'Shanghai University of Finance and Economics'], 'pdf_title_img': 'assets/pdf/title_img/2503.16252.jpg', 'data': {'categories': ['#training', '#rl', '#architecture', '#reasoning', '#dataset', '#healthcare'], 'emoji': '💹', 'ru': {'title': 'Fin-R1: Мощная языковая модель для финансового анализа и рассуждений', 'desc': 'Исследователи представляют Fin-R1 - языковую модель, специализированную для финансового сектора. Модель использует двухэтапную архитектуру и обучена на финансовом наборе данных, основанном на DeepSeek-R1. Fin-R1 демонстрирует производительность, близкую к DeepSeek-R1, при размере в 7 миллиардов параметров. Модель достигает лучших результатов в задачах FinQA и ConvFinQA среди оцениваемых моделей, показывая сильные способности к рассуждению и принятию решений в финансовой сфере.'}, 'en': {'title': 'Fin-R1: Revolutionizing Financial Reasoning with AI', 'desc': 'This paper presents Fin-R1, a specialized reasoning large language model tailored for financial tasks. It employs a two-stage architecture and is trained on a unique financial reasoning dataset derived from DeepSeek-R1. Through techniques like supervised fine-tuning and reinforcement learning, Fin-R1 achieves competitive performance with 7 billion parameters, excelling in financial reasoning benchmarks such as FinQA and ConvFinQA. The model demonstrates advanced reasoning and decision-making skills, addressing various challenges in the financial sector.'}, 'zh': {'title': '金融领域的推理新星：Fin-R1', 'desc': '本文介绍了一种专为金融领域设计的推理大型语言模型Fin-R1。Fin-R1采用两阶段架构，利用基于DeepSeek-R1的金融推理数据集进行训练。通过监督微调（SFT）和强化学习（RL），它在多个金融推理任务中表现接近DeepSeek-R1，参数规模为70亿。Fin-R1在FinQA和ConvFinQA任务中达到了当前最先进的水平，展现出强大的推理和决策能力，为金融领域的各种问题提供了解决方案。'}}}, {'id': 'https://huggingface.co/papers/2503.15851', 'title': 'Zero-1-to-A: Zero-Shot One Image to Animatable Head Avatars Using Video\n  Diffusion', 'url': 'https://huggingface.co/papers/2503.15851', 'abstract': 'Animatable head avatar generation typically requires extensive data for training. To reduce the data requirements, a natural solution is to leverage existing data-free static avatar generation methods, such as pre-trained diffusion models with score distillation sampling (SDS), which align avatars with pseudo ground-truth outputs from the diffusion model. However, directly distilling 4D avatars from video diffusion often leads to over-smooth results due to spatial and temporal inconsistencies in the generated video. To address this issue, we propose Zero-1-to-A, a robust method that synthesizes a spatial and temporal consistency dataset for 4D avatar reconstruction using the video diffusion model. Specifically, Zero-1-to-A iteratively constructs video datasets and optimizes animatable avatars in a progressive manner, ensuring that avatar quality increases smoothly and consistently throughout the learning process. This progressive learning involves two stages: (1) Spatial Consistency Learning fixes expressions and learns from front-to-side views, and (2) Temporal Consistency Learning fixes views and learns from relaxed to exaggerated expressions, generating 4D avatars in a simple-to-complex manner. Extensive experiments demonstrate that Zero-1-to-A improves fidelity, animation quality, and rendering speed compared to existing diffusion-based methods, providing a solution for lifelike avatar creation. Code is publicly available at: https://github.com/ZhenglinZhou/Zero-1-to-A.', 'score': 2, 'issue_id': 2826, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': '470193a07d663d04', 'authors': ['Zhou Zhenglin', 'Ma Fan', 'Fan Hehe', 'Chua Tat-Seng'], 'affiliations': ['National University of Singapore', 'ReLER, CCAI, Zhejiang University', 'State Key Laboratory of Brain-machine Intelligence, Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.15851.jpg', 'data': {'categories': ['#data', '#diffusion', '#dataset', '#synthetic', '#video', '#open_source'], 'emoji': '🎭', 'ru': {'title': 'Zero-1-to-A: Создание реалистичных 3D-аватаров без реальных данных', 'desc': 'Статья представляет метод Zero-1-to-A для генерации анимируемых 3D-аватаров без использования реальных данных. Авторы предлагают использовать предобученные диффузионные модели для создания псевдо-истинных данных, но с учетом пространственной и временной согласованности. Метод включает два этапа: обучение пространственной согласованности (фиксированные выражения, разные ракурсы) и обучение временной согласованности (фиксированные ракурсы, разные выражения). Эксперименты показывают, что Zero-1-to-A улучшает качество аватаров и скорость рендеринга по сравнению с существующими методами на основе диффузии.'}, 'en': {'title': 'Revolutionizing 4D Avatar Generation with Zero-1-to-A', 'desc': 'This paper presents Zero-1-to-A, a novel approach for generating animatable 4D avatars using video diffusion models while minimizing data requirements. The method addresses the challenge of over-smoothing in avatar generation by creating a dataset that ensures both spatial and temporal consistency. It employs a two-stage progressive learning process: first, it focuses on spatial consistency by learning from different views, and then it enhances temporal consistency by varying expressions. The results show that Zero-1-to-A significantly improves the quality and speed of avatar rendering compared to traditional methods.'}, 'zh': {'title': '高效生成真实感4D头像的创新方法', 'desc': '本论文提出了一种名为Zero-1-to-A的方法，用于生成可动画的4D头像，旨在减少对大量训练数据的需求。该方法利用视频扩散模型，通过构建空间和时间一致性的数据集，逐步优化头像的质量。Zero-1-to-A的学习过程分为两个阶段：空间一致性学习和时间一致性学习，确保头像在学习过程中质量平滑提升。实验结果表明，该方法在头像的真实感、动画质量和渲染速度上优于现有的扩散模型方法。'}}}, {'id': 'https://huggingface.co/papers/2503.16194', 'title': 'Improving Autoregressive Image Generation through Coarse-to-Fine Token\n  Prediction', 'url': 'https://huggingface.co/papers/2503.16194', 'abstract': "Autoregressive models have shown remarkable success in image generation by adapting sequential prediction techniques from language modeling. However, applying these approaches to images requires discretizing continuous pixel data through vector quantization methods like VQ-VAE. To alleviate the quantization errors that existed in VQ-VAE, recent works tend to use larger codebooks. However, this will accordingly expand vocabulary size, complicating the autoregressive modeling task. This paper aims to find a way to enjoy the benefits of large codebooks without making autoregressive modeling more difficult. Through empirical investigation, we discover that tokens with similar codeword representations produce similar effects on the final generated image, revealing significant redundancy in large codebooks. Based on this insight, we propose to predict tokens from coarse to fine (CTF), realized by assigning the same coarse label for similar tokens. Our framework consists of two stages: (1) an autoregressive model that sequentially predicts coarse labels for each token in the sequence, and (2) an auxiliary model that simultaneously predicts fine-grained labels for all tokens conditioned on their coarse labels. Experiments on ImageNet demonstrate our method's superior performance, achieving an average improvement of 59 points in Inception Score compared to baselines. Notably, despite adding an inference step, our approach achieves faster sampling speeds.", 'score': 1, 'issue_id': 2826, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': '0e990c414e8ba81c', 'authors': ['Ziyao Guo', 'Kaipeng Zhang', 'Michael Qizhe Shieh'], 'affiliations': ['National University of Singapore', 'Shanghai AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2503.16194.jpg', 'data': {'categories': ['#cv', '#training', '#optimization', '#architecture'], 'emoji': '🖼️', 'ru': {'title': 'От грубого к точному: новый подход к генерации изображений', 'desc': 'Статья представляет новый подход к генерации изображений с использованием авторегрессионных моделей. Авторы предлагают метод предсказания токенов от грубого к точному (CTF), чтобы использовать преимущества больших кодовых книг без усложнения задачи авторегрессионного моделирования. Метод состоит из двух этапов: авторегрессионная модель для последовательного предсказания грубых меток и вспомогательная модель для одновременного предсказания точных меток. Эксперименты на ImageNet показали значительное улучшение Inception Score и более быструю генерацию по сравнению с базовыми методами.'}, 'en': {'title': 'Enhancing Image Generation with Coarse-to-Fine Token Prediction', 'desc': 'This paper explores how autoregressive models can be improved for image generation by addressing the challenges of using large codebooks in vector quantization. It identifies that many tokens in these large codebooks are redundant, meaning they have similar effects on the generated images. To tackle this, the authors propose a coarse-to-fine (CTF) prediction method that first predicts broad categories for tokens and then refines these predictions with more detailed labels. Their experiments show that this approach not only enhances image quality, as indicated by a significant increase in Inception Score, but also speeds up the sampling process despite the added complexity.'}, 'zh': {'title': '从粗到细：优化自回归图像生成', 'desc': '自回归模型在图像生成中取得了显著成功，借鉴了语言建模中的序列预测技术。然而，将这些方法应用于图像时，需要通过向量量化方法（如VQ-VAE）将连续像素数据离散化。为了减少VQ-VAE中存在的量化误差，最近的研究倾向于使用更大的代码本，但这会增加词汇量，复杂化自回归建模任务。本文提出了一种从粗到细（CTF）预测标记的方法，利用相似代码词表示的标记对最终生成图像的相似影响，从而在不增加建模难度的情况下享受大代码本的优势。'}}}, {'id': 'https://huggingface.co/papers/2503.16031', 'title': 'Deceptive Humor: A Synthetic Multilingual Benchmark Dataset for Bridging\n  Fabricated Claims with Humorous Content', 'url': 'https://huggingface.co/papers/2503.16031', 'abstract': 'This paper presents the Deceptive Humor Dataset (DHD), a novel resource for studying humor derived from fabricated claims and misinformation. In an era of rampant misinformation, understanding how humor intertwines with deception is essential. DHD consists of humor-infused comments generated from false narratives, incorporating fabricated claims and manipulated information using the ChatGPT-4o model. Each instance is labeled with a Satire Level, ranging from 1 for subtle satire to 3 for high-level satire and classified into five distinct Humor Categories: Dark Humor, Irony, Social Commentary, Wordplay, and Absurdity. The dataset spans multiple languages including English, Telugu, Hindi, Kannada, Tamil, and their code-mixed variants (Te-En, Hi-En, Ka-En, Ta-En), making it a valuable multilingual benchmark. By introducing DHD, we establish a structured foundation for analyzing humor in deceptive contexts, paving the way for a new research direction that explores how humor not only interacts with misinformation but also influences its perception and spread. We establish strong baselines for the proposed dataset, providing a foundation for future research to benchmark and advance deceptive humor detection models.', 'score': 1, 'issue_id': 2822, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': '80726382feca8f20', 'authors': ['Sai Kartheek Reddy Kasu', 'Shankar Biradar', 'Sunil Saumya'], 'affiliations': ['IIIT Dharwad', 'MIT Manipal'], 'pdf_title_img': 'assets/pdf/title_img/2503.16031.jpg', 'data': {'categories': ['#ethics', '#low_resource', '#benchmark', '#dataset', '#multilingual'], 'emoji': '🤡', 'ru': {'title': 'Смех сквозь ложь: новый датасет для анализа обманчивого юмора', 'desc': 'Эта статья представляет новый набор данных DHD (Deceptive Humor Dataset) для изучения юмора, основанного на выдуманных утверждениях и дезинформации. DHD состоит из юмористических комментариев, сгенерированных на основе ложных нарративов с использованием модели ChatGPT-4o. Каждый экземпляр помечен уровнем сатиры и классифицирован по пяти категориям юмора. Датасет охватывает несколько языков, включая английский и индийские языки, что делает его ценным многоязычным ресурсом для исследований.'}, 'en': {'title': 'Unraveling Humor in the Age of Misinformation', 'desc': 'The Deceptive Humor Dataset (DHD) is a new resource designed to explore the relationship between humor and misinformation. It includes humor-filled comments based on false narratives, generated using the ChatGPT-4o model, and is labeled with a Satire Level and categorized into five types of humor. This dataset supports multiple languages, making it a useful tool for multilingual research in humor and deception. By providing a structured dataset, DHD aims to enhance the understanding of how humor can affect the perception and dissemination of misinformation.'}, 'zh': {'title': '揭示幽默与欺骗的交织', 'desc': '本文介绍了一个新的资源——欺骗幽默数据集（DHD），用于研究源自虚假声明和错误信息的幽默。在信息泛滥的时代，理解幽默与欺骗之间的关系至关重要。DHD包含从虚假叙述中生成的幽默评论，并使用ChatGPT-4o模型生成虚假声明和操控信息。每个实例都标注了讽刺水平，并分为五种幽默类别，为分析欺骗背景下的幽默提供了结构化基础。'}}}, {'id': 'https://huggingface.co/papers/2503.12689', 'title': 'MagicID: Hybrid Preference Optimization for ID-Consistent and\n  Dynamic-Preserved Video Customization', 'url': 'https://huggingface.co/papers/2503.12689', 'abstract': "Video identity customization seeks to produce high-fidelity videos that maintain consistent identity and exhibit significant dynamics based on users' reference images. However, existing approaches face two key challenges: identity degradation over extended video length and reduced dynamics during training, primarily due to their reliance on traditional self-reconstruction training with static images. To address these issues, we introduce MagicID, a novel framework designed to directly promote the generation of identity-consistent and dynamically rich videos tailored to user preferences. Specifically, we propose constructing pairwise preference video data with explicit identity and dynamic rewards for preference learning, instead of sticking to the traditional self-reconstruction. To address the constraints of customized preference data, we introduce a hybrid sampling strategy. This approach first prioritizes identity preservation by leveraging static videos derived from reference images, then enhances dynamic motion quality in the generated videos using a Frontier-based sampling method. By utilizing these hybrid preference pairs, we optimize the model to align with the reward differences between pairs of customized preferences. Extensive experiments show that MagicID successfully achieves consistent identity and natural dynamics, surpassing existing methods across various metrics.", 'score': 1, 'issue_id': 2824, 'pub_date': '2025-03-16', 'pub_date_card': {'ru': '16 марта', 'en': 'March 16', 'zh': '3月16日'}, 'hash': '2285de85beb9dbfb', 'authors': ['Hengjia Li', 'Lifan Jiang', 'Xi Xiao', 'Tianyang Wang', 'Hongwei Yi', 'Boxi Wu', 'Deng Cai'], 'affiliations': ['Hedra AI', 'University of Alabama at Birmingham', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.12689.jpg', 'data': {'categories': ['#video', '#multimodal'], 'emoji': '🎭', 'ru': {'title': 'MagicID: Персонализированные видео с сохранением идентичности и естественной динамикой', 'desc': 'MagicID - это новая система для создания видео с сохранением идентичности и динамики на основе пользовательских изображений. Она решает проблемы деградации идентичности и снижения динамики, характерные для существующих подходов. MagicID использует попарное обучение с предпочтениями вместо традиционной самореконструкции. Система применяет гибридную стратегию сэмплирования для сохранения идентичности и улучшения динамики движений.'}, 'en': {'title': 'MagicID: Dynamic Video Identity Customization Made Easy', 'desc': 'This paper presents MagicID, a new framework for generating high-quality videos that maintain a consistent identity while exhibiting dynamic movements based on user preferences. The authors identify two main challenges in existing methods: the loss of identity over longer videos and the lack of dynamics during training due to reliance on static images. To overcome these issues, MagicID uses pairwise preference video data that incorporates explicit rewards for identity and dynamics, moving away from traditional self-reconstruction techniques. The framework employs a hybrid sampling strategy to prioritize identity preservation and enhance motion quality, leading to improved performance in generating videos that meet user expectations.'}, 'zh': {'title': '魔法身份：定制动态视频的新方法', 'desc': '视频身份定制旨在生成高保真度的视频，这些视频能够保持一致的身份并根据用户的参考图像展现显著的动态性。然而，现有方法面临两个主要挑战：在较长视频长度下身份的退化和训练过程中动态性的减少，这主要是由于它们依赖于传统的自我重建训练与静态图像。为了解决这些问题，我们提出了MagicID，一个新颖的框架，旨在直接促进生成符合用户偏好的身份一致且动态丰富的视频。我们通过构建具有明确身份和动态奖励的成对偏好视频数据来进行偏好学习，而不是坚持传统的自我重建方法。'}}}, {'id': 'https://huggingface.co/papers/2503.13834', 'title': 'See-Saw Modality Balance: See Gradient, and Sew Impaired Vision-Language\n  Balance to Mitigate Dominant Modality Bias', 'url': 'https://huggingface.co/papers/2503.13834', 'abstract': 'Vision-language (VL) models have demonstrated strong performance across various tasks. However, these models often rely on a specific modality for predictions, leading to "dominant modality bias.\'\' This bias significantly hurts performance, especially when one modality is impaired. In this study, we analyze model behavior under dominant modality bias and theoretically show that unaligned gradients or differences in gradient magnitudes prevent balanced convergence of the loss. Based on these findings, we propose a novel framework, BalGrad to mitigate dominant modality bias. Our approach includes inter-modality gradient reweighting, adjusting the gradient of KL divergence based on each modality\'s contribution, and inter-task gradient projection to align task directions in a non-conflicting manner. Experiments on UPMC Food-101, Hateful Memes, and MM-IMDb datasets confirm that BalGrad effectively alleviates over-reliance on specific modalities when making predictions.', 'score': 0, 'issue_id': 2826, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 марта', 'en': 'March 18', 'zh': '3月18日'}, 'hash': 'd17ffa2ed0a9ff25', 'authors': ['JuneHyoung Kwon', 'MiHyeon Kim', 'Eunju Lee', 'Juhwan Choi', 'YoungBin Kim'], 'affiliations': ['Department of Artificial Intelligence, Chung-Ang University', 'Graduate School of Advanced Imaging Sciences, Multimedia and Film, Chung-Ang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.13834.jpg', 'data': {'categories': ['#interpretability', '#multimodal', '#optimization', '#training'], 'emoji': '⚖️', 'ru': {'title': 'Балансировка модальностей для улучшения мультимодальных моделей', 'desc': 'Исследование посвящено проблеме доминирующей модальности в мультимодальных моделях машинного обучения. Авторы анализируют поведение моделей и теоретически показывают, что несогласованные градиенты препятствуют сбалансированной сходимости функции потерь. Для решения этой проблемы предлагается новый фреймворк BalGrad, включающий перевзвешивание градиентов между модальностями и проекцию градиентов между задачами. Эксперименты на нескольких наборах данных подтверждают эффективность BalGrad в снижении чрезмерной зависимости от конкретных модальностей при прогнозировании.'}, 'en': {'title': 'Balancing Modalities for Better Predictions', 'desc': "This paper addresses the issue of dominant modality bias in vision-language (VL) models, which can negatively impact their performance when one modality is compromised. The authors analyze how unaligned gradients and varying gradient magnitudes hinder the model's ability to converge effectively. To counter this bias, they introduce a new framework called BalGrad, which employs techniques like inter-modality gradient reweighting and inter-task gradient projection. Experimental results on multiple datasets demonstrate that BalGrad successfully reduces the dependency on any single modality, leading to improved prediction accuracy."}, 'zh': {'title': '平衡模态偏差，提升模型性能', 'desc': '视觉语言（VL）模型在多种任务中表现出色，但它们通常依赖于特定的模态进行预测，这导致了“主导模态偏差”。这种偏差会显著影响模型性能，尤其是在某一模态受损时。我们分析了主导模态偏差下模型的行为，并理论上证明了未对齐的梯度或梯度幅度差异会阻碍损失的平衡收敛。基于这些发现，我们提出了一种新框架BalGrad，通过模态间梯度重加权和任务间梯度投影来减轻主导模态偏差。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (5)', '#agents (2)', '#agi', '#alignment', '#architecture (6)', '#audio', '#benchmark (8)', '#cv (5)', '#data (4)', '#dataset (8)', '#diffusion (5)', '#ethics (1)', '#games (3)', '#graphs', '#hallucinations', '#healthcare (1)', '#inference (4)', '#interpretability (1)', '#leakage', '#long_context (1)', '#low_resource (1)', '#machine_translation', '#math (1)', '#multilingual (1)', '#multimodal (7)', '#open_source (7)', '#optimization (12)', '#plp', '#rag', '#reasoning (5)', '#rl (4)', '#rlhf', '#robotics (1)', '#science (1)', '#security', '#small_models (1)', '#story_generation', '#survey (1)', '#synthetic (4)', '#training (12)', '#transfer_learning (1)', '#video (4)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-03-21 06:16',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-03-21 06:16')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-03-21 06:16')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    