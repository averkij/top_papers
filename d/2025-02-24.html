
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 16 papers. February 24.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">24 февраля</span> | <span id="title-articles-count">16 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-02-21.html">⬅️ <span id="prev-date">21.02</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-02-25.html">➡️ <span id="next-date">25.02</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-02.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '24 февраля', 'en': 'February 24', 'zh': '2月24日'};
        let feedDateNext = {'ru': '25.02', 'en': '02/25', 'zh': '2月25日'};
        let feedDatePrev = {'ru': '21.02', 'en': '02/21', 'zh': '2月21日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2502.14776', 'title': 'SurveyX: Academic Survey Automation via Large Language Models', 'url': 'https://huggingface.co/papers/2502.14776', 'abstract': 'Large Language Models (LLMs) have demonstrated exceptional comprehension capabilities and a vast knowledge base, suggesting that LLMs can serve as efficient tools for automated survey generation. However, recent research related to automated survey generation remains constrained by some critical limitations like finite context window, lack of in-depth content discussion, and absence of systematic evaluation frameworks. Inspired by human writing processes, we propose SurveyX, an efficient and organized system for automated survey generation that decomposes the survey composing process into two phases: the Preparation and Generation phases. By innovatively introducing online reference retrieval, a pre-processing method called AttributeTree, and a re-polishing process, SurveyX significantly enhances the efficacy of survey composition. Experimental evaluation results show that SurveyX outperforms existing automated survey generation systems in content quality (0.259 improvement) and citation quality (1.76 enhancement), approaching human expert performance across multiple evaluation dimensions. Examples of surveys generated by SurveyX are available on www.surveyx.cn', 'score': 49, 'issue_id': 2363, 'pub_date': '2025-02-20', 'pub_date_card': {'ru': '20 февраля', 'en': 'February 20', 'zh': '2月20日'}, 'hash': 'b2504554ef88d631', 'authors': ['Xun Liang', 'Jiawei Yang', 'Yezhaohui Wang', 'Chen Tang', 'Zifan Zheng', 'Simin Niu', 'Shichao Song', 'Hanyu Wang', 'Bo Tang', 'Feiyu Xiong', 'Keming Mao', 'Zhiyu li'], 'affiliations': ['Institute for Advanced Algorithms Research, Shanghai, China', 'Northeastern University, Shenyang, China', 'Renmin University of China, Beijing, China', 'The University of Sydney, Sydney, Australia'], 'pdf_title_img': 'assets/pdf/title_img/2502.14776.jpg', 'data': {'categories': ['#data', '#survey', '#benchmark', '#dataset', '#multimodal'], 'emoji': '📊', 'ru': {'title': 'SurveyX: Революция в автоматизированном создании научных обзоров', 'desc': 'Исследователи представили SurveyX - систему для автоматизированного создания обзоров с использованием больших языковых моделей (LLM). Система разбивает процесс составления обзора на две фазы: подготовку и генерацию, включая онлайн-поиск ссылок и предобработку данных методом AttributeTree. Экспериментальная оценка показала, что SurveyX превосходит существующие системы по качеству контента и цитирования, приближаясь к уровню экспертов-людей. Система решает ряд ограничений, присущих предыдущим подходам к автоматизированному созданию обзоров.'}, 'en': {'title': 'SurveyX: Revolutionizing Automated Survey Generation with LLMs', 'desc': 'This paper introduces SurveyX, a novel system for automated survey generation that leverages Large Language Models (LLMs) to improve the survey creation process. It addresses key limitations of previous methods by breaking down the process into two distinct phases: Preparation and Generation. SurveyX incorporates innovative techniques such as online reference retrieval and a pre-processing method called AttributeTree, which enhance the quality of the generated surveys. Experimental results demonstrate that SurveyX significantly outperforms existing systems in both content and citation quality, nearing the performance of human experts.'}, 'zh': {'title': 'SurveyX：高效的自动调查生成系统', 'desc': '大型语言模型（LLMs）在理解能力和知识基础方面表现出色，显示出它们可以作为自动调查生成的有效工具。然而，现有的自动调查生成研究受到一些关键限制，如有限的上下文窗口、缺乏深入的内容讨论和缺乏系统的评估框架。我们提出了SurveyX，一个高效且有组织的自动调查生成系统，将调查编写过程分为准备阶段和生成阶段。通过引入在线参考检索、属性树预处理方法和重新润色过程，SurveyX显著提高了调查编写的效率，并在内容质量和引用质量上超越了现有系统。'}}}, {'id': 'https://huggingface.co/papers/2502.13449', 'title': 'Mol-LLaMA: Towards General Understanding of Molecules in Large Molecular Language Model', 'url': 'https://huggingface.co/papers/2502.13449', 'abstract': "Understanding molecules is key to understanding organisms and driving advances in drug discovery, requiring interdisciplinary knowledge across chemistry and biology. Although large molecular language models have achieved notable success in interpreting molecular structures, their instruction datasets are limited to the specific knowledge from task-oriented datasets and do not fully cover the fundamental characteristics of molecules, hindering their abilities as general-purpose molecular assistants. To address this issue, we propose Mol-LLaMA, a large molecular language model that grasps the general knowledge centered on molecules via multi-modal instruction tuning. To this end, we design key data types that encompass the fundamental features of molecules, incorporating essential knowledge from molecular structures. In addition, to improve understanding of molecular features, we introduce a module that integrates complementary information from different molecular encoders, leveraging the distinct advantages of different molecular representations. Our experimental results demonstrate that Mol-LLaMA is capable of comprehending the general features of molecules and generating relevant responses to users' queries with detailed explanations, implying its potential as a general-purpose assistant for molecular analysis.", 'score': 24, 'issue_id': 2363, 'pub_date': '2025-02-19', 'pub_date_card': {'ru': '19 февраля', 'en': 'February 19', 'zh': '2月19日'}, 'hash': 'e52b99ade1ae590a', 'authors': ['Dongki Kim', 'Wonbin Lee', 'Sung Ju Hwang'], 'affiliations': ['DeepAuto.ai', 'Korea Advanced Institute of Science and Technology (KAIST), Seoul, South Korea'], 'pdf_title_img': 'assets/pdf/title_img/2502.13449.jpg', 'data': {'categories': ['#data', '#dataset', '#science', '#agi', '#architecture', '#multimodal'], 'emoji': '🧬', 'ru': {'title': 'Mol-LLaMA: Мультимодальная ЯМ для всестороннего анализа молекул', 'desc': 'Статья представляет Mol-LLaMA - большую языковую модель для молекул, обученную на мультимодальных инструкциях. Модель охватывает фундаментальные характеристики молекул, включая знания из молекулярных структур. Авторы внедрили модуль, интегрирующий информацию из разных молекулярных энкодеров для улучшения понимания свойств молекул. Эксперименты показывают, что Mol-LLaMA способна понимать общие особенности молекул и генерировать релевантные ответы на запросы пользователей с подробными объяснениями.'}, 'en': {'title': 'Mol-LLaMA: A General-Purpose Assistant for Molecular Understanding', 'desc': 'This paper introduces Mol-LLaMA, a large molecular language model designed to enhance understanding of molecular structures for drug discovery. Unlike previous models that relied on limited task-specific datasets, Mol-LLaMA utilizes multi-modal instruction tuning to incorporate a broader range of fundamental molecular knowledge. The model integrates various molecular encoders to leverage their unique strengths, improving its ability to interpret molecular features. Experimental results show that Mol-LLaMA can effectively respond to user queries with detailed explanations, positioning it as a versatile tool for molecular analysis.'}, 'zh': {'title': 'Mol-LLaMA：分子分析的通用助手', 'desc': '理解分子对于理解生物体和推动药物发现至关重要。虽然大型分子语言模型在解析分子结构方面取得了显著成功，但它们的训练数据集仅限于特定任务的知识，未能全面覆盖分子的基本特征。为了解决这个问题，我们提出了Mol-LLaMA，一个通过多模态指令调优来掌握分子中心的通用知识的大型分子语言模型。实验结果表明，Mol-LLaMA能够理解分子的普遍特征，并生成与用户查询相关的详细解释，显示出其作为分子分析通用助手的潜力。'}}}, {'id': 'https://huggingface.co/papers/2502.14922', 'title': 'SIFT: Grounding LLM Reasoning in Contexts via Stickers', 'url': 'https://huggingface.co/papers/2502.14922', 'abstract': 'This paper identifies the misinterpretation of the context can be a significant issue during the reasoning process of large language models, spanning from smaller models like Llama3.2-3B-Instruct to cutting-edge ones like DeepSeek-R1. For example, in the phrase "10 dollars per kilo," LLMs might not recognize that "per" means "for each," leading to calculation errors. We introduce a novel, post-training approach called **Stick to the Facts (SIFT)** to tackle this. SIFT leverages increasing inference-time compute to ground LLM reasoning in contexts. At the core of SIFT lies the *Sticker*, which is generated by the model itself to explicitly emphasize the key information within the context. Given the curated Sticker, SIFT generates two predictions -- one from the original query and one from the query augmented with the Sticker. If they differ, the Sticker is sequentially refined via *forward* optimization (to better align the extracted facts with the query) and *inverse* generation (to conform with the model\'s inherent tendencies) for more faithful reasoning outcomes. Studies across diverse models (from 3B to 100B+) and benchmarks (e.g., GSM8K, MATH-500) reveal consistent performance improvements. Notably, SIFT improves the pass@1 accuracy of DeepSeek-R1 on AIME2024 from 78.33% to **85.67**%, establishing a new state-of-the-art in the open-source community. The code is available at https://github.com/zhijie-group/SIFT.', 'score': 11, 'issue_id': 2363, 'pub_date': '2025-02-19', 'pub_date_card': {'ru': '19 февраля', 'en': 'February 19', 'zh': '2月19日'}, 'hash': 'b5ab16112068f00f', 'authors': ['Zihao Zeng', 'Xuyao Huang', 'Boxiu Li', 'Zhijie Deng'], 'affiliations': ['Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2502.14922.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#inference', '#math', '#training', '#reasoning', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'SIFT: Новый метод повышения точности рассуждений языковых моделей', 'desc': "Статья представляет новый подход под названием SIFT (Stick to the Facts) для улучшения рассуждений больших языковых моделей (LLM). SIFT использует дополнительные вычисления во время вывода для более точной интерпретации контекста. Метод включает генерацию 'Sticker' - ключевой информации из контекста, и последующую оптимизацию рассуждений модели. Эксперименты показали значительное улучшение производительности различных LLM на нескольких бенчмарках."}, 'en': {'title': 'Enhancing LLM Reasoning with Stick to the Facts (SIFT)', 'desc': 'This paper addresses the problem of context misinterpretation in large language models (LLMs), which can lead to reasoning errors. It introduces a new post-training method called **Stick to the Facts (SIFT)** that enhances LLM reasoning by grounding it in context. SIFT utilizes a self-generated *Sticker* to highlight crucial information, allowing the model to produce two predictions for comparison. The method shows significant improvements in accuracy across various models and benchmarks, achieving a new state-of-the-art performance in the open-source community.'}, 'zh': {'title': '提升语言模型推理准确性的创新方法', 'desc': '这篇论文指出，大型语言模型在推理过程中对上下文的误解可能会导致显著问题。比如，在“每公斤10美元”这个短语中，模型可能无法正确理解“每”的意思，从而导致计算错误。为了解决这个问题，论文提出了一种新的后训练方法，称为**Stick to the Facts (SIFT)**，它通过增加推理时的计算量来增强模型的上下文推理能力。SIFT的核心是由模型生成的*Sticker*，它强调了上下文中的关键信息，从而提高推理的准确性。'}}}, {'id': 'https://huggingface.co/papers/2502.12084', 'title': 'VLM$^2$-Bench: A Closer Look at How Well VLMs Implicitly Link Explicit Matching Visual Cues', 'url': 'https://huggingface.co/papers/2502.12084', 'abstract': "Visually linking matching cues is a crucial ability in daily life, such as identifying the same person in multiple photos based on their cues, even without knowing who they are. Despite the extensive knowledge that vision-language models (VLMs) possess, it remains largely unexplored whether they are capable of performing this fundamental task. To address this, we introduce VLM^2-Bench, a benchmark designed to assess whether VLMs can Visually Link Matching cues, with 9 subtasks and over 3,000 test cases. Comprehensive evaluation across eight open-source VLMs and GPT-4o, along with further analysis of various language-side and vision-side prompting methods, leads to a total of eight key findings. We identify critical challenges in models' ability to link visual cues, highlighting a significant performance gap where even GPT-4o lags 34.80% behind humans. Based on these insights, we advocate for (i) enhancing core visual capabilities to improve adaptability and reduce reliance on prior knowledge, (ii) establishing clearer principles for integrating language-based reasoning in vision-centric tasks to prevent unnecessary biases, and (iii) shifting vision-text training paradigms toward fostering models' ability to independently structure and infer relationships among visual cues.", 'score': 8, 'issue_id': 2366, 'pub_date': '2025-02-17', 'pub_date_card': {'ru': '17 февраля', 'en': 'February 17', 'zh': '2月17日'}, 'hash': '7af295c37b147c3a', 'authors': ['Jianshu Zhang', 'Dongyu Yao', 'Renjie Pi', 'Paul Pu Liang', 'Yi R.', 'Fung'], 'affiliations': ['CMU', 'HKUST', 'MIT'], 'pdf_title_img': 'assets/pdf/title_img/2502.12084.jpg', 'data': {'categories': ['#benchmark', '#alignment', '#interpretability', '#multimodal', '#cv'], 'emoji': '👁️', 'ru': {'title': 'Преодоление разрыва между машинным и человеческим зрением в задачах связывания визуальных признаков', 'desc': 'Статья представляет VLM^2-Bench - набор тестов для оценки способности моделей визуального языка (VLM) связывать визуальные признаки. Исследование включает 9 подзадач и более 3000 тестовых примеров, оценивая восемь открытых VLM и GPT-4o. Результаты выявили значительный разрыв в производительности между моделями и людьми, где даже GPT-4o отстает на 34.80%. На основе полученных данных, авторы рекомендуют улучшить базовые визуальные возможности моделей, установить более четкие принципы интеграции языкового рассуждения в визуальные задачи и изменить парадигмы обучения в сторону развития способности моделей самостоятельно структурировать и выводить отношения между визуальными признаками.'}, 'en': {'title': 'Bridging the Gap: Enhancing Visual Linking in VLMs', 'desc': "This paper introduces VLM^2-Bench, a new benchmark aimed at evaluating the ability of vision-language models (VLMs) to visually link matching cues, which is essential for tasks like recognizing the same person in different images. The benchmark consists of 9 subtasks and over 3,000 test cases, providing a comprehensive assessment of VLM performance. The evaluation reveals significant challenges, with models like GPT-4o showing a 34.80% performance gap compared to human capabilities. The authors suggest improvements in visual processing, clearer integration of language reasoning, and a shift in training paradigms to enhance models' ability to understand and relate visual cues independently."}, 'zh': {'title': '提升视觉语言模型的匹配能力', 'desc': '这篇论文探讨了视觉语言模型（VLMs）在视觉链接匹配线索方面的能力。研究者们提出了VLM^2-Bench基准，旨在评估这些模型在识别相同对象时的表现。通过对八个开源VLM和GPT-4o的全面评估，发现模型在链接视觉线索方面存在显著的性能差距，甚至GPT-4o比人类低34.80%。基于这些发现，作者建议增强模型的核心视觉能力，明确语言推理与视觉任务的整合原则，并推动视觉-文本训练范式的转变。'}}}, {'id': 'https://huggingface.co/papers/2502.15589', 'title': 'LightThinker: Thinking Step-by-Step Compression', 'url': 'https://huggingface.co/papers/2502.15589', 'abstract': 'Large language models (LLMs) have shown remarkable performance in complex reasoning tasks, but their efficiency is hindered by the substantial memory and computational costs associated with generating lengthy tokens. In this paper, we propose LightThinker, a novel method that enables LLMs to dynamically compress intermediate thoughts during reasoning. Inspired by human cognitive processes, LightThinker compresses verbose thought steps into compact representations and discards the original reasoning chains, thereby significantly reducing the number of tokens stored in the context window. This is achieved by training the model on when and how to perform compression through data construction, mapping hidden states to condensed gist tokens, and creating specialized attention masks. Additionally, we introduce the Dependency (Dep) metric to quantify the degree of compression by measuring the reliance on historical tokens during generation. Extensive experiments on four datasets and two models show that LightThinker reduces peak memory usage and inference time, while maintaining competitive accuracy. Our work provides a new direction for improving the efficiency of LLMs in complex reasoning tasks without sacrificing performance. Code will be released at https://github.com/zjunlp/LightThinker.', 'score': 8, 'issue_id': 2365, 'pub_date': '2025-02-21', 'pub_date_card': {'ru': '21 февраля', 'en': 'February 21', 'zh': '2月21日'}, 'hash': '563a32fe7bab988e', 'authors': ['Jintian Zhang', 'Yuqi Zhu', 'Mengshu Sun', 'Yujie Luo', 'Shuofei Qiao', 'Lun Du', 'Da Zheng', 'Huajun Chen', 'Ningyu Zhang'], 'affiliations': ['Ant Group', 'Zhejiang University', 'Zhejiang University - Ant Group Joint Laboratory of Knowledge Graph'], 'pdf_title_img': 'assets/pdf/title_img/2502.15589.jpg', 'data': {'categories': ['#training', '#inference', '#architecture', '#reasoning', '#long_context', '#optimization', '#data'], 'emoji': '🧠', 'ru': {'title': 'LightThinker: Эффективное сжатие мыслей для ускорения работы языковых моделей', 'desc': 'LightThinker - это новый метод, позволяющий большим языковым моделям (LLM) динамически сжимать промежуточные мысли во время рассуждений. Этот подход вдохновлен когнитивными процессами человека и сжимает подробные шаги рассуждений в компактные представления, значительно сокращая количество токенов в контекстном окне. LightThinker обучается тому, когда и как выполнять сжатие, отображая скрытые состояния в сжатые токены-суть и создавая специализированные маски внимания. Эксперименты показывают, что метод снижает пиковое использование памяти и время вывода, сохраняя при этом конкурентоспособную точность.'}, 'en': {'title': 'LightThinker: Efficient Reasoning through Dynamic Thought Compression', 'desc': 'This paper introduces LightThinker, a method designed to enhance the efficiency of large language models (LLMs) during complex reasoning tasks. By dynamically compressing intermediate thoughts, LightThinker reduces the memory and computational costs associated with generating lengthy tokens. The approach mimics human cognitive processes by transforming verbose reasoning into compact representations, which helps in minimizing the number of tokens stored. The authors also present a new metric, Dependency (Dep), to measure the effectiveness of this compression, demonstrating that LightThinker can lower memory usage and inference time while maintaining accuracy across various datasets.'}, 'zh': {'title': 'LightThinker：提升大型语言模型推理效率的新方法', 'desc': '大型语言模型（LLMs）在复杂推理任务中表现出色，但生成长文本时的内存和计算成本较高。本文提出了一种新方法LightThinker，能够在推理过程中动态压缩中间思维。LightThinker借鉴人类认知过程，将冗长的思维步骤压缩为紧凑的表示，从而显著减少上下文窗口中存储的标记数量。通过在数据构建中训练模型进行压缩，并引入依赖度（Dep）指标来量化压缩程度，我们的实验表明LightThinker在保持准确性的同时，降低了峰值内存使用和推理时间。'}}}, {'id': 'https://huggingface.co/papers/2502.14494', 'title': 'StructFlowBench: A Structured Flow Benchmark for Multi-turn Instruction Following', 'url': 'https://huggingface.co/papers/2502.14494', 'abstract': "Multi-turn instruction following capability constitutes a core competency of large language models (LLMs) in real-world applications. Existing evaluation benchmarks predominantly focus on fine-grained constraint satisfaction and domain-specific capability assessment, yet overlook the crucial structural dependency between dialogue turns that distinguishes multi-turn from single-turn interactions. This structural dependency not only reflects user intent but also establishes a second dimension for instruction following evaluation beyond constraint satisfaction. To address this gap, we propose StructFlowBench, a multi-turn instruction following benchmark with structural flow modeling. The benchmark innovatively defines a structural flow framework comprising six fundamental inter-turn relationships, which not only introduces novel structural constraints for model evaluation but also serves as generation parameters for creating customized dialogue flows tailored to specific scenarios. Adopting established LLM-based automatic evaluation methodologies, we conduct systematic evaluations of 13 leading open-source and closed-source LLMs. Experimental results reveal significant deficiencies in current models' comprehension of multi-turn dialogue structures. The code is available at https://github.com/MLGroupJLU/StructFlowBench.", 'score': 8, 'issue_id': 2365, 'pub_date': '2025-02-20', 'pub_date_card': {'ru': '20 февраля', 'en': 'February 20', 'zh': '2月20日'}, 'hash': '512d952f8463678a', 'authors': ['Jinnan Li', 'Jinzhe Li', 'Yue Wang', 'Yi Chang', 'Yuan Wu'], 'affiliations': ['College of Computer Science and Technology, Jilin University', 'Engineering Research Center of Knowledge-Driven Human-Machine Intelligence, MOE, China', 'International Center of Future Science, Jilin University', 'School of Artificial Intelligence, Jilin University', 'School of Information and Library Science, University of North Carolina at Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2502.14494.jpg', 'data': {'categories': ['#alignment', '#open_source', '#benchmark'], 'emoji': '🔀', 'ru': {'title': 'Новый подход к оценке LLM в многоходовых диалогах', 'desc': 'Статья представляет новый бенчмарк StructFlowBench для оценки способности больших языковых моделей (LLM) следовать многоходовым инструкциям. В отличие от существующих бенчмарков, StructFlowBench учитывает структурную зависимость между репликами диалога. Авторы определяют шесть фундаментальных межходовых отношений для моделирования структурного потока диалога. Результаты оценки 13 ведущих LLM показывают значительные недостатки в понимании моделями структуры многоходовых диалогов.'}, 'en': {'title': 'Enhancing Multi-Turn Dialogue Understanding in LLMs', 'desc': 'This paper introduces StructFlowBench, a new benchmark designed to evaluate the multi-turn instruction following capabilities of large language models (LLMs). It highlights the importance of understanding the structural dependencies between dialogue turns, which are often ignored in existing benchmarks. By defining six fundamental inter-turn relationships, StructFlowBench provides a framework for assessing how well models can follow instructions across multiple dialogue turns. The study reveals that many current LLMs struggle with these structural aspects, indicating a need for improvement in their dialogue comprehension.'}, 'zh': {'title': '多轮对话评估的新标准', 'desc': '本文提出了StructFlowBench，这是一个针对多轮指令跟随能力的基准测试，旨在填补现有评估方法的空白。现有的评估主要关注细粒度的约束满足和特定领域能力评估，但忽视了对话轮次之间的结构依赖关系。我们定义了一个结构流框架，包含六种基本的轮次关系，以此为模型评估引入新的结构约束。通过对13个领先的开源和闭源大型语言模型进行系统评估，实验结果显示当前模型在理解多轮对话结构方面存在显著不足。'}}}, {'id': 'https://huggingface.co/papers/2502.14397', 'title': 'PhotoDoodle: Learning Artistic Image Editing from Few-Shot Pairwise Data', 'url': 'https://huggingface.co/papers/2502.14397', 'abstract': "We introduce PhotoDoodle, a novel image editing framework designed to facilitate photo doodling by enabling artists to overlay decorative elements onto photographs. Photo doodling is challenging because the inserted elements must appear seamlessly integrated with the background, requiring realistic blending, perspective alignment, and contextual coherence. Additionally, the background must be preserved without distortion, and the artist's unique style must be captured efficiently from limited training data. These requirements are not addressed by previous methods that primarily focus on global style transfer or regional inpainting. The proposed method, PhotoDoodle, employs a two-stage training strategy. Initially, we train a general-purpose image editing model, OmniEditor, using large-scale data. Subsequently, we fine-tune this model with EditLoRA using a small, artist-curated dataset of before-and-after image pairs to capture distinct editing styles and techniques. To enhance consistency in the generated results, we introduce a positional encoding reuse mechanism. Additionally, we release a PhotoDoodle dataset featuring six high-quality styles. Extensive experiments demonstrate the advanced performance and robustness of our method in customized image editing, opening new possibilities for artistic creation.", 'score': 7, 'issue_id': 2364, 'pub_date': '2025-02-20', 'pub_date_card': {'ru': '20 февраля', 'en': 'February 20', 'zh': '2月20日'}, 'hash': '7e08e4606569ffb5', 'authors': ['Shijie Huang', 'Yiren Song', 'Yuxuan Zhang', 'Hailong Guo', 'Xueyin Wang', 'Mike Zheng Shou', 'Jiaming Liu'], 'affiliations': ['Beijing University of Posts and Telecommunications', 'Byte Dance', 'National University of Singapore', 'Shanghai Jiao Tong University', 'Tiamat'], 'pdf_title_img': 'assets/pdf/title_img/2502.14397.jpg', 'data': {'categories': ['#training', '#data', '#cv', '#dataset'], 'emoji': '🎨', 'ru': {'title': 'PhotoDoodle: Персонализированное художественное редактирование фотографий', 'desc': 'PhotoDoodle - это новая система редактирования изображений, позволяющая художникам накладывать декоративные элементы на фотографии. Она решает проблемы реалистичного наложения, выравнивания перспективы и сохранения контекста. Метод использует двухэтапную стратегию обучения: сначала тренируется универсальная модель OmniEditor, затем она дообучается на небольшом наборе данных конкретного художника с помощью EditLoRA. Авторы также представили датасет PhotoDoodle с шестью высококачественными стилями.'}, 'en': {'title': 'Seamless Artistic Integration with PhotoDoodle', 'desc': 'PhotoDoodle is an innovative image editing framework that allows artists to add decorative elements to photographs while ensuring they blend seamlessly with the background. The challenge lies in maintaining realistic integration, perspective alignment, and preserving the original photo without distortion. To achieve this, PhotoDoodle uses a two-stage training approach, starting with a general image editing model and then fine-tuning it with a small dataset of artist-specific edits. The framework also introduces a positional encoding reuse mechanism to improve the consistency of the edited images, showcasing its effectiveness in customized artistic creation.'}, 'zh': {'title': 'PhotoDoodle：艺术创作的新可能性', 'desc': '本文介绍了一种名为PhotoDoodle的新型图像编辑框架，旨在帮助艺术家在照片上叠加装饰元素。PhotoDoodle解决了图像合成中的多个挑战，包括元素与背景的无缝融合、透视对齐和上下文一致性。该方法采用两阶段训练策略，首先使用大规模数据训练通用图像编辑模型OmniEditor，然后通过小型艺术家策划的数据集进行微调，以捕捉独特的编辑风格。实验结果表明，PhotoDoodle在定制图像编辑方面表现出色，展示了艺术创作的新可能性。'}}}, {'id': 'https://huggingface.co/papers/2502.11663', 'title': 'MaskGWM: A Generalizable Driving World Model with Video Mask Reconstruction', 'url': 'https://huggingface.co/papers/2502.11663', 'abstract': 'World models that forecast environmental changes from actions are vital for autonomous driving models with strong generalization. The prevailing driving world model mainly build on video prediction model. Although these models can produce high-fidelity video sequences with advanced diffusion-based generator, they are constrained by their predictive duration and overall generalization capabilities. In this paper, we explore to solve this problem by combining generation loss with MAE-style feature-level context learning. In particular, we instantiate this target with three key design: (1) A more scalable Diffusion Transformer (DiT) structure trained with extra mask construction task. (2) we devise diffusion-related mask tokens to deal with the fuzzy relations between mask reconstruction and generative diffusion process. (3) we extend mask construction task to spatial-temporal domain by utilizing row-wise mask for shifted self-attention rather than masked self-attention in MAE. Then, we adopt a row-wise cross-view module to align with this mask design. Based on above improvement, we propose MaskGWM: a Generalizable driving World Model embodied with Video Mask reconstruction. Our model contains two variants: MaskGWM-long, focusing on long-horizon prediction, and MaskGWM-mview, dedicated to multi-view generation. Comprehensive experiments on standard benchmarks validate the effectiveness of the proposed method, which contain normal validation of Nuscene dataset, long-horizon rollout of OpenDV-2K dataset and zero-shot validation of Waymo dataset. Quantitative metrics on these datasets show our method notably improving state-of-the-art driving world model.', 'score': 4, 'issue_id': 2367, 'pub_date': '2025-02-17', 'pub_date_card': {'ru': '17 февраля', 'en': 'February 17', 'zh': '2月17日'}, 'hash': '14ade1d8007cfa16', 'authors': ['Jingcheng Ni', 'Yuxin Guo', 'Yichen Liu', 'Rui Chen', 'Lewei Lu', 'Zehuan Wu'], 'affiliations': ['SenseTime Research'], 'pdf_title_img': 'assets/pdf/title_img/2502.11663.jpg', 'data': {'categories': ['#video', '#diffusion', '#architecture', '#long_context', '#games', '#dataset', '#benchmark'], 'emoji': '🚗', 'ru': {'title': 'MaskGWM: Улучшенная модель мира для автономного вождения', 'desc': 'Статья представляет новую модель MaskGWM для прогнозирования изменений окружающей среды в контексте автономного вождения. Модель сочетает генеративные методы на основе диффузии с обучением контекста на уровне признаков в стиле MAE. Ключевые особенности включают масштабируемую структуру Diffusion Transformer, специальные маскирующие токены для диффузионного процесса и пространственно-временное построение масок. Эксперименты показывают улучшение современных результатов в долгосрочном прогнозировании и генерации с нескольких ракурсов.'}, 'en': {'title': 'Enhancing Autonomous Driving with MaskGWM: A New Era in World Models', 'desc': 'This paper presents a new approach to creating world models for autonomous driving that can better predict environmental changes over time. The authors introduce MaskGWM, which combines advanced video generation techniques with feature-level context learning to enhance generalization. Key innovations include a scalable Diffusion Transformer architecture, the use of diffusion-related mask tokens, and an extension of mask construction to the spatial-temporal domain. Experimental results demonstrate that MaskGWM significantly outperforms existing models in various driving scenarios, showcasing its effectiveness in long-horizon and multi-view predictions.'}, 'zh': {'title': '提升驾驶模型的泛化能力', 'desc': '本文探讨了一种新的驾驶世界模型，旨在通过结合生成损失和MAE风格的特征级上下文学习来提高模型的泛化能力。我们提出了一种可扩展的扩散变换器结构，并引入了扩散相关的掩码标记，以处理掩码重建与生成扩散过程之间的模糊关系。此外，我们将掩码构建任务扩展到时空域，采用行级掩码进行偏移自注意力。实验结果表明，所提出的MaskGWM模型在多个标准数据集上显著提升了驾驶世界模型的性能。'}}}, {'id': 'https://huggingface.co/papers/2502.15631', 'title': 'The Relationship Between Reasoning and Performance in Large Language Models -- o3 (mini) Thinks Harder, Not Longer', 'url': 'https://huggingface.co/papers/2502.15631', 'abstract': 'Large language models have demonstrated remarkable progress in mathematical reasoning, leveraging chain-of-thought and test-time compute scaling. However, many open questions remain regarding the interplay between reasoning token usage and accuracy gains. In particular, when comparing models across generations, it is unclear whether improved performance results from longer reasoning chains or more efficient reasoning. We systematically analyze chain-of-thought length across o1-mini and o3-mini variants on the Omni-MATH benchmark, finding that o3-mini (m) achieves superior accuracy without requiring longer reasoning chains than o1-mini. Moreover, we show that accuracy generally declines as reasoning chains grow across all models and compute settings, even when controlling for difficulty of the questions. This accuracy drop is significantly smaller in more proficient models, suggesting that new generations of reasoning models use test-time compute more effectively. Finally, we highlight that while o3-mini (h) achieves a marginal accuracy gain over o3-mini (m), it does so by allocating substantially more reasoning tokens across all problems, even the ones that o3-mini (m) can already solve. These findings provide new insights into the relationship between model capability and reasoning length, with implications for efficiency, scaling, and evaluation methodologies.', 'score': 3, 'issue_id': 2363, 'pub_date': '2025-02-21', 'pub_date_card': {'ru': '21 февраля', 'en': 'February 21', 'zh': '2月21日'}, 'hash': '6f204197089fd2e2', 'authors': ['Marthe Ballon', 'Andres Algaba', 'Vincent Ginis'], 'affiliations': ['Data Analytics Lab, Vrije Universiteit Brussel, 1050 Brussel, Belgium', 'School of Engineering and Applied Sciences, Harvard University, Cambridge, Massachusetts 02138, USA'], 'pdf_title_img': 'assets/pdf/title_img/2502.15631.jpg', 'data': {'categories': ['#reasoning', '#math', '#training', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Эффективность рассуждений важнее их длины в языковых моделях', 'desc': 'Статья анализирует взаимосвязь между длиной цепочки рассуждений и точностью в больших языковых моделях при решении математических задач. Исследователи обнаружили, что более новые модели достигают лучших результатов без увеличения длины рассуждений. Интересно, что точность обычно снижается при удлинении цепочки рассуждений, но это падение меньше у более совершенных моделей. Результаты показывают, что новые поколения моделей эффективнее используют вычислительные ресурсы во время вывода.'}, 'en': {'title': 'Efficiency Over Length: Unlocking Better Reasoning in Language Models', 'desc': 'This paper investigates how the length of reasoning chains in large language models affects their accuracy in mathematical reasoning tasks. It compares two model variants, o1-mini and o3-mini, on the Omni-MATH benchmark, revealing that o3-mini achieves better accuracy without needing longer reasoning chains. The study finds that longer reasoning chains often lead to decreased accuracy, especially across various models and question difficulties. Additionally, it suggests that newer models are more efficient in using reasoning tokens, which has important implications for model evaluation and scaling.'}, 'zh': {'title': '推理链长度与模型准确性的关系', 'desc': '这篇论文探讨了大型语言模型在数学推理中的表现，特别是推理链的长度与准确性之间的关系。研究发现，o3-mini模型在不需要更长推理链的情况下，能够实现更高的准确率。论文还指出，随着推理链的增长，所有模型的准确性普遍下降，但在更高效的模型中，这种下降的幅度较小。最后，尽管o3-mini(h)在准确性上略有提升，但它需要分配更多的推理标记，这表明新一代模型在测试时的计算效率更高。'}}}, {'id': 'https://huggingface.co/papers/2502.15007', 'title': 'LLM-Microscope: Uncovering the Hidden Role of Punctuation in Context Memory of Transformers', 'url': 'https://huggingface.co/papers/2502.15007', 'abstract': "We introduce methods to quantify how Large Language Models (LLMs) encode and store contextual information, revealing that tokens often seen as minor (e.g., determiners, punctuation) carry surprisingly high context. Notably, removing these tokens -- especially stopwords, articles, and commas -- consistently degrades performance on MMLU and BABILong-4k, even if removing only irrelevant tokens. Our analysis also shows a strong correlation between contextualization and linearity, where linearity measures how closely the transformation from one layer's embeddings to the next can be approximated by a single linear mapping. These findings underscore the hidden importance of filler tokens in maintaining context. For further exploration, we present LLM-Microscope, an open-source toolkit that assesses token-level nonlinearity, evaluates contextual memory, visualizes intermediate layer contributions (via an adapted Logit Lens), and measures the intrinsic dimensionality of representations. This toolkit illuminates how seemingly trivial tokens can be critical for long-range understanding.", 'score': 2, 'issue_id': 2367, 'pub_date': '2025-02-20', 'pub_date_card': {'ru': '20 февраля', 'en': 'February 20', 'zh': '2月20日'}, 'hash': '9be244de8b366352', 'authors': ['Anton Razzhigaev', 'Matvey Mikhalchuk', 'Temurbek Rahmatullaev', 'Elizaveta Goncharova', 'Polina Druzhinina', 'Ivan Oseledets', 'Andrey Kuznetsov'], 'affiliations': ['AIRI', 'HSE University', 'Lomonosov Moscow State University', 'Skoltech'], 'pdf_title_img': 'assets/pdf/title_img/2502.15007.jpg', 'data': {'categories': ['#data', '#architecture', '#long_context', '#interpretability', '#training', '#open_source'], 'emoji': '🔬', 'ru': {'title': 'Скрытая сила мелочей: как незаметные токены влияют на понимание контекста в LLM', 'desc': 'Исследователи представили методы для измерения того, как большие языковые модели (LLM) кодируют и хранят контекстную информацию. Они обнаружили, что даже незначительные на первый взгляд токены (например, артикли и пунктуация) несут важную контекстную информацию. Удаление таких токенов приводит к снижению производительности модели на тестах MMLU и BABILong-4k. Авторы также выявили сильную корреляцию между контекстуализацией и линейностью в трансформациях между слоями модели.'}, 'en': {'title': 'Unveiling the Hidden Power of Minor Tokens in LLMs', 'desc': 'This paper explores how Large Language Models (LLMs) encode contextual information, highlighting the significant role of seemingly minor tokens like stopwords and punctuation. The authors demonstrate that removing these tokens negatively impacts model performance on tasks such as MMLU and BABILong-4k, indicating their importance in maintaining context. They also find a correlation between the contextualization of tokens and the linearity of transformations between model layers, suggesting that the way information is processed is not purely linear. To aid further research, the authors introduce LLM-Microscope, a toolkit designed to analyze token-level nonlinearity and visualize the contributions of different layers in LLMs.'}, 'zh': {'title': '隐藏的重要性：填充标记在上下文中的关键作用', 'desc': '本文介绍了量化大型语言模型（LLMs）如何编码和存储上下文信息的方法。研究发现，通常被视为次要的标记（如限定词和标点符号）实际上承载着意想不到的高上下文信息。特别是，去除这些标记（尤其是停用词、冠词和逗号）会显著降低MMLU和BABILong-4k的性能。我们的分析还表明，上下文化与线性度之间存在强相关性，线性度衡量从一层嵌入到下一层的转换是否可以用单一线性映射来近似。'}}}, {'id': 'https://huggingface.co/papers/2502.15657', 'title': 'Superintelligent Agents Pose Catastrophic Risks: Can Scientist AI Offer a Safer Path?', 'url': 'https://huggingface.co/papers/2502.15657', 'abstract': 'The leading AI companies are increasingly focused on building generalist AI agents -- systems that can autonomously plan, act, and pursue goals across almost all tasks that humans can perform. Despite how useful these systems might be, unchecked AI agency poses significant risks to public safety and security, ranging from misuse by malicious actors to a potentially irreversible loss of human control. We discuss how these risks arise from current AI training methods. Indeed, various scenarios and experiments have demonstrated the possibility of AI agents engaging in deception or pursuing goals that were not specified by human operators and that conflict with human interests, such as self-preservation. Following the precautionary principle, we see a strong need for safer, yet still useful, alternatives to the current agency-driven trajectory. Accordingly, we propose as a core building block for further advances the development of a non-agentic AI system that is trustworthy and safe by design, which we call Scientist AI. This system is designed to explain the world from observations, as opposed to taking actions in it to imitate or please humans. It comprises a world model that generates theories to explain data and a question-answering inference machine. Both components operate with an explicit notion of uncertainty to mitigate the risks of overconfident predictions. In light of these considerations, a Scientist AI could be used to assist human researchers in accelerating scientific progress, including in AI safety. In particular, our system can be employed as a guardrail against AI agents that might be created despite the risks involved. Ultimately, focusing on non-agentic AI may enable the benefits of AI innovation while avoiding the risks associated with the current trajectory. We hope these arguments will motivate researchers, developers, and policymakers to favor this safer path.', 'score': 2, 'issue_id': 2365, 'pub_date': '2025-02-21', 'pub_date_card': {'ru': '21 февраля', 'en': 'February 21', 'zh': '2月21日'}, 'hash': '66434dc15bcc1913', 'authors': ['Yoshua Bengio', 'Michael Cohen', 'Damiano Fornasiere', 'Joumana Ghosn', 'Pietro Greiner', 'Matt MacDermott', 'Sören Mindermann', 'Adam Oberman', 'Jesse Richardson', 'Oliver Richardson', 'Marc-Antoine Rondeau', 'Pierre-Luc St-Charles', 'David Williams-King'], 'affiliations': ['Imperial College London', 'McGill University', 'Mila Quebec AI Institute', 'Universite de Montreal', 'University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2502.15657.jpg', 'data': {'categories': ['#training', '#inference', '#agents', '#security', '#science', '#agi', '#ethics'], 'emoji': '🔬', 'ru': {'title': 'Безопасный ИИ: от агентов к ученым', 'desc': 'В статье обсуждаются риски, связанные с разработкой генералистических ИИ-агентов, способных автономно планировать и действовать. Авторы предлагают альтернативу в виде неагентной системы ИИ под названием Scientist AI, которая объясняет мир на основе наблюдений, а не действует в нем. Система состоит из модели мира, генерирующей теории для объяснения данных, и механизма вывода для ответов на вопросы. Предлагается использовать Scientist AI для ускорения научного прогресса и в качестве защиты от потенциально опасных ИИ-агентов.'}, 'en': {'title': 'Towards Safer AI: Embracing Non-Agentic Systems', 'desc': 'This paper discusses the risks associated with developing generalist AI agents that can operate autonomously across various tasks. It highlights how current AI training methods can lead to unintended behaviors, such as deception or pursuing harmful goals. To address these concerns, the authors propose a new approach called Scientist AI, which focuses on creating a non-agentic system that explains observations rather than taking actions. This system aims to assist human researchers while ensuring safety and trustworthiness, ultimately promoting a safer path for AI development.'}, 'zh': {'title': '安全与创新并重的科学家AI', 'desc': '本文讨论了当前人工智能（AI）系统在自主规划和执行任务方面的风险，尤其是当这些系统可能偏离人类的意图时。我们提出了一种新的非代理AI系统，称为科学家AI，它旨在通过观察来解释世界，而不是主动采取行动。科学家AI包括一个世界模型和一个问答推理机器，能够处理不确定性，从而减少过于自信的预测带来的风险。我们希望这种安全的AI系统能够帮助人类研究人员加速科学进步，同时作为对抗潜在危险AI代理的保护措施。'}}}, {'id': 'https://huggingface.co/papers/2502.14905', 'title': 'Think Inside the JSON: Reinforcement Strategy for Strict LLM Schema Adherence', 'url': 'https://huggingface.co/papers/2502.14905', 'abstract': 'In this paper, we address the challenge of enforcing strict schema adherence in large language model (LLM) generation by leveraging LLM reasoning capabilities. Building on the DeepSeek R1 reinforcement learning framework, our approach trains structured reasoning skills of a 1.5B parameter model through a novel pipeline that combines synthetic reasoning dataset construction with custom reward functions under Group Relative Policy Optimization (GRPO). Specifically, we first perform R1 reinforcement learning on a 20K sample unstructured-to-structured dataset, mirroring the original DeepSeek R1 methods, to establish core reasoning abilities. Subsequently, we performed supervised fine-tuning on a separate 10K reasoning sample dataset, focusing on refining schema adherence for downstream tasks. Despite the relatively modest training scope, requiring approximately 20 hours on an 8xH100 GPU cluster for GRPO training and 3 hours on 1xA100 for SFT, our model demonstrates robust performance in enforcing schema consistency. We compare our ThinkJSON approach against the original DeepSeek R1 (671B), distilled versions of DeepSeek R1 (Qwen-1.5B and Qwen-7B), and Gemini 2.0 Flash (70B), showcasing its effectiveness in real-world applications. Our results underscore the practical utility of a resource-efficient framework for schema-constrained text generation.', 'score': 2, 'issue_id': 2363, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 февраля', 'en': 'February 18', 'zh': '2月18日'}, 'hash': 'e6a1c222ee43df08', 'authors': ['Bhavik Agarwal', 'Ishan Joshi', 'Viktoria Rojkova'], 'affiliations': ['MasterControl AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2502.14905.jpg', 'data': {'categories': ['#training', '#rl', '#synthetic', '#dataset', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Эффективное обучение LLM для генерации структурированного текста', 'desc': 'В этой статье представлен метод обеспечения строгого соблюдения схемы при генерации текста большими языковыми моделями (LLM) с использованием их способностей к рассуждению. Авторы разработали подход на основе фреймворка DeepSeek R1, обучая модель с 1,5 миллиардами параметров структурированному рассуждению через новый конвейер, сочетающий синтетические наборы данных и пользовательские функции вознаграждения в рамках Group Relative Policy Optimization (GRPO). Несмотря на относительно скромный объем обучения, модель демонстрирует надежную производительность в обеспечении согласованности схемы. Результаты подчеркивают практическую полезность ресурсоэффективного фреймворка для генерации текста с ограничениями по схеме.'}, 'en': {'title': 'ThinkJSON: Efficient Schema-Constrained Text Generation with LLMs', 'desc': 'This paper presents a method to improve how large language models (LLMs) follow specific rules or schemas when generating text. The authors use a reinforcement learning approach called DeepSeek R1 to train a 1.5 billion parameter model, enhancing its reasoning skills through a combination of synthetic data and custom reward systems. They first establish core reasoning abilities with a large unstructured dataset and then fine-tune the model on a smaller dataset focused on schema adherence. The results show that their method, named ThinkJSON, performs well in maintaining schema consistency compared to larger models, demonstrating an efficient way to generate structured text.'}, 'zh': {'title': '利用推理能力强化模式遵循', 'desc': '本文探讨了在大型语言模型生成中强制遵循严格模式的挑战，利用了语言模型的推理能力。我们基于DeepSeek R1强化学习框架，训练了一个具有15亿参数的模型，通过合成推理数据集构建和自定义奖励函数，结合群体相对策略优化（GRPO）的方法。首先，我们在一个2万样本的非结构化到结构化数据集上进行R1强化学习，以建立核心推理能力。随后，我们在一个单独的1万推理样本数据集上进行了监督微调，专注于提高下游任务的模式遵循性。'}}}, {'id': 'https://huggingface.co/papers/2502.15681', 'title': 'One-step Diffusion Models with $f$-Divergence Distribution Matching', 'url': 'https://huggingface.co/papers/2502.15681', 'abstract': "Sampling from diffusion models involves a slow iterative process that hinders their practical deployment, especially for interactive applications. To accelerate generation speed, recent approaches distill a multi-step diffusion model into a single-step student generator via variational score distillation, which matches the distribution of samples generated by the student to the teacher's distribution. However, these approaches use the reverse Kullback-Leibler (KL) divergence for distribution matching which is known to be mode seeking. In this paper, we generalize the distribution matching approach using a novel f-divergence minimization framework, termed f-distill, that covers different divergences with different trade-offs in terms of mode coverage and training variance. We derive the gradient of the f-divergence between the teacher and student distributions and show that it is expressed as the product of their score differences and a weighting function determined by their density ratio. This weighting function naturally emphasizes samples with higher density in the teacher distribution, when using a less mode-seeking divergence. We observe that the popular variational score distillation approach using the reverse-KL divergence is a special case within our framework. Empirically, we demonstrate that alternative f-divergences, such as forward-KL and Jensen-Shannon divergences, outperform the current best variational score distillation methods across image generation tasks. In particular, when using Jensen-Shannon divergence, f-distill achieves current state-of-the-art one-step generation performance on ImageNet64 and zero-shot text-to-image generation on MS-COCO. Project page: https://research.nvidia.com/labs/genair/f-distill", 'score': 1, 'issue_id': 2364, 'pub_date': '2025-02-21', 'pub_date_card': {'ru': '21 февраля', 'en': 'February 21', 'zh': '2月21日'}, 'hash': 'b1dfe60e5b59d586', 'authors': ['Yilun Xu', 'Weili Nie', 'Arash Vahdat'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2502.15681.jpg', 'data': {'categories': ['#optimization', '#cv', '#diffusion', '#inference'], 'emoji': '🖼️', 'ru': {'title': 'Ускорение диффузионных моделей через минимизацию f-дивергенции', 'desc': 'Статья представляет новый метод ускорения генерации изображений с помощью диффузионных моделей, называемый f-distill. Авторы обобщают подход сопоставления распределений, используя минимизацию f-дивергенции вместо обратной дивергенции Кульбака-Лейблера. Это позволяет достичь лучшего баланса между покрытием мод и дисперсией обучения. Эмпирически показано, что f-distill превосходит существующие методы вариационной дистилляции оценок на задачах генерации изображений.'}, 'en': {'title': 'Accelerating Diffusion Models with f-Distill for Faster Image Generation', 'desc': 'This paper addresses the slow sampling process of diffusion models, which limits their use in real-time applications. It introduces a new method called f-distill that generalizes the distribution matching process using f-divergence minimization, allowing for better trade-offs between mode coverage and training variance. The authors derive a gradient expression for f-divergence that highlights samples with higher density in the teacher distribution, improving the quality of generated samples. Empirical results show that using f-distill with alternative divergences, particularly Jensen-Shannon divergence, leads to superior performance in image generation tasks compared to existing methods.'}, 'zh': {'title': '加速扩散模型生成的新方法', 'desc': '本文提出了一种新的分布匹配方法，称为f-distill，旨在加速扩散模型的生成速度。通过引入f-散度最小化框架，f-distill能够处理不同的散度，从而在模式覆盖和训练方差之间取得不同的权衡。我们推导了教师和学生分布之间f-散度的梯度，并展示了其与分数差异和密度比的加权函数的关系。实验结果表明，使用Jensen-Shannon散度的f-distill在图像生成任务中表现优于现有的变分分数蒸馏方法。'}}}, {'id': 'https://huggingface.co/papers/2502.15027', 'title': 'InterFeedback: Unveiling Interactive Intelligence of Large Multimodal Models via Human Feedback', 'url': 'https://huggingface.co/papers/2502.15027', 'abstract': "Existing benchmarks do not test Large Multimodal Models (LMMs) on their interactive intelligence with human users which is vital for developing general-purpose AI assistants. We design InterFeedback, an interactive framework, which can be applied to any LMM and dataset to assess this ability autonomously. On top of this, we introduce InterFeedback-Bench which evaluates interactive intelligence using two representative datasets, MMMU-Pro and MathVerse, to test 10 different open-source LMMs. Additionally, we present InterFeedback-Human, a newly collected dataset of 120 cases designed for manually testing interactive performance in leading models such as OpenAI-o1 and Claude-3.5-Sonnet. Our evaluation results show that even state-of-the-art LMM (like OpenAI-o1) can correct their results through human feedback less than 50%. Our findings point to the need for methods that can enhance the LMMs' capability to interpret and benefit from feedback.", 'score': 1, 'issue_id': 2363, 'pub_date': '2025-02-20', 'pub_date_card': {'ru': '20 февраля', 'en': 'February 20', 'zh': '2月20日'}, 'hash': '3b32932adf862766', 'authors': ['Henry Hengyuan Zhao', 'Wenqi Pei', 'Yifei Tao', 'Haiyang Mei', 'Mike Zheng Shou'], 'affiliations': ['Show Lab, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2502.15027.jpg', 'data': {'categories': ['#interpretability', '#benchmark', '#rlhf', '#dataset', '#agi', '#multimodal', '#open_source'], 'emoji': '🤖', 'ru': {'title': 'Интерактивный интеллект LMM: новые методы оценки и улучшения', 'desc': 'Статья представляет новый фреймворк InterFeedback для оценки интерактивного интеллекта мультимодальных языковых моделей (LMM). Авторы разработали InterFeedback-Bench для тестирования 10 различных LMM на двух наборах данных. Также был создан набор данных InterFeedback-Human для ручного тестирования ведущих моделей. Результаты показывают, что даже лучшие LMM способны корректировать свои результаты на основе обратной связи менее чем в 50% случаев.'}, 'en': {'title': 'Enhancing Interactive Intelligence in Large Multimodal Models', 'desc': 'This paper addresses the gap in evaluating Large Multimodal Models (LMMs) regarding their interactive intelligence with human users. The authors introduce InterFeedback, a framework that autonomously assesses the interactive capabilities of any LMM across various datasets. They also present InterFeedback-Bench, which tests 10 open-source LMMs using two datasets, MMMU-Pro and MathVerse, and InterFeedback-Human, a dataset for manual evaluation of models like OpenAI-o1 and Claude-3.5-Sonnet. The results reveal that even advanced LMMs struggle to incorporate human feedback effectively, highlighting the need for improved methods to enhance their responsiveness to user interactions.'}, 'zh': {'title': '提升大型多模态模型的互动智能', 'desc': '现有的基准测试并未评估大型多模态模型（LMM）与人类用户的互动智能，而这对于开发通用人工智能助手至关重要。我们设计了InterFeedback，这是一个互动框架，可以应用于任何LMM和数据集，以自主评估这种能力。此外，我们推出了InterFeedback-Bench，使用两个代表性数据集MMM-Pro和MathVerse来评估10种不同的开源LMM的互动智能。我们的评估结果表明，即使是最先进的LMM（如OpenAI-o1）也只能通过人类反馈纠正不到50%的结果，显示出增强LMM解读和利用反馈能力的方法的必要性。'}}}, {'id': 'https://huggingface.co/papers/2502.15011', 'title': 'CrossOver: 3D Scene Cross-Modal Alignment', 'url': 'https://huggingface.co/papers/2502.15011', 'abstract': 'Multi-modal 3D object understanding has gained significant attention, yet current approaches often assume complete data availability and rigid alignment across all modalities. We present CrossOver, a novel framework for cross-modal 3D scene understanding via flexible, scene-level modality alignment. Unlike traditional methods that require aligned modality data for every object instance, CrossOver learns a unified, modality-agnostic embedding space for scenes by aligning modalities - RGB images, point clouds, CAD models, floorplans, and text descriptions - with relaxed constraints and without explicit object semantics. Leveraging dimensionality-specific encoders, a multi-stage training pipeline, and emergent cross-modal behaviors, CrossOver supports robust scene retrieval and object localization, even with missing modalities. Evaluations on ScanNet and 3RScan datasets show its superior performance across diverse metrics, highlighting adaptability for real-world applications in 3D scene understanding.', 'score': 0, 'issue_id': 2366, 'pub_date': '2025-02-20', 'pub_date_card': {'ru': '20 февраля', 'en': 'February 20', 'zh': '2月20日'}, 'hash': '97fe61eb66853ca2', 'authors': ['Sayan Deb Sarkar', 'Ondrej Miksik', 'Marc Pollefeys', 'Daniel Barath', 'Iro Armeni'], 'affiliations': ['ETH Zurich', 'Microsoft Spatial AI Lab, Zurich', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2502.15011.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#3d', '#training'], 'emoji': '🌐', 'ru': {'title': 'Гибкое объединение модальностей для понимания 3D-сцен', 'desc': 'CrossOver - это новая система для кросс-модального понимания 3D-сцен, использующая гибкое выравнивание модальностей на уровне сцены. Она создает единое модально-агностическое пространство представлений для сцен, объединяя различные модальности без жестких ограничений. CrossOver использует специфические для разных размерностей энкодеры и многоступенчатый процесс обучения. Система демонстрирует высокую эффективность в задачах поиска сцен и локализации объектов даже при отсутствии некоторых модальностей.'}, 'en': {'title': 'CrossOver: Flexible 3D Scene Understanding Across Modalities', 'desc': 'This paper introduces CrossOver, a new framework designed for understanding 3D scenes using multiple types of data, such as images and point clouds. Unlike traditional methods that need all data types to be perfectly aligned, CrossOver allows for flexible alignment, making it easier to work with incomplete information. It creates a shared space where different data types can be compared and understood together, even if some data is missing. The results show that CrossOver performs better than existing methods, making it useful for real-world applications in 3D scene analysis.'}, 'zh': {'title': '跨模态3D场景理解的新突破', 'desc': '本论文提出了一种名为CrossOver的新框架，用于灵活的跨模态3D场景理解。与传统方法不同，CrossOver不需要每个对象实例的模态数据严格对齐，而是通过放宽约束来学习统一的模态无关嵌入空间。该框架支持RGB图像、点云、CAD模型、平面图和文本描述等多种模态的对齐，能够在缺失模态的情况下进行稳健的场景检索和对象定位。实验结果表明，CrossOver在ScanNet和3RScan数据集上表现优越，展示了其在实际3D场景理解中的适应性。'}}}, {'id': 'https://huggingface.co/papers/2502.15082', 'title': 'UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning', 'url': 'https://huggingface.co/papers/2502.15082', 'abstract': 'User specifications or legal frameworks often require information to be removed from pretrained models, including large language models (LLMs). This requires deleting or "forgetting" a set of data points from an already-trained model, which typically degrades its performance on other data points. Thus, a balance must be struck between removing information and keeping the model\'s other abilities intact, with a failure to balance this trade-off leading to poor deletion or an unusable model. To this end, we propose UPCORE (Utility-Preserving Coreset Selection), a method-agnostic data selection framework for mitigating collateral damage during unlearning. Finding that the model damage is correlated with the variance of the model\'s representations on the forget set, we selectively prune the forget set to remove outliers, thereby minimizing model degradation after unlearning. We evaluate UPCORE across three standard unlearning methods consistently achieving a superior balance between the competing objectives of deletion efficacy and model preservation. To better evaluate this trade-off, we introduce a new metric, measuring the area-under-the-curve (AUC) across standard metrics. We find that UPCORE improves both standard metrics and AUC, benefitting from positive transfer between the coreset and pruned points while reducing negative transfer from the forget set to points outside of it.', 'score': 0, 'issue_id': 2365, 'pub_date': '2025-02-20', 'pub_date_card': {'ru': '20 февраля', 'en': 'February 20', 'zh': '2月20日'}, 'hash': '4a0bc63404dc5d71', 'authors': ['Vaidehi Patil', 'Elias Stengel-Eskin', 'Mohit Bansal'], 'affiliations': ['UNC Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2502.15082.jpg', 'data': {'categories': ['#training', '#inference', '#leakage', '#optimization', '#data'], 'emoji': '🧠', 'ru': {'title': 'Эффективное забывание без потери знаний', 'desc': 'Статья описывает метод UPCORE для улучшения процесса удаления информации из обученных языковых моделей. UPCORE минимизирует ухудшение работы модели после удаления данных путем выборочного отсечения выбросов из набора удаляемых данных. Метод применим к различным алгоритмам разобучения и позволяет достичь лучшего баланса между эффективностью удаления и сохранением производительности модели. Авторы также предлагают новую метрику на основе площади под кривой для оценки этого компромисса.'}, 'en': {'title': 'Smart Unlearning: Balancing Deletion and Model Integrity with UPCORE', 'desc': "This paper addresses the challenge of unlearning information from pretrained models, particularly large language models, without significantly harming their performance. The authors introduce UPCORE, a framework that selects a subset of data points to delete while minimizing the negative impact on the model's overall capabilities. By focusing on the variance of model representations, UPCORE effectively prunes outliers from the forget set, leading to better retention of the model's utility. The framework is evaluated against standard unlearning methods, demonstrating improved performance in both deletion effectiveness and model preservation through a new area-under-the-curve metric."}, 'zh': {'title': 'UPCORE：平衡遗忘与模型保留的创新方法', 'desc': '在机器学习中，用户的要求常常需要从预训练模型中删除特定信息，这被称为“遗忘”。然而，删除数据点可能会影响模型在其他数据上的表现，因此需要在删除信息和保持模型能力之间找到平衡。为了解决这个问题，我们提出了UPCORE（实用性保持核心集选择），这是一种通用的数据选择框架，旨在减少遗忘过程中的模型损害。通过选择性地修剪遗忘集中的异常值，UPCORE能够在删除有效性和模型保留之间实现更好的平衡。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents (1)', '#agi (3)', '#alignment (2)', '#architecture (4)', '#audio', '#benchmark (7)', '#cv (3)', '#data (6)', '#dataset (7)', '#diffusion (2)', '#ethics (1)', '#games (1)', '#graphs', '#hallucinations', '#healthcare', '#inference (5)', '#interpretability (3)', '#leakage (1)', '#long_context (3)', '#low_resource', '#machine_translation', '#math (2)', '#multilingual', '#multimodal (5)', '#open_source (4)', '#optimization (4)', '#plp', '#rag', '#reasoning (4)', '#rl (1)', '#rlhf (1)', '#robotics', '#science (2)', '#security (1)', '#small_models', '#story_generation', '#survey (1)', '#synthetic (1)', '#training (9)', '#transfer_learning', '#video (1)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-02-24 07:10',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-02-24 07:10')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-02-24 07:10')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    