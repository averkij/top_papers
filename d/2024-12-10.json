{
    "date": {
        "ru": "10 декабря",
        "en": "December 10",
        "zh": "12月10日"
    },
    "time_utc": "2024-12-10 07:11",
    "weekday": 1,
    "issue_id": 1039,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.06559",
            "title": "ProcessBench: Identifying Process Errors in Mathematical Reasoning",
            "url": "https://huggingface.co/papers/2412.06559",
            "abstract": "As language models regularly make mistakes when solving math problems, automated identification of errors in the reasoning process becomes increasingly significant for their scalable oversight. In this paper, we introduce ProcessBench for measuring the ability to identify erroneous steps in mathematical reasoning. It consists of 3,400 test cases, primarily focused on competition- and Olympiad-level math problems. Each test case contains a step-by-step solution with error location annotated by human experts. Models are required to identify the earliest step that contains an error, or conclude that all steps are correct. We conduct extensive evaluation on ProcessBench, involving two types of models: process reward models (PRMs) and critic models, where for the latter we prompt general language models to critique each solution step by step. We draw two main observations: (1) Existing PRMs typically fail to generalize to more challenging math problems beyond GSM8K and MATH. They underperform both critic models (i.e., prompted general language models) and our own trained PRM that is straightforwardly fine-tuned on the PRM800K dataset. (2) The best open-source model, QwQ-32B-Preview, has demonstrated the critique capability competitive with the proprietary model GPT-4o, despite that it still lags behind the reasoning-specialized o1-mini. We hope ProcessBench can foster future research in reasoning process assessment, paving the way toward scalable oversight of language models.",
            "score": 17,
            "issue_id": 1039,
            "pub_date": "2024-12-09",
            "pub_date_card": {
                "ru": "9 декабря",
                "en": "December 9",
                "zh": "12月9日"
            },
            "hash": "02f9abef0bc10297",
            "authors": [
                "Chujie Zheng",
                "Zhenru Zhang",
                "Beichen Zhang",
                "Runji Lin",
                "Keming Lu",
                "Bowen Yu",
                "Dayiheng Liu",
                "Jingren Zhou",
                "Junyang Lin"
            ],
            "affiliations": [
                "Qwen Team, Alibaba Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.06559.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#reasoning",
                    "#math",
                    "#benchmark",
                    "#open_source",
                    "#training"
                ],
                "emoji": "🧮",
                "ru": {
                    "title": "ProcessBench: новый бенчмарк для оценки выявления ошибок в математических рассуждениях",
                    "desc": "Статья представляет ProcessBench - набор данных для оценки способности моделей машинного обучения идентифицировать ошибки в математических рассуждениях. ProcessBench содержит 3400 тестовых примеров, в основном олимпиадного уровня, с пошаговыми решениями и аннотациями ошибок от экспертов. Авторы провели обширное тестирование различных моделей, включая Process Reward Models (PRM) и критические модели на основе больших языковых моделей. Результаты показывают, что существующие PRM плохо обобщаются на сложные задачи, а лучшая открытая модель QwQ-32B-Preview демонстрирует конкурентоспособность с проприетарной GPT-4."
                },
                "en": {
                    "title": "Enhancing Error Detection in Math Reasoning with ProcessBench",
                    "desc": "This paper presents ProcessBench, a benchmark designed to evaluate how well language models can identify errors in mathematical reasoning. It includes 3,400 test cases that focus on advanced math problems, with each case providing a detailed solution and expert-annotated error locations. The study compares the performance of process reward models (PRMs) and critic models, revealing that existing PRMs struggle with complex problems while critic models, particularly those based on general language models, perform better. The findings suggest that ProcessBench can enhance research on assessing reasoning processes in language models, contributing to their effective oversight."
                },
                "zh": {
                    "title": "提升语言模型数学推理的错误识别能力",
                    "desc": "本文介绍了一个名为ProcessBench的工具，用于评估语言模型在数学推理中识别错误步骤的能力。该工具包含3400个测试案例，主要集中在竞赛和奥林匹克级别的数学问题上。每个案例都提供了逐步解决方案，并由人类专家标注了错误位置。研究发现，现有的过程奖励模型在更具挑战性的数学问题上表现不佳，而经过微调的PRM在识别错误方面优于一般语言模型的批评能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.06769",
            "title": "Training Large Language Models to Reason in a Continuous Latent Space",
            "url": "https://huggingface.co/papers/2412.06769",
            "abstract": "Large language models (LLMs) are restricted to reason in the \"language space\", where they typically express the reasoning process with a chain-of-thought (CoT) to solve a complex reasoning problem. However, we argue that language space may not always be optimal for reasoning. For example, most word tokens are primarily for textual coherence and not essential for reasoning, while some critical tokens require complex planning and pose huge challenges to LLMs. To explore the potential of LLM reasoning in an unrestricted latent space instead of using natural language, we introduce a new paradigm Coconut (Chain of Continuous Thought). We utilize the last hidden state of the LLM as a representation of the reasoning state (termed \"continuous thought\"). Rather than decoding this into a word token, we feed it back to the LLM as the subsequent input embedding directly in the continuous space. Experiments show that Coconut can effectively augment the LLM on several reasoning tasks. This novel latent reasoning paradigm leads to emergent advanced reasoning patterns: the continuous thought can encode multiple alternative next reasoning steps, allowing the model to perform a breadth-first search (BFS) to solve the problem, rather than prematurely committing to a single deterministic path like CoT. Coconut outperforms CoT in certain logical reasoning tasks that require substantial backtracking during planning, with fewer thinking tokens during inference. These findings demonstrate the promise of latent reasoning and offer valuable insights for future research.",
            "score": 1,
            "issue_id": 1039,
            "pub_date": "2024-12-09",
            "pub_date_card": {
                "ru": "9 декабря",
                "en": "December 9",
                "zh": "12月9日"
            },
            "hash": "0ab7afee5f208244",
            "authors": [
                "Shibo Hao",
                "Sainbayar Sukhbaatar",
                "DiJia Su",
                "Xian Li",
                "Zhiting Hu",
                "Jason Weston",
                "Yuandong Tian"
            ],
            "affiliations": [
                "FAIR at Meta",
                "UC San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.06769.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#reasoning",
                    "#rl",
                    "#training"
                ],
                "emoji": "🥥",
                "ru": {
                    "title": "Coconut: непрерывные рассуждения в скрытом пространстве для больших языковых моделей",
                    "desc": "Статья представляет новую парадигму рассуждений для больших языковых моделей под названием Coconut (Chain of Continuous Thought). В отличие от традиционного подхода цепочки размышлений (CoT), Coconut использует скрытое состояние модели как непрерывное представление хода рассуждений. Эксперименты показывают, что Coconut может эффективно усиливать способности языковых моделей в задачах рассуждения, позволяя им выполнять поиск в ширину вместо детерминированного пути. Результаты демонстрируют преимущества Coconut над CoT в некоторых задачах логического вывода, требующих значительного бэктрекинга при планировании."
                },
                "en": {
                    "title": "Unlocking Reasoning Potential with Continuous Thought",
                    "desc": "This paper introduces a new reasoning approach for large language models (LLMs) called Coconut, which operates in a continuous latent space rather than the traditional language space. The authors argue that the language space can limit reasoning capabilities, as many tokens are not essential for reasoning tasks. By using the last hidden state of the LLM as a representation of reasoning, Coconut allows for more flexible exploration of reasoning paths, enabling the model to consider multiple alternatives simultaneously. Experimental results show that Coconut outperforms the conventional chain-of-thought method in logical reasoning tasks that require backtracking, demonstrating the effectiveness of this novel paradigm."
                },
                "zh": {
                    "title": "Coconut：超越语言空间的推理新范式",
                    "desc": "大型语言模型（LLMs）通常在“语言空间”中进行推理，使用链式思维（CoT）来解决复杂问题。然而，语言空间并不总是最优的推理方式，因为许多词汇主要用于文本连贯性，而非推理本身。本文提出了一种新范式Coconut（连续思维链），利用LLM的最后隐藏状态作为推理状态的表示，并直接在连续空间中进行输入嵌入。实验表明，Coconut在多个推理任务中有效增强了LLM的表现，尤其在需要大量回溯的逻辑推理任务中表现优于传统的CoT。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.06782",
            "title": "CARP: Visuomotor Policy Learning via Coarse-to-Fine Autoregressive Prediction",
            "url": "https://huggingface.co/papers/2412.06782",
            "abstract": "In robotic visuomotor policy learning, diffusion-based models have achieved significant success in improving the accuracy of action trajectory generation compared to traditional autoregressive models. However, they suffer from inefficiency due to multiple denoising steps and limited flexibility from complex constraints. In this paper, we introduce Coarse-to-Fine AutoRegressive Policy (CARP), a novel paradigm for visuomotor policy learning that redefines the autoregressive action generation process as a coarse-to-fine, next-scale approach. CARP decouples action generation into two stages: first, an action autoencoder learns multi-scale representations of the entire action sequence; then, a GPT-style transformer refines the sequence prediction through a coarse-to-fine autoregressive process. This straightforward and intuitive approach produces highly accurate and smooth actions, matching or even surpassing the performance of diffusion-based policies while maintaining efficiency on par with autoregressive policies. We conduct extensive evaluations across diverse settings, including single-task and multi-task scenarios on state-based and image-based simulation benchmarks, as well as real-world tasks. CARP achieves competitive success rates, with up to a 10% improvement, and delivers 10x faster inference compared to state-of-the-art policies, establishing a high-performance, efficient, and flexible paradigm for action generation in robotic tasks.",
            "score": 1,
            "issue_id": 1039,
            "pub_date": "2024-12-09",
            "pub_date_card": {
                "ru": "9 декабря",
                "en": "December 9",
                "zh": "12月9日"
            },
            "hash": "584dec780be05e2d",
            "authors": [
                "Zhefei Gong",
                "Pengxiang Ding",
                "Shangke Lyu",
                "Siteng Huang",
                "Mingyang Sun",
                "Wei Zhao",
                "Zhaoxin Fan",
                "Donglin Wang"
            ],
            "affiliations": [
                "Beijing Advanced Innovation Center for Future Blockchain and Privacy Computing",
                "Westlake University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.06782.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#diffusion",
                    "#agents",
                    "#training",
                    "#robotics"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "CARP: эффективное и точное обучение роботов через поэтапное уточнение действий",
                    "desc": "В этой статье представлен новый подход к обучению визуомоторной политики роботов, называемый CARP. Он использует двухэтапный процесс: сначала автоэнкодер действий обучается многомасштабным представлениям последовательности действий, а затем трансформер в стиле GPT уточняет предсказание последовательности через поэтапный авторегрессивный процесс. CARP показывает высокую точность и плавность действий, сравнимую или превосходящую диффузионные модели, при этом сохраняя эффективность авторегрессивных подходов. Метод демонстрирует конкурентоспособные показатели успешности и в 10 раз более быстрый вывод по сравнению с современными методами."
                },
                "en": {
                    "title": "Efficient and Accurate Action Generation with CARP",
                    "desc": "This paper presents the Coarse-to-Fine AutoRegressive Policy (CARP), a new method for robotic visuomotor policy learning that enhances action trajectory generation. CARP improves upon traditional autoregressive models by breaking down the action generation into two stages: first, it uses an action autoencoder to create multi-scale representations, and then a GPT-style transformer refines these predictions. This approach not only increases the accuracy and smoothness of actions but also maintains efficiency, achieving up to 10x faster inference than existing methods. Extensive evaluations show that CARP outperforms diffusion-based models and achieves competitive success rates in various robotic tasks."
                },
                "zh": {
                    "title": "高效灵活的机器人动作生成新范式",
                    "desc": "在机器人视觉运动策略学习中，基于扩散模型的技术在动作轨迹生成的准确性上取得了显著成功，但由于多次去噪步骤和复杂约束，效率较低。本文提出了一种新颖的粗到细自回归策略（CARP），将自回归动作生成过程重新定义为粗到细的下一尺度方法。CARP将动作生成分为两个阶段：首先，动作自编码器学习整个动作序列的多尺度表示；然后，GPT风格的变换器通过粗到细的自回归过程精炼序列预测。该方法在效率上与自回归策略相当，同时在准确性和流畅性上与基于扩散的策略相匹配或超越，展示了高性能、高效和灵活的机器人任务动作生成新范式。"
                }
            }
        }
    ],
    "link_prev": "2024-12-09.html",
    "link_next": "2024-12-11.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "09.12",
        "en": "12/09",
        "zh": "12月9日"
    },
    "short_date_next": {
        "ru": "11.12",
        "en": "12/11",
        "zh": "12月11日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 1,
        "#agents": 1,
        "#cv": 0,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 3,
        "#robotics": 1,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "我们介绍了 InternVL 2.5，这是一个基于 InternVL 2.0 的先进多模态大语言模型系列。它保留了核心模型架构，但在训练和测试策略以及数据质量方面进行了显著改进。我们研究了模型扩展与性能之间的关系，系统地探讨了视觉编码器、语言模型、数据集大小和测试时配置的性能趋势。通过广泛的评估，InternVL 2.5 在多个基准测试中表现出色，包括多学科推理、文档理解、多图像/视频理解、实际理解、多模态幻觉检测、视觉定位、多语言能力和纯语言处理。我们的模型是第一个在 MMMU 基准测试中超过 70% 的开源多模态大语言模型，并展示了强大的测试时扩展潜力。我们希望这个模型能为开源社区贡献新的标准，用于开发和应用多模态人工智能系统。HuggingFace 演示见 https://huggingface.co/spaces/OpenGVLab/InternVL。",
        "title": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling",
        "pinyin": "Wǒmen jièshào le InternVL 2.5, zhè shì yīgè jīyú InternVL 2.0 de xiānjìn duō móshì dà yǔyán móxíng xìliè. Tā bǎoliúle héxīn móxíng jiàgòu, dàn zài xùnliàn hé cèshì cèlüè yǐjiǎ shùjù zhìliàng fāngmiàn jìnxíng le xiǎnzhù gǎijìn. Wǒmen yánjiū le móxíng kuòzhǎn yǔ xìngnéng zhījiān de guānxì, xìtǒng de tàntào le shìjuān biānmǎqì, yǔyán móxíng, shùjùjí dàxìng hé cèshì shí pèizhì de xìngnéng qūshì. Tōngguò guǎngfàn de pínggū, InternVL 2.5 zài duōgè jīzhǔn cèshì zhōng biǎoxiàn chūsè, bāokuò duō xuékē tuīlǐ, wénjiàn lǐjiě, duō túxiàng/shìpín lǐjiě, shíjì lǐjiě, duō móshì huànjué jiǎncè, shìjuān dìngwèi, duō yǔyán nénglì hé chún yǔyán chǔlǐ. Wǒmen de móxíng shì dì-yīgè zài MMMU jīzhǔn cèshì zhōng chāoguò 70% de kāiyuán duō móshì dà yǔyán móxíng, bìng zhǎnshì le qiángdà de cèshì shí kuòzhǎn qiánlì. Wǒmen xīwàng zhègè móxíng néng wèi kāiyuán shèqū gòngxiàn xīn de biāozhǔn, yòngyú kāifā hé yìngyòng duō móshì réngōng zhìnéng xìtǒng. HuggingFace yǎnshì jiàn https://huggingface.co/spaces/OpenGVLab/InternVL.",
        "vocab": "[\n    {\"word\": \"介绍\", \"pinyin\": \"jiè shào\", \"trans\": \"introduce\"},\n    {\"word\": \"先进\", \"pinyin\": \"xiān jìn\", \"trans\": \"advanced\"},\n    {\"word\": \"多模态\", \"pinyin\": \"duō mó tài\", \"trans\": \"multimodal\"},\n    {\"word\": \"保留\", \"pinyin\": \"bǎo liú\", \"trans\": \"retain\"},\n    {\"word\": \"核心\", \"pinyin\": \"hé xīn\", \"trans\": \"core\"},\n    {\"word\": \"架构\", \"pinyin\": \"jià gòu\", \"trans\": \"architecture\"},\n    {\"word\": \"显著\", \"pinyin\": \"xiǎn zhù\", \"trans\": \"significant\"},\n    {\"word\": \"改进\", \"pinyin\": \"gǎi jìn\", \"trans\": \"improvement\"},\n    {\"word\": \"扩展\", \"pinyin\": \"kuò zhǎn\", \"trans\": \"expand\"},\n    {\"word\": \"性能\", \"pinyin\": \"xìng néng\", \"trans\": \"performance\"},\n    {\"word\": \"系统\", \"pinyin\": \"xì tǒng\", \"trans\": \"system\"},\n    {\"word\": \"探讨\", \"pinyin\": \"tàn tǎo\", \"trans\": \"explore\"},\n    {\"word\": \"视觉\", \"pinyin\": \"shì jué\", \"trans\": \"visual\"},\n    {\"word\": \"编码器\", \"pinyin\": \"biān mǎ qì\", \"trans\": \"encoder\"},\n    {\"word\": \"数据集\", \"pinyin\": \"shù jù jí\", \"trans\": \"dataset\"},\n    {\"word\": \"配置\", \"pinyin\": \"pèi zhì\", \"trans\": \"configuration\"},\n    {\"word\": \"趋势\", \"pinyin\": \"qū shì\", \"trans\": \"trend\"},\n    {\"word\": \"评估\", \"pinyin\": \"píng gū\", \"trans\": \"evaluate\"},\n    {\"word\": \"基准\", \"pinyin\": \"jī zhǔn\", \"trans\": \"benchmark\"},\n    {\"word\": \"表现\", \"pinyin\": \"biǎo xiàn\", \"trans\": \"performance\"},\n    {\"word\": \"出色\", \"pinyin\": \"chū sè\", \"trans\": \"outstanding\"},\n    {\"word\": \"多学科\", \"pinyin\": \"duō xué kē\", \"trans\": \"multidisciplinary\"},\n    {\"word\": \"推理\", \"pinyin\": \"tuī lǐ\", \"trans\": \"reasoning\"},\n    {\"word\": \"文档\", \"pinyin\": \"wén dàng\", \"trans\": \"document\"},\n    {\"word\": \"理解\", \"pinyin\": \"lǐ jiě\", \"trans\": \"understanding\"},\n    {\"word\": \"多图像\", \"pinyin\": \"duō tú xiàng\", \"trans\": \"multi-image\"},\n    {\"word\": \"视频\", \"pinyin\": \"shì pìn\", \"trans\": \"video\"},\n    {\"word\": \"实际\", \"pinyin\": \"shí jì\", \"trans\": \"practical\"},\n    {\"word\": \"幻觉\", \"pinyin\": \"huàn jué\", \"trans\": \"hallucination\"},\n    {\"word\": \"检测\", \"pinyin\": \"jiǎn cè\", \"trans\": \"detection\"},\n    {\"word\": \"定位\", \"pinyin\": \"dìng wèi\", \"trans\": \"localization\"},\n    {\"word\": \"多语言\", \"pinyin\": \"duō yǔ yán\", \"trans\": \"multilingual\"},\n    {\"word\": \"能力\", \"pinyin\": \"néng lì\", \"trans\": \"ability\"},\n    {\"word\": \"纯语言\", \"pinyin\": \"chún yǔ yán\", \"trans\": \"pure language\"},\n    {\"word\": \"处理\", \"pinyin\": \"chǔ lǐ\", \"trans\": \"process\"},\n    {\"word\": \"开源\", \"pinyin\": \"kāi yuán\", \"trans\": \"open-source\"},\n    {\"word\": \"社区\", \"pinyin\": \"shè qū\", \"trans\": \"community\"},\n    {\"word\": \"贡献\", \"pinyin\": \"gòng xiàn\", \"trans\": \"contribute\"},\n    {\"word\": \"标准\", \"pinyin\": \"biāo zhǔn\", \"trans\": \"standard\"},\n    {\"word\": \"开发\", \"pinyin\": \"kāi fā\", \"trans\": \"develop\"},\n    {\"word\": \"应用\", \"pinyin\": \"yìng yòng\", \"trans\": \"apply\"},\n    {\"word\": \"人工智能\", \"pinyin\": \"rén gōng zhì néng\", \"trans\": \"artificial intelligence\"},\n    {\"word\": \"系统\", \"pinyin\": \"xì tǒng\", \"trans\": \"system\"},\n    {\"word\": \"演示\", \"pinyin\": \"yǎn shì\", \"trans\": \"demonstration\"},\n    {\"word\": \"见\", \"pinyin\": \"jiàn\", \"trans\": \"see\"}\n]",
        "trans": "We introduced InternVL 2.5, an advanced multimodal large language model series based on InternVL 2.0. It retains the core model architecture but features significant improvements in training and testing strategies, as well as data quality. We studied the relationship between model scaling and performance, systematically exploring performance trends in visual encoders, language models, dataset sizes, and test-time configurations. Through extensive evaluations, InternVL 2.5 has demonstrated outstanding performance across multiple benchmarks, including multidisciplinary reasoning, document understanding, multi-image/video understanding, practical understanding, multimodal hallucination detection, visual localization, multilingual capabilities, and pure language processing. Our model is the first open-source multimodal large language model to exceed 70% on the MMMU benchmark and has shown strong test-time scaling potential. We hope this model will contribute new standards to the open-source community for developing and applying multimodal AI systems. See the HuggingFace demo at https://huggingface.co/spaces/OpenGVLab/InternVL.",
        "update_ts": "2024-12-09 09:12"
    }
}