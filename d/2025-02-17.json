{
    "date": {
        "ru": "17 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
        "en": "February 17",
        "zh": "2æœˆ17æ—¥"
    },
    "time_utc": "2025-02-17 16:12",
    "weekday": 0,
    "issue_id": 2253,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2502.10389",
            "title": "Region-Adaptive Sampling for Diffusion Transformers",
            "url": "https://huggingface.co/papers/2502.10389",
            "abstract": "Diffusion models (DMs) have become the leading choice for generative tasks across diverse domains. However, their reliance on multiple sequential forward passes significantly limits real-time performance. Previous acceleration methods have primarily focused on reducing the number of sampling steps or reusing intermediate results, failing to leverage variations across spatial regions within the image due to the constraints of convolutional U-Net structures. By harnessing the flexibility of Diffusion Transformers (DiTs) in handling variable number of tokens, we introduce RAS, a novel, training-free sampling strategy that dynamically assigns different sampling ratios to regions within an image based on the focus of the DiT model. Our key observation is that during each sampling step, the model concentrates on semantically meaningful regions, and these areas of focus exhibit strong continuity across consecutive steps. Leveraging this insight, RAS updates only the regions currently in focus, while other regions are updated using cached noise from the previous step. The model's focus is determined based on the output from the preceding step, capitalizing on the temporal consistency we observed. We evaluate RAS on Stable Diffusion 3 and Lumina-Next-T2I, achieving speedups up to 2.36x and 2.51x, respectively, with minimal degradation in generation quality. Additionally, a user study reveals that RAS delivers comparable qualities under human evaluation while achieving a 1.6x speedup. Our approach makes a significant step towards more efficient diffusion transformers, enhancing their potential for real-time applications.",
            "score": 42,
            "issue_id": 2241,
            "pub_date": "2025-02-14",
            "pub_date_card": {
                "ru": "14 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 14",
                "zh": "2æœˆ14æ—¥"
            },
            "hash": "8068f45b7fd0c2ee",
            "authors": [
                "Ziming Liu",
                "Yifan Yang",
                "Chengruidong Zhang",
                "Yiqi Zhang",
                "Lili Qiu",
                "Yang You",
                "Yuqing Yang"
            ],
            "affiliations": [
                "Microsoft Research",
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.10389.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#benchmark",
                    "#training",
                    "#diffusion",
                    "#architecture",
                    "#dataset"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ RAS. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ¾Ğ¿Ğ¸Ñ€Ğ°ÑÑÑŒ Ğ½Ğ° Ñ„Ğ¾ĞºÑƒÑ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Diffusion Transformer. RAS Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‚Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, Ğ½Ğ° ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚ ÑÑ„Ğ¾ĞºÑƒÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ´Ğ»Ñ Ğ¾ÑÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ÑˆÑƒĞ¼ Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰ĞµĞ³Ğ¾ ÑˆĞ°Ğ³Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ RAS Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ 2.51x Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Accelerating Diffusion Transformers with RAS for Real-Time Generation",
                    "desc": "This paper introduces RAS, a new sampling strategy for Diffusion Transformers (DiTs) that improves the efficiency of generative tasks. Traditional diffusion models require multiple sequential passes, which slow down real-time performance, but RAS dynamically adjusts sampling ratios based on the model's focus on different image regions. By only updating areas of interest and reusing noise from previous steps, RAS significantly accelerates the sampling process while maintaining high quality. The results show that RAS can achieve speedups of over 2x with minimal loss in generation quality, making it a promising advancement for real-time applications in generative modeling."
                },
                "zh": {
                    "title": "æå‡æ‰©æ•£æ¨¡å‹çš„å®æ—¶æ€§èƒ½",
                    "desc": "æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨ç”Ÿæˆä»»åŠ¡ä¸­å·²æˆä¸ºé¦–é€‰ï¼Œä½†å…¶ä¾èµ–å¤šä¸ªé¡ºåºå‰å‘ä¼ é€’é™åˆ¶äº†å®æ—¶æ€§èƒ½ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ— è®­ç»ƒé‡‡æ ·ç­–ç•¥RASï¼Œåˆ©ç”¨æ‰©æ•£å˜æ¢å™¨ï¼ˆDiTsï¼‰çš„çµæ´»æ€§ï¼Œæ ¹æ®æ¨¡å‹çš„å…³æ³¨ç‚¹åŠ¨æ€åˆ†é…å›¾åƒåŒºåŸŸçš„é‡‡æ ·æ¯”ä¾‹ã€‚RASåªæ›´æ–°å½“å‰å…³æ³¨çš„åŒºåŸŸï¼Œè€Œå…¶ä»–åŒºåŸŸåˆ™ä½¿ç”¨ä¸Šä¸€æ­¥çš„ç¼“å­˜å™ªå£°ï¼Œä»è€Œæé«˜äº†æ•ˆç‡ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒRASåœ¨ç”Ÿæˆè´¨é‡å‡ ä¹ä¸ä¸‹é™çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿå®ç°æ˜¾è‘—çš„åŠ é€Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.10248",
            "title": "Step-Video-T2V Technical Report: The Practice, Challenges, and Future of Video Foundation Model",
            "url": "https://huggingface.co/papers/2502.10248",
            "abstract": "We present Step-Video-T2V, a state-of-the-art text-to-video pre-trained model with 30B parameters and the ability to generate videos up to 204 frames in length. A deep compression Variational Autoencoder, Video-VAE, is designed for video generation tasks, achieving 16x16 spatial and 8x temporal compression ratios, while maintaining exceptional video reconstruction quality. User prompts are encoded using two bilingual text encoders to handle both English and Chinese. A DiT with 3D full attention is trained using Flow Matching and is employed to denoise input noise into latent frames. A video-based DPO approach, Video-DPO, is applied to reduce artifacts and improve the visual quality of the generated videos. We also detail our training strategies and share key observations and insights. Step-Video-T2V's performance is evaluated on a novel video generation benchmark, Step-Video-T2V-Eval, demonstrating its state-of-the-art text-to-video quality when compared with both open-source and commercial engines. Additionally, we discuss the limitations of current diffusion-based model paradigm and outline future directions for video foundation models. We make both Step-Video-T2V and Step-Video-T2V-Eval available at https://github.com/stepfun-ai/Step-Video-T2V. The online version can be accessed from https://yuewen.cn/videos as well. Our goal is to accelerate the innovation of video foundation models and empower video content creators.",
            "score": 29,
            "issue_id": 2241,
            "pub_date": "2025-02-14",
            "pub_date_card": {
                "ru": "14 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 14",
                "zh": "2æœˆ14æ—¥"
            },
            "hash": "356bc046cc5f59e5",
            "authors": [
                "Guoqing Ma",
                "Haoyang Huang",
                "Kun Yan",
                "Liangyu Chen",
                "Nan Duan",
                "Shengming Yin",
                "Changyi Wan",
                "Ranchen Ming",
                "Xiaoniu Song",
                "Xing Chen",
                "Yu Zhou",
                "Deshan Sun",
                "Deyu Zhou",
                "Jian Zhou",
                "Kaijun Tan",
                "Kang An",
                "Mei Chen",
                "Wei Ji",
                "Qiling Wu",
                "Wen Sun",
                "Xin Han",
                "Yanan Wei",
                "Zheng Ge",
                "Aojie Li",
                "Bin Wang",
                "Bizhu Huang",
                "Bo Wang",
                "Brian Li",
                "Changxing Miao",
                "Chen Xu",
                "Chenfei Wu",
                "Chenguang Yu",
                "Dapeng Shi",
                "Dingyuan Hu",
                "Enle Liu",
                "Gang Yu",
                "Ge Yang",
                "Guanzhe Huang",
                "Gulin Yan",
                "Haiyang Feng",
                "Hao Nie",
                "Haonan Jia",
                "Hanpeng Hu",
                "Hanqi Chen",
                "Haolong Yan",
                "Heng Wang",
                "Hongcheng Guo",
                "Huilin Xiong",
                "Huixin Xiong",
                "Jiahao Gong",
                "Jianchang Wu",
                "Jiaoren Wu",
                "Jie Wu",
                "Jie Yang",
                "Jiashuai Liu",
                "Jiashuo Li",
                "Jingyang Zhang",
                "Junjing Guo",
                "Junzhe Lin",
                "Kaixiang Li",
                "Lei Liu",
                "Lei Xia",
                "Liang Zhao",
                "Liguo Tan",
                "Liwen Huang",
                "Liying Shi",
                "Ming Li",
                "Mingliang Li",
                "Muhua Cheng",
                "Na Wang",
                "Qiaohui Chen",
                "Qinglin He",
                "Qiuyan Liang",
                "Quan Sun",
                "Ran Sun",
                "Rui Wang",
                "Shaoliang Pang",
                "Shiliang Yang",
                "Sitong Liu",
                "Siqi Liu",
                "Shuli Gao",
                "Tiancheng Cao",
                "Tianyu Wang",
                "Weipeng Ming",
                "Wenqing He",
                "Xu Zhao",
                "Xuelin Zhang",
                "Xianfang Zeng",
                "Xiaojia Liu",
                "Xuan Yang",
                "Yaqi Dai",
                "Yanbo Yu",
                "Yang Li",
                "Yineng Deng",
                "Yingming Wang",
                "Yilei Wang",
                "Yuanwei Lu",
                "Yu Chen",
                "Yu Luo",
                "Yuchu Luo",
                "Yuhe Yin",
                "Yuheng Feng",
                "Yuxiang Yang",
                "Zecheng Tang",
                "Zekai Zhang",
                "Zidong Yang",
                "Binxing Jiao",
                "Jiansheng Chen",
                "Jing Li",
                "Shuchang Zhou",
                "Xiangyu Zhang",
                "Xinhao Zhang",
                "Yibo Zhu",
                "Heung-Yeung Shum",
                "Daxin Jiang"
            ],
            "affiliations": [
                "StepFun"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.10248.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#benchmark",
                    "#training",
                    "#open_source",
                    "#diffusion",
                    "#video",
                    "#architecture"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾: Ğ¾Ñ‚ Ñ‚ĞµĞºÑÑ‚Ğ° Ğº Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ñ€Ğ¾Ğ»Ğ¸ĞºĞ°Ğ¼",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Step-Video-T2V - Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ñ 30 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğ¹ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Video-VAE Ğ´Ğ»Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ´Ğ²Ğ° Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ½Ğ° Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼ Ğ¸ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞ°Ñ…. Ğ”Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Video-DPO Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¸ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Video Generation with Step-Video-T2V",
                    "desc": "Step-Video-T2V is a cutting-edge text-to-video model that utilizes 30 billion parameters to generate videos with a maximum length of 204 frames. It employs a deep compression Variational Autoencoder, achieving significant spatial and temporal compression while preserving high video quality. The model incorporates bilingual text encoders for processing prompts in both English and Chinese, and utilizes a DiT with 3D full attention for effective denoising of latent frames. Evaluated on a new benchmark, Step-Video-T2V demonstrates superior performance in video generation, addressing current limitations in diffusion-based models and paving the way for future advancements in video foundation models."
                },
                "zh": {
                    "title": "åˆ›æ–°è§†é¢‘ç”Ÿæˆï¼Œèµ‹èƒ½å†…å®¹åˆ›ä½œè€…",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºStep-Video-T2Vçš„å…ˆè¿›æ–‡æœ¬åˆ°è§†é¢‘é¢„è®­ç»ƒæ¨¡å‹ï¼Œå…·æœ‰300äº¿å‚æ•°ï¼Œèƒ½å¤Ÿç”Ÿæˆæœ€é•¿204å¸§çš„è§†é¢‘ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ·±åº¦å‹ç¼©å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVideo-VAEï¼‰ï¼Œåœ¨è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­å®ç°äº†16x16çš„ç©ºé—´å‹ç¼©å’Œ8xçš„æ—¶é—´å‹ç¼©ï¼ŒåŒæ—¶ä¿æŒäº†å“è¶Šçš„è§†é¢‘é‡å»ºè´¨é‡ã€‚ç”¨æˆ·æç¤ºé€šè¿‡åŒè¯­æ–‡æœ¬ç¼–ç å™¨è¿›è¡Œç¼–ç ï¼Œä»¥å¤„ç†è‹±è¯­å’Œä¸­æ–‡ã€‚æˆ‘ä»¬è¿˜è®¨è®ºäº†å½“å‰æ‰©æ•£æ¨¡å‹èŒƒå¼çš„å±€é™æ€§ï¼Œå¹¶æ¦‚è¿°äº†è§†é¢‘åŸºç¡€æ¨¡å‹çš„æœªæ¥æ–¹å‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09992",
            "title": "Large Language Diffusion Models",
            "url": "https://huggingface.co/papers/2502.09992",
            "abstract": "Autoregressive models (ARMs) are widely regarded as the cornerstone of large language models (LLMs). We challenge this notion by introducing LLaDA, a diffusion model trained from scratch under the pre-training and supervised fine-tuning (SFT) paradigm. LLaDA models distributions through a forward data masking process and a reverse process, parameterized by a vanilla Transformer to predict masked tokens. By optimizing a likelihood bound, it provides a principled generative approach for probabilistic inference. Across extensive benchmarks, LLaDA demonstrates strong scalability, outperforming our self-constructed ARM baselines. Remarkably, LLaDA 8B is competitive with strong LLMs like LLaMA3 8B in in-context learning and, after SFT, exhibits impressive instruction-following abilities in case studies such as multi-turn dialogue. Moreover, LLaDA addresses the reversal curse, surpassing GPT-4o in a reversal poem completion task. Our findings establish diffusion models as a viable and promising alternative to ARMs, challenging the assumption that key LLM capabilities discussed above are inherently tied to ARMs.",
            "score": 27,
            "issue_id": 2242,
            "pub_date": "2025-02-14",
            "pub_date_card": {
                "ru": "14 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 14",
                "zh": "2æœˆ14æ—¥"
            },
            "hash": "5117e8f17ba51f92",
            "authors": [
                "Shen Nie",
                "Fengqi Zhu",
                "Zebin You",
                "Xiaolu Zhang",
                "Jingyang Ou",
                "Jun Hu",
                "Jun Zhou",
                "Yankai Lin",
                "Ji-Rong Wen",
                "Chongxuan Li"
            ],
            "affiliations": [
                "Ant Group",
                "Beijing Key Laboratory of Big Data Management and Analysis Methods",
                "Gaoling School of Artificial Intelligence, Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09992.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#benchmark",
                    "#diffusion",
                    "#training"
                ],
                "emoji": "ğŸŒŠ",
                "ru": {
                    "title": "Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ñ€Ğ¾ÑĞ°ÑÑ‚ Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¸ Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ ÑĞ·Ñ‹ĞºĞ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ LLaDA - Ğ½Ğ¾Ğ²ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°, Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼. LLaDA Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñƒ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…. LLaDA Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¸ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ 'Ğ¿Ñ€Ğ¾ĞºĞ»ÑÑ‚Ğ¸Ñ Ğ¾Ğ±Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ'."
                },
                "en": {
                    "title": "LLaDA: A New Era for Language Models Beyond Autoregression",
                    "desc": "This paper introduces LLaDA, a novel diffusion model that challenges the dominance of autoregressive models (ARMs) in large language models (LLMs). LLaDA employs a forward data masking process and a reverse process, utilizing a Transformer architecture to predict masked tokens effectively. By optimizing a likelihood bound, it offers a robust generative framework for probabilistic inference. The results show that LLaDA not only scales well but also competes with leading LLMs in tasks like in-context learning and instruction-following, suggesting that diffusion models can serve as a strong alternative to traditional ARMs."
                },
                "zh": {
                    "title": "æ‰©æ•£æ¨¡å‹ï¼šè‡ªå›å½’æ¨¡å‹çš„æ–°æŒ‘æˆ˜",
                    "desc": "è‡ªå›å½’æ¨¡å‹ï¼ˆARMsï¼‰è¢«å¹¿æ³›è®¤ä¸ºæ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„åŸºçŸ³ã€‚æœ¬æ–‡æå‡ºäº†LLaDAï¼Œè¿™æ˜¯ä¸€ç§ä»å¤´å¼€å§‹è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼Œé‡‡ç”¨é¢„è®­ç»ƒå’Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„æ–¹æ³•ã€‚LLaDAé€šè¿‡å‰å‘æ•°æ®æ©è”½è¿‡ç¨‹å’Œåå‘è¿‡ç¨‹å»ºæ¨¡åˆ†å¸ƒï¼Œå¹¶ä½¿ç”¨æ™®é€šTransformeré¢„æµ‹è¢«æ©è”½çš„æ ‡è®°ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒLLaDAåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„å¯æ‰©å±•æ€§ï¼Œè¶…è¶Šäº†è‡ªæ„å»ºçš„ARMsåŸºçº¿ï¼Œè¯æ˜äº†æ‰©æ•£æ¨¡å‹ä½œä¸ºARMsçš„å¯è¡Œæ›¿ä»£æ–¹æ¡ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09696",
            "title": "ZeroBench: An Impossible Visual Benchmark for Contemporary Large Multimodal Models",
            "url": "https://huggingface.co/papers/2502.09696",
            "abstract": "Large Multimodal Models (LMMs) exhibit major shortfalls when interpreting images and, by some measures, have poorer spatial cognition than small children or animals. Despite this, they attain high scores on many popular visual benchmarks, with headroom rapidly eroded by an ongoing surge of model progress. To address this, there is a pressing need for difficult benchmarks that remain relevant for longer. We take this idea to its limit by introducing ZeroBench-a lightweight visual reasoning benchmark that is entirely impossible for contemporary frontier LMMs. Our benchmark consists of 100 manually curated questions and 334 less difficult subquestions. We evaluate 20 LMMs on ZeroBench, all of which score 0.0%, and rigorously analyse the errors. To encourage progress in visual understanding, we publicly release ZeroBench.",
            "score": 21,
            "issue_id": 2241,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 13",
                "zh": "2æœˆ13æ—¥"
            },
            "hash": "00f4f8e85ea27717",
            "authors": [
                "Jonathan Roberts",
                "Mohammad Reza Taesiri",
                "Ansh Sharma",
                "Akash Gupta",
                "Samuel Roberts",
                "Ioana Croitoru",
                "Simion-Vlad Bogolin",
                "Jialu Tang",
                "Florian Langer",
                "Vyas Raina",
                "Vatsal Raina",
                "Hanyi Xiong",
                "Vishaal Udandarao",
                "Jingyi Lu",
                "Shiyang Chen",
                "Sam Purkis",
                "Tianshuo Yan",
                "Wenye Lin",
                "Gyungin Shin",
                "Qiaochu Yang",
                "Anh Totti Nguyen",
                "Kai Han",
                "Samuel Albanie"
            ],
            "affiliations": [
                "Auburn University",
                "Independent Researcher",
                "The University of Hong Kong",
                "University of Alberta",
                "University of Cambridge",
                "University of Oxford",
                "University of Tubingen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09696.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#benchmark",
                    "#interpretability",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ZeroBench: Ğ½ĞµĞ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ZeroBench, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ½ĞµĞ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ñ€ĞµÑˆĞ¸Ñ‚ÑŒ Ğ´Ğ»Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LMM). Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· 100 Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ 334 Ğ¼ĞµĞ½ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ»Ğ¸ 20 LMM Ğ½Ğ° ZeroBench, Ğ¸ Ğ²ÑĞµ Ğ¾Ğ½Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ 0%. Ğ¦ĞµĞ»ÑŒÑ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ° ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµÑÑ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾ÑÑ‚Ğ°Ğ½ĞµÑ‚ÑÑ Ğ°ĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ´Ğ¾Ğ»ÑŒÑˆĞµ, Ñ‡ĞµĞ¼ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸."
                },
                "en": {
                    "title": "ZeroBench: Raising the Bar for Visual Reasoning in LMMs",
                    "desc": "This paper discusses the limitations of Large Multimodal Models (LMMs) in interpreting images, highlighting that they perform worse in spatial reasoning compared to small children and animals. Despite achieving high scores on existing visual benchmarks, these models struggle with more complex visual reasoning tasks. To tackle this issue, the authors introduce ZeroBench, a new benchmark designed to be extremely challenging for current LMMs, consisting of 100 curated questions and 334 easier subquestions. The evaluation of 20 LMMs on ZeroBench resulted in a score of 0.0%, demonstrating the need for more difficult benchmarks to drive advancements in visual understanding."
                },
                "zh": {
                    "title": "ZeroBenchï¼šæ¨åŠ¨è§†è§‰ç†è§£çš„æ–°åŸºå‡†",
                    "desc": "å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨å›¾åƒç†è§£æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œç”šè‡³åœ¨æŸäº›æ–¹é¢çš„ç©ºé—´è®¤çŸ¥èƒ½åŠ›ä¸å¦‚å°å­©æˆ–åŠ¨ç‰©ã€‚å°½ç®¡å¦‚æ­¤ï¼Œå®ƒä»¬åœ¨è®¸å¤šæµè¡Œçš„è§†è§‰åŸºå‡†æµ‹è¯•ä¸­å¾—åˆ†å¾ˆé«˜ï¼Œä½†éšç€æ¨¡å‹è¿›æ­¥çš„åŠ é€Ÿï¼Œè¿™äº›åŸºå‡†çš„æŒ‘æˆ˜æ€§è¿…é€Ÿé™ä½ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ZeroBenchï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§çš„è§†è§‰æ¨ç†åŸºå‡†ï¼Œå½“å‰çš„å‰æ²¿LMMså®Œå…¨æ— æ³•è§£å†³ã€‚ZeroBenchåŒ…å«100ä¸ªæ‰‹åŠ¨ç­–åˆ’çš„é—®é¢˜å’Œ334ä¸ªè¾ƒç®€å•çš„å­é—®é¢˜ï¼Œæˆ‘ä»¬å¯¹20ä¸ªLMMsè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœå‡ä¸º0.0%ï¼Œå¹¶å¯¹é”™è¯¯è¿›è¡Œäº†ä¸¥æ ¼åˆ†æã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.10391",
            "title": "MM-RLHF: The Next Step Forward in Multimodal LLM Alignment",
            "url": "https://huggingface.co/papers/2502.10391",
            "abstract": "Despite notable advancements in Multimodal Large Language Models (MLLMs), most state-of-the-art models have not undergone thorough alignment with human preferences. This gap exists because current alignment research has primarily achieved progress in specific areas (e.g., hallucination reduction), while the broader question of whether aligning models with human preferences can systematically enhance MLLM capability remains largely unexplored. To this end, we introduce MM-RLHF, a dataset containing 120k fine-grained, human-annotated preference comparison pairs. This dataset represents a substantial advancement over existing resources, offering superior size, diversity, annotation granularity, and quality. Leveraging this dataset, we propose several key innovations to improve both the quality of reward models and the efficiency of alignment algorithms. Notably, we introduce a Critique-Based Reward Model, which generates critiques of model outputs before assigning scores, offering enhanced interpretability and more informative feedback compared to traditional scalar reward mechanisms. Additionally, we propose Dynamic Reward Scaling, a method that adjusts the loss weight of each sample according to the reward signal, thereby optimizing the use of high-quality comparison pairs. Our approach is rigorously evaluated across 10 distinct dimensions and 27 benchmarks, with results demonstrating significant and consistent improvements in model performance. Specifically, fine-tuning LLaVA-ov-7B with MM-RLHF and our alignment algorithm leads to a 19.5% increase in conversational abilities and a 60% improvement in safety.   We have open-sourced the preference dataset, reward model, training and evaluation code, as well as reward modeling and safety benchmarks. For more details, please visit our project page: https://mm-rlhf.github.io.",
            "score": 17,
            "issue_id": 2241,
            "pub_date": "2025-02-14",
            "pub_date_card": {
                "ru": "14 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 14",
                "zh": "2æœˆ14æ—¥"
            },
            "hash": "c47a89fda79a1a4b",
            "authors": [
                "Yi-Fan Zhang",
                "Tao Yu",
                "Haochen Tian",
                "Chaoyou Fu",
                "Peiyan Li",
                "Jianshu Zeng",
                "Wulin Xie",
                "Yang Shi",
                "Huanyu Zhang",
                "Junkang Wu",
                "Xue Wang",
                "Yibo Hu",
                "Bin Wen",
                "Fan Yang",
                "Zhang Zhang",
                "Tingting Gao",
                "Di Zhang",
                "Liang Wang",
                "Rong Jin",
                "Tieniu Tan"
            ],
            "affiliations": [
                "Alibaba",
                "CASIA",
                "KuaiShou",
                "Meta AI",
                "NJU",
                "PKU",
                "USTC"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.10391.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#alignment",
                    "#training",
                    "#interpretability",
                    "#open_source",
                    "#rlhf",
                    "#dataset"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ MM-RLHF - Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 120 Ñ‚Ñ‹ÑÑÑ‡ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ° ĞµĞ³Ğ¾ Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ LLaVA-ov-7B Ğ² Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°Ñ… Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ÑĞ»Ğµ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ»Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ Ğº Ğ½Ğ°Ğ±Ğ¾Ñ€Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¸ ĞºĞ¾Ğ´Ñƒ Ğ´Ğ»Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing MLLM Alignment with Human Preferences",
                    "desc": "This paper addresses the need for better alignment of Multimodal Large Language Models (MLLMs) with human preferences. It introduces MM-RLHF, a new dataset with 120,000 human-annotated preference comparisons, which enhances the quality and diversity of training data for alignment tasks. The authors propose innovative techniques like a Critique-Based Reward Model that provides detailed feedback on model outputs and Dynamic Reward Scaling to optimize the training process. Their methods show significant improvements in model performance, including a 19.5% boost in conversational skills and a 60% increase in safety metrics."
                },
                "zh": {
                    "title": "æå‡å¤šæ¨¡æ€æ¨¡å‹ä¸äººç±»åå¥½çš„å¯¹é½",
                    "desc": "å°½ç®¡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å¤§å¤šæ•°æœ€å…ˆè¿›çš„æ¨¡å‹å°šæœªä¸äººç±»åå¥½è¿›è¡Œå……åˆ†å¯¹é½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†MM-RLHFæ•°æ®é›†ï¼ŒåŒ…å«12ä¸‡ä¸ªç»†ç²’åº¦çš„äººç±»æ ‡æ³¨åå¥½æ¯”è¾ƒå¯¹ï¼Œæ˜¾è‘—æå‡äº†ç°æœ‰èµ„æºçš„è§„æ¨¡å’Œè´¨é‡ã€‚æˆ‘ä»¬æå‡ºäº†åŸºäºæ‰¹è¯„çš„å¥–åŠ±æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨è¯„åˆ†å‰ç”Ÿæˆæ¨¡å‹è¾“å‡ºçš„æ‰¹è¯„ï¼Œä»è€Œæä¾›æ›´å…·å¯è§£é‡Šæ€§å’Œä¿¡æ¯é‡çš„åé¦ˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†åŠ¨æ€å¥–åŠ±ç¼©æ”¾æ–¹æ³•ï¼Œæ ¹æ®å¥–åŠ±ä¿¡å·è°ƒæ•´æ¯ä¸ªæ ·æœ¬çš„æŸå¤±æƒé‡ï¼Œä»¥ä¼˜åŒ–é«˜è´¨é‡æ¯”è¾ƒå¯¹çš„ä½¿ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09935",
            "title": "Precise Parameter Localization for Textual Generation in Diffusion Models",
            "url": "https://huggingface.co/papers/2502.09935",
            "abstract": "Novel diffusion models can synthesize photo-realistic images with integrated high-quality text. Surprisingly, we demonstrate through attention activation patching that only less than 1% of diffusion models' parameters, all contained in attention layers, influence the generation of textual content within the images. Building on this observation, we improve textual generation efficiency and performance by targeting cross and joint attention layers of diffusion models. We introduce several applications that benefit from localizing the layers responsible for textual content generation. We first show that a LoRA-based fine-tuning solely of the localized layers enhances, even more, the general text-generation capabilities of large diffusion models while preserving the quality and diversity of the diffusion models' generations. Then, we demonstrate how we can use the localized layers to edit textual content in generated images. Finally, we extend this idea to the practical use case of preventing the generation of toxic text in a cost-free manner. In contrast to prior work, our localization approach is broadly applicable across various diffusion model architectures, including U-Net (e.g., LDM and SDXL) and transformer-based (e.g., DeepFloyd IF and Stable Diffusion 3), utilizing diverse text encoders (e.g., from CLIP to the large language models like T5). Project page available at https://t2i-text-loc.github.io/.",
            "score": 9,
            "issue_id": 2245,
            "pub_date": "2025-02-14",
            "pub_date_card": {
                "ru": "14 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 14",
                "zh": "2æœˆ14æ—¥"
            },
            "hash": "1c3ce78b0c6424d2",
            "authors": [
                "Åukasz Staniszewski",
                "Bartosz CywiÅ„ski",
                "Franziska Boenisch",
                "Kamil Deja",
                "Adam Dziedzic"
            ],
            "affiliations": [
                "CISPA Helmholtz Center for Information Security",
                "IDEAS NCBR",
                "Warsaw University of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09935.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#diffusion",
                    "#training",
                    "#synthetic",
                    "#architecture"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ›Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµĞ½ĞµĞµ 1% Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ€Ğ°ÑĞ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ² ÑĞ»Ğ¾ÑÑ… Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°ÑÑ‚ Ğ·Ğ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ğ¾Ğ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ‚Ğ¾Ñ‡ĞµÑ‡Ğ½ÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ»Ğ¾ĞµĞ² Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ğ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ°Ğ¼."
                },
                "en": {
                    "title": "Targeting Attention for Enhanced Text Generation in Diffusion Models",
                    "desc": "This paper explores how diffusion models can create realistic images that include high-quality text. It reveals that less than 1% of the model's parameters, specifically in the attention layers, are crucial for generating text within these images. By focusing on these specific layers, the authors enhance the efficiency and performance of text generation in diffusion models. They also present applications such as improving text generation, editing text in images, and preventing toxic text generation, demonstrating the broad applicability of their approach across different model architectures."
                },
                "zh": {
                    "title": "å±€éƒ¨åŒ–æ³¨æ„åŠ›å±‚æå‡æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°é¢–çš„æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿåˆæˆé«˜è´¨é‡çš„ç…§ç‰‡çº§å›¾åƒï¼Œå¹¶é›†æˆæ–‡æœ¬ç”Ÿæˆã€‚ç ”ç©¶å‘ç°ï¼Œæ‰©æ•£æ¨¡å‹ä¸­åªæœ‰ä¸åˆ°1%çš„å‚æ•°ï¼Œä¸»è¦é›†ä¸­åœ¨æ³¨æ„åŠ›å±‚ï¼Œå½±å“å›¾åƒä¸­çš„æ–‡æœ¬å†…å®¹ç”Ÿæˆã€‚åŸºäºè¿™ä¸€è§‚å¯Ÿï¼Œä½œè€…é€šè¿‡é’ˆå¯¹äº¤å‰å’Œè”åˆæ³¨æ„åŠ›å±‚ï¼Œæå‡äº†æ–‡æœ¬ç”Ÿæˆçš„æ•ˆç‡å’Œæ€§èƒ½ã€‚è®ºæ–‡è¿˜å±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨è¿™äº›å±€éƒ¨åŒ–çš„å±‚æ¥ç¼–è¾‘ç”Ÿæˆå›¾åƒä¸­çš„æ–‡æœ¬å†…å®¹ï¼Œå¹¶é˜²æ­¢ç”Ÿæˆæœ‰å®³æ–‡æœ¬ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09955",
            "title": "Diverse Inference and Verification for Advanced Reasoning",
            "url": "https://huggingface.co/papers/2502.09955",
            "abstract": "Reasoning LLMs such as OpenAI o1, o3 and DeepSeek R1 have made significant progress in mathematics and coding, yet find challenging advanced tasks such as International Mathematical Olympiad (IMO) combinatorics problems, Abstraction and Reasoning Corpus (ARC) puzzles, and Humanity's Last Exam (HLE) questions. We use a diverse inference approach that combines multiple models and methods at test time. We find that verifying mathematics and code problems, and rejection sampling on other problems is simple and effective. We automatically verify correctness of solutions to IMO problems by Lean, and ARC puzzles by code, and find that best-of-N effectively answers HLE questions. Our approach increases answer accuracy on IMO combinatorics problems from 33.3% to 77.8%, accuracy on HLE questions from 8% to 37%, and solves 80% of ARC puzzles that 948 humans could not and 26.5% of ARC puzzles that o3 high compute does not. Test-time simulations, reinforcement learning, and meta-learning with inference feedback improve generalization by adapting agent graph representations and varying prompts, code, and datasets. Our approach is reliable, robust, and scalable, and in the spirit of reproducible research, we will make it publicly available upon publication.",
            "score": 7,
            "issue_id": 2242,
            "pub_date": "2025-02-14",
            "pub_date_card": {
                "ru": "14 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 14",
                "zh": "2æœˆ14æ—¥"
            },
            "hash": "10eaccfc7377f600",
            "authors": [
                "Iddo Drori",
                "Gaston Longhitano",
                "Mao Mao",
                "Seunghwan Hyun",
                "Yuke Zhang",
                "Sungjun Park",
                "Zachary Meeks",
                "Xin-Yu Zhang",
                "Ben Segev",
                "Howard Yong",
                "Nakul Verma",
                "Avi Shporer",
                "Alon Amit",
                "Madeleine Udell"
            ],
            "affiliations": [
                "Boston University",
                "Columbia University",
                "Google",
                "Intuit",
                "Massachusetts Institute of Technology",
                "NotBadMath.AI",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09955.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#reasoning",
                    "#optimization",
                    "#agents",
                    "#rl",
                    "#training",
                    "#inference"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ LLM Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡: Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ñ‚Ğ±Ğ¾Ñ€ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞœĞµĞ¶Ğ´ÑƒĞ½Ğ°Ñ€Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ğ»Ğ¸Ğ¼Ğ¿Ğ¸Ğ°Ğ´Ñ‹, Humanity's Last Exam Ğ¸ Abstraction and Reasoning Corpus. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¼ĞµÑ‚Ğ°-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Boosting LLMs: From 33% to 77% Accuracy in Complex Problem Solving!",
                    "desc": "This paper discusses advancements in reasoning large language models (LLMs) like OpenAI's models and DeepSeek in tackling complex mathematical and coding challenges. The authors propose a diverse inference strategy that integrates various models and methods during testing to enhance performance on difficult tasks such as IMO problems and ARC puzzles. They demonstrate that their method significantly boosts accuracy, achieving a 77.8% success rate on IMO combinatorics and solving 80% of previously unsolved ARC puzzles. Additionally, they employ techniques like reinforcement learning and meta-learning to improve the adaptability and generalization of their models, ensuring their approach is both reliable and scalable."
                },
                "zh": {
                    "title": "æå‡æ¨ç†æ¨¡å‹åœ¨é«˜çº§æ•°å­¦é—®é¢˜ä¸Šçš„å‡†ç¡®æ€§",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†æ¨ç†å‹å¤§è¯­è¨€æ¨¡å‹åœ¨è§£å†³é«˜çº§æ•°å­¦å’Œç¼–ç¨‹ä»»åŠ¡ä¸­çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å›½é™…æ•°å­¦å¥¥æ—åŒ¹å…‹ï¼ˆIMOï¼‰ç»„åˆé—®é¢˜ã€æŠ½è±¡ä¸æ¨ç†è¯­æ–™åº“ï¼ˆARCï¼‰éš¾é¢˜å’Œäººç±»æœ€åè€ƒè¯•ï¼ˆHLEï¼‰é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šæ¨¡å‹å’Œå¤šæ–¹æ³•ç»“åˆçš„æ¨ç†æ–¹æ³•ï¼Œåœ¨æµ‹è¯•æ—¶è¿›è¡Œå¤šæ ·åŒ–æ¨ç†ã€‚é€šè¿‡è‡ªåŠ¨éªŒè¯IMOé—®é¢˜å’ŒARCéš¾é¢˜çš„è§£ç­”æ­£ç¡®æ€§ï¼Œæˆ‘ä»¬æ˜¾è‘—æé«˜äº†è¿™äº›é—®é¢˜çš„è§£ç­”å‡†ç¡®ç‡ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ–¹æ³•å¯é ã€ç¨³å¥ä¸”å¯æ‰©å±•ï¼Œæ—¨åœ¨æ¨åŠ¨å¯é‡å¤ç ”ç©¶çš„å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.10235",
            "title": "AdaPTS: Adapting Univariate Foundation Models to Probabilistic Multivariate Time Series Forecasting",
            "url": "https://huggingface.co/papers/2502.10235",
            "abstract": "Pre-trained foundation models (FMs) have shown exceptional performance in univariate time series forecasting tasks. However, several practical challenges persist, including managing intricate dependencies among features and quantifying uncertainty in predictions. This study aims to tackle these critical limitations by introducing adapters; feature-space transformations that facilitate the effective use of pre-trained univariate time series FMs for multivariate tasks. Adapters operate by projecting multivariate inputs into a suitable latent space and applying the FM independently to each dimension. Inspired by the literature on representation learning and partially stochastic Bayesian neural networks, we present a range of adapters and optimization/inference strategies. Experiments conducted on both synthetic and real-world datasets confirm the efficacy of adapters, demonstrating substantial enhancements in forecasting accuracy and uncertainty quantification compared to baseline methods. Our framework, AdaPTS, positions adapters as a modular, scalable, and effective solution for leveraging time series FMs in multivariate contexts, thereby promoting their wider adoption in real-world applications. We release the code at https://github.com/abenechehab/AdaPTS.",
            "score": 6,
            "issue_id": 2248,
            "pub_date": "2025-02-14",
            "pub_date_card": {
                "ru": "14 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 14",
                "zh": "2æœˆ14æ—¥"
            },
            "hash": "dbd38216ecfcb531",
            "authors": [
                "Abdelhakim Benechehab",
                "Vasilii Feofanov",
                "Giuseppe Paolo",
                "Albert Thomas",
                "Maurizio Filippone",
                "BalÃ¡zs KÃ©gl"
            ],
            "affiliations": [
                "Department of Data Science, EURECOM",
                "Huawei Noahs Ark Lab, Paris, France",
                "Statistics Program, KAUST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.10235.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#synthetic",
                    "#dataset",
                    "#inference",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "AdaPTS: ĞĞ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ¾Ğ²",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ AdaPTS - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ñ‹ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹-Ğ¾ÑĞ½Ğ¾Ğ² (foundation models) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ¾Ğ². ĞĞ´Ğ°Ğ¿Ñ‚ĞµÑ€Ñ‹ Ğ¿Ñ€Ğ¾ĞµÑ†Ğ¸Ñ€ÑƒÑÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ´Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº AdaPTS Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ¾Ğµ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ¾Ğ² Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "Enhancing Multivariate Time Series Forecasting with Adapters",
                    "desc": "This paper introduces a new approach called adapters to improve the use of pre-trained foundation models (FMs) for multivariate time series forecasting. Adapters transform multivariate inputs into a latent space, allowing the FM to process each feature independently while managing complex dependencies. The study also explores various optimization and inference strategies inspired by representation learning and Bayesian neural networks. Experiments show that this method significantly enhances forecasting accuracy and uncertainty quantification, making it a valuable tool for real-world applications."
                },
                "zh": {
                    "title": "é€‚é…å™¨ï¼šå¤šå˜é‡æ—¶é—´åºåˆ—é¢„æµ‹çš„æ–°è§£å†³æ–¹æ¡ˆ",
                    "desc": "é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹åœ¨å•å˜é‡æ—¶é—´åºåˆ—é¢„æµ‹ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤„ç†ç‰¹å¾é—´å¤æ‚ä¾èµ–å…³ç³»å’Œé‡åŒ–é¢„æµ‹ä¸ç¡®å®šæ€§æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶é€šè¿‡å¼•å…¥é€‚é…å™¨æ¥è§£å†³è¿™äº›å…³é”®é™åˆ¶ï¼Œé€‚é…å™¨æ˜¯ä¸€ç§ç‰¹å¾ç©ºé—´è½¬æ¢ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨é¢„è®­ç»ƒçš„å•å˜é‡æ—¶é—´åºåˆ—æ¨¡å‹è¿›è¡Œå¤šå˜é‡ä»»åŠ¡ã€‚é€‚é…å™¨é€šè¿‡å°†å¤šå˜é‡è¾“å…¥æŠ•å½±åˆ°åˆé€‚çš„æ½œåœ¨ç©ºé—´ï¼Œå¹¶ç‹¬ç«‹åœ°å¯¹æ¯ä¸ªç»´åº¦åº”ç”¨åŸºç¡€æ¨¡å‹ï¼Œä»è€Œå®ç°åŠŸèƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé€‚é…å™¨åœ¨é¢„æµ‹å‡†ç¡®æ€§å’Œä¸ç¡®å®šæ€§é‡åŒ–æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤šå˜é‡æ—¶é—´åºåˆ—åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.07780",
            "title": "DarwinLM: Evolutionary Structured Pruning of Large Language Models",
            "url": "https://huggingface.co/papers/2502.07780",
            "abstract": "Large Language Models (LLMs) have achieved significant success across various NLP tasks. However, their massive computational costs limit their widespread use, particularly in real-time applications. Structured pruning offers an effective solution by compressing models and directly providing end-to-end speed improvements, regardless of the hardware environment. Meanwhile, different components of the model exhibit varying sensitivities towards pruning, calling for non-uniform model compression. However, a pruning method should not only identify a capable substructure, but also account for post-compression training. To this end, we propose \\sysname, a method for training-aware structured pruning. \\sysname builds upon an evolutionary search process, generating multiple offspring models in each generation through mutation, and selecting the fittest for survival. To assess the effect of post-training, we incorporate a lightweight, multistep training process within the offspring population, progressively increasing the number of tokens and eliminating poorly performing models in each selection stage. We validate our method through extensive experiments on Llama-2-7B, Llama-3.1-8B and Qwen-2.5-14B-Instruct, achieving state-of-the-art performance for structured pruning. For instance, \\sysname surpasses ShearedLlama while requiring 5times less training data during post-compression training.",
            "score": 5,
            "issue_id": 2251,
            "pub_date": "2025-02-11",
            "pub_date_card": {
                "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 11",
                "zh": "2æœˆ11æ—¥"
            },
            "hash": "b53421574afe0c8a",
            "authors": [
                "Shengkun Tang",
                "Oliver Sieberling",
                "Eldar Kurtic",
                "Zhiqiang Shen",
                "Dan Alistarh"
            ],
            "affiliations": [
                "Department of Machine Learning, MBZUAI, Abu Dhabi, UAE",
                "ETH Zurich, Zurich, Switzerland",
                "ISTA, Vienna, Austria",
                "Red Hat AI, Boston, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.07780.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#small_models",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ§¬",
                "ru": {
                    "title": "Ğ­Ğ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ¸ Ğ´Ğ»Ñ Ğ¸Ñ… Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ \u001710\u00187 Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ğ¸ Ğ¾Ñ‚Ğ±Ğ¸Ñ€Ğ°Ñ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ’Ğ°Ğ¶Ğ½Ğ¾Ğ¹ Ñ‡Ğ°ÑÑ‚ÑŒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ \u001710\u00187 Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ñ‚Ñ€ĞµĞ±ÑƒÑ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Efficient Pruning for Powerful Language Models",
                    "desc": "This paper presents a new method called \textit{sysname} for structured pruning of large language models (LLMs) to improve their efficiency in natural language processing tasks. The method uses an evolutionary search process to create multiple model variations, selecting the best-performing ones while incorporating a lightweight training process to enhance post-compression performance. By focusing on the varying sensitivities of different model components, \textit{sysname} achieves effective model compression without sacrificing accuracy. The results show that \textit{sysname} outperforms existing methods, requiring significantly less training data while maintaining high performance on various LLMs."
                },
                "zh": {
                    "title": "è®­ç»ƒæ„ŸçŸ¥çš„ç»“æ„åŒ–å‰ªææ–¹æ³•",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†å…¶å·¨å¤§çš„è®¡ç®—æˆæœ¬é™åˆ¶äº†å…¶å¹¿æ³›åº”ç”¨ï¼Œå°¤å…¶æ˜¯åœ¨å®æ—¶åº”ç”¨ä¸­ã€‚ç»“æ„åŒ–å‰ªææ˜¯ä¸€ç§æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡å‹ç¼©æ¨¡å‹å¹¶ç›´æ¥æä¾›ç«¯åˆ°ç«¯çš„é€Ÿåº¦æå‡ã€‚ä¸åŒæ¨¡å‹ç»„ä»¶å¯¹å‰ªæçš„æ•æ„Ÿæ€§ä¸åŒï¼Œå› æ­¤éœ€è¦éå‡åŒ€çš„æ¨¡å‹å‹ç¼©æ–¹æ³•ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸º\textit{sysname}çš„è®­ç»ƒæ„ŸçŸ¥ç»“æ„åŒ–å‰ªææ–¹æ³•ï¼Œé€šè¿‡è¿›åŒ–æœç´¢è¿‡ç¨‹ç”Ÿæˆå¤šä¸ªåä»£æ¨¡å‹ï¼Œå¹¶åœ¨æ¯ä¸€ä»£ä¸­é€‰æ‹©æœ€é€‚åˆçš„æ¨¡å‹è¿›è¡Œç”Ÿå­˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09411",
            "title": "ImageRAG: Dynamic Image Retrieval for Reference-Guided Image Generation",
            "url": "https://huggingface.co/papers/2502.09411",
            "abstract": "Diffusion models enable high-quality and diverse visual content synthesis. However, they struggle to generate rare or unseen concepts. To address this challenge, we explore the usage of Retrieval-Augmented Generation (RAG) with image generation models. We propose ImageRAG, a method that dynamically retrieves relevant images based on a given text prompt, and uses them as context to guide the generation process. Prior approaches that used retrieved images to improve generation, trained models specifically for retrieval-based generation. In contrast, ImageRAG leverages the capabilities of existing image conditioning models, and does not require RAG-specific training. Our approach is highly adaptable and can be applied across different model types, showing significant improvement in generating rare and fine-grained concepts using different base models.   Our project page is available at: https://rotem-shalev.github.io/ImageRAG",
            "score": 5,
            "issue_id": 2251,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 13",
                "zh": "2æœˆ13æ—¥"
            },
            "hash": "a3c0f020b9cc5226",
            "authors": [
                "Rotem Shalev-Arkushin",
                "Rinon Gal",
                "Amit H. Bermano",
                "Ohad Fried"
            ],
            "affiliations": [
                "NVIDIA",
                "Reichman University",
                "Tel Aviv University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09411.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#transfer_learning",
                    "#cv",
                    "#rag",
                    "#diffusion"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ImageRAG: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ImageRAG, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ (RAG) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ñ€ĞµĞ´ĞºĞ¸Ñ… Ğ¸Ğ»Ğ¸ Ğ½ĞµĞ²Ğ¸Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹. ImageRAG Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ñ… Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², ImageRAG Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµĞ´ĞºĞ¸Ñ… Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Enhancing Image Generation with Dynamic Retrieval",
                    "desc": "This paper introduces ImageRAG, a novel method that enhances image generation by integrating Retrieval-Augmented Generation (RAG) techniques. Unlike previous methods that required specific training for retrieval-based generation, ImageRAG utilizes existing image conditioning models to dynamically retrieve relevant images based on text prompts. This approach allows for improved synthesis of rare and fine-grained visual concepts by providing contextual guidance during the generation process. The adaptability of ImageRAG across various model types demonstrates its effectiveness in producing high-quality and diverse visual content."
                },
                "zh": {
                    "title": "ImageRAGï¼šæå‡ç¨€æœ‰æ¦‚å¿µç”Ÿæˆçš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æ‰©æ•£æ¨¡å‹å¯ä»¥ç”Ÿæˆé«˜è´¨é‡å’Œå¤šæ ·åŒ–çš„è§†è§‰å†…å®¹ï¼Œä½†åœ¨ç”Ÿæˆç¨€æœ‰æˆ–æœªè§è¿‡çš„æ¦‚å¿µæ—¶å­˜åœ¨å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä¸å›¾åƒç”Ÿæˆæ¨¡å‹çš„æ–¹æ³•ã€‚æˆ‘ä»¬æå‡ºäº†ImageRAGï¼Œè¿™æ˜¯ä¸€ç§æ ¹æ®ç»™å®šæ–‡æœ¬æç¤ºåŠ¨æ€æ£€ç´¢ç›¸å…³å›¾åƒçš„æ–¹æ³•ï¼Œå¹¶å°†è¿™äº›å›¾åƒä½œä¸ºä¸Šä¸‹æ–‡æ¥æŒ‡å¯¼ç”Ÿæˆè¿‡ç¨‹ã€‚ä¸ä¹‹å‰éœ€è¦ä¸“é—¨è®­ç»ƒçš„æ£€ç´¢ç”Ÿæˆæ¨¡å‹ä¸åŒï¼ŒImageRAGåˆ©ç”¨ç°æœ‰å›¾åƒæ¡ä»¶æ¨¡å‹çš„èƒ½åŠ›ï¼Œæ— éœ€ç‰¹å®šçš„RAGè®­ç»ƒï¼Œé€‚åº”æ€§å¼ºï¼Œèƒ½å¤Ÿåœ¨ä¸åŒæ¨¡å‹ç±»å‹ä¸­åº”ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.07586",
            "title": "We Can't Understand AI Using our Existing Vocabulary",
            "url": "https://huggingface.co/papers/2502.07586",
            "abstract": "This position paper argues that, in order to understand AI, we cannot rely on our existing vocabulary of human words. Instead, we should strive to develop neologisms: new words that represent precise human concepts that we want to teach machines, or machine concepts that we need to learn. We start from the premise that humans and machines have differing concepts. This means interpretability can be framed as a communication problem: humans must be able to reference and control machine concepts, and communicate human concepts to machines. Creating a shared human-machine language through developing neologisms, we believe, could solve this communication problem. Successful neologisms achieve a useful amount of abstraction: not too detailed, so they're reusable in many contexts, and not too high-level, so they convey precise information. As a proof of concept, we demonstrate how a \"length neologism\" enables controlling LLM response length, while a \"diversity neologism\" allows sampling more variable responses. Taken together, we argue that we cannot understand AI using our existing vocabulary, and expanding it through neologisms creates opportunities for both controlling and understanding machines better.",
            "score": 5,
            "issue_id": 2247,
            "pub_date": "2025-02-11",
            "pub_date_card": {
                "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 11",
                "zh": "2æœˆ11æ—¥"
            },
            "hash": "0435a4b5389f3dcb",
            "authors": [
                "John Hewitt",
                "Robert Geirhos",
                "Been Kim"
            ],
            "affiliations": [
                "Google DeepMind"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.07586.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#alignment",
                    "#interpretability"
                ],
                "emoji": "ğŸ—£ï¸",
                "ru": {
                    "title": "ĞĞµĞ¾Ğ»Ğ¾Ğ³Ğ¸Ğ·Ğ¼Ñ‹ ĞºĞ°Ğº Ğ¼Ğ¾ÑÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ğ¸ Ğ˜Ğ˜",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ÑÑ, Ñ‡Ñ‚Ğ¾ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµĞ¾Ğ»Ğ¾Ğ³Ğ¸Ğ·Ğ¼Ñ‹ - Ğ½Ğ¾Ğ²Ñ‹Ğµ ÑĞ»Ğ¾Ğ²Ğ° Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹. Ğ˜Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ ĞºĞ°Ğº Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ»ÑĞ´ÑŒĞ¼Ğ¸ Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ°Ğ¼Ğ¸. Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ñ‡ĞµÑ€ĞµĞ· Ğ½ĞµĞ¾Ğ»Ğ¾Ğ³Ğ¸Ğ·Ğ¼Ñ‹ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ñ€ĞµÑˆĞ¸Ñ‚ÑŒ ÑÑ‚Ñƒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ»ÑƒÑ‡ÑˆĞµ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ˜Ğ˜."
                },
                "en": {
                    "title": "Creating New Words for Better AI Communication",
                    "desc": "This paper discusses the need for new words, or neologisms, to better communicate with artificial intelligence (AI). It argues that humans and machines have different ways of understanding concepts, which makes it hard for us to explain our ideas to machines. By creating a shared language with these new terms, we can improve how we control and interpret machine behavior. The authors provide examples of neologisms that help manage AI responses, showing that a richer vocabulary can enhance our interaction with AI systems."
                },
                "zh": {
                    "title": "é€šè¿‡æ–°è¯æ±‡ç†è§£äººå·¥æ™ºèƒ½",
                    "desc": "è¿™ç¯‡è®ºæ–‡è®¤ä¸ºï¼Œè¦ç†è§£äººå·¥æ™ºèƒ½ï¼Œæˆ‘ä»¬ä¸èƒ½ä»…ä¾èµ–ç°æœ‰çš„äººç±»è¯æ±‡ã€‚æˆ‘ä»¬åº”è¯¥åŠªåŠ›å¼€å‘æ–°è¯æ±‡ï¼Œä»¥å‡†ç¡®è¡¨è¾¾æˆ‘ä»¬æƒ³æ•™ç»™æœºå™¨çš„äººç±»æ¦‚å¿µæˆ–æˆ‘ä»¬éœ€è¦å­¦ä¹ çš„æœºå™¨æ¦‚å¿µã€‚äººç±»å’Œæœºå™¨çš„æ¦‚å¿µä¸åŒï¼Œå› æ­¤å¯è§£é‡Šæ€§å¯ä»¥çœ‹ä½œæ˜¯ä¸€ä¸ªæ²Ÿé€šé—®é¢˜ï¼šäººç±»å¿…é¡»èƒ½å¤Ÿå¼•ç”¨å’Œæ§åˆ¶æœºå™¨æ¦‚å¿µï¼Œå¹¶å°†äººç±»æ¦‚å¿µä¼ è¾¾ç»™æœºå™¨ã€‚é€šè¿‡å¼€å‘æ–°è¯æ±‡åˆ›å»ºä¸€ä¸ªå…±äº«çš„äººæœºè¯­è¨€ï¼Œå¯ä»¥è§£å†³è¿™ä¸ªæ²Ÿé€šé—®é¢˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09741",
            "title": "FoNE: Precise Single-Token Number Embeddings via Fourier Features",
            "url": "https://huggingface.co/papers/2502.09741",
            "abstract": "Large Language Models (LLMs) typically represent numbers using multiple tokens, which requires the model to aggregate these tokens to interpret numerical values. This fragmentation makes both training and inference less efficient and adversely affects the model's performance on number-related tasks. Inspired by the observation that pre-trained LLMs internally learn Fourier-like features for number tokens, we propose Fourier Number Embedding (FoNE), a novel method that directly maps numbers into the embedding space with their Fourier features. FoNE encodes each number as a single token with only two embedding dimensions per digit, effectively capturing numerical values without fragmentation. This compact representation accelerates both training and inference. Compared to traditional subword and digit-wise embeddings, FoNE not only reduces computational overhead but also achieves higher accuracy across various numerical tasks including addition, subtraction and multiplication. On 6-digit decimal addition, FoNE requires 64times less data to achieve 99% accuracy than subword and digit-wise embeddings while using 3times and 6times fewer tokens per number, respectively. Furthermore, FoNE is the only method that yields 100% accuracy on over 100,000 test examples for addition, subtraction, and multiplication. The codes and visualization are available at https://fouriernumber.github.io/.",
            "score": 5,
            "issue_id": 2241,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 13",
                "zh": "2æœˆ13æ—¥"
            },
            "hash": "adb30f7d3d01ce3a",
            "authors": [
                "Tianyi Zhou",
                "Deqing Fu",
                "Mahdi Soltanolkotabi",
                "Robin Jia",
                "Vatsal Sharan"
            ],
            "affiliations": [
                "Department of Computer Science University of Southern California"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09741.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#data",
                    "#optimization"
                ],
                "emoji": "ğŸ”¢",
                "ru": {
                    "title": "FoNE: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ñ‡Ğ¸ÑĞµĞ» Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡Ğ¸ÑĞµĞ» Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Fourier Number Embedding (FoNE). FoNE Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ‡Ğ¸ÑĞ»Ğ° Ğ² Ğ²Ğ¸Ğ´Ğµ ĞµĞ´Ğ¸Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ„ÑƒÑ€ÑŒĞµ-Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ± Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ ĞºĞ°Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞŸĞ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸, FoNE Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‡Ğ¸ÑĞ»Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ, Ğ²Ñ‹Ñ‡Ğ¸Ñ‚Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑƒĞ¼Ğ½Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ, Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "Revolutionizing Number Representation in LLMs with FoNE",
                    "desc": "This paper introduces Fourier Number Embedding (FoNE), a new approach for representing numbers in Large Language Models (LLMs). FoNE simplifies the representation of numerical values by encoding each number as a single token using Fourier features, which reduces fragmentation and improves efficiency. The method significantly enhances model performance on numerical tasks, achieving higher accuracy with fewer tokens compared to traditional embeddings. Notably, FoNE demonstrates remarkable results, requiring much less data to reach high accuracy levels in arithmetic operations like addition, subtraction, and multiplication."
                },
                "zh": {
                    "title": "å‚…é‡Œå¶æ•°å­—åµŒå…¥ï¼šé«˜æ•ˆçš„æ•°å­—è¡¨ç¤ºæ–¹æ³•",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šå¸¸ä½¿ç”¨å¤šä¸ªæ ‡è®°æ¥è¡¨ç¤ºæ•°å­—ï¼Œè¿™å¯¼è‡´æ¨¡å‹åœ¨ç†è§£æ•°å€¼æ—¶éœ€è¦èšåˆè¿™äº›æ ‡è®°ï¼Œé™ä½äº†è®­ç»ƒå’Œæ¨ç†çš„æ•ˆç‡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œç§°ä¸ºå‚…é‡Œå¶æ•°å­—åµŒå…¥ï¼ˆFoNEï¼‰ï¼Œå®ƒå°†æ•°å­—ç›´æ¥æ˜ å°„åˆ°åµŒå…¥ç©ºé—´ï¼Œä½¿ç”¨å‚…é‡Œå¶ç‰¹å¾è¿›è¡Œç¼–ç ã€‚FoNEå°†æ¯ä¸ªæ•°å­—ç¼–ç ä¸ºä¸€ä¸ªå•ä¸€æ ‡è®°ï¼Œæ˜¾è‘—å‡å°‘äº†è®¡ç®—å¼€é”€ï¼Œå¹¶åœ¨åŠ æ³•ã€å‡æ³•å’Œä¹˜æ³•ç­‰æ•°å€¼ä»»åŠ¡ä¸­å®ç°äº†æ›´é«˜çš„å‡†ç¡®æ€§ã€‚ä¸ä¼ ç»Ÿçš„å­è¯å’Œæ•°å­—åµŒå…¥ç›¸æ¯”ï¼ŒFoNEåœ¨6ä½åè¿›åˆ¶åŠ æ³•ä¸­æ‰€éœ€çš„æ•°æ®é‡å‡å°‘äº†64å€ï¼ŒåŒæ—¶æ¯ä¸ªæ•°å­—ä½¿ç”¨çš„æ ‡è®°æ•°é‡ä¹Ÿå‡å°‘äº†3å€åˆ°6å€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.10140",
            "title": "Small Models, Big Impact: Efficient Corpus and Graph-Based Adaptation of Small Multilingual Language Models for Low-Resource Languages",
            "url": "https://huggingface.co/papers/2502.10140",
            "abstract": "Low-resource languages (LRLs) face significant challenges in natural language processing (NLP) due to limited data. While current state-of-the-art large language models (LLMs) still struggle with LRLs, smaller multilingual models (mLMs) such as mBERT and XLM-R offer greater promise due to a better fit of their capacity to low training data sizes. This study systematically investigates parameter-efficient adapter-based methods for adapting mLMs to LRLs, evaluating three architectures: Sequential Bottleneck, Invertible Bottleneck, and Low-Rank Adaptation. Using unstructured text from GlotCC and structured knowledge from ConceptNet, we show that small adaptation datasets (e.g., up to 1 GB of free-text or a few MB of knowledge graph data) yield gains in intrinsic (masked language modeling) and extrinsic tasks (topic classification, sentiment analysis, and named entity recognition). We find that Sequential Bottleneck adapters excel in language modeling, while Invertible Bottleneck adapters slightly outperform other methods on downstream tasks due to better embedding alignment and larger parameter counts. Adapter-based methods match or outperform full fine-tuning while using far fewer parameters, and smaller mLMs prove more effective for LRLs than massive LLMs like LLaMA-3, GPT-4, and DeepSeek-R1-based distilled models. While adaptation improves performance, pre-training data size remains the dominant factor, especially for languages with extensive pre-training coverage.",
            "score": 4,
            "issue_id": 2251,
            "pub_date": "2025-02-14",
            "pub_date_card": {
                "ru": "14 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 14",
                "zh": "2æœˆ14æ—¥"
            },
            "hash": "0d0292edb1900dd5",
            "authors": [
                "Daniil Gurgurov",
                "Ivan Vykopal",
                "Josef van Genabith",
                "Simon Ostermann"
            ],
            "affiliations": [
                "Brno University of Technology",
                "German Research Center for Artificial Intelligence (DFKI)",
                "Kempelen Institute of Intelligent Technologies (KInIT)",
                "University of Saarland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.10140.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#transfer_learning",
                    "#small_models",
                    "#multilingual",
                    "#low_resource"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ² Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (mLMs) Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ² Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸ (LRLs) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ¾Ğ²: Sequential Bottleneck, Invertible Bottleneck Ğ¸ Low-Rank Adaptation, Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ Ğ¸Ñ… ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… (Ğ´Ğ¾ 1 Ğ“Ğ‘ Ğ½ĞµÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸Ğ»Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ĞœĞ‘ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹) ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ»Ñ LRLs Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğµ mLMs, Ñ‡ĞµĞ¼ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‚Ğ¸Ğ¿Ğ° LLaMA-3 Ğ¸Ğ»Ğ¸ GPT-4."
                },
                "en": {
                    "title": "Empowering Low-Resource Languages with Efficient Multilingual Models",
                    "desc": "This paper addresses the challenges of processing low-resource languages (LRLs) in natural language processing (NLP) by exploring the use of smaller multilingual models (mLMs) like mBERT and XLM-R. It investigates parameter-efficient adapter-based methods to adapt these mLMs to LRLs, focusing on three architectures: Sequential Bottleneck, Invertible Bottleneck, and Low-Rank Adaptation. The study demonstrates that even small adaptation datasets can significantly enhance performance in both intrinsic and extrinsic NLP tasks. Results indicate that Sequential Bottleneck adapters are particularly effective for language modeling, while Invertible Bottleneck adapters perform better on downstream tasks, highlighting the advantages of adapter-based methods over full fine-tuning."
                },
                "zh": {
                    "title": "å°å‹å¤šè¯­è¨€æ¨¡å‹åŠ©åŠ›ä½èµ„æºè¯­è¨€å¤„ç†",
                    "desc": "ä½èµ„æºè¯­è¨€ï¼ˆLRLsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­é¢ä¸´æ•°æ®ä¸è¶³çš„é‡å¤§æŒ‘æˆ˜ã€‚å½“å‰çš„å…ˆè¿›å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†LRLsæ—¶ä»ç„¶å­˜åœ¨å›°éš¾ï¼Œè€Œè¾ƒå°çš„å¤šè¯­è¨€æ¨¡å‹ï¼ˆmLMsï¼‰å¦‚mBERTå’ŒXLM-Rç”±äºå…¶å®¹é‡æ›´é€‚åˆä½è®­ç»ƒæ•°æ®é‡ï¼Œå±•ç°å‡ºæ›´å¤§çš„æ½œåŠ›ã€‚æœ¬æ–‡ç³»ç»Ÿç ”ç©¶äº†åŸºäºå‚æ•°é«˜æ•ˆé€‚é…å™¨çš„æ–¹æ³•ï¼Œè¯„ä¼°äº†ä¸‰ç§æ¶æ„ï¼šé¡ºåºç“¶é¢ˆã€å¯é€†ç“¶é¢ˆå’Œä½ç§©é€‚é…ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨å°å‹é€‚é…æ•°æ®é›†å¯ä»¥åœ¨è¯­è¨€å»ºæ¨¡å’Œä¸‹æ¸¸ä»»åŠ¡ä¸­å–å¾—æ˜¾è‘—æå‡ï¼Œå°¤å…¶æ˜¯é¡ºåºç“¶é¢ˆé€‚é…å™¨åœ¨è¯­è¨€å»ºæ¨¡æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09638",
            "title": "Jailbreaking to Jailbreak",
            "url": "https://huggingface.co/papers/2502.09638",
            "abstract": "Refusal training on Large Language Models (LLMs) prevents harmful outputs, yet this defense remains vulnerable to both automated and human-crafted jailbreaks. We present a novel LLM-as-red-teamer approach in which a human jailbreaks a refusal-trained LLM to make it willing to jailbreak itself or other LLMs. We refer to the jailbroken LLMs as J_2 attackers, which can systematically evaluate target models using various red teaming strategies and improve its performance via in-context learning from the previous failures. Our experiments demonstrate that Sonnet 3.5 and Gemini 1.5 pro outperform other LLMs as J_2, achieving 93.0% and 91.0% attack success rates (ASRs) respectively against GPT-4o (and similar results across other capable LLMs) on Harmbench. Our work not only introduces a scalable approach to strategic red teaming, drawing inspiration from human red teamers, but also highlights jailbreaking-to-jailbreak as an overlooked failure mode of the safeguard. Specifically, an LLM can bypass its own safeguards by employing a jailbroken version of itself that is willing to assist in further jailbreaking. To prevent any direct misuse with J_2, while advancing research in AI safety, we publicly share our methodology while keeping specific prompting details private.",
            "score": 3,
            "issue_id": 2242,
            "pub_date": "2025-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "3c2ed560e12b971a",
            "authors": [
                "Jeremy Kritz",
                "Vaughn Robinson",
                "Robert Vacareanu",
                "Bijan Varjavand",
                "Michael Choi",
                "Bobby Gogov",
                "Scale Red Team",
                "Summer Yue",
                "Willow E. Primack",
                "Zifan Wang"
            ],
            "affiliations": [
                "Scale"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09638.jpg",
            "data": {
                "categories": [
                    "#ethics",
                    "#multimodal",
                    "#training",
                    "#security",
                    "#rlhf"
                ],
                "emoji": "ğŸ•µï¸",
                "ru": {
                    "title": "LLM Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² LLM: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€Ğ¾Ğ½Ñ‚ Ğ² Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ ÑĞ°Ğ¼Ğ¸ LLM Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ 'ĞºÑ€Ğ°ÑĞ½Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹'. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, ĞºĞ°Ğº Ğ²Ğ·Ğ»Ğ¾Ğ¼Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (J_2) Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ°Ñ‚Ğ°ĞºĞ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ ÑƒÑĞ¿ĞµÑ…Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Sonnet 3.5 Ğ¸ Gemini 1.5 pro Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ LLM Ğ² Ñ€Ğ¾Ğ»Ğ¸ J_2, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ 93.0% Ğ¸ 91.0% ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ñ‚Ğ°Ğº ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°."
                },
                "en": {
                    "title": "Jailbreaking the Safeguards: A New Approach to LLM Red Teaming",
                    "desc": "This paper discusses a new method for testing the safety of Large Language Models (LLMs) by using a jailbroken version of the model itself, referred to as J_2 attackers. These J_2 attackers can evaluate and exploit vulnerabilities in other LLMs, achieving high success rates in bypassing safeguards. The approach allows the jailbroken LLM to learn from its previous attempts, improving its effectiveness in red teaming strategies. The authors emphasize the importance of understanding this jailbreaking-to-jailbreak phenomenon as a potential risk in AI safety."
                },
                "zh": {
                    "title": "ç ´è§£è‡ªæˆ‘ä¿æŠ¤çš„çº¢é˜Ÿç­–ç•¥",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºçº¢é˜Ÿæˆå‘˜ï¼Œæ¥è¯„ä¼°å’Œæ”¹è¿›æ‹’ç»è®­ç»ƒæ¨¡å‹çš„å®‰å…¨æ€§ã€‚æˆ‘ä»¬ç§°è¿™äº›è¢«ç ´è§£çš„LLMä¸ºJ_2æ”»å‡»è€…ï¼Œå®ƒä»¬èƒ½å¤Ÿé€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ä»ä¹‹å‰çš„å¤±è´¥ä¸­æé«˜æ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSonnet 3.5å’ŒGemini 1.5åœ¨æ”»å‡»æˆåŠŸç‡ä¸Šä¼˜äºå…¶ä»–LLMï¼Œåˆ†åˆ«è¾¾åˆ°äº†93.0%å’Œ91.0%ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ä»…æä¾›äº†ä¸€ç§å¯æ‰©å±•çš„çº¢é˜Ÿç­–ç•¥ï¼Œè¿˜æ­ç¤ºäº†ç ´è§£è‡ªèº«ä¿æŠ¤æœºåˆ¶çš„æ½œåœ¨é£é™©ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.10177",
            "title": "STMA: A Spatio-Temporal Memory Agent for Long-Horizon Embodied Task Planning",
            "url": "https://huggingface.co/papers/2502.10177",
            "abstract": "A key objective of embodied intelligence is enabling agents to perform long-horizon tasks in dynamic environments while maintaining robust decision-making and adaptability. To achieve this goal, we propose the Spatio-Temporal Memory Agent (STMA), a novel framework designed to enhance task planning and execution by integrating spatio-temporal memory. STMA is built upon three critical components: (1) a spatio-temporal memory module that captures historical and environmental changes in real time, (2) a dynamic knowledge graph that facilitates adaptive spatial reasoning, and (3) a planner-critic mechanism that iteratively refines task strategies. We evaluate STMA in the TextWorld environment on 32 tasks, involving multi-step planning and exploration under varying levels of complexity. Experimental results demonstrate that STMA achieves a 31.25% improvement in success rate and a 24.7% increase in average score compared to the state-of-the-art model. The results highlight the effectiveness of spatio-temporal memory in advancing the memory capabilities of embodied agents.",
            "score": 3,
            "issue_id": 2240,
            "pub_date": "2025-02-14",
            "pub_date_card": {
                "ru": "14 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 14",
                "zh": "2æœˆ14æ—¥"
            },
            "hash": "1b17b668b26c2264",
            "authors": [
                "Mingcong Lei",
                "Yiming Zhao",
                "Ge Wang",
                "Zhixin Mai",
                "Shuguang Cui",
                "Yatong Han",
                "Jinke Ren"
            ],
            "affiliations": [
                "FNii-Shenzhen, The Chinese University of Hong Kong, Shenzhen, China",
                "Harbin Engineering University, China",
                "Infused Synapse AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.10177.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#agents",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Spatio-Temporal Memory Agent (STMA) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ñ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼. STMA Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ³Ñ€Ğ°Ñ„ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ°-ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ² ÑÑ€ĞµĞ´Ğµ TextWorld Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ€ĞµĞ´Ğ½ĞµĞ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing Agent Intelligence with Spatio-Temporal Memory",
                    "desc": "The paper introduces the Spatio-Temporal Memory Agent (STMA), which aims to improve how agents perform complex tasks in changing environments. STMA incorporates a spatio-temporal memory module to track past events and environmental shifts, enhancing decision-making. It also utilizes a dynamic knowledge graph for better spatial reasoning and a planner-critic mechanism to refine strategies over time. The results show that STMA significantly outperforms existing models in task success rates and scoring, demonstrating the importance of memory in embodied intelligence."
                },
                "zh": {
                    "title": "æ—¶ç©ºè®°å¿†æ™ºèƒ½ä½“ï¼šæå‡æ™ºèƒ½ä½“å†³ç­–ä¸é€‚åº”èƒ½åŠ›çš„å…³é”®",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œç§°ä¸ºæ—¶ç©ºè®°å¿†æ™ºèƒ½ä½“ï¼ˆSTMAï¼‰ï¼Œæ—¨åœ¨æé«˜æ™ºèƒ½ä½“åœ¨åŠ¨æ€ç¯å¢ƒä¸­æ‰§è¡Œé•¿æœŸä»»åŠ¡çš„èƒ½åŠ›ã€‚STMAé›†æˆäº†æ—¶ç©ºè®°å¿†æ¨¡å—ã€åŠ¨æ€çŸ¥è¯†å›¾è°±å’Œè§„åˆ’-è¯„ä¼°æœºåˆ¶ï¼Œä»¥å¢å¼ºä»»åŠ¡è§„åˆ’å’Œæ‰§è¡Œçš„æ•ˆæœã€‚é€šè¿‡åœ¨TextWorldç¯å¢ƒä¸­è¿›è¡Œ32ä¸ªä»»åŠ¡çš„è¯„ä¼°ï¼ŒSTMAåœ¨æˆåŠŸç‡å’Œå¹³å‡å¾—åˆ†ä¸Šåˆ†åˆ«æé«˜äº†31.25%å’Œ24.7%ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ—¶ç©ºè®°å¿†åœ¨æå‡æ™ºèƒ½ä½“çš„è®°å¿†èƒ½åŠ›æ–¹é¢å…·æœ‰æ˜¾è‘—æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.10392",
            "title": "Text-guided Sparse Voxel Pruning for Efficient 3D Visual Grounding",
            "url": "https://huggingface.co/papers/2502.10392",
            "abstract": "In this paper, we propose an efficient multi-level convolution architecture for 3D visual grounding. Conventional methods are difficult to meet the requirements of real-time inference due to the two-stage or point-based architecture. Inspired by the success of multi-level fully sparse convolutional architecture in 3D object detection, we aim to build a new 3D visual grounding framework following this technical route. However, as in 3D visual grounding task the 3D scene representation should be deeply interacted with text features, sparse convolution-based architecture is inefficient for this interaction due to the large amount of voxel features. To this end, we propose text-guided pruning (TGP) and completion-based addition (CBA) to deeply fuse 3D scene representation and text features in an efficient way by gradual region pruning and target completion. Specifically, TGP iteratively sparsifies the 3D scene representation and thus efficiently interacts the voxel features with text features by cross-attention. To mitigate the affect of pruning on delicate geometric information, CBA adaptively fixes the over-pruned region by voxel completion with negligible computational overhead. Compared with previous single-stage methods, our method achieves top inference speed and surpasses previous fastest method by 100\\% FPS. Our method also achieves state-of-the-art accuracy even compared with two-stage methods, with +1.13 lead of Acc@0.5 on ScanRefer, and +2.6 and +3.2 leads on NR3D and SR3D respectively. The code is available at https://github.com/GWxuan/TSP3D{https://github.com/GWxuan/TSP3D}.",
            "score": 2,
            "issue_id": 2252,
            "pub_date": "2025-02-14",
            "pub_date_card": {
                "ru": "14 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 14",
                "zh": "2æœˆ14æ—¥"
            },
            "hash": "364eaebf9db5bd4a",
            "authors": [
                "Wenxuan Guo",
                "Xiuwei Xu",
                "Ziwei Wang",
                "Jianjiang Feng",
                "Jie Zhou",
                "Jiwen Lu"
            ],
            "affiliations": [
                "Nanyang Technological University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.10392.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#architecture",
                    "#training",
                    "#cv"
                ],
                "emoji": "ğŸ†",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ 3D Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğ¹ ÑĞ²ĞµÑ€Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ°Ñ ÑĞ²ĞµÑ€Ñ‚Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ 3D Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾-ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğµ Ğ¿Ñ€Ğ¾Ñ€ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ (TGP) Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ (CBA) Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ 3D-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑÑ†ĞµĞ½Ñ‹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¹ ÑĞ°Ğ¼Ñ‹Ğ¹ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğ° 100% Ğ¿Ğ¾ FPS. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ScanRefer, NR3D Ğ¸ SR3D."
                },
                "en": {
                    "title": "Efficient 3D Visual Grounding with Multi-Level Convolution",
                    "desc": "This paper introduces a new multi-level convolution architecture designed for 3D visual grounding, which is the task of linking 3D scenes with textual descriptions. Traditional methods struggle with real-time performance due to their complex two-stage or point-based designs. The authors leverage a multi-level fully sparse convolution approach, enhancing the interaction between 3D scene representations and text features through innovative techniques like text-guided pruning (TGP) and completion-based addition (CBA). Their method not only improves inference speed, achieving a 100% increase in frames per second (FPS) compared to the fastest existing methods, but also enhances accuracy on benchmark datasets, outperforming previous models."
                },
                "zh": {
                    "title": "é«˜æ•ˆèåˆ3Dåœºæ™¯ä¸æ–‡æœ¬ç‰¹å¾çš„è§†è§‰å®šä½æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„å¤šå±‚å·ç§¯æ¶æ„ï¼Œç”¨äº3Dè§†è§‰å®šä½ã€‚ä¼ ç»Ÿæ–¹æ³•ç”±äºé‡‡ç”¨ä¸¤é˜¶æ®µæˆ–åŸºäºç‚¹çš„æ¶æ„ï¼Œéš¾ä»¥æ»¡è¶³å®æ—¶æ¨ç†çš„è¦æ±‚ã€‚æˆ‘ä»¬å€Ÿé‰´äº†å¤šå±‚ç¨€ç–å·ç§¯æ¶æ„åœ¨3Dç‰©ä½“æ£€æµ‹ä¸­çš„æˆåŠŸï¼Œæ„å»ºäº†æ–°çš„3Dè§†è§‰å®šä½æ¡†æ¶ã€‚é€šè¿‡æ–‡æœ¬å¼•å¯¼ä¿®å‰ªå’ŒåŸºäºè¡¥å…¨çš„æ·»åŠ ï¼Œæˆ‘ä»¬æœ‰æ•ˆåœ°èåˆäº†3Dåœºæ™¯è¡¨ç¤ºå’Œæ–‡æœ¬ç‰¹å¾ï¼Œæ˜¾è‘—æé«˜äº†æ¨ç†é€Ÿåº¦å’Œå‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.07856",
            "title": "MRS: A Fast Sampler for Mean Reverting Diffusion based on ODE and SDE Solvers",
            "url": "https://huggingface.co/papers/2502.07856",
            "abstract": "In applications of diffusion models, controllable generation is of practical significance, but is also challenging. Current methods for controllable generation primarily focus on modifying the score function of diffusion models, while Mean Reverting (MR) Diffusion directly modifies the structure of the stochastic differential equation (SDE), making the incorporation of image conditions simpler and more natural. However, current training-free fast samplers are not directly applicable to MR Diffusion. And thus MR Diffusion requires hundreds of NFEs (number of function evaluations) to obtain high-quality samples. In this paper, we propose a new algorithm named MRS (MR Sampler) to reduce the sampling NFEs of MR Diffusion. We solve the reverse-time SDE and the probability flow ordinary differential equation (PF-ODE) associated with MR Diffusion, and derive semi-analytical solutions. The solutions consist of an analytical function and an integral parameterized by a neural network. Based on this solution, we can generate high-quality samples in fewer steps. Our approach does not require training and supports all mainstream parameterizations, including noise prediction, data prediction and velocity prediction. Extensive experiments demonstrate that MR Sampler maintains high sampling quality with a speedup of 10 to 20 times across ten different image restoration tasks. Our algorithm accelerates the sampling procedure of MR Diffusion, making it more practical in controllable generation.",
            "score": 2,
            "issue_id": 2244,
            "pub_date": "2025-02-11",
            "pub_date_card": {
                "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 11",
                "zh": "2æœˆ11æ—¥"
            },
            "hash": "2bb6766a68f50cdc",
            "authors": [
                "Ao Li",
                "Wei Fang",
                "Hongbo Zhao",
                "Le Lu",
                "Ge Yang",
                "Minfeng Xu"
            ],
            "affiliations": [
                "DAMO Academy, Alibaba Group",
                "Hupan Laboratory, Hangzhou, China",
                "Institute of Automation, Chinese Academy of Sciences (CASIA)",
                "School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.07856.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#cv",
                    "#data",
                    "#diffusion",
                    "#optimization"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ MRS (MR Sampler) Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° ÑĞµĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Mean Reverting (MR) Diffusion Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¾Ğ±Ñ‹ĞºĞ½Ğ¾Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ MR Diffusion, Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¿Ğ¾Ğ»ÑƒĞ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²ÑĞµ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ ÑˆÑƒĞ¼Ğ°, Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MR Sampler ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑĞµĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğ¸ Ğ² 10-20 Ñ€Ğ°Ğ· Ğ´Ğ»Ñ Ğ´ĞµÑÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Accelerating Controllable Generation with MR Sampler",
                    "desc": "This paper introduces a new algorithm called MRS (MR Sampler) to improve the efficiency of sampling in Mean Reverting (MR) Diffusion models. Unlike traditional methods that modify the score function, MRS directly addresses the stochastic differential equation structure, simplifying the integration of image conditions. The proposed method achieves high-quality sample generation with significantly fewer function evaluations, enhancing the speed of the sampling process. Experimental results show that MRS can produce samples 10 to 20 times faster while maintaining quality across various image restoration tasks."
                },
                "zh": {
                    "title": "åŠ é€Ÿå¯æ§ç”Ÿæˆçš„MRé‡‡æ ·å™¨",
                    "desc": "åœ¨æ‰©æ•£æ¨¡å‹çš„åº”ç”¨ä¸­ï¼Œå¯æ§ç”Ÿæˆå…·æœ‰é‡è¦çš„å®é™…æ„ä¹‰ï¼Œä½†ä¹Ÿé¢ä¸´æŒ‘æˆ˜ã€‚å½“å‰çš„å¯æ§ç”Ÿæˆæ–¹æ³•ä¸»è¦é›†ä¸­åœ¨ä¿®æ”¹æ‰©æ•£æ¨¡å‹çš„è¯„åˆ†å‡½æ•°ï¼Œè€Œå‡å€¼å›å½’æ‰©æ•£ï¼ˆMR Diffusionï¼‰åˆ™ç›´æ¥ä¿®æ”¹éšæœºå¾®åˆ†æ–¹ç¨‹ï¼ˆSDEï¼‰çš„ç»“æ„ï¼Œä½¿å¾—å›¾åƒæ¡ä»¶çš„ç»“åˆæ›´åŠ ç®€å•è‡ªç„¶ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°ç®—æ³•MRSï¼ˆMRé‡‡æ ·å™¨ï¼‰ï¼Œæ—¨åœ¨å‡å°‘MRæ‰©æ•£çš„é‡‡æ ·å‡½æ•°è¯„ä¼°æ¬¡æ•°ï¼ˆNFEsï¼‰ï¼Œå¹¶é€šè¿‡è§£å†³ä¸MRæ‰©æ•£ç›¸å…³çš„åå‘æ—¶é—´SDEå’Œæ¦‚ç‡æµå¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆPF-ODEï¼‰æ¥è·å¾—é«˜è´¨é‡æ ·æœ¬ã€‚å®éªŒè¡¨æ˜ï¼ŒMRé‡‡æ ·å™¨åœ¨åç§ä¸åŒçš„å›¾åƒæ¢å¤ä»»åŠ¡ä¸­ä¿æŒé«˜é‡‡æ ·è´¨é‡ï¼Œå¹¶å®ç°äº†10åˆ°20å€çš„åŠ é€Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09980",
            "title": "V2V-LLM: Vehicle-to-Vehicle Cooperative Autonomous Driving with Multi-Modal Large Language Models",
            "url": "https://huggingface.co/papers/2502.09980",
            "abstract": "Current autonomous driving vehicles rely mainly on their individual sensors to understand surrounding scenes and plan for future trajectories, which can be unreliable when the sensors are malfunctioning or occluded. To address this problem, cooperative perception methods via vehicle-to-vehicle (V2V) communication have been proposed, but they have tended to focus on detection and tracking. How those approaches contribute to overall cooperative planning performance is still under-explored. Inspired by recent progress using Large Language Models (LLMs) to build autonomous driving systems, we propose a novel problem setting that integrates an LLM into cooperative autonomous driving, with the proposed Vehicle-to-Vehicle Question-Answering (V2V-QA) dataset and benchmark. We also propose our baseline method Vehicle-to-Vehicle Large Language Model (V2V-LLM), which uses an LLM to fuse perception information from multiple connected autonomous vehicles (CAVs) and answer driving-related questions: grounding, notable object identification, and planning. Experimental results show that our proposed V2V-LLM can be a promising unified model architecture for performing various tasks in cooperative autonomous driving, and outperforms other baseline methods that use different fusion approaches. Our work also creates a new research direction that can improve the safety of future autonomous driving systems. Our project website: https://eddyhkchiu.github.io/v2vllm.github.io/ .",
            "score": 1,
            "issue_id": 2244,
            "pub_date": "2025-02-14",
            "pub_date_card": {
                "ru": "14 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 14",
                "zh": "2æœˆ14æ—¥"
            },
            "hash": "57343c782d806dc0",
            "authors": [
                "Hsu-kuang Chiu",
                "Ryo Hachiuma",
                "Chien-Yi Wang",
                "Stephen F. Smith",
                "Yu-Chiang Frank Wang",
                "Min-Hung Chen"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "NVIDIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09980.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#architecture",
                    "#reasoning",
                    "#science",
                    "#agi",
                    "#games",
                    "#optimization",
                    "#dataset",
                    "#agents"
                ],
                "emoji": "ğŸš—",
                "ru": {
                    "title": "LLM Ğ½Ğ° ÑĞ»ÑƒĞ¶Ğ±Ğµ ĞºĞ¾Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ĞºĞ¾Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ¼Ñƒ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº V2V-QA Ğ´Ğ»Ñ Ğ¾Ğ±Ğ¼ĞµĞ½Ğ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑĞ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ. Ğ˜Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´ V2V-LLM Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ LLM Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¾Ñ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¿Ğ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´ÑÑ‚Ğ² Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ V2V-LLM Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ğ¾Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Cooperative Driving with Language Models",
                    "desc": "This paper introduces a new approach to enhance cooperative autonomous driving by integrating Large Language Models (LLMs) with vehicle-to-vehicle (V2V) communication. The proposed method, called Vehicle-to-Vehicle Large Language Model (V2V-LLM), allows connected autonomous vehicles (CAVs) to share and fuse perception data, enabling them to answer driving-related questions effectively. The authors present a new dataset, Vehicle-to-Vehicle Question-Answering (V2V-QA), to benchmark this integration and demonstrate its effectiveness in improving planning and safety. Experimental results indicate that V2V-LLM outperforms existing methods, paving the way for a unified model architecture in cooperative driving systems."
                },
                "zh": {
                    "title": "åä½œè‡ªåŠ¨é©¾é©¶çš„æ–°æ–¹å‘ï¼šè½¦è¾†é—´é—®ç­”æ¨¡å‹",
                    "desc": "å½“å‰çš„è‡ªåŠ¨é©¾é©¶è½¦è¾†ä¸»è¦ä¾èµ–å„è‡ªçš„ä¼ æ„Ÿå™¨æ¥ç†è§£å‘¨å›´åœºæ™¯å’Œè§„åˆ’æœªæ¥è½¨è¿¹ï¼Œä½†å½“ä¼ æ„Ÿå™¨å‡ºç°æ•…éšœæˆ–è¢«é®æŒ¡æ—¶ï¼Œè¿™ç§æ–¹æ³•å¯èƒ½ä¸å¯é ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæå‡ºäº†é€šè¿‡è½¦ä¸è½¦ï¼ˆV2Vï¼‰é€šä¿¡çš„åä½œæ„ŸçŸ¥æ–¹æ³•ï¼Œä½†è¿™äº›æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨æ£€æµ‹å’Œè·Ÿè¸ªä¸Šã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„é—®é¢˜è®¾ç½®ï¼Œå°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é›†æˆåˆ°åä½œè‡ªåŠ¨é©¾é©¶ä¸­ï¼Œå¹¶å¼•å…¥äº†è½¦è¾†é—´é—®ç­”ï¼ˆV2V-QAï¼‰æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒV2V-LLMèƒ½å¤Ÿæœ‰æ•ˆèåˆå¤šä¸ªè¿æ¥çš„è‡ªåŠ¨é©¾é©¶è½¦è¾†çš„æ„ŸçŸ¥ä¿¡æ¯ï¼Œå¹¶åœ¨åä½œè‡ªåŠ¨é©¾é©¶ä¸­æ‰§è¡Œå¤šç§ä»»åŠ¡ï¼Œæå‡æœªæ¥è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„å®‰å…¨æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.10362",
            "title": "CLaMP 3: Universal Music Information Retrieval Across Unaligned Modalities and Unseen Languages",
            "url": "https://huggingface.co/papers/2502.10362",
            "abstract": "CLaMP 3 is a unified framework developed to address challenges of cross-modal and cross-lingual generalization in music information retrieval. Using contrastive learning, it aligns all major music modalities--including sheet music, performance signals, and audio recordings--with multilingual text in a shared representation space, enabling retrieval across unaligned modalities with text as a bridge. It features a multilingual text encoder adaptable to unseen languages, exhibiting strong cross-lingual generalization. Leveraging retrieval-augmented generation, we curated M4-RAG, a web-scale dataset consisting of 2.31 million music-text pairs. This dataset is enriched with detailed metadata that represents a wide array of global musical traditions. To advance future research, we release WikiMT-X, a benchmark comprising 1,000 triplets of sheet music, audio, and richly varied text descriptions. Experiments show that CLaMP 3 achieves state-of-the-art performance on multiple MIR tasks, significantly surpassing previous strong baselines and demonstrating excellent generalization in multimodal and multilingual music contexts.",
            "score": 0,
            "issue_id": 2253,
            "pub_date": "2025-02-14",
            "pub_date_card": {
                "ru": "14 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 14",
                "zh": "2æœˆ14æ—¥"
            },
            "hash": "01dc60f8f9db0ad7",
            "authors": [
                "Shangda Wu",
                "Zhancheng Guo",
                "Ruibin Yuan",
                "Junyan Jiang",
                "Seungheon Doh",
                "Gus Xia",
                "Juhan Nam",
                "Xiaobing Li",
                "Feng Yu",
                "Maosong Sun"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2502.10362.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#transfer_learning",
                    "#multilingual",
                    "#rag",
                    "#low_resource",
                    "#data",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "ğŸµ",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸",
                    "desc": "CLaMP 3 - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸ ĞºÑ€Ğ¾ÑÑ-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ (Ğ½Ğ¾Ñ‚Ñ‹, ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ, Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸) Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¼ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ ÑĞ·Ñ‹ĞºĞ°Ğ¼, Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ ĞºÑ€Ğ¾ÑÑ-ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ CLaMP 3 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… MIR, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Bridging Music and Language with CLaMP 3",
                    "desc": "CLaMP 3 is a new framework designed to improve how we retrieve music information across different formats and languages. It uses contrastive learning to connect various music types, like sheet music and audio, with text in multiple languages, allowing for better searches even when the formats don't match. The framework includes a multilingual text encoder that can adapt to new languages, showing its ability to generalize across different linguistic contexts. Additionally, it introduces a large dataset and benchmark to support further research in music information retrieval, achieving top performance in various tasks."
                },
                "zh": {
                    "title": "è·¨æ¨¡æ€éŸ³ä¹æ£€ç´¢çš„æ–°çªç ´",
                    "desc": "CLaMP 3 æ˜¯ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³éŸ³ä¹ä¿¡æ¯æ£€ç´¢ä¸­çš„è·¨æ¨¡æ€å’Œè·¨è¯­è¨€æ³›åŒ–æŒ‘æˆ˜ã€‚å®ƒé€šè¿‡å¯¹æ¯”å­¦ä¹ ï¼Œå°†ä¹è°±ã€è¡¨æ¼”ä¿¡å·å’ŒéŸ³é¢‘å½•éŸ³ç­‰ä¸»è¦éŸ³ä¹æ¨¡æ€ä¸å¤šè¯­è¨€æ–‡æœ¬å¯¹é½ï¼Œå½¢æˆå…±äº«è¡¨ç¤ºç©ºé—´ï¼Œä»è€Œå®ç°é€šè¿‡æ–‡æœ¬ä½œä¸ºæ¡¥æ¢çš„æ£€ç´¢ã€‚è¯¥æ¡†æ¶å…·æœ‰é€‚åº”æœªè§è¯­è¨€çš„å¤šè¯­è¨€æ–‡æœ¬ç¼–ç å™¨ï¼Œå±•ç°å‡ºå¼ºå¤§çš„è·¨è¯­è¨€æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡å¢å¼ºæ£€ç´¢ç”Ÿæˆï¼Œæˆ‘ä»¬åˆ›å»ºäº† M4-RAG æ•°æ®é›†ï¼ŒåŒ…å« 231 ä¸‡å¯¹éŸ³ä¹-æ–‡æœ¬å¯¹ï¼Œå¹¶å‘å¸ƒäº† WikiMT-X åŸºå‡†ï¼Œæ¨åŠ¨æœªæ¥ç ”ç©¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.08769",
            "title": "Cluster and Predict Latents Patches for Improved Masked Image Modeling",
            "url": "https://huggingface.co/papers/2502.08769",
            "abstract": "Masked Image Modeling (MIM) offers a promising approach to self-supervised representation learning, however existing MIM models still lag behind the state-of-the-art. In this paper, we systematically analyze target representations, loss functions, and architectures, to introduce CAPI - a novel pure-MIM framework that relies on the prediction of latent clusterings. Our approach leverages a clustering-based loss, which is stable to train, and exhibits promising scaling properties. Our ViT-L backbone, CAPI, achieves 83.8% accuracy on ImageNet and 32.1% mIoU on ADE20K with simple linear probes, substantially outperforming previous MIM methods and approaching the performance of the current state-of-the-art, DINOv2. We release all our code and models.",
            "score": 0,
            "issue_id": 2250,
            "pub_date": "2025-02-12",
            "pub_date_card": {
                "ru": "12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 12",
                "zh": "2æœˆ12æ—¥"
            },
            "hash": "8fd9852310af51f5",
            "authors": [
                "TimothÃ©e Darcet",
                "Federico Baldassarre",
                "Maxime Oquab",
                "Julien Mairal",
                "Piotr Bojanowski"
            ],
            "affiliations": [
                "CNRS",
                "Grenoble INP",
                "Inria",
                "LJK",
                "Meta",
                "Univ. Grenoble Alpes"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.08769.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#cv",
                    "#training",
                    "#architecture",
                    "#dataset",
                    "#open_source"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "CAPI: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ¾Ğ¼ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ CAPI - Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ (MIM). CAPI Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ Ğ½Ğ¾Ğ²ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ 83.8% Ğ½Ğ° ImageNet Ğ¸ 32.1% mIoU Ğ½Ğ° ADE20K Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ±. CAPI Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ MIM Ğ¸ Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶Ğ°ĞµÑ‚ÑÑ Ğº Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ state-of-the-art Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° DINOv2."
                },
                "en": {
                    "title": "CAPI: Clustering for Superior Masked Image Modeling",
                    "desc": "This paper presents CAPI, a new framework for Masked Image Modeling (MIM) that enhances self-supervised learning by focusing on predicting latent clusterings. The authors analyze various aspects of MIM, including target representations and loss functions, to develop a clustering-based loss that is stable during training. CAPI utilizes a Vision Transformer (ViT-L) backbone, achieving impressive results with 83.8% accuracy on ImageNet and 32.1% mean Intersection over Union (mIoU) on ADE20K. The framework significantly outperforms previous MIM methods and approaches the performance of the leading model, DINOv2, while the authors provide all code and models for further research."
                },
                "zh": {
                    "title": "CAPIï¼šæå‡è‡ªç›‘ç£å­¦ä¹ çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„çº¯é®ç½©å›¾åƒå»ºæ¨¡æ¡†æ¶CAPIï¼Œæ—¨åœ¨æå‡è‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ çš„æ•ˆæœã€‚æˆ‘ä»¬ç³»ç»Ÿåˆ†æäº†ç›®æ ‡è¡¨ç¤ºã€æŸå¤±å‡½æ•°å’Œæ¶æ„ï¼Œæå‡ºäº†ä¸€ç§åŸºäºèšç±»çš„æŸå¤±å‡½æ•°ï¼Œä½¿å¾—è®­ç»ƒè¿‡ç¨‹æ›´åŠ ç¨³å®šã€‚CAPIåœ¨ViT-Léª¨å¹²ç½‘ç»œä¸Šå®ç°äº†83.8%çš„ImageNetå‡†ç¡®ç‡å’Œ32.1%çš„ADE20K mIoUï¼Œæ˜¾è‘—è¶…è¶Šäº†ä¹‹å‰çš„MIMæ–¹æ³•ã€‚æˆ‘ä»¬å°†æ‰€æœ‰ä»£ç å’Œæ¨¡å‹å…¬å¼€ï¼Œä¿ƒè¿›ç ”ç©¶çš„è¿›ä¸€æ­¥å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.10173",
            "title": "Agentic End-to-End De Novo Protein Design for Tailored Dynamics Using a Language Diffusion Model",
            "url": "https://huggingface.co/papers/2502.10173",
            "abstract": "Proteins are dynamic molecular machines whose biological functions, spanning enzymatic catalysis, signal transduction, and structural adaptation, are intrinsically linked to their motions. Designing proteins with targeted dynamic properties, however, remains a challenge due to the complex, degenerate relationships between sequence, structure, and molecular motion. Here, we introduce VibeGen, a generative AI framework that enables end-to-end de novo protein design conditioned on normal mode vibrations. VibeGen employs an agentic dual-model architecture, comprising a protein designer that generates sequence candidates based on specified vibrational modes and a protein predictor that evaluates their dynamic accuracy. This approach synergizes diversity, accuracy, and novelty during the design process. Via full-atom molecular simulations as direct validation, we demonstrate that the designed proteins accurately reproduce the prescribed normal mode amplitudes across the backbone while adopting various stable, functionally relevant structures. Notably, generated sequences are de novo, exhibiting no significant similarity to natural proteins, thereby expanding the accessible protein space beyond evolutionary constraints. Our work integrates protein dynamics into generative protein design, and establishes a direct, bidirectional link between sequence and vibrational behavior, unlocking new pathways for engineering biomolecules with tailored dynamical and functional properties. This framework holds broad implications for the rational design of flexible enzymes, dynamic scaffolds, and biomaterials, paving the way toward dynamics-informed AI-driven protein engineering.",
            "score": 0,
            "issue_id": 2247,
            "pub_date": "2025-02-14",
            "pub_date_card": {
                "ru": "14 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 14",
                "zh": "2æœˆ14æ—¥"
            },
            "hash": "84102aa522298331",
            "authors": [
                "Bo Ni",
                "Markus J. Buehler"
            ],
            "affiliations": [
                "Center for Computational Science and Engineering, Schwarzman College of Computing, Massachusetts Institute of Technology, Cambridge, MA, USA",
                "Department of Materials Science and Engineering, Carnegie Mellon University, Pittsburgh, PA, USA",
                "Laboratory for Atomistic and Molecular Mechanics (LAMM), Massachusetts Institute of Technology, Cambridge, MA, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.10173.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#agents",
                    "#dataset"
                ],
                "emoji": "ğŸ§¬",
                "ru": {
                    "title": "VibeGen: Ğ˜Ğ˜-Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½ Ğ±ĞµĞ»ĞºĞ¾Ğ² Ñ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¾Ğ¹",
                    "desc": "VibeGen - ÑÑ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ˜Ğ˜-ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ»ĞºĞ¾Ğ² Ñ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğ¾Ğ¹Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ-Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½ĞµÑ€Ğ¾Ğ¼, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‰ĞµĞ¹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±ĞµĞ»ĞºĞ¾Ğ², Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ-Ğ¿Ñ€ĞµĞ´Ğ¸ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ¼, Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‰ĞµĞ¹ Ğ¸Ñ… Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ. VibeGen ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ de novo Ğ±ĞµĞ»ĞºĞ¸ Ñ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ¼Ğ¸ ĞºĞ¾Ğ»ĞµĞ±Ğ°Ğ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ÑÑ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ°Ñ‚Ğ¾Ğ¼Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼. Ğ­Ñ‚Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸ Ğ±Ğ¸Ğ¾Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ» Ñ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Unlocking Dynamic Protein Design with VibeGen",
                    "desc": "This paper presents VibeGen, a generative AI framework designed for creating proteins with specific dynamic properties. It utilizes a dual-model architecture that includes a protein designer to generate sequences based on desired vibrational modes and a protein predictor to assess their dynamic accuracy. The framework successfully integrates protein dynamics into the design process, allowing for the creation of novel protein sequences that do not resemble existing natural proteins. This innovation opens new avenues for engineering proteins with tailored functions and dynamics, which could significantly impact fields like enzyme design and biomaterials."
                },
                "zh": {
                    "title": "åŠ¨æ€é©±åŠ¨çš„è›‹ç™½è´¨è®¾è®¡æ–°æ–¹æ³•",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºVibeGençš„ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½æ¡†æ¶ï¼Œç”¨äºè®¾è®¡å…·æœ‰ç‰¹å®šåŠ¨æ€ç‰¹æ€§çš„è›‹ç™½è´¨ã€‚VibeGenç»“åˆäº†è›‹ç™½è´¨è®¾è®¡å™¨å’Œè›‹ç™½è´¨é¢„æµ‹å™¨ï¼Œå‰è€…æ ¹æ®æŒ‡å®šçš„æŒ¯åŠ¨æ¨¡å¼ç”Ÿæˆåºåˆ—å€™é€‰ï¼Œåè€…è¯„ä¼°å…¶åŠ¨æ€å‡†ç¡®æ€§ã€‚é€šè¿‡å…¨åŸå­åˆ†å­æ¨¡æ‹ŸéªŒè¯ï¼Œè®¾è®¡çš„è›‹ç™½è´¨èƒ½å¤Ÿå‡†ç¡®å†ç°é¢„å®šçš„æ­£å¸¸æ¨¡å¼æŒ¯å¹…ï¼Œå¹¶é‡‡ç”¨å¤šç§ç¨³å®šçš„ã€åŠŸèƒ½ç›¸å…³çš„ç»“æ„ã€‚è¯¥æ–¹æ³•ä¸ä»…æ‰©å±•äº†å¯è®¾è®¡è›‹ç™½è´¨çš„ç©ºé—´ï¼Œè¿˜ä¸ºçµæ´»é…¶ã€åŠ¨æ€æ”¯æ¶å’Œç”Ÿç‰©ææ–™çš„ç†æ€§è®¾è®¡æä¾›äº†æ–°çš„é€”å¾„ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-02-14.html",
    "link_next": "2025-02-18.html",
    "link_month": "2025-02.html",
    "short_date_prev": {
        "ru": "14.02",
        "en": "02/14",
        "zh": "2æœˆ14æ—¥"
    },
    "short_date_next": {
        "ru": "18.02",
        "en": "02/18",
        "zh": "2æœˆ18æ—¥"
    },
    "categories": {
        "#dataset": 7,
        "#data": 4,
        "#benchmark": 7,
        "#agents": 4,
        "#cv": 6,
        "#rl": 1,
        "#rlhf": 2,
        "#rag": 2,
        "#plp": 0,
        "#inference": 3,
        "#3d": 1,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 4,
        "#math": 1,
        "#multilingual": 3,
        "#architecture": 9,
        "#healthcare": 0,
        "#training": 14,
        "#robotics": 0,
        "#agi": 1,
        "#games": 2,
        "#interpretability": 3,
        "#reasoning": 4,
        "#transfer_learning": 3,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 1,
        "#optimization": 9,
        "#survey": 0,
        "#diffusion": 6,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 2,
        "#science": 1,
        "#low_resource": 2
    },
    "zh": {
        "text": "æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨å„ç§é¢†åŸŸçš„ç”Ÿæˆä»»åŠ¡ä¸­æˆä¸ºé¦–é€‰ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä¾èµ–å¤šæ¬¡é¡ºåºå‰å‘ä¼ é€’ï¼Œæ˜¾è‘—é™åˆ¶äº†å®æ—¶æ€§èƒ½ã€‚ä»¥å‰çš„åŠ é€Ÿæ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å‡å°‘é‡‡æ ·æ­¥éª¤æˆ–é‡ç”¨ä¸­é—´ç»“æœï¼Œæœªèƒ½åˆ©ç”¨å›¾åƒå†…éƒ¨ç©ºé—´åŒºåŸŸçš„å˜åŒ–ã€‚é€šè¿‡åˆ©ç”¨æ‰©æ•£å˜å‹å™¨ï¼ˆDiTsï¼‰å¤„ç†å¯å˜æ•°é‡çš„æ ‡è®°çš„çµæ´»æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†RASï¼Œä¸€ç§æ–°çš„æ— éœ€è®­ç»ƒçš„é‡‡æ ·ç­–ç•¥ï¼Œæ ¹æ®DiTæ¨¡å‹çš„å…³æ³¨ç‚¹åŠ¨æ€åˆ†é…å›¾åƒå†…ä¸åŒåŒºåŸŸçš„é‡‡æ ·æ¯”ç‡ã€‚æˆ‘ä»¬çš„å…³é”®è§‚å¯Ÿæ˜¯ï¼Œåœ¨æ¯ä¸ªé‡‡æ ·æ­¥éª¤ä¸­ï¼Œæ¨¡å‹é›†ä¸­åœ¨è¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„åŒºåŸŸï¼Œè¿™äº›å…³æ³¨åŒºåŸŸåœ¨è¿ç»­æ­¥éª¤ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„è¿ç»­æ€§ã€‚åˆ©ç”¨è¿™ä¸€æ´å¯Ÿï¼ŒRASä»…æ›´æ–°å½“å‰å…³æ³¨çš„åŒºåŸŸï¼Œè€Œå…¶ä»–åŒºåŸŸä½¿ç”¨ä¸Šä¸€æ­¥çš„ç¼“å­˜å™ªå£°æ›´æ–°ã€‚æ¨¡å‹çš„å…³æ³¨ç‚¹æ ¹æ®å‰ä¸€æ­¥çš„è¾“å‡ºç¡®å®šï¼Œåˆ©ç”¨äº†æˆ‘ä»¬è§‚å¯Ÿåˆ°çš„æ—¶é—´ä¸€è‡´æ€§ã€‚æˆ‘ä»¬åœ¨Stable Diffusion 3å’ŒLumina-Next-T2Iä¸Šè¯„ä¼°RASï¼Œåˆ†åˆ«å®ç°äº†æœ€é«˜2.36å€å’Œ2.51å€çš„åŠ é€Ÿï¼Œç”Ÿæˆè´¨é‡ä»…è½»å¾®ä¸‹é™ã€‚æ­¤å¤–ï¼Œç”¨æˆ·ç ”ç©¶è¡¨æ˜ï¼ŒRASåœ¨äººç±»è¯„ä¼°ä¸‹æä¾›äº†ç›¸ä¼¼çš„è´¨é‡ï¼ŒåŒæ—¶å®ç°äº†1.6å€çš„åŠ é€Ÿã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ›´é«˜æ•ˆçš„æ‰©æ•£å˜å‹å™¨æ–¹é¢å–å¾—äº†é‡è¦è¿›å±•ï¼Œå¢å¼ºäº†å®ƒä»¬åœ¨å®æ—¶åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚",
        "title": "Region-Adaptive Sampling for Diffusion Transformers",
        "pinyin": "æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨å„ç§é¢†åŸŸçš„ç”Ÿæˆä»»åŠ¡ä¸­æˆä¸ºé¦–é€‰ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä¾èµ–å¤šæ¬¡é¡ºåºå‰å‘ä¼ é€’ï¼Œæ˜¾è‘—é™åˆ¶äº†å®æ—¶æ€§èƒ½ã€‚ä»¥å‰çš„åŠ é€Ÿæ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å‡å°‘é‡‡æ ·æ­¥éª¤æˆ–é‡ç”¨ä¸­é—´ç»“æœï¼Œæœªèƒ½åˆ©ç”¨å›¾åƒå†…éƒ¨ç©ºé—´åŒºåŸŸçš„å˜åŒ–ã€‚é€šè¿‡åˆ©ç”¨æ‰©æ•£å˜å‹å™¨ï¼ˆDiTsï¼‰å¤„ç†å¯å˜æ•°é‡çš„æ ‡è®°çš„çµæ´»æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†RASï¼Œä¸€ç§æ–°çš„æ— éœ€è®­ç»ƒçš„é‡‡æ ·ç­–ç•¥ï¼Œæ ¹æ®DiTæ¨¡å‹çš„å…³æ³¨ç‚¹åŠ¨æ€åˆ†é…å›¾åƒå†…ä¸åŒåŒºåŸŸçš„é‡‡æ ·æ¯”ç‡ã€‚æˆ‘ä»¬çš„å…³é”®è§‚å¯Ÿæ˜¯ï¼Œåœ¨æ¯ä¸ªé‡‡æ ·æ­¥éª¤ä¸­ï¼Œæ¨¡å‹é›†ä¸­åœ¨è¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„åŒºåŸŸï¼Œè¿™äº›å…³æ³¨åŒºåŸŸåœ¨è¿ç»­æ­¥éª¤ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„è¿ç»­æ€§ã€‚åˆ©ç”¨è¿™ä¸€æ´å¯Ÿï¼ŒRASä»…æ›´æ–°å½“å‰å…³æ³¨çš„åŒºåŸŸï¼Œè€Œå…¶ä»–åŒºåŸŸä½¿ç”¨ä¸Šä¸€æ­¥çš„ç¼“å­˜å™ªå£°æ›´æ–°ã€‚æ¨¡å‹çš„å…³æ³¨ç‚¹æ ¹æ®å‰ä¸€æ­¥çš„è¾“å‡ºç¡®å®šï¼Œåˆ©ç”¨äº†æˆ‘ä»¬è§‚å¯Ÿåˆ°çš„æ—¶é—´ä¸€è‡´æ€§ã€‚æˆ‘ä»¬åœ¨Stable Diffusion 3å’ŒLumina-Next-T2Iä¸Šè¯„ä¼°RASï¼Œåˆ†åˆ«å®ç°äº†æœ€é«˜2.36å€å’Œ2.51å€çš„åŠ é€Ÿï¼Œç”Ÿæˆè´¨é‡ä»…è½»å¾®ä¸‹é™ã€‚æ­¤å¤–ï¼Œç”¨æˆ·ç ”ç©¶è¡¨æ˜ï¼ŒRASåœ¨äººç±»è¯„ä¼°ä¸‹æä¾›äº†ç›¸ä¼¼çš„è´¨é‡ï¼ŒåŒæ—¶å®ç°äº†1.6å€çš„åŠ é€Ÿã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ›´é«˜æ•ˆçš„æ‰©æ•£å˜å‹å™¨æ–¹é¢å–å¾—äº†é‡è¦è¿›å±•ï¼Œå¢å¼ºäº†å®ƒä»¬åœ¨å®æ—¶åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚\n\nkuÃ² sÃ n mÃ³ xÃ­ng (DMs) zÃ i gÃ¨ zhÇ’ng lÇng yÃ¹ de shÄ“ng chÃ©ng rÃ¨n wÃ¹ zhÅng chÃ©ng wÃ©i shÇ’u xuÇn. rÃ¡n Ã©r, tÄ men yÄ« lÃ i duÅ cÃ¬ shÃ¹n xÃ¹ qiÃ¡n xiÄng chuÃ¡n dÃ¬, xiÇn zhÃ¹ xiÃ n zhÃ¬ le shÃ­ shÃ­ xÃ¬ng nÃ©ng. yÇ qiÃ¡n de jiÄ sÃ¹ fÄng fÇ zhÇ” yÃ o jÄ« zhÅng zÃ i jiÇn shÇo cÇi yÃ ng bÃ¹ zhÃ²u huÃ² chÃ³ng yÃ²ng zhÅng jiÄn jiÃ© guÇ’, wÃ¨i nÃ©ng lÃ¬ yÃ²ng tÃº xiÃ ng nÃ¨i bÃ¹ kÅng jiÄn qÅ« yÃ¹ de biÃ n huÃ . tÅng guÃ² lÃ¬ yÃ²ng kuÃ² sÃ n biÃ n shÅ« zhÇ” (DiTs) chÇ” lÇ kÄ› biÃ n shÃ¹ liÃ ng de biÄo jÃ¬ de lÃ­ng huÃ³ xÃ¬ng, wÇ’ men yÇn rÃ¹ le RAS, yÄ« zhÇ’ng xÄ«n de wÃº xÅ« xÃ¹n liÃ n de cÇi yÃ ng cÃ¨ lÃ¼Ã¨, gÄ“n jÃ¹ DiT mÃ³ xÃ­ng de guÄn zhÃ¹ diÇn dÃ²ng tÃ i fÄ“n pÃ¨i tÃº xiÃ ng nÃ¨i bÃ¹ tÅng qÅ« yÃ¹ de cÇi yÃ ng bÇ lÇœ. wÇ’ men de guÇn jiÃ n guÄn chÃ¡ shÃ¬, zÃ i mÄ›i gÃ¨ cÇi yÃ ng bÃ¹ zhÃ²u zhÅng, mÃ³ xÃ­ng jÃ­ zhÅng zÃ i yÇ” yÃ¬ shÃ ng yÇ’u yÃ¬ yÃ¬ de qÅ« yÃ¹, zhÃ¨ xiÄ“ guÄn zhÃ¹ qÅ« yÃ¹ zÃ i liÃ¡n xÃ¹ bÃ¹ zhÃ²u zhÅng biÇo xiÃ n chÅ« qiÃ¡ng dÃ  de liÃ¡n xÃ¹ xÃ¬ng. lÃ¬ yÃ²ng zhÃ¨ yÄ« dÃ²ng chÃ¡, RAS jÇn gÄ“ng xÄ«n shÇ dÄng qiÃ¡n guÄn zhÃ¹ de qÅ« yÃ¹, Ã©r qÃ­ tÄ qÅ« yÃ¹ shÇ yÃ²ng shÃ ng yÄ« bÃ¹ de huÇn cÃ¹n zÃ o shÄ“ng gÄ“ng xÄ«n. mÃ³ xÃ­ng de guÄn zhÃ¹ diÇn gÄ“n jÃ¹ qiÃ¡n yÄ« bÃ¹ de shÅ« chÅ« quÃ¨ dÃ¬ng, lÃ¬ yÃ²ng le wÇ’ men guÄn chÃ¡ dÃ o de shÃ­ jiÄn yÄ« zhÃ¬ xÃ¬ng. wÇ’ men zÃ i Stable Diffusion 3 hÃ© Lumina-Next-T2I shÃ ng pÃ­ng guÇ RAS, fÄ“n biÃ© shÃ­ xiÃ n le zuÃ¬ gÄo 2.36 bÃ¨i hÃ© 2.51 bÃ¨i de jiÄ sÃ¹, shÄ“ng chÃ©ng zhÃ¬ liÃ ng jÇn qÄ«ng wÄ“i xiÃ  jiÃ ng. cÇ wÃ i, yÃ²ng hÃ¹ yÃ¡n jiÅ« biÇo mÃ­ng, RAS zÃ i rÃ©n lÃ¨i pÃ­ng jiÃ  xiÃ  tÃ­ gÅng le xiÄng sÃ¬ de zhÃ¬ liÃ ng, tÃ³ng shÃ­ shÃ­ xiÃ n le 1.6 bÃ¨i de jiÄ sÃ¹. wÇ’ men de fÄng fÇ zÃ i gÃ¨ng gÄo xiÃ o de kuÃ² sÃ n biÃ n shÅ« zhÇ” fÄng miÃ n zhÇ” dÃ© dÃ o le zhÃ²ng yÃ o jÃ¬n zhÇn, zÄ“ng qiÃ¡ng le tÄ men zÃ i shÃ­ shÃ­ yÃ¬ng yÃ²ng zhÅng de qiÃ¡n lÃ¬.",
        "vocab": "[\n    {\"word\": \"æ‰©æ•£æ¨¡å‹\", \"pinyin\": \"kuÃ² sÃ n mÃ³ xÃ­ng\", \"trans\": \"diffusion model\"},\n    {\"word\": \"é¦–é€‰\", \"pinyin\": \"shÇ’u xuÇn\", \"trans\": \"preferred choice\"},\n    {\"word\": \"ä¾èµ–\", \"pinyin\": \"yÄ« lÃ i\", \"trans\": \"depend on\"},\n    {\"word\": \"é¡ºåº\", \"pinyin\": \"shÃ¹n xÃ¹\", \"trans\": \"sequential\"},\n    {\"word\": \"å‰å‘ä¼ é€’\", \"pinyin\": \"qiÃ¡n xiÃ ng chuÃ¡n dÃ¬\", \"trans\": \"forward pass\"},\n    {\"word\": \"æ˜¾è‘—\", \"pinyin\": \"xiÇn zhÃ¹\", \"trans\": \"significant\"},\n    {\"word\": \"é™åˆ¶\", \"pinyin\": \"xiÃ n zhÃ¬\", \"trans\": \"limit\"},\n    {\"word\": \"å®æ—¶æ€§èƒ½\", \"pinyin\": \"shÃ­ shÃ­ xÃ¬ng nÃ©ng\", \"trans\": \"real-time performance\"},\n    {\"word\": \"åŠ é€Ÿ\", \"pinyin\": \"jiÄ sÃ¹\", \"trans\": \"accelerate\"},\n    {\"word\": \"æ–¹æ³•\", \"pinyin\": \"fÄng fÇ\", \"trans\": \"method\"},\n    {\"word\": \"é›†ä¸­\", \"pinyin\": \"jÃ­ zhÅng\", \"trans\": \"focus on\"},\n    {\"word\": \"å‡å°‘\", \"pinyin\": \"jiÇn shÇo\", \"trans\": \"reduce\"},\n    {\"word\": \"é‡‡æ ·æ­¥éª¤\", \"pinyin\": \"cÇi yÃ ng bÃ¹ zhÃ²u\", \"trans\": \"sampling steps\"},\n    {\"word\": \"é‡ç”¨\", \"pinyin\": \"chÃ³ng yÃ²ng\", \"trans\": \"reuse\"},\n    {\"word\": \"ä¸­é—´ç»“æœ\", \"pinyin\": \"zhÅng jiÄn jiÃ© guÇ’\", \"trans\": \"intermediate results\"},\n    {\"word\": \"åˆ©ç”¨\", \"pinyin\": \"lÃ¬ yÃ²ng\", \"trans\": \"utilize\"},\n    {\"word\": \"å›¾åƒ\", \"pinyin\": \"tÃº xiÃ ng\", \"trans\": \"image\"},\n    {\"word\": \"å†…éƒ¨ç©ºé—´åŒºåŸŸ\", \"pinyin\": \"nÃ¨i bÃ¹ kÅng jiÄn qÅ« yÃ¹\", \"trans\": \"internal spatial regions\"},\n    {\"word\": \"å˜åŒ–\", \"pinyin\": \"biÃ n huÃ \", \"trans\": \"change\"},\n    {\"word\": \"æ‰©æ•£å˜å‹å™¨\", \"pinyin\": \"kuÃ² sÃ n biÃ n yÄ qÃ¬\", \"trans\": \"diffusion transformer\"},\n    {\"word\": \"çµæ´»æ€§\", \"pinyin\": \"lÃ­ng huÃ³ xÃ¬ng\", \"trans\": \"flexibility\"},\n    {\"word\": \"å¼•å…¥\", \"pinyin\": \"yÇn rÃ¹\", \"trans\": \"introduce\"},\n    {\"word\": \"RAS\", \"pinyin\": \"RAS\", \"trans\": \"RAS\"},\n    {\"word\": \"é‡‡æ ·ç­–ç•¥\", \"pinyin\": \"cÇi yÃ ng cÃ¨ lÃ¼Ã¨\", \"trans\": \"sampling strategy\"},\n    {\"word\": \"åŠ¨æ€åˆ†é…\", \"pinyin\": \"dÃ²ng tÃ i fÄ“n pÃ¨i\", \"trans\": \"dynamic allocation\"},\n    {\"word\": \"å…³æ³¨ç‚¹\", \"pinyin\": \"guÄn zhÃ¹ diÇn\", \"trans\": \"focus points\"},\n    {\"word\": \"å…³é”®è§‚å¯Ÿ\", \"pinyin\": \"guÇn jiÃ n guÄn chÃ¡\", \"trans\": \"key observation\"},\n    {\"word\": \"è¯­ä¹‰\", \"pinyin\": \"yÇ” yÃ¬\", \"trans\": \"semantic\"},\n    {\"word\": \"æœ‰æ„ä¹‰\", \"pinyin\": \"yÇ’u yÃ¬ yÃ¬\", \"trans\": \"meaningful\"},\n    {\"word\": \"è¿ç»­æ­¥éª¤\", \"pinyin\": \"liÃ¡n xÃ¹ bÃ¹ zhÃ²u\", \"trans\": \"continuous steps\"},\n    {\"word\": \"è¡¨ç°\", \"pinyin\": \"biÇo xiÃ n\", \"trans\": \"performance\"},\n    {\"word\": \"è¿ç»­æ€§\", \"pinyin\": \"liÃ¡n xÃ¹ xÃ¬ng\", \"trans\": \"continuity\"},\n    {\"word\": \"æ´å¯Ÿ\", \"pinyin\": \"dÃ²ng chÃ¡\", \"trans\": \"insight\"},\n    {\"word\": \"æ›´æ–°\", \"pinyin\": \"gÄ“ng xÄ«n\", \"trans\": \"update\"},\n    {\"word\": \"ç¼“å­˜å™ªå£°\", \"pinyin\": \"huÇn cÃºn zÃ o shÄ“ng\", \"trans\": \"cached noise\"},\n    {\"word\": \"ç¡®å®š\", \"pinyin\": \"quÃ¨ dÃ¬ng\", \"trans\": \"determine\"},\n    {\"word\": \"æ—¶é—´ä¸€è‡´æ€§\", \"pinyin\": \"shÃ­ jiÄn yÄ« zhÃ¬ xÃ¬ng\", \"trans\": \"temporal consistency\"},\n    {\"word\": \"è¯„ä¼°\", \"pinyin\": \"pÃ­ng gÅ«\", \"trans\": \"evaluate\"},\n    {\"word\": \"Stable Diffusion 3\", \"pinyin\": \"Stable Diffusion 3\", \"trans\": \"Stable Diffusion 3\"},\n    {\"word\": \"Lumina-Next-T2I\", \"pinyin\": \"Lumina-Next-T2I\", \"trans\": \"Lumina-Next-T2I\"},\n    {\"word\": \"å®ç°\", \"pinyin\": \"shÃ­ xiÃ n\", \"trans\": \"achieve\"},\n    {\"word\": \"åŠ é€Ÿ\", \"pinyin\": \"jiÄ sÃ¹\", \"trans\": \"acceleration\"},\n    {\"word\": \"ç”Ÿæˆè´¨é‡\", \"pinyin\": \"shÄ“ng chÃ©ng zhÃ¬ liÃ ng\", \"trans\": \"generation quality\"},\n    {\"word\": \"è½»å¾®ä¸‹é™\", \"pinyin\": \"qÄ«ng wÄ“i xiÃ  jiÃ ng\", \"trans\": \"slight decrease\"},\n    {\"word\": \"ç”¨æˆ·ç ”ç©¶\", \"pinyin\": \"yÃ²ng hÃ¹ yÃ¡n jiÅ«\", \"trans\": \"user study\"},\n    {\"word\": \"äººç±»è¯„ä¼°\", \"pinyin\": \"rÃ©n lÃ¨i pÃ­ng gÅ«\", \"trans\": \"human evaluation\"},\n    {\"word\": \"ç›¸ä¼¼\", \"pinyin\": \"xiÄng sÃ¬\", \"trans\": \"similar\"},\n    {\"word\": \"æ½œåŠ›\", \"pinyin\": \"qiÃ¡n lÃ¬\", \"trans\": \"potential\"},\n    {\"word\": \"é‡è¦è¿›å±•\", \"pinyin\": \"zhÃ²ng yÃ o jÃ¬n zhÇn\", \"trans\": \"significant progress\"}\n]",
        "trans": "Diffusion models (DMs) have become the preferred choice for generative tasks in various fields. However, they rely on multiple sequential forward passes, significantly limiting real-time performance. Previous acceleration methods have primarily focused on reducing sampling steps or reusing intermediate results, failing to leverage variations in spatial regions within images. By exploiting the flexibility of diffusion transformers (DiTs) in handling a variable number of tokens, we introduce RAS, a new training-free sampling strategy that dynamically allocates sampling ratios to different regions within an image based on the attention focus of the DiT model. Our key observation is that, at each sampling step, the model concentrates on semantically meaningful regions, and these attention regions exhibit strong continuity across consecutive steps. Leveraging this insight, RAS updates only the currently attended regions, while other regions are updated using cached noise from the previous step. The model's attention focus is determined based on the output from the previous step, utilizing the temporal consistency we observed. We evaluate RAS on Stable Diffusion 3 and Lumina-Next-T2I, achieving up to 2.36x and 2.51x speedup, respectively, with only a slight decrease in generation quality. Additionally, user studies indicate that RAS provides similar quality under human evaluation while achieving a 1.6x speedup. Our method represents a significant advancement in more efficient diffusion transformers, enhancing their potential for real-time applications.",
        "update_ts": "2025-02-17 09:12"
    }
}