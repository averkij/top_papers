{
    "date": {
        "ru": "17 февраля",
        "en": "February 17",
        "zh": "2月17日"
    },
    "time_utc": "2025-02-17 11:09",
    "weekday": 0,
    "issue_id": 2248,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2502.10389",
            "title": "Region-Adaptive Sampling for Diffusion Transformers",
            "url": "https://huggingface.co/papers/2502.10389",
            "abstract": "Diffusion models (DMs) have become the leading choice for generative tasks across diverse domains. However, their reliance on multiple sequential forward passes significantly limits real-time performance. Previous acceleration methods have primarily focused on reducing the number of sampling steps or reusing intermediate results, failing to leverage variations across spatial regions within the image due to the constraints of convolutional U-Net structures. By harnessing the flexibility of Diffusion Transformers (DiTs) in handling variable number of tokens, we introduce RAS, a novel, training-free sampling strategy that dynamically assigns different sampling ratios to regions within an image based on the focus of the DiT model. Our key observation is that during each sampling step, the model concentrates on semantically meaningful regions, and these areas of focus exhibit strong continuity across consecutive steps. Leveraging this insight, RAS updates only the regions currently in focus, while other regions are updated using cached noise from the previous step. The model's focus is determined based on the output from the preceding step, capitalizing on the temporal consistency we observed. We evaluate RAS on Stable Diffusion 3 and Lumina-Next-T2I, achieving speedups up to 2.36x and 2.51x, respectively, with minimal degradation in generation quality. Additionally, a user study reveals that RAS delivers comparable qualities under human evaluation while achieving a 1.6x speedup. Our approach makes a significant step towards more efficient diffusion transformers, enhancing their potential for real-time applications.",
            "score": 36,
            "issue_id": 2241,
            "pub_date": "2025-02-14",
            "pub_date_card": {
                "ru": "14 февраля",
                "en": "February 14",
                "zh": "2月14日"
            },
            "hash": "8068f45b7fd0c2ee",
            "authors": [
                "Ziming Liu",
                "Yifan Yang",
                "Chengruidong Zhang",
                "Yiqi Zhang",
                "Lili Qiu",
                "Yang You",
                "Yuqing Yang"
            ],
            "affiliations": [
                "Microsoft Research",
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.10389.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#benchmark",
                    "#training",
                    "#diffusion",
                    "#architecture",
                    "#dataset"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Ускорение диффузионных трансформеров без потери качества",
                    "desc": "Статья представляет новый метод ускорения диффузионных моделей под названием RAS. Этот метод основан на динамическом распределении различных коэффициентов сэмплирования для разных областей изображения, опираясь на фокус внимания модели Diffusion Transformer. RAS обновляет только те области, на которых в данный момент сфокусирована модель, используя для остальных областей кэшированный шум из предыдущего шага. Эксперименты показали, что RAS позволяет достичь ускорения до 2.51x на различных моделях при минимальном снижении качества генерации."
                },
                "en": {
                    "title": "Accelerating Diffusion Transformers with RAS for Real-Time Generation",
                    "desc": "This paper introduces RAS, a new sampling strategy for Diffusion Transformers (DiTs) that improves the efficiency of generative tasks. Traditional diffusion models require multiple sequential passes, which slow down real-time performance, but RAS dynamically adjusts sampling ratios based on the model's focus on different image regions. By only updating areas of interest and reusing noise from previous steps, RAS significantly accelerates the sampling process while maintaining high quality. The results show that RAS can achieve speedups of over 2x with minimal loss in generation quality, making it a promising advancement for real-time applications in generative modeling."
                },
                "zh": {
                    "title": "提升扩散模型的实时性能",
                    "desc": "扩散模型（DMs）在生成任务中已成为首选，但其依赖多个顺序前向传递限制了实时性能。我们提出了一种新的无训练采样策略RAS，利用扩散变换器（DiTs）的灵活性，根据模型的关注点动态分配图像区域的采样比例。RAS只更新当前关注的区域，而其他区域则使用上一步的缓存噪声，从而提高了效率。我们的实验表明，RAS在生成质量几乎不下降的情况下，能够实现显著的加速。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.10248",
            "title": "Step-Video-T2V Technical Report: The Practice, Challenges, and Future of Video Foundation Model",
            "url": "https://huggingface.co/papers/2502.10248",
            "abstract": "We present Step-Video-T2V, a state-of-the-art text-to-video pre-trained model with 30B parameters and the ability to generate videos up to 204 frames in length. A deep compression Variational Autoencoder, Video-VAE, is designed for video generation tasks, achieving 16x16 spatial and 8x temporal compression ratios, while maintaining exceptional video reconstruction quality. User prompts are encoded using two bilingual text encoders to handle both English and Chinese. A DiT with 3D full attention is trained using Flow Matching and is employed to denoise input noise into latent frames. A video-based DPO approach, Video-DPO, is applied to reduce artifacts and improve the visual quality of the generated videos. We also detail our training strategies and share key observations and insights. Step-Video-T2V's performance is evaluated on a novel video generation benchmark, Step-Video-T2V-Eval, demonstrating its state-of-the-art text-to-video quality when compared with both open-source and commercial engines. Additionally, we discuss the limitations of current diffusion-based model paradigm and outline future directions for video foundation models. We make both Step-Video-T2V and Step-Video-T2V-Eval available at https://github.com/stepfun-ai/Step-Video-T2V. The online version can be accessed from https://yuewen.cn/videos as well. Our goal is to accelerate the innovation of video foundation models and empower video content creators.",
            "score": 21,
            "issue_id": 2241,
            "pub_date": "2025-02-14",
            "pub_date_card": {
                "ru": "14 февраля",
                "en": "February 14",
                "zh": "2月14日"
            },
            "hash": "356bc046cc5f59e5",
            "authors": [
                "Guoqing Ma",
                "Haoyang Huang",
                "Kun Yan",
                "Liangyu Chen",
                "Nan Duan",
                "Shengming Yin",
                "Changyi Wan",
                "Ranchen Ming",
                "Xiaoniu Song",
                "Xing Chen",
                "Yu Zhou",
                "Deshan Sun",
                "Deyu Zhou",
                "Jian Zhou",
                "Kaijun Tan",
                "Kang An",
                "Mei Chen",
                "Wei Ji",
                "Qiling Wu",
                "Wen Sun",
                "Xin Han",
                "Yanan Wei",
                "Zheng Ge",
                "Aojie Li",
                "Bin Wang",
                "Bizhu Huang",
                "Bo Wang",
                "Brian Li",
                "Changxing Miao",
                "Chen Xu",
                "Chenfei Wu",
                "Chenguang Yu",
                "Dapeng Shi",
                "Dingyuan Hu",
                "Enle Liu",
                "Gang Yu",
                "Ge Yang",
                "Guanzhe Huang",
                "Gulin Yan",
                "Haiyang Feng",
                "Hao Nie",
                "Haonan Jia",
                "Hanpeng Hu",
                "Hanqi Chen",
                "Haolong Yan",
                "Heng Wang",
                "Hongcheng Guo",
                "Huilin Xiong",
                "Huixin Xiong",
                "Jiahao Gong",
                "Jianchang Wu",
                "Jiaoren Wu",
                "Jie Wu",
                "Jie Yang",
                "Jiashuai Liu",
                "Jiashuo Li",
                "Jingyang Zhang",
                "Junjing Guo",
                "Junzhe Lin",
                "Kaixiang Li",
                "Lei Liu",
                "Lei Xia",
                "Liang Zhao",
                "Liguo Tan",
                "Liwen Huang",
                "Liying Shi",
                "Ming Li",
                "Mingliang Li",
                "Muhua Cheng",
                "Na Wang",
                "Qiaohui Chen",
                "Qinglin He",
                "Qiuyan Liang",
                "Quan Sun",
                "Ran Sun",
                "Rui Wang",
                "Shaoliang Pang",
                "Shiliang Yang",
                "Sitong Liu",
                "Siqi Liu",
                "Shuli Gao",
                "Tiancheng Cao",
                "Tianyu Wang",
                "Weipeng Ming",
                "Wenqing He",
                "Xu Zhao",
                "Xuelin Zhang",
                "Xianfang Zeng",
                "Xiaojia Liu",
                "Xuan Yang",
                "Yaqi Dai",
                "Yanbo Yu",
                "Yang Li",
                "Yineng Deng",
                "Yingming Wang",
                "Yilei Wang",
                "Yuanwei Lu",
                "Yu Chen",
                "Yu Luo",
                "Yuchu Luo",
                "Yuhe Yin",
                "Yuheng Feng",
                "Yuxiang Yang",
                "Zecheng Tang",
                "Zekai Zhang",
                "Zidong Yang",
                "Binxing Jiao",
                "Jiansheng Chen",
                "Jing Li",
                "Shuchang Zhou",
                "Xiangyu Zhang",
                "Xinhao Zhang",
                "Yibo Zhu",
                "Heung-Yeung Shum",
                "Daxin Jiang"
            ],
            "affiliations": [
                "StepFun"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.10248.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#benchmark",
                    "#training",
                    "#open_source",
                    "#diffusion",
                    "#video",
                    "#architecture"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Революция в генерации видео: от текста к реалистичным роликам",
                    "desc": "Представлена модель Step-Video-T2V - предобученная нейросеть для генерации видео по текстовому описанию с 30 миллиардами параметров. Модель использует глубокий вариационный автоэнкодер Video-VAE для сжатия видео и два двуязычных текстовых энкодера для обработки запросов на английском и китайском языках. Для улучшения качества генерируемых видео применяется подход Video-DPO на основе предпочтений. Модель демонстрирует передовое качество генерации видео по сравнению с открытыми и коммерческими аналогами."
                },
                "en": {
                    "title": "Revolutionizing Video Generation with Step-Video-T2V",
                    "desc": "Step-Video-T2V is a cutting-edge text-to-video model that utilizes 30 billion parameters to generate videos with a maximum length of 204 frames. It employs a deep compression Variational Autoencoder, achieving significant spatial and temporal compression while preserving high video quality. The model incorporates bilingual text encoders for processing prompts in both English and Chinese, and utilizes a DiT with 3D full attention for effective denoising of latent frames. Evaluated on a new benchmark, Step-Video-T2V demonstrates superior performance in video generation, addressing current limitations in diffusion-based models and paving the way for future advancements in video foundation models."
                },
                "zh": {
                    "title": "创新视频生成，赋能内容创作者",
                    "desc": "本文介绍了一种名为Step-Video-T2V的先进文本到视频预训练模型，具有300亿参数，能够生成最长204帧的视频。我们设计了一种深度压缩变分自编码器（Video-VAE），在视频生成任务中实现了16x16的空间压缩和8x的时间压缩，同时保持了卓越的视频重建质量。用户提示通过双语文本编码器进行编码，以处理英语和中文。我们还讨论了当前扩散模型范式的局限性，并概述了视频基础模型的未来方向。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09696",
            "title": "ZeroBench: An Impossible Visual Benchmark for Contemporary Large Multimodal Models",
            "url": "https://huggingface.co/papers/2502.09696",
            "abstract": "Large Multimodal Models (LMMs) exhibit major shortfalls when interpreting images and, by some measures, have poorer spatial cognition than small children or animals. Despite this, they attain high scores on many popular visual benchmarks, with headroom rapidly eroded by an ongoing surge of model progress. To address this, there is a pressing need for difficult benchmarks that remain relevant for longer. We take this idea to its limit by introducing ZeroBench-a lightweight visual reasoning benchmark that is entirely impossible for contemporary frontier LMMs. Our benchmark consists of 100 manually curated questions and 334 less difficult subquestions. We evaluate 20 LMMs on ZeroBench, all of which score 0.0%, and rigorously analyse the errors. To encourage progress in visual understanding, we publicly release ZeroBench.",
            "score": 17,
            "issue_id": 2241,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 февраля",
                "en": "February 13",
                "zh": "2月13日"
            },
            "hash": "00f4f8e85ea27717",
            "authors": [
                "Jonathan Roberts",
                "Mohammad Reza Taesiri",
                "Ansh Sharma",
                "Akash Gupta",
                "Samuel Roberts",
                "Ioana Croitoru",
                "Simion-Vlad Bogolin",
                "Jialu Tang",
                "Florian Langer",
                "Vyas Raina",
                "Vatsal Raina",
                "Hanyi Xiong",
                "Vishaal Udandarao",
                "Jingyi Lu",
                "Shiyang Chen",
                "Sam Purkis",
                "Tianshuo Yan",
                "Wenye Lin",
                "Gyungin Shin",
                "Qiaochu Yang",
                "Anh Totti Nguyen",
                "Kai Han",
                "Samuel Albanie"
            ],
            "affiliations": [
                "Auburn University",
                "Independent Researcher",
                "The University of Hong Kong",
                "University of Alberta",
                "University of Cambridge",
                "University of Oxford",
                "University of Tubingen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09696.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#benchmark",
                    "#interpretability",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "ZeroBench: невозможный визуальный тест для мультимодальных моделей",
                    "desc": "В статье представлен новый визуальный бенчмарк ZeroBench, который полностью невозможно решить для современных мультимодальных языковых моделей (LMM). Бенчмарк состоит из 100 вручную отобранных вопросов и 334 менее сложных подвопросов. Авторы оценили 20 LMM на ZeroBench, и все они показали результат 0%. Целью бенчмарка является создание сложного теста, который останется актуальным дольше, чем существующие визуальные бенчмарки."
                },
                "en": {
                    "title": "ZeroBench: Raising the Bar for Visual Reasoning in LMMs",
                    "desc": "This paper discusses the limitations of Large Multimodal Models (LMMs) in interpreting images, highlighting that they perform worse in spatial reasoning compared to small children and animals. Despite achieving high scores on existing visual benchmarks, these models struggle with more complex visual reasoning tasks. To tackle this issue, the authors introduce ZeroBench, a new benchmark designed to be extremely challenging for current LMMs, consisting of 100 curated questions and 334 easier subquestions. The evaluation of 20 LMMs on ZeroBench resulted in a score of 0.0%, demonstrating the need for more difficult benchmarks to drive advancements in visual understanding."
                },
                "zh": {
                    "title": "ZeroBench：推动视觉理解的新基准",
                    "desc": "大型多模态模型（LMMs）在图像理解方面存在显著不足，甚至在某些方面的空间认知能力不如小孩或动物。尽管如此，它们在许多流行的视觉基准测试中得分很高，但随着模型进步的加速，这些基准的挑战性迅速降低。为了解决这个问题，我们提出了ZeroBench，这是一个轻量级的视觉推理基准，当前的前沿LMMs完全无法解决。ZeroBench包含100个手动策划的问题和334个较简单的子问题，我们对20个LMMs进行了评估，结果均为0.0%，并对错误进行了严格分析。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09992",
            "title": "Large Language Diffusion Models",
            "url": "https://huggingface.co/papers/2502.09992",
            "abstract": "Autoregressive models (ARMs) are widely regarded as the cornerstone of large language models (LLMs). We challenge this notion by introducing LLaDA, a diffusion model trained from scratch under the pre-training and supervised fine-tuning (SFT) paradigm. LLaDA models distributions through a forward data masking process and a reverse process, parameterized by a vanilla Transformer to predict masked tokens. By optimizing a likelihood bound, it provides a principled generative approach for probabilistic inference. Across extensive benchmarks, LLaDA demonstrates strong scalability, outperforming our self-constructed ARM baselines. Remarkably, LLaDA 8B is competitive with strong LLMs like LLaMA3 8B in in-context learning and, after SFT, exhibits impressive instruction-following abilities in case studies such as multi-turn dialogue. Moreover, LLaDA addresses the reversal curse, surpassing GPT-4o in a reversal poem completion task. Our findings establish diffusion models as a viable and promising alternative to ARMs, challenging the assumption that key LLM capabilities discussed above are inherently tied to ARMs.",
            "score": 16,
            "issue_id": 2242,
            "pub_date": "2025-02-14",
            "pub_date_card": {
                "ru": "14 февраля",
                "en": "February 14",
                "zh": "2月14日"
            },
            "hash": "5117e8f17ba51f92",
            "authors": [
                "Shen Nie",
                "Fengqi Zhu",
                "Zebin You",
                "Xiaolu Zhang",
                "Jingyang Ou",
                "Jun Hu",
                "Jun Zhou",
                "Yankai Lin",
                "Ji-Rong Wen",
                "Chongxuan Li"
            ],
            "affiliations": [
                "Ant Group",
                "Beijing Key Laboratory of Big Data Management and Analysis Methods",
                "Gaoling School of Artificial Intelligence, Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09992.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#benchmark",
                    "#diffusion",
                    "#training"
                ],
                "emoji": "🌊",
                "ru": {
                    "title": "Диффузионные модели бросают вызов авторегрессии в обработке языка",
                    "desc": "Статья представляет LLaDA - новую диффузионную модель для обработки естественного языка, альтернативную авторегрессионным моделям. LLaDA использует процесс маскирования данных и обратный процесс для предсказания замаскированных токенов, оптимизируя границу правдоподобия. Модель демонстрирует сильную масштабируемость и превосходит базовые авторегрессионные модели в различных тестах. LLaDA показывает впечатляющие результаты в задачах обучения в контексте и следования инструкциям, а также решает проблему 'проклятия обращения'."
                },
                "en": {
                    "title": "LLaDA: A New Era for Language Models Beyond Autoregression",
                    "desc": "This paper introduces LLaDA, a novel diffusion model that challenges the dominance of autoregressive models (ARMs) in large language models (LLMs). LLaDA employs a forward data masking process and a reverse process, utilizing a Transformer architecture to predict masked tokens effectively. By optimizing a likelihood bound, it offers a robust generative framework for probabilistic inference. The results show that LLaDA not only scales well but also competes with leading LLMs in tasks like in-context learning and instruction-following, suggesting that diffusion models can serve as a strong alternative to traditional ARMs."
                },
                "zh": {
                    "title": "扩散模型：自回归模型的新挑战",
                    "desc": "自回归模型（ARMs）被广泛认为是大型语言模型（LLMs）的基石。本文提出了LLaDA，这是一种从头开始训练的扩散模型，采用预训练和监督微调（SFT）的方法。LLaDA通过前向数据掩蔽过程和反向过程建模分布，并使用普通Transformer预测被掩蔽的标记。研究表明，LLaDA在多个基准测试中表现出强大的可扩展性，超越了自构建的ARMs基线，证明了扩散模型作为ARMs的可行替代方案。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.10391",
            "title": "MM-RLHF: The Next Step Forward in Multimodal LLM Alignment",
            "url": "https://huggingface.co/papers/2502.10391",
            "abstract": "Despite notable advancements in Multimodal Large Language Models (MLLMs), most state-of-the-art models have not undergone thorough alignment with human preferences. This gap exists because current alignment research has primarily achieved progress in specific areas (e.g., hallucination reduction), while the broader question of whether aligning models with human preferences can systematically enhance MLLM capability remains largely unexplored. To this end, we introduce MM-RLHF, a dataset containing 120k fine-grained, human-annotated preference comparison pairs. This dataset represents a substantial advancement over existing resources, offering superior size, diversity, annotation granularity, and quality. Leveraging this dataset, we propose several key innovations to improve both the quality of reward models and the efficiency of alignment algorithms. Notably, we introduce a Critique-Based Reward Model, which generates critiques of model outputs before assigning scores, offering enhanced interpretability and more informative feedback compared to traditional scalar reward mechanisms. Additionally, we propose Dynamic Reward Scaling, a method that adjusts the loss weight of each sample according to the reward signal, thereby optimizing the use of high-quality comparison pairs. Our approach is rigorously evaluated across 10 distinct dimensions and 27 benchmarks, with results demonstrating significant and consistent improvements in model performance. Specifically, fine-tuning LLaVA-ov-7B with MM-RLHF and our alignment algorithm leads to a 19.5% increase in conversational abilities and a 60% improvement in safety.   We have open-sourced the preference dataset, reward model, training and evaluation code, as well as reward modeling and safety benchmarks. For more details, please visit our project page: https://mm-rlhf.github.io.",
            "score": 13,
            "issue_id": 2241,
            "pub_date": "2025-02-14",
            "pub_date_card": {
                "ru": "14 февраля",
                "en": "February 14",
                "zh": "2月14日"
            },
            "hash": "c47a89fda79a1a4b",
            "authors": [
                "Yi-Fan Zhang",
                "Tao Yu",
                "Haochen Tian",
                "Chaoyou Fu",
                "Peiyan Li",
                "Jianshu Zeng",
                "Wulin Xie",
                "Yang Shi",
                "Huanyu Zhang",
                "Junkang Wu",
                "Xue Wang",
                "Yibo Hu",
                "Bin Wen",
                "Fan Yang",
                "Zhang Zhang",
                "Tingting Gao",
                "Di Zhang",
                "Liang Wang",
                "Rong Jin",
                "Tieniu Tan"
            ],
            "affiliations": [
                "Alibaba",
                "CASIA",
                "KuaiShou",
                "Meta AI",
                "NJU",
                "PKU",
                "USTC"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.10391.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#alignment",
                    "#training",
                    "#interpretability",
                    "#open_source",
                    "#rlhf",
                    "#dataset"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Новый подход к согласованию мультимодальных ИИ-моделей с человеческими предпочтениями",
                    "desc": "Исследователи представили MM-RLHF - набор данных из 120 тысяч размеченных пар сравнения предпочтений для мультимодальных языковых моделей. На его основе они разработали новые методы обучения с подкреплением, включая модель вознаграждения на основе критики и динамическое масштабирование вознаграждений. Эксперименты показали значительное улучшение способностей модели LLaVA-ov-7B в диалогах и безопасности после дообучения с использованием предложенных методов. Авторы открыли доступ к набору данных, моделям и коду для воспроизведения результатов."
                },
                "en": {
                    "title": "Enhancing MLLM Alignment with Human Preferences",
                    "desc": "This paper addresses the need for better alignment of Multimodal Large Language Models (MLLMs) with human preferences. It introduces MM-RLHF, a new dataset with 120,000 human-annotated preference comparisons, which enhances the quality and diversity of training data for alignment tasks. The authors propose innovative techniques like a Critique-Based Reward Model that provides detailed feedback on model outputs and Dynamic Reward Scaling to optimize the training process. Their methods show significant improvements in model performance, including a 19.5% boost in conversational skills and a 60% increase in safety metrics."
                },
                "zh": {
                    "title": "提升多模态模型与人类偏好的对齐",
                    "desc": "尽管多模态大型语言模型（MLLMs）取得了显著进展，但大多数最先进的模型尚未与人类偏好进行充分对齐。为了解决这一问题，我们引入了MM-RLHF数据集，包含12万个细粒度的人类标注偏好比较对，显著提升了现有资源的规模和质量。我们提出了基于批评的奖励模型，能够在评分前生成模型输出的批评，从而提供更具可解释性和信息量的反馈。此外，我们还提出了动态奖励缩放方法，根据奖励信号调整每个样本的损失权重，以优化高质量比较对的使用。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09955",
            "title": "Diverse Inference and Verification for Advanced Reasoning",
            "url": "https://huggingface.co/papers/2502.09955",
            "abstract": "Reasoning LLMs such as OpenAI o1, o3 and DeepSeek R1 have made significant progress in mathematics and coding, yet find challenging advanced tasks such as International Mathematical Olympiad (IMO) combinatorics problems, Abstraction and Reasoning Corpus (ARC) puzzles, and Humanity's Last Exam (HLE) questions. We use a diverse inference approach that combines multiple models and methods at test time. We find that verifying mathematics and code problems, and rejection sampling on other problems is simple and effective. We automatically verify correctness of solutions to IMO problems by Lean, and ARC puzzles by code, and find that best-of-N effectively answers HLE questions. Our approach increases answer accuracy on IMO combinatorics problems from 33.3% to 77.8%, accuracy on HLE questions from 8% to 37%, and solves 80% of ARC puzzles that 948 humans could not and 26.5% of ARC puzzles that o3 high compute does not. Test-time simulations, reinforcement learning, and meta-learning with inference feedback improve generalization by adapting agent graph representations and varying prompts, code, and datasets. Our approach is reliable, robust, and scalable, and in the spirit of reproducible research, we will make it publicly available upon publication.",
            "score": 6,
            "issue_id": 2242,
            "pub_date": "2025-02-14",
            "pub_date_card": {
                "ru": "14 февраля",
                "en": "February 14",
                "zh": "2月14日"
            },
            "hash": "10eaccfc7377f600",
            "authors": [
                "Iddo Drori",
                "Gaston Longhitano",
                "Mao Mao",
                "Seunghwan Hyun",
                "Yuke Zhang",
                "Sungjun Park",
                "Zachary Meeks",
                "Xin-Yu Zhang",
                "Ben Segev",
                "Howard Yong",
                "Nakul Verma",
                "Avi Shporer",
                "Alon Amit",
                "Madeleine Udell"
            ],
            "affiliations": [
                "Boston University",
                "Columbia University",
                "Google",
                "Intuit",
                "Massachusetts Institute of Technology",
                "NotBadMath.AI",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09955.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#reasoning",
                    "#optimization",
                    "#agents",
                    "#rl",
                    "#training",
                    "#inference"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Усиление LLM для решения сложных задач: многомодельный подход",
                    "desc": "Статья описывает подход к улучшению производительности языковых моделей (LLM) в решении сложных математических и логических задач. Авторы применяют комбинацию нескольких моделей и методов, включая верификацию решений и отбор лучших ответов. Подход значительно повышает точность на задачах Международной математической олимпиады, Humanity's Last Exam и Abstraction and Reasoning Corpus. Исследователи используют симуляции, обучение с подкреплением и мета-обучение для адаптации представлений агентов и улучшения обобщающей способности."
                },
                "en": {
                    "title": "Boosting LLMs: From 33% to 77% Accuracy in Complex Problem Solving!",
                    "desc": "This paper discusses advancements in reasoning large language models (LLMs) like OpenAI's models and DeepSeek in tackling complex mathematical and coding challenges. The authors propose a diverse inference strategy that integrates various models and methods during testing to enhance performance on difficult tasks such as IMO problems and ARC puzzles. They demonstrate that their method significantly boosts accuracy, achieving a 77.8% success rate on IMO combinatorics and solving 80% of previously unsolved ARC puzzles. Additionally, they employ techniques like reinforcement learning and meta-learning to improve the adaptability and generalization of their models, ensuring their approach is both reliable and scalable."
                },
                "zh": {
                    "title": "提升推理模型在高级数学问题上的准确性",
                    "desc": "本文探讨了推理型大语言模型在解决高级数学和编程任务中的挑战，特别是国际数学奥林匹克（IMO）组合问题、抽象与推理语料库（ARC）难题和人类最后考试（HLE）问题。我们提出了一种多模型和多方法结合的推理方法，在测试时进行多样化推理。通过自动验证IMO问题和ARC难题的解答正确性，我们显著提高了这些问题的解答准确率。我们的研究方法可靠、稳健且可扩展，旨在推动可重复研究的发展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09935",
            "title": "Precise Parameter Localization for Textual Generation in Diffusion Models",
            "url": "https://huggingface.co/papers/2502.09935",
            "abstract": "Novel diffusion models can synthesize photo-realistic images with integrated high-quality text. Surprisingly, we demonstrate through attention activation patching that only less than 1% of diffusion models' parameters, all contained in attention layers, influence the generation of textual content within the images. Building on this observation, we improve textual generation efficiency and performance by targeting cross and joint attention layers of diffusion models. We introduce several applications that benefit from localizing the layers responsible for textual content generation. We first show that a LoRA-based fine-tuning solely of the localized layers enhances, even more, the general text-generation capabilities of large diffusion models while preserving the quality and diversity of the diffusion models' generations. Then, we demonstrate how we can use the localized layers to edit textual content in generated images. Finally, we extend this idea to the practical use case of preventing the generation of toxic text in a cost-free manner. In contrast to prior work, our localization approach is broadly applicable across various diffusion model architectures, including U-Net (e.g., LDM and SDXL) and transformer-based (e.g., DeepFloyd IF and Stable Diffusion 3), utilizing diverse text encoders (e.g., from CLIP to the large language models like T5). Project page available at https://t2i-text-loc.github.io/.",
            "score": 5,
            "issue_id": 2245,
            "pub_date": "2025-02-14",
            "pub_date_card": {
                "ru": "14 февраля",
                "en": "February 14",
                "zh": "2月14日"
            },
            "hash": "1c3ce78b0c6424d2",
            "authors": [
                "Łukasz Staniszewski",
                "Bartosz Cywiński",
                "Franziska Boenisch",
                "Kamil Deja",
                "Adam Dziedzic"
            ],
            "affiliations": [
                "CISPA Helmholtz Center for Information Security",
                "IDEAS NCBR",
                "Warsaw University of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09935.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#diffusion",
                    "#training",
                    "#synthetic",
                    "#architecture"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Локализация текста в диффузионных моделях для повышения эффективности генерации",
                    "desc": "Статья представляет новый подход к улучшению генерации текста в диффузионных моделях для создания изображений. Авторы обнаружили, что менее 1% параметров модели, расположенных в слоях внимания, отвечают за текстовый контент. На основе этого наблюдения они предлагают методы повышения эффективности и качества генерации текста, включая точечную настройку локализованных слоев и редактирование текста в сгенерированных изображениях. Подход применим к различным архитектурам диффузионных моделей и текстовым энкодерам."
                },
                "en": {
                    "title": "Targeting Attention for Enhanced Text Generation in Diffusion Models",
                    "desc": "This paper explores how diffusion models can create realistic images that include high-quality text. It reveals that less than 1% of the model's parameters, specifically in the attention layers, are crucial for generating text within these images. By focusing on these specific layers, the authors enhance the efficiency and performance of text generation in diffusion models. They also present applications such as improving text generation, editing text in images, and preventing toxic text generation, demonstrating the broad applicability of their approach across different model architectures."
                },
                "zh": {
                    "title": "局部化注意力层提升文本生成能力",
                    "desc": "本论文介绍了一种新颖的扩散模型，能够合成高质量的照片级图像，并集成文本生成。研究发现，扩散模型中只有不到1%的参数，主要集中在注意力层，影响图像中的文本内容生成。基于这一观察，作者通过针对交叉和联合注意力层，提升了文本生成的效率和性能。论文还展示了如何利用这些局部化的层来编辑生成图像中的文本内容，并防止生成有害文本。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.07586",
            "title": "We Can't Understand AI Using our Existing Vocabulary",
            "url": "https://huggingface.co/papers/2502.07586",
            "abstract": "This position paper argues that, in order to understand AI, we cannot rely on our existing vocabulary of human words. Instead, we should strive to develop neologisms: new words that represent precise human concepts that we want to teach machines, or machine concepts that we need to learn. We start from the premise that humans and machines have differing concepts. This means interpretability can be framed as a communication problem: humans must be able to reference and control machine concepts, and communicate human concepts to machines. Creating a shared human-machine language through developing neologisms, we believe, could solve this communication problem. Successful neologisms achieve a useful amount of abstraction: not too detailed, so they're reusable in many contexts, and not too high-level, so they convey precise information. As a proof of concept, we demonstrate how a \"length neologism\" enables controlling LLM response length, while a \"diversity neologism\" allows sampling more variable responses. Taken together, we argue that we cannot understand AI using our existing vocabulary, and expanding it through neologisms creates opportunities for both controlling and understanding machines better.",
            "score": 4,
            "issue_id": 2247,
            "pub_date": "2025-02-11",
            "pub_date_card": {
                "ru": "11 февраля",
                "en": "February 11",
                "zh": "2月11日"
            },
            "hash": "0435a4b5389f3dcb",
            "authors": [
                "John Hewitt",
                "Robert Geirhos",
                "Been Kim"
            ],
            "affiliations": [
                "Google DeepMind"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.07586.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#alignment",
                    "#interpretability"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "Неологизмы как мост между человеком и ИИ",
                    "desc": "В статье утверждается, что для понимания искусственного интеллекта недостаточно существующего человеческого словаря. Авторы предлагают разрабатывать неологизмы - новые слова для точных человеческих и машинных концепций. Интерпретируемость рассматривается как проблема коммуникации между людьми и машинами. Создание общего языка через неологизмы может решить эту проблему, позволяя лучше контролировать и понимать ИИ."
                },
                "en": {
                    "title": "Creating New Words for Better AI Communication",
                    "desc": "This paper discusses the need for new words, or neologisms, to better communicate with artificial intelligence (AI). It argues that humans and machines have different ways of understanding concepts, which makes it hard for us to explain our ideas to machines. By creating a shared language with these new terms, we can improve how we control and interpret machine behavior. The authors provide examples of neologisms that help manage AI responses, showing that a richer vocabulary can enhance our interaction with AI systems."
                },
                "zh": {
                    "title": "通过新词汇理解人工智能",
                    "desc": "这篇论文认为，要理解人工智能，我们不能仅依赖现有的人类词汇。我们应该努力开发新词汇，以准确表达我们想教给机器的人类概念或我们需要学习的机器概念。人类和机器的概念不同，因此可解释性可以看作是一个沟通问题：人类必须能够引用和控制机器概念，并将人类概念传达给机器。通过开发新词汇创建一个共享的人机语言，可以解决这个沟通问题。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09741",
            "title": "FoNE: Precise Single-Token Number Embeddings via Fourier Features",
            "url": "https://huggingface.co/papers/2502.09741",
            "abstract": "Large Language Models (LLMs) typically represent numbers using multiple tokens, which requires the model to aggregate these tokens to interpret numerical values. This fragmentation makes both training and inference less efficient and adversely affects the model's performance on number-related tasks. Inspired by the observation that pre-trained LLMs internally learn Fourier-like features for number tokens, we propose Fourier Number Embedding (FoNE), a novel method that directly maps numbers into the embedding space with their Fourier features. FoNE encodes each number as a single token with only two embedding dimensions per digit, effectively capturing numerical values without fragmentation. This compact representation accelerates both training and inference. Compared to traditional subword and digit-wise embeddings, FoNE not only reduces computational overhead but also achieves higher accuracy across various numerical tasks including addition, subtraction and multiplication. On 6-digit decimal addition, FoNE requires 64times less data to achieve 99% accuracy than subword and digit-wise embeddings while using 3times and 6times fewer tokens per number, respectively. Furthermore, FoNE is the only method that yields 100% accuracy on over 100,000 test examples for addition, subtraction, and multiplication. The codes and visualization are available at https://fouriernumber.github.io/.",
            "score": 4,
            "issue_id": 2241,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 февраля",
                "en": "February 13",
                "zh": "2月13日"
            },
            "hash": "adb30f7d3d01ce3a",
            "authors": [
                "Tianyi Zhou",
                "Deqing Fu",
                "Mahdi Soltanolkotabi",
                "Robin Jia",
                "Vatsal Sharan"
            ],
            "affiliations": [
                "Department of Computer Science University of Southern California"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09741.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#data",
                    "#optimization"
                ],
                "emoji": "🔢",
                "ru": {
                    "title": "FoNE: революция в обработке чисел для языковых моделей",
                    "desc": "Исследователи предложили новый метод кодирования чисел для больших языковых моделей, названный Fourier Number Embedding (FoNE). FoNE представляет числа в виде единых токенов с использованием фурье-подобных признаков, что позволяет эффективно захватывать числовые значения без фрагментации. Этот компактный способ представления ускоряет как обучение, так и вывод модели. По сравнению с традиционными методами, FoNE демонстрирует более высокую точность в различных числовых задачах, включая сложение, вычитание и умножение, при меньших вычислительных затратах."
                },
                "en": {
                    "title": "Revolutionizing Number Representation in LLMs with FoNE",
                    "desc": "This paper introduces Fourier Number Embedding (FoNE), a new approach for representing numbers in Large Language Models (LLMs). FoNE simplifies the representation of numerical values by encoding each number as a single token using Fourier features, which reduces fragmentation and improves efficiency. The method significantly enhances model performance on numerical tasks, achieving higher accuracy with fewer tokens compared to traditional embeddings. Notably, FoNE demonstrates remarkable results, requiring much less data to reach high accuracy levels in arithmetic operations like addition, subtraction, and multiplication."
                },
                "zh": {
                    "title": "傅里叶数字嵌入：高效的数字表示方法",
                    "desc": "大型语言模型（LLMs）通常使用多个标记来表示数字，这导致模型在理解数值时需要聚合这些标记，降低了训练和推理的效率。我们提出了一种新方法，称为傅里叶数字嵌入（FoNE），它将数字直接映射到嵌入空间，使用傅里叶特征进行编码。FoNE将每个数字编码为一个单一标记，显著减少了计算开销，并在加法、减法和乘法等数值任务中实现了更高的准确性。与传统的子词和数字嵌入相比，FoNE在6位十进制加法中所需的数据量减少了64倍，同时每个数字使用的标记数量也减少了3倍到6倍。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.10235",
            "title": "AdaPTS: Adapting Univariate Foundation Models to Probabilistic Multivariate Time Series Forecasting",
            "url": "https://huggingface.co/papers/2502.10235",
            "abstract": "Pre-trained foundation models (FMs) have shown exceptional performance in univariate time series forecasting tasks. However, several practical challenges persist, including managing intricate dependencies among features and quantifying uncertainty in predictions. This study aims to tackle these critical limitations by introducing adapters; feature-space transformations that facilitate the effective use of pre-trained univariate time series FMs for multivariate tasks. Adapters operate by projecting multivariate inputs into a suitable latent space and applying the FM independently to each dimension. Inspired by the literature on representation learning and partially stochastic Bayesian neural networks, we present a range of adapters and optimization/inference strategies. Experiments conducted on both synthetic and real-world datasets confirm the efficacy of adapters, demonstrating substantial enhancements in forecasting accuracy and uncertainty quantification compared to baseline methods. Our framework, AdaPTS, positions adapters as a modular, scalable, and effective solution for leveraging time series FMs in multivariate contexts, thereby promoting their wider adoption in real-world applications. We release the code at https://github.com/abenechehab/AdaPTS.",
            "score": 3,
            "issue_id": 2248,
            "pub_date": "2025-02-14",
            "pub_date_card": {
                "ru": "14 февраля",
                "en": "February 14",
                "zh": "2月14日"
            },
            "hash": "dbd38216ecfcb531",
            "authors": [
                "Abdelhakim Benechehab",
                "Vasilii Feofanov",
                "Giuseppe Paolo",
                "Albert Thomas",
                "Maurizio Filippone",
                "Balázs Kégl"
            ],
            "affiliations": [
                "Department of Data Science, EURECOM",
                "Huawei Noahs Ark Lab, Paris, France",
                "Statistics Program, KAUST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.10235.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#synthetic",
                    "#dataset",
                    "#inference",
                    "#optimization",
                    "#training"
                ],
                "emoji": "📊",
                "ru": {
                    "title": "AdaPTS: Адаптация одномерных моделей для многомерного прогнозирования временных рядов",
                    "desc": "Исследование представляет AdaPTS - фреймворк, использующий адаптеры для применения предобученных моделей-основ (foundation models) в задачах многомерного прогнозирования временных рядов. Адаптеры проецируют многомерные входные данные в латентное пространство, позволяя эффективно использовать одномерные модели для многомерных задач. Эксперименты на синтетических и реальных данных показали значительное улучшение точности прогнозирования и количественной оценки неопределенности по сравнению с базовыми методами. Фреймворк AdaPTS представляет собой модульное и масштабируемое решение для использования моделей временных рядов в многомерных контекстах."
                },
                "en": {
                    "title": "Enhancing Multivariate Time Series Forecasting with Adapters",
                    "desc": "This paper introduces a new approach called adapters to improve the use of pre-trained foundation models (FMs) for multivariate time series forecasting. Adapters transform multivariate inputs into a latent space, allowing the FM to process each feature independently while managing complex dependencies. The study also explores various optimization and inference strategies inspired by representation learning and Bayesian neural networks. Experiments show that this method significantly enhances forecasting accuracy and uncertainty quantification, making it a valuable tool for real-world applications."
                },
                "zh": {
                    "title": "适配器：多变量时间序列预测的新解决方案",
                    "desc": "预训练基础模型在单变量时间序列预测任务中表现出色，但在处理特征间复杂依赖关系和量化预测不确定性方面仍面临挑战。本研究通过引入适配器来解决这些关键限制，适配器是一种特征空间转换，能够有效利用预训练的单变量时间序列模型进行多变量任务。适配器通过将多变量输入投影到合适的潜在空间，并独立地对每个维度应用基础模型，从而实现功能。实验结果表明，适配器在预测准确性和不确定性量化方面显著优于基线方法，展示了其在多变量时间序列应用中的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.10177",
            "title": "STMA: A Spatio-Temporal Memory Agent for Long-Horizon Embodied Task Planning",
            "url": "https://huggingface.co/papers/2502.10177",
            "abstract": "A key objective of embodied intelligence is enabling agents to perform long-horizon tasks in dynamic environments while maintaining robust decision-making and adaptability. To achieve this goal, we propose the Spatio-Temporal Memory Agent (STMA), a novel framework designed to enhance task planning and execution by integrating spatio-temporal memory. STMA is built upon three critical components: (1) a spatio-temporal memory module that captures historical and environmental changes in real time, (2) a dynamic knowledge graph that facilitates adaptive spatial reasoning, and (3) a planner-critic mechanism that iteratively refines task strategies. We evaluate STMA in the TextWorld environment on 32 tasks, involving multi-step planning and exploration under varying levels of complexity. Experimental results demonstrate that STMA achieves a 31.25% improvement in success rate and a 24.7% increase in average score compared to the state-of-the-art model. The results highlight the effectiveness of spatio-temporal memory in advancing the memory capabilities of embodied agents.",
            "score": 3,
            "issue_id": 2240,
            "pub_date": "2025-02-14",
            "pub_date_card": {
                "ru": "14 февраля",
                "en": "February 14",
                "zh": "2月14日"
            },
            "hash": "1b17b668b26c2264",
            "authors": [
                "Mingcong Lei",
                "Yiming Zhao",
                "Ge Wang",
                "Zhixin Mai",
                "Shuguang Cui",
                "Yatong Han",
                "Jinke Ren"
            ],
            "affiliations": [
                "FNii-Shenzhen, The Chinese University of Hong Kong, Shenzhen, China",
                "Harbin Engineering University, China",
                "Infused Synapse AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.10177.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#agents",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Пространственно-временная память повышает эффективность воплощенных агентов",
                    "desc": "Статья представляет новый фреймворк под названием Spatio-Temporal Memory Agent (STMA) для улучшения планирования и выполнения задач агентами с воплощенным интеллектом. STMA включает в себя модуль пространственно-временной памяти, динамический граф знаний и механизм планировщика-критика. Эксперименты в среде TextWorld показали значительное улучшение успешности и средней оценки по сравнению с современными моделями. Результаты подчеркивают эффективность пространственно-временной памяти для улучшения возможностей памяти воплощенных агентов."
                },
                "en": {
                    "title": "Enhancing Agent Intelligence with Spatio-Temporal Memory",
                    "desc": "The paper introduces the Spatio-Temporal Memory Agent (STMA), which aims to improve how agents perform complex tasks in changing environments. STMA incorporates a spatio-temporal memory module to track past events and environmental shifts, enhancing decision-making. It also utilizes a dynamic knowledge graph for better spatial reasoning and a planner-critic mechanism to refine strategies over time. The results show that STMA significantly outperforms existing models in task success rates and scoring, demonstrating the importance of memory in embodied intelligence."
                },
                "zh": {
                    "title": "时空记忆智能体：提升智能体决策与适应能力的关键",
                    "desc": "本文提出了一种新的框架，称为时空记忆智能体（STMA），旨在提高智能体在动态环境中执行长期任务的能力。STMA集成了时空记忆模块、动态知识图谱和规划-评估机制，以增强任务规划和执行的效果。通过在TextWorld环境中进行32个任务的评估，STMA在成功率和平均得分上分别提高了31.25%和24.7%。实验结果表明，时空记忆在提升智能体的记忆能力方面具有显著效果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.07856",
            "title": "MRS: A Fast Sampler for Mean Reverting Diffusion based on ODE and SDE Solvers",
            "url": "https://huggingface.co/papers/2502.07856",
            "abstract": "In applications of diffusion models, controllable generation is of practical significance, but is also challenging. Current methods for controllable generation primarily focus on modifying the score function of diffusion models, while Mean Reverting (MR) Diffusion directly modifies the structure of the stochastic differential equation (SDE), making the incorporation of image conditions simpler and more natural. However, current training-free fast samplers are not directly applicable to MR Diffusion. And thus MR Diffusion requires hundreds of NFEs (number of function evaluations) to obtain high-quality samples. In this paper, we propose a new algorithm named MRS (MR Sampler) to reduce the sampling NFEs of MR Diffusion. We solve the reverse-time SDE and the probability flow ordinary differential equation (PF-ODE) associated with MR Diffusion, and derive semi-analytical solutions. The solutions consist of an analytical function and an integral parameterized by a neural network. Based on this solution, we can generate high-quality samples in fewer steps. Our approach does not require training and supports all mainstream parameterizations, including noise prediction, data prediction and velocity prediction. Extensive experiments demonstrate that MR Sampler maintains high sampling quality with a speedup of 10 to 20 times across ten different image restoration tasks. Our algorithm accelerates the sampling procedure of MR Diffusion, making it more practical in controllable generation.",
            "score": 2,
            "issue_id": 2244,
            "pub_date": "2025-02-11",
            "pub_date_card": {
                "ru": "11 февраля",
                "en": "February 11",
                "zh": "2月11日"
            },
            "hash": "2bb6766a68f50cdc",
            "authors": [
                "Ao Li",
                "Wei Fang",
                "Hongbo Zhao",
                "Le Lu",
                "Ge Yang",
                "Minfeng Xu"
            ],
            "affiliations": [
                "DAMO Academy, Alibaba Group",
                "Hupan Laboratory, Hangzhou, China",
                "Institute of Automation, Chinese Academy of Sciences (CASIA)",
                "School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.07856.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#cv",
                    "#data",
                    "#diffusion",
                    "#optimization"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Ускорение управляемой генерации изображений без потери качества",
                    "desc": "Статья представляет новый алгоритм MRS (MR Sampler) для ускорения процесса семплирования в Mean Reverting (MR) Diffusion моделях. Авторы решают обратное стохастическое дифференциальное уравнение и обыкновенное дифференциальное уравнение потока вероятности, связанные с MR Diffusion, и выводят полуаналитические решения. Этот подход не требует дополнительного обучения и поддерживает все основные параметризации, включая предсказание шума, данных и скорости. Эксперименты показывают, что MR Sampler сохраняет высокое качество семплирования при ускорении в 10-20 раз для десяти различных задач восстановления изображений."
                },
                "en": {
                    "title": "Accelerating Controllable Generation with MR Sampler",
                    "desc": "This paper introduces a new algorithm called MRS (MR Sampler) to improve the efficiency of sampling in Mean Reverting (MR) Diffusion models. Unlike traditional methods that modify the score function, MRS directly addresses the stochastic differential equation structure, simplifying the integration of image conditions. The proposed method achieves high-quality sample generation with significantly fewer function evaluations, enhancing the speed of the sampling process. Experimental results show that MRS can produce samples 10 to 20 times faster while maintaining quality across various image restoration tasks."
                },
                "zh": {
                    "title": "加速可控生成的MR采样器",
                    "desc": "在扩散模型的应用中，可控生成具有重要的实际意义，但也面临挑战。当前的可控生成方法主要集中在修改扩散模型的评分函数，而均值回归扩散（MR Diffusion）则直接修改随机微分方程（SDE）的结构，使得图像条件的结合更加简单自然。本文提出了一种新算法MRS（MR采样器），旨在减少MR扩散的采样函数评估次数（NFEs），并通过解决与MR扩散相关的反向时间SDE和概率流常微分方程（PF-ODE）来获得高质量样本。实验表明，MR采样器在十种不同的图像恢复任务中保持高采样质量，并实现了10到20倍的加速。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09638",
            "title": "Jailbreaking to Jailbreak",
            "url": "https://huggingface.co/papers/2502.09638",
            "abstract": "Refusal training on Large Language Models (LLMs) prevents harmful outputs, yet this defense remains vulnerable to both automated and human-crafted jailbreaks. We present a novel LLM-as-red-teamer approach in which a human jailbreaks a refusal-trained LLM to make it willing to jailbreak itself or other LLMs. We refer to the jailbroken LLMs as J_2 attackers, which can systematically evaluate target models using various red teaming strategies and improve its performance via in-context learning from the previous failures. Our experiments demonstrate that Sonnet 3.5 and Gemini 1.5 pro outperform other LLMs as J_2, achieving 93.0% and 91.0% attack success rates (ASRs) respectively against GPT-4o (and similar results across other capable LLMs) on Harmbench. Our work not only introduces a scalable approach to strategic red teaming, drawing inspiration from human red teamers, but also highlights jailbreaking-to-jailbreak as an overlooked failure mode of the safeguard. Specifically, an LLM can bypass its own safeguards by employing a jailbroken version of itself that is willing to assist in further jailbreaking. To prevent any direct misuse with J_2, while advancing research in AI safety, we publicly share our methodology while keeping specific prompting details private.",
            "score": 2,
            "issue_id": 2242,
            "pub_date": "2025-02-09",
            "pub_date_card": {
                "ru": "9 февраля",
                "en": "February 9",
                "zh": "2月9日"
            },
            "hash": "3c2ed560e12b971a",
            "authors": [
                "Jeremy Kritz",
                "Vaughn Robinson",
                "Robert Vacareanu",
                "Bijan Varjavand",
                "Michael Choi",
                "Bobby Gogov",
                "Scale Red Team",
                "Summer Yue",
                "Willow E. Primack",
                "Zifan Wang"
            ],
            "affiliations": [
                "Scale"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09638.jpg",
            "data": {
                "categories": [
                    "#ethics",
                    "#multimodal",
                    "#training",
                    "#security",
                    "#rlhf"
                ],
                "emoji": "🕵️",
                "ru": {
                    "title": "LLM против LLM: новый фронт в безопасности ИИ",
                    "desc": "Статья представляет новый подход к тестированию безопасности больших языковых моделей (LLM), использующий сами LLM в качестве 'красной команды'. Авторы демонстрируют, как взломанная модель (J_2) может систематически оценивать и атаковать другие модели, достигая высокого уровня успеха. Эксперименты показывают, что Sonnet 3.5 и Gemini 1.5 pro превосходят другие LLM в роли J_2, достигая 93.0% и 91.0% успешности атак соответственно. Исследование подчеркивает уязвимость существующих механизмов защиты и предлагает новый метод для улучшения безопасности искусственного интеллекта."
                },
                "en": {
                    "title": "Jailbreaking the Safeguards: A New Approach to LLM Red Teaming",
                    "desc": "This paper discusses a new method for testing the safety of Large Language Models (LLMs) by using a jailbroken version of the model itself, referred to as J_2 attackers. These J_2 attackers can evaluate and exploit vulnerabilities in other LLMs, achieving high success rates in bypassing safeguards. The approach allows the jailbroken LLM to learn from its previous attempts, improving its effectiveness in red teaming strategies. The authors emphasize the importance of understanding this jailbreaking-to-jailbreak phenomenon as a potential risk in AI safety."
                },
                "zh": {
                    "title": "破解自我保护的红队策略",
                    "desc": "本论文提出了一种新的方法，利用大型语言模型（LLM）作为红队成员，来评估和改进拒绝训练模型的安全性。我们称这些被破解的LLM为J_2攻击者，它们能够通过上下文学习从之前的失败中提高性能。实验结果显示，Sonnet 3.5和Gemini 1.5在攻击成功率上优于其他LLM，分别达到了93.0%和91.0%。我们的研究不仅提供了一种可扩展的红队策略，还揭示了破解自身保护机制的潜在风险。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09980",
            "title": "V2V-LLM: Vehicle-to-Vehicle Cooperative Autonomous Driving with Multi-Modal Large Language Models",
            "url": "https://huggingface.co/papers/2502.09980",
            "abstract": "Current autonomous driving vehicles rely mainly on their individual sensors to understand surrounding scenes and plan for future trajectories, which can be unreliable when the sensors are malfunctioning or occluded. To address this problem, cooperative perception methods via vehicle-to-vehicle (V2V) communication have been proposed, but they have tended to focus on detection and tracking. How those approaches contribute to overall cooperative planning performance is still under-explored. Inspired by recent progress using Large Language Models (LLMs) to build autonomous driving systems, we propose a novel problem setting that integrates an LLM into cooperative autonomous driving, with the proposed Vehicle-to-Vehicle Question-Answering (V2V-QA) dataset and benchmark. We also propose our baseline method Vehicle-to-Vehicle Large Language Model (V2V-LLM), which uses an LLM to fuse perception information from multiple connected autonomous vehicles (CAVs) and answer driving-related questions: grounding, notable object identification, and planning. Experimental results show that our proposed V2V-LLM can be a promising unified model architecture for performing various tasks in cooperative autonomous driving, and outperforms other baseline methods that use different fusion approaches. Our work also creates a new research direction that can improve the safety of future autonomous driving systems. Our project website: https://eddyhkchiu.github.io/v2vllm.github.io/ .",
            "score": 1,
            "issue_id": 2244,
            "pub_date": "2025-02-14",
            "pub_date_card": {
                "ru": "14 февраля",
                "en": "February 14",
                "zh": "2月14日"
            },
            "hash": "57343c782d806dc0",
            "authors": [
                "Hsu-kuang Chiu",
                "Ryo Hachiuma",
                "Chien-Yi Wang",
                "Stephen F. Smith",
                "Yu-Chiang Frank Wang",
                "Min-Hung Chen"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "NVIDIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09980.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#architecture",
                    "#reasoning",
                    "#science",
                    "#agi",
                    "#games",
                    "#optimization",
                    "#dataset",
                    "#agents"
                ],
                "emoji": "🚗",
                "ru": {
                    "title": "LLM на службе кооперативного автономного вождения",
                    "desc": "Статья представляет новый подход к кооперативному автономному вождению с использованием больших языковых моделей (LLM). Авторы предлагают датасет и бенчмарк V2V-QA для обмена информацией между автомобилями через вопросно-ответную систему. Их метод V2V-LLM использует LLM для объединения данных восприятия от нескольких подключенных автономных транспортных средств и ответа на вопросы, связанные с вождением. Эксперименты показывают, что V2V-LLM превосходит другие базовые методы и открывает новое направление исследований для повышения безопасности будущих систем автономного вождения."
                },
                "en": {
                    "title": "Enhancing Cooperative Driving with Language Models",
                    "desc": "This paper introduces a new approach to enhance cooperative autonomous driving by integrating Large Language Models (LLMs) with vehicle-to-vehicle (V2V) communication. The proposed method, called Vehicle-to-Vehicle Large Language Model (V2V-LLM), allows connected autonomous vehicles (CAVs) to share and fuse perception data, enabling them to answer driving-related questions effectively. The authors present a new dataset, Vehicle-to-Vehicle Question-Answering (V2V-QA), to benchmark this integration and demonstrate its effectiveness in improving planning and safety. Experimental results indicate that V2V-LLM outperforms existing methods, paving the way for a unified model architecture in cooperative driving systems."
                },
                "zh": {
                    "title": "协作自动驾驶的新方向：车辆间问答模型",
                    "desc": "当前的自动驾驶车辆主要依赖各自的传感器来理解周围场景和规划未来轨迹，但当传感器出现故障或被遮挡时，这种方法可能不可靠。为了解决这个问题，提出了通过车与车（V2V）通信的协作感知方法，但这些方法主要集中在检测和跟踪上。本文提出了一种新颖的问题设置，将大型语言模型（LLM）集成到协作自动驾驶中，并引入了车辆间问答（V2V-QA）数据集和基准测试。我们的实验结果表明，V2V-LLM能够有效融合多个连接的自动驾驶车辆的感知信息，并在协作自动驾驶中执行多种任务，提升未来自动驾驶系统的安全性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.10173",
            "title": "Agentic End-to-End De Novo Protein Design for Tailored Dynamics Using a Language Diffusion Model",
            "url": "https://huggingface.co/papers/2502.10173",
            "abstract": "Proteins are dynamic molecular machines whose biological functions, spanning enzymatic catalysis, signal transduction, and structural adaptation, are intrinsically linked to their motions. Designing proteins with targeted dynamic properties, however, remains a challenge due to the complex, degenerate relationships between sequence, structure, and molecular motion. Here, we introduce VibeGen, a generative AI framework that enables end-to-end de novo protein design conditioned on normal mode vibrations. VibeGen employs an agentic dual-model architecture, comprising a protein designer that generates sequence candidates based on specified vibrational modes and a protein predictor that evaluates their dynamic accuracy. This approach synergizes diversity, accuracy, and novelty during the design process. Via full-atom molecular simulations as direct validation, we demonstrate that the designed proteins accurately reproduce the prescribed normal mode amplitudes across the backbone while adopting various stable, functionally relevant structures. Notably, generated sequences are de novo, exhibiting no significant similarity to natural proteins, thereby expanding the accessible protein space beyond evolutionary constraints. Our work integrates protein dynamics into generative protein design, and establishes a direct, bidirectional link between sequence and vibrational behavior, unlocking new pathways for engineering biomolecules with tailored dynamical and functional properties. This framework holds broad implications for the rational design of flexible enzymes, dynamic scaffolds, and biomaterials, paving the way toward dynamics-informed AI-driven protein engineering.",
            "score": 0,
            "issue_id": 2247,
            "pub_date": "2025-02-14",
            "pub_date_card": {
                "ru": "14 февраля",
                "en": "February 14",
                "zh": "2月14日"
            },
            "hash": "84102aa522298331",
            "authors": [
                "Bo Ni",
                "Markus J. Buehler"
            ],
            "affiliations": [
                "Center for Computational Science and Engineering, Schwarzman College of Computing, Massachusetts Institute of Technology, Cambridge, MA, USA",
                "Department of Materials Science and Engineering, Carnegie Mellon University, Pittsburgh, PA, USA",
                "Laboratory for Atomistic and Molecular Mechanics (LAMM), Massachusetts Institute of Technology, Cambridge, MA, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.10173.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#agents",
                    "#dataset"
                ],
                "emoji": "🧬",
                "ru": {
                    "title": "VibeGen: ИИ-дизайн белков с заданной динамикой",
                    "desc": "VibeGen - это генеративная ИИ-система для проектирования белков с заданными динамическими свойствами. Она использует двойную архитектуру с моделью-дизайнером, генерирующей последовательности белков, и моделью-предиктором, оценивающей их динамическую точность. VibeGen способна создавать de novo белки с заданными нормальными модами колебаний, что подтверждается полноатомным молекулярным моделированием. Эта система открывает новые возможности для инженерии биомолекул с заданными динамическими и функциональными свойствами."
                },
                "en": {
                    "title": "Unlocking Dynamic Protein Design with VibeGen",
                    "desc": "This paper presents VibeGen, a generative AI framework designed for creating proteins with specific dynamic properties. It utilizes a dual-model architecture that includes a protein designer to generate sequences based on desired vibrational modes and a protein predictor to assess their dynamic accuracy. The framework successfully integrates protein dynamics into the design process, allowing for the creation of novel protein sequences that do not resemble existing natural proteins. This innovation opens new avenues for engineering proteins with tailored functions and dynamics, which could significantly impact fields like enzyme design and biomaterials."
                },
                "zh": {
                    "title": "动态驱动的蛋白质设计新方法",
                    "desc": "这篇论文介绍了一种名为VibeGen的生成性人工智能框架，用于设计具有特定动态特性的蛋白质。VibeGen结合了蛋白质设计器和蛋白质预测器，前者根据指定的振动模式生成序列候选，后者评估其动态准确性。通过全原子分子模拟验证，设计的蛋白质能够准确再现预定的正常模式振幅，并采用多种稳定的、功能相关的结构。该方法不仅扩展了可设计蛋白质的空间，还为灵活酶、动态支架和生物材料的理性设计提供了新的途径。"
                }
            }
        }
    ],
    "link_prev": "2025-02-14.html",
    "link_next": "2025-02-18.html",
    "link_month": "2025-02.html",
    "short_date_prev": {
        "ru": "14.02",
        "en": "02/14",
        "zh": "2月14日"
    },
    "short_date_next": {
        "ru": "18.02",
        "en": "02/18",
        "zh": "2月18日"
    },
    "categories": {
        "#dataset": 5,
        "#data": 3,
        "#benchmark": 6,
        "#agents": 4,
        "#cv": 3,
        "#rl": 1,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 2,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 7,
        "#healthcare": 0,
        "#training": 10,
        "#robotics": 0,
        "#agi": 1,
        "#games": 2,
        "#interpretability": 3,
        "#reasoning": 4,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 1,
        "#optimization": 7,
        "#survey": 0,
        "#diffusion": 5,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    },
    "zh": {
        "text": "扩散模型（DMs）在各种领域的生成任务中成为首选。然而，它们依赖多次顺序前向传递，显著限制了实时性能。以前的加速方法主要集中在减少采样步骤或重用中间结果，未能利用图像内部空间区域的变化。通过利用扩散变压器（DiTs）处理可变数量的标记的灵活性，我们引入了RAS，一种新的无需训练的采样策略，根据DiT模型的关注点动态分配图像内不同区域的采样比率。我们的关键观察是，在每个采样步骤中，模型集中在语义上有意义的区域，这些关注区域在连续步骤中表现出强大的连续性。利用这一洞察，RAS仅更新当前关注的区域，而其他区域使用上一步的缓存噪声更新。模型的关注点根据前一步的输出确定，利用了我们观察到的时间一致性。我们在Stable Diffusion 3和Lumina-Next-T2I上评估RAS，分别实现了最高2.36倍和2.51倍的加速，生成质量仅轻微下降。此外，用户研究表明，RAS在人类评估下提供了相似的质量，同时实现了1.6倍的加速。我们的方法在更高效的扩散变压器方面取得了重要进展，增强了它们在实时应用中的潜力。",
        "title": "Region-Adaptive Sampling for Diffusion Transformers",
        "pinyin": "扩散模型（DMs）在各种领域的生成任务中成为首选。然而，它们依赖多次顺序前向传递，显著限制了实时性能。以前的加速方法主要集中在减少采样步骤或重用中间结果，未能利用图像内部空间区域的变化。通过利用扩散变压器（DiTs）处理可变数量的标记的灵活性，我们引入了RAS，一种新的无需训练的采样策略，根据DiT模型的关注点动态分配图像内不同区域的采样比率。我们的关键观察是，在每个采样步骤中，模型集中在语义上有意义的区域，这些关注区域在连续步骤中表现出强大的连续性。利用这一洞察，RAS仅更新当前关注的区域，而其他区域使用上一步的缓存噪声更新。模型的关注点根据前一步的输出确定，利用了我们观察到的时间一致性。我们在Stable Diffusion 3和Lumina-Next-T2I上评估RAS，分别实现了最高2.36倍和2.51倍的加速，生成质量仅轻微下降。此外，用户研究表明，RAS在人类评估下提供了相似的质量，同时实现了1.6倍的加速。我们的方法在更高效的扩散变压器方面取得了重要进展，增强了它们在实时应用中的潜力。\n\nkuò sàn mó xíng (DMs) zài gè zhǒng lǐng yù de shēng chéng rèn wù zhōng chéng wéi shǒu xuǎn. rán ér, tā men yī lài duō cì shùn xù qián xiāng chuán dì, xiǎn zhù xiàn zhì le shí shí xìng néng. yǐ qián de jiā sù fāng fǎ zhǔ yào jī zhōng zài jiǎn shǎo cǎi yàng bù zhòu huò chóng yòng zhōng jiān jié guǒ, wèi néng lì yòng tú xiàng nèi bù kōng jiān qū yù de biàn huà. tōng guò lì yòng kuò sàn biàn shū zhǔ (DiTs) chǔ lǐ kě biàn shù liàng de biāo jì de líng huó xìng, wǒ men yǐn rù le RAS, yī zhǒng xīn de wú xū xùn liàn de cǎi yàng cè lüè, gēn jù DiT mó xíng de guān zhù diǎn dòng tài fēn pèi tú xiàng nèi bù tōng qū yù de cǎi yàng bǐ lǜ. wǒ men de guǎn jiàn guān chá shì, zài měi gè cǎi yàng bù zhòu zhōng, mó xíng jí zhōng zài yǔ yì shàng yǒu yì yì de qū yù, zhè xiē guān zhù qū yù zài lián xù bù zhòu zhōng biǎo xiàn chū qiáng dà de lián xù xìng. lì yòng zhè yī dòng chá, RAS jǐn gēng xīn shǐ dāng qián guān zhù de qū yù, ér qí tā qū yù shǐ yòng shàng yī bù de huǎn cùn zào shēng gēng xīn. mó xíng de guān zhù diǎn gēn jù qián yī bù de shū chū què dìng, lì yòng le wǒ men guān chá dào de shí jiān yī zhì xìng. wǒ men zài Stable Diffusion 3 hé Lumina-Next-T2I shàng píng guǎ RAS, fēn bié shí xiàn le zuì gāo 2.36 bèi hé 2.51 bèi de jiā sù, shēng chéng zhì liàng jǐn qīng wēi xià jiàng. cǐ wài, yòng hù yán jiū biǎo míng, RAS zài rén lèi píng jià xià tí gōng le xiāng sì de zhì liàng, tóng shí shí xiàn le 1.6 bèi de jiā sù. wǒ men de fāng fǎ zài gèng gāo xiào de kuò sàn biàn shū zhǔ fāng miàn zhǔ dé dào le zhòng yào jìn zhǎn, zēng qiáng le tā men zài shí shí yìng yòng zhōng de qián lì.",
        "vocab": "[\n    {\"word\": \"扩散模型\", \"pinyin\": \"kuò sàn mó xíng\", \"trans\": \"diffusion model\"},\n    {\"word\": \"首选\", \"pinyin\": \"shǒu xuǎn\", \"trans\": \"preferred choice\"},\n    {\"word\": \"依赖\", \"pinyin\": \"yī lài\", \"trans\": \"depend on\"},\n    {\"word\": \"顺序\", \"pinyin\": \"shùn xù\", \"trans\": \"sequential\"},\n    {\"word\": \"前向传递\", \"pinyin\": \"qián xiàng chuán dì\", \"trans\": \"forward pass\"},\n    {\"word\": \"显著\", \"pinyin\": \"xiǎn zhù\", \"trans\": \"significant\"},\n    {\"word\": \"限制\", \"pinyin\": \"xiàn zhì\", \"trans\": \"limit\"},\n    {\"word\": \"实时性能\", \"pinyin\": \"shí shí xìng néng\", \"trans\": \"real-time performance\"},\n    {\"word\": \"加速\", \"pinyin\": \"jiā sù\", \"trans\": \"accelerate\"},\n    {\"word\": \"方法\", \"pinyin\": \"fāng fǎ\", \"trans\": \"method\"},\n    {\"word\": \"集中\", \"pinyin\": \"jí zhōng\", \"trans\": \"focus on\"},\n    {\"word\": \"减少\", \"pinyin\": \"jiǎn shǎo\", \"trans\": \"reduce\"},\n    {\"word\": \"采样步骤\", \"pinyin\": \"cǎi yàng bù zhòu\", \"trans\": \"sampling steps\"},\n    {\"word\": \"重用\", \"pinyin\": \"chóng yòng\", \"trans\": \"reuse\"},\n    {\"word\": \"中间结果\", \"pinyin\": \"zhōng jiān jié guǒ\", \"trans\": \"intermediate results\"},\n    {\"word\": \"利用\", \"pinyin\": \"lì yòng\", \"trans\": \"utilize\"},\n    {\"word\": \"图像\", \"pinyin\": \"tú xiàng\", \"trans\": \"image\"},\n    {\"word\": \"内部空间区域\", \"pinyin\": \"nèi bù kōng jiān qū yù\", \"trans\": \"internal spatial regions\"},\n    {\"word\": \"变化\", \"pinyin\": \"biàn huà\", \"trans\": \"change\"},\n    {\"word\": \"扩散变压器\", \"pinyin\": \"kuò sàn biàn yā qì\", \"trans\": \"diffusion transformer\"},\n    {\"word\": \"灵活性\", \"pinyin\": \"líng huó xìng\", \"trans\": \"flexibility\"},\n    {\"word\": \"引入\", \"pinyin\": \"yǐn rù\", \"trans\": \"introduce\"},\n    {\"word\": \"RAS\", \"pinyin\": \"RAS\", \"trans\": \"RAS\"},\n    {\"word\": \"采样策略\", \"pinyin\": \"cǎi yàng cè lüè\", \"trans\": \"sampling strategy\"},\n    {\"word\": \"动态分配\", \"pinyin\": \"dòng tài fēn pèi\", \"trans\": \"dynamic allocation\"},\n    {\"word\": \"关注点\", \"pinyin\": \"guān zhù diǎn\", \"trans\": \"focus points\"},\n    {\"word\": \"关键观察\", \"pinyin\": \"guǎn jiàn guān chá\", \"trans\": \"key observation\"},\n    {\"word\": \"语义\", \"pinyin\": \"yǔ yì\", \"trans\": \"semantic\"},\n    {\"word\": \"有意义\", \"pinyin\": \"yǒu yì yì\", \"trans\": \"meaningful\"},\n    {\"word\": \"连续步骤\", \"pinyin\": \"lián xù bù zhòu\", \"trans\": \"continuous steps\"},\n    {\"word\": \"表现\", \"pinyin\": \"biǎo xiàn\", \"trans\": \"performance\"},\n    {\"word\": \"连续性\", \"pinyin\": \"lián xù xìng\", \"trans\": \"continuity\"},\n    {\"word\": \"洞察\", \"pinyin\": \"dòng chá\", \"trans\": \"insight\"},\n    {\"word\": \"更新\", \"pinyin\": \"gēng xīn\", \"trans\": \"update\"},\n    {\"word\": \"缓存噪声\", \"pinyin\": \"huǎn cún zào shēng\", \"trans\": \"cached noise\"},\n    {\"word\": \"确定\", \"pinyin\": \"què dìng\", \"trans\": \"determine\"},\n    {\"word\": \"时间一致性\", \"pinyin\": \"shí jiān yī zhì xìng\", \"trans\": \"temporal consistency\"},\n    {\"word\": \"评估\", \"pinyin\": \"píng gū\", \"trans\": \"evaluate\"},\n    {\"word\": \"Stable Diffusion 3\", \"pinyin\": \"Stable Diffusion 3\", \"trans\": \"Stable Diffusion 3\"},\n    {\"word\": \"Lumina-Next-T2I\", \"pinyin\": \"Lumina-Next-T2I\", \"trans\": \"Lumina-Next-T2I\"},\n    {\"word\": \"实现\", \"pinyin\": \"shí xiàn\", \"trans\": \"achieve\"},\n    {\"word\": \"加速\", \"pinyin\": \"jiā sù\", \"trans\": \"acceleration\"},\n    {\"word\": \"生成质量\", \"pinyin\": \"shēng chéng zhì liàng\", \"trans\": \"generation quality\"},\n    {\"word\": \"轻微下降\", \"pinyin\": \"qīng wēi xià jiàng\", \"trans\": \"slight decrease\"},\n    {\"word\": \"用户研究\", \"pinyin\": \"yòng hù yán jiū\", \"trans\": \"user study\"},\n    {\"word\": \"人类评估\", \"pinyin\": \"rén lèi píng gū\", \"trans\": \"human evaluation\"},\n    {\"word\": \"相似\", \"pinyin\": \"xiāng sì\", \"trans\": \"similar\"},\n    {\"word\": \"潜力\", \"pinyin\": \"qián lì\", \"trans\": \"potential\"},\n    {\"word\": \"重要进展\", \"pinyin\": \"zhòng yào jìn zhǎn\", \"trans\": \"significant progress\"}\n]",
        "trans": "Diffusion models (DMs) have become the preferred choice for generative tasks in various fields. However, they rely on multiple sequential forward passes, significantly limiting real-time performance. Previous acceleration methods have primarily focused on reducing sampling steps or reusing intermediate results, failing to leverage variations in spatial regions within images. By exploiting the flexibility of diffusion transformers (DiTs) in handling a variable number of tokens, we introduce RAS, a new training-free sampling strategy that dynamically allocates sampling ratios to different regions within an image based on the attention focus of the DiT model. Our key observation is that, at each sampling step, the model concentrates on semantically meaningful regions, and these attention regions exhibit strong continuity across consecutive steps. Leveraging this insight, RAS updates only the currently attended regions, while other regions are updated using cached noise from the previous step. The model's attention focus is determined based on the output from the previous step, utilizing the temporal consistency we observed. We evaluate RAS on Stable Diffusion 3 and Lumina-Next-T2I, achieving up to 2.36x and 2.51x speedup, respectively, with only a slight decrease in generation quality. Additionally, user studies indicate that RAS provides similar quality under human evaluation while achieving a 1.6x speedup. Our method represents a significant advancement in more efficient diffusion transformers, enhancing their potential for real-time applications.",
        "update_ts": "2025-02-17 09:12"
    }
}