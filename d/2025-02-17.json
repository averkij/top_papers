{
    "date": {
        "ru": "17 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
        "en": "February 17",
        "zh": "2æœˆ17æ—¥"
    },
    "time_utc": "2025-02-17 04:13",
    "weekday": 0,
    "issue_id": 2241,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2502.10389",
            "title": "Region-Adaptive Sampling for Diffusion Transformers",
            "url": "https://huggingface.co/papers/2502.10389",
            "abstract": "Diffusion models (DMs) have become the leading choice for generative tasks across diverse domains. However, their reliance on multiple sequential forward passes significantly limits real-time performance. Previous acceleration methods have primarily focused on reducing the number of sampling steps or reusing intermediate results, failing to leverage variations across spatial regions within the image due to the constraints of convolutional U-Net structures. By harnessing the flexibility of Diffusion Transformers (DiTs) in handling variable number of tokens, we introduce RAS, a novel, training-free sampling strategy that dynamically assigns different sampling ratios to regions within an image based on the focus of the DiT model. Our key observation is that during each sampling step, the model concentrates on semantically meaningful regions, and these areas of focus exhibit strong continuity across consecutive steps. Leveraging this insight, RAS updates only the regions currently in focus, while other regions are updated using cached noise from the previous step. The model's focus is determined based on the output from the preceding step, capitalizing on the temporal consistency we observed. We evaluate RAS on Stable Diffusion 3 and Lumina-Next-T2I, achieving speedups up to 2.36x and 2.51x, respectively, with minimal degradation in generation quality. Additionally, a user study reveals that RAS delivers comparable qualities under human evaluation while achieving a 1.6x speedup. Our approach makes a significant step towards more efficient diffusion transformers, enhancing their potential for real-time applications.",
            "score": 22,
            "issue_id": 2241,
            "pub_date": "2025-02-14",
            "pub_date_card": {
                "ru": "14 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 14",
                "zh": "2æœˆ14æ—¥"
            },
            "hash": "8068f45b7fd0c2ee",
            "authors": [
                "Ziming Liu",
                "Yifan Yang",
                "Chengruidong Zhang",
                "Yiqi Zhang",
                "Lili Qiu",
                "Yang You",
                "Yuqing Yang"
            ],
            "affiliations": [
                "Microsoft Research",
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.10389.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#benchmark",
                    "#training",
                    "#diffusion",
                    "#architecture",
                    "#dataset"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ RAS. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ¾Ğ¿Ğ¸Ñ€Ğ°ÑÑÑŒ Ğ½Ğ° Ñ„Ğ¾ĞºÑƒÑ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Diffusion Transformer. RAS Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‚Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, Ğ½Ğ° ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚ ÑÑ„Ğ¾ĞºÑƒÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ´Ğ»Ñ Ğ¾ÑÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ÑˆÑƒĞ¼ Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰ĞµĞ³Ğ¾ ÑˆĞ°Ğ³Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ RAS Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ 2.51x Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Accelerating Diffusion Transformers with RAS for Real-Time Generation",
                    "desc": "This paper introduces RAS, a new sampling strategy for Diffusion Transformers (DiTs) that improves the efficiency of generative tasks. Traditional diffusion models require multiple sequential passes, which slow down real-time performance, but RAS dynamically adjusts sampling ratios based on the model's focus on different image regions. By only updating areas of interest and reusing noise from previous steps, RAS significantly accelerates the sampling process while maintaining high quality. The results show that RAS can achieve speedups of over 2x with minimal loss in generation quality, making it a promising advancement for real-time applications in generative modeling."
                },
                "zh": {
                    "title": "æå‡æ‰©æ•£æ¨¡å‹çš„å®æ—¶æ€§èƒ½",
                    "desc": "æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨ç”Ÿæˆä»»åŠ¡ä¸­å·²æˆä¸ºé¦–é€‰ï¼Œä½†å…¶ä¾èµ–å¤šä¸ªé¡ºåºå‰å‘ä¼ é€’é™åˆ¶äº†å®æ—¶æ€§èƒ½ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ— è®­ç»ƒé‡‡æ ·ç­–ç•¥RASï¼Œåˆ©ç”¨æ‰©æ•£å˜æ¢å™¨ï¼ˆDiTsï¼‰çš„çµæ´»æ€§ï¼Œæ ¹æ®æ¨¡å‹çš„å…³æ³¨ç‚¹åŠ¨æ€åˆ†é…å›¾åƒåŒºåŸŸçš„é‡‡æ ·æ¯”ä¾‹ã€‚RASåªæ›´æ–°å½“å‰å…³æ³¨çš„åŒºåŸŸï¼Œè€Œå…¶ä»–åŒºåŸŸåˆ™ä½¿ç”¨ä¸Šä¸€æ­¥çš„ç¼“å­˜å™ªå£°ï¼Œä»è€Œæé«˜äº†æ•ˆç‡ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒRASåœ¨ç”Ÿæˆè´¨é‡å‡ ä¹ä¸ä¸‹é™çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿå®ç°æ˜¾è‘—çš„åŠ é€Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09696",
            "title": "ZeroBench: An Impossible Visual Benchmark for Contemporary Large Multimodal Models",
            "url": "https://huggingface.co/papers/2502.09696",
            "abstract": "Large Multimodal Models (LMMs) exhibit major shortfalls when interpreting images and, by some measures, have poorer spatial cognition than small children or animals. Despite this, they attain high scores on many popular visual benchmarks, with headroom rapidly eroded by an ongoing surge of model progress. To address this, there is a pressing need for difficult benchmarks that remain relevant for longer. We take this idea to its limit by introducing ZeroBench-a lightweight visual reasoning benchmark that is entirely impossible for contemporary frontier LMMs. Our benchmark consists of 100 manually curated questions and 334 less difficult subquestions. We evaluate 20 LMMs on ZeroBench, all of which score 0.0%, and rigorously analyse the errors. To encourage progress in visual understanding, we publicly release ZeroBench.",
            "score": 9,
            "issue_id": 2241,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 13",
                "zh": "2æœˆ13æ—¥"
            },
            "hash": "00f4f8e85ea27717",
            "authors": [
                "Jonathan Roberts",
                "Mohammad Reza Taesiri",
                "Ansh Sharma",
                "Akash Gupta",
                "Samuel Roberts",
                "Ioana Croitoru",
                "Simion-Vlad Bogolin",
                "Jialu Tang",
                "Florian Langer",
                "Vyas Raina",
                "Vatsal Raina",
                "Hanyi Xiong",
                "Vishaal Udandarao",
                "Jingyi Lu",
                "Shiyang Chen",
                "Sam Purkis",
                "Tianshuo Yan",
                "Wenye Lin",
                "Gyungin Shin",
                "Qiaochu Yang",
                "Anh Totti Nguyen",
                "Kai Han",
                "Samuel Albanie"
            ],
            "affiliations": [
                "Auburn University",
                "Independent Researcher",
                "The University of Hong Kong",
                "University of Alberta",
                "University of Cambridge",
                "University of Oxford",
                "University of Tubingen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09696.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#benchmark",
                    "#interpretability",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ZeroBench: Ğ½ĞµĞ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ZeroBench, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ½ĞµĞ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ñ€ĞµÑˆĞ¸Ñ‚ÑŒ Ğ´Ğ»Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LMM). Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· 100 Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ 334 Ğ¼ĞµĞ½ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ»Ğ¸ 20 LMM Ğ½Ğ° ZeroBench, Ğ¸ Ğ²ÑĞµ Ğ¾Ğ½Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ 0%. Ğ¦ĞµĞ»ÑŒÑ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ° ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµÑÑ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾ÑÑ‚Ğ°Ğ½ĞµÑ‚ÑÑ Ğ°ĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ´Ğ¾Ğ»ÑŒÑˆĞµ, Ñ‡ĞµĞ¼ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸."
                },
                "en": {
                    "title": "ZeroBench: Raising the Bar for Visual Reasoning in LMMs",
                    "desc": "This paper discusses the limitations of Large Multimodal Models (LMMs) in interpreting images, highlighting that they perform worse in spatial reasoning compared to small children and animals. Despite achieving high scores on existing visual benchmarks, these models struggle with more complex visual reasoning tasks. To tackle this issue, the authors introduce ZeroBench, a new benchmark designed to be extremely challenging for current LMMs, consisting of 100 curated questions and 334 easier subquestions. The evaluation of 20 LMMs on ZeroBench resulted in a score of 0.0%, demonstrating the need for more difficult benchmarks to drive advancements in visual understanding."
                },
                "zh": {
                    "title": "ZeroBenchï¼šæ¨åŠ¨è§†è§‰ç†è§£çš„æ–°åŸºå‡†",
                    "desc": "å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨å›¾åƒç†è§£æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œç”šè‡³åœ¨æŸäº›æ–¹é¢çš„ç©ºé—´è®¤çŸ¥èƒ½åŠ›ä¸å¦‚å°å­©æˆ–åŠ¨ç‰©ã€‚å°½ç®¡å¦‚æ­¤ï¼Œå®ƒä»¬åœ¨è®¸å¤šæµè¡Œçš„è§†è§‰åŸºå‡†æµ‹è¯•ä¸­å¾—åˆ†å¾ˆé«˜ï¼Œä½†éšç€æ¨¡å‹è¿›æ­¥çš„åŠ é€Ÿï¼Œè¿™äº›åŸºå‡†çš„æŒ‘æˆ˜æ€§è¿…é€Ÿé™ä½ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ZeroBenchï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§çš„è§†è§‰æ¨ç†åŸºå‡†ï¼Œå½“å‰çš„å‰æ²¿LMMså®Œå…¨æ— æ³•è§£å†³ã€‚ZeroBenchåŒ…å«100ä¸ªæ‰‹åŠ¨ç­–åˆ’çš„é—®é¢˜å’Œ334ä¸ªè¾ƒç®€å•çš„å­é—®é¢˜ï¼Œæˆ‘ä»¬å¯¹20ä¸ªLMMsè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœå‡ä¸º0.0%ï¼Œå¹¶å¯¹é”™è¯¯è¿›è¡Œäº†ä¸¥æ ¼åˆ†æã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.10391",
            "title": "MM-RLHF: The Next Step Forward in Multimodal LLM Alignment",
            "url": "https://huggingface.co/papers/2502.10391",
            "abstract": "Despite notable advancements in Multimodal Large Language Models (MLLMs), most state-of-the-art models have not undergone thorough alignment with human preferences. This gap exists because current alignment research has primarily achieved progress in specific areas (e.g., hallucination reduction), while the broader question of whether aligning models with human preferences can systematically enhance MLLM capability remains largely unexplored. To this end, we introduce MM-RLHF, a dataset containing 120k fine-grained, human-annotated preference comparison pairs. This dataset represents a substantial advancement over existing resources, offering superior size, diversity, annotation granularity, and quality. Leveraging this dataset, we propose several key innovations to improve both the quality of reward models and the efficiency of alignment algorithms. Notably, we introduce a Critique-Based Reward Model, which generates critiques of model outputs before assigning scores, offering enhanced interpretability and more informative feedback compared to traditional scalar reward mechanisms. Additionally, we propose Dynamic Reward Scaling, a method that adjusts the loss weight of each sample according to the reward signal, thereby optimizing the use of high-quality comparison pairs. Our approach is rigorously evaluated across 10 distinct dimensions and 27 benchmarks, with results demonstrating significant and consistent improvements in model performance. Specifically, fine-tuning LLaVA-ov-7B with MM-RLHF and our alignment algorithm leads to a 19.5% increase in conversational abilities and a 60% improvement in safety.   We have open-sourced the preference dataset, reward model, training and evaluation code, as well as reward modeling and safety benchmarks. For more details, please visit our project page: https://mm-rlhf.github.io.",
            "score": 5,
            "issue_id": 2241,
            "pub_date": "2025-02-14",
            "pub_date_card": {
                "ru": "14 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 14",
                "zh": "2æœˆ14æ—¥"
            },
            "hash": "c47a89fda79a1a4b",
            "authors": [
                "Yi-Fan Zhang",
                "Tao Yu",
                "Haochen Tian",
                "Chaoyou Fu",
                "Peiyan Li",
                "Jianshu Zeng",
                "Wulin Xie",
                "Yang Shi",
                "Huanyu Zhang",
                "Junkang Wu",
                "Xue Wang",
                "Yibo Hu",
                "Bin Wen",
                "Fan Yang",
                "Zhang Zhang",
                "Tingting Gao",
                "Di Zhang",
                "Liang Wang",
                "Rong Jin",
                "Tieniu Tan"
            ],
            "affiliations": [
                "Alibaba",
                "CASIA",
                "KuaiShou",
                "Meta AI",
                "NJU",
                "PKU",
                "USTC"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.10391.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#alignment",
                    "#training",
                    "#interpretability",
                    "#open_source",
                    "#rlhf",
                    "#dataset"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ MM-RLHF - Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 120 Ñ‚Ñ‹ÑÑÑ‡ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ° ĞµĞ³Ğ¾ Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ LLaVA-ov-7B Ğ² Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°Ñ… Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ÑĞ»Ğµ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ»Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ Ğº Ğ½Ğ°Ğ±Ğ¾Ñ€Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¸ ĞºĞ¾Ğ´Ñƒ Ğ´Ğ»Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing MLLM Alignment with Human Preferences",
                    "desc": "This paper addresses the need for better alignment of Multimodal Large Language Models (MLLMs) with human preferences. It introduces MM-RLHF, a new dataset with 120,000 human-annotated preference comparisons, which enhances the quality and diversity of training data for alignment tasks. The authors propose innovative techniques like a Critique-Based Reward Model that provides detailed feedback on model outputs and Dynamic Reward Scaling to optimize the training process. Their methods show significant improvements in model performance, including a 19.5% boost in conversational skills and a 60% increase in safety metrics."
                },
                "zh": {
                    "title": "æå‡å¤šæ¨¡æ€æ¨¡å‹ä¸äººç±»åå¥½çš„å¯¹é½",
                    "desc": "å°½ç®¡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å¤§å¤šæ•°æœ€å…ˆè¿›çš„æ¨¡å‹å°šæœªä¸äººç±»åå¥½è¿›è¡Œå……åˆ†å¯¹é½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†MM-RLHFæ•°æ®é›†ï¼ŒåŒ…å«12ä¸‡ä¸ªç»†ç²’åº¦çš„äººç±»æ ‡æ³¨åå¥½æ¯”è¾ƒå¯¹ï¼Œæ˜¾è‘—æå‡äº†ç°æœ‰èµ„æºçš„è§„æ¨¡å’Œè´¨é‡ã€‚æˆ‘ä»¬æå‡ºäº†åŸºäºæ‰¹è¯„çš„å¥–åŠ±æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨è¯„åˆ†å‰ç”Ÿæˆæ¨¡å‹è¾“å‡ºçš„æ‰¹è¯„ï¼Œä»è€Œæä¾›æ›´å…·å¯è§£é‡Šæ€§å’Œä¿¡æ¯é‡çš„åé¦ˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†åŠ¨æ€å¥–åŠ±ç¼©æ”¾æ–¹æ³•ï¼Œæ ¹æ®å¥–åŠ±ä¿¡å·è°ƒæ•´æ¯ä¸ªæ ·æœ¬çš„æŸå¤±æƒé‡ï¼Œä»¥ä¼˜åŒ–é«˜è´¨é‡æ¯”è¾ƒå¯¹çš„ä½¿ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09741",
            "title": "FoNE: Precise Single-Token Number Embeddings via Fourier Features",
            "url": "https://huggingface.co/papers/2502.09741",
            "abstract": "Large Language Models (LLMs) typically represent numbers using multiple tokens, which requires the model to aggregate these tokens to interpret numerical values. This fragmentation makes both training and inference less efficient and adversely affects the model's performance on number-related tasks. Inspired by the observation that pre-trained LLMs internally learn Fourier-like features for number tokens, we propose Fourier Number Embedding (FoNE), a novel method that directly maps numbers into the embedding space with their Fourier features. FoNE encodes each number as a single token with only two embedding dimensions per digit, effectively capturing numerical values without fragmentation. This compact representation accelerates both training and inference. Compared to traditional subword and digit-wise embeddings, FoNE not only reduces computational overhead but also achieves higher accuracy across various numerical tasks including addition, subtraction and multiplication. On 6-digit decimal addition, FoNE requires 64times less data to achieve 99% accuracy than subword and digit-wise embeddings while using 3times and 6times fewer tokens per number, respectively. Furthermore, FoNE is the only method that yields 100% accuracy on over 100,000 test examples for addition, subtraction, and multiplication. The codes and visualization are available at https://fouriernumber.github.io/.",
            "score": 2,
            "issue_id": 2241,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 13",
                "zh": "2æœˆ13æ—¥"
            },
            "hash": "adb30f7d3d01ce3a",
            "authors": [
                "Tianyi Zhou",
                "Deqing Fu",
                "Mahdi Soltanolkotabi",
                "Robin Jia",
                "Vatsal Sharan"
            ],
            "affiliations": [
                "Department of Computer Science University of Southern California"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09741.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#data",
                    "#optimization"
                ],
                "emoji": "ğŸ”¢",
                "ru": {
                    "title": "FoNE: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ñ‡Ğ¸ÑĞµĞ» Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡Ğ¸ÑĞµĞ» Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Fourier Number Embedding (FoNE). FoNE Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ‡Ğ¸ÑĞ»Ğ° Ğ² Ğ²Ğ¸Ğ´Ğµ ĞµĞ´Ğ¸Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ„ÑƒÑ€ÑŒĞµ-Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ± Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ ĞºĞ°Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞŸĞ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸, FoNE Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‡Ğ¸ÑĞ»Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ, Ğ²Ñ‹Ñ‡Ğ¸Ñ‚Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑƒĞ¼Ğ½Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ, Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "Revolutionizing Number Representation in LLMs with FoNE",
                    "desc": "This paper introduces Fourier Number Embedding (FoNE), a new approach for representing numbers in Large Language Models (LLMs). FoNE simplifies the representation of numerical values by encoding each number as a single token using Fourier features, which reduces fragmentation and improves efficiency. The method significantly enhances model performance on numerical tasks, achieving higher accuracy with fewer tokens compared to traditional embeddings. Notably, FoNE demonstrates remarkable results, requiring much less data to reach high accuracy levels in arithmetic operations like addition, subtraction, and multiplication."
                },
                "zh": {
                    "title": "å‚…é‡Œå¶æ•°å­—åµŒå…¥ï¼šé«˜æ•ˆçš„æ•°å­—è¡¨ç¤ºæ–¹æ³•",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šå¸¸ä½¿ç”¨å¤šä¸ªæ ‡è®°æ¥è¡¨ç¤ºæ•°å­—ï¼Œè¿™å¯¼è‡´æ¨¡å‹åœ¨ç†è§£æ•°å€¼æ—¶éœ€è¦èšåˆè¿™äº›æ ‡è®°ï¼Œé™ä½äº†è®­ç»ƒå’Œæ¨ç†çš„æ•ˆç‡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œç§°ä¸ºå‚…é‡Œå¶æ•°å­—åµŒå…¥ï¼ˆFoNEï¼‰ï¼Œå®ƒå°†æ•°å­—ç›´æ¥æ˜ å°„åˆ°åµŒå…¥ç©ºé—´ï¼Œä½¿ç”¨å‚…é‡Œå¶ç‰¹å¾è¿›è¡Œç¼–ç ã€‚FoNEå°†æ¯ä¸ªæ•°å­—ç¼–ç ä¸ºä¸€ä¸ªå•ä¸€æ ‡è®°ï¼Œæ˜¾è‘—å‡å°‘äº†è®¡ç®—å¼€é”€ï¼Œå¹¶åœ¨åŠ æ³•ã€å‡æ³•å’Œä¹˜æ³•ç­‰æ•°å€¼ä»»åŠ¡ä¸­å®ç°äº†æ›´é«˜çš„å‡†ç¡®æ€§ã€‚ä¸ä¼ ç»Ÿçš„å­è¯å’Œæ•°å­—åµŒå…¥ç›¸æ¯”ï¼ŒFoNEåœ¨6ä½åè¿›åˆ¶åŠ æ³•ä¸­æ‰€éœ€çš„æ•°æ®é‡å‡å°‘äº†64å€ï¼ŒåŒæ—¶æ¯ä¸ªæ•°å­—ä½¿ç”¨çš„æ ‡è®°æ•°é‡ä¹Ÿå‡å°‘äº†3å€åˆ°6å€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.10248",
            "title": "Step-Video-T2V Technical Report: The Practice, Challenges, and Future of Video Foundation Model",
            "url": "https://huggingface.co/papers/2502.10248",
            "abstract": "We present Step-Video-T2V, a state-of-the-art text-to-video pre-trained model with 30B parameters and the ability to generate videos up to 204 frames in length. A deep compression Variational Autoencoder, Video-VAE, is designed for video generation tasks, achieving 16x16 spatial and 8x temporal compression ratios, while maintaining exceptional video reconstruction quality. User prompts are encoded using two bilingual text encoders to handle both English and Chinese. A DiT with 3D full attention is trained using Flow Matching and is employed to denoise input noise into latent frames. A video-based DPO approach, Video-DPO, is applied to reduce artifacts and improve the visual quality of the generated videos. We also detail our training strategies and share key observations and insights. Step-Video-T2V's performance is evaluated on a novel video generation benchmark, Step-Video-T2V-Eval, demonstrating its state-of-the-art text-to-video quality when compared with both open-source and commercial engines. Additionally, we discuss the limitations of current diffusion-based model paradigm and outline future directions for video foundation models. We make both Step-Video-T2V and Step-Video-T2V-Eval available at https://github.com/stepfun-ai/Step-Video-T2V. The online version can be accessed from https://yuewen.cn/videos as well. Our goal is to accelerate the innovation of video foundation models and empower video content creators.",
            "score": 1,
            "issue_id": 2241,
            "pub_date": "2025-02-14",
            "pub_date_card": {
                "ru": "14 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 14",
                "zh": "2æœˆ14æ—¥"
            },
            "hash": "356bc046cc5f59e5",
            "authors": [
                "Guoqing Ma",
                "Haoyang Huang",
                "Kun Yan",
                "Liangyu Chen",
                "Nan Duan",
                "Shengming Yin",
                "Changyi Wan",
                "Ranchen Ming",
                "Xiaoniu Song",
                "Xing Chen",
                "Yu Zhou",
                "Deshan Sun",
                "Deyu Zhou",
                "Jian Zhou",
                "Kaijun Tan",
                "Kang An",
                "Mei Chen",
                "Wei Ji",
                "Qiling Wu",
                "Wen Sun",
                "Xin Han",
                "Yanan Wei",
                "Zheng Ge",
                "Aojie Li",
                "Bin Wang",
                "Bizhu Huang",
                "Bo Wang",
                "Brian Li",
                "Changxing Miao",
                "Chen Xu",
                "Chenfei Wu",
                "Chenguang Yu",
                "Dapeng Shi",
                "Dingyuan Hu",
                "Enle Liu",
                "Gang Yu",
                "Ge Yang",
                "Guanzhe Huang",
                "Gulin Yan",
                "Haiyang Feng",
                "Hao Nie",
                "Haonan Jia",
                "Hanpeng Hu",
                "Hanqi Chen",
                "Haolong Yan",
                "Heng Wang",
                "Hongcheng Guo",
                "Huilin Xiong",
                "Huixin Xiong",
                "Jiahao Gong",
                "Jianchang Wu",
                "Jiaoren Wu",
                "Jie Wu",
                "Jie Yang",
                "Jiashuai Liu",
                "Jiashuo Li",
                "Jingyang Zhang",
                "Junjing Guo",
                "Junzhe Lin",
                "Kaixiang Li",
                "Lei Liu",
                "Lei Xia",
                "Liang Zhao",
                "Liguo Tan",
                "Liwen Huang",
                "Liying Shi",
                "Ming Li",
                "Mingliang Li",
                "Muhua Cheng",
                "Na Wang",
                "Qiaohui Chen",
                "Qinglin He",
                "Qiuyan Liang",
                "Quan Sun",
                "Ran Sun",
                "Rui Wang",
                "Shaoliang Pang",
                "Shiliang Yang",
                "Sitong Liu",
                "Siqi Liu",
                "Shuli Gao",
                "Tiancheng Cao",
                "Tianyu Wang",
                "Weipeng Ming",
                "Wenqing He",
                "Xu Zhao",
                "Xuelin Zhang",
                "Xianfang Zeng",
                "Xiaojia Liu",
                "Xuan Yang",
                "Yaqi Dai",
                "Yanbo Yu",
                "Yang Li",
                "Yineng Deng",
                "Yingming Wang",
                "Yilei Wang",
                "Yuanwei Lu",
                "Yu Chen",
                "Yu Luo",
                "Yuchu Luo",
                "Yuhe Yin",
                "Yuheng Feng",
                "Yuxiang Yang",
                "Zecheng Tang",
                "Zekai Zhang",
                "Zidong Yang",
                "Binxing Jiao",
                "Jiansheng Chen",
                "Jing Li",
                "Shuchang Zhou",
                "Xiangyu Zhang",
                "Xinhao Zhang",
                "Yibo Zhu",
                "Heung-Yeung Shum",
                "Daxin Jiang"
            ],
            "affiliations": [
                "StepFun"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.10248.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#benchmark",
                    "#training",
                    "#open_source",
                    "#diffusion",
                    "#video",
                    "#architecture"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾: Ğ¾Ñ‚ Ñ‚ĞµĞºÑÑ‚Ğ° Ğº Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ñ€Ğ¾Ğ»Ğ¸ĞºĞ°Ğ¼",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Step-Video-T2V - Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ñ 30 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğ¹ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Video-VAE Ğ´Ğ»Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ´Ğ²Ğ° Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ½Ğ° Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼ Ğ¸ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞ°Ñ…. Ğ”Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Video-DPO Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¸ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Video Generation with Step-Video-T2V",
                    "desc": "Step-Video-T2V is a cutting-edge text-to-video model that utilizes 30 billion parameters to generate videos with a maximum length of 204 frames. It employs a deep compression Variational Autoencoder, achieving significant spatial and temporal compression while preserving high video quality. The model incorporates bilingual text encoders for processing prompts in both English and Chinese, and utilizes a DiT with 3D full attention for effective denoising of latent frames. Evaluated on a new benchmark, Step-Video-T2V demonstrates superior performance in video generation, addressing current limitations in diffusion-based models and paving the way for future advancements in video foundation models."
                },
                "zh": {
                    "title": "åˆ›æ–°è§†é¢‘ç”Ÿæˆï¼Œèµ‹èƒ½å†…å®¹åˆ›ä½œè€…",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºStep-Video-T2Vçš„å…ˆè¿›æ–‡æœ¬åˆ°è§†é¢‘é¢„è®­ç»ƒæ¨¡å‹ï¼Œå…·æœ‰300äº¿å‚æ•°ï¼Œèƒ½å¤Ÿç”Ÿæˆæœ€é•¿204å¸§çš„è§†é¢‘ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ·±åº¦å‹ç¼©å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVideo-VAEï¼‰ï¼Œåœ¨è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­å®ç°äº†16x16çš„ç©ºé—´å‹ç¼©å’Œ8xçš„æ—¶é—´å‹ç¼©ï¼ŒåŒæ—¶ä¿æŒäº†å“è¶Šçš„è§†é¢‘é‡å»ºè´¨é‡ã€‚ç”¨æˆ·æç¤ºé€šè¿‡åŒè¯­æ–‡æœ¬ç¼–ç å™¨è¿›è¡Œç¼–ç ï¼Œä»¥å¤„ç†è‹±è¯­å’Œä¸­æ–‡ã€‚æˆ‘ä»¬è¿˜è®¨è®ºäº†å½“å‰æ‰©æ•£æ¨¡å‹èŒƒå¼çš„å±€é™æ€§ï¼Œå¹¶æ¦‚è¿°äº†è§†é¢‘åŸºç¡€æ¨¡å‹çš„æœªæ¥æ–¹å‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.10177",
            "title": "STMA: A Spatio-Temporal Memory Agent for Long-Horizon Embodied Task Planning",
            "url": "https://huggingface.co/papers/2502.10177",
            "abstract": "A key objective of embodied intelligence is enabling agents to perform long-horizon tasks in dynamic environments while maintaining robust decision-making and adaptability. To achieve this goal, we propose the Spatio-Temporal Memory Agent (STMA), a novel framework designed to enhance task planning and execution by integrating spatio-temporal memory. STMA is built upon three critical components: (1) a spatio-temporal memory module that captures historical and environmental changes in real time, (2) a dynamic knowledge graph that facilitates adaptive spatial reasoning, and (3) a planner-critic mechanism that iteratively refines task strategies. We evaluate STMA in the TextWorld environment on 32 tasks, involving multi-step planning and exploration under varying levels of complexity. Experimental results demonstrate that STMA achieves a 31.25% improvement in success rate and a 24.7% increase in average score compared to the state-of-the-art model. The results highlight the effectiveness of spatio-temporal memory in advancing the memory capabilities of embodied agents.",
            "score": 1,
            "issue_id": 2240,
            "pub_date": "2025-02-14",
            "pub_date_card": {
                "ru": "14 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 14",
                "zh": "2æœˆ14æ—¥"
            },
            "hash": "1b17b668b26c2264",
            "authors": [
                "Mingcong Lei",
                "Yiming Zhao",
                "Ge Wang",
                "Zhixin Mai",
                "Shuguang Cui",
                "Yatong Han",
                "Jinke Ren"
            ],
            "affiliations": [
                "FNii-Shenzhen, The Chinese University of Hong Kong, Shenzhen, China",
                "Harbin Engineering University, China",
                "Infused Synapse AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.10177.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#agents",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Spatio-Temporal Memory Agent (STMA) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ñ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼. STMA Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ³Ñ€Ğ°Ñ„ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ°-ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ² ÑÑ€ĞµĞ´Ğµ TextWorld Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ€ĞµĞ´Ğ½ĞµĞ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing Agent Intelligence with Spatio-Temporal Memory",
                    "desc": "The paper introduces the Spatio-Temporal Memory Agent (STMA), which aims to improve how agents perform complex tasks in changing environments. STMA incorporates a spatio-temporal memory module to track past events and environmental shifts, enhancing decision-making. It also utilizes a dynamic knowledge graph for better spatial reasoning and a planner-critic mechanism to refine strategies over time. The results show that STMA significantly outperforms existing models in task success rates and scoring, demonstrating the importance of memory in embodied intelligence."
                },
                "zh": {
                    "title": "æ—¶ç©ºè®°å¿†æ™ºèƒ½ä½“ï¼šæå‡æ™ºèƒ½ä½“å†³ç­–ä¸é€‚åº”èƒ½åŠ›çš„å…³é”®",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œç§°ä¸ºæ—¶ç©ºè®°å¿†æ™ºèƒ½ä½“ï¼ˆSTMAï¼‰ï¼Œæ—¨åœ¨æé«˜æ™ºèƒ½ä½“åœ¨åŠ¨æ€ç¯å¢ƒä¸­æ‰§è¡Œé•¿æœŸä»»åŠ¡çš„èƒ½åŠ›ã€‚STMAé›†æˆäº†æ—¶ç©ºè®°å¿†æ¨¡å—ã€åŠ¨æ€çŸ¥è¯†å›¾è°±å’Œè§„åˆ’-è¯„ä¼°æœºåˆ¶ï¼Œä»¥å¢å¼ºä»»åŠ¡è§„åˆ’å’Œæ‰§è¡Œçš„æ•ˆæœã€‚é€šè¿‡åœ¨TextWorldç¯å¢ƒä¸­è¿›è¡Œ32ä¸ªä»»åŠ¡çš„è¯„ä¼°ï¼ŒSTMAåœ¨æˆåŠŸç‡å’Œå¹³å‡å¾—åˆ†ä¸Šåˆ†åˆ«æé«˜äº†31.25%å’Œ24.7%ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ—¶ç©ºè®°å¿†åœ¨æå‡æ™ºèƒ½ä½“çš„è®°å¿†èƒ½åŠ›æ–¹é¢å…·æœ‰æ˜¾è‘—æ•ˆæœã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-02-14.html",
    "link_next": "2025-02-18.html",
    "link_month": "2025-02.html",
    "short_date_prev": {
        "ru": "14.02",
        "en": "02/14",
        "zh": "2æœˆ14æ—¥"
    },
    "short_date_next": {
        "ru": "18.02",
        "en": "02/18",
        "zh": "2æœˆ18æ—¥"
    },
    "categories": {
        "#dataset": 2,
        "#data": 1,
        "#benchmark": 4,
        "#agents": 1,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 2,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¤„ç†è¶…é•¿æ–‡æœ¬æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ¨ç†é€Ÿåº¦æ…¢å’Œå†…å­˜æˆæœ¬é«˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œä½œè€…æå‡ºäº†InfiniteHiPï¼Œä¸€ç§æ–°çš„æ¨ç†æ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€åˆ é™¤æ— å…³çš„ä¸Šä¸‹æ–‡æ ‡è®°æ¥åŠ é€Ÿå¤„ç†ã€‚è¯¥æ–¹æ³•è¿˜é€šè¿‡é€‰æ‹©æ€§åº”ç”¨RoPEè°ƒæ•´æ–¹æ³•æ¥æ³›åŒ–åˆ°æ›´é•¿çš„åºåˆ—ï¼Œå¹¶å°†é”®å€¼ç¼“å­˜å¸è½½åˆ°ä¸»æœºå†…å­˜ä¸­ï¼Œå‡å°‘GPUå†…å­˜å‹åŠ›ã€‚ç»“æœæ˜¾ç¤ºï¼ŒInfiniteHiPå¯ä»¥åœ¨å•ä¸ªL40s 48GB GPUä¸Šå¤„ç†å¤šè¾¾300ä¸‡ä¸ªæ ‡è®°ï¼Œé€Ÿåº¦æé«˜äº†18.95å€ã€‚",
        "title": "InfiniteHiP: Extending Language Model Context Up to 3 Million Tokens on a Single GPU",
        "pinyin": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¤„ç†è¶…é•¿æ–‡æœ¬æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ¨ç†é€Ÿåº¦æ…¢å’Œå†…å­˜æˆæœ¬é«˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œä½œè€…æå‡ºäº†InfiniteHiPï¼Œä¸€ç§æ–°çš„æ¨ç†æ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€åˆ é™¤æ— å…³çš„ä¸Šä¸‹æ–‡æ ‡è®°æ¥åŠ é€Ÿå¤„ç†ã€‚è¯¥æ–¹æ³•è¿˜é€šè¿‡é€‰æ‹©æ€§åº”ç”¨RoPEè°ƒæ•´æ–¹æ³•æ¥æ³›åŒ–åˆ°æ›´é•¿çš„åºåˆ—ï¼Œå¹¶å°†é”®å€¼ç¼“å­˜å¸è½½åˆ°ä¸»æœºå†…å­˜ä¸­ï¼Œå‡å°‘GPUå†…å­˜å‹åŠ›ã€‚ç»“æœæ˜¾ç¤ºï¼ŒInfiniteHiPå¯ä»¥åœ¨å•ä¸ªL40s 48GB GPUä¸Šå¤„ç†å¤šè¾¾300ä¸‡ä¸ªæ ‡è®°ï¼Œé€Ÿåº¦æé«˜äº†18.95å€ã€‚\n\nzhÃ¨ piÄn wÃ©n zhÄng tÇo lÃ¹n le dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng (LLMs) chÇ” lÇ chÄo chÃ¡ng wÃ©n bÄ›n shÃ­ miÃ n lÃ­n de tiÇo zhÃ n, bÄo kuÃ² tuÄ« lÇ sÃ¹ dÃ¹ mÃ n hÃ© nÃ¨i cÃºn chÃ©ng bÄ›n gÄo. wÃ¨i jiÄ› juÃ© zhÃ¨ xiÄ“ wÃ¨n tÃ­, zuÃ² zhÄ› tÃ­ chÅ« le InfiniteHiP, yÄ« zhÇ’ng xÄ«n de tuÄ« lÇ kuÃ ng jiÃ , tÅng guÃ² dÃ²ng tÃ i shÄn chÃº wÃº guÄn de shÃ ng xiÃ  wÃ©n biÄo jÃ¬ lÃ¡i jiÄ sÃ¹ chÇ” lÇ. gÇi fÇng fÇ hÃ¡i tÅng guÃ² xuÇn zÃ© xÃ¬ng yÃ¬ng yÃ²ng RoPE tiÃ¡o zhÄ›ng fÇng fÇ lÃ¡i fÃ n huÃ  dÃ o gÃ¨ng chÃ¡ng de xÃ¹ liÃ¨, bÃ¬ng jiÄng jiÃ n zhÃ­ huÃ n cÃºn xiÃ¨ zÃ i zhÇ” jÄ« nÃ¨i cÃºn zhÅng, jiÇn shÇo GPU nÃ¨i cÃºn yÄ lÃ¬. jiÃ© guÇ’ xiÇn shÃ¬, InfiniteHiP kÄ› yÇ zÃ i dÄn gÃ¨ L40s 48GB GPU shÃ ng chÇ” lÇ duÅ dÃ  300 wÃ n gÃ¨ biÄo jÃ¬, sÃ¹ dÃ¹ tÃ­ gÄo le 18.95 bÃ¨i.",
        "vocab": "[{'word': 'è®¨è®º', 'pinyin': 'tÇo lÃ¹n', 'trans': 'discuss'},\n{'word': 'å¤§è¯­è¨€æ¨¡å‹', 'pinyin': 'dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng', 'trans': 'large language model'},\n{'word': 'å¤„ç†', 'pinyin': 'chÇ” lÇ', 'trans': 'process'},\n{'word': 'è¶…é•¿', 'pinyin': 'chÄo chÃ¡ng', 'trans': 'ultra-long'},\n{'word': 'æ–‡æœ¬', 'pinyin': 'wÃ©n bÄ›n', 'trans': 'text'},\n{'word': 'é¢ä¸´', 'pinyin': 'miÃ n lÃ­n', 'trans': 'face'},\n{'word': 'æŒ‘æˆ˜', 'pinyin': 'tiÇo zhÃ n', 'trans': 'challenge'},\n{'word': 'æ¨ç†', 'pinyin': 'tuÄ« lÇ', 'trans': 'inference'},\n{'word': 'é€Ÿåº¦', 'pinyin': 'sÃ¹ dÃ¹', 'trans': 'speed'},\n{'word': 'å†…å­˜', 'pinyin': 'nÃ¨i cÃºn', 'trans': 'memory'},\n{'word': 'æˆæœ¬', 'pinyin': 'chÃ©ng bÄ›n', 'trans': 'cost'},\n{'word': 'æå‡º', 'pinyin': 'tÃ­ chÅ«', 'trans': 'propose'},\n{'word': 'æ¡†æ¶', 'pinyin': 'kuÃ ng jiÃ ', 'trans': 'framework'},\n{'word': 'åŠ¨æ€', 'pinyin': 'dÃ²ng tÃ i', 'trans': 'dynamic'},\n{'word': 'åˆ é™¤', 'pinyin': 'shÄn chÃº', 'trans': 'delete'},\n{'word': 'æ— å…³', 'pinyin': 'wÃº guÄn', 'trans': 'irrelevant'},\n{'word': 'ä¸Šä¸‹æ–‡', 'pinyin': 'shÃ ng xiÃ  wÃ©n', 'trans': 'context'},\n{'word': 'æ ‡è®°', 'pinyin': 'biÄo jÃ¬', 'trans': 'token'},\n{'word': 'åŠ é€Ÿ', 'pinyin': 'jiÄ sÃ¹', 'trans': 'accelerate'},\n{'word': 'é€‰æ‹©æ€§', 'pinyin': 'xuÇn zÃ© xÃ¬ng', 'trans': 'selective'},\n{'word': 'åº”ç”¨', 'pinyin': 'yÃ¬ng yÃ²ng', 'trans': 'apply'},\n{'word': 'RoPE', 'pinyin': 'RoPE', 'trans': 'RoPE'},\n{'word': 'è°ƒæ•´', 'pinyin': 'tiÃ¡o zhÄ›ng', 'trans': 'adjust'},\n{'word': 'æ–¹æ³•', 'pinyin': 'fÄng fÇ', 'trans': 'method'},\n{'word': 'æ³›åŒ–', 'pinyin': 'fÃ n huÃ ', 'trans': 'generalize'},\n{'word': 'åºåˆ—', 'pinyin': 'xÃ¹ liÃ¨', 'trans': 'sequence'},\n{'word': 'é”®å€¼', 'pinyin': 'jiÃ n zhÃ­', 'trans': 'key-value'},\n{'word': 'ç¼“å­˜', 'pinyin': 'huÇn cÃºn', 'trans': 'cache'},\n{'word': 'å¸è½½', 'pinyin': 'xiÃ¨ zÃ i', 'trans': 'unload'},\n{'word': 'ä¸»æœº', 'pinyin': 'zhÇ” jÄ«', 'trans': 'host'},\n{'word': 'å‹åŠ›', 'pinyin': 'yÄ lÃ¬', 'trans': 'pressure'},\n{'word': 'ç»“æœ', 'pinyin': 'jiÃ© guÇ’', 'trans': 'result'},\n{'word': 'æ˜¾ç¤º', 'pinyin': 'xiÇn shÃ¬', 'trans': 'show'},\n{'word': 'å•ä¸ª', 'pinyin': 'dÄn gÃ¨', 'trans': 'single'},\n{'word': 'L40s', 'pinyin': 'L40s', 'trans': 'L40s'},\n{'word': '48GB', 'pinyin': '48GB', 'trans': '48GB'},\n{'word': 'GPU', 'pinyin': 'GPU', 'trans': 'GPU'},\n{'word': 'å¤šè¾¾', 'pinyin': 'duÅ dÃ¡', 'trans': 'up to'},\n{'word': 'æé«˜', 'pinyin': 'tÃ­ gÄo', 'trans': 'improve'},\n{'word': 'å€', 'pinyin': 'bÃ¨i', 'trans': 'times'}]",
        "trans": "This article discusses the challenges faced by large language models (LLMs) when processing extremely long texts, including slow inference speed and high memory costs. To address these issues, the authors propose InfiniteHiP, a new inference framework that accelerates processing by dynamically removing irrelevant context tokens. This method also generalizes to longer sequences by selectively applying the RoPE adjustment method and offloads key-value caching to host memory, reducing GPU memory pressure. The results show that InfiniteHiP can handle up to 30 million tokens on a single L40s 48GB GPU, with a speed increase of 18.95 times.",
        "update_ts": "2025-02-16 12:40"
    }
}