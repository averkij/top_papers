{
    "date": {
        "ru": "16 июня",
        "en": "June 16",
        "zh": "6月16日"
    },
    "time_utc": "2025-06-16 06:19",
    "weekday": 0,
    "issue_id": 4305,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2506.11924",
            "title": "Aligned Novel View Image and Geometry Synthesis via Cross-modal\n  Attention Instillation",
            "url": "https://huggingface.co/papers/2506.11924",
            "abstract": "A diffusion-based framework generates aligned novel views of images and geometry using warping-and-inpainting with cross-modal attention distillation and proximity-based mesh conditioning, achieving high-fidelity synthesis and 3D completion.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce a diffusion-based framework that performs aligned novel view image and geometry generation via a warping-and-inpainting methodology. Unlike prior methods that require dense posed images or pose-embedded generative models limited to in-domain views, our method leverages off-the-shelf geometry predictors to predict partial geometries viewed from reference images, and formulates novel-view synthesis as an inpainting task for both image and geometry. To ensure accurate alignment between generated images and geometry, we propose cross-modal attention distillation, where attention maps from the image diffusion branch are injected into a parallel geometry diffusion branch during both training and inference. This multi-task approach achieves synergistic effects, facilitating geometrically robust image synthesis as well as well-defined geometry prediction. We further introduce proximity-based mesh conditioning to integrate depth and normal cues, interpolating between point cloud and filtering erroneously predicted geometry from influencing the generation process. Empirically, our method achieves high-fidelity extrapolative view synthesis on both image and geometry across a range of unseen scenes, delivers competitive reconstruction quality under interpolation settings, and produces geometrically aligned colored point clouds for comprehensive 3D completion. Project page is available at https://cvlab-kaist.github.io/MoAI.",
            "score": 13,
            "issue_id": 4305,
            "pub_date": "2025-06-13",
            "pub_date_card": {
                "ru": "13 июня",
                "en": "June 13",
                "zh": "6月13日"
            },
            "hash": "bf8d340f29d7ad95",
            "authors": [
                "Min-Seop Kwak",
                "Junho Kim",
                "Sangdoo Yun",
                "Dongyoon Han",
                "Taekyoung Kim",
                "Seungryong Kim",
                "Jin-Hwa Kim"
            ],
            "affiliations": [
                "KAIST AI",
                "NAVER AI Lab",
                "SNU AIIS"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.11924.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#3d",
                    "#diffusion"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Диффузионная модель для согласованной генерации изображений и геометрии с новых ракурсов",
                    "desc": "Эта статья представляет новую систему генерации изображений и геометрии с новых ракурсов, основанную на диффузионных моделях. Метод использует искажение и заполнение пробелов, а также дистилляцию внимания между модальностями для точного выравнивания генерируемых изображений и геометрии. Система применяет условное моделирование на основе близости для интеграции информации о глубине и нормалях. Результаты демонстрируют высококачественный синтез с новых ракурсов и полную трехмерную реконструкцию сцен."
                },
                "en": {
                    "title": "High-Fidelity 3D View Synthesis through Diffusion and Attention",
                    "desc": "This paper presents a diffusion-based framework for generating new views of images and their corresponding 3D geometry. It uses a technique called warping-and-inpainting, which allows for the synthesis of images and geometry without needing a lot of pre-existing data. The method incorporates cross-modal attention distillation to ensure that the generated images and geometries are well-aligned, enhancing the quality of the output. Additionally, it employs proximity-based mesh conditioning to improve the accuracy of the generated 3D structures, resulting in high-fidelity synthesis and completion of 3D scenes."
                },
                "zh": {
                    "title": "基于扩散的高保真图像与几何体生成",
                    "desc": "本文提出了一种基于扩散的框架，通过扭曲和修复的方法生成对齐的新视图图像和几何体。与以往需要密集姿态图像或限制于特定领域视图的生成模型不同，我们的方法利用现成的几何预测器来预测参考图像的部分几何体，并将新视图合成视为图像和几何体的修复任务。为了确保生成的图像和几何体之间的准确对齐，我们提出了跨模态注意力蒸馏，将图像扩散分支的注意力图注入到并行的几何扩散分支中。通过这种多任务方法，我们实现了几何稳健的图像合成和清晰的几何预测，最终在未见场景中实现了高保真度的视图合成。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.10892",
            "title": "The Diffusion Duality",
            "url": "https://huggingface.co/papers/2506.10892",
            "abstract": "Duo improves uniform-state discrete diffusion models by transferring techniques from Gaussian diffusion, enhancing training speed and enabling fast few-step text generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Uniform-state discrete diffusion models hold the promise of fast text generation due to their inherent ability to self-correct. However, they are typically outperformed by autoregressive models and masked diffusion models. In this work, we narrow this performance gap by leveraging a key insight: Uniform-state diffusion processes naturally emerge from an underlying Gaussian diffusion. Our method, Duo, transfers powerful techniques from Gaussian diffusion to improve both training and sampling. First, we introduce a curriculum learning strategy guided by the Gaussian process, doubling training speed by reducing variance. Models trained with curriculum learning surpass autoregressive models in zero-shot perplexity on 3 of 7 benchmarks. Second, we present Discrete Consistency Distillation, which adapts consistency distillation from the continuous to the discrete setting. This algorithm unlocks few-step generation in diffusion language models by accelerating sampling by two orders of magnitude. We provide the code and model checkpoints on the project page: http://s-sahoo.github.io/duo",
            "score": 3,
            "issue_id": 4305,
            "pub_date": "2025-06-12",
            "pub_date_card": {
                "ru": "12 июня",
                "en": "June 12",
                "zh": "6月12日"
            },
            "hash": "974b708b2e781af0",
            "authors": [
                "Subham Sekhar Sahoo",
                "Justin Deschenaux",
                "Aaron Gokaslan",
                "Guanghan Wang",
                "Justin Chiu",
                "Volodymyr Kuleshov"
            ],
            "affiliations": [
                "Computer and Information Science, Cornell Tech, NYC, USA",
                "School of Computer and Communication Sciences, EPFL Lausanne, Switzerland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.10892.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#benchmark",
                    "#optimization",
                    "#open_source",
                    "#diffusion"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "Duo: Ускорение диффузионных языковых моделей с помощью гауссовских техник",
                    "desc": "Метод Duo улучшает дискретные диффузионные модели с равномерным состоянием, перенося техники из гауссовской диффузии. Он вводит стратегию курируемого обучения, управляемую гауссовским процессом, что удваивает скорость обучения за счет снижения дисперсии. Duo также представляет дискретную дистилляцию согласованности, адаптируя метод из непрерывной в дискретную среду. Это позволяет ускорить генерацию текста в диффузионных языковых моделях на два порядка."
                },
                "en": {
                    "title": "Duo: Accelerating Diffusion Models for Fast Text Generation",
                    "desc": "This paper presents Duo, a method that enhances uniform-state discrete diffusion models by incorporating techniques from Gaussian diffusion. The authors introduce a curriculum learning strategy that accelerates training speed by reducing variance, allowing models to outperform autoregressive models in zero-shot perplexity on several benchmarks. Additionally, they propose Discrete Consistency Distillation, which enables faster few-step text generation by adapting consistency distillation for discrete settings. Overall, Duo significantly improves the efficiency and performance of diffusion language models."
                },
                "zh": {
                    "title": "Duo：加速文本生成的创新方法",
                    "desc": "本文提出了一种名为Duo的方法，旨在通过将高斯扩散的技术转移到均匀状态离散扩散模型中，从而提高训练速度和快速文本生成能力。均匀状态离散扩散模型具有自我纠正的能力，但通常在性能上不及自回归模型和掩蔽扩散模型。Duo通过引入基于高斯过程的课程学习策略，显著提高了训练速度，并在多个基准测试中超越了自回归模型。该方法还采用了离散一致性蒸馏技术，使得扩散语言模型能够实现快速的少步生成。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.11928",
            "title": "LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive\n  Programming?",
            "url": "https://huggingface.co/papers/2506.11928",
            "abstract": "LLMs perform well on implementation-heavy competitive programming problems but struggle with nuanced algorithmic reasoning, as highlighted by LiveCodeBench Pro.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent reports claim that large language models (LLMs) now outperform elite humans in competitive programming. Drawing on knowledge from a group of medalists in international algorithmic contests, we revisit this claim, examining how LLMs differ from human experts and where limitations still remain. We introduce LiveCodeBench Pro, a benchmark composed of problems from Codeforces, ICPC, and IOI that are continuously updated to reduce the likelihood of data contamination. A team of Olympiad medalists annotates every problem for algorithmic categories and conducts a line-by-line analysis of failed model-generated submissions. Using this new data and benchmark, we find that frontier models still have significant limitations: without external tools, the best model achieves only 53% pass@1 on medium-difficulty problems and 0% on hard problems, domains where expert humans still excel. We also find that LLMs succeed at implementation-heavy problems but struggle with nuanced algorithmic reasoning and complex case analysis, often generating confidently incorrect justifications. High performance appears largely driven by implementation precision and tool augmentation, not superior reasoning. LiveCodeBench Pro thus highlights the significant gap to human grandmaster levels, while offering fine-grained diagnostics to steer future improvements in code-centric LLM reasoning.",
            "score": 2,
            "issue_id": 4305,
            "pub_date": "2025-06-13",
            "pub_date_card": {
                "ru": "13 июня",
                "en": "June 13",
                "zh": "6月13日"
            },
            "hash": "4d3f2213d58dd8dd",
            "authors": [
                "Zihan Zheng",
                "Zerui Cheng",
                "Zeyu Shen",
                "Shang Zhou",
                "Kaiyuan Liu",
                "Hansen He",
                "Dongruixuan Li",
                "Stanley Wei",
                "Hangyi Hao",
                "Jianzhu Yao",
                "Peiyao Sheng",
                "Zixuan Wang",
                "Wenhao Chai",
                "Aleksandra Korolova",
                "Peter Henderson",
                "Sanjeev Arora",
                "Pramod Viswanath",
                "Jingbo Shang",
                "Saining Xie"
            ],
            "affiliations": [
                "Canyon Crest Academy",
                "McGill University",
                "New York University",
                "Princeton University",
                "Sentient Foundation",
                "University of California San Diego",
                "University of Washington",
                "University of Waterloo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.11928.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#dataset",
                    "#games"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "LLM в программировании: сила в реализации, слабость в алгоритмах",
                    "desc": "Исследование показывает, что крупные языковые модели (LLM) хорошо справляются с задачами по программированию, требующими сложной реализации, но испытывают трудности с тонким алгоритмическим мышлением. Для оценки этого был создан бенчмарк LiveCodeBench Pro, включающий задачи из Codeforces, ICPC и IOI. Анализ выявил, что лучшая модель достигает только 53% pass@1 на задачах средней сложности и 0% на сложных задачах без внешних инструментов. Исследование подчеркивает значительный разрыв между возможностями LLM и уровнем человека-гроссмейстера в программировании."
                },
                "en": {
                    "title": "Bridging the Gap: LLMs vs. Human Algorithmic Mastery",
                    "desc": "This paper evaluates the performance of large language models (LLMs) in competitive programming using a new benchmark called LiveCodeBench Pro. It reveals that while LLMs excel in implementation-heavy tasks, they struggle with complex algorithmic reasoning and nuanced problem-solving. The study shows that even the best LLMs achieve only 53% success on medium-difficulty problems and none on hard problems, indicating a significant gap compared to human experts. The findings suggest that LLMs rely more on implementation accuracy and external tools rather than advanced reasoning skills, highlighting areas for future improvement in AI-driven coding solutions."
                },
                "zh": {
                    "title": "大型语言模型在算法推理中的局限性",
                    "desc": "这篇论文探讨了大型语言模型（LLMs）在竞争编程中的表现，尤其是在实现密集型问题上表现良好，但在复杂算法推理方面存在不足。研究引入了LiveCodeBench Pro，这是一个基于Codeforces、ICPC和IOI的问题基准，旨在减少数据污染的可能性。通过对模型生成的提交进行逐行分析，发现当前的前沿模型在中等难度问题上的通过率仅为53%，而在困难问题上则为0%。这表明，尽管LLMs在实现精度上表现出色，但在复杂的算法推理和案例分析中仍然存在显著的局限性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.09366",
            "title": "SkillBlender: Towards Versatile Humanoid Whole-Body Loco-Manipulation\n  via Skill Blending",
            "url": "https://huggingface.co/papers/2506.09366",
            "abstract": "SkillBlender is a hierarchical reinforcement learning framework that uses pretrained primitive skills to efficiently solve diverse loco-manipulation tasks for humanoid robots.  \t\t\t\t\tAI-generated summary \t\t\t\t Humanoid robots hold significant potential in accomplishing daily tasks across diverse environments thanks to their flexibility and human-like morphology. Recent works have made significant progress in humanoid whole-body control and loco-manipulation leveraging optimal control or reinforcement learning. However, these methods require tedious task-specific tuning for each task to achieve satisfactory behaviors, limiting their versatility and scalability to diverse tasks in daily scenarios. To that end, we introduce SkillBlender, a novel hierarchical reinforcement learning framework for versatile humanoid loco-manipulation. SkillBlender first pretrains goal-conditioned task-agnostic primitive skills, and then dynamically blends these skills to accomplish complex loco-manipulation tasks with minimal task-specific reward engineering. We also introduce SkillBench, a parallel, cross-embodiment, and diverse simulated benchmark containing three embodiments, four primitive skills, and eight challenging loco-manipulation tasks, accompanied by a set of scientific evaluation metrics balancing accuracy and feasibility. Extensive simulated experiments show that our method significantly outperforms all baselines, while naturally regularizing behaviors to avoid reward hacking, resulting in more accurate and feasible movements for diverse loco-manipulation tasks in our daily scenarios. Our code and benchmark will be open-sourced to the community to facilitate future research. Project page: https://usc-gvl.github.io/SkillBlender-web/.",
            "score": 2,
            "issue_id": 4305,
            "pub_date": "2025-06-11",
            "pub_date_card": {
                "ru": "11 июня",
                "en": "June 11",
                "zh": "6月11日"
            },
            "hash": "411c39c85d7cabe0",
            "authors": [
                "Yuxuan Kuang",
                "Haoran Geng",
                "Amine Elhafsi",
                "Tan-Dzung Do",
                "Pieter Abbeel",
                "Jitendra Malik",
                "Marco Pavone",
                "Yue Wang"
            ],
            "affiliations": [
                "Peking University",
                "Stanford University",
                "University of California, Berkeley",
                "University of Southern California"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.09366.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#robotics",
                    "#rl",
                    "#open_source",
                    "#games"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "SkillBlender: умное сочетание навыков для универсальных гуманоидных роботов",
                    "desc": "SkillBlender - это новая иерархическая система обучения с подкреплением для универсального управления гуманоидными роботами. Она предварительно обучает примитивные навыки, а затем динамически комбинирует их для выполнения сложных задач локомоции и манипуляции. Авторы также представляют SkillBench - разнообразный симулированный бенчмарк для оценки таких систем. Эксперименты показывают, что SkillBlender значительно превосходит базовые методы, обеспечивая более точные и реалистичные движения роботов в повседневных сценариях."
                },
                "en": {
                    "title": "Empowering Humanoid Robots with SkillBlender: Efficient Loco-Manipulation through Skill Blending",
                    "desc": "SkillBlender is a hierarchical reinforcement learning framework designed to enhance the performance of humanoid robots in loco-manipulation tasks. It utilizes pretrained primitive skills that are goal-conditioned and task-agnostic, allowing for efficient blending of these skills to tackle complex tasks without extensive reward tuning. This approach not only improves the versatility of the robots but also ensures that their movements are accurate and feasible in real-world scenarios. Additionally, SkillBench provides a comprehensive benchmark for evaluating the performance of these skills across different robot embodiments and tasks, promoting further research in the field."
                },
                "zh": {
                    "title": "SkillBlender：高效的人形机器人运动操控框架",
                    "desc": "SkillBlender 是一个层次化的强化学习框架，利用预训练的基本技能高效解决人形机器人在多样化环境中的运动操控任务。该框架首先预训练与任务无关的目标导向基本技能，然后动态融合这些技能，以最小的任务特定奖励设计完成复杂的运动操控任务。通过引入 SkillBench，一个包含多种模拟环境和挑战性任务的基准测试，SkillBlender 提供了科学的评估指标，平衡了准确性和可行性。大量的模拟实验表明，SkillBlender 显著优于所有基线方法，能够自然地规范行为，避免奖励黑客行为，从而实现更准确和可行的运动。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.11702",
            "title": "Configurable Preference Tuning with Rubric-Guided Synthetic Data",
            "url": "https://huggingface.co/papers/2506.11702",
            "abstract": "Configurable Preference Tuning enables language models to dynamically adjust their behavior based on human-interprettable directives, using rubric-guided preference data for fine-tuning and inference-time modulation.  \t\t\t\t\tAI-generated summary \t\t\t\t Models of human feedback for AI alignment, such as those underpinning Direct Preference Optimization (DPO), often bake in a singular, static set of preferences, limiting adaptability. This paper challenges the assumption of monolithic preferences by introducing Configurable Preference Tuning (CPT), a novel framework for endowing language models with the ability to dynamically adjust their behavior based on explicit, human-interpretable directives. CPT leverages synthetically generated preference data, conditioned on system prompts derived from structured, fine-grained rubrics that define desired attributes like writing style. By fine-tuning with these rubric-guided preferences, the LLM learns to modulate its outputs at inference time in response to the system prompt, without retraining. This approach not only offers fine-grained control but also provides a mechanism for modeling more nuanced and context-dependent human feedback. Several experimental artifacts, such as training code, generated datasets and fine-tuned models are released at https://github.com/vicgalle/configurable-preference-tuning",
            "score": 1,
            "issue_id": 4305,
            "pub_date": "2025-06-13",
            "pub_date_card": {
                "ru": "13 июня",
                "en": "June 13",
                "zh": "6月13日"
            },
            "hash": "7a7eb1af4ef17eef",
            "authors": [
                "Víctor Gallego"
            ],
            "affiliations": [
                "Komorebi AI Technologies, Madrid, Spain"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.11702.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#synthetic",
                    "#training",
                    "#dataset",
                    "#alignment",
                    "#open_source"
                ],
                "emoji": "🎛️",
                "ru": {
                    "title": "Гибкая настройка языковых моделей под меняющиеся предпочтения пользователей",
                    "desc": "Эта статья представляет новый подход под названием Configurable Preference Tuning (CPT) для настройки языковых моделей. CPT позволяет динамически корректировать поведение моделей на основе явных, понятных человеку директив. Метод использует синтетически сгенерированные данные о предпочтениях, основанные на структурированных рубриках, определяющих желаемые атрибуты. Такой подход обеспечивает тонкую настройку и моделирование более нюансированной обратной связи от человека."
                },
                "en": {
                    "title": "Dynamic Adaptation of Language Models with Configurable Preference Tuning",
                    "desc": "This paper introduces Configurable Preference Tuning (CPT), a new method that allows language models to adapt their responses based on clear, human-understandable instructions. Unlike traditional models that rely on a fixed set of preferences, CPT uses dynamically generated preference data to fine-tune the model's behavior. By employing structured rubrics that specify desired traits, the model can adjust its outputs in real-time without needing to be retrained. This innovation enhances the model's ability to respond to complex and varied human feedback, making it more flexible and context-aware."
                },
                "zh": {
                    "title": "动态调整语言模型行为的可配置偏好调优",
                    "desc": "可配置偏好调优（CPT）是一种新框架，使语言模型能够根据人类可理解的指令动态调整其行为。与传统的直接偏好优化（DPO）方法不同，CPT允许模型使用合成生成的偏好数据进行微调，从而在推理时根据系统提示调节输出。通过这种方式，模型能够在不重新训练的情况下，响应不同的上下文和需求。该方法不仅提供了更细致的控制，还能更好地模拟复杂和依赖上下文的人类反馈。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.09427",
            "title": "A High-Quality Dataset and Reliable Evaluation for Interleaved\n  Image-Text Generation",
            "url": "https://huggingface.co/papers/2506.09427",
            "abstract": "InterSyn, a large-scale dataset with tightly interleaved image-text outputs and automated quality refinement, improves multimodal understanding and generation through the SEIR method and SynJudge, an automatic evaluation tool.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in Large Multimodal Models (LMMs) have significantly improved multimodal understanding and generation. However, these models still struggle to generate tightly interleaved image-text outputs, primarily due to the limited scale, quality and instructional richness of current training datasets. To address this, we introduce InterSyn, a large-scale multimodal dataset constructed using our Self-Evaluation with Iterative Refinement (SEIR) method. InterSyn features multi-turn, instruction-driven dialogues with tightly interleaved imagetext responses, providing rich object diversity and rigorous automated quality refinement, making it well-suited for training next-generation instruction-following LMMs. Furthermore, to address the lack of reliable evaluation tools capable of assessing interleaved multimodal outputs, we introduce SynJudge, an automatic evaluation model designed to quantitatively assess multimodal outputs along four dimensions: text content, image content, image quality, and image-text synergy.   Experimental studies show that the SEIR method leads to substantially higher dataset quality compared to an otherwise identical process without refinement.   Moreover, LMMs trained on InterSyn achieve uniform performance gains across all evaluation metrics, confirming InterSyn's utility for advancing multimodal systems.",
            "score": 1,
            "issue_id": 4305,
            "pub_date": "2025-06-11",
            "pub_date_card": {
                "ru": "11 июня",
                "en": "June 11",
                "zh": "6月11日"
            },
            "hash": "5c1dd5f02a121213",
            "authors": [
                "Yukang Feng",
                "Jianwen Sun",
                "Chuanhao Li",
                "Zizhen Li",
                "Jiaxin Ai",
                "Fanrui Zhang",
                "Yifan Chang",
                "Sizhuo Zhou",
                "Shenglin Zhang",
                "Yu Dai",
                "Kaipeng Zhang"
            ],
            "affiliations": [
                "Nankai University",
                "Shanghai AI Laboratory",
                "Shanghai Innovation Institute",
                "University of Science and Technology of China",
                "Wuhan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.09427.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#optimization",
                    "#multimodal",
                    "#games"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "InterSyn: новый уровень мультимодального обучения",
                    "desc": "Статья представляет InterSyn - крупномасштабный мультимодальный датасет для обучения языковых моделей. InterSyn создан с использованием метода самооценки с итеративным уточнением (SEIR) и содержит диалоги с тесно переплетенными изображениями и текстом. Авторы также представляют SynJudge - инструмент для автоматической оценки качества мультимодальных выходных данных. Эксперименты показывают, что обучение на InterSyn улучшает производительность мультимодальных моделей по всем метрикам оценки."
                },
                "en": {
                    "title": "Enhancing Multimodal AI with InterSyn and SEIR",
                    "desc": "The paper introduces InterSyn, a large-scale dataset designed to enhance multimodal understanding and generation in AI models. It utilizes the Self-Evaluation with Iterative Refinement (SEIR) method to create high-quality, tightly interleaved image-text outputs through multi-turn dialogues. Additionally, the paper presents SynJudge, an automatic evaluation tool that assesses multimodal outputs based on text content, image quality, and their synergy. Experimental results demonstrate that models trained on InterSyn show significant performance improvements across various evaluation metrics, highlighting its effectiveness for next-generation instruction-following models."
                },
                "zh": {
                    "title": "InterSyn：提升多模态理解与生成的关键数据集",
                    "desc": "InterSyn是一个大规模的数据集，旨在提高多模态理解和生成能力。它通过自我评估与迭代精炼（SEIR）方法构建，包含多轮指令驱动的对话和紧密交织的图像-文本输出。为了评估这些输出的质量，文章还介绍了SynJudge，一个自动评估工具，可以从文本内容、图像内容、图像质量和图像-文本协同四个维度进行量化评估。实验结果表明，使用SEIR方法构建的数据集质量显著提高，训练在InterSyn上的大型多模态模型在所有评估指标上均表现出一致的性能提升。"
                }
            }
        }
    ],
    "link_prev": "2025-06-13.html",
    "link_next": "2025-06-17.html",
    "link_month": "2025-06.html",
    "short_date_prev": {
        "ru": "13.06",
        "en": "06/13",
        "zh": "6月13日"
    },
    "short_date_next": {
        "ru": "17.06",
        "en": "06/17",
        "zh": "6月17日"
    },
    "categories": {
        "#dataset": 4,
        "#data": 0,
        "#benchmark": 4,
        "#agents": 0,
        "#cv": 1,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 1,
        "#agi": 0,
        "#games": 3,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}