{
    "date": {
        "ru": "29 ÑĞ½Ğ²Ğ°Ñ€Ñ",
        "en": "January 29",
        "zh": "1æœˆ29æ—¥"
    },
    "time_utc": "2025-01-29 21:09",
    "weekday": 2,
    "issue_id": 1935,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.17161",
            "title": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training",
            "url": "https://huggingface.co/papers/2501.17161",
            "abstract": "Supervised fine-tuning (SFT) and reinforcement learning (RL) are widely used post-training techniques for foundation models. However, their roles in enhancing model generalization capabilities remain unclear. This paper studies the difference between SFT and RL on generalization and memorization, focusing on text-based rule variants and visual variants. We introduce GeneralPoints, an arithmetic reasoning card game, and adopt V-IRL, a real-world navigation environment, to assess how models trained with SFT and RL generalize to unseen variants in both textual and visual domains. We show that RL, especially when trained with an outcome-based reward, generalizes across both rule-based textual and visual variants. SFT, in contrast, tends to memorize training data and struggles to generalize out-of-distribution scenarios. Further analysis reveals that RL improves the model's underlying visual recognition capabilities, contributing to its enhanced generalization in the visual domain. Despite RL's superior generalization, we show that SFT remains essential for effective RL training; SFT stabilizes the model's output format, enabling subsequent RL to achieve its performance gains. These findings demonstrates the capability of RL for acquiring generalizable knowledge in complex, multi-modal tasks.",
            "score": 27,
            "issue_id": 1920,
            "pub_date": "2025-01-28",
            "pub_date_card": {
                "ru": "28 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 28",
                "zh": "1æœˆ28æ—¥"
            },
            "hash": "ce9300709a3cdc7a",
            "authors": [
                "Tianzhe Chu",
                "Yuexiang Zhai",
                "Jihan Yang",
                "Shengbang Tong",
                "Saining Xie",
                "Dale Schuurmans",
                "Quoc V. Le",
                "Sergey Levine",
                "Yi Ma"
            ],
            "affiliations": [
                "Google DeepMind",
                "HKU",
                "NYU",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.17161.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#optimization",
                    "#rl",
                    "#multimodal",
                    "#games"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "RL Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ SFT Ğ² Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡",
                    "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ (SFT) Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ñ‹ Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ RL Ğ»ÑƒÑ‡ÑˆĞµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸Ğ¸, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ½Ğ° Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ. SFT, Ğ½Ğ°Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ², ÑĞºĞ»Ğ¾Ğ½Ğ½Ğ¾ Ğº Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ…ÑƒĞ¶Ğµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸ĞµĞ¼."
                },
                "en": {
                    "title": "Unlocking Generalization: RL Outshines SFT in Multi-Modal Tasks",
                    "desc": "This paper investigates how supervised fine-tuning (SFT) and reinforcement learning (RL) affect the generalization abilities of foundation models. It highlights that while SFT often leads to memorization of training data, RL, particularly with outcome-based rewards, enhances generalization across unseen textual and visual variants. The study introduces GeneralPoints, a reasoning game, and V-IRL, a navigation environment, to evaluate model performance. The results indicate that RL not only improves generalization but also strengthens visual recognition, although SFT is still crucial for stabilizing the model before RL training."
                },
                "zh": {
                    "title": "å¼ºåŒ–å­¦ä¹ æå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›çš„ç ”ç©¶",
                    "desc": "è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨åŸºç¡€æ¨¡å‹ä¸­çš„ä½œç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›æ–¹é¢ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒRLåœ¨å¤„ç†æ–‡æœ¬å’Œè§†è§‰å˜ä½“æ—¶ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ³›åŒ–ï¼Œè€ŒSFTåˆ™å€¾å‘äºè®°å¿†è®­ç»ƒæ•°æ®ï¼Œéš¾ä»¥åº”å¯¹æœªè§è¿‡çš„æƒ…å†µã€‚é€šè¿‡å¼•å…¥ç®—æœ¯æ¨ç†å¡ç‰Œæ¸¸æˆGeneralPointså’ŒçœŸå®ä¸–ç•Œå¯¼èˆªç¯å¢ƒV-IRLï¼Œä½œè€…è¯„ä¼°äº†è¿™ä¸¤ç§æ–¹æ³•çš„æ•ˆæœã€‚å°½ç®¡RLåœ¨æ³›åŒ–èƒ½åŠ›ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œä½†SFTä»ç„¶å¯¹æœ‰æ•ˆçš„RLè®­ç»ƒè‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒç¨³å®šäº†æ¨¡å‹çš„è¾“å‡ºæ ¼å¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.17116",
            "title": "Optimizing Large Language Model Training Using FP4 Quantization",
            "url": "https://huggingface.co/papers/2501.17116",
            "abstract": "The growing computational demands of training large language models (LLMs) necessitate more efficient methods. Quantized training presents a promising solution by enabling low-bit arithmetic operations to reduce these costs. While FP8 precision has demonstrated feasibility, leveraging FP4 remains a challenge due to significant quantization errors and limited representational capacity. This work introduces the first FP4 training framework for LLMs, addressing these challenges with two key innovations: a differentiable quantization estimator for precise weight updates and an outlier clamping and compensation strategy to prevent activation collapse. To ensure stability, the framework integrates a mixed-precision training scheme and vector-wise quantization. Experimental results demonstrate that our FP4 framework achieves accuracy comparable to BF16 and FP8, with minimal degradation, scaling effectively to 13B-parameter LLMs trained on up to 100B tokens. With the emergence of next-generation hardware supporting FP4, our framework sets a foundation for efficient ultra-low precision training.",
            "score": 12,
            "issue_id": 1920,
            "pub_date": "2025-01-28",
            "pub_date_card": {
                "ru": "28 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 28",
                "zh": "1æœˆ28æ—¥"
            },
            "hash": "9ce85dc91aee17fc",
            "authors": [
                "Ruizhe Wang",
                "Yeyun Gong",
                "Xiao Liu",
                "Guoshuai Zhao",
                "Ziyue Yang",
                "Baining Guo",
                "Zhengjun Zha",
                "Peng Cheng"
            ],
            "affiliations": [
                "Microsoft Research Asia",
                "Microsoft SIGMA Team",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.17116.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#inference"
                ],
                "emoji": "ğŸ”¢",
                "ru": {
                    "title": "FP4: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 4-Ğ±Ğ¸Ñ‚Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¿Ğ»Ğ°Ğ²Ğ°ÑÑ‰ĞµĞ¹ Ğ·Ğ°Ğ¿ÑÑ‚Ğ¾Ğ¹ (FP4). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸Ğº ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²ĞµÑĞ¾Ğ² Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¸ ĞºĞ¾Ğ¼Ğ¿ĞµĞ½ÑĞ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ğ±Ñ€Ğ¾ÑĞ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ»Ğ»Ğ°Ğ¿ÑĞ° Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑÑ…ĞµĞ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¾ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğµ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ FP4-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ¹ Ñ BF16 Ğ¸ FP8, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒÑÑÑŒ Ğ´Ğ¾ LLM Ñ 13 Ğ¼Ğ»Ñ€Ğ´ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "Efficient Training of Large Language Models with FP4 Precision",
                    "desc": "This paper addresses the high computational costs associated with training large language models (LLMs) by introducing a novel FP4 training framework. The framework utilizes quantized training techniques, specifically focusing on low-bit arithmetic to enhance efficiency while maintaining model accuracy. Key innovations include a differentiable quantization estimator for better weight updates and a strategy to manage outliers, which helps prevent activation collapse. Experimental results show that this FP4 approach achieves performance similar to higher precision formats like BF16 and FP8, making it suitable for large-scale LLMs."
                },
                "zh": {
                    "title": "FP4è®­ç»ƒæ¡†æ¶ï¼šé«˜æ•ˆçš„è¶…ä½ç²¾åº¦è®­ç»ƒæ–°æ–¹æ¡ˆ",
                    "desc": "éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è®­ç»ƒå¯¹è®¡ç®—èµ„æºçš„éœ€æ±‚ä¸æ–­å¢åŠ ï¼Œå¯»æ‰¾æ›´é«˜æ•ˆçš„æ–¹æ³•å˜å¾—å°¤ä¸ºé‡è¦ã€‚é‡åŒ–è®­ç»ƒé€šè¿‡å…è®¸ä½ä½æ•°ç®—æœ¯è¿ç®—æ¥é™ä½è¿™äº›æˆæœ¬ï¼Œå±•ç°å‡ºè‰¯å¥½çš„å‰æ™¯ã€‚å°½ç®¡FP8ç²¾åº¦å·²è¢«è¯æ˜å¯è¡Œï¼Œä½†FP4çš„åº”ç”¨ä»é¢ä¸´æ˜¾è‘—çš„é‡åŒ–è¯¯å·®å’Œæœ‰é™çš„è¡¨ç¤ºèƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºäº†é¦–ä¸ªFP4è®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡å¯å¾®åˆ†é‡åŒ–ä¼°è®¡å™¨å’Œå¼‚å¸¸å€¼é’³åˆ¶ä¸è¡¥å¿ç­–ç•¥ï¼Œè§£å†³äº†è¿™äº›æŒ‘æˆ˜ï¼Œå¹¶åœ¨ç¨³å®šæ€§æ–¹é¢ç»“åˆäº†æ··åˆç²¾åº¦è®­ç»ƒæ–¹æ¡ˆå’Œå‘é‡çº§é‡åŒ–ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.16975",
            "title": "Over-Tokenized Transformer: Vocabulary is Generally Worth Scaling",
            "url": "https://huggingface.co/papers/2501.16975",
            "abstract": "Tokenization is a fundamental component of large language models (LLMs), yet its influence on model scaling and performance is not fully explored. In this paper, we introduce Over-Tokenized Transformers, a novel framework that decouples input and output vocabularies to improve language modeling performance. Specifically, our approach scales up input vocabularies to leverage multi-gram tokens. Through extensive experiments, we uncover a log-linear relationship between input vocabulary size and training loss, demonstrating that larger input vocabularies consistently enhance model performance, regardless of model size. Using a large input vocabulary, we achieve performance comparable to double-sized baselines with no additional cost. Our findings highlight the importance of tokenization in scaling laws and provide practical insight for tokenizer design, paving the way for more efficient and powerful LLMs.",
            "score": 10,
            "issue_id": 1920,
            "pub_date": "2025-01-28",
            "pub_date_card": {
                "ru": "28 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 28",
                "zh": "1æœˆ28æ—¥"
            },
            "hash": "27930c2f5d17471e",
            "authors": [
                "Hongzhi Huang",
                "Defa Zhu",
                "Banggu Wu",
                "Yutao Zeng",
                "Ya Wang",
                "Qiyang Min",
                "Xun Zhou"
            ],
            "affiliations": [
                "Seed-Foundation-Model Team, Bytedance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.16975.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#architecture"
                ],
                "emoji": "ğŸ”¤",
                "ru": {
                    "title": "Ğ‘Ğ¾Ğ»ÑŒÑˆĞµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² - Ğ²Ñ‹ÑˆĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Over-Tokenized Transformers. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¸ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ğ¸, ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ Ğ´Ğ»Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ»Ğ¾Ğ³Ğ°Ñ€Ğ¸Ñ„Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸-Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½ÑƒÑ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑĞ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ consistently ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¾Ñ‚ ĞµÑ‘ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°."
                },
                "en": {
                    "title": "Unlocking Performance: The Power of Over-Tokenization in Language Models",
                    "desc": "This paper presents a new approach called Over-Tokenized Transformers, which focuses on improving the tokenization process in large language models (LLMs). By separating the input and output vocabularies, the authors demonstrate that increasing the input vocabulary size can significantly reduce training loss and enhance model performance. Their experiments reveal a consistent log-linear relationship between the size of the input vocabulary and the model's effectiveness, showing that larger vocabularies lead to better results without increasing computational costs. This research emphasizes the critical role of tokenization in the scaling of LLMs and offers valuable insights for designing more efficient tokenizers."
                },
                "zh": {
                    "title": "åˆ†è¯æŠ€æœ¯æå‡å¤§è¯­è¨€æ¨¡å‹æ€§èƒ½çš„å…³é”®",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ä¸­çš„åˆ†è¯æŠ€æœ¯å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶â€”â€”è¿‡åº¦åˆ†è¯å˜æ¢å™¨ï¼Œæ—¨åœ¨é€šè¿‡è§£è€¦è¾“å…¥å’Œè¾“å‡ºè¯æ±‡è¡¨æ¥æå‡è¯­è¨€å»ºæ¨¡æ€§èƒ½ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå¢å¤§è¾“å…¥è¯æ±‡è¡¨å¯ä»¥æœ‰æ•ˆé™ä½è®­ç»ƒæŸå¤±ï¼Œä»è€Œæé«˜æ¨¡å‹æ€§èƒ½ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨æ›´å¤§çš„è¾“å…¥è¯æ±‡è¡¨å¯ä»¥åœ¨ä¸å¢åŠ æˆæœ¬çš„æƒ…å†µä¸‹ï¼Œè¾¾åˆ°ä¸åŒå€åŸºçº¿ç›¸å½“çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.16764",
            "title": "DiffSplat: Repurposing Image Diffusion Models for Scalable Gaussian Splat Generation",
            "url": "https://huggingface.co/papers/2501.16764",
            "abstract": "Recent advancements in 3D content generation from text or a single image struggle with limited high-quality 3D datasets and inconsistency from 2D multi-view generation. We introduce DiffSplat, a novel 3D generative framework that natively generates 3D Gaussian splats by taming large-scale text-to-image diffusion models. It differs from previous 3D generative models by effectively utilizing web-scale 2D priors while maintaining 3D consistency in a unified model. To bootstrap the training, a lightweight reconstruction model is proposed to instantly produce multi-view Gaussian splat grids for scalable dataset curation. In conjunction with the regular diffusion loss on these grids, a 3D rendering loss is introduced to facilitate 3D coherence across arbitrary views. The compatibility with image diffusion models enables seamless adaptions of numerous techniques for image generation to the 3D realm. Extensive experiments reveal the superiority of DiffSplat in text- and image-conditioned generation tasks and downstream applications. Thorough ablation studies validate the efficacy of each critical design choice and provide insights into the underlying mechanism.",
            "score": 8,
            "issue_id": 1921,
            "pub_date": "2025-01-28",
            "pub_date_card": {
                "ru": "28 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 28",
                "zh": "1æœˆ28æ—¥"
            },
            "hash": "00ee1a0338716711",
            "authors": [
                "Chenguo Lin",
                "Panwang Pan",
                "Bangbang Yang",
                "Zeming Li",
                "Yadong Mu"
            ],
            "affiliations": [
                "ByteDance",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.16764.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#optimization",
                    "#training",
                    "#dataset",
                    "#3d"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "DiffSplat: Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ 3D ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ",
                    "desc": "DiffSplat - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ñ… ÑĞ¿Ğ»Ğ°Ñ‚Ğ¾Ğ². ĞĞ½Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… 3D Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ¸ Ğ½ĞµÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ğ¾Ğ¹ 2D Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. DiffSplat Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğµ 2D-Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ñ‹ Ñ 3D-ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ DiffSplat Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼."
                },
                "en": {
                    "title": "Revolutionizing 3D Generation with DiffSplat",
                    "desc": "DiffSplat is a new framework for generating 3D content from text or images, addressing challenges like the lack of high-quality 3D datasets. It uses advanced text-to-image diffusion models to create 3D Gaussian splats while ensuring consistency across different views. The framework includes a lightweight reconstruction model that helps quickly generate multi-view datasets for training. Through extensive testing, DiffSplat shows improved performance in generating 3D content and offers insights into its effective design choices."
                },
                "zh": {
                    "title": "DiffSplatï¼š3Dç”Ÿæˆçš„æ–°çªç ´",
                    "desc": "æœ€è¿‘ï¼Œ3Då†…å®¹ç”Ÿæˆä»æ–‡æœ¬æˆ–å•å¼ å›¾åƒä¸­å–å¾—äº†è¿›å±•ï¼Œä½†é«˜è´¨é‡3Dæ•°æ®é›†æœ‰é™ï¼Œä¸”2Då¤šè§†å›¾ç”Ÿæˆå­˜åœ¨ä¸ä¸€è‡´æ€§ã€‚æˆ‘ä»¬æå‡ºäº†DiffSplatï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„3Dç”Ÿæˆæ¡†æ¶ï¼Œèƒ½å¤Ÿé€šè¿‡æ§åˆ¶å¤§è§„æ¨¡æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ï¼ŒåŸç”Ÿç”Ÿæˆ3Dé«˜æ–¯ç‚¹äº‘ã€‚ä¸ä»¥å¾€çš„3Dç”Ÿæˆæ¨¡å‹ä¸åŒï¼ŒDiffSplatæœ‰æ•ˆåˆ©ç”¨äº†ç½‘ç»œè§„æ¨¡çš„2Då…ˆéªŒï¼ŒåŒæ—¶åœ¨ç»Ÿä¸€æ¨¡å‹ä¸­ä¿æŒ3Dä¸€è‡´æ€§ã€‚é€šè¿‡å¼•å…¥è½»é‡çº§é‡å»ºæ¨¡å‹å’Œ3Dæ¸²æŸ“æŸå¤±ï¼ŒDiffSplatåœ¨æ–‡æœ¬å’Œå›¾åƒæ¡ä»¶ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä¸”åœ¨ä¸‹æ¸¸åº”ç”¨ä¸­ä¹Ÿæ˜¾ç¤ºå‡ºå…¶ä¼˜è¶Šæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.16496",
            "title": "Open Problems in Mechanistic Interpretability",
            "url": "https://huggingface.co/papers/2501.16496",
            "abstract": "Mechanistic interpretability aims to understand the computational mechanisms underlying neural networks' capabilities in order to accomplish concrete scientific and engineering goals. Progress in this field thus promises to provide greater assurance over AI system behavior and shed light on exciting scientific questions about the nature of intelligence. Despite recent progress toward these goals, there are many open problems in the field that require solutions before many scientific and practical benefits can be realized: Our methods require both conceptual and practical improvements to reveal deeper insights; we must figure out how best to apply our methods in pursuit of specific goals; and the field must grapple with socio-technical challenges that influence and are influenced by our work. This forward-facing review discusses the current frontier of mechanistic interpretability and the open problems that the field may benefit from prioritizing.",
            "score": 7,
            "issue_id": 1920,
            "pub_date": "2025-01-27",
            "pub_date_card": {
                "ru": "27 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 27",
                "zh": "1æœˆ27æ—¥"
            },
            "hash": "5a7a914accebfa33",
            "authors": [
                "Lee Sharkey",
                "Bilal Chughtai",
                "Joshua Batson",
                "Jack Lindsey",
                "Jeff Wu",
                "Lucius Bushnaq",
                "Nicholas Goldowsky-Dill",
                "Stefan Heimersheim",
                "Alejandro Ortega",
                "Joseph Bloom",
                "Stella Biderman",
                "Adria Garriga-Alonso",
                "Arthur Conmy",
                "Neel Nanda",
                "Jessica Rumbelow",
                "Martin Wattenberg",
                "Nandi Schoots",
                "Joseph Miller",
                "Eric J. Michaud",
                "Stephen Casper",
                "Max Tegmark",
                "William Saunders",
                "David Bau",
                "Eric Todd",
                "Atticus Geiger",
                "Mor Geva",
                "Jesse Hoogland",
                "Daniel Murfet",
                "Tom McGrath"
            ],
            "affiliations": [
                "Anthropic",
                "Apollo Research",
                "Google DeepMind",
                "Harvard University",
                "Imperial College London",
                "Kings College London",
                "Leap Laboratories",
                "MIT",
                "Northeastern University",
                "Tel Aviv University",
                "University of Melbourne"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.16496.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#survey"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°Ñ Ñ‚Ğ°Ğ¹Ğ½Ñ‹ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹: Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹, Ñ†ĞµĞ»ÑŒ ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ - Ğ¿Ğ¾Ğ½ÑÑ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹, Ğ»ĞµĞ¶Ğ°Ñ‰Ğ¸Ğµ Ğ² Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹. ĞŸÑ€Ğ¾Ğ³Ñ€ĞµÑÑ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¾Ğ±ĞµÑ‰Ğ°ĞµÑ‚ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ñ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ¸ Ğ¿Ñ€Ğ¾Ğ»Ğ¸Ñ‚ÑŒ ÑĞ²ĞµÑ‚ Ğ½Ğ° Ğ¿Ñ€Ğ¸Ñ€Ğ¾Ğ´Ñƒ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ². Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğµ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞ³Ğ¾ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Unlocking the Secrets of Neural Networks for Reliable AI",
                    "desc": "Mechanistic interpretability focuses on understanding how neural networks work to achieve specific tasks, which can enhance the reliability of AI systems. This area of research aims to uncover the underlying processes that contribute to the intelligence exhibited by these models. Despite advancements, there are still significant challenges that need to be addressed, including improving methods for deeper insights and applying these methods effectively. Additionally, the field must consider socio-technical issues that affect and are affected by mechanistic interpretability efforts."
                },
                "zh": {
                    "title": "æ­ç¤ºç¥ç»ç½‘ç»œçš„è®¡ç®—æœºåˆ¶",
                    "desc": "æœºæ¢°è§£é‡Šæ€§æ—¨åœ¨ç†è§£ç¥ç»ç½‘ç»œèƒ½åŠ›èƒŒåçš„è®¡ç®—æœºåˆ¶ï¼Œä»¥å®ç°å…·ä½“çš„ç§‘å­¦å’Œå·¥ç¨‹ç›®æ ‡ã€‚è¯¥é¢†åŸŸçš„è¿›å±•æœ‰æœ›æé«˜å¯¹äººå·¥æ™ºèƒ½ç³»ç»Ÿè¡Œä¸ºçš„ä¿¡å¿ƒï¼Œå¹¶æ­ç¤ºå…³äºæ™ºèƒ½æœ¬è´¨çš„æœ‰è¶£ç§‘å­¦é—®é¢˜ã€‚å°½ç®¡æœ€è¿‘åœ¨è¿™äº›ç›®æ ‡ä¸Šå–å¾—äº†ä¸€äº›è¿›å±•ï¼Œä½†ä»æœ‰è®¸å¤šæœªè§£å†³çš„é—®é¢˜éœ€è¦è§£å†³ï¼Œä»¥ä¾¿å®ç°æ›´å¤šçš„ç§‘å­¦å’Œå®é™…åˆ©ç›Šã€‚æœ¬æ–‡å›é¡¾äº†æœºæ¢°è§£é‡Šæ€§çš„å½“å‰å‰æ²¿åŠè¯¥é¢†åŸŸåº”ä¼˜å…ˆè§£å†³çš„å¼€æ”¾é—®é¢˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.16372",
            "title": "Low-Rank Adapters Meet Neural Architecture Search for LLM Compression",
            "url": "https://huggingface.co/papers/2501.16372",
            "abstract": "The rapid expansion of Large Language Models (LLMs) has posed significant challenges regarding the computational resources required for fine-tuning and deployment. Recent advancements in low-rank adapters have demonstrated their efficacy in parameter-efficient fine-tuning (PEFT) of these models. This retrospective paper comprehensively discusses innovative approaches that synergize low-rank representations with Neural Architecture Search (NAS) techniques, particularly weight-sharing super-networks. Robust solutions for compressing and fine-tuning large pre-trained models are developed by integrating these methodologies. Our analysis highlights the potential of these combined strategies to democratize the use of LLMs, making them more accessible for deployment in resource-constrained environments. The resulting models exhibit reduced memory footprints and faster inference times, paving the way for more practical and scalable applications of LLMs. Models and code are available at https://github.com/IntelLabs/Hardware-Aware-Automated-Machine-Learning.",
            "score": 4,
            "issue_id": 1918,
            "pub_date": "2025-01-23",
            "pub_date_card": {
                "ru": "23 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 23",
                "zh": "1æœˆ23æ—¥"
            },
            "hash": "f1d43a985dbea0af",
            "authors": [
                "J. Pablo MuÃ±oz",
                "Jinjie Yuan",
                "Nilesh Jain"
            ],
            "affiliations": [
                "Intel Corporation",
                "Intel Labs"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.16372.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#optimization",
                    "#open_source",
                    "#training",
                    "#low_resource",
                    "#architecture"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ²",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ², Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ñ… Ğ´Ğ»Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¸ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ²Ñ‹Ğµ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ñ‹ Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ (NAS) Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¶Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ´ĞµĞ»Ğ°Ñ Ğ¸Ñ… Ğ±Ğ¾Ğ»ĞµĞµ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ². Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ÑÑ‚ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¼ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ LLM."
                },
                "en": {
                    "title": "Democratizing Large Language Models with Efficient Fine-Tuning Techniques",
                    "desc": "This paper addresses the challenges of using Large Language Models (LLMs) due to their high computational demands. It explores the use of low-rank adapters for parameter-efficient fine-tuning (PEFT), which helps reduce the resources needed. The authors combine low-rank representations with Neural Architecture Search (NAS) techniques, particularly through weight-sharing super-networks, to create efficient solutions for model compression and fine-tuning. The findings suggest that these strategies can make LLMs more accessible and practical for deployment in environments with limited resources, resulting in models that are faster and require less memory."
                },
                "zh": {
                    "title": "ä½ç§©é€‚é…å™¨åŠ©åŠ›å¤§å‹è¯­è¨€æ¨¡å‹çš„é«˜æ•ˆå¾®è°ƒ",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•å¸¦æ¥äº†åœ¨å¾®è°ƒå’Œéƒ¨ç½²æ—¶å¯¹è®¡ç®—èµ„æºçš„å·¨å¤§æŒ‘æˆ˜ã€‚æœ€è¿‘ï¼Œä½ç§©é€‚é…å™¨åœ¨å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹é¢æ˜¾ç¤ºå‡ºäº†è‰¯å¥½çš„æ•ˆæœã€‚æœ¬æ–‡å›é¡¾äº†å°†ä½ç§©è¡¨ç¤ºä¸ç¥ç»æ¶æ„æœç´¢ï¼ˆNASï¼‰æŠ€æœ¯ç›¸ç»“åˆçš„åˆ›æ–°æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯æƒé‡å…±äº«è¶…ç½‘ç»œã€‚é€šè¿‡æ•´åˆè¿™äº›æ–¹æ³•ï¼Œå¼€å‘äº†å‹ç¼©å’Œå¾®è°ƒå¤§å‹é¢„è®­ç»ƒæ¨¡å‹çš„ç¨³å¥è§£å†³æ–¹æ¡ˆï¼Œä½¿å¾—LLMsåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­æ›´æ˜“äºéƒ¨ç½²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.15747",
            "title": "IndicMMLU-Pro: Benchmarking Indic Large Language Models on Multi-Task Language Understanding",
            "url": "https://huggingface.co/papers/2501.15747",
            "abstract": "Known by more than 1.5 billion people in the Indian subcontinent, Indic languages present unique challenges and opportunities for natural language processing (NLP) research due to their rich cultural heritage, linguistic diversity, and complex structures. IndicMMLU-Pro is a comprehensive benchmark designed to evaluate Large Language Models (LLMs) across Indic languages, building upon the MMLU Pro (Massive Multitask Language Understanding) framework. Covering major languages such as Hindi, Bengali, Gujarati, Marathi, Kannada, Punjabi, Tamil, Telugu, and Urdu, our benchmark addresses the unique challenges and opportunities presented by the linguistic diversity of the Indian subcontinent. This benchmark encompasses a wide range of tasks in language comprehension, reasoning, and generation, meticulously crafted to capture the intricacies of Indian languages. IndicMMLU-Pro provides a standardized evaluation framework to push the research boundaries in Indic language AI, facilitating the development of more accurate, efficient, and culturally sensitive models. This paper outlines the benchmarks' design principles, task taxonomy, and data collection methodology, and presents baseline results from state-of-the-art multilingual models.",
            "score": 4,
            "issue_id": 1918,
            "pub_date": "2025-01-27",
            "pub_date_card": {
                "ru": "27 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 27",
                "zh": "1æœˆ27æ—¥"
            },
            "hash": "4b666d035c5e5c4c",
            "authors": [
                "Sankalp KJ",
                "Ashutosh Kumar",
                "Laxmaan Balaji",
                "Nikunj Kotecha",
                "Vinija Jain",
                "Aman Chadha",
                "Sreyoshi Bhaduri"
            ],
            "affiliations": [
                "Amazon Gen AI",
                "Artificial Intelligence Institute, University of South Carolina",
                "Independent Researcher",
                "Meta AI",
                "Rochester Institute of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.15747.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#low_resource",
                    "#multilingual",
                    "#benchmark"
                ],
                "emoji": "ğŸ‡®ğŸ‡³",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² NLP: ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¸Ğ½Ğ´Ğ¸Ğ¹ÑĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²",
                    "desc": "IndicMMLU-Pro - ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¸Ğ½Ğ´Ğ¸Ğ¹ÑĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ…. ĞĞ½ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ 9 Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ˜Ğ½Ğ´Ğ¸Ğ¹ÑĞºĞ¾Ğ³Ğ¾ ÑÑƒĞ±ĞºĞ¾Ğ½Ñ‚Ğ¸Ğ½ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¹ ÑĞ¿ĞµĞºÑ‚Ñ€ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ°, Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸Ğ½Ğ´Ğ¸Ğ¹ÑĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ². IndicMMLU-Pro Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ˜Ğ˜ Ğ´Ğ»Ñ Ğ¸Ğ½Ğ´Ğ¸Ğ¹ÑĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²."
                },
                "en": {
                    "title": "Empowering Indic Languages with Advanced NLP Benchmarks",
                    "desc": "The paper introduces IndicMMLU-Pro, a benchmark specifically designed to assess Large Language Models (LLMs) in the context of Indic languages. It builds on the existing MMLU Pro framework and includes major languages like Hindi, Bengali, and Tamil, addressing the unique linguistic challenges of the Indian subcontinent. The benchmark features a variety of tasks that test language comprehension, reasoning, and generation, ensuring a comprehensive evaluation of models. By providing a standardized framework, IndicMMLU-Pro aims to enhance the development of more accurate and culturally aware AI models for Indic languages."
                },
                "zh": {
                    "title": "æ¨åŠ¨å°åº¦è¯­è¨€AIç ”ç©¶çš„åŸºå‡†",
                    "desc": "IndicMMLU-Proæ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºå°åº¦è¯­è¨€è®¾è®¡çš„åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¡¨ç°ã€‚è¯¥åŸºå‡†åŸºäºMMLU Proæ¡†æ¶ï¼Œæ¶µç›–äº†å°åœ°è¯­ã€å­ŸåŠ æ‹‰è¯­ã€å¤å‰æ‹‰ç‰¹è¯­ç­‰ä¸»è¦è¯­è¨€ï¼Œè§£å†³äº†å°åº¦æ¬¡å¤§é™†è¯­è¨€çš„å¤šæ ·æ€§å¸¦æ¥çš„æŒ‘æˆ˜ã€‚å®ƒåŒ…æ‹¬è¯­è¨€ç†è§£ã€æ¨ç†å’Œç”Ÿæˆç­‰å¤šç§ä»»åŠ¡ï¼Œæ—¨åœ¨æ•æ‰å°åº¦è¯­è¨€çš„å¤æ‚æ€§ã€‚é€šè¿‡æä¾›æ ‡å‡†åŒ–çš„è¯„ä¼°æ¡†æ¶ï¼ŒIndicMMLU-Proæ¨åŠ¨äº†å°åº¦è¯­è¨€äººå·¥æ™ºèƒ½çš„ç ”ç©¶ï¼Œä¿ƒè¿›äº†æ›´å‡†ç¡®ã€é«˜æ•ˆå’Œæ–‡åŒ–æ•æ„Ÿçš„æ¨¡å‹çš„å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.17117",
            "title": "Histoires Morales: A French Dataset for Assessing Moral Alignment",
            "url": "https://huggingface.co/papers/2501.17117",
            "abstract": "Aligning language models with human values is crucial, especially as they become more integrated into everyday life. While models are often adapted to user preferences, it is equally important to ensure they align with moral norms and behaviours in real-world social situations. Despite significant progress in languages like English and Chinese, French has seen little attention in this area, leaving a gap in understanding how LLMs handle moral reasoning in this language. To address this gap, we introduce Histoires Morales, a French dataset derived from Moral Stories, created through translation and subsequently refined with the assistance of native speakers to guarantee grammatical accuracy and adaptation to the French cultural context. We also rely on annotations of the moral values within the dataset to ensure their alignment with French norms. Histoires Morales covers a wide range of social situations, including differences in tipping practices, expressions of honesty in relationships, and responsibilities toward animals. To foster future research, we also conduct preliminary experiments on the alignment of multilingual models on French and English data and the robustness of the alignment. We find that while LLMs are generally aligned with human moral norms by default, they can be easily influenced with user-preference optimization for both moral and immoral data.",
            "score": 2,
            "issue_id": 1924,
            "pub_date": "2025-01-28",
            "pub_date_card": {
                "ru": "28 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 28",
                "zh": "1æœˆ28æ—¥"
            },
            "hash": "d2d1461e245219e8",
            "authors": [
                "Thibaud Leteno",
                "Irina Proskurina",
                "Antoine Gourru",
                "Julien Velcin",
                "Charlotte Laclau",
                "Guillaume Metzler",
                "Christophe Gravier"
            ],
            "affiliations": [
                "Laboratoire Hubert Curien, UMR CNRS 5516, Saint-Etienne, France",
                "TÃ©lÃ©com Paris, Institut Polytechnique de Paris, Paris, France",
                "UniversitÃ© LumiÃ¨re Lyon 2, UniversitÃ© Claude Bernard Lyon 1, ERIC, 69007, Lyon, France"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.17117.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#multilingual",
                    "#alignment",
                    "#ethics"
                ],
                "emoji": "ğŸ‡«ğŸ‡·",
                "ru": {
                    "title": "Ğ¤Ñ€Ğ°Ğ½Ñ†ÑƒĞ·ÑĞºĞ¸Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ¼Ğ¾Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… 'Histoires Morales' Ğ½Ğ° Ñ„Ñ€Ğ°Ğ½Ñ†ÑƒĞ·ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ 'Moral Stories' Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº Ñ„Ñ€Ğ°Ğ½Ñ†ÑƒĞ·ÑĞºĞ¾Ğ¼Ñƒ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¼Ñƒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñƒ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ñ„Ñ€Ğ°Ğ½Ñ†ÑƒĞ·ÑĞºĞ¸Ñ… Ğ¸ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ñ†ĞµĞ»Ğ¾Ğ¼ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ Ğ¼Ğ¾Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ¼, Ğ½Ğ¾ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ»ĞµĞ³ĞºĞ¾ Ğ¿Ğ¾Ğ´Ğ²ĞµÑ€Ğ¶ĞµĞ½Ñ‹ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Bridging Language Models and French Moral Values",
                    "desc": "This paper emphasizes the importance of aligning language models with human values, particularly in the context of the French language. It introduces Histoires Morales, a dataset created from Moral Stories, which has been translated and refined to reflect French cultural norms and moral reasoning. The dataset includes various social situations to better understand how language models handle moral values in French. Preliminary experiments show that while language models generally align with human morals, they can be swayed by user preferences, highlighting the need for careful optimization."
                },
                "zh": {
                    "title": "è®©è¯­è¨€æ¨¡å‹ä¸äººç±»ä»·å€¼è§‚å¯¹é½",
                    "desc": "æœ¬è®ºæ–‡å¼ºè°ƒäº†å°†è¯­è¨€æ¨¡å‹ä¸äººç±»ä»·å€¼è§‚å¯¹é½çš„é‡è¦æ€§ï¼Œå°¤å…¶æ˜¯åœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­ã€‚æˆ‘ä»¬ä»‹ç»äº†ä¸€ä¸ªåä¸ºHistoires Moralesçš„æ³•è¯­æ•°æ®é›†ï¼Œæ—¨åœ¨å¡«è¡¥æ³•è¯­åœ¨é“å¾·æ¨ç†æ–¹é¢çš„ç ”ç©¶ç©ºç™½ã€‚è¯¥æ•°æ®é›†é€šè¿‡ç¿»è¯‘å’Œæ¯è¯­è€…çš„å¸®åŠ©è¿›è¡Œç²¾ç»†åŒ–ï¼Œç¡®ä¿å…¶è¯­æ³•å‡†ç¡®å¹¶é€‚åº”æ³•å›½æ–‡åŒ–èƒŒæ™¯ã€‚æˆ‘ä»¬çš„åˆæ­¥å®éªŒè¡¨æ˜ï¼Œå°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹é€šå¸¸ä¸äººç±»é“å¾·è§„èŒƒä¸€è‡´ï¼Œä½†å®ƒä»¬å¯ä»¥é€šè¿‡ç”¨æˆ·åå¥½ä¼˜åŒ–è½»æ˜“å—åˆ°å½±å“ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-01-28.html",
    "link_next": "2025-01-30.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "28.01",
        "en": "01/28",
        "zh": "1æœˆ28æ—¥"
    },
    "short_date_next": {
        "ru": "30.01",
        "en": "01/30",
        "zh": "1æœˆ30æ—¥"
    },
    "categories": {
        "#dataset": 2,
        "#data": 0,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 0,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 1,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 2,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 5,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 5,
        "#survey": 1,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 2
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« æ¯”è¾ƒäº†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨åŸºç¡€æ¨¡å‹ä¸Šçš„ä½œç”¨ã€‚ç ”ç©¶å‘ç°ï¼ŒRLåœ¨æ–‡æœ¬å’Œè§†è§‰ä»»åŠ¡ä¸Šéƒ½è¡¨ç°å‡ºæ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚SFTå€¾å‘äºè®°ä½è®­ç»ƒæ•°æ®ï¼Œè€ŒRLèƒ½å¤Ÿå¤„ç†æœªè§è¿‡çš„å˜ä½“ã€‚RLè¿˜æé«˜äº†æ¨¡å‹çš„è§†è§‰è¯†åˆ«èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒSFTå¯¹äºRLçš„æœ‰æ•ˆè®­ç»ƒä»ç„¶ä¸å¯æˆ–ç¼ºã€‚",
        "title": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training",
        "pinyin": "è¿™ç¯‡æ–‡ç« æ¯”è¾ƒäº†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨åŸºç¡€æ¨¡å‹ä¸Šçš„ä½œç”¨ã€‚ç ”ç©¶å‘ç°ï¼ŒRLåœ¨æ–‡æœ¬å’Œè§†è§‰ä»»åŠ¡ä¸Šéƒ½è¡¨ç°å‡ºæ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚SFTå€¾å‘äºè®°ä½è®­ç»ƒæ•°æ®ï¼Œè€ŒRLèƒ½å¤Ÿå¤„ç†æœªè§è¿‡çš„å˜ä½“ã€‚RLè¿˜æé«˜äº†æ¨¡å‹çš„è§†è§‰è¯†åˆ«èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒSFTå¯¹äºRLçš„æœ‰æ•ˆè®­ç»ƒä»ç„¶ä¸å¯æˆ–ç¼ºã€‚\n\nZhÃ¨ piÄn wÃ©nzhÄng bÇjiÃ o le jiÃ ndÅ« wÄ“itiÃ¡o (SFT) hÃ© qiÃ¡ng huÃ  xuÃ©xÃ­ (RL) zÃ i jÄ«chÇ” mÃ³xÃ­ng shÃ ng de zuÃ²yÃ²ng. YÃ¡njiÅ« fÄxiÃ n, RL zÃ i wÃ©nbÄ›n hÃ© shÃ¬juÃ© rÃ¨nwÃ¹ shÃ ng dÅu biÇoxiÃ n chÅ« gÃ¨ng hÇo de fÃ nhuÃ  nÃ©nglÃ¬. SFT qÄ«ngxiÃ ng yÃº jÃ¬zhÃ¹ xÃ¹nliÃ n shÃ¹jÃ¹, Ã©r RL nÃ©nggÃ²u chÇ”lÇ wÃ¨i jiÃ nguÃ² de biÃ ntÇ. RL hÃ¡i tÃ­gÄo le mÃ³xÃ­ng de shÃ¬juÃ© shÃ­biÃ© nÃ©nglÃ¬. RÃ¡n'Ã©r, SFT duÃ¬yÃº RL de yÇ’uxiÃ o xÃ¹nliÃ n rÃ©ngrÃ¡n bÃ¹kÄ› huÃ²quÄ“.",
        "vocab": "[{'word': 'ç›‘ç£', 'pinyin': 'jiÃ n dÅ«', 'trans': 'supervised'},\n{'word': 'å¾®è°ƒ', 'pinyin': 'wÄ“i tiÃ¡o', 'trans': 'fine-tuning'},\n{'word': 'å¼ºåŒ–å­¦ä¹ ', 'pinyin': 'qiÃ¡ng huÃ  xuÃ© xÃ­', 'trans': 'reinforcement learning'},\n{'word': 'åŸºç¡€æ¨¡å‹', 'pinyin': 'jÄ« chÇ” mÃ³ xÃ­ng', 'trans': 'foundational model'},\n{'word': 'ä½œç”¨', 'pinyin': 'zuÃ² yÃ²ng', 'trans': 'effect'},\n{'word': 'æ³›åŒ–', 'pinyin': 'fÃ n huÃ ', 'trans': 'generalization'},\n{'word': 'å€¾å‘äº', 'pinyin': 'qÄ«ng xiÃ ng yÃº', 'trans': 'tend to'},\n{'word': 'æœªè§è¿‡', 'pinyin': 'wÃ¨i jiÃ n guÃ²', 'trans': 'unseen'},\n{'word': 'å˜ä½“', 'pinyin': 'biÃ n tÇ', 'trans': 'variant'},\n{'word': 'è§†è§‰è¯†åˆ«', 'pinyin': 'shÃ¬ juÃ© shÃ­ biÃ©', 'trans': 'visual recognition'},\n{'word': 'ä¸å¯æˆ–ç¼º', 'pinyin': 'bÃ¹ kÄ› huÃ² quÄ“', 'trans': 'indispensable'}]",
        "trans": "This article compares the roles of Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on base models. The study found that RL demonstrates better generalization capabilities in both textual and visual tasks. SFT tends to memorize training data, while RL can handle unseen variants. RL also enhances the model's visual recognition capabilities. However, SFT remains indispensable for effective RL training.",
        "update_ts": "2025-01-29 09:10"
    }
}