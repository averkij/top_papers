{
    "date": {
        "ru": "25 августа",
        "en": "August 25",
        "zh": "8月25日"
    },
    "time_utc": "2025-08-25 03:42",
    "weekday": 0,
    "issue_id": 5517,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2508.15881",
            "title": "TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated\n  Prefill \\& Decode Inference",
            "url": "https://huggingface.co/papers/2508.15881",
            "abstract": "Tensor-Parallel Latent Attention (TPLA) enhances tensor parallelism efficiency by partitioning latent representations and input dimensions, preserving the benefits of compressed key-value caches while maintaining strong representational capacity.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-Head Latent Attention (MLA), introduced in DeepSeek-V2, compresses key-value states into a low-rank latent vector, caching only this vector to reduce memory. In tensor parallelism (TP), however, attention heads are computed across multiple devices, and each device must load the full cache, eroding the advantage of MLA over Grouped Query Attention (GQA). We propose Tensor-Parallel Latent Attention (TPLA): a scheme that partitions both the latent representation and each head's input dimension across devices, performs attention independently per shard, and then combines results with an all-reduce. TPLA preserves the benefits of a compressed KV cache while unlocking TP efficiency. Unlike Grouped Latent Attention (GLA), every head in TPLA still leverages the full latent representation, maintaining stronger representational capacity. TPLA is drop-in compatible with models pre-trained using MLA: it supports MLA-style prefilling and enables efficient tensor-parallel decoding without retraining. Applying simple orthogonal transforms -- e.g., the Hadamard transform or PCA -- before TP slicing further mitigates cross-shard interference, yielding minimal accuracy degradation. By reducing the per-device KV cache for DeepSeek-V3 and Kimi-K2, we achieve 1.79x and 1.93x speedups, respectively, at a 32K-token context length while maintaining performance on commonsense and LongBench benchmarks. TPLA can be implemented with FlashAttention-3, enabling practical end-to-end acceleration.",
            "score": 3,
            "issue_id": 5516,
            "pub_date": "2025-08-21",
            "pub_date_card": {
                "ru": "21 августа",
                "en": "August 21",
                "zh": "8月21日"
            },
            "hash": "603c38aea1aa4a8f",
            "authors": [
                "Xiaojuan Tang",
                "Fanxu Meng",
                "Pingzhi Tang",
                "Yuxuan Wang",
                "Di Yin",
                "Xing Sun",
                "Muhan Zhang"
            ],
            "affiliations": [
                "Institute for Artificial Intelligence, Peking University",
                "State Key Laboratory of General Artificial Intelligence, BIGAI",
                "Tencent Youtu Lab, Shanghai, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.15881.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#training",
                    "#benchmark",
                    "#long_context"
                ],
                "emoji": "⚡",
                "ru": {
                    "title": "TPLA: Эффективный тензорный параллелизм для ускорения обработки длинных последовательностей",
                    "desc": "Статья представляет новый метод тензорного параллелизма для моделей машинного обучения - Tensor-Parallel Latent Attention (TPLA). TPLA разделяет латентные представления и входные размерности между устройствами, сохраняя преимущества сжатого кэша ключей и значений. Этот подход позволяет эффективно распараллеливать вычисления, сохраняя при этом сильную репрезентативную способность модели. TPLA совместим с предобученными моделями и показывает значительное ускорение обработки длинных последовательностей."
                },
                "en": {
                    "title": "Boosting Tensor Parallelism with TPLA",
                    "desc": "Tensor-Parallel Latent Attention (TPLA) improves the efficiency of tensor parallelism by dividing latent representations and input dimensions across multiple devices. This method retains the advantages of compressed key-value caches while ensuring that each attention head can still utilize the full latent representation, thus enhancing its representational capacity. TPLA is compatible with models that have been pre-trained using Multi-Head Latent Attention (MLA), allowing for efficient tensor-parallel decoding without the need for retraining. By applying orthogonal transforms before partitioning, TPLA minimizes cross-shard interference, leading to significant speedups in processing time while maintaining accuracy on various benchmarks."
                },
                "zh": {
                    "title": "张量并行潜在注意力：提升效率与表现的完美结合",
                    "desc": "Tensor-Parallel Latent Attention (TPLA) 是一种提高张量并行效率的方法，通过在设备之间划分潜在表示和输入维度，保持压缩的键值缓存的优势，同时保持强大的表示能力。与多头潜在注意力（MLA）相比，TPLA 允许每个头在不同设备上独立计算注意力，并通过全归约组合结果，从而提高了效率。TPLA 兼容使用 MLA 预训练的模型，支持 MLA 风格的预填充，并实现高效的张量并行解码，而无需重新训练。通过在 TP 切片之前应用简单的正交变换，可以进一步减少跨分片干扰，确保在保持性能的同时实现显著的加速。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.16402",
            "title": "AetherCode: Evaluating LLMs' Ability to Win In Premier Programming\n  Competitions",
            "url": "https://huggingface.co/papers/2508.16402",
            "abstract": "AetherCode is a new benchmark for evaluating Large Language Models in competitive programming, offering more challenging and expert-validated test cases than existing benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Competitive programming has emerged as a critical benchmark for evaluating the reasoning and coding capabilities of Large Language Models (LLMs). Despite impressive progress on existing benchmarks, we argue that current evaluations overstate model proficiency, masking a substantial gap between LLMs and elite human programmers. This gap arises from two key limitations: insufficient difficulty and scope of benchmark problems, and evaluation bias from low-quality test cases. To address these shortcomings, we present AetherCode, a new benchmark that draws problems from premier programming competitions such as IOI and ICPC, offering broader coverage and higher difficulty. AetherCode further incorporates comprehensive, expert-validated test suites built through a hybrid of automated generation and human curation, ensuring rigorous and reliable assessment. By combining challenging problem design with robust evaluation, AetherCode provides a more faithful measure of LLM capabilities and sets a new standard for future research in code reasoning.",
            "score": 2,
            "issue_id": 5517,
            "pub_date": "2025-08-22",
            "pub_date_card": {
                "ru": "22 августа",
                "en": "August 22",
                "zh": "8月22日"
            },
            "hash": "e6c79bec20f431d6",
            "authors": [
                "Zihan Wang",
                "Jiaze Chen",
                "Zhicheng Liu",
                "Markus Mak",
                "Yidi Du",
                "Geonsik Moon",
                "Luoqi Xu",
                "Aaron Tua",
                "Kunshuo Peng",
                "Jiayi Lu",
                "Mingfei Xia",
                "Boqian Zou",
                "Chenyang Ran",
                "Guang Tian",
                "Shoutai Zhu",
                "Yeheng Duan",
                "Zhenghui Kang",
                "Zhenxing Lin",
                "Shangshu Li",
                "Qiang Luo",
                "Qingshen Long",
                "Zhiyong Chen",
                "Yihan Xiao",
                "Yurong Wu",
                "Daoguang Zan",
                "Yuyi Fu",
                "Mingxuan Wang",
                "Ming Ding"
            ],
            "affiliations": [
                "ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.16402.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "🏆",
                "ru": {
                    "title": "AetherCode: Новая планка в оценке ИИ-программистов",
                    "desc": "AetherCode - новый эталон для оценки больших языковых моделей в соревновательном программировании. Он предлагает более сложные и экспертно проверенные тестовые случаи, чем существующие бенчмарки. AetherCode использует задачи из престижных соревнований по программированию, таких как IOI и ICPC, обеспечивая более широкий охват и повышенную сложность. Бенчмарк включает в себя комплексные наборы тестов, созданные с помощью гибридного подхода автоматической генерации и ручной курации."
                },
                "en": {
                    "title": "AetherCode: Raising the Bar for LLM Evaluation in Competitive Programming",
                    "desc": "AetherCode is a benchmark designed to evaluate Large Language Models (LLMs) specifically in the context of competitive programming. It addresses the limitations of existing benchmarks by providing more difficult and expertly validated test cases, which better reflect the skills of elite human programmers. The benchmark includes problems sourced from prestigious competitions like IOI and ICPC, ensuring a wider range of challenges. By combining automated generation with human curation, AetherCode aims to deliver a more accurate assessment of LLM capabilities in coding and reasoning tasks."
                },
                "zh": {
                    "title": "AetherCode：提升大型语言模型评估标准的基准",
                    "desc": "AetherCode是一个新的基准，用于评估大型语言模型在竞争编程中的表现。与现有基准相比，它提供了更具挑战性和经过专家验证的测试案例。当前的评估往往夸大了模型的能力，掩盖了大型语言模型与顶尖人类程序员之间的差距。AetherCode通过从顶级编程竞赛中提取问题，结合自动生成和人工策划的测试套件，确保了评估的严格性和可靠性。"
                }
            }
        }
    ],
    "link_prev": "2025-08-22.html",
    "link_next": "2025-08-26.html",
    "link_month": "2025-08.html",
    "short_date_prev": {
        "ru": "22.08",
        "en": "08/22",
        "zh": "8月22日"
    },
    "short_date_next": {
        "ru": "26.08",
        "en": "08/26",
        "zh": "8月26日"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}