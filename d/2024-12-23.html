
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 10 papers. December 23.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">23 декабря</span> | <span id="title-articles-count">10 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2024-12-20.html">⬅️ <span id="prev-date">20.12</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-12-24.html">➡️ <span id="next-date">24.12</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2024-12.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '23 декабря', 'en': 'December 23', 'zh': '12月23日'};
        let feedDateNext = {'ru': '24.12', 'en': '12/24', 'zh': '12月24日'};
        let feedDatePrev = {'ru': '20.12', 'en': '12/20', 'zh': '12月20日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2412.15119', 'title': 'Parallelized Autoregressive Visual Generation', 'url': 'https://huggingface.co/papers/2412.15119', 'abstract': 'Autoregressive models have emerged as a powerful approach for visual generation but suffer from slow inference speed due to their sequential token-by-token prediction process. In this paper, we propose a simple yet effective approach for parallelized autoregressive visual generation that improves generation efficiency while preserving the advantages of autoregressive modeling. Our key insight is that parallel generation depends on visual token dependencies-tokens with weak dependencies can be generated in parallel, while strongly dependent adjacent tokens are difficult to generate together, as their independent sampling may lead to inconsistencies. Based on this observation, we develop a parallel generation strategy that generates distant tokens with weak dependencies in parallel while maintaining sequential generation for strongly dependent local tokens. Our approach can be seamlessly integrated into standard autoregressive models without modifying the architecture or tokenizer. Experiments on ImageNet and UCF-101 demonstrate that our method achieves a 3.6x speedup with comparable quality and up to 9.5x speedup with minimal quality degradation across both image and video generation tasks. We hope this work will inspire future research in efficient visual generation and unified autoregressive modeling. Project page: https://epiphqny.github.io/PAR-project.', 'score': 29, 'issue_id': 1258, 'pub_date': '2024-12-19', 'pub_date_card': {'ru': '19 декабря', 'en': 'December 19', 'zh': '12月19日'}, 'hash': '0933582baa02f7a6', 'authors': ['Yuqing Wang', 'Shuhuai Ren', 'Zhijie Lin', 'Yujin Han', 'Haoyuan Guo', 'Zhenheng Yang', 'Difan Zou', 'Jiashi Feng', 'Xihui Liu'], 'affiliations': ['ByteDance Seed', 'Peking University', 'University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2412.15119.jpg', 'data': {'categories': ['#training', '#inference', '#video', '#cv', '#architecture', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'Ускорение авторегрессионной генерации изображений и видео без потери качества', 'desc': 'Статья предлагает метод параллельного авторегрессионного генерирования визуального контента, который ускоряет процесс, сохраняя преимущества авторегрессионного моделирования. Авторы разработали стратегию, которая генерирует удаленные токены с слабыми зависимостями параллельно, но сохраняет последовательное генерирование для сильно зависимых локальных токенов. Метод легко интегрируется в стандартные авторегрессионные модели без изменения архитектуры или токенизатора. Эксперименты показали ускорение до 3.6 раз с сопоставимым качеством и до 9.5 раз с минимальной деградацией качества для задач генерации изображений и видео.'}, 'en': {'title': 'Speeding Up Visual Generation with Smart Token Parallelization', 'desc': 'This paper addresses the slow inference speed of autoregressive models used for visual generation, which typically generate images or videos one token at a time. The authors propose a new method that allows for parallel generation of visual tokens, focusing on the dependencies between tokens to determine which can be generated simultaneously. By identifying weakly dependent tokens that can be generated in parallel, while keeping strongly dependent tokens in a sequential order, the method enhances efficiency without altering the existing model architecture. Experiments show significant speed improvements in generating images and videos, making this approach a promising direction for future research in visual generation.'}, 'zh': {'title': '并行自回归生成，提升视觉生成效率', 'desc': '自回归模型在视觉生成中表现出色，但由于逐个预测的过程，推理速度较慢。本文提出了一种简单有效的并行自回归视觉生成方法，旨在提高生成效率，同时保留自回归建模的优点。我们的关键见解是，视觉标记之间的依赖关系决定了并行生成的可能性，弱依赖的标记可以并行生成，而强依赖的标记则难以一起生成。实验结果表明，我们的方法在图像和视频生成任务中实现了3.6倍的速度提升，且质量保持相当，甚至在某些情况下可达到9.5倍的速度提升。'}}}, {'id': 'https://huggingface.co/papers/2412.16145', 'title': 'Offline Reinforcement Learning for LLM Multi-Step Reasoning', 'url': 'https://huggingface.co/papers/2412.16145', 'abstract': 'Improving the multi-step reasoning ability of large language models (LLMs) with offline reinforcement learning (RL) is essential for quickly adapting them to complex tasks. While Direct Preference Optimization (DPO) has shown promise in aligning LLMs with human preferences, it is less suitable for multi-step reasoning tasks because (1) DPO relies on paired preference data, which is not readily available for multi-step reasoning tasks, and (2) it treats all tokens uniformly, making it ineffective for credit assignment in multi-step reasoning tasks, which often come with sparse reward. In this work, we propose OREO (Offline Reasoning Optimization), an offline RL method for enhancing LLM multi-step reasoning. Building on insights from previous works of maximum entropy reinforcement learning, it jointly learns a policy model and value function by optimizing the soft Bellman Equation. We show in principle that it reduces the need to collect pairwise data and enables better credit assignment. Empirically, OREO surpasses existing offline learning methods on multi-step reasoning benchmarks, including mathematical reasoning tasks (GSM8K, MATH) and embodied agent control (ALFWorld). The approach can be extended to a multi-iteration framework when additional resources are available. Furthermore, the learned value function can be leveraged to guide the tree search for free, which can further boost performance during test time.', 'score': 15, 'issue_id': 1260, 'pub_date': '2024-12-20', 'pub_date_card': {'ru': '20 декабря', 'en': 'December 20', 'zh': '12月20日'}, 'hash': '5779a845f782fb45', 'authors': ['Huaijie Wang', 'Shibo Hao', 'Hanze Dong', 'Shenao Zhang', 'Yilin Bao', 'Ziran Yang', 'Yi Wu'], 'affiliations': ['Northwestern University', 'Salesforce Research', 'Tsinghua University', 'UC San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2412.16145.jpg', 'data': {'categories': ['#benchmark', '#rl', '#optimization', '#math', '#rlhf', '#agents', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'OREO: Оптимизация многошаговых рассуждений для языковых моделей', 'desc': 'В этой статье представлен метод OREO (Offline Reasoning Optimization) для улучшения способностей больших языковых моделей (LLM) к многошаговым рассуждениям с помощью обучения с подкреплением в офлайн-режиме. OREO совместно обучает модель политики и функцию ценности, оптимизируя мягкое уравнение Беллмана. Метод превосходит существующие офлайн-методы обучения на задачах многошагового рассуждения, включая математические задачи и управление агентами. OREO также может быть расширен до многоитерационной структуры и использован для направления древовидного поиска во время тестирования.'}, 'en': {'title': 'OREO: Enhancing Multi-Step Reasoning in LLMs with Offline RL', 'desc': "This paper introduces OREO, an offline reinforcement learning method designed to improve the multi-step reasoning capabilities of large language models (LLMs). Unlike Direct Preference Optimization, which struggles with multi-step tasks due to its reliance on paired preference data and uniform token treatment, OREO effectively addresses these challenges by optimizing the soft Bellman Equation. The method enhances credit assignment and reduces the need for extensive data collection, leading to superior performance on reasoning benchmarks. Additionally, OREO's learned value function can be utilized to enhance search strategies during testing, further improving outcomes."}, 'zh': {'title': 'OREO：提升大型语言模型的多步推理能力', 'desc': '本论文提出了一种名为OREO的离线强化学习方法，旨在提高大型语言模型（LLMs）的多步推理能力。与直接偏好优化（DPO）不同，OREO不依赖于成对的偏好数据，适用于多步推理任务。该方法通过优化软贝尔曼方程，联合学习策略模型和价值函数，从而改善了奖励稀疏情况下的信用分配问题。实验结果表明，OREO在数学推理和智能体控制等多步推理基准测试中优于现有的离线学习方法。'}}}, {'id': 'https://huggingface.co/papers/2412.13649', 'title': 'SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation', 'url': 'https://huggingface.co/papers/2412.13649', 'abstract': 'Key-Value (KV) cache has become a bottleneck of LLMs for long-context generation. Despite the numerous efforts in this area, the optimization for the decoding phase is generally ignored. However, we believe such optimization is crucial, especially for long-output generation tasks based on the following two observations: (i) Excessive compression during the prefill phase, which requires specific full context impairs the comprehension of the reasoning task; (ii) Deviation of heavy hitters occurs in the reasoning tasks with long outputs. Therefore, SCOPE, a simple yet efficient framework that separately performs KV cache optimization during the prefill and decoding phases, is introduced. Specifically, the KV cache during the prefill phase is preserved to maintain the essential information, while a novel strategy based on sliding is proposed to select essential heavy hitters for the decoding phase. Memory usage and memory transfer are further optimized using adaptive and discontinuous strategies. Extensive experiments on LongGenBench show the effectiveness and generalization of SCOPE and its compatibility as a plug-in to other prefill-only KV compression methods.', 'score': 14, 'issue_id': 1258, 'pub_date': '2024-12-18', 'pub_date_card': {'ru': '18 декабря', 'en': 'December 18', 'zh': '12月18日'}, 'hash': '885d11532659dd95', 'authors': ['Jialong Wu', 'Zhenglin Wang', 'Linhai Zhang', 'Yilong Lai', 'Yulan He', 'Deyu Zhou'], 'affiliations': ['Department of Informatics, Kings College London, UK', 'School of Computer Science and Engineering, Key Laboratory of Computer Network and Information Integration, Ministry of Education, Southeast University, China', 'The Alan Turing Institute, UK'], 'pdf_title_img': 'assets/pdf/title_img/2412.13649.jpg', 'data': {'categories': ['#training', '#long_context', '#inference', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Эффективная оптимизация KV-кэша для длинных контекстов в LLM', 'desc': 'Статья представляет SCOPE - фреймворк для оптимизации KV-кэша в моделях LLM при генерации длинных текстов. Авторы предлагают раздельную оптимизацию для этапов prefill и decoding, сохраняя важную информацию на первом этапе и выбирая ключевые элементы на втором. Используются адаптивные и прерывистые стратегии для оптимизации использования памяти. Эксперименты на LongGenBench показывают эффективность и обобщаемость SCOPE, а также его совместимость с другими методами сжатия KV-кэша.'}, 'en': {'title': 'Optimizing KV Caches for Enhanced Long-Context Generation', 'desc': 'This paper addresses the limitations of Key-Value (KV) caches in large language models (LLMs) when generating long outputs. It highlights that optimizing the decoding phase is essential, as excessive compression during the prefill phase can hinder reasoning tasks. The proposed framework, SCOPE, optimizes KV cache usage by preserving crucial information during prefill and employing a sliding strategy to select important data during decoding. Experimental results demonstrate that SCOPE improves memory efficiency and can be integrated with existing KV compression methods.'}, 'zh': {'title': '优化KV缓存，提升长输出生成效率', 'desc': '本文提出了一种名为SCOPE的框架，旨在优化长输出生成任务中的KV缓存。研究表明，在预填充阶段过度压缩会影响推理任务的理解，因此需要保留关键信息。SCOPE通过在预填充和解码阶段分别优化KV缓存，采用滑动策略选择重要的重击项，从而提高解码效率。实验结果表明，SCOPE在LongGenBench上表现出色，并且可以作为其他KV压缩方法的插件使用。'}}}, {'id': 'https://huggingface.co/papers/2412.16112', 'title': 'CLEAR: Conv-Like Linearization Revs Pre-Trained Diffusion Transformers Up', 'url': 'https://huggingface.co/papers/2412.16112', 'abstract': 'Diffusion Transformers (DiT) have become a leading architecture in image generation. However, the quadratic complexity of attention mechanisms, which are responsible for modeling token-wise relationships, results in significant latency when generating high-resolution images. To address this issue, we aim at a linear attention mechanism in this paper that reduces the complexity of pre-trained DiTs to linear. We begin our exploration with a comprehensive summary of existing efficient attention mechanisms and identify four key factors crucial for successful linearization of pre-trained DiTs: locality, formulation consistency, high-rank attention maps, and feature integrity. Based on these insights, we introduce a convolution-like local attention strategy termed CLEAR, which limits feature interactions to a local window around each query token, and thus achieves linear complexity. Our experiments indicate that, by fine-tuning the attention layer on merely 10K self-generated samples for 10K iterations, we can effectively transfer knowledge from a pre-trained DiT to a student model with linear complexity, yielding results comparable to the teacher model. Simultaneously, it reduces attention computations by 99.5% and accelerates generation by 6.3 times for generating 8K-resolution images. Furthermore, we investigate favorable properties in the distilled attention layers, such as zero-shot generalization cross various models and plugins, and improved support for multi-GPU parallel inference. Models and codes are available here: https://github.com/Huage001/CLEAR.', 'score': 9, 'issue_id': 1259, 'pub_date': '2024-12-20', 'pub_date_card': {'ru': '20 декабря', 'en': 'December 20', 'zh': '12月20日'}, 'hash': 'c17ca50dc03ea86c', 'authors': ['Songhua Liu', 'Zhenxiong Tan', 'Xinchao Wang'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2412.16112.jpg', 'data': {'categories': ['#optimization', '#transfer_learning', '#training', '#inference', '#architecture', '#diffusion'], 'emoji': '🚀', 'ru': {'title': 'CLEAR: Ускорение DiT без потери качества', 'desc': 'Статья представляет новый метод линейного внимания для Diffusion Transformers (DiT), называемый CLEAR. Этот подход снижает сложность предобученных DiT с квадратичной до линейной, сохраняя при этом качество генерации изображений. CLEAR использует локальное внимание, подобное свёрточным операциям, ограничивая взаимодействие признаков локальным окном вокруг каждого токена запроса. Эксперименты показывают, что fine-tuning слоя внимания на всего 10 тысячах сгенерированных образцов позволяет эффективно передать знания от предобученной модели к модели с линейной сложностью.'}, 'en': {'title': 'Linearizing Attention for Faster Image Generation with CLEAR', 'desc': 'This paper presents a new approach to improve the efficiency of Diffusion Transformers (DiT) in image generation by introducing a linear attention mechanism. The authors identify key factors necessary for linearizing DiTs, such as locality and feature integrity, and propose a local attention strategy called CLEAR. This method significantly reduces the computational complexity of attention mechanisms, achieving a 99.5% reduction in attention computations and a 6.3 times speedup in generating high-resolution images. Additionally, the study shows that the distilled model retains performance comparable to the original DiT while enabling better generalization and multi-GPU support.'}, 'zh': {'title': '线性注意力，快速生成高分辨率图像！', 'desc': '本文提出了一种新的线性注意力机制，旨在解决扩散变换器（DiT）在生成高分辨率图像时的延迟问题。我们总结了现有的高效注意力机制，并确定了成功线性化预训练DiT的四个关键因素。基于这些见解，我们引入了一种名为CLEAR的局部注意力策略，限制特征交互在每个查询标记周围的局部窗口内，从而实现线性复杂度。实验结果表明，通过对注意力层进行微调，我们可以有效地将知识从预训练的DiT转移到学生模型，同时显著减少计算量并加速生成过程。'}}}, {'id': 'https://huggingface.co/papers/2412.15322', 'title': 'Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis', 'url': 'https://huggingface.co/papers/2412.15322', 'abstract': 'We propose to synthesize high-quality and synchronized audio, given video and optional text conditions, using a novel multimodal joint training framework MMAudio. In contrast to single-modality training conditioned on (limited) video data only, MMAudio is jointly trained with larger-scale, readily available text-audio data to learn to generate semantically aligned high-quality audio samples. Additionally, we improve audio-visual synchrony with a conditional synchronization module that aligns video conditions with audio latents at the frame level. Trained with a flow matching objective, MMAudio achieves new video-to-audio state-of-the-art among public models in terms of audio quality, semantic alignment, and audio-visual synchronization, while having a low inference time (1.23s to generate an 8s clip) and just 157M parameters. MMAudio also achieves surprisingly competitive performance in text-to-audio generation, showing that joint training does not hinder single-modality performance. Code and demo are available at: https://hkchengrex.github.io/MMAudio', 'score': 8, 'issue_id': 1258, 'pub_date': '2024-12-19', 'pub_date_card': {'ru': '19 декабря', 'en': 'December 19', 'zh': '12月19日'}, 'hash': '9d724184f50de930', 'authors': ['Ho Kei Cheng', 'Masato Ishii', 'Akio Hayakawa', 'Takashi Shibuya', 'Alexander Schwing', 'Yuki Mitsufuji'], 'affiliations': ['Sony AI', 'Sony Group Corporation', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2412.15322.jpg', 'data': {'categories': ['#small_models', '#audio', '#inference', '#video', '#multimodal', '#synthetic'], 'emoji': '🎵', 'ru': {'title': 'MMAudio: Революция в синтезе аудио по видео и тексту', 'desc': 'MMAudio - это новая мультимодальная система для синтеза высококачественного и синхронизированного аудио на основе видео и опционального текста. Она использует совместное обучение на аудио-текстовых данных и условный модуль синхронизации для улучшения соответствия видео и звука. MMAudio достигает нового уровня качества в задаче генерации аудио по видео, превосходя существующие модели по качеству звука, семантическому соответствию и синхронизации. Модель также показывает хорошие результаты в генерации аудио по тексту.'}, 'en': {'title': 'MMAudio: High-Quality Audio Synthesis with Video and Text Integration', 'desc': 'This paper introduces MMAudio, a novel framework for generating high-quality audio that is synchronized with video and can also utilize text inputs. Unlike traditional methods that rely solely on video data, MMAudio leverages a larger dataset of text-audio pairs to enhance the semantic alignment of the generated audio. The framework includes a conditional synchronization module that ensures audio is aligned with video at the frame level, improving overall coherence. With a flow matching objective, MMAudio sets new benchmarks in audio quality and synchronization while maintaining efficient performance with a low number of parameters.'}, 'zh': {'title': 'MMAudio：高质量音频合成的新方法', 'desc': '我们提出了一种新的多模态联合训练框架MMAudio，用于合成高质量和同步的音频，基于视频和可选的文本条件。与仅依赖视频数据的单模态训练不同，MMAudio结合了大规模的文本-音频数据进行联合训练，以生成语义对齐的高质量音频样本。此外，我们通过条件同步模块在帧级别上对视频条件和音频潜在特征进行对齐，从而提高音频与视频的同步性。MMAudio在音频质量、语义对齐和音频-视觉同步方面达到了新的公共模型的最佳水平，同时推理时间低（生成8秒片段仅需1.23秒），参数量仅为157M。'}}}, {'id': 'https://huggingface.co/papers/2412.14590', 'title': 'MixLLM: LLM Quantization with Global Mixed-precision between Output-features and Highly-efficient System Design', 'url': 'https://huggingface.co/papers/2412.14590', 'abstract': 'Quantization has become one of the most effective methodologies to compress LLMs into smaller size. However, the existing quantization solutions still show limitations of either non-negligible accuracy drop or system inefficiency. In this paper, we make a comprehensive analysis of the general quantization principles on their effect to the triangle of accuracy, memory consumption and system efficiency. We propose MixLLM that explores the new optimization space of mixed-precision quantization between output features based on the insight that different output features matter differently in the model. MixLLM identifies the output features with high salience in the global view rather than within each single layer, effectively assigning the larger bit-width to output features that need it most to achieve good accuracy with low memory consumption. We present the sweet spot of quantization configuration of algorithm-system co-design that leads to high accuracy and system efficiency. To address the system challenge, we design the two-step dequantization to make use of the int8 Tensor Core easily and fast data type conversion to reduce dequantization overhead significantly, and present the software pipeline to overlap the memory access, dequantization and the MatMul to the best. Extensive experiments show that with only 10% more bits, the PPL increasement can be reduced from about 0.5 in SOTA to within 0.2 for Llama 3.1 70B, while on average MMLU-Pro improves by 0.93 over the SOTA of three popular models. In addition to its superior accuracy, MixLLM also achieves state-of-the-art system efficiency.', 'score': 4, 'issue_id': 1260, 'pub_date': '2024-12-19', 'pub_date_card': {'ru': '19 декабря', 'en': 'December 19', 'zh': '12月19日'}, 'hash': '3a5b6d590eec2c6e', 'authors': ['Zhen Zheng', 'Xiaonan Song', 'Chuanjie Liu'], 'affiliations': ['Microsoft'], 'pdf_title_img': 'assets/pdf/title_img/2412.14590.jpg', 'data': {'categories': ['#optimization', '#inference', '#training'], 'emoji': '🧠', 'ru': {'title': 'MixLLM: Оптимальное квантование для эффективных языковых моделей', 'desc': 'Статья представляет новый метод квантования больших языковых моделей под названием MixLLM. Авторы предлагают использовать смешанную точность квантования для выходных признаков, основываясь на их важности в глобальном контексте модели. MixLLM оптимизирует баланс между точностью, потреблением памяти и эффективностью системы. Эксперименты показывают, что MixLLM достигает лучшей точности и эффективности по сравнению с существующими методами квантования для популярных языковых моделей.'}, 'en': {'title': 'MixLLM: Optimizing Quantization for Accuracy and Efficiency in LLMs', 'desc': 'This paper discusses a new method called MixLLM for quantizing large language models (LLMs) to make them smaller and more efficient. The authors analyze how different quantization techniques affect accuracy, memory use, and system performance. MixLLM uses mixed-precision quantization, assigning more bits to important output features, which helps maintain accuracy while reducing memory consumption. The proposed method also includes a two-step dequantization process to improve speed and efficiency, resulting in better performance compared to existing solutions.'}, 'zh': {'title': 'MixLLM：高效的混合精度量化方案', 'desc': '量化技术已成为压缩大型语言模型（LLMs）的一种有效方法，但现有的量化方案在准确性和系统效率上仍存在局限性。本文对量化原则进行了全面分析，探讨了准确性、内存消耗和系统效率之间的关系。我们提出了MixLLM，利用混合精度量化优化输出特征，确保重要特征获得更高的位宽，从而在保持良好准确性的同时降低内存消耗。通过设计两步反量化和优化软件管道，MixLLM在准确性和系统效率上都达到了最先进的水平。'}}}, {'id': 'https://huggingface.co/papers/2412.15035', 'title': 'LLMs Lost in Translation: M-ALERT uncovers Cross-Linguistic Safety Gaps', 'url': 'https://huggingface.co/papers/2412.15035', 'abstract': 'Building safe Large Language Models (LLMs) across multiple languages is essential in ensuring both safe access and linguistic diversity. To this end, we introduce M-ALERT, a multilingual benchmark that evaluates the safety of LLMs in five languages: English, French, German, Italian, and Spanish. M-ALERT includes 15k high-quality prompts per language, totaling 75k, following the detailed ALERT taxonomy. Our extensive experiments on 10 state-of-the-art LLMs highlight the importance of language-specific safety analysis, revealing that models often exhibit significant inconsistencies in safety across languages and categories. For instance, Llama3.2 shows high unsafety in the category crime_tax for Italian but remains safe in other languages. Similar differences can be observed across all models. In contrast, certain categories, such as substance_cannabis and crime_propaganda, consistently trigger unsafe responses across models and languages. These findings underscore the need for robust multilingual safety practices in LLMs to ensure safe and responsible usage across diverse user communities.', 'score': 2, 'issue_id': 1265, 'pub_date': '2024-12-19', 'pub_date_card': {'ru': '19 декабря', 'en': 'December 19', 'zh': '12月19日'}, 'hash': 'ec88d82d8720bf3d', 'authors': ['Felix Friedrich', 'Simone Tedeschi', 'Patrick Schramowski', 'Manuel Brack', 'Roberto Navigli', 'Huu Nguyen', 'Bo Li', 'Kristian Kersting'], 'affiliations': ['CERTAIN', 'DFKI', 'Hessian.AI', 'Ontocord.AI', 'Sapienza University of Rome', 'TU Darmstadt', 'UIUC', 'University of Chicago', 'Virtue.ai'], 'pdf_title_img': 'assets/pdf/title_img/2412.15035.jpg', 'data': {'categories': ['#low_resource', '#multilingual', '#ethics', '#benchmark'], 'emoji': '🌐', 'ru': {'title': 'Многоязычная оценка безопасности LLM: неожиданные различия между языками', 'desc': 'Статья представляет M-ALERT - многоязычный бенчмарк для оценки безопасности больших языковых моделей (LLM) на пяти языках. Авторы провели эксперименты на 10 современных LLM, выявив значительные различия в безопасности между языками и категориями. Например, модель Llama3.2 показала высокую небезопасность в категории преступлений и налогов для итальянского языка, оставаясь безопасной на других языках. Результаты подчеркивают необходимость разработки надежных многоязычных практик безопасности для LLM.'}, 'en': {'title': 'Ensuring Multilingual Safety in Large Language Models', 'desc': 'This paper presents M-ALERT, a multilingual benchmark designed to assess the safety of Large Language Models (LLMs) in five different languages. It includes a comprehensive set of 75,000 prompts categorized according to the ALERT taxonomy, allowing for detailed safety evaluations. The study reveals that LLMs often show varying levels of safety across languages, with some models performing poorly in specific categories for certain languages. These results highlight the necessity for tailored safety measures in LLMs to accommodate linguistic diversity and ensure responsible usage.'}, 'zh': {'title': '构建安全的多语言大型语言模型', 'desc': '本文介绍了M-ALERT，这是一个多语言基准，用于评估大型语言模型（LLMs）的安全性。该基准涵盖英语、法语、德语、意大利语和西班牙语，共包含75,000个高质量提示。研究表明，不同语言和类别之间的安全性存在显著不一致，某些模型在特定语言中表现出高风险。研究结果强调了在多语言环境中实施强有力的安全实践的重要性，以确保用户的安全和负责任的使用。'}}}, {'id': 'https://huggingface.co/papers/2412.11525', 'title': 'Sequence Matters: Harnessing Video Models in 3D Super-Resolution', 'url': 'https://huggingface.co/papers/2412.11525', 'abstract': "3D super-resolution aims to reconstruct high-fidelity 3D models from low-resolution (LR) multi-view images. Early studies primarily focused on single-image super-resolution (SISR) models to upsample LR images into high-resolution images. However, these methods often lack view consistency because they operate independently on each image. Although various post-processing techniques have been extensively explored to mitigate these inconsistencies, they have yet to fully resolve the issues. In this paper, we perform a comprehensive study of 3D super-resolution by leveraging video super-resolution (VSR) models. By utilizing VSR models, we ensure a higher degree of spatial consistency and can reference surrounding spatial information, leading to more accurate and detailed reconstructions. Our findings reveal that VSR models can perform remarkably well even on sequences that lack precise spatial alignment. Given this observation, we propose a simple yet practical approach to align LR images without involving fine-tuning or generating 'smooth' trajectory from the trained 3D models over LR images. The experimental results show that the surprisingly simple algorithms can achieve the state-of-the-art results of 3D super-resolution tasks on standard benchmark datasets, such as the NeRF-synthetic and MipNeRF-360 datasets. Project page: https://ko-lani.github.io/Sequence-Matters", 'score': 2, 'issue_id': 1265, 'pub_date': '2024-12-16', 'pub_date_card': {'ru': '16 декабря', 'en': 'December 16', 'zh': '12月16日'}, 'hash': '39960ceb17dab1a5', 'authors': ['Hyun-kyu Ko', 'Dongheok Park', 'Youngin Park', 'Byeonghyeon Lee', 'Juhee Han', 'Eunbyung Park'], 'affiliations': ['Department of Artificial Intelligence, Sungkyunkwan University, Suwon, Korea', 'Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon, Korea', 'Visual Display Division, Samsung Electronics'], 'pdf_title_img': 'assets/pdf/title_img/2412.11525.jpg', 'data': {'categories': ['#video', '#3d', '#benchmark'], 'emoji': '🔍', 'ru': {'title': 'Видео-суперразрешение открывает новые горизонты в 3D реконструкции', 'desc': 'Эта статья посвящена 3D суперразрешению - методу реконструкции высококачественных 3D моделей из низкоразрешающих многоракурсных изображений. Авторы предлагают использовать модели видео-суперразрешения (VSR) вместо традиционных методов суперразрешения одиночных изображений. Такой подход обеспечивает лучшую пространственную согласованность и позволяет использовать информацию из окружающего контекста. Предложенный метод включает простой алгоритм выравнивания изображений низкого разрешения и демонстрирует высокие результаты на стандартных наборах данных.'}, 'en': {'title': 'Enhancing 3D Models with Video Super-Resolution Techniques', 'desc': 'This paper focuses on improving 3D super-resolution, which is the process of creating high-quality 3D models from low-resolution images taken from different angles. Traditional methods often struggle with maintaining consistency across different views because they treat each image separately. The authors propose using video super-resolution (VSR) techniques to enhance spatial consistency and leverage information from surrounding images, resulting in better 3D reconstructions. Their approach is simple and effective, achieving state-of-the-art results on well-known datasets without the need for complex fine-tuning.'}, 'zh': {'title': '利用视频超分辨率提升3D重建质量', 'desc': '本文研究了3D超分辨率技术，旨在从低分辨率的多视图图像中重建高保真度的3D模型。传统的单图像超分辨率(SISR)方法在处理每张图像时缺乏视图一致性，导致重建效果不佳。我们提出利用视频超分辨率(VSR)模型来提高空间一致性，并参考周围的空间信息，从而实现更准确和详细的重建。实验结果表明，我们的方法在标准基准数据集上达到了最先进的3D超分辨率效果。'}}}, {'id': 'https://huggingface.co/papers/2412.15450', 'title': 'Fietje: An open, efficient LLM for Dutch', 'url': 'https://huggingface.co/papers/2412.15450', 'abstract': 'This paper introduces Fietje, a family of small language models (SLMs) specifically designed for the Dutch language. The model is based on Phi 2, an English-centric model of 2.7 billion parameters. Fietje demonstrated competitive results with larger language models upon its release. A core emphasis of this work is transparency and reproducibility: Fietje is fully open-source, with model weights, datasets, training, and evaluation code all publicly accessible.   The paper discusses the performance of Fietje and many other models on an extensive evaluation suite of benchmarks on reasoning, sentiment analysis, world knowledge, linguistic acceptability and word sense disambiguation. Evaluation results illustrate the rapid progress in the field of LLMs, where recent small models outperform older, larger models that were fine-tuned for Dutch. This trend signals an exciting future for Dutch language processing, suggesting that even compact LLMs are becoming increasingly capable. Furthermore, ongoing and future efforts to adapt LLMs to Dutch are poised to enhance these models even further, broadening their applicability and accessibility. Fietje is only an intermediate step in improving accessibility to language technology for users of the Dutch language.', 'score': 2, 'issue_id': 1263, 'pub_date': '2024-12-19', 'pub_date_card': {'ru': '19 декабря', 'en': 'December 19', 'zh': '12月19日'}, 'hash': '0a377666ad38be9a', 'authors': ['Bram Vanroy'], 'affiliations': ['Dutch Language Institute, Rapenburg 61, 2311 GJ Leiden, The Netherlands', 'KU Leuven, Blijde Inkomststraat 21, 3000 Leuven, Belgium'], 'pdf_title_img': 'assets/pdf/title_img/2412.15450.jpg', 'data': {'categories': ['#multilingual', '#dataset', '#reasoning', '#open_source', '#small_models', '#benchmark'], 'emoji': '🇳🇱', 'ru': {'title': 'Fietje: Открытая малая языковая модель для нидерландского языка', 'desc': 'Статья представляет Fietje - семейство малых языковых моделей (SLM), специально разработанных для нидерландского языка. Модель основана на Phi 2 и показывает конкурентоспособные результаты с более крупными языковыми моделями. Особое внимание уделяется прозрачности и воспроизводимости: Fietje полностью открыт, включая веса модели, наборы данных и код. Оценка производительности Fietje на различных бенчмарках демонстрирует быстрый прогресс в области языковых моделей, где недавние малые модели превосходят более старые и крупные модели, адаптированные для нидерландского языка.'}, 'en': {'title': 'Fietje: Small Models, Big Impact for Dutch Language Processing', 'desc': 'This paper presents Fietje, a series of small language models tailored for the Dutch language, built upon the Phi 2 architecture. Despite its smaller size, Fietje achieves competitive performance against larger models, showcasing the advancements in small language models (SLMs). The authors emphasize the importance of transparency and reproducibility by making all resources, including model weights and training data, publicly available. The evaluation results indicate that recent small models like Fietje are outperforming older, larger models in various linguistic tasks, highlighting a promising trend in Dutch language processing.'}, 'zh': {'title': 'Fietje：荷兰语处理的新希望', 'desc': '本文介绍了Fietje，一个专为荷兰语设计的小型语言模型（SLM）系列。该模型基于一个拥有27亿参数的以英语为中心的Phi 2模型。Fietje在发布时展示了与更大语言模型的竞争性结果，强调了透明性和可重复性，所有模型权重、数据集、训练和评估代码均为开源。评估结果表明，最近的小型模型在荷兰语处理上超越了较旧的、更大的模型，预示着荷兰语处理的未来充满希望。'}}}, {'id': 'https://huggingface.co/papers/2412.14963', 'title': 'IDOL: Instant Photorealistic 3D Human Creation from a Single Image', 'url': 'https://huggingface.co/papers/2412.14963', 'abstract': 'Creating a high-fidelity, animatable 3D full-body avatar from a single image is a challenging task due to the diverse appearance and poses of humans and the limited availability of high-quality training data. To achieve fast and high-quality human reconstruction, this work rethinks the task from the perspectives of dataset, model, and representation. First, we introduce a large-scale HUman-centric GEnerated dataset, HuGe100K, consisting of 100K diverse, photorealistic sets of human images. Each set contains 24-view frames in specific human poses, generated using a pose-controllable image-to-multi-view model. Next, leveraging the diversity in views, poses, and appearances within HuGe100K, we develop a scalable feed-forward transformer model to predict a 3D human Gaussian representation in a uniform space from a given human image. This model is trained to disentangle human pose, body shape, clothing geometry, and texture. The estimated Gaussians can be animated without post-processing. We conduct comprehensive experiments to validate the effectiveness of the proposed dataset and method. Our model demonstrates the ability to efficiently reconstruct photorealistic humans at 1K resolution from a single input image using a single GPU instantly. Additionally, it seamlessly supports various applications, as well as shape and texture editing tasks.', 'score': 1, 'issue_id': 1269, 'pub_date': '2024-12-19', 'pub_date_card': {'ru': '19 декабря', 'en': 'December 19', 'zh': '12月19日'}, 'hash': 'ff4454342b061f2d', 'authors': ['Yiyu Zhuang', 'Jiaxi Lv', 'Hao Wen', 'Qing Shuai', 'Ailing Zeng', 'Hao Zhu', 'Shifeng Chen', 'Yujiu Yang', 'Xun Cao', 'Wei Liu'], 'affiliations': ['Nanjing University', 'Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences', 'Shenzhen University of Advanced Technology', 'Tencent', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2412.14963.jpg', 'data': {'categories': ['#3d', '#dataset', '#architecture'], 'emoji': '🕺', 'ru': {'title': 'Мгновенное создание анимируемых 3D-аватаров по одному фото', 'desc': 'Статья представляет новый подход к созданию анимируемых 3D-аватаров человека по одному изображению. Авторы разработали крупномасштабный датасет HuGe100K, содержащий 100 тысяч наборов фотореалистичных изображений людей в различных позах и ракурсах. На основе этого датасета они обучили трансформерную модель, способную предсказывать 3D-представление человека в виде гауссовых примитивов, разделяя позу, форму тела, геометрию одежды и текстуру. Модель позволяет мгновенно реконструировать фотореалистичных людей с разрешением 1K на одном GPU и поддерживает различные приложения и задачи редактирования.'}, 'en': {'title': 'Transforming Single Images into Lifelike 3D Avatars', 'desc': 'This paper presents a novel approach to creating detailed 3D avatars from a single image by introducing a new dataset and a transformer model. The HuGe100K dataset contains 100,000 diverse human images with multiple views and poses, which helps in training the model effectively. The proposed feed-forward transformer model predicts a 3D Gaussian representation, allowing for the separation of human features like pose, shape, and texture. This method enables fast and high-quality reconstruction of photorealistic avatars, which can be animated and edited easily, demonstrating its versatility in various applications.'}, 'zh': {'title': '从单图像生成3D全身头像的创新方法', 'desc': '本研究旨在从单张图像创建高保真、可动画的3D全身头像，解决了人类外观和姿势多样性以及高质量训练数据稀缺的问题。我们引入了一个大型人类中心生成数据集HuGe100K，包含10万个多样化的、逼真的人类图像集，每个图像集包含特定姿势下的24个视角帧。基于该数据集的多样性，我们开发了一种可扩展的前馈变换器模型，能够从给定的人类图像预测3D高斯表示。该模型能够高效重建1K分辨率的逼真图像，并支持多种应用和形状、纹理编辑任务。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (1)', '#agi', '#alignment', '#architecture (3)', '#audio (1)', '#benchmark (4)', '#cv (1)', '#data', '#dataset (2)', '#diffusion (1)', '#ethics (1)', '#games', '#graphs', '#hallucinations', '#healthcare', '#inference (5)', '#interpretability', '#leakage', '#long_context (1)', '#low_resource (1)', '#machine_translation', '#math (1)', '#multilingual (2)', '#multimodal (1)', '#open_source (1)', '#optimization (5)', '#plp', '#rag', '#reasoning (2)', '#rl (1)', '#rlhf (1)', '#robotics', '#science', '#security', '#small_models (2)', '#story_generation', '#survey', '#synthetic (1)', '#training (4)', '#transfer_learning (1)', '#video (3)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2024-12-23 14:09',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-12-23 14:09')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-12-23 14:09')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    