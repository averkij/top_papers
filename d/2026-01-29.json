{
    "date": {
        "ru": "29 ÑĞ½Ğ²Ğ°Ñ€Ñ",
        "en": "January 29",
        "zh": "1æœˆ29æ—¥"
    },
    "time_utc": "2026-01-29 04:28",
    "weekday": 3,
    "issue_id": 822,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2601.20614",
            "title": "Harder Is Better: Boosting Mathematical Reasoning via Difficulty-Aware GRPO and Multi-Aspect Question Reformulation",
            "url": "https://huggingface.co/papers/2601.20614",
            "abstract": "MathForge enhances mathematical reasoning in large models through a dual framework combining difficulty-aware policy optimization and multi-aspect question reformulation to address limitations in existing reinforcement learning methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) offers a robust mechanism for enhancing mathematical reasoning in large models. However, we identify a systematic lack of emphasis on more challenging questions in existing methods from both algorithmic and data perspectives, despite their importance for refining underdeveloped capabilities. Algorithmically, widely used Group Relative Policy Optimization (GRPO) suffers from an implicit imbalance where the magnitude of policy updates is lower for harder questions. Data-wise, augmentation approaches primarily rephrase questions to enhance diversity without systematically increasing intrinsic difficulty. To address these issues, we propose a two-dual MathForge framework to improve mathematical reasoning by targeting harder questions from both perspectives, which comprises a Difficulty-Aware Group Policy Optimization (DGPO) algorithm and a Multi-Aspect Question Reformulation (MQR) strategy. Specifically, DGPO first rectifies the implicit imbalance in GRPO via difficulty-balanced group advantage estimation, and further prioritizes harder questions by difficulty-aware question-level weighting. Meanwhile, MQR reformulates questions across multiple aspects to increase difficulty while maintaining the original gold answer. Overall, MathForge forms a synergistic loop: MQR expands the data frontier, and DGPO effectively learns from the augmented data. Extensive experiments show that MathForge significantly outperforms existing methods on various mathematical reasoning tasks. The code and augmented data are all available at https://github.com/AMAP-ML/MathForge.",
            "score": 47,
            "issue_id": 822,
            "pub_date": "2026-01-28",
            "pub_date_card": {
                "ru": "28 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 28",
                "zh": "1æœˆ28æ—¥"
            },
            "hash": "9018ece1c76bc891",
            "authors": [
                "Yanqi Dai",
                "Yuxiang Ji",
                "Xiao Zhang",
                "Yong Wang",
                "Xiangxiang Chu",
                "Zhiwu Lu"
            ],
            "affiliations": [
                "AMAP, Alibaba Group",
                "Dalian University of Technology",
                "Gaoling School of Artificial Intelligence, Renmin University of China",
                "Xiamen University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.20614.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#optimization",
                    "#open_source",
                    "#training",
                    "#math",
                    "#reasoning",
                    "#data"
                ],
                "emoji": "ğŸ§®",
                "ru": {
                    "title": "Ğ¦ĞµĞ»ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ´Ğ»Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ LLM",
                    "desc": "MathForge Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Group Relative Policy Optimization, Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ ÑƒĞ´ĞµĞ»ÑÑÑ‚ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ DGPO, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒĞµÑ‚ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°, Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ MQR Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ñ… Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ MathForge Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Math Reasoning with Difficulty-Aware Learning",
                    "desc": "MathForge is a new framework designed to improve mathematical reasoning in large machine learning models. It combines two key strategies: Difficulty-Aware Group Policy Optimization (DGPO) and Multi-Aspect Question Reformulation (MQR). DGPO addresses the issue of existing methods not focusing enough on harder questions by balancing the learning process and prioritizing these challenging tasks. MQR enhances the dataset by reformulating questions in various ways to increase their difficulty while keeping the correct answers intact, leading to better performance in mathematical reasoning tasks."
                },
                "zh": {
                    "title": "MathForgeï¼šæå‡æ•°å­¦æ¨ç†èƒ½åŠ›çš„åŒé‡æ¡†æ¶",
                    "desc": "MathForge æ˜¯ä¸€ä¸ªå¢å¼ºå¤§å‹æ¨¡å‹æ•°å­¦æ¨ç†çš„æ¡†æ¶ï¼Œç»“åˆäº†éš¾åº¦æ„ŸçŸ¥çš„ç­–ç•¥ä¼˜åŒ–å’Œå¤šæ–¹é¢çš„é—®é¢˜é‡æ„ã€‚å®ƒè§£å†³äº†ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤„ç†æ›´å…·æŒ‘æˆ˜æ€§é—®é¢˜æ—¶çš„ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯åœ¨ç®—æ³•å’Œæ•°æ®ä¸¤ä¸ªæ–¹é¢ã€‚é€šè¿‡éš¾åº¦æ„ŸçŸ¥çš„ç¾¤ä½“ç­–ç•¥ä¼˜åŒ–ï¼ˆDGPOï¼‰å’Œå¤šæ–¹é¢é—®é¢˜é‡æ„ï¼ˆMQRï¼‰ï¼ŒMathForge èƒ½å¤Ÿä¼˜å…ˆå¤„ç†æ›´éš¾çš„é—®é¢˜ï¼Œå¹¶åœ¨ä¿æŒåŸå§‹ç­”æ¡ˆçš„åŒæ—¶å¢åŠ é—®é¢˜çš„éš¾åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMathForge åœ¨å„ç§æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.20540",
            "title": "Advancing Open-source World Models",
            "url": "https://huggingface.co/papers/2601.20540",
            "abstract": "LingBot-World is an open-source world simulator with high-fidelity dynamics, long-term memory capabilities, and real-time interactivity for diverse environments.  \t\t\t\t\tAI-generated summary \t\t\t\t We present LingBot-World, an open-sourced world simulator stemming from video generation. Positioned as a top-tier world model, LingBot-World offers the following features. (1) It maintains high fidelity and robust dynamics in a broad spectrum of environments, including realism, scientific contexts, cartoon styles, and beyond. (2) It enables a minute-level horizon while preserving contextual consistency over time, which is also known as \"long-term memory\". (3) It supports real-time interactivity, achieving a latency of under 1 second when producing 16 frames per second. We provide public access to the code and model in an effort to narrow the divide between open-source and closed-source technologies. We believe our release will empower the community with practical applications across areas like content creation, gaming, and robot learning.",
            "score": 26,
            "issue_id": 822,
            "pub_date": "2026-01-28",
            "pub_date_card": {
                "ru": "28 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 28",
                "zh": "1æœˆ28æ—¥"
            },
            "hash": "7c7953b68949dd9f",
            "authors": [
                "Robbyant Team",
                "Zelin Gao",
                "Qiuyu Wang",
                "Yanhong Zeng",
                "Jiapeng Zhu",
                "Ka Leong Cheng",
                "Yixuan Li",
                "Hanlin Wang",
                "Yinghao Xu",
                "Shuailei Ma",
                "Yihang Chen",
                "Jie Liu",
                "Yansong Cheng",
                "Yao Yao",
                "Jiayi Zhu",
                "Yihao Meng",
                "Kecheng Zheng",
                "Qingyan Bai",
                "Jingye Chen",
                "Zehong Shen",
                "Yue Yu",
                "Xing Zhu",
                "Yujun Shen",
                "Hao Ouyang"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2601.20540.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#open_source",
                    "#games",
                    "#robotics",
                    "#agents"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ world model Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¸Ñ€Ğ°Ğ¼Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸",
                    "desc": "LingBot-World â€” ÑÑ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ‚Ğ¸Ğ»ÑÑ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½ÑƒÑ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ‚ÑĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ¸Ğ½ÑƒÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ° Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ 16 ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ² ÑĞµĞºÑƒĞ½Ğ´Ñƒ Ğ¼ĞµĞ½ĞµĞµ Ñ‡ĞµĞ¼ Ğ·Ğ° ÑĞµĞºÑƒĞ½Ğ´Ñƒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ»Ğ¸ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ Ğ¸ Ğ²ĞµÑĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°, Ğ¸Ğ³Ñ€ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Empowering Creativity with Real-Time World Simulation",
                    "desc": "LingBot-World is an advanced open-source world simulator designed for generating high-quality video content. It features high-fidelity dynamics that can adapt to various environments, from realistic to cartoonish styles. The simulator incorporates long-term memory capabilities, allowing it to maintain contextual consistency over extended periods. Additionally, it supports real-time interactivity with low latency, making it suitable for applications in gaming, content creation, and robotics."
                },
                "zh": {
                    "title": "LingBot-Worldï¼šå¼€æºä¸–ç•Œæ¨¡æ‹Ÿçš„æœªæ¥",
                    "desc": "LingBot-World æ˜¯ä¸€ä¸ªå¼€æºçš„ä¸–ç•Œæ¨¡æ‹Ÿå™¨ï¼Œå…·æœ‰é«˜ä¿çœŸåº¦çš„åŠ¨æ€è¡¨ç°å’Œé•¿æœŸè®°å¿†èƒ½åŠ›ã€‚å®ƒèƒ½å¤Ÿåœ¨å¤šç§ç¯å¢ƒä¸­ä¿æŒå¼ºå¤§çš„åŠ¨æ€æ•ˆæœï¼ŒåŒ…æ‹¬ç°å®ä¸»ä¹‰ã€ç§‘å­¦èƒŒæ™¯å’Œå¡é€šé£æ ¼ç­‰ã€‚è¯¥æ¨¡æ‹Ÿå™¨æ”¯æŒå®æ—¶äº¤äº’ï¼Œèƒ½å¤Ÿåœ¨æ¯ç§’ç”Ÿæˆ16å¸§çš„æƒ…å†µä¸‹ï¼Œå»¶è¿Ÿä½äº1ç§’ã€‚æˆ‘ä»¬å¸Œæœ›é€šè¿‡å…¬å¼€ä»£ç å’Œæ¨¡å‹ï¼Œä¿ƒè¿›å¼€æºæŠ€æœ¯ä¸é—­æºæŠ€æœ¯ä¹‹é—´çš„èåˆï¼Œæ¨åŠ¨å†…å®¹åˆ›ä½œã€æ¸¸æˆå’Œæœºå™¨äººå­¦ä¹ ç­‰é¢†åŸŸçš„åº”ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.20552",
            "title": "DeepSeek-OCR 2: Visual Causal Flow",
            "url": "https://huggingface.co/papers/2601.20552",
            "abstract": "DeepSeek-OCR 2 introduces DeepEncoder V2 that dynamically reorders visual tokens based on semantic content, enabling more human-like causal reasoning in 2D image understanding through cascaded 1D causal structures.  \t\t\t\t\tAI-generated summary \t\t\t\t We present DeepSeek-OCR 2 to investigate the feasibility of a novel encoder-DeepEncoder V2-capable of dynamically reordering visual tokens upon image semantics. Conventional vision-language models (VLMs) invariably process visual tokens in a rigid raster-scan order (top-left to bottom-right) with fixed positional encoding when fed into LLMs. However, this contradicts human visual perception, which follows flexible yet semantically coherent scanning patterns driven by inherent logical structures. Particularly for images with complex layouts, human vision exhibits causally-informed sequential processing. Inspired by this cognitive mechanism, DeepEncoder V2 is designed to endow the encoder with causal reasoning capabilities, enabling it to intelligently reorder visual tokens prior to LLM-based content interpretation. This work explores a novel paradigm: whether 2D image understanding can be effectively achieved through two-cascaded 1D causal reasoning structures, thereby offering a new architectural approach with the potential to achieve genuine 2D reasoning. Codes and model weights are publicly accessible at http://github.com/deepseek-ai/DeepSeek-OCR-2.",
            "score": 6,
            "issue_id": 822,
            "pub_date": "2026-01-28",
            "pub_date_card": {
                "ru": "28 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 28",
                "zh": "1æœˆ28æ—¥"
            },
            "hash": "93565144dc050049",
            "authors": [
                "Haoran Wei",
                "Yaofeng Sun",
                "Yukun Li"
            ],
            "affiliations": [
                "DeepSeek-AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.20552.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#multimodal",
                    "#architecture",
                    "#reasoning",
                    "#cv"
                ],
                "emoji": "ğŸ‘ï¸",
                "ru": {
                    "title": "Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿ĞµÑ€ĞµÑƒĞ¿Ğ¾Ñ€ÑĞ´Ğ¾Ñ‡ĞµĞ½Ğ¸Ğµ: Ğ¾Ñ‚ Ñ€Ğ°ÑÑ‚Ñ€Ğ¾Ğ²Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ñ‚ĞºĞ¸ Ğº Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ·Ñ€ĞµĞ½Ğ¸Ñ",
                    "desc": "DeepSeek-OCR 2 Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ DeepEncoder V2, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿ĞµÑ€ĞµÑƒĞ¿Ğ¾Ñ€ÑĞ´Ğ¾Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ² ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ñ‹Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ² Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞµ ÑĞ»ĞµĞ²Ğ°-Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ¾ Ğ¸ ÑĞ²ĞµÑ€Ñ…Ñƒ-Ğ²Ğ½Ğ¸Ğ·, Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğµ Ğ·Ñ€ĞµĞ½Ğ¸Ğµ Ñ ĞµĞ³Ğ¾ Ğ³Ğ¸Ğ±ĞºĞ¸Ğ¼, ÑĞ¼Ñ‹ÑĞ»Ğ¾Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ğ¼ ÑĞºĞ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ°ÑĞºĞ°Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ğ´Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ²ÑƒĞ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ñ‚ĞµÑ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸Ğ¼ĞµÑÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¼Ğ°ĞºĞµÑ‚Ñ‹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ LLM Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²."
                },
                "en": {
                    "title": "Revolutionizing Image Understanding with Dynamic Token Reordering",
                    "desc": "DeepSeek-OCR 2 introduces a new encoder called DeepEncoder V2, which can reorder visual tokens based on their semantic meaning. This approach contrasts with traditional vision-language models that process images in a fixed order, which does not reflect how humans perceive images. By mimicking human-like causal reasoning, DeepEncoder V2 allows for more flexible and coherent understanding of complex images. The research aims to enhance 2D image comprehension through a novel method that utilizes cascaded 1D causal structures for better reasoning."
                },
                "zh": {
                    "title": "åŠ¨æ€é‡æ’åºï¼Œæå‡å›¾åƒç†è§£çš„å› æœæ¨ç†èƒ½åŠ›",
                    "desc": "DeepSeek-OCR 2 ä»‹ç»äº†ä¸€ç§æ–°çš„ç¼–ç å™¨ DeepEncoder V2ï¼Œèƒ½å¤Ÿæ ¹æ®å›¾åƒçš„è¯­ä¹‰åŠ¨æ€é‡æ–°æ’åºè§†è§‰æ ‡è®°ã€‚è¿™ç§æ–¹æ³•ä½¿å¾—åœ¨äºŒç»´å›¾åƒç†è§£ä¸­å®ç°æ›´æ¥è¿‘äººç±»çš„å› æœæ¨ç†æˆä¸ºå¯èƒ½ã€‚ä¼ ç»Ÿçš„è§†è§‰è¯­è¨€æ¨¡å‹é€šå¸¸ä»¥å›ºå®šçš„é¡ºåºå¤„ç†è§†è§‰æ ‡è®°ï¼Œè€Œ DeepEncoder V2 åˆ™æ¨¡ä»¿äººç±»çš„è§†è§‰æ„ŸçŸ¥ï¼Œé‡‡ç”¨çµæ´»çš„æ‰«ææ¨¡å¼ã€‚é€šè¿‡è¿™ç§æ–°é¢–çš„æ¶æ„ï¼Œç ”ç©¶è€…æ¢ç´¢äº†å¦‚ä½•é€šè¿‡ä¸¤ä¸ªçº§è”çš„ä¸€ç»´å› æœæ¨ç†ç»“æ„æœ‰æ•ˆå®ç°äºŒç»´å›¾åƒç†è§£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.20209",
            "title": "Spark: Strategic Policy-Aware Exploration via Dynamic Branching for Long-Horizon Agentic Learning",
            "url": "https://huggingface.co/papers/2601.20209",
            "abstract": "Spark is a reinforcement learning framework that strategically allocates computational resources by branching at critical decision states, improving sample efficiency and generalization for long-horizon tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning has empowered large language models to act as intelligent agents, yet training them for long-horizon tasks remains challenging due to the scarcity of high-quality trajectories, especially under limited resources. Existing methods typically scale up rollout sizes and indiscriminately allocate computational resources among intermediate steps. Such attempts inherently waste substantial computation budget on trivial steps while failing to guarantee sample quality. To address this, we propose Spark (Strategic Policy-Aware exploRation via Key-state dynamic branching), a novel framework that selectively branches at critical decision states for resource-efficient exploration. Our key insight is to activate adaptive branching exploration at critical decision points to probe promising trajectories, thereby achieving precise resource allocation that prioritizes sampling quality over blind coverage. This design leverages the agent's intrinsic decision-making signals to reduce dependence on human priors, enabling the agent to autonomously expand exploration and achieve stronger generalization. Experiments across diverse tasks (e.g., embodied planning), demonstrate that Spark achieves superior success rates with significantly fewer training samples, exhibiting robust generalization even in unseen scenarios.",
            "score": 5,
            "issue_id": 822,
            "pub_date": "2026-01-28",
            "pub_date_card": {
                "ru": "28 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 28",
                "zh": "1æœˆ28æ—¥"
            },
            "hash": "86ca3ed3a2e5cd06",
            "authors": [
                "Jinyang Wu",
                "Shuo Yang",
                "Changpeng Yang",
                "Yuhao Shen",
                "Shuai Zhang",
                "Zhengqi Wen",
                "Jianhua Tao"
            ],
            "affiliations": [
                "Peking University",
                "Tsinghua University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.20209.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#training",
                    "#robotics",
                    "#agents"
                ],
                "emoji": "ğŸŒ³",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ²ĞµÑ‚Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ² ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ğ°Ñ… Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Spark â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ¼Ğ½Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑÑƒÑ€ÑÑ‹ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼Ğ¸ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ÑĞ¼Ğ¸ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ€ĞµĞ´Ñ‹. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¼ Ğ²ĞµÑ‚Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¸Ğ¼ĞµĞ½Ğ½Ğ¾ Ğ² ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚Ğ¾Ñ‡ĞºĞ°Ñ… Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ñ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Spark Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ ÑƒÑĞ¿ĞµÑ…Ğ° Ñ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Efficient Exploration through Strategic Decision Branching",
                    "desc": "Spark is a reinforcement learning framework designed to enhance the efficiency of resource allocation during training for long-horizon tasks. It strategically branches at critical decision points, allowing for more effective exploration of promising trajectories while minimizing wasted computational resources. By focusing on sample quality rather than merely increasing the number of rollouts, Spark improves the learning process for intelligent agents. Experiments show that this approach leads to higher success rates with fewer training samples, demonstrating better generalization in new situations."
                },
                "zh": {
                    "title": "Sparkï¼šé«˜æ•ˆèµ„æºåˆ†é…çš„å¼ºåŒ–å­¦ä¹ æ–°æ¡†æ¶",
                    "desc": "Sparkæ˜¯ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡åœ¨å…³é”®å†³ç­–çŠ¶æ€è¿›è¡Œåˆ†æ”¯ï¼Œæˆ˜ç•¥æ€§åœ°åˆ†é…è®¡ç®—èµ„æºï¼Œä»è€Œæé«˜é•¿æ—¶é—´ä»»åŠ¡çš„æ ·æœ¬æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸æ‰©å¤§å›æ»šè§„æ¨¡ï¼Œå¹¶åœ¨ä¸­é—´æ­¥éª¤ä¸­æ— å·®åˆ«åœ°åˆ†é…è®¡ç®—èµ„æºï¼Œå¯¼è‡´åœ¨æ— å…³æ­¥éª¤ä¸Šæµªè´¹å¤§é‡è®¡ç®—é¢„ç®—ã€‚Sparké€šè¿‡åœ¨å…³é”®å†³ç­–ç‚¹æ¿€æ´»è‡ªé€‚åº”åˆ†æ”¯æ¢ç´¢ï¼Œä¼˜å…ˆè€ƒè™‘æ ·æœ¬è´¨é‡ï¼Œå®ç°èµ„æºçš„é«˜æ•ˆåˆ†é…ã€‚å®éªŒè¡¨æ˜ï¼ŒSparkåœ¨å¤šç§ä»»åŠ¡ä¸­ä»¥æ˜¾è‘—æ›´å°‘çš„è®­ç»ƒæ ·æœ¬è·å¾—æ›´é«˜çš„æˆåŠŸç‡ï¼Œå¹¶åœ¨æœªè§åœºæ™¯ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.20834",
            "title": "Linear representations in language models can change dramatically over a conversation",
            "url": "https://huggingface.co/papers/2601.20834",
            "abstract": "Linear representation directions in language models dynamically shift during conversations, affecting how factual information is encoded while preserving generic content, with implications for interpretability and context-adaptive model behavior.  \t\t\t\t\tAI-generated summary \t\t\t\t Language model representations often contain linear directions that correspond to high-level concepts. Here, we study the dynamics of these representations: how representations evolve along these dimensions within the context of (simulated) conversations. We find that linear representations can change dramatically over a conversation; for example, information that is represented as factual at the beginning of a conversation can be represented as non-factual at the end and vice versa. These changes are content-dependent; while representations of conversation-relevant information may change, generic information is generally preserved. These changes are robust even for dimensions that disentangle factuality from more superficial response patterns, and occur across different model families and layers of the model. These representation changes do not require on-policy conversations; even replaying a conversation script written by an entirely different model can produce similar changes. However, adaptation is much weaker from simply having a sci-fi story in context that is framed more explicitly as such. We also show that steering along a representational direction can have dramatically different effects at different points in a conversation. These results are consistent with the idea that representations may evolve in response to the model playing a particular role that is cued by a conversation. Our findings may pose challenges for interpretability and steering -- in particular, they imply that it may be misleading to use static interpretations of features or directions, or probes that assume a particular range of features consistently corresponds to a particular ground-truth value. However, these types of representational dynamics also point to exciting new research directions for understanding how models adapt to context.",
            "score": 2,
            "issue_id": 822,
            "pub_date": "2026-01-28",
            "pub_date_card": {
                "ru": "28 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 28",
                "zh": "1æœˆ28æ—¥"
            },
            "hash": "f978eae64fbb3b0d",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#architecture",
                    "#interpretability"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹: ĞºĞ°Ğº ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿ĞµÑ€ĞµĞ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒÑÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ, ĞºĞ°Ğº Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑÑ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¼ĞµĞ½ÑÑÑ‚ÑÑ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ, Ğ·Ğ°ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ĞºĞ°Ğº Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ² Ğ½Ğ°Ñ‡Ğ°Ğ»Ğµ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ°, Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° ĞºĞ°Ğº Ğ½ĞµÑ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğº ĞµĞ³Ğ¾ ĞºĞ¾Ğ½Ñ†Ñƒ, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¾Ğ±Ñ‰Ğ°Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ÑÑ‚Ğ°Ñ‘Ñ‚ÑÑ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹. Ğ­Ñ‚Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¾Ñ‚ Ñ‚Ğ¾Ğ³Ğ¾, ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ»Ğ¸ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ»Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´Ñ‘Ğ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ· Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¸ Ğ¾Ğ½Ğ¸ Ğ²Ğ»Ğ¸ÑÑÑ‚ Ğ½Ğ° Ñ‚Ğ¾, ĞºĞ°Ğº ÑÑ‚Ğ¸Ñ€Ğ¸Ğ½Ğ³ Ğ²Ğ´Ğ¾Ğ»ÑŒ ÑÑ‚Ğ¸Ñ… Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¾Ğ·Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ½Ğ° Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸ Ğ·Ğ¾Ğ½Ğ´Ğ¾Ğ² Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ½ĞµÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸, Ğ½Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…."
                },
                "en": {
                    "title": "Dynamic Shifts in Language Model Representations During Conversations",
                    "desc": "This paper explores how language models change their internal representations during conversations. It shows that the way information is encoded can shift from factual to non-factual as the dialogue progresses, depending on the content discussed. While specific conversation-related information may change, general or generic information tends to remain stable. These findings highlight the challenges in interpreting model behavior and suggest that understanding these dynamic representations could lead to new research opportunities in context-aware AI."
                },
                "zh": {
                    "title": "å¯¹è¯ä¸­è¯­è¨€æ¨¡å‹è¡¨ç¤ºçš„åŠ¨æ€å˜åŒ–",
                    "desc": "æœ¬æ–‡ç ”ç©¶äº†è¯­è¨€æ¨¡å‹ä¸­çº¿æ€§è¡¨ç¤ºæ–¹å‘åœ¨å¯¹è¯è¿‡ç¨‹ä¸­å¦‚ä½•åŠ¨æ€å˜åŒ–ã€‚è¿™äº›å˜åŒ–å½±å“äº†äº‹å®ä¿¡æ¯çš„ç¼–ç ï¼ŒåŒæ—¶ä¿æŒäº†é€šç”¨å†…å®¹çš„å®Œæ•´æ€§ã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨å¯¹è¯çš„ä¸åŒé˜¶æ®µï¼Œä¿¡æ¯çš„è¡¨ç¤ºå¯ä»¥ä»äº‹å®è½¬å˜ä¸ºéäº‹å®ï¼Œåä¹‹äº¦ç„¶ã€‚è¿™ç§å˜åŒ–æ˜¯å†…å®¹ä¾èµ–çš„ï¼Œå°½ç®¡å¯¹è¯ç›¸å…³çš„ä¿¡æ¯ä¼šå˜åŒ–ï¼Œä½†é€šç”¨ä¿¡æ¯é€šå¸¸ä¼šè¢«ä¿ç•™ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.20789",
            "title": "SERA: Soft-Verified Efficient Repository Agents",
            "url": "https://huggingface.co/papers/2601.20789",
            "abstract": "Soft-Verified Efficient Repository Agents (SERA) enables cost-effective training of coding agents through supervised fine-tuning, achieving state-of-the-art performance while enabling specialization to private codebases at a fraction of the cost of previous methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Open-weight coding agents should hold a fundamental advantage over closed-source systems: they can be specialized to private codebases, encoding repository-specific information directly in their weights. Yet the cost and complexity of training has kept this advantage theoretical. We show it is now practical. We present Soft-Verified Efficient Repository Agents (SERA), an efficient method for training coding agents that enables the rapid and cheap creation of agents specialized to private codebases. Using only supervised finetuning (SFT), SERA achieves state-of-the-art results among fully open-source (open data, method, code) models while matching the performance of frontier open-weight models like Devstral-Small-2. Creating SERA models is 26x cheaper than reinforcement learning and 57x cheaper than previous synthetic data methods to reach equivalent performance. Our method, Soft Verified Generation (SVG), generates thousands of trajectories from a single code repository. Combined with cost-efficiency, this enables specialization to private codebases. Beyond repository specialization, we apply SVG to a larger corpus of codebases, generating over 200,000 synthetic trajectories. We use this dataset to provide detailed analysis of scaling laws, ablations, and confounding factors for training coding agents. Overall, we believe our work will greatly accelerate research on open coding agents and showcase the advantage of open-source models that can specialize to private codebases. We release SERA as the first model in Ai2's Open Coding Agents series, along with all our code, data, and Claude Code integration to support the research community.",
            "score": 2,
            "issue_id": 822,
            "pub_date": "2026-01-28",
            "pub_date_card": {
                "ru": "28 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 28",
                "zh": "1æœˆ28æ—¥"
            },
            "hash": "d62ea829467acb11",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#optimization",
                    "#open_source",
                    "#training",
                    "#dataset",
                    "#plp",
                    "#synthetic",
                    "#agents"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ”ĞµÑˆÑ‘Ğ²Ğ°Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ñ‹Ğµ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ SERA â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· supervised fine-tuning Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼ÑĞ³ĞºĞ¾Ğ¹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Soft Verified Generation (SVG), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ñ‹ÑÑÑ‡Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ ĞºĞ¾Ğ´Ğ°, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°Ñ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ÑÑ€ĞµĞ´Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ñ‹ Ñ frontier Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ² 26 Ñ€Ğ°Ğ· Ğ´ĞµÑˆĞµĞ²Ğ»Ğµ, Ñ‡ĞµĞ¼ reinforcement learning. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ´Ğ¾Ğ²Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ñ‹Ğµ ĞºĞ¾Ğ´Ğ¾Ğ²Ñ‹Ğµ Ğ±Ğ°Ğ·Ñ‹, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ°Ğ´ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Cost-Effective Specialization for Coding Agents with SERA",
                    "desc": "The paper introduces Soft-Verified Efficient Repository Agents (SERA), a novel approach for training coding agents that allows for cost-effective specialization to private codebases. By utilizing supervised fine-tuning (SFT), SERA achieves state-of-the-art performance while being significantly cheaper than traditional methods like reinforcement learning. The method generates thousands of synthetic training trajectories from a single code repository, enhancing the efficiency of the training process. This work aims to advance the field of open-source coding agents and demonstrates the practical benefits of open-weight models in adapting to specific coding environments."
                },
                "zh": {
                    "title": "é«˜æ•ˆè®­ç»ƒç§æœ‰ä»£ç åº“ä»£ç†çš„é©å‘½æ€§æ–¹æ³•",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºè½¯éªŒè¯é«˜æ•ˆä»£ç åº“ä»£ç†ï¼ˆSERAï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡ç›‘ç£å¾®è°ƒå®ç°æˆæœ¬æ•ˆç›Šé«˜çš„ç¼–ç ä»£ç†è®­ç»ƒã€‚SERAèƒ½å¤Ÿå¿«é€Ÿä¸”ä½æˆæœ¬åœ°åˆ›å»ºä¸“é—¨é’ˆå¯¹ç§æœ‰ä»£ç åº“çš„ä»£ç†ï¼Œå¹¶åœ¨å®Œå…¨å¼€æºæ¨¡å‹ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä¸ä¹‹å‰çš„æ–¹æ³•ç›¸æ¯”ï¼Œä½¿ç”¨SERAåˆ›å»ºæ¨¡å‹çš„æˆæœ¬é™ä½äº†26å€ï¼Œä¸”åœ¨æ€§èƒ½ä¸Šä¸å‰æ²¿çš„å¼€æ”¾æƒé‡æ¨¡å‹ç›¸åŒ¹é…ã€‚æˆ‘ä»¬çš„ç ”ç©¶å°†åŠ é€Ÿå¼€æ”¾ç¼–ç ä»£ç†çš„ç ”ç©¶ï¼Œå¹¶å±•ç¤ºå¼€æºæ¨¡å‹åœ¨ç§æœ‰ä»£ç åº“ä¸“é—¨åŒ–æ–¹é¢çš„ä¼˜åŠ¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.19325",
            "title": "Innovator-VL: A Multimodal Large Language Model for Scientific Discovery",
            "url": "https://huggingface.co/papers/2601.19325",
            "abstract": "Innovator-VL demonstrates that principled training design and transparent methodology can achieve strong scientific intelligence with reduced data requirements while maintaining general vision performance.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Innovator-VL, a scientific multimodal large language model designed to advance understanding and reasoning across diverse scientific domains while maintaining excellent performance on general vision tasks. Contrary to the trend of relying on massive domain-specific pretraining and opaque pipelines, our work demonstrates that principled training design and transparent methodology can yield strong scientific intelligence with substantially reduced data requirements. (i) First, we provide a fully transparent, end-to-end reproducible training pipeline, covering data collection, cleaning, preprocessing, supervised fine-tuning, reinforcement learning, and evaluation, along with detailed optimization recipes. This facilitates systematic extension by the community. (ii) Second, Innovator-VL exhibits remarkable data efficiency, achieving competitive performance on various scientific tasks using fewer than five million curated samples without large-scale pretraining. These results highlight that effective reasoning can be achieved through principled data selection rather than indiscriminate scaling. (iii) Third, Innovator-VL demonstrates strong generalization, achieving competitive performance on general vision, multimodal reasoning, and scientific benchmarks. This indicates that scientific alignment can be integrated into a unified model without compromising general-purpose capabilities. Our practices suggest that efficient, reproducible, and high-performing scientific multimodal models can be built even without large-scale data, providing a practical foundation for future research.",
            "score": 2,
            "issue_id": 822,
            "pub_date": "2026-01-27",
            "pub_date_card": {
                "ru": "27 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 27",
                "zh": "1æœˆ27æ—¥"
            },
            "hash": "722484bdbc5b171b",
            "authors": [
                "Zichen Wen",
                "Boxue Yang",
                "Shuang Chen",
                "Yaojie Zhang",
                "Yuhang Han",
                "Junlong Ke",
                "Cong Wang",
                "Yicheng Fu",
                "Jiawang Zhao",
                "Jiangchao Yao",
                "Xi Fang",
                "Zhen Wang",
                "Henxing Cai",
                "Lin Yao",
                "Zhifeng Gao",
                "Yanhui Hong",
                "Nang Yuan",
                "Yixuan Li",
                "Guojiang Zhao",
                "Haoyi Tao",
                "Nan Wang",
                "Han Lyu",
                "Guolin Ke",
                "Ning Liao",
                "Xiaoxing Wang",
                "Kai Chen",
                "Zhiyu Li",
                "Feiyu Xiong",
                "Sihan Hu",
                "Kun Chen",
                "Yanfeng Wang",
                "Weinan E",
                "Linfeng Zhang",
                "Linfeng Zhang"
            ],
            "affiliations": [
                "DP Technology",
                "Institute of Theoretical Physics, Chinese Academy of Sciences",
                "MemTensor",
                "School of Artificial Intelligence, Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.19325.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#open_source",
                    "#training",
                    "#multimodal",
                    "#science",
                    "#synthetic",
                    "#reasoning",
                    "#data"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "ĞĞ°ÑƒÑ‡Ğ½Ñ‹Ğ¹ Ğ˜Ğ˜ Ğ±ĞµĞ· Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ: ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹, Ğ° Ğ½Ğµ Ğ¾Ğ±ÑŠÑ‘Ğ¼",
                    "desc": "Innovator-VL â€” ÑÑ‚Ğ¾ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ°Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ²Ñ‹Ğ±Ğ¾Ñ€Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğº Ğ¾Ğ±ÑŠÑ‘Ğ¼Ñƒ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸, Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼Ğ°ÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹Ğ´Ğ°ÑÑ‰ÑƒÑÑÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ: ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ÑÑ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ĞµĞ½ĞµĞµ Ğ¿ÑÑ‚Ğ¸ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾Ğ´Ñ‡Ñ‘Ñ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ÑÑ Ğ·Ğ° ÑÑ‡Ñ‘Ñ‚ ÑƒĞ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ° Ğ½Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Efficient Science Meets Vision: Innovator-VL's Data-Smart Approach",
                    "desc": "Innovator-VL is a multimodal large language model that enhances scientific understanding and reasoning while excelling in general vision tasks. It emphasizes a principled training design and transparent methodology, allowing it to achieve strong performance with significantly less data. The model is built on a fully reproducible training pipeline that includes data collection, preprocessing, and evaluation, making it accessible for community extension. Innovator-VL demonstrates that effective reasoning can be achieved through careful data selection, achieving competitive results in scientific and general tasks without the need for extensive pretraining."
                },
                "zh": {
                    "title": "é«˜æ•ˆç§‘å­¦æ™ºèƒ½ï¼Œå°‘é‡æ•°æ®ä¹Ÿèƒ½å®ç°",
                    "desc": "Innovator-VL æ˜¯ä¸€ç§ç§‘å­¦å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨æé«˜å¯¹ä¸åŒç§‘å­¦é¢†åŸŸçš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶åœ¨ä¸€èˆ¬è§†è§‰ä»»åŠ¡ä¸Šä¿æŒå‡ºè‰²çš„è¡¨ç°ã€‚ä¸ä¾èµ–å¤§é‡ç‰¹å®šé¢†åŸŸé¢„è®­ç»ƒå’Œä¸é€æ˜æµç¨‹çš„è¶‹åŠ¿ç›¸åï¼Œæˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œåˆç†çš„è®­ç»ƒè®¾è®¡å’Œé€æ˜çš„æ–¹æ³•è®ºå¯ä»¥åœ¨å‡å°‘æ•°æ®éœ€æ±‚çš„æƒ…å†µä¸‹å®ç°å¼ºå¤§çš„ç§‘å­¦æ™ºèƒ½ã€‚æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªå®Œå…¨é€æ˜çš„ç«¯åˆ°ç«¯å¯é‡å¤è®­ç»ƒæµç¨‹ï¼Œæ¶µç›–æ•°æ®æ”¶é›†ã€æ¸…ç†ã€é¢„å¤„ç†ã€ç›‘ç£å¾®è°ƒã€å¼ºåŒ–å­¦ä¹ å’Œè¯„ä¼°ï¼Œä¾¿äºç¤¾åŒºçš„ç³»ç»Ÿæ‰©å±•ã€‚æ­¤å¤–ï¼ŒInnovator-VL åœ¨æ•°æ®æ•ˆç‡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½¿ç”¨ä¸åˆ°äº”ç™¾ä¸‡ä¸ªç²¾å¿ƒæŒ‘é€‰çš„æ ·æœ¬å°±èƒ½åœ¨å„ç§ç§‘å­¦ä»»åŠ¡ä¸Šå–å¾—ç«äº‰åŠ›çš„è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.20618",
            "title": "GDCNet: Generative Discrepancy Comparison Network for Multimodal Sarcasm Detection",
            "url": "https://huggingface.co/papers/2601.20618",
            "abstract": "A multimodal sarcasm detection approach uses generative models to create stable semantic anchors and measures cross-modal discrepancies for improved accuracy and robustness.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal sarcasm detection (MSD) aims to identify sarcasm within image-text pairs by modeling semantic incongruities across modalities. Existing methods often exploit cross-modal embedding misalignment to detect inconsistency but struggle when visual and textual content are loosely related or semantically indirect. While recent approaches leverage large language models (LLMs) to generate sarcastic cues, the inherent diversity and subjectivity of these generations often introduce noise. To address these limitations, we propose the Generative Discrepancy Comparison Network (GDCNet). This framework captures cross-modal conflicts by utilizing descriptive, factually grounded image captions generated by Multimodal LLMs (MLLMs) as stable semantic anchors. Specifically, GDCNet computes semantic and sentiment discrepancies between the generated objective description and the original text, alongside measuring visual-textual fidelity. These discrepancy features are then fused with visual and textual representations via a gated module to adaptively balance modality contributions. Extensive experiments on MSD benchmarks demonstrate GDCNet's superior accuracy and robustness, establishing a new state-of-the-art on the MMSD2.0 benchmark.",
            "score": 1,
            "issue_id": 822,
            "pub_date": "2026-01-28",
            "pub_date_card": {
                "ru": "28 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 28",
                "zh": "1æœˆ28æ—¥"
            },
            "hash": "d817f75631925037",
            "authors": [
                "Shuguang Zhang",
                "Junhong Lian",
                "Guoxin Yu",
                "Baoxun Xu",
                "Xiang Ao"
            ],
            "affiliations": [
                "Pengcheng Laboratory",
                "Shenzhen Stock Exchange",
                "State Key Laboratory of AI Safety, Institute of Computing Technology (ICT), Chinese Academy of Sciences (CAS)",
                "University of Chinese Academy of Sciences, CAS"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.20618.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "ğŸ˜",
                "ru": {
                    "title": "Ğ¯ĞºĞ¾Ñ€Ñ ÑĞ¼Ñ‹ÑĞ»Ğ° Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² ÑĞ°Ñ€ĞºĞ°Ğ·Ğ¼Ğ°: Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ°Ñ€ĞºĞ°Ğ·Ğ¼Ğ° Ğ² Ğ¿Ğ°Ñ€Ğ°Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ñ‚ĞµĞºÑÑ‚ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Generative Discrepancy Comparison Network (GDCNet), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ LLM, Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞºĞ¾Ñ€ĞµĞ¹ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°ÑÑ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ GDCNet Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞºĞ¾Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ MMSD2.0."
                },
                "en": {
                    "title": "Harnessing Generative Models for Accurate Sarcasm Detection",
                    "desc": "This paper presents a new method for detecting sarcasm in pairs of images and text, called the Generative Discrepancy Comparison Network (GDCNet). It improves upon existing techniques by using stable semantic anchors created from descriptive captions generated by multimodal large language models (MLLMs). GDCNet measures discrepancies in meaning and sentiment between these captions and the original text, as well as the alignment between visual and textual content. The results show that GDCNet achieves higher accuracy and robustness in sarcasm detection, setting a new benchmark in the field."
                },
                "zh": {
                    "title": "å¤šæ¨¡æ€è®½åˆºæ£€æµ‹çš„æ–°çªç ´",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€è®½åˆºæ£€æµ‹æ–¹æ³•ï¼Œåˆ©ç”¨ç”Ÿæˆæ¨¡å‹åˆ›å»ºç¨³å®šçš„è¯­ä¹‰é”šç‚¹ï¼Œå¹¶æµ‹é‡è·¨æ¨¡æ€çš„ä¸ä¸€è‡´æ€§ï¼Œä»¥æé«˜å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡ç”Ÿæˆæè¿°æ€§ã€åŸºäºäº‹å®çš„å›¾åƒæ ‡é¢˜ï¼Œæ•æ‰è§†è§‰å’Œæ–‡æœ¬ä¹‹é—´çš„è¯­ä¹‰å’Œæƒ…æ„Ÿå·®å¼‚ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒGDCNetèƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†è§†è§‰å’Œæ–‡æœ¬å†…å®¹ä¹‹é—´çš„æ¾æ•£å…³ç³»ï¼Œå‡å°‘å™ªå£°å½±å“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGDCNetåœ¨å¤šæ¨¡æ€è®½åˆºæ£€æµ‹åŸºå‡†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.19949",
            "title": "RIR-Mega-Speech: A Reverberant Speech Corpus with Comprehensive Acoustic Metadata and Reproducible Evaluation",
            "url": "https://huggingface.co/papers/2601.19949",
            "abstract": "A large-scale reverberant speech corpus with detailed acoustic annotations is introduced to facilitate standardized comparison and reproduction of speech processing research.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite decades of research on reverberant speech, comparing methods remains difficult because most corpora lack per-file acoustic annotations or provide limited documentation for reproduction. We present RIR-Mega-Speech, a corpus of approximately 117.5 hours created by convolving LibriSpeech utterances with roughly 5,000 simulated room impulse responses from the RIR-Mega collection. Every file includes RT60, direct-to-reverberant ratio (DRR), and clarity index (C_{50}) computed from the source RIR using clearly defined, reproducible procedures. We also provide scripts to rebuild the dataset and reproduce all evaluation results.   Using Whisper small on 1,500 paired utterances, we measure 5.20% WER (95% CI: 4.69--5.78) on clean speech and 7.70% (7.04--8.35) on reverberant versions, corresponding to a paired increase of 2.50 percentage points (2.06--2.98). This represents a 48% relative degradation. WER increases monotonically with RT60 and decreases with DRR, consistent with prior perceptual studies. While the core finding that reverberation harms recognition is well established, we aim to provide the community with a standardized resource where acoustic conditions are transparent and results can be verified independently. The repository includes one-command rebuild instructions for both Windows and Linux environments.",
            "score": 1,
            "issue_id": 822,
            "pub_date": "2026-01-25",
            "pub_date_card": {
                "ru": "25 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 25",
                "zh": "1æœˆ25æ—¥"
            },
            "hash": "929449e4dced756e",
            "authors": [
                "Mandip Goswami"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2601.19949.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#audio",
                    "#open_source"
                ],
                "emoji": "ğŸ¤",
                "ru": {
                    "title": "Ğ¡Ñ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ±Ğ°Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€ĞµĞ²ĞµÑ€Ğ±ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° RIR-Mega-Speech â€” Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ°Ñ Ğ±Ğ°Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ¾ 117,5 Ñ‡Ğ°ÑĞ°Ğ¼Ğ¸ Ñ€ĞµĞ²ĞµÑ€Ğ±ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ€ĞµÑ‡Ğ¸, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ğ°Ñ Ğ¿ÑƒÑ‚ĞµĞ¼ ÑĞ²Ñ‘Ñ€Ñ‚ĞºĞ¸ Ğ²Ñ‹ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğ¹ LibriSpeech Ñ 5000 ÑĞ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ¼Ğ¿ÑƒĞ»ÑŒÑĞ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ‚ĞºĞ»Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ¿Ğ¾Ğ¼ĞµÑ‰ĞµĞ½Ğ¸Ğ¹. ĞšĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ñ„Ğ°Ğ¹Ğ» ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğµ Ğ°ĞºÑƒÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ²Ñ€ĞµĞ¼Ñ Ñ€ĞµĞ²ĞµÑ€Ğ±ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ (RT60), ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ğº Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ñ‘Ğ½Ğ½Ğ¾Ğ¼Ñƒ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñƒ (DRR) Ğ¸ Ğ¸Ğ½Ğ´ĞµĞºÑ Ñ€Ğ°Ğ·Ğ±Ğ¾Ñ€Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ (C50), Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ Ñ‡Ñ‘Ñ‚ĞºĞ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ñ‹Ğ¼ Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğ°Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ñ€ĞµĞ²ĞµÑ€Ğ±ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Whisper, Ğ·Ğ°Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ² ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° 2,50 Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿ÑƒĞ½ĞºÑ‚Ğ°, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ 48% Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞšĞ¾Ñ€Ğ¿ÑƒÑ ÑĞ¾Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ¶Ğ´Ğ°ĞµÑ‚ÑÑ ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ğ¾Ğ¹ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ€ĞµĞ²ĞµÑ€Ğ±ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ€ĞµÑ‡Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ»Ğ½ÑƒÑ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Standardizing Speech Research with RIR-Mega-Speech",
                    "desc": "This paper introduces RIR-Mega-Speech, a large-scale speech corpus designed to improve the reproducibility and comparison of speech processing research. It consists of 117.5 hours of audio created by applying simulated room impulse responses to LibriSpeech utterances, with detailed acoustic annotations for each file. Key metrics such as RT60, direct-to-reverberant ratio (DRR), and clarity index (C_{50}) are provided to facilitate standardized evaluations. The study also demonstrates the impact of reverberation on speech recognition performance, highlighting a measurable increase in word error rate (WER) as reverberation increases, thus underscoring the importance of transparent acoustic conditions in research."
                },
                "zh": {
                    "title": "æ ‡å‡†åŒ–æ··å“è¯­éŸ³ç ”ç©¶çš„èµ„æº",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªå¤§å‹çš„æ··å“è¯­éŸ³è¯­æ–™åº“RIR-Mega-Speechï¼Œæ—¨åœ¨ä¿ƒè¿›è¯­éŸ³å¤„ç†ç ”ç©¶çš„æ ‡å‡†åŒ–æ¯”è¾ƒå’Œé‡ç°ã€‚è¯¥è¯­æ–™åº“åŒ…å«çº¦117.5å°æ—¶çš„è¯­éŸ³æ•°æ®ï¼Œé€šè¿‡å°†LibriSpeechçš„è¯­éŸ³ä¸5000ä¸ªæ¨¡æ‹Ÿæˆ¿é—´è„‰å†²å“åº”è¿›è¡Œå·ç§¯ç”Ÿæˆã€‚æ¯ä¸ªæ–‡ä»¶éƒ½åŒ…å«RT60ã€ç›´æ¥åˆ°æ··å“æ¯”ï¼ˆDRRï¼‰å’Œæ¸…æ™°åº¦æŒ‡æ•°ï¼ˆC_{50}ï¼‰ï¼Œè¿™äº›æŒ‡æ ‡æ˜¯é€šè¿‡æ˜ç¡®çš„ã€å¯é‡ç°çš„ç¨‹åºè®¡ç®—å¾—å‡ºçš„ã€‚æˆ‘ä»¬è¿˜æä¾›äº†é‡å»ºæ•°æ®é›†å’Œé‡ç°æ‰€æœ‰è¯„ä¼°ç»“æœçš„è„šæœ¬ï¼Œä»¥ä¾¿ç ”ç©¶äººå‘˜èƒ½å¤ŸéªŒè¯ç»“æœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.20622",
            "title": "SketchDynamics: Exploring Free-Form Sketches for Dynamic Intent Expression in Animation Generation",
            "url": "https://huggingface.co/papers/2601.20622",
            "abstract": "Free-form sketching enables intuitive dynamic intent communication for automated content creation, bridging human intention and digital output in animation workflows.  \t\t\t\t\tAI-generated summary \t\t\t\t Sketching provides an intuitive way to convey dynamic intent in animation authoring (i.e., how elements change over time and space), making it a natural medium for automatic content creation. Yet existing approaches often constrain sketches to fixed command tokens or predefined visual forms, overlooking their freeform nature and the central role of humans in shaping intention. To address this, we introduce an interaction paradigm where users convey dynamic intent to a vision-language model via free-form sketching, instantiated here in a sketch storyboard to motion graphics workflow. We implement an interface and improve it through a three-stage study with 24 participants. The study shows how sketches convey motion with minimal input, how their inherent ambiguity requires users to be involved for clarification, and how sketches can visually guide video refinement. Our findings reveal the potential of sketch and AI interaction to bridge the gap between intention and outcome, and demonstrate its applicability to 3D animation and video generation.",
            "score": 0,
            "issue_id": 822,
            "pub_date": "2026-01-28",
            "pub_date_card": {
                "ru": "28 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 28",
                "zh": "1æœˆ28æ—¥"
            },
            "hash": "9ef7fcfd651bd18a",
            "authors": [
                "Boyu Li",
                "Lin-Ping Yuan",
                "Zeyu Wang",
                "Hongbo Fu"
            ],
            "affiliations": [
                "The Hong Kong University of Science and Technology",
                "The Hong Kong University of Science and Technology (Guangzhou)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.20622.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#3d",
                    "#multimodal",
                    "#cv",
                    "#story_generation"
                ],
                "emoji": "âœï¸",
                "ru": {
                    "title": "Ğ¡Ğ²Ğ¾Ğ±Ğ¾Ğ´Ğ½Ñ‹Ğ¹ Ñ€Ğ¸ÑÑƒĞ½Ğ¾Ğº ĞºĞ°Ğº ÑĞ·Ñ‹Ğº Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ğ¸ AI Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ³Ğ´Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ğ½ÑƒÑ Ñ„Ğ¾Ñ€Ğ¼Ñƒ Ñ€Ğ¸ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ ÑĞ¾ ÑĞºĞµÑ‚Ñ‡-ÑÑ‚Ğ¾Ñ€Ğ¸Ğ±Ğ¾Ñ€Ğ´ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. Ğ§ĞµÑ€ĞµĞ· Ñ‚Ñ€Ñ‘Ñ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ 24 ÑƒÑ‡Ğ°ÑÑ‚Ğ½Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¾, ĞºĞ°Ğº Ğ½Ğ°Ğ±Ñ€Ğ¾ÑĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿ĞµÑ€ĞµĞ´Ğ°ÑÑ‚ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ, Ğ¿Ğ¾Ñ‡ĞµĞ¼Ñƒ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑĞºĞ¸Ğ·Ğ¾Ğ² Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ ÑƒÑ‡Ğ°ÑÑ‚Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ´Ğ»Ñ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ, Ğ¸ ĞºĞ°Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ñ€ĞµÑ„Ğ°Ğ¹Ğ½Ğ¼ĞµĞ½Ñ‚ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ¸ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ AI ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¼Ğ¾ÑÑ‚Ğ¸Ñ‚ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸ĞµĞ¼ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ° Ğ¸ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ¼, Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ² 3D-Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Bridging Intention and Animation through Free-Form Sketching",
                    "desc": "This paper presents a new way for users to communicate their creative ideas through free-form sketching in animation workflows. It highlights how sketches can express dynamic intent, allowing for automatic content creation without being limited to fixed commands. The authors developed an interface that connects user sketches to a vision-language model, facilitating a more intuitive interaction. Their study shows that sketches can effectively convey motion and guide video refinement, emphasizing the importance of human involvement in the creative process."
                },
                "zh": {
                    "title": "è‡ªç”±ç´ æï¼šè¿æ¥åˆ›ä½œæ„å›¾ä¸æ•°å­—è¾“å‡ºçš„æ¡¥æ¢",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†è‡ªç”±å½¢å¼ç´ æåœ¨è‡ªåŠ¨å†…å®¹åˆ›ä½œä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨åŠ¨ç”»åˆ¶ä½œæµç¨‹ä¸­å¦‚ä½•ä¼ è¾¾åŠ¨æ€æ„å›¾ã€‚é€šè¿‡ä¸è§†è§‰-è¯­è¨€æ¨¡å‹çš„äº¤äº’ï¼Œç”¨æˆ·å¯ä»¥ä½¿ç”¨è‡ªç”±ç´ ææ¥è¡¨è¾¾ä»–ä»¬çš„åˆ›ä½œæ„å›¾ï¼Œè€Œä¸æ˜¯ä¾èµ–å›ºå®šçš„å‘½ä»¤æˆ–é¢„å®šä¹‰çš„è§†è§‰å½¢å¼ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç´ æèƒ½å¤Ÿä»¥æœ€å°çš„è¾“å…¥ä¼ è¾¾è¿åŠ¨ä¿¡æ¯ï¼ŒåŒæ—¶å…¶å›ºæœ‰çš„æ¨¡ç³Šæ€§éœ€è¦ç”¨æˆ·å‚ä¸ä»¥è¿›è¡Œæ¾„æ¸…ã€‚æˆ‘ä»¬çš„å‘ç°å±•ç¤ºäº†ç´ æä¸äººå·¥æ™ºèƒ½äº¤äº’çš„æ½œåŠ›ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°è¿æ¥åˆ›ä½œæ„å›¾ä¸æœ€ç»ˆç»“æœï¼Œé€‚ç”¨äº3DåŠ¨ç”»å’Œè§†é¢‘ç”Ÿæˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.20380",
            "title": "OmegaUse: Building a General-Purpose GUI Agent for Autonomous Task Execution",
            "url": "https://huggingface.co/papers/2601.20380",
            "abstract": "OmegaUse is a general-purpose GUI agent model that achieves state-of-the-art performance on mobile and desktop platforms through a combination of high-quality data construction, decoupled training methods, and a Mixture-of-Experts architecture.  \t\t\t\t\tAI-generated summary \t\t\t\t Graphical User Interface (GUI) agents show great potential for enabling foundation models to complete real-world tasks, revolutionizing human-computer interaction and improving human productivity. In this report, we present OmegaUse, a general-purpose GUI agent model for autonomous task execution on both mobile and desktop platforms, supporting computer-use and phone-use scenarios. Building an effective GUI agent model relies on two factors: (1) high-quality data and (2) effective training methods. To address these, we introduce a carefully engineered data-construction pipeline and a decoupled training paradigm. For data construction, we leverage rigorously curated open-source datasets and introduce a novel automated synthesis framework that integrates bottom-up autonomous exploration with top-down taxonomy-guided generation to create high-fidelity synthetic data. For training, to better leverage these data, we adopt a two-stage strategy: Supervised Fine-Tuning (SFT) to establish fundamental interaction syntax, followed by Group Relative Policy Optimization (GRPO) to improve spatial grounding and sequential planning. To balance computational efficiency with agentic reasoning capacity, OmegaUse is built on a Mixture-of-Experts (MoE) backbone. To evaluate cross-terminal capabilities in an offline setting, we introduce OS-Nav, a benchmark suite spanning multiple operating systems: ChiM-Nav, targeting Chinese Android mobile environments, and Ubu-Nav, focusing on routine desktop interactions on Ubuntu. Extensive experiments show that OmegaUse is highly competitive across established GUI benchmarks, achieving a state-of-the-art (SOTA) score of 96.3% on ScreenSpot-V2 and a leading 79.1% step success rate on AndroidControl. OmegaUse also performs strongly on OS-Nav, reaching 74.24% step success on ChiM-Nav and 55.9% average success on Ubu-Nav.",
            "score": 0,
            "issue_id": 822,
            "pub_date": "2026-01-28",
            "pub_date_card": {
                "ru": "28 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 28",
                "zh": "1æœˆ28æ—¥"
            },
            "hash": "d5b6498a955243f2",
            "authors": [
                "Le Zhang",
                "Yixiong Xiao",
                "Xinjiang Lu",
                "Jingjia Cao",
                "Yusai Zhao",
                "Jingbo Zhou",
                "Lang An",
                "Zikan Feng",
                "Wanxiang Sha",
                "Yu Shi",
                "Congxi Xiao",
                "Jian Xiong",
                "Yankai Zhang",
                "Hua Wu",
                "Haifeng Wang"
            ],
            "affiliations": [
                "Baidu Frontier Research Department"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.20380.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#optimization",
                    "#training",
                    "#dataset",
                    "#rlhf",
                    "#architecture",
                    "#synthetic",
                    "#agents"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ»ÑĞ±Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞµ",
                    "desc": "OmegaUse â€” ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ğ´ĞµÑĞºÑ‚Ğ¾Ğ¿Ğ½Ñ‹Ñ… Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ… Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñƒ Ğ¸ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ´Ğ²Ğ° ÑÑ‚Ğ°Ğ¿Ğ°: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ supervised fine-tuning Ğ´Ğ»Ñ Ğ¾ÑĞ²Ğ¾ĞµĞ½Ğ¸Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Group Relative Policy Optimization Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Mixture-of-Experts Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑÑƒÑ€ÑÑ‹ Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğ° Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ñ‚ĞµÑÑ‚Ğ¾Ğ² OS-Nav Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºÑ€Ğ¾ÑÑ-Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹."
                },
                "en": {
                    "title": "OmegaUse: Revolutionizing GUI Agents for Seamless Task Execution",
                    "desc": "OmegaUse is a versatile GUI agent model designed for both mobile and desktop platforms, achieving top performance through advanced data construction and training techniques. It utilizes a Mixture-of-Experts architecture to balance computational efficiency with reasoning capabilities. The model is trained using a two-stage approach, starting with Supervised Fine-Tuning to establish interaction syntax, followed by Group Relative Policy Optimization for enhanced spatial and sequential planning. Extensive evaluations demonstrate OmegaUse's competitive edge, achieving state-of-the-art results on various benchmarks, including a 96.3% score on ScreenSpot-V2."
                },
                "zh": {
                    "title": "OmegaUseï¼šæå‡äººæœºäº¤äº’çš„é€šç”¨GUIä»£ç†",
                    "desc": "OmegaUseæ˜¯ä¸€ç§é€šç”¨çš„å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨ç§»åŠ¨å’Œæ¡Œé¢å¹³å°ä¸Šå®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¯¥æ¨¡å‹ä¾èµ–äºé«˜è´¨é‡çš„æ•°æ®æ„å»ºå’Œæœ‰æ•ˆçš„è®­ç»ƒæ–¹æ³•ï¼Œé‡‡ç”¨äº†æ··åˆä¸“å®¶æ¶æ„ã€‚ä¸ºäº†æ„å»ºæœ‰æ•ˆçš„GUIä»£ç†ï¼ŒOmegaUseå¼•å…¥äº†ç²¾å¿ƒè®¾è®¡çš„æ•°æ®æ„å»ºæµç¨‹å’Œè§£è€¦è®­ç»ƒèŒƒå¼ã€‚é€šè¿‡è¿™äº›åˆ›æ–°ï¼ŒOmegaUseåœ¨å¤šä¸ªæ“ä½œç³»ç»Ÿçš„åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå±•ç¤ºäº†å…¶åœ¨çœŸå®ä¸–ç•Œä»»åŠ¡ä¸­çš„å¼ºå¤§èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.19194",
            "title": "SE-DiCoW: Self-Enrolled Diarization-Conditioned Whisper",
            "url": "https://huggingface.co/papers/2601.19194",
            "abstract": "SE-DiCoW improves speaker-attributed ASR performance by using diarization output to identify enrollment segments for fixed conditioning in cross-attention layers, achieving significant reductions in transcription error rates.  \t\t\t\t\tAI-generated summary \t\t\t\t Speaker-attributed automatic speech recognition (ASR) in multi-speaker environments remains a major challenge. While some approaches achieve strong performance when fine-tuned on specific domains, few systems generalize well across out-of-domain datasets. Our prior work, Diarization-Conditioned Whisper (DiCoW), leverages speaker diarization outputs as conditioning information and, with minimal fine-tuning, demonstrated strong multilingual and multi-domain performance. In this paper, we address a key limitation of DiCoW: ambiguity in Silence-Target-Non-target-Overlap (STNO) masks, where two or more fully overlapping speakers may have nearly identical conditioning despite differing transcriptions. We introduce SE-DiCoW (Self-Enrolled Diarization-Conditioned Whisper), which uses diarization output to locate an enrollment segment anywhere in the conversation where the target speaker is most active. This enrollment segment is used as fixed conditioning via cross-attention at each encoder layer. We further refine DiCoW with improved data segmentation, model initialization, and augmentation. Together, these advances yield substantial gains: SE-DiCoW reduces macro-averaged tcpWER by 52.4% relative to the original DiCoW on the EMMA MT-ASR benchmark.",
            "score": 0,
            "issue_id": 822,
            "pub_date": "2026-01-27",
            "pub_date_card": {
                "ru": "27 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 27",
                "zh": "1æœˆ27æ—¥"
            },
            "hash": "e1a51f625cb87a96",
            "authors": [
                "Alexander Polok",
                "Dominik Klement",
                "Samuele Cornell",
                "Matthew Wiesner",
                "Jan ÄŒernockÃ½",
                "Sanjeev Khudanpur",
                "LukÃ¡Å¡ Burget"
            ],
            "affiliations": [
                "CLSP & HLTCOE, Johns Hopkins University, USA",
                "Language Technologies Institute, Carnegie Mellon University, USA",
                "Speech@FIT, Brno University of Technology, Czechia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.19194.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#audio",
                    "#multilingual"
                ],
                "emoji": "ğŸ¤",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ¸Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ ÑĞ¿Ğ¸ĞºĞµÑ€Ğ¾Ğ²",
                    "desc": "SE-DiCoW ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµÑ‡Ğ¸ Ñ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸ĞµĞ¹ ÑĞ¿Ğ¸ĞºĞµÑ€Ğ° Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰Ğ¸Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ÑÑ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²Ñ‹Ñ…Ğ¾Ğ´ Ğ´Ğ¸Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ° Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ ÑĞ¿Ğ¸ĞºĞµÑ€Ğ°. Ğ­Ğ½ĞºĞ¾Ğ´ĞµÑ€ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ cross-attention ÑĞ»Ğ¾Ñ‘Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ÑÑ‚ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ½Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ· Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ°ÑĞº STNO Ğ² Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğµ DiCoW, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¿ĞµÑ€ĞµĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸ĞµÑÑ ÑĞ¿Ğ¸ĞºĞµÑ€Ñ‹ Ğ¸Ğ¼ĞµĞ»Ğ¸ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ: 52.4% Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ EMMA MT-ASR."
                },
                "en": {
                    "title": "Enhancing ASR with Targeted Speaker Conditioning",
                    "desc": "The paper presents SE-DiCoW, an enhancement of the Diarization-Conditioned Whisper model for speaker-attributed automatic speech recognition (ASR) in multi-speaker settings. It addresses the challenge of overlapping speech by using diarization outputs to identify the most active segments of a target speaker for conditioning in cross-attention layers. This method significantly reduces transcription error rates by providing clearer context for the model during processing. The improvements in data segmentation, model initialization, and augmentation contribute to a notable 52.4% reduction in error rates compared to the original DiCoW model."
                },
                "zh": {
                    "title": "æå‡å¤šè¯´è¯è€…ASRæ€§èƒ½çš„æ–°æ–¹æ³•",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºSE-DiCoWçš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜å¤šè¯´è¯è€…ç¯å¢ƒä¸‹çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ€§èƒ½ã€‚é€šè¿‡ä½¿ç”¨è¯´è¯è€…åˆ†ç¦»è¾“å‡ºï¼ŒSE-DiCoWèƒ½å¤Ÿå‡†ç¡®è¯†åˆ«ç›®æ ‡è¯´è¯è€…çš„æ´»è·ƒæ®µè½ï¼Œå¹¶åœ¨äº¤å‰æ³¨æ„åŠ›å±‚ä¸­è¿›è¡Œå›ºå®šæ¡ä»¶å¤„ç†ã€‚ä¸ä¹‹å‰çš„DiCoWæ–¹æ³•ç›¸æ¯”ï¼ŒSE-DiCoWåœ¨æ•°æ®åˆ†å‰²ã€æ¨¡å‹åˆå§‹åŒ–å’Œæ•°æ®å¢å¼ºæ–¹é¢è¿›è¡Œäº†æ”¹è¿›ï¼Œæ˜¾è‘—é™ä½äº†è½¬å½•é”™è¯¯ç‡ã€‚æœ€ç»ˆï¼ŒSE-DiCoWåœ¨EMMA MT-ASRåŸºå‡†æµ‹è¯•ä¸­ï¼Œç›¸è¾ƒäºåŸå§‹DiCoWï¼Œå®è§‚å¹³å‡tcpWERé™ä½äº†52.4%ã€‚"
                }
            }
        }
    ],
    "link_prev": "2026-01-28.html",
    "link_next": "2026-01-30.html",
    "link_month": "2026-01.html",
    "short_date_prev": {
        "ru": "28.01",
        "en": "01/28",
        "zh": "1æœˆ28æ—¥"
    },
    "short_date_next": {
        "ru": "30.01",
        "en": "01/30",
        "zh": "1æœˆ30æ—¥"
    },
    "categories": {
        "#dataset": 3,
        "#data": 2,
        "#benchmark": 5,
        "#agents": 4,
        "#cv": 2,
        "#rl": 2,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 1,
        "#inference": 0,
        "#3d": 1,
        "#audio": 2,
        "#video": 2,
        "#multimodal": 4,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 5,
        "#robotics": 2,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 1,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 3,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 6,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    }
}