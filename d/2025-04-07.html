
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 13 papers. April 7.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">7 апреля</span> | <span id="title-articles-count">13 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-04-04.html">⬅️ <span id="prev-date">04.04</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-04-08.html">➡️ <span id="next-date">08.04</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-04.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '7 апреля', 'en': 'April 7', 'zh': '4月7日'};
        let feedDateNext = {'ru': '08.04', 'en': '04/08', 'zh': '4月8日'};
        let feedDatePrev = {'ru': '04.04', 'en': '04/04', 'zh': '4月4日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2504.02605', 'title': 'Multi-SWE-bench: A Multilingual Benchmark for Issue Resolving', 'url': 'https://huggingface.co/papers/2504.02605', 'abstract': 'The task of issue resolving is to modify a codebase to generate a patch that addresses a given issue. However, existing benchmarks, such as SWE-bench, focus almost exclusively on Python, making them insufficient for evaluating Large Language Models (LLMs) across diverse software ecosystems. To address this, we introduce a multilingual issue-resolving benchmark, called Multi-SWE-bench, covering Java, TypeScript, JavaScript, Go, Rust, C, and C++. It includes a total of 1,632 high-quality instances, which were carefully annotated from 2,456 candidates by 68 expert annotators, ensuring that the benchmark can provide an accurate and reliable evaluation. Based on Multi-SWE-bench, we evaluate a series of state-of-the-art models using three representative methods (Agentless, SWE-agent, and OpenHands) and present a comprehensive analysis with key empirical insights. In addition, we launch a Multi-SWE-RL open-source community, aimed at building large-scale reinforcement learning (RL) training datasets for issue-resolving tasks. As an initial contribution, we release a set of 4,723 well-structured instances spanning seven programming languages, laying a solid foundation for RL research in this domain. More importantly, we open-source our entire data production pipeline, along with detailed tutorials, encouraging the open-source community to continuously contribute and expand the dataset. We envision our Multi-SWE-bench and the ever-growing Multi-SWE-RL community as catalysts for advancing RL toward its full potential, bringing us one step closer to the dawn of AGI.', 'score': 28, 'issue_id': 3095, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 апреля', 'en': 'April 3', 'zh': '4月3日'}, 'hash': 'bcf7d7c20685c914', 'authors': ['Daoguang Zan', 'Zhirong Huang', 'Wei Liu', 'Hanwu Chen', 'Linhao Zhang', 'Shulin Xin', 'Lu Chen', 'Qi Liu', 'Xiaojian Zhong', 'Aoyan Li', 'Siyao Liu', 'Yongsheng Xiao', 'Liangqiang Chen', 'Yuyu Zhang', 'Jing Su', 'Tianyu Liu', 'Rui Long', 'Kai Shen', 'Liang Xiang'], 'affiliations': ['bytedance.com'], 'pdf_title_img': 'assets/pdf/title_img/2504.02605.jpg', 'data': {'categories': ['#agi', '#benchmark', '#rl', '#open_source', '#multilingual', '#dataset'], 'emoji': '🌐', 'ru': {'title': 'Многоязычный бенчмарк для оценки ИИ в решении программных задач', 'desc': 'Представлен новый многоязычный бенчмарк Multi-SWE-bench для оценки способности языковых моделей решать задачи в различных программных экосистемах. Бенчмарк включает 1632 аннотированных примера на семи языках программирования. Проведена оценка современных моделей с использованием трех методов: Agentless, SWE-agent и OpenHands. Запущено сообщество Multi-SWE-RL для создания наборов данных для обучения с подкреплением в задачах исправления ошибок.'}, 'en': {'title': 'Empowering Issue Resolution with Multi-SWE-bench for Diverse Languages', 'desc': 'This paper introduces Multi-SWE-bench, a multilingual benchmark designed to evaluate Large Language Models (LLMs) in issue resolving across various programming languages, including Java, TypeScript, and C++. The benchmark consists of 1,632 high-quality instances, meticulously annotated by experts to ensure reliability in assessing model performance. The authors also present an analysis of state-of-the-art models using different evaluation methods and launch the Multi-SWE-RL community to foster the development of reinforcement learning datasets for issue-resolving tasks. By open-sourcing their data production pipeline and tutorials, they aim to encourage community contributions and advance research in this area, ultimately pushing towards the goal of Artificial General Intelligence (AGI).'}, 'zh': {'title': '多语言问题解决基准，推动强化学习研究', 'desc': '本论文介绍了一种多语言问题解决基准，称为Multi-SWE-bench，旨在评估大型语言模型在不同软件生态系统中的表现。该基准涵盖了Java、TypeScript、JavaScript、Go、Rust、C和C++等七种编程语言，共包含1,632个高质量实例，确保评估的准确性和可靠性。我们还推出了Multi-SWE-RL开源社区，旨在为问题解决任务构建大规模的强化学习训练数据集，并发布了4,723个结构良好的实例。通过开放数据生产流程和详细教程，我们希望激励开源社区持续贡献，推动强化学习研究的发展。'}}}, {'id': 'https://huggingface.co/papers/2504.03553', 'title': 'Agentic Knowledgeable Self-awareness', 'url': 'https://huggingface.co/papers/2504.03553', 'abstract': 'Large Language Models (LLMs) have achieved considerable performance across various agentic planning tasks. However, traditional agent planning approaches adopt a "flood irrigation" methodology that indiscriminately injects gold trajectories, external feedback, and domain knowledge into agent models. This practice overlooks the fundamental human cognitive principle of situational self-awareness during decision-making-the ability to dynamically assess situational demands and strategically employ resources during decision-making. We propose agentic knowledgeable self-awareness to address this gap, a novel paradigm enabling LLM-based agents to autonomously regulate knowledge utilization. Specifically, we propose KnowSelf, a data-centric approach that applies agents with knowledgeable self-awareness like humans. Concretely, we devise a heuristic situation judgement criterion to mark special tokens on the agent\'s self-explored trajectories for collecting training data. Through a two-stage training process, the agent model can switch between different situations by generating specific special tokens, achieving optimal planning effects with minimal costs. Our experiments demonstrate that KnowSelf can outperform various strong baselines on different tasks and models with minimal use of external knowledge. Code is available at https://github.com/zjunlp/KnowSelf.', 'score': 15, 'issue_id': 3096, 'pub_date': '2025-04-04', 'pub_date_card': {'ru': '4 апреля', 'en': 'April 4', 'zh': '4月4日'}, 'hash': '4a06cb6959ea30d3', 'authors': ['Shuofei Qiao', 'Zhisong Qiu', 'Baochang Ren', 'Xiaobin Wang', 'Xiangyuan Ru', 'Ningyu Zhang', 'Xiang Chen', 'Yong Jiang', 'Pengjun Xie', 'Fei Huang', 'Huajun Chen'], 'affiliations': ['Alibaba Group', 'Nanjing University of Aeronautics and Astronautics', 'Zhejiang Key Laboratory of Big Data Intelligent Computing', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2504.03553.jpg', 'data': {'categories': ['#training', '#reasoning', '#optimization', '#agents', '#agi'], 'emoji': '🧠', 'ru': {'title': 'Самоосознанность в планировании: эффективное использование знаний языковыми моделями', 'desc': 'Статья представляет новый подход к обучению языковых моделей для задач планирования, называемый KnowSelf. Он основан на принципе ситуативной самоосознанности, позволяющем модели динамически оценивать ситуацию и стратегически использовать знания. KnowSelf использует двухэтапный процесс обучения и специальные токены для переключения между различными ситуациями. Эксперименты показывают, что этот метод превосходит базовые подходы на различных задачах, минимально используя внешние знания.'}, 'en': {'title': 'Empowering LLMs with Self-Aware Decision Making', 'desc': "This paper introduces a new approach called agentic knowledgeable self-awareness for Large Language Models (LLMs) in planning tasks. Unlike traditional methods that flood models with external information, this approach emphasizes the importance of situational awareness, allowing agents to assess their environment and use knowledge more effectively. The proposed method, KnowSelf, utilizes a heuristic to identify key moments in an agent's learning process, enabling it to adapt its strategies based on the situation. Experimental results show that KnowSelf significantly improves performance on various tasks while minimizing reliance on external knowledge."}, 'zh': {'title': '自主调节知识使用的智能代理', 'desc': '大型语言模型（LLMs）在多种代理规划任务中表现出色。然而，传统的代理规划方法采用了"洪水灌溉"的方式，随意注入黄金轨迹、外部反馈和领域知识，这种做法忽视了人类在决策过程中动态评估情境需求的能力。我们提出了代理知识自我意识的概念，旨在填补这一空白，使基于LLM的代理能够自主调节知识的使用。具体而言，我们设计了一种启发式情境判断标准，通过标记代理自我探索轨迹上的特殊标记，收集训练数据，从而实现更高效的规划效果。'}}}, {'id': 'https://huggingface.co/papers/2504.02807', 'title': 'MegaMath: Pushing the Limits of Open Math Corpora', 'url': 'https://huggingface.co/papers/2504.02807', 'abstract': 'Mathematical reasoning is a cornerstone of human intelligence and a key benchmark for advanced capabilities in large language models (LLMs). However, the research community still lacks an open, large-scale, high-quality corpus tailored to the demands of math-centric LLM pre-training. We present MegaMath, an open dataset curated from diverse, math-focused sources through following practices: (1) Revisiting web data: We re-extracted mathematical documents from Common Crawl with math-oriented HTML optimizations, fasttext-based filtering and deduplication, all for acquiring higher-quality data on the Internet. (2) Recalling Math-related code data: We identified high quality math-related code from large code training corpus, Stack-V2, further enhancing data diversity. (3) Exploring Synthetic data: We synthesized QA-style text, math-related code, and interleaved text-code blocks from web data or code data. By integrating these strategies and validating their effectiveness through extensive ablations, MegaMath delivers 371B tokens with the largest quantity and top quality among existing open math pre-training datasets.', 'score': 13, 'issue_id': 3102, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 апреля', 'en': 'April 3', 'zh': '4月3日'}, 'hash': 'a6dd17864afc6dca', 'authors': ['Fan Zhou', 'Zengzhi Wang', 'Nikhil Ranjan', 'Zhoujun Cheng', 'Liping Tang', 'Guowei He', 'Zhengzhong Liu', 'Eric P. Xing'], 'affiliations': ['MBZUAI', 'MegaMath'], 'pdf_title_img': 'assets/pdf/title_img/2504.02807.jpg', 'data': {'categories': ['#open_source', '#dataset', '#reasoning', '#synthetic', '#data'], 'emoji': '🧮', 'ru': {'title': 'MegaMath: Большие данные для умных вычислений', 'desc': 'Статья представляет MegaMath - крупномасштабный открытый датасет для предобучения языковых моделей в области математики. Авторы использовали оптимизированные методы извлечения математических документов из веб-данных, а также включили математический код и синтетические данные. Датасет содержит 371 миллиард токенов и отличается высоким качеством и разнообразием. MegaMath призван улучшить математические рассуждения в больших языковых моделях.'}, 'en': {'title': 'MegaMath: Elevating LLMs with a Massive Math Dataset', 'desc': "This paper introduces MegaMath, a comprehensive dataset designed to enhance the mathematical reasoning capabilities of large language models (LLMs). The dataset is created by extracting and optimizing mathematical documents from the web, ensuring high quality through filtering and deduplication. Additionally, it incorporates high-quality math-related code from existing code corpora, further enriching the dataset's diversity. By synthesizing various forms of data, MegaMath provides a substantial resource of 371 billion tokens, making it the largest and highest quality open dataset for math pre-training available."}, 'zh': {'title': 'MegaMath：数学推理的开放数据集', 'desc': '数学推理是人类智能的基石，也是大型语言模型（LLM）高级能力的重要基准。然而，目前研究界缺乏一个开放的大规模高质量数学数据集，以满足数学中心的LLM预训练需求。我们提出了MegaMath，这是一个从多种数学相关来源精心策划的开放数据集，包含3710亿个标记，具有现有开放数学预训练数据集中最大的数量和最佳质量。该数据集通过重新提取网络数据、回收数学相关代码数据和探索合成数据等策略，确保了数据的多样性和高质量。'}}}, {'id': 'https://huggingface.co/papers/2504.03561', 'title': 'SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge\n  Refinement', 'url': 'https://huggingface.co/papers/2504.03561', 'abstract': 'In the interaction between agents and their environments, agents expand their capabilities by planning and executing actions. However, LLM-based agents face substantial challenges when deployed in novel environments or required to navigate unconventional action spaces. To empower agents to autonomously explore environments, optimize workflows, and enhance their understanding of actions, we propose SynWorld, a framework that allows agents to synthesize possible scenarios with multi-step action invocation within the action space and perform Monte Carlo Tree Search (MCTS) exploration to effectively refine their action knowledge in the current environment. Our experiments demonstrate that SynWorld is an effective and general approach to learning action knowledge in new environments. Code is available at https://github.com/zjunlp/SynWorld.', 'score': 11, 'issue_id': 3096, 'pub_date': '2025-04-04', 'pub_date_card': {'ru': '4 апреля', 'en': 'April 4', 'zh': '4月4日'}, 'hash': '469f9f28c32e1a1a', 'authors': ['Runnan Fang', 'Xiaobin Wang', 'Yuan Liang', 'Shuofei Qiao', 'Jialong Wu', 'Zekun Xi', 'Ningyu Zhang', 'Yong Jiang', 'Pengjun Xie', 'Fei Huang', 'Huajun Chen'], 'affiliations': ['Alibaba Group', 'Zhejiang Key Laboratory of Big Data Intelligent Computing', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2504.03561.jpg', 'data': {'categories': ['#optimization', '#transfer_learning', '#agents', '#rl'], 'emoji': '🌐', 'ru': {'title': 'SynWorld: Автономное исследование и обучение агентов в новых средах', 'desc': 'SynWorld - это фреймворк, позволяющий агентам на основе больших языковых моделей (LLM) автономно исследовать новые среды и оптимизировать рабочие процессы. Он использует синтез возможных сценариев с многошаговым вызовом действий и применяет метод Монте-Карло для поиска по дереву (MCTS) для эффективного уточнения знаний о действиях в текущей среде. Эксперименты показывают, что SynWorld является эффективным и универсальным подходом к изучению знаний о действиях в новых средах. Этот метод решает проблему ограниченных возможностей LLM-агентов при работе в незнакомых окружениях или нестандартных пространствах действий.'}, 'en': {'title': 'Empowering Agents to Explore with SynWorld', 'desc': "This paper introduces SynWorld, a framework designed to help agents improve their capabilities in unfamiliar environments. It allows agents to create and evaluate different scenarios by using multi-step actions and Monte Carlo Tree Search (MCTS) for exploration. By synthesizing possible actions, agents can better understand how to navigate and optimize their workflows. The results show that SynWorld effectively enhances agents' action knowledge in new settings, making it a valuable tool for autonomous exploration."}, 'zh': {'title': 'SynWorld：赋能代理探索新环境的框架', 'desc': '在代理与环境的互动中，代理通过规划和执行动作来扩展其能力。然而，基于大型语言模型的代理在新环境中或需要在非常规动作空间中导航时面临重大挑战。为了解决这个问题，我们提出了SynWorld框架，使代理能够合成可能的场景，并在动作空间内进行多步动作调用，同时执行蒙特卡洛树搜索（MCTS）探索，以有效地优化其在当前环境中的动作知识。实验结果表明，SynWorld是学习新环境中动作知识的有效且通用的方法。'}}}, {'id': 'https://huggingface.co/papers/2504.03641', 'title': 'MME-Unify: A Comprehensive Benchmark for Unified Multimodal\n  Understanding and Generation Models', 'url': 'https://huggingface.co/papers/2504.03641', 'abstract': 'Existing MLLM benchmarks face significant challenges in evaluating Unified MLLMs (U-MLLMs) due to: 1) lack of standardized benchmarks for traditional tasks, leading to inconsistent comparisons; 2) absence of benchmarks for mixed-modality generation, which fails to assess multimodal reasoning capabilities. We present a comprehensive evaluation framework designed to systematically assess U-MLLMs. Our benchmark includes: Standardized Traditional Task Evaluation. We sample from 12 datasets, covering 10 tasks with 30 subtasks, ensuring consistent and fair comparisons across studies." 2. Unified Task Assessment. We introduce five novel tasks testing multimodal reasoning, including image editing, commonsense QA with image generation, and geometric reasoning. 3. Comprehensive Model Benchmarking. We evaluate 12 leading U-MLLMs, such as Janus-Pro, EMU3, VILA-U, and Gemini2-flash, alongside specialized understanding (e.g., Claude-3.5-Sonnet) and generation models (e.g., DALL-E-3). Our findings reveal substantial performance gaps in existing U-MLLMs, highlighting the need for more robust models capable of handling mixed-modality tasks effectively. The code and evaluation data can be found in https://mme-unify.github.io/.', 'score': 8, 'issue_id': 3095, 'pub_date': '2025-04-04', 'pub_date_card': {'ru': '4 апреля', 'en': 'April 4', 'zh': '4月4日'}, 'hash': '45da77ffd9c21caf', 'authors': ['Wulin Xie', 'Yi-Fan Zhang', 'Chaoyou Fu', 'Yang Shi', 'Bingyan Nie', 'Hongkai Chen', 'Zhang Zhang', 'Liang Wang', 'Tieniu Tan'], 'affiliations': ['CASIA', 'M-M-E Project', 'NJU', 'PKU', 'Vivo'], 'pdf_title_img': 'assets/pdf/title_img/2504.03641.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#survey'], 'emoji': '🧠', 'ru': {'title': 'Новый стандарт оценки мультимодальных языковых моделей', 'desc': 'Эта статья представляет новую систему оценки для унифицированных мультимодальных языковых моделей (U-MLLM). Авторы разработали комплексный фреймворк, включающий стандартизированную оценку традиционных задач и новые задачи для тестирования мультимодальных рассуждений. Было проведено сравнительное тестирование 12 ведущих U-MLLM и специализированных моделей. Результаты выявили существенные пробелы в производительности существующих U-MLLM, подчеркивая необходимость разработки более надежных моделей для эффективного решения задач со смешанной модальностью.'}, 'en': {'title': 'Enhancing Evaluation for Unified Multimodal Language Models', 'desc': 'This paper addresses the challenges in evaluating Unified Multimodal Language Models (U-MLLMs) due to inconsistent benchmarks and the lack of assessments for mixed-modality tasks. It introduces a comprehensive evaluation framework that includes standardized traditional task evaluations across multiple datasets and novel tasks that test multimodal reasoning capabilities. The framework assesses 12 leading U-MLLMs, revealing significant performance gaps and underscoring the necessity for improved models that can effectively manage mixed-modality tasks. The findings aim to enhance the evaluation process and guide future developments in U-MLLMs.'}, 'zh': {'title': '全面评估统一多模态大语言模型的必要性', 'desc': '现有的多模态大语言模型（U-MLLM）基准在评估时面临重大挑战，包括缺乏标准化的传统任务基准和混合模态生成的基准。我们提出了一个全面的评估框架，系统地评估U-MLLM。该基准包括标准化的传统任务评估和五个新颖的多模态推理任务，如图像编辑和常识问答。我们的研究发现现有U-MLLM在性能上存在显著差距，强调了开发更强大模型的必要性，以有效处理混合模态任务。'}}}, {'id': 'https://huggingface.co/papers/2504.02949', 'title': 'VARGPT-v1.1: Improve Visual Autoregressive Large Unified Model via\n  Iterative Instruction Tuning and Reinforcement Learning', 'url': 'https://huggingface.co/papers/2504.02949', 'abstract': 'In this work, we present VARGPT-v1.1, an advanced unified visual autoregressive model that builds upon our previous framework VARGPT. The model preserves the dual paradigm of next-token prediction for visual understanding and next-scale generation for image synthesis. Specifically, VARGPT-v1.1 integrates: (1) a novel training strategy combining iterative visual instruction tuning with reinforcement learning through Direct Preference Optimization (DPO), (2) an expanded training corpus containing 8.3M visual-generative instruction pairs, (3) an upgraded language model backbone using Qwen2, (4) enhanced image generation resolution, and (5) emergent image editing capabilities without architectural modifications. These advancements enable VARGPT-v1.1 to achieve state-of-the-art performance in multimodal understanding and text-to-image instruction-following tasks, demonstrating significant improvements in both comprehension and generation metrics. Notably, through visual instruction tuning, the model acquires image editing functionality while maintaining architectural consistency with its predecessor, revealing the potential for unified visual understanding, generation, and editing. Our findings suggest that well-designed unified visual autoregressive models can effectively adopt flexible training strategies from large language models (LLMs), exhibiting promising scalability. The codebase and model weights are publicly available at https://github.com/VARGPT-family/VARGPT-v1.1.', 'score': 8, 'issue_id': 3095, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 апреля', 'en': 'April 3', 'zh': '4月3日'}, 'hash': '71423989b2bed2d8', 'authors': ['Xianwei Zhuang', 'Yuxin Xie', 'Yufan Deng', 'Dongchao Yang', 'Liming Liang', 'Jinghan Ru', 'Yuguo Yin', 'Yuexian Zou'], 'affiliations': ['School of Electronic and Computer Engineering, Peking University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2504.02949.jpg', 'data': {'categories': ['#training', '#multimodal', '#rlhf', '#open_source', '#cv', '#optimization'], 'emoji': '🖼️', 'ru': {'title': 'Единая модель для понимания, генерации и редактирования изображений', 'desc': 'VARGPT-v1.1 - это усовершенствованная унифицированная визуальная авторегрессионная модель, развивающая предыдущую версию VARGPT. Она использует новую стратегию обучения, сочетающую итеративную настройку визуальных инструкций с обучением с подкреплением через Direct Preference Optimization (DPO). Модель обучена на расширенном корпусе из 8,3 млн пар визуально-генеративных инструкций и использует улучшенную языковую модель Qwen2 в качестве основы. VARGPT-v1.1 достигает передовых результатов в задачах мультимодального понимания и генерации изображений по текстовым инструкциям.'}, 'en': {'title': 'Unifying Visual Understanding and Generation with VARGPT-v1.1', 'desc': 'VARGPT-v1.1 is a cutting-edge visual autoregressive model that enhances its predecessor by integrating advanced training techniques and a larger dataset. It employs a unique combination of visual instruction tuning and reinforcement learning to improve its performance in understanding and generating images. The model also features an upgraded backbone and higher image resolution, allowing for better quality outputs and new image editing capabilities. Overall, VARGPT-v1.1 demonstrates the effectiveness of unified models in handling multimodal tasks, showcasing significant advancements in both comprehension and generation.'}, 'zh': {'title': '统一视觉自回归模型的突破性进展', 'desc': '本研究介绍了VARGPT-v1.1，这是一个先进的统一视觉自回归模型，基于我们之前的VARGPT框架。该模型结合了视觉理解的下一个标记预测和图像合成的下一个尺度生成的双重范式。VARGPT-v1.1采用了一种新颖的训练策略，结合了迭代视觉指令调优和通过直接偏好优化（DPO）的强化学习。通过这些改进，VARGPT-v1.1在多模态理解和文本到图像指令跟随任务中实现了最先进的性能，展示了在理解和生成指标上的显著提升。'}}}, {'id': 'https://huggingface.co/papers/2504.03601', 'title': 'APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated\n  Agent-Human Interplay', 'url': 'https://huggingface.co/papers/2504.03601', 'abstract': 'Training effective AI agents for multi-turn interactions requires high-quality data that captures realistic human-agent dynamics, yet such data is scarce and expensive to collect manually. We introduce APIGen-MT, a two-phase framework that generates verifiable and diverse multi-turn agent data. In the first phase, our agentic pipeline produces detailed task blueprints with ground-truth actions, leveraging a committee of LLM reviewers and iterative feedback loops. These blueprints are then transformed into complete interaction trajectories through simulated human-agent interplay. We train a family of models -- the xLAM-2-fc-r series with sizes ranging from 1B to 70B parameters. Our models outperform frontier models such as GPT-4o and Claude 3.5 on tau-bench and BFCL benchmarks, with the smaller models surpassing their larger counterparts, particularly in multi-turn settings, while maintaining superior consistency across multiple trials. Comprehensive experiments demonstrate that our verified blueprint-to-details approach yields high-quality training data, enabling the development of more reliable, efficient, and capable agents. We open-source both the synthetic data collected and the trained xLAM-2-fc-r models to advance research in AI agents. Models are available on HuggingFace at https://huggingface.co/collections/Salesforce/xlam-2-67ef5be12949d8dcdae354c4 and project website is https://apigen-mt.github.io', 'score': 6, 'issue_id': 3097, 'pub_date': '2025-04-04', 'pub_date_card': {'ru': '4 апреля', 'en': 'April 4', 'zh': '4月4日'}, 'hash': '05921cbfa42a13b4', 'authors': ['Akshara Prabhakar', 'Zuxin Liu', 'Weiran Yao', 'Jianguo Zhang', 'Ming Zhu', 'Shiyu Wang', 'Zhiwei Liu', 'Tulika Awalgaonkar', 'Haolin Chen', 'Thai Hoang', 'Juan Carlos Niebles', 'Shelby Heinecke', 'Huan Wang', 'Silvio Savarese', 'Caiming Xiong'], 'affiliations': ['Salesforce AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2504.03601.jpg', 'data': {'categories': ['#dataset', '#synthetic', '#agents', '#open_source', '#data', '#training'], 'emoji': '🤖', 'ru': {'title': 'Революция в обучении ИИ-агентов: синтетические данные для реалистичного многоходового взаимодействия', 'desc': 'APIGen-MT - это новый фреймворк для генерации качественных данных для обучения ИИ-агентов многоходовому взаимодействию. Он использует двухфазный подход: сначала создаются детальные планы задач с помощью ревьюеров на основе больших языковых моделей, затем эти планы преобразуются в полные траектории взаимодействия. На основе полученных данных обучено семейство моделей xLAM-2-fc-r, превосходящих по ряду показателей такие модели как GPT-4 и Claude 3.5. Исследователи открыли доступ к синтетическим данным и обученным моделям для дальнейшего развития области ИИ-агентов.'}, 'en': {'title': 'Generating High-Quality Data for AI Agents with APIGen-MT', 'desc': 'The paper presents APIGen-MT, a framework designed to generate high-quality multi-turn interaction data for training AI agents. It consists of two phases: first, creating detailed task blueprints with accurate actions using a committee of large language model (LLM) reviewers and feedback loops. In the second phase, these blueprints are turned into full interaction sequences through simulated human-agent interactions. The resulting models, particularly the xLAM-2-fc-r series, show superior performance on benchmark tests, especially in multi-turn scenarios, and the authors provide open access to the generated data and models to support further research.'}, 'zh': {'title': '高效生成多轮交互数据的AI代理训练框架', 'desc': '为了训练有效的AI代理进行多轮交互，我们提出了APIGen-MT框架，该框架能够生成可验证和多样化的多轮代理数据。该框架分为两个阶段，首先通过大型语言模型（LLM）评审委员会和迭代反馈生成详细的任务蓝图，并提供真实的行动。接着，这些蓝图被转化为完整的交互轨迹，通过模拟人机互动实现。我们的xLAM-2-fc-r系列模型在多个基准测试中表现优异，尤其是在多轮设置中，小模型的表现超过了大模型，展示了我们的方法在生成高质量训练数据方面的有效性。'}}}, {'id': 'https://huggingface.co/papers/2504.03536', 'title': 'HumanDreamer-X: Photorealistic Single-image Human Avatars Reconstruction\n  via Gaussian Restoration', 'url': 'https://huggingface.co/papers/2504.03536', 'abstract': 'Single-image human reconstruction is vital for digital human modeling applications but remains an extremely challenging task. Current approaches rely on generative models to synthesize multi-view images for subsequent 3D reconstruction and animation. However, directly generating multiple views from a single human image suffers from geometric inconsistencies, resulting in issues like fragmented or blurred limbs in the reconstructed models. To tackle these limitations, we introduce HumanDreamer-X, a novel framework that integrates multi-view human generation and reconstruction into a unified pipeline, which significantly enhances the geometric consistency and visual fidelity of the reconstructed 3D models. In this framework, 3D Gaussian Splatting serves as an explicit 3D representation to provide initial geometry and appearance priority. Building upon this foundation, HumanFixer is trained to restore 3DGS renderings, which guarantee photorealistic results. Furthermore, we delve into the inherent challenges associated with attention mechanisms in multi-view human generation, and propose an attention modulation strategy that effectively enhances geometric details identity consistency across multi-view. Experimental results demonstrate that our approach markedly improves generation and reconstruction PSNR quality metrics by 16.45% and 12.65%, respectively, achieving a PSNR of up to 25.62 dB, while also showing generalization capabilities on in-the-wild data and applicability to various human reconstruction backbone models.', 'score': 6, 'issue_id': 3097, 'pub_date': '2025-04-04', 'pub_date_card': {'ru': '4 апреля', 'en': 'April 4', 'zh': '4月4日'}, 'hash': '9a6ad8e0086d88eb', 'authors': ['Boyuan Wang', 'Runqi Ouyang', 'Xiaofeng Wang', 'Zheng Zhu', 'Guosheng Zhao', 'Chaojun Ni', 'Guan Huang', 'Lihong Liu', 'Xingang Wang'], 'affiliations': ['GigaAI', 'Institute of Automation, Chinese Academy of Sciences', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2504.03536.jpg', 'data': {'categories': ['#synthetic', '#cv', '#3d'], 'emoji': '🧑\u200d🦰', 'ru': {'title': 'Реалистичные 3D-модели людей из одного фото', 'desc': 'HumanDreamer-X - это новая система для реконструкции 3D-моделей человека по одному изображению. Она объединяет генерацию мультиракурсных изображений и 3D-реконструкцию в единый процесс, что значительно улучшает геометрическую согласованность и визуальное качество моделей. Система использует 3D Gaussian Splatting для начальной геометрии и HumanFixer для улучшения рендеров. Предложенная стратегия модуляции внимания повышает детализацию и согласованность между ракурсами.'}, 'en': {'title': 'Revolutionizing Human Reconstruction with HumanDreamer-X', 'desc': 'This paper presents HumanDreamer-X, a new framework for single-image human reconstruction that combines multi-view generation and 3D reconstruction into one process. The framework addresses common issues like geometric inconsistencies and blurred limbs by using 3D Gaussian Splatting for better initial geometry and appearance. Additionally, it includes a component called HumanFixer, which enhances the photorealism of the 3D models. The authors also introduce an attention modulation strategy to improve detail consistency across different views, resulting in significant improvements in image quality metrics.'}, 'zh': {'title': '统一多视图生成与重建，提升人类模型质量', 'desc': '单图像人类重建对数字人类建模应用至关重要，但仍然是一个极具挑战性的任务。目前的方法依赖生成模型合成多视图图像以进行后续的3D重建和动画。然而，从单个人体图像直接生成多个视图会导致几何不一致，重建模型中出现肢体碎片或模糊的问题。为了解决这些限制，我们提出了HumanDreamer-X，一个将多视图人类生成和重建整合为统一流程的新框架，显著提高了重建3D模型的几何一致性和视觉真实感。'}}}, {'id': 'https://huggingface.co/papers/2503.24067', 'title': 'TransMamba: Flexibly Switching between Transformer and Mamba', 'url': 'https://huggingface.co/papers/2503.24067', 'abstract': 'Transformers are the cornerstone of modern large language models, but their quadratic computational complexity limits efficiency in long-sequence processing. Recent advancements in Mamba, a state space model (SSM) with linear complexity, offer promising efficiency gains but suffer from unstable contextual learning and multitask generalization. This paper proposes TransMamba, a novel framework that unifies Transformer and Mamba through shared parameter matrices (e.g., QKV and CBx), and thus could dynamically switch between attention and SSM mechanisms at different token lengths and layers. We design the Memory converter to bridge Transformer and Mamba by converting attention outputs into SSM-compatible states, ensuring seamless information flow at TransPoints where the transformation happens. The TransPoint scheduling is also thoroughly explored for further improvements. We conducted extensive experiments demonstrating that TransMamba achieves superior training efficiency and performance compared to baselines, and validated the deeper consistency between Transformer and Mamba paradigms, offering a scalable solution for next-generation sequence modeling.', 'score': 6, 'issue_id': 3100, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': 'c397bc55eaf9dd26', 'authors': ['Yixing Li', 'Ruobing Xie', 'Zhen Yang', 'Xingwu Sun', 'Shuaipeng Li', 'Weidong Han', 'Zhanhui Kang', 'Yu Cheng', 'Chengzhong Xu', 'Di Wang', 'Jie Jiang'], 'affiliations': ['Tencent Hunyuan', 'The Chinese University of Hong Kong', 'University of Macau'], 'pdf_title_img': 'assets/pdf/title_img/2503.24067.jpg', 'data': {'categories': ['#training', '#optimization', '#architecture', '#long_context'], 'emoji': '🔀', 'ru': {'title': 'Объединение Transformer и Mamba для эффективной обработки длинных последовательностей', 'desc': 'TransMamba - это новая архитектура, объединяющая Transformer и Mamba через общие матрицы параметров. Она позволяет динамически переключаться между механизмами внимания и моделями пространства состояний (SSM) на разных длинах токенов и слоях. Модель использует конвертер памяти для преобразования выходов внимания в состояния, совместимые с SSM. Эксперименты показали, что TransMamba превосходит базовые модели по эффективности обучения и производительности.'}, 'en': {'title': 'TransMamba: Bridging Transformers and State Space Models for Efficient Sequence Processing', 'desc': 'This paper introduces TransMamba, a new framework that combines the strengths of Transformers and Mamba, a state space model, to improve efficiency in processing long sequences. By using shared parameter matrices, TransMamba can switch between attention mechanisms and state space models based on the length of the input tokens. The Memory converter is designed to ensure smooth transitions between these two methods, allowing for effective information flow. Experimental results show that TransMamba outperforms existing models in both training efficiency and performance, making it a promising solution for future sequence modeling tasks.'}, 'zh': {'title': 'TransMamba：高效的序列建模新方案', 'desc': '本文提出了一种新的框架TransMamba，旨在结合Transformer和Mamba模型，以提高长序列处理的效率。通过共享参数矩阵，TransMamba能够在不同的token长度和层次之间动态切换注意力机制和状态空间模型（SSM）。我们设计了记忆转换器，将注意力输出转换为SSM兼容的状态，确保信息在转换点的无缝流动。此外，本文还深入探讨了TransPoint调度，以进一步提升性能。'}}}, {'id': 'https://huggingface.co/papers/2504.03011', 'title': 'Comprehensive Relighting: Generalizable and Consistent Monocular Human\n  Relighting and Harmonization', 'url': 'https://huggingface.co/papers/2504.03011', 'abstract': 'This paper introduces Comprehensive Relighting, the first all-in-one approach that can both control and harmonize the lighting from an image or video of humans with arbitrary body parts from any scene. Building such a generalizable model is extremely challenging due to the lack of dataset, restricting existing image-based relighting models to a specific scenario (e.g., face or static human). To address this challenge, we repurpose a pre-trained diffusion model as a general image prior and jointly model the human relighting and background harmonization in the coarse-to-fine framework. To further enhance the temporal coherence of the relighting, we introduce an unsupervised temporal lighting model that learns the lighting cycle consistency from many real-world videos without any ground truth. In inference time, our temporal lighting module is combined with the diffusion models through the spatio-temporal feature blending algorithms without extra training; and we apply a new guided refinement as a post-processing to preserve the high-frequency details from the input image. In the experiments, Comprehensive Relighting shows a strong generalizability and lighting temporal coherence, outperforming existing image-based human relighting and harmonization methods.', 'score': 5, 'issue_id': 3096, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 апреля', 'en': 'April 3', 'zh': '4月3日'}, 'hash': '94d3411c37993837', 'authors': ['Junying Wang', 'Jingyuan Liu', 'Xin Sun', 'Krishna Kumar Singh', 'Zhixin Shu', 'He Zhang', 'Jimei Yang', 'Nanxuan Zhao', 'Tuanfeng Y. Wang', 'Simon S. Chen', 'Ulrich Neumann', 'Jae Shin Yoon'], 'affiliations': ['Adobe Research', 'Runway', 'University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2504.03011.jpg', 'data': {'categories': ['#inference', '#video', '#cv', '#diffusion'], 'emoji': '💡', 'ru': {'title': 'Универсальное управление освещением людей на изображениях и видео', 'desc': 'Статья представляет Comprehensive Relighting - первый универсальный подход к контролю и гармонизации освещения людей на изображениях и видео с произвольными частями тела в любых сценах. Авторы используют предобученную диффузионную модель в качестве общего априорного распределения изображений и совместно моделируют перелозировку человека и гармонизацию фона. Для улучшения временной согласованности освещения вводится модель временного освещения, обучаемая без учителя на реальных видео. Эксперименты показывают, что метод превосходит существующие подходы к перелозировке и гармонизации изображений людей.'}, 'en': {'title': 'Revolutionizing Lighting Control in Images and Videos', 'desc': 'This paper presents Comprehensive Relighting, a novel method that allows for flexible control and harmonization of lighting in images or videos featuring humans. The challenge lies in the limited datasets available, which typically restrict existing models to specific scenarios like faces or static poses. To overcome this, the authors utilize a pre-trained diffusion model to create a unified approach that addresses both human relighting and background harmonization. Additionally, they introduce an unsupervised temporal lighting model that ensures consistent lighting across frames, enhancing the overall quality and realism of the relit images.'}, 'zh': {'title': '全面重光照：人类图像光照的全能解决方案', 'desc': '本文介绍了全面重光照（Comprehensive Relighting），这是首个能够控制和协调来自任意场景中人类图像或视频的光照的全能方法。构建这样一个通用模型非常具有挑战性，因为缺乏数据集，限制了现有基于图像的重光照模型只能应用于特定场景（例如，面部或静态人类）。为了解决这个问题，我们重新利用了一个预训练的扩散模型作为通用图像先验，并在粗到细的框架中联合建模人类重光照和背景协调。实验结果表明，全面重光照在通用性和光照时间一致性方面表现出色，超越了现有的基于图像的人类重光照和协调方法。'}}}, {'id': 'https://huggingface.co/papers/2504.02402', 'title': 'EvMic: Event-based Non-contact sound recovery from effective\n  spatial-temporal modeling', 'url': 'https://huggingface.co/papers/2504.02402', 'abstract': 'When sound waves hit an object, they induce vibrations that produce high-frequency and subtle visual changes, which can be used for recovering the sound. Early studies always encounter trade-offs related to sampling rate, bandwidth, field of view, and the simplicity of the optical path. Recent advances in event camera hardware show good potential for its application in visual sound recovery, because of its superior ability in capturing high-frequency signals. However, existing event-based vibration recovery methods are still sub-optimal for sound recovery. In this work, we propose a novel pipeline for non-contact sound recovery, fully utilizing spatial-temporal information from the event stream. We first generate a large training set using a novel simulation pipeline. Then we designed a network that leverages the sparsity of events to capture spatial information and uses Mamba to model long-term temporal information. Lastly, we train a spatial aggregation block to aggregate information from different locations to further improve signal quality. To capture event signals caused by sound waves, we also designed an imaging system using a laser matrix to enhance the gradient and collected multiple data sequences for testing. Experimental results on synthetic and real-world data demonstrate the effectiveness of our method.', 'score': 4, 'issue_id': 3099, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 апреля', 'en': 'April 3', 'zh': '4月3日'}, 'hash': 'ca80ca19171ef86b', 'authors': ['Hao Yin', 'Shi Guo', 'Xu Jia', 'Xudong XU', 'Lu Zhang', 'Si Liu', 'Dong Wang', 'Huchuan Lu', 'Tianfan Xue'], 'affiliations': ['Beihang University', 'Dalian University of Technology', 'Shanghai AI Laboratory', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2504.02402.jpg', 'data': {'categories': ['#training', '#dataset', '#data', '#cv'], 'emoji': '🔊', 'ru': {'title': 'Новый подход к бесконтактному восстановлению звука с помощью событийных камер', 'desc': 'Статья представляет новый метод бесконтактного восстановления звука с использованием событийных камер. Авторы разработали конвейер, который полностью использует пространственно-временную информацию из потока событий камеры. Они создали большой набор данных для обучения с помощью нового метода симуляции и спроектировали нейронную сеть, использующую разреженность событий и архитектуру Mamba для моделирования долгосрочной временной информации. Экспериментальные результаты на синтетических и реальных данных демонстрируют эффективность предложенного метода.'}, 'en': {'title': 'Revolutionizing Sound Recovery with Event Cameras', 'desc': 'This paper presents a new method for recovering sound from visual changes caused by sound-induced vibrations. It addresses limitations in previous techniques by utilizing event camera technology, which excels at capturing high-frequency signals. The authors developed a training pipeline to create a large dataset and designed a neural network that effectively captures both spatial and temporal information from the event data. Their approach includes a specialized imaging system to enhance signal detection, leading to improved sound recovery performance in experiments.'}, 'zh': {'title': '利用事件流实现高效声音恢复', 'desc': '本研究提出了一种新颖的非接触声音恢复方法，充分利用事件流中的时空信息。我们首先通过新的仿真管道生成了一个大型训练集，然后设计了一个网络，利用事件的稀疏性捕捉空间信息，并使用Mamba模型来处理长期的时间信息。最后，我们训练了一个空间聚合模块，以聚合来自不同位置的信息，从而进一步提高信号质量。实验结果表明，我们的方法在合成和真实数据上都表现出良好的效果。'}}}, {'id': 'https://huggingface.co/papers/2504.03600', 'title': 'MedSAM2: Segment Anything in 3D Medical Images and Videos', 'url': 'https://huggingface.co/papers/2504.03600', 'abstract': 'Medical image and video segmentation is a critical task for precision medicine, which has witnessed considerable progress in developing task or modality-specific and generalist models for 2D images. However, there have been limited studies on building general-purpose models for 3D images and videos with comprehensive user studies. Here, we present MedSAM2, a promptable segmentation foundation model for 3D image and video segmentation. The model is developed by fine-tuning the Segment Anything Model 2 on a large medical dataset with over 455,000 3D image-mask pairs and 76,000 frames, outperforming previous models across a wide range of organs, lesions, and imaging modalities. Furthermore, we implement a human-in-the-loop pipeline to facilitate the creation of large-scale datasets resulting in, to the best of our knowledge, the most extensive user study to date, involving the annotation of 5,000 CT lesions, 3,984 liver MRI lesions, and 251,550 echocardiogram video frames, demonstrating that MedSAM2 can reduce manual costs by more than 85%. MedSAM2 is also integrated into widely used platforms with user-friendly interfaces for local and cloud deployment, making it a practical tool for supporting efficient, scalable, and high-quality segmentation in both research and healthcare environments.', 'score': 3, 'issue_id': 3103, 'pub_date': '2025-04-04', 'pub_date_card': {'ru': '4 апреля', 'en': 'April 4', 'zh': '4月4日'}, 'hash': 'c1ef5354c6e2cdcb', 'authors': ['Jun Ma', 'Zongxin Yang', 'Sumin Kim', 'Bihui Chen', 'Mohammed Baharoon', 'Adibvafa Fallahpour', 'Reza Asakereh', 'Hongwei Lyu', 'Bo Wang'], 'affiliations': ['AI Collaborative Centre, University Health Network', 'AI Hub, University Health Network', 'Department of Biomedical Informatics, Harvard Medical School, Harvard University, Boston, USA', 'Department of Computer Science, University of Toronto', 'Department of Laboratory Medicine and Pathobiology and Department of Computer Science, University of Toronto', 'Peter Munk Cardiac Centre, University Health Network', 'University of Toronto, Toronto, Canada', 'Vector Institute, Toronto, Canada'], 'pdf_title_img': 'assets/pdf/title_img/2504.03600.jpg', 'data': {'categories': ['#data', '#healthcare', '#3d', '#dataset', '#training', '#cv'], 'emoji': '\U0001fa7b', 'ru': {'title': 'MedSAM2: Революция в сегментации медицинских 3D-изображений и видео', 'desc': 'MedSAM2 - это модель сегментации медицинских 3D-изображений и видео, основанная на fine-tuning Segment Anything Model 2. Модель обучена на большом наборе данных, включающем более 455 000 пар 3D-изображений и масок, а также 76 000 кадров, и превосходит предыдущие модели по широкому спектру органов, поражений и модальностей визуализации. Исследователи реализовали подход human-in-the-loop для создания масштабных датасетов, что позволило сократить затраты на ручную разметку более чем на 85%. MedSAM2 интегрирована в популярные платформы с удобным интерфейсом для локального и облачного развертывания, что делает ее практичным инструментом для эффективной и качественной сегментации в исследованиях и здравоохранении.'}, 'en': {'title': 'MedSAM2: Revolutionizing 3D Medical Segmentation', 'desc': 'This paper introduces MedSAM2, a new model designed for segmenting 3D medical images and videos, which is essential for precision medicine. It builds on the Segment Anything Model 2 and has been fine-tuned using a vast dataset of over 455,000 3D image-mask pairs and 76,000 video frames. MedSAM2 outperforms existing models in segmenting various organs and lesions, while also significantly reducing manual annotation costs by over 85% through a human-in-the-loop approach. Additionally, it is user-friendly and can be deployed on both local and cloud platforms, making it accessible for research and healthcare applications.'}, 'zh': {'title': 'MedSAM2：高效的3D医学图像分割工具', 'desc': 'MedSAM2是一种用于3D医学图像和视频分割的可提示分割基础模型。该模型通过在一个包含超过455,000个3D图像-掩膜对和76,000帧的大型医学数据集上微调Segment Anything Model 2而开发。MedSAM2在多个器官、病变和成像模式上超越了之前的模型，并通过人机协作的流程创建了大规模数据集，进行了一项广泛的用户研究。该模型能够将人工成本降低超过85%，并且已集成到广泛使用的平台中，便于本地和云端部署。'}}}, {'id': 'https://huggingface.co/papers/2503.24310', 'title': 'BEATS: Bias Evaluation and Assessment Test Suite for Large Language\n  Models', 'url': 'https://huggingface.co/papers/2503.24310', 'abstract': 'In this research, we introduce BEATS, a novel framework for evaluating Bias, Ethics, Fairness, and Factuality in Large Language Models (LLMs). Building upon the BEATS framework, we present a bias benchmark for LLMs that measure performance across 29 distinct metrics. These metrics span a broad range of characteristics, including demographic, cognitive, and social biases, as well as measures of ethical reasoning, group fairness, and factuality related misinformation risk. These metrics enable a quantitative assessment of the extent to which LLM generated responses may perpetuate societal prejudices that reinforce or expand systemic inequities. To achieve a high score on this benchmark a LLM must show very equitable behavior in their responses, making it a rigorous standard for responsible AI evaluation. Empirical results based on data from our experiment show that, 37.65\\% of outputs generated by industry leading models contained some form of bias, highlighting a substantial risk of using these models in critical decision making systems. BEATS framework and benchmark offer a scalable and statistically rigorous methodology to benchmark LLMs, diagnose factors driving biases, and develop mitigation strategies. With the BEATS framework, our goal is to help the development of more socially responsible and ethically aligned AI models.', 'score': 2, 'issue_id': 3097, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': '890bba46601fef07', 'authors': ['Alok Abhishek', 'Lisa Erickson', 'Tushar Bandopadhyay'], 'affiliations': ['Boston, USA', 'San Francisco, USA'], 'pdf_title_img': 'assets/pdf/title_img/2503.24310.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#ethics'], 'emoji': '⚖️', 'ru': {'title': 'BEATS: комплексная оценка этичности языковых моделей', 'desc': 'Исследователи представили BEATS - новую систему оценки предвзятости, этики, справедливости и фактической точности в больших языковых моделях (LLM). На основе BEATS разработан эталонный тест, измеряющий производительность LLM по 29 различным метрикам, включая демографические, когнитивные и социальные предубеждения. Результаты показали, что 37,65% выходных данных ведущих моделей содержали некоторую форму предвзятости. BEATS предлагает масштабируемую методологию для оценки LLM, диагностики факторов, вызывающих предвзятость, и разработки стратегий по ее снижению.'}, 'en': {'title': 'BEATS: A Framework for Fair and Ethical AI Evaluation', 'desc': 'This paper presents BEATS, a new framework designed to evaluate Bias, Ethics, Fairness, and Factuality in Large Language Models (LLMs). It introduces a comprehensive bias benchmark that assesses LLM performance using 29 different metrics, covering various biases and ethical considerations. The framework aims to quantitatively measure how LLM outputs may reinforce societal prejudices and systemic inequities. The findings reveal that a significant portion of outputs from leading models exhibit bias, underscoring the need for responsible AI practices and the potential for BEATS to guide improvements in AI ethics.'}, 'zh': {'title': 'BEATS框架：推动负责任的人工智能评估', 'desc': '本研究介绍了BEATS框架，用于评估大型语言模型（LLMs）中的偏见、伦理、公平性和事实性。我们建立了一个偏见基准，涵盖29个不同的指标，评估LLMs在多样性、认知和社会偏见等方面的表现。通过这些指标，可以定量评估LLM生成的响应在多大程度上可能延续社会偏见，强化或扩大系统性不平等。我们的目标是通过BEATS框架，促进更具社会责任感和伦理对齐的人工智能模型的发展。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (3)', '#agi (2)', '#alignment', '#architecture (1)', '#audio', '#benchmark (3)', '#cv (5)', '#data (4)', '#dataset (6)', '#diffusion (1)', '#ethics (1)', '#games', '#graphs', '#hallucinations', '#healthcare (1)', '#inference (1)', '#interpretability', '#leakage', '#long_context (1)', '#low_resource', '#machine_translation', '#math', '#multilingual (1)', '#multimodal (2)', '#open_source (4)', '#optimization (4)', '#plp', '#rag', '#reasoning (2)', '#rl (2)', '#rlhf (1)', '#robotics', '#science', '#security', '#small_models', '#story_generation', '#survey (1)', '#synthetic (3)', '#training (6)', '#transfer_learning (1)', '#video (1)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-04-07 12:21',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-04-07 12:21')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-04-07 12:21')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    