{
    "date": {
        "ru": "14 февраля",
        "en": "February 14",
        "zh": "2月14日"
    },
    "time_utc": "2025-02-14 07:10",
    "weekday": 4,
    "issue_id": 2213,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2502.08910",
            "title": "InfiniteHiP: Extending Language Model Context Up to 3 Million Tokens on a Single GPU",
            "url": "https://huggingface.co/papers/2502.08910",
            "abstract": "In modern large language models (LLMs), handling very long context lengths presents significant challenges as it causes slower inference speeds and increased memory costs. Additionally, most existing pre-trained LLMs fail to generalize beyond their original training sequence lengths. To enable efficient and practical long-context utilization, we introduce InfiniteHiP, a novel, and practical LLM inference framework that accelerates processing by dynamically eliminating irrelevant context tokens through a modular hierarchical token pruning algorithm. Our method also allows generalization to longer sequences by selectively applying various RoPE adjustment methods according to the internal attention patterns within LLMs. Furthermore, we offload the key-value cache to host memory during inference, significantly reducing GPU memory pressure. As a result, InfiniteHiP enables the processing of up to 3 million tokens on a single L40s 48GB GPU -- 3x larger -- without any permanent loss of context information. Our framework achieves an 18.95x speedup in attention decoding for a 1 million token context without requiring additional training. We implement our method in the SGLang framework and demonstrate its effectiveness and practicality through extensive evaluations.",
            "score": 36,
            "issue_id": 2210,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 февраля",
                "en": "February 13",
                "zh": "2月13日"
            },
            "hash": "eed812d17aeec57e",
            "authors": [
                "Heejun Lee",
                "Geon Park",
                "Jaduk Suh",
                "Sung Ju Hwang"
            ],
            "affiliations": [
                "DeepAuto.ai, Seoul, Korea",
                "Graduate School of AI, KAIST, Seoul, Korea"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.08910.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#optimization",
                    "#long_context",
                    "#inference"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Преодоление барьера длинного контекста в LLM",
                    "desc": "InfiniteHiP - это новая система для обработки больших языковых моделей (LLM) с длинным контекстом. Она ускоряет обработку, удаляя неважные токены контекста с помощью иерархического алгоритма. Система позволяет обобщать на более длинные последовательности, применяя методы настройки RoPE. InfiniteHiP достигает 18.95-кратного ускорения в декодировании внимания для контекста в 1 миллион токенов без дополнительного обучения."
                },
                "en": {
                    "title": "InfiniteHiP: Unlocking Long Contexts in Language Models Efficiently",
                    "desc": "This paper presents InfiniteHiP, a new framework designed to improve the efficiency of large language models (LLMs) when processing very long contexts. It addresses the issues of slow inference speeds and high memory costs by using a hierarchical token pruning algorithm to remove irrelevant tokens dynamically. Additionally, InfiniteHiP enhances the model's ability to generalize to longer sequences by adjusting the relative positional encodings based on attention patterns. The framework allows for processing up to 3 million tokens on a single GPU while achieving significant speed improvements without the need for extra training."
                },
                "zh": {
                    "title": "InfiniteHiP：高效处理超长上下文的LLM框架",
                    "desc": "在现代大型语言模型（LLMs）中，处理非常长的上下文长度面临显著挑战，导致推理速度变慢和内存成本增加。现有的大多数预训练LLMs无法超出其原始训练序列长度进行泛化。为了解决这一问题，我们提出了InfiniteHiP，这是一种新颖且实用的LLM推理框架，通过模块化的分层令牌修剪算法动态消除无关的上下文令牌，从而加速处理。我们的框架能够在不需要额外训练的情况下，实现对长达300万令牌的处理，并在1百万令牌上下文中实现18.95倍的注意力解码加速。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.08690",
            "title": "Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation",
            "url": "https://huggingface.co/papers/2502.08690",
            "abstract": "Large-scale text encoders in text-to-image (T2I) diffusion models have demonstrated exceptional performance in generating high-quality images from textual prompts. Unlike denoising modules that rely on multiple iterative steps, text encoders require only a single forward pass to produce text embeddings. However, despite their minimal contribution to total inference time and floating-point operations (FLOPs), text encoders demand significantly higher memory usage, up to eight times more than denoising modules. To address this inefficiency, we propose Skip and Re-use layers (Skrr), a simple yet effective pruning strategy specifically designed for text encoders in T2I diffusion models. Skrr exploits the inherent redundancy in transformer blocks by selectively skipping or reusing certain layers in a manner tailored for T2I tasks, thereby reducing memory consumption without compromising performance. Extensive experiments demonstrate that Skrr maintains image quality comparable to the original model even under high sparsity levels, outperforming existing blockwise pruning methods. Furthermore, Skrr achieves state-of-the-art memory efficiency while preserving performance across multiple evaluation metrics, including the FID, CLIP, DreamSim, and GenEval scores.",
            "score": 19,
            "issue_id": 2210,
            "pub_date": "2025-02-12",
            "pub_date_card": {
                "ru": "12 февраля",
                "en": "February 12",
                "zh": "2月12日"
            },
            "hash": "c7734d31994dcb35",
            "authors": [
                "Hoigi Seo",
                "Wongi Jeong",
                "Jae-sun Seo",
                "Se Young Chun"
            ],
            "affiliations": [
                "Dept. of Electrical and Computer Engineering, Seoul National University, Republic of Korea",
                "INMC & IPAI, Seoul National University, Republic of Korea",
                "School of Electrical and Computer Engineering, Cornell Tech, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.08690.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#optimization",
                    "#inference",
                    "#diffusion"
                ],
                "emoji": "✂️",
                "ru": {
                    "title": "Эффективное сжатие текстовых энкодеров для генерации изображений без потери качества",
                    "desc": "Статья представляет новый метод оптимизации текстовых энкодеров в моделях диффузии текст-в-изображение (T2I). Авторы предлагают стратегию прунинга под названием Skip and Re-use layers (Skrr), которая позволяет значительно сократить потребление памяти без ущерба для качества генерируемых изображений. Метод Skrr избирательно пропускает или повторно использует определенные слои в трансформерных блоках, учитывая специфику задач T2I. Эксперименты показывают, что Skrr превосходит существующие методы поблочного прунинга и достигает наилучших показателей эффективности использования памяти при сохранении производительности по различным метрикам оценки."
                },
                "en": {
                    "title": "Efficient Memory Use in Text-to-Image Models with Skrr",
                    "desc": "This paper introduces a new method called Skip and Re-use layers (Skrr) to improve the efficiency of text encoders in text-to-image diffusion models. Text encoders are crucial for generating images from text but use a lot of memory compared to denoising modules. Skrr reduces memory usage by selectively skipping or reusing layers in the transformer architecture, which helps maintain performance while lowering resource demands. The results show that Skrr achieves high image quality and outperforms other pruning methods, making it a significant advancement in memory efficiency for T2I tasks."
                },
                "zh": {
                    "title": "提升文本编码器的内存效率",
                    "desc": "本文提出了一种名为Skip and Re-use layers（Skrr）的新策略，旨在提高文本到图像扩散模型中文本编码器的内存效率。尽管文本编码器在推理时间和浮点运算方面的贡献较小，但它们的内存需求却高达去噪模块的八倍。Skrr通过选择性跳过或重用某些变换器层，利用了变换器块中的冗余性，从而在不影响性能的情况下减少内存消耗。实验结果表明，Skrr在高稀疏度下仍能保持与原始模型相当的图像质量，并在多个评估指标上实现了最先进的内存效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09604",
            "title": "SelfCite: Self-Supervised Alignment for Context Attribution in Large Language Models",
            "url": "https://huggingface.co/papers/2502.09604",
            "abstract": "We introduce SelfCite, a novel self-supervised approach that aligns LLMs to generate high-quality, fine-grained, sentence-level citations for the statements in their generated responses. Instead of only relying on costly and labor-intensive annotations, SelfCite leverages a reward signal provided by the LLM itself through context ablation: If a citation is necessary, removing the cited text from the context should prevent the same response; if sufficient, retaining the cited text alone should preserve the same response. This reward can guide the inference-time best-of-N sampling strategy to improve citation quality significantly, as well as be used in preference optimization to directly fine-tune the models for generating better citations. The effectiveness of SelfCite is demonstrated by increasing citation F1 up to 5.3 points on the LongBench-Cite benchmark across five long-form question answering tasks.",
            "score": 15,
            "issue_id": 2209,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 февраля",
                "en": "February 13",
                "zh": "2月13日"
            },
            "hash": "7aa5ce3731848736",
            "authors": [
                "Yung-Sung Chuang",
                "Benjamin Cohen-Wang",
                "Shannon Zejiang Shen",
                "Zhaofeng Wu",
                "Hu Xu",
                "Xi Victoria Lin",
                "James Glass",
                "Shang-Wen Li",
                "Wen-tau Yih"
            ],
            "affiliations": [
                "Massachusetts Institute of Technology",
                "Meta FAIR"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09604.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#long_context",
                    "#training",
                    "#alignment",
                    "#rlhf",
                    "#benchmark"
                ],
                "emoji": "📚",
                "ru": {
                    "title": "SelfCite: Самообучение ИИ искусству цитирования",
                    "desc": "SelfCite - это новый подход к самообучению больших языковых моделей (LLM) для генерации качественных цитат на уровне предложений. Метод использует сигнал награды, предоставляемый самой моделью через абляцию контекста, что позволяет избежать дорогостоящей ручной разметки. SelfCite применяет стратегию выборки best-of-N во время вывода и оптимизацию предпочтений для точной настройки моделей. Эффективность подхода подтверждается увеличением F1-меры цитирования до 5.3 пунктов на бенчмарке LongBench-Cite."
                },
                "en": {
                    "title": "Enhancing Citation Quality with SelfCite",
                    "desc": "SelfCite is a self-supervised method designed to enhance the citation quality in responses generated by large language models (LLMs). It uses a unique reward signal derived from the LLM itself, which assesses the necessity of citations by analyzing the impact of removing or retaining cited text. This approach not only improves the citation generation process during inference but also allows for fine-tuning the models to produce better citations through preference optimization. The results show a significant increase in citation accuracy, as evidenced by a 5.3 point improvement in citation F1 scores on the LongBench-Cite benchmark."
                },
                "zh": {
                    "title": "自监督引用生成的创新方法",
                    "desc": "SelfCite是一种新颖的自监督方法，旨在使大型语言模型（LLM）生成高质量、细粒度的句子级引用。该方法通过上下文消融提供的奖励信号，减少对昂贵和劳动密集型注释的依赖。具体来说，如果引用是必要的，移除被引用文本应防止相同的响应；如果足够，保留被引用文本应保持相同的响应。SelfCite在LongBench-Cite基准测试中显示出有效性，使引用的F1分数提高了5.3个百分点。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09560",
            "title": "EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents",
            "url": "https://huggingface.co/papers/2502.09560",
            "abstract": "Leveraging Multi-modal Large Language Models (MLLMs) to create embodied agents offers a promising avenue for tackling real-world tasks. While language-centric embodied agents have garnered substantial attention, MLLM-based embodied agents remain underexplored due to the lack of comprehensive evaluation frameworks. To bridge this gap, we introduce EmbodiedBench, an extensive benchmark designed to evaluate vision-driven embodied agents. EmbodiedBench features: (1) a diverse set of 1,128 testing tasks across four environments, ranging from high-level semantic tasks (e.g., household) to low-level tasks involving atomic actions (e.g., navigation and manipulation); and (2) six meticulously curated subsets evaluating essential agent capabilities like commonsense reasoning, complex instruction understanding, spatial awareness, visual perception, and long-term planning. Through extensive experiments, we evaluated 13 leading proprietary and open-source MLLMs within EmbodiedBench. Our findings reveal that: MLLMs excel at high-level tasks but struggle with low-level manipulation, with the best model, GPT-4o, scoring only 28.9% on average. EmbodiedBench provides a multifaceted standardized evaluation platform that not only highlights existing challenges but also offers valuable insights to advance MLLM-based embodied agents. Our code is available at https://embodiedbench.github.io.",
            "score": 9,
            "issue_id": 2211,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 февраля",
                "en": "February 13",
                "zh": "2月13日"
            },
            "hash": "019b4d19788a85cc",
            "authors": [
                "Rui Yang",
                "Hanyang Chen",
                "Junyu Zhang",
                "Mark Zhao",
                "Cheng Qian",
                "Kangrui Wang",
                "Qineng Wang",
                "Teja Venkat Koripella",
                "Marziyeh Movahedi",
                "Manling Li",
                "Heng Ji",
                "Huan Zhang",
                "Tong Zhang"
            ],
            "affiliations": [
                "Northwestern University",
                "Toyota Technological Institute at Chicago",
                "University of Illinois Urbana-Champaign",
                "University of Toronto"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09560.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#open_source",
                    "#games",
                    "#reasoning",
                    "#agents"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "EmbodiedBench: новый стандарт оценки воплощенного ИИ",
                    "desc": "В статье представлен EmbodiedBench - комплексный инструмент для оценки мультимодальных языковых моделей (MLLM) в задачах воплощенного искусственного интеллекта. Бенчмарк включает 1128 тестовых заданий в четырех средах, охватывающих как высокоуровневые семантические задачи, так и низкоуровневые действия. Эксперименты с 13 ведущими MLLM показали, что модели успешно справляются с высокоуровневыми задачами, но испытывают трудности с низкоуровневыми манипуляциями. EmbodiedBench предоставляет стандартизированную платформу для оценки и развития воплощенных агентов на основе MLLM."
                },
                "en": {
                    "title": "Empowering Embodied Agents with Comprehensive Evaluation",
                    "desc": "This paper discusses the development of EmbodiedBench, a benchmark for evaluating multi-modal large language models (MLLMs) in the context of embodied agents. It highlights the gap in existing evaluation frameworks for MLLM-based agents, which are crucial for performing real-world tasks. The benchmark includes a wide range of tasks that assess various capabilities such as commonsense reasoning and spatial awareness. The results indicate that while MLLMs perform well on high-level tasks, they face significant challenges with low-level manipulation tasks."
                },
                "zh": {
                    "title": "多模态大型语言模型助力具身智能体的评估与发展",
                    "desc": "本论文探讨了多模态大型语言模型（MLLMs）在创建具身智能体方面的应用，旨在解决现实世界任务。我们提出了EmbodiedBench，这是一个全面的基准测试框架，用于评估视觉驱动的具身智能体。EmbodiedBench包含1288个测试任务，涵盖高层语义任务和低层原子动作任务，并评估智能体的常识推理、复杂指令理解、空间意识、视觉感知和长期规划等能力。实验结果表明，尽管MLLMs在高层任务中表现优异，但在低层操作任务上仍然存在挑战，最好的模型GPT-4o的平均得分仅为28.9%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09056",
            "title": "An Open Recipe: Adapting Language-Specific LLMs to a Reasoning Model in One Day via Model Merging",
            "url": "https://huggingface.co/papers/2502.09056",
            "abstract": "This paper investigates data selection and model merging methodologies aimed at incorporating advanced reasoning capabilities such as those of DeepSeek R1 into language-specific large language models (LLMs), with a particular focus on the Thai LLM. Our goal is to enhance the reasoning capabilities of language-specific LLMs while maintaining their target language abilities. DeepSeek R1 excels in reasoning but primarily benefits high-resource languages such as English and Chinese. However, low-resource languages remain underserved due to the dominance of English-centric training data and model optimizations, which limit performance in these languages. This limitation results in unreliable code-switching and diminished effectiveness on tasks in low-resource languages. Meanwhile, local and regional LLM initiatives have attempted to bridge this gap by developing language-specific LLMs that focus on improving local linguistic fidelity. We demonstrate that, with only publicly available datasets and a computational budget of $120, it is possible to enhance the reasoning capabilities of language-specific LLMs to match the level of DeepSeek R1, without compromising their performance on target language tasks.",
            "score": 9,
            "issue_id": 2209,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 февраля",
                "en": "February 13",
                "zh": "2月13日"
            },
            "hash": "36c3c29072ae279d",
            "authors": [
                "Kunat Pipatanakul",
                "Pittawat Taveekitworachai",
                "Potsawee Manakul",
                "Kasima Tharnpipitchai"
            ],
            "affiliations": [
                "SCB 10X R&D SCBX Group Bangkok, Thailand"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09056.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#dataset",
                    "#low_resource",
                    "#multilingual",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Усиление логики локальных языковых моделей",
                    "desc": "Данное исследование посвящено методам улучшения способностей к рассуждению у языково-специфичных больших языковых моделей (LLM), в частности для тайского языка. Авторы предлагают подходы к отбору данных и слиянию моделей, чтобы перенести продвинутые навыки рассуждения из модели DeepSeek R1 в локальные LLM. Цель состоит в том, чтобы усилить логические возможности языково-специфичных моделей, сохраняя при этом их способности в целевом языке. Исследователи показывают, что даже с ограниченным бюджетом и общедоступными данными можно значительно улучшить рассуждения локальных LLM до уровня DeepSeek R1."
                },
                "en": {
                    "title": "Empowering Thai LLMs with Enhanced Reasoning Capabilities",
                    "desc": "This paper explores methods for selecting data and merging models to improve reasoning abilities in language-specific large language models (LLMs), particularly for the Thai language. The authors aim to enhance these models' reasoning skills while ensuring they remain effective in their target languages. They highlight the challenges faced by low-resource languages, which often lack the extensive training data available for high-resource languages like English. The study demonstrates that it is feasible to boost the reasoning capabilities of these LLMs using only publicly available datasets and a modest budget, achieving results comparable to advanced models like DeepSeek R1."
                },
                "zh": {
                    "title": "提升低资源语言LLM的推理能力",
                    "desc": "本文研究了数据选择和模型合并的方法，旨在将先进的推理能力（如DeepSeek R1）融入特定语言的大型语言模型（LLMs），特别关注泰语LLM。我们的目标是增强特定语言LLMs的推理能力，同时保持其目标语言的能力。DeepSeek R1在推理方面表现出色，但主要受益于高资源语言，如英语和中文，而低资源语言则受到忽视。我们展示了仅使用公开数据集和120美元的计算预算，就可以提升特定语言LLMs的推理能力，使其达到DeepSeek R1的水平，而不影响其在目标语言任务上的表现。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09100",
            "title": "Logical Reasoning in Large Language Models: A Survey",
            "url": "https://huggingface.co/papers/2502.09100",
            "abstract": "With the emergence of advanced reasoning models like OpenAI o3 and DeepSeek-R1, large language models (LLMs) have demonstrated remarkable reasoning capabilities. However, their ability to perform rigorous logical reasoning remains an open question. This survey synthesizes recent advancements in logical reasoning within LLMs, a critical area of AI research. It outlines the scope of logical reasoning in LLMs, its theoretical foundations, and the benchmarks used to evaluate reasoning proficiency. We analyze existing capabilities across different reasoning paradigms - deductive, inductive, abductive, and analogical - and assess strategies to enhance reasoning performance, including data-centric tuning, reinforcement learning, decoding strategies, and neuro-symbolic approaches. The review concludes with future directions, emphasizing the need for further exploration to strengthen logical reasoning in AI systems.",
            "score": 6,
            "issue_id": 2209,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 февраля",
                "en": "February 13",
                "zh": "2月13日"
            },
            "hash": "72b32d40c559c7e4",
            "authors": [
                "Hanmeng Liu",
                "Zhizhang Fu",
                "Mengru Ding",
                "Ruoxi Ning",
                "Chaoli Zhang",
                "Xiaozhang Liu",
                "Yue Zhang"
            ],
            "affiliations": [
                "Hainan University",
                "Westlake University",
                "Zhejiang Normal University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09100.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#survey",
                    "#reasoning",
                    "#rl",
                    "#benchmark"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Логическое мышление в больших языковых моделях: прогресс и перспективы",
                    "desc": "Этот обзор посвящен последним достижениям в области логического рассуждения в больших языковых моделях (LLM). В нем рассматриваются теоретические основы и методы оценки способностей LLM к логическому мышлению. Авторы анализируют существующие возможности в различных парадигмах рассуждений, включая дедуктивное, индуктивное, абдуктивное и аналогическое. Также обсуждаются стратегии улучшения производительности рассуждений, такие как настройка данных, обучение с подкреплением и нейросимволические подходы."
                },
                "en": {
                    "title": "Enhancing Logical Reasoning in Large Language Models",
                    "desc": "This paper reviews the progress made in enhancing logical reasoning capabilities of large language models (LLMs) like OpenAI o3 and DeepSeek-R1. It discusses various reasoning paradigms such as deductive, inductive, abductive, and analogical reasoning, highlighting their theoretical foundations and evaluation benchmarks. The authors analyze strategies to improve reasoning performance, including data-centric tuning and neuro-symbolic approaches. The paper concludes by suggesting future research directions to further develop logical reasoning in AI systems."
                },
                "zh": {
                    "title": "提升AI系统逻辑推理能力的探索",
                    "desc": "随着OpenAI o3和DeepSeek-R1等先进推理模型的出现，大型语言模型（LLMs）展现了出色的推理能力。然而，它们在进行严格逻辑推理方面的能力仍然是一个未解之谜。本文综述了LLMs中逻辑推理的最新进展，探讨了逻辑推理的范围、理论基础以及评估推理能力的基准。我们分析了不同推理范式（如演绎、归纳、溯因和类比）的现有能力，并评估了增强推理性能的策略，包括数据中心调优、强化学习、解码策略和神经符号方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09042",
            "title": "Typhoon T1: An Open Thai Reasoning Model",
            "url": "https://huggingface.co/papers/2502.09042",
            "abstract": "This paper introduces Typhoon T1, an open effort to develop an open Thai reasoning model. A reasoning model is a relatively new type of generative model built on top of large language models (LLMs). A reasoning model generates a long chain of thought before arriving at a final answer, an approach found to improve performance on complex tasks. However, details on developing such a model are limited, especially for reasoning models that can generate traces in a low-resource language. Typhoon T1 presents an open effort that dives into the details of developing a reasoning model in a more cost-effective way by leveraging supervised fine-tuning using open datasets, instead of reinforcement learning. This paper shares the details about synthetic data generation and training, as well as our dataset and model weights. Additionally, we provide insights gained from developing a reasoning model that generalizes across domains and is capable of generating reasoning traces in a low-resource language, using Thai as an example. We hope this open effort provides a foundation for further research in this field.",
            "score": 5,
            "issue_id": 2213,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 февраля",
                "en": "February 13",
                "zh": "2月13日"
            },
            "hash": "5cb078b546437366",
            "authors": [
                "Pittawat Taveekitworachai",
                "Potsawee Manakul",
                "Kasima Tharnpipitchai",
                "Kunat Pipatanakul"
            ],
            "affiliations": [
                "SCB 10X R&D SCBX Group Bangkok, Thailand"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09042.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#multilingual",
                    "#dataset",
                    "#open_source",
                    "#low_resource",
                    "#data",
                    "#synthetic"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Открытая модель рассуждений для тайского языка",
                    "desc": "Статья представляет Typhoon T1 - открытую модель рассуждений на тайском языке. Модель рассуждений - это новый тип генеративной модели, построенной на основе больших языковых моделей (LLM). Она генерирует длинную цепочку мыслей перед выдачей окончательного ответа, что улучшает производительность на сложных задачах. Авторы делятся деталями разработки такой модели с использованием обучения с учителем на открытых датасетах вместо обучения с подкреплением."
                },
                "en": {
                    "title": "Typhoon T1: Advancing Thai Reasoning Models for Complex Tasks",
                    "desc": "This paper presents Typhoon T1, an initiative to create an open reasoning model specifically for the Thai language. Reasoning models are advanced generative models that produce a sequence of thoughts leading to a conclusion, enhancing performance on intricate tasks. The authors focus on developing this model using supervised fine-tuning with open datasets, rather than relying on reinforcement learning, making it more accessible and cost-effective. The paper details the synthetic data generation process, training methods, and shares the model weights, aiming to support future research in low-resource language reasoning models."
                },
                "zh": {
                    "title": "开放泰语推理模型的创新之路",
                    "desc": "本文介绍了Typhoon T1，这是一个开发开放泰语推理模型的项目。推理模型是一种新型的生成模型，基于大型语言模型（LLMs）构建，能够在得出最终答案之前生成长链思维，从而提高复杂任务的表现。Typhoon T1通过利用监督微调和开放数据集，以更具成本效益的方式深入探讨推理模型的开发，避免了强化学习的复杂性。我们希望这个开放项目为该领域的进一步研究奠定基础。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09601",
            "title": "CoT-Valve: Length-Compressible Chain-of-Thought Tuning",
            "url": "https://huggingface.co/papers/2502.09601",
            "abstract": "Chain-of-Thought significantly enhances a model's reasoning capability, but it also comes with a considerable increase in inference costs due to long chains. With the observation that the reasoning path can be easily compressed under easy tasks but struggle on hard tasks, we explore the feasibility of elastically controlling the length of reasoning paths with only one model, thereby reducing the inference overhead of reasoning models dynamically based on task difficulty. We introduce a new tuning and inference strategy named CoT-Valve, designed to allow models to generate reasoning chains of varying lengths. To achieve this, we propose to identify a direction in the parameter space that, when manipulated, can effectively control the length of generated CoT. Moreover, we show that this property is valuable for compressing the reasoning chain. We construct datasets with chains from long to short for the same questions and explore two enhanced strategies for CoT-Valve: (1) a precise length-compressible CoT tuning method, and (2) a progressive chain length compression approach. Our experiments show that CoT-Valve successfully enables controllability and compressibility of the chain and shows better performance than the prompt-based control. We applied this method to QwQ-32B-Preview, reducing reasoning chains on GSM8K from 741 to 225 tokens with a minor performance drop (95.07% to 94.92%) and on AIME from 6827 to 4629 tokens, with only one additional incorrect answer.",
            "score": 4,
            "issue_id": 2212,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 февраля",
                "en": "February 13",
                "zh": "2月13日"
            },
            "hash": "4b0518724dea8b37",
            "authors": [
                "Xinyin Ma",
                "Guangnian Wan",
                "Runpeng Yu",
                "Gongfan Fang",
                "Xinchao Wang"
            ],
            "affiliations": [
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09601.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#dataset",
                    "#optimization",
                    "#inference"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Эластичное управление длиной цепочки рассуждений в моделях машинного обучения",
                    "desc": "Статья представляет новую стратегию обучения и вывода под названием CoT-Valve, которая позволяет моделям генерировать цепочки рассуждений различной длины. Авторы предлагают идентифицировать направление в пространстве параметров, манипулируя которым, можно эффективно контролировать длину генерируемой цепочки рассуждений. Они разрабатывают методы точного обучения сжимаемых цепочек рассуждений и прогрессивного сжатия длины цепочки. Эксперименты показывают, что CoT-Valve успешно обеспечивает управляемость и сжимаемость цепочки, превосходя контроль на основе промптов."
                },
                "en": {
                    "title": "Dynamic Reasoning Control with CoT-Valve",
                    "desc": "This paper presents CoT-Valve, a new strategy for controlling the length of reasoning chains in machine learning models, particularly in Chain-of-Thought (CoT) reasoning. The authors observe that while longer reasoning paths improve performance, they also increase inference costs, especially on harder tasks. CoT-Valve allows a single model to dynamically adjust the length of its reasoning based on task difficulty, thus optimizing efficiency. The experiments demonstrate that this method not only compresses reasoning chains significantly but also maintains high performance compared to traditional prompt-based controls."
                },
                "zh": {
                    "title": "动态控制推理链长度的创新策略",
                    "desc": "本文提出了一种名为CoT-Valve的新策略，用于动态控制推理链的长度，以降低推理模型的计算开销。研究发现，推理路径在简单任务中容易压缩，但在困难任务中则较为困难，因此我们探索了如何在单一模型中实现这一目标。通过调整参数空间中的方向，CoT-Valve能够有效控制生成的推理链长度，并且在压缩推理链方面表现出色。实验结果表明，CoT-Valve在控制和压缩推理链方面优于基于提示的控制方法，且在性能上仅有轻微下降。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.06608",
            "title": "TripoSG: High-Fidelity 3D Shape Synthesis using Large-Scale Rectified Flow Models",
            "url": "https://huggingface.co/papers/2502.06608",
            "abstract": "Recent advancements in diffusion techniques have propelled image and video generation to unprece- dented levels of quality, significantly accelerating the deployment and application of generative AI. However, 3D shape generation technology has so far lagged behind, constrained by limitations in 3D data scale, complexity of 3D data process- ing, and insufficient exploration of advanced tech- niques in the 3D domain. Current approaches to 3D shape generation face substantial challenges in terms of output quality, generalization capa- bility, and alignment with input conditions. We present TripoSG, a new streamlined shape diffu- sion paradigm capable of generating high-fidelity 3D meshes with precise correspondence to input images. Specifically, we propose: 1) A large-scale rectified flow transformer for 3D shape generation, achieving state-of-the-art fidelity through training on extensive, high-quality data. 2) A hybrid supervised training strategy combining SDF, normal, and eikonal losses for 3D VAE, achieving high- quality 3D reconstruction performance. 3) A data processing pipeline to generate 2 million high- quality 3D samples, highlighting the crucial rules for data quality and quantity in training 3D gen- erative models. Through comprehensive experi- ments, we have validated the effectiveness of each component in our new framework. The seamless integration of these parts has enabled TripoSG to achieve state-of-the-art performance in 3D shape generation. The resulting 3D shapes exhibit en- hanced detail due to high-resolution capabilities and demonstrate exceptional fidelity to input im- ages. Moreover, TripoSG demonstrates improved versatility in generating 3D models from diverse image styles and contents, showcasing strong gen- eralization capabilities. To foster progress and innovation in the field of 3D generation, we will make our model publicly available.",
            "score": 4,
            "issue_id": 2210,
            "pub_date": "2025-02-10",
            "pub_date_card": {
                "ru": "10 февраля",
                "en": "February 10",
                "zh": "2月10日"
            },
            "hash": "0602cc4a46e4c69c",
            "authors": [
                "Yangguang Li",
                "Zi-Xin Zou",
                "Zexiang Liu",
                "Dehu Wang",
                "Yuan Liang",
                "Zhipeng Yu",
                "Xingchao Liu",
                "Yuan-Chen Guo",
                "Ding Liang",
                "Wanli Ouyang",
                "Yan-Pei Cao"
            ],
            "affiliations": [
                "Shanghai AI Laboratory",
                "The Chinese University of Hong Kong",
                "The University of Texas at Austin",
                "VAST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.06608.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#open_source",
                    "#training",
                    "#dataset",
                    "#diffusion"
                ],
                "emoji": "🧊",
                "ru": {
                    "title": "TripoSG: Революция в генерации трехмерных форм",
                    "desc": "TripoSG - это новая парадигма генерации трехмерных форм, способная создавать высококачественные 3D-модели с точным соответствием входным изображениям. Она использует масштабный трансформер с выпрямленным потоком для генерации 3D-форм, обученный на обширных высококачественных данных. TripoSG применяет гибридную стратегию обучения, сочетающую потери SDF, нормалей и эйконала для 3D VAE. Благодаря интеграции этих компонентов, TripoSG достигает передового уровня в генерации 3D-форм с улучшенной детализацией и верностью входным изображениям."
                },
                "en": {
                    "title": "TripoSG: Revolutionizing 3D Shape Generation with Diffusion Techniques",
                    "desc": "This paper introduces TripoSG, a new method for generating high-quality 3D shapes using diffusion techniques. It addresses the challenges of 3D shape generation by employing a large-scale rectified flow transformer and a hybrid supervised training strategy that enhances reconstruction performance. The model is trained on a vast dataset, producing 2 million high-quality 3D samples, which improves the fidelity and detail of the generated shapes. TripoSG not only achieves state-of-the-art results but also shows strong generalization across various input images, making it a significant advancement in 3D generative models."
                },
                "zh": {
                    "title": "TripoSG：高保真3D形状生成的新范式",
                    "desc": "本论文介绍了一种新的3D形状生成方法TripoSG，旨在提高3D网格的生成质量。我们提出了一种大规模的流动变换器，能够在大量高质量数据上进行训练，从而实现高保真度的3D形状生成。此外，我们采用了一种混合监督训练策略，结合了SDF、法线和Eikonal损失，以提高3D重建性能。通过全面的实验验证，我们的框架在3D形状生成方面达到了最先进的性能，展现了对输入图像的高保真度和多样化的生成能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09621",
            "title": "MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for Reasoning Quality, Robustness, and Efficiency",
            "url": "https://huggingface.co/papers/2502.09621",
            "abstract": "Answering questions with Chain-of-Thought (CoT) has significantly enhanced the reasoning capabilities of Large Language Models (LLMs), yet its impact on Large Multimodal Models (LMMs) still lacks a systematic assessment and in-depth investigation. In this paper, we introduce MME-CoT, a specialized benchmark evaluating the CoT reasoning performance of LMMs, spanning six domains: math, science, OCR, logic, space-time, and general scenes. As the first comprehensive study in this area, we propose a thorough evaluation suite incorporating three novel metrics that assess the reasoning quality, robustness, and efficiency at a fine-grained level. Leveraging curated high-quality data and a unique evaluation strategy, we conduct an in-depth analysis of state-of-the-art LMMs, uncovering several key insights: 1) Models with reflection mechanism demonstrate a superior CoT quality, with Kimi k1.5 outperforming GPT-4o and demonstrating the highest quality results; 2) CoT prompting often degrades LMM performance on perception-heavy tasks, suggesting a potentially harmful overthinking behavior; and 3) Although the CoT quality is high, LMMs with reflection exhibit significant inefficiency in both normal response and self-correction phases. We hope MME-CoT serves as a foundation for advancing multimodal reasoning in LMMs. Project Page: https://mmecot.github.io/",
            "score": 3,
            "issue_id": 2213,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 февраля",
                "en": "February 13",
                "zh": "2月13日"
            },
            "hash": "ac755f4f5584a58c",
            "authors": [
                "Dongzhi Jiang",
                "Renrui Zhang",
                "Ziyu Guo",
                "Yanwei Li",
                "Yu Qi",
                "Xinyan Chen",
                "Liuhui Wang",
                "Jianhan Jin",
                "Claire Guo",
                "Shen Yan",
                "Bo Zhang",
                "Chaoyou Fu",
                "Peng Gao",
                "Hongsheng Li"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2502.09621.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Оценка мультимодальных рассуждений: новый взгляд на Chain-of-Thought в LMM",
                    "desc": "Статья представляет MME-CoT - специализированный бенчмарк для оценки производительности цепочки рассуждений (Chain-of-Thought) в мультимодальных языковых моделях (LMM). Исследование охватывает шесть областей и предлагает три новых метрики для оценки качества, надежности и эффективности рассуждений. Анализ показывает, что модели с механизмом рефлексии демонстрируют превосходное качество CoT, при этом Kimi k1.5 превосходит GPT-4o. Однако выявлено, что CoT может ухудшать производительность LMM в задачах, требующих восприятия, а модели с рефлексией демонстрируют значительную неэффективность."
                },
                "en": {
                    "title": "Unlocking Multimodal Reasoning with MME-CoT",
                    "desc": "This paper presents MME-CoT, a benchmark designed to evaluate the Chain-of-Thought (CoT) reasoning capabilities of Large Multimodal Models (LMMs) across various domains. It introduces three new metrics to assess reasoning quality, robustness, and efficiency in a detailed manner. The study reveals that models utilizing a reflection mechanism, like Kimi k1.5, outperform others such as GPT-4o in CoT quality, but may struggle with perception-heavy tasks due to overthinking. Overall, MME-CoT aims to enhance the understanding and development of multimodal reasoning in LMMs."
                },
                "zh": {
                    "title": "MME-CoT：提升多模态模型推理能力的基准",
                    "desc": "本文介绍了MME-CoT，这是一个专门评估大型多模态模型（LMMs）在链式思维（CoT）推理性能的基准，涵盖数学、科学、光学字符识别、逻辑、时空和一般场景六个领域。我们提出了一套全面的评估工具，包含三种新颖的指标，细致评估推理质量、鲁棒性和效率。研究发现，具有反思机制的模型在CoT质量上表现优越，而在感知密集型任务中，CoT提示可能会降低LMM的性能。尽管CoT质量较高，但具有反思的LMM在正常响应和自我修正阶段表现出显著的低效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.08946",
            "title": "The Stochastic Parrot on LLM's Shoulder: A Summative Assessment of Physical Concept Understanding",
            "url": "https://huggingface.co/papers/2502.08946",
            "abstract": "In a systematic way, we investigate a widely asked question: Do LLMs really understand what they say?, which relates to the more familiar term Stochastic Parrot. To this end, we propose a summative assessment over a carefully designed physical concept understanding task, PhysiCo. Our task alleviates the memorization issue via the usage of grid-format inputs that abstractly describe physical phenomena. The grids represents varying levels of understanding, from the core phenomenon, application examples to analogies to other abstract patterns in the grid world. A comprehensive study on our task demonstrates: (1) state-of-the-art LLMs, including GPT-4o, o1 and Gemini 2.0 flash thinking, lag behind humans by ~40%; (2) the stochastic parrot phenomenon is present in LLMs, as they fail on our grid task but can describe and recognize the same concepts well in natural language; (3) our task challenges the LLMs due to intrinsic difficulties rather than the unfamiliar grid format, as in-context learning and fine-tuning on same formatted data added little to their performance.",
            "score": 3,
            "issue_id": 2209,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 февраля",
                "en": "February 13",
                "zh": "2月13日"
            },
            "hash": "daecc7f38306f7b8",
            "authors": [
                "Mo Yu",
                "Lemao Liu",
                "Junjie Wu",
                "Tsz Ting Chung",
                "Shunchi Zhang",
                "Jiangnan Li",
                "Dit-Yan Yeung",
                "Jie Zhou"
            ],
            "affiliations": [
                "HKUST",
                "JHU",
                "WeChat AI, Tencent"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.08946.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#dataset",
                    "#training",
                    "#reasoning",
                    "#interpretability"
                ],
                "emoji": "🦜",
                "ru": {
                    "title": "Языковые модели: понимание или имитация?",
                    "desc": "Исследователи изучают вопрос о действительном понимании языковыми моделями (LLM) того, что они говорят. Они разработали задачу PhysiCo для оценки понимания физических концепций, используя абстрактные сетки вместо естественного языка. Результаты показывают, что современные LLM, включая GPT-4 и Gemini 2.0, отстают от людей примерно на 40% в этой задаче. Исследование подтверждает феномен 'стохастического попугая', так как модели не справляются с сеточной задачей, но хорошо описывают те же концепции на естественном языке."
                },
                "en": {
                    "title": "Unveiling the Limits of LLM Understanding",
                    "desc": "This paper investigates whether large language models (LLMs) truly understand the content they generate, addressing the concept of the 'Stochastic Parrot'. The authors introduce a new assessment task called PhysiCo, which uses grid-format inputs to evaluate understanding of physical concepts. Their findings reveal that top-performing LLMs, like GPT-4o and Gemini 2.0, significantly underperform compared to humans, indicating a gap in comprehension. Additionally, the study shows that LLMs struggle with the task due to inherent challenges in understanding rather than just the format of the input data."
                },
                "zh": {
                    "title": "探究大型语言模型的理解能力",
                    "desc": "本研究系统性地探讨了一个常见问题：大型语言模型（LLMs）是否真正理解它们所说的内容。我们提出了一种名为PhysiCo的评估任务，旨在通过网格格式的输入来减轻记忆问题，这些输入抽象地描述了物理现象。研究表明，当前最先进的LLMs在理解能力上落后于人类约40%，并且在网格任务中表现不佳，显示出随机鹦鹉现象的存在。我们的任务挑战了LLMs，主要是由于内在的困难，而非网格格式的不熟悉。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.08468",
            "title": "mmE5: Improving Multimodal Multilingual Embeddings via High-quality Synthetic Data",
            "url": "https://huggingface.co/papers/2502.08468",
            "abstract": "Multimodal embedding models have gained significant attention for their ability to map data from different modalities, such as text and images, into a unified representation space. However, the limited labeled multimodal data often hinders embedding performance. Recent approaches have leveraged data synthesis to address this problem, yet the quality of synthetic data remains a critical bottleneck. In this work, we identify three criteria for high-quality synthetic multimodal data. First, broad scope ensures that the generated data covers diverse tasks and modalities, making it applicable to various downstream scenarios. Second, robust cross-modal alignment makes different modalities semantically consistent. Third, high fidelity ensures that the synthetic data maintains realistic details to enhance its reliability. Guided by these principles, we synthesize datasets that: (1) cover a wide range of tasks, modality combinations, and languages, (2) are generated via a deep thinking process within a single pass of a multimodal large language model, and (3) incorporate real-world images with accurate and relevant texts, ensuring fidelity through self-evaluation and refinement. Leveraging these high-quality synthetic and labeled datasets, we train a multimodal multilingual E5 model mmE5. Extensive experiments demonstrate that mmE5 achieves state-of-the-art performance on the MMEB Benchmark and superior multilingual performance on the XTD benchmark. Our codes, datasets and models are released in https://github.com/haon-chen/mmE5.",
            "score": 1,
            "issue_id": 2211,
            "pub_date": "2025-02-12",
            "pub_date_card": {
                "ru": "12 февраля",
                "en": "February 12",
                "zh": "2月12日"
            },
            "hash": "ac7814f15d1e9616",
            "authors": [
                "Haonan Chen",
                "Liang Wang",
                "Nan Yang",
                "Yutao Zhu",
                "Ziliang Zhao",
                "Furu Wei",
                "Zhicheng Dou"
            ],
            "affiliations": [
                "Gaoling School of Artificial Intelligence, Renmin University of China",
                "Microsoft Corporation"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.08468.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#open_source",
                    "#synthetic",
                    "#dataset",
                    "#training",
                    "#data",
                    "#multilingual"
                ],
                "emoji": "🌐",
                "ru": {
                    "title": "Синтетические данные открывают новые горизонты для мультимодальных эмбеддингов",
                    "desc": "В статье представлена новая модель мультимодальных эмбеддингов mmE5, обученная на синтетических данных высокого качества. Авторы определили три критерия для создания таких данных: широкий охват задач и модальностей, надежное кросс-модальное выравнивание и высокая достоверность. Используя эти принципы, они синтезировали наборы данных с помощью мультимодальной большой языковой модели. Эксперименты показали, что mmE5 достигает передовых результатов на бенчмарках MMEB и XTD."
                },
                "en": {
                    "title": "Enhancing Multimodal Learning with High-Quality Synthetic Data",
                    "desc": "This paper discusses the development of multimodal embedding models that integrate different types of data, like text and images, into a single representation. It highlights the challenge of limited labeled multimodal data and proposes a solution through high-quality synthetic data generation. The authors establish three key criteria for effective synthetic data: broad scope, robust cross-modal alignment, and high fidelity. By adhering to these principles, they create a multimodal multilingual E5 model, mmE5, which demonstrates exceptional performance on various benchmarks."
                },
                "zh": {
                    "title": "高质量合成数据助力多模态模型",
                    "desc": "多模态嵌入模型能够将文本和图像等不同模态的数据映射到统一的表示空间，但有限的标注多模态数据常常影响嵌入性能。本文提出了高质量合成多模态数据的三个标准：广泛的范围、稳健的跨模态对齐和高保真度。通过这些标准，我们合成了覆盖多种任务和模态组合的数据集，并利用多模态大语言模型进行生成。最终，我们训练的多模态多语言E5模型mmE5在MMEB基准测试中表现出色，并在XTD基准测试中展现了卓越的多语言性能。"
                }
            }
        }
    ],
    "link_prev": "2025-02-13.html",
    "link_next": "2025-02-17.html",
    "link_month": "2025-02.html",
    "short_date_prev": {
        "ru": "13.02",
        "en": "02/13",
        "zh": "2月13日"
    },
    "short_date_next": {
        "ru": "17.02",
        "en": "02/17",
        "zh": "2月17日"
    },
    "categories": {
        "#dataset": 6,
        "#data": 3,
        "#benchmark": 5,
        "#agents": 1,
        "#cv": 0,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 3,
        "#3d": 1,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 3,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 10,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 7,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 4,
        "#survey": 1,
        "#diffusion": 2,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 2,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 2
    },
    "zh": {
        "text": "最近的图像重光模型进展使得一致的光照效果成为可能。然而，视频重光仍然滞后，主要由于训练成本高昂和缺乏多样化、高质量的视频重光数据集。逐帧应用图像重光模型会导致光源和外观不一致，生成的视频会出现闪烁。我们提出了Light-A-Video，一种无需训练的视频重光方法。它通过一致光照注意力模块和渐进光融合策略来增强光照一致性，确保视频的时间连贯性。",
        "title": "Light-A-Video: Training-free Video Relighting via Progressive Light Fusion",
        "pinyin": "最近的图像重光模型进展使得一致的光照效果成为可能。然而，视频重光仍然滞后，主要由于训练成本高昂和缺乏多样化、高质量的视频重光数据集。逐帧应用图像重光模型会导致光源和外观不一致，生成的视频会出现闪烁。我们提出了Light-A-Video，一种无需训练的视频重光方法。它通过一致光照注意力模块和渐进光融合策略来增强光照一致性，确保视频的时间连贯性。\n\nZuìjìn de túxiàng zhòngguāng móxíng jìnzhǎn shǐdé yīzhì de guāngzhào xiàoguǒ chéngwéi kěnéng. Rán'ér, shìpín zhòngguāng réngrán zhìhòu, zhǔyào yóuyú xùnliàn chéngběn gāotáng hé quēfá duōyànghuà, gāo zhìliàng de shìpín zhòngguāng shùjùjí. Zhúzhèn yìngyòng túxiàng zhòngguāng móxíng huì dǎozhì guāngyuán hé wàixiàn bù yīzhì, shēngchéng de shìpín huì chūxiàn shǎnshuò. Wǒmen tíchūle Light-A-Video, yīzhǒng wúxū xùnliàn de shìpín zhòngguāng fāngfǎ. Tā tōngguò yīzhì guāngzhào zhùyìlì mókuài hé jiànjìn guāng rónghé cèlüè lái zēngqiáng guāngzhào yīzhìxìng, quèbǎo shìpín de shíjiān liánhéxìng.",
        "vocab": "[{'word': '重光', 'pinyin': 'chóng guāng', 'trans': 'relighting'},\n{'word': '进展', 'pinyin': 'jìn zhǎn', 'trans': 'progress'},\n{'word': '一致', 'pinyin': 'yī zhì', 'trans': 'consistent'},\n{'word': '光照', 'pinyin': 'guāng zhào', 'trans': 'illumination'},\n{'word': '滞后', 'pinyin': 'zhì hòu', 'trans': 'lag behind'},\n{'word': '主要', 'pinyin': 'zhǔ yào', 'trans': 'mainly'},\n{'word': '由于', 'pinyin': 'yóu yú', 'trans': 'due to'},\n{'word': '高昂', 'pinyin': 'gāo áng', 'trans': 'high'},\n{'word': '缺乏', 'pinyin': 'quē fá', 'trans': 'lack'},\n{'word': '多样化', 'pinyin': 'duō yàng huà', 'trans': 'diversified'},\n{'word': '高质量', 'pinyin': 'gāo zhì liàng', 'trans': 'high quality'},\n{'word': '数据集', 'pinyin': 'shù jù jí', 'trans': 'dataset'},\n{'word': '逐帧', 'pinyin': 'zhú zhēn', 'trans': 'frame-by-frame'},\n{'word': '导致', 'pinyin': 'dǎo zhì', 'trans': 'lead to'},\n{'word': '外观', 'pinyin': 'wài guǎn', 'trans': 'appearance'},\n{'word': '闪烁', 'pinyin': 'shǎn shuò', 'trans': 'flicker'},\n{'word': '提出', 'pinyin': 'tí chū', 'trans': 'propose'},\n{'word': '无需', 'pinyin': 'wú xū', 'trans': 'without needing'},\n{'word': '方法', 'pinyin': 'fāng fǎ', 'trans': 'method'},\n{'word': '模块', 'pinyin': 'mó kuài', 'trans': 'module'},\n{'word': '策略', 'pinyin': 'cè lüè', 'trans': 'strategy'},\n{'word': '增强', 'pinyin': 'zēng qiáng', 'trans': 'enhance'},\n{'word': '确保', 'pinyin': 'què bǎo', 'trans': 'ensure'},\n{'word': '连贯性', 'pinyin': 'lián guàn xìng', 'trans': 'coherence'}]",
        "trans": "Recent advancements in image relighting models have made consistent lighting effects possible. However, video relighting still lags behind, primarily due to the high training costs and the lack of diverse, high-quality video relighting datasets. Applying image relighting models frame-by-frame can result in inconsistent light sources and appearances, causing flickering in the generated videos. We propose Light-A-Video, a training-free video relighting method. It enhances lighting consistency through a consistent lighting attention module and a progressive light blending strategy, ensuring temporal coherence in the video.",
        "update_ts": "2025-02-13 09:11"
    }
}