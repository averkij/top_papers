{
    "date": {
        "ru": "14 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
        "en": "February 14",
        "zh": "2æœˆ14æ—¥"
    },
    "time_utc": "2025-02-14 10:10",
    "weekday": 4,
    "issue_id": 2216,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2502.08910",
            "title": "InfiniteHiP: Extending Language Model Context Up to 3 Million Tokens on a Single GPU",
            "url": "https://huggingface.co/papers/2502.08910",
            "abstract": "In modern large language models (LLMs), handling very long context lengths presents significant challenges as it causes slower inference speeds and increased memory costs. Additionally, most existing pre-trained LLMs fail to generalize beyond their original training sequence lengths. To enable efficient and practical long-context utilization, we introduce InfiniteHiP, a novel, and practical LLM inference framework that accelerates processing by dynamically eliminating irrelevant context tokens through a modular hierarchical token pruning algorithm. Our method also allows generalization to longer sequences by selectively applying various RoPE adjustment methods according to the internal attention patterns within LLMs. Furthermore, we offload the key-value cache to host memory during inference, significantly reducing GPU memory pressure. As a result, InfiniteHiP enables the processing of up to 3 million tokens on a single L40s 48GB GPU -- 3x larger -- without any permanent loss of context information. Our framework achieves an 18.95x speedup in attention decoding for a 1 million token context without requiring additional training. We implement our method in the SGLang framework and demonstrate its effectiveness and practicality through extensive evaluations.",
            "score": 43,
            "issue_id": 2210,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 13",
                "zh": "2æœˆ13æ—¥"
            },
            "hash": "eed812d17aeec57e",
            "authors": [
                "Heejun Lee",
                "Geon Park",
                "Jaduk Suh",
                "Sung Ju Hwang"
            ],
            "affiliations": [
                "DeepAuto.ai, Seoul, Korea",
                "Graduate School of AI, KAIST, Seoul, Korea"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.08910.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#optimization",
                    "#long_context",
                    "#inference"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ±Ğ°Ñ€ÑŒĞµÑ€Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² LLM",
                    "desc": "InfiniteHiP - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. ĞĞ½Ğ° ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ, ÑƒĞ´Ğ°Ğ»ÑÑ Ğ½ĞµĞ²Ğ°Ğ¶Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°Ñ‚ÑŒ Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ RoPE. InfiniteHiP Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 18.95-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ² Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² 1 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "InfiniteHiP: Unlocking Long Contexts in Language Models Efficiently",
                    "desc": "This paper presents InfiniteHiP, a new framework designed to improve the efficiency of large language models (LLMs) when processing very long contexts. It addresses the issues of slow inference speeds and high memory costs by using a hierarchical token pruning algorithm to remove irrelevant tokens dynamically. Additionally, InfiniteHiP enhances the model's ability to generalize to longer sequences by adjusting the relative positional encodings based on attention patterns. The framework allows for processing up to 3 million tokens on a single GPU while achieving significant speed improvements without the need for extra training."
                },
                "zh": {
                    "title": "InfiniteHiPï¼šé«˜æ•ˆå¤„ç†è¶…é•¿ä¸Šä¸‹æ–‡çš„LLMæ¡†æ¶",
                    "desc": "åœ¨ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­ï¼Œå¤„ç†éå¸¸é•¿çš„ä¸Šä¸‹æ–‡é•¿åº¦é¢ä¸´æ˜¾è‘—æŒ‘æˆ˜ï¼Œå¯¼è‡´æ¨ç†é€Ÿåº¦å˜æ…¢å’Œå†…å­˜æˆæœ¬å¢åŠ ã€‚ç°æœ‰çš„å¤§å¤šæ•°é¢„è®­ç»ƒLLMsæ— æ³•è¶…å‡ºå…¶åŸå§‹è®­ç»ƒåºåˆ—é•¿åº¦è¿›è¡Œæ³›åŒ–ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†InfiniteHiPï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–ä¸”å®ç”¨çš„LLMæ¨ç†æ¡†æ¶ï¼Œé€šè¿‡æ¨¡å—åŒ–çš„åˆ†å±‚ä»¤ç‰Œä¿®å‰ªç®—æ³•åŠ¨æ€æ¶ˆé™¤æ— å…³çš„ä¸Šä¸‹æ–‡ä»¤ç‰Œï¼Œä»è€ŒåŠ é€Ÿå¤„ç†ã€‚æˆ‘ä»¬çš„æ¡†æ¶èƒ½å¤Ÿåœ¨ä¸éœ€è¦é¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œå®ç°å¯¹é•¿è¾¾300ä¸‡ä»¤ç‰Œçš„å¤„ç†ï¼Œå¹¶åœ¨1ç™¾ä¸‡ä»¤ç‰Œä¸Šä¸‹æ–‡ä¸­å®ç°18.95å€çš„æ³¨æ„åŠ›è§£ç åŠ é€Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.08690",
            "title": "Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation",
            "url": "https://huggingface.co/papers/2502.08690",
            "abstract": "Large-scale text encoders in text-to-image (T2I) diffusion models have demonstrated exceptional performance in generating high-quality images from textual prompts. Unlike denoising modules that rely on multiple iterative steps, text encoders require only a single forward pass to produce text embeddings. However, despite their minimal contribution to total inference time and floating-point operations (FLOPs), text encoders demand significantly higher memory usage, up to eight times more than denoising modules. To address this inefficiency, we propose Skip and Re-use layers (Skrr), a simple yet effective pruning strategy specifically designed for text encoders in T2I diffusion models. Skrr exploits the inherent redundancy in transformer blocks by selectively skipping or reusing certain layers in a manner tailored for T2I tasks, thereby reducing memory consumption without compromising performance. Extensive experiments demonstrate that Skrr maintains image quality comparable to the original model even under high sparsity levels, outperforming existing blockwise pruning methods. Furthermore, Skrr achieves state-of-the-art memory efficiency while preserving performance across multiple evaluation metrics, including the FID, CLIP, DreamSim, and GenEval scores.",
            "score": 24,
            "issue_id": 2210,
            "pub_date": "2025-02-12",
            "pub_date_card": {
                "ru": "12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 12",
                "zh": "2æœˆ12æ—¥"
            },
            "hash": "c7734d31994dcb35",
            "authors": [
                "Hoigi Seo",
                "Wongi Jeong",
                "Jae-sun Seo",
                "Se Young Chun"
            ],
            "affiliations": [
                "Dept. of Electrical and Computer Engineering, Seoul National University, Republic of Korea",
                "INMC & IPAI, Seoul National University, Republic of Korea",
                "School of Electrical and Computer Engineering, Cornell Tech, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.08690.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#optimization",
                    "#inference",
                    "#diffusion"
                ],
                "emoji": "âœ‚ï¸",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚-Ğ²-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ (T2I). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿Ñ€ÑƒĞ½Ğ¸Ğ½Ğ³Ğ° Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Skip and Re-use layers (Skrr), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ±ĞµĞ· ÑƒÑ‰ĞµÑ€Ğ±Ğ° Ğ´Ğ»Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Skrr Ğ¸Ğ·Ğ±Ğ¸Ñ€Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµÑ‚ Ğ¸Ğ»Ğ¸ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ»Ğ¾Ğ¸ Ğ² Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ±Ğ»Ğ¾ĞºĞ°Ñ…, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸ĞºÑƒ Ğ·Ğ°Ğ´Ğ°Ñ‡ T2I. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Skrr Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾Ğ±Ğ»Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ÑƒĞ½Ğ¸Ğ½Ğ³Ğ° Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ¸."
                },
                "en": {
                    "title": "Efficient Memory Use in Text-to-Image Models with Skrr",
                    "desc": "This paper introduces a new method called Skip and Re-use layers (Skrr) to improve the efficiency of text encoders in text-to-image diffusion models. Text encoders are crucial for generating images from text but use a lot of memory compared to denoising modules. Skrr reduces memory usage by selectively skipping or reusing layers in the transformer architecture, which helps maintain performance while lowering resource demands. The results show that Skrr achieves high image quality and outperforms other pruning methods, making it a significant advancement in memory efficiency for T2I tasks."
                },
                "zh": {
                    "title": "æå‡æ–‡æœ¬ç¼–ç å™¨çš„å†…å­˜æ•ˆç‡",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºSkip and Re-use layersï¼ˆSkrrï¼‰çš„æ–°ç­–ç•¥ï¼Œæ—¨åœ¨æé«˜æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­æ–‡æœ¬ç¼–ç å™¨çš„å†…å­˜æ•ˆç‡ã€‚å°½ç®¡æ–‡æœ¬ç¼–ç å™¨åœ¨æ¨ç†æ—¶é—´å’Œæµ®ç‚¹è¿ç®—æ–¹é¢çš„è´¡çŒ®è¾ƒå°ï¼Œä½†å®ƒä»¬çš„å†…å­˜éœ€æ±‚å´é«˜è¾¾å»å™ªæ¨¡å—çš„å…«å€ã€‚Skrré€šè¿‡é€‰æ‹©æ€§è·³è¿‡æˆ–é‡ç”¨æŸäº›å˜æ¢å™¨å±‚ï¼Œåˆ©ç”¨äº†å˜æ¢å™¨å—ä¸­çš„å†—ä½™æ€§ï¼Œä»è€Œåœ¨ä¸å½±å“æ€§èƒ½çš„æƒ…å†µä¸‹å‡å°‘å†…å­˜æ¶ˆè€—ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSkrråœ¨é«˜ç¨€ç–åº¦ä¸‹ä»èƒ½ä¿æŒä¸åŸå§‹æ¨¡å‹ç›¸å½“çš„å›¾åƒè´¨é‡ï¼Œå¹¶åœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„å†…å­˜æ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09604",
            "title": "SelfCite: Self-Supervised Alignment for Context Attribution in Large Language Models",
            "url": "https://huggingface.co/papers/2502.09604",
            "abstract": "We introduce SelfCite, a novel self-supervised approach that aligns LLMs to generate high-quality, fine-grained, sentence-level citations for the statements in their generated responses. Instead of only relying on costly and labor-intensive annotations, SelfCite leverages a reward signal provided by the LLM itself through context ablation: If a citation is necessary, removing the cited text from the context should prevent the same response; if sufficient, retaining the cited text alone should preserve the same response. This reward can guide the inference-time best-of-N sampling strategy to improve citation quality significantly, as well as be used in preference optimization to directly fine-tune the models for generating better citations. The effectiveness of SelfCite is demonstrated by increasing citation F1 up to 5.3 points on the LongBench-Cite benchmark across five long-form question answering tasks.",
            "score": 19,
            "issue_id": 2209,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 13",
                "zh": "2æœˆ13æ—¥"
            },
            "hash": "7aa5ce3731848736",
            "authors": [
                "Yung-Sung Chuang",
                "Benjamin Cohen-Wang",
                "Shannon Zejiang Shen",
                "Zhaofeng Wu",
                "Hu Xu",
                "Xi Victoria Lin",
                "James Glass",
                "Shang-Wen Li",
                "Wen-tau Yih"
            ],
            "affiliations": [
                "Massachusetts Institute of Technology",
                "Meta FAIR"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09604.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#long_context",
                    "#training",
                    "#alignment",
                    "#rlhf",
                    "#benchmark"
                ],
                "emoji": "ğŸ“š",
                "ru": {
                    "title": "SelfCite: Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ñƒ Ñ†Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "SelfCite - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ†Ğ¸Ñ‚Ğ°Ñ‚ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸Ğ³Ğ½Ğ°Ğ» Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğ¹ ÑĞ°Ğ¼Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ±Ğ»ÑÑ†Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰ĞµĞ¹ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸. SelfCite Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ best-of-N Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ÑÑ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ F1-Ğ¼ĞµÑ€Ñ‹ Ñ†Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¾ 5.3 Ğ¿ÑƒĞ½ĞºÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ LongBench-Cite."
                },
                "en": {
                    "title": "Enhancing Citation Quality with SelfCite",
                    "desc": "SelfCite is a self-supervised method designed to enhance the citation quality in responses generated by large language models (LLMs). It uses a unique reward signal derived from the LLM itself, which assesses the necessity of citations by analyzing the impact of removing or retaining cited text. This approach not only improves the citation generation process during inference but also allows for fine-tuning the models to produce better citations through preference optimization. The results show a significant increase in citation accuracy, as evidenced by a 5.3 point improvement in citation F1 scores on the LongBench-Cite benchmark."
                },
                "zh": {
                    "title": "è‡ªç›‘ç£å¼•ç”¨ç”Ÿæˆçš„åˆ›æ–°æ–¹æ³•",
                    "desc": "SelfCiteæ˜¯ä¸€ç§æ–°é¢–çš„è‡ªç›‘ç£æ–¹æ³•ï¼Œæ—¨åœ¨ä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆé«˜è´¨é‡ã€ç»†ç²’åº¦çš„å¥å­çº§å¼•ç”¨ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸Šä¸‹æ–‡æ¶ˆèæä¾›çš„å¥–åŠ±ä¿¡å·ï¼Œå‡å°‘å¯¹æ˜‚è´µå’ŒåŠ³åŠ¨å¯†é›†å‹æ³¨é‡Šçš„ä¾èµ–ã€‚å…·ä½“æ¥è¯´ï¼Œå¦‚æœå¼•ç”¨æ˜¯å¿…è¦çš„ï¼Œç§»é™¤è¢«å¼•ç”¨æ–‡æœ¬åº”é˜²æ­¢ç›¸åŒçš„å“åº”ï¼›å¦‚æœè¶³å¤Ÿï¼Œä¿ç•™è¢«å¼•ç”¨æ–‡æœ¬åº”ä¿æŒç›¸åŒçš„å“åº”ã€‚SelfCiteåœ¨LongBench-CiteåŸºå‡†æµ‹è¯•ä¸­æ˜¾ç¤ºå‡ºæœ‰æ•ˆæ€§ï¼Œä½¿å¼•ç”¨çš„F1åˆ†æ•°æé«˜äº†5.3ä¸ªç™¾åˆ†ç‚¹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09620",
            "title": "Exploring the Potential of Encoder-free Architectures in 3D LMMs",
            "url": "https://huggingface.co/papers/2502.09620",
            "abstract": "Encoder-free architectures have been preliminarily explored in the 2D visual domain, yet it remains an open question whether they can be effectively applied to 3D understanding scenarios. In this paper, we present the first comprehensive investigation into the potential of encoder-free architectures to overcome the challenges of encoder-based 3D Large Multimodal Models (LMMs). These challenges include the failure to adapt to varying point cloud resolutions and the point features from the encoder not meeting the semantic needs of Large Language Models (LLMs). We identify key aspects for 3D LMMs to remove the encoder and enable the LLM to assume the role of the 3D encoder: 1) We propose the LLM-embedded Semantic Encoding strategy in the pre-training stage, exploring the effects of various point cloud self-supervised losses. And we present the Hybrid Semantic Loss to extract high-level semantics. 2) We introduce the Hierarchical Geometry Aggregation strategy in the instruction tuning stage. This incorporates inductive bias into the LLM early layers to focus on the local details of the point clouds. To the end, we present the first Encoder-free 3D LMM, ENEL. Our 7B model rivals the current state-of-the-art model, ShapeLLM-13B, achieving 55.0%, 50.92%, and 42.7% on the classification, captioning, and VQA tasks, respectively. Our results demonstrate that the encoder-free architecture is highly promising for replacing encoder-based architectures in the field of 3D understanding. The code is released at https://github.com/Ivan-Tang-3D/ENEL",
            "score": 17,
            "issue_id": 2214,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 13",
                "zh": "2æœˆ13æ—¥"
            },
            "hash": "2e519b64f13f6506",
            "authors": [
                "Yiwen Tang",
                "Zoey Guo",
                "Zhuhao Wang",
                "Ray Zhang",
                "Qizhi Chen",
                "Junli Liu",
                "Delin Qu",
                "Zhigang Wang",
                "Dong Wang",
                "Xuelong Li",
                "Bin Zhao"
            ],
            "affiliations": [
                "Northwestern Polytechnical University",
                "Shanghai AI Laboratory",
                "The Chinese University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09620.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#architecture",
                    "#optimization",
                    "#3d"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² 3D-Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸: Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ±ĞµĞ· ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° Ğ¿Ğ¾ĞºĞ¾Ñ€ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²ĞµÑ€ÑˆĞ¸Ğ½Ñ‹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€Ğ²Ğ¾Ğµ Ğ²ÑĞµÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½ĞµĞµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ğ±ĞµĞ· ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° Ğ´Ğ»Ñ 3D-Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ LLM-embedded Semantic Encoding Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Hierarchical Geometry Aggregation Ğ´Ğ»Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ENEL Ñ 7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ñ‹Ğµ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¸Ğ¼ĞµÑÑ‰Ğ¸Ğ¼Ğ¸ 13 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ğ±ĞµĞ· ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ 3D-Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Revolutionizing 3D Understanding with Encoder-Free Architectures",
                    "desc": "This paper explores the use of encoder-free architectures for 3D understanding, addressing limitations of traditional encoder-based models. It introduces the LLM-embedded Semantic Encoding strategy during pre-training to enhance point cloud representation and proposes a Hybrid Semantic Loss for better semantic extraction. Additionally, the Hierarchical Geometry Aggregation strategy is introduced in the instruction tuning phase to improve local detail focus in point clouds. The proposed model, ENEL, demonstrates competitive performance against existing models, indicating the potential of encoder-free approaches in 3D Large Multimodal Models."
                },
                "zh": {
                    "title": "æ— ç¼–ç å™¨æ¶æ„ï¼š3Dç†è§£çš„æ–°çªç ´",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†æ— ç¼–ç å™¨æ¶æ„åœ¨3Dç†è§£ä¸­çš„åº”ç”¨æ½œåŠ›ï¼Œé¦–æ¬¡å¯¹å…¶è¿›è¡Œå…¨é¢ç ”ç©¶ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ç­–ç•¥ï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿæ›¿ä»£ä¼ ç»Ÿçš„3Dç¼–ç å™¨ï¼Œè§£å†³äº†ç‚¹äº‘åˆ†è¾¨ç‡å˜åŒ–å’Œç‰¹å¾è¯­ä¹‰éœ€æ±‚ä¸åŒ¹é…çš„é—®é¢˜ã€‚é€šè¿‡å¼•å…¥åµŒå…¥å¼è¯­ä¹‰ç¼–ç å’Œåˆ†å±‚å‡ ä½•èšåˆç­–ç•¥ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨åˆ†ç±»ã€æè¿°å’Œè§†è§‰é—®ç­”ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬çš„7Bæ¨¡å‹ENELåœ¨æ€§èƒ½ä¸Šä¸å½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹ShapeLLM-13Bç›¸åª²ç¾ï¼Œæ˜¾ç¤ºå‡ºæ— ç¼–ç å™¨æ¶æ„åœ¨3Dç†è§£é¢†åŸŸçš„å·¨å¤§æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09082",
            "title": "CoSER: Coordinating LLM-Based Persona Simulation of Established Roles",
            "url": "https://huggingface.co/papers/2502.09082",
            "abstract": "Role-playing language agents (RPLAs) have emerged as promising applications of large language models (LLMs). However, simulating established characters presents a challenging task for RPLAs, due to the lack of authentic character datasets and nuanced evaluation methods using such data. In this paper, we present CoSER, a collection of a high-quality dataset, open models, and an evaluation protocol towards effective RPLAs of established characters. The CoSER dataset covers 17,966 characters from 771 renowned books. It provides authentic dialogues with real-world intricacies, as well as diverse data types such as conversation setups, character experiences and internal thoughts. Drawing from acting methodology, we introduce given-circumstance acting for training and evaluating role-playing LLMs, where LLMs sequentially portray multiple characters in book scenes. Using our dataset, we develop CoSER 8B and CoSER 70B, i.e., advanced open role-playing LLMs built on LLaMA-3.1 models. Extensive experiments demonstrate the value of the CoSER dataset for RPLA training, evaluation and retrieval. Moreover, CoSER 70B exhibits state-of-the-art performance surpassing or matching GPT-4o on our evaluation and three existing benchmarks, i.e., achieving 75.80% and 93.47% accuracy on the InCharacter and LifeChoice benchmarks respectively.",
            "score": 15,
            "issue_id": 2214,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 13",
                "zh": "2æœˆ13æ—¥"
            },
            "hash": "3d13dc492344cf84",
            "authors": [
                "Xintao Wang",
                "Heng Wang",
                "Yifei Zhang",
                "Xinfeng Yuan",
                "Rui Xu",
                "Jen-tse Huang",
                "Siyu Yuan",
                "Haoran Guo",
                "Jiangjie Chen",
                "Wei Wang",
                "Yanghua Xiao",
                "Shuchang Zhou"
            ],
            "affiliations": [
                "Fudan University",
                "Johns Hopkins University",
                "StepFun"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09082.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#story_generation",
                    "#benchmark",
                    "#agents",
                    "#open_source"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "CoSER: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ñ€Ğ¾Ğ»ĞµĞ²Ğ¾Ğ¼ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ CoSER - Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ¾Ğ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… CoSER ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 17 966 Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ¸Ğ· 771 Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ğ¾Ğ¹ ĞºĞ½Ğ¸Ğ³Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ°ÑƒÑ‚ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ¸Ğµ 'Ğ°ĞºÑ‚ĞµÑ€ÑĞºĞ¾Ğ¹ Ğ¸Ğ³Ñ€Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°Ñ…' Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ¾Ğ»ĞµĞ²Ñ‹Ñ… LLM. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ CoSER 8B Ğ¸ CoSER 70B Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¿Ñ€Ğ¸Ñ‡ĞµĞ¼ CoSER 70B Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¸Ğ»Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ GPT-4 Ğ¿Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ÑĞ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ¸."
                },
                "en": {
                    "title": "Empowering Role-Playing Agents with CoSER: A New Dataset and Methodology",
                    "desc": "This paper introduces CoSER, a comprehensive dataset designed to enhance role-playing language agents (RPLAs) using large language models (LLMs). It includes 17,966 characters from 771 well-known books, providing authentic dialogues and various data types that reflect real-world interactions. The authors propose a novel training and evaluation method based on acting techniques, allowing LLMs to portray multiple characters in specific scenes. The results show that the CoSER 70B model outperforms existing models like GPT-4o in several benchmarks, demonstrating the effectiveness of the CoSER dataset for training and evaluating RPLAs."
                },
                "zh": {
                    "title": "æå‡è§’è‰²æ‰®æ¼”è¯­è¨€ä»£ç†çš„æœ‰æ•ˆæ€§",
                    "desc": "è§’è‰²æ‰®æ¼”è¯­è¨€ä»£ç†ï¼ˆRPLAï¼‰æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–°å…´åº”ç”¨ï¼Œä½†æ¨¡æ‹Ÿå·²å»ºç«‹è§’è‰²çš„ä»»åŠ¡å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºç¼ºä¹çœŸå®çš„è§’è‰²æ•°æ®é›†å’Œç»†è‡´çš„è¯„ä¼°æ–¹æ³•ã€‚æœ¬æ–‡ä»‹ç»äº†CoSERï¼Œä¸€ä¸ªé«˜è´¨é‡çš„æ•°æ®é›†ã€å¼€æ”¾æ¨¡å‹å’Œè¯„ä¼°åè®®ï¼Œæ—¨åœ¨æœ‰æ•ˆæ”¯æŒå·²å»ºç«‹è§’è‰²çš„RPLAã€‚CoSERæ•°æ®é›†æ¶µç›–äº†771æœ¬è‘—åä¹¦ç±ä¸­çš„17,966ä¸ªè§’è‰²ï¼Œæä¾›äº†çœŸå®å¯¹è¯å’Œå¤šæ ·åŒ–çš„æ•°æ®ç±»å‹ã€‚é€šè¿‡å¼•å…¥ç»™å®šæƒ…å¢ƒè¡¨æ¼”çš„æ–¹æ³•ï¼Œæˆ‘ä»¬è®­ç»ƒå’Œè¯„ä¼°è§’è‰²æ‰®æ¼”çš„LLMï¼Œå®éªŒç»“æœè¡¨æ˜CoSERæ•°æ®é›†åœ¨RPLAè®­ç»ƒã€è¯„ä¼°å’Œæ£€ç´¢ä¸­çš„é‡è¦ä»·å€¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09056",
            "title": "An Open Recipe: Adapting Language-Specific LLMs to a Reasoning Model in One Day via Model Merging",
            "url": "https://huggingface.co/papers/2502.09056",
            "abstract": "This paper investigates data selection and model merging methodologies aimed at incorporating advanced reasoning capabilities such as those of DeepSeek R1 into language-specific large language models (LLMs), with a particular focus on the Thai LLM. Our goal is to enhance the reasoning capabilities of language-specific LLMs while maintaining their target language abilities. DeepSeek R1 excels in reasoning but primarily benefits high-resource languages such as English and Chinese. However, low-resource languages remain underserved due to the dominance of English-centric training data and model optimizations, which limit performance in these languages. This limitation results in unreliable code-switching and diminished effectiveness on tasks in low-resource languages. Meanwhile, local and regional LLM initiatives have attempted to bridge this gap by developing language-specific LLMs that focus on improving local linguistic fidelity. We demonstrate that, with only publicly available datasets and a computational budget of $120, it is possible to enhance the reasoning capabilities of language-specific LLMs to match the level of DeepSeek R1, without compromising their performance on target language tasks.",
            "score": 15,
            "issue_id": 2209,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 13",
                "zh": "2æœˆ13æ—¥"
            },
            "hash": "36c3c29072ae279d",
            "authors": [
                "Kunat Pipatanakul",
                "Pittawat Taveekitworachai",
                "Potsawee Manakul",
                "Kasima Tharnpipitchai"
            ],
            "affiliations": [
                "SCB 10X R&D SCBX Group Bangkok, Thailand"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09056.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#dataset",
                    "#low_resource",
                    "#multilingual",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¸ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ”Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñƒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), Ğ² Ñ‡Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğº Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿ĞµÑ€ĞµĞ½ĞµÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸Ğ· Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ DeepSeek R1 Ğ² Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ LLM. Ğ¦ĞµĞ»ÑŒ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ÑƒÑĞ¸Ğ»Ğ¸Ñ‚ÑŒ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ¾Ğ¼ Ğ¸ Ğ¾Ğ±Ñ‰ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… LLM Ğ´Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ DeepSeek R1."
                },
                "en": {
                    "title": "Empowering Thai LLMs with Enhanced Reasoning Capabilities",
                    "desc": "This paper explores methods for selecting data and merging models to improve reasoning abilities in language-specific large language models (LLMs), particularly for the Thai language. The authors aim to enhance these models' reasoning skills while ensuring they remain effective in their target languages. They highlight the challenges faced by low-resource languages, which often lack the extensive training data available for high-resource languages like English. The study demonstrates that it is feasible to boost the reasoning capabilities of these LLMs using only publicly available datasets and a modest budget, achieving results comparable to advanced models like DeepSeek R1."
                },
                "zh": {
                    "title": "æå‡ä½èµ„æºè¯­è¨€LLMçš„æ¨ç†èƒ½åŠ›",
                    "desc": "æœ¬æ–‡ç ”ç©¶äº†æ•°æ®é€‰æ‹©å’Œæ¨¡å‹åˆå¹¶çš„æ–¹æ³•ï¼Œæ—¨åœ¨å°†å…ˆè¿›çš„æ¨ç†èƒ½åŠ›ï¼ˆå¦‚DeepSeek R1ï¼‰èå…¥ç‰¹å®šè¯­è¨€çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œç‰¹åˆ«å…³æ³¨æ³°è¯­LLMã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å¢å¼ºç‰¹å®šè¯­è¨€LLMsçš„æ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒå…¶ç›®æ ‡è¯­è¨€çš„èƒ½åŠ›ã€‚DeepSeek R1åœ¨æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ä¸»è¦å—ç›Šäºé«˜èµ„æºè¯­è¨€ï¼Œå¦‚è‹±è¯­å’Œä¸­æ–‡ï¼Œè€Œä½èµ„æºè¯­è¨€åˆ™å—åˆ°å¿½è§†ã€‚æˆ‘ä»¬å±•ç¤ºäº†ä»…ä½¿ç”¨å…¬å¼€æ•°æ®é›†å’Œ120ç¾å…ƒçš„è®¡ç®—é¢„ç®—ï¼Œå°±å¯ä»¥æå‡ç‰¹å®šè¯­è¨€LLMsçš„æ¨ç†èƒ½åŠ›ï¼Œä½¿å…¶è¾¾åˆ°DeepSeek R1çš„æ°´å¹³ï¼Œè€Œä¸å½±å“å…¶åœ¨ç›®æ ‡è¯­è¨€ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.06608",
            "title": "TripoSG: High-Fidelity 3D Shape Synthesis using Large-Scale Rectified Flow Models",
            "url": "https://huggingface.co/papers/2502.06608",
            "abstract": "Recent advancements in diffusion techniques have propelled image and video generation to unprece- dented levels of quality, significantly accelerating the deployment and application of generative AI. However, 3D shape generation technology has so far lagged behind, constrained by limitations in 3D data scale, complexity of 3D data process- ing, and insufficient exploration of advanced tech- niques in the 3D domain. Current approaches to 3D shape generation face substantial challenges in terms of output quality, generalization capa- bility, and alignment with input conditions. We present TripoSG, a new streamlined shape diffu- sion paradigm capable of generating high-fidelity 3D meshes with precise correspondence to input images. Specifically, we propose: 1) A large-scale rectified flow transformer for 3D shape generation, achieving state-of-the-art fidelity through training on extensive, high-quality data. 2) A hybrid supervised training strategy combining SDF, normal, and eikonal losses for 3D VAE, achieving high- quality 3D reconstruction performance. 3) A data processing pipeline to generate 2 million high- quality 3D samples, highlighting the crucial rules for data quality and quantity in training 3D gen- erative models. Through comprehensive experi- ments, we have validated the effectiveness of each component in our new framework. The seamless integration of these parts has enabled TripoSG to achieve state-of-the-art performance in 3D shape generation. The resulting 3D shapes exhibit en- hanced detail due to high-resolution capabilities and demonstrate exceptional fidelity to input im- ages. Moreover, TripoSG demonstrates improved versatility in generating 3D models from diverse image styles and contents, showcasing strong gen- eralization capabilities. To foster progress and innovation in the field of 3D generation, we will make our model publicly available.",
            "score": 11,
            "issue_id": 2210,
            "pub_date": "2025-02-10",
            "pub_date_card": {
                "ru": "10 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 10",
                "zh": "2æœˆ10æ—¥"
            },
            "hash": "0602cc4a46e4c69c",
            "authors": [
                "Yangguang Li",
                "Zi-Xin Zou",
                "Zexiang Liu",
                "Dehu Wang",
                "Yuan Liang",
                "Zhipeng Yu",
                "Xingchao Liu",
                "Yuan-Chen Guo",
                "Ding Liang",
                "Wanli Ouyang",
                "Yan-Pei Cao"
            ],
            "affiliations": [
                "Shanghai AI Laboratory",
                "The Chinese University of Hong Kong",
                "The University of Texas at Austin",
                "VAST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.06608.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#open_source",
                    "#training",
                    "#dataset",
                    "#diffusion"
                ],
                "emoji": "ğŸ§Š",
                "ru": {
                    "title": "TripoSG: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ñ„Ğ¾Ñ€Ğ¼",
                    "desc": "TripoSG - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ñ„Ğ¾Ñ€Ğ¼, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸ĞµĞ¼ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ñ Ğ²Ñ‹Ğ¿Ñ€ÑĞ¼Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ¼ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ñ„Ğ¾Ñ€Ğ¼, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ñ… Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. TripoSG Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ÑƒÑ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ SDF, Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ĞµĞ¹ Ğ¸ ÑĞ¹ĞºĞ¾Ğ½Ğ°Ğ»Ğ° Ğ´Ğ»Ñ 3D VAE. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ğ¸Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ², TripoSG Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ñ„Ğ¾Ñ€Ğ¼ Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸ Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼."
                },
                "en": {
                    "title": "TripoSG: Revolutionizing 3D Shape Generation with Diffusion Techniques",
                    "desc": "This paper introduces TripoSG, a new method for generating high-quality 3D shapes using diffusion techniques. It addresses the challenges of 3D shape generation by employing a large-scale rectified flow transformer and a hybrid supervised training strategy that enhances reconstruction performance. The model is trained on a vast dataset, producing 2 million high-quality 3D samples, which improves the fidelity and detail of the generated shapes. TripoSG not only achieves state-of-the-art results but also shows strong generalization across various input images, making it a significant advancement in 3D generative models."
                },
                "zh": {
                    "title": "TripoSGï¼šé«˜ä¿çœŸ3Då½¢çŠ¶ç”Ÿæˆçš„æ–°èŒƒå¼",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„3Då½¢çŠ¶ç”Ÿæˆæ–¹æ³•TripoSGï¼Œæ—¨åœ¨æé«˜3Dç½‘æ ¼çš„ç”Ÿæˆè´¨é‡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤§è§„æ¨¡çš„æµåŠ¨å˜æ¢å™¨ï¼Œèƒ½å¤Ÿåœ¨å¤§é‡é«˜è´¨é‡æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»è€Œå®ç°é«˜ä¿çœŸåº¦çš„3Då½¢çŠ¶ç”Ÿæˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§æ··åˆç›‘ç£è®­ç»ƒç­–ç•¥ï¼Œç»“åˆäº†SDFã€æ³•çº¿å’ŒEikonalæŸå¤±ï¼Œä»¥æé«˜3Dé‡å»ºæ€§èƒ½ã€‚é€šè¿‡å…¨é¢çš„å®éªŒéªŒè¯ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨3Då½¢çŠ¶ç”Ÿæˆæ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå±•ç°äº†å¯¹è¾“å…¥å›¾åƒçš„é«˜ä¿çœŸåº¦å’Œå¤šæ ·åŒ–çš„ç”Ÿæˆèƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09100",
            "title": "Logical Reasoning in Large Language Models: A Survey",
            "url": "https://huggingface.co/papers/2502.09100",
            "abstract": "With the emergence of advanced reasoning models like OpenAI o3 and DeepSeek-R1, large language models (LLMs) have demonstrated remarkable reasoning capabilities. However, their ability to perform rigorous logical reasoning remains an open question. This survey synthesizes recent advancements in logical reasoning within LLMs, a critical area of AI research. It outlines the scope of logical reasoning in LLMs, its theoretical foundations, and the benchmarks used to evaluate reasoning proficiency. We analyze existing capabilities across different reasoning paradigms - deductive, inductive, abductive, and analogical - and assess strategies to enhance reasoning performance, including data-centric tuning, reinforcement learning, decoding strategies, and neuro-symbolic approaches. The review concludes with future directions, emphasizing the need for further exploration to strengthen logical reasoning in AI systems.",
            "score": 11,
            "issue_id": 2209,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 13",
                "zh": "2æœˆ13æ—¥"
            },
            "hash": "72b32d40c559c7e4",
            "authors": [
                "Hanmeng Liu",
                "Zhizhang Fu",
                "Mengru Ding",
                "Ruoxi Ning",
                "Chaoli Zhang",
                "Xiaozhang Liu",
                "Yue Zhang"
            ],
            "affiliations": [
                "Hainan University",
                "Westlake University",
                "Zhejiang Normal University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09100.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#survey",
                    "#reasoning",
                    "#rl",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ›Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…: Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ Ğ¸ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹",
                    "desc": "Ğ­Ñ‚Ğ¾Ñ‚ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğ¼ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸ÑĞ¼ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). Ğ’ Ğ½ĞµĞ¼ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹ Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ LLM Ğº Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ´ĞµĞ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ, Ğ¸Ğ½Ğ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ, Ğ°Ğ±Ğ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹."
                },
                "en": {
                    "title": "Enhancing Logical Reasoning in Large Language Models",
                    "desc": "This paper reviews the progress made in enhancing logical reasoning capabilities of large language models (LLMs) like OpenAI o3 and DeepSeek-R1. It discusses various reasoning paradigms such as deductive, inductive, abductive, and analogical reasoning, highlighting their theoretical foundations and evaluation benchmarks. The authors analyze strategies to improve reasoning performance, including data-centric tuning and neuro-symbolic approaches. The paper concludes by suggesting future research directions to further develop logical reasoning in AI systems."
                },
                "zh": {
                    "title": "æå‡AIç³»ç»Ÿé€»è¾‘æ¨ç†èƒ½åŠ›çš„æ¢ç´¢",
                    "desc": "éšç€OpenAI o3å’ŒDeepSeek-R1ç­‰å…ˆè¿›æ¨ç†æ¨¡å‹çš„å‡ºç°ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å±•ç°äº†å‡ºè‰²çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨è¿›è¡Œä¸¥æ ¼é€»è¾‘æ¨ç†æ–¹é¢çš„èƒ½åŠ›ä»ç„¶æ˜¯ä¸€ä¸ªæœªè§£ä¹‹è°œã€‚æœ¬æ–‡ç»¼è¿°äº†LLMsä¸­é€»è¾‘æ¨ç†çš„æœ€æ–°è¿›å±•ï¼Œæ¢è®¨äº†é€»è¾‘æ¨ç†çš„èŒƒå›´ã€ç†è®ºåŸºç¡€ä»¥åŠè¯„ä¼°æ¨ç†èƒ½åŠ›çš„åŸºå‡†ã€‚æˆ‘ä»¬åˆ†æäº†ä¸åŒæ¨ç†èŒƒå¼ï¼ˆå¦‚æ¼”ç»ã€å½’çº³ã€æº¯å› å’Œç±»æ¯”ï¼‰çš„ç°æœ‰èƒ½åŠ›ï¼Œå¹¶è¯„ä¼°äº†å¢å¼ºæ¨ç†æ€§èƒ½çš„ç­–ç•¥ï¼ŒåŒ…æ‹¬æ•°æ®ä¸­å¿ƒè°ƒä¼˜ã€å¼ºåŒ–å­¦ä¹ ã€è§£ç ç­–ç•¥å’Œç¥ç»ç¬¦å·æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09621",
            "title": "MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for Reasoning Quality, Robustness, and Efficiency",
            "url": "https://huggingface.co/papers/2502.09621",
            "abstract": "Answering questions with Chain-of-Thought (CoT) has significantly enhanced the reasoning capabilities of Large Language Models (LLMs), yet its impact on Large Multimodal Models (LMMs) still lacks a systematic assessment and in-depth investigation. In this paper, we introduce MME-CoT, a specialized benchmark evaluating the CoT reasoning performance of LMMs, spanning six domains: math, science, OCR, logic, space-time, and general scenes. As the first comprehensive study in this area, we propose a thorough evaluation suite incorporating three novel metrics that assess the reasoning quality, robustness, and efficiency at a fine-grained level. Leveraging curated high-quality data and a unique evaluation strategy, we conduct an in-depth analysis of state-of-the-art LMMs, uncovering several key insights: 1) Models with reflection mechanism demonstrate a superior CoT quality, with Kimi k1.5 outperforming GPT-4o and demonstrating the highest quality results; 2) CoT prompting often degrades LMM performance on perception-heavy tasks, suggesting a potentially harmful overthinking behavior; and 3) Although the CoT quality is high, LMMs with reflection exhibit significant inefficiency in both normal response and self-correction phases. We hope MME-CoT serves as a foundation for advancing multimodal reasoning in LMMs. Project Page: https://mmecot.github.io/",
            "score": 10,
            "issue_id": 2213,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 13",
                "zh": "2æœˆ13æ—¥"
            },
            "hash": "ac755f4f5584a58c",
            "authors": [
                "Dongzhi Jiang",
                "Renrui Zhang",
                "Ziyu Guo",
                "Yanwei Li",
                "Yu Qi",
                "Xinyan Chen",
                "Liuhui Wang",
                "Jianhan Jin",
                "Claire Guo",
                "Shen Yan",
                "Bo Zhang",
                "Chaoyou Fu",
                "Peng Gao",
                "Hongsheng Li"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2502.09621.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞÑ†ĞµĞ½ĞºĞ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Chain-of-Thought Ğ² LMM",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MME-CoT - ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (Chain-of-Thought) Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LMM). Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ÑˆĞµÑÑ‚ÑŒ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ CoT, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Kimi k1.5 Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ GPT-4o. ĞĞ´Ğ½Ğ°ĞºĞ¾ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ CoT Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑƒÑ…ÑƒĞ´ÑˆĞ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ LMM Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ, Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸ĞµĞ¹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "Unlocking Multimodal Reasoning with MME-CoT",
                    "desc": "This paper presents MME-CoT, a benchmark designed to evaluate the Chain-of-Thought (CoT) reasoning capabilities of Large Multimodal Models (LMMs) across various domains. It introduces three new metrics to assess reasoning quality, robustness, and efficiency in a detailed manner. The study reveals that models utilizing a reflection mechanism, like Kimi k1.5, outperform others such as GPT-4o in CoT quality, but may struggle with perception-heavy tasks due to overthinking. Overall, MME-CoT aims to enhance the understanding and development of multimodal reasoning in LMMs."
                },
                "zh": {
                    "title": "MME-CoTï¼šæå‡å¤šæ¨¡æ€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„åŸºå‡†",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†MME-CoTï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨è¯„ä¼°å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†æ€§èƒ½çš„åŸºå‡†ï¼Œæ¶µç›–æ•°å­¦ã€ç§‘å­¦ã€å…‰å­¦å­—ç¬¦è¯†åˆ«ã€é€»è¾‘ã€æ—¶ç©ºå’Œä¸€èˆ¬åœºæ™¯å…­ä¸ªé¢†åŸŸã€‚æˆ‘ä»¬æå‡ºäº†ä¸€å¥—å…¨é¢çš„è¯„ä¼°å·¥å…·ï¼ŒåŒ…å«ä¸‰ç§æ–°é¢–çš„æŒ‡æ ‡ï¼Œç»†è‡´è¯„ä¼°æ¨ç†è´¨é‡ã€é²æ£’æ€§å’Œæ•ˆç‡ã€‚ç ”ç©¶å‘ç°ï¼Œå…·æœ‰åæ€æœºåˆ¶çš„æ¨¡å‹åœ¨CoTè´¨é‡ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œè€Œåœ¨æ„ŸçŸ¥å¯†é›†å‹ä»»åŠ¡ä¸­ï¼ŒCoTæç¤ºå¯èƒ½ä¼šé™ä½LMMçš„æ€§èƒ½ã€‚å°½ç®¡CoTè´¨é‡è¾ƒé«˜ï¼Œä½†å…·æœ‰åæ€çš„LMMåœ¨æ­£å¸¸å“åº”å’Œè‡ªæˆ‘ä¿®æ­£é˜¶æ®µè¡¨ç°å‡ºæ˜¾è‘—çš„ä½æ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09042",
            "title": "Typhoon T1: An Open Thai Reasoning Model",
            "url": "https://huggingface.co/papers/2502.09042",
            "abstract": "This paper introduces Typhoon T1, an open effort to develop an open Thai reasoning model. A reasoning model is a relatively new type of generative model built on top of large language models (LLMs). A reasoning model generates a long chain of thought before arriving at a final answer, an approach found to improve performance on complex tasks. However, details on developing such a model are limited, especially for reasoning models that can generate traces in a low-resource language. Typhoon T1 presents an open effort that dives into the details of developing a reasoning model in a more cost-effective way by leveraging supervised fine-tuning using open datasets, instead of reinforcement learning. This paper shares the details about synthetic data generation and training, as well as our dataset and model weights. Additionally, we provide insights gained from developing a reasoning model that generalizes across domains and is capable of generating reasoning traces in a low-resource language, using Thai as an example. We hope this open effort provides a foundation for further research in this field.",
            "score": 10,
            "issue_id": 2213,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 13",
                "zh": "2æœˆ13æ—¥"
            },
            "hash": "5cb078b546437366",
            "authors": [
                "Pittawat Taveekitworachai",
                "Potsawee Manakul",
                "Kasima Tharnpipitchai",
                "Kunat Pipatanakul"
            ],
            "affiliations": [
                "SCB 10X R&D SCBX Group Bangkok, Thailand"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09042.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#multilingual",
                    "#dataset",
                    "#open_source",
                    "#low_resource",
                    "#data",
                    "#synthetic"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Typhoon T1 - Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ‚Ğ¸Ğ¿ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ»Ğ¸Ğ½Ğ½ÑƒÑ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºÑƒ Ğ¼Ñ‹ÑĞ»ĞµĞ¹ Ğ¿ĞµÑ€ĞµĞ´ Ğ²Ñ‹Ğ´Ğ°Ñ‡ĞµĞ¹ Ğ¾ĞºĞ¾Ğ½Ñ‡Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ»ÑÑ‚ÑÑ Ğ´ĞµÑ‚Ğ°Ğ»ÑĞ¼Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ½Ğ° Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼."
                },
                "en": {
                    "title": "Typhoon T1: Advancing Thai Reasoning Models for Complex Tasks",
                    "desc": "This paper presents Typhoon T1, an initiative to create an open reasoning model specifically for the Thai language. Reasoning models are advanced generative models that produce a sequence of thoughts leading to a conclusion, enhancing performance on intricate tasks. The authors focus on developing this model using supervised fine-tuning with open datasets, rather than relying on reinforcement learning, making it more accessible and cost-effective. The paper details the synthetic data generation process, training methods, and shares the model weights, aiming to support future research in low-resource language reasoning models."
                },
                "zh": {
                    "title": "å¼€æ”¾æ³°è¯­æ¨ç†æ¨¡å‹çš„åˆ›æ–°ä¹‹è·¯",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†Typhoon T1ï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€å‘å¼€æ”¾æ³°è¯­æ¨ç†æ¨¡å‹çš„é¡¹ç›®ã€‚æ¨ç†æ¨¡å‹æ˜¯ä¸€ç§æ–°å‹çš„ç”Ÿæˆæ¨¡å‹ï¼ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ„å»ºï¼Œèƒ½å¤Ÿåœ¨å¾—å‡ºæœ€ç»ˆç­”æ¡ˆä¹‹å‰ç”Ÿæˆé•¿é“¾æ€ç»´ï¼Œä»è€Œæé«˜å¤æ‚ä»»åŠ¡çš„è¡¨ç°ã€‚Typhoon T1é€šè¿‡åˆ©ç”¨ç›‘ç£å¾®è°ƒå’Œå¼€æ”¾æ•°æ®é›†ï¼Œä»¥æ›´å…·æˆæœ¬æ•ˆç›Šçš„æ–¹å¼æ·±å…¥æ¢è®¨æ¨ç†æ¨¡å‹çš„å¼€å‘ï¼Œé¿å…äº†å¼ºåŒ–å­¦ä¹ çš„å¤æ‚æ€§ã€‚æˆ‘ä»¬å¸Œæœ›è¿™ä¸ªå¼€æ”¾é¡¹ç›®ä¸ºè¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶å¥ å®šåŸºç¡€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09560",
            "title": "EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents",
            "url": "https://huggingface.co/papers/2502.09560",
            "abstract": "Leveraging Multi-modal Large Language Models (MLLMs) to create embodied agents offers a promising avenue for tackling real-world tasks. While language-centric embodied agents have garnered substantial attention, MLLM-based embodied agents remain underexplored due to the lack of comprehensive evaluation frameworks. To bridge this gap, we introduce EmbodiedBench, an extensive benchmark designed to evaluate vision-driven embodied agents. EmbodiedBench features: (1) a diverse set of 1,128 testing tasks across four environments, ranging from high-level semantic tasks (e.g., household) to low-level tasks involving atomic actions (e.g., navigation and manipulation); and (2) six meticulously curated subsets evaluating essential agent capabilities like commonsense reasoning, complex instruction understanding, spatial awareness, visual perception, and long-term planning. Through extensive experiments, we evaluated 13 leading proprietary and open-source MLLMs within EmbodiedBench. Our findings reveal that: MLLMs excel at high-level tasks but struggle with low-level manipulation, with the best model, GPT-4o, scoring only 28.9% on average. EmbodiedBench provides a multifaceted standardized evaluation platform that not only highlights existing challenges but also offers valuable insights to advance MLLM-based embodied agents. Our code is available at https://embodiedbench.github.io.",
            "score": 9,
            "issue_id": 2211,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 13",
                "zh": "2æœˆ13æ—¥"
            },
            "hash": "019b4d19788a85cc",
            "authors": [
                "Rui Yang",
                "Hanyang Chen",
                "Junyu Zhang",
                "Mark Zhao",
                "Cheng Qian",
                "Kangrui Wang",
                "Qineng Wang",
                "Teja Venkat Koripella",
                "Marziyeh Movahedi",
                "Manling Li",
                "Heng Ji",
                "Huan Zhang",
                "Tong Zhang"
            ],
            "affiliations": [
                "Northwestern University",
                "Toyota Technological Institute at Chicago",
                "University of Illinois Urbana-Champaign",
                "University of Toronto"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09560.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#open_source",
                    "#games",
                    "#reasoning",
                    "#agents"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "EmbodiedBench: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ EmbodiedBench - ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 1128 Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… ÑÑ€ĞµĞ´Ğ°Ñ…, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… ĞºĞ°Ğº Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ 13 Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ MLLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸, Ğ½Ğ¾ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ½Ğ¸Ğ·ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸ÑĞ¼Ğ¸. EmbodiedBench Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñƒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ MLLM."
                },
                "en": {
                    "title": "Empowering Embodied Agents with Comprehensive Evaluation",
                    "desc": "This paper discusses the development of EmbodiedBench, a benchmark for evaluating multi-modal large language models (MLLMs) in the context of embodied agents. It highlights the gap in existing evaluation frameworks for MLLM-based agents, which are crucial for performing real-world tasks. The benchmark includes a wide range of tasks that assess various capabilities such as commonsense reasoning and spatial awareness. The results indicate that while MLLMs perform well on high-level tasks, they face significant challenges with low-level manipulation tasks."
                },
                "zh": {
                    "title": "å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åŠ©åŠ›å…·èº«æ™ºèƒ½ä½“çš„è¯„ä¼°ä¸å‘å±•",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨åˆ›å»ºå…·èº«æ™ºèƒ½ä½“æ–¹é¢çš„åº”ç”¨ï¼Œæ—¨åœ¨è§£å†³ç°å®ä¸–ç•Œä»»åŠ¡ã€‚æˆ‘ä»¬æå‡ºäº†EmbodiedBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°è§†è§‰é©±åŠ¨çš„å…·èº«æ™ºèƒ½ä½“ã€‚EmbodiedBenchåŒ…å«1288ä¸ªæµ‹è¯•ä»»åŠ¡ï¼Œæ¶µç›–é«˜å±‚è¯­ä¹‰ä»»åŠ¡å’Œä½å±‚åŸå­åŠ¨ä½œä»»åŠ¡ï¼Œå¹¶è¯„ä¼°æ™ºèƒ½ä½“çš„å¸¸è¯†æ¨ç†ã€å¤æ‚æŒ‡ä»¤ç†è§£ã€ç©ºé—´æ„è¯†ã€è§†è§‰æ„ŸçŸ¥å’Œé•¿æœŸè§„åˆ’ç­‰èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡MLLMsåœ¨é«˜å±‚ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨ä½å±‚æ“ä½œä»»åŠ¡ä¸Šä»ç„¶å­˜åœ¨æŒ‘æˆ˜ï¼Œæœ€å¥½çš„æ¨¡å‹GPT-4oçš„å¹³å‡å¾—åˆ†ä»…ä¸º28.9%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09601",
            "title": "CoT-Valve: Length-Compressible Chain-of-Thought Tuning",
            "url": "https://huggingface.co/papers/2502.09601",
            "abstract": "Chain-of-Thought significantly enhances a model's reasoning capability, but it also comes with a considerable increase in inference costs due to long chains. With the observation that the reasoning path can be easily compressed under easy tasks but struggle on hard tasks, we explore the feasibility of elastically controlling the length of reasoning paths with only one model, thereby reducing the inference overhead of reasoning models dynamically based on task difficulty. We introduce a new tuning and inference strategy named CoT-Valve, designed to allow models to generate reasoning chains of varying lengths. To achieve this, we propose to identify a direction in the parameter space that, when manipulated, can effectively control the length of generated CoT. Moreover, we show that this property is valuable for compressing the reasoning chain. We construct datasets with chains from long to short for the same questions and explore two enhanced strategies for CoT-Valve: (1) a precise length-compressible CoT tuning method, and (2) a progressive chain length compression approach. Our experiments show that CoT-Valve successfully enables controllability and compressibility of the chain and shows better performance than the prompt-based control. We applied this method to QwQ-32B-Preview, reducing reasoning chains on GSM8K from 741 to 225 tokens with a minor performance drop (95.07% to 94.92%) and on AIME from 6827 to 4629 tokens, with only one additional incorrect answer.",
            "score": 8,
            "issue_id": 2212,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 13",
                "zh": "2æœˆ13æ—¥"
            },
            "hash": "4b0518724dea8b37",
            "authors": [
                "Xinyin Ma",
                "Guangnian Wan",
                "Runpeng Yu",
                "Gongfan Fang",
                "Xinchao Wang"
            ],
            "affiliations": [
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09601.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#dataset",
                    "#optimization",
                    "#inference"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ğ»Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ğ¸Ğ½Ğ¾Ğ¹ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ CoT-Valve, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»Ğ¸Ñ€ÑƒÑ ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¼, Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¶Ğ¸Ğ¼Ğ°ĞµĞ¼Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ CoT-Valve ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞ¶Ğ¸Ğ¼Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Dynamic Reasoning Control with CoT-Valve",
                    "desc": "This paper presents CoT-Valve, a new strategy for controlling the length of reasoning chains in machine learning models, particularly in Chain-of-Thought (CoT) reasoning. The authors observe that while longer reasoning paths improve performance, they also increase inference costs, especially on harder tasks. CoT-Valve allows a single model to dynamically adjust the length of its reasoning based on task difficulty, thus optimizing efficiency. The experiments demonstrate that this method not only compresses reasoning chains significantly but also maintains high performance compared to traditional prompt-based controls."
                },
                "zh": {
                    "title": "åŠ¨æ€æ§åˆ¶æ¨ç†é“¾é•¿åº¦çš„åˆ›æ–°ç­–ç•¥",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºCoT-Valveçš„æ–°ç­–ç•¥ï¼Œç”¨äºåŠ¨æ€æ§åˆ¶æ¨ç†é“¾çš„é•¿åº¦ï¼Œä»¥é™ä½æ¨ç†æ¨¡å‹çš„è®¡ç®—å¼€é”€ã€‚ç ”ç©¶å‘ç°ï¼Œæ¨ç†è·¯å¾„åœ¨ç®€å•ä»»åŠ¡ä¸­å®¹æ˜“å‹ç¼©ï¼Œä½†åœ¨å›°éš¾ä»»åŠ¡ä¸­åˆ™è¾ƒä¸ºå›°éš¾ï¼Œå› æ­¤æˆ‘ä»¬æ¢ç´¢äº†å¦‚ä½•åœ¨å•ä¸€æ¨¡å‹ä¸­å®ç°è¿™ä¸€ç›®æ ‡ã€‚é€šè¿‡è°ƒæ•´å‚æ•°ç©ºé—´ä¸­çš„æ–¹å‘ï¼ŒCoT-Valveèƒ½å¤Ÿæœ‰æ•ˆæ§åˆ¶ç”Ÿæˆçš„æ¨ç†é“¾é•¿åº¦ï¼Œå¹¶ä¸”åœ¨å‹ç¼©æ¨ç†é“¾æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCoT-Valveåœ¨æ§åˆ¶å’Œå‹ç¼©æ¨ç†é“¾æ–¹é¢ä¼˜äºåŸºäºæç¤ºçš„æ§åˆ¶æ–¹æ³•ï¼Œä¸”åœ¨æ€§èƒ½ä¸Šä»…æœ‰è½»å¾®ä¸‹é™ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09390",
            "title": "SQuARE: Sequential Question Answering Reasoning Engine for Enhanced Chain-of-Thought in Large Language Models",
            "url": "https://huggingface.co/papers/2502.09390",
            "abstract": "In the rapidly evolving field of Natural Language Processing, Large Language Models (LLMs) are tasked with increasingly complex reasoning challenges. Traditional methods like chain-of-thought prompting have shown promise but often fall short in fully leveraging a model's reasoning capabilities. This paper introduces SQuARE (Sequential Question Answering Reasoning Engine), a novel prompting technique designed to improve reasoning through a self-interrogation paradigm. Building upon CoT frameworks, SQuARE prompts models to generate and resolve multiple auxiliary questions before tackling the main query, promoting a more thorough exploration of various aspects of a topic. Our expansive evaluations, conducted with Llama 3 and GPT-4o models across multiple question-answering datasets, demonstrate that SQuARE significantly surpasses traditional CoT prompts and existing rephrase-and-respond methods. By systematically decomposing queries, SQuARE advances LLM capabilities in reasoning tasks. The code is publicly available at https://github.com/IntelLabs/RAG-FiT/tree/square.",
            "score": 6,
            "issue_id": 2214,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 13",
                "zh": "2æœˆ13æ—¥"
            },
            "hash": "4caf9cec56011350",
            "authors": [
                "Daniel Fleischer",
                "Moshe Berchansky",
                "Gad Markovits",
                "Moshe Wasserblat"
            ],
            "affiliations": [
                "IntelLabs"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09390.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#dataset",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "SQuARE: ÑĞ°Ğ¼Ğ¾Ğ¾Ğ¿Ñ€Ğ¾Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ SQuARE, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. SQuARE Ğ¿Ğ¾Ğ±ÑƒĞ¶Ğ´Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿ĞµÑ€ĞµĞ´ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ, Ñ‡Ñ‚Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ¼Ñƒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞ¼Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Llama 3 Ğ¸ GPT-4 Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ SQuARE Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°Ğ·Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿ĞµÑ€ĞµÑ„Ñ€Ğ°Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Unlocking Deeper Reasoning with SQuARE",
                    "desc": "This paper presents SQuARE, a new prompting technique aimed at enhancing the reasoning abilities of Large Language Models (LLMs) in Natural Language Processing. Unlike traditional methods that may not fully utilize a model's reasoning potential, SQuARE employs a self-interrogation approach, encouraging models to generate and answer multiple auxiliary questions before addressing the main question. This method builds on existing chain-of-thought frameworks, allowing for a deeper exploration of topics. Evaluations with Llama 3 and GPT-4o show that SQuARE outperforms conventional prompting techniques, leading to improved reasoning in question-answering tasks."
                },
                "zh": {
                    "title": "SQuAREï¼šæå‡æ¨ç†èƒ½åŠ›çš„æ–°æ–¹æ³•",
                    "desc": "åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é¢ä¸´è¶Šæ¥è¶Šå¤æ‚çš„æ¨ç†æŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„é“¾å¼æ€ç»´æç¤ºæ–¹æ³•è™½ç„¶æœ‰ä¸€å®šæ•ˆæœï¼Œä½†å¾€å¾€æ— æ³•å……åˆ†å‘æŒ¥æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æç¤ºæŠ€æœ¯SQuAREï¼ˆé¡ºåºé—®ç­”æ¨ç†å¼•æ“ï¼‰ï¼Œé€šè¿‡è‡ªæˆ‘è´¨è¯¢çš„æ–¹å¼æ¥æ”¹å–„æ¨ç†è¿‡ç¨‹ã€‚SQuAREåœ¨å¤šä¸ªé—®ç­”æ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œå…¶åœ¨æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„é“¾å¼æ€ç»´æç¤ºå’Œç°æœ‰çš„é‡è¿°-å›åº”æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09619",
            "title": "Can this Model Also Recognize Dogs? Zero-Shot Model Search from Weights",
            "url": "https://huggingface.co/papers/2502.09619",
            "abstract": "With the increasing numbers of publicly available models, there are probably pretrained, online models for most tasks users require. However, current model search methods are rudimentary, essentially a text-based search in the documentation, thus users cannot find the relevant models. This paper presents ProbeLog, a method for retrieving classification models that can recognize a target concept, such as \"Dog\", without access to model metadata or training data. Differently from previous probing methods, ProbeLog computes a descriptor for each output dimension (logit) of each model, by observing its responses on a fixed set of inputs (probes). Our method supports both logit-based retrieval (\"find more logits like this\") and zero-shot, text-based retrieval (\"find all logits corresponding to dogs\"). As probing-based representations require multiple costly feedforward passes through the model, we develop a method, based on collaborative filtering, that reduces the cost of encoding repositories by 3x. We demonstrate that ProbeLog achieves high retrieval accuracy, both in real-world and fine-grained search tasks and is scalable to full-size repositories.",
            "score": 4,
            "issue_id": 2214,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 13",
                "zh": "2æœˆ13æ—¥"
            },
            "hash": "2222d8b83a19a957",
            "authors": [
                "Jonathan Kahana",
                "Or Nathan",
                "Eliahu Horwitz",
                "Yedid Hoshen"
            ],
            "affiliations": [
                "School of Computer Science and Engineering The Hebrew University of Jerusalem, Israel"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09619.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#dataset",
                    "#optimization"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ProbeLog: Ğ£Ğ¼Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ½ÑƒĞ¶Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "ProbeLog - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ¸ÑĞºĞ° ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ° Ğº Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼. ĞĞ½ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµÑ‚ Ğ´ĞµÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°Ñ ĞµĞµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ½Ğ° Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ProbeLog Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ğº Ğ¿Ğ¾Ğ¸ÑĞº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ»Ğ¾Ğ³Ğ¸Ñ‚Ğ¾Ğ², Ñ‚Ğ°Ğº Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ñ Ğ½ÑƒĞ»ĞµĞ²Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½ÑƒÑ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "Efficient Model Retrieval with ProbeLog",
                    "desc": "This paper introduces ProbeLog, a novel method for retrieving classification models that can identify specific concepts without needing detailed model information. It generates a descriptor for each model's output dimension by analyzing its responses to a set of predefined inputs, known as probes. ProbeLog allows users to perform both logit-based and zero-shot retrieval, making it easier to find models relevant to their needs. Additionally, it employs collaborative filtering to significantly reduce the computational cost of processing large model repositories, achieving high accuracy in model retrieval tasks."
                },
                "zh": {
                    "title": "é«˜æ•ˆæ¨¡å‹æ£€ç´¢ï¼Œè½»æ¾æ‰¾åˆ°æ‰€éœ€ï¼",
                    "desc": "éšç€å…¬å¼€æ¨¡å‹æ•°é‡çš„å¢åŠ ï¼Œç”¨æˆ·æ‰€éœ€çš„ä»»åŠ¡å‡ ä¹éƒ½æœ‰é¢„è®­ç»ƒçš„åœ¨çº¿æ¨¡å‹ã€‚ç„¶è€Œï¼Œç›®å‰çš„æ¨¡å‹æœç´¢æ–¹æ³•ç›¸å¯¹ç®€å•ï¼Œä¸»è¦ä¾èµ–æ–‡æ¡£ä¸­çš„æ–‡æœ¬æœç´¢ï¼Œå¯¼è‡´ç”¨æˆ·æ— æ³•æ‰¾åˆ°ç›¸å…³æ¨¡å‹ã€‚æœ¬æ–‡æå‡ºäº†ProbeLogï¼Œä¸€ç§æ£€ç´¢åˆ†ç±»æ¨¡å‹çš„æ–¹æ³•ï¼Œå¯ä»¥è¯†åˆ«ç›®æ ‡æ¦‚å¿µï¼Œå¦‚â€œç‹—â€ï¼Œè€Œæ— éœ€è®¿é—®æ¨¡å‹å…ƒæ•°æ®æˆ–è®­ç»ƒæ•°æ®ã€‚ä¸ä¹‹å‰çš„æ¢æµ‹æ–¹æ³•ä¸åŒï¼ŒProbeLogé€šè¿‡è§‚å¯Ÿæ¨¡å‹åœ¨å›ºå®šè¾“å…¥é›†ä¸Šçš„å“åº”ï¼Œä¸ºæ¯ä¸ªæ¨¡å‹çš„æ¯ä¸ªè¾“å‡ºç»´åº¦ï¼ˆlogitï¼‰è®¡ç®—æè¿°ç¬¦ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„æ¨¡å‹æ£€ç´¢ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.08946",
            "title": "The Stochastic Parrot on LLM's Shoulder: A Summative Assessment of Physical Concept Understanding",
            "url": "https://huggingface.co/papers/2502.08946",
            "abstract": "In a systematic way, we investigate a widely asked question: Do LLMs really understand what they say?, which relates to the more familiar term Stochastic Parrot. To this end, we propose a summative assessment over a carefully designed physical concept understanding task, PhysiCo. Our task alleviates the memorization issue via the usage of grid-format inputs that abstractly describe physical phenomena. The grids represents varying levels of understanding, from the core phenomenon, application examples to analogies to other abstract patterns in the grid world. A comprehensive study on our task demonstrates: (1) state-of-the-art LLMs, including GPT-4o, o1 and Gemini 2.0 flash thinking, lag behind humans by ~40%; (2) the stochastic parrot phenomenon is present in LLMs, as they fail on our grid task but can describe and recognize the same concepts well in natural language; (3) our task challenges the LLMs due to intrinsic difficulties rather than the unfamiliar grid format, as in-context learning and fine-tuning on same formatted data added little to their performance.",
            "score": 4,
            "issue_id": 2209,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 13",
                "zh": "2æœˆ13æ—¥"
            },
            "hash": "daecc7f38306f7b8",
            "authors": [
                "Mo Yu",
                "Lemao Liu",
                "Junjie Wu",
                "Tsz Ting Chung",
                "Shunchi Zhang",
                "Jiangnan Li",
                "Dit-Yan Yeung",
                "Jie Zhou"
            ],
            "affiliations": [
                "HKUST",
                "JHU",
                "WeChat AI, Tencent"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.08946.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#dataset",
                    "#training",
                    "#reasoning",
                    "#interpretability"
                ],
                "emoji": "ğŸ¦œ",
                "ru": {
                    "title": "Ğ¯Ğ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ»Ğ¸ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ñ?",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ Ğ¾ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LLM) Ñ‚Ğ¾Ğ³Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ½Ğ¸ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‚. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ PhysiCo Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ ÑĞµÑ‚ĞºĞ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ LLM, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ GPT-4 Ğ¸ Gemini 2.0, Ğ¾Ñ‚ÑÑ‚Ğ°ÑÑ‚ Ğ¾Ñ‚ Ğ»ÑĞ´ĞµĞ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ¾ Ğ½Ğ° 40% Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ Ñ„ĞµĞ½Ğ¾Ğ¼ĞµĞ½ 'ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¿ÑƒĞ³Ğ°Ñ', Ñ‚Ğ°Ğº ĞºĞ°Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ ÑĞµÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡ĞµĞ¹, Ğ½Ğ¾ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ğµ Ğ¶Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ."
                },
                "en": {
                    "title": "Unveiling the Limits of LLM Understanding",
                    "desc": "This paper investigates whether large language models (LLMs) truly understand the content they generate, addressing the concept of the 'Stochastic Parrot'. The authors introduce a new assessment task called PhysiCo, which uses grid-format inputs to evaluate understanding of physical concepts. Their findings reveal that top-performing LLMs, like GPT-4o and Gemini 2.0, significantly underperform compared to humans, indicating a gap in comprehension. Additionally, the study shows that LLMs struggle with the task due to inherent challenges in understanding rather than just the format of the input data."
                },
                "zh": {
                    "title": "æ¢ç©¶å¤§å‹è¯­è¨€æ¨¡å‹çš„ç†è§£èƒ½åŠ›",
                    "desc": "æœ¬ç ”ç©¶ç³»ç»Ÿæ€§åœ°æ¢è®¨äº†ä¸€ä¸ªå¸¸è§é—®é¢˜ï¼šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ˜¯å¦çœŸæ­£ç†è§£å®ƒä»¬æ‰€è¯´çš„å†…å®¹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºPhysiCoçš„è¯„ä¼°ä»»åŠ¡ï¼Œæ—¨åœ¨é€šè¿‡ç½‘æ ¼æ ¼å¼çš„è¾“å…¥æ¥å‡è½»è®°å¿†é—®é¢˜ï¼Œè¿™äº›è¾“å…¥æŠ½è±¡åœ°æè¿°äº†ç‰©ç†ç°è±¡ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå½“å‰æœ€å…ˆè¿›çš„LLMsåœ¨ç†è§£èƒ½åŠ›ä¸Šè½åäºäººç±»çº¦40%ï¼Œå¹¶ä¸”åœ¨ç½‘æ ¼ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼Œæ˜¾ç¤ºå‡ºéšæœºé¹¦é¹‰ç°è±¡çš„å­˜åœ¨ã€‚æˆ‘ä»¬çš„ä»»åŠ¡æŒ‘æˆ˜äº†LLMsï¼Œä¸»è¦æ˜¯ç”±äºå†…åœ¨çš„å›°éš¾ï¼Œè€Œéç½‘æ ¼æ ¼å¼çš„ä¸ç†Ÿæ‚‰ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.08468",
            "title": "mmE5: Improving Multimodal Multilingual Embeddings via High-quality Synthetic Data",
            "url": "https://huggingface.co/papers/2502.08468",
            "abstract": "Multimodal embedding models have gained significant attention for their ability to map data from different modalities, such as text and images, into a unified representation space. However, the limited labeled multimodal data often hinders embedding performance. Recent approaches have leveraged data synthesis to address this problem, yet the quality of synthetic data remains a critical bottleneck. In this work, we identify three criteria for high-quality synthetic multimodal data. First, broad scope ensures that the generated data covers diverse tasks and modalities, making it applicable to various downstream scenarios. Second, robust cross-modal alignment makes different modalities semantically consistent. Third, high fidelity ensures that the synthetic data maintains realistic details to enhance its reliability. Guided by these principles, we synthesize datasets that: (1) cover a wide range of tasks, modality combinations, and languages, (2) are generated via a deep thinking process within a single pass of a multimodal large language model, and (3) incorporate real-world images with accurate and relevant texts, ensuring fidelity through self-evaluation and refinement. Leveraging these high-quality synthetic and labeled datasets, we train a multimodal multilingual E5 model mmE5. Extensive experiments demonstrate that mmE5 achieves state-of-the-art performance on the MMEB Benchmark and superior multilingual performance on the XTD benchmark. Our codes, datasets and models are released in https://github.com/haon-chen/mmE5.",
            "score": 3,
            "issue_id": 2211,
            "pub_date": "2025-02-12",
            "pub_date_card": {
                "ru": "12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 12",
                "zh": "2æœˆ12æ—¥"
            },
            "hash": "ac7814f15d1e9616",
            "authors": [
                "Haonan Chen",
                "Liang Wang",
                "Nan Yang",
                "Yutao Zhu",
                "Ziliang Zhao",
                "Furu Wei",
                "Zhicheng Dou"
            ],
            "affiliations": [
                "Gaoling School of Artificial Intelligence, Renmin University of China",
                "Microsoft Corporation"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.08468.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#open_source",
                    "#synthetic",
                    "#dataset",
                    "#training",
                    "#data",
                    "#multilingual"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ²",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² mmE5, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ğ»Ğ¸ Ñ‚Ñ€Ğ¸ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ°ĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…: ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¹ Ğ¾Ñ…Ğ²Ğ°Ñ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹, Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğµ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ°Ñ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹, Ğ¾Ğ½Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ mmE5 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… MMEB Ğ¸ XTD."
                },
                "en": {
                    "title": "Enhancing Multimodal Learning with High-Quality Synthetic Data",
                    "desc": "This paper discusses the development of multimodal embedding models that integrate different types of data, like text and images, into a single representation. It highlights the challenge of limited labeled multimodal data and proposes a solution through high-quality synthetic data generation. The authors establish three key criteria for effective synthetic data: broad scope, robust cross-modal alignment, and high fidelity. By adhering to these principles, they create a multimodal multilingual E5 model, mmE5, which demonstrates exceptional performance on various benchmarks."
                },
                "zh": {
                    "title": "é«˜è´¨é‡åˆæˆæ•°æ®åŠ©åŠ›å¤šæ¨¡æ€æ¨¡å‹",
                    "desc": "å¤šæ¨¡æ€åµŒå…¥æ¨¡å‹èƒ½å¤Ÿå°†æ–‡æœ¬å’Œå›¾åƒç­‰ä¸åŒæ¨¡æ€çš„æ•°æ®æ˜ å°„åˆ°ç»Ÿä¸€çš„è¡¨ç¤ºç©ºé—´ï¼Œä½†æœ‰é™çš„æ ‡æ³¨å¤šæ¨¡æ€æ•°æ®å¸¸å¸¸å½±å“åµŒå…¥æ€§èƒ½ã€‚æœ¬æ–‡æå‡ºäº†é«˜è´¨é‡åˆæˆå¤šæ¨¡æ€æ•°æ®çš„ä¸‰ä¸ªæ ‡å‡†ï¼šå¹¿æ³›çš„èŒƒå›´ã€ç¨³å¥çš„è·¨æ¨¡æ€å¯¹é½å’Œé«˜ä¿çœŸåº¦ã€‚é€šè¿‡è¿™äº›æ ‡å‡†ï¼Œæˆ‘ä»¬åˆæˆäº†è¦†ç›–å¤šç§ä»»åŠ¡å’Œæ¨¡æ€ç»„åˆçš„æ•°æ®é›†ï¼Œå¹¶åˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œç”Ÿæˆã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬è®­ç»ƒçš„å¤šæ¨¡æ€å¤šè¯­è¨€E5æ¨¡å‹mmE5åœ¨MMEBåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶åœ¨XTDåŸºå‡†æµ‹è¯•ä¸­å±•ç°äº†å“è¶Šçš„å¤šè¯­è¨€æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09614",
            "title": "DexTrack: Towards Generalizable Neural Tracking Control for Dexterous Manipulation from Human References",
            "url": "https://huggingface.co/papers/2502.09614",
            "abstract": "We address the challenge of developing a generalizable neural tracking controller for dexterous manipulation from human references. This controller aims to manage a dexterous robot hand to manipulate diverse objects for various purposes defined by kinematic human-object interactions. Developing such a controller is complicated by the intricate contact dynamics of dexterous manipulation and the need for adaptivity, generalizability, and robustness. Current reinforcement learning and trajectory optimization methods often fall short due to their dependence on task-specific rewards or precise system models. We introduce an approach that curates large-scale successful robot tracking demonstrations, comprising pairs of human references and robot actions, to train a neural controller. Utilizing a data flywheel, we iteratively enhance the controller's performance, as well as the number and quality of successful tracking demonstrations. We exploit available tracking demonstrations and carefully integrate reinforcement learning and imitation learning to boost the controller's performance in dynamic environments. At the same time, to obtain high-quality tracking demonstrations, we individually optimize per-trajectory tracking by leveraging the learned tracking controller in a homotopy optimization method. The homotopy optimization, mimicking chain-of-thought, aids in solving challenging trajectory tracking problems to increase demonstration diversity. We showcase our success by training a generalizable neural controller and evaluating it in both simulation and real world. Our method achieves over a 10% improvement in success rates compared to leading baselines. The project website with animated results is available at https://meowuu7.github.io/DexTrack/.",
            "score": 1,
            "issue_id": 2216,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 13",
                "zh": "2æœˆ13æ—¥"
            },
            "hash": "f83cdb806eef0812",
            "authors": [
                "Xueyi Liu",
                "Jianibieke Adalibieke",
                "Qianwei Han",
                "Yuzhe Qin",
                "Li Yi"
            ],
            "affiliations": [
                "Shanghai AI Laboratory",
                "Shanghai Qi Zhi Institute",
                "Tsinghua University",
                "UC San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09614.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#training",
                    "#optimization",
                    "#rl",
                    "#games",
                    "#agents"
                ],
                "emoji": "ğŸ¦¾",
                "ru": {
                    "title": "ĞĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ»ĞµÑ€ Ğ´Ğ»Ñ Ğ»Ğ¾Ğ²ĞºĞ¸Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ€ÑƒĞºĞ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ»ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸ÑÑ… Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ€ÑƒĞºĞ¾Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¹ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ»ĞµÑ€Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ³Ğ¾Ğ¼Ğ¾Ñ‚Ğ¾Ğ¿Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Empowering Robots with Human-Like Dexterity through Neural Tracking!",
                    "desc": "This paper presents a novel neural tracking controller designed for dexterous manipulation of objects by a robot hand, using human references as a guide. The authors tackle the complexities of contact dynamics and the need for the controller to be adaptable and robust across various tasks. They propose a method that combines reinforcement learning and imitation learning, leveraging a large dataset of successful robot tracking demonstrations to improve performance iteratively. The results show a significant improvement in success rates, demonstrating the effectiveness of their approach in both simulated and real-world environments."
                },
                "zh": {
                    "title": "é€šç”¨ç¥ç»æ§åˆ¶å™¨ï¼šæå‡çµå·§æ“ä½œçš„æˆåŠŸç‡",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šç”¨çš„ç¥ç»è·Ÿè¸ªæ§åˆ¶å™¨ï¼Œç”¨äºä»äººç±»å‚è€ƒä¸­è¿›è¡Œçµå·§æ“ä½œã€‚è¯¥æ§åˆ¶å™¨æ—¨åœ¨ç®¡ç†çµå·§æœºå™¨äººæ‰‹ï¼Œä»¥æ“æ§å¤šç§ç‰©ä½“ï¼Œé€‚åº”ä¸åŒçš„äººæœºäº¤äº’éœ€æ±‚ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å¤§è§„æ¨¡æˆåŠŸçš„æœºå™¨äººè·Ÿè¸ªç¤ºä¾‹æ¥è®­ç»ƒæ§åˆ¶å™¨ï¼Œå¹¶ç»“åˆå¼ºåŒ–å­¦ä¹ å’Œæ¨¡ä»¿å­¦ä¹ æ¥æå‡å…¶åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„è¡¨ç°ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬åœ¨ä»¿çœŸå’Œç°å®ä¸–ç•Œä¸­è¯„ä¼°äº†è¯¥æ§åˆ¶å™¨ï¼ŒæˆåŠŸç‡æ¯”ç°æœ‰æ–¹æ³•æé«˜äº†10%ä»¥ä¸Šã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.05761",
            "title": "3CAD: A Large-Scale Real-World 3C Product Dataset for Unsupervised Anomaly",
            "url": "https://huggingface.co/papers/2502.05761",
            "abstract": "Industrial anomaly detection achieves progress thanks to datasets such as MVTec-AD and VisA. However, they suf- fer from limitations in terms of the number of defect sam- ples, types of defects, and availability of real-world scenes. These constraints inhibit researchers from further exploring the performance of industrial detection with higher accuracy. To this end, we propose a new large-scale anomaly detection dataset called 3CAD, which is derived from real 3C produc- tion lines. Specifically, the proposed 3CAD includes eight different types of manufactured parts, totaling 27,039 high- resolution images labeled with pixel-level anomalies. The key features of 3CAD are that it covers anomalous regions of different sizes, multiple anomaly types, and the possibility of multiple anomalous regions and multiple anomaly types per anomaly image. This is the largest and first anomaly de- tection dataset dedicated to 3C product quality control for community exploration and development. Meanwhile, we in- troduce a simple yet effective framework for unsupervised anomaly detection: a Coarse-to-Fine detection paradigm with Recovery Guidance (CFRG). To detect small defect anoma- lies, the proposed CFRG utilizes a coarse-to-fine detection paradigm. Specifically, we utilize a heterogeneous distilla- tion model for coarse localization and then fine localiza- tion through a segmentation model. In addition, to better capture normal patterns, we introduce recovery features as guidance. Finally, we report the results of our CFRG frame- work and popular anomaly detection methods on the 3CAD dataset, demonstrating strong competitiveness and providing a highly challenging benchmark to promote the development of the anomaly detection field. Data and code are available: https://github.com/EnquanYang2022/3CAD.",
            "score": 1,
            "issue_id": 2215,
            "pub_date": "2025-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "f7f244334d53bca7",
            "authors": [
                "Enquan Yang",
                "Peng Xing",
                "Hanyang Sun",
                "Wenbo Guo",
                "Yuanwei Ma",
                "Zechao Li",
                "Dan Zeng"
            ],
            "affiliations": [
                "Changzhou Microintelligence Corporation",
                "Nanjing University of Science and Technology",
                "School of Communication and Information Engineering, Shanghai University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.05761.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "3CAD: ĞšÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 3CAD, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ»Ğ¸Ğ½Ğ¸Ğ¹ 3C. Ğ­Ñ‚Ğ¾Ñ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 27,039 Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑŒĞ¼Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹. 3CAD Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ÑÑ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸ĞµĞ¼ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ² Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹, Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¸ Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¸. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ±ĞµĞ· ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ CFRG, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ Ğ³Ñ€ÑƒĞ±Ğ¾Ğ³Ğ¾ Ğº Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¼Ñƒ Ñ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¿Ğ¾ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "3CAD: A New Benchmark for Anomaly Detection in 3C Products",
                    "desc": "This paper introduces a new dataset called 3CAD for industrial anomaly detection, specifically focusing on 3C product quality control. The 3CAD dataset contains 27,039 high-resolution images with pixel-level annotations for eight types of manufactured parts, addressing the limitations of existing datasets like MVTec-AD and VisA. To enhance anomaly detection, the authors propose a Coarse-to-Fine detection framework with Recovery Guidance (CFRG), which improves the localization of small defects by first identifying coarse regions and then refining the detection through segmentation. The results show that the CFRG framework is competitive with existing methods, providing a valuable resource for advancing research in anomaly detection."
                },
                "zh": {
                    "title": "3CADï¼šæ¨åŠ¨3Cäº§å“å¼‚å¸¸æ£€æµ‹çš„æ–°æ•°æ®é›†",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„å¤§è§„æ¨¡å¼‚å¸¸æ£€æµ‹æ•°æ®é›†ï¼Œåä¸º3CADï¼Œä¸“æ³¨äº3Cäº§å“çš„è´¨é‡æ§åˆ¶ã€‚è¯¥æ•°æ®é›†åŒ…å«27,039å¼ é«˜åˆ†è¾¨ç‡å›¾åƒï¼Œæ ‡æ³¨äº†ä¸åŒç±»å‹å’Œå¤§å°çš„åƒç´ çº§å¼‚å¸¸ã€‚ä¸ºäº†æé«˜å¼‚å¸¸æ£€æµ‹çš„å‡†ç¡®æ€§ï¼Œè®ºæ–‡è¿˜ä»‹ç»äº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹æ¡†æ¶ï¼Œç§°ä¸ºCFRGï¼Œé‡‡ç”¨ç²—åˆ°ç»†çš„æ£€æµ‹èŒƒå¼ã€‚é€šè¿‡è¯¥æ¡†æ¶ï¼Œç ”ç©¶äººå‘˜å¯ä»¥æ›´å¥½åœ°æ•æ‰æ­£å¸¸æ¨¡å¼å¹¶å®šä½å°ç¼ºé™·ï¼Œæ¨åŠ¨å¼‚å¸¸æ£€æµ‹é¢†åŸŸçš„å‘å±•ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-02-13.html",
    "link_next": "2025-02-17.html",
    "link_month": "2025-02.html",
    "short_date_prev": {
        "ru": "13.02",
        "en": "02/13",
        "zh": "2æœˆ13æ—¥"
    },
    "short_date_next": {
        "ru": "17.02",
        "en": "02/17",
        "zh": "2æœˆ17æ—¥"
    },
    "categories": {
        "#dataset": 10,
        "#data": 3,
        "#benchmark": 7,
        "#agents": 3,
        "#cv": 1,
        "#rl": 2,
        "#rlhf": 1,
        "#rag": 1,
        "#plp": 0,
        "#inference": 3,
        "#3d": 2,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 5,
        "#math": 0,
        "#multilingual": 3,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 12,
        "#robotics": 1,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 1,
        "#reasoning": 8,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 7,
        "#survey": 1,
        "#diffusion": 2,
        "#alignment": 1,
        "#story_generation": 1,
        "#hallucinations": 1,
        "#long_context": 2,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 5,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 2
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†ç°ä»£å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¤„ç†éå¸¸é•¿çš„ä¸Šä¸‹æ–‡æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ¨ç†é€Ÿåº¦å˜æ…¢å’Œå†…å­˜æˆæœ¬å¢åŠ ã€‚å¤§å¤šæ•°é¢„è®­ç»ƒçš„LLMsæ— æ³•æ¨å¹¿åˆ°å…¶åŸå§‹è®­ç»ƒåºåˆ—é•¿åº¦ä¹‹å¤–ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½œè€…æå‡ºäº†InfiniteHiPï¼Œä¸€ç§æ–°çš„LLMæ¨ç†æ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€æ¶ˆé™¤ä¸ç›¸å…³çš„ä¸Šä¸‹æ–‡æ ‡è®°æ¥åŠ é€Ÿå¤„ç†ã€‚è¯¥æ–¹æ³•è¿˜å…è®¸é€šè¿‡é€‰æ‹©æ€§åº”ç”¨å„ç§RoPEè°ƒæ•´æ–¹æ³•æ¥æ¨å¹¿åˆ°æ›´é•¿çš„åºåˆ—ã€‚æ­¤å¤–ï¼Œä½œè€…åœ¨æ¨ç†è¿‡ç¨‹ä¸­å°†é”®å€¼ç¼“å­˜å¸è½½åˆ°ä¸»æœºå†…å­˜ï¼Œæ˜¾è‘—å‡å°‘äº†GPUå†…å­˜å‹åŠ›ã€‚ç»“æœæ˜¯ï¼ŒInfiniteHiPèƒ½å¤Ÿåœ¨å•ä¸ªL40s 48GB GPUä¸Šå¤„ç†å¤šè¾¾300ä¸‡ä¸ªæ ‡è®°ï¼Œè€Œä¸ä¼šæ°¸ä¹…ä¸¢å¤±ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¹¶ä¸”åœ¨100ä¸‡ä¸ªæ ‡è®°çš„ä¸Šä¸‹æ–‡ä¸­å®ç°äº†18.95å€çš„æ³¨æ„åŠ›è§£ç åŠ é€Ÿã€‚",
        "title": "InfiniteHiP: Extending Language Model Context Up to 3 Million Tokens on a Single GPU",
        "pinyin": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†ç°ä»£å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¤„ç†éå¸¸é•¿çš„ä¸Šä¸‹æ–‡æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ¨ç†é€Ÿåº¦å˜æ…¢å’Œå†…å­˜æˆæœ¬å¢åŠ ã€‚å¤§å¤šæ•°é¢„è®­ç»ƒçš„LLMsæ— æ³•æ¨å¹¿åˆ°å…¶åŸå§‹è®­ç»ƒåºåˆ—é•¿åº¦ä¹‹å¤–ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½œè€…æå‡ºäº†InfiniteHiPï¼Œä¸€ç§æ–°çš„LLMæ¨ç†æ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€æ¶ˆé™¤ä¸ç›¸å…³çš„ä¸Šä¸‹æ–‡æ ‡è®°æ¥åŠ é€Ÿå¤„ç†ã€‚è¯¥æ–¹æ³•è¿˜å…è®¸é€šè¿‡é€‰æ‹©æ€§åº”ç”¨å„ç§RoPEè°ƒæ•´æ–¹æ³•æ¥æ¨å¹¿åˆ°æ›´é•¿çš„åºåˆ—ã€‚æ­¤å¤–ï¼Œä½œè€…åœ¨æ¨ç†è¿‡ç¨‹ä¸­å°†é”®å€¼ç¼“å­˜å¸è½½åˆ°ä¸»æœºå†…å­˜ï¼Œæ˜¾è‘—å‡å°‘äº†GPUå†…å­˜å‹åŠ›ã€‚ç»“æœæ˜¯ï¼ŒInfiniteHiPèƒ½å¤Ÿåœ¨å•ä¸ªL40s 48GB GPUä¸Šå¤„ç†å¤šè¾¾300ä¸‡ä¸ªæ ‡è®°ï¼Œè€Œä¸ä¼šæ°¸ä¹…ä¸¢å¤±ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¹¶ä¸”åœ¨100ä¸‡ä¸ªæ ‡è®°çš„ä¸Šä¸‹æ–‡ä¸­å®ç°äº†18.95å€çš„æ³¨æ„åŠ›è§£ç åŠ é€Ÿã€‚\n\nzhÃ¨ piÄn wÃ©n zhÄng tÇo lÃ¹n le xiÃ n dÃ i dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng (LLMs) chÇ” lÇ fÄ“i chÃ¡ng chÃ¡ng de shÃ ng xiÃ  wÃ©n shÃ­ miÃ n lÃ­n de tiÇo zhÃ n, bÄo kuÃ² tuÄ« lÇ sÃ¹ dÃ¹ biÃ n mÃ n hÃ© nÃ¨i cÃºn chÃ©ng bÃ¨n zÄ“ng jiÄ. dÃ  duÅ shÃ¹ yÃ¹ xÃ¹n liÃ n de LLMs wÃº fÇ tuÄ« guÇng dÃ o qÃ­ yuÃ¡n shÇ xÃ¹n liÃ n xÃ¹ liÃ¨ chÃ¡ng dÃ¹ zhÄ« wÃ i. wÃ¨i le jiÄ› juÃ© zhÃ¨ ge wÃ¨n tÃ­, zuÃ² zhÄ› tÃ­ chÅ« le InfiniteHiP, yÄ« zhÇ’ng xÄ«n de LLM tuÄ« lÇ kuÃ ng jiÃ , tÅng guÃ² dÃ²ng tÃ i xiÄo chÃº bÃ¹ xiÄng guÄn de shÃ ng xiÃ  wÃ©n biÄo jÃ¬ lÃ¡i jiÄ sÃ¹ chÇ” lÇ. gÇi fÇng huÃ² yÇ”n xÃ¹ tÅng guÃ² xuÇn zÃ© xÃ¬ng yÃ¬ng yÃ²ng gÃ¨ zhÇ’ng RoPE tiÃ¡o zhÄ›ng fÇng fÇ lÃ¡i tuÄ« guÇng dÃ o gÃ¨ng chÃ¡ng de xÃ¹ liÃ¨. cÇ wÃ i, zuÃ² zhÄ› zÃ i tuÄ« lÇ guÃ² chÃ©ng zhÅng jiÄng jiÃ n zhÃ­ bÃ¨i huÃ n cÃºn dÃ o zhÇ” jÄ« nÃ¨i cÃºn, xiÇn zhÃ¹ jiÇn shÇo le GPU nÃ¨i cÃºn yÄ lÃ¬. jiÃ© guÇ’ shÃ¬, InfiniteHiP nÃ©ng gÃ²u zÃ i dÄn gÃ¨ L40s 48GB GPU shÃ ng chÇ” lÇ duÅ dÃ  300 wÃ n gÃ¨ biÄo jÃ¬, Ã©r bÃ¹ huÃ¬ yÇ’ng jiÇ” diÅ« shÄ« shÃ ng xiÃ  wÃ©n xÃ¬n xÄ«, bÃ¬ng qiÄ› zÃ i 100 wÃ n gÃ¨ biÄo jÃ¬ de shÃ ng xiÃ  wÃ©n zhÅng shÃ­ xiÃ n le 18.95 bÃ¨i de zhÃ¹ yÃ¬ jiÄ› mÇ jiÄ sÃ¹.",
        "vocab": "[\n    {\"word\": \"è®¨è®º\", \"pinyin\": \"tÇo lÃ¹n\", \"trans\": \"discuss\"},\n    {\"word\": \"ç°ä»£\", \"pinyin\": \"xiÃ n dÃ i\", \"trans\": \"modern\"},\n    {\"word\": \"å¤§è¯­è¨€æ¨¡å‹\", \"pinyin\": \"dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng\", \"trans\": \"large language model\"},\n    {\"word\": \"å¤„ç†\", \"pinyin\": \"chÇ” lÇ\", \"trans\": \"process\"},\n    {\"word\": \"éå¸¸\", \"pinyin\": \"fÄ“i chÃ¡ng\", \"trans\": \"very\"},\n    {\"word\": \"ä¸Šä¸‹æ–‡\", \"pinyin\": \"shÃ ng xiÃ  wÃ©n\", \"trans\": \"context\"},\n    {\"word\": \"é¢ä¸´\", \"pinyin\": \"miÃ n lÃ­n\", \"trans\": \"face\"},\n    {\"word\": \"æŒ‘æˆ˜\", \"pinyin\": \"tiÇo zhÃ n\", \"trans\": \"challenge\"},\n    {\"word\": \"æ¨ç†\", \"pinyin\": \"tuÄ« lÇ\", \"trans\": \"inference\"},\n    {\"word\": \"é€Ÿåº¦\", \"pinyin\": \"sÃ¹ dÃ¹\", \"trans\": \"speed\"},\n    {\"word\": \"å˜æ…¢\", \"pinyin\": \"biÃ n mÃ n\", \"trans\": \"slow down\"},\n    {\"word\": \"å†…å­˜\", \"pinyin\": \"nÃ¨i cÃºn\", \"trans\": \"memory\"},\n    {\"word\": \"æˆæœ¬\", \"pinyin\": \"chÃ©ng bÄ›n\", \"trans\": \"cost\"},\n    {\"word\": \"å¢åŠ \", \"pinyin\": \"zÄ“ng jiÄ\", \"trans\": \"increase\"},\n    {\"word\": \"é¢„è®­ç»ƒ\", \"pinyin\": \"yÃ¹ xÃ¹n liÃ n\", \"trans\": \"pre-trained\"},\n    {\"word\": \"æ— æ³•\", \"pinyin\": \"wÃº fÇ\", \"trans\": \"unable\"},\n    {\"word\": \"æ¨å¹¿\", \"pinyin\": \"tuÄ« guÇng\", \"trans\": \"extend\"},\n    {\"word\": \"åŸå§‹\", \"pinyin\": \"yuÃ¡n shÇ\", \"trans\": \"original\"},\n    {\"word\": \"è®­ç»ƒ\", \"pinyin\": \"xÃ¹n liÃ n\", \"trans\": \"training\"},\n    {\"word\": \"åºåˆ—\", \"pinyin\": \"xÃ¹ liÃ¨\", \"trans\": \"sequence\"},\n    {\"word\": \"é•¿åº¦\", \"pinyin\": \"chÃ¡ng dÃ¹\", \"trans\": \"length\"},\n    {\"word\": \"ä¹‹å¤–\", \"pinyin\": \"zhÄ« wÃ i\", \"trans\": \"beyond\"},\n    {\"word\": \"æå‡º\", \"pinyin\": \"tÃ­ chÅ«\", \"trans\": \"propose\"},\n    {\"word\": \"æ¡†æ¶\", \"pinyin\": \"kuÃ ng jiÃ \", \"trans\": \"framework\"},\n    {\"word\": \"åŠ¨æ€\", \"pinyin\": \"dÃ²ng tÃ i\", \"trans\": \"dynamic\"},\n    {\"word\": \"æ¶ˆé™¤\", \"pinyin\": \"xiÄo chÃº\", \"trans\": \"eliminate\"},\n    {\"word\": \"ä¸ç›¸å…³\", \"pinyin\": \"bÃ¹ xiÄng guÄn\", \"trans\": \"irrelevant\"},\n    {\"word\": \"æ ‡è®°\", \"pinyin\": \"biÄo jÃ¬\", \"trans\": \"token\"},\n    {\"word\": \"åŠ é€Ÿ\", \"pinyin\": \"jiÄ sÃ¹\", \"trans\": \"accelerate\"},\n    {\"word\": \"æ–¹æ³•\", \"pinyin\": \"fÄng fÇ\", \"trans\": \"method\"},\n    {\"word\": \"é€‰æ‹©æ€§\", \"pinyin\": \"xuÇn zÃ© xÃ¬ng\", \"trans\": \"selective\"},\n    {\"word\": \"åº”ç”¨\", \"pinyin\": \"yÃ¬ng yÃ²ng\", \"trans\": \"apply\"},\n    {\"word\": \"å„ç§\", \"pinyin\": \"gÃ¨ zhÇ’ng\", \"trans\": \"various\"},\n    {\"word\": \"RoPE\", \"pinyin\": \"RoPE\", \"trans\": \"RoPE\"},\n    {\"word\": \"è°ƒæ•´\", \"pinyin\": \"tiÃ¡o zhÄ›ng\", \"trans\": \"adjust\"},\n    {\"word\": \"é”®å€¼\", \"pinyin\": \"jiÃ n zhÃ­\", \"trans\": \"key-value\"},\n    {\"word\": \"ç¼“å­˜\", \"pinyin\": \"huÇn cÃºn\", \"trans\": \"cache\"},\n    {\"word\": \"å¸è½½\", \"pinyin\": \"xiÃ¨ zÃ i\", \"trans\": \"offload\"},\n    {\"word\": \"ä¸»æœº\", \"pinyin\": \"zhÇ” jÄ«\", \"trans\": \"host\"},\n    {\"word\": \"æ˜¾è‘—\", \"pinyin\": \"xiÇn zhÃ¹\", \"trans\": \"significant\"},\n    {\"word\": \"å‡å°‘\", \"pinyin\": \"jiÇn shÇo\", \"trans\": \"reduce\"},\n    {\"word\": \"å‹åŠ›\", \"pinyin\": \"yÄ lÃ¬\", \"trans\": \"pressure\"},\n    {\"word\": \"ç»“æœ\", \"pinyin\": \"jiÃ© guÇ’\", \"trans\": \"result\"},\n    {\"word\": \"èƒ½å¤Ÿ\", \"pinyin\": \"nÃ©ng gÃ²u\", \"trans\": \"be able to\"},\n    {\"word\": \"å•ä¸ª\", \"pinyin\": \"dÄn gÃ¨\", \"trans\": \"single\"},\n    {\"word\": \"L40s\", \"pinyin\": \"L40s\", \"trans\": \"L40s\"},\n    {\"word\": \"48GB\", \"pinyin\": \"48GB\", \"trans\": \"48GB\"},\n    {\"word\": \"GPU\", \"pinyin\": \"GPU\", \"trans\": \"GPU\"},\n    {\"word\": \"æ°¸ä¹…\", \"pinyin\": \"yÇ’ng jiÇ”\", \"trans\": \"permanent\"},\n    {\"word\": \"ä¸¢å¤±\", \"pinyin\": \"diÅ« shÄ«\", \"trans\": \"lose\"},\n    {\"word\": \"ä¿¡æ¯\", \"pinyin\": \"xÃ¬n xÄ«\", \"trans\": \"information\"},\n    {\"word\": \"å®ç°\", \"pinyin\": \"shÃ­ xiÃ n\", \"trans\": \"achieve\"},\n    {\"word\": \"æ³¨æ„åŠ›\", \"pinyin\": \"zhÃ¹ yÃ¬ lÃ¬\", \"trans\": \"attention\"},\n    {\"word\": \"è§£ç \", \"pinyin\": \"jiÄ› mÇ\", \"trans\": \"decode\"},\n    {\"word\": \"åŠ é€Ÿ\", \"pinyin\": \"jiÄ sÃ¹\", \"trans\": \"accelerate\"}\n]",
        "trans": "This article discusses the challenges faced by modern large language models (LLMs) when handling very long contexts, including slower inference speeds and increased memory costs. Most pre-trained LLMs cannot generalize beyond their original training sequence lengths. To address this issue, the authors propose InfiniteHiP, a new LLM inference framework that accelerates processing by dynamically eliminating irrelevant context tokens. This method also allows for generalization to longer sequences by selectively applying various RoPE adjustment methods. Additionally, the authors offload key-value caches to host memory during inference, significantly reducing GPU memory pressure. As a result, InfiniteHiP can process up to 3 million tokens on a single L40s 48GB GPU without permanently losing context information and achieves an 18.95-fold speedup in attention decoding within a context of 1 million tokens.",
        "update_ts": "2025-02-14 09:10"
    }
}