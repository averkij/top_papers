{
    "date": {
        "ru": "23 июня",
        "en": "June 23",
        "zh": "6月23日"
    },
    "time_utc": "2025-06-23 05:15",
    "weekday": 0,
    "issue_id": 4427,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2506.16406",
            "title": "Drag-and-Drop LLMs: Zero-Shot Prompt-to-Weights",
            "url": "https://huggingface.co/papers/2506.16406",
            "abstract": "Drag-and-Drop LLMs generate task-specific parameters through prompt-conditioned parameter generation, achieving significant efficiency gains and cross-domain generalization without per-task training.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern Parameter-Efficient Fine-Tuning (PEFT) methods such as low-rank adaptation (LoRA) reduce the cost of customizing large language models (LLMs), yet still require a separate optimization run for every downstream dataset. We introduce Drag-and-Drop LLMs (\\textit{DnD)}, a prompt-conditioned parameter generator that eliminates per-task training by mapping a handful of unlabeled task prompts directly to LoRA weight updates. A lightweight text encoder distills each prompt batch into condition embeddings, which are then transformed by a cascaded hyper-convolutional decoder into the full set of LoRA matrices. Once trained in a diverse collection of prompt-checkpoint pairs, DnD produces task-specific parameters in seconds, yielding i) up to 12,000times lower overhead than full fine-tuning, ii) average gains up to 30\\% in performance over the strongest training LoRAs on unseen common-sense reasoning, math, coding, and multimodal benchmarks, and iii) robust cross-domain generalization despite never seeing the target data or labels. Our results demonstrate that prompt-conditioned parameter generation is a viable alternative to gradient-based adaptation for rapidly specializing LLMs. Our project is available at https://jerryliang24.github.io/DnD{https://jerryliang24.github.io/DnD}.",
            "score": 23,
            "issue_id": 4426,
            "pub_date": "2025-06-19",
            "pub_date_card": {
                "ru": "19 июня",
                "en": "June 19",
                "zh": "6月19日"
            },
            "hash": "7ec3cdc6c01d1d79",
            "authors": [
                "Zhiyuan Liang",
                "Dongwen Tang",
                "Yuhao Zhou",
                "Xuanlei Zhao",
                "Mingjia Shi",
                "Wangbo Zhao",
                "Zekai Li",
                "Peihao Wang",
                "Konstantin Schürholt",
                "Damian Borth",
                "Michael M. Bronstein",
                "Yang You",
                "Zhangyang Wang",
                "Kai Wang"
            ],
            "affiliations": [
                "National University of Singapore",
                "Oxford University",
                "UT Austin",
                "University of St. Gallen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.16406.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#multimodal",
                    "#training",
                    "#optimization"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Революция в настройке ЯМ: быстрая адаптация без дополнительного обучения",
                    "desc": "Статья представляет новый метод Drag-and-Drop LLMs (DnD) для эффективной настройки больших языковых моделей. DnD использует генератор параметров, обусловленный промптами, для создания обновлений весов LoRA без необходимости обучения для каждой задачи. Этот подход обеспечивает значительное снижение вычислительных затрат и улучшение производительности на различных тестах по сравнению с традиционными методами тонкой настройки. DnD также демонстрирует надежную межпредметную генерализацию без доступа к целевым данным или меткам."
                },
                "en": {
                    "title": "Efficient Task-Specific Adaptation with Drag-and-Drop LLMs",
                    "desc": "The paper introduces Drag-and-Drop LLMs (DnD), a novel approach that generates task-specific parameters without the need for separate training on each task. By using prompt-conditioned parameter generation, DnD maps unlabeled task prompts directly to updates for low-rank adaptation (LoRA) weights. This method significantly reduces the computational overhead, achieving up to 12,000 times less than traditional fine-tuning while improving performance by an average of 30% on various benchmarks. DnD demonstrates strong cross-domain generalization, effectively adapting to new tasks without prior exposure to their data or labels."
                },
                "zh": {
                    "title": "拖放大语言模型：高效的任务特定参数生成",
                    "desc": "本文介绍了一种新的机器学习方法，称为拖放大语言模型（DnD），它通过提示条件生成参数，消除了每个任务的训练需求。DnD使用轻量级文本编码器将提示批次提炼为条件嵌入，并通过级联超卷积解码器转换为完整的LoRA矩阵。与传统的参数高效微调方法相比，DnD在性能上有显著提升，并且在未见过的任务上也能保持良好的泛化能力。该方法在多个基准测试中表现出色，证明了提示条件参数生成是快速专门化大语言模型的有效替代方案。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.09049",
            "title": "VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement\n  Learning",
            "url": "https://huggingface.co/papers/2506.09049",
            "abstract": "VIKI-Bench and VIKI-R provide a benchmark and framework for evaluating and improving visual-driven cooperation among diverse embodied agents using vision-language models and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Coordinating multiple embodied agents in dynamic environments remains a core challenge in artificial intelligence, requiring both perception-driven reasoning and scalable cooperation strategies. While recent works have leveraged large language models (LLMs) for multi-agent planning, a few have begun to explore vision-language models (VLMs) for visual reasoning. However, these VLM-based approaches remain limited in their support for diverse embodiment types. In this work, we introduce VIKI-Bench, the first hierarchical benchmark tailored for embodied multi-agent cooperation, featuring three structured levels: agent activation, task planning, and trajectory perception. VIKI-Bench includes diverse robot embodiments, multi-view visual observations, and structured supervision signals to evaluate reasoning grounded in visual inputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a two-stage framework that fine-tunes a pretrained vision-language model (VLM) using Chain-of-Thought annotated demonstrations, followed by reinforcement learning under multi-level reward signals. Our extensive experiments show that VIKI-R significantly outperforms baselines method across all task levels. Furthermore, we show that reinforcement learning enables the emergence of compositional cooperation patterns among heterogeneous agents. Together, VIKI-Bench and VIKI-R offer a unified testbed and method for advancing multi-agent, visual-driven cooperation in embodied AI systems.",
            "score": 21,
            "issue_id": 4425,
            "pub_date": "2025-06-10",
            "pub_date_card": {
                "ru": "10 июня",
                "en": "June 10",
                "zh": "6月10日"
            },
            "hash": "91caa1a0b1cb2b54",
            "authors": [
                "Li Kang",
                "Xiufeng Song",
                "Heng Zhou",
                "Yiran Qin",
                "Jie Yang",
                "Xiaohong Liu",
                "Philip Torr",
                "Lei Bai",
                "Zhenfei Yin"
            ],
            "affiliations": [
                "Shanghai Artificial Intelligence Laboratory",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong, Shenzhen",
                "University of Oxford",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.09049.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#agents",
                    "#games",
                    "#rl",
                    "#benchmark",
                    "#cv"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Визуальная кооперация агентов с помощью глубокого обучения",
                    "desc": "VIKI-Bench представляет собой иерархический бенчмарк для оценки кооперации между воплощенными агентами с использованием визуально-языковых моделей. VIKI-R - это двухэтапный фреймворк, который дообучает предобученную визуально-языковую модель с помощью аннотированных демонстраций и обучения с подкреплением. Эксперименты показывают, что VIKI-R значительно превосходит базовые методы на всех уровнях задач. Обучение с подкреплением позволяет появиться композиционным паттернам кооперации между разнородными агентами."
                },
                "en": {
                    "title": "Enhancing Multi-Agent Cooperation with VIKI-Bench and VIKI-R",
                    "desc": "This paper introduces VIKI-Bench and VIKI-R, a benchmark and framework designed to enhance cooperation among various embodied agents using vision-language models and reinforcement learning. VIKI-Bench is structured into three levels: agent activation, task planning, and trajectory perception, allowing for comprehensive evaluation of visual reasoning in multi-agent scenarios. VIKI-R fine-tunes a pretrained vision-language model with Chain-of-Thought demonstrations and applies reinforcement learning to optimize performance across different tasks. The results demonstrate that VIKI-R significantly improves cooperation patterns among diverse agents, showcasing the potential of visual-driven strategies in embodied AI."
                },
                "zh": {
                    "title": "VIKI-Bench与VIKI-R：推动具身智能体的视觉驱动合作",
                    "desc": "VIKI-Bench和VIKI-R是用于评估和改善多样化具身智能体之间视觉驱动合作的基准和框架。该研究提出了一个分层基准，包含代理激活、任务规划和轨迹感知三个结构化层次。VIKI-R是一个两阶段框架，通过链式思维注释示例微调预训练的视觉语言模型，并在多层次奖励信号下进行强化学习。实验结果表明，VIKI-R在所有任务层次上显著优于基线方法，并且强化学习促进了异构智能体之间的组合合作模式。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.17206",
            "title": "DreamCube: 3D Panorama Generation via Multi-plane Synchronization",
            "url": "https://huggingface.co/papers/2506.17206",
            "abstract": "Multi-plane synchronization extends 2D foundation models to 3D panorama generation, introducing DreamCube to achieve diverse appearances and accurate geometry.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D panorama synthesis is a promising yet challenging task that demands high-quality and diverse visual appearance and geometry of the generated omnidirectional content. Existing methods leverage rich image priors from pre-trained 2D foundation models to circumvent the scarcity of 3D panoramic data, but the incompatibility between 3D panoramas and 2D single views limits their effectiveness. In this work, we demonstrate that by applying multi-plane synchronization to the operators from 2D foundation models, their capabilities can be seamlessly extended to the omnidirectional domain. Based on this design, we further introduce DreamCube, a multi-plane RGB-D diffusion model for 3D panorama generation, which maximizes the reuse of 2D foundation model priors to achieve diverse appearances and accurate geometry while maintaining multi-view consistency. Extensive experiments demonstrate the effectiveness of our approach in panoramic image generation, panoramic depth estimation, and 3D scene generation.",
            "score": 9,
            "issue_id": 4425,
            "pub_date": "2025-06-20",
            "pub_date_card": {
                "ru": "20 июня",
                "en": "June 20",
                "zh": "6月20日"
            },
            "hash": "bbec38fbe4e9ddd8",
            "authors": [
                "Yukun Huang",
                "Yanning Zhou",
                "Jianan Wang",
                "Kaiyi Huang",
                "Xihui Liu"
            ],
            "affiliations": [
                "Astribot",
                "Tencent",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.17206.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#diffusion",
                    "#3d",
                    "#cv"
                ],
                "emoji": "🌐",
                "ru": {
                    "title": "Многоплоскостная синхронизация: мост между 2D и 3D в генерации панорам",
                    "desc": "Эта статья представляет новый подход к генерации трехмерных панорам с использованием многоплоскостной синхронизации двумерных фундаментальных моделей. Авторы предлагают модель DreamCube, которая максимально использует преимущества двумерных моделей для создания разнообразных и геометрически точных трехмерных панорам. Метод решает проблему несовместимости между трехмерными панорамами и двумерными изображениями, расширяя возможности существующих алгоритмов. Эксперименты показывают эффективность подхода в генерации панорамных изображений, оценке глубины и создании трехмерных сцен."
                },
                "en": {
                    "title": "DreamCube: Bridging 2D and 3D for Stunning Panoramas",
                    "desc": "This paper presents a method called DreamCube that enhances 2D foundation models for generating 3D panoramas. It addresses the challenge of creating high-quality and diverse omnidirectional content by using multi-plane synchronization. This technique allows the model to effectively utilize existing 2D image data to improve the geometry and appearance of 3D images. The results show that DreamCube can generate consistent panoramic images and accurately estimate depth, making it a significant advancement in 3D scene generation."
                },
                "zh": {
                    "title": "多平面同步，开启三维全景新视界",
                    "desc": "本论文提出了一种名为DreamCube的多平面同步方法，旨在将二维基础模型扩展到三维全景生成。该方法通过利用预训练的二维模型的丰富图像先验，克服了三维全景数据稀缺的问题。我们的方法能够实现多样化的视觉效果和准确的几何形状，同时保持多视图的一致性。实验结果表明，我们的技术在全景图像生成、深度估计和三维场景生成方面表现出色。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.16035",
            "title": "Vision-Guided Chunking Is All You Need: Enhancing RAG with Multimodal\n  Document Understanding",
            "url": "https://huggingface.co/papers/2506.16035",
            "abstract": "A novel multimodal document chunking approach using Large Multimodal Models (LMMs) enhances RAG performance by accurately processing complex PDF documents, including multi-page tables and embedded visuals.  \t\t\t\t\tAI-generated summary \t\t\t\t Retrieval-Augmented Generation (RAG) systems have revolutionized information retrieval and question answering, but traditional text-based chunking methods struggle with complex document structures, multi-page tables, embedded figures, and contextual dependencies across page boundaries. We present a novel multimodal document chunking approach that leverages Large Multimodal Models (LMMs) to process PDF documents in batches while maintaining semantic coherence and structural integrity. Our method processes documents in configurable page batches with cross-batch context preservation, enabling accurate handling of tables spanning multiple pages, embedded visual elements, and procedural content. We evaluate our approach on a curated dataset of PDF documents with manually crafted queries, demonstrating improvements in chunk quality and downstream RAG performance. Our vision-guided approach achieves better accuracy compared to traditional vanilla RAG systems, with qualitative analysis showing superior preservation of document structure and semantic coherence.",
            "score": 8,
            "issue_id": 4427,
            "pub_date": "2025-06-19",
            "pub_date_card": {
                "ru": "19 июня",
                "en": "June 19",
                "zh": "6月19日"
            },
            "hash": "08abcd6ec6f0d9e1",
            "authors": [
                "Vishesh Tripathi",
                "Tanmay Odapally",
                "Indraneel Das",
                "Uday Allu",
                "Biddwan Ahmed"
            ],
            "affiliations": [
                "AI Research Team, Yellow.ai"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.16035.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#long_context",
                    "#optimization",
                    "#rag",
                    "#multimodal"
                ],
                "emoji": "📄",
                "ru": {
                    "title": "Умное разбиение документов для улучшения RAG-систем",
                    "desc": "Предложен новый подход к разбиению сложных PDF-документов на фрагменты с использованием мультимодальных языковых моделей (LMM). Метод обрабатывает документы партиями страниц, сохраняя контекст между партиями, что позволяет корректно обрабатывать многостраничные таблицы и встроенные визуальные элементы. Эксперименты показали улучшение качества фрагментов и повышение эффективности систем генерации с извлечением (RAG). Новый подход превосходит традиционные методы RAG в точности и сохранении структуры документа."
                },
                "en": {
                    "title": "Enhancing RAG with Multimodal Document Chunking",
                    "desc": "This paper introduces a new method for breaking down complex PDF documents using Large Multimodal Models (LMMs). Traditional methods struggle with intricate layouts, such as multi-page tables and embedded visuals, which can lead to poor information retrieval. The proposed approach processes documents in batches while keeping the context intact across pages, ensuring that the meaning and structure are preserved. Evaluations show that this method significantly enhances the quality of document chunks and improves the performance of Retrieval-Augmented Generation (RAG) systems."
                },
                "zh": {
                    "title": "多模态文档分块，提升信息检索新高度",
                    "desc": "本文提出了一种新颖的多模态文档分块方法，利用大型多模态模型（LMMs）来处理复杂的PDF文档。该方法能够有效处理多页表格和嵌入视觉元素，同时保持语义一致性和结构完整性。通过配置页面批次进行处理，我们的方法能够跨批次保留上下文，从而提高了分块质量和后续的检索增强生成（RAG）性能。实验结果表明，与传统的RAG系统相比，我们的方法在准确性和文档结构保留方面表现更优。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.17201",
            "title": "Hunyuan-GameCraft: High-dynamic Interactive Game Video Generation with\n  Hybrid History Condition",
            "url": "https://huggingface.co/papers/2506.17201",
            "abstract": "Hunyuan-GameCraft is a novel framework for high-dynamic interactive video generation in game environments that addresses limitations in dynamics, generality, and efficiency through unified input representation, hybrid history-conditioned training, and model distillation.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in diffusion-based and controllable video generation have enabled high-quality and temporally coherent video synthesis, laying the groundwork for immersive interactive gaming experiences. However, current methods face limitations in dynamics, generality, long-term consistency, and efficiency, which limit the ability to create various gameplay videos. To address these gaps, we introduce Hunyuan-GameCraft, a novel framework for high-dynamic interactive video generation in game environments. To achieve fine-grained action control, we unify standard keyboard and mouse inputs into a shared camera representation space, facilitating smooth interpolation between various camera and movement operations. Then we propose a hybrid history-conditioned training strategy that extends video sequences autoregressively while preserving game scene information. Additionally, to enhance inference efficiency and playability, we achieve model distillation to reduce computational overhead while maintaining consistency across long temporal sequences, making it suitable for real-time deployment in complex interactive environments. The model is trained on a large-scale dataset comprising over one million gameplay recordings across over 100 AAA games, ensuring broad coverage and diversity, then fine-tuned on a carefully annotated synthetic dataset to enhance precision and control. The curated game scene data significantly improves the visual fidelity, realism and action controllability. Extensive experiments demonstrate that Hunyuan-GameCraft significantly outperforms existing models, advancing the realism and playability of interactive game video generation.",
            "score": 4,
            "issue_id": 4425,
            "pub_date": "2025-06-20",
            "pub_date_card": {
                "ru": "20 июня",
                "en": "June 20",
                "zh": "6月20日"
            },
            "hash": "def0daf064c03fe6",
            "authors": [
                "Jiaqi Li",
                "Junshu Tang",
                "Zhiyong Xu",
                "Longhuang Wu",
                "Yuan Zhou",
                "Shuai Shao",
                "Tianbao Yu",
                "Zhiguo Cao",
                "Qinglin Lu"
            ],
            "affiliations": [
                "Huazhong University of Science and Technology",
                "Tencent Hunyuan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.17201.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#video",
                    "#diffusion",
                    "#synthetic",
                    "#dataset",
                    "#games",
                    "#training"
                ],
                "emoji": "🎮",
                "ru": {
                    "title": "Революция в генерации интерактивного игрового видео",
                    "desc": "Hunyuan-GameCraft - это новая система для генерации интерактивного видео в игровых средах с высокой динамикой. Она решает проблемы ограничений в динамике, универсальности и эффективности с помощью унифицированного представления ввода, гибридного обучения с учетом истории и дистилляции модели. Система обучается на масштабном наборе данных из более чем миллиона записей геймплея из более 100 ААА-игр, а затем дообучается на тщательно размеченном синтетическом наборе данных для повышения точности и контроля. Эксперименты показывают, что Hunyuan-GameCraft значительно превосходит существующие модели, повышая реалистичность и играбельность интерактивной генерации игрового видео."
                },
                "en": {
                    "title": "Revolutionizing Interactive Video Generation in Gaming",
                    "desc": "Hunyuan-GameCraft is a new framework designed for generating interactive videos in gaming environments, overcoming issues related to dynamics, generality, and efficiency. It uses a unified input representation that combines keyboard and mouse controls, allowing for smooth transitions in camera and movement actions. The framework employs a hybrid history-conditioned training method to extend video sequences while keeping important game scene details intact. Additionally, model distillation is utilized to improve computational efficiency, making it suitable for real-time applications in complex gaming scenarios."
                },
                "zh": {
                    "title": "Hunyuan-GameCraft：提升互动游戏视频生成的真实感与可玩性",
                    "desc": "Hunyuan-GameCraft是一个新颖的框架，旨在解决游戏环境中高动态互动视频生成的局限性。它通过统一输入表示、混合历史条件训练和模型蒸馏来提高动态性、通用性和效率。该框架能够实现精细的动作控制，并在复杂的互动环境中保持长时间的一致性。经过大规模数据集的训练，Hunyuan-GameCraft在生成互动游戏视频的真实感和可玩性方面显著优于现有模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.16504",
            "title": "Hunyuan3D 2.5: Towards High-Fidelity 3D Assets Generation with Ultimate\n  Details",
            "url": "https://huggingface.co/papers/2506.16504",
            "abstract": "Hunyuan3D 2.5, a suite of 3D diffusion models, advances shape and texture generation with a new LATTICE model and physical-based rendering in a multi-view architecture.  \t\t\t\t\tAI-generated summary \t\t\t\t In this report, we present Hunyuan3D 2.5, a robust suite of 3D diffusion models aimed at generating high-fidelity and detailed textured 3D assets. Hunyuan3D 2.5 follows two-stages pipeline of its previous version Hunyuan3D 2.0, while demonstrating substantial advancements in both shape and texture generation. In terms of shape generation, we introduce a new shape foundation model -- LATTICE, which is trained with scaled high-quality datasets, model-size, and compute. Our largest model reaches 10B parameters and generates sharp and detailed 3D shape with precise image-3D following while keeping mesh surface clean and smooth, significantly closing the gap between generated and handcrafted 3D shapes. In terms of texture generation, it is upgraded with phyiscal-based rendering (PBR) via a novel multi-view architecture extended from Hunyuan3D 2.0 Paint model. Our extensive evaluation shows that Hunyuan3D 2.5 significantly outperforms previous methods in both shape and end-to-end texture generation.",
            "score": 4,
            "issue_id": 4424,
            "pub_date": "2025-06-19",
            "pub_date_card": {
                "ru": "19 июня",
                "en": "June 19",
                "zh": "6月19日"
            },
            "hash": "7ea16d5712b67fcc",
            "authors": [
                "Zeqiang Lai",
                "Yunfei Zhao",
                "Haolin Liu",
                "Zibo Zhao",
                "Qingxiang Lin",
                "Huiwen Shi",
                "Xianghui Yang",
                "Mingxin Yang",
                "Shuhui Yang",
                "Yifei Feng",
                "Sheng Zhang",
                "Xin Huang",
                "Di Luo",
                "Fan Yang",
                "Fang Yang",
                "Lifu Wang",
                "Sicong Liu",
                "Yixuan Tang",
                "Yulin Cai",
                "Zebin He",
                "Tian Liu",
                "Yuhong Liu",
                "Jie Jiang",
                "Linus",
                "Jingwei Huang",
                "Chunchao Guo"
            ],
            "affiliations": [
                "Tencent"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.16504.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#3d"
                ],
                "emoji": "🧊",
                "ru": {
                    "title": "Новый уровень 3D-генерации: точные формы и реалистичные текстуры",
                    "desc": "Hunyuan3D 2.5 представляет собой набор моделей диффузии для 3D-генерации, улучшающий создание форм и текстур. Ключевое нововведение - модель LATTICE для генерации форм, обученная на масштабных высококачественных датасетах. Для генерации текстур используется физически корректный рендеринг (PBR) в многоракурсной архитектуре. Согласно оценкам, Hunyuan3D 2.5 значительно превосходит предыдущие методы в генерации как форм, так и текстур."
                },
                "en": {
                    "title": "Revolutionizing 3D Asset Generation with Hunyuan3D 2.5",
                    "desc": "Hunyuan3D 2.5 is a suite of advanced 3D diffusion models designed to enhance the generation of high-quality 3D shapes and textures. It introduces the LATTICE model, a new foundation for shape generation that utilizes large, high-quality datasets and boasts a model size of up to 10 billion parameters. This model produces sharp and detailed 3D shapes while maintaining clean mesh surfaces, bridging the gap between generated and handcrafted designs. Additionally, the suite incorporates physical-based rendering in a multi-view architecture to improve texture generation, demonstrating significant improvements over previous versions."
                },
                "zh": {
                    "title": "Hunyuan3D 2.5：形状与纹理生成的新突破",
                    "desc": "Hunyuan3D 2.5 是一套先进的 3D 扩散模型，旨在生成高保真和细致的纹理 3D 资产。它在形状生成方面引入了新的基础模型 LATTICE，经过大规模高质量数据集的训练，能够生成清晰且细致的 3D 形状。纹理生成方面，采用了基于物理的渲染（PBR）技术，结合了多视角架构，显著提升了纹理质量。综合评估表明，Hunyuan3D 2.5 在形状和纹理生成方面均优于之前的方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.15925",
            "title": "Reranking-based Generation for Unbiased Perspective Summarization",
            "url": "https://huggingface.co/papers/2506.15925",
            "abstract": "Reranking and preference tuning improve the quality of perspective summaries generated by LLMs, as measured by language model-based metrics that outperform traditional ones.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating unbiased summaries in real-world settings such as political perspective summarization remains a crucial application of Large Language Models (LLMs). Yet, existing evaluation frameworks rely on traditional metrics for measuring key attributes such as coverage and faithfulness without verifying their applicability, and efforts to develop improved summarizers are still nascent. We address these gaps by (1) identifying reliable metrics for measuring perspective summary quality, and (2) investigating the efficacy of LLM-based methods beyond zero-shot inference. Namely, we build a test set for benchmarking metric reliability using human annotations and show that traditional metrics underperform compared to language model-based metrics, which prove to be strong evaluators. Using these metrics, we show that reranking-based methods yield strong results, and preference tuning with synthetically generated and reranking-labeled data further boosts performance. Our findings aim to contribute to the reliable evaluation and development of perspective summarization methods.",
            "score": 1,
            "issue_id": 4425,
            "pub_date": "2025-06-19",
            "pub_date_card": {
                "ru": "19 июня",
                "en": "June 19",
                "zh": "6月19日"
            },
            "hash": "8cb1a06d9566bcfc",
            "authors": [
                "Narutatsu Ri",
                "Nicholas Deas",
                "Kathleen McKeown"
            ],
            "affiliations": [
                "Department of Computer Science, Columbia University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.15925.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#multimodal",
                    "#dataset",
                    "#interpretability",
                    "#alignment",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Улучшение генерации обзоров с разных точек зрения с помощью ранжирования и настройки LLM",
                    "desc": "Исследование посвящено улучшению качества генерации кратких обзоров с учетом различных точек зрения с помощью больших языковых моделей (LLM). Авторы разработали надежные метрики для оценки качества таких обзоров, основанные на языковых моделях. Они показали, что методы ранжирования и настройки предпочтений с использованием синтетических данных значительно повышают эффективность генерации. Результаты исследования вносят вклад в развитие методов надежной оценки и создания систем суммаризации с учетом различных перспектив."
                },
                "en": {
                    "title": "Enhancing Perspective Summaries with Reranking and Preference Tuning",
                    "desc": "This paper discusses how to improve the quality of summaries generated by Large Language Models (LLMs) for political perspectives. It highlights the limitations of traditional evaluation metrics, which do not effectively measure important aspects like coverage and faithfulness. The authors propose new, reliable metrics based on language models and demonstrate that these outperform traditional methods. They also show that using reranking and preference tuning techniques can significantly enhance the performance of LLM-generated summaries."
                },
                "zh": {
                    "title": "提升观点摘要质量的关键在于重排序与偏好调优",
                    "desc": "本文探讨了如何提高大型语言模型（LLMs）生成的观点摘要的质量。我们发现，传统的评估指标在测量摘要的覆盖率和忠实度时效果不佳，而基于语言模型的指标表现更为出色。通过建立一个基准测试集并进行人类标注，我们验证了这些新指标的可靠性。最终，我们提出了重排序和偏好调优的方法，显著提升了摘要生成的效果。"
                }
            }
        }
    ],
    "link_prev": "2025-06-20.html",
    "link_next": "2025-06-24.html",
    "link_month": "2025-06.html",
    "short_date_prev": {
        "ru": "20.06",
        "en": "06/20",
        "zh": "6月20日"
    },
    "short_date_next": {
        "ru": "24.06",
        "en": "06/24",
        "zh": "6月24日"
    },
    "categories": {
        "#dataset": 3,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 1,
        "#cv": 2,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 1,
        "#3d": 2,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 3,
        "#robotics": 0,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 1,
        "#reasoning": 1,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 3,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}