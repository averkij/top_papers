{
    "date": {
        "ru": "23 Ğ¸ÑĞ½Ñ",
        "en": "June 23",
        "zh": "6æœˆ23æ—¥"
    },
    "time_utc": "2025-06-23 05:15",
    "weekday": 0,
    "issue_id": 4427,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2506.16406",
            "title": "Drag-and-Drop LLMs: Zero-Shot Prompt-to-Weights",
            "url": "https://huggingface.co/papers/2506.16406",
            "abstract": "Drag-and-Drop LLMs generate task-specific parameters through prompt-conditioned parameter generation, achieving significant efficiency gains and cross-domain generalization without per-task training.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern Parameter-Efficient Fine-Tuning (PEFT) methods such as low-rank adaptation (LoRA) reduce the cost of customizing large language models (LLMs), yet still require a separate optimization run for every downstream dataset. We introduce Drag-and-Drop LLMs (\\textit{DnD)}, a prompt-conditioned parameter generator that eliminates per-task training by mapping a handful of unlabeled task prompts directly to LoRA weight updates. A lightweight text encoder distills each prompt batch into condition embeddings, which are then transformed by a cascaded hyper-convolutional decoder into the full set of LoRA matrices. Once trained in a diverse collection of prompt-checkpoint pairs, DnD produces task-specific parameters in seconds, yielding i) up to 12,000times lower overhead than full fine-tuning, ii) average gains up to 30\\% in performance over the strongest training LoRAs on unseen common-sense reasoning, math, coding, and multimodal benchmarks, and iii) robust cross-domain generalization despite never seeing the target data or labels. Our results demonstrate that prompt-conditioned parameter generation is a viable alternative to gradient-based adaptation for rapidly specializing LLMs. Our project is available at https://jerryliang24.github.io/DnD{https://jerryliang24.github.io/DnD}.",
            "score": 23,
            "issue_id": 4426,
            "pub_date": "2025-06-19",
            "pub_date_card": {
                "ru": "19 Ğ¸ÑĞ½Ñ",
                "en": "June 19",
                "zh": "6æœˆ19æ—¥"
            },
            "hash": "7ec3cdc6c01d1d79",
            "authors": [
                "Zhiyuan Liang",
                "Dongwen Tang",
                "Yuhao Zhou",
                "Xuanlei Zhao",
                "Mingjia Shi",
                "Wangbo Zhao",
                "Zekai Li",
                "Peihao Wang",
                "Konstantin SchÃ¼rholt",
                "Damian Borth",
                "Michael M. Bronstein",
                "Yang You",
                "Zhangyang Wang",
                "Kai Wang"
            ],
            "affiliations": [
                "National University of Singapore",
                "Oxford University",
                "UT Austin",
                "University of St. Gallen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.16406.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#multimodal",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞµ Ğ¯Ğœ: Ğ±Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Drag-and-Drop LLMs (DnD) Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. DnD Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°Ğ¼Ğ¸, Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ²ĞµÑĞ¾Ğ² LoRA Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸. DnD Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½ÑƒÑ Ğ¼ĞµĞ¶Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ±ĞµĞ· Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ° Ğº Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚ĞºĞ°Ğ¼."
                },
                "en": {
                    "title": "Efficient Task-Specific Adaptation with Drag-and-Drop LLMs",
                    "desc": "The paper introduces Drag-and-Drop LLMs (DnD), a novel approach that generates task-specific parameters without the need for separate training on each task. By using prompt-conditioned parameter generation, DnD maps unlabeled task prompts directly to updates for low-rank adaptation (LoRA) weights. This method significantly reduces the computational overhead, achieving up to 12,000 times less than traditional fine-tuning while improving performance by an average of 30% on various benchmarks. DnD demonstrates strong cross-domain generalization, effectively adapting to new tasks without prior exposure to their data or labels."
                },
                "zh": {
                    "title": "æ‹–æ”¾å¤§è¯­è¨€æ¨¡å‹ï¼šé«˜æ•ˆçš„ä»»åŠ¡ç‰¹å®šå‚æ•°ç”Ÿæˆ",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œç§°ä¸ºæ‹–æ”¾å¤§è¯­è¨€æ¨¡å‹ï¼ˆDnDï¼‰ï¼Œå®ƒé€šè¿‡æç¤ºæ¡ä»¶ç”Ÿæˆå‚æ•°ï¼Œæ¶ˆé™¤äº†æ¯ä¸ªä»»åŠ¡çš„è®­ç»ƒéœ€æ±‚ã€‚DnDä½¿ç”¨è½»é‡çº§æ–‡æœ¬ç¼–ç å™¨å°†æç¤ºæ‰¹æ¬¡æç‚¼ä¸ºæ¡ä»¶åµŒå…¥ï¼Œå¹¶é€šè¿‡çº§è”è¶…å·ç§¯è§£ç å™¨è½¬æ¢ä¸ºå®Œæ•´çš„LoRAçŸ©é˜µã€‚ä¸ä¼ ç»Ÿçš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ç›¸æ¯”ï¼ŒDnDåœ¨æ€§èƒ½ä¸Šæœ‰æ˜¾è‘—æå‡ï¼Œå¹¶ä¸”åœ¨æœªè§è¿‡çš„ä»»åŠ¡ä¸Šä¹Ÿèƒ½ä¿æŒè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¯æ˜äº†æç¤ºæ¡ä»¶å‚æ•°ç”Ÿæˆæ˜¯å¿«é€Ÿä¸“é—¨åŒ–å¤§è¯­è¨€æ¨¡å‹çš„æœ‰æ•ˆæ›¿ä»£æ–¹æ¡ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.09049",
            "title": "VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement\n  Learning",
            "url": "https://huggingface.co/papers/2506.09049",
            "abstract": "VIKI-Bench and VIKI-R provide a benchmark and framework for evaluating and improving visual-driven cooperation among diverse embodied agents using vision-language models and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Coordinating multiple embodied agents in dynamic environments remains a core challenge in artificial intelligence, requiring both perception-driven reasoning and scalable cooperation strategies. While recent works have leveraged large language models (LLMs) for multi-agent planning, a few have begun to explore vision-language models (VLMs) for visual reasoning. However, these VLM-based approaches remain limited in their support for diverse embodiment types. In this work, we introduce VIKI-Bench, the first hierarchical benchmark tailored for embodied multi-agent cooperation, featuring three structured levels: agent activation, task planning, and trajectory perception. VIKI-Bench includes diverse robot embodiments, multi-view visual observations, and structured supervision signals to evaluate reasoning grounded in visual inputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a two-stage framework that fine-tunes a pretrained vision-language model (VLM) using Chain-of-Thought annotated demonstrations, followed by reinforcement learning under multi-level reward signals. Our extensive experiments show that VIKI-R significantly outperforms baselines method across all task levels. Furthermore, we show that reinforcement learning enables the emergence of compositional cooperation patterns among heterogeneous agents. Together, VIKI-Bench and VIKI-R offer a unified testbed and method for advancing multi-agent, visual-driven cooperation in embodied AI systems.",
            "score": 21,
            "issue_id": 4425,
            "pub_date": "2025-06-10",
            "pub_date_card": {
                "ru": "10 Ğ¸ÑĞ½Ñ",
                "en": "June 10",
                "zh": "6æœˆ10æ—¥"
            },
            "hash": "91caa1a0b1cb2b54",
            "authors": [
                "Li Kang",
                "Xiufeng Song",
                "Heng Zhou",
                "Yiran Qin",
                "Jie Yang",
                "Xiaohong Liu",
                "Philip Torr",
                "Lei Bai",
                "Zhenfei Yin"
            ],
            "affiliations": [
                "Shanghai Artificial Intelligence Laboratory",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong, Shenzhen",
                "University of Oxford",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.09049.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#agents",
                    "#games",
                    "#rl",
                    "#benchmark",
                    "#cv"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ ĞºĞ¾Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "VIKI-Bench Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ¾Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. VIKI-R - ÑÑ‚Ğ¾ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ VIKI-R Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ²ÑĞµÑ… ÑƒÑ€Ğ¾Ğ²Ğ½ÑÑ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾ÑĞ²Ğ¸Ñ‚ÑŒÑÑ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°Ğ¼ ĞºĞ¾Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Multi-Agent Cooperation with VIKI-Bench and VIKI-R",
                    "desc": "This paper introduces VIKI-Bench and VIKI-R, a benchmark and framework designed to enhance cooperation among various embodied agents using vision-language models and reinforcement learning. VIKI-Bench is structured into three levels: agent activation, task planning, and trajectory perception, allowing for comprehensive evaluation of visual reasoning in multi-agent scenarios. VIKI-R fine-tunes a pretrained vision-language model with Chain-of-Thought demonstrations and applies reinforcement learning to optimize performance across different tasks. The results demonstrate that VIKI-R significantly improves cooperation patterns among diverse agents, showcasing the potential of visual-driven strategies in embodied AI."
                },
                "zh": {
                    "title": "VIKI-Benchä¸VIKI-Rï¼šæ¨åŠ¨å…·èº«æ™ºèƒ½ä½“çš„è§†è§‰é©±åŠ¨åˆä½œ",
                    "desc": "VIKI-Benchå’ŒVIKI-Ræ˜¯ç”¨äºè¯„ä¼°å’Œæ”¹å–„å¤šæ ·åŒ–å…·èº«æ™ºèƒ½ä½“ä¹‹é—´è§†è§‰é©±åŠ¨åˆä½œçš„åŸºå‡†å’Œæ¡†æ¶ã€‚è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåˆ†å±‚åŸºå‡†ï¼ŒåŒ…å«ä»£ç†æ¿€æ´»ã€ä»»åŠ¡è§„åˆ’å’Œè½¨è¿¹æ„ŸçŸ¥ä¸‰ä¸ªç»“æ„åŒ–å±‚æ¬¡ã€‚VIKI-Ræ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µæ¡†æ¶ï¼Œé€šè¿‡é“¾å¼æ€ç»´æ³¨é‡Šç¤ºä¾‹å¾®è°ƒé¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå¹¶åœ¨å¤šå±‚æ¬¡å¥–åŠ±ä¿¡å·ä¸‹è¿›è¡Œå¼ºåŒ–å­¦ä¹ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVIKI-Råœ¨æ‰€æœ‰ä»»åŠ¡å±‚æ¬¡ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå¹¶ä¸”å¼ºåŒ–å­¦ä¹ ä¿ƒè¿›äº†å¼‚æ„æ™ºèƒ½ä½“ä¹‹é—´çš„ç»„åˆåˆä½œæ¨¡å¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.17206",
            "title": "DreamCube: 3D Panorama Generation via Multi-plane Synchronization",
            "url": "https://huggingface.co/papers/2506.17206",
            "abstract": "Multi-plane synchronization extends 2D foundation models to 3D panorama generation, introducing DreamCube to achieve diverse appearances and accurate geometry.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D panorama synthesis is a promising yet challenging task that demands high-quality and diverse visual appearance and geometry of the generated omnidirectional content. Existing methods leverage rich image priors from pre-trained 2D foundation models to circumvent the scarcity of 3D panoramic data, but the incompatibility between 3D panoramas and 2D single views limits their effectiveness. In this work, we demonstrate that by applying multi-plane synchronization to the operators from 2D foundation models, their capabilities can be seamlessly extended to the omnidirectional domain. Based on this design, we further introduce DreamCube, a multi-plane RGB-D diffusion model for 3D panorama generation, which maximizes the reuse of 2D foundation model priors to achieve diverse appearances and accurate geometry while maintaining multi-view consistency. Extensive experiments demonstrate the effectiveness of our approach in panoramic image generation, panoramic depth estimation, and 3D scene generation.",
            "score": 9,
            "issue_id": 4425,
            "pub_date": "2025-06-20",
            "pub_date_card": {
                "ru": "20 Ğ¸ÑĞ½Ñ",
                "en": "June 20",
                "zh": "6æœˆ20æ—¥"
            },
            "hash": "bbec38fbe4e9ddd8",
            "authors": [
                "Yukun Huang",
                "Yanning Zhou",
                "Jianan Wang",
                "Kaiyi Huang",
                "Xihui Liu"
            ],
            "affiliations": [
                "Astribot",
                "Tencent",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.17206.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#diffusion",
                    "#3d",
                    "#cv"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "ĞœĞ½Ğ¾Ğ³Ğ¾Ğ¿Ğ»Ğ¾ÑĞºĞ¾ÑÑ‚Ğ½Ğ°Ñ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ: Ğ¼Ğ¾ÑÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ 2D Ğ¸ 3D Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¿Ğ»Ğ¾ÑĞºĞ¾ÑÑ‚Ğ½Ğ¾Ğ¹ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²ÑƒĞ¼ĞµÑ€Ğ½Ñ‹Ñ… Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ DreamCube, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ²ÑƒĞ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ°Ğ¼Ğ¸ Ğ¸ Ğ´Ğ²ÑƒĞ¼ĞµÑ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½."
                },
                "en": {
                    "title": "DreamCube: Bridging 2D and 3D for Stunning Panoramas",
                    "desc": "This paper presents a method called DreamCube that enhances 2D foundation models for generating 3D panoramas. It addresses the challenge of creating high-quality and diverse omnidirectional content by using multi-plane synchronization. This technique allows the model to effectively utilize existing 2D image data to improve the geometry and appearance of 3D images. The results show that DreamCube can generate consistent panoramic images and accurately estimate depth, making it a significant advancement in 3D scene generation."
                },
                "zh": {
                    "title": "å¤šå¹³é¢åŒæ­¥ï¼Œå¼€å¯ä¸‰ç»´å…¨æ™¯æ–°è§†ç•Œ",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºDreamCubeçš„å¤šå¹³é¢åŒæ­¥æ–¹æ³•ï¼Œæ—¨åœ¨å°†äºŒç»´åŸºç¡€æ¨¡å‹æ‰©å±•åˆ°ä¸‰ç»´å…¨æ™¯ç”Ÿæˆã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒçš„äºŒç»´æ¨¡å‹çš„ä¸°å¯Œå›¾åƒå…ˆéªŒï¼Œå…‹æœäº†ä¸‰ç»´å…¨æ™¯æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿå®ç°å¤šæ ·åŒ–çš„è§†è§‰æ•ˆæœå’Œå‡†ç¡®çš„å‡ ä½•å½¢çŠ¶ï¼ŒåŒæ—¶ä¿æŒå¤šè§†å›¾çš„ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æŠ€æœ¯åœ¨å…¨æ™¯å›¾åƒç”Ÿæˆã€æ·±åº¦ä¼°è®¡å’Œä¸‰ç»´åœºæ™¯ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.16035",
            "title": "Vision-Guided Chunking Is All You Need: Enhancing RAG with Multimodal\n  Document Understanding",
            "url": "https://huggingface.co/papers/2506.16035",
            "abstract": "A novel multimodal document chunking approach using Large Multimodal Models (LMMs) enhances RAG performance by accurately processing complex PDF documents, including multi-page tables and embedded visuals.  \t\t\t\t\tAI-generated summary \t\t\t\t Retrieval-Augmented Generation (RAG) systems have revolutionized information retrieval and question answering, but traditional text-based chunking methods struggle with complex document structures, multi-page tables, embedded figures, and contextual dependencies across page boundaries. We present a novel multimodal document chunking approach that leverages Large Multimodal Models (LMMs) to process PDF documents in batches while maintaining semantic coherence and structural integrity. Our method processes documents in configurable page batches with cross-batch context preservation, enabling accurate handling of tables spanning multiple pages, embedded visual elements, and procedural content. We evaluate our approach on a curated dataset of PDF documents with manually crafted queries, demonstrating improvements in chunk quality and downstream RAG performance. Our vision-guided approach achieves better accuracy compared to traditional vanilla RAG systems, with qualitative analysis showing superior preservation of document structure and semantic coherence.",
            "score": 8,
            "issue_id": 4427,
            "pub_date": "2025-06-19",
            "pub_date_card": {
                "ru": "19 Ğ¸ÑĞ½Ñ",
                "en": "June 19",
                "zh": "6æœˆ19æ—¥"
            },
            "hash": "08abcd6ec6f0d9e1",
            "authors": [
                "Vishesh Tripathi",
                "Tanmay Odapally",
                "Indraneel Das",
                "Uday Allu",
                "Biddwan Ahmed"
            ],
            "affiliations": [
                "AI Research Team, Yellow.ai"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.16035.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#long_context",
                    "#optimization",
                    "#rag",
                    "#multimodal"
                ],
                "emoji": "ğŸ“„",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ±Ğ¸ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ RAG-ÑĞ¸ÑÑ‚ĞµĞ¼",
                    "desc": "ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€Ğ°Ğ·Ğ±Ğ¸ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… PDF-Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LMM). ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ°Ñ€Ñ‚Ğ¸ÑĞ¼Ğ¸ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ°Ñ€Ñ‚Ğ¸ÑĞ¼Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ Ğ¸ Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼ (RAG). ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ RAG Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°."
                },
                "en": {
                    "title": "Enhancing RAG with Multimodal Document Chunking",
                    "desc": "This paper introduces a new method for breaking down complex PDF documents using Large Multimodal Models (LMMs). Traditional methods struggle with intricate layouts, such as multi-page tables and embedded visuals, which can lead to poor information retrieval. The proposed approach processes documents in batches while keeping the context intact across pages, ensuring that the meaning and structure are preserved. Evaluations show that this method significantly enhances the quality of document chunks and improves the performance of Retrieval-Augmented Generation (RAG) systems."
                },
                "zh": {
                    "title": "å¤šæ¨¡æ€æ–‡æ¡£åˆ†å—ï¼Œæå‡ä¿¡æ¯æ£€ç´¢æ–°é«˜åº¦",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¤šæ¨¡æ€æ–‡æ¡£åˆ†å—æ–¹æ³•ï¼Œåˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰æ¥å¤„ç†å¤æ‚çš„PDFæ–‡æ¡£ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å¤šé¡µè¡¨æ ¼å’ŒåµŒå…¥è§†è§‰å…ƒç´ ï¼ŒåŒæ—¶ä¿æŒè¯­ä¹‰ä¸€è‡´æ€§å’Œç»“æ„å®Œæ•´æ€§ã€‚é€šè¿‡é…ç½®é¡µé¢æ‰¹æ¬¡è¿›è¡Œå¤„ç†ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿè·¨æ‰¹æ¬¡ä¿ç•™ä¸Šä¸‹æ–‡ï¼Œä»è€Œæé«˜äº†åˆ†å—è´¨é‡å’Œåç»­çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿçš„RAGç³»ç»Ÿç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å‡†ç¡®æ€§å’Œæ–‡æ¡£ç»“æ„ä¿ç•™æ–¹é¢è¡¨ç°æ›´ä¼˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.17201",
            "title": "Hunyuan-GameCraft: High-dynamic Interactive Game Video Generation with\n  Hybrid History Condition",
            "url": "https://huggingface.co/papers/2506.17201",
            "abstract": "Hunyuan-GameCraft is a novel framework for high-dynamic interactive video generation in game environments that addresses limitations in dynamics, generality, and efficiency through unified input representation, hybrid history-conditioned training, and model distillation.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in diffusion-based and controllable video generation have enabled high-quality and temporally coherent video synthesis, laying the groundwork for immersive interactive gaming experiences. However, current methods face limitations in dynamics, generality, long-term consistency, and efficiency, which limit the ability to create various gameplay videos. To address these gaps, we introduce Hunyuan-GameCraft, a novel framework for high-dynamic interactive video generation in game environments. To achieve fine-grained action control, we unify standard keyboard and mouse inputs into a shared camera representation space, facilitating smooth interpolation between various camera and movement operations. Then we propose a hybrid history-conditioned training strategy that extends video sequences autoregressively while preserving game scene information. Additionally, to enhance inference efficiency and playability, we achieve model distillation to reduce computational overhead while maintaining consistency across long temporal sequences, making it suitable for real-time deployment in complex interactive environments. The model is trained on a large-scale dataset comprising over one million gameplay recordings across over 100 AAA games, ensuring broad coverage and diversity, then fine-tuned on a carefully annotated synthetic dataset to enhance precision and control. The curated game scene data significantly improves the visual fidelity, realism and action controllability. Extensive experiments demonstrate that Hunyuan-GameCraft significantly outperforms existing models, advancing the realism and playability of interactive game video generation.",
            "score": 4,
            "issue_id": 4425,
            "pub_date": "2025-06-20",
            "pub_date_card": {
                "ru": "20 Ğ¸ÑĞ½Ñ",
                "en": "June 20",
                "zh": "6æœˆ20æ—¥"
            },
            "hash": "def0daf064c03fe6",
            "authors": [
                "Jiaqi Li",
                "Junshu Tang",
                "Zhiyong Xu",
                "Longhuang Wu",
                "Yuan Zhou",
                "Shuai Shao",
                "Tianbao Yu",
                "Zhiguo Cao",
                "Qinglin Lu"
            ],
            "affiliations": [
                "Huazhong University of Science and Technology",
                "Tencent Hunyuan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.17201.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#video",
                    "#diffusion",
                    "#synthetic",
                    "#dataset",
                    "#games",
                    "#training"
                ],
                "emoji": "ğŸ®",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Hunyuan-GameCraft - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ… Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¾Ğ¹. ĞĞ½Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞµ, ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ²Ğ¾Ğ´Ğ°, Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¸ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹ Ğ³ĞµĞ¹Ğ¼Ğ¿Ğ»ĞµÑ Ğ¸Ğ· Ğ±Ğ¾Ğ»ĞµĞµ 100 ĞĞĞ-Ğ¸Ğ³Ñ€, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Hunyuan-GameCraft Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¸Ğ³Ñ€Ğ°Ğ±ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Revolutionizing Interactive Video Generation in Gaming",
                    "desc": "Hunyuan-GameCraft is a new framework designed for generating interactive videos in gaming environments, overcoming issues related to dynamics, generality, and efficiency. It uses a unified input representation that combines keyboard and mouse controls, allowing for smooth transitions in camera and movement actions. The framework employs a hybrid history-conditioned training method to extend video sequences while keeping important game scene details intact. Additionally, model distillation is utilized to improve computational efficiency, making it suitable for real-time applications in complex gaming scenarios."
                },
                "zh": {
                    "title": "Hunyuan-GameCraftï¼šæå‡äº’åŠ¨æ¸¸æˆè§†é¢‘ç”Ÿæˆçš„çœŸå®æ„Ÿä¸å¯ç©æ€§",
                    "desc": "Hunyuan-GameCraftæ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ¸¸æˆç¯å¢ƒä¸­é«˜åŠ¨æ€äº’åŠ¨è§†é¢‘ç”Ÿæˆçš„å±€é™æ€§ã€‚å®ƒé€šè¿‡ç»Ÿä¸€è¾“å…¥è¡¨ç¤ºã€æ··åˆå†å²æ¡ä»¶è®­ç»ƒå’Œæ¨¡å‹è’¸é¦æ¥æé«˜åŠ¨æ€æ€§ã€é€šç”¨æ€§å’Œæ•ˆç‡ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿå®ç°ç²¾ç»†çš„åŠ¨ä½œæ§åˆ¶ï¼Œå¹¶åœ¨å¤æ‚çš„äº’åŠ¨ç¯å¢ƒä¸­ä¿æŒé•¿æ—¶é—´çš„ä¸€è‡´æ€§ã€‚ç»è¿‡å¤§è§„æ¨¡æ•°æ®é›†çš„è®­ç»ƒï¼ŒHunyuan-GameCraftåœ¨ç”Ÿæˆäº’åŠ¨æ¸¸æˆè§†é¢‘çš„çœŸå®æ„Ÿå’Œå¯ç©æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.16504",
            "title": "Hunyuan3D 2.5: Towards High-Fidelity 3D Assets Generation with Ultimate\n  Details",
            "url": "https://huggingface.co/papers/2506.16504",
            "abstract": "Hunyuan3D 2.5, a suite of 3D diffusion models, advances shape and texture generation with a new LATTICE model and physical-based rendering in a multi-view architecture.  \t\t\t\t\tAI-generated summary \t\t\t\t In this report, we present Hunyuan3D 2.5, a robust suite of 3D diffusion models aimed at generating high-fidelity and detailed textured 3D assets. Hunyuan3D 2.5 follows two-stages pipeline of its previous version Hunyuan3D 2.0, while demonstrating substantial advancements in both shape and texture generation. In terms of shape generation, we introduce a new shape foundation model -- LATTICE, which is trained with scaled high-quality datasets, model-size, and compute. Our largest model reaches 10B parameters and generates sharp and detailed 3D shape with precise image-3D following while keeping mesh surface clean and smooth, significantly closing the gap between generated and handcrafted 3D shapes. In terms of texture generation, it is upgraded with phyiscal-based rendering (PBR) via a novel multi-view architecture extended from Hunyuan3D 2.0 Paint model. Our extensive evaluation shows that Hunyuan3D 2.5 significantly outperforms previous methods in both shape and end-to-end texture generation.",
            "score": 4,
            "issue_id": 4424,
            "pub_date": "2025-06-19",
            "pub_date_card": {
                "ru": "19 Ğ¸ÑĞ½Ñ",
                "en": "June 19",
                "zh": "6æœˆ19æ—¥"
            },
            "hash": "7ea16d5712b67fcc",
            "authors": [
                "Zeqiang Lai",
                "Yunfei Zhao",
                "Haolin Liu",
                "Zibo Zhao",
                "Qingxiang Lin",
                "Huiwen Shi",
                "Xianghui Yang",
                "Mingxin Yang",
                "Shuhui Yang",
                "Yifei Feng",
                "Sheng Zhang",
                "Xin Huang",
                "Di Luo",
                "Fan Yang",
                "Fang Yang",
                "Lifu Wang",
                "Sicong Liu",
                "Yixuan Tang",
                "Yulin Cai",
                "Zebin He",
                "Tian Liu",
                "Yuhong Liu",
                "Jie Jiang",
                "Linus",
                "Jingwei Huang",
                "Chunchao Guo"
            ],
            "affiliations": [
                "Tencent"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.16504.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#3d"
                ],
                "emoji": "ğŸ§Š",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ 3D-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸: Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ñ‹",
                    "desc": "Hunyuan3D 2.5 Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ 3D-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ¸Ğ¹ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ¾Ñ€Ğ¼ Ğ¸ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ Ğ½Ğ¾Ğ²Ğ¾Ğ²Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ - Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ LATTICE Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ„Ğ¾Ñ€Ğ¼, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…. Ğ”Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³ (PBR) Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ. Ğ¡Ğ¾Ğ³Ğ»Ğ°ÑĞ½Ğ¾ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼, Hunyuan3D 2.5 Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ğº Ñ„Ğ¾Ñ€Ğ¼, Ñ‚Ğ°Ğº Ğ¸ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€."
                },
                "en": {
                    "title": "Revolutionizing 3D Asset Generation with Hunyuan3D 2.5",
                    "desc": "Hunyuan3D 2.5 is a suite of advanced 3D diffusion models designed to enhance the generation of high-quality 3D shapes and textures. It introduces the LATTICE model, a new foundation for shape generation that utilizes large, high-quality datasets and boasts a model size of up to 10 billion parameters. This model produces sharp and detailed 3D shapes while maintaining clean mesh surfaces, bridging the gap between generated and handcrafted designs. Additionally, the suite incorporates physical-based rendering in a multi-view architecture to improve texture generation, demonstrating significant improvements over previous versions."
                },
                "zh": {
                    "title": "Hunyuan3D 2.5ï¼šå½¢çŠ¶ä¸çº¹ç†ç”Ÿæˆçš„æ–°çªç ´",
                    "desc": "Hunyuan3D 2.5 æ˜¯ä¸€å¥—å…ˆè¿›çš„ 3D æ‰©æ•£æ¨¡å‹ï¼Œæ—¨åœ¨ç”Ÿæˆé«˜ä¿çœŸå’Œç»†è‡´çš„çº¹ç† 3D èµ„äº§ã€‚å®ƒåœ¨å½¢çŠ¶ç”Ÿæˆæ–¹é¢å¼•å…¥äº†æ–°çš„åŸºç¡€æ¨¡å‹ LATTICEï¼Œç»è¿‡å¤§è§„æ¨¡é«˜è´¨é‡æ•°æ®é›†çš„è®­ç»ƒï¼Œèƒ½å¤Ÿç”Ÿæˆæ¸…æ™°ä¸”ç»†è‡´çš„ 3D å½¢çŠ¶ã€‚çº¹ç†ç”Ÿæˆæ–¹é¢ï¼Œé‡‡ç”¨äº†åŸºäºç‰©ç†çš„æ¸²æŸ“ï¼ˆPBRï¼‰æŠ€æœ¯ï¼Œç»“åˆäº†å¤šè§†è§’æ¶æ„ï¼Œæ˜¾è‘—æå‡äº†çº¹ç†è´¨é‡ã€‚ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒHunyuan3D 2.5 åœ¨å½¢çŠ¶å’Œçº¹ç†ç”Ÿæˆæ–¹é¢å‡ä¼˜äºä¹‹å‰çš„æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.15925",
            "title": "Reranking-based Generation for Unbiased Perspective Summarization",
            "url": "https://huggingface.co/papers/2506.15925",
            "abstract": "Reranking and preference tuning improve the quality of perspective summaries generated by LLMs, as measured by language model-based metrics that outperform traditional ones.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating unbiased summaries in real-world settings such as political perspective summarization remains a crucial application of Large Language Models (LLMs). Yet, existing evaluation frameworks rely on traditional metrics for measuring key attributes such as coverage and faithfulness without verifying their applicability, and efforts to develop improved summarizers are still nascent. We address these gaps by (1) identifying reliable metrics for measuring perspective summary quality, and (2) investigating the efficacy of LLM-based methods beyond zero-shot inference. Namely, we build a test set for benchmarking metric reliability using human annotations and show that traditional metrics underperform compared to language model-based metrics, which prove to be strong evaluators. Using these metrics, we show that reranking-based methods yield strong results, and preference tuning with synthetically generated and reranking-labeled data further boosts performance. Our findings aim to contribute to the reliable evaluation and development of perspective summarization methods.",
            "score": 1,
            "issue_id": 4425,
            "pub_date": "2025-06-19",
            "pub_date_card": {
                "ru": "19 Ğ¸ÑĞ½Ñ",
                "en": "June 19",
                "zh": "6æœˆ19æ—¥"
            },
            "hash": "8cb1a06d9566bcfc",
            "authors": [
                "Narutatsu Ri",
                "Nicholas Deas",
                "Kathleen McKeown"
            ],
            "affiliations": [
                "Department of Computer Science, Columbia University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.15925.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#multimodal",
                    "#dataset",
                    "#interpretability",
                    "#alignment",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ¾Ğ² Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ LLM",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºÑ€Ğ°Ñ‚ĞºĞ¸Ñ… Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ¾Ğ² Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ‚Ğ°ĞºĞ¸Ñ… Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ½Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ½Ğ¾ÑÑÑ‚ Ğ²ĞºĞ»Ğ°Ğ´ Ğ² Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¸ÑÑ‚ĞµĞ¼ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²."
                },
                "en": {
                    "title": "Enhancing Perspective Summaries with Reranking and Preference Tuning",
                    "desc": "This paper discusses how to improve the quality of summaries generated by Large Language Models (LLMs) for political perspectives. It highlights the limitations of traditional evaluation metrics, which do not effectively measure important aspects like coverage and faithfulness. The authors propose new, reliable metrics based on language models and demonstrate that these outperform traditional methods. They also show that using reranking and preference tuning techniques can significantly enhance the performance of LLM-generated summaries."
                },
                "zh": {
                    "title": "æå‡è§‚ç‚¹æ‘˜è¦è´¨é‡çš„å…³é”®åœ¨äºé‡æ’åºä¸åå¥½è°ƒä¼˜",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆçš„è§‚ç‚¹æ‘˜è¦çš„è´¨é‡ã€‚æˆ‘ä»¬å‘ç°ï¼Œä¼ ç»Ÿçš„è¯„ä¼°æŒ‡æ ‡åœ¨æµ‹é‡æ‘˜è¦çš„è¦†ç›–ç‡å’Œå¿ å®åº¦æ—¶æ•ˆæœä¸ä½³ï¼Œè€ŒåŸºäºè¯­è¨€æ¨¡å‹çš„æŒ‡æ ‡è¡¨ç°æ›´ä¸ºå‡ºè‰²ã€‚é€šè¿‡å»ºç«‹ä¸€ä¸ªåŸºå‡†æµ‹è¯•é›†å¹¶è¿›è¡Œäººç±»æ ‡æ³¨ï¼Œæˆ‘ä»¬éªŒè¯äº†è¿™äº›æ–°æŒ‡æ ‡çš„å¯é æ€§ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬æå‡ºäº†é‡æ’åºå’Œåå¥½è°ƒä¼˜çš„æ–¹æ³•ï¼Œæ˜¾è‘—æå‡äº†æ‘˜è¦ç”Ÿæˆçš„æ•ˆæœã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-06-20.html",
    "link_next": "2025-06-24.html",
    "link_month": "2025-06.html",
    "short_date_prev": {
        "ru": "20.06",
        "en": "06/20",
        "zh": "6æœˆ20æ—¥"
    },
    "short_date_next": {
        "ru": "24.06",
        "en": "06/24",
        "zh": "6æœˆ24æ—¥"
    },
    "categories": {
        "#dataset": 3,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 1,
        "#cv": 2,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 1,
        "#3d": 2,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 3,
        "#robotics": 0,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 1,
        "#reasoning": 1,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 3,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}