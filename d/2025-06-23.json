{
    "date": {
        "ru": "23 июня",
        "en": "June 23",
        "zh": "6月23日"
    },
    "time_utc": "2025-06-23 10:13",
    "weekday": 0,
    "issue_id": 4432,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2506.16406",
            "title": "Drag-and-Drop LLMs: Zero-Shot Prompt-to-Weights",
            "url": "https://huggingface.co/papers/2506.16406",
            "abstract": "Drag-and-Drop LLMs generate task-specific parameters through prompt-conditioned parameter generation, achieving significant efficiency gains and cross-domain generalization without per-task training.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern Parameter-Efficient Fine-Tuning (PEFT) methods such as low-rank adaptation (LoRA) reduce the cost of customizing large language models (LLMs), yet still require a separate optimization run for every downstream dataset. We introduce Drag-and-Drop LLMs (\\textit{DnD)}, a prompt-conditioned parameter generator that eliminates per-task training by mapping a handful of unlabeled task prompts directly to LoRA weight updates. A lightweight text encoder distills each prompt batch into condition embeddings, which are then transformed by a cascaded hyper-convolutional decoder into the full set of LoRA matrices. Once trained in a diverse collection of prompt-checkpoint pairs, DnD produces task-specific parameters in seconds, yielding i) up to 12,000times lower overhead than full fine-tuning, ii) average gains up to 30\\% in performance over the strongest training LoRAs on unseen common-sense reasoning, math, coding, and multimodal benchmarks, and iii) robust cross-domain generalization despite never seeing the target data or labels. Our results demonstrate that prompt-conditioned parameter generation is a viable alternative to gradient-based adaptation for rapidly specializing LLMs. Our project is available at https://jerryliang24.github.io/DnD{https://jerryliang24.github.io/DnD}.",
            "score": 56,
            "issue_id": 4426,
            "pub_date": "2025-06-19",
            "pub_date_card": {
                "ru": "19 июня",
                "en": "June 19",
                "zh": "6月19日"
            },
            "hash": "7ec3cdc6c01d1d79",
            "authors": [
                "Zhiyuan Liang",
                "Dongwen Tang",
                "Yuhao Zhou",
                "Xuanlei Zhao",
                "Mingjia Shi",
                "Wangbo Zhao",
                "Zekai Li",
                "Peihao Wang",
                "Konstantin Schürholt",
                "Damian Borth",
                "Michael M. Bronstein",
                "Yang You",
                "Zhangyang Wang",
                "Kai Wang"
            ],
            "affiliations": [
                "National University of Singapore",
                "Oxford University",
                "UT Austin",
                "University of St. Gallen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.16406.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#multimodal",
                    "#training",
                    "#optimization"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Революция в настройке ЯМ: быстрая адаптация без дополнительного обучения",
                    "desc": "Статья представляет новый метод Drag-and-Drop LLMs (DnD) для эффективной настройки больших языковых моделей. DnD использует генератор параметров, обусловленный промптами, для создания обновлений весов LoRA без необходимости обучения для каждой задачи. Этот подход обеспечивает значительное снижение вычислительных затрат и улучшение производительности на различных тестах по сравнению с традиционными методами тонкой настройки. DnD также демонстрирует надежную межпредметную генерализацию без доступа к целевым данным или меткам."
                },
                "en": {
                    "title": "Efficient Task-Specific Adaptation with Drag-and-Drop LLMs",
                    "desc": "The paper introduces Drag-and-Drop LLMs (DnD), a novel approach that generates task-specific parameters without the need for separate training on each task. By using prompt-conditioned parameter generation, DnD maps unlabeled task prompts directly to updates for low-rank adaptation (LoRA) weights. This method significantly reduces the computational overhead, achieving up to 12,000 times less than traditional fine-tuning while improving performance by an average of 30% on various benchmarks. DnD demonstrates strong cross-domain generalization, effectively adapting to new tasks without prior exposure to their data or labels."
                },
                "zh": {
                    "title": "拖放大语言模型：高效的任务特定参数生成",
                    "desc": "本文介绍了一种新的机器学习方法，称为拖放大语言模型（DnD），它通过提示条件生成参数，消除了每个任务的训练需求。DnD使用轻量级文本编码器将提示批次提炼为条件嵌入，并通过级联超卷积解码器转换为完整的LoRA矩阵。与传统的参数高效微调方法相比，DnD在性能上有显著提升，并且在未见过的任务上也能保持良好的泛化能力。该方法在多个基准测试中表现出色，证明了提示条件参数生成是快速专门化大语言模型的有效替代方案。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.16054",
            "title": "PAROAttention: Pattern-Aware ReOrdering for Efficient Sparse and\n  Quantized Attention in Visual Generation Models",
            "url": "https://huggingface.co/papers/2506.16054",
            "abstract": "PAROAttention reorganizes visual attention patterns to enable efficient sparsification and quantization, reducing memory and computational costs with minimal impact on performance.  \t\t\t\t\tAI-generated summary \t\t\t\t In visual generation, the quadratic complexity of attention mechanisms results in high memory and computational costs, especially for longer token sequences required in high-resolution image or multi-frame video generation. To address this, prior research has explored techniques such as sparsification and quantization. However, these techniques face significant challenges under low density and reduced bitwidths. Through systematic analysis, we identify that the core difficulty stems from the dispersed and irregular characteristics of visual attention patterns. Therefore, instead of introducing specialized sparsification and quantization design to accommodate such patterns, we propose an alternative strategy: *reorganizing* the attention pattern to alleviate the challenges. Inspired by the local aggregation nature of visual feature extraction, we design a novel **Pattern-Aware token ReOrdering (PARO)** technique, which unifies the diverse attention patterns into a hardware-friendly block-wise pattern. This unification substantially simplifies and enhances both sparsification and quantization. We evaluate the performance-efficiency trade-offs of various design choices and finalize a methodology tailored for the unified pattern. Our approach, **PAROAttention**, achieves video and image generation with lossless metrics, and nearly identical results from full-precision (FP) baselines, while operating at notably lower density (~20%-30%) and bitwidth (**INT8/INT4**), achieving a **1.9x** to **2.7x** end-to-end latency speedup.",
            "score": 35,
            "issue_id": 4428,
            "pub_date": "2025-06-19",
            "pub_date_card": {
                "ru": "19 июня",
                "en": "June 19",
                "zh": "6月19日"
            },
            "hash": "141661e70ce4b65f",
            "authors": [
                "Tianchen Zhao",
                "Ke Hong",
                "Xinhao Yang",
                "Xuefeng Xiao",
                "Huixia Li",
                "Feng Ling",
                "Ruiqi Xie",
                "Siqi Chen",
                "Hongyu Zhu",
                "Yichong Zhang",
                "Yu Wang"
            ],
            "affiliations": [
                "ByteDance",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.16054.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#cv",
                    "#inference",
                    "#architecture",
                    "#video"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "Эффективное внимание: меньше вычислений, та же точность",
                    "desc": "Статья представляет новый метод PAROAttention для оптимизации механизмов внимания в задачах генерации изображений и видео. Авторы предлагают реорганизовать паттерны внимания в блочную структуру, что упрощает разреживание и квантование. Это позволяет значительно снизить требования к памяти и вычислительным ресурсам без существенной потери качества. Эксперименты показывают ускорение в 1.9-2.7 раза по сравнению с базовыми моделями полной точности при использовании низкой плотности (20-30%) и 8/4-битного квантования."
                },
                "en": {
                    "title": "Revolutionizing Visual Attention for Efficient AI",
                    "desc": "PAROAttention is a novel approach that reorganizes visual attention patterns to improve the efficiency of sparsification and quantization in machine learning models. By addressing the challenges posed by the irregular characteristics of attention mechanisms, this method simplifies the process of reducing memory and computational costs. The technique, known as Pattern-Aware token ReOrdering (PARO), consolidates diverse attention patterns into a more manageable block-wise format. As a result, PAROAttention achieves high-quality image and video generation with significantly reduced resource requirements, maintaining performance comparable to full-precision models."
                },
                "zh": {
                    "title": "PAROAttention：高效的视觉注意力重组",
                    "desc": "PAROAttention是一种通过重新组织视觉注意力模式来提高稀疏化和量化效率的方法。这种方法能够在减少内存和计算成本的同时，尽量不影响性能。研究表明，视觉注意力模式的分散和不规则特性是造成高计算成本的主要原因。PARO技术通过将多样的注意力模式统一为硬件友好的块状模式，显著简化了稀疏化和量化过程。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.16035",
            "title": "Vision-Guided Chunking Is All You Need: Enhancing RAG with Multimodal\n  Document Understanding",
            "url": "https://huggingface.co/papers/2506.16035",
            "abstract": "A novel multimodal document chunking approach using Large Multimodal Models (LMMs) enhances RAG performance by accurately processing complex PDF documents, including multi-page tables and embedded visuals.  \t\t\t\t\tAI-generated summary \t\t\t\t Retrieval-Augmented Generation (RAG) systems have revolutionized information retrieval and question answering, but traditional text-based chunking methods struggle with complex document structures, multi-page tables, embedded figures, and contextual dependencies across page boundaries. We present a novel multimodal document chunking approach that leverages Large Multimodal Models (LMMs) to process PDF documents in batches while maintaining semantic coherence and structural integrity. Our method processes documents in configurable page batches with cross-batch context preservation, enabling accurate handling of tables spanning multiple pages, embedded visual elements, and procedural content. We evaluate our approach on a curated dataset of PDF documents with manually crafted queries, demonstrating improvements in chunk quality and downstream RAG performance. Our vision-guided approach achieves better accuracy compared to traditional vanilla RAG systems, with qualitative analysis showing superior preservation of document structure and semantic coherence.",
            "score": 33,
            "issue_id": 4427,
            "pub_date": "2025-06-19",
            "pub_date_card": {
                "ru": "19 июня",
                "en": "June 19",
                "zh": "6月19日"
            },
            "hash": "08abcd6ec6f0d9e1",
            "authors": [
                "Vishesh Tripathi",
                "Tanmay Odapally",
                "Indraneel Das",
                "Uday Allu",
                "Biddwan Ahmed"
            ],
            "affiliations": [
                "AI Research Team, Yellow.ai"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.16035.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#long_context",
                    "#optimization",
                    "#rag",
                    "#multimodal"
                ],
                "emoji": "📄",
                "ru": {
                    "title": "Умное разбиение документов для улучшения RAG-систем",
                    "desc": "Предложен новый подход к разбиению сложных PDF-документов на фрагменты с использованием мультимодальных языковых моделей (LMM). Метод обрабатывает документы партиями страниц, сохраняя контекст между партиями, что позволяет корректно обрабатывать многостраничные таблицы и встроенные визуальные элементы. Эксперименты показали улучшение качества фрагментов и повышение эффективности систем генерации с извлечением (RAG). Новый подход превосходит традиционные методы RAG в точности и сохранении структуры документа."
                },
                "en": {
                    "title": "Enhancing RAG with Multimodal Document Chunking",
                    "desc": "This paper introduces a new method for breaking down complex PDF documents using Large Multimodal Models (LMMs). Traditional methods struggle with intricate layouts, such as multi-page tables and embedded visuals, which can lead to poor information retrieval. The proposed approach processes documents in batches while keeping the context intact across pages, ensuring that the meaning and structure are preserved. Evaluations show that this method significantly enhances the quality of document chunks and improves the performance of Retrieval-Augmented Generation (RAG) systems."
                },
                "zh": {
                    "title": "多模态文档分块，提升信息检索新高度",
                    "desc": "本文提出了一种新颖的多模态文档分块方法，利用大型多模态模型（LMMs）来处理复杂的PDF文档。该方法能够有效处理多页表格和嵌入视觉元素，同时保持语义一致性和结构完整性。通过配置页面批次进行处理，我们的方法能够跨批次保留上下文，从而提高了分块质量和后续的检索增强生成（RAG）性能。实验结果表明，与传统的RAG系统相比，我们的方法在准确性和文档结构保留方面表现更优。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.09049",
            "title": "VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement\n  Learning",
            "url": "https://huggingface.co/papers/2506.09049",
            "abstract": "VIKI-Bench and VIKI-R provide a benchmark and framework for evaluating and improving visual-driven cooperation among diverse embodied agents using vision-language models and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Coordinating multiple embodied agents in dynamic environments remains a core challenge in artificial intelligence, requiring both perception-driven reasoning and scalable cooperation strategies. While recent works have leveraged large language models (LLMs) for multi-agent planning, a few have begun to explore vision-language models (VLMs) for visual reasoning. However, these VLM-based approaches remain limited in their support for diverse embodiment types. In this work, we introduce VIKI-Bench, the first hierarchical benchmark tailored for embodied multi-agent cooperation, featuring three structured levels: agent activation, task planning, and trajectory perception. VIKI-Bench includes diverse robot embodiments, multi-view visual observations, and structured supervision signals to evaluate reasoning grounded in visual inputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a two-stage framework that fine-tunes a pretrained vision-language model (VLM) using Chain-of-Thought annotated demonstrations, followed by reinforcement learning under multi-level reward signals. Our extensive experiments show that VIKI-R significantly outperforms baselines method across all task levels. Furthermore, we show that reinforcement learning enables the emergence of compositional cooperation patterns among heterogeneous agents. Together, VIKI-Bench and VIKI-R offer a unified testbed and method for advancing multi-agent, visual-driven cooperation in embodied AI systems.",
            "score": 24,
            "issue_id": 4425,
            "pub_date": "2025-06-10",
            "pub_date_card": {
                "ru": "10 июня",
                "en": "June 10",
                "zh": "6月10日"
            },
            "hash": "91caa1a0b1cb2b54",
            "authors": [
                "Li Kang",
                "Xiufeng Song",
                "Heng Zhou",
                "Yiran Qin",
                "Jie Yang",
                "Xiaohong Liu",
                "Philip Torr",
                "Lei Bai",
                "Zhenfei Yin"
            ],
            "affiliations": [
                "Shanghai Artificial Intelligence Laboratory",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong, Shenzhen",
                "University of Oxford",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.09049.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#agents",
                    "#games",
                    "#rl",
                    "#benchmark",
                    "#cv"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Визуальная кооперация агентов с помощью глубокого обучения",
                    "desc": "VIKI-Bench представляет собой иерархический бенчмарк для оценки кооперации между воплощенными агентами с использованием визуально-языковых моделей. VIKI-R - это двухэтапный фреймворк, который дообучает предобученную визуально-языковую модель с помощью аннотированных демонстраций и обучения с подкреплением. Эксперименты показывают, что VIKI-R значительно превосходит базовые методы на всех уровнях задач. Обучение с подкреплением позволяет появиться композиционным паттернам кооперации между разнородными агентами."
                },
                "en": {
                    "title": "Enhancing Multi-Agent Cooperation with VIKI-Bench and VIKI-R",
                    "desc": "This paper introduces VIKI-Bench and VIKI-R, a benchmark and framework designed to enhance cooperation among various embodied agents using vision-language models and reinforcement learning. VIKI-Bench is structured into three levels: agent activation, task planning, and trajectory perception, allowing for comprehensive evaluation of visual reasoning in multi-agent scenarios. VIKI-R fine-tunes a pretrained vision-language model with Chain-of-Thought demonstrations and applies reinforcement learning to optimize performance across different tasks. The results demonstrate that VIKI-R significantly improves cooperation patterns among diverse agents, showcasing the potential of visual-driven strategies in embodied AI."
                },
                "zh": {
                    "title": "VIKI-Bench与VIKI-R：推动具身智能体的视觉驱动合作",
                    "desc": "VIKI-Bench和VIKI-R是用于评估和改善多样化具身智能体之间视觉驱动合作的基准和框架。该研究提出了一个分层基准，包含代理激活、任务规划和轨迹感知三个结构化层次。VIKI-R是一个两阶段框架，通过链式思维注释示例微调预训练的视觉语言模型，并在多层次奖励信号下进行强化学习。实验结果表明，VIKI-R在所有任务层次上显著优于基线方法，并且强化学习促进了异构智能体之间的组合合作模式。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.17206",
            "title": "DreamCube: 3D Panorama Generation via Multi-plane Synchronization",
            "url": "https://huggingface.co/papers/2506.17206",
            "abstract": "Multi-plane synchronization extends 2D foundation models to 3D panorama generation, introducing DreamCube to achieve diverse appearances and accurate geometry.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D panorama synthesis is a promising yet challenging task that demands high-quality and diverse visual appearance and geometry of the generated omnidirectional content. Existing methods leverage rich image priors from pre-trained 2D foundation models to circumvent the scarcity of 3D panoramic data, but the incompatibility between 3D panoramas and 2D single views limits their effectiveness. In this work, we demonstrate that by applying multi-plane synchronization to the operators from 2D foundation models, their capabilities can be seamlessly extended to the omnidirectional domain. Based on this design, we further introduce DreamCube, a multi-plane RGB-D diffusion model for 3D panorama generation, which maximizes the reuse of 2D foundation model priors to achieve diverse appearances and accurate geometry while maintaining multi-view consistency. Extensive experiments demonstrate the effectiveness of our approach in panoramic image generation, panoramic depth estimation, and 3D scene generation.",
            "score": 13,
            "issue_id": 4425,
            "pub_date": "2025-06-20",
            "pub_date_card": {
                "ru": "20 июня",
                "en": "June 20",
                "zh": "6月20日"
            },
            "hash": "bbec38fbe4e9ddd8",
            "authors": [
                "Yukun Huang",
                "Yanning Zhou",
                "Jianan Wang",
                "Kaiyi Huang",
                "Xihui Liu"
            ],
            "affiliations": [
                "Astribot",
                "Tencent",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.17206.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#diffusion",
                    "#3d",
                    "#cv"
                ],
                "emoji": "🌐",
                "ru": {
                    "title": "Многоплоскостная синхронизация: мост между 2D и 3D в генерации панорам",
                    "desc": "Эта статья представляет новый подход к генерации трехмерных панорам с использованием многоплоскостной синхронизации двумерных фундаментальных моделей. Авторы предлагают модель DreamCube, которая максимально использует преимущества двумерных моделей для создания разнообразных и геометрически точных трехмерных панорам. Метод решает проблему несовместимости между трехмерными панорамами и двумерными изображениями, расширяя возможности существующих алгоритмов. Эксперименты показывают эффективность подхода в генерации панорамных изображений, оценке глубины и создании трехмерных сцен."
                },
                "en": {
                    "title": "DreamCube: Bridging 2D and 3D for Stunning Panoramas",
                    "desc": "This paper presents a method called DreamCube that enhances 2D foundation models for generating 3D panoramas. It addresses the challenge of creating high-quality and diverse omnidirectional content by using multi-plane synchronization. This technique allows the model to effectively utilize existing 2D image data to improve the geometry and appearance of 3D images. The results show that DreamCube can generate consistent panoramic images and accurately estimate depth, making it a significant advancement in 3D scene generation."
                },
                "zh": {
                    "title": "多平面同步，开启三维全景新视界",
                    "desc": "本论文提出了一种名为DreamCube的多平面同步方法，旨在将二维基础模型扩展到三维全景生成。该方法通过利用预训练的二维模型的丰富图像先验，克服了三维全景数据稀缺的问题。我们的方法能够实现多样化的视觉效果和准确的几何形状，同时保持多视图的一致性。实验结果表明，我们的技术在全景图像生成、深度估计和三维场景生成方面表现出色。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.17201",
            "title": "Hunyuan-GameCraft: High-dynamic Interactive Game Video Generation with\n  Hybrid History Condition",
            "url": "https://huggingface.co/papers/2506.17201",
            "abstract": "Hunyuan-GameCraft is a novel framework for high-dynamic interactive video generation in game environments that addresses limitations in dynamics, generality, and efficiency through unified input representation, hybrid history-conditioned training, and model distillation.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in diffusion-based and controllable video generation have enabled high-quality and temporally coherent video synthesis, laying the groundwork for immersive interactive gaming experiences. However, current methods face limitations in dynamics, generality, long-term consistency, and efficiency, which limit the ability to create various gameplay videos. To address these gaps, we introduce Hunyuan-GameCraft, a novel framework for high-dynamic interactive video generation in game environments. To achieve fine-grained action control, we unify standard keyboard and mouse inputs into a shared camera representation space, facilitating smooth interpolation between various camera and movement operations. Then we propose a hybrid history-conditioned training strategy that extends video sequences autoregressively while preserving game scene information. Additionally, to enhance inference efficiency and playability, we achieve model distillation to reduce computational overhead while maintaining consistency across long temporal sequences, making it suitable for real-time deployment in complex interactive environments. The model is trained on a large-scale dataset comprising over one million gameplay recordings across over 100 AAA games, ensuring broad coverage and diversity, then fine-tuned on a carefully annotated synthetic dataset to enhance precision and control. The curated game scene data significantly improves the visual fidelity, realism and action controllability. Extensive experiments demonstrate that Hunyuan-GameCraft significantly outperforms existing models, advancing the realism and playability of interactive game video generation.",
            "score": 11,
            "issue_id": 4425,
            "pub_date": "2025-06-20",
            "pub_date_card": {
                "ru": "20 июня",
                "en": "June 20",
                "zh": "6月20日"
            },
            "hash": "def0daf064c03fe6",
            "authors": [
                "Jiaqi Li",
                "Junshu Tang",
                "Zhiyong Xu",
                "Longhuang Wu",
                "Yuan Zhou",
                "Shuai Shao",
                "Tianbao Yu",
                "Zhiguo Cao",
                "Qinglin Lu"
            ],
            "affiliations": [
                "Huazhong University of Science and Technology",
                "Tencent Hunyuan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.17201.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#video",
                    "#diffusion",
                    "#synthetic",
                    "#dataset",
                    "#games",
                    "#training"
                ],
                "emoji": "🎮",
                "ru": {
                    "title": "Революция в генерации интерактивного игрового видео",
                    "desc": "Hunyuan-GameCraft - это новая система для генерации интерактивного видео в игровых средах с высокой динамикой. Она решает проблемы ограничений в динамике, универсальности и эффективности с помощью унифицированного представления ввода, гибридного обучения с учетом истории и дистилляции модели. Система обучается на масштабном наборе данных из более чем миллиона записей геймплея из более 100 ААА-игр, а затем дообучается на тщательно размеченном синтетическом наборе данных для повышения точности и контроля. Эксперименты показывают, что Hunyuan-GameCraft значительно превосходит существующие модели, повышая реалистичность и играбельность интерактивной генерации игрового видео."
                },
                "en": {
                    "title": "Revolutionizing Interactive Video Generation in Gaming",
                    "desc": "Hunyuan-GameCraft is a new framework designed for generating interactive videos in gaming environments, overcoming issues related to dynamics, generality, and efficiency. It uses a unified input representation that combines keyboard and mouse controls, allowing for smooth transitions in camera and movement actions. The framework employs a hybrid history-conditioned training method to extend video sequences while keeping important game scene details intact. Additionally, model distillation is utilized to improve computational efficiency, making it suitable for real-time applications in complex gaming scenarios."
                },
                "zh": {
                    "title": "Hunyuan-GameCraft：提升互动游戏视频生成的真实感与可玩性",
                    "desc": "Hunyuan-GameCraft是一个新颖的框架，旨在解决游戏环境中高动态互动视频生成的局限性。它通过统一输入表示、混合历史条件训练和模型蒸馏来提高动态性、通用性和效率。该框架能够实现精细的动作控制，并在复杂的互动环境中保持长时间的一致性。经过大规模数据集的训练，Hunyuan-GameCraft在生成互动游戏视频的真实感和可玩性方面显著优于现有模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.16504",
            "title": "Hunyuan3D 2.5: Towards High-Fidelity 3D Assets Generation with Ultimate\n  Details",
            "url": "https://huggingface.co/papers/2506.16504",
            "abstract": "Hunyuan3D 2.5, a suite of 3D diffusion models, advances shape and texture generation with a new LATTICE model and physical-based rendering in a multi-view architecture.  \t\t\t\t\tAI-generated summary \t\t\t\t In this report, we present Hunyuan3D 2.5, a robust suite of 3D diffusion models aimed at generating high-fidelity and detailed textured 3D assets. Hunyuan3D 2.5 follows two-stages pipeline of its previous version Hunyuan3D 2.0, while demonstrating substantial advancements in both shape and texture generation. In terms of shape generation, we introduce a new shape foundation model -- LATTICE, which is trained with scaled high-quality datasets, model-size, and compute. Our largest model reaches 10B parameters and generates sharp and detailed 3D shape with precise image-3D following while keeping mesh surface clean and smooth, significantly closing the gap between generated and handcrafted 3D shapes. In terms of texture generation, it is upgraded with phyiscal-based rendering (PBR) via a novel multi-view architecture extended from Hunyuan3D 2.0 Paint model. Our extensive evaluation shows that Hunyuan3D 2.5 significantly outperforms previous methods in both shape and end-to-end texture generation.",
            "score": 9,
            "issue_id": 4424,
            "pub_date": "2025-06-19",
            "pub_date_card": {
                "ru": "19 июня",
                "en": "June 19",
                "zh": "6月19日"
            },
            "hash": "7ea16d5712b67fcc",
            "authors": [
                "Zeqiang Lai",
                "Yunfei Zhao",
                "Haolin Liu",
                "Zibo Zhao",
                "Qingxiang Lin",
                "Huiwen Shi",
                "Xianghui Yang",
                "Mingxin Yang",
                "Shuhui Yang",
                "Yifei Feng",
                "Sheng Zhang",
                "Xin Huang",
                "Di Luo",
                "Fan Yang",
                "Fang Yang",
                "Lifu Wang",
                "Sicong Liu",
                "Yixuan Tang",
                "Yulin Cai",
                "Zebin He",
                "Tian Liu",
                "Yuhong Liu",
                "Jie Jiang",
                "Linus",
                "Jingwei Huang",
                "Chunchao Guo"
            ],
            "affiliations": [
                "Tencent"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.16504.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#3d"
                ],
                "emoji": "🧊",
                "ru": {
                    "title": "Новый уровень 3D-генерации: точные формы и реалистичные текстуры",
                    "desc": "Hunyuan3D 2.5 представляет собой набор моделей диффузии для 3D-генерации, улучшающий создание форм и текстур. Ключевое нововведение - модель LATTICE для генерации форм, обученная на масштабных высококачественных датасетах. Для генерации текстур используется физически корректный рендеринг (PBR) в многоракурсной архитектуре. Согласно оценкам, Hunyuan3D 2.5 значительно превосходит предыдущие методы в генерации как форм, так и текстур."
                },
                "en": {
                    "title": "Revolutionizing 3D Asset Generation with Hunyuan3D 2.5",
                    "desc": "Hunyuan3D 2.5 is a suite of advanced 3D diffusion models designed to enhance the generation of high-quality 3D shapes and textures. It introduces the LATTICE model, a new foundation for shape generation that utilizes large, high-quality datasets and boasts a model size of up to 10 billion parameters. This model produces sharp and detailed 3D shapes while maintaining clean mesh surfaces, bridging the gap between generated and handcrafted designs. Additionally, the suite incorporates physical-based rendering in a multi-view architecture to improve texture generation, demonstrating significant improvements over previous versions."
                },
                "zh": {
                    "title": "Hunyuan3D 2.5：形状与纹理生成的新突破",
                    "desc": "Hunyuan3D 2.5 是一套先进的 3D 扩散模型，旨在生成高保真和细致的纹理 3D 资产。它在形状生成方面引入了新的基础模型 LATTICE，经过大规模高质量数据集的训练，能够生成清晰且细致的 3D 形状。纹理生成方面，采用了基于物理的渲染（PBR）技术，结合了多视角架构，显著提升了纹理质量。综合评估表明，Hunyuan3D 2.5 在形状和纹理生成方面均优于之前的方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.15745",
            "title": "InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video\n  Understanding",
            "url": "https://huggingface.co/papers/2506.15745",
            "abstract": "InfiniPot-V is a training-free, query-agnostic framework that compresses the key-value cache during video encoding to maintain a fixed memory cap for streaming video understanding, enhancing real-time performance and accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern multimodal large language models (MLLMs) can reason over hour-long video, yet their key-value (KV) cache grows linearly with time--quickly exceeding the fixed memory of phones, AR glasses, and edge robots. Prior compression schemes either assume the whole video and user query are available offline or must first build the full cache, so memory still scales with stream length. InfiniPot-V is the first training-free, query-agnostic framework that enforces a hard, length-independent memory cap for streaming video understanding. During video encoding it monitors the cache and, once a user-set threshold is reached, runs a lightweight compression pass that (i) removes temporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii) keeps semantically significant tokens via Value-Norm (VaN) ranking. Across four open-source MLLMs and four long-video and two streaming-video benchmarks, InfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation, and matches or surpasses full-cache accuracy--even in multi-turn dialogues. By dissolving the KV cache bottleneck without retraining or query knowledge, InfiniPot-V closes the gap for on-device streaming video assistants.",
            "score": 4,
            "issue_id": 4431,
            "pub_date": "2025-06-18",
            "pub_date_card": {
                "ru": "18 июня",
                "en": "June 18",
                "zh": "6月18日"
            },
            "hash": "f6babb4a5e89bb3f",
            "authors": [
                "Minsoo Kim",
                "Kyuhong Shim",
                "Jungwook Choi",
                "Simyung Chang"
            ],
            "affiliations": [
                "Hanyang University",
                "Qualcomm AI Research, Qualcomm Korea",
                "Sungkyunkwan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.15745.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#multimodal",
                    "#optimization",
                    "#long_context",
                    "#video"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Эффективное сжатие кэша для потокового анализа видео без потери точности",
                    "desc": "InfiniPot-V - это фреймворк для сжатия кэша ключ-значение при кодировании видео, не требующий обучения и независимый от запросов. Он поддерживает фиксированный объем памяти для потокового анализа видео, улучшая производительность и точность в реальном времени. Фреймворк использует метрику временной избыточности (TaR) для удаления избыточных токенов и ранжирование по норме значений (VaN) для сохранения семантически значимых токенов. InfiniPot-V сокращает пиковое использование памяти GPU до 94%, обеспечивая генерацию в реальном времени и сохраняя или превосходя точность полного кэша."
                },
                "en": {
                    "title": "Streamline Video Understanding with InfiniPot-V!",
                    "desc": "InfiniPot-V is a novel framework designed to optimize memory usage during video encoding for real-time video understanding. It operates without the need for training or prior knowledge of user queries, making it flexible and efficient. The framework compresses the key-value cache by removing redundant information while preserving important data, ensuring that memory usage remains constant regardless of video length. This approach significantly reduces GPU memory requirements while maintaining high accuracy in video processing tasks."
                },
                "zh": {
                    "title": "InfiniPot-V：实时视频理解的内存压缩新方案",
                    "desc": "InfiniPot-V 是一个无需训练且与查询无关的框架，旨在压缩视频编码中的关键值缓存，以保持固定的内存限制，从而提升实时性能和准确性。该框架通过监控缓存，当达到用户设定的阈值时，执行轻量级压缩，去除时间上冗余的标记，并保留语义上重要的标记。与以往的压缩方案不同，InfiniPot-V 不需要离线获取整个视频或用户查询，因此能够有效控制内存使用。通过在多个开源多模态大语言模型和视频基准测试中验证，InfiniPot-V 显著降低了 GPU 内存峰值，同时保持实时生成和准确性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.17213",
            "title": "Long-term Traffic Simulation with Interleaved Autoregressive Motion and\n  Scenario Generation",
            "url": "https://huggingface.co/papers/2506.17213",
            "abstract": "InfGen, a unified next-token prediction model, enables stable long-term traffic simulation by interleaving closed-loop motion simulation and scene generation.  \t\t\t\t\tAI-generated summary \t\t\t\t An ideal traffic simulator replicates the realistic long-term point-to-point trip that a self-driving system experiences during deployment. Prior models and benchmarks focus on closed-loop motion simulation for initial agents in a scene. This is problematic for long-term simulation. Agents enter and exit the scene as the ego vehicle enters new regions. We propose InfGen, a unified next-token prediction model that performs interleaved closed-loop motion simulation and scene generation. InfGen automatically switches between closed-loop motion simulation and scene generation mode. It enables stable long-term rollout simulation. InfGen performs at the state-of-the-art in short-term (9s) traffic simulation, and significantly outperforms all other methods in long-term (30s) simulation. The code and model of InfGen will be released at https://orangesodahub.github.io/InfGen",
            "score": 2,
            "issue_id": 4431,
            "pub_date": "2025-06-20",
            "pub_date_card": {
                "ru": "20 июня",
                "en": "June 20",
                "zh": "6月20日"
            },
            "hash": "0940c34932f5181b",
            "authors": [
                "Xiuyu Yang",
                "Shuhan Tan",
                "Philipp Krähenbühl"
            ],
            "affiliations": [
                "UT Austin"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.17213.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#agents",
                    "#open_source",
                    "#video"
                ],
                "emoji": "🚗",
                "ru": {
                    "title": "InfGen: Революция в долгосрочном моделировании дорожного движения",
                    "desc": "InfGen - это унифицированная модель предсказания следующего токена, которая позволяет проводить стабильное долгосрочное моделирование дорожного движения. Она объединяет симуляцию движения в замкнутом цикле и генерацию сцен. InfGen автоматически переключается между режимами симуляции движения и генерации сцен, что обеспечивает стабильное долгосрочное моделирование. Модель демонстрирует лучшие результаты как в краткосрочном (9 секунд), так и в долгосрочном (30 секунд) моделировании трафика."
                },
                "en": {
                    "title": "InfGen: Revolutionizing Long-Term Traffic Simulation with Unified Prediction",
                    "desc": "InfGen is a novel model designed for traffic simulation that predicts the next movement of vehicles in a scene. It combines two key processes: closed-loop motion simulation, which tracks the movement of vehicles, and scene generation, which creates the environment around them. This interleaving allows InfGen to maintain stability during long-term simulations, addressing the challenges faced by previous models that struggled with dynamic agent interactions. The model demonstrates superior performance in both short-term and long-term traffic simulations, making it a significant advancement in the field."
                },
                "zh": {
                    "title": "InfGen：稳定的长期交通仿真新方法",
                    "desc": "InfGen是一种统一的下一个标记预测模型，能够通过交替进行闭环运动仿真和场景生成，实现稳定的长期交通仿真。传统模型主要关注初始代理的闭环运动仿真，这在长期仿真中存在问题，因为代理在自驾车进入新区域时会进出场景。InfGen能够自动切换闭环运动仿真和场景生成模式，从而实现稳定的长期仿真。该模型在短期（9秒）交通仿真中表现出色，并在长期（30秒）仿真中显著优于其他方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.17202",
            "title": "UniFork: Exploring Modality Alignment for Unified Multimodal\n  Understanding and Generation",
            "url": "https://huggingface.co/papers/2506.17202",
            "abstract": "A Y-shaped architecture, UniFork, balances shared learning and task specialization for unified image understanding and generation, outperforming conventional fully shared Transformer models.  \t\t\t\t\tAI-generated summary \t\t\t\t Unified image understanding and generation has emerged as a promising paradigm in multimodal artificial intelligence. Despite recent progress, the optimal architectural design for such unified models remains an open challenge. In this work, we start by analyzing the modality alignment behaviors of task-specific expert models for understanding and generation, as well as current unified models. Our analysis reveals a crucial observation: understanding tasks benefit from a progressively increasing modality alignment across network depth, which helps build up semantic information for better comprehension; In contrast, generation tasks follow a different trend: modality alignment increases in the early layers but decreases in the deep layers to recover spatial details. These divergent alignment patterns create a fundamental conflict in fully shared Transformer backbones, where a uniform representational flow often leads to performance compromises across two tasks. Motivated by this finding, we introduce UniFork, a novel Y-shaped architecture that shares the shallow layers for cross-task representation learning, while employing task-specific branches in deeper layers to avoid task interference. This design effectively balances shared learning and task specialization. Through extensive ablation experiments, we demonstrate that Unifork consistently outperforms conventional fully shared Transformer architectures, and achieves performance on par with or better than task-specific models.",
            "score": 2,
            "issue_id": 4432,
            "pub_date": "2025-06-20",
            "pub_date_card": {
                "ru": "20 июня",
                "en": "June 20",
                "zh": "6月20日"
            },
            "hash": "3796f42ead32837a",
            "authors": [
                "Teng Li",
                "Quanfeng Lu",
                "Lirui Zhao",
                "Hao Li",
                "Xizhou Zhu",
                "Yu Qiao",
                "Jun Zhang",
                "Wenqi Shao"
            ],
            "affiliations": [
                "HKUST",
                "SJTU",
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.17202.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#multimodal"
                ],
                "emoji": "🍴",
                "ru": {
                    "title": "UniFork: оптимальный баланс между общим обучением и специализацией задач",
                    "desc": "UniFork - это новая Y-образная архитектура для объединенного понимания и генерации изображений. Она сочетает общие слои для обучения между задачами и специализированные ветви для избежания интерференции. Анализ показал, что задачи понимания и генерации требуют разных паттернов выравнивания модальностей. UniFork превосходит обычные полностью разделяемые трансформерные модели и достигает результатов на уровне специализированных моделей."
                },
                "en": {
                    "title": "UniFork: Balancing Shared Learning and Task Specialization in Image AI",
                    "desc": "The paper introduces UniFork, a Y-shaped architecture designed to improve unified image understanding and generation in machine learning. It addresses the challenge of balancing shared learning and task specialization by analyzing how different tasks align modalities at various network depths. The findings show that understanding tasks benefit from deeper alignment, while generation tasks require a different approach, leading to conflicts in traditional models. UniFork resolves these issues by sharing shallow layers for common learning and using specialized branches for deeper layers, resulting in superior performance compared to fully shared Transformer models."
                },
                "zh": {
                    "title": "UniFork：平衡共享学习与任务专门化的创新架构",
                    "desc": "本文提出了一种新的Y形架构UniFork，旨在平衡共享学习和任务专门化，以实现统一的图像理解和生成。研究发现，理解任务在网络深度上需要逐渐增加的模态对齐，而生成任务则在早期层增加模态对齐但在深层减少，以恢复空间细节。这种对齐模式的差异在传统的完全共享Transformer模型中造成了性能妥协。UniFork通过在浅层共享表示学习，在深层采用任务特定的分支，有效地解决了这一问题，实验结果表明其性能优于传统模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.15442",
            "title": "Hunyuan3D 2.1: From Images to High-Fidelity 3D Assets with\n  Production-Ready PBR Material",
            "url": "https://huggingface.co/papers/2506.15442",
            "abstract": "The tutorial provides a comprehensive guide on using Hunyuan3D 2.1 for generating high-resolution, textured 3D models, covering data preparation, model architecture, training, evaluation, and deployment.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D AI-generated content (AIGC) is a passionate field that has significantly accelerated the creation of 3D models in gaming, film, and design. Despite the development of several groundbreaking models that have revolutionized 3D generation, the field remains largely accessible only to researchers, developers, and designers due to the complexities involved in collecting, processing, and training 3D models. To address these challenges, we introduce Hunyuan3D 2.1 as a case study in this tutorial. This tutorial offers a comprehensive, step-by-step guide on processing 3D data, training a 3D generative model, and evaluating its performance using Hunyuan3D 2.1, an advanced system for producing high-resolution, textured 3D assets. The system comprises two core components: the Hunyuan3D-DiT for shape generation and the Hunyuan3D-Paint for texture synthesis. We will explore the entire workflow, including data preparation, model architecture, training strategies, evaluation metrics, and deployment. By the conclusion of this tutorial, you will have the knowledge to finetune or develop a robust 3D generative model suitable for applications in gaming, virtual reality, and industrial design.",
            "score": 2,
            "issue_id": 4431,
            "pub_date": "2025-06-18",
            "pub_date_card": {
                "ru": "18 июня",
                "en": "June 18",
                "zh": "6月18日"
            },
            "hash": "c5c9b452dd58395e",
            "authors": [
                "Team Hunyuan3D",
                "Shuhui Yang",
                "Mingxin Yang",
                "Yifei Feng",
                "Xin Huang",
                "Sheng Zhang",
                "Zebin He",
                "Di Luo",
                "Haolin Liu",
                "Yunfei Zhao",
                "Qingxiang Lin",
                "Zeqiang Lai",
                "Xianghui Yang",
                "Huiwen Shi",
                "Zibo Zhao",
                "Bowen Zhang",
                "Hongyu Yan",
                "Lifu Wang",
                "Sicong Liu",
                "Jihong Zhang",
                "Meng Chen",
                "Liang Dong",
                "Yiwen Jia",
                "Yulin Cai",
                "Jiaao Yu",
                "Yixuan Tang",
                "Dongyuan Guo",
                "Junlin Yu",
                "Hao Zhang",
                "Zheng Ye",
                "Peng He",
                "Runzhou Wu",
                "Shida Wei",
                "Chao Zhang",
                "Yonghao Tan",
                "Yifu Sun",
                "Lin Niu",
                "Shirui Huang",
                "Bojian Zheng",
                "Shu Liu",
                "Shilin Chen",
                "Xiang Yuan",
                "Xiaofeng Yang",
                "Kai Liu",
                "Jianchen Zhu",
                "Peng Chen",
                "Tian Liu",
                "Di Wang",
                "Yuhong Liu",
                "Linus",
                "Jie Jiang",
                "Jingwei Huang",
                "Chunchao Guo"
            ],
            "affiliations": [
                "Tencent Hunyuan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.15442.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#3d",
                    "#architecture",
                    "#synthetic",
                    "#games",
                    "#data"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Революция в 3D-моделировании: доступное руководство по Hunyuan3D 2.1",
                    "desc": "Статья представляет собой подробное руководство по использованию Hunyuan3D 2.1 для генерации высококачественных текстурированных 3D-моделей. В ней рассматриваются этапы подготовки данных, архитектура модели, процесс обучения, методы оценки и развертывания. Система Hunyuan3D 2.1 состоит из двух основных компонентов: Hunyuan3D-DiT для генерации форм и Hunyuan3D-Paint для синтеза текстур. Цель руководства - сделать технологию 3D-генерации более доступной для исследователей, разработчиков и дизайнеров."
                },
                "en": {
                    "title": "Unlocking 3D Model Creation with Hunyuan3D 2.1",
                    "desc": "This paper presents a tutorial on Hunyuan3D 2.1, a system designed for generating high-resolution, textured 3D models. It addresses the complexities of 3D model creation by providing a detailed guide on data preparation, model architecture, training, evaluation, and deployment. The system features two main components: Hunyuan3D-DiT for shape generation and Hunyuan3D-Paint for texture synthesis. By following this tutorial, users will learn how to finetune or create effective 3D generative models for various applications such as gaming and virtual reality."
                },
                "zh": {
                    "title": "掌握Hunyuan3D 2.1，轻松生成高质量3D模型",
                    "desc": "本教程全面介绍了如何使用Hunyuan3D 2.1生成高分辨率、带纹理的3D模型，涵盖了数据准备、模型架构、训练、评估和部署等方面。尽管3D AI生成内容领域取得了显著进展，但由于收集、处理和训练3D模型的复杂性，仍然主要面向研究人员和开发者。Hunyuan3D 2.1作为案例研究，提供了逐步指导，帮助用户处理3D数据、训练生成模型并评估其性能。通过本教程，您将掌握微调或开发适用于游戏、虚拟现实和工业设计的强大3D生成模型的知识。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.15925",
            "title": "Reranking-based Generation for Unbiased Perspective Summarization",
            "url": "https://huggingface.co/papers/2506.15925",
            "abstract": "Reranking and preference tuning improve the quality of perspective summaries generated by LLMs, as measured by language model-based metrics that outperform traditional ones.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating unbiased summaries in real-world settings such as political perspective summarization remains a crucial application of Large Language Models (LLMs). Yet, existing evaluation frameworks rely on traditional metrics for measuring key attributes such as coverage and faithfulness without verifying their applicability, and efforts to develop improved summarizers are still nascent. We address these gaps by (1) identifying reliable metrics for measuring perspective summary quality, and (2) investigating the efficacy of LLM-based methods beyond zero-shot inference. Namely, we build a test set for benchmarking metric reliability using human annotations and show that traditional metrics underperform compared to language model-based metrics, which prove to be strong evaluators. Using these metrics, we show that reranking-based methods yield strong results, and preference tuning with synthetically generated and reranking-labeled data further boosts performance. Our findings aim to contribute to the reliable evaluation and development of perspective summarization methods.",
            "score": 1,
            "issue_id": 4425,
            "pub_date": "2025-06-19",
            "pub_date_card": {
                "ru": "19 июня",
                "en": "June 19",
                "zh": "6月19日"
            },
            "hash": "8cb1a06d9566bcfc",
            "authors": [
                "Narutatsu Ri",
                "Nicholas Deas",
                "Kathleen McKeown"
            ],
            "affiliations": [
                "Department of Computer Science, Columbia University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.15925.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#multimodal",
                    "#dataset",
                    "#interpretability",
                    "#alignment",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Улучшение генерации обзоров с разных точек зрения с помощью ранжирования и настройки LLM",
                    "desc": "Исследование посвящено улучшению качества генерации кратких обзоров с учетом различных точек зрения с помощью больших языковых моделей (LLM). Авторы разработали надежные метрики для оценки качества таких обзоров, основанные на языковых моделях. Они показали, что методы ранжирования и настройки предпочтений с использованием синтетических данных значительно повышают эффективность генерации. Результаты исследования вносят вклад в развитие методов надежной оценки и создания систем суммаризации с учетом различных перспектив."
                },
                "en": {
                    "title": "Enhancing Perspective Summaries with Reranking and Preference Tuning",
                    "desc": "This paper discusses how to improve the quality of summaries generated by Large Language Models (LLMs) for political perspectives. It highlights the limitations of traditional evaluation metrics, which do not effectively measure important aspects like coverage and faithfulness. The authors propose new, reliable metrics based on language models and demonstrate that these outperform traditional methods. They also show that using reranking and preference tuning techniques can significantly enhance the performance of LLM-generated summaries."
                },
                "zh": {
                    "title": "提升观点摘要质量的关键在于重排序与偏好调优",
                    "desc": "本文探讨了如何提高大型语言模型（LLMs）生成的观点摘要的质量。我们发现，传统的评估指标在测量摘要的覆盖率和忠实度时效果不佳，而基于语言模型的指标表现更为出色。通过建立一个基准测试集并进行人类标注，我们验证了这些新指标的可靠性。最终，我们提出了重排序和偏好调优的方法，显著提升了摘要生成的效果。"
                }
            }
        }
    ],
    "link_prev": "2025-06-20.html",
    "link_next": "2025-06-24.html",
    "link_month": "2025-06.html",
    "short_date_prev": {
        "ru": "20.06",
        "en": "06/20",
        "zh": "6月20日"
    },
    "short_date_next": {
        "ru": "24.06",
        "en": "06/24",
        "zh": "6月24日"
    },
    "categories": {
        "#dataset": 3,
        "#data": 1,
        "#benchmark": 2,
        "#agents": 2,
        "#cv": 3,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 2,
        "#3d": 3,
        "#audio": 0,
        "#video": 4,
        "#multimodal": 5,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 6,
        "#robotics": 0,
        "#agi": 0,
        "#games": 3,
        "#interpretability": 1,
        "#reasoning": 1,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 7,
        "#survey": 0,
        "#diffusion": 3,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 2,
        "#synthetic": 3,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}