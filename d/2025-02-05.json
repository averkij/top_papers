{
    "date": {
        "ru": "5 февраля",
        "en": "February 5",
        "zh": "2月5日"
    },
    "time_utc": "2025-02-05 03:14",
    "weekday": 2,
    "issue_id": 2040,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2502.02584",
            "title": "QLASS: Boosting Language Agent Inference via Q-Guided Stepwise Search",
            "url": "https://huggingface.co/papers/2502.02584",
            "abstract": "Language agents have become a promising solution to complex interactive tasks. One of the key ingredients to the success of language agents is the reward model on the trajectory of the agentic workflow, which provides valuable guidance during training or inference. However, due to the lack of annotations of intermediate interactions, most existing works use an outcome reward model to optimize policies across entire trajectories. This may lead to sub-optimal policies and hinder the overall performance. To address this, we propose QLASS (Q-guided Language Agent Stepwise Search), to automatically generate annotations by estimating Q-values in a stepwise manner for open language agents. By introducing a reasoning tree and performing process reward modeling, QLASS provides effective intermediate guidance for each step. With the stepwise guidance, we propose a Q-guided generation strategy to enable language agents to better adapt to long-term value, resulting in significant performance improvement during model inference on complex interactive agent tasks. Notably, even with almost half the annotated data, QLASS retains strong performance, demonstrating its efficiency in handling limited supervision. We also empirically demonstrate that QLASS can lead to more effective decision making through qualitative analysis. We will release our code and data.",
            "score": 3,
            "issue_id": 2040,
            "pub_date": "2025-02-04",
            "pub_date_card": {
                "ru": "4 февраля",
                "en": "February 4",
                "zh": "2月4日"
            },
            "hash": "f2d3938d4ad71761",
            "authors": [
                "Zongyu Lin",
                "Yao Tang",
                "Xingcheng Yao",
                "Da Yin",
                "Ziniu Hu",
                "Yizhou Sun",
                "Kai-Wei Chang"
            ],
            "affiliations": [
                "Shanghai Jiaotong University, Shanghai, China",
                "University of California, Los Angeles, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.02584.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#open_source",
                    "#inference",
                    "#reasoning",
                    "#agents",
                    "#training",
                    "#optimization"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "QLASS: Пошаговое обучение языковых агентов для повышения эффективности",
                    "desc": "Статья представляет новый метод QLASS для обучения языковых агентов. QLASS использует пошаговую оценку Q-значений для генерации аннотаций и улучшения промежуточного обучения. Метод вводит дерево рассуждений и моделирование вознаграждений процесса для эффективного пошагового руководства. QLASS позволяет языковым агентам лучше адаптироваться к долгосрочным целям, значительно улучшая производительность при решении сложных интерактивных задач."
                },
                "en": {
                    "title": "Enhancing Language Agents with Stepwise Q-Guidance",
                    "desc": "This paper introduces QLASS, a method designed to enhance the performance of language agents in complex tasks by providing stepwise guidance through Q-value estimation. Traditional approaches rely on outcome reward models, which can result in sub-optimal decision-making due to the lack of intermediate feedback. QLASS addresses this by creating a reasoning tree that models rewards at each step, allowing agents to learn from their interactions more effectively. The results show that QLASS can maintain strong performance with less annotated data, proving its efficiency in environments with limited supervision."
                },
                "zh": {
                    "title": "QLASS：提升语言代理的决策能力",
                    "desc": "本文提出了一种名为QLASS的模型，用于改进语言代理在复杂交互任务中的表现。QLASS通过逐步估计Q值来自动生成中间交互的注释，从而为语言代理提供有效的指导。与传统的结果奖励模型不同，QLASS引入了推理树和过程奖励建模，使得每一步都有明确的指导。实验结果表明，即使在标注数据减少的情况下，QLASS仍能保持良好的性能，展示了其在有限监督下的高效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.02508",
            "title": "Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search",
            "url": "https://huggingface.co/papers/2502.02508",
            "abstract": "Large language models (LLMs) have demonstrated remarkable reasoning capabilities across diverse domains. Recent studies have shown that increasing test-time computation enhances LLMs' reasoning capabilities. This typically involves extensive sampling at inference time guided by an external LLM verifier, resulting in a two-player system. Despite external guidance, the effectiveness of this system demonstrates the potential of a single LLM to tackle complex tasks. Thus, we pose a new research problem: Can we internalize the searching capabilities to fundamentally enhance the reasoning abilities of a single LLM? This work explores an orthogonal direction focusing on post-training LLMs for autoregressive searching (i.e., an extended reasoning process with self-reflection and self-exploration of new strategies). To achieve this, we propose the Chain-of-Action-Thought (COAT) reasoning and a two-stage training paradigm: 1) a small-scale format tuning stage to internalize the COAT reasoning format and 2) a large-scale self-improvement stage leveraging reinforcement learning. Our approach results in Satori, a 7B LLM trained on open-source models and data. Extensive empirical evaluations demonstrate that Satori achieves state-of-the-art performance on mathematical reasoning benchmarks while exhibits strong generalization to out-of-domain tasks. Code, data, and models will be fully open-sourced.",
            "score": 2,
            "issue_id": 2040,
            "pub_date": "2025-02-04",
            "pub_date_card": {
                "ru": "4 февраля",
                "en": "February 4",
                "zh": "2月4日"
            },
            "hash": "80bd687783bd609b",
            "authors": [
                "Maohao Shen",
                "Guangtao Zeng",
                "Zhenting Qi",
                "Zhang-Wei Hong",
                "Zhenfang Chen",
                "Wei Lu",
                "Gregory Wornell",
                "Subhro Das",
                "David Cox",
                "Chuang Gan"
            ],
            "affiliations": [
                "Harvard",
                "MIT",
                "MIT-IBM Watson AI Lab, IBM Research",
                "Singapore University of Technology and Design",
                "UMass Amherst"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.02508.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#open_source",
                    "#training",
                    "#reasoning",
                    "#math",
                    "#rl"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Satori: LLM с внутренним поиском для улучшенного рассуждения",
                    "desc": "Исследователи представили новый подход к улучшению способностей больших языковых моделей (LLM) к рассуждению. Они разработали метод Chain-of-Action-Thought (COAT), который позволяет модели проводить самоанализ и исследовать новые стратегии решения задач. Процесс обучения включает два этапа: настройку формата на небольшом масштабе и масштабное самосовершенствование с использованием обучения с подкреплением. В результате была создана 7-миллиардная модель Satori, показавшая отличные результаты в задачах математического рассуждения и обобщения на новые области."
                },
                "en": {
                    "title": "Empowering LLMs with Internalized Reasoning through COAT",
                    "desc": "This paper discusses the development of Satori, a large language model (LLM) that enhances reasoning capabilities through a novel approach called Chain-of-Action-Thought (COAT). The authors propose a two-stage training process that first tunes the model to internalize COAT reasoning and then employs reinforcement learning for self-improvement. By internalizing the search capabilities typically guided by an external verifier, Satori can perform complex reasoning tasks more effectively. The results show that Satori not only excels in mathematical reasoning benchmarks but also generalizes well to other tasks, indicating its robust performance."
                },
                "zh": {
                    "title": "内化搜索能力，提升推理能力！",
                    "desc": "大型语言模型（LLMs）在多个领域展示了出色的推理能力。研究表明，增加测试时的计算可以提升LLMs的推理能力，通常需要在推理时进行大量采样，并由外部LLM验证器指导。本文提出了一个新问题：能否将搜索能力内化，以根本性地增强单个LLM的推理能力？我们提出了行动思维链（COAT）推理方法，并设计了一个两阶段的训练范式，以实现LLM的自我改进和自我反思。"
                }
            }
        }
    ],
    "link_prev": "2025-02-04.html",
    "link_next": "2025-02-06.html",
    "link_month": "2025-02.html",
    "short_date_prev": {
        "ru": "04.02",
        "en": "02/04",
        "zh": "2月4日"
    },
    "short_date_next": {
        "ru": "06.02",
        "en": "02/06",
        "zh": "2月6日"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 0,
        "#agents": 1,
        "#cv": 0,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章介绍了直接对齐算法（DAAs），它通过直接策略优化简化了语言模型对齐。DAAs可以根据排名损失、奖励类型或是否需要监督微调阶段来分类。文章指出，单阶段方法表现不如双阶段方法，但通过添加显式的监督微调阶段和引入beta参数，可以提高其性能。分析显示，关键因素在于使用的是成对目标还是点目标，而不是具体的隐式奖励或损失函数。结果强调了仔细评估的重要性，以避免过早声称性能提升或整体优越性。",
        "title": "The Differences Between Direct Alignment Algorithms are a Blur",
        "pinyin": "这篇文章介绍了直接对齐算法（DAAs），它通过直接策略优化简化了语言模型对齐。DAAs可以根据排名损失、奖励类型或是否需要监督微调阶段来分类。文章指出，单阶段方法表现不如双阶段方法，但通过添加显式的监督微调阶段和引入beta参数，可以提高其性能。分析显示，关键因素在于使用的是成对目标还是点目标，而不是具体的隐式奖励或损失函数。结果强调了仔细评估的重要性，以避免过早声称性能提升或整体优越性。\n\nZhè piān wénzhāng jièshào le zhíjiē duìqí suànfǎ (DAAs), tā tōngguò zhíjiē cèlüè yōuhuà jiǎnhuà le yǔyán móxíng duìqí. DAAs kěyǐ gēnjù páimíng sǔnshī, jiǎnglì lèixíng huò shìfǒu xūyào jiàndū wēitiáo jiēduàn lái fēnlèi. Wénzhāng zhǐchū, dān jiēduàn fāngfǎ biǎoxiàn bùrú shuāng jiēduàn fāngfǎ, dàn tōngguò tiānjiā xiǎnshì de jiàndū wēitiáo jiēduàn hé yǐnrù bètā cānshù, kěyǐ tígāo qí xìngnéng. Fēnxi xiǎnshì, guǎnjiàn yīnsù zài yú shǐyòng de shì chéngduī mùbiāo háishì diǎn mùbiāo, ér bùshì jùtǐ de yǐnshì jiǎnglì huò sǔnshī hánshù. Jiéguǒ qiángdiào le zhìxì píngjià de zhòngyàoxìng, yǐ fángmí guòzǎo shēngchēng xìngnéng tíshēng huò zhěngtǐ yōuyuèxìng.",
        "vocab": "[{'word': '直接对齐算法', 'pinyin': 'zhíjiē duìqí suànfǎ', 'trans': 'direct alignment algorithm'}, {'word': '策略', 'pinyin': 'cèlüè', 'trans': 'strategy'}, {'word': '优化', 'pinyin': 'yōuhuà', 'trans': 'optimize'}, {'word': '简化', 'pinyin': 'jiǎnhuà', 'trans': 'simplify'}, {'word': '语言模型', 'pinyin': 'yǔyán móxíng', 'trans': 'language model'}, {'word': '对齐', 'pinyin': 'duìqí', 'trans': 'align'}, {'word': '排名', 'pinyin': 'páimíng', 'trans': 'rank'}, {'word': '损失', 'pinyin': 'sǔnshī', 'trans': 'loss'}, {'word': '奖励', 'pinyin': 'jiǎnglì', 'trans': 'reward'}, {'word': '类型', 'pinyin': 'lèixíng', 'trans': 'type'}, {'word': '监督', 'pinyin': 'jiàndū', 'trans': 'supervise'}, {'word': '微调', 'pinyin': 'wēitiáo', 'trans': 'fine-tune'}, {'word': '阶段', 'pinyin': 'jiēduàn', 'trans': 'stage'}, {'word': '单阶段', 'pinyin': 'dān jiēduàn', 'trans': 'single-stage'}, {'word': '双阶段', 'pinyin': 'shuāng jiēduàn', 'trans': 'two-stage'}, {'word': '方法', 'pinyin': 'fāngfǎ', 'trans': 'method'}, {'word': '表现', 'pinyin': 'biǎoxiàn', 'trans': 'performance'}, {'word': '显式', 'pinyin': 'xiǎnshì', 'trans': 'explicit'}, {'word': '参数', 'pinyin': 'cānshǔ', 'trans': 'parameter'}, {'word': '性能', 'pinyin': 'xíngnéng', 'trans': 'performance'}, {'word': '分析', 'pinyin': 'fēnxī', 'trans': 'analysis'}, {'word': '显示', 'pinyin': 'xiǎnshì', 'trans': 'show'}, {'word': '关键', 'pinyin': 'guǎnjiàn', 'trans': 'key'}, {'word': '因素', 'pinyin': 'yīnsù', 'trans': 'factor'}, {'word': '成对', 'pinyin': 'chéngduì', 'trans': 'pair'}, {'word': '目标', 'pinyin': 'mùbiāo', 'trans': 'target'}, {'word': '点', 'pinyin': 'diǎn', 'trans': 'point'}, {'word': '隐式', 'pinyin': 'yǐnshì', 'trans': 'implicit'}, {'word': '函数', 'pinyin': 'hánshù', 'trans': 'function'}, {'word': '结果', 'pinyin': 'jiéguǒ', 'trans': 'result'}, {'word': '强调', 'pinyin': 'qiángdiào', 'trans': 'emphasize'}, {'word': '仔细', 'pinyin': 'zǐxì', 'trans': 'careful'}, {'word': '评估', 'pinyin': 'pínggū', 'trans': 'evaluate'}, {'word': '重要性', 'pinyin': 'zhòngyàoxìng', 'trans': 'importance'}, {'word': '避免', 'pinyin': 'bìmiǎn', 'trans': 'avoid'}, {'word': '过早', 'pinyin': 'guòzǎo', 'trans': 'premature'}, {'word': '声称', 'pinyin': 'shēngchēng', 'trans': 'claim'}, {'word': '提升', 'pinyin': 'tíshēng', 'trans': 'improvement'}, {'word': '整体', 'pinyin': 'zhěngtǐ', 'trans': 'overall'}, {'word': '优越性', 'pinyin': 'yōuyuèxìng', 'trans': 'superiority'}]",
        "trans": "This article introduces Direct Alignment Algorithms (DAAs), which simplify language model alignment through direct strategy optimization. DAAs can be classified based on ranking loss, reward type, or whether a supervised fine-tuning stage is required. The article notes that single-stage methods perform less well than dual-stage methods, but their performance can be improved by adding an explicit supervised fine-tuning stage and introducing a beta parameter. The analysis indicates that the key factor lies in the use of paired targets versus point targets, rather than specific implicit rewards or loss functions. The results emphasize the importance of careful evaluation to avoid premature claims of performance improvement or overall superiority.",
        "update_ts": "2025-02-04 09:10"
    }
}