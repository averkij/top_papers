
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 13 papers. March 26.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">26 марта</span> | <span id="title-articles-count">13 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-03-25.html">⬅️ <span id="prev-date">25.03</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-03-27.html">➡️ <span id="next-date">27.03</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-03.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '26 марта', 'en': 'March 26', 'zh': '3月26日'};
        let feedDateNext = {'ru': '27.03', 'en': '03/27', 'zh': '3月27日'};
        let feedDatePrev = {'ru': '25.03', 'en': '03/25', 'zh': '3月25日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2503.19325', 'title': 'Long-Context Autoregressive Video Modeling with Next-Frame Prediction', 'url': 'https://huggingface.co/papers/2503.19325', 'abstract': 'Long-context autoregressive modeling has significantly advanced language generation, but video generation still struggles to fully utilize extended temporal contexts. To investigate long-context video modeling, we introduce Frame AutoRegressive (FAR), a strong baseline for video autoregressive modeling. Just as language models learn causal dependencies between tokens (i.e., Token AR), FAR models temporal causal dependencies between continuous frames, achieving better convergence than Token AR and video diffusion transformers. Building on FAR, we observe that long-context vision modeling faces challenges due to visual redundancy. Existing RoPE lacks effective temporal decay for remote context and fails to extrapolate well to long video sequences. Additionally, training on long videos is computationally expensive, as vision tokens grow much faster than language tokens. To tackle these issues, we propose balancing locality and long-range dependency. We introduce FlexRoPE, an test-time technique that adds flexible temporal decay to RoPE, enabling extrapolation to 16x longer vision contexts. Furthermore, we propose long short-term context modeling, where a high-resolution short-term context window ensures fine-grained temporal consistency, while an unlimited long-term context window encodes long-range information using fewer tokens. With this approach, we can train on long video sequences with a manageable token context length. We demonstrate that FAR achieves state-of-the-art performance in both short- and long-video generation, providing a simple yet effective baseline for video autoregressive modeling.', 'score': 38, 'issue_id': 2896, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 марта', 'en': 'March 25', 'zh': '3月25日'}, 'hash': '543c7dbfad83ed73', 'authors': ['Yuchao Gu', 'Weijia Mao', 'Mike Zheng Shou'], 'affiliations': ['Show Lab, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.19325.jpg', 'data': {'categories': ['#training', '#multimodal', '#long_context', '#video'], 'emoji': '🎬', 'ru': {'title': 'FAR: эффективное моделирование длинных видеопоследовательностей', 'desc': 'Статья представляет Frame AutoRegressive (FAR) - новый подход к авторегрессионному моделированию видео с длинным контекстом. Авторы вводят FlexRoPE - технику, позволяющую экстраполировать модель на контексты в 16 раз длиннее обучающих. Предлагается комбинировать моделирование краткосрочного и долгосрочного контекста для эффективной обработки длинных видеопоследовательностей. FAR демонстрирует наилучшие результаты в генерации как коротких, так и длинных видео.'}, 'en': {'title': 'Revolutionizing Video Generation with Frame AutoRegressive Modeling', 'desc': 'This paper presents Frame AutoRegressive (FAR), a new method for generating videos by modeling the temporal dependencies between frames. FAR improves upon existing models by addressing the challenges of visual redundancy and the limitations of current positional encoding techniques like RoPE. The authors introduce FlexRoPE, which allows for flexible temporal decay, enabling the model to handle longer video sequences effectively. By combining short-term and long-term context modeling, FAR achieves state-of-the-art results in video generation while maintaining computational efficiency.'}, 'zh': {'title': '长时间上下文视频生成的新突破', 'desc': '本文介绍了一种新的视频自回归建模方法，称为Frame AutoRegressive (FAR)，旨在解决长时间上下文视频生成中的挑战。FAR通过建模连续帧之间的时间因果关系，超越了传统的语言模型，取得了更好的收敛效果。为了应对视觉冗余和计算成本问题，本文提出了FlexRoPE技术，能够灵活调整远程上下文的时间衰减，并引入了长短期上下文建模方法，以确保时间一致性。实验结果表明，FAR在短视频和长视频生成任务中均达到了最先进的性能，成为视频自回归建模的有效基线。'}}}, {'id': 'https://huggingface.co/papers/2503.18931', 'title': 'CoMP: Continual Multimodal Pre-training for Vision Foundation Models', 'url': 'https://huggingface.co/papers/2503.18931', 'abstract': 'Pre-trained Vision Foundation Models (VFMs) provide strong visual representations for a wide range of applications. In this paper, we continually pre-train prevailing VFMs in a multimodal manner such that they can effortlessly process visual inputs of varying sizes and produce visual representations that are more aligned with language representations, regardless of their original pre-training process. To this end, we introduce CoMP, a carefully designed multimodal pre-training pipeline. CoMP uses a Continual Rotary Position Embedding to support native resolution continual pre-training, and an Alignment Loss between visual and textual features through language prototypes to align multimodal representations. By three-stage training, our VFMs achieve remarkable improvements not only in multimodal understanding but also in other downstream tasks such as classification and segmentation. Remarkably, CoMP-SigLIP achieves scores of 66.7 on ChartQA and 75.9 on DocVQA with a 0.5B LLM, while maintaining an 87.4% accuracy on ImageNet-1K and a 49.5 mIoU on ADE20K under frozen chunk evaluation.', 'score': 15, 'issue_id': 2897, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': '59128d6a0bd3862c', 'authors': ['Yitong Chen', 'Lingchen Meng', 'Wujian Peng', 'Zuxuan Wu', 'Yu-Gang Jiang'], 'affiliations': ['Shanghai Innovation Institute', 'Shanghai Key Lab of Intell. Info. Processing, School of CS, Fudan University'], 'pdf_title_img': 'assets/pdf/title_img/2503.18931.jpg', 'data': {'categories': ['#alignment', '#multimodal', '#optimization', '#training', '#cv'], 'emoji': '🔀', 'ru': {'title': 'Универсальное мультимодальное дообучение для улучшения визуальных моделей', 'desc': 'Статья представляет CoMP - новый метод мультимодального дообучения предобученных моделей компьютерного зрения (VFM). CoMP использует непрерывное ротационное позиционное кодирование для обработки изображений разного размера и функцию выравнивания для согласования визуальных и текстовых представлений. Трехэтапное обучение значительно улучшает результаты как в мультимодальном понимании, так и в задачах классификации и сегментации. Модель CoMP-SigLIP достигает высоких показателей на различных бенчмарках, сохраняя при этом хорошую точность на ImageNet-1K и ADE20K.'}, 'en': {'title': 'Enhancing Visual Models with Multimodal Pre-Training', 'desc': 'This paper presents a method to enhance Vision Foundation Models (VFMs) by using a multimodal pre-training approach. The proposed method, called CoMP, allows VFMs to process visual inputs of different sizes and align visual representations with language representations. CoMP employs a Continual Rotary Position Embedding and an Alignment Loss to improve the integration of visual and textual features. The results show significant performance gains in multimodal tasks and other applications like classification and segmentation, demonstrating the effectiveness of the proposed training pipeline.'}, 'zh': {'title': '提升视觉模型的多模态预训练方法', 'desc': '本文介绍了一种新的多模态预训练方法CoMP，用于提升视觉基础模型（VFM）的表现。通过持续的多模态预训练，模型能够处理不同大小的视觉输入，并生成与语言表示更一致的视觉表示。CoMP采用了持续旋转位置嵌入和视觉与文本特征之间的对齐损失，以实现多模态表示的对齐。经过三阶段训练，我们的模型在多模态理解和其他下游任务上都取得了显著的提升。'}}}, {'id': 'https://huggingface.co/papers/2503.19385', 'title': 'Inference-Time Scaling for Flow Models via Stochastic Generation and\n  Rollover Budget Forcing', 'url': 'https://huggingface.co/papers/2503.19385', 'abstract': 'We propose an inference-time scaling approach for pretrained flow models. Recently, inference-time scaling has gained significant attention in LLMs and diffusion models, improving sample quality or better aligning outputs with user preferences by leveraging additional computation. For diffusion models, particle sampling has allowed more efficient scaling due to the stochasticity at intermediate denoising steps. On the contrary, while flow models have gained popularity as an alternative to diffusion models--offering faster generation and high-quality outputs in state-of-the-art image and video generative models--efficient inference-time scaling methods used for diffusion models cannot be directly applied due to their deterministic generative process. To enable efficient inference-time scaling for flow models, we propose three key ideas: 1) SDE-based generation, enabling particle sampling in flow models, 2) Interpolant conversion, broadening the search space and enhancing sample diversity, and 3) Rollover Budget Forcing (RBF), an adaptive allocation of computational resources across timesteps to maximize budget utilization. Our experiments show that SDE-based generation, particularly variance-preserving (VP) interpolant-based generation, improves the performance of particle sampling methods for inference-time scaling in flow models. Additionally, we demonstrate that RBF with VP-SDE achieves the best performance, outperforming all previous inference-time scaling approaches.', 'score': 12, 'issue_id': 2896, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 марта', 'en': 'March 25', 'zh': '3月25日'}, 'hash': 'e0ead8fbe973f326', 'authors': ['Jaihoon Kim', 'Taehoon Yoon', 'Jisung Hwang', 'Minhyuk Sung'], 'affiliations': ['KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2503.19385.jpg', 'data': {'categories': ['#inference', '#diffusion', '#optimization', '#video'], 'emoji': '🌊', 'ru': {'title': 'Эффективное масштабирование потоковых моделей при выводе', 'desc': 'Статья предлагает метод масштабирования во время вывода для предобученных потоковых моделей. Авторы вводят три ключевые идеи: генерация на основе стохастических дифференциальных уравнений (SDE), преобразование интерполянтов и адаптивное распределение вычислительных ресурсов. Эксперименты показывают, что генерация на основе SDE, особенно с сохранением дисперсии, улучшает производительность методов выборки частиц для масштабирования во время вывода в потоковых моделях. Предложенный подход превосходит предыдущие методы масштабирования во время вывода.'}, 'en': {'title': 'Enhancing Flow Models with Efficient Inference-Time Scaling', 'desc': 'This paper introduces a new method for improving flow models during inference, which is the process of generating outputs from trained models. The authors focus on three innovative techniques: using stochastic differential equations (SDE) for particle sampling, converting interpolants to increase diversity in generated samples, and implementing Rollover Budget Forcing (RBF) to optimize computational resource allocation. These methods allow flow models to achieve better sample quality and efficiency, similar to advancements seen in diffusion models. The results show that their approach significantly enhances performance, making flow models more competitive in generating high-quality images and videos.'}, 'zh': {'title': '流模型的高效推理时间缩放新方法', 'desc': '本文提出了一种针对预训练流模型的推理时间缩放方法。近年来，推理时间缩放在大语言模型和扩散模型中受到广泛关注，通过利用额外的计算来提高样本质量或更好地符合用户偏好。尽管流模型作为扩散模型的替代方案越来越受欢迎，但由于其确定性生成过程，现有的扩散模型推理时间缩放方法无法直接应用于流模型。我们提出了三种关键思想，以实现流模型的高效推理时间缩放：基于SDE的生成、插值转换和自适应计算资源分配。'}}}, {'id': 'https://huggingface.co/papers/2503.19622', 'title': 'Exploring Hallucination of Large Multimodal Models in Video\n  Understanding: Benchmark, Analysis and Mitigation', 'url': 'https://huggingface.co/papers/2503.19622', 'abstract': 'The hallucination of large multimodal models (LMMs), providing responses that appear correct but are actually incorrect, limits their reliability and applicability. This paper aims to study the hallucination problem of LMMs in video modality, which is dynamic and more challenging compared to static modalities like images and text. From this motivation, we first present a comprehensive benchmark termed HAVEN for evaluating hallucinations of LMMs in video understanding tasks. It is built upon three dimensions, i.e., hallucination causes, hallucination aspects, and question formats, resulting in 6K questions. Then, we quantitatively study 7 influential factors on hallucinations, e.g., duration time of videos, model sizes, and model reasoning, via experiments of 16 LMMs on the presented benchmark. In addition, inspired by recent thinking models like OpenAI o1, we propose a video-thinking model to mitigate the hallucinations of LMMs via supervised reasoning fine-tuning (SRFT) and direct preference optimization (TDPO)-- where SRFT enhances reasoning capabilities while TDPO reduces hallucinations in the thinking process. Extensive experiments and analyses demonstrate the effectiveness. Remarkably, it improves the baseline by 7.65% in accuracy on hallucination evaluation and reduces the bias score by 4.5%. The code and data are public at https://github.com/Hongcheng-Gao/HAVEN.', 'score': 10, 'issue_id': 2897, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 марта', 'en': 'March 25', 'zh': '3月25日'}, 'hash': 'c727aefeccdca380', 'authors': ['Hongcheng Gao', 'Jiashu Qu', 'Jingyi Tang', 'Baolong Bi', 'Yue Liu', 'Hongyu Chen', 'Li Liang', 'Li Su', 'Qingming Huang'], 'affiliations': ['Beijing Jiaotong University', 'Key Lab of Intell. Info. Process., Inst. of Comput. Tech., CAS', 'National University of Singapore', 'University of Chinese Academy of Sciences', 'University of Cincinnati'], 'pdf_title_img': 'assets/pdf/title_img/2503.19622.jpg', 'data': {'categories': ['#video', '#multimodal', '#benchmark', '#hallucinations', '#training', '#reasoning'], 'emoji': '🎥', 'ru': {'title': 'Борьба с галлюцинациями в видеоанализе: новый бенчмарк и мыслящая модель', 'desc': "Статья представляет комплексный подход к изучению проблемы галлюцинаций в крупных мультимодальных моделях (LMM) при обработке видео. Авторы создали бенчмарк HAVEN для оценки галлюцинаций LMM в задачах понимания видео, охватывающий различные аспекты и форматы вопросов. Исследование включает анализ семи факторов, влияющих на галлюцинации, и эксперименты с 16 различными LMM. Предложена модель 'video-thinking' для снижения галлюцинаций с использованием методов SRFT и TDPO, показавшая значительное улучшение точности и снижение предвзятости."}, 'en': {'title': 'Mitigating Hallucinations in Video Understanding with HAVEN', 'desc': 'This paper addresses the issue of hallucinations in large multimodal models (LMMs), particularly in video understanding tasks. It introduces a benchmark called HAVEN, which evaluates hallucinations based on various factors such as causes, aspects, and question formats, comprising 6,000 questions. The authors analyze seven influential factors affecting hallucinations and propose a novel video-thinking model that employs supervised reasoning fine-tuning and direct preference optimization to reduce these hallucinations. Experimental results show a significant improvement in accuracy and a reduction in bias, demonstrating the effectiveness of the proposed methods.'}, 'zh': {'title': '解决视频模态中的幻觉问题', 'desc': '本文研究了大型多模态模型（LMMs）在视频理解任务中的幻觉问题，这种问题使得模型的输出看似正确但实际上不准确。我们提出了一个名为HAVEN的基准，用于评估LMMs在视频模态下的幻觉，涵盖了幻觉的原因、方面和问题格式等三个维度，共包含6000个问题。通过对16个LMMs进行实验，我们定量分析了影响幻觉的7个因素，如视频时长、模型规模和推理能力。最后，我们提出了一种视频思维模型，通过监督推理微调（SRFT）和直接偏好优化（TDPO）来减轻幻觉现象，实验结果显示该方法在准确性上提高了7.65%。'}}}, {'id': 'https://huggingface.co/papers/2503.19903', 'title': 'Scaling Vision Pre-Training to 4K Resolution', 'url': 'https://huggingface.co/papers/2503.19903', 'abstract': 'High-resolution perception of visual details is crucial for daily tasks. Current vision pre-training, however, is still limited to low resolutions (e.g., 378 x 378 pixels) due to the quadratic cost of processing larger images. We introduce PS3 that scales CLIP-style vision pre-training to 4K resolution with a near-constant cost. Instead of contrastive learning on global image representation, PS3 is pre-trained by selectively processing local regions and contrasting them with local detailed captions, enabling high-resolution representation learning with greatly reduced computational overhead. The pre-trained PS3 is able to both encode the global image at low resolution and selectively process local high-resolution regions based on their saliency or relevance to a text prompt. When applying PS3 to multi-modal LLM (MLLM), the resulting model, named VILA-HD, significantly improves high-resolution visual perception compared to baselines without high-resolution vision pre-training such as AnyRes and S^2 while using up to 4.3x fewer tokens. PS3 also unlocks appealing scaling properties of VILA-HD, including scaling up resolution for free and scaling up test-time compute for better performance. Compared to state of the arts, VILA-HD outperforms previous MLLMs such as NVILA and Qwen2-VL across multiple benchmarks and achieves better efficiency than latest token pruning approaches. Finally, we find current benchmarks do not require 4K-resolution perception, which motivates us to propose 4KPro, a new benchmark of image QA at 4K resolution, on which VILA-HD outperforms all previous MLLMs, including a 14.5% improvement over GPT-4o, and a 3.2% improvement and 2.96x speedup over Qwen2-VL.', 'score': 6, 'issue_id': 2898, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 марта', 'en': 'March 25', 'zh': '3月25日'}, 'hash': '7b81adffd1f39557', 'authors': ['Baifeng Shi', 'Boyi Li', 'Han Cai', 'Yao Lu', 'Sifei Liu', 'Marco Pavone', 'Jan Kautz', 'Song Han', 'Trevor Darrell', 'Pavlo Molchanov', 'Hongxu Yin'], 'affiliations': ['NVIDIA', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2503.19903.jpg', 'data': {'categories': ['#optimization', '#training', '#cv', '#multimodal', '#benchmark'], 'emoji': '🔍', 'ru': {'title': 'PS3: Эффективное предобучение для восприятия изображений сверхвысокого разрешения', 'desc': 'PS3 - это новый метод предобучения моделей компьютерного зрения, позволяющий работать с изображениями сверхвысокого разрешения (4K) при почти постоянных вычислительных затратах. Вместо контрастивного обучения на глобальном представлении изображения, PS3 обрабатывает выборочно локальные области и сопоставляет их с детальными текстовыми описаниями. Применение PS3 в мультимодальных языковых моделях (MLLM) значительно улучшает восприятие деталей изображений высокого разрешения по сравнению с базовыми моделями. PS3 также позволяет масштабировать разрешение и вычислительные ресурсы без дополнительных затрат, что приводит к повышению производительности.'}, 'en': {'title': 'Scaling Vision Pre-Training to 4K with PS3', 'desc': 'This paper presents PS3, a novel approach to vision pre-training that enables high-resolution image processing at 4K resolution while maintaining a near-constant computational cost. By focusing on local regions of images and contrasting them with detailed captions, PS3 enhances the learning of visual representations without the heavy resource demands typically associated with high-resolution data. The resulting model, VILA-HD, demonstrates significant improvements in visual perception tasks compared to existing models, achieving better efficiency and performance across various benchmarks. Additionally, the authors introduce 4KPro, a new benchmark for image question answering at 4K resolution, where VILA-HD shows superior results over previous models.'}, 'zh': {'title': '高分辨率视觉感知的新突破', 'desc': '本论文介绍了一种名为PS3的视觉预训练方法，能够以接近恒定的成本将CLIP风格的视觉预训练扩展到4K分辨率。PS3通过选择性处理局部区域并与局部详细描述进行对比，来实现高分辨率表示学习，从而大幅降低计算开销。预训练后的PS3能够在低分辨率下编码全局图像，并根据文本提示的显著性或相关性选择性处理局部高分辨率区域。最终，基于PS3的多模态大语言模型VILA-HD在多个基准测试中显著提升了高分辨率视觉感知能力，超越了现有的最先进模型。'}}}, {'id': 'https://huggingface.co/papers/2503.19910', 'title': 'CoLLM: A Large Language Model for Composed Image Retrieval', 'url': 'https://huggingface.co/papers/2503.19910', 'abstract': 'Composed Image Retrieval (CIR) is a complex task that aims to retrieve images based on a multimodal query. Typical training data consists of triplets containing a reference image, a textual description of desired modifications, and the target image, which are expensive and time-consuming to acquire. The scarcity of CIR datasets has led to zero-shot approaches utilizing synthetic triplets or leveraging vision-language models (VLMs) with ubiquitous web-crawled image-caption pairs. However, these methods have significant limitations: synthetic triplets suffer from limited scale, lack of diversity, and unnatural modification text, while image-caption pairs hinder joint embedding learning of the multimodal query due to the absence of triplet data. Moreover, existing approaches struggle with complex and nuanced modification texts that demand sophisticated fusion and understanding of vision and language modalities. We present CoLLM, a one-stop framework that effectively addresses these limitations. Our approach generates triplets on-the-fly from image-caption pairs, enabling supervised training without manual annotation. We leverage Large Language Models (LLMs) to generate joint embeddings of reference images and modification texts, facilitating deeper multimodal fusion. Additionally, we introduce Multi-Text CIR (MTCIR), a large-scale dataset comprising 3.4M samples, and refine existing CIR benchmarks (CIRR and Fashion-IQ) to enhance evaluation reliability. Experimental results demonstrate that CoLLM achieves state-of-the-art performance across multiple CIR benchmarks and settings. MTCIR yields competitive results, with up to 15% performance improvement. Our refined benchmarks provide more reliable evaluation metrics for CIR models, contributing to the advancement of this important field.', 'score': 5, 'issue_id': 2896, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 марта', 'en': 'March 25', 'zh': '3月25日'}, 'hash': '63f36082e6c27f3e', 'authors': ['Chuong Huynh', 'Jinyu Yang', 'Ashish Tawari', 'Mubarak Shah', 'Son Tran', 'Raffay Hamid', 'Trishul Chilimbi', 'Abhinav Shrivastava'], 'affiliations': ['Amazon', 'Center for Research in Computer Vision, University of Central Florida', 'University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2503.19910.jpg', 'data': {'categories': ['#optimization', '#synthetic', '#dataset', '#multimodal', '#benchmark'], 'emoji': '🖼️', 'ru': {'title': 'CoLLM: Революция в композиционном поиске изображений', 'desc': 'Статья представляет CoLLM - новый фреймворк для решения задачи композиционного поиска изображений (CIR). CoLLM генерирует обучающие триплеты на лету из пар изображение-подпись, что позволяет обучаться без ручной разметки. Авторы используют большие языковые модели для создания совместных эмбеддингов изображений и текстов модификации, улучшая мультимодальное слияние. Также представлен новый крупномасштабный датасет MTCIR и уточнены существующие бенчмарки для более надежной оценки моделей CIR.'}, 'en': {'title': 'Revolutionizing Composed Image Retrieval with CoLLM', 'desc': 'This paper introduces CoLLM, a novel framework for Composed Image Retrieval (CIR) that overcomes the challenges of limited training data. It generates triplets from existing image-caption pairs, allowing for supervised training without the need for manual annotations. By utilizing Large Language Models (LLMs), CoLLM creates joint embeddings that enhance the understanding of complex multimodal queries. The authors also present the Multi-Text CIR (MTCIR) dataset, which significantly improves evaluation metrics and demonstrates state-of-the-art performance in CIR tasks.'}, 'zh': {'title': 'CoLLM：复合图像检索的新突破', 'desc': '本文介绍了一种名为CoLLM的框架，用于解决复合图像检索（CIR）中的数据稀缺问题。该框架通过从图像-文本对中动态生成三元组，避免了手动标注的需求，从而实现了监督学习。我们利用大型语言模型（LLMs）生成参考图像和修改文本的联合嵌入，促进了多模态的深度融合。此外，我们还推出了一个包含340万样本的大规模数据集MTCIR，并改进了现有的CIR基准，以提高评估的可靠性。'}}}, {'id': 'https://huggingface.co/papers/2503.19041', 'title': 'LookAhead Tuning: Safer Language Models via Partial Answer Previews', 'url': 'https://huggingface.co/papers/2503.19041', 'abstract': "Fine-tuning enables large language models (LLMs) to adapt to specific domains, but often undermines their previously established safety alignment. To mitigate the degradation of model safety during fine-tuning, we introduce LookAhead Tuning, which comprises two simple, low-resource, and effective data-driven methods that modify training data by previewing partial answer prefixes. Both methods aim to preserve the model's inherent safety mechanisms by minimizing perturbations to initial token distributions. Comprehensive experiments demonstrate that LookAhead Tuning effectively maintains model safety without sacrificing robust performance on downstream tasks. Our findings position LookAhead Tuning as a reliable and efficient solution for the safe and effective adaptation of LLMs. Code is released at https://github.com/zjunlp/LookAheadTuning.", 'score': 3, 'issue_id': 2896, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': '8495b5a09ddf5611', 'authors': ['Kangwei Liu', 'Mengru Wang', 'Yujie Luo', 'Lin Yuan', 'Mengshu Sun', 'Ningyu Zhang', 'Lei Liang', 'Zhiqiang Zhang', 'Jun Zhou', 'Huajun Chen'], 'affiliations': ['Ant Group', 'Zhejiang University', 'Zhejiang University - Ant Group Joint Laboratory of Knowledge Graph'], 'pdf_title_img': 'assets/pdf/title_img/2503.19041.jpg', 'data': {'categories': ['#training', '#alignment', '#low_resource'], 'emoji': '🔍', 'ru': {'title': 'Безопасная адаптация языковых моделей с сохранением производительности', 'desc': 'Статья представляет новый метод тонкой настройки больших языковых моделей под названием LookAhead Tuning. Этот подход позволяет адаптировать модели к конкретным доменам, сохраняя при этом их изначальную безопасность. Метод основан на модификации обучающих данных путем предварительного просмотра частичных префиксов ответов. Эксперименты показывают, что LookAhead Tuning эффективно поддерживает безопасность модели без ущерба для производительности на целевых задачах.'}, 'en': {'title': 'LookAhead Tuning: Safeguarding LLMs During Fine-Tuning', 'desc': "This paper presents LookAhead Tuning, a novel approach to fine-tuning large language models (LLMs) while preserving their safety alignment. The method involves two data-driven techniques that adjust training data by examining partial answer prefixes, which helps maintain the model's safety mechanisms. By minimizing changes to the initial token distributions, LookAhead Tuning effectively prevents safety degradation during the adaptation process. Experimental results show that this approach not only safeguards model safety but also ensures strong performance on various downstream tasks."}, 'zh': {'title': 'LookAhead Tuning：安全微调大型语言模型的新方法', 'desc': '本论文介绍了一种名为LookAhead Tuning的技术，旨在解决在微调大型语言模型（LLMs）时安全性下降的问题。该方法通过预览部分答案前缀，采用两种简单且低资源的数据驱动方法来修改训练数据。其目标是通过最小化初始标记分布的扰动，保持模型固有的安全机制。实验结果表明，LookAhead Tuning能够有效维护模型的安全性，同时在下游任务中保持强大的性能。'}}}, {'id': 'https://huggingface.co/papers/2503.19470', 'title': 'ReSearch: Learning to Reason with Search for LLMs via Reinforcement\n  Learning', 'url': 'https://huggingface.co/papers/2503.19470', 'abstract': 'Large Language Models (LLMs) have shown remarkable capabilities in reasoning, exemplified by the success of OpenAI-o1 and DeepSeek-R1. However, integrating reasoning with external search processes remains challenging, especially for complex multi-hop questions requiring multiple retrieval steps. We propose ReSearch, a novel framework that trains LLMs to Reason with Search via reinforcement learning without using any supervised data on reasoning steps. Our approach treats search operations as integral components of the reasoning chain, where when and how to perform searches is guided by text-based thinking, and search results subsequently influence further reasoning. We train ReSearch on Qwen2.5-7B(-Instruct) and Qwen2.5-32B(-Instruct) models and conduct extensive experiments. Despite being trained on only one dataset, our models demonstrate strong generalizability across various benchmarks. Analysis reveals that ReSearch naturally elicits advanced reasoning capabilities such as reflection and self-correction during the reinforcement learning process.', 'score': 2, 'issue_id': 2897, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 марта', 'en': 'March 25', 'zh': '3月25日'}, 'hash': '3e50fa3f4f4a6c0c', 'authors': ['Mingyang Chen', 'Tianpeng Li', 'Haoze Sun', 'Yijie Zhou', 'Chenzheng Zhu', 'Fan Yang', 'Zenan Zhou', 'Weipeng Chen', 'Haofen Wang', 'Jeff Z. Pan', 'Wen Zhang', 'Huajun Chen'], 'affiliations': ['Baichuan Inc.', 'The University of Edinburgh', 'Tongji University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.19470.jpg', 'data': {'categories': ['#rl', '#benchmark', '#rag', '#training', '#reasoning'], 'emoji': '🔍', 'ru': {'title': 'Усиление рассуждений ИИ через интеграцию поиска', 'desc': 'Авторы предлагают новый фреймворк ReSearch, который обучает большие языковые модели (LLM) рассуждать с использованием поиска через обучение с подкреплением. Модель учится интегрировать операции поиска в цепочку рассуждений, где текстовое мышление направляет, когда и как выполнять поиск. Эксперименты показывают, что, несмотря на обучение только на одном наборе данных, модели демонстрируют сильную обобщаемость на различных бенчмарках. Анализ выявляет, что ReSearch естественным образом вызывает продвинутые способности рассуждения, такие как рефлексия и самокоррекция.'}, 'en': {'title': 'Empowering LLMs: Reasoning Meets Search with ReSearch', 'desc': 'This paper introduces ReSearch, a new framework that enhances Large Language Models (LLMs) by integrating reasoning with external search processes. It uses reinforcement learning to train LLMs to effectively decide when and how to perform searches, treating these operations as key parts of the reasoning process. The framework is tested on Qwen2.5 models and shows strong performance across different benchmarks, even though it was trained on a single dataset. Notably, ReSearch enables advanced reasoning skills like reflection and self-correction, showcasing its potential for complex question answering.'}, 'zh': {'title': '推理与搜索的完美结合', 'desc': '大型语言模型（LLMs）在推理方面表现出色，但将推理与外部搜索过程结合仍然具有挑战性，尤其是对于复杂的多跳问题。我们提出了ReSearch，一个新颖的框架，通过强化学习训练LLMs进行搜索推理，而不使用任何监督数据。该方法将搜索操作视为推理链的核心部分，搜索的时机和方式由基于文本的思维指导，搜索结果进一步影响推理过程。我们的实验表明，尽管只在一个数据集上训练，ReSearch模型在多个基准测试中展现出强大的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2503.19855', 'title': 'Think Twice: Enhancing LLM Reasoning by Scaling Multi-round Test-time\n  Thinking', 'url': 'https://huggingface.co/papers/2503.19855', 'abstract': "Recent advances in large language models (LLMs), such as OpenAI-o1 and DeepSeek-R1, have demonstrated the effectiveness of test-time scaling, where extended reasoning processes substantially enhance model performance. Despite this, current models are constrained by limitations in handling long texts and reinforcement learning (RL) training efficiency. To address these issues, we propose a simple yet effective test-time scaling approach Multi-round Thinking. This method iteratively refines model reasoning by leveraging previous answers as prompts for subsequent rounds. Extensive experiments across multiple models, including QwQ-32B and DeepSeek-R1, consistently show performance improvements on various benchmarks such as AIME 2024, MATH-500, GPQA-diamond, and LiveCodeBench. For instance, the accuracy of QwQ-32B improved from 80.3% (Round 1) to 82.1% (Round 2) on the AIME 2024 dataset, while DeepSeek-R1 showed a similar increase from 79.7% to 82.0%. These results confirm that Multi-round Thinking is a broadly applicable, straightforward approach to achieving stable enhancements in model performance, underscoring its potential for future developments in test-time scaling techniques. The key prompt: {Original question prompt} The assistant's previous answer is: <answer> {last round answer} </answer>, and please re-answer.", 'score': 1, 'issue_id': 2897, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 марта', 'en': 'March 25', 'zh': '3月25日'}, 'hash': 'c9d16e0d2423104a', 'authors': ['Xiaoyu Tian', 'Sitong Zhao', 'Haotian Wang', 'Shuaiting Chen', 'Yunjie Ji', 'Yiping Peng', 'Han Zhao', 'Xiangang Li'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.19855.jpg', 'data': {'categories': ['#long_context', '#benchmark', '#optimization', '#training', '#reasoning'], 'emoji': '🔄', 'ru': {'title': 'Итеративное улучшение ответов ИИ: простой путь к повышению точности', 'desc': "Статья представляет новый подход к масштабированию языковых моделей во время тестирования, называемый 'Многораундовое мышление'. Этот метод итеративно улучшает рассуждения модели, используя предыдущие ответы в качестве подсказок для последующих раундов. Эксперименты с различными моделями, включая QwQ-32B и DeepSeek-R1, показали стабильное улучшение производительности на нескольких бенчмарках. Результаты подтверждают, что 'Многораундовое мышление' - это широко применимый подход для повышения эффективности языковых моделей."}, 'en': {'title': 'Enhancing Model Performance with Multi-round Thinking', 'desc': "This paper introduces a new method called Multi-round Thinking to improve the performance of large language models (LLMs) during test-time scaling. The approach involves iteratively refining the model's reasoning by using previous answers as prompts for subsequent rounds of questioning. Experiments show that this method leads to significant accuracy improvements across various benchmarks, demonstrating its effectiveness in enhancing model performance. The results indicate that Multi-round Thinking is a simple yet powerful technique that can be widely applied to improve LLMs' reasoning capabilities."}, 'zh': {'title': '多轮思考：提升模型推理的有效方法', 'desc': '本文介绍了一种名为多轮思考的测试时间扩展方法，旨在提高大型语言模型的推理能力。该方法通过将之前的答案作为后续轮次的提示，迭代地优化模型的推理过程。实验结果表明，使用多轮思考后，多个模型在不同基准测试上的表现均有显著提升。比如，QwQ-32B在AIME 2024数据集上的准确率从80.3%提升至82.1%。'}}}, {'id': 'https://huggingface.co/papers/2503.18783', 'title': 'Frequency Dynamic Convolution for Dense Image Prediction', 'url': 'https://huggingface.co/papers/2503.18783', 'abstract': '', 'score': 1, 'issue_id': 2897, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 марта', 'en': 'March 25', 'zh': '3月25日'}, 'hash': '0be08263b46c092e', 'authors': ['Linwei Chen', 'Lin Gu', 'Liang Li', 'Chenggang Yan', 'Ying Fu'], 'affiliations': ['Beijing Institute of Technology', 'Chinese Academy of Sciences', 'Hangzhou Dianzi University', 'RIKEN', 'The University of Tokyo', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.18783.jpg', 'data': {'categories': [], 'emoji': '🤖', 'ru': {'title': 'Эффективное обучение больших языковых моделей', 'desc': 'В статье рассматривается новый подход к обучению больших языковых моделей (LLM), который позволяет значительно сократить время и ресурсы, необходимые для их тренировки. Авторы предлагают использовать метод оптимизации, который адаптируется к особенностям данных, что повышает эффективность обучения. Эксперименты показывают, что предложенный метод позволяет достичь более высоких результатов на стандартных тестах. Это открывает новые возможности для применения LLM в различных областях, таких как обработка естественного языка и генерация текста.'}, 'en': {'title': 'Hybrid Models: Bridging Spatial and Temporal Learning', 'desc': "This paper presents a novel approach to improve the performance of deep learning models by utilizing a hybrid architecture that combines convolutional neural networks (CNNs) with recurrent neural networks (RNNs). The proposed method enhances feature extraction from spatial data while also capturing temporal dependencies, making it suitable for tasks like video analysis and time-series prediction. The authors demonstrate that their model outperforms existing state-of-the-art techniques on several benchmark datasets. Additionally, they provide insights into the model's interpretability and robustness against adversarial attacks."}, 'zh': {'title': '提升预测准确性的创新算法', 'desc': '这篇论文探讨了一种新的机器学习算法，旨在提高模型的预测准确性。作者提出了一种改进的特征选择方法，可以有效减少数据维度，同时保留重要信息。实验结果表明，该算法在多个数据集上表现优于传统方法。通过优化模型的训练过程，研究者希望推动机器学习在实际应用中的效果。'}}}, {'id': 'https://huggingface.co/papers/2503.18446', 'title': 'Latent Space Super-Resolution for Higher-Resolution Image Generation\n  with Diffusion Models', 'url': 'https://huggingface.co/papers/2503.18446', 'abstract': 'In this paper, we propose LSRNA, a novel framework for higher-resolution (exceeding 1K) image generation using diffusion models by leveraging super-resolution directly in the latent space. Existing diffusion models struggle with scaling beyond their training resolutions, often leading to structural distortions or content repetition. Reference-based methods address the issues by upsampling a low-resolution reference to guide higher-resolution generation. However, they face significant challenges: upsampling in latent space often causes manifold deviation, which degrades output quality. On the other hand, upsampling in RGB space tends to produce overly smoothed outputs. To overcome these limitations, LSRNA combines Latent space Super-Resolution (LSR) for manifold alignment and Region-wise Noise Addition (RNA) to enhance high-frequency details. Our extensive experiments demonstrate that integrating LSRNA outperforms state-of-the-art reference-based methods across various resolutions and metrics, while showing the critical role of latent space upsampling in preserving detail and sharpness. The code is available at https://github.com/3587jjh/LSRNA.', 'score': 1, 'issue_id': 2897, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': 'd3e203fb399d6eee', 'authors': ['Jinho Jeong', 'Sangmin Han', 'Jinwoo Kim', 'Seon Joo Kim'], 'affiliations': ['Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2503.18446.jpg', 'data': {'categories': ['#open_source', '#diffusion', '#3d', '#optimization', '#cv'], 'emoji': '🖼️', 'ru': {'title': 'LSRNA: Суперразрешение в латентном пространстве для генерации детализированных изображений', 'desc': 'LSRNA - это новый фреймворк для генерации изображений высокого разрешения (более 1K) с использованием диффузионных моделей. Он решает проблемы существующих методов, которые часто приводят к искажениям структуры или повторению контента при масштабировании. LSRNA сочетает суперразрешение в латентном пространстве (LSR) для выравнивания многообразия и добавление шума по регионам (RNA) для улучшения высокочастотных деталей. Эксперименты показывают, что LSRNA превосходит современные методы на основе референсов по различным разрешениям и метрикам.'}, 'en': {'title': 'Enhancing High-Resolution Image Generation with LSRNA', 'desc': 'This paper introduces LSRNA, a new framework designed to generate high-resolution images (over 1K) using diffusion models by applying super-resolution techniques in the latent space. Traditional diffusion models often struggle with generating images at higher resolutions, leading to issues like distortions and repeated content. The proposed method addresses these challenges by utilizing Latent space Super-Resolution (LSR) for better alignment and Region-wise Noise Addition (RNA) to improve detail in the generated images. Experimental results show that LSRNA significantly outperforms existing reference-based methods, highlighting the importance of latent space upsampling for maintaining image quality.'}, 'zh': {'title': 'LSRNA：超分辨率生成的创新框架', 'desc': '本文提出了一种新颖的框架LSRNA，用于生成高分辨率（超过1K）的图像，利用扩散模型直接在潜在空间中进行超分辨率处理。现有的扩散模型在超出训练分辨率时常常出现结构失真或内容重复的问题。参考基础的方法通过将低分辨率参考图像上采样来指导高分辨率生成，但在潜在空间中上采样会导致流形偏差，从而降低输出质量。LSRNA结合了潜在空间超分辨率（LSR）和区域噪声添加（RNA），有效提升了高频细节，实验结果表明其在各个分辨率和指标上均优于现有的参考基础方法。'}}}, {'id': 'https://huggingface.co/papers/2503.17361', 'title': 'Gumbel-Softmax Flow Matching with Straight-Through Guidance for\n  Controllable Biological Sequence Generation', 'url': 'https://huggingface.co/papers/2503.17361', 'abstract': 'Flow matching in the continuous simplex has emerged as a promising strategy for DNA sequence design, but struggles to scale to higher simplex dimensions required for peptide and protein generation. We introduce Gumbel-Softmax Flow and Score Matching, a generative framework on the simplex based on a novel Gumbel-Softmax interpolant with a time-dependent temperature. Using this interpolant, we introduce Gumbel-Softmax Flow Matching by deriving a parameterized velocity field that transports from smooth categorical distributions to distributions concentrated at a single vertex of the simplex. We alternatively present Gumbel-Softmax Score Matching which learns to regress the gradient of the probability density. Our framework enables high-quality, diverse generation and scales efficiently to higher-dimensional simplices. To enable training-free guidance, we propose Straight-Through Guided Flows (STGFlow), a classifier-based guidance method that leverages straight-through estimators to steer the unconditional velocity field toward optimal vertices of the simplex. STGFlow enables efficient inference-time guidance using classifiers pre-trained on clean sequences, and can be used with any discrete flow method. Together, these components form a robust framework for controllable de novo sequence generation. We demonstrate state-of-the-art performance in conditional DNA promoter design, sequence-only protein generation, and target-binding peptide design for rare disease treatment.', 'score': 1, 'issue_id': 2896, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 марта', 'en': 'March 21', 'zh': '3月21日'}, 'hash': 'b5389a3e5ab241c3', 'authors': ['Sophia Tang', 'Yinuo Zhang', 'Alexander Tong', 'Pranam Chatterjee'], 'affiliations': ['Center of Computational Biology, Duke-NUS Medical School', 'Department of Biomedical Engineering, Duke University', 'Department of Biostatistics and Bioinformatics, Duke University', 'Department of Computer Science, Duke University', 'Management and Technology Program, University of Pennsylvania', 'Mila, Quebec AI Institute', 'Université de Montréal'], 'pdf_title_img': 'assets/pdf/title_img/2503.17361.jpg', 'data': {'categories': ['#diffusion', '#data', '#optimization', '#training', '#architecture', '#healthcare'], 'emoji': '🧬', 'ru': {'title': 'Новый подход к генерации биологических последовательностей на симплексе', 'desc': 'Статья представляет новый метод генеративного моделирования на симплексе, называемый Gumbel-Softmax Flow and Score Matching. Авторы вводят новый интерполянт Gumbel-Softmax с зависящей от времени температурой и используют его для создания параметризованного поля скоростей. Метод позволяет получать высококачественные и разнообразные результаты, эффективно масштабируясь на симплексы высокой размерности. Также предложен метод Straight-Through Guided Flows для управления генерацией без дополнительного обучения.'}, 'en': {'title': 'Revolutionizing Sequence Generation with Gumbel-Softmax Flows', 'desc': 'This paper presents a new method called Gumbel-Softmax Flow and Score Matching for generating DNA sequences, peptides, and proteins. It introduces a Gumbel-Softmax interpolant that helps in transitioning between smooth categorical distributions and specific points in a simplex, which is a mathematical space used for these types of data. The authors also propose a technique called Straight-Through Guided Flows (STGFlow) that uses classifiers to guide the generation process towards optimal outcomes without needing extensive training. Overall, this framework allows for efficient and high-quality generation of biological sequences, achieving impressive results in various applications.'}, 'zh': {'title': '高效生成高维序列的创新框架', 'desc': '本文提出了一种新的生成框架，称为Gumbel-Softmax流和评分匹配，旨在解决DNA序列设计中的高维简单形问题。通过引入时间依赖的Gumbel-Softmax插值，我们能够在简单形上实现高质量和多样化的生成。该框架还包括一种名为STGFlow的分类器引导方法，能够在推理时有效地引导生成过程。我们的研究在条件DNA启动子设计、序列生成的蛋白质和靶向结合肽的设计中展示了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2503.16965', 'title': 'When Words Outperform Vision: VLMs Can Self-Improve Via Text-Only\n  Training For Human-Centered Decision Making', 'url': 'https://huggingface.co/papers/2503.16965', 'abstract': "Embodied decision-making is fundamental for AI agents operating in real-world environments. While Visual Language Models (VLMs) have advanced this capability, they still struggle with complex decisions, particularly in human-centered situations that require deep reasoning about human needs and values. In this study, we systematically evaluate open-sourced VLMs on multimodal human-centered decision-making tasks. We find that LLMs receiving only textual descriptions unexpectedly outperform their VLM counterparts of similar scale that process actual images, suggesting that visual alignment may hinder VLM abilities. To address this challenge, we propose a novel text-only training approach with synthesized textual data. This method strengthens VLMs' language components and transfers the learned abilities to multimodal inference, eliminating the need for expensive image-text paired data. Furthermore, we show that VLMs can achieve substantial performance gains through self-improvement, using training data generated by their LLM counterparts rather than relying on larger teacher models like GPT-4. Our findings establish a more efficient and scalable approach to enhancing VLMs' human-centered decision-making capabilities, opening new avenues for optimizing VLMs through self-improvement mechanisms.", 'score': 1, 'issue_id': 2896, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 марта', 'en': 'March 21', 'zh': '3月21日'}, 'hash': 'ca2e599ff0665dfe', 'authors': ['Zhe Hu', 'Jing Li', 'Yu Yin'], 'affiliations': ['Department of Computer and Data Sciences, Case Western Reserve University', 'Department of Computing, The Hong Kong Polytechnic University', 'Research Centre for Data Science & Artificial Intelligence'], 'pdf_title_img': 'assets/pdf/title_img/2503.16965.jpg', 'data': {'categories': ['#optimization', '#transfer_learning', '#training', '#agents', '#multimodal'], 'emoji': '🤖', 'ru': {'title': 'Самосовершенствование VLM через текстовое обучение', 'desc': 'Это исследование сосредоточено на оценке визуальных языковых моделей (VLM) в задачах принятия решений, ориентированных на человека. Авторы обнаружили, что языковые модели, работающие только с текстом, превосходят VLM аналогичного масштаба, обрабатывающие изображения. Для решения этой проблемы предложен новый подход к обучению, использующий синтезированные текстовые данные. Исследование также демонстрирует, что VLM могут значительно улучшить свою производительность через самосовершенствование, используя данные, сгенерированные их LLM-аналогами.'}, 'en': {'title': 'Enhancing VLMs through Text-Only Training and Self-Improvement', 'desc': 'This paper explores the challenges faced by Visual Language Models (VLMs) in making complex decisions that involve understanding human needs and values. The authors find that VLMs that rely on visual data often perform worse than those using only text, indicating that visual information may complicate decision-making. To improve VLM performance, they propose a new training method that uses synthesized textual data, enhancing the language understanding of VLMs without needing paired image-text data. Additionally, the study demonstrates that VLMs can improve their decision-making abilities by learning from their own generated data, rather than depending on larger models, making the training process more efficient and scalable.'}, 'zh': {'title': '提升VLM人类中心决策能力的新方法', 'desc': '本研究探讨了视觉语言模型（VLMs）在复杂人类中心决策中的表现。我们发现，仅使用文本描述的语言模型（LLMs）在某些任务上意外地超越了处理图像的VLMs，这表明视觉对齐可能会限制VLM的能力。为了解决这个问题，我们提出了一种新的仅基于文本的训练方法，利用合成文本数据来增强VLM的语言能力。我们的研究结果表明，通过自我改进，VLMs可以显著提升其人类中心决策能力，开辟了优化VLM的新途径。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents (1)', '#agi', '#alignment (2)', '#architecture (1)', '#audio', '#benchmark (5)', '#cv (3)', '#data (1)', '#dataset (1)', '#diffusion (3)', '#ethics', '#games', '#graphs', '#hallucinations (1)', '#healthcare (1)', '#inference (1)', '#interpretability', '#leakage', '#long_context (2)', '#low_resource (1)', '#machine_translation', '#math', '#multilingual', '#multimodal (6)', '#open_source (1)', '#optimization (8)', '#plp', '#rag (1)', '#reasoning (3)', '#rl (1)', '#rlhf', '#robotics', '#science', '#security', '#small_models', '#story_generation', '#survey', '#synthetic (1)', '#training (9)', '#transfer_learning (1)', '#video (3)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-03-26 05:11',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-03-26 05:11')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-03-26 05:11')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    