{
    "date": {
        "ru": "9 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
        "en": "September 9",
        "zh": "9æœˆ9æ—¥"
    },
    "time_utc": "2024-09-09 09:00",
    "weekday": 0,
    "issue_id": 1,
    "home_page_url": "https://huggingface.co/papers?date=2024-09-09",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2409.03810",
            "title": "How Do Your Code LLMs Perform? Empowering Code Instruction Tuning with High-Quality Data",
            "url": "https://huggingface.co/papers/2409.03810",
            "abstract": "Recently, there has been a growing interest in studying how to construct better code instruction tuning data. However, we observe Code models trained with these datasets exhibit high performance on HumanEval but perform worse on other benchmarks such as LiveCodeBench. Upon further investigation, we find that many datasets suffer from severe data leakage. After cleaning up most of the leaked data, some well-known high-quality datasets perform poorly. This discovery reveals a new challenge: identifying which dataset genuinely qualify as high-quality code instruction data. To address this, we propose an efficient code data pruning strategy for selecting good samples. Our approach is based on three dimensions: instruction complexity, response quality, and instruction diversity. Based on our selected data, we present XCoder, a family of models finetuned from LLaMA3. Our experiments show XCoder achieves new state-of-the-art performance using fewer training data, which verify the effectiveness of our data strategy. Moreover, we perform a comprehensive analysis on the data composition and find existing code datasets have different characteristics according to their construction methods, which provide new insights for future code LLMs. Our models and dataset are released in https://github.com/banksy23/XCoder",
            "score": 30,
            "issue_id": 1,
            "pub_date": "2024-09-05",
            "pub_date_card": {
                "ru": "5 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 5",
                "zh": "9æœˆ5æ—¥"
            },
            "hash": "a49522a282f4ae2f",
            "data": {
                "categories": [
                    "#dataset",
                    "#data",
                    "#training"
                ],
                "emoji": "ğŸ§¹",
                "ru": {
                    "title": "ĞÑ‡Ğ¸ÑÑ‚ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑƒÑ‚ĞµÑ‡ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² ĞºĞ¾Ğ´Ğ° Ğ¿Ğ¾ Ñ‚Ñ€ĞµĞ¼ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ÑĞ¼: ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹, ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±Ñ‹Ğ»Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ° ÑĞµĞ¼ÑŒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ XCoder, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³ÑˆĞ°Ñ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¼ Ğ¾Ğ±ÑŠĞµĞ¼Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ», Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ¼ĞµÑÑ‚ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¸Ñ… ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Unlocking Code Quality: Pruning for Performance",
                    "desc": "This paper addresses the challenge of creating high-quality code instruction tuning datasets for machine learning models. It identifies that many existing datasets have issues with data leakage, which can lead to misleading performance results on benchmarks. The authors propose a data pruning strategy that evaluates datasets based on instruction complexity, response quality, and instruction diversity. They introduce XCoder, a family of models fine-tuned from LLaMA3, which demonstrates state-of-the-art performance with less training data, highlighting the importance of dataset quality in training effective code models."
                },
                "zh": {
                    "title": "ä¼˜åŒ–ä»£ç æŒ‡ä»¤æ•°æ®ï¼Œæå‡æ¨¡å‹æ€§èƒ½",
                    "desc": "æœ€è¿‘ï¼Œç ”ç©¶å¦‚ä½•æ„å»ºæ›´å¥½çš„ä»£ç æŒ‡ä»¤è°ƒä¼˜æ•°æ®å¼•èµ·äº†è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚æˆ‘ä»¬å‘ç°ï¼Œä½¿ç”¨è¿™äº›æ•°æ®é›†è®­ç»ƒçš„ä»£ç æ¨¡å‹åœ¨HumanEvalä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å…¶ä»–åŸºå‡†æµ‹è¯•ï¼ˆå¦‚LiveCodeBenchï¼‰ä¸Šè¡¨ç°è¾ƒå·®ã€‚ç»è¿‡è°ƒæŸ¥ï¼Œæˆ‘ä»¬å‘ç°è®¸å¤šæ•°æ®é›†å­˜åœ¨ä¸¥é‡çš„æ•°æ®æ³„æ¼é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„ä»£ç æ•°æ®ä¿®å‰ªç­–ç•¥ï¼Œä»¥é€‰æ‹©ä¼˜è´¨æ ·æœ¬ï¼Œå¹¶åŸºäºæ­¤å¼€å‘äº†XCoderæ¨¡å‹ï¼Œå®éªŒç»“æœè¡¨æ˜å…¶åœ¨ä½¿ç”¨æ›´å°‘è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹è¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.02877",
            "title": "Configurable Foundation Models: Building LLMs from a Modular Perspective",
            "url": "https://huggingface.co/papers/2409.02877",
            "abstract": "Advancements in LLMs have recently unveiled challenges tied to computational efficiency and continual scalability due to their requirements of huge parameters, making the applications and evolution of these models on devices with limited computation resources and scenarios requiring various abilities increasingly cumbersome. Inspired by modularity within the human brain, there is a growing tendency to decompose LLMs into numerous functional modules, allowing for inference with part of modules and dynamic assembly of modules to tackle complex tasks, such as mixture-of-experts. To highlight the inherent efficiency and composability of the modular approach, we coin the term brick to represent each functional module, designating the modularized structure as configurable foundation models. In this paper, we offer a comprehensive overview and investigation of the construction, utilization, and limitation of configurable foundation models. We first formalize modules into emergent bricks - functional neuron partitions that emerge during the pre-training phase, and customized bricks - bricks constructed via additional post-training to improve the capabilities and knowledge of LLMs. Based on diverse functional bricks, we further present four brick-oriented operations: retrieval and routing, merging, updating, and growing. These operations allow for dynamic configuration of LLMs based on instructions to handle complex tasks. To verify our perspective, we conduct an empirical analysis on widely-used LLMs. We find that the FFN layers follow modular patterns with functional specialization of neurons and functional neuron partitions. Finally, we highlight several open issues and directions for future research. Overall, this paper aims to offer a fresh modular perspective on existing LLM research and inspire the future creation of more efficient and scalable foundational models.",
            "score": 27,
            "issue_id": 1,
            "pub_date": "2024-09-04",
            "pub_date_card": {
                "ru": "4 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 4",
                "zh": "9æœˆ4æ—¥"
            },
            "hash": "3052a767e1ef6d17",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ§±",
                "ru": {
                    "title": "ĞœĞ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· 'ĞºĞ¸Ñ€Ğ¿Ğ¸Ñ‡Ğ¸ĞºĞ¸'",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… 'ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸'. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸ ('ĞºĞ¸Ñ€Ğ¿Ğ¸Ñ‡Ğ¸ĞºĞ¸') Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸. ĞĞ¿Ğ¸ÑÑ‹Ğ²Ğ°ÑÑ‚ÑÑ Ğ´Ğ²Ğ° Ñ‚Ğ¸Ğ¿Ğ° 'ĞºĞ¸Ñ€Ğ¿Ğ¸Ñ‡Ğ¸ĞºĞ¾Ğ²': Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‰Ğ¸Ğµ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµĞ¼Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ 'ĞºĞ¸Ñ€Ğ¿Ğ¸Ñ‡Ğ¸ĞºĞ°Ğ¼Ğ¸': Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ, Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ½Ğ°Ñ€Ğ°Ñ‰Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğµ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡."
                },
                "en": {
                    "title": "Modular Bricks: Redefining Efficiency in Large Language Models",
                    "desc": "This paper discusses the challenges of using large language models (LLMs) due to their high computational demands and the need for scalability. It proposes a modular approach inspired by the human brain, where LLMs are broken down into smaller functional units called 'bricks'. These bricks can be dynamically assembled and configured to perform complex tasks more efficiently, allowing for operations like retrieval, merging, and updating. The authors provide an analysis of existing LLMs to demonstrate the modular patterns and suggest future research directions to enhance the efficiency and scalability of foundational models."
                },
                "zh": {
                    "title": "æ¨¡å—åŒ–æ€ç»´ï¼Œæå‡LLMsæ•ˆç‡ä¸å¯æ‰©å±•æ€§",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è®¡ç®—æ•ˆç‡å’Œå¯æ‰©å±•æ€§æ–¹é¢çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºæœ‰é™çš„è®¾å¤‡ä¸Šåº”ç”¨æ—¶çš„å›°éš¾ã€‚å—äººè„‘æ¨¡å—åŒ–çš„å¯å‘ï¼Œç ”ç©¶è€…ä»¬æå‡ºå°†LLMsåˆ†è§£ä¸ºå¤šä¸ªåŠŸèƒ½æ¨¡å—ï¼Œä»¥ä¾¿åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶å¯ä»¥åŠ¨æ€ç»„åˆè¿™äº›æ¨¡å—ã€‚æˆ‘ä»¬å¼•å…¥äº†â€œç –å—â€è¿™ä¸€æœ¯è¯­ï¼Œè¡¨ç¤ºæ¯ä¸ªåŠŸèƒ½æ¨¡å—ï¼Œå¹¶å°†è¿™ç§æ¨¡å—åŒ–ç»“æ„ç§°ä¸ºå¯é…ç½®åŸºç¡€æ¨¡å‹ã€‚é€šè¿‡å¯¹ç°æœ‰LLMsçš„å®è¯åˆ†æï¼Œè®ºæ–‡å±•ç¤ºäº†æ¨¡å—åŒ–æ–¹æ³•çš„æ•ˆç‡å’Œç»„åˆæ€§ï¼Œå¹¶æå‡ºäº†æœªæ¥ç ”ç©¶çš„æ–¹å‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.04410",
            "title": "Open-MAGVIT2: An Open-Source Project Toward Democratizing Auto-regressive Visual Generation",
            "url": "https://huggingface.co/papers/2409.04410",
            "abstract": "We present Open-MAGVIT2, a family of auto-regressive image generation models ranging from 300M to 1.5B. The Open-MAGVIT2 project produces an open-source replication of Google's MAGVIT-v2 tokenizer, a tokenizer with a super-large codebook (i.e., 2^{18} codes), and achieves the state-of-the-art reconstruction performance (1.17 rFID) on ImageNet 256 times 256. Furthermore, we explore its application in plain auto-regressive models and validate scalability properties. To assist auto-regressive models in predicting with a super-large vocabulary, we factorize it into two sub-vocabulary of different sizes by asymmetric token factorization, and further introduce \"next sub-token prediction\" to enhance sub-token interaction for better generation quality. We release all models and codes to foster innovation and creativity in the field of auto-regressive visual generation.",
            "score": 23,
            "issue_id": 1,
            "pub_date": "2024-09-06",
            "pub_date_card": {
                "ru": "6 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 6",
                "zh": "9æœˆ6æ—¥"
            },
            "hash": "edc9198079ffccb5",
            "data": {
                "categories": [
                    "#cv",
                    "#architecture",
                    "#synthetic"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Open-MAGVIT2: ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Open-MAGVIT2 - ÑÑ‚Ğ¾ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸ Ğ¾Ñ‚ 300Ğœ Ğ´Ğ¾ 1.5B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞŸÑ€Ğ¾ĞµĞºÑ‚ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ€ĞµĞ¿Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ° MAGVIT-v2 Ğ¾Ñ‚ Google Ñ Ğ¾Ğ³Ñ€Ğ¾Ğ¼Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ±ÑƒĞºĞ¾Ğ¼ Ğ¸Ğ· 2^18 ĞºĞ¾Ğ´Ğ¾Ğ², Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ImageNet 256x256. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ°ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡Ğ½ÑƒÑ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ ÑÑƒĞ±-Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ’ÑĞµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ĞºĞ¾Ğ´ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ° Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹ Ğ´Ğ»Ñ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°."
                },
                "en": {
                    "title": "Unlocking Image Generation with Open-MAGVIT2",
                    "desc": "Open-MAGVIT2 is a series of advanced auto-regressive models designed for generating images, with sizes ranging from 300 million to 1.5 billion parameters. It replicates Google's MAGVIT-v2 tokenizer, which features a very large codebook of 2^{18} codes, achieving top-notch image reconstruction performance on ImageNet. The paper discusses the scalability of these models and introduces a method called asymmetric token factorization to manage a super-large vocabulary by splitting it into two sub-vocabularies. Additionally, the authors propose a technique for 'next sub-token prediction' to improve the interaction between sub-tokens, ultimately enhancing the quality of generated images."
                },
                "zh": {
                    "title": "å¼€æ”¾è‡ªå›å½’å›¾åƒç”Ÿæˆçš„æ–°çªç ´",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†Open-MAGVIT2ï¼Œè¿™æ˜¯ä¸€ä¸ªè‡ªå›å½’å›¾åƒç”Ÿæˆæ¨¡å‹ç³»åˆ—ï¼Œè§„æ¨¡ä»3äº¿åˆ°15äº¿å‚æ•°ä¸ç­‰ã€‚è¯¥é¡¹ç›®å¤ç°äº†è°·æ­Œçš„MAGVIT-v2åˆ†è¯å™¨ï¼Œå…·æœ‰è¶…å¤§è¯æ±‡è¡¨ï¼Œè¾¾åˆ°å›¾åƒé‡å»ºçš„æœ€å…ˆè¿›æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜æ¢è®¨äº†å…¶åœ¨è‡ªå›å½’æ¨¡å‹ä¸­çš„åº”ç”¨ï¼Œå¹¶éªŒè¯äº†å…¶å¯æ‰©å±•æ€§ã€‚é€šè¿‡ä¸å¯¹ç§°çš„ä»¤ç‰Œå› å¼åˆ†è§£ï¼Œæˆ‘ä»¬å°†è¶…å¤§è¯æ±‡è¡¨åˆ†è§£ä¸ºä¸¤ä¸ªä¸åŒå¤§å°çš„å­è¯æ±‡ï¼Œå¹¶å¼•å…¥â€œä¸‹ä¸€ä¸ªå­ä»¤ç‰Œé¢„æµ‹â€ä»¥å¢å¼ºå­ä»¤ç‰Œä¹‹é—´çš„äº¤äº’ï¼Œä»è€Œæé«˜ç”Ÿæˆè´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.04005",
            "title": "Qihoo-T2X: An Efficiency-Focused Diffusion Transformer via Proxy Tokens for Text-to-Any-Task",
            "url": "https://huggingface.co/papers/2409.04005",
            "abstract": "The global self-attention mechanism in diffusion transformers involves redundant computation due to the sparse and redundant nature of visual information, and the attention map of tokens within a spatial window shows significant similarity. To address this redundancy, we propose the Proxy Token Diffusion Transformer (PT-DiT), which employs sparse representative token attention (where the number of representative tokens is much smaller than the total number of tokens) to model global visual information efficiently. Specifically, in each transformer block, we randomly sample one token from each spatial-temporal window to serve as a proxy token for that region. The global semantics are captured through the self-attention of these proxy tokens and then injected into all latent tokens via cross-attention. Simultaneously, we introduce window and shift window attention to address the limitations in detail modeling caused by the sparse attention mechanism. Building on the well-designed PT-DiT, we further develop the Qihoo-T2X family, which includes a variety of models for T2I, T2V, and T2MV tasks. Experimental results show that PT-DiT achieves competitive performance while reducing the computational complexity in both image and video generation tasks (e.g., a 48% reduction compared to DiT and a 35% reduction compared to Pixart-alpha). Our source code is available at https://github.com/360CVGroup/Qihoo-T2X.",
            "score": 16,
            "issue_id": 1,
            "pub_date": "2024-09-06",
            "pub_date_card": {
                "ru": "6 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 6",
                "zh": "9æœˆ6æ—¥"
            },
            "hash": "80e3fb2cd8dfe22d",
            "data": {
                "categories": [
                    "#cv",
                    "#video",
                    "#diffusion",
                    "#architecture",
                    "#training"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾ĞºÑĞ¸-Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Proxy Token Diffusion Transformer (PT-DiT) - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ…. PT-DiT Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ñ€ĞµĞ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ñ… Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¾ĞºĞ¾Ğ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑĞ´Ğ²Ğ¸Ğ³Ğ¾Ğ²Ğ¾Ğµ Ğ¾ĞºĞ¾Ğ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ PT-DiT Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ¾ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Qihoo-T2X Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Efficient Visual Processing with Proxy Tokens",
                    "desc": "The paper introduces the Proxy Token Diffusion Transformer (PT-DiT) to improve efficiency in visual information processing by reducing redundant computations in self-attention mechanisms. It utilizes sparse representative token attention, where only a few tokens are selected from each spatial-temporal window to represent global visual information. This method captures global semantics through self-attention on proxy tokens and integrates this information into all latent tokens using cross-attention. The PT-DiT framework is further extended into the Qihoo-T2X family, demonstrating significant reductions in computational complexity while maintaining competitive performance in image and video generation tasks."
                },
                "zh": {
                    "title": "é«˜æ•ˆå»ºæ¨¡è§†è§‰ä¿¡æ¯çš„ä»£ç†ä»¤ç‰Œæ‰©æ•£å˜æ¢å™¨",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¨¡å‹ï¼Œç§°ä¸ºä»£ç†ä»¤ç‰Œæ‰©æ•£å˜æ¢å™¨ï¼ˆPT-DiTï¼‰ï¼Œæ—¨åœ¨è§£å†³æ‰©æ•£å˜æ¢å™¨ä¸­å…¨å±€è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„å†—ä½™è®¡ç®—é—®é¢˜ã€‚PT-DiTé€šè¿‡ç¨€ç–ä»£è¡¨æ€§ä»¤ç‰Œæ³¨æ„åŠ›æ¥é«˜æ•ˆå»ºæ¨¡å…¨å±€è§†è§‰ä¿¡æ¯ï¼Œæ¯ä¸ªç©ºé—´-æ—¶é—´çª—å£éšæœºæŠ½å–ä¸€ä¸ªä»¤ç‰Œä½œä¸ºä»£ç†ä»¤ç‰Œã€‚é€šè¿‡è¿™äº›ä»£ç†ä»¤ç‰Œçš„è‡ªæ³¨æ„åŠ›æ•æ‰å…¨å±€è¯­ä¹‰ï¼Œå¹¶é€šè¿‡äº¤å‰æ³¨æ„åŠ›æ³¨å…¥åˆ°æ‰€æœ‰æ½œåœ¨ä»¤ç‰Œä¸­ã€‚åŒæ—¶ï¼Œæœ¬æ–‡è¿˜å¼•å…¥äº†çª—å£å’Œç§»ä½çª—å£æ³¨æ„åŠ›ï¼Œä»¥è§£å†³ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶åœ¨ç»†èŠ‚å»ºæ¨¡ä¸­çš„å±€é™æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.04196",
            "title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting Transformers",
            "url": "https://huggingface.co/papers/2409.04196",
            "abstract": "Reconstructing realistic 3D human models from monocular images has significant applications in creative industries, human-computer interfaces, and healthcare. We base our work on 3D Gaussian Splatting (3DGS), a scene representation composed of a mixture of Gaussians. Predicting such mixtures for a human from a single input image is challenging, as it is a non-uniform density (with a many-to-one relationship with input pixels) with strict physical constraints. At the same time, it needs to be flexible to accommodate a variety of clothes and poses. Our key observation is that the vertices of standardized human meshes (such as SMPL) can provide an adequate density and approximate initial position for Gaussians. We can then train a transformer model to jointly predict comparatively small adjustments to these positions, as well as the other Gaussians' attributes and the SMPL parameters. We show empirically that this combination (using only multi-view supervision) can achieve fast inference of 3D human models from a single image without test-time optimization, expensive diffusion models, or 3D points supervision. We also show that it can improve 3D pose estimation by better fitting human models that account for clothes and other variations. The code is available on the project website https://abdullahamdi.com/gst/ .",
            "score": 11,
            "issue_id": 1,
            "pub_date": "2024-09-06",
            "pub_date_card": {
                "ru": "6 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 6",
                "zh": "9æœˆ6æ—¥"
            },
            "hash": "49ac50adb0f8ba25",
            "data": {
                "categories": [
                    "#3d",
                    "#cv",
                    "#medicine"
                ],
                "emoji": "ğŸ‘¤",
                "ru": {
                    "title": "Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ»ÑĞ´ĞµĞ¹ Ğ¸Ğ· 2D-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ»ÑĞ´ĞµĞ¹ Ğ¸Ğ· Ğ´Ğ²ÑƒĞ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ 3D Gaussian Splatting. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²ĞµÑ€ÑˆĞ¸Ğ½Ñ‹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, SMPL) Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹. Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ ÑÑ‚Ğ¸Ñ… Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ñ‹ Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ¾Ğ² Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ SMPL. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ»ÑĞ´ĞµĞ¹ Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Transforming Single Images into 3D Human Models with Gaussian Splatting",
                    "desc": "This paper presents a method for creating realistic 3D human models from single images using 3D Gaussian Splatting (3DGS). The authors leverage the structure of standardized human meshes to establish a starting point for Gaussian mixtures, which helps in accurately representing the human form. A transformer model is trained to refine these initial positions and predict additional attributes, allowing for flexibility in clothing and poses. The approach demonstrates efficient inference without the need for complex optimization or supervision, enhancing 3D pose estimation in the process."
                },
                "zh": {
                    "title": "ä»å•å¹…å›¾åƒå¿«é€Ÿé‡å»º3Däººç±»æ¨¡å‹",
                    "desc": "æœ¬æ–‡ç ”ç©¶äº†ä»å•å¹…å›¾åƒé‡å»ºé€¼çœŸçš„3Däººç±»æ¨¡å‹çš„æ–¹æ³•ï¼Œè¿™åœ¨åˆ›æ„äº§ä¸šã€äººæœºäº¤äº’å’ŒåŒ»ç–—ä¿å¥ç­‰é¢†åŸŸå…·æœ‰é‡è¦åº”ç”¨ã€‚æˆ‘ä»¬åŸºäº3Dé«˜æ–¯ç‚¹äº‘ï¼ˆ3DGSï¼‰ä½œä¸ºåœºæ™¯è¡¨ç¤ºï¼Œåˆ©ç”¨æ ‡å‡†åŒ–äººç±»ç½‘æ ¼çš„é¡¶ç‚¹æ¥æä¾›é«˜æ–¯çš„åˆå§‹ä½ç½®å’Œå¯†åº¦ã€‚é€šè¿‡è®­ç»ƒå˜æ¢å™¨æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥é¢„æµ‹è¿™äº›ä½ç½®çš„å°è°ƒæ•´ä»¥åŠå…¶ä»–é«˜æ–¯å±æ€§å’ŒSMPLå‚æ•°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå¿«é€Ÿæ¨æ–­3Däººç±»æ¨¡å‹ï¼Œå¹¶åœ¨3Då§¿æ€ä¼°è®¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œé€‚åº”æœè£…å’Œå§¿åŠ¿çš„å˜åŒ–ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.02076",
            "title": "Spinning the Golden Thread: Benchmarking Long-Form Generation in Language Models",
            "url": "https://huggingface.co/papers/2409.02076",
            "abstract": "The abilities of long-context language models (LMs) are often evaluated using the \"Needle-in-a-Haystack\" (NIAH) test, which comprises tasks designed to assess a model's ability to identify specific information (\"needle\") within large text sequences (\"haystack\"). While these benchmarks measure how well models understand long-context input sequences, they do not effectively gauge the quality of long-form text generation--a critical aspect for applications such as design proposals and creative writing. To address this gap, we have introduced a new long-form text evaluation benchmark, Spinning the Golden Thread (SGT), which tests models' ability to identify specific events within generated long text sequences. In this benchmark, we prompt long-context LMs to create long-form text that must include particular events or constraints and evaluate their ability to incorporate these elements. We evaluated ten long-context LMs across four distinct scenarios, three types of prompt instructions, and two different generation-length settings (16K and 32K). Although these models perform well on NIAH benchmarks, none demonstrated satisfactory performance on the Spinning the Golden Thread, raising concerns about their ability to generate coherent long-form text that follows instructions. Additionally, as the length of the generated text increases, all models exhibit a significant drop in performance.",
            "score": 9,
            "issue_id": 1,
            "pub_date": "2024-09-03",
            "pub_date_card": {
                "ru": "3 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 3",
                "zh": "9æœˆ3æ—¥"
            },
            "hash": "693dae430ac3c4c8",
            "data": {
                "categories": [
                    "#benchmark",
                    "#long_context"
                ],
                "emoji": "ğŸ§µ",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ¿Ğ¾ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ - Spinning the Golden Thread (SGT). Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ñ‚Ğ¸Ğ¿Ğ° 'Ğ¸Ğ³Ğ¾Ğ»ĞºĞ° Ğ² ÑÑ‚Ğ¾Ğ³Ğµ ÑĞµĞ½Ğ°', SGT Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚Ñ‹ Ñ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ğ¸Ğ»Ğ¾ 10 Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ°Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¸Ğ¼Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑĞ¼Ğ¸ Ğ² Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ½Ğµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ SGT, Ñ‡Ñ‚Ğ¾ Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¾Ğ± Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ²ÑĞ·Ğ½Ñ‹Ğµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚Ñ‹ Ğ¿Ğ¾ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Evaluating Long-Form Text Generation: The SGT Benchmark",
                    "desc": "This paper introduces a new evaluation benchmark called Spinning the Golden Thread (SGT) to assess long-context language models (LMs) on their ability to generate coherent long-form text. Unlike the existing Needle-in-a-Haystack (NIAH) test, which focuses on information retrieval, SGT evaluates how well models can incorporate specific events or constraints into their generated text. The study tested ten long-context LMs across various scenarios and prompt types, revealing that while these models excel in NIAH tasks, they struggle with long-form generation. The findings indicate a significant decline in performance as the length of the generated text increases, highlighting a critical gap in the capabilities of current LMs for creative writing and design applications."
                },
                "zh": {
                    "title": "è¯„ä¼°é•¿æ–‡æœ¬ç”Ÿæˆçš„æ–°æ ‡å‡†",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†é•¿æ–‡æœ¬ç”Ÿæˆæ¨¡å‹çš„è¯„ä¼°æ–¹æ³•ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„åŸºå‡†æµ‹è¯•ï¼Œç§°ä¸ºâ€œç¼–ç»‡é‡‘çº¿â€ï¼ˆSGTï¼‰ã€‚è¯¥æµ‹è¯•æ—¨åœ¨è¯„ä¼°æ¨¡å‹åœ¨ç”Ÿæˆé•¿æ–‡æœ¬æ—¶ï¼Œæ˜¯å¦èƒ½å¤Ÿæœ‰æ•ˆåœ°åŒ…å«ç‰¹å®šäº‹ä»¶æˆ–çº¦æŸæ¡ä»¶ã€‚å°½ç®¡ç°æœ‰çš„â€œå¹²è‰å †ä¸­çš„é’ˆâ€ï¼ˆNIAHï¼‰æµ‹è¯•èƒ½å¤Ÿè¯„ä¼°æ¨¡å‹å¯¹é•¿æ–‡æœ¬è¾“å…¥çš„ç†è§£ï¼Œä½†æ— æ³•æœ‰æ•ˆè¡¡é‡é•¿æ–‡æœ¬ç”Ÿæˆçš„è´¨é‡ã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡æ¨¡å‹åœ¨NIAHåŸºå‡†ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨SGTæµ‹è¯•ä¸­å´æœªèƒ½è¾¾åˆ°ä»¤äººæ»¡æ„çš„æ•ˆæœï¼Œå°¤å…¶æ˜¯åœ¨ç”Ÿæˆæ–‡æœ¬é•¿åº¦å¢åŠ æ—¶ï¼Œæ¨¡å‹çš„è¡¨ç°æ˜¾è‘—ä¸‹é™ã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-09-06.html",
    "link_next": "2024-09-10.html",
    "link_month": "2024-09.html",
    "short_date_prev": {
        "ru": "06.09",
        "en": "09/06",
        "zh": "9æœˆ6æ—¥"
    },
    "short_date_next": {
        "ru": "10.09",
        "en": "09/10",
        "zh": "9æœˆ10æ—¥"
    },
    "categories": {
        "#dataset": 1,
        "#data": 1,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 3,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 3,
        "#medicine": 1,
        "#training": 3,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#edge_computing": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#translation": 0
    }
}