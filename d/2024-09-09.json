{
    "date": {
        "ru": "9 сентября",
        "en": "September 9",
        "zh": "9月9日"
    },
    "time_utc": "2024-09-09 09:00",
    "weekday": 0,
    "issue_id": 1,
    "home_page_url": "https://huggingface.co/papers?date=2024-09-09",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2409.03810",
            "title": "How Do Your Code LLMs Perform? Empowering Code Instruction Tuning with High-Quality Data",
            "url": "https://huggingface.co/papers/2409.03810",
            "abstract": "Recently, there has been a growing interest in studying how to construct better code instruction tuning data. However, we observe Code models trained with these datasets exhibit high performance on HumanEval but perform worse on other benchmarks such as LiveCodeBench. Upon further investigation, we find that many datasets suffer from severe data leakage. After cleaning up most of the leaked data, some well-known high-quality datasets perform poorly. This discovery reveals a new challenge: identifying which dataset genuinely qualify as high-quality code instruction data. To address this, we propose an efficient code data pruning strategy for selecting good samples. Our approach is based on three dimensions: instruction complexity, response quality, and instruction diversity. Based on our selected data, we present XCoder, a family of models finetuned from LLaMA3. Our experiments show XCoder achieves new state-of-the-art performance using fewer training data, which verify the effectiveness of our data strategy. Moreover, we perform a comprehensive analysis on the data composition and find existing code datasets have different characteristics according to their construction methods, which provide new insights for future code LLMs. Our models and dataset are released in https://github.com/banksy23/XCoder",
            "score": 30,
            "issue_id": 1,
            "pub_date": "2024-09-05",
            "pub_date_card": {
                "ru": "5 сентября",
                "en": "September 5",
                "zh": "9月5日"
            },
            "hash": "a49522a282f4ae2f",
            "data": {
                "categories": [
                    "#dataset",
                    "#data",
                    "#training"
                ],
                "emoji": "🧹",
                "ru": {
                    "title": "Очистка данных для создания более эффективных моделей программирования",
                    "desc": "Исследователи обнаружили проблему утечки данных в наборах для обучения моделей программирования. Они предложили стратегию отбора качественных образцов кода по трем критериям: сложность инструкций, качество ответов и разнообразие инструкций. На основе отобранных данных была создана семья моделей XCoder, достигшая нового уровня производительности при меньшем объеме обучающих данных. Анализ показал, что существующие наборы данных имеют разные характеристики в зависимости от методов их создания."
                },
                "en": {
                    "title": "Unlocking Code Quality: Pruning for Performance",
                    "desc": "This paper addresses the challenge of creating high-quality code instruction tuning datasets for machine learning models. It identifies that many existing datasets have issues with data leakage, which can lead to misleading performance results on benchmarks. The authors propose a data pruning strategy that evaluates datasets based on instruction complexity, response quality, and instruction diversity. They introduce XCoder, a family of models fine-tuned from LLaMA3, which demonstrates state-of-the-art performance with less training data, highlighting the importance of dataset quality in training effective code models."
                },
                "zh": {
                    "title": "优化代码指令数据，提升模型性能",
                    "desc": "最近，研究如何构建更好的代码指令调优数据引起了越来越多的关注。我们发现，使用这些数据集训练的代码模型在HumanEval上表现良好，但在其他基准测试（如LiveCodeBench）上表现较差。经过调查，我们发现许多数据集存在严重的数据泄漏问题。为了解决这个问题，我们提出了一种高效的代码数据修剪策略，以选择优质样本，并基于此开发了XCoder模型，实验结果表明其在使用更少训练数据的情况下达到了新的最先进性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.02877",
            "title": "Configurable Foundation Models: Building LLMs from a Modular Perspective",
            "url": "https://huggingface.co/papers/2409.02877",
            "abstract": "Advancements in LLMs have recently unveiled challenges tied to computational efficiency and continual scalability due to their requirements of huge parameters, making the applications and evolution of these models on devices with limited computation resources and scenarios requiring various abilities increasingly cumbersome. Inspired by modularity within the human brain, there is a growing tendency to decompose LLMs into numerous functional modules, allowing for inference with part of modules and dynamic assembly of modules to tackle complex tasks, such as mixture-of-experts. To highlight the inherent efficiency and composability of the modular approach, we coin the term brick to represent each functional module, designating the modularized structure as configurable foundation models. In this paper, we offer a comprehensive overview and investigation of the construction, utilization, and limitation of configurable foundation models. We first formalize modules into emergent bricks - functional neuron partitions that emerge during the pre-training phase, and customized bricks - bricks constructed via additional post-training to improve the capabilities and knowledge of LLMs. Based on diverse functional bricks, we further present four brick-oriented operations: retrieval and routing, merging, updating, and growing. These operations allow for dynamic configuration of LLMs based on instructions to handle complex tasks. To verify our perspective, we conduct an empirical analysis on widely-used LLMs. We find that the FFN layers follow modular patterns with functional specialization of neurons and functional neuron partitions. Finally, we highlight several open issues and directions for future research. Overall, this paper aims to offer a fresh modular perspective on existing LLM research and inspire the future creation of more efficient and scalable foundational models.",
            "score": 27,
            "issue_id": 1,
            "pub_date": "2024-09-04",
            "pub_date_card": {
                "ru": "4 сентября",
                "en": "September 4",
                "zh": "9月4日"
            },
            "hash": "3052a767e1ef6d17",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#optimization"
                ],
                "emoji": "🧱",
                "ru": {
                    "title": "Модульный подход к языковым моделям: эффективность через 'кирпичики'",
                    "desc": "Статья рассматривает концепцию модульных языковых моделей, названных 'конфигурируемыми базовыми моделями'. Авторы предлагают разделить большие языковые модели на функциональные модули ('кирпичики') для повышения эффективности и масштабируемости. Описываются два типа 'кирпичиков': возникающие в процессе предобучения и настраиваемые после обучения. Представлены четыре операции с 'кирпичиками': извлечение и маршрутизация, слияние, обновление и наращивание, позволяющие динамически конфигурировать модели для сложных задач."
                },
                "en": {
                    "title": "Modular Bricks: Redefining Efficiency in Large Language Models",
                    "desc": "This paper discusses the challenges of using large language models (LLMs) due to their high computational demands and the need for scalability. It proposes a modular approach inspired by the human brain, where LLMs are broken down into smaller functional units called 'bricks'. These bricks can be dynamically assembled and configured to perform complex tasks more efficiently, allowing for operations like retrieval, merging, and updating. The authors provide an analysis of existing LLMs to demonstrate the modular patterns and suggest future research directions to enhance the efficiency and scalability of foundational models."
                },
                "zh": {
                    "title": "模块化思维，提升LLMs效率与可扩展性",
                    "desc": "本论文探讨了大型语言模型（LLMs）在计算效率和可扩展性方面的挑战，尤其是在资源有限的设备上应用时的困难。受人脑模块化的启发，研究者们提出将LLMs分解为多个功能模块，以便在处理复杂任务时可以动态组合这些模块。我们引入了“砖块”这一术语，表示每个功能模块，并将这种模块化结构称为可配置基础模型。通过对现有LLMs的实证分析，论文展示了模块化方法的效率和组合性，并提出了未来研究的方向。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.04410",
            "title": "Open-MAGVIT2: An Open-Source Project Toward Democratizing Auto-regressive Visual Generation",
            "url": "https://huggingface.co/papers/2409.04410",
            "abstract": "We present Open-MAGVIT2, a family of auto-regressive image generation models ranging from 300M to 1.5B. The Open-MAGVIT2 project produces an open-source replication of Google's MAGVIT-v2 tokenizer, a tokenizer with a super-large codebook (i.e., 2^{18} codes), and achieves the state-of-the-art reconstruction performance (1.17 rFID) on ImageNet 256 times 256. Furthermore, we explore its application in plain auto-regressive models and validate scalability properties. To assist auto-regressive models in predicting with a super-large vocabulary, we factorize it into two sub-vocabulary of different sizes by asymmetric token factorization, and further introduce \"next sub-token prediction\" to enhance sub-token interaction for better generation quality. We release all models and codes to foster innovation and creativity in the field of auto-regressive visual generation.",
            "score": 23,
            "issue_id": 1,
            "pub_date": "2024-09-06",
            "pub_date_card": {
                "ru": "6 сентября",
                "en": "September 6",
                "zh": "9月6日"
            },
            "hash": "edc9198079ffccb5",
            "data": {
                "categories": [
                    "#cv",
                    "#architecture",
                    "#synthetic"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Open-MAGVIT2: Открытая платформа для масштабируемой генерации изображений",
                    "desc": "Open-MAGVIT2 - это семейство авторегрессионных моделей для генерации изображений с размерами от 300М до 1.5B параметров. Проект включает репликацию токенизатора MAGVIT-v2 от Google с огромным кодбуком из 2^18 кодов, достигая наилучших результатов в реконструкции изображений на ImageNet 256x256. Исследователи применяют асимметричную факторизацию токенов и вводят предсказание следующего суб-токена для улучшения качества генерации. Все модели и код проекта открыты для стимулирования инноваций в области авторегрессионной генерации визуального контента."
                },
                "en": {
                    "title": "Unlocking Image Generation with Open-MAGVIT2",
                    "desc": "Open-MAGVIT2 is a series of advanced auto-regressive models designed for generating images, with sizes ranging from 300 million to 1.5 billion parameters. It replicates Google's MAGVIT-v2 tokenizer, which features a very large codebook of 2^{18} codes, achieving top-notch image reconstruction performance on ImageNet. The paper discusses the scalability of these models and introduces a method called asymmetric token factorization to manage a super-large vocabulary by splitting it into two sub-vocabularies. Additionally, the authors propose a technique for 'next sub-token prediction' to improve the interaction between sub-tokens, ultimately enhancing the quality of generated images."
                },
                "zh": {
                    "title": "开放自回归图像生成的新突破",
                    "desc": "我们介绍了Open-MAGVIT2，这是一个自回归图像生成模型系列，规模从3亿到15亿参数不等。该项目复现了谷歌的MAGVIT-v2分词器，具有超大词汇表，达到图像重建的最先进性能。我们还探讨了其在自回归模型中的应用，并验证了其可扩展性。通过不对称的令牌因式分解，我们将超大词汇表分解为两个不同大小的子词汇，并引入“下一个子令牌预测”以增强子令牌之间的交互，从而提高生成质量。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.04005",
            "title": "Qihoo-T2X: An Efficiency-Focused Diffusion Transformer via Proxy Tokens for Text-to-Any-Task",
            "url": "https://huggingface.co/papers/2409.04005",
            "abstract": "The global self-attention mechanism in diffusion transformers involves redundant computation due to the sparse and redundant nature of visual information, and the attention map of tokens within a spatial window shows significant similarity. To address this redundancy, we propose the Proxy Token Diffusion Transformer (PT-DiT), which employs sparse representative token attention (where the number of representative tokens is much smaller than the total number of tokens) to model global visual information efficiently. Specifically, in each transformer block, we randomly sample one token from each spatial-temporal window to serve as a proxy token for that region. The global semantics are captured through the self-attention of these proxy tokens and then injected into all latent tokens via cross-attention. Simultaneously, we introduce window and shift window attention to address the limitations in detail modeling caused by the sparse attention mechanism. Building on the well-designed PT-DiT, we further develop the Qihoo-T2X family, which includes a variety of models for T2I, T2V, and T2MV tasks. Experimental results show that PT-DiT achieves competitive performance while reducing the computational complexity in both image and video generation tasks (e.g., a 48% reduction compared to DiT and a 35% reduction compared to Pixart-alpha). Our source code is available at https://github.com/360CVGroup/Qihoo-T2X.",
            "score": 16,
            "issue_id": 1,
            "pub_date": "2024-09-06",
            "pub_date_card": {
                "ru": "6 сентября",
                "en": "September 6",
                "zh": "9月6日"
            },
            "hash": "80e3fb2cd8dfe22d",
            "data": {
                "categories": [
                    "#cv",
                    "#video",
                    "#diffusion",
                    "#architecture",
                    "#training"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Эффективное глобальное внимание через прокси-токены в диффузионных трансформерах",
                    "desc": "Статья представляет Proxy Token Diffusion Transformer (PT-DiT) - новый подход к оптимизации механизма глобального внимания в диффузионных трансформерах. PT-DiT использует разреженное внимание на репрезентативных токенах для эффективного моделирования глобальной визуальной информации. Авторы также вводят оконное внимание и сдвиговое оконное внимание для улучшения детализации. На основе PT-DiT разработано семейство моделей Qihoo-T2X для задач генерации изображений и видео."
                },
                "en": {
                    "title": "Efficient Visual Processing with Proxy Tokens",
                    "desc": "The paper introduces the Proxy Token Diffusion Transformer (PT-DiT) to improve efficiency in visual information processing by reducing redundant computations in self-attention mechanisms. It utilizes sparse representative token attention, where only a few tokens are selected from each spatial-temporal window to represent global visual information. This method captures global semantics through self-attention on proxy tokens and integrates this information into all latent tokens using cross-attention. The PT-DiT framework is further extended into the Qihoo-T2X family, demonstrating significant reductions in computational complexity while maintaining competitive performance in image and video generation tasks."
                },
                "zh": {
                    "title": "高效建模视觉信息的代理令牌扩散变换器",
                    "desc": "本文提出了一种新的模型，称为代理令牌扩散变换器（PT-DiT），旨在解决扩散变换器中全局自注意力机制的冗余计算问题。PT-DiT通过稀疏代表性令牌注意力来高效建模全局视觉信息，每个空间-时间窗口随机抽取一个令牌作为代理令牌。通过这些代理令牌的自注意力捕捉全局语义，并通过交叉注意力注入到所有潜在令牌中。同时，本文还引入了窗口和移位窗口注意力，以解决稀疏注意力机制在细节建模中的局限性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.04196",
            "title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting Transformers",
            "url": "https://huggingface.co/papers/2409.04196",
            "abstract": "Reconstructing realistic 3D human models from monocular images has significant applications in creative industries, human-computer interfaces, and healthcare. We base our work on 3D Gaussian Splatting (3DGS), a scene representation composed of a mixture of Gaussians. Predicting such mixtures for a human from a single input image is challenging, as it is a non-uniform density (with a many-to-one relationship with input pixels) with strict physical constraints. At the same time, it needs to be flexible to accommodate a variety of clothes and poses. Our key observation is that the vertices of standardized human meshes (such as SMPL) can provide an adequate density and approximate initial position for Gaussians. We can then train a transformer model to jointly predict comparatively small adjustments to these positions, as well as the other Gaussians' attributes and the SMPL parameters. We show empirically that this combination (using only multi-view supervision) can achieve fast inference of 3D human models from a single image without test-time optimization, expensive diffusion models, or 3D points supervision. We also show that it can improve 3D pose estimation by better fitting human models that account for clothes and other variations. The code is available on the project website https://abdullahamdi.com/gst/ .",
            "score": 11,
            "issue_id": 1,
            "pub_date": "2024-09-06",
            "pub_date_card": {
                "ru": "6 сентября",
                "en": "September 6",
                "zh": "9月6日"
            },
            "hash": "49ac50adb0f8ba25",
            "data": {
                "categories": [
                    "#3d",
                    "#cv",
                    "#medicine"
                ],
                "emoji": "👤",
                "ru": {
                    "title": "Реалистичные 3D-модели людей из 2D-изображений с помощью гауссовых распределений",
                    "desc": "Эта статья описывает метод реконструкции трехмерных моделей людей из двумерных изображений, основанный на представлении 3D Gaussian Splatting. Авторы предлагают использовать вершины стандартизированных человеческих моделей (например, SMPL) в качестве начальных позиций для гауссовых распределений. Трансформерная модель обучается предсказывать небольшие корректировки этих позиций, а также другие атрибуты гауссианов и параметры SMPL. Метод позволяет быстро генерировать 3D-модели людей по одному изображению без оптимизации во время вывода и использования дорогостоящих диффузионных моделей."
                },
                "en": {
                    "title": "Transforming Single Images into 3D Human Models with Gaussian Splatting",
                    "desc": "This paper presents a method for creating realistic 3D human models from single images using 3D Gaussian Splatting (3DGS). The authors leverage the structure of standardized human meshes to establish a starting point for Gaussian mixtures, which helps in accurately representing the human form. A transformer model is trained to refine these initial positions and predict additional attributes, allowing for flexibility in clothing and poses. The approach demonstrates efficient inference without the need for complex optimization or supervision, enhancing 3D pose estimation in the process."
                },
                "zh": {
                    "title": "从单幅图像快速重建3D人类模型",
                    "desc": "本文研究了从单幅图像重建逼真的3D人类模型的方法，这在创意产业、人机交互和医疗保健等领域具有重要应用。我们基于3D高斯点云（3DGS）作为场景表示，利用标准化人类网格的顶点来提供高斯的初始位置和密度。通过训练变换器模型，我们可以预测这些位置的小调整以及其他高斯属性和SMPL参数。实验结果表明，该方法能够快速推断3D人类模型，并在3D姿态估计中表现出色，适应服装和姿势的变化。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.02076",
            "title": "Spinning the Golden Thread: Benchmarking Long-Form Generation in Language Models",
            "url": "https://huggingface.co/papers/2409.02076",
            "abstract": "The abilities of long-context language models (LMs) are often evaluated using the \"Needle-in-a-Haystack\" (NIAH) test, which comprises tasks designed to assess a model's ability to identify specific information (\"needle\") within large text sequences (\"haystack\"). While these benchmarks measure how well models understand long-context input sequences, they do not effectively gauge the quality of long-form text generation--a critical aspect for applications such as design proposals and creative writing. To address this gap, we have introduced a new long-form text evaluation benchmark, Spinning the Golden Thread (SGT), which tests models' ability to identify specific events within generated long text sequences. In this benchmark, we prompt long-context LMs to create long-form text that must include particular events or constraints and evaluate their ability to incorporate these elements. We evaluated ten long-context LMs across four distinct scenarios, three types of prompt instructions, and two different generation-length settings (16K and 32K). Although these models perform well on NIAH benchmarks, none demonstrated satisfactory performance on the Spinning the Golden Thread, raising concerns about their ability to generate coherent long-form text that follows instructions. Additionally, as the length of the generated text increases, all models exhibit a significant drop in performance.",
            "score": 9,
            "issue_id": 1,
            "pub_date": "2024-09-03",
            "pub_date_card": {
                "ru": "3 сентября",
                "en": "September 3",
                "zh": "9月3日"
            },
            "hash": "693dae430ac3c4c8",
            "data": {
                "categories": [
                    "#benchmark",
                    "#long_context"
                ],
                "emoji": "🧵",
                "ru": {
                    "title": "Новый вызов для языковых моделей: генерация длинных текстов по заданию",
                    "desc": "Статья представляет новый метод оценки языковых моделей с длинным контекстом - Spinning the Golden Thread (SGT). В отличие от существующих тестов типа 'иголка в стоге сена', SGT оценивает способность моделей генерировать длинные тексты с заданными элементами. Исследование охватило 10 моделей в различных сценариях и настройках. Результаты показали, что даже модели с хорошими показателями в традиционных тестах не справляются с SGT, что вызывает вопросы об их способности генерировать связные длинные тексты по инструкции."
                },
                "en": {
                    "title": "Evaluating Long-Form Text Generation: The SGT Benchmark",
                    "desc": "This paper introduces a new evaluation benchmark called Spinning the Golden Thread (SGT) to assess long-context language models (LMs) on their ability to generate coherent long-form text. Unlike the existing Needle-in-a-Haystack (NIAH) test, which focuses on information retrieval, SGT evaluates how well models can incorporate specific events or constraints into their generated text. The study tested ten long-context LMs across various scenarios and prompt types, revealing that while these models excel in NIAH tasks, they struggle with long-form generation. The findings indicate a significant decline in performance as the length of the generated text increases, highlighting a critical gap in the capabilities of current LMs for creative writing and design applications."
                },
                "zh": {
                    "title": "评估长文本生成的新标准",
                    "desc": "本文探讨了长文本生成模型的评估方法，提出了一种新的基准测试，称为“编织金线”（SGT）。该测试旨在评估模型在生成长文本时，是否能够有效地包含特定事件或约束条件。尽管现有的“干草堆中的针”（NIAH）测试能够评估模型对长文本输入的理解，但无法有效衡量长文本生成的质量。研究发现，尽管模型在NIAH基准上表现良好，但在SGT测试中却未能达到令人满意的效果，尤其是在生成文本长度增加时，模型的表现显著下降。"
                }
            }
        }
    ],
    "link_prev": "2024-09-06.html",
    "link_next": "2024-09-10.html",
    "link_month": "2024-09.html",
    "short_date_prev": {
        "ru": "06.09",
        "en": "09/06",
        "zh": "9月6日"
    },
    "short_date_next": {
        "ru": "10.09",
        "en": "09/10",
        "zh": "9月10日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 1,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 3,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 3,
        "#medicine": 1,
        "#training": 3,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#edge_computing": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#translation": 0
    }
}