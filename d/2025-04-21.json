{
    "date": {
        "ru": "21 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
        "en": "April 21",
        "zh": "4æœˆ21æ—¥"
    },
    "time_utc": "2025-04-21 07:12",
    "weekday": 0,
    "issue_id": 3339,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.13835",
            "title": "MIG: Automatic Data Selection for Instruction Tuning by Maximizing\n  Information Gain in Semantic Space",
            "url": "https://huggingface.co/papers/2504.13835",
            "abstract": "Data quality and diversity are key to the construction of effective instruction-tuning datasets. % With the increasing availability of open-source instruction-tuning datasets, it is advantageous to automatically select high-quality and diverse subsets from a vast amount of data. % Existing methods typically prioritize instance quality and use heuristic rules to maintain diversity. % However, this absence of a comprehensive view of the entire collection often leads to suboptimal results. % Moreover, heuristic rules generally focus on distance or clustering within the embedding space, which fails to accurately capture the intent of complex instructions in the semantic space. % To bridge this gap, we propose a unified method for quantifying the information content of datasets. This method models the semantic space by constructing a label graph and quantifies diversity based on the distribution of information within the graph. % Based on such a measurement, we further introduce an efficient sampling method that selects data samples iteratively to Maximize the Information Gain (MIG) in semantic space. % Experiments on various datasets and base models demonstrate that MIG consistently outperforms state-of-the-art methods. % Notably, the model fine-tuned with 5\\% Tulu3 data sampled by MIG achieves comparable performance to the official SFT model trained on the full dataset, with improvements of +5.73\\% on AlpacaEval and +6.89\\% on Wildbench.",
            "score": 24,
            "issue_id": 3335,
            "pub_date": "2025-04-18",
            "pub_date_card": {
                "ru": "18 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 18",
                "zh": "4æœˆ18æ—¥"
            },
            "hash": "12926d762a03519c",
            "authors": [
                "Yicheng Chen",
                "Yining Li",
                "Kai Hu",
                "Zerun Ma",
                "Haochen Ye",
                "Kai Chen"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "Fudan University",
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.13835.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#open_source",
                    "#training",
                    "#dataset",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞœĞ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚Ğ° Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³Ñ€Ğ°Ñ„Ğ° Ğ¼ĞµÑ‚Ğ¾Ğº. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸, Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ»Ğ¸ÑˆÑŒ 5% Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Maximizing Information for Better Instruction-Tuning Datasets",
                    "desc": "This paper addresses the importance of data quality and diversity in creating effective instruction-tuning datasets for machine learning. It critiques existing methods that rely on heuristic rules for maintaining diversity, which often leads to suboptimal dataset selections. The authors propose a new approach that quantifies the information content of datasets by modeling the semantic space with a label graph, allowing for a more comprehensive understanding of data diversity. Their method, called Maximize the Information Gain (MIG), iteratively selects samples that enhance the dataset's information content, showing significant performance improvements in experiments compared to traditional methods."
                },
                "zh": {
                    "title": "æå‡æ•°æ®é›†è´¨é‡ä¸å¤šæ ·æ€§çš„ç»Ÿä¸€æ–¹æ³•",
                    "desc": "æ•°æ®è´¨é‡å’Œå¤šæ ·æ€§æ˜¯æ„å»ºæœ‰æ•ˆæŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†çš„å…³é”®ã€‚éšç€å¼€æºæŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†çš„å¢åŠ ï¼Œè‡ªåŠ¨é€‰æ‹©é«˜è´¨é‡å’Œå¤šæ ·åŒ–çš„å­é›†å˜å¾—å°¤ä¸ºé‡è¦ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¼˜å…ˆè€ƒè™‘å®ä¾‹è´¨é‡ï¼Œå¹¶ä½¿ç”¨å¯å‘å¼è§„åˆ™æ¥ç»´æŒå¤šæ ·æ€§ï¼Œä½†ç¼ºä¹å¯¹æ•´ä¸ªæ•°æ®é›†çš„å…¨é¢è§†è§’ï¼Œå¯¼è‡´ç»“æœä¸ç†æƒ³ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„æ–¹æ³•ï¼Œé€šè¿‡æ„å»ºæ ‡ç­¾å›¾æ¥é‡åŒ–æ•°æ®é›†çš„ä¿¡æ¯å†…å®¹ï¼Œå¹¶åŸºäºä¿¡æ¯åˆ†å¸ƒæ¥é‡åŒ–å¤šæ ·æ€§ï¼Œä»è€Œå¼•å…¥äº†ä¸€ç§é«˜æ•ˆçš„é‡‡æ ·æ–¹æ³•ï¼Œä»¥æœ€å¤§åŒ–è¯­ä¹‰ç©ºé—´ä¸­çš„ä¿¡æ¯å¢ç›Šã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.13837",
            "title": "Does Reinforcement Learning Really Incentivize Reasoning Capacity in\n  LLMs Beyond the Base Model?",
            "url": "https://huggingface.co/papers/2504.13837",
            "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently demonstrated notable success in enhancing the reasoning capabilities of LLMs, particularly in mathematics and programming tasks. It is widely believed that RLVR enables LLMs to continuously self-improve, thus acquiring novel reasoning abilities that exceed corresponding base models' capacity. In this study, however, we critically re-examines this assumption by measuring the pass@k metric with large values of k to explore the reasoning capability boundary of the models across a wide range of model families and benchmarks. Surprisingly, the RL does not, in fact, elicit fundamentally new reasoning patterns. While RL-trained models outperform their base models at smaller values of k (\\eg, k=1), base models can achieve a comparable or even higher pass@k score compared to their RL counterparts at large k values. The reasoning paths generated by RL-trained models are already included in the base models' sampling distribution, suggesting that most reasoning abilities manifested in RL-trained models are already obtained by base models. Further analysis shows that RL training boosts the performance by biasing the model's output distribution toward paths that are more likely to yield rewards, therefore sampling correct responses more efficiently. But this also results in a narrower reasoning capability boundary compared to base models. Similar results are observed in visual reasoning tasks trained with RLVR. Moreover, we find that distillation can genuinely introduce new knowledge into the model, different from RLVR. These findings underscore a critical limitation of RLVR in advancing LLM reasoning abilities which requires us to fundamentally rethink the impact of RL training in reasoning LLMs and the need of a better paradigm. Project Page: https://limit-of-RLVR.github.io",
            "score": 19,
            "issue_id": 3335,
            "pub_date": "2025-04-18",
            "pub_date_card": {
                "ru": "18 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 18",
                "zh": "4æœˆ18æ—¥"
            },
            "hash": "2fe56493fe3aec80",
            "authors": [
                "Yang Yue",
                "Zhiqi Chen",
                "Rui Lu",
                "Andrew Zhao",
                "Zhaokai Wang",
                "Yang Yue",
                "Shiji Song",
                "Gao Huang"
            ],
            "affiliations": [
                "LeapLab, Tsinghua University",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.13837.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#training",
                    "#reasoning",
                    "#optimization"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğµ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸ (RLVR) Ğ½Ğµ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ¿Ğ¾ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñƒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ¥Ğ¾Ñ‚Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ RL, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸ÑÑ… k Ğ² Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ pass@k, Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ñ‹Ñ… Ğ¸Ğ»Ğ¸ Ğ´Ğ°Ğ¶Ğµ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… k. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ñ‹ÑĞ²Ğ¸Ğ», Ñ‡Ñ‚Ğ¾ RL-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ¼ĞµÑ‰Ğ°ĞµÑ‚ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñƒ Ğ¿ÑƒÑ‚ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞ¹ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ñ€Ğ¸Ğ½Ğ¾ÑÑÑ‚ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹, Ğ½Ğ¾ ÑÑ‚Ğ¾ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ÑÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ† ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ RLVR Ğ² ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Rethinking RLVR: Limits of Reinforcement Learning in Reasoning",
                    "desc": "This paper critically evaluates the effectiveness of Reinforcement Learning with Verifiable Rewards (RLVR) in enhancing the reasoning capabilities of large language models (LLMs). The authors find that while RLVR improves performance at lower complexity tasks, it does not introduce fundamentally new reasoning patterns compared to base models when evaluated at higher complexity levels. Instead, RL-trained models tend to sample reasoning paths that are already present in base models, leading to a narrower range of reasoning capabilities. The study suggests that distillation may be a more effective method for introducing new knowledge into models, highlighting the limitations of RLVR in advancing LLM reasoning."
                },
                "zh": {
                    "title": "é‡æ–°æ€è€ƒå¼ºåŒ–å­¦ä¹ åœ¨æ¨ç†ä¸­çš„ä½œç”¨",
                    "desc": "å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›æ–¹é¢å–å¾—äº†ä¸€å®šæˆåŠŸï¼Œå°¤å…¶æ˜¯åœ¨æ•°å­¦å’Œç¼–ç¨‹ä»»åŠ¡ä¸­ã€‚ç„¶è€Œï¼Œæœ¬ç ”ç©¶é‡æ–°å®¡è§†äº†è¿™ä¸€å‡è®¾ï¼Œå‘ç°RLVRå¹¶æœªçœŸæ­£å¼•å…¥æ–°çš„æ¨ç†æ¨¡å¼ã€‚å°½ç®¡RLè®­ç»ƒçš„æ¨¡å‹åœ¨å°çš„kå€¼ä¸‹è¡¨ç°ä¼˜äºåŸºç¡€æ¨¡å‹ï¼Œä½†åœ¨è¾ƒå¤§çš„kå€¼ä¸‹ï¼ŒåŸºç¡€æ¨¡å‹çš„è¡¨ç°å¯ä»¥ä¸RLæ¨¡å‹ç›¸åª²ç¾ï¼Œç”šè‡³æ›´å¥½ã€‚è¿™è¡¨æ˜ï¼ŒRLè®­ç»ƒæ¨¡å‹çš„æ¨ç†è·¯å¾„å®é™…ä¸Šå·²ç»åŒ…å«åœ¨åŸºç¡€æ¨¡å‹çš„é‡‡æ ·åˆ†å¸ƒä¸­ï¼Œå¼ºè°ƒäº†RLVRåœ¨æå‡LLMæ¨ç†èƒ½åŠ›æ–¹é¢çš„å±€é™æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.11833",
            "title": "Could Thinking Multilingually Empower LLM Reasoning?",
            "url": "https://huggingface.co/papers/2504.11833",
            "abstract": "Previous work indicates that large language models exhibit a significant \"English bias\", i.e. they often perform better when tasks are presented in English. Interestingly, we have observed that using certain other languages in reasoning tasks can yield better performance than English. However, this phenomenon remains under-explored. In this paper, we explore the upper bound of harnessing multilingualism in reasoning tasks, suggesting that multilingual reasoning promises significantly (by nearly 10 Acc@k points) and robustly (tolerance for variations in translation quality and language choice) higher upper bounds than English-only reasoning. Besides analyzing the reason behind the upper bound and challenges in reaching it, we also find that common answer selection methods cannot achieve this upper bound, due to their limitations and biases. These insights could pave the way for future research aimed at fully harnessing the potential of multilingual reasoning in LLMs.",
            "score": 13,
            "issue_id": 3335,
            "pub_date": "2025-04-16",
            "pub_date_card": {
                "ru": "16 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 16",
                "zh": "4æœˆ16æ—¥"
            },
            "hash": "463f5ddc1d75970e",
            "authors": [
                "Changjiang Gao",
                "Xu Huang",
                "Wenhao Zhu",
                "Shujian Huang",
                "Lei Li",
                "Fei Yuan"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "National Key Laboratory for Novel Software Technology, Nanjing University",
                "Shanghai Artificial Intelligence Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.11833.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#low_resource",
                    "#multilingual"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "ĞœĞ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ - ĞºĞ»ÑÑ‡ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ñ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° 10 Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿ÑƒĞ½ĞºÑ‚Ğ¾Ğ². ĞĞ´Ğ½Ğ°ĞºĞ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑÑ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ¸Ğ·-Ğ·Ğ° ÑĞ²Ğ¾Ğ¸Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚ĞµĞ¹. Ğ­Ñ‚Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ñ‹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…."
                },
                "en": {
                    "title": "Unlocking the Power of Multilingual Reasoning in LLMs",
                    "desc": "This paper investigates the performance of large language models (LLMs) in reasoning tasks across multiple languages. It reveals that using certain non-English languages can lead to better outcomes than relying solely on English, highlighting a significant opportunity for multilingual reasoning. The authors suggest that multilingual approaches can achieve higher accuracy and robustness compared to English-only methods, even when translation quality varies. Additionally, they identify limitations in current answer selection methods that prevent reaching the full potential of multilingual reasoning, setting the stage for future research in this area."
                },
                "zh": {
                    "title": "å¤šè¯­è¨€æ¨ç†çš„æ½œåŠ›è¶…è¶Šè‹±è¯­",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸­çš„å¤šè¯­è¨€èƒ½åŠ›ï¼Œå‘ç°æŸäº›è¯­è¨€åœ¨æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ä¼˜äºè‹±è¯­ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå¤šè¯­è¨€æ¨ç†çš„ä¸Šé™æ¯”ä»…ä½¿ç”¨è‹±è¯­çš„æ¨ç†é«˜å‡ºè¿‘10ä¸ªå‡†ç¡®ç‡ç‚¹ï¼Œå¹¶ä¸”å¯¹ç¿»è¯‘è´¨é‡å’Œè¯­è¨€é€‰æ‹©çš„å˜åŒ–å…·æœ‰æ›´å¼ºçš„å®¹å¿åº¦ã€‚æˆ‘ä»¬åˆ†æäº†è¾¾åˆ°è¿™ä¸€ä¸Šé™çš„åŸå› å’ŒæŒ‘æˆ˜ï¼Œå¹¶æŒ‡å‡ºå¸¸è§çš„ç­”æ¡ˆé€‰æ‹©æ–¹æ³•ç”±äºå…¶å±€é™æ€§å’Œåè§ï¼Œæ— æ³•å®ç°è¿™ä¸€ä¸Šé™ã€‚è¿™äº›å‘ç°ä¸ºæœªæ¥ç ”ç©¶å……åˆ†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šè¯­è¨€æ¨ç†æ½œåŠ›é“ºå¹³äº†é“è·¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.11544",
            "title": "NodeRAG: Structuring Graph-based RAG with Heterogeneous Nodes",
            "url": "https://huggingface.co/papers/2504.11544",
            "abstract": "Retrieval-augmented generation (RAG) empowers large language models to access external and private corpus, enabling factually consistent responses in specific domains. By exploiting the inherent structure of the corpus, graph-based RAG methods further enrich this process by building a knowledge graph index and leveraging the structural nature of graphs. However, current graph-based RAG approaches seldom prioritize the design of graph structures. Inadequately designed graph not only impede the seamless integration of diverse graph algorithms but also result in workflow inconsistencies and degraded performance. To further unleash the potential of graph for RAG, we propose NodeRAG, a graph-centric framework introducing heterogeneous graph structures that enable the seamless and holistic integration of graph-based methodologies into the RAG workflow. By aligning closely with the capabilities of LLMs, this framework ensures a fully cohesive and efficient end-to-end process. Through extensive experiments, we demonstrate that NodeRAG exhibits performance advantages over previous methods, including GraphRAG and LightRAG, not only in indexing time, query time, and storage efficiency but also in delivering superior question-answering performance on multi-hop benchmarks and open-ended head-to-head evaluations with minimal retrieval tokens. Our GitHub repository could be seen at https://github.com/Terry-Xu-666/NodeRAG.",
            "score": 4,
            "issue_id": 3335,
            "pub_date": "2025-04-15",
            "pub_date_card": {
                "ru": "15 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 15",
                "zh": "4æœˆ15æ—¥"
            },
            "hash": "86dd4da356ad5ef0",
            "authors": [
                "Tianyang Xu",
                "Haojie Zheng",
                "Chengze Li",
                "Haoxiang Chen",
                "Yixin Liu",
                "Ruoxi Chen",
                "Lichao Sun"
            ],
            "affiliations": [
                "Columbia University",
                "Lehigh University",
                "University of Pennsylvania"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.11544.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#graphs",
                    "#open_source",
                    "#rag",
                    "#multimodal",
                    "#optimization"
                ],
                "emoji": "ğŸ•¸ï¸",
                "ru": {
                    "title": "NodeRAG: Ğ“Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ· Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ²",
                    "desc": "NodeRAG - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ· Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² (RAG), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½Ñ‹Ğµ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­Ñ‚Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ğµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ RAG, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ NodeRAG Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ğ¸, Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸ÑÑ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "NodeRAG: Enhancing RAG with Smart Graph Structures",
                    "desc": "This paper introduces NodeRAG, a new framework that enhances retrieval-augmented generation (RAG) by using heterogeneous graph structures. By focusing on the design of graph structures, NodeRAG improves the integration of various graph algorithms into the RAG workflow, leading to better performance. The framework aligns with the capabilities of large language models (LLMs), ensuring a smooth and efficient process for generating responses. Experimental results show that NodeRAG outperforms existing methods like GraphRAG and LightRAG in terms of indexing time, query time, storage efficiency, and question-answering accuracy."
                },
                "zh": {
                    "title": "NodeRAGï¼šå›¾ç»“æ„åŠ©åŠ›æ£€ç´¢å¢å¼ºç”Ÿæˆ",
                    "desc": "æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä½¿å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿè®¿é—®å¤–éƒ¨å’Œç§æœ‰è¯­æ–™åº“ï¼Œä»è€Œåœ¨ç‰¹å®šé¢†åŸŸæä¾›äº‹å®ä¸€è‡´çš„å“åº”ã€‚é€šè¿‡åˆ©ç”¨è¯­æ–™åº“çš„å†…åœ¨ç»“æ„ï¼ŒåŸºäºå›¾çš„RAGæ–¹æ³•é€šè¿‡æ„å»ºçŸ¥è¯†å›¾è°±ç´¢å¼•è¿›ä¸€æ­¥ä¸°å¯Œäº†è¿™ä¸€è¿‡ç¨‹ã€‚ç„¶è€Œï¼Œç›®å‰çš„åŸºäºå›¾çš„RAGæ–¹æ³•å¾ˆå°‘é‡è§†å›¾ç»“æ„çš„è®¾è®¡ã€‚ä¸ºäº†è§£æ”¾å›¾åœ¨RAGä¸­çš„æ½œåŠ›ï¼Œæˆ‘ä»¬æå‡ºäº†NodeRAGï¼Œä¸€ä¸ªä»¥å›¾ä¸ºä¸­å¿ƒçš„æ¡†æ¶ï¼Œå¼•å…¥å¼‚æ„å›¾ç»“æ„ï¼Œå®ç°å›¾æ–¹æ³•ä¸RAGå·¥ä½œæµç¨‹çš„æ— ç¼æ•´åˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.13157",
            "title": "AerialMegaDepth: Learning Aerial-Ground Reconstruction and View\n  Synthesis",
            "url": "https://huggingface.co/papers/2504.13157",
            "abstract": "We explore the task of geometric reconstruction of images captured from a mixture of ground and aerial views. Current state-of-the-art learning-based approaches fail to handle the extreme viewpoint variation between aerial-ground image pairs. Our hypothesis is that the lack of high-quality, co-registered aerial-ground datasets for training is a key reason for this failure. Such data is difficult to assemble precisely because it is difficult to reconstruct in a scalable way. To overcome this challenge, we propose a scalable framework combining pseudo-synthetic renderings from 3D city-wide meshes (e.g., Google Earth) with real, ground-level crowd-sourced images (e.g., MegaDepth). The pseudo-synthetic data simulates a wide range of aerial viewpoints, while the real, crowd-sourced images help improve visual fidelity for ground-level images where mesh-based renderings lack sufficient detail, effectively bridging the domain gap between real images and pseudo-synthetic renderings. Using this hybrid dataset, we fine-tune several state-of-the-art algorithms and achieve significant improvements on real-world, zero-shot aerial-ground tasks. For example, we observe that baseline DUSt3R localizes fewer than 5% of aerial-ground pairs within 5 degrees of camera rotation error, while fine-tuning with our data raises accuracy to nearly 56%, addressing a major failure point in handling large viewpoint changes. Beyond camera estimation and scene reconstruction, our dataset also improves performance on downstream tasks like novel-view synthesis in challenging aerial-ground scenarios, demonstrating the practical value of our approach in real-world applications.",
            "score": 3,
            "issue_id": 3335,
            "pub_date": "2025-04-17",
            "pub_date_card": {
                "ru": "17 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 17",
                "zh": "4æœˆ17æ—¥"
            },
            "hash": "bbd51434265e3614",
            "authors": [
                "Khiem Vuong",
                "Anurag Ghosh",
                "Deva Ramanan",
                "Srinivasa Narasimhan",
                "Shubham Tulsiani"
            ],
            "affiliations": [
                "Carnegie Mellon University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.13157.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#transfer_learning",
                    "#synthetic",
                    "#dataset",
                    "#3d"
                ],
                "emoji": "ğŸ™ï¸",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·ĞµĞ¼Ğ»ĞµĞ¹ Ğ¸ Ğ½ĞµĞ±Ğ¾Ğ¼ Ğ² ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ·Ñ€ĞµĞ½Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ½Ğ°Ğ·ĞµĞ¼Ğ½Ñ‹Ñ… Ğ¸ Ğ°ÑÑ€Ğ¾ÑÑŠĞµĞ¼Ğ¾Ğº. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¿ÑĞµĞ²Ğ´Ğ¾-ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞ½Ğ´ĞµÑ€Ñ‹ Ğ¸Ğ· 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³Ğ¾Ñ€Ğ¾Ğ´Ğ¾Ğ² Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ·ĞµĞ¼Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ´Ğ»Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ ÑÑ†ĞµĞ½Ñ‹ Ğ¿Ñ€Ğ¸ ÑĞºÑÑ‚Ñ€ĞµĞ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑÑ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½Ğ°Ğ·ĞµĞ¼Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ°ÑÑ€Ğ¾ÑĞ½Ğ¸Ğ¼ĞºĞ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Bridging the Viewpoint Gap: Enhanced Aerial-Ground Image Reconstruction",
                    "desc": "This paper addresses the challenge of reconstructing images from both ground and aerial perspectives, which current machine learning methods struggle with due to significant viewpoint differences. The authors suggest that the lack of high-quality datasets that pair aerial and ground images is a major obstacle. To tackle this, they introduce a scalable framework that combines pseudo-synthetic images generated from 3D city models with real ground-level images, effectively bridging the gap between these two domains. By fine-tuning existing algorithms with this hybrid dataset, they achieve substantial improvements in accuracy for aerial-ground tasks, demonstrating the framework's effectiveness in real-world applications."
                },
                "zh": {
                    "title": "æ‰“ç ´è§†è§’é™åˆ¶ï¼Œå®ç°å›¾åƒå‡ ä½•é‡å»º",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†ä»åœ°é¢å’Œç©ºä¸­è§†è§’æ•è·çš„å›¾åƒè¿›è¡Œå‡ ä½•é‡å»ºçš„ä»»åŠ¡ã€‚ç°æœ‰çš„åŸºäºå­¦ä¹ çš„æ–¹æ³•åœ¨å¤„ç†ç©ºä¸­ä¸åœ°é¢å›¾åƒå¯¹ä¹‹é—´çš„æç«¯è§†è§’å˜åŒ–æ—¶è¡¨ç°ä¸ä½³ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œç¼ºä¹é«˜è´¨é‡çš„ã€å…±åŒæ³¨å†Œçš„ç©ºä¸­-åœ°é¢æ•°æ®é›†æ˜¯å¯¼è‡´è¿™ä¸€å¤±è´¥çš„å…³é”®åŸå› ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¯æ‰©å±•çš„æ¡†æ¶ï¼Œç»“åˆäº†æ¥è‡ª3DåŸå¸‚ç½‘æ ¼çš„ä¼ªåˆæˆæ¸²æŸ“å’ŒçœŸå®çš„åœ°é¢ä¼—åŒ…å›¾åƒï¼Œä»è€Œæœ‰æ•ˆåœ°ç¼©å°äº†çœŸå®å›¾åƒä¸ä¼ªåˆæˆæ¸²æŸ“ä¹‹é—´çš„é¢†åŸŸå·®è·ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.13072",
            "title": "HiScene: Creating Hierarchical 3D Scenes with Isometric View Generation",
            "url": "https://huggingface.co/papers/2504.13072",
            "abstract": "Scene-level 3D generation represents a critical frontier in multimedia and computer graphics, yet existing approaches either suffer from limited object categories or lack editing flexibility for interactive applications. In this paper, we present HiScene, a novel hierarchical framework that bridges the gap between 2D image generation and 3D object generation and delivers high-fidelity scenes with compositional identities and aesthetic scene content. Our key insight is treating scenes as hierarchical \"objects\" under isometric views, where a room functions as a complex object that can be further decomposed into manipulatable items. This hierarchical approach enables us to generate 3D content that aligns with 2D representations while maintaining compositional structure. To ensure completeness and spatial alignment of each decomposed instance, we develop a video-diffusion-based amodal completion technique that effectively handles occlusions and shadows between objects, and introduce shape prior injection to ensure spatial coherence within the scene. Experimental results demonstrate that our method produces more natural object arrangements and complete object instances suitable for interactive applications, while maintaining physical plausibility and alignment with user inputs.",
            "score": 3,
            "issue_id": 3337,
            "pub_date": "2025-04-17",
            "pub_date_card": {
                "ru": "17 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 17",
                "zh": "4æœˆ17æ—¥"
            },
            "hash": "6eb45708f6cb0c26",
            "authors": [
                "Wenqi Dong",
                "Bangbang Yang",
                "Zesong Yang",
                "Yuan Li",
                "Tao Hu",
                "Hujun Bao",
                "Yuewen Ma",
                "Zhaopeng Cui"
            ],
            "affiliations": [
                "ByteDance",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.13072.jpg",
            "data": {
                "categories": [
                    "#3d"
                ],
                "emoji": "ğŸ ",
                "ru": {
                    "title": "HiScene: Ğ˜ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ°Ñ 3D-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑÑ†ĞµĞ½ Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¾Ğ¹",
                    "desc": "HiScene - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-ÑÑ†ĞµĞ½, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ 2D-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ 3D-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ†ĞµĞ½Ñ‹ ĞºĞ°Ğº Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ 'Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹' Ğ² Ğ¸Ğ·Ğ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ğ¸, Ğ³Ğ´Ğµ ĞºĞ¾Ğ¼Ğ½Ğ°Ñ‚Ğ° Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ°Ğº ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ÑŒ Ğ½Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ñ‹. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ° Ğ´Ğ»Ñ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ñ‡Ğ°ÑÑ‚ĞµĞ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ¸Ğ½ÑŠĞµĞºÑ†Ğ¸Ñ Ğ°Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ Ñ„Ğ¾Ñ€Ğ¼Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ HiScene ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½Ğ¾Ğ²ĞºĞ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "HiScene: Bridging 2D and 3D for Interactive Scene Generation",
                    "desc": "This paper introduces HiScene, a new framework for generating 3D scenes that combines the strengths of 2D image generation with 3D object creation. It treats scenes as hierarchical structures, allowing for detailed manipulation of individual elements within a room. The method employs a video-diffusion-based technique for amodal completion, addressing issues like occlusions and shadows to ensure realistic object interactions. Experimental results show that HiScene produces coherent and aesthetically pleasing 3D scenes that are well-suited for interactive applications."
                },
                "zh": {
                    "title": "HiSceneï¼šå±‚æ¬¡åŒ–çš„3Dåœºæ™¯ç”Ÿæˆæ–°æ–¹æ³•",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºHiSceneçš„å±‚æ¬¡æ¡†æ¶ï¼Œç”¨äºåœºæ™¯çº§3Dç”Ÿæˆï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åœ¨å¯¹è±¡ç±»åˆ«å’Œç¼–è¾‘çµæ´»æ€§æ–¹é¢çš„å±€é™ã€‚æˆ‘ä»¬å°†åœºæ™¯è§†ä¸ºåœ¨ç­‰è·è§†å›¾ä¸‹çš„å±‚æ¬¡â€œå¯¹è±¡â€ï¼Œä½¿å¾—æˆ¿é—´å¯ä»¥è¢«è¿›ä¸€æ­¥åˆ†è§£ä¸ºå¯æ“ä½œçš„ç‰©å“ã€‚é€šè¿‡è¿™ç§å±‚æ¬¡åŒ–çš„æ–¹æ³•ï¼Œæˆ‘ä»¬èƒ½å¤Ÿç”Ÿæˆä¸2Dè¡¨ç¤ºç›¸ä¸€è‡´çš„3Då†…å®¹ï¼ŒåŒæ—¶ä¿æŒç»„åˆç»“æ„ã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€ç§åŸºäºè§†é¢‘æ‰©æ•£çš„æ¨¡æ€è¡¥å…¨æŠ€æœ¯ï¼Œä»¥å¤„ç†å¯¹è±¡ä¹‹é—´çš„é®æŒ¡å’Œé˜´å½±ï¼Œç¡®ä¿æ¯ä¸ªåˆ†è§£å®ä¾‹çš„å®Œæ•´æ€§å’Œç©ºé—´å¯¹é½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.13173",
            "title": "It's All Connected: A Journey Through Test-Time Memorization,\n  Attentional Bias, Retention, and Online Optimization",
            "url": "https://huggingface.co/papers/2504.13173",
            "abstract": "Designing efficient and effective architectural backbones has been in the core of research efforts to enhance the capability of foundation models. Inspired by the human cognitive phenomenon of attentional bias-the natural tendency to prioritize certain events or stimuli-we reconceptualize neural architectures, including Transformers, Titans, and modern linear recurrent neural networks as associative memory modules that learn a mapping of keys and values using an internal objective, referred to as attentional bias. Surprisingly, we observed that most existing sequence models leverage either (1) dot-product similarity, or (2) L2 regression objectives as their attentional bias. Going beyond these objectives, we present a set of alternative attentional bias configurations along with their effective approximations to stabilize their training procedure. We then reinterpret forgetting mechanisms in modern deep learning architectures as a form of retention regularization, providing a novel set of forget gates for sequence models. Building upon these insights, we present Miras, a general framework to design deep learning architectures based on four choices of: (i) associative memory architecture, (ii) attentional bias objective, (iii) retention gate, and (iv) memory learning algorithm. We present three novel sequence models-Moneta, Yaad, and Memora-that go beyond the power of existing linear RNNs while maintaining a fast parallelizable training process. Our experiments show different design choices in Miras yield models with varying strengths. For example, certain instances of Miras achieve exceptional performance in special tasks such as language modeling, commonsense reasoning, and recall intensive tasks, even outperforming Transformers and other modern linear recurrent models.",
            "score": 2,
            "issue_id": 3336,
            "pub_date": "2025-04-17",
            "pub_date_card": {
                "ru": "17 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 17",
                "zh": "4æœˆ17æ—¥"
            },
            "hash": "809d2f1facd3aed9",
            "authors": [
                "Ali Behrouz",
                "Meisam Razaviyayn",
                "Peilin Zhong",
                "Vahab Mirrokni"
            ],
            "affiliations": [
                "Google Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.13173.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#reasoning",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ğ·Ğ¼Ñƒ ÑĞµĞ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹, Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ñ„ĞµĞ½Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ¼ ÑĞµĞ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ framework Miras Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°ÑÑĞ¾Ñ†Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ°Ğ¼Ğ¸ Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ. ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ñ‚Ñ€Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ - Moneta, Yaad Ğ¸ Memora, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğµ RNN. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ñ‹ Miras Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ³Ğ¾ ÑĞ¼Ñ‹ÑĞ»Ğ° Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ½ÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Revolutionizing Neural Architectures with Attentional Bias",
                    "desc": "This paper explores new ways to design neural network architectures, particularly focusing on how they can mimic human attention. It introduces the concept of attentional bias, which helps models prioritize important information, and critiques existing methods that rely on simple similarity measures. The authors propose a framework called Miras, which allows for flexible design choices in memory architecture and training objectives. They also present new models that outperform traditional approaches in specific tasks, demonstrating the effectiveness of their innovative strategies."
                },
                "zh": {
                    "title": "åŸºäºæ³¨æ„åå‘çš„æ·±åº¦å­¦ä¹ æ¶æ„è®¾è®¡",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•è®¾è®¡é«˜æ•ˆä¸”æœ‰æ•ˆçš„åŸºç¡€æ¨¡å‹æ¶æ„ï¼Œçµæ„Ÿæ¥æºäºäººç±»çš„æ³¨æ„åå‘ç°è±¡ã€‚æˆ‘ä»¬å°†ç¥ç»ç½‘ç»œæ¶æ„é‡æ–°æ¦‚å¿µåŒ–ä¸ºå…³è”è®°å¿†æ¨¡å—ï¼Œåˆ©ç”¨å†…éƒ¨ç›®æ ‡ï¼ˆæ³¨æ„åå‘ï¼‰æ¥å­¦ä¹ é”®å€¼æ˜ å°„ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰åºåˆ—æ¨¡å‹ä¸»è¦ä¾èµ–ç‚¹ç§¯ç›¸ä¼¼æ€§æˆ–L2å›å½’ç›®æ ‡ï¼Œè€Œæˆ‘ä»¬æå‡ºäº†ä¸€ç³»åˆ—æ›¿ä»£çš„æ³¨æ„åå‘é…ç½®åŠå…¶æœ‰æ•ˆè¿‘ä¼¼ï¼Œä»¥ç¨³å®šè®­ç»ƒè¿‡ç¨‹ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†Mirasæ¡†æ¶ï¼Œè®¾è®¡æ·±åº¦å­¦ä¹ æ¶æ„ï¼Œå¹¶å±•ç¤ºäº†ä¸‰ç§æ–°å‹åºåˆ—æ¨¡å‹ï¼Œè¶…è¶Šäº†ç°æœ‰çº¿æ€§RNNçš„èƒ½åŠ›ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-04-18.html",
    "link_next": "2025-04-22.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "18.04",
        "en": "04/18",
        "zh": "4æœˆ18æ—¥"
    },
    "short_date_next": {
        "ru": "22.04",
        "en": "04/22",
        "zh": "4æœˆ22æ—¥"
    },
    "categories": {
        "#dataset": 2,
        "#data": 2,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 0,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 0,
        "#3d": 2,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 3,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 3,
        "#transfer_learning": 1,
        "#graphs": 1,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 4,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 1
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†é¢„è®­ç»ƒæ•°æ®é›†çš„æ”¶é›†å’Œä¼˜åŒ–é—®é¢˜ã€‚ç°æœ‰æ•°æ®é›†é€šå¸¸æ¥è‡ªç½‘ç»œå†…å®¹ï¼Œç¼ºä¹æ˜ç¡®çš„é¢†åŸŸåˆ’åˆ†ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§åä¸ºCLIMBçš„è‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œé€šè¿‡åµŒå…¥å’Œèšç±»å¤§è§„æ¨¡æ•°æ®é›†ï¼Œè¿­ä»£æœç´¢æœ€ä½³æ•°æ®æ··åˆã€‚ä½¿ç”¨è¿™ç§æ–¹æ³•ï¼Œ1Bæ¨¡å‹åœ¨400B tokensä¸Šçš„è®­ç»ƒæ•ˆæœè¶…è¿‡äº†Llama-3.2-1Bã€‚æ–‡ç« è¿˜ä»‹ç»äº†ClimbLabå’ŒClimbMixä¸¤ä¸ªæ•°æ®é›†ï¼Œå¹¶åˆ†æäº†æœ€ä½³æ•°æ®æ··åˆçš„ç‰¹å¾ã€‚",
        "title": "CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for\n  Language Model Pre-training",
        "pinyin": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†é¢„è®­ç»ƒæ•°æ®é›†çš„æ”¶é›†å’Œä¼˜åŒ–é—®é¢˜ã€‚\nZhÃ¨ piÄn wÃ©n zhÄng tÇo lÃ¹n le yÃ¹ xÃ¹n liÃ n shÃ¹ jÃ¹ jÃ­ de shÅu jÃ­ hÃ© yÅu huÃ  wÃ¨n tÃ­.\n\nç°æœ‰æ•°æ®é›†é€šå¸¸æ¥è‡ªç½‘ç»œå†…å®¹ï¼Œç¼ºä¹æ˜ç¡®çš„é¢†åŸŸåˆ’åˆ†ã€‚\nXiÃ n yÇ’u shÃ¹ jÃ¹ jÃ­ tÅng chÃ¡ng lÃ¡i zÃ¬ wÇng luÃ² nÃ¨i rÃ³ng, quÄ“ fÃ¡ mÃ­ng quÃ¨ de lÇng yÃ¹ huÃ  fÄ“n.\n\næ–‡ç« æå‡ºäº†ä¸€ç§åä¸ºCLIMBçš„è‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œé€šè¿‡åµŒå…¥å’Œèšç±»å¤§è§„æ¨¡æ•°æ®é›†ï¼Œè¿­ä»£æœç´¢æœ€ä½³æ•°æ®æ··åˆã€‚\nWÃ©n zhÄng tÃ­ chÅ« le yÄ« zhÇ’ng mÃ­ng wÃ©i CLIMB de zÃ¬ dÃ²ng huÃ  kuÃ ng jiÃ , tÅng guÃ² qiÃ n rÃ¹ hÃ© jÃ¹ lÃ¨i dÃ  guÄ« mÃ³ shÃ¹ jÃ¹ jÃ­, diÃ© dÃ i sÅu suÇ’ zuÃ¬ jiÄ shÃ¹ jÃ¹ hÃ¹n hÃ©.\n\nä½¿ç”¨è¿™ç§æ–¹æ³•ï¼Œ1Bæ¨¡å‹åœ¨400B tokensä¸Šçš„è®­ç»ƒæ•ˆæœè¶…è¿‡äº†Llama-3.2-1Bã€‚\nShÇ yÃ²ng zhÃ¨ zhÇ’ng fÄng fÇ, 1B mÃ³ xÃ­ng zÃ i 400B tokens shÃ ng de xÃ¹n liÃ n xiÃ o guÇ’ chÄo guÃ² le Llama-3.2-1B.\n\næ–‡ç« è¿˜ä»‹ç»äº†ClimbLabå’ŒClimbMixä¸¤ä¸ªæ•°æ®é›†ï¼Œå¹¶åˆ†æäº†æœ€ä½³æ•°æ®æ··åˆçš„ç‰¹å¾ã€‚\nWÃ©n zhÄng hÃ¡i jiÃ¨ shÃ o le ClimbLab hÃ© ClimbMix liÇng gÃ¨ shÃ¹ jÃ¹ jÃ­, bÃ¬ng fÄ“n xÄ« le zuÃ¬ jiÄ shÃ¹ jÃ¹ hÃ¹n hÃ© de tÃ¨ zhÄ“ng.",
        "vocab": "[\n    {\"word\": \"è®¨è®º\", \"pinyin\": \"tÇo lÃ¹n\", \"trans\": \"discuss\"},\n    {\"word\": \"é¢„è®­ç»ƒ\", \"pinyin\": \"yÃ¹ xÃ¹n liÃ n\", \"trans\": \"pre-train\"},\n    {\"word\": \"æ•°æ®é›†\", \"pinyin\": \"shÃ¹ jÃ¹ jÃ­\", \"trans\": \"dataset\"},\n    {\"word\": \"æ”¶é›†\", \"pinyin\": \"shÅu jÃ­\", \"trans\": \"collect\"},\n    {\"word\": \"ä¼˜åŒ–\", \"pinyin\": \"yÅu huÃ \", \"trans\": \"optimize\"},\n    {\"word\": \"ç°æœ‰\", \"pinyin\": \"xiÃ n yÇ’u\", \"trans\": \"existing\"},\n    {\"word\": \"æ¥è‡ª\", \"pinyin\": \"lÃ¡i zÃ¬\", \"trans\": \"come from\"},\n    {\"word\": \"ç½‘ç»œ\", \"pinyin\": \"wÇng luÃ²\", \"trans\": \"network\"},\n    {\"word\": \"å†…å®¹\", \"pinyin\": \"nÃ¨i rÃ³ng\", \"trans\": \"content\"},\n    {\"word\": \"ç¼ºä¹\", \"pinyin\": \"quÄ“ fÃ¡\", \"trans\": \"lack\"},\n    {\"word\": \"æ˜ç¡®\", \"pinyin\": \"mÃ­ng quÃ¨\", \"trans\": \"clear\"},\n    {\"word\": \"é¢†åŸŸ\", \"pinyin\": \"lÇng yÃ¹\", \"trans\": \"domain\"},\n    {\"word\": \"åˆ’åˆ†\", \"pinyin\": \"huÃ  fÄ“n\", \"trans\": \"divide\"},\n    {\"word\": \"æå‡º\", \"pinyin\": \"tÃ­ chÅ«\", \"trans\": \"propose\"},\n    {\"word\": \"åä¸º\", \"pinyin\": \"mÃ­ng wÃ©i\", \"trans\": \"named\"},\n    {\"word\": \"è‡ªåŠ¨åŒ–\", \"pinyin\": \"zÃ¬ dÃ²ng huÃ \", \"trans\": \"automate\"},\n    {\"word\": \"æ¡†æ¶\", \"pinyin\": \"kuÃ ng jiÃ \", \"trans\": \"framework\"},\n    {\"word\": \"åµŒå…¥\", \"pinyin\": \"qiÃ n rÃ¹\", \"trans\": \"embed\"},\n    {\"word\": \"èšç±»\", \"pinyin\": \"jÃ¹ lÃ¨i\", \"trans\": \"cluster\"},\n    {\"word\": \"å¤§è§„æ¨¡\", \"pinyin\": \"dÃ  guÄ« mÃ³\", \"trans\": \"large-scale\"},\n    {\"word\": \"è¿­ä»£\", \"pinyin\": \"diÃ© dÃ i\", \"trans\": \"iterate\"},\n    {\"word\": \"æœç´¢\", \"pinyin\": \"sÅu suÇ’\", \"trans\": \"search\"},\n    {\"word\": \"æœ€ä½³\", \"pinyin\": \"zuÃ¬ jiÄ\", \"trans\": \"optimal\"},\n    {\"word\": \"æ··åˆ\", \"pinyin\": \"hÃ¹n hÃ©\", \"trans\": \"mix\"},\n    {\"word\": \"ä½¿ç”¨\", \"pinyin\": \"shÇ yÃ²ng\", \"trans\": \"use\"},\n    {\"word\": \"æ•ˆæœ\", \"pinyin\": \"xiÃ o guÇ’\", \"trans\": \"effect\"},\n    {\"word\": \"è¶…è¿‡\", \"pinyin\": \"chÄo guÃ²\", \"trans\": \"exceed\"},\n    {\"word\": \"ä»‹ç»\", \"pinyin\": \"jiÃ¨ shÃ o\", \"trans\": \"introduce\"},\n    {\"word\": \"åˆ†æ\", \"pinyin\": \"fÄ“n xÄ«\", \"trans\": \"analyze\"},\n    {\"word\": \"ç‰¹å¾\", \"pinyin\": \"tÃ¨ zhÄ“ng\", \"trans\": \"feature\"}\n]",
        "trans": "This article discusses the collection and optimization of pretraining datasets. Existing datasets typically come from web content and lack clear domain distinctions. The article proposes an automated framework called CLIMB, which embeds and clusters large-scale datasets to iteratively search for the optimal data mix. Using this method, the 1B model's training performance on 400B tokens surpassed that of Llama-3.2-1B. The article also introduces the ClimbLab and ClimbMix datasets and analyzes the characteristics of the optimal data mix.",
        "update_ts": "2025-04-20 12:41"
    }
}