{
    "date": {
        "ru": "18 апреля",
        "en": "April 18",
        "zh": "4月18日"
    },
    "time_utc": "2025-04-20 18:29",
    "weekday": 4,
    "issue_id": 3332,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.13161",
            "title": "CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for\n  Language Model Pre-training",
            "url": "https://huggingface.co/papers/2504.13161",
            "abstract": "Pre-training datasets are typically collected from web content and lack inherent domain divisions. For instance, widely used datasets like Common Crawl do not include explicit domain labels, while manually curating labeled datasets such as The Pile is labor-intensive. Consequently, identifying an optimal pre-training data mixture remains a challenging problem, despite its significant benefits for pre-training performance. To address these challenges, we propose CLustering-based Iterative Data Mixture Bootstrapping (CLIMB), an automated framework that discovers, evaluates, and refines data mixtures in a pre-training setting. Specifically, CLIMB embeds and clusters large-scale datasets in a semantic space and then iteratively searches for optimal mixtures using a smaller proxy model and a predictor. When continuously trained on 400B tokens with this mixture, our 1B model exceeds the state-of-the-art Llama-3.2-1B by 2.0%. Moreover, we observe that optimizing for a specific domain (e.g., Social Sciences) yields a 5% improvement over random sampling. Finally, we introduce ClimbLab, a filtered 1.2-trillion-token corpus with 20 clusters as a research playground, and ClimbMix, a compact yet powerful 400-billion-token dataset designed for efficient pre-training that delivers superior performance under an equal token budget. We analyze the final data mixture, elucidating the characteristics of an optimal data mixture. Our data is available at: https://research.nvidia.com/labs/lpr/climb/",
            "score": 83,
            "issue_id": 3306,
            "pub_date": "2025-04-17",
            "pub_date_card": {
                "ru": "17 апреля",
                "en": "April 17",
                "zh": "4月17日"
            },
            "hash": "cf9b6f2a06448097",
            "authors": [
                "Shizhe Diao",
                "Yu Yang",
                "Yonggan Fu",
                "Xin Dong",
                "Dan Su",
                "Markus Kliegl",
                "Zijia Chen",
                "Peter Belcak",
                "Yoshi Suhara",
                "Hongxu Yin",
                "Mostofa Patwary",
                "Yingyan",
                "Lin",
                "Jan Kautz",
                "Pavlo Molchanov"
            ],
            "affiliations": [
                "Georgia Institute of Technology, USA",
                "NVIDIA",
                "OpenAI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.13161.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#dataset",
                    "#optimization",
                    "#data"
                ],
                "emoji": "🧗",
                "ru": {
                    "title": "Восхождение к оптимальным данным для предобучения языковых моделей",
                    "desc": "Статья представляет CLIMB - автоматизированный фреймворк для обнаружения, оценки и улучшения смесей данных для предварительного обучения языковых моделей. CLIMB использует кластеризацию в семантическом пространстве и итеративный поиск оптимальных смесей с помощью прокси-модели. Применение CLIMB позволило создать модель, превосходящую Llama-3.2-1B на 2%, а также улучшить результаты для конкретных доменов. Авторы также представляют ClimbLab и ClimbMix - наборы данных для исследований и эффективного предобучения."
                },
                "en": {
                    "title": "Optimizing Pre-Training Data with CLIMB for Superior Model Performance",
                    "desc": "This paper introduces CLustering-based Iterative Data Mixture Bootstrapping (CLIMB), a novel framework for optimizing pre-training datasets in machine learning. CLIMB automates the process of discovering and refining data mixtures by embedding and clustering large datasets in a semantic space, which helps in identifying the best combinations for training models. The results show that a model trained on a carefully optimized mixture of 400 billion tokens outperforms existing models, demonstrating the importance of domain-specific data selection. Additionally, the paper presents ClimbLab and ClimbMix, two new datasets designed to facilitate research and improve pre-training efficiency."
                },
                "zh": {
                    "title": "优化预训练数据的智能框架",
                    "desc": "本论文提出了一种名为CLIMB的自动化框架，用于优化预训练数据的混合。CLIMB通过在语义空间中嵌入和聚类大规模数据集，迭代搜索最佳数据组合。实验表明，使用这种混合数据进行训练的模型在性能上超过了现有的最先进模型，并且针对特定领域的优化可以显著提高效果。最后，我们还推出了ClimbLab和ClimbMix两个数据集，以支持进一步的研究和高效的预训练。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.13146",
            "title": "Antidistillation Sampling",
            "url": "https://huggingface.co/papers/2504.13146",
            "abstract": "Frontier models that generate extended reasoning traces inadvertently produce rich token sequences that can facilitate model distillation. Recognizing this vulnerability, model owners may seek sampling strategies that limit the effectiveness of distillation without compromising model performance. Antidistillation sampling provides exactly this capability. By strategically modifying a model's next-token probability distribution, antidistillation sampling poisons reasoning traces, rendering them significantly less effective for distillation while preserving the model's practical utility. For further details, see https://antidistillation.com.",
            "score": 55,
            "issue_id": 3304,
            "pub_date": "2025-04-17",
            "pub_date_card": {
                "ru": "17 апреля",
                "en": "April 17",
                "zh": "4月17日"
            },
            "hash": "6aa117b5c441eb3d",
            "authors": [
                "Yash Savani",
                "Asher Trockman",
                "Zhili Feng",
                "Avi Schwarzschild",
                "Alexander Robey",
                "Marc Finzi",
                "J. Zico Kolter"
            ],
            "affiliations": [
                "Carnegie Mellon University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.13146.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "Защита моделей от дистилляции с помощью антидистилляции",
                    "desc": "В статье рассматривается проблема защиты моделей от дистилляции, когда модели генерируют сложные последовательности токенов. Авторы предлагают метод антидистилляционной выборки, который изменяет распределение вероятностей следующего токена. Это делает следы рассуждений менее полезными для дистилляции, сохраняя при этом производительность модели. Таким образом, можно защитить интеллектуальную собственность моделей, не ухудшая их работу."
                },
                "en": {
                    "title": "Protecting Model Knowledge with Antidistillation Sampling",
                    "desc": "This paper discusses a method called antidistillation sampling, which aims to protect advanced models from being easily distilled into simpler versions. Distillation is a process where a complex model's knowledge is transferred to a smaller model, but this can be exploited if the complex model generates detailed reasoning traces. Antidistillation sampling works by altering the probability distribution of the next token generated by the model, making the reasoning less useful for distillation. This approach allows the model to maintain its performance while reducing the risk of its knowledge being easily extracted."
                },
                "zh": {
                    "title": "抗蒸馏采样：保护模型性能的创新策略",
                    "desc": "前沿模型生成的推理轨迹会产生丰富的标记序列，这些序列可以帮助模型蒸馏。为了应对这一漏洞，模型拥有者可能会寻找采样策略，以限制蒸馏的有效性，同时不影响模型性能。抗蒸馏采样正是提供这种能力的策略。通过战略性地修改模型的下一个标记概率分布，抗蒸馏采样可以破坏推理轨迹，使其在蒸馏中变得不那么有效，同时保持模型的实际效用。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.12626",
            "title": "Packing Input Frame Context in Next-Frame Prediction Models for Video\n  Generation",
            "url": "https://huggingface.co/papers/2504.12626",
            "abstract": "We present a neural network structure, FramePack, to train next-frame (or next-frame-section) prediction models for video generation. The FramePack compresses input frames to make the transformer context length a fixed number regardless of the video length. As a result, we are able to process a large number of frames using video diffusion with computation bottleneck similar to image diffusion. This also makes the training video batch sizes significantly higher (batch sizes become comparable to image diffusion training). We also propose an anti-drifting sampling method that generates frames in inverted temporal order with early-established endpoints to avoid exposure bias (error accumulation over iterations). Finally, we show that existing video diffusion models can be finetuned with FramePack, and their visual quality may be improved because the next-frame prediction supports more balanced diffusion schedulers with less extreme flow shift timesteps.",
            "score": 38,
            "issue_id": 3303,
            "pub_date": "2025-04-17",
            "pub_date_card": {
                "ru": "17 апреля",
                "en": "April 17",
                "zh": "4月17日"
            },
            "hash": "fd1688a4e26dbb32",
            "authors": [
                "Lvmin Zhang",
                "Maneesh Agrawala"
            ],
            "affiliations": [
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.12626.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#diffusion",
                    "#video",
                    "#training"
                ],
                "emoji": "🎞️",
                "ru": {
                    "title": "FramePack: эффективное предсказание кадров для генерации видео",
                    "desc": "FramePack - это новая структура нейронной сети для предсказания следующего кадра в видео. Она сжимает входные кадры, что позволяет обрабатывать большое количество кадров с вычислительной сложностью, сравнимой с диффузией изображений. Авторы также предлагают метод сэмплирования, предотвращающий накопление ошибок. FramePack может быть использован для дообучения существующих моделей видеодиффузии, потенциально улучшая их визуальное качество."
                },
                "en": {
                    "title": "FramePack: Efficient Video Generation with Next-Frame Prediction",
                    "desc": "The paper introduces FramePack, a novel neural network architecture designed for predicting the next frame in video generation. By compressing input frames, FramePack ensures that the transformer can handle a fixed context length, making it efficient for processing long videos. This approach allows for larger training batch sizes, similar to those used in image diffusion, while maintaining computational efficiency. Additionally, the authors present an anti-drifting sampling method to mitigate exposure bias, enhancing the quality of generated frames and improving existing video diffusion models through fine-tuning."
                },
                "zh": {
                    "title": "FramePack：提升视频生成的下一帧预测能力",
                    "desc": "我们提出了一种神经网络结构，FramePack，用于训练视频生成的下一帧预测模型。FramePack通过压缩输入帧，使得变换器的上下文长度固定，无论视频长度如何。这样，我们能够使用与图像扩散相似的计算瓶颈处理大量帧，从而显著提高视频训练的批量大小。我们还提出了一种反漂移采样方法，以避免迭代过程中的曝光偏差，从而提高生成帧的质量。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.13169",
            "title": "Generate, but Verify: Reducing Hallucination in Vision-Language Models\n  with Retrospective Resampling",
            "url": "https://huggingface.co/papers/2504.13169",
            "abstract": "Vision-Language Models (VLMs) excel at visual understanding but often suffer from visual hallucinations, where they generate descriptions of nonexistent objects, actions, or concepts, posing significant risks in safety-critical applications. Existing hallucination mitigation methods typically follow one of two paradigms: generation adjustment, which modifies decoding behavior to align text with visual inputs, and post-hoc verification, where external models assess and correct outputs. While effective, generation adjustment methods often rely on heuristics and lack correction mechanisms, while post-hoc verification is complicated, typically requiring multiple models and tending to reject outputs rather than refine them. In this work, we introduce REVERSE, a unified framework that integrates hallucination-aware training with on-the-fly self-verification. By leveraging a new hallucination-verification dataset containing over 1.3M semi-synthetic samples, along with a novel inference-time retrospective resampling technique, our approach enables VLMs to both detect hallucinations during generation and dynamically revise those hallucinations. Our evaluations show that REVERSE achieves state-of-the-art hallucination reduction, outperforming the best existing methods by up to 12% on CHAIR-MSCOCO and 28% on HaloQuest. Our dataset, model, and code are available at: https://reverse-vlm.github.io.",
            "score": 36,
            "issue_id": 3304,
            "pub_date": "2025-04-17",
            "pub_date_card": {
                "ru": "17 апреля",
                "en": "April 17",
                "zh": "4月17日"
            },
            "hash": "f1ebb64cfce24e47",
            "authors": [
                "Tsung-Han Wu",
                "Heekyung Lee",
                "Jiaxin Ge",
                "Joseph E. Gonzalez",
                "Trevor Darrell",
                "David M. Chan"
            ],
            "affiliations": [
                "POSTECH",
                "University of California, Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.13169.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#inference",
                    "#hallucinations",
                    "#data",
                    "#cv"
                ],
                "emoji": "👁️",
                "ru": {
                    "title": "REVERSE: самокорректирующиеся VLM без галлюцинаций",
                    "desc": "Исследователи представили REVERSE - новый подход к снижению визуальных галлюцинаций в моделях компьютерного зрения и обработки естественного языка (VLM). Метод объединяет обучение с учетом галлюцинаций и самопроверку в режиме реального времени. REVERSE использует новый датасет из 1,3 млн полусинтетических образцов и технику ретроспективной выборки во время вывода. Эксперименты показали, что REVERSE превосходит существующие методы на 12-28% по снижению галлюцинаций на стандартных бенчмарках."
                },
                "en": {
                    "title": "REVERSE: Correcting Visual Hallucinations in VLMs Dynamically",
                    "desc": "This paper addresses the issue of visual hallucinations in Vision-Language Models (VLMs), where models incorrectly describe non-existent elements. The authors propose a new framework called REVERSE, which combines hallucination-aware training with real-time self-verification to improve the accuracy of VLM outputs. By utilizing a large dataset of semi-synthetic samples, REVERSE can identify and correct hallucinations during the generation process. The results demonstrate that this approach significantly reduces hallucinations, outperforming existing methods in various benchmarks."
                },
                "zh": {
                    "title": "REVERSE：动态修正视觉幻觉的统一框架",
                    "desc": "视觉语言模型（VLMs）在视觉理解方面表现出色，但常常出现视觉幻觉，即生成不存在的物体、动作或概念的描述，这在安全关键应用中带来了重大风险。现有的幻觉缓解方法通常分为两类：生成调整和后期验证。生成调整方法依赖启发式规则，缺乏有效的修正机制，而后期验证则复杂，通常需要多个模型，并倾向于拒绝输出而不是进行修正。我们提出的REVERSE框架结合了幻觉感知训练和实时自我验证，能够在生成过程中检测幻觉并动态修正，从而显著降低幻觉的发生。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.12369",
            "title": "WORLDMEM: Long-term Consistent World Simulation with Memory",
            "url": "https://huggingface.co/papers/2504.12369",
            "abstract": "World simulation has gained increasing popularity due to its ability to model virtual environments and predict the consequences of actions. However, the limited temporal context window often leads to failures in maintaining long-term consistency, particularly in preserving 3D spatial consistency. In this work, we present WorldMem, a framework that enhances scene generation with a memory bank consisting of memory units that store memory frames and states (e.g., poses and timestamps). By employing a memory attention mechanism that effectively extracts relevant information from these memory frames based on their states, our method is capable of accurately reconstructing previously observed scenes, even under significant viewpoint or temporal gaps. Furthermore, by incorporating timestamps into the states, our framework not only models a static world but also captures its dynamic evolution over time, enabling both perception and interaction within the simulated world. Extensive experiments in both virtual and real scenarios validate the effectiveness of our approach.",
            "score": 28,
            "issue_id": 3303,
            "pub_date": "2025-04-16",
            "pub_date_card": {
                "ru": "16 апреля",
                "en": "April 16",
                "zh": "4月16日"
            },
            "hash": "79cf162a1b60f887",
            "authors": [
                "Zeqi Xiao",
                "Yushi Lan",
                "Yifan Zhou",
                "Wenqi Ouyang",
                "Shuai Yang",
                "Yanhong Zeng",
                "Xingang Pan"
            ],
            "affiliations": [
                "S-Lab, Nanyang Technological University",
                "Shanghai AI Laboratory",
                "Wangxuan Institute of Computer Technology, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.12369.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#3d",
                    "#long_context"
                ],
                "emoji": "🌐",
                "ru": {
                    "title": "WorldMem: Улучшение долгосрочной согласованности в симуляции миров с помощью памяти",
                    "desc": "WorldMem - это фреймворк для улучшения симуляции виртуальных миров с помощью банка памяти. Он использует механизм внимания для извлечения релевантной информации из кадров памяти на основе их состояний. Это позволяет точно реконструировать ранее наблюдаемые сцены даже при значительных изменениях ракурса или временных промежутках. Включение временных меток в состояния позволяет моделировать не только статичный мир, но и его динамическое развитие во времени."
                },
                "en": {
                    "title": "Enhancing World Simulation with Memory-Driven Consistency",
                    "desc": "This paper introduces WorldMem, a novel framework designed to improve world simulation by utilizing a memory bank that stores various memory frames and states. The framework addresses the challenge of maintaining long-term consistency in 3D spatial representations, which is often hindered by limited temporal context. By implementing a memory attention mechanism, WorldMem can retrieve relevant information from stored memory frames, allowing for accurate scene reconstruction despite changes in viewpoint or time. Additionally, the integration of timestamps enables the framework to model both static and dynamic aspects of the environment, enhancing interaction and perception in simulated worlds."
                },
                "zh": {
                    "title": "增强世界模拟的一致性与动态性",
                    "desc": "世界模拟因其建模虚拟环境和预测行为后果的能力而越来越受欢迎。然而，有限的时间上下文窗口常常导致长期一致性维护的失败，特别是在保持三维空间一致性方面。在这项工作中，我们提出了WorldMem框架，通过一个包含记忆单元的记忆库来增强场景生成，这些记忆单元存储记忆帧和状态（例如，姿势和时间戳）。通过采用记忆注意机制，我们的方法能够准确重建先前观察到的场景，即使在显著的视角或时间间隔下也能有效提取相关信息。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.12322",
            "title": "A Strategic Coordination Framework of Small LLMs Matches Large LLMs in\n  Data Synthesis",
            "url": "https://huggingface.co/papers/2504.12322",
            "abstract": "While data synthesis and distillation are promising strategies to enhance small language models, current approaches heavily rely on Large Language Models (LLMs), which suffer from high computational costs, environmental inefficiency, and potential biases inherited from monolithic architectures. In contrast, smaller LLMs are more accessible and sustainable, but their individual capabilities often fall short in generating high-quality, diverse, and reliable data. Inspired by collaborative human processes (e.g., peer review), we propose a multiple small LLMs involved framework, GRA, that aggregates specialized roles across small LLMs to iterative refinement and quality control typically achieved by a single large LLM. In this collaborative framework, multiple small LLMs assume distinct roles-Generator, Reviewer, and Adjudicator-to simulate a peer-review-inspired data synthesis pipeline. The Generator proposes initial data samples, the Reviewer critiques their quality and diversity, and the Adjudicator resolves conflicts to finalize the output. By decomposing the synthesis process into specialized sub-tasks, collaborative small LLMs can achieve data-level parity with large LLM-based distillation. Through experiments across multiple benchmarks, we demonstrate that GRA-produced data matches or exceeds the quality of single large LLM outputs, e.g., Qwen-2.5-72B-Instruct. Our results challenge the necessity of monolithic large models for high-quality data synthesis, advocating instead for strategic coordination of smaller agents. Our datasets, models, and code are publicly available at https://github.com/GX-XinGao/GRA.",
            "score": 25,
            "issue_id": 3307,
            "pub_date": "2025-04-11",
            "pub_date_card": {
                "ru": "11 апреля",
                "en": "April 11",
                "zh": "4月11日"
            },
            "hash": "cc80e8015cf7f78d",
            "authors": [
                "Xin Gao",
                "Qizhi Pei",
                "Zinan Tang",
                "Yu Li",
                "Honglin Lin",
                "Jiang Wu",
                "Conghui He",
                "Lijun Wu"
            ],
            "affiliations": [
                "Renmin University of China",
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.12322.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#dataset",
                    "#synthetic",
                    "#small_models",
                    "#open_source"
                ],
                "emoji": "🤝",
                "ru": {
                    "title": "Коллаборативный синтез данных: малые модели, большие результаты",
                    "desc": "Статья представляет новый подход к синтезу данных с использованием малых языковых моделей (LLM) вместо крупных. Авторы предлагают фреймворк GRA, в котором несколько малых LLM выполняют роли Генератора, Рецензента и Арбитра, имитируя процесс экспертной оценки. Этот метод позволяет достичь качества данных, сравнимого с результатами крупных LLM, но с меньшими вычислительными затратами. Эксперименты показывают, что данные, созданные GRA, не уступают или превосходят по качеству выходные данные крупных LLM, таких как Qwen-2.5-72B-Instruct."
                },
                "en": {
                    "title": "Collaborative Small Models for High-Quality Data Synthesis",
                    "desc": "This paper introduces a framework called GRA, which uses multiple small language models (LLMs) to improve data synthesis and distillation. Instead of relying on a single large LLM, GRA assigns specialized roles to smaller models: a Generator creates data samples, a Reviewer evaluates their quality, and an Adjudicator resolves any discrepancies. This collaborative approach mimics peer review processes, allowing for iterative refinement and quality control. The results show that the data produced by GRA can match or even surpass the quality of data generated by large LLMs, suggesting that smaller models can be effectively coordinated for high-quality outcomes."
                },
                "zh": {
                    "title": "小模型协作，超越大模型的高质量数据合成",
                    "desc": "本文提出了一种名为GRA的框架，旨在通过多个小型语言模型（LLMs）协作来生成高质量的数据。该框架模拟了同行评审的过程，分配了生成器、审阅者和裁决者等不同角色，以实现数据合成的迭代优化和质量控制。通过将合成过程分解为专门的子任务，GRA能够在数据质量上与大型语言模型相媲美。实验结果表明，GRA生成的数据在质量上与单一大型模型的输出相当或更优，挑战了依赖大型模型进行高质量数据合成的必要性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.13181",
            "title": "Perception Encoder: The best visual embeddings are not at the output of\n  the network",
            "url": "https://huggingface.co/papers/2504.13181",
            "abstract": "We introduce Perception Encoder (PE), a state-of-the-art encoder for image and video understanding trained via simple vision-language learning. Traditionally, vision encoders have relied on a variety of pretraining objectives, each tailored to specific downstream tasks such as classification, captioning, or localization. Surprisingly, after scaling our carefully tuned image pretraining recipe and refining with our robust video data engine, we find that contrastive vision-language training alone can produce strong, general embeddings for all of these downstream tasks. There is only one caveat: these embeddings are hidden within the intermediate layers of the network. To draw them out, we introduce two alignment methods, language alignment for multimodal language modeling, and spatial alignment for dense prediction. Together with the core contrastive checkpoint, our PE family of models achieves state-of-the-art performance on a wide variety of tasks, including zero-shot image and video classification and retrieval; document, image, and video Q&A; and spatial tasks such as detection, depth estimation, and tracking. To foster further research, we are releasing our models, code, and a novel dataset of synthetically and human-annotated videos.",
            "score": 20,
            "issue_id": 3313,
            "pub_date": "2025-04-17",
            "pub_date_card": {
                "ru": "17 апреля",
                "en": "April 17",
                "zh": "4月17日"
            },
            "hash": "3c5cea92d3dd07c4",
            "authors": [
                "Daniel Bolya",
                "Po-Yao Huang",
                "Peize Sun",
                "Jang Hyun Cho",
                "Andrea Madotto",
                "Chen Wei",
                "Tengyu Ma",
                "Jiale Zhi",
                "Jathushan Rajasegaran",
                "Hanoona Rasheed",
                "Junke Wang",
                "Marco Monteiro",
                "Hu Xu",
                "Shiyu Dong",
                "Nikhila Ravi",
                "Daniel Li",
                "Piotr Dollár",
                "Christoph Feichtenhofer"
            ],
            "affiliations": [
                "Fudan University",
                "MBZUAI",
                "Meta FAIR",
                "Meta Reality Labs",
                "UT Austin"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.13181.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#multimodal",
                    "#dataset",
                    "#alignment",
                    "#open_source",
                    "#cv"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Универсальный энкодер для понимания изображений и видео",
                    "desc": "Исследователи представляют Perception Encoder (PE) - современный энкодер для понимания изображений и видео, обученный с помощью простого обучения на основе зрения и языка. В отличие от традиционных подходов, PE использует только контрастивное обучение для создания универсальных эмбеддингов. Авторы вводят методы языкового и пространственного выравнивания для извлечения этих эмбеддингов из промежуточных слоев сети. Модели семейства PE достигают передовых результатов в широком спектре задач компьютерного зрения и обработки естественного языка."
                },
                "en": {
                    "title": "Unlocking Versatile Visual Understanding with Perception Encoder",
                    "desc": "The paper presents the Perception Encoder (PE), an advanced model designed for understanding images and videos through vision-language learning. Unlike traditional encoders that depend on specific pretraining tasks, PE utilizes contrastive vision-language training to create versatile embeddings applicable to various tasks. The authors introduce two alignment techniques to extract these embeddings from the network's intermediate layers, enhancing performance in tasks like classification and question answering. By releasing their models and a new dataset, they aim to encourage further exploration in the field of multimodal learning."
                },
                "zh": {
                    "title": "感知编码器：图像与视频理解的新突破",
                    "desc": "我们介绍了一种名为感知编码器（PE）的先进编码器，用于图像和视频理解，采用简单的视觉-语言学习进行训练。传统的视觉编码器依赖于多种预训练目标，针对特定的下游任务如分类、描述或定位进行优化。令人惊讶的是，通过扩展我们精心调整的图像预训练方案，并结合强大的视频数据引擎，发现对比视觉-语言训练可以为所有这些下游任务生成强大的通用嵌入。为了提取这些嵌入，我们引入了两种对齐方法，分别用于多模态语言建模和密集预测，从而使我们的PE模型在多种任务上实现了最先进的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.13122",
            "title": "VistaDPO: Video Hierarchical Spatial-Temporal Direct Preference\n  Optimization for Large Video Models",
            "url": "https://huggingface.co/papers/2504.13122",
            "abstract": "Large Video Models (LVMs) built upon Large Language Models (LLMs) have shown promise in video understanding but often suffer from misalignment with human intuition and video hallucination issues. To address these challenges, we introduce VistaDPO, a novel framework for Video Hierarchical Spatial-Temporal Direct Preference Optimization. VistaDPO enhances text-video preference alignment across three hierarchical levels: i) Instance Level, aligning overall video content with responses; ii) Temporal Level, aligning video temporal semantics with event descriptions; and iii) Perceptive Level, aligning spatial objects with language tokens. Given the lack of datasets for fine-grained video-language preference alignment, we construct VistaDPO-7k, a dataset of 7.2K QA pairs annotated with chosen and rejected responses, along with spatial-temporal grounding information such as timestamps, keyframes, and bounding boxes. Extensive experiments on benchmarks such as Video Hallucination, Video QA, and Captioning performance tasks demonstrate that VistaDPO significantly improves the performance of existing LVMs, effectively mitigating video-language misalignment and hallucination. The code and data are available at https://github.com/HaroldChen19/VistaDPO.",
            "score": 20,
            "issue_id": 3303,
            "pub_date": "2025-04-17",
            "pub_date_card": {
                "ru": "17 апреля",
                "en": "April 17",
                "zh": "4月17日"
            },
            "hash": "efbc3b240498ce70",
            "authors": [
                "Haojian Huang",
                "Haodong Chen",
                "Shengqiong Wu",
                "Meng Luo",
                "Jinlan Fu",
                "Xinya Du",
                "Hanwang Zhang",
                "Hao Fei"
            ],
            "affiliations": [
                "Nanyang Technological University",
                "National University of Singapore",
                "The Hong Kong University of Science and Technology",
                "The University of Hong Kong",
                "University of Texas at Dallas"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.13122.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#alignment",
                    "#hallucinations",
                    "#dataset"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "VistaDPO: Точное согласование видео и языка на всех уровнях",
                    "desc": "VistaDPO - это новая система для оптимизации предпочтений в видео с использованием иерархического пространственно-временного подхода. Она улучшает согласование текста и видео на трех уровнях: общем содержании, временной семантике и пространственных объектах. Авторы создали датасет VistaDPO-7k с 7200 парами вопросов-ответов, аннотированными предпочтительными и отвергнутыми ответами, а также временными метками и ограничивающими рамками. Эксперименты показали, что VistaDPO значительно улучшает работу существующих больших видеомоделей, эффективно снижая рассогласование видео и языка и галлюцинации."
                },
                "en": {
                    "title": "Aligning Video and Language: Introducing VistaDPO",
                    "desc": "This paper presents VistaDPO, a new framework designed to improve the alignment between video content and human language understanding in Large Video Models (LVMs). It operates on three hierarchical levels: aligning overall video content with user responses, matching temporal semantics of videos with event descriptions, and correlating spatial objects with language tokens. To support this framework, the authors created a dataset called VistaDPO-7k, which includes 7.2K question-answer pairs with detailed annotations for better preference alignment. The results show that VistaDPO enhances the performance of LVMs by reducing issues related to video hallucination and misalignment with human intuition."
                },
                "zh": {
                    "title": "VistaDPO：提升视频理解的偏好对齐",
                    "desc": "本文介绍了一种新框架VistaDPO，用于视频层次空间-时间直接偏好优化，旨在解决大型视频模型（LVMs）在视频理解中的人类直觉不一致和视频幻觉问题。VistaDPO通过三个层次增强文本-视频偏好对齐：实例层、时间层和感知层，分别对齐视频内容、时间语义和空间对象。为了支持细粒度视频-语言偏好对齐，研究团队构建了VistaDPO-7k数据集，包含7200个问答对及其空间-时间信息。实验结果表明，VistaDPO显著提升了现有LVMs的性能，有效减轻了视频-语言的不一致性和幻觉现象。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.05506",
            "title": "ChartQAPro: A More Diverse and Challenging Benchmark for Chart Question\n  Answering",
            "url": "https://huggingface.co/papers/2504.05506",
            "abstract": "Charts are ubiquitous, as people often use them to analyze data, answer questions, and discover critical insights. However, performing complex analytical tasks with charts requires significant perceptual and cognitive effort. Chart Question Answering (CQA) systems automate this process by enabling models to interpret and reason with visual representations of data. However, existing benchmarks like ChartQA lack real-world diversity and have recently shown performance saturation with modern large vision-language models (LVLMs). To address these limitations, we introduce ChartQAPro, a new benchmark that includes 1,341 charts from 157 diverse sources, spanning various chart types, including infographics and dashboards, and featuring 1,948 questions in various types, such as multiple-choice, conversational, hypothetical, and unanswerable questions, to better reflect real-world challenges. Our evaluations with 21 models show a substantial performance drop for LVLMs on ChartQAPro; e.g., Claude Sonnet 3.5 scores 90.5% on ChartQA but only 55.81% on ChartQAPro, underscoring the complexity of chart reasoning. We complement our findings with detailed error analyses and ablation studies, identifying key challenges and opportunities for advancing LVLMs in chart understanding and reasoning. We release ChartQAPro at https://github.com/vis-nlp/ChartQAPro.",
            "score": 19,
            "issue_id": 3303,
            "pub_date": "2025-04-07",
            "pub_date_card": {
                "ru": "7 апреля",
                "en": "April 7",
                "zh": "4月7日"
            },
            "hash": "a727d08eac22e920",
            "authors": [
                "Ahmed Masry",
                "Mohammed Saidul Islam",
                "Mahir Ahmed",
                "Aayush Bajaj",
                "Firoz Kabir",
                "Aaryaman Kartha",
                "Md Tahmid Rahman Laskar",
                "Mizanur Rahman",
                "Shadikur Rahman",
                "Mehrad Shahmohammadi",
                "Megh Thakkar",
                "Md Rizwan Parvez",
                "Enamul Hoque",
                "Shafiq Joty"
            ],
            "affiliations": [
                "Dialpad Inc., Canada",
                "MILA - Quebec AI Institute, Canada",
                "Nanyang Technological University, Singapore",
                "Qatar Computing Research Institute (QCRI)",
                "RBC, Canada",
                "Salesforce Research, USA",
                "York University, Canada"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.05506.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#interpretability",
                    "#cv",
                    "#dataset",
                    "#reasoning"
                ],
                "emoji": "📊",
                "ru": {
                    "title": "ChartQAPro: новый вызов для ИИ в понимании графиков",
                    "desc": "ChartQAPro - новый набор данных для задачи ответов на вопросы по графикам. Он включает более 1300 разнообразных графиков из реальных источников и около 2000 вопросов различных типов. Тестирование показало, что современные мультимодальные языковые модели значительно хуже справляются с ChartQAPro по сравнению с предыдущими наборами данных. Авторы провели детальный анализ ошибок и выявили ключевые проблемы в понимании и рассуждении о графиках для языковых моделей."
                },
                "en": {
                    "title": "ChartQAPro: Elevating Chart Understanding for AI",
                    "desc": "This paper introduces ChartQAPro, a new benchmark designed to improve Chart Question Answering (CQA) systems by providing a more diverse and realistic set of charts and questions. Unlike previous benchmarks, ChartQAPro includes 1,341 charts from 157 sources and features various question types, which better reflect the complexities of real-world data analysis. The study reveals that modern large vision-language models (LVLMs) struggle significantly with this new benchmark, demonstrating a performance drop from 90.5% on the previous ChartQA to only 55.81% on ChartQAPro. Through error analyses and ablation studies, the authors identify challenges in chart reasoning that can guide future improvements in LVLMs."
                },
                "zh": {
                    "title": "提升图表问答系统的挑战与机遇",
                    "desc": "本论文介绍了ChartQAPro，这是一个新的基准测试，旨在提高图表问答系统的性能。它包含来自157个不同来源的1,341个图表，涵盖多种图表类型，并提供1,948个多样化的问题。通过对21个模型的评估，我们发现现代大型视觉语言模型在ChartQAPro上的表现显著下降，显示出图表推理的复杂性。我们的研究还包括详细的错误分析和消融研究，以识别图表理解和推理中的关键挑战和机遇。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.13055",
            "title": "NoisyRollout: Reinforcing Visual Reasoning with Data Augmentation",
            "url": "https://huggingface.co/papers/2504.13055",
            "abstract": "Recent advances in reinforcement learning (RL) have strengthened the reasoning capabilities of vision-language models (VLMs). However, enhancing policy exploration to more effectively scale test-time compute remains underexplored in VLMs. In addition, VLMs continue to struggle with imperfect visual perception, which in turn affects the subsequent reasoning process. To this end, we propose NoisyRollout, a simple yet effective RL approach that mixes trajectories from both clean and moderately distorted images to introduce targeted diversity in visual perception and the resulting reasoning patterns. Without additional training cost, NoisyRollout enhances the exploration capabilities of VLMs by incorporating a vision-oriented inductive bias. Furthermore, NoisyRollout employs a noise annealing schedule that gradually reduces distortion strength over training, ensuring benefit from noisy signals early while maintaining training stability and scalability in later stages. With just 2.1K training samples, NoisyRollout achieves state-of-the-art performance among open-source RL-tuned models on 5 out-of-domain benchmarks spanning both reasoning and perception tasks, while preserving comparable or even better in-domain performance.",
            "score": 16,
            "issue_id": 3307,
            "pub_date": "2025-04-17",
            "pub_date_card": {
                "ru": "17 апреля",
                "en": "April 17",
                "zh": "4月17日"
            },
            "hash": "058d7aa285231e64",
            "authors": [
                "Xiangyan Liu",
                "Jinjie Ni",
                "Zijian Wu",
                "Chao Du",
                "Longxu Dou",
                "Haonan Wang",
                "Tianyu Pang",
                "Michael Qizhe Shieh"
            ],
            "affiliations": [
                "National University of Singapore",
                "Sea AI Lab, Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.13055.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#rl",
                    "#training",
                    "#multimodal",
                    "#reasoning",
                    "#optimization"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "NoisyRollout: Улучшение исследования и рассуждений ВЯМ через контролируемый визуальный шум",
                    "desc": "Статья представляет новый подход в обучении с подкреплением для визуально-языковых моделей под названием NoisyRollout. Этот метод улучшает способности моделей к исследованию и рассуждению, смешивая траектории чистых и умеренно искаженных изображений. NoisyRollout использует график уменьшения шума, постепенно снижая силу искажений в процессе обучения. Метод достигает передовых результатов на пяти внедоменных бенчмарках, используя всего 2100 обучающих примеров."
                },
                "en": {
                    "title": "Enhancing VLMs with NoisyRollout for Better Reasoning and Perception",
                    "desc": "This paper introduces NoisyRollout, a reinforcement learning method designed to improve vision-language models (VLMs) by enhancing their policy exploration capabilities. It addresses the challenge of imperfect visual perception by mixing clean and distorted image trajectories, which helps diversify the reasoning patterns of the models. The approach uses a noise annealing schedule to gradually reduce distortion, allowing the model to benefit from noisy inputs during early training while ensuring stability later on. Remarkably, NoisyRollout achieves state-of-the-art results on various benchmarks with minimal training samples, demonstrating its effectiveness in both reasoning and perception tasks."
                },
                "zh": {
                    "title": "NoisyRollout：提升视觉-语言模型的推理能力",
                    "desc": "最近，强化学习（RL）的进展增强了视觉-语言模型（VLM）的推理能力。然而，在VLM中，增强策略探索以更有效地扩展测试时间计算仍然未被充分研究。此外，VLM在视觉感知不完美方面仍然面临挑战，这影响了后续的推理过程。为此，我们提出了NoisyRollout，这是一种简单而有效的RL方法，通过混合干净和适度失真的图像轨迹，引入目标多样性，从而改善视觉感知和推理模式。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.12364",
            "title": "DMM: Building a Versatile Image Generation Model via Distillation-Based\n  Model Merging",
            "url": "https://huggingface.co/papers/2504.12364",
            "abstract": "The success of text-to-image (T2I) generation models has spurred a proliferation of numerous model checkpoints fine-tuned from the same base model on various specialized datasets. This overwhelming specialized model production introduces new challenges for high parameter redundancy and huge storage cost, thereby necessitating the development of effective methods to consolidate and unify the capabilities of diverse powerful models into a single one. A common practice in model merging adopts static linear interpolation in the parameter space to achieve the goal of style mixing. However, it neglects the features of T2I generation task that numerous distinct models cover sundry styles which may lead to incompatibility and confusion in the merged model. To address this issue, we introduce a style-promptable image generation pipeline which can accurately generate arbitrary-style images under the control of style vectors. Based on this design, we propose the score distillation based model merging paradigm (DMM), compressing multiple models into a single versatile T2I model. Moreover, we rethink and reformulate the model merging task in the context of T2I generation, by presenting new merging goals and evaluation protocols. Our experiments demonstrate that DMM can compactly reorganize the knowledge from multiple teacher models and achieve controllable arbitrary-style generation.",
            "score": 15,
            "issue_id": 3306,
            "pub_date": "2025-04-16",
            "pub_date_card": {
                "ru": "16 апреля",
                "en": "April 16",
                "zh": "4月16日"
            },
            "hash": "49755535f03790d4",
            "authors": [
                "Tianhui Song",
                "Weixin Feng",
                "Shuai Wang",
                "Xubin Li",
                "Tiezheng Ge",
                "Bo Zheng",
                "Limin Wang"
            ],
            "affiliations": [
                "Nanjing University",
                "Nanjing University, Shanghai AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.12364.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#multimodal",
                    "#cv",
                    "#diffusion",
                    "#training",
                    "#dataset"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Объединение T2I моделей для гибкой генерации стилей",
                    "desc": "В статье представлен новый метод объединения моделей генерации изображений по тексту (T2I). Авторы предлагают конвейер генерации изображений, управляемый векторами стилей, и парадигму слияния моделей на основе дистилляции оценок (DMM). Этот подход позволяет сжать несколько моделей в одну универсальную T2I модель. Эксперименты показывают, что DMM может эффективно реорганизовать знания из нескольких учительских моделей и достичь контролируемой генерации изображений произвольного стиля."
                },
                "en": {
                    "title": "Unifying Diverse T2I Models with Style-Promptable Merging",
                    "desc": "This paper addresses the challenges posed by the proliferation of specialized text-to-image (T2I) generation models, which often lead to high parameter redundancy and storage costs. The authors propose a novel approach called score distillation based model merging (DMM) that consolidates multiple models into a single, versatile T2I model. Unlike traditional methods that use static linear interpolation, DMM incorporates style vectors to enable accurate generation of images in various styles while avoiding incompatibility issues. The paper also introduces new goals and evaluation protocols for model merging in the context of T2I generation, demonstrating that DMM effectively reorganizes knowledge from multiple models for controllable style generation."
                },
                "zh": {
                    "title": "高效合并多模型，实现可控图像生成",
                    "desc": "本文探讨了文本到图像生成模型（T2I）的合并问题。随着众多模型在不同数据集上微调，产生了大量的专用模型，这导致了参数冗余和存储成本高的问题。为了解决这些问题，本文提出了一种基于评分蒸馏的模型合并方法（DMM），能够将多个模型压缩为一个多功能的T2I模型。实验结果表明，DMM能够有效整合多个教师模型的知识，实现可控的任意风格图像生成。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.13180",
            "title": "PerceptionLM: Open-Access Data and Models for Detailed Visual\n  Understanding",
            "url": "https://huggingface.co/papers/2504.13180",
            "abstract": "Vision-language models are integral to computer vision research, yet many high-performing models remain closed-source, obscuring their data, design and training recipe. The research community has responded by using distillation from black-box models to label training data, achieving strong benchmark results, at the cost of measurable scientific progress. However, without knowing the details of the teacher model and its data sources, scientific progress remains difficult to measure. In this paper, we study building a Perception Language Model (PLM) in a fully open and reproducible framework for transparent research in image and video understanding. We analyze standard training pipelines without distillation from proprietary models and explore large-scale synthetic data to identify critical data gaps, particularly in detailed video understanding. To bridge these gaps, we release 2.8M human-labeled instances of fine-grained video question-answer pairs and spatio-temporally grounded video captions. Additionally, we introduce PLM-VideoBench, a suite for evaluating challenging video understanding tasks focusing on the ability to reason about \"what\", \"where\", \"when\", and \"how\" of a video. We make our work fully reproducible by providing data, training recipes, code & models.",
            "score": 13,
            "issue_id": 3323,
            "pub_date": "2025-04-17",
            "pub_date_card": {
                "ru": "17 апреля",
                "en": "April 17",
                "zh": "4月17日"
            },
            "hash": "b923d02610ef004b",
            "authors": [
                "Jang Hyun Cho",
                "Andrea Madotto",
                "Effrosyni Mavroudi",
                "Triantafyllos Afouras",
                "Tushar Nagarajan",
                "Muhammad Maaz",
                "Yale Song",
                "Tengyu Ma",
                "Shuming Hu",
                "Suyog Jain",
                "Miguel Martin",
                "Huiyu Wang",
                "Hanoona Rasheed",
                "Peize Sun",
                "Po-Yao Huang",
                "Daniel Bolya",
                "Nikhila Ravi",
                "Shashank Jain",
                "Tammy Stark",
                "Shane Moon",
                "Babak Damavandi",
                "Vivian Lee",
                "Andrew Westbury",
                "Salman Khan",
                "Philipp Krähenbühl",
                "Piotr Dollár",
                "Lorenzo Torresani",
                "Kristen Grauman",
                "Christoph Feichtenhofer"
            ],
            "affiliations": [
                "MBZUAI",
                "Meta FAIR",
                "Meta Reality Labs",
                "UT Austin"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.13180.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#open_source",
                    "#synthetic",
                    "#training",
                    "#reasoning",
                    "#benchmark",
                    "#data",
                    "#cv",
                    "#video"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Открытая модель восприятия и языка для прозрачного анализа видео",
                    "desc": "Статья посвящена созданию открытой и воспроизводимой модели восприятия и языка (PLM) для прозрачных исследований в области понимания изображений и видео. Авторы анализируют стандартные методы обучения без дистилляции от проприетарных моделей и исследуют синтетические данные большого масштаба. Для преодоления выявленных пробелов в данных, особенно в детальном понимании видео, авторы выпускают 2,8 миллиона размеченных вручную видео-вопросов с ответами и пространственно-временными аннотациями. Также представлен PLM-VideoBench - набор инструментов для оценки сложных задач понимания видео."
                },
                "en": {
                    "title": "Open and Reproducible Video Understanding with PLM",
                    "desc": "This paper focuses on creating a Perception Language Model (PLM) that is fully open and reproducible, addressing the limitations of closed-source models in computer vision. The authors highlight the challenges of using black-box models for training data labeling, which hinders scientific progress. They propose a new approach that utilizes large-scale synthetic data and provides 2.8 million human-labeled video question-answer pairs to improve video understanding. Additionally, they introduce PLM-VideoBench, a comprehensive evaluation suite for assessing video reasoning tasks, ensuring transparency and reproducibility in their research."
                },
                "zh": {
                    "title": "开放透明的感知语言模型研究",
                    "desc": "本文研究了一种开放且可重复的感知语言模型（PLM），旨在促进图像和视频理解的透明研究。我们分析了标准训练流程，避免使用封闭模型的蒸馏方法，并探索大规模合成数据以识别视频理解中的关键数据缺口。为了解决这些缺口，我们发布了280万个人工标注的细粒度视频问答对和时空基础的视频标题。我们还推出了PLM-VideoBench，一个评估视频理解任务的工具，专注于推理视频的“什么”、“哪里”、“何时”和“如何”。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.13171",
            "title": "Sleep-time Compute: Beyond Inference Scaling at Test-time",
            "url": "https://huggingface.co/papers/2504.13171",
            "abstract": "Scaling test-time compute has emerged as a key ingredient for enabling large language models (LLMs) to solve difficult problems, but comes with high latency and inference cost. We introduce sleep-time compute, which allows models to \"think\" offline about contexts before queries are presented: by anticipating what queries users might ask and pre-computing useful quantities, we can significantly reduce the compute requirements at test-time. To demonstrate the efficacy of our method, we create modified versions of two reasoning tasks - Stateful GSM-Symbolic and Stateful AIME. We find that sleep-time compute can reduce the amount of test-time compute needed to achieve the same accuracy by ~ 5x on Stateful GSM-Symbolic and Stateful AIME and that by scaling sleep-time compute we can further increase accuracy by up to 13% on Stateful GSM-Symbolic and 18% on Stateful AIME. Furthermore, we introduce Multi-Query GSM-Symbolic, which extends GSM-Symbolic by including multiple related queries per context. By amortizing sleep-time compute across related queries about the same context using Multi-Query GSM-Symbolic, we can decrease the average cost per query by 2.5x. We then conduct additional analysis to understand when sleep-time compute is most effective, finding the predictability of the user query to be well correlated with the efficacy of sleep-time compute. Finally, we conduct a case-study of applying sleep-time compute to a realistic agentic SWE task.",
            "score": 13,
            "issue_id": 3310,
            "pub_date": "2025-04-17",
            "pub_date_card": {
                "ru": "17 апреля",
                "en": "April 17",
                "zh": "4月17日"
            },
            "hash": "094d6f648b644327",
            "authors": [
                "Kevin Lin",
                "Charlie Snell",
                "Yu Wang",
                "Charles Packer",
                "Sarah Wooders",
                "Ion Stoica",
                "Joseph E. Gonzalez"
            ],
            "affiliations": [
                "Letta",
                "University of California, Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.13171.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#inference",
                    "#reasoning",
                    "#agents"
                ],
                "emoji": "💤",
                "ru": {
                    "title": "Оптимизация LLM: думай заранее, отвечай быстрее",
                    "desc": "Эта статья представляет концепцию 'вычислений во время сна' для больших языковых моделей (LLM). Метод предполагает предварительные вычисления для ожидаемых запросов, что значительно снижает вычислительные затраты во время выполнения. Авторы демонстрируют эффективность подхода на модифицированных версиях задач рассуждения, показывая пятикратное сокращение необходимых вычислений при сохранении точности. Исследование также выявляет, что предсказуемость пользовательских запросов коррелирует с эффективностью метода 'вычислений во время сна'."
                },
                "en": {
                    "title": "Optimize Query Processing with Sleep-Time Compute",
                    "desc": "This paper presents a novel approach called sleep-time compute, which allows large language models (LLMs) to prepare for user queries by pre-computing relevant information offline. By anticipating potential questions, the model can significantly reduce the computational load during test-time, achieving up to 5 times less compute while maintaining accuracy. The authors demonstrate that scaling sleep-time compute can enhance accuracy by up to 18% on specific reasoning tasks. Additionally, they introduce Multi-Query GSM-Symbolic, which optimizes the processing of multiple related queries, further decreasing the average cost per query by 2.5 times."
                },
                "zh": {
                    "title": "睡眠时间计算：提升语言模型效率的新方法",
                    "desc": "本文提出了一种新的计算方法，称为睡眠时间计算，旨在减少大型语言模型在测试时的计算需求。通过在用户提出查询之前，模型可以离线思考并预计算有用的信息，从而降低延迟和推理成本。研究表明，使用睡眠时间计算可以在保持相同准确率的情况下，将测试时的计算需求减少约5倍，并在某些任务上提高准确率。我们还引入了多查询GSM-符号化，允许在同一上下文中处理多个相关查询，从而进一步降低每个查询的平均成本。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.12395",
            "title": "InstantCharacter: Personalize Any Characters with a Scalable Diffusion\n  Transformer Framework",
            "url": "https://huggingface.co/papers/2504.12395",
            "abstract": "Current learning-based subject customization approaches, predominantly relying on U-Net architectures, suffer from limited generalization ability and compromised image quality. Meanwhile, optimization-based methods require subject-specific fine-tuning, which inevitably degrades textual controllability. To address these challenges, we propose InstantCharacter, a scalable framework for character customization built upon a foundation diffusion transformer. InstantCharacter demonstrates three fundamental advantages: first, it achieves open-domain personalization across diverse character appearances, poses, and styles while maintaining high-fidelity results. Second, the framework introduces a scalable adapter with stacked transformer encoders, which effectively processes open-domain character features and seamlessly interacts with the latent space of modern diffusion transformers. Third, to effectively train the framework, we construct a large-scale character dataset containing 10-million-level samples. The dataset is systematically organized into paired (multi-view character) and unpaired (text-image combinations) subsets. This dual-data structure enables simultaneous optimization of identity consistency and textual editability through distinct learning pathways. Qualitative experiments demonstrate the advanced capabilities of InstantCharacter in generating high-fidelity, text-controllable, and character-consistent images, setting a new benchmark for character-driven image generation. Our source code is available at https://github.com/Tencent/InstantCharacter.",
            "score": 13,
            "issue_id": 3308,
            "pub_date": "2025-04-16",
            "pub_date_card": {
                "ru": "16 апреля",
                "en": "April 16",
                "zh": "4月16日"
            },
            "hash": "4000d7f0fc525f6d",
            "authors": [
                "Jiale Tao",
                "Yanbing Zhang",
                "Qixun Wang",
                "Yiji Cheng",
                "Haofan Wang",
                "Xu Bai",
                "Zhengguang Zhou",
                "Ruihuang Li",
                "Linqing Wang",
                "Chunyu Wang",
                "Qin Lin",
                "Qinglin Lu"
            ],
            "affiliations": [
                "Hunyuan, Tencent",
                "InstantX Team"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.12395.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#data",
                    "#diffusion",
                    "#dataset",
                    "#benchmark",
                    "#cv"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Мгновенная кастомизация персонажей с сохранением высокого качества и текстового контроля",
                    "desc": "InstantCharacter - это новый подход к кастомизации персонажей, основанный на диффузионном трансформере. Он решает проблемы ограниченной обобщаемости и ухудшения качества изображений, характерные для существующих методов. Фреймворк использует масштабируемый адаптер со стековыми энкодерами трансформеров для обработки характеристик персонажей. Для обучения был создан большой набор данных из 10 миллионов образцов, организованный в парные и непарные подмножества."
                },
                "en": {
                    "title": "InstantCharacter: Revolutionizing Character Customization with Diffusion Transformers",
                    "desc": "This paper introduces InstantCharacter, a new framework for customizing characters using machine learning. Unlike traditional methods that struggle with generalization and image quality, InstantCharacter leverages a diffusion transformer to achieve high-fidelity results across various character styles and poses. The framework includes a scalable adapter with transformer encoders that effectively manage character features and interact with the latent space of diffusion models. Additionally, it utilizes a large-scale dataset of 10 million samples to optimize both identity consistency and textual editability, demonstrating superior performance in generating controllable character images."
                },
                "zh": {
                    "title": "InstantCharacter：高保真角色定制的新标准",
                    "desc": "本文提出了一种名为InstantCharacter的角色定制框架，旨在解决现有学习基础方法在图像质量和泛化能力上的不足。该框架基于扩散变换器，能够在多样化的角色外观、姿势和风格中实现开放域个性化，同时保持高保真度。InstantCharacter引入了可扩展的适配器，利用堆叠的变换器编码器有效处理开放域角色特征，并与现代扩散变换器的潜在空间无缝交互。此外，构建了一个包含千万级样本的大规模角色数据集，以支持框架的有效训练，优化身份一致性和文本可编辑性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.11651",
            "title": "70% Size, 100% Accuracy: Lossless LLM Compression for Efficient GPU\n  Inference via Dynamic-Length Float",
            "url": "https://huggingface.co/papers/2504.11651",
            "abstract": "Large Language Models (LLMs) have grown rapidly in size, creating significant challenges for efficient deployment on resource-constrained hardware. In this paper, we introduce Dynamic-Length Float (DFloat11), a lossless compression framework that reduces LLM size by 30% while preserving outputs that are bit-for-bit identical to the original model. DFloat11 is motivated by the low entropy in the BFloat16 weight representation of LLMs, which reveals significant inefficiency in existing storage format. By applying entropy coding, DFloat11 assigns dynamic-length encodings to weights based on frequency, achieving near information-optimal compression without any loss of precision. To facilitate efficient inference with dynamic-length encodings, we develop a custom GPU kernel for fast online decompression. Our design incorporates the following: (i) decomposition of memory-intensive lookup tables (LUTs) into compact LUTs that fit in GPU SRAM, (ii) a two-phase kernel for coordinating thread read/write positions using lightweight auxiliary variables, and (iii) transformer-block-level decompression to minimize latency. Experiments on recent models, including Llama-3.1, Qwen-2.5, and Gemma-3, validates our hypothesis that DFloat11 achieves around 30% model size reduction while preserving bit-for-bit exact outputs. Compared to a potential alternative of offloading parts of an uncompressed model to the CPU to meet memory constraints, DFloat11 achieves 1.9-38.8x higher throughput in token generation. With a fixed GPU memory budget, DFloat11 enables 5.3-13.17x longer context lengths than uncompressed models. Notably, our method enables lossless inference of Llama-3.1-405B, an 810GB model, on a single node equipped with 8x80GB GPUs. Our code and models are available at https://github.com/LeanModels/DFloat11.",
            "score": 13,
            "issue_id": 3321,
            "pub_date": "2025-04-15",
            "pub_date_card": {
                "ru": "15 апреля",
                "en": "April 15",
                "zh": "4月15日"
            },
            "hash": "bbc8b58a0d8e7d08",
            "authors": [
                "Tianyi Zhang",
                "Yang Sui",
                "Shaochen Zhong",
                "Vipin Chaudhary",
                "Xia Hu",
                "Anshumali Shrivastava"
            ],
            "affiliations": [
                "Department of Computer Science, Rice University",
                "Department of Computer and Data Sciences, Case Western Reserve University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.11651.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#optimization",
                    "#long_context",
                    "#inference"
                ],
                "emoji": "🗜️",
                "ru": {
                    "title": "DFloat11: Эффективное сжатие больших языковых моделей без потерь",
                    "desc": "Статья представляет Dynamic-Length Float (DFloat11) - фреймворк для сжатия больших языковых моделей (LLM) без потерь. DFloat11 использует энтропийное кодирование для присвоения динамических кодов весам модели, что позволяет сократить размер LLM на 30% без изменения выходных данных. Авторы разработали специальное ядро GPU для быстрой онлайн-декомпрессии, включающее декомпозицию таблиц поиска и двухфазный алгоритм для координации потоков. Эксперименты на современных моделях, таких как Llama-3.1 и Gemma-3, подтверждают эффективность метода, позволяя значительно увеличить пропускную способность и длину контекста при фиксированном объеме памяти GPU."
                },
                "en": {
                    "title": "Efficient Compression for Large Language Models with DFloat11",
                    "desc": "This paper presents DFloat11, a novel lossless compression framework designed to reduce the size of Large Language Models (LLMs) by 30% while maintaining exact output fidelity. The framework leverages the low entropy in the BFloat16 weight representation, applying entropy coding to assign dynamic-length encodings to model weights based on their frequency. DFloat11 includes a custom GPU kernel for efficient online decompression, optimizing memory usage and minimizing latency during inference. Experimental results demonstrate that DFloat11 significantly enhances throughput and allows for longer context lengths in LLMs, making it a powerful solution for deploying large models on resource-constrained hardware."
                },
                "zh": {
                    "title": "DFloat11：高效压缩大型语言模型的无损框架",
                    "desc": "大型语言模型（LLMs）在规模上迅速增长，这给资源有限的硬件部署带来了重大挑战。本文介绍了一种无损压缩框架DFloat11，能够在保持输出与原始模型逐位相同的情况下，将LLM的大小减少30%。DFloat11通过熵编码，根据权重的频率为其分配动态长度编码，实现接近信息最优的压缩而不损失精度。我们的实验表明，DFloat11在多个模型上验证了其有效性，显著提高了生成速度和上下文长度。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.13145",
            "title": "Exploring Expert Failures Improves LLM Agent Tuning",
            "url": "https://huggingface.co/papers/2504.13145",
            "abstract": "Large Language Models (LLMs) have shown tremendous potential as agents, excelling at tasks that require multiple rounds of reasoning and interactions. Rejection Sampling Fine-Tuning (RFT) has emerged as an effective method for finetuning LLMs as agents: it first imitates expert-generated successful trajectories and further improves agentic skills through iterative fine-tuning on successful, self-generated trajectories. However, since the expert (e.g., GPT-4) succeeds primarily on simpler subtasks and RFT inherently favors simpler scenarios, many complex subtasks remain unsolved and persistently out-of-distribution (OOD). Upon investigating these challenging subtasks, we discovered that previously failed expert trajectories can often provide valuable guidance, e.g., plans and key actions, that can significantly improve agent exploration efficiency and acquisition of critical skills. Motivated by these observations, we propose Exploring Expert Failures (EEF), which identifies beneficial actions from failed expert trajectories and integrates them into the training dataset. Potentially harmful actions are meticulously excluded to prevent contamination of the model learning process. By leveraging the beneficial actions in expert failures, EEF successfully solves some previously unsolvable subtasks and improves agent tuning performance. Remarkably, our approach achieved a 62\\% win rate in WebShop, outperforming RFT (53. 6\\%) and GPT-4 (35. 6\\%), and to the best of our knowledge, setting a new state-of-the-art as the first method to surpass a score of 0.81 in WebShop and exceed 81 in SciWorld.",
            "score": 11,
            "issue_id": 3305,
            "pub_date": "2025-04-17",
            "pub_date_card": {
                "ru": "17 апреля",
                "en": "April 17",
                "zh": "4月17日"
            },
            "hash": "597cd9806c8a07ff",
            "authors": [
                "Li-Cheng Lan",
                "Andrew Bai",
                "Minhao Cheng",
                "Ruochen Wang",
                "Cho-Jui Hsieh",
                "Tianyi Zhou"
            ],
            "affiliations": [
                "OpenAI",
                "Pennsylvania State University",
                "UCLA",
                "University of Maryland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.13145.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#agents",
                    "#training",
                    "#optimization"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Учимся на ошибках: новый метод обучения ИИ-агентов",
                    "desc": "Статья представляет новый метод обучения больших языковых моделей (LLM) для выполнения сложных задач. Метод называется Exploring Expert Failures (EEF) и использует информацию из неудачных попыток экспертной модели для улучшения процесса обучения. EEF превзошел предыдущие методы, такие как Rejection Sampling Fine-Tuning (RFT), в задачах WebShop и SciWorld. Авторы показывают, что использование полезных действий из неудачных экспертных траекторий может значительно улучшить эффективность исследования и приобретение критических навыков агентом."
                },
                "en": {
                    "title": "Learning from Mistakes: Enhancing LLMs with Expert Failures",
                    "desc": "This paper introduces Exploring Expert Failures (EEF), a novel approach to enhance the performance of Large Language Models (LLMs) in complex tasks. EEF builds on Rejection Sampling Fine-Tuning (RFT) by utilizing insights from previously failed expert trajectories to identify beneficial actions that can improve agent exploration and skill acquisition. By integrating these valuable actions into the training dataset while excluding harmful ones, EEF addresses previously unsolvable subtasks and boosts overall agent performance. The results demonstrate that EEF outperforms existing methods, achieving a 62% win rate in WebShop and setting new benchmarks in task performance."
                },
                "zh": {
                    "title": "从失败中学习，提升智能体能力",
                    "desc": "大型语言模型（LLMs）在多轮推理和交互任务中表现出色。拒绝采样微调（RFT）是一种有效的微调方法，通过模仿专家生成的成功轨迹并在自生成的成功轨迹上进行迭代微调来提升模型的能力。然而，由于RFT偏向于简单场景，许多复杂子任务仍然未能解决。我们提出的探索专家失败（EEF）方法，通过从失败的专家轨迹中提取有益的行动，显著提高了模型的探索效率和关键技能的获取。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.07959",
            "title": "CCMNet: Leveraging Calibrated Color Correction Matrices for Cross-Camera\n  Color Constancy",
            "url": "https://huggingface.co/papers/2504.07959",
            "abstract": "Computational color constancy, or white balancing, is a key module in a camera's image signal processor (ISP) that corrects color casts from scene lighting. Because this operation occurs in the camera-specific raw color space, white balance algorithms must adapt to different cameras. This paper introduces a learning-based method for cross-camera color constancy that generalizes to new cameras without retraining. Our method leverages pre-calibrated color correction matrices (CCMs) available on ISPs that map the camera's raw color space to a standard space (e.g., CIE XYZ). Our method uses these CCMs to transform predefined illumination colors (i.e., along the Planckian locus) into the test camera's raw space. The mapped illuminants are encoded into a compact camera fingerprint embedding (CFE) that enables the network to adapt to unseen cameras. To prevent overfitting due to limited cameras and CCMs during training, we introduce a data augmentation technique that interpolates between cameras and their CCMs. Experimental results across multiple datasets and backbones show that our method achieves state-of-the-art cross-camera color constancy while remaining lightweight and relying only on data readily available in camera ISPs.",
            "score": 10,
            "issue_id": 3311,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 апреля",
                "en": "April 10",
                "zh": "4月10日"
            },
            "hash": "f4c62f5c36856c86",
            "authors": [
                "Dongyoung Kim",
                "Mahmoud Afifi",
                "Dongyun Kim",
                "Michael S. Brown",
                "Seon Joo Kim"
            ],
            "affiliations": [
                "AI Center - Toronto, Samsung Electronics",
                "Yonsei University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.07959.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#cv",
                    "#dataset"
                ],
                "emoji": "📸",
                "ru": {
                    "title": "Адаптивный баланс белого для любой камеры",
                    "desc": "Эта статья представляет новый метод машинного обучения для коррекции баланса белого в камерах, который может адаптироваться к новым камерам без переобучения. Метод использует предварительно откалиброванные матрицы коррекции цвета (CCM) для создания компактного встраивания-отпечатка камеры (CFE). Авторы также вводят технику аугментации данных, интерполирующую между камерами и их CCM для предотвращения переобучения. Экспериментальные результаты показывают, что метод достигает современного уровня в межкамерной цветовой константности, оставаясь при этом легковесным."
                },
                "en": {
                    "title": "Adaptive Color Constancy Across Cameras Without Retraining",
                    "desc": "This paper presents a novel approach to computational color constancy, specifically focusing on white balancing in images captured by different cameras. The proposed method utilizes pre-calibrated color correction matrices (CCMs) to adaptively transform illumination colors into the raw color space of new cameras without the need for retraining. By creating a compact camera fingerprint embedding (CFE), the model can effectively generalize to unseen cameras, enhancing its versatility. Additionally, a data augmentation technique is introduced to mitigate overfitting, ensuring robust performance across various datasets and camera types."
                },
                "zh": {
                    "title": "跨相机颜色恒常性的创新方法",
                    "desc": "本文介绍了一种基于学习的跨相机颜色恒常性方法，旨在解决不同相机的白平衡问题。该方法利用预先校准的颜色校正矩阵（CCM），将相机的原始颜色空间映射到标准空间。通过将预定义的照明颜色转换为测试相机的原始空间，生成紧凑的相机指纹嵌入（CFE），使网络能够适应未见过的相机。实验结果表明，该方法在多个数据集上实现了最先进的跨相机颜色恒常性，同时保持轻量级，依赖于相机ISP中现成的数据。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.13143",
            "title": "Complex-Edit: CoT-Like Instruction Generation for\n  Complexity-Controllable Image Editing Benchmark",
            "url": "https://huggingface.co/papers/2504.13143",
            "abstract": "We introduce Complex-Edit, a comprehensive benchmark designed to systematically evaluate instruction-based image editing models across instructions of varying complexity. To develop this benchmark, we harness GPT-4o to automatically collect a diverse set of editing instructions at scale. Our approach follows a well-structured ``Chain-of-Edit'' pipeline: we first generate individual atomic editing tasks independently and then integrate them to form cohesive, complex instructions. Additionally, we introduce a suite of metrics to assess various aspects of editing performance, along with a VLM-based auto-evaluation pipeline that supports large-scale assessments. Our benchmark yields several notable insights: 1) Open-source models significantly underperform relative to proprietary, closed-source models, with the performance gap widening as instruction complexity increases; 2) Increased instructional complexity primarily impairs the models' ability to retain key elements from the input images and to preserve the overall aesthetic quality; 3) Decomposing a complex instruction into a sequence of atomic steps, executed in a step-by-step manner, substantially degrades performance across multiple metrics; 4) A straightforward Best-of-N selection strategy improves results for both direct editing and the step-by-step sequential approach; and 5) We observe a ``curse of synthetic data'': when synthetic data is involved in model training, the edited images from such models tend to appear increasingly synthetic as the complexity of the editing instructions rises -- a phenomenon that intriguingly also manifests in the latest GPT-4o outputs.",
            "score": 7,
            "issue_id": 3324,
            "pub_date": "2025-04-17",
            "pub_date_card": {
                "ru": "17 апреля",
                "en": "April 17",
                "zh": "4月17日"
            },
            "hash": "5cf48cefe0cb6c8d",
            "authors": [
                "Siwei Yang",
                "Mude Hui",
                "Bingchen Zhao",
                "Yuyin Zhou",
                "Nataniel Ruiz",
                "Cihang Xie"
            ],
            "affiliations": [
                "Google",
                "University of California, Santa Cruz",
                "University of Edinburgh"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.13143.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#synthetic",
                    "#open_source",
                    "#data",
                    "#benchmark"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Complex-Edit: новый стандарт оценки моделей редактирования изображений",
                    "desc": "Статья представляет Complex-Edit - комплексный бенчмарк для оценки моделей редактирования изображений на основе инструкций различной сложности. Авторы используют GPT-4 для автоматического сбора разнообразных инструкций по редактированию в масштабе. Они вводят набор метрик для оценки различных аспектов производительности редактирования, а также конвейер автоматической оценки на основе VLM. Исследование выявляет ряд важных наблюдений, включая значительное отставание моделей с открытым исходным кодом от проприетарных моделей и ухудшение качества при разложении сложных инструкций на последовательность атомарных шагов."
                },
                "en": {
                    "title": "Evaluating Image Editing Models: Complexity Matters!",
                    "desc": "The paper presents Complex-Edit, a benchmark for evaluating instruction-based image editing models based on the complexity of editing tasks. It utilizes GPT-4o to generate a wide range of editing instructions, which are then organized into a structured 'Chain-of-Edit' pipeline. The study reveals that open-source models lag behind proprietary ones, especially as task complexity increases, affecting their ability to maintain image quality. Additionally, it highlights the negative impact of breaking down complex instructions into simpler steps and introduces a Best-of-N selection strategy to enhance editing performance."
                },
                "zh": {
                    "title": "复杂指令下的图像编辑性能评估",
                    "desc": "本文介绍了Complex-Edit，这是一个全面的基准测试，旨在系统评估基于指令的图像编辑模型，涵盖不同复杂度的指令。我们利用GPT-4o自动收集多样化的编辑指令，并采用结构化的“编辑链”流程生成独立的原子编辑任务，再将其整合为复杂指令。研究发现，开源模型在性能上显著低于闭源模型，且随着指令复杂度的增加，性能差距加大。此外，复杂指令的分解执行会显著降低模型的表现，而简单的选择策略则能改善编辑效果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.12157",
            "title": "FocusedAD: Character-centric Movie Audio Description",
            "url": "https://huggingface.co/papers/2504.12157",
            "abstract": "Movie Audio Description (AD) aims to narrate visual content during dialogue-free segments, particularly benefiting blind and visually impaired (BVI) audiences. Compared with general video captioning, AD demands plot-relevant narration with explicit character name references, posing unique challenges in movie understanding.To identify active main characters and focus on storyline-relevant regions, we propose FocusedAD, a novel framework that delivers character-centric movie audio descriptions. It includes: (i) a Character Perception Module(CPM) for tracking character regions and linking them to names; (ii) a Dynamic Prior Module(DPM) that injects contextual cues from prior ADs and subtitles via learnable soft prompts; and (iii) a Focused Caption Module(FCM) that generates narrations enriched with plot-relevant details and named characters. To overcome limitations in character identification, we also introduce an automated pipeline for building character query banks. FocusedAD achieves state-of-the-art performance on multiple benchmarks, including strong zero-shot results on MAD-eval-Named and our newly proposed Cinepile-AD dataset. Code and data will be released at https://github.com/Thorin215/FocusedAD .",
            "score": 7,
            "issue_id": 3306,
            "pub_date": "2025-04-16",
            "pub_date_card": {
                "ru": "16 апреля",
                "en": "April 16",
                "zh": "4月16日"
            },
            "hash": "2bfe38936e42dd11",
            "authors": [
                "Xiaojun Ye",
                "Chun Wang",
                "Yiren Song",
                "Sheng Zhou",
                "Liangcheng Li",
                "Jiajun Bu"
            ],
            "affiliations": [
                "National University of Singapore Singapore",
                "Zhejiang University China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.12157.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#multimodal",
                    "#audio",
                    "#benchmark",
                    "#video",
                    "#dataset",
                    "#science"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Умное аудиоописание фильмов с фокусом на персонажах",
                    "desc": "Статья представляет FocusedAD - новый фреймворк для создания аудиоописаний фильмов, ориентированных на персонажей. Система включает модуль восприятия персонажей (CPM) для отслеживания и именования персонажей, динамический модуль предыстории (DPM) для внедрения контекстных подсказок, и модуль фокусированных подписей (FCM) для генерации релевантных описаний с именами персонажей. FocusedAD также предлагает автоматизированный процесс создания банков запросов персонажей для улучшения их идентификации. Фреймворк достигает наилучших результатов на нескольких бенчмарках, включая сильные результаты в режиме zero-shot."
                },
                "en": {
                    "title": "FocusedAD: Enhancing Movie Accessibility with Character-Centric Audio Descriptions",
                    "desc": "This paper presents FocusedAD, a new framework designed to create audio descriptions for movies, specifically targeting blind and visually impaired audiences. It addresses the unique challenges of audio description by focusing on character-centric narration that includes character names and relevant plot details. The framework consists of three main components: a Character Perception Module for tracking characters, a Dynamic Prior Module for incorporating contextual information, and a Focused Caption Module for generating detailed narrations. FocusedAD demonstrates superior performance on various benchmarks, including zero-shot evaluations, showcasing its effectiveness in enhancing movie accessibility."
                },
                "zh": {
                    "title": "以角色为中心的电影音频描述",
                    "desc": "电影音频描述（AD）旨在为盲人和视力障碍者在无对话的片段中叙述视觉内容。与一般视频字幕相比，AD需要与情节相关的叙述，并明确提及角色名称，这给电影理解带来了独特的挑战。我们提出了FocusedAD框架，通过角色感知模块、动态先验模块和聚焦字幕模块，提供以角色为中心的电影音频描述。FocusedAD在多个基准测试中实现了最先进的性能，包括在MAD-eval-Named和新提出的Cinepile-AD数据集上的强大零样本结果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.13079",
            "title": "Retrieval-Augmented Generation with Conflicting Evidence",
            "url": "https://huggingface.co/papers/2504.13079",
            "abstract": "Large language model (LLM) agents are increasingly employing retrieval-augmented generation (RAG) to improve the factuality of their responses. However, in practice, these systems often need to handle ambiguous user queries and potentially conflicting information from multiple sources while also suppressing inaccurate information from noisy or irrelevant documents. Prior work has generally studied and addressed these challenges in isolation, considering only one aspect at a time, such as handling ambiguity or robustness to noise and misinformation. We instead consider multiple factors simultaneously, proposing (i) RAMDocs (Retrieval with Ambiguity and Misinformation in Documents), a new dataset that simulates complex and realistic scenarios for conflicting evidence for a user query, including ambiguity, misinformation, and noise; and (ii) MADAM-RAG, a multi-agent approach in which LLM agents debate over the merits of an answer over multiple rounds, allowing an aggregator to collate responses corresponding to disambiguated entities while discarding misinformation and noise, thereby handling diverse sources of conflict jointly. We demonstrate the effectiveness of MADAM-RAG using both closed and open-source models on AmbigDocs -- which requires presenting all valid answers for ambiguous queries -- improving over strong RAG baselines by up to 11.40% and on FaithEval -- which requires suppressing misinformation -- where we improve by up to 15.80% (absolute) with Llama3.3-70B-Instruct. Furthermore, we find that RAMDocs poses a challenge for existing RAG baselines (Llama3.3-70B-Instruct only obtains 32.60 exact match score). While MADAM-RAG begins to address these conflicting factors, our analysis indicates that a substantial gap remains especially when increasing the level of imbalance in supporting evidence and misinformation.",
            "score": 6,
            "issue_id": 3304,
            "pub_date": "2025-04-17",
            "pub_date_card": {
                "ru": "17 апреля",
                "en": "April 17",
                "zh": "4月17日"
            },
            "hash": "13305db862567e7f",
            "authors": [
                "Han Wang",
                "Archiki Prasad",
                "Elias Stengel-Eskin",
                "Mohit Bansal"
            ],
            "affiliations": [
                "University of North Carolina at Chapel Hill"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.13079.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#dataset",
                    "#rag",
                    "#optimization",
                    "#agents",
                    "#hallucinations"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Многоагентный подход для борьбы с неоднозначностью и дезинформацией в RAG-системах",
                    "desc": "Статья представляет новый подход к решению проблем неоднозначности и дезинформации в системах генерации текста с использованием извлечения информации (RAG). Авторы предлагают датасет RAMDocs, моделирующий сложные сценарии с противоречивыми данными, и метод MADAM-RAG, использующий несколько агентов на основе больших языковых моделей для обсуждения ответов. MADAM-RAG показывает улучшение результатов на 11.40% на датасете AmbigDocs и на 15.80% на FaithEval по сравнению с базовыми методами RAG. Однако, несмотря на прогресс, остаются значительные проблемы, особенно при увеличении дисбаланса между поддерживающими и дезинформирующими данными."
                },
                "en": {
                    "title": "Enhancing LLM Accuracy through Debate and Retrieval",
                    "desc": "This paper discusses the challenges faced by large language model (LLM) agents when using retrieval-augmented generation (RAG) to provide accurate responses to user queries. It introduces RAMDocs, a new dataset designed to simulate complex scenarios involving ambiguity, misinformation, and noise in documents. The authors propose MADAM-RAG, a multi-agent system where LLMs debate answers over multiple rounds, allowing for better aggregation of responses while filtering out inaccuracies. The results show that MADAM-RAG significantly improves performance on tasks requiring disambiguation and misinformation suppression compared to traditional RAG methods, although challenges remain in handling conflicting evidence."
                },
                "zh": {
                    "title": "多代理辩论：提升语言模型的准确性与鲁棒性",
                    "desc": "大型语言模型（LLM）代理越来越多地使用检索增强生成（RAG）来提高回答的准确性。然而，这些系统在处理模糊用户查询和来自多个来源的潜在冲突信息时，常常需要抑制来自嘈杂或无关文档的不准确信息。本文提出了RAMDocs数据集，模拟了复杂的用户查询场景，并提出了MADAM-RAG多代理方法，通过多轮辩论来处理答案的优劣，从而有效整合不同来源的响应。实验结果表明，MADAM-RAG在处理模糊查询和抑制错误信息方面显著优于现有的RAG基线。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.12782",
            "title": "Set You Straight: Auto-Steering Denoising Trajectories to Sidestep\n  Unwanted Concepts",
            "url": "https://huggingface.co/papers/2504.12782",
            "abstract": "Ensuring the ethical deployment of text-to-image models requires effective techniques to prevent the generation of harmful or inappropriate content. While concept erasure methods offer a promising solution, existing finetuning-based approaches suffer from notable limitations. Anchor-free methods risk disrupting sampling trajectories, leading to visual artifacts, while anchor-based methods rely on the heuristic selection of anchor concepts. To overcome these shortcomings, we introduce a finetuning framework, dubbed ANT, which Automatically guides deNoising Trajectories to avoid unwanted concepts. ANT is built on a key insight: reversing the condition direction of classifier-free guidance during mid-to-late denoising stages enables precise content modification without sacrificing early-stage structural integrity. This inspires a trajectory-aware objective that preserves the integrity of the early-stage score function field, which steers samples toward the natural image manifold, without relying on heuristic anchor concept selection. For single-concept erasure, we propose an augmentation-enhanced weight saliency map to precisely identify the critical parameters that most significantly contribute to the unwanted concept, enabling more thorough and efficient erasure. For multi-concept erasure, our objective function offers a versatile plug-and-play solution that significantly boosts performance. Extensive experiments demonstrate that ANT achieves state-of-the-art results in both single and multi-concept erasure, delivering high-quality, safe outputs without compromising the generative fidelity. Code is available at https://github.com/lileyang1210/ANT",
            "score": 4,
            "issue_id": 3305,
            "pub_date": "2025-04-17",
            "pub_date_card": {
                "ru": "17 апреля",
                "en": "April 17",
                "zh": "4月17日"
            },
            "hash": "6c74497f6fd8b96b",
            "authors": [
                "Leyang Li",
                "Shilin Lu",
                "Yan Ren",
                "Adams Wai-Kin Kong"
            ],
            "affiliations": [
                "Nanyang Technological University, Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.12782.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#data",
                    "#training",
                    "#optimization",
                    "#ethics"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "ANT: Точное удаление нежелательных концепций из генеративных моделей изображений",
                    "desc": "Статья представляет новый метод ANT для удаления нежелательных концепций из генеративных моделей изображений. ANT использует обращение направления условной генерации на средних и поздних этапах шумоподавления для точной модификации контента. Метод сохраняет целостность ранних этапов генерации, не полагаясь на эвристический выбор якорных концепций. ANT показывает отличные результаты как для удаления одиночных, так и множественных концепций, обеспечивая высокое качество и безопасность сгенерированных изображений."
                },
                "en": {
                    "title": "ANT: Ethical Image Generation Through Smart Concept Erasure",
                    "desc": "This paper presents ANT, a new finetuning framework designed to improve the ethical deployment of text-to-image models by effectively erasing harmful or inappropriate content. ANT addresses the limitations of existing methods by guiding denoising trajectories to avoid unwanted concepts without disrupting the image quality. It introduces a trajectory-aware objective that maintains the integrity of early-stage image features while allowing for precise content modification. The framework also includes an innovative weight saliency map for identifying critical parameters in single-concept erasure and offers a versatile solution for multi-concept erasure, achieving state-of-the-art results in generating safe and high-quality images."
                },
                "zh": {
                    "title": "ANT：高效去除不当内容的文本到图像模型框架",
                    "desc": "本文提出了一种名为ANT的微调框架，用于确保文本到图像模型的伦理部署，特别是防止生成有害或不当内容。ANT通过自动引导去噪轨迹，避免了现有方法中的一些局限性，如锚点方法的启发式选择和无锚点方法导致的视觉伪影。该框架利用了在去噪中后期反转分类器无指导的条件方向的关键见解，从而实现了精确的内容修改，同时保持了早期阶段的结构完整性。通过增强的权重显著性图，ANT能够有效识别并去除单一或多个不当概念，实验结果表明其在去除效果和生成质量上均达到了最先进的水平。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.12563",
            "title": "MetaSynth: Meta-Prompting-Driven Agentic Scaffolds for Diverse Synthetic\n  Data Generation",
            "url": "https://huggingface.co/papers/2504.12563",
            "abstract": "Recent smaller language models such Phi-3.5 and Phi-4 rely on synthetic data generated using larger Language models. Questions remain about leveraging synthetic data for other use cases, such as adapting LLMs to specific domains. A key limitation of synthetic data is low diversity, which negatively impacts its downstream applicability for improving other models. To address this, we propose MetaSynth, a method for generating synthetic data that enhances diversity through meta-prompting, where a language model orchestrates multiple \"expert\" LLM agents to collaboratively generate data. Using only 25 million tokens of synthetic data generated with MetaSynth, we successfully adapt a well-trained LLM (Mistral-7B-v0.3) to two specialized domains-Finance and Biomedicine-without compromising the capabilities of the resulting model in general tasks. In addition, we evaluate the diversity of our synthetic data using seven automated metrics, and find that it approaches the diversity of LLM pre-training corpora.   Continually pre-training Mistral-7B-v0.3 with MetaSynth notably outperforms the base LLM, showing improvements of up to 4.08% in Finance and 13.75% in Biomedicine. The same model shows degraded performance when trained on data generated using a template prompt, even when the template includes prior generations and varying In-Context exemplars of real data. Our findings suggest that a few million tokens of diverse synthetic data without mixing any real data, is sufficient for effective domain adaptation when using MetaSynth.",
            "score": 4,
            "issue_id": 3316,
            "pub_date": "2025-04-17",
            "pub_date_card": {
                "ru": "17 апреля",
                "en": "April 17",
                "zh": "4月17日"
            },
            "hash": "0272e3b4f0ac9209",
            "authors": [
                "Haris Riaz",
                "Sourav Bhabesh",
                "Vinayak Arannil",
                "Miguel Ballesteros",
                "Graham Horwood"
            ],
            "affiliations": [
                "AWS AI Labs",
                "University of Arizona"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.12563.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#transfer_learning",
                    "#multimodal",
                    "#training",
                    "#dataset",
                    "#synthetic"
                ],
                "emoji": "🧬",
                "ru": {
                    "title": "MetaSynth: Разнообразные синтетические данные для эффективной доменной адаптации языковых моделей",
                    "desc": "MetaSynth - это новый метод генерации синтетических данных для адаптации больших языковых моделей к специализированным областям. Он использует мета-промптинг, где языковая модель координирует нескольких 'экспертных' LLM-агентов для совместной генерации разнообразных данных. Используя всего 25 миллионов токенов синтетических данных, созданных с помощью MetaSynth, авторам удалось адаптировать Mistral-7B-v0.3 к финансовой и биомедицинской областям без ущерба для общих способностей модели. Результаты показывают значительное улучшение производительности в специализированных задачах по сравнению с базовой моделью."
                },
                "en": {
                    "title": "Enhancing Domain Adaptation with Diverse Synthetic Data",
                    "desc": "This paper introduces MetaSynth, a novel method for generating synthetic data that improves diversity by using multiple expert language models to collaboratively create data. The authors demonstrate that with just 25 million tokens of synthetic data, they can effectively adapt a well-trained language model, Mistral-7B-v0.3, to specialized domains like Finance and Biomedicine. The results show that this approach not only maintains the model's general capabilities but also significantly enhances its performance in the targeted domains. Additionally, the diversity of the synthetic data generated by MetaSynth is evaluated and found to be comparable to that of traditional LLM pre-training datasets, indicating its potential for broader applications in domain adaptation."
                },
                "zh": {
                    "title": "MetaSynth：提升合成数据多样性，助力领域适应",
                    "desc": "本文提出了一种名为MetaSynth的方法，用于生成多样化的合成数据，以提高大型语言模型（LLM）的适应性。通过元提示技术，MetaSynth协调多个“专家”LLM代理共同生成数据，从而增强合成数据的多样性。研究表明，仅使用2500万个合成数据令牌，MetaSynth成功将Mistral-7B-v0.3模型适应于金融和生物医学两个专业领域，同时保持其在一般任务中的能力。与使用模板提示生成的数据相比，MetaSynth生成的数据在多样性和性能上显著优越，证明了其在领域适应中的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.09228",
            "title": "Learning Occlusion-Robust Vision Transformers for Real-Time UAV Tracking",
            "url": "https://huggingface.co/papers/2504.09228",
            "abstract": "Single-stream architectures using Vision Transformer (ViT) backbones show great potential for real-time UAV tracking recently. However, frequent occlusions from obstacles like buildings and trees expose a major drawback: these models often lack strategies to handle occlusions effectively. New methods are needed to enhance the occlusion resilience of single-stream ViT models in aerial tracking. In this work, we propose to learn Occlusion-Robust Representations (ORR) based on ViTs for UAV tracking by enforcing an invariance of the feature representation of a target with respect to random masking operations modeled by a spatial Cox process. Hopefully, this random masking approximately simulates target occlusions, thereby enabling us to learn ViTs that are robust to target occlusion for UAV tracking. This framework is termed ORTrack. Additionally, to facilitate real-time applications, we propose an Adaptive Feature-Based Knowledge Distillation (AFKD) method to create a more compact tracker, which adaptively mimics the behavior of the teacher model ORTrack according to the task's difficulty. This student model, dubbed ORTrack-D, retains much of ORTrack's performance while offering higher efficiency. Extensive experiments on multiple benchmarks validate the effectiveness of our method, demonstrating its state-of-the-art performance. Codes is available at https://github.com/wuyou3474/ORTrack.",
            "score": 4,
            "issue_id": 3325,
            "pub_date": "2025-04-12",
            "pub_date_card": {
                "ru": "12 апреля",
                "en": "April 12",
                "zh": "4月12日"
            },
            "hash": "f8abea0d3480728f",
            "authors": [
                "You Wu",
                "Xucheng Wang",
                "Xiangyang Yang",
                "Mengyuan Liu",
                "Dan Zeng",
                "Hengzhou Ye",
                "Shuiwang Li"
            ],
            "affiliations": [
                "College of Computer Science and Engineering, Guilin University of Technology, China",
                "School of Artificial Intelligence, Sun Yat-sen University, Zhuhai, China",
                "School of Computer Science, Fudan University, Shanghai, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.09228.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#benchmark",
                    "#training",
                    "#cv"
                ],
                "emoji": "🚁",
                "ru": {
                    "title": "Устойчивое отслеживание БПЛА с помощью ViT и дистилляции знаний",
                    "desc": "Эта статья представляет новый подход к отслеживанию БПЛА с использованием архитектуры Vision Transformer (ViT). Авторы предлагают метод обучения устойчивым к окклюзии представлениям (ORR) на основе случайного маскирования, моделируемого пространственным процессом Кокса. Они также разрабатывают адаптивный метод дистилляции знаний (AFKD) для создания более компактной модели трекера. Эксперименты показывают, что предложенный подход ORTrack и его облегченная версия ORTrack-D достигают современного уровня производительности в задаче отслеживания БПЛА."
                },
                "en": {
                    "title": "Enhancing UAV Tracking Resilience with Occlusion-Robust ViTs",
                    "desc": "This paper addresses the challenge of occlusions in UAV tracking using single-stream Vision Transformer (ViT) architectures. The authors introduce a method called Occlusion-Robust Representations (ORR) that enhances the resilience of ViTs to occlusions by simulating them through random masking operations. They also propose an Adaptive Feature-Based Knowledge Distillation (AFKD) technique to create a more efficient student model, ORTrack-D, which maintains high performance while being compact. Extensive experiments demonstrate that their approach achieves state-of-the-art results in real-time UAV tracking tasks."
                },
                "zh": {
                    "title": "提升无人机跟踪的遮挡鲁棒性",
                    "desc": "本文提出了一种基于视觉变换器（ViT）的单流架构，用于无人机（UAV）跟踪，旨在提高对遮挡的鲁棒性。我们引入了遮挡鲁棒表示（ORR），通过随机遮挡操作来模拟目标遮挡，从而增强模型的鲁棒性。为了实现实时应用，我们还提出了一种自适应特征知识蒸馏（AFKD）方法，创建了一个更紧凑的跟踪器，称为ORTrack-D。实验结果表明，我们的方法在多个基准测试中表现出色，具有先进的性能。"
                }
            }
        }
    ],
    "link_prev": "2025-04-17.html",
    "link_next": "2025-04-21.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "17.04",
        "en": "04/17",
        "zh": "4月17日"
    },
    "short_date_next": {
        "ru": "21.04",
        "en": "04/21",
        "zh": "4月21日"
    },
    "categories": {
        "#dataset": 14,
        "#data": 7,
        "#benchmark": 7,
        "#agents": 3,
        "#cv": 9,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 1,
        "#plp": 0,
        "#inference": 3,
        "#3d": 1,
        "#audio": 1,
        "#video": 3,
        "#multimodal": 8,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 10,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 2,
        "#reasoning": 6,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 11,
        "#survey": 0,
        "#diffusion": 3,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 3,
        "#long_context": 2,
        "#synthetic": 6,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 6,
        "#small_models": 2,
        "#science": 1,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章讨论了预训练数据集的收集和优化问题。现有数据集通常来自网络内容，缺乏明确的领域划分。文章提出了一种名为CLIMB的自动化框架，通过嵌入和聚类大规模数据集，迭代搜索最佳数据混合。使用这种方法，1B模型在400B tokens上的训练效果超过了Llama-3.2-1B。文章还介绍了ClimbLab和ClimbMix两个数据集，并分析了最佳数据混合的特征。",
        "title": "CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for\n  Language Model Pre-training",
        "pinyin": "这篇文章讨论了预训练数据集的收集和优化问题。\nZhè piān wén zhāng tǎo lùn le yù xùn liàn shù jù jí de shōu jí hé yōu huà wèn tí.\n\n现有数据集通常来自网络内容，缺乏明确的领域划分。\nXiàn yǒu shù jù jí tōng cháng lái zì wǎng luò nèi róng, quē fá míng què de lǐng yù huà fēn.\n\n文章提出了一种名为CLIMB的自动化框架，通过嵌入和聚类大规模数据集，迭代搜索最佳数据混合。\nWén zhāng tí chū le yī zhǒng míng wéi CLIMB de zì dòng huà kuàng jià, tōng guò qiàn rù hé jù lèi dà guī mó shù jù jí, dié dài sōu suǒ zuì jiā shù jù hùn hé.\n\n使用这种方法，1B模型在400B tokens上的训练效果超过了Llama-3.2-1B。\nShǐ yòng zhè zhǒng fāng fǎ, 1B mó xíng zài 400B tokens shàng de xùn liàn xiào guǒ chāo guò le Llama-3.2-1B.\n\n文章还介绍了ClimbLab和ClimbMix两个数据集，并分析了最佳数据混合的特征。\nWén zhāng hái jiè shào le ClimbLab hé ClimbMix liǎng gè shù jù jí, bìng fēn xī le zuì jiā shù jù hùn hé de tè zhēng.",
        "vocab": "[\n    {\"word\": \"讨论\", \"pinyin\": \"tǎo lùn\", \"trans\": \"discuss\"},\n    {\"word\": \"预训练\", \"pinyin\": \"yù xùn liàn\", \"trans\": \"pre-train\"},\n    {\"word\": \"数据集\", \"pinyin\": \"shù jù jí\", \"trans\": \"dataset\"},\n    {\"word\": \"收集\", \"pinyin\": \"shōu jí\", \"trans\": \"collect\"},\n    {\"word\": \"优化\", \"pinyin\": \"yōu huà\", \"trans\": \"optimize\"},\n    {\"word\": \"现有\", \"pinyin\": \"xiàn yǒu\", \"trans\": \"existing\"},\n    {\"word\": \"来自\", \"pinyin\": \"lái zì\", \"trans\": \"come from\"},\n    {\"word\": \"网络\", \"pinyin\": \"wǎng luò\", \"trans\": \"network\"},\n    {\"word\": \"内容\", \"pinyin\": \"nèi róng\", \"trans\": \"content\"},\n    {\"word\": \"缺乏\", \"pinyin\": \"quē fá\", \"trans\": \"lack\"},\n    {\"word\": \"明确\", \"pinyin\": \"míng què\", \"trans\": \"clear\"},\n    {\"word\": \"领域\", \"pinyin\": \"lǐng yù\", \"trans\": \"domain\"},\n    {\"word\": \"划分\", \"pinyin\": \"huà fēn\", \"trans\": \"divide\"},\n    {\"word\": \"提出\", \"pinyin\": \"tí chū\", \"trans\": \"propose\"},\n    {\"word\": \"名为\", \"pinyin\": \"míng wéi\", \"trans\": \"named\"},\n    {\"word\": \"自动化\", \"pinyin\": \"zì dòng huà\", \"trans\": \"automate\"},\n    {\"word\": \"框架\", \"pinyin\": \"kuàng jià\", \"trans\": \"framework\"},\n    {\"word\": \"嵌入\", \"pinyin\": \"qiàn rù\", \"trans\": \"embed\"},\n    {\"word\": \"聚类\", \"pinyin\": \"jù lèi\", \"trans\": \"cluster\"},\n    {\"word\": \"大规模\", \"pinyin\": \"dà guī mó\", \"trans\": \"large-scale\"},\n    {\"word\": \"迭代\", \"pinyin\": \"dié dài\", \"trans\": \"iterate\"},\n    {\"word\": \"搜索\", \"pinyin\": \"sōu suǒ\", \"trans\": \"search\"},\n    {\"word\": \"最佳\", \"pinyin\": \"zuì jiā\", \"trans\": \"optimal\"},\n    {\"word\": \"混合\", \"pinyin\": \"hùn hé\", \"trans\": \"mix\"},\n    {\"word\": \"使用\", \"pinyin\": \"shǐ yòng\", \"trans\": \"use\"},\n    {\"word\": \"效果\", \"pinyin\": \"xiào guǒ\", \"trans\": \"effect\"},\n    {\"word\": \"超过\", \"pinyin\": \"chāo guò\", \"trans\": \"exceed\"},\n    {\"word\": \"介绍\", \"pinyin\": \"jiè shào\", \"trans\": \"introduce\"},\n    {\"word\": \"分析\", \"pinyin\": \"fēn xī\", \"trans\": \"analyze\"},\n    {\"word\": \"特征\", \"pinyin\": \"tè zhēng\", \"trans\": \"feature\"}\n]",
        "trans": "This article discusses the collection and optimization of pretraining datasets. Existing datasets typically come from web content and lack clear domain distinctions. The article proposes an automated framework called CLIMB, which embeds and clusters large-scale datasets to iteratively search for the optimal data mix. Using this method, the 1B model's training performance on 400B tokens surpassed that of Llama-3.2-1B. The article also introduces the ClimbLab and ClimbMix datasets and analyzes the characteristics of the optimal data mix.",
        "update_ts": "2025-04-20 12:41"
    }
}