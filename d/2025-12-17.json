{
    "date": {
        "ru": "17 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
        "en": "December 17",
        "zh": "12æœˆ17æ—¥"
    },
    "time_utc": "2025-12-17 04:35",
    "weekday": 2,
    "issue_id": 96,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2512.14691",
            "title": "MMGR: Multi-Modal Generative Reasoning",
            "url": "https://huggingface.co/papers/2512.14691",
            "abstract": "MMGR evaluates video and image models across reasoning abilities in multiple domains, revealing performance gaps and highlighting limitations in perceptual data and causal correctness.  \t\t\t\t\tAI-generated summary \t\t\t\t Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.",
            "score": 28,
            "issue_id": 95,
            "pub_date": "2025-12-16",
            "pub_date_card": {
                "ru": "16 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 16",
                "zh": "12æœˆ16æ—¥"
            },
            "hash": "8daed328c3a9296c",
            "authors": [
                "Zefan Cai",
                "Haoyi Qiu",
                "Tianyi Ma",
                "Haozhe Zhao",
                "Gengze Zhou",
                "Kung-Hsiang Huang",
                "Parisa Kordjamshidi",
                "Minjia Zhang",
                "Xiao Wen",
                "Jiuxiang Gu",
                "Nanyun Peng",
                "Junjie Hu"
            ],
            "affiliations": [
                "Adobe Research",
                "Michigan State University",
                "Microsoft",
                "Salesforce AI Research",
                "University of Adelaide",
                "University of California, Los Angeles",
                "University of Illinois Urbana-Champaign",
                "University of Wisconsin-Madison"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.14691.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#survey",
                    "#multimodal",
                    "#video",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ›Ğ¾Ğ³Ğ¸ĞºĞ° Ğ²Ğ°Ğ¶Ğ½ĞµĞµ ĞºÑ€Ğ°ÑĞ¾Ñ‚Ñ‹: Ğ¾Ñ†ĞµĞ½ĞºĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ MMGR â€” ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹æ¡†æ¶Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ¿ÑÑ‚Ğ¸ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑÑ…: Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ, Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ, Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ (3D Ğ¸ 2D) Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸: Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ, Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰Ñ‘Ğ½Ğ½ÑƒÑ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ¼Ñ‹ÑĞ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ² ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… (Ğ¼ĞµĞ½ĞµĞµ 10% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ARC-AGI). ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ğ°Ñ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚ Ğ¿ĞµÑ€Ñ†ĞµĞ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ¸Ñ Ğ½Ğ°Ğ´ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ."
                },
                "en": {
                    "title": "Bridging the Gap: Evaluating Reasoning in Video and Image Models with MMGR",
                    "desc": "The paper introduces MMGR, a new evaluation framework designed to assess the reasoning abilities of video and image models across various domains. It identifies significant performance gaps in current models, particularly in tasks requiring abstract reasoning and long-term spatial planning. The framework emphasizes the importance of causal correctness and global consistency, rather than just perceptual quality. By benchmarking leading models, the study reveals their limitations and suggests a need for more reasoning-aware generative models."
                },
                "zh": {
                    "title": "æ¨ç†èƒ½åŠ›è¯„ä¼°ï¼šæ­ç¤ºç”Ÿæˆæ¨¡å‹çš„å±€é™æ€§",
                    "desc": "MMGRï¼ˆå¤šæ¨¡æ€ç”Ÿæˆæ¨ç†è¯„ä¼°ä¸åŸºå‡†ï¼‰æ˜¯ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨æµ‹è¯•è§†é¢‘å’Œå›¾åƒæ¨¡å‹åœ¨å¤šä¸ªé¢†åŸŸçš„æ¨ç†èƒ½åŠ›ã€‚å®ƒåŸºäºäº”ç§æ¨ç†èƒ½åŠ›ï¼šç‰©ç†ã€é€»è¾‘ã€ä¸‰ç»´ç©ºé—´ã€äºŒç»´ç©ºé—´å’Œæ—¶é—´ã€‚é€šè¿‡å¯¹ç°æœ‰è§†é¢‘å’Œå›¾åƒç”Ÿæˆæ¨¡å‹çš„è¯„ä¼°ï¼ŒMMGRæ­ç¤ºäº†å®ƒä»¬åœ¨æŠ½è±¡æ¨ç†å’Œé•¿æ—¶é—´ç©ºé—´è§„åˆ’æ–¹é¢çš„æ€§èƒ½å·®è·ã€‚è¯¥æ¡†æ¶å¼ºè°ƒäº†å½“å‰æ¨¡å‹çš„å±€é™æ€§ï¼ŒåŒ…æ‹¬å¯¹æ„ŸçŸ¥æ•°æ®çš„è¿‡åº¦ä¾èµ–å’Œå› æœæ­£ç¡®æ€§çš„ä¸è¶³ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.13660",
            "title": "RoboTracer: Mastering Spatial Trace with Reasoning in Vision-Language Models for Robotics",
            "url": "https://huggingface.co/papers/2512.13660",
            "abstract": "RoboTracer, a 3D-aware visual language model, enhances spatial tracing by combining supervised and reinforcement fine-tuning with a universal spatial encoder and regression-supervised decoder, achieving state-of-the-art performance on TraceSpatial-Bench.  \t\t\t\t\tAI-generated summary \t\t\t\t Spatial tracing, as a fundamental embodied interaction ability for robots, is inherently challenging as it requires multi-step metric-grounded reasoning compounded with complex spatial referring and real-world metric measurement. However, existing methods struggle with this compositional task. To this end, we propose RoboTracer, a 3D-aware VLM that first achieves both 3D spatial referring and measuring via a universal spatial encoder and a regression-supervised decoder to enhance scale awareness during supervised fine-tuning (SFT). Moreover, RoboTracer advances multi-step metric-grounded reasoning via reinforcement fine-tuning (RFT) with metric-sensitive process rewards, supervising key intermediate perceptual cues to accurately generate spatial traces. To support SFT and RFT training, we introduce TraceSpatial, a large-scale dataset of 30M QA pairs, spanning outdoor/indoor/tabletop scenes and supporting complex reasoning processes (up to 9 steps). We further present TraceSpatial-Bench, a challenging benchmark filling the gap to evaluate spatial tracing. Experimental results show that RoboTracer surpasses baselines in spatial understanding, measuring, and referring, with an average success rate of 79.1%, and also achieves SOTA performance on TraceSpatial-Bench by a large margin, exceeding Gemini-2.5-Pro by 36% accuracy. Notably, RoboTracer can be integrated with various control policies to execute long-horizon, dynamic tasks across diverse robots (UR5, G1 humanoid) in cluttered real-world scenes.",
            "score": 21,
            "issue_id": 96,
            "pub_date": "2025-12-15",
            "pub_date_card": {
                "ru": "15 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 15",
                "zh": "12æœˆ15æ—¥"
            },
            "hash": "5ffd03cb4f6287d8",
            "authors": [
                "Enshen Zhou",
                "Cheng Chi",
                "Yibo Li",
                "Jingkun An",
                "Jiayuan Zhang",
                "Shanyu Rong",
                "Yi Han",
                "Yuheng Ji",
                "Mengzhen Liu",
                "Pengwei Wang",
                "Zhongyuan Wang",
                "Lu Sheng",
                "Shanghang Zhang"
            ],
            "affiliations": [
                "Beijing Academy of Artificial Intelligence",
                "CASIA",
                "School of Software, Beihang University",
                "State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.13660.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#rl",
                    "#3d",
                    "#robotics",
                    "#multimodal",
                    "#benchmark",
                    "#training"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ¢Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²",
                    "desc": "RoboTracer â€” ÑÑ‚Ğ¾ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ°Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°ÑÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¸ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ğ¸ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑÑÑ‹Ğ»Ğ¾Ğº Ğ² Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ TraceSpatial Ñ 30 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ğ¼Ğ¸ QA-Ğ¿Ğ°Ñ€, Ğ¿Ğ¾ĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ñ‹ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. RoboTracer Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ TraceSpatial-Bench Ğ¸ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ."
                },
                "en": {
                    "title": "RoboTracer: Mastering Spatial Tracing for Robots",
                    "desc": "RoboTracer is a 3D-aware visual language model designed to improve spatial tracing, which is crucial for robots to interact with their environment. It combines supervised fine-tuning and reinforcement fine-tuning to enhance its ability to understand and measure spatial relationships. The model uses a universal spatial encoder and a regression-supervised decoder to achieve accurate 3D spatial referring and measuring. With the introduction of the TraceSpatial dataset and the TraceSpatial-Bench benchmark, RoboTracer demonstrates superior performance in spatial reasoning tasks, achieving a success rate of 79.1% and significantly outperforming previous models."
                },
                "zh": {
                    "title": "RoboTracerï¼šæå‡æœºå™¨äººç©ºé—´è¿½è¸ªèƒ½åŠ›çš„åˆ›æ–°æ¨¡å‹",
                    "desc": "RoboTraceræ˜¯ä¸€ç§3Dæ„ŸçŸ¥çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨æå‡ç©ºé—´è¿½è¸ªèƒ½åŠ›ã€‚å®ƒç»“åˆäº†ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å¾®è°ƒï¼Œä½¿ç”¨é€šç”¨ç©ºé—´ç¼–ç å™¨å’Œå›å½’ç›‘ç£è§£ç å™¨ï¼Œèƒ½å¤Ÿåœ¨TraceSpatial-Benchä¸Šå®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¯¥æ¨¡å‹é€šè¿‡å¼•å…¥TraceSpatialæ•°æ®é›†ï¼Œæ”¯æŒå¤æ‚çš„æ¨ç†è¿‡ç¨‹ï¼Œå¹¶åœ¨ç©ºé—´ç†è§£ã€æµ‹é‡å’Œå¼•ç”¨æ–¹é¢è¶…è¶Šäº†ç°æœ‰åŸºçº¿ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRoboTraceråœ¨å¤šæ­¥åº¦é‡æ¨ç†ä¸­è¡¨ç°å‡ºè‰²ï¼ŒæˆåŠŸç‡è¾¾åˆ°79.1%ï¼Œå¹¶åœ¨å®é™…åº”ç”¨ä¸­èƒ½å¤Ÿä¸å¤šç§æ§åˆ¶ç­–ç•¥é›†æˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.12675",
            "title": "Scone: Bridging Composition and Distinction in Subject-Driven Image Generation via Unified Understanding-Generation Modeling",
            "url": "https://huggingface.co/papers/2512.12675",
            "abstract": "Scone integrates composition and distinction in image generation by using a two-stage training scheme with semantic alignment and attention-based masking, outperforming existing models on benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Subject-driven image generation has advanced from single- to multi-subject composition, while neglecting distinction, the ability to identify and generate the correct subject when inputs contain multiple candidates. This limitation restricts effectiveness in complex, realistic visual settings. We propose Scone, a unified understanding-generation method that integrates composition and distinction. Scone enables the understanding expert to act as a semantic bridge, conveying semantic information and guiding the generation expert to preserve subject identity while minimizing interference. A two-stage training scheme first learns composition, then enhances distinction through semantic alignment and attention-based masking. We also introduce SconeEval, a benchmark for evaluating both composition and distinction across diverse scenarios. Experiments demonstrate that Scone outperforms existing open-source models in composition and distinction tasks on two benchmarks. Our model, benchmark, and training data are available at: https://github.com/Ryann-Ran/Scone.",
            "score": 20,
            "issue_id": 95,
            "pub_date": "2025-12-14",
            "pub_date_card": {
                "ru": "14 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 14",
                "zh": "12æœˆ14æ—¥"
            },
            "hash": "2e708e488331ba96",
            "authors": [
                "Yuran Wang",
                "Bohan Zeng",
                "Chengzhuo Tong",
                "Wenxuan Liu",
                "Yang Shi",
                "Xiaochen Ma",
                "Hao Liang",
                "Yuanxing Zhang",
                "Wentao Zhang"
            ],
            "affiliations": [
                "Kling Team, Kuaishou Technology",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.12675.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#cv",
                    "#benchmark",
                    "#multimodal",
                    "#open_source"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "ĞšĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼: Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Scone â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¹ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¸Ğ·ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ, Ğ·Ğ°Ñ‚ĞµĞ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ÑÑ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ñ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒĞµÑ‚ ĞºĞ°Ğº ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¼Ğ¾ÑÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ½Ğ¾Ğµ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ SconeEval â€” Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Scone: Bridging Composition and Distinction in Image Generation",
                    "desc": "The paper introduces Scone, a novel approach to image generation that effectively combines composition and distinction. It addresses the challenge of generating images with multiple subjects by ensuring that each subject is accurately represented and distinguished. Scone employs a two-stage training process that first focuses on learning how to compose images and then enhances the model's ability to distinguish between subjects using semantic alignment and attention-based masking techniques. The results show that Scone outperforms existing models in both composition and distinction tasks, and a new benchmark, SconeEval, is introduced for evaluating these capabilities."
                },
                "zh": {
                    "title": "Sconeï¼šå›¾åƒç”Ÿæˆä¸­çš„ç»„åˆä¸åŒºåˆ†çš„å®Œç¾ç»“åˆ",
                    "desc": "Sconeæ˜¯ä¸€ç§å›¾åƒç”Ÿæˆæ–¹æ³•ï¼Œå®ƒç»“åˆäº†ç»„åˆå’ŒåŒºåˆ†çš„èƒ½åŠ›ã€‚é€šè¿‡ä¸¤é˜¶æ®µçš„è®­ç»ƒæ–¹æ¡ˆï¼ŒSconeé¦–å…ˆå­¦ä¹ ç»„åˆï¼Œç„¶åé€šè¿‡è¯­ä¹‰å¯¹é½å’ŒåŸºäºæ³¨æ„åŠ›çš„æ©è”½æ¥å¢å¼ºåŒºåˆ†èƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•ä½¿å¾—ç”Ÿæˆæ¨¡å‹èƒ½å¤Ÿåœ¨å¤æ‚çš„è§†è§‰åœºæ™¯ä¸­æ›´å¥½åœ°è¯†åˆ«å’Œç”Ÿæˆæ­£ç¡®çš„ä¸»é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSconeåœ¨ç»„åˆå’ŒåŒºåˆ†ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰çš„å¼€æºæ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.14614",
            "title": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling",
            "url": "https://huggingface.co/papers/2512.14614",
            "abstract": "WorldPlay is a streaming video diffusion model that achieves real-time, interactive world modeling with long-term geometric consistency by using a Dual Action Representation, Reconstituted Context Memory, and Context Forcing.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper presents WorldPlay, a streaming video diffusion model that enables real-time, interactive world modeling with long-term geometric consistency, resolving the trade-off between speed and memory that limits current methods. WorldPlay draws power from three key innovations. 1) We use a Dual Action Representation to enable robust action control in response to the user's keyboard and mouse inputs. 2) To enforce long-term consistency, our Reconstituted Context Memory dynamically rebuilds context from past frames and uses temporal reframing to keep geometrically important but long-past frames accessible, effectively alleviating memory attenuation. 3) We also propose Context Forcing, a novel distillation method designed for memory-aware model. Aligning memory context between the teacher and student preserves the student's capacity to use long-range information, enabling real-time speeds while preventing error drift. Taken together, WorldPlay generates long-horizon streaming 720p video at 24 FPS with superior consistency, comparing favorably with existing techniques and showing strong generalization across diverse scenes. Project page and online demo can be found: https://3d-models.hunyuan.tencent.com/world/ and https://3d.hunyuan.tencent.com/sceneTo3D.",
            "score": 13,
            "issue_id": 95,
            "pub_date": "2025-12-16",
            "pub_date_card": {
                "ru": "16 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 16",
                "zh": "12æœˆ16æ—¥"
            },
            "hash": "1f6f5ff8f0e4af75",
            "authors": [
                "Wenqiang Sun",
                "Haiyu Zhang",
                "Haoyuan Wang",
                "Junta Wu",
                "Zehan Wang",
                "Zhenwei Wang",
                "Yunhong Wang",
                "Jun Zhang",
                "Tengfei Wang",
                "Chunchao Guo"
            ],
            "affiliations": [
                "Beihang University",
                "Hong Kong University of Science and Technology",
                "Tencent Hunyuan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.14614.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#3d",
                    "#diffusion",
                    "#architecture",
                    "#video",
                    "#long_context"
                ],
                "emoji": "ğŸ®",
                "ru": {
                    "title": "Ğ˜Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¼Ğ¸Ñ€Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ",
                    "desc": "WorldPlay â€” ÑÑ‚Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€Ñ‘Ñ…ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´: Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¹ Ğ²Ğ²Ğ¾Ğ´, Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½ÑƒÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ° Ğº Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğ¼ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼ Ğ¸Ğ· Ğ¿Ñ€Ğ¾ÑˆĞ»Ğ¾Ğ³Ğ¾, Ğ¸ Context Forcing â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ¾Ğ»Ğ³Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒÑ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¸ Ğ¾Ğ±ÑŠÑ‘Ğ¼Ğ¾Ğ¼ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼ 720p Ğ¿Ñ€Ğ¸ 24 FPS Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ… Ğ¸ ÑÑ†ĞµĞ½ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ñ… ÑÑ€ĞµĞ´."
                },
                "en": {
                    "title": "Real-Time Interactive World Modeling with Long-Term Consistency",
                    "desc": "WorldPlay is a novel streaming video diffusion model that allows for real-time interactive world modeling while maintaining long-term geometric consistency. It introduces a Dual Action Representation to effectively manage user inputs, ensuring responsive action control. The model employs a Reconstituted Context Memory that dynamically reconstructs context from previous frames, which helps retain important geometric information over time. Additionally, Context Forcing is utilized to align memory context between teacher and student models, enabling efficient use of long-range information and achieving high-quality video output at 24 frames per second."
                },
                "zh": {
                    "title": "å®æ—¶äº’åŠ¨ä¸–ç•Œå»ºæ¨¡çš„åˆ›æ–°è§£å†³æ–¹æ¡ˆ",
                    "desc": "WorldPlayæ˜¯ä¸€ç§æµåª’ä½“è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿå®ç°å®æ—¶ã€äº’åŠ¨çš„ä¸–ç•Œå»ºæ¨¡ï¼Œå¹¶ä¿æŒé•¿æœŸçš„å‡ ä½•ä¸€è‡´æ€§ã€‚è¯¥æ¨¡å‹é€šè¿‡ä¸‰é¡¹å…³é”®åˆ›æ–°æ¥è§£å†³é€Ÿåº¦å’Œå†…å­˜ä¹‹é—´çš„æƒè¡¡ã€‚é¦–å…ˆï¼ŒåŒé‡åŠ¨ä½œè¡¨ç¤ºä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ ¹æ®ç”¨æˆ·çš„é”®ç›˜å’Œé¼ æ ‡è¾“å…¥è¿›è¡Œç¨³å¥çš„åŠ¨ä½œæ§åˆ¶ã€‚å…¶æ¬¡ï¼Œé‡æ„ä¸Šä¸‹æ–‡è®°å¿†åŠ¨æ€é‡å»ºè¿‡å»å¸§çš„ä¸Šä¸‹æ–‡ï¼Œå¹¶é€šè¿‡æ—¶é—´é‡æ„ä¿æŒå‡ ä½•é‡è¦çš„æ—§å¸§å¯è®¿é—®ï¼Œä»è€Œå‡è½»å†…å­˜è¡°å‡çš„é—®é¢˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.14699",
            "title": "MemFlow: Flowing Adaptive Memory for Consistent and Efficient Long Video Narratives",
            "url": "https://huggingface.co/papers/2512.14699",
            "abstract": "MemFlow dynamically updates a memory bank by retrieving relevant historical frames for each video chunk, ensuring narrative coherence and generation efficiency with minimal computational overhead.  \t\t\t\t\tAI-generated summary \t\t\t\t The core challenge for streaming video generation is maintaining the content consistency in long context, which poses high requirement for the memory design. Most existing solutions maintain the memory by compressing historical frames with predefined strategies. However, different to-generate video chunks should refer to different historical cues, which is hard to satisfy with fixed strategies. In this work, we propose MemFlow to address this problem. Specifically, before generating the coming chunk, we dynamically update the memory bank by retrieving the most relevant historical frames with the text prompt of this chunk. This design enables narrative coherence even if new event happens or scenario switches in future frames. In addition, during generation, we only activate the most relevant tokens in the memory bank for each query in the attention layers, which effectively guarantees the generation efficiency. In this way, MemFlow achieves outstanding long-context consistency with negligible computation burden (7.9% speed reduction compared with the memory-free baseline) and keeps the compatibility with any streaming video generation model with KV cache.",
            "score": 10,
            "issue_id": 95,
            "pub_date": "2025-12-16",
            "pub_date_card": {
                "ru": "16 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 16",
                "zh": "12æœˆ16æ—¥"
            },
            "hash": "540fd982b661f559",
            "authors": [
                "Sihui Ji",
                "Xi Chen",
                "Shuai Yang",
                "Xin Tao",
                "Pengfei Wan",
                "Hengshuang Zhao"
            ],
            "affiliations": [
                "HKU",
                "HKUST(GZ)",
                "Kling Team, Kuaishou Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.14699.jpg",
            "data": {
                "categories": [
                    "#long_context"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ ÑĞ²ÑĞ·Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "MemFlow â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ, Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ĞºĞ°Ğ´Ñ€Ñ‹ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°. Ğ’Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑĞ°Ğ¼Ñ‹Ğµ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¸Ğ· Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼ Ñ Ğ»ÑĞ±Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ KV-ĞºÑÑˆĞµĞ¼ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ½Ğ°Ñ€Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑĞ²ÑĞ·Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ ÑĞ¼ĞµĞ½Ğµ ÑÑ†ĞµĞ½."
                },
                "en": {
                    "title": "Dynamic Memory for Coherent Video Generation",
                    "desc": "MemFlow is a novel approach for streaming video generation that enhances narrative coherence by dynamically updating a memory bank. Instead of using fixed strategies to compress historical frames, MemFlow retrieves relevant frames based on the specific text prompt of the upcoming video chunk. This allows the model to maintain content consistency even when new events or scenarios arise. Additionally, by activating only the most pertinent tokens during the generation process, MemFlow achieves efficient computation with minimal speed reduction compared to traditional methods."
                },
                "zh": {
                    "title": "åŠ¨æ€è®°å¿†æ›´æ–°ï¼Œç¡®ä¿è§†é¢‘ç”Ÿæˆä¸€è‡´æ€§",
                    "desc": "MemFlowæ˜¯ä¸€ç§åŠ¨æ€æ›´æ–°è®°å¿†åº“çš„æ–¹æ³•ï¼Œé€šè¿‡æ£€ç´¢ä¸æ¯ä¸ªè§†é¢‘ç‰‡æ®µç›¸å…³çš„å†å²å¸§ï¼Œç¡®ä¿å™äº‹çš„ä¸€è‡´æ€§å’Œç”Ÿæˆçš„é«˜æ•ˆæ€§ã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸ä½¿ç”¨é¢„å®šä¹‰ç­–ç•¥å‹ç¼©å†å²å¸§ï¼Œéš¾ä»¥æ»¡è¶³ä¸åŒè§†é¢‘ç‰‡æ®µå¯¹å†å²çº¿ç´¢çš„ä¸åŒéœ€æ±‚ã€‚MemFlowåœ¨ç”Ÿæˆæ–°ç‰‡æ®µä¹‹å‰ï¼ŒåŠ¨æ€æ›´æ–°è®°å¿†åº“ï¼Œæå–ä¸å½“å‰æ–‡æœ¬æç¤ºæœ€ç›¸å…³çš„å†å²å¸§ã€‚è¿™æ ·ï¼Œå³ä½¿åœ¨æœªæ¥å¸§ä¸­å‘ç”Ÿæ–°äº‹ä»¶æˆ–åœºæ™¯åˆ‡æ¢ï¼ŒMemFlowä¹Ÿèƒ½ä¿æŒå™äº‹çš„è¿è´¯æ€§ï¼ŒåŒæ—¶åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ä»…æ¿€æ´»æœ€ç›¸å…³çš„è®°å¿†ä»¤ç‰Œï¼Œä»è€Œæœ‰æ•ˆæé«˜ç”Ÿæˆæ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.14503",
            "title": "RecGPT-V2 Technical Report",
            "url": "https://huggingface.co/papers/2512.14503",
            "abstract": "RecGPT-V2 enhances recommender systems by integrating a Hierarchical Multi-Agent System, Hybrid Representation Inference, Meta-Prompting, constrained reinforcement learning, and an Agent-as-a-Judge framework to improve efficiency, explanation diversity, generalization, and human preference alignment.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have demonstrated remarkable potential in transforming recommender systems from implicit behavioral pattern matching to explicit intent reasoning. While RecGPT-V1 successfully pioneered this paradigm by integrating LLM-based reasoning into user interest mining and item tag prediction, it suffers from four fundamental limitations: (1) computational inefficiency and cognitive redundancy across multiple reasoning routes; (2) insufficient explanation diversity in fixed-template generation; (3) limited generalization under supervised learning paradigms; and (4) simplistic outcome-focused evaluation that fails to match human standards.   To address these challenges, we present RecGPT-V2 with four key innovations. First, a Hierarchical Multi-Agent System restructures intent reasoning through coordinated collaboration, eliminating cognitive duplication while enabling diverse intent coverage. Combined with Hybrid Representation Inference that compresses user-behavior contexts, our framework reduces GPU consumption by 60% and improves exclusive recall from 9.39% to 10.99%. Second, a Meta-Prompting framework dynamically generates contextually adaptive prompts, improving explanation diversity by +7.3%. Third, constrained reinforcement learning mitigates multi-reward conflicts, achieving +24.1% improvement in tag prediction and +13.0% in explanation acceptance. Fourth, an Agent-as-a-Judge framework decomposes assessment into multi-step reasoning, improving human preference alignment. Online A/B tests on Taobao demonstrate significant improvements: +2.98% CTR, +3.71% IPV, +2.19% TV, and +11.46% NER. RecGPT-V2 establishes both the technical feasibility and commercial viability of deploying LLM-powered intent reasoning at scale, bridging the gap between cognitive exploration and industrial utility.",
            "score": 10,
            "issue_id": 95,
            "pub_date": "2025-12-16",
            "pub_date_card": {
                "ru": "16 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 16",
                "zh": "12æœˆ16æ—¥"
            },
            "hash": "6c1b9760c14723cf",
            "authors": [
                "Chao Yi",
                "Dian Chen",
                "Gaoyang Guo",
                "Jiakai Tang",
                "Jian Wu",
                "Jing Yu",
                "Mao Zhang",
                "Wen Chen",
                "Wenjun Yang",
                "Yujie Luo",
                "Yuning Jiang",
                "Zhujin Gao",
                "Bo Zheng",
                "Binbin Cao",
                "Changfa Wu",
                "Dixuan Wang",
                "Han Wu",
                "Haoyi Hu",
                "Kewei Zhu",
                "Lang Tian",
                "Lin Yang",
                "Qiqi Huang",
                "Siqi Yang",
                "Wenbo Su",
                "Xiaoxiao He",
                "Xin Tong",
                "Xu Chen",
                "Xunke Xi",
                "Xiaowei Huang",
                "Yaxuan Wu",
                "Yeqiu Yang",
                "Yi Hu",
                "Yujin Yuan",
                "Yuliang Yan",
                "Zile Zhou"
            ],
            "affiliations": [
                "Taobao"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.14503.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#optimization",
                    "#alignment",
                    "#agents",
                    "#rlhf",
                    "#reasoning"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞœĞ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ LLM Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹ Ñ ÑĞ²Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ğ± Ğ¸Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°Ñ…",
                    "desc": "RecGPT-V2 ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¾Ğ± Ğ¸Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ½Ğ° 60%. Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ°Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¼ĞµÑ‚Ğ°-Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğº ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ€ĞµÑˆĞ°ĞµÑ‚ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ñ‹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğ¼Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ÑĞ¼Ğ¸, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞ³Ğ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ğµ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğ¹. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Agent-as-a-Judge Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ»ÑƒÑ‡ÑˆĞµĞµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ² Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Revolutionizing Recommendations with Intelligent Intent Reasoning",
                    "desc": "RecGPT-V2 improves recommender systems by using a Hierarchical Multi-Agent System that enhances intent reasoning while reducing computational redundancy. It introduces Hybrid Representation Inference to optimize user behavior context, leading to significant reductions in GPU usage and better recall rates. The Meta-Prompting framework increases the diversity of explanations provided to users, while constrained reinforcement learning resolves conflicts in reward systems, enhancing tag prediction and user acceptance of explanations. Finally, the Agent-as-a-Judge framework refines evaluation processes to align more closely with human preferences, demonstrating substantial performance gains in real-world applications."
                },
                "zh": {
                    "title": "RecGPT-V2ï¼šæ™ºèƒ½æ¨èç³»ç»Ÿçš„æ–°çªç ´",
                    "desc": "RecGPT-V2é€šè¿‡æ•´åˆå±‚æ¬¡å¤šæ™ºèƒ½ä½“ç³»ç»Ÿã€æ··åˆè¡¨ç¤ºæ¨ç†ã€å…ƒæç¤ºã€å—é™å¼ºåŒ–å­¦ä¹ å’Œä»£ç†è¯„åˆ¤æ¡†æ¶ï¼Œæå‡äº†æ¨èç³»ç»Ÿçš„æ•ˆç‡å’Œè§£é‡Šå¤šæ ·æ€§ã€‚è¯¥ç³»ç»Ÿè§£å†³äº†RecGPT-V1çš„è®¡ç®—æ•ˆç‡ä½ã€è§£é‡Šå¤šæ ·æ€§ä¸è¶³ã€æ³›åŒ–èƒ½åŠ›æœ‰é™å’Œè¯„ä¼°æ ‡å‡†ç®€å•ç­‰é—®é¢˜ã€‚é€šè¿‡åŠ¨æ€ç”Ÿæˆé€‚åº”ä¸Šä¸‹æ–‡çš„æç¤ºï¼ŒRecGPT-V2æé«˜äº†ç”¨æˆ·å…´è¶£æŒ–æ˜å’Œæ ‡ç­¾é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚åœ¨çº¿æµ‹è¯•ç»“æœæ˜¾ç¤ºï¼ŒRecGPT-V2åœ¨å¤šä¸ªå…³é”®æŒ‡æ ‡ä¸Šå‡æœ‰æ˜¾è‘—æå‡ï¼Œè¯æ˜äº†å…¶åœ¨å¤§è§„æ¨¡åº”ç”¨ä¸­çš„æŠ€æœ¯å¯è¡Œæ€§å’Œå•†ä¸šä»·å€¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.14531",
            "title": "VersatileFFN: Achieving Parameter Efficiency in LLMs via Adaptive Wide-and-Deep Reuse",
            "url": "https://huggingface.co/papers/2512.14531",
            "abstract": "The rapid scaling of Large Language Models (LLMs) has achieved remarkable performance, but it also leads to prohibitive memory costs. Existing parameter-efficient approaches such as pruning and quantization mainly compress pretrained models without enhancing architectural capacity, thereby hitting the representational ceiling of the base model. In this work, we propose VersatileFFN, a novel feed-forward network (FFN) that enables flexible reuse of parameters in both width and depth dimensions within a fixed parameter budget. Inspired by the dual-process theory of cognition, VersatileFFN comprises two adaptive pathways: a width-versatile path that generates a mixture of sub-experts from a single shared FFN, mimicking sparse expert routing without increasing parameters, and a depth-versatile path that recursively applies the same FFN to emulate deeper processing for complex tokens. A difficulty-aware gating dynamically balances the two pathways, steering \"easy\" tokens through the efficient width-wise route and allocating deeper iterative refinement to \"hard\" tokens. Crucially, both pathways reuse the same parameters, so all additional capacity comes from computation rather than memory. Experiments across diverse benchmarks and model scales demonstrate the effectiveness of the method. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/VersatileFFN.",
            "score": 7,
            "issue_id": 95,
            "pub_date": "2025-12-16",
            "pub_date_card": {
                "ru": "16 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 16",
                "zh": "12æœˆ16æ—¥"
            },
            "hash": "fe4e0736a1391e23",
            "authors": [
                "Ying Nie",
                "Kai Han",
                "Hongguang Li",
                "Hang Zhou",
                "Tianyu Guo",
                "Enhua Wu",
                "Xinghao Chen",
                "Yunhe Wang"
            ],
            "affiliations": [
                "Huawei Noah's Ark Lab",
                "ISCAS",
                "University of Macau"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.14531.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#architecture",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ“Ğ¸Ğ±ĞºĞ¾Ğµ Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ LLM",
                    "desc": "Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ VersatileFFN â€” Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑĞ²ÑĞ·Ğ½Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ñ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±ĞµĞ· ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ: Ğ¿ÑƒÑ‚ÑŒ Ğ¿Ğ¾ ÑˆĞ¸Ñ€Ğ¸Ğ½Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¼ĞµÑÑŒ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸, Ğ° Ğ¿ÑƒÑ‚ÑŒ Ğ¿Ğ¾ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğµ Ñ€ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ‚Ñƒ Ğ¶Ğµ ÑĞµÑ‚ÑŒ Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ³ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ñ‡ĞµÑ€ĞµĞ· ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚, Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ. Ğ’ÑĞµ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ÑÑ‚ÑÑ Ğ·Ğ° ÑÑ‡Ñ‘Ñ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹, Ğ° Ğ½Ğµ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Enhancing LLM Efficiency with VersatileFFN",
                    "desc": "This paper introduces VersatileFFN, a new type of feed-forward network designed to improve the efficiency of Large Language Models (LLMs) without increasing memory costs. It features two adaptive pathways: one that allows for flexible width adjustments to create a mixture of sub-experts, and another that enables deeper processing for complex inputs. By using a difficulty-aware gating mechanism, the model can efficiently route simpler tokens through a less complex path while applying deeper processing to more challenging tokens. This innovative approach allows for enhanced model capacity through computation rather than additional parameters, making it a significant advancement in parameter-efficient model design."
                },
                "zh": {
                    "title": "çµæ´»é‡ç”¨å‚æ•°ï¼Œæå‡æ¨¡å‹èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„å‰é¦ˆç½‘ç»œVersatileFFNï¼Œæ—¨åœ¨æé«˜å¤§è¯­è¨€æ¨¡å‹çš„å‚æ•°åˆ©ç”¨æ•ˆç‡ã€‚è¯¥ç½‘ç»œé€šè¿‡åœ¨å›ºå®šçš„å‚æ•°é¢„ç®—å†…çµæ´»åœ°é‡ç”¨å‚æ•°ï¼Œå¢å¼ºäº†æ¨¡å‹çš„è¡¨ç¤ºèƒ½åŠ›ã€‚VersatileFFNåŒ…å«ä¸¤ä¸ªè‡ªé€‚åº”è·¯å¾„ï¼Œåˆ†åˆ«å¤„ç†ç®€å•å’Œå¤æ‚çš„è¾“å…¥ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„è®¡ç®—è€Œä¸å¢åŠ å†…å­˜æ¶ˆè€—ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.14442",
            "title": "A4-Agent: An Agentic Framework for Zero-Shot Affordance Reasoning",
            "url": "https://huggingface.co/papers/2512.14442",
            "abstract": "A4-Agent, a training-free framework, decouples affordance prediction into three stages using specialized pre-trained models to enhance generalization and performance in real-world settings.  \t\t\t\t\tAI-generated summary \t\t\t\t Affordance prediction, which identifies interaction regions on objects based on language instructions, is critical for embodied AI. Prevailing end-to-end models couple high-level reasoning and low-level grounding into a single monolithic pipeline and rely on training over annotated datasets, which leads to poor generalization on novel objects and unseen environments. In this paper, we move beyond this paradigm by proposing A4-Agent, a training-free agentic framework that decouples affordance prediction into a three-stage pipeline. Our framework coordinates specialized foundation models at test time: (1) a Dreamer that employs generative models to visualize how an interaction would look; (2) a Thinker that utilizes large vision-language models to decide what object part to interact with; and (3) a Spotter that orchestrates vision foundation models to precisely locate where the interaction area is. By leveraging the complementary strengths of pre-trained models without any task-specific fine-tuning, our zero-shot framework significantly outperforms state-of-the-art supervised methods across multiple benchmarks and demonstrates robust generalization to real-world settings.",
            "score": 7,
            "issue_id": 96,
            "pub_date": "2025-12-16",
            "pub_date_card": {
                "ru": "16 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 16",
                "zh": "12æœˆ16æ—¥"
            },
            "hash": "2a25782f9d642d9c",
            "authors": [
                "Zixin Zhang",
                "Kanghao Chen",
                "Hanqing Wang",
                "Hongfei Zhang",
                "Harold Haodong Chen",
                "Chenfei Liao",
                "Litao Guo",
                "Ying-Cong Chen"
            ],
            "affiliations": [
                "HKUST",
                "HKUST(GZ)",
                "Knowin",
                "SJTU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.14442.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ”ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "A4-Agent â€” ÑÑ‚Ğ¾ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ğ¾-ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ affordance (Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹) Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ½Ğ° Ñ‚Ñ€Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑÑ‚Ğ°Ğ¿Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ, Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ñ‡Ğ°ÑÑ‚Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°, Ğ¸ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. ĞŸĞ¾ÑĞºĞ¾Ğ»ÑŒĞºÑƒ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ğ½Ğ° Ğ»ÑƒÑ‡ÑˆĞµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ¸ Ğ½ĞµĞ·Ğ½Ğ°ĞºĞ¾Ğ¼Ñ‹Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ² ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½ÑƒÑ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Decoupling Affordance Prediction for Better Generalization",
                    "desc": "The paper introduces A4-Agent, a novel framework for affordance prediction that operates without the need for training on specific datasets. It breaks down the prediction process into three distinct stages: visualization of interactions, decision-making on object parts, and precise location identification. By using specialized pre-trained models at test time, A4-Agent enhances generalization and performance in real-world scenarios. This approach allows the framework to outperform traditional supervised methods while maintaining robustness across various benchmarks."
                },
                "zh": {
                    "title": "A4-Agentï¼šæ— è®­ç»ƒçš„å¯ç”¨æ€§é¢„æµ‹æ–°æ¡†æ¶",
                    "desc": "A4-Agentæ˜¯ä¸€ä¸ªæ— è®­ç»ƒçš„æ¡†æ¶ï¼Œå°†å¯ç”¨æ€§é¢„æµ‹åˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼Œä½¿ç”¨ä¸“é—¨çš„é¢„è®­ç»ƒæ¨¡å‹æ¥å¢å¼ºåœ¨ç°å®ä¸–ç•Œä¸­çš„æ³›åŒ–èƒ½åŠ›å’Œæ€§èƒ½ã€‚å¯ç”¨æ€§é¢„æµ‹æ˜¯åŸºäºè¯­è¨€æŒ‡ä»¤è¯†åˆ«ç‰©ä½“äº¤äº’åŒºåŸŸçš„å…³é”®æŠ€æœ¯ï¼Œä¼ ç»Ÿçš„ç«¯åˆ°ç«¯æ¨¡å‹å°†é«˜å±‚æ¨ç†å’Œä½å±‚åŸºç¡€ç»“åˆåœ¨ä¸€èµ·ï¼Œå¯¼è‡´åœ¨æ–°ç‰©ä½“å’Œæœªè§ç¯å¢ƒä¸­çš„æ³›åŒ–èƒ½åŠ›å·®ã€‚A4-Agenté€šè¿‡åè°ƒä¸‰ä¸ªä¸“é—¨çš„åŸºç¡€æ¨¡å‹æ¥å®ç°ï¼šDreamerç”¨äºç”Ÿæˆäº¤äº’çš„å¯è§†åŒ–ï¼ŒThinkerå†³å®šä¸å“ªä¸ªç‰©ä½“éƒ¨åˆ†äº¤äº’ï¼ŒSpotterç²¾ç¡®å®šä½äº¤äº’åŒºåŸŸã€‚æˆ‘ä»¬çš„é›¶-shotæ¡†æ¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—è¶…è¶Šäº†æœ€å…ˆè¿›çš„ç›‘ç£æ–¹æ³•ï¼Œå¹¶åœ¨ç°å®ä¸–ç•Œä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.13961",
            "title": "Olmo 3",
            "url": "https://huggingface.co/papers/2512.13961",
            "abstract": "Olmo 3, a family of state-of-the-art fully-open language models at 7B and 32B parameter scales, excels in long-context reasoning, function calling, coding, instruction following, general chat, and knowledge recall.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Olmo 3, a family of state-of-the-art, fully-open language models at the 7B and 32B parameter scales. Olmo 3 model construction targets long-context reasoning, function calling, coding, instruction following, general chat, and knowledge recall. This release includes the entire model flow, i.e., the full lifecycle of the family of models, including every stage, checkpoint, data point, and dependency used to build it. Our flagship model, Olmo 3 Think 32B, is the strongest fully-open thinking model released to-date.",
            "score": 3,
            "issue_id": 95,
            "pub_date": "2025-12-15",
            "pub_date_card": {
                "ru": "15 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 15",
                "zh": "12æœˆ15æ—¥"
            },
            "hash": "d6b303f339039660",
            "authors": [
                "Team Olmo",
                ":",
                "Allyson Ettinger",
                "Amanda Bertsch",
                "Bailey Kuehl",
                "David Graham",
                "David Heineman",
                "Dirk Groeneveld",
                "Faeze Brahman",
                "Finbarr Timbers",
                "Hamish Ivison",
                "Jacob Morrison",
                "Jake Poznanski",
                "Kyle Lo",
                "Luca Soldaini",
                "Matt Jordan",
                "Mayee Chen",
                "Michael Noukhovitch",
                "Nathan Lambert",
                "Pete Walsh",
                "Pradeep Dasigi",
                "Robert Berry",
                "Saumya Malik",
                "Saurabh Shah",
                "Scott Geng",
                "Shane Arora",
                "Shashank Gupta",
                "Taira Anderson",
                "Teng Xiao",
                "Tyler Murray",
                "Tyler Romero",
                "Victoria Graf",
                "Akari Asai",
                "Akshita Bhagia",
                "Alexander Wettig",
                "Alisa Liu",
                "Aman Rangapur",
                "Chloe Anastasiades",
                "Costa Huang",
                "Dustin Schwenk",
                "Harsh Trivedi",
                "Ian Magnusson",
                "Jaron Lochner",
                "Jiacheng Liu",
                "Lester James V. Miranda",
                "Maarten Sap",
                "Malia Morgan",
                "Michael Schmitz",
                "Michal Guerquin",
                "Michael Wilson",
                "Regan Huff",
                "Ronan Le Bras",
                "Rui Xin",
                "Rulin Shao",
                "Sam Skjonsberg",
                "Shannon Zejiang Shen",
                "Shuyue Stella Li",
                "Tucker Wilde",
                "Valentina Pyatkin",
                "Will Merrill",
                "Yapei Chang",
                "Yuling Gu",
                "Zhiyuan Zeng",
                "Ashish Sabharwal",
                "Luke Zettlemoyer",
                "Pang Wei Koh",
                "Ali Farhadi",
                "Noah A. Smith",
                "Hannaneh Hajishirzi"
            ],
            "affiliations": [
                "Allen Institute for AI",
                "Carnegie Mellon University",
                "Massachusetts Institute of Technology",
                "Mila",
                "Princeton University",
                "Stanford University",
                "University of Maryland",
                "University of Washington",
                "UniversitÃ© de MontrÃ©al"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.13961.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#reasoning",
                    "#long_context"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ»Ğ¼Ğ¾ 3: Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞµĞ¼ÑŒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Olmo 3 â€” Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 7 Ğ¼Ğ»Ñ€Ğ´ Ğ¸ 32 Ğ¼Ğ»Ñ€Ğ´ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…, Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ¸ Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ñ‹Ğ»Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ´Ğ»Ñ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼, Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ° Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ¶Ğ¸Ğ·Ğ½ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ†Ğ¸ĞºĞ» Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ²ÑĞµ ÑÑ‚Ğ°Ğ¿Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸, Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾Ñ‡ĞºĞ¸, Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸. Ğ¤Ğ»Ğ°Ğ³Ğ¼Ğ°Ğ½ÑĞºĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Olmo 3 Think 32B Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°Ğ½Ğ° ÑĞ°Ğ¼Ğ¾Ğ¹ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ñ Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚."
                },
                "en": {
                    "title": "Unlocking Advanced Language Understanding with Olmo 3",
                    "desc": "Olmo 3 is a new family of advanced language models with 7 billion and 32 billion parameters, designed to perform well in various tasks such as long-context reasoning and coding. These models are fully open, meaning that researchers and developers can access the entire model lifecycle, including all stages and data used in their creation. The models excel in instruction following, general chat, and knowledge recall, making them versatile tools for different applications. The standout model, Olmo 3 Think 32B, is highlighted as the most powerful open thinking model available so far."
                },
                "zh": {
                    "title": "Olmo 3ï¼šæœ€å¼ºå¤§çš„å¼€æ”¾è¯­è¨€æ¨¡å‹",
                    "desc": "Olmo 3æ˜¯ä¸€ç³»åˆ—æœ€å…ˆè¿›çš„å®Œå…¨å¼€æ”¾çš„è¯­è¨€æ¨¡å‹ï¼Œå…·æœ‰7Bå’Œ32Bå‚æ•°è§„æ¨¡ã€‚è¯¥æ¨¡å‹ä¸“æ³¨äºé•¿ä¸Šä¸‹æ–‡æ¨ç†ã€å‡½æ•°è°ƒç”¨ã€ç¼–ç ã€æŒ‡ä»¤è·Ÿéšã€ä¸€èˆ¬èŠå¤©å’ŒçŸ¥è¯†å›å¿†ç­‰ä»»åŠ¡ã€‚æ­¤æ¬¡å‘å¸ƒåŒ…æ‹¬æ¨¡å‹çš„å®Œæ•´ç”Ÿå‘½å‘¨æœŸï¼Œæ¶µç›–äº†æ„å»ºæ¨¡å‹æ‰€éœ€çš„æ¯ä¸ªé˜¶æ®µã€æ£€æŸ¥ç‚¹ã€æ•°æ®ç‚¹å’Œä¾èµ–å…³ç³»ã€‚æˆ‘ä»¬çš„æ——èˆ°æ¨¡å‹Olmo 3 Think 32Bæ˜¯è¿„ä»Šä¸ºæ­¢å‘å¸ƒçš„æœ€å¼ºå¤§çš„å®Œå…¨å¼€æ”¾æ€ç»´æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.13678",
            "title": "Feedforward 3D Editing via Text-Steerable Image-to-3D",
            "url": "https://huggingface.co/papers/2512.13678",
            "abstract": "Steer3D enables text-based editing of AI-generated 3D assets by adapting ControlNet for image-to-3D generation with flow-matching training and Direct Preference Optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent progress in image-to-3D has opened up immense possibilities for design, AR/VR, and robotics. However, to use AI-generated 3D assets in real applications, a critical requirement is the capability to edit them easily. We present a feedforward method, Steer3D, to add text steerability to image-to-3D models, which enables editing of generated 3D assets with language. Our approach is inspired by ControlNet, which we adapt to image-to-3D generation to enable text steering directly in a forward pass. We build a scalable data engine for automatic data generation, and develop a two-stage training recipe based on flow-matching training and Direct Preference Optimization (DPO). Compared to competing methods, Steer3D more faithfully follows the language instruction and maintains better consistency with the original 3D asset, while being 2.4x to 28.5x faster. Steer3D demonstrates that it is possible to add a new modality (text) to steer the generation of pretrained image-to-3D generative models with 100k data. Project website: https://glab-caltech.github.io/steer3d/",
            "score": 3,
            "issue_id": 95,
            "pub_date": "2025-12-15",
            "pub_date_card": {
                "ru": "15 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 15",
                "zh": "12æœˆ15æ—¥"
            },
            "hash": "91a4ee8c2c1e471e",
            "authors": [
                "Ziqi Ma",
                "Hongqiao Chen",
                "Yisong Yue",
                "Georgia Gkioxari"
            ],
            "affiliations": [
                "Caltech"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.13678.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#3d",
                    "#training",
                    "#multimodal"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ 3D Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ°Ğ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ControlNet",
                    "desc": "Steer3D â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ÑĞ¼Ğ¸, Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ ControlNet Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ 3D Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ¸Ğ² ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ² Ğ´Ğ²Ğ° ÑÑ‚Ğ°Ğ¿Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ flow-matching Ğ¸ Direct Preference Optimization, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² 2.4-28.5 Ñ€Ğ°Ğ· Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ğ¸ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ²ÑĞµĞ³Ğ¾ 100 Ñ‚Ñ‹ÑÑÑ‡ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Text-Driven Editing for 3D Assets with Steer3D",
                    "desc": "Steer3D is a novel method that enhances the editing of AI-generated 3D assets by integrating text-based commands into the image-to-3D generation process. It builds on the ControlNet framework, allowing users to modify 3D models directly through language inputs during a single forward pass. The method employs flow-matching training and Direct Preference Optimization to ensure that the generated assets align closely with user instructions while maintaining high fidelity to the original designs. Steer3D significantly improves the speed and accuracy of 3D asset editing, making it a powerful tool for applications in design, AR/VR, and robotics."
                },
                "zh": {
                    "title": "æ–‡æœ¬å¼•å¯¼ï¼Œè½»æ¾ç¼–è¾‘3Dèµ„äº§",
                    "desc": "Steer3D æ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œå…è®¸ç”¨æˆ·é€šè¿‡æ–‡æœ¬ç¼–è¾‘ AI ç”Ÿæˆçš„ 3D èµ„äº§ã€‚å®ƒåŸºäº ControlNet çš„ç†å¿µï¼Œé€‚ç”¨äºå›¾åƒåˆ° 3D çš„ç”Ÿæˆï¼Œèƒ½å¤Ÿåœ¨å‰å‘ä¼ é€’ä¸­ç›´æ¥è¿›è¡Œæ–‡æœ¬å¼•å¯¼ã€‚è¯¥æ–¹æ³•ä½¿ç”¨æµåŒ¹é…è®­ç»ƒå’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰è¿›è¡Œä¸¤é˜¶æ®µè®­ç»ƒï¼Œæ˜¾è‘—æé«˜äº†ç”Ÿæˆé€Ÿåº¦å’Œä¸åŸå§‹ 3D èµ„äº§çš„ä¸€è‡´æ€§ã€‚Steer3D å±•ç¤ºäº†å¦‚ä½•å°†æ–‡æœ¬è¿™ä¸€æ–°æ¨¡æ€æ·»åŠ åˆ°é¢„è®­ç»ƒçš„å›¾åƒåˆ° 3D ç”Ÿæˆæ¨¡å‹ä¸­ï¼Œæå‡äº†ç”¨æˆ·çš„ç¼–è¾‘ä½“éªŒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.13303",
            "title": "ShowTable: Unlocking Creative Table Visualization with Collaborative Reflection and Refinement",
            "url": "https://huggingface.co/papers/2512.13303",
            "abstract": "ShowTable, a pipeline combining MLLMs and diffusion models, excels in creative table visualization by generating high-fidelity infographics from data tables, outperforming existing methods in multi-modal reasoning and error correction.  \t\t\t\t\tAI-generated summary \t\t\t\t While existing generation and unified models excel at general image generation, they struggle with tasks requiring deep reasoning, planning, and precise data-to-visual mapping abilities beyond general scenarios. To push beyond the existing limitations, we introduce a new and challenging task: creative table visualization, requiring the model to generate an infographic that faithfully and aesthetically visualizes the data from a given table. To address this challenge, we propose ShowTable, a pipeline that synergizes MLLMs with diffusion models via a progressive self-correcting process. The MLLM acts as the central orchestrator for reasoning the visual plan and judging visual errors to provide refined instructions, the diffusion execute the commands from MLLM, achieving high-fidelity results. To support this task and our pipeline, we introduce three automated data construction pipelines for training different modules. Furthermore, we introduce TableVisBench, a new benchmark with 800 challenging instances across 5 evaluation dimensions, to assess performance on this task. Experiments demonstrate that our pipeline, instantiated with different models, significantly outperforms baselines, highlighting its effective multi-modal reasoning, generation, and error correction capabilities.",
            "score": 2,
            "issue_id": 96,
            "pub_date": "2025-12-15",
            "pub_date_card": {
                "ru": "15 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 15",
                "zh": "12æœˆ15æ—¥"
            },
            "hash": "613d8f8e6a0347ec",
            "authors": [
                "Zhihang Liu",
                "Xiaoyi Bao",
                "Pandeng Li",
                "Junjie Zhou",
                "Zhaohe Liao",
                "Yefei He",
                "Kaixun Jiang",
                "Chen-Wei Xie",
                "Yun Zheng",
                "Hongtao Xie"
            ],
            "affiliations": [
                "CASIA",
                "FDU",
                "NJU",
                "SJTU",
                "Tongyi Lab",
                "USTC",
                "ZJU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.13303.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#cv",
                    "#dataset",
                    "#reasoning",
                    "#benchmark",
                    "#multimodal",
                    "#diffusion"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğ¾: ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ÑÑ pipeline Ğ´Ğ»Ñ Ñ‚Ğ²Ğ¾Ñ€Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†",
                    "desc": "ShowTable â€” ÑÑ‚Ğ¾ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ pipeline, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (MLLM) Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ½Ñ„Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ğº Ğ¸Ğ· Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ† Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. MLLM Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ñ€Ğ¾Ğ»ÑŒ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ğ¾Ñ€Ğ°, Ğ¾ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ»ÑÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ğ»Ğ°Ğ½Ğµ Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº, Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ‚Ñ€Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº TableVisBench Ñ 800 ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº."
                },
                "en": {
                    "title": "Revolutionizing Table Visualization with ShowTable!",
                    "desc": "The paper presents ShowTable, a novel pipeline that combines Multi-Modal Language Models (MLLMs) and diffusion models to enhance creative table visualization. This approach generates high-quality infographics from data tables, addressing the limitations of existing models in multi-modal reasoning and error correction. ShowTable utilizes MLLMs to orchestrate visual planning and error judgment, while diffusion models execute the visual commands, resulting in superior fidelity. Additionally, the authors introduce TableVisBench, a benchmark for evaluating the performance of their method across various dimensions, demonstrating significant improvements over traditional methods."
                },
                "zh": {
                    "title": "ShowTableï¼šåˆ›æ–°è¡¨æ ¼å¯è§†åŒ–çš„è§£å†³æ–¹æ¡ˆ",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºShowTableçš„ç®¡é“ï¼Œå®ƒç»“åˆäº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å’Œæ‰©æ•£æ¨¡å‹ï¼Œä¸“æ³¨äºåˆ›é€ æ€§è¡¨æ ¼å¯è§†åŒ–ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿä»æ•°æ®è¡¨ä¸­ç”Ÿæˆé«˜ä¿çœŸä¿¡æ¯å›¾ï¼Œè¶…è¶Šäº†ç°æœ‰æ–¹æ³•åœ¨å¤šæ¨¡æ€æ¨ç†å’Œé”™è¯¯ä¿®æ­£æ–¹é¢çš„è¡¨ç°ã€‚ShowTableé€šè¿‡æ¸è¿›å¼è‡ªæˆ‘ä¿®æ­£è¿‡ç¨‹ï¼Œåˆ©ç”¨MLLMè¿›è¡Œè§†è§‰è§„åˆ’å’Œé”™è¯¯åˆ¤æ–­ï¼Œå¹¶ç”±æ‰©æ•£æ¨¡å‹æ‰§è¡ŒæŒ‡ä»¤ï¼Œä»è€Œå®ç°é«˜è´¨é‡çš„å¯è§†åŒ–ç»“æœã€‚æˆ‘ä»¬è¿˜æå‡ºäº†TableVisBenchï¼Œä¸€ä¸ªåŒ…å«800ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„å®ä¾‹çš„æ–°åŸºå‡†ï¼Œç”¨äºè¯„ä¼°è¯¥ä»»åŠ¡çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.14067",
            "title": "Efficient-DLM: From Autoregressive to Diffusion Language Models, and Beyond in Speed",
            "url": "https://huggingface.co/papers/2512.14067",
            "abstract": "AR-to-dLM conversion enhances diffusion language models' efficiency and speed while maintaining task accuracy through refined attention patterns and token masking strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion language models (dLMs) have emerged as a promising paradigm that enables parallel, non-autoregressive generation, but their learning efficiency lags behind that of autoregressive (AR) language models when trained from scratch. To this end, we study AR-to-dLM conversion to transform pretrained AR models into efficient dLMs that excel in speed while preserving AR models' task accuracy. We achieve this by identifying limitations in the attention patterns and objectives of existing AR-to-dLM methods and then proposing principles and methodologies for more effective AR-to-dLM conversion. Specifically, we first systematically compare different attention patterns and find that maintaining pretrained AR weight distributions is critical for effective AR-to-dLM conversion. As such, we introduce a continuous pretraining scheme with a block-wise attention pattern, which remains causal across blocks while enabling bidirectional modeling within each block. We find that this approach can better preserve pretrained AR models' weight distributions than fully bidirectional modeling, in addition to its known benefit of enabling KV caching, and leads to a win-win in accuracy and efficiency. Second, to mitigate the training-test gap in mask token distributions (uniform vs. highly left-to-right), we propose a position-dependent token masking strategy that assigns higher masking probabilities to later tokens during training to better mimic test-time behavior. Leveraging this framework, we conduct extensive studies of dLMs' attention patterns, training dynamics, and other design choices, providing actionable insights into scalable AR-to-dLM conversion. These studies lead to the Efficient-DLM family, which outperforms state-of-the-art AR models and dLMs, e.g., our Efficient-DLM 8B achieves +5.4%/+2.7% higher accuracy with 4.5x/2.7x higher throughput compared to Dream 7B and Qwen3 4B, respectively.",
            "score": 1,
            "issue_id": 95,
            "pub_date": "2025-12-16",
            "pub_date_card": {
                "ru": "16 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 16",
                "zh": "12æœˆ16æ—¥"
            },
            "hash": "6705a84657cf61dd",
            "authors": [
                "Yonggan Fu",
                "Lexington Whalen",
                "Zhifan Ye",
                "Xin Dong",
                "Shizhe Diao",
                "Jingyu Liu",
                "Chengyue Wu",
                "Hao Zhang",
                "Enze Xie",
                "Song Han",
                "Maksim Khadkevich",
                "Jan Kautz",
                "Yingyan Celine Lin",
                "Pavlo Molchanov"
            ],
            "affiliations": [
                "Georgia Tech",
                "MIT",
                "NVIDIA",
                "University of Chicago",
                "University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.14067.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#transfer_learning",
                    "#diffusion"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ñ‚ĞµĞºÑÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ĞºĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ±Ğ»Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²ĞµÑĞ¾Ğ² Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ¾Ğ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼ÑƒÑ Ğ¾Ñ‚ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ¸Ñ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼Ğ°ÑĞ¾Ğº Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ¸Ñ€ÑƒÑÑ‰ĞµĞµ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Efficient-DLM Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½Ğ¾Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "Transforming AR Models for Faster, Accurate dLMs",
                    "desc": "This paper discusses a method to improve diffusion language models (dLMs) by converting pretrained autoregressive (AR) models into more efficient dLMs. The authors identify issues with existing AR-to-dLM conversion techniques, particularly in attention patterns and token masking strategies. They propose a continuous pretraining scheme that maintains the weight distributions of AR models while allowing for efficient bidirectional modeling. Their approach results in significant improvements in both speed and accuracy, leading to the development of the Efficient-DLM family, which surpasses current state-of-the-art models."
                },
                "zh": {
                    "title": "é«˜æ•ˆè½¬æ¢ï¼šæå‡æ‰©æ•£è¯­è¨€æ¨¡å‹çš„é€Ÿåº¦ä¸å‡†ç¡®æ€§",
                    "desc": "æœ¬æ–‡ç ”ç©¶äº†å°†è‡ªå›å½’ï¼ˆARï¼‰è¯­è¨€æ¨¡å‹è½¬æ¢ä¸ºæ‰©æ•£è¯­è¨€æ¨¡å‹ï¼ˆdLMï¼‰çš„è¿‡ç¨‹ï¼Œä»¥æé«˜æ¨¡å‹çš„æ•ˆç‡å’Œé€Ÿåº¦ï¼ŒåŒæ—¶ä¿æŒä»»åŠ¡çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬å‘ç°ï¼Œä¿æŒé¢„è®­ç»ƒARæ¨¡å‹çš„æƒé‡åˆ†å¸ƒå¯¹äºæœ‰æ•ˆçš„ARåˆ°dLMè½¬æ¢è‡³å…³é‡è¦ï¼Œå› æ­¤æå‡ºäº†ä¸€ç§è¿ç»­é¢„è®­ç»ƒæ–¹æ¡ˆï¼Œé‡‡ç”¨å—çº§æ³¨æ„åŠ›æ¨¡å¼ã€‚ä¸ºäº†ç¼©å°è®­ç»ƒå’Œæµ‹è¯•ä¹‹é—´çš„å·®è·ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§ä½ç½®ä¾èµ–çš„æ ‡è®°æ©è”½ç­–ç•¥ï¼Œä»¥æ›´å¥½åœ°æ¨¡æ‹Ÿæµ‹è¯•æ—¶çš„è¡Œä¸ºã€‚é€šè¿‡è¿™äº›æ–¹æ³•ï¼Œæˆ‘ä»¬çš„Efficient-DLMç³»åˆ—æ¨¡å‹åœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡ä¸Šå‡ä¼˜äºç°æœ‰çš„ARå’ŒdLMæ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.13655",
            "title": "Comparative Analysis of LLM Abliteration Methods: A Cross-Architecture Evaluation",
            "url": "https://huggingface.co/papers/2512.13655",
            "abstract": "Four abliteration tools are evaluated for their effectiveness in removing refusal representations from large language models, with findings showing variability in capability preservation and distribution shift across different models and tools.  \t\t\t\t\tAI-generated summary \t\t\t\t Safety alignment mechanisms in large language models prevent responses to harmful queries through learned refusal behavior, yet these same mechanisms impede legitimate research applications including cognitive modeling, adversarial testing, and security analysis. While abliteration techniques enable surgical removal of refusal representations through directional orthogonalization, the relative effectiveness of available implementations remains uncharacterized. This study evaluates four abliteration tools (Heretic, DECCP, ErisForge, FailSpy) across sixteen instruction-tuned models (7B-14B parameters), reporting tool compatibility on all 16 models and quantitative metrics on subsets dictated by tool support. Single-pass methods demonstrated superior capability preservation on the benchmarked subset (avg GSM8K change across three models: ErisForge -0.28 pp; DECCP -0.13 pp), while Bayesian-optimized abliteration produced variable distribution shift (KL divergence: 0.043-1.646) with model-dependent capability impact. These findings provide researchers with evidence-based selection criteria for abliteration tool deployment across diverse model architectures. The principal finding indicates that mathematical reasoning capabilities exhibit the highest sensitivity to abliteration interventions, with GSM8K change ranging from +1.51 pp to -18.81 pp (-26.5% relative) depending on tool selection and model architecture.",
            "score": 1,
            "issue_id": 95,
            "pub_date": "2025-12-15",
            "pub_date_card": {
                "ru": "15 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 15",
                "zh": "12æœˆ15æ—¥"
            },
            "hash": "7535b4116fe7160c",
            "authors": [
                "Richard J. Young"
            ],
            "affiliations": [
                "University of Nevada Las Vegas, Department of Neuroscience"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.13655.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#security",
                    "#benchmark",
                    "#alignment",
                    "#interpretability"
                ],
                "emoji": "ğŸ”ª",
                "ru": {
                    "title": "Ğ¥Ğ¸Ñ€ÑƒÑ€Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚ĞºĞ°Ğ·Ğ¾Ğ²: ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ°Ğ±Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ LLM",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‡ĞµÑ‚Ñ‹Ñ€Ñ‘Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ°Ğ±Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ¾Ñ‚ĞºĞ°Ğ·Ğ° Ğ¸Ğ· Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ ÑÑ‚Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° 16 Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº Ğ±Ğ°Ğ¹ĞµÑĞ¾Ğ²ÑĞºĞ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ²Ñ‹Ğ²Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğº Ğ°Ğ±Ğ»ÑÑ†Ğ¸Ğ¸, Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ +1.51% Ğ´Ğ¾ -26.5% Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ° Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Choosing the Right Tool for Effective Abliteration in Language Models",
                    "desc": "This paper evaluates four different abliteration tools designed to remove refusal representations from large language models. The study finds that these tools vary in their ability to preserve model capabilities and manage distribution shifts across different model architectures. It highlights that single-pass methods tend to maintain better performance compared to Bayesian-optimized approaches, which can lead to significant variability in model behavior. The results provide valuable insights for researchers on how to choose the right abliteration tool based on the specific model and application needs."
                },
                "zh": {
                    "title": "æ¶ˆé™¤å·¥å…·çš„æœ‰æ•ˆæ€§è¯„ä¼°ä¸é€‰æ‹©æ ‡å‡†",
                    "desc": "æœ¬ç ”ç©¶è¯„ä¼°äº†å››ç§æ¶ˆé™¤å·¥å…·åœ¨å»é™¤å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æ‹’ç»è¡¨ç¤ºçš„æœ‰æ•ˆæ€§ã€‚è¿™äº›å·¥å…·åœ¨ä¸åŒæ¨¡å‹å’Œå·¥å…·ä¹‹é—´çš„èƒ½åŠ›ä¿ç•™å’Œåˆ†å¸ƒå˜åŒ–ä¸Šè¡¨ç°å‡ºå·®å¼‚ã€‚ç ”ç©¶å‘ç°ï¼Œæ•°å­¦æ¨ç†èƒ½åŠ›å¯¹æ¶ˆé™¤å¹²é¢„çš„æ•æ„Ÿæ€§æœ€é«˜ï¼Œå˜åŒ–èŒƒå›´ä»+1.51 ppåˆ°-18.81 ppã€‚ç ”ç©¶ç»“æœä¸ºç ”ç©¶äººå‘˜åœ¨ä¸åŒæ¨¡å‹æ¶æ„ä¸­é€‰æ‹©æ¶ˆé™¤å·¥å…·æä¾›äº†åŸºäºè¯æ®çš„æ ‡å‡†ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-12-16.html",
    "link_next": "2025-12-18.html",
    "link_month": "2025-12.html",
    "short_date_prev": {
        "ru": "16.12",
        "en": "12/16",
        "zh": "12æœˆ16æ—¥"
    },
    "short_date_next": {
        "ru": "18.12",
        "en": "12/18",
        "zh": "12æœˆ18æ—¥"
    },
    "categories": {
        "#dataset": 4,
        "#data": 0,
        "#benchmark": 5,
        "#agents": 1,
        "#cv": 2,
        "#rl": 2,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 3,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 5,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 7,
        "#robotics": 1,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 4,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 2,
        "#survey": 1,
        "#diffusion": 3,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 3,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}