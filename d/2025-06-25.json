{
    "date": {
        "ru": "25 июня",
        "en": "June 25",
        "zh": "6月25日"
    },
    "time_utc": "2025-06-25 02:45",
    "weekday": 2,
    "issue_id": 4470,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2506.19851",
            "title": "AnimaX: Animating the Inanimate in 3D with Joint Video-Pose Diffusion\n  Models",
            "url": "https://huggingface.co/papers/2506.19851",
            "abstract": "AnimaX creates multi-skeleton 3D animations by blending video diffusion model priors with skeleton-based control, using joint video-pose diffusion and shared positional encodings.  \t\t\t\t\tAI-generated summary \t\t\t\t We present AnimaX, a feed-forward 3D animation framework that bridges the motion priors of video diffusion models with the controllable structure of skeleton-based animation. Traditional motion synthesis methods are either restricted to fixed skeletal topologies or require costly optimization in high-dimensional deformation spaces. In contrast, AnimaX effectively transfers video-based motion knowledge to the 3D domain, supporting diverse articulated meshes with arbitrary skeletons. Our method represents 3D motion as multi-view, multi-frame 2D pose maps, and enables joint video-pose diffusion conditioned on template renderings and a textual motion prompt. We introduce shared positional encodings and modality-aware embeddings to ensure spatial-temporal alignment between video and pose sequences, effectively transferring video priors to motion generation task. The resulting multi-view pose sequences are triangulated into 3D joint positions and converted into mesh animation via inverse kinematics. Trained on a newly curated dataset of 160,000 rigged sequences, AnimaX achieves state-of-the-art results on VBench in generalization, motion fidelity, and efficiency, offering a scalable solution for category-agnostic 3D animation. Project page: https://anima-x.github.io/{https://anima-x.github.io/}.",
            "score": 5,
            "issue_id": 4470,
            "pub_date": "2025-06-24",
            "pub_date_card": {
                "ru": "24 июня",
                "en": "June 24",
                "zh": "6月24日"
            },
            "hash": "ce9a811b1a9e7d9d",
            "authors": [
                "Zehuan Huang",
                "Haoran Feng",
                "Yangtian Sun",
                "Yuanchen Guo",
                "Yanpei Cao",
                "Lu Sheng"
            ],
            "affiliations": [
                "Beihang University, China",
                "The University of Hong Kong, China",
                "Tsinghua University, China",
                "VAST, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.19851.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#3d",
                    "#transfer_learning",
                    "#diffusion",
                    "#dataset"
                ],
                "emoji": "🦾",
                "ru": {
                    "title": "AnimaX: универсальная 3D-анимация с помощью диффузионных моделей видео",
                    "desc": "AnimaX - это фреймворк для создания 3D-анимации, объединяющий преимущества диффузионных моделей видео с контролируемой структурой скелетной анимации. Метод представляет 3D-движение в виде многоракурсных 2D-карт поз и позволяет выполнять совместную диффузию видео и поз на основе шаблонных рендеров и текстового описания движения. AnimaX использует общие позиционные кодировки и модально-специфичные эмбеддинги для обеспечения пространственно-временного выравнивания между видео и последовательностями поз. Результаты демонстрируют передовую производительность в обобщении, точности движений и эффективности для анимации различных 3D-моделей."
                },
                "en": {
                    "title": "Bridging Video Motion and 3D Animation with AnimaX",
                    "desc": "AnimaX is a novel framework for creating 3D animations that combines the strengths of video diffusion models with skeleton-based control. It allows for the generation of animations without being limited to fixed skeletal structures, overcoming the challenges of high-dimensional optimization. By using multi-view, multi-frame 2D pose maps and shared positional encodings, AnimaX ensures that video motion knowledge is effectively transferred to 3D motion generation. This method has been trained on a large dataset and demonstrates superior performance in terms of generalization, motion fidelity, and efficiency for diverse animated characters."
                },
                "zh": {
                    "title": "AnimaX：无类别 3D 动画的创新解决方案",
                    "desc": "AnimaX 是一个前馈式的 3D 动画框架，它结合了视频扩散模型的运动先验和基于骨骼的可控结构。与传统的运动合成方法不同，AnimaX 支持任意骨骼的多样化关节网格，并有效地将视频中的运动知识转移到 3D 领域。该方法通过多视角、多帧的 2D 姿态图表示 3D 运动，并利用共享位置编码和模态感知嵌入确保视频和姿态序列之间的时空对齐。经过在一个包含 160,000 个绑定序列的新数据集上训练，AnimaX 在 VBench 上实现了最先进的结果，提供了一种可扩展的无类别 3D 动画解决方案。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.19838",
            "title": "SimpleGVR: A Simple Baseline for Latent-Cascaded Video Super-Resolution",
            "url": "https://huggingface.co/papers/2506.19838",
            "abstract": "Researchers propose design principles for cascaded video super-resolution models to improve high-resolution video generation by introduces degradation strategies, timestep sampling, noise augmentation, and interleaving temporal units with sparse local attention.  \t\t\t\t\tAI-generated summary \t\t\t\t Latent diffusion models have emerged as a leading paradigm for efficient video generation. However, as user expectations shift toward higher-resolution outputs, relying solely on latent computation becomes inadequate. A promising approach involves decoupling the process into two stages: semantic content generation and detail synthesis. The former employs a computationally intensive base model at lower resolutions, while the latter leverages a lightweight cascaded video super-resolution (VSR) model to achieve high-resolution output. In this work, we focus on studying key design principles for latter cascaded VSR models, which are underexplored currently. First, we propose two degradation strategies to generate training pairs that better mimic the output characteristics of the base model, ensuring alignment between the VSR model and its upstream generator. Second, we provide critical insights into VSR model behavior through systematic analysis of (1) timestep sampling strategies, (2) noise augmentation effects on low-resolution (LR) inputs. These findings directly inform our architectural and training innovations. Finally, we introduce interleaving temporal unit and sparse local attention to achieve efficient training and inference, drastically reducing computational overhead. Extensive experiments demonstrate the superiority of our framework over existing methods, with ablation studies confirming the efficacy of each design choice. Our work establishes a simple yet effective baseline for cascaded video super-resolution generation, offering practical insights to guide future advancements in efficient cascaded synthesis systems.",
            "score": 0,
            "issue_id": 4470,
            "pub_date": "2025-06-24",
            "pub_date_card": {
                "ru": "24 июня",
                "en": "June 24",
                "zh": "6月24日"
            },
            "hash": "a00bfa00a7f0f869",
            "authors": [
                "Liangbin Xie",
                "Yu Li",
                "Shian Du",
                "Menghan Xia",
                "Xintao Wang",
                "Fanghua Yu",
                "Ziyan Chen",
                "Pengfei Wan",
                "Jiantao Zhou",
                "Chao Dong"
            ],
            "affiliations": [
                "Kuaishou Technology",
                "Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences",
                "Shenzhen University of Advanced Technology",
                "State Key Laboratory of Internet of Things for Smart City, University of Macau",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.19838.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#video",
                    "#diffusion",
                    "#architecture"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Эффективное сверхразрешение видео с помощью каскадных моделей",
                    "desc": "Исследователи предлагают принципы проектирования каскадных моделей сверхразрешения видео для улучшения генерации видео высокого разрешения. Они вводят стратегии деградации, выборку временных шагов, аугментацию шума и чередование временных блоков с разреженным локальным вниманием. Эти методы позволяют эффективно обучать и использовать модели, значительно снижая вычислительные затраты. Эксперименты показывают превосходство предложенного подхода над существующими методами."
                },
                "en": {
                    "title": "Enhancing Video Quality with Smart Cascaded Super-Resolution",
                    "desc": "This paper presents new design principles for improving cascaded video super-resolution (VSR) models, which are essential for generating high-resolution videos. The authors introduce innovative degradation strategies and timestep sampling methods to enhance the training process, ensuring that the VSR model aligns well with the base model's output. They also explore the impact of noise augmentation on low-resolution inputs and propose techniques like interleaving temporal units and sparse local attention to optimize training efficiency. The results show that their framework outperforms existing methods, providing a solid foundation for future developments in video super-resolution."
                },
                "zh": {
                    "title": "提升视频超分辨率生成的设计原则",
                    "desc": "本研究提出了级联视频超分辨率模型的设计原则，以提高高分辨率视频生成的效果。我们引入了退化策略、时间步采样、噪声增强和稀疏局部注意力等方法，来优化模型的训练和推理过程。通过系统分析，我们揭示了时间步采样策略和噪声增强对低分辨率输入的影响，从而指导模型架构和训练创新。实验结果表明，我们的方法在现有技术中具有显著优势，为未来高效级联合成系统的发展提供了实用的见解。"
                }
            }
        }
    ],
    "link_prev": "2025-06-24.html",
    "link_next": "2025-06-26.html",
    "link_month": "2025-06.html",
    "short_date_prev": {
        "ru": "24.06",
        "en": "06/24",
        "zh": "6月24日"
    },
    "short_date_next": {
        "ru": "26.06",
        "en": "06/26",
        "zh": "6月26日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}