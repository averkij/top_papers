{
    "date": {
        "ru": "25 Ğ¸ÑĞ½Ñ",
        "en": "June 25",
        "zh": "6æœˆ25æ—¥"
    },
    "time_utc": "2025-06-25 06:18",
    "weekday": 2,
    "issue_id": 4474,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2506.19851",
            "title": "AnimaX: Animating the Inanimate in 3D with Joint Video-Pose Diffusion\n  Models",
            "url": "https://huggingface.co/papers/2506.19851",
            "abstract": "AnimaX creates multi-skeleton 3D animations by blending video diffusion model priors with skeleton-based control, using joint video-pose diffusion and shared positional encodings.  \t\t\t\t\tAI-generated summary \t\t\t\t We present AnimaX, a feed-forward 3D animation framework that bridges the motion priors of video diffusion models with the controllable structure of skeleton-based animation. Traditional motion synthesis methods are either restricted to fixed skeletal topologies or require costly optimization in high-dimensional deformation spaces. In contrast, AnimaX effectively transfers video-based motion knowledge to the 3D domain, supporting diverse articulated meshes with arbitrary skeletons. Our method represents 3D motion as multi-view, multi-frame 2D pose maps, and enables joint video-pose diffusion conditioned on template renderings and a textual motion prompt. We introduce shared positional encodings and modality-aware embeddings to ensure spatial-temporal alignment between video and pose sequences, effectively transferring video priors to motion generation task. The resulting multi-view pose sequences are triangulated into 3D joint positions and converted into mesh animation via inverse kinematics. Trained on a newly curated dataset of 160,000 rigged sequences, AnimaX achieves state-of-the-art results on VBench in generalization, motion fidelity, and efficiency, offering a scalable solution for category-agnostic 3D animation. Project page: https://anima-x.github.io/{https://anima-x.github.io/}.",
            "score": 17,
            "issue_id": 4470,
            "pub_date": "2025-06-24",
            "pub_date_card": {
                "ru": "24 Ğ¸ÑĞ½Ñ",
                "en": "June 24",
                "zh": "6æœˆ24æ—¥"
            },
            "hash": "ce9a811b1a9e7d9d",
            "authors": [
                "Zehuan Huang",
                "Haoran Feng",
                "Yangtian Sun",
                "Yuanchen Guo",
                "Yanpei Cao",
                "Lu Sheng"
            ],
            "affiliations": [
                "Beihang University, China",
                "The University of Hong Kong, China",
                "Tsinghua University, China",
                "VAST, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.19851.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#3d",
                    "#transfer_learning",
                    "#diffusion",
                    "#dataset"
                ],
                "emoji": "ğŸ¦¾",
                "ru": {
                    "title": "AnimaX: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ 3D-Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "AnimaX - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ 3D-Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ ÑĞºĞµĞ»ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ 3D-Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ñ… 2D-ĞºĞ°Ñ€Ñ‚ Ğ¿Ğ¾Ğ· Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¿Ğ¾Ğ· Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¾Ğ² Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. AnimaX Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¿Ğ¾Ğ·. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¸, Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Bridging Video Motion and 3D Animation with AnimaX",
                    "desc": "AnimaX is a novel framework for creating 3D animations that combines the strengths of video diffusion models with skeleton-based control. It allows for the generation of animations without being limited to fixed skeletal structures, overcoming the challenges of high-dimensional optimization. By using multi-view, multi-frame 2D pose maps and shared positional encodings, AnimaX ensures that video motion knowledge is effectively transferred to 3D motion generation. This method has been trained on a large dataset and demonstrates superior performance in terms of generalization, motion fidelity, and efficiency for diverse animated characters."
                },
                "zh": {
                    "title": "AnimaXï¼šæ— ç±»åˆ« 3D åŠ¨ç”»çš„åˆ›æ–°è§£å†³æ–¹æ¡ˆ",
                    "desc": "AnimaX æ˜¯ä¸€ä¸ªå‰é¦ˆå¼çš„ 3D åŠ¨ç”»æ¡†æ¶ï¼Œå®ƒç»“åˆäº†è§†é¢‘æ‰©æ•£æ¨¡å‹çš„è¿åŠ¨å…ˆéªŒå’ŒåŸºäºéª¨éª¼çš„å¯æ§ç»“æ„ã€‚ä¸ä¼ ç»Ÿçš„è¿åŠ¨åˆæˆæ–¹æ³•ä¸åŒï¼ŒAnimaX æ”¯æŒä»»æ„éª¨éª¼çš„å¤šæ ·åŒ–å…³èŠ‚ç½‘æ ¼ï¼Œå¹¶æœ‰æ•ˆåœ°å°†è§†é¢‘ä¸­çš„è¿åŠ¨çŸ¥è¯†è½¬ç§»åˆ° 3D é¢†åŸŸã€‚è¯¥æ–¹æ³•é€šè¿‡å¤šè§†è§’ã€å¤šå¸§çš„ 2D å§¿æ€å›¾è¡¨ç¤º 3D è¿åŠ¨ï¼Œå¹¶åˆ©ç”¨å…±äº«ä½ç½®ç¼–ç å’Œæ¨¡æ€æ„ŸçŸ¥åµŒå…¥ç¡®ä¿è§†é¢‘å’Œå§¿æ€åºåˆ—ä¹‹é—´çš„æ—¶ç©ºå¯¹é½ã€‚ç»è¿‡åœ¨ä¸€ä¸ªåŒ…å« 160,000 ä¸ªç»‘å®šåºåˆ—çš„æ–°æ•°æ®é›†ä¸Šè®­ç»ƒï¼ŒAnimaX åœ¨ VBench ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œæä¾›äº†ä¸€ç§å¯æ‰©å±•çš„æ— ç±»åˆ« 3D åŠ¨ç”»è§£å†³æ–¹æ¡ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.16141",
            "title": "GRPO-CARE: Consistency-Aware Reinforcement Learning for Multimodal\n  Reasoning",
            "url": "https://huggingface.co/papers/2506.16141",
            "abstract": "GRPO-CARE, a reinforcement learning framework optimizing for consistency and correctness, outperforms standard GRPO on a new video understanding benchmark, SEED-Bench-R1, improving both performance and logical coherence in multimodal large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent reinforcement learning approaches, such as outcome-supervised GRPO, have advanced Chain-of-Thought reasoning in large language models (LLMs), yet their adaptation to multimodal LLMs (MLLMs) is unexplored. To address the lack of rigorous evaluation for MLLM post-training methods, we introduce SEED-Bench-R1, a benchmark with complex real-world videos requiring balanced perception and reasoning. It offers a large training set and evaluates generalization across three escalating challenges: in-distribution, cross-environment, and cross-environment-task scenarios. Using SEED-Bench-R1, we find that standard GRPO, while improving answer accuracy, often reduces logical coherence between reasoning steps and answers, with only a 57.9% consistency rate. This stems from reward signals focusing solely on final answers, encouraging shortcuts, and strict KL penalties limiting exploration.To address this, we propose GRPO-CARE, a consistency-aware RL framework optimizing both answer correctness and reasoning coherence without explicit supervision. GRPO-CARE introduces a two-tiered reward: (1) a base reward for answer correctness, and (2) an adaptive consistency bonus, computed by comparing the model's reasoning-to-answer likelihood (via a slowly-evolving reference model) against group peers.This dual mechanism amplifies rewards for reasoning paths that are both correct and logically consistent. Replacing KL penalties with this adaptive bonus, GRPO-CARE outperforms standard GRPO on SEED-Bench-R1, achieving a 6.7% performance gain on the hardest evaluation level and a 24.5% improvement in consistency. It also shows strong transferability, improving model performance across diverse video understanding benchmarks. Our work contributes a systematically designed benchmark and a generalizable post-training framework, advancing the development of more interpretable and robust MLLMs.",
            "score": 17,
            "issue_id": 4471,
            "pub_date": "2025-06-19",
            "pub_date_card": {
                "ru": "19 Ğ¸ÑĞ½Ñ",
                "en": "June 19",
                "zh": "6æœˆ19æ—¥"
            },
            "hash": "0d8fc795754c4210",
            "authors": [
                "Yi Chen",
                "Yuying Ge",
                "Rui Wang",
                "Yixiao Ge",
                "Junhao Cheng",
                "Ying Shan",
                "Xihui Liu"
            ],
            "affiliations": [
                "ARC Lab, Tencent PCG",
                "The Chinese University of Hong Kong",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.16141.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#rl",
                    "#interpretability",
                    "#multimodal",
                    "#benchmark",
                    "#training",
                    "#optimization",
                    "#video",
                    "#reasoning"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "GRPO-CARE: Ğ¡Ğ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜",
                    "desc": "GRPO-CARE - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ°Ğº Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ñ‚Ğ°Ğº Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· ÑĞ²Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ. GRPO-CARE Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ğ²ÑƒÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹: Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ Ğ·Ğ° Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ±Ğ¾Ğ½ÑƒÑ Ğ·Ğ° ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¹ GRPO Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ SEED-Bench-R1 Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ²ÑĞ·Ğ½Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "Enhancing Consistency and Correctness in Multimodal Learning",
                    "desc": "The paper introduces GRPO-CARE, a new reinforcement learning framework that enhances the performance and logical coherence of multimodal large language models (MLLMs) in video understanding tasks. It addresses the limitations of standard GRPO, which often sacrifices reasoning consistency for accuracy, by implementing a two-tiered reward system that promotes both correct answers and coherent reasoning. The authors present SEED-Bench-R1, a benchmark designed to rigorously evaluate MLLMs on complex video tasks, revealing that GRPO-CARE significantly outperforms GRPO with a notable increase in consistency and performance. This work not only proposes a novel framework but also contributes a valuable benchmark for future research in MLLM development."
                },
                "zh": {
                    "title": "æå‡ä¸€è‡´æ€§ä¸æ­£ç¡®æ€§çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶",
                    "desc": "GRPO-CAREæ˜¯ä¸€ç§å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜åŒ–ä¸€è‡´æ€§å’Œæ­£ç¡®æ€§ï¼Œè¶…è¶Šäº†æ ‡å‡†çš„GRPOæ–¹æ³•ã€‚å®ƒåœ¨æ–°çš„è§†é¢‘ç†è§£åŸºå‡†SEED-Bench-R1ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæå‡äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ€§èƒ½å’Œé€»è¾‘è¿è´¯æ€§ã€‚é€šè¿‡å¼•å…¥åŒé‡å¥–åŠ±æœºåˆ¶ï¼ŒGRPO-CAREä¸ä»…å…³æ³¨ç­”æ¡ˆçš„æ­£ç¡®æ€§ï¼Œè¿˜é¼“åŠ±æ¨ç†è¿‡ç¨‹çš„é€»è¾‘ä¸€è‡´æ€§ã€‚è¯¥æ¡†æ¶åœ¨å¤æ‚çš„çœŸå®ä¸–ç•Œè§†é¢‘ä»»åŠ¡ä¸­å±•ç°äº†å¼ºå¤§çš„è¿ç§»èƒ½åŠ›ï¼Œæ¨åŠ¨äº†æ›´å…·å¯è§£é‡Šæ€§å’Œé²æ£’æ€§çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.19848",
            "title": "ScaleCap: Inference-Time Scalable Image Captioning via Dual-Modality\n  Debiasing",
            "url": "https://huggingface.co/papers/2506.19848",
            "abstract": "ScaleCap enhances image captioning by iteratively enriching and calibrating captions using heuristic question answering and contrastive sentence rating, addressing multimodal and linguistic biases to improve accuracy, balance, and informativeness.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper presents ScaleCap, an inference-time scalable image captioning strategy that generates comprehensive and detailed image captions. The key challenges of high-quality image captioning lie in the inherent biases of LVLMs: multimodal bias resulting in imbalanced descriptive granularity, offering detailed accounts of some elements while merely skimming over others; linguistic bias leading to hallucinated descriptions of non-existent objects. To address these issues, we propose a scalable debiased captioning strategy, which continuously enriches and calibrates the caption with increased inference budget. Specifically, we propose two novel components: heuristic question answering and contrastive sentence rating. The former generates content-specific questions based on the image and answers them to progressively inject relevant information into the caption. The latter employs sentence-level offline contrastive decoding to effectively identify and eliminate hallucinations caused by linguistic biases. With increased inference cost, more heuristic questions are raised by ScaleCap to progressively capture additional visual details, generating captions that are more accurate, balanced, and informative. Extensive modality alignment experiments demonstrate the effectiveness of ScaleCap. Annotating 450K images with ScaleCap and using them for LVLM pretraining leads to consistent performance gains across 11 widely used benchmarks. Furthermore, ScaleCap showcases superb richness and fidelity of generated captions with two additional tasks: replacing images with captions in VQA task, and reconstructing images from captions to assess semantic coverage. Code is available at https://github.com/Cooperx521/ScaleCap.",
            "score": 14,
            "issue_id": 4473,
            "pub_date": "2025-06-24",
            "pub_date_card": {
                "ru": "24 Ğ¸ÑĞ½Ñ",
                "en": "June 24",
                "zh": "6æœˆ24æ—¥"
            },
            "hash": "0017ef0507a6b74d",
            "authors": [
                "Long Xing",
                "Qidong Huang",
                "Xiaoyi Dong",
                "Pan Zhang",
                "Yuhang Zang",
                "Yuhang Cao",
                "Jinsong Li",
                "Shuangrui Ding",
                "Weiming Zhang",
                "Nenghai Yu",
                "Jiaqi Wang",
                "Feng Wu",
                "Dahua Lin"
            ],
            "affiliations": [
                "Shanghai Artificial Intelligence Laboratory",
                "The Chinese University of Hong Kong",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.19848.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#benchmark",
                    "#hallucinations",
                    "#interpretability",
                    "#multimodal",
                    "#inference"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "ScaleCap: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹ Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼",
                    "desc": "ScaleCap - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹ Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±Ğ¾Ğ³Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¸ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºÑƒ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹. ScaleCap Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ° ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ·Ñ€ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ, ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞ¸ Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼."
                },
                "en": {
                    "title": "Enhancing Image Captions with ScaleCap: Accurate, Balanced, and Informative!",
                    "desc": "ScaleCap is a novel approach to image captioning that improves the quality of generated captions by addressing biases in large vision-language models (LVLMs). It uses heuristic question answering to generate specific questions about the image, which helps to enrich the captions with relevant details. Additionally, it employs contrastive sentence rating to identify and remove inaccuracies or hallucinations in the descriptions. By iteratively refining captions with these techniques, ScaleCap produces more accurate, balanced, and informative image descriptions."
                },
                "zh": {
                    "title": "ScaleCapï¼šæå‡å›¾åƒæè¿°çš„æ™ºèƒ½ç­–ç•¥",
                    "desc": "ScaleCapæ˜¯ä¸€ç§å¢å¼ºå›¾åƒæè¿°ç”Ÿæˆçš„ç­–ç•¥ï¼Œé€šè¿‡è¿­ä»£ä¸°å¯Œå’Œæ ¡å‡†æè¿°ï¼Œè§£å†³å¤šæ¨¡æ€å’Œè¯­è¨€åè§é—®é¢˜ï¼Œä»è€Œæé«˜å‡†ç¡®æ€§å’Œä¿¡æ¯é‡ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†å¯å‘å¼é—®ç­”å’Œå¯¹æ¯”å¥å­è¯„åˆ†ä¸¤ä¸ªæ–°ç»„ä»¶ï¼Œå‰è€…æ ¹æ®å›¾åƒç”Ÿæˆç‰¹å®šé—®é¢˜å¹¶å›ç­”ï¼Œä»¥é€æ­¥æ³¨å…¥ç›¸å…³ä¿¡æ¯ã€‚åè€…é€šè¿‡å¥å­çº§çš„å¯¹æ¯”è§£ç ï¼Œæœ‰æ•ˆè¯†åˆ«å¹¶æ¶ˆé™¤ç”±äºè¯­è¨€åè§å¯¼è‡´çš„è™šå‡æè¿°ã€‚é€šè¿‡å¢åŠ æ¨ç†é¢„ç®—ï¼ŒScaleCapèƒ½å¤Ÿç”Ÿæˆæ›´å‡†ç¡®ã€å¹³è¡¡å’Œä¿¡æ¯ä¸°å¯Œçš„å›¾åƒæè¿°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.19290",
            "title": "Skywork-SWE: Unveiling Data Scaling Laws for Software Engineering in\n  LLMs",
            "url": "https://huggingface.co/papers/2506.19290",
            "abstract": "An automated data-curation pipeline for software engineering improves large language model performance on SWE tasks, achieving state-of-the-art results with and without test-time scaling techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Software engineering (SWE) has recently emerged as a crucial testbed for next-generation LLM agents, demanding inherent capabilities in two critical dimensions: sustained iterative problem-solving (e.g., >50 interaction rounds) and long-context dependency resolution (e.g., >32k tokens). However, the data curation process in SWE remains notoriously time-consuming, as it heavily relies on manual annotation for code file filtering and the setup of dedicated runtime environments to execute and validate unit tests. Consequently, most existing datasets are limited to only a few thousand GitHub-sourced instances. To this end, we propose an incremental, automated data-curation pipeline that systematically scales both the volume and diversity of SWE datasets. Our dataset comprises 10,169 real-world Python task instances from 2,531 distinct GitHub repositories, each accompanied by a task specified in natural language and a dedicated runtime-environment image for automated unit-test validation. We have carefully curated over 8,000 successfully runtime-validated training trajectories from our proposed SWE dataset. When fine-tuning the Skywork-SWE model on these trajectories, we uncover a striking data scaling phenomenon: the trained model's performance for software engineering capabilities in LLMs continues to improve as the data size increases, showing no signs of saturation. Notably, our Skywork-SWE model achieves 38.0% pass@1 accuracy on the SWE-bench Verified benchmark without using verifiers or multiple rollouts, establishing a new state-of-the-art (SOTA) among the Qwen2.5-Coder-32B-based LLMs built on the OpenHands agent framework. Furthermore, with the incorporation of test-time scaling techniques, the performance further improves to 47.0% accuracy, surpassing the previous SOTA results for sub-32B parameter models. We release the Skywork-SWE-32B model checkpoint to accelerate future research.",
            "score": 8,
            "issue_id": 4471,
            "pub_date": "2025-06-24",
            "pub_date_card": {
                "ru": "24 Ğ¸ÑĞ½Ñ",
                "en": "June 24",
                "zh": "6æœˆ24æ—¥"
            },
            "hash": "7c26a879c3db6096",
            "authors": [
                "Liang Zeng",
                "Yongcong Li",
                "Yuzhen Xiao",
                "Changshi Li",
                "Chris Yuhao Liu",
                "Rui Yan",
                "Tianwen Wei",
                "Jujie He",
                "Xuchen Song",
                "Yang Liu",
                "Yahui Zhou"
            ],
            "affiliations": [
                "Skywork AI, Kunlun Inc"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.19290.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#dataset",
                    "#data",
                    "#long_context",
                    "#benchmark",
                    "#training"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ĞºÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ LLM Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ ĞºÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 10 000 Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Python Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğµ Ñ‚ĞµÑÑ‚Ñ‹. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Skywork-SWE Ğ½Ğ° ÑÑ‚Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑŠĞµĞ¼Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ÑÑ€ĞµĞ´Ğ¸ LLM Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ Qwen2.5-Coder-32B, ĞºĞ°Ğº Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼ Ñ‚ĞµÑ…Ğ½Ğ¸Ğº Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‚Ğ°Ğº Ğ¸ Ğ±ĞµĞ· Ğ½Ğ¸Ñ…."
                },
                "en": {
                    "title": "Automating Data Curation to Boost LLMs in Software Engineering",
                    "desc": "This paper presents an automated data-curation pipeline designed to enhance the performance of large language models (LLMs) in software engineering (SWE) tasks. The pipeline addresses the challenges of manual data annotation and environment setup by providing a diverse dataset of over 10,000 real-world Python task instances from GitHub. The authors demonstrate that increasing the dataset size leads to improved model performance, with their Skywork-SWE model achieving state-of-the-art accuracy on the SWE-bench Verified benchmark. Additionally, the incorporation of test-time scaling techniques further boosts the model's performance, establishing new benchmarks for LLMs in SWE."
                },
                "zh": {
                    "title": "è‡ªåŠ¨åŒ–æ•°æ®æ•´ç†ï¼Œæå‡è½¯ä»¶å·¥ç¨‹æ¨¡å‹è¡¨ç°",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªåŠ¨åŒ–æ•°æ®æ•´ç†ç®¡é“ï¼Œæ—¨åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è½¯ä»¶å·¥ç¨‹ï¼ˆSWEï¼‰ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚è¯¥ç®¡é“é€šè¿‡ç³»ç»Ÿæ€§åœ°æ‰©å±•æ•°æ®é›†çš„è§„æ¨¡å’Œå¤šæ ·æ€§ï¼Œè§£å†³äº†ä¼ ç»Ÿæ•°æ®æ•´ç†è¿‡ç¨‹ä¸­çš„æ—¶é—´æ¶ˆè€—é—®é¢˜ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«10,169ä¸ªçœŸå®Pythonä»»åŠ¡å®ä¾‹çš„æ•°æ®é›†ï¼Œå¹¶æˆåŠŸéªŒè¯äº†è¶…è¿‡8,000ä¸ªè®­ç»ƒè½¨è¿¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œéšç€æ•°æ®é‡çš„å¢åŠ ï¼Œæ¨¡å‹åœ¨è½¯ä»¶å·¥ç¨‹èƒ½åŠ›ä¸Šçš„è¡¨ç°æŒç»­æå‡ï¼Œæœ€ç»ˆåœ¨SWE-bench VerifiedåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†38.0%çš„å‡†ç¡®ç‡ï¼Œåˆ›é€ äº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.19767",
            "title": "SRFT: A Single-Stage Method with Supervised and Reinforcement\n  Fine-Tuning for Reasoning",
            "url": "https://huggingface.co/papers/2506.19767",
            "abstract": "Supervised Reinforcement Fine-Tuning (SRFT) integrates Supervised Fine-Tuning and Reinforcement Learning through entropy-aware weighting to achieve high accuracy in language model optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have achieved remarkable progress in reasoning tasks, yet the optimal integration of Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) remains a fundamental challenge. Through comprehensive analysis of token distributions, learning dynamics, and integration mechanisms from entropy-based perspectives, we reveal key differences between these paradigms: SFT induces coarse-grained global changes to LLM policy distributions, while RL performs fine-grained selective optimizations, with entropy serving as a critical indicator of training effectiveness. Building on these observations, we propose Supervised Reinforcement Fine-Tuning (SRFT), a single-stage method that unifies both fine-tuning paradigms through entropy-aware weighting mechanisms. Our approach simultaneously applies SFT and RL to directly optimize the LLM using demonstrations and self-exploration rollouts rather than through two-stage sequential methods. Extensive experiments show that SRFT achieves 59.1% average accuracy, outperforming zero-RL methods by 9.0% on five mathematical reasoning benchmarks and 10.9% on three out-of-distribution benchmarks.",
            "score": 5,
            "issue_id": 4471,
            "pub_date": "2025-06-24",
            "pub_date_card": {
                "ru": "24 Ğ¸ÑĞ½Ñ",
                "en": "June 24",
                "zh": "6æœˆ24æ—¥"
            },
            "hash": "dfdf02be41523939",
            "authors": [
                "Yuqian Fu",
                "Tinghong Chen",
                "Jiajun Chai",
                "Xihuai Wang",
                "Songjun Tu",
                "Guojun Yin",
                "Wei Lin",
                "Qichao Zhang",
                "Yuanheng Zhu",
                "Dongbin Zhao"
            ],
            "affiliations": [
                "Institute of Automation, Chinese Academy of Sciences",
                "Meituan",
                "School of Artificial Intelligence, University of Chinese Academy of Sciences",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.19767.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#rlhf",
                    "#training",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ supervised Ğ¸ reinforcement learning Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ - Supervised Reinforcement Fine-Tuning (SRFT). Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Supervised Fine-Tuning Ğ¸ Reinforcement Learning, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸. SRFT Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¾Ğ±Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ ÑĞ°Ğ¼Ğ¾Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SRFT Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ½Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Unifying Fine-Tuning for Superior Language Model Performance",
                    "desc": "Supervised Reinforcement Fine-Tuning (SRFT) combines Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to enhance the performance of language models. It uses entropy-aware weighting to balance the strengths of both methods, allowing for better optimization of model policies. The paper highlights how SFT makes broad changes to model behavior, while RL focuses on specific improvements, with entropy being a key measure of success. SRFT has been shown to significantly improve accuracy on various reasoning tasks compared to traditional methods."
                },
                "zh": {
                    "title": "ç›‘ç£å¼ºåŒ–å¾®è°ƒï¼šä¼˜åŒ–è¯­è¨€æ¨¡å‹çš„æ–°æ–¹æ³•",
                    "desc": "ç›‘ç£å¼ºåŒ–å¾®è°ƒï¼ˆSRFTï¼‰ç»“åˆäº†ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ï¼Œé€šè¿‡ç†µæ„ŸçŸ¥åŠ æƒå®ç°è¯­è¨€æ¨¡å‹ä¼˜åŒ–çš„é«˜å‡†ç¡®æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç›‘ç£å¾®è°ƒä¼šå¯¹è¯­è¨€æ¨¡å‹çš„ç­–ç•¥åˆ†å¸ƒäº§ç”Ÿç²—ç²’åº¦çš„å…¨å±€å˜åŒ–ï¼Œè€Œå¼ºåŒ–å­¦ä¹ åˆ™è¿›è¡Œç»†ç²’åº¦çš„é€‰æ‹©æ€§ä¼˜åŒ–ï¼Œç†µæ˜¯è®­ç»ƒæ•ˆæœçš„é‡è¦æŒ‡æ ‡ã€‚SRFTæ–¹æ³•é€šè¿‡ç†µæ„ŸçŸ¥åŠ æƒæœºåˆ¶ï¼Œå°†è¿™ä¸¤ç§å¾®è°ƒèŒƒå¼ç»Ÿä¸€ä¸ºå•é˜¶æ®µæ–¹æ³•ï¼Œç›´æ¥ä¼˜åŒ–è¯­è¨€æ¨¡å‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSRFTåœ¨äº”ä¸ªæ•°å­¦æ¨ç†åŸºå‡†ä¸Šå¹³å‡å‡†ç¡®ç‡è¾¾åˆ°59.1%ï¼Œæ¯”é›¶å¼ºåŒ–å­¦ä¹ æ–¹æ³•æé«˜äº†9.0%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.19838",
            "title": "SimpleGVR: A Simple Baseline for Latent-Cascaded Video Super-Resolution",
            "url": "https://huggingface.co/papers/2506.19838",
            "abstract": "Researchers propose design principles for cascaded video super-resolution models to improve high-resolution video generation by introduces degradation strategies, timestep sampling, noise augmentation, and interleaving temporal units with sparse local attention.  \t\t\t\t\tAI-generated summary \t\t\t\t Latent diffusion models have emerged as a leading paradigm for efficient video generation. However, as user expectations shift toward higher-resolution outputs, relying solely on latent computation becomes inadequate. A promising approach involves decoupling the process into two stages: semantic content generation and detail synthesis. The former employs a computationally intensive base model at lower resolutions, while the latter leverages a lightweight cascaded video super-resolution (VSR) model to achieve high-resolution output. In this work, we focus on studying key design principles for latter cascaded VSR models, which are underexplored currently. First, we propose two degradation strategies to generate training pairs that better mimic the output characteristics of the base model, ensuring alignment between the VSR model and its upstream generator. Second, we provide critical insights into VSR model behavior through systematic analysis of (1) timestep sampling strategies, (2) noise augmentation effects on low-resolution (LR) inputs. These findings directly inform our architectural and training innovations. Finally, we introduce interleaving temporal unit and sparse local attention to achieve efficient training and inference, drastically reducing computational overhead. Extensive experiments demonstrate the superiority of our framework over existing methods, with ablation studies confirming the efficacy of each design choice. Our work establishes a simple yet effective baseline for cascaded video super-resolution generation, offering practical insights to guide future advancements in efficient cascaded synthesis systems.",
            "score": 4,
            "issue_id": 4470,
            "pub_date": "2025-06-24",
            "pub_date_card": {
                "ru": "24 Ğ¸ÑĞ½Ñ",
                "en": "June 24",
                "zh": "6æœˆ24æ—¥"
            },
            "hash": "a00bfa00a7f0f869",
            "authors": [
                "Liangbin Xie",
                "Yu Li",
                "Shian Du",
                "Menghan Xia",
                "Xintao Wang",
                "Fanghua Yu",
                "Ziyan Chen",
                "Pengfei Wan",
                "Jiantao Zhou",
                "Chao Dong"
            ],
            "affiliations": [
                "Kuaishou Technology",
                "Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences",
                "Shenzhen University of Advanced Technology",
                "State Key Laboratory of Internet of Things for Smart City, University of Macau",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.19838.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#video",
                    "#diffusion",
                    "#architecture"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ²ĞµÑ€Ñ…Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ°ÑĞºĞ°Ğ´Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ°ÑĞºĞ°Ğ´Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ²ĞµÑ€Ñ…Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. ĞĞ½Ğ¸ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸, Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºÑƒ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ², Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ ÑˆÑƒĞ¼Ğ° Ğ¸ Ñ‡ĞµÑ€ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ±Ğ»Ğ¾ĞºĞ¾Ğ² Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼. Ğ­Ñ‚Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Video Quality with Smart Cascaded Super-Resolution",
                    "desc": "This paper presents new design principles for improving cascaded video super-resolution (VSR) models, which are essential for generating high-resolution videos. The authors introduce innovative degradation strategies and timestep sampling methods to enhance the training process, ensuring that the VSR model aligns well with the base model's output. They also explore the impact of noise augmentation on low-resolution inputs and propose techniques like interleaving temporal units and sparse local attention to optimize training efficiency. The results show that their framework outperforms existing methods, providing a solid foundation for future developments in video super-resolution."
                },
                "zh": {
                    "title": "æå‡è§†é¢‘è¶…åˆ†è¾¨ç‡ç”Ÿæˆçš„è®¾è®¡åŸåˆ™",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†çº§è”è§†é¢‘è¶…åˆ†è¾¨ç‡æ¨¡å‹çš„è®¾è®¡åŸåˆ™ï¼Œä»¥æé«˜é«˜åˆ†è¾¨ç‡è§†é¢‘ç”Ÿæˆçš„æ•ˆæœã€‚æˆ‘ä»¬å¼•å…¥äº†é€€åŒ–ç­–ç•¥ã€æ—¶é—´æ­¥é‡‡æ ·ã€å™ªå£°å¢å¼ºå’Œç¨€ç–å±€éƒ¨æ³¨æ„åŠ›ç­‰æ–¹æ³•ï¼Œæ¥ä¼˜åŒ–æ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ã€‚é€šè¿‡ç³»ç»Ÿåˆ†æï¼Œæˆ‘ä»¬æ­ç¤ºäº†æ—¶é—´æ­¥é‡‡æ ·ç­–ç•¥å’Œå™ªå£°å¢å¼ºå¯¹ä½åˆ†è¾¨ç‡è¾“å…¥çš„å½±å“ï¼Œä»è€ŒæŒ‡å¯¼æ¨¡å‹æ¶æ„å’Œè®­ç»ƒåˆ›æ–°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç°æœ‰æŠ€æœ¯ä¸­å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸ºæœªæ¥é«˜æ•ˆçº§è”åˆæˆç³»ç»Ÿçš„å‘å±•æä¾›äº†å®ç”¨çš„è§è§£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.18843",
            "title": "USAD: Universal Speech and Audio Representation via Distillation",
            "url": "https://huggingface.co/papers/2506.18843",
            "abstract": "USAD integrates diverse audio types using efficient layer-to-layer distillation from domain-specific models, achieving competitive performance across various benchmarks with a single encoder.  \t\t\t\t\tAI-generated summary \t\t\t\t Self-supervised learning (SSL) has revolutionized audio representations, yet models often remain domain-specific, focusing on either speech or non-speech tasks. In this work, we present Universal Speech and Audio Distillation (USAD), a unified approach to audio representation learning that integrates diverse audio types - speech, sound, and music - into a single model. USAD employs efficient layer-to-layer distillation from domain-specific SSL models to train a student on a comprehensive audio dataset. USAD offers competitive performance across various benchmarks and datasets, including frame and instance-level speech processing tasks, audio tagging, and sound classification, achieving near state-of-the-art results with a single encoder on SUPERB and HEAR benchmarks.",
            "score": 4,
            "issue_id": 4472,
            "pub_date": "2025-06-23",
            "pub_date_card": {
                "ru": "23 Ğ¸ÑĞ½Ñ",
                "en": "June 23",
                "zh": "6æœˆ23æ—¥"
            },
            "hash": "c7855e42be638a70",
            "authors": [
                "Heng-Jui Chang",
                "Saurabhchand Bhati",
                "James Glass",
                "Alexander H. Liu"
            ],
            "affiliations": [
                "MIT CSAIL Cambridge, MA, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.18843.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#audio"
                ],
                "emoji": "ğŸ§",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ°ÑƒĞ´Ğ¸Ğ¾",
                    "desc": "USAD - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ñ€ĞµÑ‡ÑŒ, Ğ·Ğ²ÑƒĞºĞ¸ Ğ¸ Ğ¼ÑƒĞ·Ñ‹ĞºÑƒ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ¾Ñ‚ ÑĞ»Ğ¾Ñ Ğº ÑĞ»Ğ¾Ñ Ğ¸Ğ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚Ğ° Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¼ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ. USAD Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ€ĞµÑ‡Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ¸ ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ¾Ğ², Ñ‚ĞµĞ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ·Ğ²ÑƒĞºĞ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ±Ğ»Ğ¸Ğ·ĞºĞ¸Ñ… Ğº state-of-the-art Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ĞµĞ´Ğ¸Ğ½Ğ¾Ğ³Ğ¾ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… SUPERB Ğ¸ HEAR."
                },
                "en": {
                    "title": "One Model, Many Sounds: USAD Unifies Audio Learning",
                    "desc": "The paper introduces Universal Speech and Audio Distillation (USAD), a novel method for learning audio representations that combines different audio types such as speech, sound, and music into one model. It utilizes layer-to-layer distillation from specialized self-supervised learning (SSL) models, allowing a student model to learn from a wide-ranging audio dataset. This approach enables USAD to perform well on various tasks, including speech processing, audio tagging, and sound classification. The results demonstrate that USAD achieves competitive performance on multiple benchmarks with just a single encoder, showcasing its efficiency and versatility in audio representation learning."
                },
                "zh": {
                    "title": "ç»Ÿä¸€éŸ³é¢‘è¡¨ç¤ºï¼Œæå‡å¤šä»»åŠ¡æ€§èƒ½",
                    "desc": "USADæ˜¯ä¸€ç§ç»Ÿä¸€çš„éŸ³é¢‘è¡¨ç¤ºå­¦ä¹ æ–¹æ³•ï¼Œèƒ½å¤Ÿæ•´åˆå¤šç§éŸ³é¢‘ç±»å‹ï¼ŒåŒ…æ‹¬è¯­éŸ³ã€å£°éŸ³å’ŒéŸ³ä¹ã€‚å®ƒé€šè¿‡ä»ç‰¹å®šé¢†åŸŸçš„è‡ªç›‘ç£å­¦ä¹ æ¨¡å‹ä¸­è¿›è¡Œé«˜æ•ˆçš„å±‚é—´è’¸é¦ï¼Œè®­ç»ƒä¸€ä¸ªå­¦ç”Ÿæ¨¡å‹ï¼Œä»¥ä¾¿åœ¨ä¸€ä¸ªå…¨é¢çš„éŸ³é¢‘æ•°æ®é›†ä¸Šè¿›è¡Œå­¦ä¹ ã€‚USADåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼ŒåŒ…æ‹¬è¯­éŸ³å¤„ç†ã€éŸ³é¢‘æ ‡è®°å’Œå£°éŸ³åˆ†ç±»ä»»åŠ¡ã€‚è¯¥æ–¹æ³•ä½¿ç”¨å•ä¸€ç¼–ç å™¨ï¼Œè¾¾åˆ°äº†æ¥è¿‘æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.19807",
            "title": "KnowRL: Exploring Knowledgeable Reinforcement Learning for Factuality",
            "url": "https://huggingface.co/papers/2506.19807",
            "abstract": "KnowRL, a knowledge-enhanced reinforcement learning approach, reduces hallucinations in slow-thinking large language models by incorporating factuality rewards based on knowledge verification during training.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs), particularly slow-thinking models, often exhibit severe hallucination, outputting incorrect content due to an inability to accurately recognize knowledge boundaries during reasoning. While Reinforcement Learning (RL) can enhance complex reasoning abilities, its outcome-oriented reward mechanism often lacks factual supervision over the thinking process, further exacerbating the hallucination problem. To address the high hallucination in slow-thinking models, we propose Knowledge-enhanced RL, KnowRL. KnowRL guides models to perform fact-based slow thinking by integrating a factuality reward, based on knowledge verification, into the RL training process, helping them recognize their knowledge boundaries. KnowRL guides models to perform fact-based slow thinking by integrating a factuality reward, based on knowledge verification, into the RL training process, helping them recognize their knowledge boundaries. This targeted factual input during RL training enables the model to learn and internalize fact-based reasoning strategies. By directly rewarding adherence to facts within the reasoning steps, KnowRL fosters a more reliable thinking process. Experimental results on three hallucination evaluation datasets and two reasoning evaluation datasets demonstrate that KnowRL effectively mitigates hallucinations in slow-thinking models while maintaining their original strong reasoning capabilities. Our code is available at https://github.com/zjunlp/KnowRL.",
            "score": 3,
            "issue_id": 4472,
            "pub_date": "2025-06-24",
            "pub_date_card": {
                "ru": "24 Ğ¸ÑĞ½Ñ",
                "en": "June 24",
                "zh": "6æœˆ24æ—¥"
            },
            "hash": "2f1016b014bf6ee5",
            "authors": [
                "Baochang Ren",
                "Shuofei Qiao",
                "Wenhao Yu",
                "Huajun Chen",
                "Ningyu Zhang"
            ],
            "affiliations": [
                "Tencent AI Seattle Lab",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.19807.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#rl",
                    "#rlhf",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¤Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ˜Ğ˜: KnowRL Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹",
                    "desc": "KnowRL - ÑÑ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¹ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾ Ğ¼Ñ‹ÑĞ»ÑÑ‰Ğ¸Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ·Ğ° Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. KnowRL Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ° Ñ„Ğ°ĞºÑ‚Ğ°Ñ…, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ·Ğ° Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ KnowRL ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾ Ğ¼Ñ‹ÑĞ»ÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¸Ñ… Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Truthfulness in AI with KnowRL",
                    "desc": "KnowRL is a novel approach that enhances reinforcement learning by incorporating factuality rewards to reduce hallucinations in slow-thinking large language models. These models often generate incorrect information due to their inability to recognize the limits of their knowledge. By integrating knowledge verification into the training process, KnowRL encourages models to engage in fact-based reasoning. Experimental results show that this method effectively decreases hallucinations while preserving the models' reasoning abilities."
                },
                "zh": {
                    "title": "çŸ¥è¯†å¢å¼ºçš„å¼ºåŒ–å­¦ä¹ ï¼Œå‡å°‘å¹»è§‰ç°è±¡",
                    "desc": "KnowRLæ˜¯ä¸€ç§å¢å¼ºçŸ¥è¯†çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥åŸºäºçŸ¥è¯†éªŒè¯çš„äº‹å®å¥–åŠ±ï¼Œå‡å°‘äº†æ…¢æ€ç»´å¤§å‹è¯­è¨€æ¨¡å‹çš„å¹»è§‰ç°è±¡ã€‚æ…¢æ€ç»´æ¨¡å‹å¸¸å¸¸å› ä¸ºæ— æ³•å‡†ç¡®è¯†åˆ«çŸ¥è¯†è¾¹ç•Œè€Œè¾“å‡ºé”™è¯¯å†…å®¹ï¼Œå¯¼è‡´ä¸¥é‡çš„å¹»è§‰é—®é¢˜ã€‚KnowRLé€šè¿‡å°†äº‹å®å¥–åŠ±æ•´åˆåˆ°å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸­ï¼ŒæŒ‡å¯¼æ¨¡å‹è¿›è¡ŒåŸºäºäº‹å®çš„æ…¢æ€ç»´ï¼Œä»è€Œå¸®åŠ©å®ƒä»¬è¯†åˆ«çŸ¥è¯†è¾¹ç•Œã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒKnowRLåœ¨å‡è½»å¹»è§‰çš„åŒæ—¶ï¼Œä¿æŒäº†æ¨¡å‹çš„å¼ºå¤§æ¨ç†èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.19794",
            "title": "Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic\n  Empirical Study",
            "url": "https://huggingface.co/papers/2506.19794",
            "abstract": "Enhancements to open-source large language models' data analysis capabilities through strategic planning, interaction design, and data quality improvements were identified and applied.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) hold promise in automating data analysis tasks, yet open-source models face significant limitations in these kinds of reasoning-intensive scenarios. In this work, we investigate strategies to enhance the data analysis capabilities of open-source LLMs. By curating a seed dataset of diverse, realistic scenarios, we evaluate models across three dimensions: data understanding, code generation, and strategic planning. Our analysis reveals three key findings: (1) Strategic planning quality serves as the primary determinant of model performance; (2) Interaction design and task complexity significantly influence reasoning capabilities; (3) Data quality demonstrates a greater impact than diversity in achieving optimal performance. We leverage these insights to develop a data synthesis methodology, demonstrating significant improvements in open-source LLMs' analytical reasoning capabilities.",
            "score": 3,
            "issue_id": 4472,
            "pub_date": "2025-06-24",
            "pub_date_card": {
                "ru": "24 Ğ¸ÑĞ½Ñ",
                "en": "June 24",
                "zh": "6æœˆ24æ—¥"
            },
            "hash": "590e2169fc8a24c8",
            "authors": [
                "Yuqi Zhu",
                "Yi Zhong",
                "Jintian Zhang",
                "Ziheng Zhang",
                "Shuofei Qiao",
                "Yujie Luo",
                "Lun Du",
                "Da Zheng",
                "Huajun Chen",
                "Ningyu Zhang"
            ],
            "affiliations": [
                "Ant Group",
                "Zhejiang University",
                "Zhejiang University - Ant Group Joint Laboratory of Knowledge Graph"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.19794.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#multimodal",
                    "#benchmark",
                    "#reasoning",
                    "#open_source",
                    "#dataset",
                    "#data"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "Ğ¡Ñ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ - ĞºĞ»ÑÑ‡ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñƒ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… LLM",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñƒ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ Ñ‚Ñ€ĞµĞ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼: Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. Ğ‘Ñ‹Ğ»Ğ¾ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… LLM."
                },
                "en": {
                    "title": "Boosting Open-Source LLMs for Better Data Analysis",
                    "desc": "This paper explores how to improve the data analysis abilities of open-source Large Language Models (LLMs). It identifies that strategic planning is crucial for enhancing model performance in reasoning tasks. The study also highlights the importance of interaction design and task complexity, as well as the role of data quality over diversity in achieving better results. By applying these findings, the authors propose a new data synthesis method that significantly boosts the analytical reasoning skills of LLMs."
                },
                "zh": {
                    "title": "æå‡å¼€æºLLMsçš„æ•°æ®åˆ†æèƒ½åŠ›",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†å¦‚ä½•æå‡å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•°æ®åˆ†æä»»åŠ¡ä¸­çš„èƒ½åŠ›ã€‚æˆ‘ä»¬é€šè¿‡ç­–åˆ’å¤šæ ·åŒ–çš„çœŸå®åœºæ™¯æ•°æ®é›†ï¼Œè¯„ä¼°æ¨¡å‹åœ¨æ•°æ®ç†è§£ã€ä»£ç ç”Ÿæˆå’Œæˆ˜ç•¥è§„åˆ’ç­‰ä¸‰ä¸ªç»´åº¦çš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼Œæˆ˜ç•¥è§„åˆ’çš„è´¨é‡æ˜¯æ¨¡å‹æ€§èƒ½çš„ä¸»è¦å†³å®šå› ç´ ï¼Œäº¤äº’è®¾è®¡å’Œä»»åŠ¡å¤æ‚æ€§ä¹Ÿæ˜¾è‘—å½±å“æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæ•°æ®è´¨é‡å¯¹å®ç°æœ€ä½³æ€§èƒ½çš„å½±å“å¤§äºæ•°æ®çš„å¤šæ ·æ€§ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ•°æ®åˆæˆæ–¹æ³•ï¼Œæ˜¾è‘—æå‡äº†å¼€æºLLMsçš„åˆ†ææ¨ç†èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.17612",
            "title": "JarvisArt: Liberating Human Artistic Creativity via an Intelligent Photo\n  Retouching Agent",
            "url": "https://huggingface.co/papers/2506.17612",
            "abstract": "JarvisArt, an MLLM-driven agent, achieves superior photo retouching by understanding user intent and coordinating multiple retouching tools in Lightroom, outperforming GPT-4o on a novel benchmark.  \t\t\t\t\tAI-generated summary \t\t\t\t Photo retouching has become integral to contemporary visual storytelling, enabling users to capture aesthetics and express creativity. While professional tools such as Adobe Lightroom offer powerful capabilities, they demand substantial expertise and manual effort. In contrast, existing AI-based solutions provide automation but often suffer from limited adjustability and poor generalization, failing to meet diverse and personalized editing needs. To bridge this gap, we introduce JarvisArt, a multi-modal large language model (MLLM)-driven agent that understands user intent, mimics the reasoning process of professional artists, and intelligently coordinates over 200 retouching tools within Lightroom. JarvisArt undergoes a two-stage training process: an initial Chain-of-Thought supervised fine-tuning to establish basic reasoning and tool-use skills, followed by Group Relative Policy Optimization for Retouching (GRPO-R) to further enhance its decision-making and tool proficiency. We also propose the Agent-to-Lightroom Protocol to facilitate seamless integration with Lightroom. To evaluate performance, we develop MMArt-Bench, a novel benchmark constructed from real-world user edits. JarvisArt demonstrates user-friendly interaction, superior generalization, and fine-grained control over both global and local adjustments, paving a new avenue for intelligent photo retouching. Notably, it outperforms GPT-4o with a 60% improvement in average pixel-level metrics on MMArt-Bench for content fidelity, while maintaining comparable instruction-following capabilities. Project Page: https://jarvisart.vercel.app/.",
            "score": 1,
            "issue_id": 4472,
            "pub_date": "2025-06-21",
            "pub_date_card": {
                "ru": "21 Ğ¸ÑĞ½Ñ",
                "en": "June 21",
                "zh": "6æœˆ21æ—¥"
            },
            "hash": "2504aa6b996e0739",
            "authors": [
                "Yunlong Lin",
                "Zixu Lin",
                "Kunjie Lin",
                "Jinbin Bai",
                "Panwang Pan",
                "Chenxin Li",
                "Haoyu Chen",
                "Zhongdao Wang",
                "Xinghao Ding",
                "Wenbo Li",
                "Shuicheng Yan"
            ],
            "affiliations": [
                "Bytedance",
                "Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, Xiamen, Fujian, China",
                "National University of Singapore",
                "The Chinese University of Hong Kong",
                "The Hong Kong University of Science and Technology (Guangzhou)",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.17612.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#games",
                    "#reasoning",
                    "#optimization",
                    "#agents",
                    "#cv",
                    "#training"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "JarvisArt: Ğ˜Ğ˜-Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸Ğº Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€ĞµÑ‚ÑƒÑˆĞ¸ Ñ„Ğ¾Ñ‚Ğ¾",
                    "desc": "JarvisArt - ÑÑ‚Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (ĞœLLM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€ĞµÑ‚ÑƒÑˆÑŒ Ñ„Ğ¾Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ğ¹, Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ¸ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ¸Ñ€ÑƒÑ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Lightroom. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° fine-tuning Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Group Relative Policy Optimization for Retouching (GRPO-R). JarvisArt Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ GPT-4o Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ MMArt-Bench, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 60% Ğ² Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ñ… Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Revolutionizing Photo Retouching with Intelligent AI",
                    "desc": "JarvisArt is an advanced multi-modal large language model (MLLM) agent designed for photo retouching, which excels in understanding user intent and effectively coordinating over 200 tools in Adobe Lightroom. Unlike traditional AI solutions that lack flexibility, JarvisArt mimics the reasoning of professional artists, allowing for personalized and nuanced editing. It employs a two-stage training process that enhances its decision-making abilities and tool proficiency, ensuring high-quality results. The performance of JarvisArt is validated through a new benchmark, MMArt-Bench, where it significantly outperforms existing models like GPT-4o in pixel-level metrics for content fidelity."
                },
                "zh": {
                    "title": "æ™ºèƒ½ä¿®é¥°ï¼Œè¶…è¶Šä¼ ç»Ÿï¼",
                    "desc": "JarvisArt æ˜¯ä¸€ä¸ªåŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„æ™ºèƒ½ä»£ç†ï¼Œä¸“æ³¨äºç…§ç‰‡ä¿®é¥°ã€‚å®ƒèƒ½å¤Ÿç†è§£ç”¨æˆ·æ„å›¾ï¼Œå¹¶åè°ƒ Lightroom ä¸­çš„200å¤šç§ä¿®é¥°å·¥å…·ï¼Œæä¾›æ›´ä¼˜è´¨çš„ä¿®é¥°æ•ˆæœã€‚é€šè¿‡ä¸¤é˜¶æ®µçš„è®­ç»ƒè¿‡ç¨‹ï¼ŒJarvisArt æå‡äº†å†³ç­–èƒ½åŠ›å’Œå·¥å…·ä½¿ç”¨ç†Ÿç»ƒåº¦ï¼Œèƒ½å¤Ÿæ»¡è¶³ç”¨æˆ·ä¸ªæ€§åŒ–çš„ç¼–è¾‘éœ€æ±‚ã€‚ä¸ç°æœ‰çš„ AI è§£å†³æ–¹æ¡ˆç›¸æ¯”ï¼ŒJarvisArt åœ¨å†…å®¹ä¿çœŸåº¦ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå¹³å‡åƒç´ çº§æŒ‡æ ‡æé«˜äº†60%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.14012",
            "title": "Lost in the Mix: Evaluating LLM Understanding of Code-Switched Text",
            "url": "https://huggingface.co/papers/2506.14012",
            "abstract": "LLMs' comprehension and reasoning skills are evaluated under code-switching conditions, revealing that embedding English into other languages can improve understanding, while prompts and fine-tuning affect degradation mitigation differently.  \t\t\t\t\tAI-generated summary \t\t\t\t Code-switching (CSW) is the act of alternating between two or more languages within a single discourse. This phenomenon is widespread in multilingual communities, and increasingly prevalent in online content, where users naturally mix languages in everyday communication. As a result, Large Language Models (LLMs), now central to content processing and generation, are frequently exposed to code-switched inputs. Given their widespread use, it is crucial to understand how LLMs process and reason about such mixed-language text. This paper presents a systematic evaluation of LLM comprehension under code-switching by generating CSW variants of established reasoning and comprehension benchmarks. While degradation is evident when foreign tokens disrupt English textx2013even under linguistic constraintsx2013embedding English into other languages often improves comprehension. Though prompting yields mixed results, fine-tuning offers a more stable path to degradation mitigation.",
            "score": 1,
            "issue_id": 4474,
            "pub_date": "2025-06-16",
            "pub_date_card": {
                "ru": "16 Ğ¸ÑĞ½Ñ",
                "en": "June 16",
                "zh": "6æœˆ16æ—¥"
            },
            "hash": "d8a7231ee69205ba",
            "authors": [
                "Amr Mohamed",
                "Yang Zhang",
                "Michalis Vazirgiannis",
                "Guokan Shang"
            ],
            "affiliations": [
                "Ecole Polytechnique",
                "MBZUAI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.14012.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#low_resource",
                    "#multilingual",
                    "#long_context"
                ],
                "emoji": "ğŸ”€",
                "ru": {
                    "title": "Ğ¯Ğ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ: Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¼ĞµÑˆĞµĞ½Ğ¸Ğµ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ¾Ğ². Ğ’Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ² Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ. ĞĞ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ¸ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ½Ğ° ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ¾Ğ½ĞºĞ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğº ÑĞ¼ÑĞ³Ñ‡ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑÑ„Ñ„ĞµĞºÑ‚Ğ¾Ğ² Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing LLM Comprehension through Code-Switching",
                    "desc": "This paper investigates how Large Language Models (LLMs) understand and reason when faced with code-switching, which is the mixing of languages in communication. The study finds that including English words within other languages can enhance the models' comprehension, despite some degradation when foreign words disrupt English text. It also examines the effects of different prompting techniques and highlights that fine-tuning the models provides a more reliable way to reduce comprehension issues. Overall, the research emphasizes the importance of understanding LLM performance in multilingual contexts, especially as code-switching becomes more common in digital communication."
                },
                "zh": {
                    "title": "ä»£ç åˆ‡æ¢æå‡ç†è§£èƒ½åŠ›çš„ç ”ç©¶",
                    "desc": "æœ¬æ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä»£ç åˆ‡æ¢æ¡ä»¶ä¸‹çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚ä»£ç åˆ‡æ¢æ˜¯æŒ‡åœ¨åŒä¸€äº¤æµä¸­äº¤æ›¿ä½¿ç”¨ä¸¤ç§æˆ–å¤šç§è¯­è¨€ï¼Œè¿™åœ¨å¤šè¯­è¨€ç¤¾åŒºä¸­éå¸¸æ™®éã€‚ç ”ç©¶å‘ç°ï¼Œå°†è‹±è¯­åµŒå…¥å…¶ä»–è¯­è¨€ä¸­å¯ä»¥æé«˜ç†è§£èƒ½åŠ›ï¼Œè€Œæç¤ºå’Œå¾®è°ƒå¯¹å‡è½»æ€§èƒ½ä¸‹é™çš„å½±å“åˆ™æœ‰æ‰€ä¸åŒã€‚æ€»ä½“è€Œè¨€ï¼Œå¾®è°ƒæä¾›äº†ä¸€ç§æ›´ç¨³å®šçš„æ–¹å¼æ¥åº”å¯¹è¯­è¨€æ··åˆå¸¦æ¥çš„æŒ‘æˆ˜ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-06-24.html",
    "link_next": "2025-06-26.html",
    "link_month": "2025-06.html",
    "short_date_prev": {
        "ru": "24.06",
        "en": "06/24",
        "zh": "6æœˆ24æ—¥"
    },
    "short_date_next": {
        "ru": "26.06",
        "en": "06/26",
        "zh": "6æœˆ26æ—¥"
    },
    "categories": {
        "#dataset": 3,
        "#data": 2,
        "#benchmark": 6,
        "#agents": 1,
        "#cv": 2,
        "#rl": 3,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 1,
        "#audio": 1,
        "#video": 2,
        "#multimodal": 4,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 6,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 2,
        "#reasoning": 6,
        "#transfer_learning": 2,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 5,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 2,
        "#long_context": 2,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 1
    }
}