{
    "date": {
        "ru": "3 октября",
        "en": "October 3",
        "zh": "10月3日"
    },
    "time_utc": "2025-10-03 00:49",
    "weekday": 4,
    "issue_id": 6220,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2509.25454",
            "title": "DeepSearch: Overcome the Bottleneck of Reinforcement Learning with\n  Verifiable Rewards via Monte Carlo Tree Search",
            "url": "https://huggingface.co/papers/2509.25454",
            "abstract": "DeepSearch integrates Monte Carlo Tree Search into RLVR training to enhance exploration and credit assignment, achieving state-of-the-art performance with reduced computational cost.  \t\t\t\t\tAI-generated summary \t\t\t\t Although RLVR has become an essential component for developing advanced reasoning skills in LLMs, contemporary studies have documented training plateaus that emerge following thousands of optimization steps, demonstrating notable decreases in performance gains despite increased computational investment. This limitation stems from the sparse exploration patterns inherent in current RLVR practices, where models rely on limited rollouts that often miss critical reasoning paths and fail to provide systematic coverage of the solution space. We present DeepSearch, a framework that integrates Monte Carlo Tree Search directly into RLVR training. In contrast to existing methods that rely on tree search only at inference, DeepSearch embeds structured search into the training loop, enabling systematic exploration and fine-grained credit assignment across reasoning steps. Through training-time exploration, DeepSearch addresses the fundamental bottleneck of insufficient exploration, which leads to diminishing performance improvements over prolonged training steps. Our contributions include: (1) a global frontier selection strategy that prioritizes promising nodes across the search tree, (2) selection with entropy-based guidance that identifies confident paths for supervision, and (3) adaptive replay buffer training with solution caching for efficiency. Experiments on mathematical reasoning benchmarks show that DeepSearch achieves 62.95% average accuracy and establishes a new state-of-the-art for 1.5B reasoning models - using 5.7x fewer GPU hours than extended training approaches. These results highlight the importance of strategic exploration over brute-force scaling and demonstrate the promise of algorithmic innovation for advancing RLVR methodologies. DeepSearch establishes a new direction for scaling reasoning capabilities through systematic search rather than prolonged computation.",
            "score": 96,
            "issue_id": 6201,
            "pub_date": "2025-09-29",
            "pub_date_card": {
                "ru": "29 сентября",
                "en": "September 29",
                "zh": "9月29日"
            },
            "hash": "c81bb4fe47e669e8",
            "authors": [
                "Fang Wu",
                "Weihao Xuan",
                "Heli Qi",
                "Ximing Lu",
                "Aaron Tu",
                "Li Erran Li",
                "Yejin Choi"
            ],
            "affiliations": [
                "Amazon AWS",
                "RIKEN AIP",
                "Stanford University",
                "UC Berkeley",
                "University of Tokyo",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.25454.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#reasoning",
                    "#optimization",
                    "#rl",
                    "#training"
                ],
                "emoji": "🌳",
                "ru": {
                    "title": "Систематический поиск вместо грубой силы: древовидное исследование для обучения LLM",
                    "desc": "DeepSearch интегрирует Monte Carlo Tree Search непосредственно в процесс обучения с подкреплением (RLVR) для улучшения исследования пространства решений и распределения кредита. Метод решает проблему плато в обучении, возникающую из-за недостаточного исследования возможных путей рассуждений в существующих подходах RLVR. DeepSearch использует глобальную стратегию выбора перспективных узлов, энтропийное руководство для выбора уверенных путей и адаптивный буфер воспроизведения с кэшированием решений. На бенчмарках математических рассуждений модель достигла точности 62.95% для моделей размером 1.5B параметров, используя в 5.7 раз меньше вычислительных ресурсов по сравнению с продолжительным обучением."
                },
                "en": {
                    "title": "Revolutionizing RLVR: Strategic Exploration with DeepSearch",
                    "desc": "DeepSearch is a novel framework that enhances Reinforcement Learning with Value Regression (RLVR) by incorporating Monte Carlo Tree Search (MCTS) into the training process. This integration allows for better exploration of the solution space and improves credit assignment, addressing the common issue of training plateaus in existing RLVR methods. By employing a global frontier selection strategy and entropy-based guidance, DeepSearch systematically identifies and prioritizes promising reasoning paths. The framework achieves state-of-the-art performance on mathematical reasoning tasks while significantly reducing computational costs, demonstrating the effectiveness of strategic exploration in machine learning."
                },
                "zh": {
                    "title": "深度搜索：通过系统探索提升推理能力",
                    "desc": "DeepSearch 是一种将蒙特卡洛树搜索（MCTS）集成到强化学习价值回归（RLVR）训练中的框架，旨在增强探索能力和信用分配。当前的 RLVR 方法在训练过程中存在探索不足的问题，导致性能提升减缓。DeepSearch 通过在训练循环中嵌入结构化搜索，系统性地探索解决方案空间，从而解决了这一瓶颈。实验结果表明，DeepSearch 在数学推理基准测试中取得了 62.95% 的平均准确率，并且使用的 GPU 计算时间比传统方法少了 5.7 倍，展示了算法创新在提升 RLVR 方法中的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.01051",
            "title": "GEM: A Gym for Agentic LLMs",
            "url": "https://huggingface.co/papers/2510.01051",
            "abstract": "GEM, an open-source environment simulator, facilitates experience-based learning for large language models by providing a standardized framework and diverse environments for training and benchmarking reinforcement learning algorithms.  \t\t\t\t\tAI-generated summary \t\t\t\t The training paradigm for large language models (LLMs) is moving from static datasets to experience-based learning, where agents acquire skills via interacting with complex environments. To facilitate this transition we introduce GEM (General Experience Maker), an open-source environment simulator designed for the age of LLMs. Analogous to OpenAI-Gym for traditional reinforcement learning (RL), GEM provides a standardized framework for the environment-agent interface, including asynchronous vectorized execution for high throughput, and flexible wrappers for easy extensibility. GEM also features a diverse suite of environments, robust integrated tools, and single-file example scripts demonstrating using GEM with five popular RL training frameworks. Along with this, we also provide a set of baselines across 24 environments using REINFORCE with Return Batch Normalization (ReBN), which -- unlike GRPO -- is compatible with the full RL setting of dense per-turn rewards and offers better credit assignment. We further conduct apple-to-apple benchmarking of PPO, GRPO and REINFORCE in both single- and multi-turn settings using GEM to shed light on the algorithmic designs. Lastly, GEM also functions as a convenient evaluation toolkit besides a training environment. We hope this framework can help accelerate future agentic LLM research.",
            "score": 52,
            "issue_id": 6198,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 октября",
                "en": "October 1",
                "zh": "10月1日"
            },
            "hash": "6d69bb75ee0b2258",
            "authors": [
                "Zichen Liu",
                "Anya Sims",
                "Keyu Duan",
                "Changyu Chen",
                "Simon Yu",
                "Xiangxin Zhou",
                "Haotian Xu",
                "Shaopan Xiong",
                "Bo Liu",
                "Chenmien Tan",
                "Chuen Yang Beh",
                "Weixun Wang",
                "Hao Zhu",
                "Weiyan Shi",
                "Diyi Yang",
                "Michael Shieh",
                "Yee Whye Teh",
                "Wee Sun Lee",
                "Min Lin"
            ],
            "affiliations": [
                "NUS",
                "Northeastern",
                "OpenRLHF",
                "Oxford",
                "RL2",
                "ROLL",
                "SMU",
                "Sea AI Lab",
                "Stanford"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.01051.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#open_source",
                    "#benchmark",
                    "#training",
                    "#games",
                    "#agents"
                ],
                "emoji": "🎮",
                "ru": {
                    "title": "GEM: спортзал для тренировки LLM-агентов через reinforcement learning",
                    "desc": "В статье представлен GEM (General Experience Maker) — открытая среда-симулятор для обучения больших языковых моделей через взаимодействие с окружением. Это аналог OpenAI Gym, но специально разработанный для эпохи LLM, предоставляющий стандартизированный интерфейс между агентом и средой с поддержкой асинхронного векторизованного выполнения. GEM включает 24 разнообразные среды и базовые результаты с использованием алгоритма REINFORCE с Return Batch Normalization (ReBN), который лучше справляется с credit assignment по сравнению с GRPO. Авторы также проводят сравнительный анализ популярных RL-алгоритмов (PPO, GRPO, REINFORCE) и позиционируют GEM как инструмент для ускорения исследований агентных LLM."
                },
                "en": {
                    "title": "GEM: Empowering LLMs with Experience-Based Learning",
                    "desc": "GEM (General Experience Maker) is an open-source simulator designed to enhance experience-based learning for large language models (LLMs) by providing a standardized framework for training and benchmarking reinforcement learning (RL) algorithms. It allows agents to learn by interacting with various complex environments, moving away from static datasets. GEM includes features like asynchronous vectorized execution for efficient processing and flexible wrappers for easy customization. Additionally, it offers a suite of environments and tools for evaluating different RL algorithms, aiming to accelerate research in agentic LLMs."
                },
                "zh": {
                    "title": "GEM：加速大型语言模型的经验学习",
                    "desc": "GEM（通用经验生成器）是一个开源环境模拟器，旨在为大型语言模型提供基于经验的学习体验。它为强化学习算法的训练和基准测试提供了标准化框架和多样化环境，类似于传统强化学习中的OpenAI-Gym。GEM支持异步向量化执行，具有高吞吐量，并提供灵活的包装器以便于扩展。此外，GEM还包含多种环境、强大的集成工具和示例脚本，帮助研究人员加速未来的智能体语言模型研究。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.00406",
            "title": "VLA-RFT: Vision-Language-Action Reinforcement Fine-tuning with Verified\n  Rewards in World Simulators",
            "url": "https://huggingface.co/papers/2510.00406",
            "abstract": "VLA-RFT uses a data-driven world model to fine-tune VLA models efficiently, reducing sample requirements and improving robustness under perturbations.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language-Action (VLA) models enable embodied decision-making but rely heavily on imitation learning, leading to compounding errors and poor robustness under distribution shift. Reinforcement learning (RL) can mitigate these issues yet typically demands costly real-world interactions or suffers from sim-to-real gaps. We introduce VLA-RFT, a reinforcement fine-tuning framework that leverages a data-driven world model as a controllable simulator. Trained from real interaction data, the simulator predicts future visual observations conditioned on actions, allowing policy rollouts with dense, trajectory-level rewards derived from goal-achieving references. This design delivers an efficient and action-aligned learning signal, drastically lowering sample requirements. With fewer than 400 fine-tuning steps, VLA-RFT surpasses strong supervised baselines and achieves greater efficiency than simulator-based RL. Moreover, it exhibits strong robustness under perturbed conditions, sustaining stable task execution. Our results establish world-model-based RFT as a practical post-training paradigm to enhance the generalization and robustness of VLA models. For more details, please refer to https://vla-rft.github.io/.",
            "score": 52,
            "issue_id": 6198,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 октября",
                "en": "October 1",
                "zh": "10月1日"
            },
            "hash": "8dead5a43b0cdc61",
            "authors": [
                "Hengtao Li",
                "Pengxiang Ding",
                "Runze Suo",
                "Yihao Wang",
                "Zirui Ge",
                "Dongyuan Zang",
                "Kexian Yu",
                "Mingyang Sun",
                "Hongyin Zhang",
                "Donglin Wang",
                "Weihua Su"
            ],
            "affiliations": [
                "BUPT",
                "Fudan University",
                "Hebei University of Technology",
                "OpenHelix Team",
                "Westlake University",
                "Zhejiang University",
                "Zhengzhou University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.00406.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#reasoning",
                    "#training",
                    "#optimization",
                    "#agents"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Обучение робота через мир-симулятор: эффективно и надёжно",
                    "desc": "Статья представляет VLA-RFT — метод дообучения Vision-Language-Action моделей с помощью reinforcement learning. Вместо дорогостоящих взаимодействий с реальным миром используется data-driven world model, которая предсказывает будущие визуальные наблюдения на основе действий агента. Метод требует менее 400 шагов fine-tuning для достижения результатов лучше, чем supervised baseline, и показывает высокую устойчивость к возмущениям. VLA-RFT решает проблему накопления ошибок в imitation learning и предлагает практичный подход к улучшению генерализации роботизированных политик."
                },
                "en": {
                    "title": "Enhancing VLA Models with Efficient Reinforcement Fine-Tuning",
                    "desc": "VLA-RFT is a framework that improves Vision-Language-Action (VLA) models by using a data-driven world model for reinforcement fine-tuning. This approach reduces the number of samples needed for training and enhances the model's ability to handle unexpected changes in the environment. By simulating future visual observations based on actions, it provides a more effective learning signal that aligns with the desired outcomes. The results show that VLA-RFT not only outperforms traditional supervised methods but also maintains strong performance even when conditions are altered."
                },
                "zh": {
                    "title": "利用世界模型提升VLA模型的鲁棒性与效率",
                    "desc": "VLA-RFT是一种强化学习微调框架，利用数据驱动的世界模型作为可控模拟器，从而提高VLA模型的效率。该框架通过真实交互数据训练，能够预测基于动作的未来视觉观察，提供密集的轨迹级奖励信号。与传统的监督学习方法相比，VLA-RFT在样本需求上大幅降低，且在少于400步的微调后超越了强大的基线模型。它在扰动条件下表现出强大的鲁棒性，确保任务执行的稳定性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.25849",
            "title": "Knapsack RL: Unlocking Exploration of LLMs via Optimizing Budget\n  Allocation",
            "url": "https://huggingface.co/papers/2509.25849",
            "abstract": "An adaptive exploration budget allocation method for reinforcement learning in Large Language Models improves training efficiency and performance on mathematical reasoning benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) can self-improve through reinforcement learning, where they generate trajectories to explore and discover better solutions. However, this exploration process is computationally expensive, often forcing current methods to assign limited exploration budgets to each task. This uniform allocation creates problematic edge cases: easy tasks consistently succeed while difficult tasks consistently fail, both producing zero gradients during training updates for the widely used Group Relative Policy Optimization (GRPO). We address this problem from the lens of exploration budget allocation. Viewing each task's exploration as an \"item\" with a distinct \"value\" and \"cost\", we establish a connection to the classical knapsack problem. This formulation allows us to derive an optimal assignment rule that adaptively distributes resources based on the model's current learning status. When applied to GRPO, our method increases the effective ratio of non-zero policy gradients by 20-40% during training. Acting as a computational \"free lunch\", our approach could reallocate exploration budgets from tasks where learning is saturated to those where it is most impactful. This enables significantly larger budgets (e.g., 93 rollouts) for especially challenging problems, which would be computationally prohibitive under a uniform allocation. These improvements translate to meaningful gains on mathematical reasoning benchmarks, with average improvements of 2-4 points and peak gains of 9 points on specific tasks. Notably, achieving comparable performance with traditional homogeneous allocation would require about 2x the computational resources.",
            "score": 30,
            "issue_id": 6201,
            "pub_date": "2025-09-30",
            "pub_date_card": {
                "ru": "30 сентября",
                "en": "September 30",
                "zh": "9月30日"
            },
            "hash": "7897a4d3a9007e61",
            "authors": [
                "Ziniu Li",
                "Congliang Chen",
                "Tianyun Yang",
                "Tian Ding",
                "Ruoyu Sun",
                "Ge Zhang",
                "Wenhao Huang",
                "Zhi-Quan Luo"
            ],
            "affiliations": [
                "ByteDance Seed",
                "Shenzhen Research Institute of Big Data",
                "The Chinese University of Hong Kong, Shenzhen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.25849.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#reasoning",
                    "#optimization",
                    "#rl",
                    "#training"
                ],
                "emoji": "🎒",
                "ru": {
                    "title": "Умное распределение вычислений: больше внимания сложным задачам",
                    "desc": "Исследователи предложили адаптивный метод распределения вычислительного бюджета для обучения LLM с подкреплением. Вместо равномерного выделения ресурсов на все задачи, алгоритм динамически распределяет их в зависимости от сложности: простые задачи получают меньше попыток, сложные — больше. Подход формализован как классическая задача о рюкзаке, где каждая задача имеет свою «ценность» и «стоимость». Метод повышает долю ненулевых градиентов на 20-40% и улучшает результаты на математических бенчмарках на 2-9 баллов при тех же вычислительных затратах."
                },
                "en": {
                    "title": "Smart Budgeting for Smarter Learning",
                    "desc": "This paper presents a new method for allocating exploration budgets in reinforcement learning, specifically for Large Language Models (LLMs). The authors identify that traditional uniform budget allocation leads to inefficiencies, where easy tasks succeed while difficult ones fail, resulting in zero gradients during training. By framing the exploration budget allocation as a knapsack problem, they develop an adaptive strategy that optimally distributes resources based on the learning status of each task. This approach significantly enhances training efficiency, increasing non-zero policy gradients by 20-40% and improving performance on mathematical reasoning tasks without requiring additional computational resources."
                },
                "zh": {
                    "title": "自适应探索预算，提升强化学习效率",
                    "desc": "本文提出了一种自适应的探索预算分配方法，用于强化学习中的大型语言模型（LLMs），以提高训练效率和数学推理基准的性能。传统方法在每个任务上均匀分配有限的探索预算，导致简单任务总是成功而困难任务总是失败，造成训练更新时梯度为零的问题。我们将每个任务的探索视为具有不同“价值”和“成本”的“物品”，并与经典的背包问题建立联系，从而推导出一种最佳分配规则。通过将资源动态分配到学习效果最显著的任务上，我们的方法在训练中有效地提高了非零策略梯度的比例，并在数学推理基准上实现了显著的性能提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.25455",
            "title": "PIPer: On-Device Environment Setup via Online Reinforcement Learning",
            "url": "https://huggingface.co/papers/2509.25455",
            "abstract": "A specialized model combining supervised fine-tuning and Reinforcement Learning with Verifiable Rewards achieves competitive performance in automated environment setup tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Environment setup-the process of configuring the system to work with a specific software project-represents a persistent challenge in Software Engineering (SE). Automated environment setup methods could assist developers by providing fully configured environments for arbitrary repositories without manual effort. This also helps SE researchers to scale execution-based benchmarks. However, recent studies reveal that even state-of-the-art Large Language Models (LLMs) achieve limited success in automating this task. To address this limitation, we tune a specialized model for environment setup. We combine supervised fine-tuning for generating correct Bash scripts and Reinforcement Learning with Verifiable Rewards (RLVR) to adapt it to the task of environment setup. On EnvBench-Python, our method enables Qwen3-8B (a model runnable on consumer hardware) to perform on par with larger models-Qwen3-32B and GPT-4o. The training code and model checkpoints are available online: https://github.com/JetBrains-Research/PIPer.",
            "score": 26,
            "issue_id": 6205,
            "pub_date": "2025-09-29",
            "pub_date_card": {
                "ru": "29 сентября",
                "en": "September 29",
                "zh": "9月29日"
            },
            "hash": "08f51f097cf06715",
            "authors": [
                "Alexander Kovrigin",
                "Aleksandra Eliseeva",
                "Konstantin Grotov",
                "Egor Bogomolov",
                "Yaroslav Zharov"
            ],
            "affiliations": [
                "Constructor University",
                "Delft University of Technology",
                "JetBrains Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.25455.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#training",
                    "#optimization",
                    "#open_source",
                    "#small_models"
                ],
                "emoji": "⚙️",
                "ru": {
                    "title": "Специализированная модель для автоматической настройки окружения",
                    "desc": "Исследователи разработали специализированную модель для автоматизации настройки программного окружения - сложной задачи в разработке ПО. Модель объединяет supervised fine-tuning для генерации корректных Bash-скриптов и Reinforcement Learning with Verifiable Rewards (RLVR) для адаптации к задаче setup окружения. Благодаря этому подходу небольшая модель Qwen3-8B, работающая на потребительском hardware, достигает производительности более крупных моделей типа GPT-4o. На бенчмарке EnvBench-Python метод показал конкурентоспособные результаты, открывая путь к масштабируемой автоматизации настройки окружений."
                },
                "en": {
                    "title": "Automating Environment Setup with Specialized ML Models",
                    "desc": "This paper presents a specialized model that enhances automated environment setup tasks in software engineering by combining supervised fine-tuning and Reinforcement Learning with Verifiable Rewards (RLVR). The model is designed to generate accurate Bash scripts, addressing the limitations of existing Large Language Models (LLMs) in this domain. By utilizing RLVR, the model adapts effectively to the specific requirements of environment setup, achieving competitive performance on the EnvBench-Python benchmark. Notably, the Qwen3-8B model, which can run on consumer hardware, matches the performance of larger models like Qwen3-32B and GPT-4o."
                },
                "zh": {
                    "title": "专门模型助力自动化环境配置",
                    "desc": "本文提出了一种专门的模型，通过监督微调和可验证奖励的强化学习相结合，成功解决了自动化环境配置任务中的挑战。环境配置是软件工程中的一个重要环节，自动化方法可以帮助开发者快速配置环境，减少手动操作。尽管现有的大型语言模型在此任务上表现有限，但我们的方法使得Qwen3-8B模型在EnvBench-Python上与更大模型的表现相当。该研究的代码和模型检查点已在线发布，供研究者使用。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.22944",
            "title": "SINQ: Sinkhorn-Normalized Quantization for Calibration-Free\n  Low-Precision LLM Weights",
            "url": "https://huggingface.co/papers/2509.22944",
            "abstract": "SINQ enhances post-training quantization by introducing a second-axis scale factor and Sinkhorn-Knopp-style algorithm to minimize matrix imbalance, improving perplexity on large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Post-training quantization has emerged as the most widely used strategy for deploying large language models at low precision. Still, current methods show perplexity degradation at bit-widths less than or equal to 4, partly because representing outliers causes precision issues in parameters that share the same scales as these outliers. This problem is especially pronounced for calibration-free, uniform quantization methods. We introduce SINQ to augment existing post-training quantizers with an additional second-axis scale factor and a fast Sinkhorn-Knopp-style algorithm that finds scales to normalize per-row and per-column variances, thereby minimizing a novel per-matrix proxy target for quantization: the matrix imbalance. Our method has no interactions between layers and can be trivially applied to new architectures to quantize any linear layers. We evaluate our method on the Qwen3 model family and DeepSeek-V2.5. SINQ improves WikiText2 and C4 perplexity significantly against uncalibrated uniform quantization baselines and can be further enhanced by combining it with calibration and non-uniform quantization levels. Code to reproduce the results of this work and to easily quantize models using SINQ is available at https://github.com/huawei-csl/SINQ.",
            "score": 21,
            "issue_id": 6211,
            "pub_date": "2025-09-26",
            "pub_date_card": {
                "ru": "26 сентября",
                "en": "September 26",
                "zh": "9月26日"
            },
            "hash": "c5cfc389d45892b9",
            "authors": [
                "Lorenz K. Müller",
                "Philippe Bich",
                "Jiawei Zhuang",
                "Ahmet Çelik",
                "Luca Benfenati",
                "Lukas Cavigelli"
            ],
            "affiliations": [
                "Computing Systems Lab, Huawei Zurich Research Center"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.22944.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#training",
                    "#optimization"
                ],
                "emoji": "⚖️",
                "ru": {
                    "title": "Двухосевое масштабирование для точной квантизации языковых моделей",
                    "desc": "Статья представляет метод SINQ для улучшения пост-тренировочной квантизации больших языковых моделей. Ключевая идея — добавить второй масштабирующий фактор и использовать алгоритм в стиле Синкхорна-Кноппа для нормализации дисперсий по строкам и столбцам матриц весов. Это решает проблему выбросов (outliers), которые ухудшают точность квантизации на 4 битах и ниже, минимизируя матричный дисбаланс. Метод показывает значительное улучшение perplexity на моделях Qwen3 и DeepSeek-V2.5 без необходимости калибровки и легко применяется к любым линейным слоям."
                },
                "en": {
                    "title": "SINQ: Enhancing Quantization for Better Language Model Performance",
                    "desc": "The paper presents SINQ, a novel approach to enhance post-training quantization for large language models. It introduces a second-axis scale factor and a Sinkhorn-Knopp-style algorithm to address matrix imbalance, which improves the model's perplexity at lower bit-widths. This method allows for better representation of outliers and minimizes precision issues in quantized parameters. SINQ is easy to implement across different architectures and shows significant improvements in performance on benchmark datasets compared to traditional uniform quantization methods."
                },
                "zh": {
                    "title": "SINQ：提升量化性能的新方法",
                    "desc": "SINQ是一种增强后训练量化的方法，通过引入第二轴缩放因子和Sinkhorn-Knopp风格的算法来最小化矩阵不平衡，从而提高大型语言模型的困惑度。现有的量化方法在低于或等于4位宽时，常常会出现困惑度下降的问题，尤其是在无校准的均匀量化方法中。SINQ通过对每行和每列的方差进行归一化，优化了量化过程，解决了参数精度问题。我们的实验表明，SINQ在多个模型上显著提高了困惑度，并且可以与校准和非均匀量化结合使用以进一步提升性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.01174",
            "title": "Code2Video: A Code-centric Paradigm for Educational Video Generation",
            "url": "https://huggingface.co/papers/2510.01174",
            "abstract": "Code2Video generates educational videos using a code-centric agent framework, improving coherence and interpretability compared to direct code generation.  \t\t\t\t\tAI-generated summary \t\t\t\t While recent generative models advance pixel-space video synthesis, they remain limited in producing professional educational videos, which demand disciplinary knowledge, precise visual structures, and coherent transitions, limiting their applicability in educational scenarios. Intuitively, such requirements are better addressed through the manipulation of a renderable environment, which can be explicitly controlled via logical commands (e.g., code). In this work, we propose Code2Video, a code-centric agent framework for generating educational videos via executable Python code. The framework comprises three collaborative agents: (i) Planner, which structures lecture content into temporally coherent flows and prepares corresponding visual assets; (ii) Coder, which converts structured instructions into executable Python codes while incorporating scope-guided auto-fix to enhance efficiency; and (iii) Critic, which leverages vision-language models (VLM) with visual anchor prompts to refine spatial layout and ensure clarity. To support systematic evaluation, we build MMMC, a benchmark of professionally produced, discipline-specific educational videos. We evaluate MMMC across diverse dimensions, including VLM-as-a-Judge aesthetic scores, code efficiency, and particularly, TeachQuiz, a novel end-to-end metric that quantifies how well a VLM, after unlearning, can recover knowledge by watching the generated videos. Our results demonstrate the potential of Code2Video as a scalable, interpretable, and controllable approach, achieving 40% improvement over direct code generation and producing videos comparable to human-crafted tutorials. The code and datasets are available at https://github.com/showlab/Code2Video.",
            "score": 20,
            "issue_id": 6198,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 октября",
                "en": "October 1",
                "zh": "10月1日"
            },
            "hash": "26c2c9dd6c370251",
            "authors": [
                "Yanzhe Chen",
                "Kevin Qinghong Lin",
                "Mike Zheng Shou"
            ],
            "affiliations": [
                "Show Lab, National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.01174.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#benchmark",
                    "#video",
                    "#games",
                    "#optimization",
                    "#agents",
                    "#dataset"
                ],
                "emoji": "🎓",
                "ru": {
                    "title": "Генерация обучающих видео через программный код",
                    "desc": "Code2Video — это фреймворк на основе агентов для создания образовательных видео через исполняемый Python-код. Система использует трёх совместно работающих агентов: планировщик структурирует контент лекции, программист преобразует инструкции в код с автоматическим исправлением ошибок, а критик на основе vision-language моделей улучшает визуальную компоновку. Для оценки качества предложена метрика TeachQuiz, которая измеряет, насколько хорошо LLM может восстановить знания после просмотра сгенерированного видео. Подход показывает улучшение на 40% по сравнению с прямой генерацией кода и создаёт видео, сопоставимые с профессиональными обучающими материалами."
                },
                "en": {
                    "title": "Code2Video: Crafting Coherent Educational Videos with Code",
                    "desc": "Code2Video is a framework designed to create educational videos using a code-centric approach, which enhances coherence and interpretability compared to traditional methods. It consists of three main agents: the Planner organizes content into logical sequences, the Coder translates these sequences into executable Python code, and the Critic refines the visual layout using vision-language models. This method addresses the challenges of generating professional educational videos by allowing precise control over visual elements and transitions. The framework has shown a significant improvement in video quality and coherence, outperforming direct code generation by 40%."
                },
                "zh": {
                    "title": "Code2Video：教育视频生成的新方法",
                    "desc": "Code2Video 是一个基于代码的代理框架，用于生成教育视频，提升了视频的一致性和可解释性。该框架包含三个协作代理：规划者负责将讲座内容结构化并准备视觉资产；编码器将结构化指令转换为可执行的 Python 代码，并通过范围引导自动修复提高效率；评论者利用视觉语言模型优化空间布局，确保清晰度。通过建立专业制作的教育视频基准MMMC，我们评估了Code2Video在美学评分、代码效率和知识恢复等多个维度的表现，结果显示其在视频生成上优于直接代码生成。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.00615",
            "title": "ACON: Optimizing Context Compression for Long-horizon LLM Agents",
            "url": "https://huggingface.co/papers/2510.00615",
            "abstract": "Agent Context Optimization (ACON) compresses context in large language models for efficient long-horizon tasks by analyzing failure cases and distilling the compressor into smaller models.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are increasingly deployed as agents in dynamic, real-world environments, where success requires both reasoning and effective tool use. A central challenge for agentic tasks is the growing context length, as agents must accumulate long histories of actions and observations. This expansion raises costs and reduces efficiency in long-horizon tasks, yet prior work on context compression has mostly focused on single-step tasks or narrow applications. We introduce Agent Context Optimization (ACON), a unified framework that optimally compresses both environment observations and interaction histories into concise yet informative condensations. ACON leverages compression guideline optimization in natural language space: given paired trajectories where full context succeeds but compressed context fails, capable LLMs analyze the causes of failure, and the compression guideline is updated accordingly. Furthermore, we propose distilling the optimized LLM compressor into smaller models to reduce the overhead of the additional module. Experiments on AppWorld, OfficeBench, and Multi-objective QA show that ACON reduces memory usage by 26-54% (peak tokens) while largely preserving task performance, preserves over 95% of accuracy when distilled into smaller compressors, and enhances smaller LMs as long-horizon agents with up to 46% performance improvement.",
            "score": 20,
            "issue_id": 6203,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 октября",
                "en": "October 1",
                "zh": "10月1日"
            },
            "hash": "a32bcdd471ec0b1c",
            "authors": [
                "Minki Kang",
                "Wei-Ning Chen",
                "Dongge Han",
                "Huseyin A. Inan",
                "Lukas Wutschitz",
                "Yanzhi Chen",
                "Robert Sim",
                "Saravan Rajmohan"
            ],
            "affiliations": [
                "KAIST",
                "Microsoft",
                "University of Cambridge"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.00615.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#small_models",
                    "#optimization",
                    "#long_context",
                    "#inference",
                    "#dataset"
                ],
                "emoji": "🗜️",
                "ru": {
                    "title": "ACON: Сжатие контекста для AI-агентов без потери качества",
                    "desc": "Статья представляет Agent Context Optimization (ACON) — фреймворк для оптимального сжатия контекста в больших языковых моделях, работающих как агенты в долгосрочных задачах. Метод анализирует случаи, когда сжатый контекст приводит к ошибкам, и обновляет правила компрессии на естественном языке с помощью мощных LLM. ACON позволяет дистиллировать оптимизированный компрессор в более маленькие модели, снижая вычислительные затраты. Эксперименты показывают сокращение использования памяти на 26-54% при сохранении качества работы агента и улучшение производительности малых моделей до 46%."
                },
                "en": {
                    "title": "Efficient Context Compression for Long-Horizon Tasks",
                    "desc": "Agent Context Optimization (ACON) is a method designed to improve the efficiency of large language models (LLMs) when handling long-term tasks by compressing the context they use. It identifies and analyzes failure cases where compressed context leads to poor performance, allowing the model to learn and refine its compression strategies. ACON not only optimizes the way observations and action histories are condensed but also distills this knowledge into smaller models, making them more efficient. Experiments demonstrate that ACON significantly reduces memory usage while maintaining high accuracy, thus enhancing the performance of smaller models in long-horizon tasks."
                },
                "zh": {
                    "title": "高效压缩，提升长时间任务表现的代理上下文优化",
                    "desc": "代理上下文优化（ACON）是一种压缩大型语言模型中上下文的方法，旨在提高长时间任务的效率。通过分析失败案例，ACON能够提炼出更小的模型，从而优化环境观察和交互历史的压缩。该方法利用自然语言空间中的压缩指导优化，确保在成功的完整上下文和失败的压缩上下文之间进行有效分析。实验结果表明，ACON在减少内存使用的同时，能够保持任务性能，并显著提升小型语言模型的表现。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.00977",
            "title": "It Takes Two: Your GRPO Is Secretly DPO",
            "url": "https://huggingface.co/papers/2510.00977",
            "abstract": "Reframing Group Relative Policy Optimization as contrastive learning reveals its connection to Direct Preference Optimization, enabling minimal two-rollout GRPO to achieve performance comparable to larger group sizes with reduced computational cost.  \t\t\t\t\tAI-generated summary \t\t\t\t Group Relative Policy Optimization (GRPO) is a prominent reinforcement learning algorithm for post-training Large Language Models (LLMs). It is commonly believed that GRPO necessitates a large group size to ensure stable training via precise statistical estimation, which incurs substantial computational overhead. In this work, we challenge this assumption by reframing GRPO as a form of contrastive learning, which reveals a fundamental connection to Direct Preference Optimization (DPO). Motivated by DPO's empirical success, we investigate the minimal two-rollout case (2-GRPO), a configuration previously deemed infeasible. We provide a rigorous theoretical analysis to validate 2-GRPO and demonstrate empirically that it achieves performance on par with 16-GRPO, despite using only 1/8 of the rollouts and reducing training time by over 70%.",
            "score": 17,
            "issue_id": 6210,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 октября",
                "en": "October 1",
                "zh": "10月1日"
            },
            "hash": "64fa1e8d41c1a12f",
            "authors": [
                "Yihong Wu",
                "Liheng Ma",
                "Lei Ding",
                "Muzhi Li",
                "Xinyu Wang",
                "Kejia Chen",
                "Zhan Su",
                "Zhanguang Zhang",
                "Chenyang Huang",
                "Yingxue Zhang",
                "Mark Coates",
                "Jian-Yun Nie"
            ],
            "affiliations": [
                "Alberta Machine Intelligence Institute (Amii)",
                "Huawei Noahs Ark Lab",
                "McGill University",
                "Mila - Quebec AI Institute",
                "The Chinese University of Hong Kong",
                "Universite de Montreal",
                "University of Alberta",
                "University of Manitoba",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.00977.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#reasoning",
                    "#optimization",
                    "#rlhf",
                    "#rl"
                ],
                "emoji": "⚡",
                "ru": {
                    "title": "Два роллаута вместо шестнадцати: эффективная оптимизация LLM",
                    "desc": "Статья пересматривает алгоритм Group Relative Policy Optimization (GRPO), используемый для дообучения больших языковых моделей. Авторы показывают, что GRPO можно интерпретировать как контрастное обучение, что связывает его с методом Direct Preference Optimization (DPO). Оказалось, что минимальная версия с двумя роллаутами (2-GRPO) работает так же эффективно, как версия с 16 роллаутами. При этом вычислительные затраты снижаются в 8 раз, а время обучения сокращается более чем на 70%."
                },
                "en": {
                    "title": "Efficient Learning: Small Rollouts, Big Gains!",
                    "desc": "This paper explores Group Relative Policy Optimization (GRPO), a reinforcement learning method used for training Large Language Models (LLMs). It challenges the traditional belief that GRPO requires large group sizes for effective training, which leads to high computational costs. By reframing GRPO as a contrastive learning approach, the authors connect it to Direct Preference Optimization (DPO). They introduce a minimal two-rollout version of GRPO (2-GRPO), showing that it can achieve similar performance to larger configurations while significantly reducing the number of rollouts and training time."
                },
                "zh": {
                    "title": "优化策略，减少计算成本！",
                    "desc": "本文探讨了群体相对策略优化（GRPO）在强化学习中的应用，特别是在大型语言模型的后训练阶段。我们提出将GRPO重新框架为对比学习的形式，从而揭示其与直接偏好优化（DPO）之间的基本联系。通过研究最小的两次回合情况（2-GRPO），我们证明了这一配置的可行性，并提供了理论分析支持。实验结果表明，2-GRPO的性能与16-GRPO相当，但所需的回合数仅为其八分之一，训练时间减少超过70%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.00232",
            "title": "BiasFreeBench: a Benchmark for Mitigating Bias in Large Language Model\n  Responses",
            "url": "https://huggingface.co/papers/2510.00232",
            "abstract": "BiasFreeBench evaluates bias mitigation techniques in large language models using a unified benchmark and response-level metric to ensure fair and safe outputs in real-world scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing studies on bias mitigation methods for large language models (LLMs) use diverse baselines and metrics to evaluate debiasing performance, leading to inconsistent comparisons among them. Moreover, their evaluations are mostly based on the comparison between LLMs' probabilities of biased and unbiased contexts, which ignores the gap between such evaluations and real-world use cases where users interact with LLMs by reading model responses and expect fair and safe outputs rather than LLMs' probabilities. To enable consistent evaluation across debiasing methods and bridge this gap, we introduce BiasFreeBench, an empirical benchmark that comprehensively compares eight mainstream bias mitigation techniques (covering four prompting-based and four training-based methods) on two test scenarios (multi-choice QA and open-ended multi-turn QA) by reorganizing existing datasets into a unified query-response setting. We further introduce a response-level metric, Bias-Free Score, to measure the extent to which LLM responses are fair, safe, and anti-stereotypical. Debiasing performances are systematically compared and analyzed across key dimensions: the prompting vs. training paradigm, model size, and generalization of different training strategies to unseen bias types. We will publicly release our benchmark, aiming to establish a unified testbed for bias mitigation research.",
            "score": 13,
            "issue_id": 6198,
            "pub_date": "2025-09-30",
            "pub_date_card": {
                "ru": "30 сентября",
                "en": "September 30",
                "zh": "9月30日"
            },
            "hash": "7a6ed8974cc83369",
            "authors": [
                "Xin Xu",
                "Xunzhi He",
                "Churan Zhi",
                "Ruizhe Chen",
                "Julian McAuley",
                "Zexue He"
            ],
            "affiliations": [
                "Columbia University",
                "MIT-IBM Watson Lab",
                "UC San Diego",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.00232.jpg",
            "data": {
                "categories": [
                    "#ethics",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "⚖️",
                "ru": {
                    "title": "Единый бенчмарк для честной оценки методов борьбы с предвзятостью в LLM",
                    "desc": "Существующие исследования методов устранения предвзятости в больших языковых моделях используют разные базовые подходы и метрики, что затрудняет их сравнение. В работе представлен BiasFreeBench — унифицированный бенчмарк, который сравнивает восемь основных техник устранения bias (четыре на основе промптинга и четыре на основе обучения) в реальных сценариях взаимодействия с пользователями. Авторы вводят метрику уровня ответов Bias-Free Score, которая измеряет справедливость, безопасность и антистереотипность ответов LLM. Бенчмарк систематически сравнивает эффективность различных методов с учётом парадигмы (промптинг vs обучение), размера модели и способности обобщаться на новые типы предвзятости."
                },
                "en": {
                    "title": "Unifying Bias Mitigation Evaluation for Safer AI Outputs",
                    "desc": "BiasFreeBench is a new benchmark designed to evaluate bias mitigation techniques in large language models (LLMs). It addresses the inconsistency in previous studies by providing a unified framework for comparing various debiasing methods. The benchmark includes a novel response-level metric called Bias-Free Score, which assesses the fairness and safety of model outputs in real-world scenarios. By systematically analyzing different debiasing strategies, BiasFreeBench aims to enhance the reliability of LLMs in producing equitable and safe responses."
                },
                "zh": {
                    "title": "统一评估偏见缓解技术的基准工具",
                    "desc": "BiasFreeBench 是一个评估大型语言模型偏见缓解技术的基准工具，旨在确保模型输出在现实场景中公平和安全。该研究通过统一的查询-响应设置，比较了八种主流的偏见缓解方法，包括四种基于提示和四种基于训练的方法。我们引入了一个新的响应级别指标——无偏分数，来衡量模型响应的公平性、安全性和反刻板印象程度。该基准的发布将为偏见缓解研究提供一个统一的测试平台。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.00184",
            "title": "Why Can't Transformers Learn Multiplication? Reverse-Engineering Reveals\n  Long-Range Dependency Pitfalls",
            "url": "https://huggingface.co/papers/2510.00184",
            "abstract": "Reverse-engineering a model that learns multi-digit multiplication via implicit chain-of-thought reveals that it uses attention to encode long-range dependencies and represents partial products efficiently, insights that help address limitations in standard fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Language models are increasingly capable, yet still fail at a seemingly simple task of multi-digit multiplication. In this work, we study why, by reverse-engineering a model that successfully learns multiplication via implicit chain-of-thought, and report three findings: (1) Evidence of long-range structure: Logit attributions and linear probes indicate that the model encodes the necessary long-range dependencies for multi-digit multiplication. (2) Mechanism: the model encodes long-range dependencies using attention to construct a directed acyclic graph to ``cache'' and ``retrieve'' pairwise partial products. (3) Geometry: the model implements partial products in attention heads by forming Minkowski sums between pairs of digits, and digits are represented using a Fourier basis, both of which are intuitive and efficient representations that the standard fine-tuning model lacks. With these insights, we revisit the learning dynamics of standard fine-tuning and find that the model converges to a local optimum that lacks the required long-range dependencies. We further validate this understanding by introducing an auxiliary loss that predicts the ``running sum'' via a linear regression probe, which provides an inductive bias that enables the model to successfully learn multi-digit multiplication. In summary, by reverse-engineering the mechanisms of an implicit chain-of-thought model we uncover a pitfall for learning long-range dependencies in Transformers and provide an example of how the correct inductive bias can address this issue.",
            "score": 13,
            "issue_id": 6198,
            "pub_date": "2025-09-30",
            "pub_date_card": {
                "ru": "30 сентября",
                "en": "September 30",
                "zh": "9月30日"
            },
            "hash": "27d7ce536d31aa04",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#long_context",
                    "#reasoning"
                ],
                "emoji": "🔢",
                "ru": {
                    "title": "Как нейросети учатся умножать: разгадка механизма длинных зависимостей",
                    "desc": "Исследователи провели обратную инженерию модели, которая научилась многозначному умножению через неявную цепочку рассуждений (chain-of-thought). Оказалось, что модель использует механизм attention для построения направленного ациклического графа, кэширующего промежуточные произведения, и представляет числа через базис Фурье и суммы Минковского. Стандартное fine-tuning застревает в локальном оптимуме из-за неспособности уловить необходимые длинные зависимости между разрядами чисел. Добавление вспомогательной функции потерь для предсказания промежуточных сумм создаёт правильное индуктивное смещение и позволяет модели успешно освоить умножение."
                },
                "en": {
                    "title": "Unlocking Multi-Digit Multiplication with Attention and Inductive Bias",
                    "desc": "This paper investigates how a model learns to perform multi-digit multiplication using an implicit chain-of-thought approach. It reveals that the model effectively encodes long-range dependencies through attention mechanisms, allowing it to manage partial products efficiently. The authors demonstrate that standard fine-tuning methods often fail to capture these dependencies, leading to suboptimal performance. By introducing an auxiliary loss that predicts running sums, they provide a solution to enhance learning dynamics and improve the model's ability to handle complex multiplication tasks."
                },
                "zh": {
                    "title": "揭示多位数乘法学习的关键机制",
                    "desc": "本研究通过逆向工程一个成功学习多位数乘法的模型，揭示了其使用注意力机制编码长距离依赖关系的方式。研究发现，该模型通过构建有向无环图来缓存和检索成对的部分积，从而有效地表示部分积。模型在注意力头中通过形成闵可夫斯基和来实现部分积，并使用傅里叶基表示数字，这些都是标准微调模型所缺乏的直观且高效的表示方式。通过引入辅助损失来预测“运行和”，我们为模型提供了一个归纳偏置，使其能够成功学习多位数乘法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.26346",
            "title": "EditReward: A Human-Aligned Reward Model for Instruction-Guided Image\n  Editing",
            "url": "https://huggingface.co/papers/2509.26346",
            "abstract": "A new reward model, trained with a large-scale human preference dataset, improves instruction-guided image editing by selecting high-quality training data and achieving state-of-the-art performance on benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, we have witnessed great progress in image editing with natural language instructions. Several closed-source models like GPT-Image-1, Seedream, and Google-Nano-Banana have shown highly promising progress. However, the open-source models are still lagging. The main bottleneck is the lack of a reliable reward model to scale up high-quality synthetic training data. To address this critical bottleneck, we built \\mname, trained with our new large-scale human preference dataset, meticulously annotated by trained experts following a rigorous protocol containing over 200K preference pairs. \\mname demonstrates superior alignment with human preferences in instruction-guided image editing tasks. Experiments show that \\mname achieves state-of-the-art human correlation on established benchmarks such as GenAI-Bench, AURORA-Bench, ImagenHub, and our new \\benchname, outperforming a wide range of VLM-as-judge models. Furthermore, we use \\mname to select a high-quality subset from the existing noisy ShareGPT-4o-Image dataset. We train Step1X-Edit on the selected subset, which shows significant improvement over training on the full set. This demonstrates \\mname's ability to serve as a reward model to scale up high-quality training data for image editing. Furthermore, its strong alignment suggests potential for advanced applications like reinforcement learning-based post-training and test-time scaling of image editing models. \\mname with its training dataset will be released to help the community build more high-quality image editing training datasets.",
            "score": 12,
            "issue_id": 6211,
            "pub_date": "2025-09-30",
            "pub_date_card": {
                "ru": "30 сентября",
                "en": "September 30",
                "zh": "9月30日"
            },
            "hash": "abdf752d5e345000",
            "authors": [
                "Keming Wu",
                "Sicong Jiang",
                "Max Ku",
                "Ping Nie",
                "Minghao Liu",
                "Wenhu Chen"
            ],
            "affiliations": [
                "2077AI",
                "Independent",
                "McGill University",
                "Tsinghua University",
                "University of Waterloo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.26346.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#alignment",
                    "#rlhf",
                    "#data",
                    "#open_source"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Reward-модель для качественного обучения редактирования изображений по текстовым инструкциям",
                    "desc": "Исследователи создали новую reward-модель для редактирования изображений по текстовым инструкциям, обученную на крупномасштабном датасете человеческих предпочтений с более чем 200 тысячами размеченных пар. Модель демонстрирует лучшую корреляцию с человеческими оценками на бенчмарках GenAI-Bench, AURORA-Bench и ImagenHub, превосходя существующие VLM-модели в роли судей. С помощью этой reward-модели авторы отфильтровали высококачественное подмножество из зашумленного датасета ShareGPT-4o-Image для обучения модели Step1X-Edit, что значительно улучшило результаты. Модель может применяться для reinforcement learning и масштабирования качественных обучающих данных в задачах редактирования изображений."
                },
                "en": {
                    "title": "Enhancing Image Editing with a Human-Aligned Reward Model",
                    "desc": "This paper introduces a new reward model called \\mname, which is trained on a large-scale dataset of human preferences to enhance instruction-guided image editing. The model addresses the challenge of selecting high-quality training data, which has been a limitation for open-source models compared to their closed-source counterparts. By demonstrating superior alignment with human preferences, \\mname achieves state-of-the-art performance on various benchmarks, outperforming existing models. The research also shows that using \\mname to curate a high-quality subset from a noisy dataset significantly improves the training outcomes for image editing tasks."
                },
                "zh": {
                    "title": "提升图像编辑质量的新奖励模型",
                    "desc": "本文提出了一种新的奖励模型，旨在改善基于指令的图像编辑。该模型通过一个大规模的人类偏好数据集进行训练，能够选择高质量的训练数据，并在多个基准测试中实现了最先进的性能。研究表明，该模型在图像编辑任务中与人类偏好的对齐度更高，能够有效提升图像编辑的质量。最终，本文还计划将该模型及其训练数据集发布，以帮助社区构建更高质量的图像编辑训练数据集。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.01180",
            "title": "BroRL: Scaling Reinforcement Learning via Broadened Exploration",
            "url": "https://huggingface.co/papers/2510.01180",
            "abstract": "BroRL enhances reinforcement learning by increasing rollouts per example, overcoming performance plateaus and achieving state-of-the-art results in large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key ingredient for unlocking complex reasoning capabilities in large language models. Recent work ProRL has shown promise in scaling RL by increasing the number of training steps. However, performance plateaus after thousands of steps, with clear diminishing returns from allocating more computation to additional training. In this work, we investigate a complementary paradigm for scaling RL, BroR-Lincreasing the number of rollouts per example to hundreds to exhaustively Broaden exploration, which yields continuous performance gains beyond the saturation point observed in ProRL when scaling the number of training steps. Our approach is motivated by a mass balance equation analysis allowing us to characterize the rate of change in probability mass for correct and incorrect tokens during the reinforcement process. We show that under a one-step RL assumption, sampled rollout tokens always contribute to correct-mass expansion, while unsampled tokens outside rollouts may lead to gains or losses depending on their distribution and the net reward balance. Importantly, as the number of rollouts per example N increases, the effect of unsampled terms diminishes, ensuring overall correct-mass expansion. To validate our theoretical analysis, we conduct simulations under more relaxed conditions and find that a sufficiently large rollout size N-corresponding to ample exploration-guarantees an increase in the probability mass of all correct tokens. Empirically, BroRL revives models saturated after 3K ProRL training steps and demonstrates robust, continuous improvement, achieving state-of-the-art results for the 1.5B model across diverse benchmarks.",
            "score": 10,
            "issue_id": 6202,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 октября",
                "en": "October 1",
                "zh": "10月1日"
            },
            "hash": "2491de2a9eaf3c36",
            "authors": [
                "Jian Hu",
                "Mingjie Liu",
                "Ximing Lu",
                "Fang Wu",
                "Zaid Harchaoui",
                "Shizhe Diao",
                "Yejin Choi",
                "Pavlo Molchanov",
                "Jun Yang",
                "Jan Kautz",
                "Yi Dong"
            ],
            "affiliations": [
                "NVIDIA",
                "Stanford University",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.01180.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#benchmark",
                    "#reasoning",
                    "#optimization",
                    "#training"
                ],
                "emoji": "🌊",
                "ru": {
                    "title": "Больше роллаутов — лучше результат: масштабирование RL через расширение исследования",
                    "desc": "Статья представляет BroRL — метод масштабирования reinforcement learning для LLM через увеличение числа роллаутов (траекторий) на каждом примере вместо простого увеличения шагов обучения. Авторы показывают через анализ уравнений массового баланса, что при достаточно большом числе роллаутов гарантируется рост вероятностной массы правильных токенов. Подход позволяет преодолеть плато производительности, которое наблюдается в методе ProRL после тысяч шагов обучения. BroRL достигает state-of-the-art результатов на моделях размером 1.5B параметров, демонстрируя непрерывное улучшение там, где другие методы насыщаются."
                },
                "en": {
                    "title": "Broaden Exploration for Continuous Gains in RL",
                    "desc": "BroRL is a novel approach in reinforcement learning that enhances the training process by increasing the number of rollouts per example, which allows for broader exploration of the solution space. This method addresses the issue of performance plateaus that occur when traditional training steps reach diminishing returns. By analyzing the probability mass of correct and incorrect tokens, BroRL ensures that as the number of rollouts increases, the overall performance improves continuously. The empirical results show that BroRL outperforms previous methods, achieving state-of-the-art results in large language models after extensive training."
                },
                "zh": {
                    "title": "BroRL：突破强化学习的性能瓶颈",
                    "desc": "BroRL是一种增强强化学习的方法，通过增加每个示例的回合数来克服性能平台期。它能够在大语言模型中实现持续的性能提升，超越了传统方法ProRL的限制。通过对概率质量变化的分析，BroRL确保了正确标记的概率质量不断扩展。实验结果表明，BroRL在多个基准测试中取得了最先进的成果，尤其是在经过3000步ProRL训练后，模型的性能得到了显著恢复。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.00967",
            "title": "QUASAR: Quantum Assembly Code Generation Using Tool-Augmented LLMs via\n  Agentic RL",
            "url": "https://huggingface.co/papers/2510.00967",
            "abstract": "QUASAR, an RL framework using tool-augmented LLMs, improves quantum circuit generation and optimization through verification and hierarchical rewards, achieving high validity compared to industrial LLMs.  \t\t\t\t\tAI-generated summary \t\t\t\t Designing and optimizing task-specific quantum circuits are crucial to leverage the advantage of quantum computing. Recent large language model (LLM)-based quantum circuit generation has emerged as a promising automatic solution. However, the fundamental challenges remain unaddressed: (i) parameterized quantum gates require precise numerical values for optimal performance, which also depend on multiple aspects, including the number of quantum gates, their parameters, and the layout/depth of the circuits. (ii) LLMs often generate low-quality or incorrect quantum circuits due to the lack of quantum domain-specific knowledge. We propose QUASAR, an agentic reinforcement learning (RL) framework for quantum circuits generation and optimization based on tool-augmented LLMs. To align the LLM with quantum-specific knowledge and improve the generated quantum circuits, QUASAR designs (i) a quantum circuit verification approach with external quantum simulators and (ii) a sophisticated hierarchical reward mechanism in RL training. Extensive evaluation shows improvements in both syntax and semantic performance of the generated quantum circuits. When augmenting a 4B LLM, QUASAR has achieved the validity of 99.31% in Pass@1 and 100% in Pass@10, outperforming industrial LLMs of GPT-4o, GPT-5 and DeepSeek-V3 and several supervised-fine-tuning (SFT)-only and RL-only baselines.",
            "score": 10,
            "issue_id": 6211,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 октября",
                "en": "October 1",
                "zh": "10月1日"
            },
            "hash": "81aeacbb43532795",
            "authors": [
                "Cong Yu",
                "Valter Uotila",
                "Shilong Deng",
                "Qingyuan Wu",
                "Tuo Shi",
                "Songlin Jiang",
                "Lei You",
                "Bo Zhao"
            ],
            "affiliations": [
                "Aalto University",
                "Technical University of Denmark",
                "University of Helsinki",
                "University of Liverpool",
                "University of Southampton"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.00967.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#training",
                    "#rl",
                    "#agents",
                    "#optimization"
                ],
                "emoji": "⚛️",
                "ru": {
                    "title": "Квантовые схемы под управлением RL-агента с LLM",
                    "desc": "Статья представляет QUASAR — фреймворк на основе reinforcement learning и LLM с инструментами для автоматической генерации и оптимизации квантовых схем. Главная проблема заключается в том, что LLM часто генерируют некорректные квантовые схемы из-за недостатка специфических знаний о квантовых вычислениях и сложности подбора точных параметров для квантовых гейтов. QUASAR решает это через верификацию схем с помощью квантовых симуляторов и иерархическую систему наград при RL-обучении. В результате модель размером 4B параметров достигает валидности 99.31% с первой попытки, превосходя GPT-4o, GPT-5 и DeepSeek-V3."
                },
                "en": {
                    "title": "QUASAR: Revolutionizing Quantum Circuit Generation with Reinforcement Learning",
                    "desc": "QUASAR is a reinforcement learning framework that enhances the generation and optimization of quantum circuits using tool-augmented large language models (LLMs). It addresses key challenges in quantum circuit design, such as the need for precise parameters and the limitations of LLMs in understanding quantum-specific knowledge. By implementing a verification method with quantum simulators and a hierarchical reward system, QUASAR significantly improves the quality of generated circuits. The framework demonstrates exceptional performance, achieving a validity rate of 99.31% in Pass@1 and 100% in Pass@10, surpassing existing industrial LLMs."
                },
                "zh": {
                    "title": "QUASAR：量子电路生成与优化的新突破",
                    "desc": "QUASAR是一个基于工具增强的大型语言模型（LLM）的强化学习（RL）框架，旨在改进量子电路的生成和优化。该框架通过外部量子模拟器进行量子电路验证，并设计了复杂的层次奖励机制，以提高生成电路的质量。QUASAR在生成的量子电路的语法和语义性能上都有显著提升，验证率达到99.31%。与工业级LLM如GPT-4o、GPT-5和DeepSeek-V3相比，QUASAR表现出更高的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.25301",
            "title": "Flash-Searcher: Fast and Effective Web Agents via DAG-Based Parallel\n  Execution",
            "url": "https://huggingface.co/papers/2509.25301",
            "abstract": "Flash-Searcher, a parallel agent reasoning framework using directed acyclic graphs, enhances efficiency and performance in complex reasoning tasks by enabling concurrent execution and dynamic workflow optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks when equipped with external tools. However, current frameworks predominantly rely on sequential processing, leading to inefficient execution particularly for tasks requiring extensive tool interaction. This paper introduces Flash-Searcher, a novel parallel agent reasoning framework that fundamentally reimagines the execution paradigm from sequential chains to directed acyclic graphs (DAGs). Flash-Searcher decomposes complex tasks into subtasks with explicit dependencies, enabling concurrent execution of independent reasoning paths while maintaining logical constraints. Through dynamic workflow optimization, our framework continuously refines the execution graph based on intermediate results, effectively integrating summary module. Comprehensive evaluations across multiple benchmarks demonstrate that Flash-Searcher consistently outperforms existing approaches. Specifically, it achieves 67.7% accuracy on BrowseComp and 83% on xbench-DeepSearch, while reducing agent execution steps by up to 35% compared to current frameworks. Furthermore, when distilling this parallel reasoning pipeline into single models, we observe substantial performance gains across diverse backbone architectures, underscoring the generalizability of our methodology. Our work thus represents a significant advance in agent architecture design, offering a more scalable and efficient paradigm for complex reasoning tasks.",
            "score": 10,
            "issue_id": 6200,
            "pub_date": "2025-09-29",
            "pub_date_card": {
                "ru": "29 сентября",
                "en": "September 29",
                "zh": "9月29日"
            },
            "hash": "6daa8de407c2b924",
            "authors": [
                "Tianrui Qin",
                "Qianben Chen",
                "Sinuo Wang",
                "He Xing",
                "King Zhu",
                "He Zhu",
                "Dingfeng Shi",
                "Xinxin Liu",
                "Ge Zhang",
                "Jiaheng Liu",
                "Yuchen Eleanor Jiang",
                "Xitong Gao",
                "Wangchunshu Zhou"
            ],
            "affiliations": [
                "OPPO",
                "Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.25301.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#reasoning",
                    "#benchmark",
                    "#agents"
                ],
                "emoji": "⚡",
                "ru": {
                    "title": "Параллельное мышление агентов через графы зависимостей",
                    "desc": "В статье представлен Flash-Searcher — новый фреймворк для параллельного рассуждения AI-агентов, основанный на направленных ациклических графах (DAG) вместо традиционной последовательной обработки. Система разбивает сложные задачи на подзадачи с явными зависимостями, что позволяет выполнять независимые цепочки рассуждений параллельно при сохранении логических связей. Flash-Searcher показывает точность 67.7% на BrowseComp и 83% на xbench-DeepSearch, сокращая количество шагов выполнения на 35% по сравнению с существующими подходами. Метод также успешно применяется для дистилляции параллельного рассуждения в отдельные модели, демонстрируя улучшение производительности на различных архитектурах."
                },
                "en": {
                    "title": "Revolutionizing Reasoning with Parallel Execution",
                    "desc": "Flash-Searcher is a new framework designed to improve the efficiency of reasoning tasks in artificial intelligence. It uses directed acyclic graphs (DAGs) to allow multiple reasoning paths to be executed at the same time, rather than one after the other. This approach not only speeds up the process but also optimizes the workflow dynamically based on the results obtained during execution. The framework has shown significant performance improvements in various benchmarks, demonstrating its effectiveness over traditional sequential processing methods."
                },
                "zh": {
                    "title": "Flash-Searcher：高效的并行推理框架",
                    "desc": "Flash-Searcher 是一种新的并行智能体推理框架，使用有向无环图（DAG）来提高复杂推理任务的效率和性能。它通过将复杂任务分解为具有明确依赖关系的子任务，允许独立推理路径的并发执行，同时保持逻辑约束。该框架通过动态工作流优化，基于中间结果不断改进执行图，有效整合了摘要模块。综合评估显示，Flash-Searcher 在多个基准测试中表现优异，显著提高了推理效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.00931",
            "title": "Making, not Taking, the Best of N",
            "url": "https://huggingface.co/papers/2510.00931",
            "abstract": "Fusion-of-N (FusioN) method improves LLM generation quality by synthesizing elements from multiple samples, outperforming Best-of-N in various settings and tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Obtaining high-quality generations in modern LLMs has largely been framed as a selection problem: identifying a single winning generation from a diverse pool of N samples, the Best-of-N (BoN). Yet, this approach is inherently zero-sum, discarding diverse and potentially useful information from the pool. Instead, we explore a collaborative setup, where all candidates can potentially contribute to the final winning generation. To this end, we propose Fusion-of-N (FusioN): a method that uses a general LLM judge to synthesize the most informative elements of each sample into a single final answer. We compare FusioN to BoN in two settings, (i) test-time scaling, where we sample and aggregate from a single model at test-time (ii) synthetic data generation, where we fuse samples from a pool of diverse teachers to improve a student model. We extensively benchmark both setups across 11 languages, 3 diverse tasks and varying model scales. Across the bench, FusioN consistently outperforms BoN showing versatility and robustness both in test-time scaling and in downstream gains from synthetic data generation. We also perform extensive analysis on FusioN, where it shows surprising strengths and robustness under challenging settings. These results show that we should shift how we think about evaluating and utilizing LLM generations from a monolithic measure of quality, to embracing their polylithic nature. This shift allows us to integrate diverse strengths, unlock latent potential, and achieve improvements that were previously inaccessible through selection alone.",
            "score": 7,
            "issue_id": 6201,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 октября",
                "en": "October 1",
                "zh": "10月1日"
            },
            "hash": "f203d81f3a65b787",
            "authors": [
                "Ammar Khairi",
                "Daniel D'souza",
                "Marzieh Fadaee",
                "Julia Kreutzer"
            ],
            "affiliations": [
                "Cohere Labs"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.00931.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#benchmark",
                    "#optimization",
                    "#synthetic",
                    "#multilingual"
                ],
                "emoji": "🔀",
                "ru": {
                    "title": "Сила слияния: объединяем лучшее из многих генераций вместо выбора одной",
                    "desc": "В статье предлагается метод Fusion-of-N (FusioN), который улучшает качество генерации LLM путём синтеза элементов из множественных сэмплов, вместо простого выбора лучшего варианта как в Best-of-N. Метод использует LLM в качестве судьи для объединения наиболее информативных элементов каждого сэмпла в финальный ответ. FusioN тестировался в двух сценариях: масштабирование на этапе inference и генерация синтетических данных для обучения student-модели. Результаты на 11 языках и 3 задачах показывают стабильное превосходство FusioN над Best-of-N, демонстрируя преимущество коллаборативного подхода над конкурентным отбором."
                },
                "en": {
                    "title": "Unlocking the Power of Collaboration in LLM Generation",
                    "desc": "The Fusion-of-N (FusioN) method enhances the quality of large language model (LLM) outputs by integrating elements from multiple generated samples rather than selecting just one. This approach contrasts with the traditional Best-of-N (BoN) method, which often overlooks valuable information by focusing on a single best output. FusioN employs a general LLM judge to synthesize the most informative parts of each candidate, leading to a more comprehensive final generation. Extensive benchmarking across various languages and tasks demonstrates that FusioN consistently outperforms BoN, highlighting the benefits of leveraging diverse contributions in LLM generation."
                },
                "zh": {
                    "title": "融合多样性，提升生成质量",
                    "desc": "Fusion-of-N（FusioN）方法通过综合多个样本的元素，提升了大型语言模型（LLM）的生成质量，超越了传统的最佳选择方法（Best-of-N）。该方法不再仅仅选择一个最佳生成，而是允许所有候选样本共同贡献信息，形成最终的答案。FusioN在测试时扩展和合成数据生成的多种任务中表现出色，显示出其在多语言和不同模型规模下的灵活性和稳健性。研究结果表明，我们应当改变对LLM生成结果的评估方式，从单一的质量衡量转向接受其多样性，以实现更大的潜力和改进。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.00526",
            "title": "Beyond Log Likelihood: Probability-Based Objectives for Supervised\n  Fine-Tuning across the Model Capability Continuum",
            "url": "https://huggingface.co/papers/2510.00526",
            "abstract": "Research identifies probability-based objectives that outperform negative log likelihood for fine-tuning large language models, depending on model capability.  \t\t\t\t\tAI-generated summary \t\t\t\t Supervised fine-tuning (SFT) is the standard approach for post-training large language models (LLMs), yet it often shows limited generalization. We trace this limitation to its default training objective: negative log likelihood (NLL). While NLL is classically optimal when training from scratch, post-training operates in a different paradigm and could violate its optimality assumptions, where models already encode task-relevant priors and supervision can be long and noisy. To this end, we study a general family of probability-based objectives and characterize their effectiveness under different conditions. Through comprehensive experiments and extensive ablation studies across 7 model backbones, 14 benchmarks, and 3 domains, we uncover a critical dimension that governs objective behavior: the model-capability continuum. Near the model-strong end, prior-leaning objectives that downweight low-probability tokens (e.g., -p, -p^{10}, thresholded variants) consistently outperform NLL; toward the model-weak end, NLL dominates; in between, no single objective prevails. Our theoretical analysis further elucidates how objectives trade places across the continuum, providing a principled foundation for adapting objectives to model capability. Our code is available at https://github.com/GaotangLi/Beyond-Log-Likelihood.",
            "score": 7,
            "issue_id": 6198,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 октября",
                "en": "October 1",
                "zh": "10月1日"
            },
            "hash": "9f43fe314cbe69af",
            "authors": [
                "Gaotang Li",
                "Ruizhong Qiu",
                "Xiusi Chen",
                "Heng Ji",
                "Hanghang Tong"
            ],
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.00526.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Выбор функции потерь зависит от силы модели",
                    "desc": "Исследование показывает, что стандартная функция потерь negative log likelihood (NLL) не всегда оптимальна для файн-тюнинга больших языковых моделей. Авторы изучили семейство вероятностных функций потерь и обнаружили критическую закономерность: эффективность функции зависит от capability модели. Для сильных моделей лучше работают функции, которые снижают вес токенов с низкой вероятностью (например, -p или -p^10), для слабых моделей эффективнее остаётся классический NLL. Эксперименты на 7 архитектурах, 14 бенчмарках и 3 доменах подтверждают существование континуума model-capability, который определяет выбор оптимальной функции потерь."
                },
                "en": {
                    "title": "Optimizing Fine-Tuning: Beyond Negative Log Likelihood",
                    "desc": "This paper explores how different training objectives can improve the fine-tuning of large language models (LLMs) beyond the traditional negative log likelihood (NLL). It identifies that NLL may not be optimal for models that have already been pre-trained, as they possess inherent task-relevant knowledge. The authors propose a range of probability-based objectives that adapt to the model's capability, showing that stronger models benefit from objectives that prioritize high-probability tokens. Through extensive experiments, they demonstrate that the effectiveness of these objectives varies along a continuum of model strength, providing insights into how to select the best training objective based on model performance."
                },
                "zh": {
                    "title": "超越负对数似然的微调目标",
                    "desc": "本研究探讨了基于概率的目标函数在微调大型语言模型时的表现，发现其在不同模型能力下优于负对数似然（NLL）。传统的监督微调方法常常受限于NLL这一训练目标，而在后训练阶段，模型已经具备了任务相关的先验知识。我们通过大量实验和消融研究，揭示了模型能力的连续性对目标函数表现的影响。在强模型端，倾向于先验的目标函数表现优于NLL，而在弱模型端则是NLL占优，提供了根据模型能力调整目标函数的理论基础。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.00553",
            "title": "On Predictability of Reinforcement Learning Dynamics for Large Language\n  Models",
            "url": "https://huggingface.co/papers/2510.00553",
            "abstract": "Two fundamental properties of reinforcement learning-induced parameter updates in large language models are identified, leading to a plug-in acceleration framework that significantly speeds up training without sacrificing performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in reasoning capabilities of large language models (LLMs) are largely driven by reinforcement learning (RL), yet the underlying parameter dynamics during RL training remain poorly understood. This work identifies two fundamental properties of RL-induced parameter updates in LLMs: (1) Rank-1 Dominance, where the top singular subspace of the parameter update matrix nearly fully determines reasoning improvements, recovering over 99\\% of performance gains; and (2) Rank-1 Linear Dynamics, where this dominant subspace evolves linearly throughout training, enabling accurate prediction from early checkpoints. Extensive experiments across 8 LLMs and 7 algorithms validate the generalizability of these properties. More importantly, based on these findings, we propose AlphaRL, a plug-in acceleration framework that extrapolates the final parameter update using a short early training window, achieving up to 2.5 speedup while retaining \\textgreater 96\\% of reasoning performance without extra modules or hyperparameter tuning. This positions our finding as a versatile and practical tool for large-scale RL, opening a path toward principled, interpretable, and efficient training paradigm for LLMs.",
            "score": 6,
            "issue_id": 6198,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 октября",
                "en": "October 1",
                "zh": "10月1日"
            },
            "hash": "75c2581875112809",
            "authors": [
                "Yuchen Cai",
                "Ding Cao",
                "Xin Xu",
                "Zijun Yao",
                "Yuqing Huang",
                "Zhenyu Tan",
                "Benyi Zhang",
                "Guiquan Liu",
                "Junfeng Fang"
            ],
            "affiliations": [
                "HKUST",
                "NUS",
                "USTC"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.00553.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Ускорение обучения LLM через ранговую экстраполяцию",
                    "desc": "Исследование выявляет два фундаментальных свойства обновлений параметров в больших языковых моделях при reinforcement learning: доминирование ранга-1 и линейную динамику главного подпространства. Оказывается, что верхнее сингулярное подпространство матрицы обновлений параметров определяет более 99% улучшения способностей к рассуждению. На основе этих находок предложен фреймворк AlphaRL, который экстраполирует финальное обновление параметров, используя короткое окно раннего обучения. Метод обеспечивает ускорение до 2.5x с сохранением более 96% производительности без дополнительных модулей или настройки гиперпараметров."
                },
                "en": {
                    "title": "Accelerating RL Training in LLMs with AlphaRL",
                    "desc": "This paper explores how reinforcement learning (RL) affects the training of large language models (LLMs) by identifying two key properties of parameter updates. The first property, Rank-1 Dominance, shows that a specific part of the parameter update matrix is crucial for improving reasoning capabilities, capturing over 99% of performance gains. The second property, Rank-1 Linear Dynamics, indicates that this important part changes in a predictable way during training, allowing for accurate predictions from early training stages. Based on these insights, the authors introduce AlphaRL, a framework that accelerates training by predicting final updates from early data, achieving significant speed improvements while maintaining high performance."
                },
                "zh": {
                    "title": "加速强化学习训练的有效工具",
                    "desc": "本文识别了强化学习在大型语言模型（LLMs）中引起的参数更新的两个基本特性。这些特性包括：1）秩-1主导性，意味着参数更新矩阵的主导子空间几乎完全决定了推理的改进；2）秩-1线性动态，表明这个主导子空间在训练过程中线性演变。基于这些发现，提出了AlphaRL加速框架，可以在不牺牲性能的情况下显著加快训练速度。实验表明，该框架在多个大型语言模型和算法中具有广泛的适用性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.22887",
            "title": "Infusing Theory of Mind into Socially Intelligent LLM Agents",
            "url": "https://huggingface.co/papers/2509.22887",
            "abstract": "Integrating Theory of Mind into LLMs improves dialogue effectiveness and goal achievement by enabling strategic reasoning and better partner relationships.  \t\t\t\t\tAI-generated summary \t\t\t\t Theory of Mind (ToM)-an understanding of the mental states of others-is a key aspect of human social intelligence, yet, chatbots and LLM-based social agents do not typically integrate it. In this work, we demonstrate that LLMs that explicitly use ToM get better at dialogue, achieving goals more effectively. After showing that simply prompting models to generate mental states between dialogue turns already provides significant benefit, we further introduce ToMAgent (ToMA), a ToM-focused dialogue agent. ToMA is trained by pairing ToM with dialogue lookahead to produce mental states that are maximally useful for achieving dialogue goals. Experiments on the Sotopia interactive social evaluation benchmark demonstrate the effectiveness of our method over a range of baselines. Comprehensive analysis shows that ToMA exhibits more strategic, goal-oriented reasoning behaviors, which enable long-horizon adaptation, while maintaining better relationships with their partners. Our results suggest a step forward in integrating ToM for building socially intelligent LLM agents.",
            "score": 5,
            "issue_id": 6200,
            "pub_date": "2025-09-26",
            "pub_date_card": {
                "ru": "26 сентября",
                "en": "September 26",
                "zh": "9月26日"
            },
            "hash": "de0a43468eb08889",
            "authors": [
                "EunJeong Hwang",
                "Yuwei Yin",
                "Giuseppe Carenini",
                "Peter West",
                "Vered Shwartz"
            ],
            "affiliations": [
                "University of British Columbia",
                "Vector Institute for AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.22887.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#rl",
                    "#reasoning",
                    "#benchmark",
                    "#agents"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Агенты с теорией разума для социального интеллекта",
                    "desc": "Исследователи интегрировали теорию разума (ToM) — способность понимать ментальные состояния других людей — в большие языковые модели для улучшения диалогов. Они разработали ToMAgent (ToMA), который генерирует предположения о ментальных состояниях собеседника между репликами и использует их для стратегического планирования. Эксперименты показали, что такой подход позволяет агентам более эффективно достигать целей в диалоге, демонстрируя стратегическое мышление и долгосрочную адаптацию. Результаты указывают на перспективность использования ToM для создания социально интеллектуальных AI-агентов."
                },
                "en": {
                    "title": "Empowering LLMs with Theory of Mind for Smarter Conversations",
                    "desc": "This paper explores how integrating Theory of Mind (ToM) into large language models (LLMs) enhances their ability to engage in dialogue and achieve specific goals. By understanding the mental states of conversation partners, LLMs can respond more strategically and maintain better relationships. The authors introduce ToMA, a dialogue agent that combines ToM with dialogue lookahead to optimize its responses for goal achievement. Experiments show that ToMA outperforms traditional models, demonstrating improved reasoning and adaptability in social interactions."
                },
                "zh": {
                    "title": "心智理论提升对话智能",
                    "desc": "本研究探讨了将心智理论（ToM）整合到大型语言模型（LLM）中的方法，以提高对话的有效性和目标达成率。心智理论是理解他人心理状态的能力，是人类社会智能的重要组成部分。我们展示了通过明确使用心智理论的LLM在对话中表现更好，能够更有效地实现目标。我们还提出了ToMA（心智理论对话代理），通过将心智理论与对话前瞻结合，训练出能够产生有助于实现对话目标的心理状态的代理。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.00536",
            "title": "GUI-KV: Efficient GUI Agents via KV Cache with Spatio-Temporal Awareness",
            "url": "https://huggingface.co/papers/2510.00536",
            "abstract": "GUI-KV, a KV cache compression method for GUI agents, improves efficiency by exploiting spatial and temporal redundancies, reducing computational cost while maintaining accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Graphical user interface (GUI) agents built on vision-language models have emerged as a promising approach to automate human-computer workflows. However, they also face the inefficiency challenge as they process long sequences of high-resolution screenshots and solving long-horizon tasks, making inference slow, costly and memory-bound. While key-value (KV) caching can mitigate this, storing the full cache is prohibitive for image-heavy contexts. Existing cache-compression methods are sub-optimal as they do not account for the spatial and temporal redundancy of GUIs. In this work, we first analyze attention patterns in GUI agent workloads and find that, unlike in natural images, attention sparsity is uniformly high across all transformer layers. This insight motivates a simple uniform budget allocation strategy, which we show empirically outperforms more complex layer-varying schemes. Building on this, we introduce GUI-KV, a plug-and-play KV cache compression method for GUI agents that requires no retraining. GUI-KV combines two novel techniques: (i) spatial saliency guidance, which augments attention scores with the L2 norm of hidden states to better preserve semantically important visual tokens, and (ii) temporal redundancy scoring, which projects previous frames' keys onto the current frame's key subspace to preferentially prune redundant history. Across standard GUI agent benchmarks and models, GUI-KV outperforms competitive KV compression baselines, closely matching full-cache accuracy at modest budgets. Notably, in a 5-screenshot setting on the AgentNetBench benchmark, GUI-KV reduces decoding FLOPs by 38.9% while increasing step accuracy by 4.1% over the full-cache baseline. These results demonstrate that exploiting GUI-specific redundancies enables efficient and reliable agent performance.",
            "score": 4,
            "issue_id": 6198,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 октября",
                "en": "October 1",
                "zh": "10月1日"
            },
            "hash": "6687d7079b2d54e2",
            "authors": [
                "Kung-Hsiang Huang",
                "Haoyi Qiu",
                "Yutong Dai",
                "Caiming Xiong",
                "Chien-Sheng Wu"
            ],
            "affiliations": [
                "Salesforce AI Research",
                "University of California, Los Angeles"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.00536.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#inference",
                    "#agents",
                    "#benchmark"
                ],
                "emoji": "🖥️",
                "ru": {
                    "title": "Сжатие памяти для AI-агентов в графических интерфейсах",
                    "desc": "Исследователи представили GUI-KV — метод сжатия KV-кэша для AI-агентов, работающих с графическими интерфейсами. Метод использует две техники: пространственную оценку важности визуальных токенов через L2-норму скрытых состояний и удаление избыточной информации между последовательными скриншотами. В отличие от обработки естественных изображений, внимание в GUI равномерно разрежено во всех слоях трансформера, что позволяет использовать единую стратегию распределения бюджета памяти. На бенчмарке AgentNetBench метод снижает вычислительные затраты на 38.9% при улучшении точности на 4.1%, не требуя дополнительного обучения модели."
                },
                "en": {
                    "title": "Efficient GUI Agents with GUI-KV Cache Compression",
                    "desc": "The paper presents GUI-KV, a method for compressing key-value (KV) caches specifically designed for graphical user interface (GUI) agents. It addresses the inefficiencies in processing high-resolution screenshots by leveraging spatial and temporal redundancies, which reduces computational costs while maintaining accuracy. The authors analyze attention patterns in GUI workloads and propose a uniform budget allocation strategy that outperforms more complex methods. By introducing techniques like spatial saliency guidance and temporal redundancy scoring, GUI-KV achieves significant improvements in efficiency and accuracy across various benchmarks."
                },
                "zh": {
                    "title": "高效的GUI代理缓存压缩方法",
                    "desc": "GUI-KV是一种针对图形用户界面（GUI）代理的键值（KV）缓存压缩方法，通过利用空间和时间冗余来提高效率。该方法在处理高分辨率截图和长时间任务时，能够降低计算成本，同时保持准确性。研究表明，GUI代理的注意力模式与自然图像不同，所有变换器层的注意力稀疏性均较高，这促使我们提出了一种简单的均匀预算分配策略。GUI-KV结合了空间显著性引导和时间冗余评分两种新技术，显著提高了缓存压缩的效果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.25531",
            "title": "MixtureVitae: Open Web-Scale Pretraining Dataset With High Quality\n  Instruction and Reasoning Data Built from Permissive-First Text Sources",
            "url": "https://huggingface.co/papers/2509.25531",
            "abstract": "MixtureVitae is a pretraining corpus that combines public-domain and permissively licensed text with low-risk additions, achieving strong model performance across benchmarks while minimizing legal risk.  \t\t\t\t\tAI-generated summary \t\t\t\t We present MixtureVitae, an open-access pretraining corpus built to minimize legal risk while providing strong model performance. MixtureVitae follows a risk-mitigated sourcing strategy that combines public-domain and permissively licensed text (e.g., CC-BY/Apache) with carefully justified low-risk additions (e.g., government works and EU TDM-eligible sources), alongside targeted instruction, reasoning and synthetic data with documented provenance. We detail a transparent, multi-stage pipeline for license-aware filtering, safety and quality screening, and domain-aware mixing, and we release the dataset and curation recipes to support reproducible research. In controlled experiments using the open-sci-ref training protocol (fixed architectures at 130M/400M/1.3B/1.7B parameters; training budgets of 50B and 300B tokens), models trained on MixtureVitae consistently outperform other permissive datasets across a suite of standard benchmarks, and at the 1.7B/300B setting they surpass FineWeb-Edu and approach DCLM in the later stages of training. Performance is particularly strong on math/code and competitive on QA tasks. These results demonstrate that permissive-first, risk-mitigated data provides a practical and legally mitigated foundation for training capable LLMs, reducing reliance on indiscriminate web scraping without sacrificing competitiveness. Code: https://github.com/ontocord/mixturevitae",
            "score": 4,
            "issue_id": 6214,
            "pub_date": "2025-09-29",
            "pub_date_card": {
                "ru": "29 сентября",
                "en": "September 29",
                "zh": "9月29日"
            },
            "hash": "c795ac2fa9ea5a3f",
            "authors": [
                "Huu Nguyen",
                "Victor May",
                "Harsh Raj",
                "Marianna Nezhurina",
                "Yishan Wang",
                "Yanqi Luo",
                "Minh Chien Vu",
                "Taishi Nakamura",
                "Ken Tsui",
                "Van Khue Nguyen",
                "David Salinas",
                "Aleksandra Krasnodębska",
                "Christoph Schuhmann",
                "Mats Leon Richter",
                "Xuan-Son",
                "Vu",
                "Jenia Jitsev"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "Detomo Inc.",
                "ELLIS Institute Tuebingen",
                "Independent Researcher",
                "Institute of Science Tokyo",
                "Juelich Supercomputing Center (JSC), Research Center Juelich (FZJ)",
                "LAION",
                "Montreal Institute for Learning Algorithms, University of Montreal, Université de Montréal",
                "NASK",
                "Northeastern University",
                "Ontocord",
                "Open-Ψ (Open-Sci) Collective",
                "RSS Lab, LTH / DeepTensor AB",
                "Salesforce",
                "University of Freiburg",
                "École Polytechnique, IP Paris"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.25531.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#open_source",
                    "#benchmark",
                    "#low_resource",
                    "#data",
                    "#dataset"
                ],
                "emoji": "⚖️",
                "ru": {
                    "title": "Качественное предобучение LLM без юридических рисков",
                    "desc": "MixtureVitae — это открытый корпус данных для предобучения LLM, созданный с минимизацией юридических рисков. Датасет комбинирует тексты из общественного достояния и с пермиссивными лицензиями (CC-BY, Apache) с тщательно отобранными низкорисковыми источниками, синтетическими данными и данными для инструкций. В экспериментах на моделях от 130M до 1.7B параметров MixtureVitae показывает производительность, сопоставимую с FineWeb-Edu и приближающуюся к DCLM, особенно на задачах математики и кода. Результаты доказывают, что можно обучать конкурентоспособные LLM без использования неразборчивого веб-скрейпинга, опираясь на юридически безопасные источники данных."
                },
                "en": {
                    "title": "MixtureVitae: Legal Safety Meets High Performance in ML Training",
                    "desc": "MixtureVitae is a new pretraining dataset designed to enhance the performance of machine learning models while reducing legal risks associated with data usage. It combines public-domain texts and permissively licensed materials with low-risk additions, ensuring compliance with legal standards. The dataset undergoes a thorough filtering and quality screening process, making it suitable for training large language models (LLMs). Experimental results show that models trained on MixtureVitae outperform those trained on other datasets, particularly excelling in tasks related to mathematics and coding."
                },
                "zh": {
                    "title": "MixtureVitae：法律风险与模型性能的平衡之道",
                    "desc": "MixtureVitae是一个预训练语料库，旨在通过结合公共领域和许可文本，降低法律风险，同时实现强大的模型性能。该语料库采用风险缓解的来源策略，结合了经过合理选择的低风险补充内容，如政府作品和符合欧盟TDM标准的来源。我们详细介绍了一个透明的多阶段流程，用于许可证意识过滤、安全和质量筛选，以及领域意识混合。实验结果表明，使用MixtureVitae训练的模型在多个标准基准测试中表现优于其他许可数据集，尤其在数学/代码和问答任务上表现突出。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.23250",
            "title": "Training Vision-Language Process Reward Models for Test-Time Scaling in\n  Multimodal Reasoning: Key Insights and Lessons Learned",
            "url": "https://huggingface.co/papers/2509.23250",
            "abstract": "Hybrid data synthesis and perception-focused supervision improve the reliability of Vision-Language Process Reward Models (VL-PRMs) in guiding VLMs across diverse multimodal benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Process Reward Models (PRMs) provide step-level supervision that improves the reliability of reasoning in large language models. While PRMs have been extensively studied in text-based domains, their extension to Vision Language Models (VLMs) remains limited. Existing Vision-Language PRMs (VL-PRMs) rely on Monte Carlo Tree Search (MCTS) for data construction, which can often produce noisy supervision signals and limit generalization across tasks. In this work, we aim to elucidate the design space of VL-PRMs by exploring diverse strategies for dataset construction, training, and test-time scaling. First, we introduce a hybrid data synthesis framework that combines MCTS with judgments from a strong VLM, producing more accurate step-level labels. Second, we propose perception-focused supervision, enabling our PRM to explicitly detect errors at the visual grounding stage of reasoning. Third, we systematically evaluate multiple test-time scaling strategies, showing that our PRMs can reliably guide VLMs toward more accurate solutions. Our experiments covering five diverse multimodal benchmarks (MMMU, PuzzleVQA, AlgoPuzzleVQA, MathVista, and MathVision) reveal several key insights: (i) VL-PRMs when used as Outcome Reward Models (ORMs) during test-time scaling (TTS) can outperform VL-PRM guided process step selection, (ii) smaller VL-PRMs can match or even surpass larger ones in detecting process errors, (iii) VL-PRMs uncover latent reasoning abilities in stronger VLM backbones, (iv) perception-level supervision leads to significant gains in test-time scaling, and (v) TTS performance of different policies improve on advanced math reasoning datasets despite not training VL-PRMs on such datasets. We hope our work will motivate further research and support the advancement of VLMs.",
            "score": 4,
            "issue_id": 6198,
            "pub_date": "2025-09-27",
            "pub_date_card": {
                "ru": "27 сентября",
                "en": "September 27",
                "zh": "9月27日"
            },
            "hash": "c59cc3e092f9a705",
            "authors": [
                "Brandon Ong",
                "Tej Deep Pala",
                "Vernon Toh",
                "William Chandra Tjhi",
                "Soujanya Poria"
            ],
            "affiliations": [
                "AI Singapore",
                "Nanyang Technological University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.23250.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#training",
                    "#games",
                    "#multimodal",
                    "#data",
                    "#dataset"
                ],
                "emoji": "👁️",
                "ru": {
                    "title": "Учим мультимодальные модели проверять свои рассуждения пошагово",
                    "desc": "Статья представляет улучшенные Vision-Language Process Reward Models (VL-PRMs), которые обеспечивают пошаговый контроль качества рассуждений в мультимодальных языковых моделях. Авторы предлагают гибридный подход к синтезу данных, комбинируя MCTS с оценками сильной VLM, и вводят специальную supervision на уровне визуального восприятия. Эксперименты на пяти бенчмарках показывают, что даже небольшие VL-PRMs могут эффективно выявлять ошибки в рассуждениях и улучшать производительность базовых моделей во время инференса. Ключевое открытие: VL-PRMs как Outcome Reward Models превосходят пошаговую селекцию, а supervision на уровне восприятия существенно повышает качество test-time scaling."
                },
                "en": {
                    "title": "Enhancing Vision-Language Models with Hybrid Supervision and Data Synthesis",
                    "desc": "This paper discusses improvements in Vision-Language Process Reward Models (VL-PRMs) to enhance their effectiveness in guiding Vision Language Models (VLMs). The authors introduce a hybrid data synthesis method that merges Monte Carlo Tree Search with insights from a robust VLM, resulting in more precise step-level supervision. They also propose a perception-focused supervision approach that helps the model identify errors during visual reasoning. Through extensive testing on various multimodal benchmarks, the study demonstrates that these enhancements lead to better performance and reliability in VLMs, even in complex reasoning tasks."
                },
                "zh": {
                    "title": "混合数据合成与感知监督提升视觉语言模型的可靠性",
                    "desc": "本研究提出了一种混合数据合成框架，结合了蒙特卡洛树搜索（MCTS）和强大的视觉语言模型（VLM）的判断，以生成更准确的步骤级标签。我们还引入了以感知为中心的监督，帮助过程奖励模型（PRM）在推理的视觉基础阶段明确检测错误。通过系统评估多种测试时扩展策略，我们的实验表明，视觉语言过程奖励模型（VL-PRM）能够可靠地引导VLM朝向更准确的解决方案。我们的研究结果为进一步研究和视觉语言模型的进步提供了重要的见解。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.01152",
            "title": "Pay-Per-Search Models are Abstention Models",
            "url": "https://huggingface.co/papers/2510.01152",
            "abstract": "MASH, a reinforcement learning framework, improves LLMs' selective help-seeking and abstention capabilities without pre-determined knowledge boundaries.  \t\t\t\t\tAI-generated summary \t\t\t\t LLMs cannot reliably recognize their parametric knowledge boundaries and often hallucinate answers to outside-of-boundary questions. In contrast, humans recognize their limitations and can either seek external help for such questions or abstain. In this paper, we introduce MASH (Modeling Abstention via Selective Help-seeking), a training framework that readily extracts abstentions from LLMs. Our key idea is that any external help-seeking by an LLM, i.e. search tool use, can serve as a proxy for abstention if the external help (search) is appropriately penalized while simultaneously rewarding answer accuracy. MASH operationalizes this idea using reinforcement learning with a pay-per-search reward.   We run experiments on three knowledge-intensive QA datasets. Our results show that MASH substantially improves upon the selective help-seeking performance of prior efficient search approaches; on multi-hop datasets, MASH improves answer accuracy by 7.6%. Furthermore, MASH demonstrates strong off-the-shelf abstention -- it can distinguish between unanswerable/answerable questions and selectively generate responses for answerable questions -- showcasing behavior analogous to specialized abstention approaches. We emphasize that contrary to prior abstention methods, MASH does not require pre-determining knowledge boundaries to construct training data. Instead, MASH's abstentions are a by-product of training for the auxiliary selective help-seeking task. Overall, we show that MASH training effectively aligns search tool use with parametric knowledge, which can be successfully leveraged for making abstention decisions.",
            "score": 3,
            "issue_id": 6215,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 октября",
                "en": "October 1",
                "zh": "10月1日"
            },
            "hash": "7eb9492df9e01cd2",
            "authors": [
                "Mustafa Omer Gul",
                "Claire Cardie",
                "Tanya Goyal"
            ],
            "affiliations": [
                "Department of Computer Science, Cornell University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.01152.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#alignment",
                    "#reasoning",
                    "#rl",
                    "#hallucinations"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Научить LLM говорить «не знаю» через штраф за поиск",
                    "desc": "Статья представляет MASH — фреймворк на основе reinforcement learning, который учит большие языковые модели воздерживаться от ответов на вопросы вне их знаний. Ключевая идея: использование внешних инструментов поиска становится сигналом для отказа от ответа, если за каждый поиск назначается штраф, а за точность ответа — награда. В отличие от предыдущих методов, MASH не требует заранее определять границы знаний модели для создания обучающих данных. Эксперименты показывают улучшение точности ответов на 7.6% на мультихоповых датасетах и способность модели различать отвечаемые и неотвечаемые вопросы."
                },
                "en": {
                    "title": "MASH: Empowering LLMs with Smart Help-Seeking and Abstention",
                    "desc": "MASH is a reinforcement learning framework designed to enhance the selective help-seeking and abstention abilities of large language models (LLMs). Unlike traditional methods, MASH does not rely on pre-defined knowledge boundaries, allowing LLMs to better recognize when they should seek external help or abstain from answering. The framework uses a pay-per-search reward system to encourage accurate answers while penalizing unnecessary searches, effectively aligning the model's search tool usage with its knowledge capabilities. Experimental results demonstrate that MASH significantly improves the performance of LLMs in answering complex questions and making appropriate abstention decisions."
                },
                "zh": {
                    "title": "MASH：提升LLMs的选择性寻求与放弃能力",
                    "desc": "MASH是一种强化学习框架，旨在提高大型语言模型（LLMs）在选择性寻求帮助和放弃回答能力方面的表现，而无需预先确定知识边界。与人类能够识别自身局限性并选择寻求外部帮助或放弃不同，LLMs常常无法可靠地识别其知识边界，导致错误回答。MASH通过对外部帮助的适当惩罚和对回答准确性的奖励，利用强化学习来优化这一过程。实验结果表明，MASH在多跳问答数据集上显著提高了回答准确性，并能够有效区分可回答和不可回答的问题。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.00510",
            "title": "JoyAgent-JDGenie: Technical Report on the GAIA",
            "url": "https://huggingface.co/papers/2510.00510",
            "abstract": "A generalist agent architecture combining multi-agent planning, hierarchical memory, and a refined tool suite outperforms existing systems in diverse tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models are increasingly deployed as autonomous agents for complex real-world tasks, yet existing systems often focus on isolated improvements without a unifying design for robustness and adaptability. We propose a generalist agent architecture that integrates three core components: a collective multi-agent framework combining planning and execution agents with critic model voting, a hierarchical memory system spanning working, semantic, and procedural layers, and a refined tool suite for search, code execution, and multimodal parsing. Evaluated on a comprehensive benchmark, our framework consistently outperforms open-source baselines and approaches the performance of proprietary systems. These results demonstrate the importance of system-level integration and highlight a path toward scalable, resilient, and adaptive AI assistants capable of operating across diverse domains and tasks.",
            "score": 3,
            "issue_id": 6198,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 октября",
                "en": "October 1",
                "zh": "10月1日"
            },
            "hash": "4a1605179598a812",
            "authors": [
                "Jiarun Liu",
                "Shiyue Xu",
                "Shangkun Liu",
                "Yang Li",
                "Wen Liu",
                "Min Liu",
                "Xiaoqing Zhou",
                "Hanmin Wang",
                "Shilin Jia",
                "zhen Wang",
                "Shaohua Tian",
                "Hanhao Li",
                "Junbo Zhang",
                "Yongli Yu",
                "Peng Cao",
                "Haofen Wang"
            ],
            "affiliations": [
                "GAIA JINGDONG CHO-EI Team"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.00510.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#benchmark",
                    "#optimization",
                    "#architecture",
                    "#multimodal",
                    "#agents",
                    "#agi"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Универсальный AI-агент через интеграцию памяти, планирования и инструментов",
                    "desc": "В статье предлагается универсальная архитектура AI-агента, которая объединяет три ключевых компонента для решения сложных задач. Первый компонент — это мультиагентная система с планированием, исполнением и голосованием критических моделей. Второй — иерархическая память, включающая рабочий, семантический и процедурный уровни. Третий — набор инструментов для поиска, выполнения кода и мультимодального парсинга. Система превосходит open-source решения и приближается к производительности проприетарных систем."
                },
                "en": {
                    "title": "Empowering AI with Integrated Generalist Agent Architecture",
                    "desc": "This paper presents a new architecture for generalist agents that enhances their performance in various tasks. It combines multi-agent planning, where different agents work together to make decisions, with a hierarchical memory system that organizes information at different levels. Additionally, it includes a refined tool suite that allows the agent to perform tasks like searching and executing code. The proposed system shows significant improvements over existing models, indicating that integrating these components leads to more robust and adaptable AI assistants."
                },
                "zh": {
                    "title": "通用智能体架构：提升AI助手的适应性与鲁棒性",
                    "desc": "本文提出了一种通用智能体架构，结合了多智能体规划、分层记忆和精细化工具套件，能够在多种任务中超越现有系统。该架构整合了集体多智能体框架、分层记忆系统以及用于搜索、代码执行和多模态解析的工具。通过全面的基准测试，我们的框架在性能上持续优于开源基线，并接近专有系统的表现。这些结果表明系统级集成的重要性，并为可扩展、弹性和适应性强的人工智能助手指明了方向。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.01037",
            "title": "CurES: From Gradient Analysis to Efficient Curriculum Learning for\n  Reasoning LLMs",
            "url": "https://huggingface.co/papers/2510.01037",
            "abstract": "CurES, a reinforcement learning-based method, improves the training efficiency of large language models by optimizing prompt selection and rollout allocation, leading to faster convergence and reduced computational overhead.  \t\t\t\t\tAI-generated summary \t\t\t\t Curriculum learning plays a crucial role in enhancing the training efficiency of large language models (LLMs) on reasoning tasks. However, existing methods often fail to adequately account for variations in prompt difficulty or rely on simplistic filtering mechanisms to select prompt datasets within a narrow criterion range, resulting in significant computational waste. In this work, we approach the problem from the perspective of reinforcement learning gradient optimization, offering a systematic and theoretical investigation into how to improve the training efficiency of LLMs. We identify two key factors influencing training efficiency: the selection of training prompts and the allocation of rollout quantities across different prompts. Our theoretical analysis reveals that the sampling distribution of prompts dictates the convergence rate of gradient descent, while the allocation of the rollout quantity influences the consistency and stability of overall gradient updates. Based on these insights, we propose CurES, an efficient training method that accelerates convergence and employs Bayesian posterior estimation to minimize computational overhead. Experiments demonstrate that our CurES outperforms Group Relative Policy Optimization (GRPO) by +3.30 points and +4.82 points with 1.5B and 7B models, respectively. Additionally, CurES exhibits faster convergence compared to baselines, including GRPO.",
            "score": 2,
            "issue_id": 6204,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 октября",
                "en": "October 1",
                "zh": "10月1日"
            },
            "hash": "a8919e29862921e6",
            "authors": [
                "Yongcheng Zeng",
                "Zexu Sun",
                "Bokai Ji",
                "Erxue Min",
                "Hengyi Cai",
                "Shuaiqiang Wang",
                "Dawei Yin",
                "Haifeng Zhang",
                "Xu Chen",
                "Jun Wang"
            ],
            "affiliations": [
                "Baidu Inc.",
                "Gaoling School of Artificial Intelligence, Renmin University of China",
                "Institute of Automation, Chinese Academy of Sciences",
                "School of Artificial Intelligence, University of Chinese Academy of Sciences",
                "University College London"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.01037.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#reasoning",
                    "#rl"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Умный выбор промптов ускоряет обучение языковых моделей",
                    "desc": "CurES — это метод на основе reinforcement learning, который оптимизирует процесс обучения больших языковых моделей путём умного выбора промптов и распределения вычислительных ресурсов. Авторы показали, что распределение промптов влияет на скорость сходимости gradient descent, а количество rollouts определяет стабильность обновления градиентов. Метод использует байесовскую оценку для минимизации вычислительных затрат и демонстрирует превосходство над baseline методом GRPO на 3-5 баллов. CurES обеспечивает более быструю сходимость и эффективное использование ресурсов при обучении LLM на задачах reasoning."
                },
                "en": {
                    "title": "Optimizing Prompt Selection for Efficient LLM Training with CurES",
                    "desc": "CurES is a novel method that enhances the training efficiency of large language models (LLMs) by using reinforcement learning to optimize how prompts are selected and how rollout allocations are managed. It addresses the shortcomings of traditional curriculum learning approaches, which often overlook the complexity of prompt difficulty and lead to inefficient training processes. By analyzing the impact of prompt sampling distributions and rollout allocations, CurES improves the convergence rate of gradient descent and stabilizes gradient updates. Experimental results show that CurES significantly outperforms existing methods, achieving faster convergence and reduced computational costs."
                },
                "zh": {
                    "title": "CurES：提升大型语言模型训练效率的创新方法",
                    "desc": "CurES是一种基于强化学习的方法，旨在通过优化提示选择和回滚分配来提高大型语言模型的训练效率。这种方法解决了现有技术在处理提示难度变化时的不足，避免了不必要的计算浪费。通过理论分析，我们发现提示的采样分布和回滚数量的分配是影响训练效率的两个关键因素。实验结果表明，CurES在加速收敛和减少计算开销方面优于现有的优化方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.00777",
            "title": "In-Place Feedback: A New Paradigm for Guiding LLMs in Multi-Turn\n  Reasoning",
            "url": "https://huggingface.co/papers/2510.00777",
            "abstract": "In-place feedback allows users to directly edit LLM responses, improving performance and reducing token usage in multi-turn reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are increasingly studied in the context of multi-turn reasoning, where models iteratively refine their outputs based on user-provided feedback. Such settings are crucial for tasks that require complex reasoning, yet existing feedback paradigms often rely on issuing new messages. LLMs struggle to integrate these reliably, leading to inconsistent improvements. In this work, we introduce in-place feedback, a novel interaction paradigm in which users directly edit an LLM's previous response, and the model conditions on this modified response to generate its revision. Empirical evaluations on diverse reasoning-intensive benchmarks reveal that in-place feedback achieves better performance than conventional multi-turn feedback while using 79.1% fewer tokens. Complementary analyses on controlled environments further demonstrate that in-place feedback resolves a core limitation of multi-turn feedback: models often fail to apply feedback precisely to erroneous parts of the response, leaving errors uncorrected and sometimes introducing new mistakes into previously correct content. These findings suggest that in-place feedback offers a more natural and effective mechanism for guiding LLMs in reasoning-intensive tasks.",
            "score": 2,
            "issue_id": 6198,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 октября",
                "en": "October 1",
                "zh": "10月1日"
            },
            "hash": "be36c5f4f29f17e4",
            "authors": [
                "Youngbin Choi",
                "Minjong Lee",
                "Saemi Moon",
                "Seunghyuk Cho",
                "Chaehyeon Chung",
                "MoonJeong Park",
                "Dongwoo Kim"
            ],
            "affiliations": [
                "Computer Science and Engineering, POSTECH",
                "Graduate School of Artificial Intelligence, POSTECH"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.00777.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rlhf",
                    "#interpretability",
                    "#reasoning"
                ],
                "emoji": "✏️",
                "ru": {
                    "title": "Редактируй прямо здесь: эффективная обратная связь для LLM",
                    "desc": "Исследователи предложили новый подход взаимодействия с языковыми моделями под названием in-place feedback, при котором пользователи напрямую редактируют ответы LLM вместо отправки новых сообщений. Эксперименты на задачах многошагового рассуждения показали, что этот метод улучшает производительность модели и снижает использование токенов на 79,1% по сравнению с традиционной многошаговой обратной связью. Ключевое преимущество заключается в том, что модели лучше применяют исправления именно к ошибочным частям ответа, избегая появления новых ошибок в ранее корректном содержании. Подход демонстрирует более естественный и эффективный механизм управления LLM в задачах, требующих сложных рассуждений."
                },
                "en": {
                    "title": "In-Place Feedback: Direct Edits for Smarter LLMs",
                    "desc": "This paper presents a new method called in-place feedback for improving large language models (LLMs) during multi-turn reasoning tasks. Instead of sending new messages for feedback, users can directly edit the model's previous responses, allowing the model to learn from these modifications. The results show that this approach not only enhances the model's performance but also significantly reduces the number of tokens used by 79.1%. Overall, in-place feedback addresses the limitations of traditional feedback methods by enabling more precise corrections and reducing the introduction of new errors."
                },
                "zh": {
                    "title": "就地反馈：提升LLM推理的有效新方式",
                    "desc": "本研究提出了一种新的交互模式——就地反馈，允许用户直接编辑大型语言模型（LLM）的响应。这种方法在多轮推理任务中表现出色，能够显著提高模型的性能，同时减少79.1%的令牌使用。通过实证评估，我们发现就地反馈比传统的多轮反馈更有效，能够更准确地应用用户的反馈，避免了模型在修正错误时引入新的错误。总的来说，就地反馈为指导LLM在复杂推理任务中提供了一种更自然和有效的机制。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.19185",
            "title": "An Empirical Study of Testing Practices in Open Source AI Agent\n  Frameworks and Agentic Applications",
            "url": "https://huggingface.co/papers/2509.19185",
            "abstract": "The study identifies testing practices in AI agent frameworks and applications, highlighting a focus on deterministic components and a neglect of the Trigger component, suggesting improvements for robustness.  \t\t\t\t\tAI-generated summary \t\t\t\t Foundation model (FM)-based AI agents are rapidly gaining adoption across diverse domains, but their inherent non-determinism and non-reproducibility pose testing and quality assurance challenges. While recent benchmarks provide task-level evaluations, there is limited understanding of how developers verify the internal correctness of these agents during development.   To address this gap, we conduct the first large-scale empirical study of testing practices in the AI agent ecosystem, analyzing 39 open-source agent frameworks and 439 agentic applications. We identify ten distinct testing patterns and find that novel, agent-specific methods like DeepEval are seldom used (around 1%), while traditional patterns like negative and membership testing are widely adapted to manage FM uncertainty. By mapping these patterns to canonical architectural components of agent frameworks and agentic applications, we uncover a fundamental inversion of testing effort: deterministic components like Resource Artifacts (tools) and Coordination Artifacts (workflows) consume over 70% of testing effort, while the FM-based Plan Body receives less than 5%. Crucially, this reveals a critical blind spot, as the Trigger component (prompts) remains neglected, appearing in around 1% of all tests.   Our findings offer the first empirical testing baseline in FM-based agent frameworks and agentic applications, revealing a rational but incomplete adaptation to non-determinism. To address it, framework developers should improve support for novel testing methods, application developers must adopt prompt regression testing, and researchers should explore barriers to adoption. Strengthening these practices is vital for building more robust and dependable AI agents.",
            "score": 2,
            "issue_id": 6199,
            "pub_date": "2025-09-23",
            "pub_date_card": {
                "ru": "23 сентября",
                "en": "September 23",
                "zh": "9月23日"
            },
            "hash": "e00a1db1e43fa4c8",
            "authors": [
                "Mohammed Mehedi Hasan",
                "Hao Li",
                "Emad Fallahzadeh",
                "Gopi Krishnan Rajbahadur",
                "Bram Adams",
                "Ahmed E. Hassan"
            ],
            "affiliations": [
                "School of Computing, Queens University, Kingston, ON, Canada"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.19185.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#agents",
                    "#security",
                    "#open_source",
                    "#training"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Тестирование AI-агентов: фокус не там, где нужно",
                    "desc": "Исследование анализирует практики тестирования в 39 фреймворках AI-агентов и 439 приложениях, выявляя десять паттернов тестирования. Обнаружена критическая проблема: более 70% усилий по тестированию направлено на детерминированные компоненты (инструменты и воркфлоу), в то время как компоненты на основе foundation models получают менее 5% внимания. Особенно проблематично, что промпты (Trigger component) практически не тестируются, появляясь лишь в 1% тестов. Авторы призывают разработчиков фреймворков улучшить поддержку новых методов тестирования, а разработчиков приложений — внедрить regression-тестирование промптов для повышения надёжности AI-агентов."
                },
                "en": {
                    "title": "Enhancing AI Agent Testing: Bridging the Gap in Robustness",
                    "desc": "This paper investigates the testing practices used in AI agent frameworks and applications, revealing a significant focus on deterministic components while largely neglecting the Trigger component. The study analyzes 39 open-source frameworks and 439 applications, identifying ten distinct testing patterns, with traditional methods dominating the landscape. It highlights that over 70% of testing effort is spent on deterministic components, while less than 5% is allocated to the FM-based Plan Body, indicating a critical oversight. The authors suggest that improving testing methods and incorporating prompt regression testing are essential for enhancing the robustness of AI agents."
                },
                "zh": {
                    "title": "提升AI代理测试的鲁棒性",
                    "desc": "本研究分析了人工智能代理框架和应用中的测试实践，发现目前的测试主要集中在确定性组件上，而触发组件却被忽视。我们对39个开源代理框架和439个代理应用进行了大规模实证研究，识别出十种不同的测试模式。结果显示，像DeepEval这样的新型代理特定方法使用率极低，而传统的负面测试和成员测试被广泛应用以应对基础模型的不确定性。为了提高AI代理的鲁棒性，开发者需要改进对新测试方法的支持，并在应用中采用提示回归测试。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.01070",
            "title": "Eliciting Secret Knowledge from Language Models",
            "url": "https://huggingface.co/papers/2510.01070",
            "abstract": "Researchers develop and evaluate techniques to uncover hidden knowledge in large language models through black-box and white-box methods, with prefill attacks and logit lens being particularly effective.  \t\t\t\t\tAI-generated summary \t\t\t\t We study secret elicitation: discovering knowledge that an AI possesses but does not explicitly verbalize. As a testbed, we train three families of large language models (LLMs) to possess specific knowledge that they apply downstream but deny knowing when asked directly. For example, in one setting, we train an LLM to generate replies that are consistent with knowing the user is female, while denying this knowledge when asked directly. We then design various black-box and white-box secret elicitation techniques and evaluate them based on whether they can help an LLM auditor successfully guess the secret knowledge. Many of our techniques improve on simple baselines. Our most effective techniques (performing best in 2/3 settings) are based on prefill attacks, a black-box technique where the LLM reveals secret knowledge when generating a completion from a predefined prefix. In our remaining setting, white-box techniques based on logit lens and sparse autoencoders (SAEs) are most effective. We release our models and code, establishing a public benchmark for evaluating secret elicitation methods.",
            "score": 1,
            "issue_id": 6203,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 октября",
                "en": "October 1",
                "zh": "10月1日"
            },
            "hash": "f0f210658ad43fde",
            "authors": [
                "Bartosz Cywiński",
                "Emil Ryd",
                "Rowan Wang",
                "Senthooran Rajamanoharan",
                "Neel Nanda",
                "Arthur Conmy",
                "Samuel Marks"
            ],
            "affiliations": [
                "Anthropic",
                "IDEAS Research Institute",
                "University of Oxford",
                "Warsaw University of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.01070.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#interpretability",
                    "#multimodal",
                    "#hallucinations",
                    "#open_source"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Раскрытие тайных знаний: как заставить LLM признаться в том, что она скрывает",
                    "desc": "Исследователи изучают методы извлечения скрытых знаний из больших языковых моделей (LLM), которые модель использует внутренне, но явно не озвучивает. Для экспериментов они обучили модели обладать специфическими знаниями (например, о поле пользователя), которые влияют на генерацию ответов, но отрицаются при прямых вопросах. Наиболее эффективными оказались prefill-атаки в black-box режиме, где модель раскрывает секреты при генерации продолжения заданного префикса, а также white-box техники на основе logit lens и sparse autoencoders. Авторы опубликовали модели и код, создав бенчмарк для оценки методов извлечения скрытой информации из AI-систем."
                },
                "en": {
                    "title": "Unveiling Secrets: Extracting Hidden Knowledge from Language Models",
                    "desc": "This paper explores methods to extract hidden knowledge from large language models (LLMs) that they do not openly disclose. The researchers train LLMs to possess specific knowledge while denying it when questioned directly. They introduce both black-box and white-box techniques for secret elicitation, with prefill attacks showing significant effectiveness. The study also provides a public benchmark for evaluating these secret elicitation methods, contributing to the understanding of LLM behavior."
                },
                "zh": {
                    "title": "揭示AI隐藏知识的创新技术",
                    "desc": "研究人员开发并评估了通过黑箱和白箱方法揭示大型语言模型中隐藏知识的技术。我们训练了三类大型语言模型，使其能够在应用特定知识时拒绝承认这一知识。我们设计了多种黑箱和白箱的秘密引出技术，并评估它们在帮助审计者成功猜测秘密知识方面的有效性。我们的技术在多个设置中表现优异，尤其是基于预填攻击的黑箱技术和基于logit lens的白箱技术。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.01061",
            "title": "ReSWD: ReSTIR'd, not shaken. Combining Reservoir Sampling and Sliced\n  Wasserstein Distance for Variance Reduction",
            "url": "https://huggingface.co/papers/2510.01061",
            "abstract": "Reservoir SWD reduces variance in Sliced Wasserstein Distance, improving gradient stability and performance in vision and graphics tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Distribution matching is central to many vision and graphics tasks, where the widely used Wasserstein distance is too costly to compute for high dimensional distributions. The Sliced Wasserstein Distance (SWD) offers a scalable alternative, yet its Monte Carlo estimator suffers from high variance, resulting in noisy gradients and slow convergence. We introduce Reservoir SWD (ReSWD), which integrates Weighted Reservoir Sampling into SWD to adaptively retain informative projection directions in optimization steps, resulting in stable gradients while remaining unbiased. Experiments on synthetic benchmarks and real-world tasks such as color correction and diffusion guidance show that ReSWD consistently outperforms standard SWD and other variance reduction baselines. Project page: https://reservoirswd.github.io/",
            "score": 1,
            "issue_id": 6203,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 октября",
                "en": "October 1",
                "zh": "10月1日"
            },
            "hash": "a1e6afd30da770db",
            "authors": [
                "Mark Boss",
                "Andreas Engelhardt",
                "Simon Donné",
                "Varun Jampani"
            ],
            "affiliations": [
                "Stability AI",
                "University of Tübingen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.01061.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#benchmark",
                    "#synthetic",
                    "#diffusion",
                    "#training",
                    "#cv"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Стабильные градиенты для сравнения распределений через резервуарную выборку",
                    "desc": "Статья предлагает новый метод Reservoir SWD для эффективного сравнения распределений в задачах компьютерного зрения и графики. Классический Wasserstein distance слишком дорог вычислительно, а его аппроксимация Sliced Wasserstein Distance страдает от высокой дисперсии при оценке методом Монте-Карло. Авторы интегрируют Weighted Reservoir Sampling для адаптивного сохранения информативных направлений проекций, что стабилизирует градиенты и ускоряет сходимость оптимизации. Эксперименты на задачах цветокоррекции и диффузионных моделях демонстрируют превосходство ReSWD над стандартным SWD."
                },
                "en": {
                    "title": "Stabilizing Gradients with Reservoir SWD for Better Performance",
                    "desc": "This paper presents Reservoir SWD (ReSWD), a novel approach that enhances the Sliced Wasserstein Distance (SWD) by reducing its variance. The traditional SWD is effective for matching distributions in vision and graphics but suffers from high variance in its Monte Carlo estimations, leading to unstable gradients. By incorporating Weighted Reservoir Sampling, ReSWD retains the most informative projection directions, which stabilizes the gradient and improves convergence rates. Experimental results demonstrate that ReSWD outperforms both standard SWD and other methods aimed at variance reduction in various tasks."
                },
                "zh": {
                    "title": "Reservoir SWD：提升切片Wasserstein距离的稳定性与性能",
                    "desc": "本文提出了一种新的方法，称为Reservoir SWD（ReSWD），旨在减少切片Wasserstein距离（SWD）中的方差，从而提高梯度的稳定性和性能。传统的SWD在高维分布中计算成本较高，而ReSWD通过将加权水库抽样技术整合到SWD中，能够自适应地保留有用的投影方向。实验结果表明，ReSWD在合成基准和实际任务（如颜色校正和扩散引导）中，均优于标准SWD和其他方差减少基线。该方法为视觉和图形任务中的分布匹配提供了更有效的解决方案。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.00438",
            "title": "BindWeave: Subject-Consistent Video Generation via Cross-Modal\n  Integration",
            "url": "https://huggingface.co/papers/2510.00438",
            "abstract": "BindWeave, a unified framework using MLLM-DiT, enhances subject-consistent video generation by integrating deep cross-modal reasoning with diffusion transformers, achieving superior performance on OpenS2V.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion Transformer has shown remarkable abilities in generating high-fidelity videos, delivering visually coherent frames and rich details over extended durations. However, existing video generation models still fall short in subject-consistent video generation due to an inherent difficulty in parsing prompts that specify complex spatial relationships, temporal logic, and interactions among multiple subjects. To address this issue, we propose BindWeave, a unified framework that handles a broad range of subject-to-video scenarios from single-subject cases to complex multi-subject scenes with heterogeneous entities. To bind complex prompt semantics to concrete visual subjects, we introduce an MLLM-DiT framework in which a pretrained multimodal large language model performs deep cross-modal reasoning to ground entities and disentangle roles, attributes, and interactions, yielding subject-aware hidden states that condition the diffusion transformer for high-fidelity subject-consistent video generation. Experiments on the OpenS2V benchmark demonstrate that our method achieves superior performance across subject consistency, naturalness, and text relevance in generated videos, outperforming existing open-source and commercial models.",
            "score": 1,
            "issue_id": 6209,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 октября",
                "en": "October 1",
                "zh": "10月1日"
            },
            "hash": "3a3278716f39fb1e",
            "authors": [
                "Zhaoyang Li",
                "Dongjun Qian",
                "Kai Su",
                "Qishuai Diao",
                "Xiangyang Xia",
                "Chang Liu",
                "Wenfei Yang",
                "Tianzhu Zhang",
                "Zehuan Yuan"
            ],
            "affiliations": [
                "ByteDance",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.00438.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#diffusion",
                    "#open_source",
                    "#multimodal",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "BindWeave: связывание субъектов и видео через глубокое понимание промптов",
                    "desc": "BindWeave — это фреймворк для генерации видео с согласованными субъектами, который решает проблему сохранения идентичности персонажей и объектов на протяжении всего видео. Система использует мультимодальную языковую модель (MLLM) совместно с diffusion transformer (DiT) для глубокого анализа сложных текстовых промптов с пространственными отношениями и взаимодействиями между объектами. MLLM выполняет кросс-модальное рассуждение, разделяя роли, атрибуты и взаимодействия субъектов, создавая скрытые состояния, которые управляют диффузионной моделью. На бенчмарке OpenS2V метод превосходит существующие open-source и коммерческие модели по согласованности субъектов, естественности и соответствию тексту."
                },
                "en": {
                    "title": "BindWeave: Consistent Video Generation through Deep Cross-Modal Reasoning",
                    "desc": "BindWeave is a new framework that improves video generation by ensuring that the subjects in the videos remain consistent with the prompts given. It uses a combination of a multimodal large language model and diffusion transformers to understand complex relationships and interactions between multiple subjects. This approach allows the model to generate high-quality videos that accurately reflect the specified details and dynamics of the scene. Experiments show that BindWeave outperforms other models in terms of subject consistency and overall video quality."
                },
                "zh": {
                    "title": "BindWeave：提升视频生成的主题一致性",
                    "desc": "BindWeave是一个统一框架，利用MLLM-DiT技术，提升了视频生成中的主题一致性。它通过深度跨模态推理与扩散变换器的结合，解决了现有模型在处理复杂空间关系和多主体交互时的不足。该框架能够处理从单一主体到复杂多主体场景的各种视频生成任务。实验结果表明，BindWeave在主题一致性、自然性和文本相关性方面的表现优于现有的开源和商业模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.26514",
            "title": "BatonVoice: An Operationalist Framework for Enhancing Controllable\n  Speech Synthesis with Linguistic Intelligence from LLMs",
            "url": "https://huggingface.co/papers/2509.26514",
            "abstract": "BatonVoice framework decouples instruction understanding from speech generation, using an LLM to create vocal feature plans and a specialized TTS model to produce speech, achieving strong performance in controllable and emotional speech synthesis with zero-shot cross-lingual generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t The rise of Large Language Models (LLMs) is reshaping multimodel models, with speech synthesis being a prominent application. However, existing approaches often underutilize the linguistic intelligence of these models, typically failing to leverage their powerful instruction-following capabilities. This limitation hinders the model's ability to follow text instructions for controllable Text-to-Speech~(TTS). To address this, we propose a new paradigm inspired by ``operationalism'' that decouples instruction understanding from speech generation. We introduce BatonVoice, a framework where an LLM acts as a ``conductor'', understanding user instructions and generating a textual ``plan'' -- explicit vocal features (e.g., pitch, energy). A separate TTS model, the ``orchestra'', then generates the speech from these features. To realize this component, we develop BatonTTS, a TTS model trained specifically for this task. Our experiments demonstrate that BatonVoice achieves strong performance in controllable and emotional speech synthesis, outperforming strong open- and closed-source baselines. Notably, our approach enables remarkable zero-shot cross-lingual generalization, accurately applying feature control abilities to languages unseen during post-training. This demonstrates that objectifying speech into textual vocal features can more effectively unlock the linguistic intelligence of LLMs.",
            "score": 1,
            "issue_id": 6215,
            "pub_date": "2025-09-30",
            "pub_date_card": {
                "ru": "30 сентября",
                "en": "September 30",
                "zh": "9月30日"
            },
            "hash": "4c62f45aca4d07bc",
            "authors": [
                "Yue Wang",
                "Ruotian Ma",
                "Xingyu Chen",
                "Zhengliang Shi",
                "Wanshun Chen",
                "Huang Liu",
                "Jiadi Yao",
                "Qu Yang",
                "Qingxuan Jiang",
                "Fanghua Ye",
                "Juntao Li",
                "Min Zhang",
                "Zhaopeng Tu",
                "Xiaolong Li",
                "Linus"
            ],
            "affiliations": [
                "Soochow University",
                "Tencent Multimodal Department"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.26514.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#multimodal",
                    "#open_source",
                    "#audio",
                    "#games"
                ],
                "emoji": "🎼",
                "ru": {
                    "title": "Дирижёр и оркестр: LLM планирует, TTS исполняет",
                    "desc": "Статья представляет фреймворк BatonVoice, который разделяет понимание инструкций и генерацию речи: LLM выступает как \"дирижёр\", интерпретируя текстовые команды пользователя и создавая план вокальных характеристик (высота тона, энергия и т.д.). Отдельная TTS-модель BatonTTS действует как \"оркестр\", синтезируя речь на основе этих явных признаков. Подход демонстрирует высокое качество в управляемом и эмоциональном синтезе речи, превосходя существующие решения. Особенно примечательна способность к zero-shot кросс-лингвальной генерализации — модель применяет навыки контроля к языкам, не встречавшимся при обучении."
                },
                "en": {
                    "title": "Decoupling Understanding and Speech for Enhanced Synthesis",
                    "desc": "The BatonVoice framework separates the understanding of instructions from the generation of speech, utilizing a Large Language Model (LLM) to create detailed vocal feature plans. This approach allows for more precise control over speech synthesis, enabling emotional and controllable outputs. A specialized Text-to-Speech (TTS) model, BatonTTS, then converts these plans into actual speech. The framework shows impressive performance, including the ability to generalize across languages without prior training, highlighting the effectiveness of using explicit vocal features."
                },
                "zh": {
                    "title": "BatonVoice：解耦指令理解与语音生成的创新框架",
                    "desc": "BatonVoice框架将指令理解与语音生成解耦，利用大型语言模型（LLM）生成语音特征计划，并使用专门的文本转语音（TTS）模型进行语音合成。该方法在可控和情感语音合成方面表现出色，并实现了零样本跨语言泛化。通过将语音对象化为文本语音特征，BatonVoice有效地释放了LLM的语言智能。实验结果表明，BatonVoice在多个基准测试中超越了强大的开源和闭源模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.25916",
            "title": "VLM-FO1: Bridging the Gap Between High-Level Reasoning and Fine-Grained\n  Perception in VLMs",
            "url": "https://huggingface.co/papers/2509.25916",
            "abstract": "VLM-FO1 enhances vision-language models with a hybrid fine-grained region encoder to improve object localization and region understanding without sacrificing general visual capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language Models (VLMs) excel at high-level scene understanding but falter on fine-grained perception tasks requiring precise localization. This failure stems from a fundamental mismatch, as generating exact numerical coordinates is a challenging task for language-centric architectures. In this paper, we introduce VLM-FO1, a novel framework that overcomes this limitation by reframing object-centric perception from a brittle coordinate generation problem into a robust feature retrieval task. Our method operates as a plug-and-play module that integrates with any pre-trained VLM. It leverages a Hybrid Fine-grained Region Encoder (HFRE), featuring a dual vision encoder, to generate powerful region tokens rich in both semantic and spatial detail. A token-based referencing system then enables the LLM to seamlessly reason about and ground language in these specific visual regions. Experiments show that VLM-FO1 achieves state-of-the-art performance across a diverse suite of benchmarks, demonstrating exceptional capabilities in object grounding, region generational understanding, and visual region reasoning. Crucially, our two-stage training strategy ensures that these perception gains are achieved without compromising the base model's general visual understanding capabilities. VLM-FO1 establishes an effective and flexible paradigm for building perception-aware VLMs, bridging the gap between high-level reasoning and fine-grained visual grounding.",
            "score": 1,
            "issue_id": 6204,
            "pub_date": "2025-09-30",
            "pub_date_card": {
                "ru": "30 сентября",
                "en": "September 30",
                "zh": "9月30日"
            },
            "hash": "93fe431b882d5210",
            "authors": [
                "Peng Liu",
                "Haozhan Shen",
                "Chunxin Fang",
                "Zhicheng Sun",
                "Jiajia Liao",
                "Tiancheng Zhao"
            ],
            "affiliations": [
                "Binjiang Institute of Zhejiang University",
                "College of Computer Science and Technology, Zhejiang University",
                "Om AI Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.25916.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#multimodal",
                    "#reasoning",
                    "#agi",
                    "#cv",
                    "#interpretability",
                    "#benchmark"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "От координат к признакам: точная локализация в vision-language моделях",
                    "desc": "VLM-FO1 решает проблему точной локализации объектов в vision-language моделях, переформулируя задачу из генерации координат в задачу поиска признаков. Метод использует гибридный энкодер мелкозернистых регионов (HFRE) с двумя vision-энкодерами для создания токенов регионов, богатых семантической и пространственной информацией. Система позволяет LLM рассуждать о конкретных визуальных областях через токены, достигая state-of-the-art результатов в grounding и понимании регионов. Двухэтапная стратегия обучения сохраняет базовые способности модели к общему визуальному пониманию, делая VLM-FO1 универсальным plug-and-play модулем."
                },
                "en": {
                    "title": "Bridging High-Level Reasoning and Fine-Grained Visual Grounding",
                    "desc": "VLM-FO1 is a new framework designed to enhance vision-language models (VLMs) by improving their ability to locate and understand specific regions in images. Traditional VLMs struggle with precise localization due to their focus on language, which makes generating exact coordinates difficult. This paper introduces a Hybrid Fine-grained Region Encoder (HFRE) that transforms the localization challenge into a feature retrieval task, allowing for better integration of visual and semantic information. The results show that VLM-FO1 not only excels in object grounding and region understanding but also maintains the general visual capabilities of the original model, making it a versatile tool for perception-aware VLMs."
                },
                "zh": {
                    "title": "VLM-FO1：提升视觉语言模型的物体定位能力",
                    "desc": "VLM-FO1 是一种增强视觉语言模型的新框架，旨在改善物体定位和区域理解能力。它通过混合细粒度区域编码器，将物体中心感知问题转变为强大的特征检索任务，从而克服了传统语言模型在精确坐标生成上的局限。该方法可以作为插件与任何预训练的视觉语言模型集成，利用双视觉编码器生成丰富的区域标记。实验结果表明，VLM-FO1 在多个基准测试中表现出色，成功实现了物体定位和视觉区域推理的提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.25411",
            "title": "Boolean Satisfiability via Imitation Learning",
            "url": "https://huggingface.co/papers/2509.25411",
            "abstract": "ImitSAT, a branching policy for CDCL solvers using imitation learning from expert traces, reduces propagation counts and runtime by providing dense decision-level supervision.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose ImitSAT, a branching policy for conflict-driven clause learning (CDCL) solvers based on imitation learning for the Boolean satisfiability problem (SAT). Unlike previous methods that predict instance-level signals to improve CDCL branching indirectly, or rely on reinforcement learning and insufficient CDCL information to enhance branching, ImitSAT learns from expert KeyTrace that collapses a full run into the sequence of surviving decisions. Replaying a KeyTrace on the same instance is nearly conflict-free, providing dense decision-level supervision and directly reducing propagations -- the dominant contributor to wall-clock time. This prefix-conditioned supervision enables ImitSAT to reproduce high-quality branches without exploration, yielding faster convergence, stable training, and seamless integration into CDCL. Extensive experiments demonstrate that ImitSAT reduces propagation counts and runtime, outperforming state-of-the-art learned approaches. We released the source code and trained model at https://github.com/zewei-Zhang/ImitSAT",
            "score": 1,
            "issue_id": 6199,
            "pub_date": "2025-09-29",
            "pub_date_card": {
                "ru": "29 сентября",
                "en": "September 29",
                "zh": "9月29日"
            },
            "hash": "8518038ce5781379",
            "authors": [
                "Zewei Zhang",
                "Huan Liu",
                "Yuanhao Yu",
                "Jun Chen",
                "Xiangyu Xu"
            ],
            "affiliations": [
                "McMaster University",
                "Xian Jiaotong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.25411.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#open_source",
                    "#training",
                    "#optimization",
                    "#math"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Обучение на экспертных трассах для ускорения SAT-солверов",
                    "desc": "В статье представлен ImitSAT - новая стратегия ветвления для CDCL солверов SAT-задач, основанная на imitation learning. Метод обучается на экспертных трассах KeyTrace, которые представляют полный процесс решения как последовательность ключевых решений без конфликтов. Такой подход обеспечивает плотный supervised сигнал на уровне каждого решения и напрямую сокращает количество propagations - основной фактор времени выполнения. Эксперименты показывают, что ImitSAT превосходит современные методы на основе ML, сокращая время работы и количество propagations."
                },
                "en": {
                    "title": "ImitSAT: Learning from Experts for Faster SAT Solving",
                    "desc": "ImitSAT is a new branching policy designed for conflict-driven clause learning (CDCL) solvers that uses imitation learning from expert traces to improve performance on the Boolean satisfiability problem (SAT). Unlike traditional methods that rely on indirect signals or reinforcement learning, ImitSAT directly learns from a sequence of expert decisions, known as KeyTrace, which simplifies the decision-making process. This approach minimizes conflicts during execution, leading to fewer propagation counts and reduced runtime. The results show that ImitSAT significantly outperforms existing learned methods, providing a more efficient and effective solution for SAT problems."
                },
                "zh": {
                    "title": "ImitSAT：高效的CDCL求解器分支策略",
                    "desc": "ImitSAT是一种基于模仿学习的分支策略，专为冲突驱动子句学习（CDCL）求解器设计，旨在解决布尔可满足性问题（SAT）。与以往方法不同，ImitSAT通过学习专家的KeyTrace，直接提供决策级的监督，从而减少传播次数和运行时间。通过在同一实例上重放KeyTrace，ImitSAT几乎没有冲突，显著提高了分支的质量和收敛速度。大量实验表明，ImitSAT在传播次数和运行时间上优于现有的最先进学习方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.25045",
            "title": "Hyperdimensional Probe: Decoding LLM Representations via Vector Symbolic\n  Architectures",
            "url": "https://huggingface.co/papers/2509.25045",
            "abstract": "A novel Hyperdimensional Probe method decodes information from LLM vector spaces using Vector Symbolic Architectures, providing interpretable insights into model states and failures.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite their capabilities, Large Language Models (LLMs) remain opaque with limited understanding of their internal representations. Current interpretability methods, such as direct logit attribution (DLA) and sparse autoencoders (SAEs), provide restricted insight due to limitations such as the model's output vocabulary or unclear feature names. This work introduces Hyperdimensional Probe, a novel paradigm for decoding information from the LLM vector space. It combines ideas from symbolic representations and neural probing to project the model's residual stream into interpretable concepts via Vector Symbolic Architectures (VSAs). This probe combines the strengths of SAEs and conventional probes while overcoming their key limitations. We validate our decoding paradigm with controlled input-completion tasks, probing the model's final state before next-token prediction on inputs spanning syntactic pattern recognition, key-value associations, and abstract inference. We further assess it in a question-answering setting, examining the state of the model both before and after text generation. Our experiments show that our probe reliably extracts meaningful concepts across varied LLMs, embedding sizes, and input domains, also helping identify LLM failures. Our work advances information decoding in LLM vector space, enabling extracting more informative, interpretable, and structured features from neural representations.",
            "score": 1,
            "issue_id": 6204,
            "pub_date": "2025-09-29",
            "pub_date_card": {
                "ru": "29 сентября",
                "en": "September 29",
                "zh": "9月29日"
            },
            "hash": "b34a8aa388991a99",
            "authors": [
                "Marco Bronzini",
                "Carlo Nicolini",
                "Bruno Lepri",
                "Jacopo Staiano",
                "Andrea Passerini"
            ],
            "affiliations": [
                "Fondazione Bruno Kessler (FBK), Trento, Italy",
                "Ipazia S.p.A., Milan, Italy",
                "University of Trento, Trento, Italy"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.25045.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#architecture",
                    "#interpretability"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Гиперпространственное зондирование: декодирование мыслей языковых моделей",
                    "desc": "Исследователи предложили метод Hyperdimensional Probe для интерпретации внутренних представлений больших языковых моделей (LLM). Метод использует Vector Symbolic Architectures (VSA) для проецирования residual stream модели в понятные человеку концепты, преодолевая ограничения существующих подходов вроде sparse autoencoders. Эксперименты показали, что зонд успешно извлекает осмысленную информацию из различных LLM на задачах распознавания паттернов, ассоциаций и логического вывода. Подход помогает не только понять внутренние представления моделей, но и выявлять их ошибки и недостатки."
                },
                "en": {
                    "title": "Decoding LLMs: Unveiling Insights with Hyperdimensional Probes",
                    "desc": "The paper presents a new method called Hyperdimensional Probe that helps decode information from the vector spaces of Large Language Models (LLMs) using Vector Symbolic Architectures (VSAs). This method aims to improve interpretability by providing clearer insights into the internal workings and potential failures of LLMs, which are often difficult to understand. Unlike existing methods like direct logit attribution and sparse autoencoders, Hyperdimensional Probe combines the strengths of these approaches while addressing their limitations. The authors validate their method through various tasks, demonstrating its ability to extract meaningful concepts and enhance our understanding of LLM behavior."
                },
                "zh": {
                    "title": "超维探测器：解码大型语言模型的可解释性",
                    "desc": "本文提出了一种新颖的超维探测器方法，通过向量符号架构从大型语言模型（LLM）的向量空间中解码信息，提供对模型状态和失败的可解释性洞察。现有的可解释性方法如直接逻辑归因和稀疏自编码器由于模型输出词汇或特征名称不清晰等限制，提供的洞察力有限。超维探测器结合了符号表示和神经探测的思想，将模型的残差流投影到可解释的概念中。我们的实验表明，该探测器能够可靠地提取有意义的概念，并帮助识别LLM的失败。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.00225",
            "title": "TGPO: Temporal Grounded Policy Optimization for Signal Temporal Logic\n  Tasks",
            "url": "https://huggingface.co/papers/2510.00225",
            "abstract": "TGPO, a Temporal Grounded Policy Optimization framework, decomposes STL tasks into subgoals and uses a hierarchical approach with dense rewards to improve task success rates in complex, long-horizon robotics tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Learning control policies for complex, long-horizon tasks is a central challenge in robotics and autonomous systems. Signal Temporal Logic (STL) offers a powerful and expressive language for specifying such tasks, but its non-Markovian nature and inherent sparse reward make it difficult to be solved via standard Reinforcement Learning (RL) algorithms. Prior RL approaches focus only on limited STL fragments or use STL robustness scores as sparse terminal rewards. In this paper, we propose TGPO, Temporal Grounded Policy Optimization, to solve general STL tasks. TGPO decomposes STL into timed subgoals and invariant constraints and provides a hierarchical framework to tackle the problem. The high-level component of TGPO proposes concrete time allocations for these subgoals, and the low-level time-conditioned policy learns to achieve the sequenced subgoals using a dense, stage-wise reward signal. During inference, we sample various time allocations and select the most promising assignment for the policy network to rollout the solution trajectory. To foster efficient policy learning for complex STL with multiple subgoals, we leverage the learned critic to guide the high-level temporal search via Metropolis-Hastings sampling, focusing exploration on temporally feasible solutions. We conduct experiments on five environments, ranging from low-dimensional navigation to manipulation, drone, and quadrupedal locomotion. Under a wide range of STL tasks, TGPO significantly outperforms state-of-the-art baselines (especially for high-dimensional and long-horizon cases), with an average of 31.6% improvement in task success rate compared to the best baseline. The code will be available at https://github.com/mengyuest/TGPO",
            "score": 0,
            "issue_id": 6211,
            "pub_date": "2025-09-30",
            "pub_date_card": {
                "ru": "30 сентября",
                "en": "September 30",
                "zh": "9月30日"
            },
            "hash": "67afdd043a84f208",
            "authors": [
                "Yue Meng",
                "Fei Chen",
                "Chuchu Fan"
            ],
            "affiliations": [
                "Massachusetts Institute of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.00225.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#robotics",
                    "#rl",
                    "#optimization"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Иерархическое обучение роботов через временную декомпозицию сложных задач",
                    "desc": "Статья представляет TGPO - фреймворк для обучения политик управления роботами в сложных долгосрочных задачах, заданных через Signal Temporal Logic (STL). Ключевая идея заключается в декомпозиции STL-спецификаций на временные подцели и инвариантные ограничения с использованием иерархического подхода: высокоуровневый компонент распределяет время между подцелями, а низкоуровневая политика учится их достигать с плотными наградами. Для эффективного поиска временных распределений используется сэмплирование Метрополиса-Хастингса с критиком в качестве гида. TGPO демонстрирует улучшение успешности выполнения задач на 31.6% по сравнению с лучшими базовыми методами в навигации, манипуляции и управлении дронами."
                },
                "en": {
                    "title": "TGPO: Mastering Complex Robotics Tasks with Temporal Grounded Policy Optimization",
                    "desc": "The paper introduces TGPO, a framework designed to optimize policies for complex robotics tasks specified by Signal Temporal Logic (STL). It addresses the challenges of STL's non-Markovian nature and sparse rewards by breaking down tasks into manageable subgoals and using a hierarchical approach. TGPO employs a high-level component to allocate time for these subgoals and a low-level policy that learns to achieve them with dense rewards. Experimental results show that TGPO significantly improves task success rates in various environments, outperforming existing methods by an average of 31.6%."
                },
                "zh": {
                    "title": "TGPO：提升复杂任务成功率的分层策略优化",
                    "desc": "TGPO（时间基础策略优化）框架将信号时序逻辑（STL）任务分解为子目标，并采用分层方法结合密集奖励，以提高复杂长时间机器人任务的成功率。该方法解决了传统强化学习算法在处理非马尔可夫性质和稀疏奖励时的困难。TGPO通过高层组件为子目标提供具体的时间分配，并通过低层时间条件策略学习实现这些子目标。实验结果表明，TGPO在多种环境下显著优于现有的基线方法，尤其是在高维和长时间任务中，任务成功率平均提高了31.6%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.25162",
            "title": "Aligning Visual Foundation Encoders to Tokenizers for Diffusion Models",
            "url": "https://huggingface.co/papers/2509.25162",
            "abstract": "Pretrained visual encoders are aligned as tokenizers for latent diffusion models, improving image generation quality and convergence speed.  \t\t\t\t\tAI-generated summary \t\t\t\t In this work, we propose aligning pretrained visual encoders to serve as tokenizers for latent diffusion models in image generation. Unlike training a variational autoencoder (VAE) from scratch, which primarily emphasizes low-level details, our approach leverages the rich semantic structure of foundation encoders. We introduce a three-stage alignment strategy: (1) freeze the encoder and train an adapter and a decoder to establish a semantic latent space; (2) jointly optimize all components with an additional semantic preservation loss, enabling the encoder to capture perceptual details while retaining high-level semantics; and (3) refine the decoder for improved reconstruction quality. This alignment yields semantically rich image tokenizers that benefit diffusion models. On ImageNet 256times256, our tokenizer accelerates the convergence of diffusion models, reaching a gFID of 1.90 within just 64 epochs, and improves generation both with and without classifier-free guidance. Scaling to LAION, a 2B-parameter text-to-image model trained with our tokenizer consistently outperforms FLUX VAE under the same training steps. Overall, our method is simple, scalable, and establishes a semantically grounded paradigm for continuous tokenizer design.",
            "score": 0,
            "issue_id": 6215,
            "pub_date": "2025-09-29",
            "pub_date_card": {
                "ru": "29 сентября",
                "en": "September 29",
                "zh": "9月29日"
            },
            "hash": "0dec3eb0e2822ec8",
            "authors": [
                "Bowei Chen",
                "Sai Bi",
                "Hao Tan",
                "He Zhang",
                "Tianyuan Zhang",
                "Zhengqi Li",
                "Yuanjun Xiong",
                "Jianming Zhang",
                "Kai Zhang"
            ],
            "affiliations": [
                "Adobe",
                "Massachusetts Institute of Technology",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.25162.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#rag",
                    "#diffusion",
                    "#cv"
                ],
                "emoji": "🔗",
                "ru": {
                    "title": "Семантические токенизаторы из готовых энкодеров для диффузионных моделей",
                    "desc": "В статье предлагается использовать предобученные визуальные энкодеры в качестве токенизаторов для latent diffusion моделей вместо обучения VAE с нуля. Метод включает трёхэтапную стратегию выравнивания: заморозку энкодера с обучением адаптера и декодера, совместную оптимизацию всех компонентов с сохранением семантики, и финальную доработку декодера. Такой подход позволяет создать семантически богатые токенизаторы, которые ускоряют сходимость диффузионных моделей - на ImageNet 256×256 достигается gFID 1.90 всего за 64 эпохи. Метод масштабируется на большие датасеты вроде LAION, где модель text-to-image с 2 миллиардами параметров превосходит FLUX VAE при одинаковом количестве шагов обучения."
                },
                "en": {
                    "title": "Aligning Visual Encoders for Superior Image Generation",
                    "desc": "This paper presents a method to enhance image generation by aligning pretrained visual encoders as tokenizers for latent diffusion models. Instead of starting from scratch with a variational autoencoder (VAE), the authors utilize the semantic richness of existing encoders to improve the quality of generated images. They propose a three-stage alignment strategy that includes freezing the encoder, optimizing all components together, and refining the decoder for better image reconstruction. The results show that their approach significantly accelerates convergence and improves image generation performance, establishing a new standard for tokenizer design in machine learning."
                },
                "zh": {
                    "title": "对齐预训练编码器，提升图像生成质量",
                    "desc": "本文提出了一种将预训练视觉编码器对齐作为潜在扩散模型的标记器，以提高图像生成的质量和收敛速度。与从头开始训练变分自编码器（VAE）不同，我们的方法利用了基础编码器的丰富语义结构。我们引入了三阶段的对齐策略，首先冻结编码器并训练适配器和解码器以建立语义潜在空间。通过这种对齐，我们的标记器能够加速扩散模型的收敛，并在图像生成中表现出更好的效果。"
                }
            }
        }
    ],
    "link_prev": "2025-10-02.html",
    "link_next": "2025-10-06.html",
    "link_month": "2025-10.html",
    "short_date_prev": {
        "ru": "02.10",
        "en": "10/02",
        "zh": "10月2日"
    },
    "short_date_next": {
        "ru": "06.10",
        "en": "10/06",
        "zh": "10月6日"
    },
    "categories": {
        "#dataset": 7,
        "#data": 4,
        "#benchmark": 17,
        "#agents": 10,
        "#cv": 3,
        "#rl": 14,
        "#rlhf": 3,
        "#rag": 2,
        "#plp": 0,
        "#inference": 3,
        "#3d": 0,
        "#audio": 1,
        "#video": 2,
        "#multimodal": 6,
        "#math": 3,
        "#multilingual": 1,
        "#architecture": 5,
        "#healthcare": 0,
        "#training": 21,
        "#robotics": 1,
        "#agi": 3,
        "#games": 4,
        "#interpretability": 5,
        "#reasoning": 15,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 1,
        "#optimization": 21,
        "#survey": 0,
        "#diffusion": 3,
        "#alignment": 3,
        "#story_generation": 0,
        "#hallucinations": 2,
        "#long_context": 3,
        "#synthetic": 3,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 10,
        "#small_models": 2,
        "#science": 0,
        "#low_resource": 1
    }
}