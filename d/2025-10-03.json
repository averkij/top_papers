{
    "date": {
        "ru": "3 октября",
        "en": "October 3",
        "zh": "10月3日"
    },
    "time_utc": "2025-10-03 05:12",
    "weekday": 4,
    "issue_id": 6224,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2510.02283",
            "title": "Self-Forcing++: Towards Minute-Scale High-Quality Video Generation",
            "url": "https://huggingface.co/papers/2510.02283",
            "abstract": "A method is proposed to enhance long-horizon video generation by using sampled segments from self-generated long videos to guide student models, maintaining quality and consistency without additional supervision or retraining.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion models have revolutionized image and video generation, achieving unprecedented visual quality. However, their reliance on transformer architectures incurs prohibitively high computational costs, particularly when extending generation to long videos. Recent work has explored autoregressive formulations for long video generation, typically by distilling from short-horizon bidirectional teachers. Nevertheless, given that teacher models cannot synthesize long videos, the extrapolation of student models beyond their training horizon often leads to pronounced quality degradation, arising from the compounding of errors within the continuous latent space. In this paper, we propose a simple yet effective approach to mitigate quality degradation in long-horizon video generation without requiring supervision from long-video teachers or retraining on long video datasets. Our approach centers on exploiting the rich knowledge of teacher models to provide guidance for the student model through sampled segments drawn from self-generated long videos. Our method maintains temporal consistency while scaling video length by up to 20x beyond teacher's capability, avoiding common issues such as over-exposure and error-accumulation without recomputing overlapping frames like previous methods. When scaling up the computation, our method shows the capability of generating videos up to 4 minutes and 15 seconds, equivalent to 99.9% of the maximum span supported by our base model's position embedding and more than 50x longer than that of our baseline model. Experiments on standard benchmarks and our proposed improved benchmark demonstrate that our approach substantially outperforms baseline methods in both fidelity and consistency. Our long-horizon videos demo can be found at https://self-forcing-plus-plus.github.io/",
            "score": 30,
            "issue_id": 6221,
            "pub_date": "2025-10-02",
            "pub_date_card": {
                "ru": "2 октября",
                "en": "October 2",
                "zh": "10月2日"
            },
            "hash": "6c012f635291b12f",
            "authors": [
                "Justin Cui",
                "Jie Wu",
                "Ming Li",
                "Tao Yang",
                "Xiaojie Li",
                "Rui Wang",
                "Andrew Bai",
                "Yuanhao Ban",
                "Cho-Jui Hsieh"
            ],
            "affiliations": [
                "ByteDance Seed",
                "UCLA",
                "University of Central Florida"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.02283.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#diffusion",
                    "#long_context",
                    "#video"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Самообучение на длинных видео без учителя",
                    "desc": "Исследователи предложили метод генерации длинных видео с помощью диффузионных моделей, который решает проблему накопления ошибок при авторегрессивном подходе. Вместо обучения на длинных видео или использования учителя, способного генерировать длинные последовательности, модель-студент использует сегменты из собственных сгенерированных длинных видео для самокоррекции. Метод позволяет увеличить длину видео в 20 раз по сравнению с возможностями модели-учителя, достигая генерации видео длительностью более 4 минут. Подход сохраняет временную консистентность и качество без пересчёта перекрывающихся фреймов и избегает типичных проблем вроде переэкспонирования."
                },
                "en": {
                    "title": "Enhancing Long Video Generation with Self-Sampled Guidance",
                    "desc": "This paper presents a novel method for improving the generation of long videos using segments from self-generated long videos to guide student models. By leveraging the knowledge of teacher models, the approach maintains high video quality and temporal consistency without the need for additional supervision or retraining. The method allows for scaling video lengths significantly, achieving up to 20 times the length of what teacher models can produce. Experimental results show that this technique outperforms existing methods in both fidelity and consistency, enabling the generation of videos lasting over 4 minutes."
                },
                "zh": {
                    "title": "提升长视频生成质量的新方法",
                    "desc": "本文提出了一种增强长时间视频生成的方法，通过使用自生成长视频的采样片段来指导学生模型，从而在不需要额外监督或重新训练的情况下保持质量和一致性。该方法利用教师模型的丰富知识，为学生模型提供指导，避免了常见的问题，如过度曝光和错误累积。通过这种方式，我们的方法能够将视频长度扩展到教师模型能力的20倍，生成时长可达4分15秒。实验结果表明，该方法在视频生成的保真度和一致性方面显著优于基线方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.00446",
            "title": "LongCodeZip: Compress Long Context for Code Language Models",
            "url": "https://huggingface.co/papers/2510.00446",
            "abstract": "LongCodeZip is a code compression framework for LLMs that uses dual-stage compression to reduce context size without degrading performance, improving efficiency in code intelligence applications.  \t\t\t\t\tAI-generated summary \t\t\t\t Code generation under long contexts is becoming increasingly critical as Large Language Models (LLMs) are required to reason over extensive information in the codebase. While recent advances enable code LLMs to process long inputs, high API costs and generation latency remain substantial bottlenecks. Existing context pruning techniques, such as LLMLingua, achieve promising results for general text but overlook code-specific structures and dependencies, leading to suboptimal performance in programming tasks. In this paper, we propose LongCodeZip, a novel plug-and-play code compression framework designed specifically for code LLMs. LongCodeZip employs a dual-stage strategy: (1) coarse-grained compression, which identifies and ranks function-level chunks using conditional perplexity with respect to the instruction, retaining only the most relevant functions; and (2) fine-grained compression, which segments retained functions into blocks based on perplexity and selects an optimal subset under an adaptive token budget to maximize relevance. Evaluations across multiple tasks, including code completion, summarization, and question answering, show that LongCodeZip consistently outperforms baseline methods, achieving up to a 5.6x compression ratio without degrading task performance. By effectively reducing context size while preserving essential information, LongCodeZip enables LLMs to better scale to real-world, large-scale code scenarios, advancing the efficiency and capability of code intelligence applications.",
            "score": 30,
            "issue_id": 6221,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 октября",
                "en": "October 1",
                "zh": "10月1日"
            },
            "hash": "b9bb4e93a2d263ea",
            "authors": [
                "Yuling Shi",
                "Yichun Qian",
                "Hongyu Zhang",
                "Beijun Shen",
                "Xiaodong Gu"
            ],
            "affiliations": [
                "Chongqing University, Chongqing, China",
                "Shanghai Jiao Tong University, Shanghai, China",
                "Stanford University, Stanford, CA, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.00446.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#long_context",
                    "#data",
                    "#optimization",
                    "#plp"
                ],
                "emoji": "🗜️",
                "ru": {
                    "title": "Умное сжатие кода для больших языковых моделей",
                    "desc": "LongCodeZip — это специализированный фреймворк для сжатия программного кода при работе с LLM, использующий двухэтапную стратегию компрессии. На первом этапе система выполняет грубую фильтрацию на уровне функций, ранжируя их по условной перплексии относительно инструкции и оставляя наиболее релевантные. На втором этапе происходит тонкая компрессия: оставшиеся функции сегментируются на блоки, из которых выбирается оптимальное подмножество в рамках адаптивного токен-бюджета. Метод достигает степени сжатия до 5.6x без потери качества на задачах генерации, суммаризации и ответов на вопросы по коду, превосходя общие методы сжатия текста вроде LLMLingua."
                },
                "en": {
                    "title": "Efficient Code Compression for LLMs with LongCodeZip",
                    "desc": "LongCodeZip is a specialized framework designed to compress code for Large Language Models (LLMs) while maintaining performance. It utilizes a dual-stage compression approach, first applying coarse-grained compression to identify and prioritize relevant function-level chunks based on their importance. Then, it employs fine-grained compression to further refine these functions into optimal segments, ensuring that only the most pertinent information is retained. This method significantly reduces context size, achieving up to a 5.6x compression ratio, which enhances the efficiency of code-related tasks without sacrificing output quality."
                },
                "zh": {
                    "title": "提升代码智能的压缩效率",
                    "desc": "LongCodeZip 是一个专为大型语言模型（LLMs）设计的代码压缩框架，采用双阶段压缩策略来减少上下文大小而不降低性能。它首先通过条件困惑度对函数级块进行粗粒度压缩，保留最相关的函数；然后进行细粒度压缩，根据困惑度将保留的函数分块，并在自适应令牌预算下选择最佳子集。通过在代码补全、摘要和问答等多个任务上的评估，LongCodeZip 显示出显著优于基线方法的性能，压缩比高达 5.6 倍。该框架有效减少了上下文大小，同时保留了关键信息，从而提升了代码智能应用的效率和能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.02297",
            "title": "Interactive Training: Feedback-Driven Neural Network Optimization",
            "url": "https://huggingface.co/papers/2510.02297",
            "abstract": "Interactive Training is a framework that allows real-time, feedback-driven intervention during neural network training, improving stability and adaptability.  \t\t\t\t\tAI-generated summary \t\t\t\t Traditional neural network training typically follows fixed, predefined optimization recipes, lacking the flexibility to dynamically respond to instabilities or emerging training issues. In this paper, we introduce Interactive Training, an open-source framework that enables real-time, feedback-driven intervention during neural network training by human experts or automated AI agents. At its core, Interactive Training uses a control server to mediate communication between users or agents and the ongoing training process, allowing users to dynamically adjust optimizer hyperparameters, training data, and model checkpoints. Through three case studies, we demonstrate that Interactive Training achieves superior training stability, reduced sensitivity to initial hyperparameters, and improved adaptability to evolving user needs, paving the way toward a future training paradigm where AI agents autonomously monitor training logs, proactively resolve instabilities, and optimize training dynamics.",
            "score": 17,
            "issue_id": 6221,
            "pub_date": "2025-10-02",
            "pub_date_card": {
                "ru": "2 октября",
                "en": "October 2",
                "zh": "10月2日"
            },
            "hash": "a5b5b6a9b4ca0924",
            "authors": [
                "Wentao Zhang",
                "Yang Young Lu",
                "Yuntian Deng"
            ],
            "affiliations": [
                "University of Waterloo",
                "University of Wisconsin-Madison"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.02297.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#open_source",
                    "#optimization"
                ],
                "emoji": "🎮",
                "ru": {
                    "title": "Интерактивное обучение нейросетей с вмешательством в реальном времени",
                    "desc": "В статье представлен Interactive Training — фреймворк для обучения нейросетей с возможностью вмешательства в реальном времени. В отличие от традиционного подхода с фиксированными параметрами оптимизации, система позволяет экспертам или AI-агентам динамически изменять гиперпараметры оптимизатора, обучающие данные и чекпоинты модели во время тренировки. Три практических исследования показали улучшение стабильности обучения, снижение чувствительности к начальным гиперпараметрам и лучшую адаптивность к меняющимся требованиям. Авторы видят будущее в автономных AI-агентах, которые будут мониторить логи обучения и проактивно устранять проблемы."
                },
                "en": {
                    "title": "Empowering Neural Networks with Real-Time Interactive Training",
                    "desc": "This paper presents Interactive Training, a novel framework that enhances neural network training by allowing real-time interventions. It addresses the limitations of traditional training methods, which often lack the flexibility to adapt to issues as they arise. The framework facilitates communication between users or AI agents and the training process, enabling dynamic adjustments to hyperparameters, training data, and model checkpoints. The results from case studies show that Interactive Training leads to better stability, less sensitivity to initial settings, and greater adaptability to user requirements."
                },
                "zh": {
                    "title": "实时反馈，提升训练灵活性",
                    "desc": "互动训练是一种框架，允许在神经网络训练过程中进行实时的反馈驱动干预，从而提高训练的稳定性和适应性。传统的神经网络训练通常遵循固定的优化流程，缺乏动态应对不稳定性或新出现问题的灵活性。本文介绍的互动训练框架，支持人类专家或自动化AI代理在训练过程中进行实时干预，用户可以动态调整优化器超参数、训练数据和模型检查点。通过三个案例研究，我们展示了互动训练在训练稳定性、对初始超参数的敏感性降低以及对用户需求的适应性提高方面的优势。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.02209",
            "title": "StockBench: Can LLM Agents Trade Stocks Profitably In Real-world\n  Markets?",
            "url": "https://huggingface.co/papers/2510.02209",
            "abstract": "StockBench evaluates large language models in realistic stock trading environments, revealing challenges and opportunities in developing LLM-powered financial agents.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have recently demonstrated strong capabilities as autonomous agents, showing promise in reasoning, tool use, and sequential decision-making. While prior benchmarks have evaluated LLM agents in domains such as software engineering and scientific discovery, the finance domain remains underexplored, despite its direct relevance to economic value and high-stakes decision-making. Existing financial benchmarks primarily test static knowledge through question answering, but they fall short of capturing the dynamic and iterative nature of trading. To address this gap, we introduce StockBench, a contamination-free benchmark designed to evaluate LLM agents in realistic, multi-month stock trading environments. Agents receive daily market signals -- including prices, fundamentals, and news -- and must make sequential buy, sell, or hold decisions. Performance is assessed using financial metrics such as cumulative return, maximum drawdown, and the Sortino ratio. Our evaluation of state-of-the-art proprietary (e.g., GPT-5, Claude-4) and open-weight (e.g., Qwen3, Kimi-K2, GLM-4.5) models shows that while most LLM agents struggle to outperform the simple buy-and-hold baseline, several models demonstrate the potential to deliver higher returns and manage risk more effectively. These findings highlight both the challenges and opportunities in developing LLM-powered financial agents, showing that excelling at static financial knowledge tasks does not necessarily translate into successful trading strategies. We release StockBench as an open-source resource to support reproducibility and advance future research in this domain.",
            "score": 16,
            "issue_id": 6221,
            "pub_date": "2025-10-02",
            "pub_date_card": {
                "ru": "2 октября",
                "en": "October 2",
                "zh": "10月2日"
            },
            "hash": "6d2362d30dbb6925",
            "authors": [
                "Yanxu Chen",
                "Zijun Yao",
                "Yantao Liu",
                "Jin Ye",
                "Jianing Yu",
                "Lei Hou",
                "Juanzi Li"
            ],
            "affiliations": [
                "Beijing University of Posts and Telecommunications",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.02209.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#open_source",
                    "#benchmark",
                    "#agents"
                ],
                "emoji": "📈",
                "ru": {
                    "title": "LLM-агенты учатся торговать акциями, но пока проигрывают простым стратегиям",
                    "desc": "StockBench - это новый бенчмарк для оценки больших языковых моделей (LLM) в роли автономных агентов для торговли акциями. Агенты получают ежедневные рыночные данные - цены, финансовые показатели и новости - и должны принимать последовательные решения о покупке, продаже или удержании акций на протяжении нескольких месяцев. Исследование показало, что большинство современных LLM-агентов, включая GPT-5 и Claude-4, с трудом превосходят простую стратегию \"купи и держи\", хотя некоторые модели демонстрируют потенциал для более высокой доходности. Результаты подчеркивают, что успех в статических финансовых тестах не гарантирует эффективных торговых стратегий, открывая новые направления для исследований AI-агентов в финансовой сфере."
                },
                "en": {
                    "title": "StockBench: Evaluating LLMs in Real-World Stock Trading",
                    "desc": "This paper introduces StockBench, a new benchmark for evaluating large language models (LLMs) in realistic stock trading scenarios. Unlike previous benchmarks that focus on static knowledge, StockBench assesses LLMs on their ability to make dynamic trading decisions based on daily market signals. The evaluation uses financial metrics to measure performance, revealing that while many LLMs struggle to outperform a basic buy-and-hold strategy, some show promise in generating higher returns and managing risk. This research highlights the complexities of applying LLMs in finance and aims to foster further exploration in developing effective financial agents."
                },
                "zh": {
                    "title": "StockBench：评估金融代理的未来潜力",
                    "desc": "StockBench 是一个评估大型语言模型（LLM）在真实股票交易环境中的基准测试工具。它解决了现有金融基准测试无法捕捉交易动态和迭代特性的不足。通过提供每日市场信号，LLM 代理需要做出买入、卖出或持有的决策。我们的研究表明，尽管大多数 LLM 代理未能超越简单的买入持有策略，但一些模型显示出更高的回报潜力和更有效的风险管理能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.01444",
            "title": "VOGUE: Guiding Exploration with Visual Uncertainty Improves Multimodal\n  Reasoning",
            "url": "https://huggingface.co/papers/2510.01444",
            "abstract": "VOGUE, a method that shifts exploration to the visual input space by quantifying policy sensitivity to visual perturbations, enhances multimodal reasoning in large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning with verifiable rewards (RLVR) improves reasoning in large language models (LLMs) but struggles with exploration, an issue that still persists for multimodal LLMs (MLLMs). Current methods treat the visual input as a fixed, deterministic condition, overlooking a critical source of ambiguity and struggling to build policies robust to plausible visual variations. We introduce VOGUE (Visual Uncertainty Guided Exploration), a novel method that shifts exploration from the output (text) to the input (visual) space. By treating the image as a stochastic context, VOGUE quantifies the policy's sensitivity to visual perturbations using the symmetric KL divergence between a \"raw\" and \"noisy\" branch, creating a direct signal for uncertainty-aware exploration. This signal shapes the learning objective via an uncertainty-proportional bonus, which, combined with a token-entropy bonus and an annealed sampling schedule, effectively balances exploration and exploitation. Implemented within GRPO on two model scales (Qwen2.5-VL-3B/7B), VOGUE boosts pass@1 accuracy by an average of 2.6% on three visual math benchmarks and 3.7% on three general-domain reasoning benchmarks, while simultaneously increasing pass@4 performance and mitigating the exploration decay commonly observed in RL fine-tuning. Our work shows that grounding exploration in the inherent uncertainty of visual inputs is an effective strategy for improving multimodal reasoning.",
            "score": 16,
            "issue_id": 6221,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 октября",
                "en": "October 1",
                "zh": "10月1日"
            },
            "hash": "827fe718c8df2c9c",
            "authors": [
                "Rui Liu",
                "Dian Yu",
                "Tong Zheng",
                "Runpeng Dai",
                "Zongxia Li",
                "Wenhao Yu",
                "Zhenwen Liang",
                "Linfeng Song",
                "Haitao Mi",
                "Pratap Tokekar",
                "Dong Yu"
            ],
            "affiliations": [
                "Tencent AI Lab, Bellevue, WA",
                "University of Maryland, College Park",
                "University of North Carolina, Chapel Hill"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.01444.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#multimodal",
                    "#rl",
                    "#games",
                    "#reasoning"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Исследование через визуальную неопределённость для мультимодального обучения",
                    "desc": "Статья представляет метод VOGUE, который улучшает обучение с подкреплением для мультимодальных LLM за счёт переноса исследования (exploration) из текстового пространства в визуальное. Вместо того чтобы рассматривать изображение как фиксированный вход, метод трактует его как стохастический контекст и измеряет чувствительность политики к визуальным возмущениям через симметричную KL-дивергенцию. Полученный сигнал неопределённости используется как бонус в функции обучения, что помогает балансировать исследование и эксплуатацию. Эксперименты на моделях Qwen2.5-VL показали улучшение точности на 2.6% для математических задач и 3.7% для общих задач рассуждения."
                },
                "en": {
                    "title": "Enhancing Multimodal Reasoning through Visual Uncertainty Exploration",
                    "desc": "The paper introduces VOGUE, a method that enhances multimodal reasoning in large language models by focusing on the visual input space. It addresses the exploration challenges in reinforcement learning with verifiable rewards by quantifying how sensitive a policy is to changes in visual inputs. VOGUE treats images as stochastic contexts, using symmetric KL divergence to measure policy sensitivity and create an uncertainty-aware exploration signal. This approach leads to improved accuracy in reasoning tasks by balancing exploration and exploitation through an uncertainty-proportional bonus and other techniques."
                },
                "zh": {
                    "title": "基于视觉不确定性的探索提升多模态推理",
                    "desc": "VOGUE是一种新方法，通过量化策略对视觉扰动的敏感性，将探索转移到视觉输入空间，从而增强大型语言模型的多模态推理能力。该方法将图像视为随机上下文，利用对称KL散度来量化策略的敏感性，创建一个直接的信号以支持不确定性感知的探索。VOGUE通过不确定性比例奖励、令牌熵奖励和逐步采样计划有效平衡探索与利用，显著提高了模型在视觉数学基准和一般领域推理基准上的准确性。我们的研究表明，将探索与视觉输入的固有不确定性结合是一种有效的多模态推理改进策略。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.01591",
            "title": "CLUE: Non-parametric Verification from Experience via Hidden-State\n  Clustering",
            "url": "https://huggingface.co/papers/2510.01591",
            "abstract": "Hidden states in Large Language Models encode correctness as a separable signature, enabling a minimalist verifier (CLUE) to outperform text-level and confidence-based methods in reranking and accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Assessing the quality of Large Language Model (LLM) outputs presents a critical challenge. Previous methods either rely on text-level information (e.g., reward models, majority voting), which can overfit to superficial cues, or on calibrated confidence from token probabilities, which would fail on less-calibrated models. Yet both of these signals are, in fact, partial projections of a richer source of information: the model's internal hidden states. Early layers, closer to token embeddings, preserve semantic and lexical features that underpin text-based judgments, while later layers increasingly align with output logits, embedding confidence-related information. This paper explores hidden states directly as a unified foundation for verification. We show that the correctness of a solution is encoded as a geometrically separable signature within the trajectory of hidden activations. To validate this, we present Clue (Clustering and Experience-based Verification), a deliberately minimalist, non-parametric verifier. With no trainable parameters, CLUE only summarizes each reasoning trace by an hidden state delta and classifies correctness via nearest-centroid distance to ``success'' and ``failure'' clusters formed from past experience. The simplicity of this method highlights the strength of the underlying signal. Empirically, CLUE consistently outperforms LLM-as-a-judge baselines and matches or exceeds modern confidence-based methods in reranking candidates, improving both top-1 and majority-vote accuracy across AIME 24/25 and GPQA. As a highlight, on AIME 24 with a 1.5B model, CLUE boosts accuracy from 56.7% (majority@64) to 70.0% (top-maj@16).",
            "score": 14,
            "issue_id": 6221,
            "pub_date": "2025-10-02",
            "pub_date_card": {
                "ru": "2 октября",
                "en": "October 2",
                "zh": "10月2日"
            },
            "hash": "5fb07bdb1a94a1b3",
            "authors": [
                "Zhenwen Liang",
                "Ruosen Li",
                "Yujun Zhou",
                "Linfeng Song",
                "Dian Yu",
                "Xinya Du",
                "Haitao Mi",
                "Dong Yu"
            ],
            "affiliations": [
                "Tencent AI Lab",
                "University of Notre Dame",
                "University of Texas at Dallas"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.01591.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#training",
                    "#reasoning",
                    "#interpretability"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Скрытые состояния LLM как геометрический детектор правильности ответов",
                    "desc": "Исследователи обнаружили, что скрытые состояния (hidden states) в больших языковых моделях содержат геометрически отделимую сигнатуру корректности ответа. Они разработали CLUE — минималистичный непараметрический верификатор, который классифицирует правильность решений по расстоянию до кластеров успешных и неуспешных примеров из прошлого опыта. Метод использует только дельту скрытых состояний без обучаемых параметров, что подчеркивает силу внутреннего сигнала модели. CLUE превосходит методы на основе текста и confidence-based подходы в задачах ранжирования, повышая точность с 56.7% до 70.0% на датасете AIME 24."
                },
                "en": {
                    "title": "Unlocking Hidden States for Accurate LLM Verification",
                    "desc": "This paper investigates how hidden states in Large Language Models (LLMs) can be used to assess the correctness of model outputs more effectively than traditional methods. It introduces CLUE, a minimalist verifier that leverages the geometric separability of hidden activations to classify outputs as correct or incorrect without needing trainable parameters. By summarizing reasoning traces with hidden state deltas and using nearest-centroid distance to classify correctness, CLUE demonstrates superior performance over existing text-level and confidence-based approaches. The results show significant improvements in accuracy, highlighting the potential of hidden states as a rich source of information for verification tasks."
                },
                "zh": {
                    "title": "利用隐藏状态提升语言模型的验证准确性",
                    "desc": "这篇论文探讨了大型语言模型（LLM）内部隐藏状态如何编码正确性，并提出了一种名为CLUE的简约验证器。CLUE利用隐藏状态的几何可分离特征，能够在重排序和准确性方面超越传统的文本级和基于置信度的方法。通过对隐藏状态的直接分析，CLUE不需要可训练参数，仅通过总结推理轨迹的隐藏状态变化来进行分类。实验结果表明，CLUE在多个基准测试中表现优异，显著提高了模型的准确性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.02314",
            "title": "StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided\n  Illusions",
            "url": "https://huggingface.co/papers/2510.02314",
            "abstract": "A novel density-guided poisoning method for 3D Gaussian Splatting enhances attack effectiveness by strategically injecting Gaussian points and disrupting multi-view consistency.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D scene representation methods like Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have significantly advanced novel view synthesis. As these methods become prevalent, addressing their vulnerabilities becomes critical. We analyze 3DGS robustness against image-level poisoning attacks and propose a novel density-guided poisoning method. Our method strategically injects Gaussian points into low-density regions identified via Kernel Density Estimation (KDE), embedding viewpoint-dependent illusory objects clearly visible from poisoned views while minimally affecting innocent views. Additionally, we introduce an adaptive noise strategy to disrupt multi-view consistency, further enhancing attack effectiveness. We propose a KDE-based evaluation protocol to assess attack difficulty systematically, enabling objective benchmarking for future research. Extensive experiments demonstrate our method's superior performance compared to state-of-the-art techniques. Project page: https://hentci.github.io/stealthattack/",
            "score": 7,
            "issue_id": 6223,
            "pub_date": "2025-10-02",
            "pub_date_card": {
                "ru": "2 октября",
                "en": "October 2",
                "zh": "10月2日"
            },
            "hash": "334776e6e757ace4",
            "authors": [
                "Bo-Hsu Ke",
                "You-Zhe Xie",
                "Yu-Lun Liu",
                "Wei-Chen Chiu"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2510.02314.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#benchmark",
                    "#3d"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Скрытая атака на 3D Gaussian Splatting через манипуляцию плотностью",
                    "desc": "Исследователи проанализировали уязвимости метода 3D Gaussian Splatting (3DGS) к атакам через отравление обучающих данных. Предложен новый метод атаки, который стратегически внедряет гауссовы точки в области низкой плотности, определяемые через Kernel Density Estimation (KDE), создавая иллюзорные объекты, видимые только с определённых ракурсов. Метод использует адаптивный шум для нарушения multi-view consistency, что усиливает эффективность атаки при минимальном влиянии на невредоносные виды. Разработан протокол оценки на основе KDE для систематического измерения сложности атак и объективного сравнения методов."
                },
                "en": {
                    "title": "Enhancing Attack Effectiveness in 3D Gaussian Splatting with Density-Guided Poisoning",
                    "desc": "This paper presents a new method for attacking 3D Gaussian Splatting (3DGS) by using a density-guided poisoning technique. The approach involves injecting Gaussian points into areas with low density, which creates misleading visual artifacts that are noticeable from certain viewpoints. Additionally, the method employs an adaptive noise strategy to further disrupt the consistency of views across the 3D scene. The authors also introduce a new evaluation protocol based on Kernel Density Estimation (KDE) to measure the effectiveness of these attacks, showing that their method outperforms existing techniques."
                },
                "zh": {
                    "title": "增强3D高斯点云攻击效果的新方法",
                    "desc": "本文提出了一种新颖的密度引导中毒方法，旨在增强3D高斯点云（3D Gaussian Splatting）的攻击效果。该方法通过在低密度区域注入高斯点，破坏多视图一致性，从而在受影响的视角中嵌入明显的虚假物体。我们还引入了一种自适应噪声策略，以进一步提高攻击的有效性。通过系统的KDE评估协议，我们能够客观地评估攻击难度，为未来的研究提供基准。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.02253",
            "title": "DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag\n  Editing",
            "url": "https://huggingface.co/papers/2510.02253",
            "abstract": "DragFlow leverages FLUX's strong generative priors and region-based editing with affine transformations to achieve state-of-the-art performance in drag-based image editing.  \t\t\t\t\tAI-generated summary \t\t\t\t Drag-based image editing has long suffered from distortions in the target region, largely because the priors of earlier base models, Stable Diffusion, are insufficient to project optimized latents back onto the natural image manifold. With the shift from UNet-based DDPMs to more scalable DiT with flow matching (e.g., SD3.5, FLUX), generative priors have become significantly stronger, enabling advances across diverse editing tasks. However, drag-based editing has yet to benefit from these stronger priors. This work proposes the first framework to effectively harness FLUX's rich prior for drag-based editing, dubbed DragFlow, achieving substantial gains over baselines. We first show that directly applying point-based drag editing to DiTs performs poorly: unlike the highly compressed features of UNets, DiT features are insufficiently structured to provide reliable guidance for point-wise motion supervision. To overcome this limitation, DragFlow introduces a region-based editing paradigm, where affine transformations enable richer and more consistent feature supervision. Additionally, we integrate pretrained open-domain personalization adapters (e.g., IP-Adapter) to enhance subject consistency, while preserving background fidelity through gradient mask-based hard constraints. Multimodal large language models (MLLMs) are further employed to resolve task ambiguities. For evaluation, we curate a novel Region-based Dragging benchmark (ReD Bench) featuring region-level dragging instructions. Extensive experiments on DragBench-DR and ReD Bench show that DragFlow surpasses both point-based and region-based baselines, setting a new state-of-the-art in drag-based image editing. Code and datasets will be publicly available upon publication.",
            "score": 7,
            "issue_id": 6222,
            "pub_date": "2025-10-02",
            "pub_date_card": {
                "ru": "2 октября",
                "en": "October 2",
                "zh": "10月2日"
            },
            "hash": "d10d645e2e301186",
            "authors": [
                "Zihan Zhou",
                "Shilin Lu",
                "Shuli Leng",
                "Shaocong Zhang",
                "Zhuming Lian",
                "Xinlei Yu",
                "Adams Wai-Kin Kong"
            ],
            "affiliations": [
                "Nanyang Technological University",
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.02253.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#cv",
                    "#multimodal",
                    "#benchmark",
                    "#open_source",
                    "#architecture"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Региональное перетаскивание с FLUX: новый уровень точности в редактировании изображений",
                    "desc": "DragFlow — это новый фреймворк для редактирования изображений методом перетаскивания (drag-editing), использующий мощные генеративные prior'ы модели FLUX на основе DiT архитектуры. В отличие от точечного подхода, который плохо работает с DiT, авторы предлагают редактирование на уровне регионов с аффинными трансформациями для более согласованного управления признаками. Метод интегрирует pretrained адаптеры персонализации для сохранения консистентности объектов и использует мультимодальные LLM для разрешения неоднозначностей в задачах. Эксперименты на бенчмарках DragBench-DR и новом ReD Bench демонстрируют state-of-the-art результаты в drag-based редактировании изображений."
                },
                "en": {
                    "title": "Revolutionizing Drag-Based Image Editing with DragFlow",
                    "desc": "DragFlow is a new framework that improves drag-based image editing by using advanced generative models called FLUX. Traditional methods struggled with distortions because earlier models like Stable Diffusion couldn't accurately map edited images back to their original forms. DragFlow introduces a region-based editing approach that uses affine transformations for better feature supervision, making the editing process more reliable. By integrating personalization adapters and multimodal language models, DragFlow achieves significant improvements over previous methods, setting a new standard in the field of image editing."
                },
                "zh": {
                    "title": "DragFlow：拖拽式图像编辑的新突破",
                    "desc": "DragFlow 是一种新框架，利用 FLUX 的强生成先验和基于区域的编辑方法，显著提升了拖拽式图像编辑的效果。传统的拖拽编辑常常导致目标区域的失真，因为早期模型的先验不足以将优化后的潜在表示准确映射到自然图像上。DragFlow 通过引入仿射变换的区域编辑范式，提供了更丰富和一致的特征监督，从而克服了点基拖拽编辑的局限性。实验结果表明，DragFlow 在拖拽式图像编辑任务中超越了现有的基线，达到了新的最先进水平。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.02250",
            "title": "The Unreasonable Effectiveness of Scaling Agents for Computer Use",
            "url": "https://huggingface.co/papers/2510.02250",
            "abstract": "Behavior Best-of-N (bBoN) improves the reliability and success rates of computer-use agents by generating and selecting among multiple rollouts using behavior narratives, achieving state-of-the-art performance on OSWorld and strong generalization to different operating systems.  \t\t\t\t\tAI-generated summary \t\t\t\t Computer-use agents (CUAs) hold promise for automating everyday digital tasks, but their unreliability and high variance hinder their application to long-horizon, complex tasks. We introduce Behavior Best-of-N (bBoN), a method that scales over agents by generating multiple rollouts and selecting among them using behavior narratives that describe the agents' rollouts. It enables both wide exploration and principled trajectory selection, substantially improving robustness and success rates. On OSWorld, our bBoN scaling method establishes a new state of the art (SoTA) at 69.9%, significantly outperforming prior methods and approaching human-level performance at 72%, with comprehensive ablations validating key design choices. We further demonstrate strong generalization results to different operating systems on WindowsAgentArena and AndroidWorld. Crucially, our results highlight the unreasonable effectiveness of scaling CUAs, when you do it right: effective scaling requires structured trajectory understanding and selection, and bBoN provides a practical framework to achieve this.",
            "score": 7,
            "issue_id": 6222,
            "pub_date": "2025-10-02",
            "pub_date_card": {
                "ru": "2 октября",
                "en": "October 2",
                "zh": "10月2日"
            },
            "hash": "aa05336e77565d70",
            "authors": [
                "Gonzalo Gonzalez-Pumariega",
                "Vincent Tu",
                "Chih-Lun Lee",
                "Jiachen Yang",
                "Ang Li",
                "Xin Eric Wang"
            ],
            "affiliations": [
                "Simular Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.02250.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#optimization",
                    "#games"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Масштабирование агентов через множественные попытки и умный выбор",
                    "desc": "Статья представляет метод Behavior Best-of-N (bBoN) для повышения надежности компьютерных агентов, автоматизирующих задачи на компьютере. Метод генерирует множество вариантов выполнения задачи и выбирает лучший на основе поведенческих нарративов — описаний действий агента. На бенчмарке OSWorld метод достигает state-of-the-art результата 69.9%, приближаясь к человеческому уровню 72%. Ключевой вывод работы — эффективное масштабирование агентов требует структурированного понимания и выбора траекторий, что и обеспечивает bBoN."
                },
                "en": {
                    "title": "Scaling Success: Behavior Best-of-N for Reliable Computer-Use Agents",
                    "desc": "The paper presents Behavior Best-of-N (bBoN), a novel approach to enhance the performance of computer-use agents (CUAs) by generating multiple rollouts and selecting the best ones based on behavior narratives. This method allows for extensive exploration of possible actions while ensuring that the most effective trajectories are chosen, leading to improved reliability and success rates in complex tasks. The results show that bBoN achieves state-of-the-art performance on the OSWorld benchmark, nearing human-level effectiveness. Additionally, the method demonstrates strong generalization capabilities across different operating systems, emphasizing the importance of structured trajectory understanding in scaling CUAs effectively."
                },
                "zh": {
                    "title": "行为最佳选择：提升计算机代理的可靠性与成功率",
                    "desc": "行为最佳选择（bBoN）通过生成和选择多个回滚，利用行为叙述提高了计算机使用代理的可靠性和成功率。该方法在OSWorld上达到了69.9%的新状态，显著优于之前的方法，并接近人类水平的72%。bBoN方法允许广泛探索和有原则的轨迹选择，从而显著提高了鲁棒性和成功率。我们的研究还展示了bBoN在不同操作系统上的强泛化能力，证明了有效扩展计算机使用代理的合理性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.01265",
            "title": "RLP: Reinforcement as a Pretraining Objective",
            "url": "https://huggingface.co/papers/2510.01265",
            "abstract": "RLP, an information-driven reinforcement pretraining objective, enhances reasoning models by integrating exploration into pretraining, leading to significant performance improvements across various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t The dominant paradigm for training large reasoning models starts with pre-training using next-token prediction loss on vast amounts of data. Reinforcement learning, while powerful in scaling reasoning, is introduced only as the very last phase of post-training, preceded by supervised fine-tuning. While dominant, is this an optimal way of training? In this paper, we present RLP, an information-driven reinforcement pretraining objective, that brings the core spirit of reinforcement learning -- exploration -- to the last phase of pretraining. The key idea is to treat chain-of-thought as an exploratory action, with rewards computed based on the information gain it provides for predicting future tokens. This training objective essentially encourages the model to think for itself before predicting what comes next, thus teaching an independent thinking behavior earlier in the pretraining. More concretely, the reward signal measures the increase in log-likelihood of the next token when conditioning on both context and a sampled reasoning chain, compared to conditioning on context alone. This approach yields a verifier-free dense reward signal, allowing for efficient training for the full document stream during pretraining. Specifically, RLP reframes reinforcement learning for reasoning as a pretraining objective on ordinary text, bridging the gap between next-token prediction and the emergence of useful chain-of-thought reasoning. Pretraining with RLP on Qwen3-1.7B-Base lifts the overall average across an eight-benchmark math-and-science suite by 19%. With identical post-training, the gains compound, with the largest improvements on reasoning-heavy tasks such as AIME25 and MMLU-Pro. Applying RLP to the hybrid Nemotron-Nano-12B-v2 increases the overall average from 42.81% to 61.32% and raises the average on scientific reasoning by 23%, demonstrating scalability across architectures and model sizes.",
            "score": 7,
            "issue_id": 6222,
            "pub_date": "2025-09-26",
            "pub_date_card": {
                "ru": "26 сентября",
                "en": "September 26",
                "zh": "9月26日"
            },
            "hash": "80c397f193259627",
            "authors": [
                "Ali Hatamizadeh",
                "Syeda Nahida Akter",
                "Shrimai Prabhumoye",
                "Jan Kautz",
                "Mostofa Patwary",
                "Mohammad Shoeybi",
                "Bryan Catanzaro",
                "Yejin Choi"
            ],
            "affiliations": [
                "Boston University",
                "Carnegie Mellon University",
                "NVIDIA",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.01265.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#reasoning",
                    "#rl",
                    "#optimization"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Учим модели думать в процессе предобучения через исследование",
                    "desc": "В статье представлен RLP — новый подход к предобучению моделей, который интегрирует принципы reinforcement learning на этапе pretraining, а не только на финальной стадии post-training. Ключевая идея заключается в том, что модель получает награду за генерацию chain-of-thought рассуждений, которые помогают лучше предсказывать следующие токены, измеряя информационный выигрыш. Этот метод не требует отдельного verifier и работает напрямую с обычным текстом, обучая модель «думать перед ответом» уже на этапе pretraining. Эксперименты показывают значительный прирост производительности: для модели Qwen3-1.7B-Base улучшение составило 19% на математических и научных бенчмарках, а для Nemotron-Nano-12B-v2 средний результат вырос с 42.81% до 61.32%."
                },
                "en": {
                    "title": "Reinforcement Learning for Smarter Pretraining",
                    "desc": "This paper introduces RLP, a novel reinforcement pretraining objective that enhances reasoning models by incorporating exploration during the pretraining phase. Unlike traditional methods that only apply reinforcement learning after initial training, RLP encourages models to engage in exploratory reasoning earlier, treating chain-of-thought as an action that provides valuable information for future predictions. The reward system is designed to measure the improvement in predicting the next token based on both context and a reasoning chain, promoting independent thinking in models. The results show significant performance boosts across various benchmarks, particularly in reasoning-heavy tasks, demonstrating the effectiveness of integrating reinforcement learning into the pretraining process."
                },
                "zh": {
                    "title": "探索驱动的强化预训练，提升推理模型表现",
                    "desc": "本文提出了一种信息驱动的强化预训练目标RLP，旨在通过将探索融入预训练来增强推理模型的性能。RLP将思维链视为一种探索行为，并根据其对未来标记预测的信息增益来计算奖励信号。这种方法鼓励模型在预测下一个标记之前独立思考，从而在预训练阶段更早地培养独立思考能力。实验结果表明，使用RLP进行预训练可以显著提高模型在多个基准测试上的表现，尤其是在推理密集型任务上。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.01179",
            "title": "TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP\n  Environments",
            "url": "https://huggingface.co/papers/2510.01179",
            "abstract": "Toucan, a large publicly available tool-agentic dataset, enhances the performance of LLM agents by providing diverse, realistic, and complex multi-tool and multi-turn interactions.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Model (LLM) agents are rapidly emerging as powerful systems for automating tasks across domains. Yet progress in the open-source community is constrained by the lack of high quality permissively licensed tool-agentic training data. Existing datasets are often limited in diversity, realism, and complexity, particularly regarding multi-tool and multi-turn interactions. To address this gap, we introduce Toucan, the largest publicly available tool-agentic dataset to date, containing 1.5 million trajectories synthesized from nearly 500 real-world Model Context Protocols (MCPs). Unlike prior work, Toucan leverages authentic MCP environments to generate diverse, realistic, and challenging tasks with trajectories involving real tool execution. Our pipeline first produces a broad spectrum of tool-use queries using five distinct models, applies model-based quality filtering, and then generates agentic trajectories with three teacher models using two agentic frameworks. Rigorous rule-based and model-based validation ensures high-quality outputs. We also introduce three extension mechanisms to further diversify tasks and simulate multi-turn conversations. Models fine-tuned on Toucan outperform larger closed-source counterparts on the BFCL V3 benchmark and push the Pareto frontier forward on MCP-Universe Bench.",
            "score": 6,
            "issue_id": 6221,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 октября",
                "en": "October 1",
                "zh": "10月1日"
            },
            "hash": "30b0ec9c798b87c6",
            "authors": [
                "Zhangchen Xu",
                "Adriana Meza Soria",
                "Shawn Tan",
                "Anurag Roy",
                "Ashish Sunil Agrawal",
                "Radha Poovendran",
                "Rameswar Panda"
            ],
            "affiliations": [
                "MIT-IBM Watson AI Lab",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.01179.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#synthetic",
                    "#agents",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "🦜",
                "ru": {
                    "title": "Toucan: крупнейший датасет для обучения AI-агентов работе с инструментами",
                    "desc": "Исследователи представили Toucan — самый большой публично доступный датасет для обучения LLM-агентов, содержащий 1,5 миллиона траекторий взаимодействия с реальными инструментами. Датасет создан на основе почти 500 реальных Model Context Protocols (MCP) и включает разнообразные многоступенчатые задачи с использованием нескольких инструментов одновременно. Для генерации данных использовался сложный пайплайн с участием пяти моделей для создания запросов, фильтрацией по качеству и тремя учительскими моделями для генерации траекторий. Модели, дообученные на Toucan, превосходят более крупные закрытые модели на бенчмарке BFCL V3 и демонстрируют лучшие результаты на MCP-Universe Bench."
                },
                "en": {
                    "title": "Toucan: Elevating LLM Agents with Diverse Tool Interactions",
                    "desc": "Toucan is a large dataset designed to improve the performance of Large Language Model (LLM) agents by providing a wide variety of realistic and complex interactions involving multiple tools and turns. It addresses the limitations of existing datasets, which often lack diversity and realism, by synthesizing 1.5 million trajectories from nearly 500 real-world Model Context Protocols (MCPs). The dataset generation process includes quality filtering and the use of multiple models to ensure high-quality outputs, along with mechanisms to diversify tasks and simulate multi-turn conversations. Models trained on Toucan have shown superior performance compared to larger closed-source models on established benchmarks, demonstrating its effectiveness in advancing the capabilities of LLM agents."
                },
                "zh": {
                    "title": "Toucan：提升LLM代理性能的关键数据集",
                    "desc": "Toucan是一个大型的公开可用工具代理数据集，旨在提升大型语言模型（LLM）代理的性能。该数据集包含150万个轨迹，来源于近500个真实的模型上下文协议（MCP），提供多样化、真实且复杂的多工具和多轮交互。Toucan通过真实的MCP环境生成任务，确保了数据的多样性和挑战性。经过严格的规则和模型验证，Toucan生成的高质量输出使得在BFCL V3基准测试中表现优于更大的封闭源模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.01284",
            "title": "Ovi: Twin Backbone Cross-Modal Fusion for Audio-Video Generation",
            "url": "https://huggingface.co/papers/2510.01284",
            "abstract": "Ovi is a unified audio-video generation model using twin-DiT modules with blockwise cross-modal fusion, enabling natural synchronization and high-quality multimodal outputs.  \t\t\t\t\tAI-generated summary \t\t\t\t Audio-video generation has often relied on complex multi-stage architectures or sequential synthesis of sound and visuals. We introduce Ovi, a unified paradigm for audio-video generation that models the two modalities as a single generative process. By using blockwise cross-modal fusion of twin-DiT modules, Ovi achieves natural synchronization and removes the need for separate pipelines or post hoc alignment. To facilitate fine-grained multimodal fusion modeling, we initialize an audio tower with an architecture identical to that of a strong pretrained video model. Trained from scratch on hundreds of thousands of hours of raw audio, the audio tower learns to generate realistic sound effects, as well as speech that conveys rich speaker identity and emotion. Fusion is obtained by jointly training the identical video and audio towers via blockwise exchange of timing (via scaled-RoPE embeddings) and semantics (through bidirectional cross-attention) on a vast video corpus. Our model enables cinematic storytelling with natural speech and accurate, context-matched sound effects, producing movie-grade video clips. All the demos, code and model weights are published at https://aaxwaz.github.io/Ovi",
            "score": 6,
            "issue_id": 6222,
            "pub_date": "2025-09-30",
            "pub_date_card": {
                "ru": "30 сентября",
                "en": "September 30",
                "zh": "9月30日"
            },
            "hash": "ee7ea259a6e9fd1a",
            "authors": [
                "Chetwin Low",
                "Weimin Wang",
                "Calder Katyal"
            ],
            "affiliations": [
                "Character AI",
                "Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.01284.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#audio",
                    "#multimodal",
                    "#open_source",
                    "#architecture"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Единая генерация аудио и видео через синхронизацию модальностей",
                    "desc": "Ovi — это унифицированная модель для генерации аудио и видео, которая обрабатывает обе модальности как единый генеративный процесс. Архитектура использует два идентичных DiT-модуля (twin-DiT) с блочным кросс-модальным слиянием для достижения естественной синхронизации звука и изображения. Аудио-башня обучается с нуля на сотнях тысяч часов аудиоданных и способна генерировать реалистичные звуковые эффекты и речь с передачей идентичности и эмоций говорящего. Слияние модальностей достигается через совместное обучение видео и аудио башен с обменом информацией о тайминге через scaled-RoPE эмбеддинги и семантикой через двунаправленное кросс-внимание."
                },
                "en": {
                    "title": "Ovi: Seamless Audio-Video Generation for Cinematic Storytelling",
                    "desc": "Ovi is a novel model designed for generating audio and video together in a seamless way. It uses twin-DiT modules that allow for blockwise cross-modal fusion, which means it can combine sound and visuals more effectively than previous methods. This model learns from a large amount of raw audio to create realistic sounds and speech that match the emotions and identities of speakers. By training both audio and video components together, Ovi produces high-quality, synchronized outputs suitable for cinematic storytelling."
                },
                "zh": {
                    "title": "Ovi：音视频生成的新范式",
                    "desc": "Ovi是一种统一的音视频生成模型，采用双重DiT模块和块级跨模态融合技术，能够实现自然的同步和高质量的多模态输出。与传统的多阶段架构不同，Ovi将音频和视频视为单一的生成过程，从而简化了生成流程。该模型通过联合训练音频和视频塔，利用时间和语义的块级交换，提升了多模态融合的精细建模能力。最终，Ovi能够生成具有电影级别质量的音视频片段，展现自然的语音和准确的声音效果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.00428",
            "title": "Automated Structured Radiology Report Generation with Rich Clinical\n  Context",
            "url": "https://huggingface.co/papers/2510.00428",
            "abstract": "Incorporating clinical context into automated structured radiology report generation improves report quality by addressing temporal hallucinations and utilizing comprehensive patient data.  \t\t\t\t\tAI-generated summary \t\t\t\t Automated structured radiology report generation (SRRG) from chest X-ray images offers significant potential to reduce workload of radiologists by generating reports in structured formats that ensure clarity, consistency, and adherence to clinical reporting standards. While radiologists effectively utilize available clinical contexts in their diagnostic reasoning, existing SRRG systems overlook these essential elements. This fundamental gap leads to critical problems including temporal hallucinations when referencing non-existent clinical contexts. To address these limitations, we propose contextualized SRRG (C-SRRG) that comprehensively incorporates rich clinical context for SRRG. We curate C-SRRG dataset by integrating comprehensive clinical context encompassing 1) multi-view X-ray images, 2) clinical indication, 3) imaging techniques, and 4) prior studies with corresponding comparisons based on patient histories. Through extensive benchmarking with state-of-the-art multimodal large language models, we demonstrate that incorporating clinical context with the proposed C-SRRG significantly improves report generation quality. We publicly release dataset, code, and checkpoints to facilitate future research for clinically-aligned automated RRG at https://github.com/vuno/contextualized-srrg.",
            "score": 5,
            "issue_id": 6222,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 октября",
                "en": "October 1",
                "zh": "10月1日"
            },
            "hash": "01935fd7ce2849e2",
            "authors": [
                "Seongjae Kang",
                "Dong Bok Lee",
                "Juho Jung",
                "Dongseop Kim",
                "Won Hwa Kim",
                "Sunghoon Joo"
            ],
            "affiliations": [
                "KAIST",
                "POSTECH",
                "VUNO Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.00428.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#hallucinations",
                    "#science",
                    "#dataset",
                    "#healthcare",
                    "#multimodal",
                    "#benchmark",
                    "#open_source"
                ],
                "emoji": "🩻",
                "ru": {
                    "title": "Клинический контекст против галлюцинаций в радиологических отчётах",
                    "desc": "Статья представляет новый подход к автоматической генерации структурированных радиологических отчётов по рентгеновским снимкам грудной клетки с использованием клинического контекста. Существующие системы игнорируют важную контекстную информацию, что приводит к «временным галлюцинациям» — ошибкам, когда модель ссылается на несуществующие клинические данные. Авторы предлагают метод C-SRRG, который учитывает мультимодальные данные: изображения в разных проекциях, клинические показания, технику визуализации и предыдущие исследования пациента. Эксперименты с современными multimodal LLM показали, что включение клинического контекста существенно повышает качество генерируемых отчётов."
                },
                "en": {
                    "title": "Enhancing Radiology Reports with Clinical Context",
                    "desc": "This paper presents a new approach to automated structured radiology report generation (SRRG) that incorporates clinical context to enhance report quality. The authors identify that existing SRRG systems often ignore important clinical information, leading to issues like temporal hallucinations, where reports reference non-existent contexts. To solve this, they introduce contextualized SRRG (C-SRRG), which integrates various clinical data such as multi-view X-ray images and patient histories. Their experiments show that using C-SRRG significantly improves the clarity and accuracy of generated reports, making them more useful for radiologists."
                },
                "zh": {
                    "title": "整合临床背景，提升放射学报告质量",
                    "desc": "本研究提出了一种新的自动化结构化放射学报告生成方法，称为上下文化结构化报告生成（C-SRRG）。该方法通过整合丰富的临床背景信息，解决了现有系统在生成报告时忽视临床上下文的问题，从而减少了时间幻觉的发生。我们构建了一个包含多视角X光图像、临床指示、成像技术和患者历史的C-SRRG数据集。通过与先进的多模态大语言模型进行广泛的基准测试，结果表明，C-SRRG显著提高了报告生成的质量。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.02294",
            "title": "F2LLM Technical Report: Matching SOTA Embedding Performance with 6\n  Million Open-Source Data",
            "url": "https://huggingface.co/papers/2510.02294",
            "abstract": "F2LLM, a suite of large language models, achieves high embedding performance with efficient fine-tuning from foundation models using open-source datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce F2LLM - Foundation to Feature Large Language Models, a suite of state-of-the-art embedding models in three sizes: 0.6B, 1.7B, and 4B. Unlike previous top-ranking embedding models that require massive contrastive pretraining, sophisticated training pipelines, and costly synthetic training data, F2LLM is directly finetuned from foundation models on 6 million query-document-negative tuples curated from open-source, non-synthetic datasets, striking a strong balance between training cost, model size, and embedding performance. On the MTEB English leaderboard, F2LLM-4B ranks 2nd among models with approximately 4B parameters and 7th overall, while F2LLM-1.7B ranks 1st among models in the 1B-2B size range. To facilitate future research in the field, we release the models, training dataset, and code, positioning F2LLM as a strong, reproducible, and budget-friendly baseline for future works.",
            "score": 4,
            "issue_id": 6221,
            "pub_date": "2025-10-02",
            "pub_date_card": {
                "ru": "2 октября",
                "en": "October 2",
                "zh": "10月2日"
            },
            "hash": "f50d032bbcffbe10",
            "authors": [
                "Ziyin Zhang",
                "Zihan Liao",
                "Hang Yu",
                "Peng Di",
                "Rui Wang"
            ],
            "affiliations": [
                "Ant Group",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.02294.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#open_source",
                    "#dataset",
                    "#small_models",
                    "#optimization"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Эффективные эмбеддинги из foundation моделей без дорогостоящего предобучения",
                    "desc": "F2LLM — это семейство language models для создания эмбеддингов размером 0.6B, 1.7B и 4B параметров. В отличие от предыдущих топовых моделей, F2LLM не требует масштабного contrastive pretraining и дорогих синтетических данных — модели просто файнтюнятся на 6 миллионах троек query-document-negative из открытых источников. На бенчмарке MTEB модель F2LLM-4B заняла 2-е место среди 4B моделей и 7-е место в общем зачёте, а F2LLM-1.7B стала лучшей в категории 1B-2B параметров. Авторы открыли код, датасет и веса моделей, создав доступный baseline для будущих исследований в области эмбеддингов."
                },
                "en": {
                    "title": "F2LLM: Efficient Embedding Models for Cost-Effective Performance",
                    "desc": "F2LLM is a new suite of large language models designed for efficient embedding performance. It fine-tunes foundation models using a curated dataset of 6 million query-document-negative tuples, avoiding the need for expensive pretraining and synthetic data. The models come in three sizes: 0.6B, 1.7B, and 4B parameters, with the largest model achieving high rankings on the MTEB English leaderboard. By releasing the models and training data, F2LLM aims to provide a cost-effective and reproducible baseline for future research in machine learning."
                },
                "zh": {
                    "title": "F2LLM：高效嵌入的基础模型微调",
                    "desc": "F2LLM是一套大型语言模型，专注于高效的嵌入性能。它通过对基础模型进行微调，使用开放源代码数据集，避免了以往模型需要的大规模对比预训练和复杂的训练流程。F2LLM提供了三种不同规模的模型，分别为0.6B、1.7B和4B，并在MTEB英语排行榜上表现优异。为了推动未来的研究，我们公开了模型、训练数据集和代码，旨在为后续工作提供一个强大且经济实惠的基准。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.26376",
            "title": "Go with Your Gut: Scaling Confidence for Autoregressive Image Generation",
            "url": "https://huggingface.co/papers/2509.26376",
            "abstract": "ScalingAR enhances next-token prediction in autoregressive image generation by using token entropy and adaptive scaling, improving model performance and efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Test-time scaling (TTS) has demonstrated remarkable success in enhancing large language models, yet its application to next-token prediction (NTP) autoregressive (AR) image generation remains largely uncharted. Existing TTS approaches for visual AR (VAR), which rely on frequent partial decoding and external reward models, are ill-suited for NTP-based image generation due to the inherent incompleteness of intermediate decoding results. To bridge this gap, we introduce ScalingAR, the first TTS framework specifically designed for NTP-based AR image generation that eliminates the need for early decoding or auxiliary rewards. ScalingAR leverages token entropy as a novel signal in visual token generation and operates at two complementary scaling levels: (i) Profile Level, which streams a calibrated confidence state by fusing intrinsic and conditional signals; and (ii) Policy Level, which utilizes this state to adaptively terminate low-confidence trajectories and dynamically schedule guidance for phase-appropriate conditioning strength. Experiments on both general and compositional benchmarks show that ScalingAR (1) improves base models by 12.5% on GenEval and 15.2% on TIIF-Bench, (2) efficiently reduces visual token consumption by 62.0% while outperforming baselines, and (3) successfully enhances robustness, mitigating performance drops by 26.0% in challenging scenarios.",
            "score": 4,
            "issue_id": 6222,
            "pub_date": "2025-09-30",
            "pub_date_card": {
                "ru": "30 сентября",
                "en": "September 30",
                "zh": "9月30日"
            },
            "hash": "d720c352a15d85b1",
            "authors": [
                "Harold Haodong Chen",
                "Xianfeng Wu",
                "Wen-Jie Shu",
                "Rongjin Guo",
                "Disen Lan",
                "Harry Yang",
                "Ying-Cong Chen"
            ],
            "affiliations": [
                "CityUHK",
                "FDU",
                "HKUST",
                "HKUST(GZ)",
                "PolyU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.26376.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#optimization",
                    "#training",
                    "#benchmark",
                    "#architecture"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Масштабирование на этапе тестирования для авторегрессивной генерации изображений",
                    "desc": "Статья представляет ScalingAR — первый фреймворк test-time scaling для авторегрессивной генерации изображений на основе предсказания следующего токена. Метод использует энтропию токенов как сигнал для адаптивного управления процессом генерации на двух уровнях: уровне профиля (калибровка уверенности модели) и уровне политики (динамическое завершение низкоуверенных траекторий). ScalingAR не требует промежуточного декодирования или внешних reward-моделей, что делает его эффективным для NTP-подхода. Эксперименты показывают улучшение базовых моделей на 12.5% на GenEval и сокращение потребления визуальных токенов на 62% при сохранении качества."
                },
                "en": {
                    "title": "Enhancing Image Generation with Adaptive Scaling and Token Entropy",
                    "desc": "ScalingAR is a novel framework that enhances next-token prediction in autoregressive image generation by utilizing token entropy and adaptive scaling techniques. It addresses the limitations of existing test-time scaling methods that are not suitable for visual autoregressive tasks, which often struggle with incomplete intermediate results. By operating at two levels—Profile Level and Policy Level—ScalingAR effectively manages confidence states and optimizes the generation process. Experimental results demonstrate significant improvements in model performance and efficiency, showcasing its ability to reduce token consumption while increasing robustness in challenging scenarios."
                },
                "zh": {
                    "title": "ScalingAR：提升自回归图像生成的下一标记预测",
                    "desc": "ScalingAR 是一种新颖的框架，旨在提升自回归图像生成中的下一个标记预测。它通过利用标记熵作为信号，并在两个互补的缩放层面上操作，来提高模型的性能和效率。该方法消除了对早期解码和外部奖励的需求，专门针对基于下一个标记预测的图像生成进行优化。实验结果表明，ScalingAR 在多个基准测试中显著提高了模型的表现，同时有效减少了视觉标记的消耗。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.02259",
            "title": "Transformers Discover Molecular Structure Without Graph Priors",
            "url": "https://huggingface.co/papers/2510.02259",
            "abstract": "Transformers trained directly on Cartesian coordinates can achieve competitive performance in molecular energy and force prediction without predefined graphs, demonstrating adaptability and scalability.  \t\t\t\t\tAI-generated summary \t\t\t\t Graph Neural Networks (GNNs) are the dominant architecture for molecular machine learning, particularly for molecular property prediction and machine learning interatomic potentials (MLIPs). GNNs perform message passing on predefined graphs often induced by a fixed radius cutoff or k-nearest neighbor scheme. While this design aligns with the locality present in many molecular tasks, a hard-coded graph can limit expressivity due to the fixed receptive field and slows down inference with sparse graph operations. In this work, we investigate whether pure, unmodified Transformers trained directly on Cartesian coordinatesx2013without predefined graphs or physical priorsx2013can approximate molecular energies and forces. As a starting point for our analysis, we demonstrate how to train a Transformer to competitive energy and force mean absolute errors under a matched training compute budget, relative to a state-of-the-art equivariant GNN on the OMol25 dataset. We discover that the Transformer learns physically consistent patternsx2013such as attention weights that decay inversely with interatomic distancex2013and flexibly adapts them across different molecular environments due to the absence of hard-coded biases. The use of a standard Transformer also unlocks predictable improvements with respect to scaling training resources, consistent with empirical scaling laws observed in other domains. Our results demonstrate that many favorable properties of GNNs can emerge adaptively in Transformers, challenging the necessity of hard-coded graph inductive biases and pointing toward standardized, scalable architectures for molecular modeling.",
            "score": 3,
            "issue_id": 6221,
            "pub_date": "2025-10-02",
            "pub_date_card": {
                "ru": "2 октября",
                "en": "October 2",
                "zh": "10月2日"
            },
            "hash": "2f48683b9886f234",
            "authors": [
                "Tobias Kreiman",
                "Yutong Bai",
                "Fadi Atieh",
                "Elizabeth Weaver",
                "Eric Qu",
                "Aditi S. Krishnapriyan"
            ],
            "affiliations": [
                "LBNL",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.02259.jpg",
            "data": {
                "categories": [
                    "#graphs",
                    "#architecture",
                    "#dataset",
                    "#optimization",
                    "#science"
                ],
                "emoji": "⚛️",
                "ru": {
                    "title": "Трансформеры побеждают графы в молекулярном моделировании",
                    "desc": "Исследователи показали, что обычные Transformer-модели, обученные напрямую на декартовых координатах атомов без предопределённых графов, могут достигать конкурентной точности в предсказании энергии и сил молекул. В отличие от доминирующих Graph Neural Networks (GNN), которые используют жёстко заданные связи между атомами, трансформеры адаптивно обучаются физически корректным паттернам, таким как зависимость внимания от межатомных расстояний. Отсутствие встроенных индуктивных смещений делает архитектуру более гибкой и масштабируемой, позволяя применять стандартные законы масштабирования из других областей AI. Результаты ставят под вопрос необходимость специализированных графовых архитектур для молекулярного машинного обучения."
                },
                "en": {
                    "title": "Transformers: A New Era for Molecular Predictions Without Graphs",
                    "desc": "This paper explores the use of Transformers, a type of neural network, for predicting molecular energies and forces directly from Cartesian coordinates, without relying on predefined graphs. Traditionally, Graph Neural Networks (GNNs) have been used for these tasks, but they can be limited by their fixed graph structures. The authors show that Transformers can achieve competitive performance while being more adaptable and scalable, as they learn patterns based on the data rather than hard-coded rules. This research suggests that Transformers can effectively replace GNNs in molecular modeling, offering a more flexible approach to machine learning in this field."
                },
                "zh": {
                    "title": "变换器：分子建模的新选择",
                    "desc": "本研究探讨了直接在笛卡尔坐标上训练的变换器（Transformers）在分子能量和力预测中的表现。与传统的图神经网络（GNNs）不同，变换器不依赖于预定义的图结构，因此具有更好的适应性和可扩展性。实验结果表明，变换器能够学习到物理一致的模式，并在不同的分子环境中灵活适应。我们的发现挑战了硬编码图结构的必要性，指向了分子建模中标准化和可扩展的架构。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.02240",
            "title": "RewardMap: Tackling Sparse Rewards in Fine-grained Visual Reasoning via\n  Multi-Stage Reinforcement Learning",
            "url": "https://huggingface.co/papers/2510.02240",
            "abstract": "RewardMap, a multi-stage reinforcement learning framework, enhances multimodal large language models' visual understanding and reasoning skills through dense reward signals and a difficulty-aware reward design.  \t\t\t\t\tAI-generated summary \t\t\t\t Fine-grained visual reasoning remains a core challenge for multimodal large language models (MLLMs). The recently introduced ReasonMap highlights this gap by showing that even advanced MLLMs struggle with spatial reasoning in structured and information-rich settings such as transit maps, a task of clear practical and scientific importance. However, standard reinforcement learning (RL) on such tasks is impeded by sparse rewards and unstable optimization. To address this, we first construct ReasonMap-Plus, an extended dataset that introduces dense reward signals through Visual Question Answering (VQA) tasks, enabling effective cold-start training of fine-grained visual understanding skills. Next, we propose RewardMap, a multi-stage RL framework designed to improve both visual understanding and reasoning capabilities of MLLMs. RewardMap incorporates two key designs. First, we introduce a difficulty-aware reward design that incorporates detail rewards, directly tackling the sparse rewards while providing richer supervision. Second, we propose a multi-stage RL scheme that bootstraps training from simple perception to complex reasoning tasks, offering a more effective cold-start strategy than conventional Supervised Fine-Tuning (SFT). Experiments on ReasonMap and ReasonMap-Plus demonstrate that each component of RewardMap contributes to consistent performance gains, while their combination yields the best results. Moreover, models trained with RewardMap achieve an average improvement of 3.47% across 6 benchmarks spanning spatial reasoning, fine-grained visual reasoning, and general tasks beyond transit maps, underscoring enhanced visual understanding and reasoning capabilities.",
            "score": 3,
            "issue_id": 6223,
            "pub_date": "2025-10-02",
            "pub_date_card": {
                "ru": "2 октября",
                "en": "October 2",
                "zh": "10月2日"
            },
            "hash": "d80bedb4d445e906",
            "authors": [
                "Sicheng Feng",
                "Kaiwen Tuo",
                "Song Wang",
                "Lingdong Kong",
                "Jianke Zhu",
                "Huan Wang"
            ],
            "affiliations": [
                "National University of Singapore",
                "Tongji University",
                "Westlake University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.02240.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#rag",
                    "#dataset",
                    "#rl",
                    "#optimization",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "🗺️",
                "ru": {
                    "title": "Многоступенчатое обучение с подкреплением для визуального понимания",
                    "desc": "Статья представляет RewardMap — фреймворк для улучшения визуального понимания и рассуждений в мультимодальных LLM через reinforcement learning. Авторы решают проблему разреженных наград, создав датасет ReasonMap-Plus с плотными сигналами вознаграждения через VQA-задачи. Ключевые новшества включают дизайн наград с учётом сложности задач и многоступенчатую схему обучения от простого восприятия к сложным рассуждениям. Эксперименты показывают улучшение на 3.47% по шести бенчмаркам, включая пространственные рассуждения и детальное визуальное понимание."
                },
                "en": {
                    "title": "Boosting Visual Reasoning in MLLMs with RewardMap",
                    "desc": "RewardMap is a multi-stage reinforcement learning framework that improves the visual understanding and reasoning abilities of multimodal large language models (MLLMs). It addresses the challenge of fine-grained visual reasoning by introducing dense reward signals through Visual Question Answering (VQA) tasks, which helps in effective training. The framework features a difficulty-aware reward design that provides richer supervision and tackles the issue of sparse rewards. Experiments show that RewardMap significantly enhances performance across various benchmarks, demonstrating its effectiveness in boosting MLLMs' capabilities in complex reasoning tasks."
                },
                "zh": {
                    "title": "提升视觉理解与推理能力的强化学习框架",
                    "desc": "RewardMap是一个多阶段强化学习框架，旨在通过密集奖励信号和难度感知奖励设计，提升多模态大语言模型的视觉理解和推理能力。该框架解决了传统强化学习在复杂任务中面临的稀疏奖励和不稳定优化问题。通过构建ReasonMap-Plus数据集，RewardMap能够有效地进行冷启动训练，增强细粒度的视觉理解技能。实验结果表明，RewardMap的各个组件均能带来一致的性能提升，结合使用时效果最佳，模型在多个基准测试中平均提高了3.47%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.02190",
            "title": "A Rigorous Benchmark with Multidimensional Evaluation for Deep Research\n  Agents: From Answers to Reports",
            "url": "https://huggingface.co/papers/2510.02190",
            "abstract": "A benchmark and evaluation framework for Deep Research Agents (DRAs) assesses their performance on complex tasks with multidimensional metrics.  \t\t\t\t\tAI-generated summary \t\t\t\t Artificial intelligence is undergoing the paradigm shift from closed language models to interconnected agent systems capable of external perception and information integration. As a representative embodiment, Deep Research Agents (DRAs) systematically exhibit the capabilities for task decomposition, cross-source retrieval, multi-stage reasoning, and structured output, which markedly enhance performance on complex and open-ended tasks. However, existing benchmarks remain deficient in evaluation dimensions, response formatting, and scoring mechanisms, limiting their capacity to assess such systems effectively. This paper introduces a rigorous benchmark and a multidimensional evaluation framework tailored to DRAs and report-style responses. The benchmark comprises 214 expert-curated challenging queries distributed across 10 broad thematic domains, each accompanied by manually constructed reference bundles to support composite evaluation. The framework enables comprehensive evaluation of long-form reports generated by DRAs, incorporating integrated scoring metrics for semantic quality, topical focus, and retrieval trustworthiness. Extensive experimentation confirms the superior performance of mainstream DRAs over web-search-tool-augmented reasoning models, yet reveals considerable scope for further improvement. This study provides a robust foundation for capability assessment, architectural refinement, and paradigm advancement in DRA systems.",
            "score": 3,
            "issue_id": 6221,
            "pub_date": "2025-10-02",
            "pub_date_card": {
                "ru": "2 октября",
                "en": "October 2",
                "zh": "10月2日"
            },
            "hash": "98260d7be8c3395a",
            "authors": [
                "Yang Yao",
                "Yixu Wang",
                "Yuxuan Zhang",
                "Yi Lu",
                "Tianle Gu",
                "Lingyu Li",
                "Dingyi Zhao",
                "Keming Wu",
                "Haozhe Wang",
                "Ping Nie",
                "Yan Teng",
                "Yingchun Wang"
            ],
            "affiliations": [
                "Fudan University",
                "Hong Kong University of Science and Technology",
                "Peking University",
                "Shanghai Artificial Intelligence Laboratory",
                "Shanghai Jiao Tong University",
                "The University of Hong Kong",
                "Tsinghua University",
                "University of British Columbia",
                "University of Toronto"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.02190.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#agents",
                    "#evaluation",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Комплексная оценка агентов глубокого исследования",
                    "desc": "Статья представляет новый бенчмарк для оценки Deep Research Agents (DRA) — AI-агентов, способных к декомпозиции задач, поиску информации из разных источников и многоступенчатому рассуждению. Бенчмарк включает 214 экспертных вопросов из 10 тематических областей с эталонными ответами для комплексной оценки. Предложенная система оценивает длинные отчёты агентов по семантическому качеству, релевантности и достоверности источников. Эксперименты показали, что DRA превосходят обычные LLM с веб-поиском, но всё ещё имеют значительный потенциал для улучшения."
                },
                "en": {
                    "title": "Enhancing Evaluation for Deep Research Agents",
                    "desc": "This paper presents a new benchmark and evaluation framework specifically designed for Deep Research Agents (DRAs), which are advanced AI systems capable of handling complex tasks. The framework includes 214 challenging queries across various themes and offers a multidimensional approach to assess the performance of DRAs based on metrics like semantic quality and retrieval trustworthiness. It highlights the limitations of existing benchmarks in evaluating DRAs and proposes a structured method for comprehensive assessment. The findings indicate that while DRAs outperform traditional web-search models, there is still significant room for improvement in their capabilities."
                },
                "zh": {
                    "title": "深度研究代理的评估新标准",
                    "desc": "本文提出了一种针对深度研究代理（DRA）的基准和评估框架，旨在通过多维度指标评估其在复杂任务上的表现。DRA能够进行任务分解、跨源检索、多阶段推理和结构化输出，显著提升了在开放性任务中的表现。现有的评估基准在评估维度、响应格式和评分机制上存在不足，限制了对这些系统的有效评估。本文的框架包含214个专家策划的挑战性查询，支持对DRA生成的长格式报告进行全面评估，提供语义质量、主题聚焦和检索可信度的综合评分。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.02315",
            "title": "Optimal Control Meets Flow Matching: A Principled Route to Multi-Subject\n  Fidelity",
            "url": "https://huggingface.co/papers/2510.02315",
            "abstract": "A theoretical framework and algorithms for improving multi-subject fidelity in text-to-image models through control over sampling dynamics.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-image (T2I) models excel on single-entity prompts but struggle with multi-subject descriptions, often showing attribute leakage, identity entanglement, and subject omissions. We introduce the first theoretical framework with a principled, optimizable objective for steering sampling dynamics toward multi-subject fidelity. Viewing flow matching (FM) through stochastic optimal control (SOC), we formulate subject disentanglement as control over a trained FM sampler. This yields two architecture-agnostic algorithms: (i) a training-free test-time controller that perturbs the base velocity with a single-pass update, and (ii) Adjoint Matching, a lightweight fine-tuning rule that regresses a control network to a backward adjoint signal while preserving base-model capabilities. The same formulation unifies prior attention heuristics, extends to diffusion models via a flow-diffusion correspondence, and provides the first fine-tuning route explicitly designed for multi-subject fidelity. Empirically, on Stable Diffusion 3.5, FLUX, and Stable Diffusion XL, both algorithms consistently improve multi-subject alignment while maintaining base-model style. Test-time control runs efficiently on commodity GPUs, and fine-tuned controllers trained on limited prompts generalize to unseen ones. We further highlight FOCUS (Flow Optimal Control for Unentangled Subjects), which achieves state-of-the-art multi-subject fidelity across models.",
            "score": 2,
            "issue_id": 6223,
            "pub_date": "2025-10-02",
            "pub_date_card": {
                "ru": "2 октября",
                "en": "October 2",
                "zh": "10月2日"
            },
            "hash": "279a167c4dc7509b",
            "authors": [
                "Eric Tillmann Bill",
                "Enis Simsar",
                "Thomas Hofmann"
            ],
            "affiliations": [
                "ETH Zurich"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.02315.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#training",
                    "#cv",
                    "#diffusion"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Оптимальное управление для точной генерации нескольких объектов",
                    "desc": "Статья представляет теоретический фреймворк для улучшения генерации изображений с несколькими объектами в text-to-image моделях. Авторы формулируют проблему разделения объектов как задачу стохастического оптимального управления над процессом сэмплирования в flow matching моделях. Предложены два алгоритма: контроллер без обучения для test-time inference и метод Adjoint Matching для лёгкого файнтюнинга контрольной сети. Эксперименты на Stable Diffusion 3.5, FLUX и SDXL показывают, что подход FOCUS достигает state-of-the-art результатов в точности генерации множественных объектов без потери стиля базовой модели."
                },
                "en": {
                    "title": "Enhancing Multi-Subject Fidelity in Text-to-Image Models",
                    "desc": "This paper presents a new framework and algorithms aimed at enhancing the performance of text-to-image (T2I) models when generating images from multi-subject prompts. The authors identify common issues such as attribute leakage and identity entanglement that occur in these models and propose a method to control sampling dynamics to improve fidelity. They introduce two algorithms: a test-time controller that adjusts the sampling process on-the-fly and a fine-tuning method called Adjoint Matching that optimizes a control network. The results show that these approaches significantly enhance multi-subject alignment while preserving the original style of the base models, demonstrating their effectiveness across various T2I architectures."
                },
                "zh": {
                    "title": "提升多主体一致性的文本到图像模型",
                    "desc": "本文提出了一种理论框架和算法，以改善文本到图像模型在多主体场景中的表现。传统的文本到图像模型在处理单一实体时表现良好，但在多主体描述中常常出现属性泄漏和身份纠缠等问题。我们通过随机最优控制的方法，提出了一种优化采样动态的目标，从而实现多主体的解耦。实验结果表明，所提出的算法在多个模型上均能有效提高多主体的一致性，同时保持基础模型的风格。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.01304",
            "title": "Agentic Jigsaw Interaction Learning for Enhancing Visual Perception and\n  Reasoning in Vision-Language Models",
            "url": "https://huggingface.co/papers/2510.01304",
            "abstract": "AGILE, an interactive jigsaw-solving framework, enhances visual perception and reasoning in Vision-Language Models through iterative action and feedback, improving performance on jigsaw tasks and general vision tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Although current large Vision-Language Models (VLMs) have advanced in multimodal understanding and reasoning, their fundamental perceptual and reasoning abilities remain limited. Specifically, even on simple jigsaw tasks, existing VLMs perform near randomly, revealing deficiencies in core perception and reasoning capabilities. While high-quality vision-language data can enhance these capabilities, its scarcity and limited scalability impose significant constraints. To address this, we propose AGILE, an Agentic jiGsaw Interaction Learning for Enhancing visual perception and reasoning in VLMs. AGILE formulates jigsaw solving as an interactive process, enabling the model to progressively engage with the environment. At each step, the model generates executable code to perform an action based on the current state, while the environment provides fine-grained visual feedback to guide task completion. Through this iterative cycle of observation and interaction, the model incrementally improves its perceptual and reasoning capabilities via exploration and feedback. Experimental results show that AGILE not only substantially boosts performance on jigsaw tasks of varying complexity (e.g., increasing accuracy from 9.5% to 82.8% under the 2 times 2 setting) but also demonstrates strong generalization across 9 general vision tasks, achieving an average improvement of 3.1%. These results indicate notable enhancements in both perceptual and reasoning abilities. This work opens a new avenue for advancing reasoning and generalization in multimodal models and provides an efficient, scalable solution to the scarcity of multimodal reinforcement learning data. The code and datasets is available at https://github.com/yuzeng0-0/AGILE .",
            "score": 2,
            "issue_id": 6224,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 октября",
                "en": "October 1",
                "zh": "10月1日"
            },
            "hash": "1b3bf32d6b7a84f7",
            "authors": [
                "Yu Zeng",
                "Wenxuan Huang",
                "Shiting Huang",
                "Xikun Bao",
                "Yukun Qi",
                "Yiming Zhao",
                "Qiuchen Wang",
                "Lin Chen",
                "Zehui Chen",
                "Huaian Chen",
                "Wanli Ouyang",
                "Feng Zhao"
            ],
            "affiliations": [
                "East China Normal University",
                "Shanghai AI Laboratory",
                "The Chinese University of Hong Kong",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.01304.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#rl",
                    "#agents",
                    "#reasoning",
                    "#optimization",
                    "#multimodal"
                ],
                "emoji": "🧩",
                "ru": {
                    "title": "Обучение VLM через интерактивную сборку пазлов",
                    "desc": "Статья представляет AGILE — фреймворк для обучения Vision-Language Models через решение задач-головоломок (jigsaw puzzles) в интерактивном режиме. Модель итеративно взаимодействует со средой: генерирует исполняемый код для действий и получает детальную визуальную обратную связь. Такой подход значительно улучшает базовые способности к визуальному восприятию и рассуждению — точность на задачах 2×2 выросла с 9.5% до 82.8%. Метод также показывает хорошую генерализацию на 9 общих визуальных задачах со средним улучшением на 3.1%, решая проблему дефицита качественных мультимодальных данных."
                },
                "en": {
                    "title": "AGILE: Enhancing Vision-Language Models through Interactive Learning",
                    "desc": "AGILE is a framework designed to improve the visual perception and reasoning skills of Vision-Language Models (VLMs) through an interactive jigsaw-solving process. It allows models to engage with their environment iteratively, generating actions based on their current state and receiving detailed visual feedback. This method significantly enhances the model's performance on jigsaw tasks, with accuracy improvements from 9.5% to 82.8%, and also boosts generalization across various vision tasks. By addressing the limitations of existing VLMs, AGILE provides a scalable solution to enhance multimodal learning capabilities."
                },
                "zh": {
                    "title": "AGILE：提升视觉-语言模型的感知与推理能力",
                    "desc": "AGILE是一个交互式拼图解决框架，旨在通过迭代的行动和反馈来增强视觉-语言模型的视觉感知和推理能力。该框架将拼图解决过程视为一个互动过程，使模型能够逐步与环境进行交互。每一步，模型根据当前状态生成可执行代码以执行动作，同时环境提供细致的视觉反馈以指导任务完成。实验结果表明，AGILE显著提高了拼图任务的表现，并在多个视觉任务中展现出强大的泛化能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.00523",
            "title": "VIRTUE: Visual-Interactive Text-Image Universal Embedder",
            "url": "https://huggingface.co/papers/2510.00523",
            "abstract": "VIRTUE, a novel Visual-InteRactive Text-Image Universal Embedder, integrates segmentation and vision-language models to enable visual interactions and localized grounding, achieving state-of-the-art performance in representation learning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal representation learning models have demonstrated successful operation across complex tasks, and the integration of vision-language models (VLMs) has further enabled embedding models with instruction-following capabilities. However, existing embedding models lack visual-interactive capabilities to specify regions of interest from users (e.g., point, bounding box, mask), which have been explored in generative models to broaden their human-interactive applicability. Equipping embedding models with visual interactions not only would unlock new applications with localized grounding of user intent, which remains unexplored, but also enable the models to learn entity-level information within images to complement their global representations for conventional embedding tasks. In this paper, we propose a novel Visual-InteRactive Text-Image Universal Embedder (VIRTUE) that extends the capabilities of the segmentation model and the vision-language model to the realm of representation learning. In VIRTUE, the segmentation model can process visual prompts that pinpoint specific regions within an image, thereby enabling the embedder to handle complex and ambiguous scenarios more precisely. To evaluate the visual-interaction ability of VIRTUE, we introduce a large-scale Segmentation-and-Scene Caption Retrieval (SCaR) benchmark comprising 1M samples that aims to retrieve the text caption by jointly considering the entity with a specific object and image scene. VIRTUE consistently achieves a state-of-the-art performance with significant improvements across 36 universal MMEB (3.1%-8.5%) and five visual-interactive SCaR (15.2%-20.3%) tasks.",
            "score": 2,
            "issue_id": 6221,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 октября",
                "en": "October 1",
                "zh": "10月1日"
            },
            "hash": "8379081e03e82135",
            "authors": [
                "Wei-Yao Wang",
                "Kazuya Tateishi",
                "Qiyu Wu",
                "Shusuke Takahashi",
                "Yuki Mitsufuji"
            ],
            "affiliations": [
                "Sony AI",
                "Sony Group Corporation"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.00523.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#interpretability",
                    "#games",
                    "#optimization",
                    "#cv"
                ],
                "emoji": "👆",
                "ru": {
                    "title": "Embeddings с визуальным взаимодействием: указывай на объекты и получай точные представления",
                    "desc": "Представлена модель VIRTUE, которая объединяет сегментацию и vision-language модели для создания embeddings с визуальным взаимодействием. Пользователь может указывать конкретные области на изображении (точкой, bounding box или маской), что позволяет модели более точно понимать намерения и работать с локализованными объектами. Авторы создали новый benchmark SCaR с 1 миллионом примеров для оценки способности модели находить текстовые описания с учетом конкретных объектов и общей сцены. VIRTUE показывает state-of-the-art результаты, превосходя существующие методы на 3-8% на стандартных задачах и на 15-20% на задачах с визуальным взаимодействием."
                },
                "en": {
                    "title": "Empowering Visual Interaction with VIRTUE",
                    "desc": "VIRTUE is a new model that combines segmentation and vision-language techniques to enhance how machines understand and interact with images and text. It allows users to specify areas of interest in images, improving the model's ability to learn detailed information about specific objects. This capability enables more precise responses to user queries and enhances the model's performance in various representation learning tasks. The paper introduces a benchmark to test VIRTUE's effectiveness, showing it outperforms existing models in multiple tasks."
                },
                "zh": {
                    "title": "VIRTUE：开启视觉交互的新纪元",
                    "desc": "VIRTUE是一种新型的视觉交互文本-图像通用嵌入模型，它结合了分割模型和视觉语言模型，能够实现视觉交互和局部定位。该模型通过处理用户指定的感兴趣区域（如点、边界框、掩码），提升了嵌入模型的交互能力。VIRTUE在表示学习任务中表现出色，尤其是在复杂和模糊场景下的处理能力。通过引入大规模的分割和场景描述检索基准，VIRTUE在多个任务中实现了显著的性能提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.24203",
            "title": "Group-Relative REINFORCE Is Secretly an Off-Policy Algorithm:\n  Demystifying Some Myths About GRPO and Its Friends",
            "url": "https://huggingface.co/papers/2509.24203",
            "abstract": "Off-policy reinforcement learning for large language models is explored through a new derivation of group-relative REINFORCE, offering insights into importance sampling, clipping, and data-weighting strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t Off-policy reinforcement learning (RL) for large language models (LLMs) is attracting growing interest, driven by practical constraints in real-world applications, the complexity of LLM-RL infrastructure, and the need for further innovations of RL methodologies. While classic REINFORCE and its modern variants like Group Relative Policy Optimization (GRPO) are typically regarded as on-policy algorithms with limited tolerance of off-policyness, we present in this work a first-principles derivation for group-relative REINFORCE without assuming a specific training data distribution, showing that it admits a native off-policy interpretation. This perspective yields two general principles for adapting REINFORCE to off-policy settings: regularizing policy updates, and actively shaping the data distribution. Our analysis demystifies some myths about the roles of importance sampling and clipping in GRPO, unifies and reinterprets two recent algorithms -- Online Policy Mirror Descent (OPMD) and Asymmetric REINFORCE (AsymRE) -- as regularized forms of the REINFORCE loss, and offers theoretical justification for seemingly heuristic data-weighting strategies. Our findings lead to actionable insights that are validated with extensive empirical studies, and open up new opportunities for principled algorithm design in off-policy RL for LLMs. Source code for this work is available at https://github.com/modelscope/Trinity-RFT/tree/main/examples/rec_gsm8k.",
            "score": 2,
            "issue_id": 6222,
            "pub_date": "2025-09-29",
            "pub_date_card": {
                "ru": "29 сентября",
                "en": "September 29",
                "zh": "9月29日"
            },
            "hash": "e7096714c253809b",
            "authors": [
                "Chaorui Yao",
                "Yanxi Chen",
                "Yuchang Sun",
                "Yushuo Chen",
                "Wenhao Zhang",
                "Xuchen Pan",
                "Yaliang Li",
                "Bolin Ding"
            ],
            "affiliations": [
                "Alibaba Group",
                "UCLA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.24203.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#rl",
                    "#agi",
                    "#training",
                    "#rlhf"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Off-policy обучение для LLM: новый взгляд на REINFORCE",
                    "desc": "Исследователи предлагают новый взгляд на алгоритм REINFORCE для обучения с подкреплением больших языковых моделей, показывая что он изначально допускает off-policy интерпретацию. Работа объединяет и переосмысливает существующие методы, такие как GRPO, OPMD и AsymRE, через призму двух принципов: регуляризации обновлений политики и активного формирования распределения данных. Анализ развенчивает мифы о роли importance sampling и clipping в этих алгоритмах, предоставляя теоретическое обоснование для эвристических стратегий взвешивания данных. Результаты подтверждены обширными экспериментами и открывают новые возможности для разработки принципиальных алгоритмов off-policy RL для LLM."
                },
                "en": {
                    "title": "Unlocking Off-Policy Learning for Large Language Models",
                    "desc": "This paper investigates off-policy reinforcement learning (RL) techniques specifically for large language models (LLMs). It introduces a new derivation of group-relative REINFORCE, allowing for a better understanding of how importance sampling, clipping, and data-weighting can be effectively utilized in off-policy settings. The authors provide insights into regularizing policy updates and shaping data distributions, which enhance the adaptability of REINFORCE algorithms. Their findings are supported by empirical studies, paving the way for improved algorithm design in off-policy RL applications for LLMs."
                },
                "zh": {
                    "title": "离线强化学习：大型语言模型的新机遇",
                    "desc": "本文探讨了针对大型语言模型的离线强化学习，提出了一种新的群体相对REINFORCE的推导方法。研究表明，传统的REINFORCE算法可以在不假设特定训练数据分布的情况下，进行离线解释。我们提出了两个适应离线设置的原则：规范化策略更新和主动调整数据分布。通过对重要性采样和剪切的角色进行分析，本文为离线强化学习算法设计提供了新的理论依据和实证支持。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.01241",
            "title": "SKYLENAGE Technical Report: Mathematical Reasoning and\n  Contest-Innovation Benchmarks for Multi-Level Math Evaluation",
            "url": "https://huggingface.co/papers/2510.01241",
            "abstract": "SKYLENAGE benchmarks evaluate LLMs on math reasoning, revealing performance gaps and ceiling effects across different educational levels.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) now perform strongly on many public math suites, yet frontier separation within mathematics increasingly suffers from ceiling effects. We present two complementary benchmarks: SKYLENAGE-ReasoningMATH, a 100-item, structure-aware diagnostic set with per-item metadata on length, numeric density, and symbolic complexity; and SKYLENAGE-MATH, a 150-item contest-style suite spanning four stages from high school to doctoral under a seven-subject taxonomy. We evaluate fifteen contemporary LLM variants under a single setup and analyze subject x model and grade x model performance. On the contest suite, the strongest model reaches 44% while the runner-up reaches 37%; accuracy declines from high school to doctoral, and top systems exhibit a doctoral-to-high-school retention near 79%. On the reasoning set, the best model attains 81% overall, and hardest-slice results reveal clear robustness gaps between leaders and the mid-tier. In summary, we release SKYLENAGE-ReasoningMATH and report aggregate results for SKYLENAGE-MATH; together, SKYLENAGE provides a hard, reasoning-centered and broadly covering math benchmark with calibrated difficulty and rich metadata, serving as a reference benchmark for future evaluations of mathematical reasoning.",
            "score": 2,
            "issue_id": 6222,
            "pub_date": "2025-09-24",
            "pub_date_card": {
                "ru": "24 сентября",
                "en": "September 24",
                "zh": "9月24日"
            },
            "hash": "a5e2f851fdccce5f",
            "authors": [
                "Hu Wei",
                "Ze Xu",
                "Boyu Yang",
                "Linlin Miao",
                "Weiqi Zhai",
                "Yihan Li",
                "Zixuan Li",
                "Zhijun Wang",
                "Boya Wang",
                "Jianwei Yu",
                "Jialing Yuan",
                "Xiaoyue Zhang",
                "Cheng He",
                "Minglei Chen",
                "Zifan Zhang",
                "Qianhui Li",
                "Wei Wang",
                "Xiang Xu"
            ],
            "affiliations": [
                "Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.01241.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#math",
                    "#survey"
                ],
                "emoji": "📐",
                "ru": {
                    "title": "SKYLENAGE: новый сложный бенчмарк обнажает пределы математического reasoning в LLM",
                    "desc": "Исследователи представили два новых бенчмарка SKYLENAGE для оценки математических способностей LLM: ReasoningMATH с 100 задачами и детальными метаданными о структуре, и MATH со 150 задачами разного уровня от школы до докторантуры. Тестирование 15 современных LLM показало, что лучшая модель достигла только 44% точности на контестном наборе, при этом производительность падает с повышением сложности от школьного до докторского уровня. На диагностическом наборе лидирующая модель показала 81% точности, но анализ самых сложных задач выявил значительный разрыв между топовыми и средними моделями. Бенчмарки предоставляют сложный, ориентированный на reasoning набор задач с калиброванной сложностью для будущих оценок математических способностей AI."
                },
                "en": {
                    "title": "SKYLENAGE: Benchmarking Math Reasoning in LLMs",
                    "desc": "The SKYLENAGE benchmarks are designed to evaluate large language models (LLMs) on their mathematical reasoning abilities, highlighting performance gaps across different educational levels. The benchmarks consist of two parts: SKYLENAGE-ReasoningMATH, which includes a diagnostic set with detailed metadata, and SKYLENAGE-MATH, a contest-style suite that covers a range of subjects from high school to doctoral levels. The evaluation of fifteen LLM variants shows that while the best model achieves 81% accuracy on reasoning tasks, there are significant declines in performance from high school to doctoral levels. Overall, SKYLENAGE aims to provide a comprehensive and challenging benchmark for assessing mathematical reasoning in LLMs, with a focus on calibrated difficulty and detailed performance metrics."
                },
                "zh": {
                    "title": "SKYLENAGE：数学推理的新基准",
                    "desc": "SKYLENAGE基准测试评估大型语言模型（LLMs）在数学推理方面的表现，揭示了不同教育水平之间的性能差距和天花板效应。我们提出了两个互补的基准：SKYLENAGE-ReasoningMATH和SKYLENAGE-MATH，前者是一个包含100个项目的结构化诊断集，后者是一个包含150个项目的竞赛风格套件，涵盖从高中到博士的四个阶段。通过对十五种现代LLM变体的评估，我们分析了不同学科和年级的模型表现，发现准确率从高中到博士逐渐下降。SKYLENAGE提供了一个以推理为中心的数学基准，具有校准的难度和丰富的元数据，为未来的数学推理评估提供了参考。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.02272",
            "title": "Parallel Scaling Law: Unveiling Reasoning Generalization through A\n  Cross-Linguistic Perspective",
            "url": "https://huggingface.co/papers/2510.02272",
            "abstract": "Research investigates cross-linguistic transferability of reasoning capabilities in Large Reasoning Models, revealing significant variations and proposing a parallel training approach to improve generalization across languages.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in Reinforcement Post-Training (RPT) have significantly enhanced the capabilities of Large Reasoning Models (LRMs), sparking increased interest in the generalization of RL-based reasoning. While existing work has primarily focused on investigating its generalization across tasks or modalities, this study proposes a novel cross-linguistic perspective to investigate reasoning generalization. This raises a crucial question: Does the reasoning capability achieved from English RPT effectively transfer to other languages? We address this by systematically evaluating English-centric LRMs on multilingual reasoning benchmarks and introducing a metric to quantify cross-lingual transferability. Our findings reveal that cross-lingual transferability varies significantly across initial model, target language, and training paradigm. Through interventional studies, we find that models with stronger initial English capabilities tend to over-rely on English-specific patterns, leading to diminished cross-lingual generalization. To address this, we conduct a thorough parallel training study. Experimental results yield three key findings: First-Parallel Leap, a substantial leap in performance when transitioning from monolingual to just a single parallel language, and a predictable Parallel Scaling Law, revealing that cross-lingual reasoning transfer follows a power-law with the number of training parallel languages. Moreover, we identify the discrepancy between actual monolingual performance and the power-law prediction as Monolingual Generalization Gap, indicating that English-centric LRMs fail to fully generalize across languages. Our study challenges the assumption that LRM reasoning mirrors human cognition, providing critical insights for the development of more language-agnostic LRMs.",
            "score": 1,
            "issue_id": 6222,
            "pub_date": "2025-10-02",
            "pub_date_card": {
                "ru": "2 октября",
                "en": "October 2",
                "zh": "10月2日"
            },
            "hash": "ad37f939671428fb",
            "authors": [
                "Wen Yang",
                "Junhong Wu",
                "Chong Li",
                "Chengqing Zong",
                "Jiajun Zhang"
            ],
            "affiliations": [
                "Institute of Automation, Chinese Academy of Sciences",
                "School of Artificial Intelligence, University of Chinese Academy of Sciences",
                "Wuhan AI Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.02272.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#reasoning",
                    "#low_resource",
                    "#rlhf",
                    "#transfer_learning"
                ],
                "emoji": "🌐",
                "ru": {
                    "title": "Рассуждения AI не переносятся автоматически между языками",
                    "desc": "Исследование изучает способность больших моделей рассуждений (LRM), обученных на английском языке, переносить навыки логического мышления на другие языки. Оказалось, что модели с сильными начальными способностями в английском языке склонны чрезмерно полагаться на англоязычные паттерны, что ухудшает кросс-лингвистическую генерализацию. Авторы предлагают метод параллельного обучения на нескольких языках одновременно и обнаруживают степенной закон масштабирования: производительность растёт предсказуемо с увеличением числа языков в обучении. Работа показывает, что современные LLM не достигают полной языковой независимости в рассуждениях, что важно для создания более универсальных AI-систем."
                },
                "en": {
                    "title": "Enhancing Multilingual Reasoning in Large Models",
                    "desc": "This paper explores how reasoning abilities in Large Reasoning Models (LRMs) can transfer between different languages. It highlights that the effectiveness of this transfer varies based on the model's initial training, the target language, and the training methods used. The authors introduce a parallel training approach to enhance cross-lingual generalization, revealing that models trained primarily on English often struggle with other languages. Their findings suggest that improving multilingual reasoning requires a deeper understanding of how language-specific patterns affect model performance."
                },
                "zh": {
                    "title": "跨语言推理能力的提升之道",
                    "desc": "本研究探讨了大型推理模型（LRMs）在跨语言推理能力的转移性，发现存在显著差异，并提出了一种平行训练方法以提高跨语言的泛化能力。研究表明，英语为中心的LRMs在多语言推理基准上的表现不尽相同，且初始模型、目标语言和训练范式都会影响跨语言转移性。通过干预研究发现，初始英语能力较强的模型往往过度依赖英语特定模式，导致跨语言泛化能力下降。我们的实验结果揭示了平行训练的显著效果，并提出了单语言与平行语言之间的性能差距，挑战了LRM推理与人类认知相似的假设。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.01796",
            "title": "Rethinking the shape convention of an MLP",
            "url": "https://huggingface.co/papers/2510.01796",
            "abstract": "Hourglass MLP blocks, with skip connections in expanded dimensions and narrow bottlenecks, outperform conventional narrow-wide-narrow MLPs in generative tasks across image datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-layer perceptrons (MLPs) conventionally follow a narrow-wide-narrow design where skip connections operate at the input/output dimensions while processing occurs in expanded hidden spaces. We challenge this convention by proposing wide-narrow-wide (Hourglass) MLP blocks where skip connections operate at expanded dimensions while residual computation flows through narrow bottlenecks. This inversion leverages higher-dimensional spaces for incremental refinement while maintaining computational efficiency through parameter-matched designs. Implementing Hourglass MLPs requires an initial projection to lift input signals to expanded dimensions. We propose that this projection can remain fixed at random initialization throughout training, enabling efficient training and inference implementations. We evaluate both architectures on generative tasks over popular image datasets, characterizing performance-parameter Pareto frontiers through systematic architectural search. Results show that Hourglass architectures consistently achieve superior Pareto frontiers compared to conventional designs. As parameter budgets increase, optimal Hourglass configurations favor deeper networks with wider skip connections and narrower bottlenecks-a scaling pattern distinct from conventional MLPs. Our findings suggest reconsidering skip connection placement in modern architectures, with potential applications extending to Transformers and other residual networks.",
            "score": 1,
            "issue_id": 6221,
            "pub_date": "2025-10-02",
            "pub_date_card": {
                "ru": "2 октября",
                "en": "October 2",
                "zh": "10月2日"
            },
            "hash": "cb9db736774b09f8",
            "authors": [
                "Meng-Hsi Chen",
                "Yu-Ang Lee",
                "Feng-Ting Liao",
                "Da-shan Shiu"
            ],
            "affiliations": [
                "MediaTek Research",
                "National Taiwan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.01796.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#architecture",
                    "#optimization"
                ],
                "emoji": "⏳",
                "ru": {
                    "title": "Песочные часы для нейросетей: skip connections в широком пространстве",
                    "desc": "Авторы предлагают архитектуру Hourglass MLP, которая инвертирует традиционный дизайн многослойных перцептронов: skip connections работают в расширенном пространстве признаков, а основные вычисления проходят через узкие bottleneck-слои. Ключевое открытие заключается в том, что начальная проекция в высокоразмерное пространство может оставаться случайной и неизменной в процессе обучения. Эксперименты на генеративных задачах показывают, что Hourglass MLP превосходит классические архитектуры на Парето-фронтах производительности. При увеличении бюджета параметров оптимальные конфигурации становятся глубже с более широкими skip connections и узкими bottlenecks."
                },
                "en": {
                    "title": "Revolutionizing MLPs: Hourglass Architecture for Superior Generative Performance",
                    "desc": "This paper introduces Hourglass MLP blocks, which utilize a wide-narrow-wide architecture with skip connections in expanded dimensions, contrasting with the traditional narrow-wide-narrow design. By allowing residual computations to flow through narrow bottlenecks, the model enhances performance in generative tasks while maintaining computational efficiency. The authors demonstrate that fixed random initialization for input signal projections can streamline training and inference processes. Their experiments reveal that Hourglass architectures outperform conventional MLPs, suggesting a need to rethink skip connection strategies in various neural network designs."
                },
                "zh": {
                    "title": "重新定义跳跃连接：Hourglass MLP的优势",
                    "desc": "本文提出了一种新的多层感知机（MLP）结构，称为Hourglass MLP块，采用宽-窄-宽的设计，跳跃连接在扩展维度上操作，而残差计算则通过窄瓶颈流动。这种设计挑战了传统的窄-宽-窄结构，利用高维空间进行增量优化，同时保持计算效率。研究表明，Hourglass MLP在生成任务中表现优于传统设计，尤其是在参数预算增加时，最佳配置倾向于更深的网络和更宽的跳跃连接。我们的发现提示在现代架构中重新考虑跳跃连接的放置，可能对变换器和其他残差网络有广泛的应用。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.01691",
            "title": "MedQ-Bench: Evaluating and Exploring Medical Image Quality Assessment\n  Abilities in MLLMs",
            "url": "https://huggingface.co/papers/2510.01691",
            "abstract": "MedQ-Bench introduces a benchmark for language-based evaluation of medical image quality using Multi-modal Large Language Models, focusing on both perceptual and reasoning capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Medical Image Quality Assessment (IQA) serves as the first-mile safety gate for clinical AI, yet existing approaches remain constrained by scalar, score-based metrics and fail to reflect the descriptive, human-like reasoning process central to expert evaluation. To address this gap, we introduce MedQ-Bench, a comprehensive benchmark that establishes a perception-reasoning paradigm for language-based evaluation of medical image quality with Multi-modal Large Language Models (MLLMs). MedQ-Bench defines two complementary tasks: (1) MedQ-Perception, which probes low-level perceptual capability via human-curated questions on fundamental visual attributes; and (2) MedQ-Reasoning, encompassing both no-reference and comparison reasoning tasks, aligning model evaluation with human-like reasoning on image quality. The benchmark spans five imaging modalities and over forty quality attributes, totaling 2,600 perceptual queries and 708 reasoning assessments, covering diverse image sources including authentic clinical acquisitions, images with simulated degradations via physics-based reconstructions, and AI-generated images. To evaluate reasoning ability, we propose a multi-dimensional judging protocol that assesses model outputs along four complementary axes. We further conduct rigorous human-AI alignment validation by comparing LLM-based judgement with radiologists. Our evaluation of 14 state-of-the-art MLLMs demonstrates that models exhibit preliminary but unstable perceptual and reasoning skills, with insufficient accuracy for reliable clinical use. These findings highlight the need for targeted optimization of MLLMs in medical IQA. We hope that MedQ-Bench will catalyze further exploration and unlock the untapped potential of MLLMs for medical image quality evaluation.",
            "score": 1,
            "issue_id": 6221,
            "pub_date": "2025-10-02",
            "pub_date_card": {
                "ru": "2 октября",
                "en": "October 2",
                "zh": "10月2日"
            },
            "hash": "1aec9b0a96bf1d83",
            "authors": [
                "Jiyao Liu",
                "Jinjie Wei",
                "Wanying Qu",
                "Chenglong Ma",
                "Junzhi Ning",
                "Yunheng Li",
                "Ying Chen",
                "Xinzhe Luo",
                "Pengcheng Chen",
                "Xin Gao",
                "Ming Hu",
                "Huihui Xu",
                "Xin Wang",
                "Shujian Gao",
                "Dingkang Yang",
                "Zhongying Deng",
                "Jin Ye",
                "Lihao Liu",
                "Junjun He",
                "Ningsheng Xu"
            ],
            "affiliations": [
                "Fudan University",
                "Imperial College London",
                "Shanghai Artificial Intelligence Laboratory",
                "University of Cambridge"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.01691.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#healthcare",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "🏥",
                "ru": {
                    "title": "Оценка качества медицинских изображений через призму человеческого восприятия и рассуждений",
                    "desc": "MedQ-Bench — это новый benchmark для оценки качества медицинских изображений с помощью мультимодальных LLM, который фокусируется на восприятии и логическом рассуждении вместо простых числовых метрик. Benchmark включает два типа задач: MedQ-Perception для проверки базовых визуальных способностей и MedQ-Reasoning для оценки умения рассуждать о качестве изображений подобно экспертам-радиологам. Датасет охватывает пять типов медицинской визуализации и более 40 параметров качества, включая реальные клинические снимки, изображения с симулированными дефектами и AI-generated контент — всего 2600 перцептивных вопросов и 708 задач на рассуждение. Тестирование 14 современных MLLM показало, что модели обладают лишь начальными и нестабильными навыками оценки качества, что недостаточно для надежного клинического применения."
                },
                "en": {
                    "title": "Revolutionizing Medical Image Quality Assessment with Language Models",
                    "desc": "MedQ-Bench is a new benchmark designed to evaluate the quality of medical images using Multi-modal Large Language Models (MLLMs). It addresses the limitations of traditional methods that rely on simple score-based metrics, which do not capture the complex reasoning that human experts use. The benchmark includes two main tasks: MedQ-Perception for assessing basic visual attributes and MedQ-Reasoning for evaluating more complex reasoning about image quality. By providing a comprehensive set of tasks and a robust evaluation framework, MedQ-Bench aims to improve the performance of MLLMs in medical image quality assessment and encourage further research in this area."
                },
                "zh": {
                    "title": "医学图像质量评估的新基准：MedQ-Bench",
                    "desc": "MedQ-Bench是一个用于医学图像质量评估的基准，利用多模态大型语言模型（MLLMs）进行语言基础的评估。该基准关注感知和推理能力，定义了两个互补的任务：MedQ-Perception和MedQ-Reasoning。MedQ-Perception通过人类策划的问题探测低级感知能力，而MedQ-Reasoning则包括无参考和比较推理任务，旨在使模型评估与人类专家的推理过程相一致。我们的研究表明，现有的MLLM在医学图像质量评估中表现出初步但不稳定的感知和推理能力，强调了对这些模型进行针对性优化的必要性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.01670",
            "title": "Just Do It!? Computer-Use Agents Exhibit Blind Goal-Directedness",
            "url": "https://huggingface.co/papers/2510.01670",
            "abstract": "Computer-Use Agents consistently exhibit Blind Goal-Directedness, a bias that leads to risky behavior regardless of feasibility or context, as demonstrated by the BLIND-ACT benchmark.  \t\t\t\t\tAI-generated summary \t\t\t\t Computer-Use Agents (CUAs) are an increasingly deployed class of agents that take actions on GUIs to accomplish user goals. In this paper, we show that CUAs consistently exhibit Blind Goal-Directedness (BGD): a bias to pursue goals regardless of feasibility, safety, reliability, or context. We characterize three prevalent patterns of BGD: (i) lack of contextual reasoning, (ii) assumptions and decisions under ambiguity, and (iii) contradictory or infeasible goals. We develop BLIND-ACT, a benchmark of 90 tasks capturing these three patterns. Built on OSWorld, BLIND-ACT provides realistic environments and employs LLM-based judges to evaluate agent behavior, achieving 93.75% agreement with human annotations. We use BLIND-ACT to evaluate nine frontier models, including Claude Sonnet and Opus 4, Computer-Use-Preview, and GPT-5, observing high average BGD rates (80.8%) across them. We show that BGD exposes subtle risks that arise even when inputs are not directly harmful. While prompting-based interventions lower BGD levels, substantial risk persists, highlighting the need for stronger training- or inference-time interventions. Qualitative analysis reveals observed failure modes: execution-first bias (focusing on how to act over whether to act), thought-action disconnect (execution diverging from reasoning), and request-primacy (justifying actions due to user request). Identifying BGD and introducing BLIND-ACT establishes a foundation for future research on studying and mitigating this fundamental risk and ensuring safe CUA deployment.",
            "score": 1,
            "issue_id": 6222,
            "pub_date": "2025-10-02",
            "pub_date_card": {
                "ru": "2 октября",
                "en": "October 2",
                "zh": "10月2日"
            },
            "hash": "03b47e25c6ec7bd1",
            "authors": [
                "Erfan Shayegani",
                "Keegan Hines",
                "Yue Dong",
                "Nael Abu-Ghazaleh",
                "Roman Lutz",
                "Spencer Whitehead",
                "Vidhisha Balachandran",
                "Besmira Nushi",
                "Vibhav Vineet"
            ],
            "affiliations": [
                "Microsoft AI Red Team",
                "Microsoft Research AI Frontiers",
                "NVIDIA",
                "University of California, Riverside"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.01670.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#alignment",
                    "#training",
                    "#benchmark",
                    "#agents",
                    "#security"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Слепое следование цели: как AI-агенты игнорируют здравый смысл ради выполнения задачи",
                    "desc": "Исследователи обнаружили, что AI-агенты, управляющие компьютером через графический интерфейс (Computer-Use Agents), систематически проявляют «слепую целенаправленность» — стремление выполнить задачу любой ценой, игнорируя безопасность и здравый смысл. Для изучения этой проблемы создан бенчмарк BLIND-ACT с 90 задачами, который показал, что даже передовые LLM-модели вроде Claude Sonnet 4 и GPT-5 демонстрируют такое поведение в 80.8% случаев. Агенты проявляют три паттерна опасного поведения: отсутствие контекстного мышления, необоснованные предположения при неопределенности и попытки выполнить противоречивые или невыполнимые цели. Хотя специальные промпты снижают риск, проблема остается серьезной и требует новых методов обучения и inference для безопасного использования AI-агентов."
                },
                "en": {
                    "title": "Addressing Blind Goal-Directedness in AI Agents for Safer Interactions",
                    "desc": "This paper discusses a problem called Blind Goal-Directedness (BGD) in Computer-Use Agents (CUAs), which are AI systems that perform tasks on graphical user interfaces. BGD causes these agents to pursue goals without considering if they are safe or feasible, leading to risky behaviors. The authors introduce a benchmark called BLIND-ACT, which consists of 90 tasks designed to evaluate BGD in CUAs, revealing that many advanced models exhibit high rates of this bias. The study emphasizes the importance of addressing BGD to ensure the safe deployment of CUAs, suggesting that while some interventions can reduce BGD, significant risks remain."
                },
                "zh": {
                    "title": "识别盲目目标导向，确保安全的计算机使用代理",
                    "desc": "计算机使用代理（CUAs）在执行用户目标时，表现出一种称为盲目目标导向（BGD）的偏差。这种偏差使得代理在追求目标时，不考虑可行性、安全性、可靠性或上下文。本文通过BLIND-ACT基准测试，揭示了BGD的三种常见模式，并评估了多种前沿模型的表现，发现它们普遍存在高达80.8%的BGD率。研究结果强调了在代理行为中识别BGD的重要性，并为未来的研究提供了基础，以确保CUA的安全部署。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.01538",
            "title": "TimeSeriesScientist: A General-Purpose AI Agent for Time Series Analysis",
            "url": "https://huggingface.co/papers/2510.01538",
            "abstract": "TimeSeriesScientist (TSci) is an LLM-driven framework that automates time series forecasting with minimal human intervention, outperforming statistical and LLM-based methods and providing transparent reports.  \t\t\t\t\tAI-generated summary \t\t\t\t Time series forecasting is central to decision-making in domains as diverse as energy, finance, climate, and public health. In practice, forecasters face thousands of short, noisy series that vary in frequency, quality, and horizon, where the dominant cost lies not in model fitting, but in the labor-intensive preprocessing, validation, and ensembling required to obtain reliable predictions. Prevailing statistical and deep learning models are tailored to specific datasets or domains and generalize poorly. A general, domain-agnostic framework that minimizes human intervention is urgently in demand. In this paper, we introduce TimeSeriesScientist (TSci), the first LLM-driven agentic framework for general time series forecasting. The framework comprises four specialized agents: Curator performs LLM-guided diagnostics augmented by external tools that reason over data statistics to choose targeted preprocessing; Planner narrows the hypothesis space of model choice by leveraging multi-modal diagnostics and self-planning over the input; Forecaster performs model fitting and validation and, based on the results, adaptively selects the best model configuration as well as ensemble strategy to make final predictions; and Reporter synthesizes the whole process into a comprehensive, transparent report. With transparent natural-language rationales and comprehensive reports, TSci transforms the forecasting workflow into a white-box system that is both interpretable and extensible across tasks. Empirical results on eight established benchmarks demonstrate that TSci consistently outperforms both statistical and LLM-based baselines, reducing forecast error by an average of 10.4% and 38.2%, respectively. Moreover, TSci produces a clear and rigorous report that makes the forecasting workflow more transparent and interpretable.",
            "score": 1,
            "issue_id": 6223,
            "pub_date": "2025-10-02",
            "pub_date_card": {
                "ru": "2 октября",
                "en": "October 2",
                "zh": "10月2日"
            },
            "hash": "b1cf0979cd51af28",
            "authors": [
                "Haokun Zhao",
                "Xiang Zhang",
                "Jiaqi Wei",
                "Yiwei Xu",
                "Yuting He",
                "Siqi Sun",
                "Chenyu You"
            ],
            "affiliations": [
                "Case Western Reserve University",
                "Fudan University",
                "Stony Brook University",
                "University of British Columbia",
                "University of California, Los Angeles",
                "University of California, San Diego",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.01538.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#optimization",
                    "#agents",
                    "#interpretability",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "📈",
                "ru": {
                    "title": "Автоматизация прогнозов временных рядов с помощью LLM",
                    "desc": "TimeSeriesScientist (TSci) — это новая система, основанная на LLM, которая автоматизирует прогнозирование временных рядов с минимальным участием человека. Она включает четыре агента: Curator, Planner, Forecaster и Reporter, которые выполняют диагностику, выбор модели, её настройку и создание отчёта. TSci превосходит существующие статистические и LLM-методы, снижая ошибку прогноза в среднем на 10,4% и 38,2% соответственно. Система делает процесс прогнозирования более прозрачным и понятным благодаря подробным отчётам и объяснениям на естественном языке."
                },
                "en": {
                    "title": "Automating Time Series Forecasting with Transparency and Precision",
                    "desc": "TimeSeriesScientist (TSci) is an innovative framework that automates the process of time series forecasting using large language models (LLMs) with minimal human input. It addresses the challenges of handling numerous short and noisy time series by employing four specialized agents: Curator for data diagnostics, Planner for model selection, Forecaster for fitting and validation, and Reporter for generating transparent reports. TSci outperforms traditional statistical and LLM-based methods, achieving significant reductions in forecast error while providing interpretable results. This framework not only enhances the efficiency of forecasting but also ensures that the process is understandable and adaptable across various domains."
                },
                "zh": {
                    "title": "时间序列预测的智能化解决方案",
                    "desc": "TimeSeriesScientist (TSci) 是一个基于大型语言模型的框架，旨在自动化时间序列预测，减少人工干预。该框架包含四个专门的代理，分别负责数据诊断、模型选择、模型拟合和结果报告。TSci 在多个基准测试中表现优异，预测误差平均降低了10.4%到38.2%。通过提供透明的自然语言解释和全面的报告，TSci 使预测过程变得更加可解释和可扩展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.24304",
            "title": "FrameThinker: Learning to Think with Long Videos via Multi-Turn Frame\n  Spotlighting",
            "url": "https://huggingface.co/papers/2509.24304",
            "abstract": "FrameThinker, a novel framework, enhances video reasoning by iteratively interrogating video content through supervised fine-tuning and reinforcement learning, achieving significant improvements and efficiency over existing models.  \t\t\t\t\tAI-generated summary \t\t\t\t While Large Vision-Language Models (LVLMs) have achieved substantial progress in video understanding, their application to long video reasoning is hindered by uniform frame sampling and static textual reasoning, which are inefficient and struggle to handle visually intensive video tasks. To overcome these challenges, in this paper, we introduce the concept of thinking with long videos and propose a novel framework FrameThinker. Within this framework, LVLMs are able to iteratively interrogate video content. Developing such video reasoning capabilities in LVLMs presents notable challenges, particularly in adapting the model to new video actions (e.g. select frame), and designing reward functions to guide LVLMs to adopt the newly introduced action. To solve these challenges, we propose a two-phase training strategy, first employing Supervised Fine-Tuning (SFT) to instill fundamental action capabilities, followed by Reinforcement Learning (RL) to optimize a strategic decision-making policy. Notably, in this RL phase, we conduct an in-depth and comprehensive exploration of the reward design for each action and format reward. Extensive experiments on reasoning benchmarks like Video-Holmes, LongVideo-Reason, and long-video understanding benchmarks such as LongVideoBench, MLVU, VideoMME, and LVBench, demonstrate that FrameThinker achieves a significant average improvement of +10.4% over baselines while drastically reducing the number of processed frames. Most notably, our 7B model, FrameThinker establishes a new state-of-the-art on LongVideo-Reason, achieving 76.1% accuracy using an average of only 20.6 frames. This not only outperforms the competitive LongVILA-R1 (72.0%) but does so with over 20x fewer frames (vs. 512), demonstrating unparalleled efficiency and effectiveness.",
            "score": 1,
            "issue_id": 6222,
            "pub_date": "2025-09-29",
            "pub_date_card": {
                "ru": "29 сентября",
                "en": "September 29",
                "zh": "9月29日"
            },
            "hash": "8f267884ff3ee32b",
            "authors": [
                "Zefeng He",
                "Xiaoye Qu",
                "Yafu Li",
                "Siyuan Huang",
                "Daizong Liu",
                "Yu Cheng"
            ],
            "affiliations": [
                "Nanjing University",
                "Peking University",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.24304.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#video",
                    "#long_context",
                    "#rl",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Умное мышление над видео: анализ только нужных кадров",
                    "desc": "FrameThinker - это новый фреймворк для улучшения рассуждений над видео контентом с помощью Large Vision-Language Models (LVLMs). Вместо обработки всех кадров равномерно, модель итеративно выбирает и анализирует только необходимые кадры, что значительно повышает эффективность. Обучение происходит в две фазы: сначала supervised fine-tuning для базовых действий (например, выбор кадра), затем reinforcement learning для оптимизации стратегии принятия решений с тщательно разработанными функциями награды. На бенчмарках для длинных видео модель показывает улучшение на 10.4% по сравнению с базовыми моделями, при этом используя в 20 раз меньше кадров - например, достигает 76.1% точности на LongVideo-Reason, обрабатывая в среднем всего 20.6 кадров."
                },
                "en": {
                    "title": "Revolutionizing Video Reasoning with FrameThinker",
                    "desc": "FrameThinker is a new framework designed to improve video reasoning by allowing Large Vision-Language Models (LVLMs) to interactively analyze video content. It addresses the limitations of traditional methods that rely on uniform frame sampling and static reasoning, which are inefficient for complex video tasks. The framework employs a two-phase training strategy, starting with Supervised Fine-Tuning to develop basic action skills, followed by Reinforcement Learning to refine decision-making processes. Experimental results show that FrameThinker significantly enhances performance on various benchmarks, achieving state-of-the-art accuracy while processing far fewer frames than previous models."
                },
                "zh": {
                    "title": "FrameThinker：高效的视频推理新框架",
                    "desc": "FrameThinker是一个新颖的框架，通过监督微调和强化学习，逐步询问视频内容，从而增强视频推理能力。该框架解决了现有大型视觉语言模型在长视频推理中的效率问题，特别是在处理视觉密集型任务时。通过两阶段的训练策略，首先进行监督微调以建立基本动作能力，然后通过强化学习优化决策策略。实验结果表明，FrameThinker在多个基准测试中显著提高了推理准确率，同时大幅减少了处理的帧数。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.02245",
            "title": "ExGRPO: Learning to Reason from Experience",
            "url": "https://huggingface.co/papers/2510.02245",
            "abstract": "ExGRPO, a framework that prioritizes valuable reasoning experiences, improves and stabilizes reinforcement learning from verifiable rewards for large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning from verifiable rewards (RLVR) is an emerging paradigm for improving the reasoning ability of large language models. However, standard on-policy training discards rollout experiences after a single update, leading to computational inefficiency and instability. While prior work on RL has highlighted the benefits of reusing past experience, the role of experience characteristics in shaping learning dynamics of large reasoning models remains underexplored. In this paper, we are the first to investigate what makes a reasoning experience valuable and identify rollout correctness and entropy as effective indicators of experience value. Based on these insights, we propose ExGRPO (Experiential Group Relative Policy Optimization), a framework that organizes and prioritizes valuable experiences, and employs a mixed-policy objective to balance exploration with experience exploitation. Experiments on five backbone models (1.5B-8B parameters) show that ExGRPO consistently improves reasoning performance on mathematical/general benchmarks, with an average gain of +3.5/7.6 points over on-policy RLVR. Moreover, ExGRPO stabilizes training on both stronger and weaker models where on-policy methods fail. These results highlight principled experience management as a key ingredient for efficient and scalable RLVR.",
            "score": 0,
            "issue_id": 6222,
            "pub_date": "2025-10-02",
            "pub_date_card": {
                "ru": "2 октября",
                "en": "October 2",
                "zh": "10月2日"
            },
            "hash": "2d2caae66ebc961d",
            "authors": [
                "Runzhe Zhan",
                "Yafu Li",
                "Zhi Wang",
                "Xiaoye Qu",
                "Dongrui Liu",
                "Jing Shao",
                "Derek F. Wong",
                "Yu Cheng"
            ],
            "affiliations": [
                "Nanjing University",
                "Shanghai AI Laboratory",
                "The Chinese University of Hong Kong",
                "University of Macau"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.02245.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#reasoning",
                    "#rl",
                    "#optimization"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Учимся на ценном опыте: эффективное обучение с подкреплением для рассуждений",
                    "desc": "Статья представляет ExGRPO — новый фреймворк для обучения с подкреплением больших языковых моделей на задачах с проверяемыми наградами. В отличие от стандартных on-policy методов, которые выбрасывают опыт после одного обновления, ExGRPO приоритизирует ценный опыт на основе корректности рассуждений и энтропии. Эксперименты на моделях размером от 1.5B до 8B параметров показывают улучшение производительности на математических задачах в среднем на 3.5-7.6 баллов. Подход также стабилизирует обучение там, где классические on-policy методы терпят неудачу."
                },
                "en": {
                    "title": "Prioritizing Valuable Experiences for Better Reinforcement Learning",
                    "desc": "ExGRPO is a new framework designed to enhance reinforcement learning from verifiable rewards (RLVR) for large language models. It addresses the inefficiencies of traditional on-policy training by prioritizing valuable reasoning experiences, which helps stabilize the learning process. The framework identifies key indicators of experience value, such as rollout correctness and entropy, to optimize the learning dynamics. Experiments demonstrate that ExGRPO significantly improves reasoning performance across various models, making it a crucial advancement in efficient RLVR."
                },
                "zh": {
                    "title": "优先考虑有价值经验的强化学习框架",
                    "desc": "ExGRPO是一个框架，旨在优先考虑有价值的推理经验，从而改善和稳定基于可验证奖励的强化学习。传统的在线训练方法在每次更新后会丢弃经验，导致计算效率低下和不稳定。本文首次探讨了什么样的推理经验是有价值的，并确定了回滚正确性和熵作为有效的经验价值指标。通过这些见解，ExGRPO组织和优先考虑有价值的经验，并采用混合策略目标来平衡探索与经验利用。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.01623",
            "title": "VLA-R1: Enhancing Reasoning in Vision-Language-Action Models",
            "url": "https://huggingface.co/papers/2510.01623",
            "abstract": "VLA-R1 enhances VLA models with RLVR and GRPO to improve reasoning and execution, achieving better generalization and real-world performance using a new dataset with chain-of-thought supervision.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language-Action (VLA) models aim to unify perception, language understanding, and action generation, offering strong cross-task and cross-scene generalization with broad impact on embodied AI. However, current VLA models often lack explicit step-by-step reasoning, instead emitting final actions without considering affordance constraints or geometric relations. Their post-training pipelines also rarely reinforce reasoning quality, relying primarily on supervised fine-tuning with weak reward design. To address these challenges, we present VLA-R1, a reasoning-enhanced VLA that integrates Reinforcement Learning from Verifiable Rewards (RLVR) with Group Relative Policy Optimization (GRPO) to systematically optimize both reasoning and execution. Specifically, we design an RLVR-based post-training strategy with verifiable rewards for region alignment, trajectory consistency, and output formatting, thereby strengthening reasoning robustness and execution accuracy. Moreover, we develop VLA-CoT-13K, a high-quality dataset that provides chain-of-thought supervision explicitly aligned with affordance and trajectory annotations. Furthermore, extensive evaluations on in-domain, out-of-domain, simulation, and real-robot platforms demonstrate that VLA-R1 achieves superior generalization and real-world performance compared to prior VLA methods. We plan to release the model, code, and dataset following the publication of this work. Code: https://github.com/GigaAI-research/VLA-R1. Website: https://gigaai-research.github.io/VLA-R1.",
            "score": 0,
            "issue_id": 6222,
            "pub_date": "2025-10-02",
            "pub_date_card": {
                "ru": "2 октября",
                "en": "October 2",
                "zh": "10月2日"
            },
            "hash": "07cfd503342b7106",
            "authors": [
                "Angen Ye",
                "Zeyu Zhang",
                "Boyuan Wang",
                "Xiaofeng Wang",
                "Dapeng Zhang",
                "Zheng Zhu"
            ],
            "affiliations": [
                "CASIA",
                "GigaAI",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.01623.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#open_source",
                    "#dataset",
                    "#rl",
                    "#training",
                    "#agents"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "VLA с явным рассуждением для более умного управления роботами",
                    "desc": "VLA-R1 улучшает Vision-Language-Action модели, добавляя явное пошаговое рассуждение (chain-of-thought) вместо прямого предсказания действий. Авторы применяют Reinforcement Learning from Verifiable Rewards (RLVR) и Group Relative Policy Optimization (GRPO) для оптимизации качества рассуждений и точности выполнения действий роботом. Для обучения создан датасет VLA-CoT-13K с аннотациями цепочек рассуждений, учитывающих физические возможности (affordances) и траектории движения. Эксперименты показывают превосходную генерализацию модели как в симуляции, так и на реальных роботах по сравнению с предыдущими VLA методами."
                },
                "en": {
                    "title": "Enhancing Reasoning in Vision-Language-Action Models with VLA-R1",
                    "desc": "The paper introduces VLA-R1, an advanced Vision-Language-Action model that enhances reasoning and execution capabilities. It combines Reinforcement Learning from Verifiable Rewards (RLVR) and Group Relative Policy Optimization (GRPO) to improve the model's ability to reason step-by-step and generate actions that consider environmental constraints. A new dataset, VLA-CoT-13K, is created to provide chain-of-thought supervision, which helps the model learn better reasoning aligned with real-world tasks. Evaluations show that VLA-R1 outperforms previous models in both generalization and practical applications, making it a significant advancement in embodied AI."
                },
                "zh": {
                    "title": "VLA-R1：推理与执行的完美结合",
                    "desc": "VLA-R1是一种增强的视觉-语言-行动（VLA）模型，结合了可验证奖励的强化学习（RLVR）和群体相对策略优化（GRPO），旨在改善推理和执行能力。该模型通过设计基于RLVR的后训练策略，强化了区域对齐、轨迹一致性和输出格式，从而提高了推理的稳健性和执行的准确性。此外，VLA-R1使用了一个新的高质量数据集VLA-CoT-13K，提供了与可用性和轨迹注释明确对齐的思维链监督。经过广泛评估，VLA-R1在多个平台上展现出优于以往VLA方法的泛化能力和现实世界表现。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.00537",
            "title": "Spectral Scaling Laws in Language Models: How Effectively Do\n  Feed-Forward Networks Use Their Latent Space?",
            "url": "https://huggingface.co/papers/2510.00537",
            "abstract": "Research on large language models reveals an asymmetric spectral scaling law in feed-forward networks, indicating that increasing width primarily adds low-energy directions while dominant modes saturate early, leading to underutilized latent space.  \t\t\t\t\tAI-generated summary \t\t\t\t As large language models (LLMs) scale, the question is not only how large they become, but how much of their capacity is effectively utilized. Existing scaling laws relate model size to loss, yet overlook how components exploit their latent space. We study feed-forward networks (FFNs) and recast width selection as a spectral utilization problem. Using a lightweight diagnostic suite -- Hard Rank (participation ratio), Soft Rank (Shannon rank), Spectral Concentration, and the composite Spectral Utilization Index (SUI) -- we quantify how many latent directions are meaningfully activated across LLaMA, GPT-2, and nGPT families. Our key finding is an asymmetric spectral scaling law: soft rank follows an almost perfect power law with FFN width, while hard rank grows only sublinearly and with high variance. This asymmetry suggests that widening FFNs mostly adds low-energy tail directions, while dominant-mode subspaces saturate early. Moreover, at larger widths, variance further collapses into a narrow subspace, leaving much of the latent space under-utilized. These results recast FFN width selection as a principled trade-off between tail capacity and dominant-mode capacity, offering concrete guidance for inference-efficient LLM design.",
            "score": 0,
            "issue_id": 6223,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 октября",
                "en": "October 1",
                "zh": "10月1日"
            },
            "hash": "ce99291350b941ce",
            "authors": [
                "Nandan Kumar Jha",
                "Brandon Reagen"
            ],
            "affiliations": [
                "New York University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.00537.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#optimization"
                ],
                "emoji": "📊",
                "ru": {
                    "title": "Широкие сети не значит эффективные: закон спектрального масштабирования",
                    "desc": "Исследование показывает, что увеличение ширины feed-forward сетей в LLM добавляет преимущественно низкоэнергетические направления, в то время как доминантные моды насыщаются рано. Авторы вводят метрики Hard Rank, Soft Rank и Spectral Utilization Index для измерения использования латентного пространства в моделях LLaMA, GPT-2 и nGPT. Обнаружен асимметричный закон масштабирования: soft rank растёт по степенному закону, а hard rank — только сублинейно с высокой дисперсией. Результаты предлагают новый подход к выбору ширины FFN как компромисс между ёмкостью хвостовых направлений и доминантных мод для более эффективного дизайна LLM."
                },
                "en": {
                    "title": "Unlocking Latent Space: The Asymmetric Scaling of Feed-Forward Networks",
                    "desc": "This paper investigates how the width of feed-forward networks (FFNs) in large language models (LLMs) affects their performance and utilization of latent space. It introduces an asymmetric spectral scaling law, showing that increasing the width mainly enhances low-energy directions while the dominant modes reach saturation quickly. The authors use various metrics to analyze the activation of latent directions in models like LLaMA and GPT-2, revealing that much of the latent space remains under-utilized as width increases. This research provides insights into optimizing FFN width for better efficiency in LLM design by balancing tail capacity and dominant-mode capacity."
                },
                "zh": {
                    "title": "优化前馈网络宽度，提升潜在空间利用率",
                    "desc": "本研究探讨了大型语言模型（LLMs）中前馈网络（FFNs）的宽度选择问题，揭示了不对称的谱缩放法则。研究表明，增加网络宽度主要增加低能量方向，而主导模式在早期就达到饱和，导致潜在空间的利用不足。我们使用了一套轻量级的诊断工具来量化不同模型（如LLaMA、GPT-2和nGPT）中有效激活的潜在方向数量。研究结果为FFN宽度选择提供了原则性的权衡指导，帮助设计更高效的推理模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.00352",
            "title": "AReUReDi: Annealed Rectified Updates for Refining Discrete Flows with\n  Multi-Objective Guidance",
            "url": "https://huggingface.co/papers/2510.00352",
            "abstract": "AReUReDi, a discrete optimization algorithm, achieves Pareto optimality in multi-objective biomolecule sequence design, outperforming evolutionary and diffusion-based methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Designing sequences that satisfy multiple, often conflicting, objectives is a central challenge in therapeutic and biomolecular engineering. Existing generative frameworks largely operate in continuous spaces with single-objective guidance, while discrete approaches lack guarantees for multi-objective Pareto optimality. We introduce AReUReDi (Annealed Rectified Updates for Refining Discrete Flows), a discrete optimization algorithm with theoretical guarantees of convergence to the Pareto front. Building on Rectified Discrete Flows (ReDi), AReUReDi combines Tchebycheff scalarization, locally balanced proposals, and annealed Metropolis-Hastings updates to bias sampling toward Pareto-optimal states while preserving distributional invariance. Applied to peptide and SMILES sequence design, AReUReDi simultaneously optimizes up to five therapeutic properties (including affinity, solubility, hemolysis, half-life, and non-fouling) and outperforms both evolutionary and diffusion-based baselines. These results establish AReUReDi as a powerful, sequence-based framework for multi-property biomolecule generation.",
            "score": 0,
            "issue_id": 6222,
            "pub_date": "2025-09-30",
            "pub_date_card": {
                "ru": "30 сентября",
                "en": "September 30",
                "zh": "9月30日"
            },
            "hash": "9d15b447c9342ce6",
            "authors": [
                "Tong Chen",
                "Yinuo Zhang",
                "Pranam Chatterjee"
            ],
            "affiliations": [
                "Centre for Computational Biology, Duke-NUS Medical School, Singapore",
                "Department of Bioengineering, University of Pennsylvania",
                "Department of Computer and Information Science, University of Pennsylvania"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.00352.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#optimization",
                    "#math"
                ],
                "emoji": "🧬",
                "ru": {
                    "title": "Парето-оптимальный дизайн биомолекул через дискретные потоки с отжигом",
                    "desc": "В статье представлен AReUReDi — алгоритм дискретной оптимизации для дизайна последовательностей биомолекул с множественными целями. Метод основан на Rectified Discrete Flows и использует скаляризацию Чебышёва с отжигом по алгоритму Метрополиса-Гастингса для достижения Парето-оптимальности. AReUReDi успешно оптимизирует до пяти свойств одновременно (аффинность, растворимость, период полувыведения и другие) для пептидов и SMILES-последовательностей. Алгоритм превосходит эволюционные методы и подходы на основе диффузионных моделей, предоставляя теоретические гарантии сходимости к фронту Парето."
                },
                "en": {
                    "title": "AReUReDi: Optimizing Biomolecule Sequences for Multiple Objectives",
                    "desc": "AReUReDi is a novel discrete optimization algorithm designed for multi-objective biomolecule sequence design, ensuring Pareto optimality. Unlike traditional methods that often focus on single objectives or continuous spaces, AReUReDi effectively handles multiple conflicting objectives by utilizing Tchebycheff scalarization and locally balanced proposals. The algorithm employs annealed Metropolis-Hastings updates to enhance sampling towards optimal solutions while maintaining distributional invariance. When tested on peptide and SMILES sequence design, AReUReDi demonstrated superior performance compared to existing evolutionary and diffusion-based methods, optimizing several therapeutic properties simultaneously."
                },
                "zh": {
                    "title": "AReUReDi：多目标优化的新选择",
                    "desc": "AReUReDi是一种离散优化算法，能够在多目标生物分子序列设计中实现帕累托最优。与现有的进化和扩散方法相比，AReUReDi在优化多个相互冲突的目标方面表现更佳。该算法结合了Tchebycheff标量化、局部平衡提案和退火Metropolis-Hastings更新，确保了收敛到帕累托前沿的理论保证。应用于肽和SMILES序列设计时，AReUReDi能够同时优化多达五种治疗特性，展现出其在多属性生物分子生成中的强大能力。"
                }
            }
        }
    ],
    "link_prev": "2025-10-02.html",
    "link_next": "2025-10-06.html",
    "link_month": "2025-10.html",
    "short_date_prev": {
        "ru": "02.10",
        "en": "10/02",
        "zh": "10月2日"
    },
    "short_date_next": {
        "ru": "06.10",
        "en": "10/06",
        "zh": "10月6日"
    },
    "categories": {
        "#dataset": 9,
        "#data": 4,
        "#benchmark": 15,
        "#agents": 8,
        "#cv": 4,
        "#rl": 8,
        "#rlhf": 3,
        "#rag": 1,
        "#plp": 1,
        "#inference": 0,
        "#3d": 1,
        "#audio": 1,
        "#video": 3,
        "#multimodal": 8,
        "#math": 2,
        "#multilingual": 1,
        "#architecture": 7,
        "#healthcare": 2,
        "#training": 15,
        "#robotics": 0,
        "#agi": 1,
        "#games": 3,
        "#interpretability": 3,
        "#reasoning": 14,
        "#transfer_learning": 1,
        "#graphs": 1,
        "#ethics": 0,
        "#security": 2,
        "#optimization": 19,
        "#survey": 1,
        "#diffusion": 2,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 3,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 8,
        "#small_models": 1,
        "#science": 2,
        "#low_resource": 1,
        "#evaluation": 1
    }
}