{
    "date": {
        "ru": "12 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
        "en": "November 12",
        "zh": "11æœˆ12æ—¥"
    },
    "time_utc": "2025-11-12 09:00",
    "weekday": 2,
    "issue_id": 1,
    "home_page_url": "https://huggingface.co/papers?date=2025-11-12",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2511.06221",
            "title": "Tiny Model, Big Logic: Diversity-Driven Optimization Elicits Large-Model\n  Reasoning Ability in VibeThinker-1.5B",
            "url": "https://huggingface.co/papers/2511.06221",
            "abstract": "VibeThinker-1.5B, a 1.5B-parameter model using the Spectrum-to-Signal Principle, achieves superior reasoning capabilities compared to larger models at a significantly lower cost.  \t\t\t\t\tAI-generated summary \t\t\t\t Challenging the prevailing consensus that small models inherently lack robust reasoning, this report introduces VibeThinker-1.5B, a 1.5B-parameter dense model developed via our Spectrum-to-Signal Principle (SSP). This challenges the prevailing approach of scaling model parameters to enhance capabilities, as seen in models like DeepSeek R1 (671B) and Kimi k2 (>1T). The SSP framework first employs a Two-Stage Diversity-Exploring Distillation (SFT) to generate a broad spectrum of solutions, followed by MaxEnt-Guided Policy Optimization (RL) to amplify the correct signal. With a total training cost of only $7,800, VibeThinker-1.5B demonstrates superior reasoning capabilities compared to closed-source models like Magistral Medium and Claude Opus 4, and performs on par with open-source models like GPT OSS-20B Medium. Remarkably, it surpasses the 400x larger DeepSeek R1 on three math benchmarks: AIME24 (80.3 vs. 79.8), AIME25 (74.4 vs. 70.0), and HMMT25 (50.4 vs. 41.7). This is a substantial improvement over its base model (6.7, 4.3, and 0.6, respectively). On LiveCodeBench V6, it scores 51.1, outperforming Magistral Medium's 50.3 and its base model's 0.0. These findings demonstrate that small models can achieve reasoning capabilities comparable to large models, drastically reducing training and inference costs and thereby democratizing advanced AI research.",
            "score": 129,
            "issue_id": 1,
            "pub_date": "2025-11-09",
            "pub_date_card": {
                "ru": "9 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 9",
                "zh": "11æœˆ9æ—¥"
            },
            "hash": "a5a147e6e481bd5e",
            "authors": [
                "Sen Xu",
                "Yi Zhou",
                "Wei Wang",
                "Jixin Min",
                "Zhibin Yin",
                "Yingwei Dai",
                "Shixi Liu",
                "Lianyu Pang",
                "Yirong Chen",
                "Junlin Zhang"
            ],
            "affiliations": [
                "Sina Weibo Inc."
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.06221.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#open_source",
                    "#optimization",
                    "#training",
                    "#rlhf",
                    "#math",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞœĞ°Ğ»ĞµĞ½ÑŒĞºĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ÑƒĞ¼Ğ¾Ğ¼: ĞºĞ¾Ğ³Ğ´Ğ° Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ½Ğµ Ğ¸Ğ¼ĞµĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "VibeThinker-1.5B â€” ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 1.5 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹Ğ´Ğ°ÑÑ‰Ğ¸ĞµÑÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ° Spectrum-to-Signal. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ¹ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ supervised fine-tuning, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ policy optimization Ñ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²ÑĞµĞ³Ğ¾ Ğ² $7,800. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ°Ğ»Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸, Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°Ñ Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ´ĞµĞ¼Ğ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ñ… AI-ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚."
                },
                "en": {
                    "title": "Small Model, Big Reasoning: VibeThinker-1.5B Redefines AI Efficiency",
                    "desc": "VibeThinker-1.5B is a 1.5 billion parameter model that utilizes the Spectrum-to-Signal Principle (SSP) to achieve impressive reasoning abilities without the need for massive parameter counts. This model challenges the common belief that larger models are always better by demonstrating that a smaller model can outperform much larger counterparts in reasoning tasks. It employs a Two-Stage Diversity-Exploring Distillation followed by MaxEnt-Guided Policy Optimization to refine its performance. With a low training cost of $7,800, VibeThinker-1.5B not only matches but exceeds the performance of larger models on several benchmarks, showcasing the potential of smaller models in AI research."
                },
                "zh": {
                    "title": "å°æ¨¡å‹ï¼Œå¤§æ™ºæ…§ï¼",
                    "desc": "VibeThinker-1.5B æ˜¯ä¸€ä¸ªæ‹¥æœ‰ 15 äº¿å‚æ•°çš„æ¨¡å‹ï¼Œé‡‡ç”¨äº†é¢‘è°±åˆ°ä¿¡å·åŸç†ï¼ˆSSPï¼‰ï¼Œåœ¨æ¨ç†èƒ½åŠ›ä¸Šè¶…è¶Šäº†æ›´å¤§çš„æ¨¡å‹ï¼Œä¸”æˆæœ¬æ˜¾è‘—æ›´ä½ã€‚è¯¥æ¨¡å‹é€šè¿‡ä¸¤é˜¶æ®µå¤šæ ·æ€§æ¢ç´¢è’¸é¦ï¼ˆSFTï¼‰ç”Ÿæˆå¹¿æ³›çš„è§£å†³æ–¹æ¡ˆï¼Œç„¶ååˆ©ç”¨æœ€å¤§ç†µå¼•å¯¼ç­–ç•¥ä¼˜åŒ–ï¼ˆRLï¼‰æ¥å¢å¼ºæ­£ç¡®ä¿¡å·ã€‚ä¸å¤§å‹æ¨¡å‹å¦‚ DeepSeek R1ï¼ˆ671Bï¼‰å’Œ Kimi k2ï¼ˆ>1Tï¼‰ç›¸æ¯”ï¼ŒVibeThinker-1.5B ä»¥ä»… 7800 ç¾å…ƒçš„è®­ç»ƒæˆæœ¬ï¼Œå±•ç°å‡ºå“è¶Šçš„æ¨ç†èƒ½åŠ›ï¼Œç”šè‡³åœ¨æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº† 400 å€å¤§çš„ DeepSeek R1ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œå°æ¨¡å‹ä¹Ÿèƒ½å®ç°ä¸å¤§æ¨¡å‹ç›¸å½“çš„æ¨ç†èƒ½åŠ›ï¼Œä»è€Œå¤§å¹…é™ä½è®­ç»ƒå’Œæ¨ç†æˆæœ¬ï¼Œæ¨åŠ¨å…ˆè¿› AI ç ”ç©¶çš„æ™®åŠã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.07332",
            "title": "Grounding Computer Use Agents on Human Demonstrations",
            "url": "https://huggingface.co/papers/2511.07332",
            "abstract": "GroundCUA, a large-scale desktop grounding dataset, enables the development of GroundNext models that achieve state-of-the-art performance in mapping instructions to UI elements with less training data.  \t\t\t\t\tAI-generated summary \t\t\t\t Building reliable computer-use agents requires grounding: accurately connecting natural language instructions to the correct on-screen elements. While large datasets exist for web and mobile interactions, high-quality resources for desktop environments are limited. To address this gap, we introduce GroundCUA, a large-scale desktop grounding dataset built from expert human demonstrations. It covers 87 applications across 12 categories and includes 56K screenshots, with every on-screen element carefully annotated for a total of over 3.56M human-verified annotations. From these demonstrations, we generate diverse instructions that capture a wide range of real-world tasks, providing high-quality data for model training. Using GroundCUA, we develop the GroundNext family of models that map instructions to their target UI elements. At both 3B and 7B scales, GroundNext achieves state-of-the-art results across five benchmarks using supervised fine-tuning, while requiring less than one-tenth the training data of prior work. Reinforcement learning post-training further improves performance, and when evaluated in an agentic setting on the OSWorld benchmark using o3 as planner, GroundNext attains comparable or superior results to models trained with substantially more data,. These results demonstrate the critical role of high-quality, expert-driven datasets in advancing general-purpose computer-use agents.",
            "score": 104,
            "issue_id": 1,
            "pub_date": "2025-11-10",
            "pub_date_card": {
                "ru": "10 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 10",
                "zh": "11æœˆ10æ—¥"
            },
            "hash": "c514ab35d2b714dd",
            "authors": [
                "Aarash Feizi",
                "Shravan Nayak",
                "Xiangru Jian",
                "Kevin Qinghong Lin",
                "Kaixin Li",
                "Rabiul Awal",
                "Xing Han LÃ¹",
                "Johan Obando-Ceron",
                "Juan A. Rodriguez",
                "Nicolas Chapados",
                "David Vazquez",
                "Adriana Romero-Soriano",
                "Reihaneh Rabbany",
                "Perouz Taslakian",
                "Christopher Pal",
                "Spandana Gella",
                "Sai Rajeswar"
            ],
            "affiliations": [
                "CIFAR AI Chair",
                "Ecole de Technologie Superieure",
                "McGill University",
                "Mila - Quebec AI Institute",
                "National University of Singapore",
                "Polytechnique Montreal",
                "ServiceNow Research",
                "Universite de Montreal",
                "University of Oxford",
                "University of Waterloo"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.07332.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#open_source",
                    "#agents",
                    "#multimodal",
                    "#training",
                    "#dataset",
                    "#synthetic"
                ],
                "emoji": "ğŸ–¥ï¸",
                "ru": {
                    "title": "Ğ’Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… â€” ĞºĞ»ÑÑ‡ Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "GroundCUA â€” ÑÑ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ² Ñ€Ğ°Ğ±Ğ¾Ñ‡ĞµĞ³Ğ¾ ÑÑ‚Ğ¾Ğ»Ğ°, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¹. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 56 Ñ‚Ñ‹ÑÑÑ‡ ÑĞºÑ€Ğ¸Ğ½ÑˆĞ¾Ñ‚Ğ¾Ğ² Ğ¸Ğ· 87 Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 3.56 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ UI-ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ GroundCUA Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ GroundNext, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°ÑÑ‚ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ñ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ ÑĞºÑ€Ğ°Ğ½Ğ°. GroundNext Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ² 10 Ñ€Ğ°Ğ· Ğ¼ĞµĞ½ÑŒÑˆĞµĞ³Ğ¾ Ğ¾Ğ±ÑŠÑ‘Ğ¼Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¼Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ°."
                },
                "en": {
                    "title": "GroundCUA: Bridging Language and Desktop UI with Less Data",
                    "desc": "GroundCUA is a new dataset designed to improve how AI understands and interacts with desktop applications by linking natural language instructions to specific UI elements. It includes a vast collection of 56,000 screenshots from 87 different applications, with over 3.56 million annotations made by experts to ensure accuracy. The dataset allows the development of GroundNext models, which achieve top performance in mapping instructions to UI elements while using significantly less training data than previous models. By employing techniques like supervised fine-tuning and reinforcement learning, GroundNext demonstrates that high-quality datasets are essential for creating effective computer-use agents."
                },
                "zh": {
                    "title": "é«˜è´¨é‡æ•°æ®é›†æ¨åŠ¨æ¡Œé¢æ™ºèƒ½ä»£ç†çš„å‘å±•",
                    "desc": "GroundCUAæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„æ¡Œé¢åŸºç¡€æ•°æ®é›†ï¼Œæ—¨åœ¨å¸®åŠ©å¼€å‘GroundNextæ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹èƒ½å¤Ÿå°†è‡ªç„¶è¯­è¨€æŒ‡ä»¤å‡†ç¡®æ˜ å°„åˆ°ç”¨æˆ·ç•Œé¢å…ƒç´ ä¸Šã€‚è¯¥æ•°æ®é›†æ¶µç›–äº†12ä¸ªç±»åˆ«çš„87ä¸ªåº”ç”¨ç¨‹åºï¼ŒåŒ…å«56,000å¼ æˆªå›¾ï¼Œå¹¶å¯¹æ¯ä¸ªå±å¹•å…ƒç´ è¿›è¡Œäº†è¯¦ç»†æ ‡æ³¨ï¼Œæ€»è®¡è¶…è¿‡356ä¸‡æ¡ç»è¿‡äººå·¥éªŒè¯çš„æ³¨é‡Šã€‚é€šè¿‡è¿™äº›ä¸“å®¶æ¼”ç¤ºï¼Œæˆ‘ä»¬ç”Ÿæˆäº†å¤šæ ·åŒ–çš„æŒ‡ä»¤ï¼Œæ•æ‰äº†å¹¿æ³›çš„ç°å®ä»»åŠ¡ï¼Œä¸ºæ¨¡å‹è®­ç»ƒæä¾›äº†é«˜è´¨é‡çš„æ•°æ®ã€‚ä½¿ç”¨GroundCUAï¼ŒGroundNextæ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶æ‰€éœ€çš„è®­ç»ƒæ•°æ®é‡ä¸åˆ°ä¹‹å‰å·¥ä½œçš„ååˆ†ä¹‹ä¸€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.08319",
            "title": "Adaptive Multi-Agent Response Refinement in Conversational Systems",
            "url": "https://huggingface.co/papers/2511.08319",
            "abstract": "A multi-agent framework enhances conversational quality by refining responses through agents responsible for factuality, personalization, and coherence, outperforming existing methods on challenging datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have demonstrated remarkable success in conversational systems by generating human-like responses. However, they can fall short, especially when required to account for personalization or specific knowledge. In real-life settings, it is impractical to rely on users to detect these errors and request a new response. One way to address this problem is to refine the response before returning it to the user. While existing approaches focus on refining responses within a single LLM, this method struggles to consider diverse aspects needed for effective conversations. In this work, we propose refining responses through a multi-agent framework, where each agent is assigned a specific role for each aspect. We focus on three key aspects crucial to conversational quality: factuality, personalization, and coherence. Each agent is responsible for reviewing and refining one of these aspects, and their feedback is then merged to improve the overall response. To enhance collaboration among them, we introduce a dynamic communication strategy. Instead of following a fixed sequence of agents, our approach adaptively selects and coordinates the most relevant agents based on the specific requirements of each query. We validate our framework on challenging conversational datasets, demonstrating that ours significantly outperforms relevant baselines, particularly in tasks involving knowledge or user's persona, or both.",
            "score": 40,
            "issue_id": 1,
            "pub_date": "2025-11-11",
            "pub_date_card": {
                "ru": "11 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 11",
                "zh": "11æœˆ11æ—¥"
            },
            "hash": "f6496f6917a5836b",
            "authors": [
                "Soyeong Jeong",
                "Aparna Elangovan",
                "Emine Yilmaz",
                "Oleg Rokhlenko"
            ],
            "affiliations": [
                "Amazon",
                "Collate",
                "KAIST",
                "University College London"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.08319.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ¤",
                "ru": {
                    "title": "ĞœĞ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞšĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ğ°ÑĞ¿ĞµĞºÑ‚Ğµ: Ñ„Ğ°ĞºÑ‚-Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ°, Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ÑÑ‰Ğ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ° Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing Conversations with Specialized Agents",
                    "desc": "This paper presents a multi-agent framework designed to improve the quality of conversational responses generated by AI. Each agent in the framework specializes in one of three critical aspects: factuality, personalization, and coherence, ensuring that responses are well-rounded and accurate. By dynamically coordinating these agents based on the needs of each query, the framework effectively refines responses before they reach the user. The results show that this approach outperforms existing methods, particularly in complex scenarios requiring specific knowledge or user preferences."
                },
                "zh": {
                    "title": "å¤šæ™ºèƒ½ä½“æ¡†æ¶æå‡å¯¹è¯è´¨é‡",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œé€šè¿‡ä¸“é—¨è´Ÿè´£äº‹å®æ€§ã€ä¸ªæ€§åŒ–å’Œè¿è´¯æ€§çš„æ™ºèƒ½ä½“æ¥æå‡å¯¹è¯è´¨é‡ã€‚æ¯ä¸ªæ™ºèƒ½ä½“è´Ÿè´£å®¡æŸ¥å’Œæ”¹è¿›ç‰¹å®šæ–¹é¢çš„å“åº”ï¼Œæœ€ç»ˆå°†åé¦ˆåˆå¹¶ä»¥æé«˜æ•´ä½“å“åº”æ•ˆæœã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŠ¨æ€æ²Ÿé€šç­–ç•¥ï¼Œæ ¹æ®æ¯ä¸ªæŸ¥è¯¢çš„å…·ä½“éœ€æ±‚è‡ªé€‚åº”é€‰æ‹©å’Œåè°ƒæœ€ç›¸å…³çš„æ™ºèƒ½ä½“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤„ç†å¤æ‚å¯¹è¯æ•°æ®é›†æ—¶æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå°¤å…¶æ˜¯åœ¨æ¶‰åŠçŸ¥è¯†æˆ–ç”¨æˆ·ä¸ªæ€§åŒ–çš„ä»»åŠ¡ä¸­ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.05664",
            "title": "KLASS: KL-Guided Fast Inference in Masked Diffusion Models",
            "url": "https://huggingface.co/papers/2511.05664",
            "abstract": "KL-Adaptive Stability Sampling (KLASS) accelerates diffusion-based generation by identifying stable predictions, achieving significant speedups and quality improvements across various domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Masked diffusion models have demonstrated competitive results on various tasks including language generation. However, due to its iterative refinement process, the inference is often bottlenecked by slow and static sampling speed. To overcome this problem, we introduce `KL-Adaptive Stability Sampling' (KLASS), a fast yet effective sampling method that exploits token-level KL divergence to identify stable, high-confidence predictions. By unmasking multiple tokens in each iteration without any additional model training, our approach speeds up generation significantly while maintaining sample quality. On reasoning benchmarks, KLASS achieves up to 2.78times wall-clock speedups while improving performance over standard greedy decoding, attaining state-of-the-art results among diffusion-based samplers. We further validate KLASS across diverse domains, including text, image, and molecular generation, showing its effectiveness as a broadly applicable sampler across different models.",
            "score": 35,
            "issue_id": 1,
            "pub_date": "2025-11-07",
            "pub_date_card": {
                "ru": "7 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 7",
                "zh": "11æœˆ7æ—¥"
            },
            "hash": "20ad03bf2e79ccfb",
            "authors": [
                "Seo Hyun Kim",
                "Sunwoo Hong",
                "Hojung Jung",
                "Youngrok Park",
                "Se-Young Yun"
            ],
            "affiliations": [
                "KAIST AI"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.05664.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#diffusion",
                    "#optimization"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ½ĞµĞ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ: ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ KL-Adaptive Stability Sampling (KLASS), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ´Ğ¸Ğ²ĞµÑ€Ğ³ĞµĞ½Ñ†Ğ¸Ñ ĞšÑƒĞ»ÑŒĞ±Ğ°ĞºĞ°-Ğ›ĞµĞ¹Ğ±Ğ»ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ´ĞµĞ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ·Ğ° Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ 2.78 Ñ€Ğ°Ğ·Ğ° Ğ¿Ñ€Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼ Ğ¶Ğ°Ğ´Ğ½Ñ‹Ğ¼ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼. KLASS Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…: Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ», Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑ ÑĞµĞ±Ñ ĞºĞ°Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞµĞ¼Ğ¿Ğ»ĞµÑ€ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Accelerating Diffusion with KLASS: Fast and Stable Sampling!",
                    "desc": "KL-Adaptive Stability Sampling (KLASS) is a novel method designed to enhance the efficiency of diffusion-based generation models. It leverages token-level KL divergence to pinpoint stable and high-confidence predictions, allowing for faster sampling without the need for additional model training. By unmasking multiple tokens in each iteration, KLASS significantly accelerates the generation process while preserving the quality of the outputs. The method has demonstrated impressive speed improvements and performance gains across various tasks, including text, image, and molecular generation, establishing itself as a leading approach in the field."
                },
                "zh": {
                    "title": "åŠ é€Ÿç”Ÿæˆï¼Œç¨³å®šé‡‡æ ·çš„é©å‘½",
                    "desc": "KLè‡ªé€‚åº”ç¨³å®šé‡‡æ ·ï¼ˆKLASSï¼‰æ˜¯ä¸€ç§åŠ é€ŸåŸºäºæ‰©æ•£çš„ç”Ÿæˆæ–¹æ³•ï¼Œé€šè¿‡è¯†åˆ«ç¨³å®šçš„é¢„æµ‹æ¥å®ç°æ˜¾è‘—çš„é€Ÿåº¦æå‡å’Œè´¨é‡æ”¹å–„ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ä»¤ç‰Œçº§çš„KLæ•£åº¦æ¥è¯†åˆ«é«˜ç½®ä¿¡åº¦çš„é¢„æµ‹ï¼Œä»è€Œåœ¨æ¯æ¬¡è¿­ä»£ä¸­è§£é”å¤šä¸ªä»¤ç‰Œï¼Œè€Œæ— éœ€é¢å¤–çš„æ¨¡å‹è®­ç»ƒã€‚KLASSåœ¨æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†é«˜è¾¾2.78å€çš„é€Ÿåº¦æå‡ï¼ŒåŒæ—¶åœ¨æ ‡å‡†è´ªå©ªè§£ç ä¸Šæé«˜äº†æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨æ–‡æœ¬ã€å›¾åƒå’Œåˆ†å­ç”Ÿæˆç­‰å¤šä¸ªé¢†åŸŸéªŒè¯äº†KLASSçš„æœ‰æ•ˆæ€§ï¼Œæ˜¾ç¤ºå‡ºå…¶ä½œä¸ºå¹¿æ³›é€‚ç”¨çš„é‡‡æ ·å™¨çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.08567",
            "title": "The Path Not Taken: RLVR Provably Learns Off the Principals",
            "url": "https://huggingface.co/papers/2511.08567",
            "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) improves large language models by updating a limited set of parameters, which is explained by a Three-Gate Theory, revealing distinct optimization dynamics compared to supervised fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) reliably improves the reasoning performance of large language models, yet it appears to modify only a small fraction of parameters. We revisit this paradox and show that sparsity is a surface artifact of a model-conditioned optimization bias: for a fixed pretrained model, updates consistently localize to preferred parameter regions, highly consistent across runs and largely invariant to datasets and RL recipes. We mechanistically explain these dynamics with a Three-Gate Theory: Gate I (KL Anchor) imposes a KL-constrained update; Gate II (Model Geometry) steers the step off principal directions into low-curvature, spectrum-preserving subspaces; and Gate III (Precision) hides micro-updates in non-preferred regions, making the off-principal bias appear as sparsity. We then validate this theory and, for the first time, provide a parameter-level characterization of RLVR's learning dynamics: RLVR learns off principal directions in weight space, achieving gains via minimal spectral drift, reduced principal-subspace rotation, and off-principal update alignment. In contrast, SFT targets principal weights, distorts the spectrum, and even lags RLVR.   Together, these results provide the first parameter-space account of RLVR's training dynamics, revealing clear regularities in how parameters evolve. Crucially, we show that RL operates in a distinct optimization regime from SFT, so directly adapting SFT-era parameter-efficient fine-tuning (PEFT) methods can be flawed, as evidenced by our case studies on advanced sparse fine-tuning and LoRA variants. We hope this work charts a path toward a white-box understanding of RLVR and the design of geometry-aware, RLVR-native learning algorithms, rather than repurposed SFT-era heuristics.",
            "score": 32,
            "issue_id": 1,
            "pub_date": "2025-11-11",
            "pub_date_card": {
                "ru": "11 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 11",
                "zh": "11æœˆ11æ—¥"
            },
            "hash": "54a6e05baaa5180b",
            "authors": [
                "Hanqing Zhu",
                "Zhenyu Zhang",
                "Hanxian Huang",
                "DiJia Su",
                "Zechun Liu",
                "Jiawei Zhao",
                "Igor Fedorov",
                "Hamed Pirsiavash",
                "Zhizhou Sha",
                "Jinwon Lee",
                "David Z. Pan",
                "Zhangyang Wang",
                "Yuandong Tian",
                "Kai Sheng Tai"
            ],
            "affiliations": [
                "Meta AI",
                "The University of Texas at Austin"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.08567.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#interpretability",
                    "#alignment",
                    "#architecture",
                    "#training",
                    "#rl",
                    "#reasoning"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ²Ğ¾Ğ¹ Ğ¿ÑƒÑ‚ÑŒ: Ğ¿Ğ¾Ñ‡ĞµĞ¼Ñƒ RLVR Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµÑ‚ Ğ½ĞµĞ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ½Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¾ĞºÑ, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ (RLVR) ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ…Ğ¾Ñ‚Ñ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆÑƒÑ Ñ‡Ğ°ÑÑ‚ÑŒ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±ÑŠÑÑĞ½ÑÑÑ‚ ÑÑ‚Ğ¾Ñ‚ Ñ„ĞµĞ½Ğ¾Ğ¼ĞµĞ½ Ñ‚ĞµĞ¾Ñ€Ğ¸ĞµĞ¹ Ñ‚Ñ€Ñ‘Ñ… Ğ·Ğ°Ñ‚Ğ²Ğ¾Ñ€Ğ¾Ğ²: KL-Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹, Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ½Ğ¸Ğ·ĞºĞ¾ĞºÑ€Ğ¸Ğ²Ğ¸Ğ·Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ¸ ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ¼Ğ¸ĞºÑ€Ğ¾-Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ½ĞµĞ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ğ¸Ñ‚Ğ°ĞµĞ¼Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ RLVR Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ² Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑÑ…, Ğ¾Ñ€Ñ‚Ğ¾Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ³Ğ»Ğ°Ğ²Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ²ĞµÑĞ¾Ğ², Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ñ€ĞµĞ¹Ñ„ Ğ¸ Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ° RLVR Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ÑÑ Ğ¾Ñ‚ supervised fine-tuning, Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ¼Ñƒ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€-ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ SFT, Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼."
                },
                "en": {
                    "title": "Unlocking Efficient Learning with RLVR's Three-Gate Theory",
                    "desc": "Reinforcement Learning with Verifiable Rewards (RLVR) enhances the reasoning capabilities of large language models by making targeted updates to a small number of parameters. This paper introduces the Three-Gate Theory, which explains how RLVR operates differently from traditional supervised fine-tuning (SFT) by focusing on specific parameter regions rather than the entire model. The theory outlines three gates that control the optimization process, ensuring that updates are efficient and localized, which leads to improved performance without significant changes to the model's overall structure. The findings suggest that RLVR's unique optimization dynamics require new approaches to fine-tuning that are tailored to its specific learning behavior, rather than relying on methods developed for SFT."
                },
                "zh": {
                    "title": "å¼ºåŒ–å­¦ä¹ çš„æ–°è§†è§’ï¼šä¼˜åŒ–ä¸å‚æ•°æ¼”å˜çš„ç§˜å¯†",
                    "desc": "å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰é€šè¿‡æ›´æ–°æœ‰é™çš„å‚æ•°é›†æ¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†æ€§èƒ½ã€‚æœ¬æ–‡æå‡ºäº†ä¸‰é—¨ç†è®ºï¼Œæ­ç¤ºäº†ä¸ç›‘ç£å¾®è°ƒç›¸æ¯”ï¼ŒRLVRå…·æœ‰ä¸åŒçš„ä¼˜åŒ–åŠ¨æ€ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒRLVRåœ¨æƒé‡ç©ºé—´ä¸­æ²¿éä¸»æ–¹å‘å­¦ä¹ ï¼Œåˆ©ç”¨æœ€å°çš„è°±æ¼‚ç§»å’Œå‡å°‘çš„ä¸»å­ç©ºé—´æ—‹è½¬æ¥å®ç°æ€§èƒ½æå‡ã€‚ä¸ä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒæ–¹æ³•ç›¸æ¯”ï¼ŒRLVRå±•ç°å‡ºæ›´é«˜çš„å‚æ•°æ•ˆç‡å’Œæ›´ç¨³å®šçš„å­¦ä¹ è¿‡ç¨‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.07003",
            "title": "Beyond English: Toward Inclusive and Scalable Multilingual Machine Translation with LLMs",
            "url": "https://huggingface.co/papers/2511.07003",
            "abstract": "LMT, a suite of large-scale multilingual translation models, addresses challenges in multilingual machine translation through strategic downsampling and parallel multilingual prompting, achieving state-of-the-art performance across 60 languages.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models have significantly advanced Multilingual Machine Translation (MMT), yet the broad language coverage, consistent translation quality, and English-centric bias remain open challenges. To address these challenges, we introduce LMT, a suite of Large-scale Multilingual Translation models centered on both Chinese and English, covering 60 languages and 234 translation directions. During development, we identify a previously overlooked phenomenon of directional degeneration, where symmetric multi-way fine-tuning data overemphasize reverse directions (X to En/Zh), leading to excessive many-to-one mappings and degraded translation quality. We propose Strategic Downsampling, a simple yet effective method to mitigate this degeneration. In addition, we design Parallel Multilingual Prompting (PMP), which leverages typologically related auxiliary languages to enhance cross-lingual transfer. Through rigorous data curation and refined adaptation strategies, LMT achieves SOTA performance among models of comparable language coverage, with our 4B model (LMT-60-4B) surpassing the much larger Aya-101-13B and NLLB-54B models by a substantial margin. We release LMT in four sizes (0.6B/1.7B/4B/8B) to catalyze future research and provide strong baselines for inclusive, scalable, and high-quality MMT \\href{https://github.com/NiuTrans/LMT{https://github.com/NiuTrans/LMT}}.",
            "score": 32,
            "issue_id": 1,
            "pub_date": "2025-11-10",
            "pub_date_card": {
                "ru": "10 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 10",
                "zh": "11æœˆ10æ—¥"
            },
            "hash": "63857de97af80d83",
            "authors": [
                "Yingfeng Luo",
                "Ziqiang Xu",
                "Yuxuan Ouyang",
                "Murun Yang",
                "Dingyang Lin",
                "Kaiyan Chang",
                "Tong Zheng",
                "Bei Li",
                "Peinan Feng",
                "Quan Du",
                "Tong Xiao",
                "Jingbo Zhu"
            ],
            "affiliations": [
                "NiuTrans Research, Shenyang, China",
                "School of Computer Science and Engineering, Northeastern University, Shenyang, China"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.07003.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#low_resource",
                    "#open_source",
                    "#transfer_learning",
                    "#training",
                    "#dataset",
                    "#multilingual",
                    "#translation"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ¡Ñ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ: Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğµ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° LMT â€” Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… 60 ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¸ 234 Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ñ ÑƒĞ¿Ğ¾Ñ€Ğ¾Ğ¼ Ğ½Ğ° ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ğ¹ Ğ¸ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸, Ğ¿Ñ€Ğ¸ ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ many-to-one Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼ Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Strategic Downsampling, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ° Parallel Multilingual Prompting, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ñ‚Ğ¸Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ±Ğ»Ğ¸Ğ·ĞºĞ¸Ğµ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºÑ€Ğ¾ÑÑ-Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„ĞµÑ€Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ LMT Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ SOTA Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ¼ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²."
                },
                "en": {
                    "title": "LMT: Breaking Language Barriers with Advanced Multilingual Translation",
                    "desc": "The paper introduces LMT, a suite of large-scale multilingual translation models designed to improve multilingual machine translation (MMT) across 60 languages. It addresses key challenges such as English-centric bias and inconsistent translation quality by implementing Strategic Downsampling to prevent directional degeneration in translation data. Additionally, Parallel Multilingual Prompting (PMP) is utilized to enhance translation performance by leveraging related auxiliary languages. LMT achieves state-of-the-art results, outperforming larger models while being available in multiple sizes to support further research in MMT."
                },
                "zh": {
                    "title": "LMTï¼šæ¨åŠ¨å¤šè¯­è¨€ç¿»è¯‘çš„æ–°çªç ´",
                    "desc": "LMTæ˜¯ä¸€å¥—å¤§å‹å¤šè¯­è¨€ç¿»è¯‘æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³å¤šè¯­è¨€æœºå™¨ç¿»è¯‘ä¸­çš„æŒ‘æˆ˜ã€‚é€šè¿‡æˆ˜ç•¥ä¸‹é‡‡æ ·å’Œå¹¶è¡Œå¤šè¯­è¨€æç¤ºï¼ŒLMTåœ¨60ç§è¯­è¨€ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„ç¿»è¯‘æ€§èƒ½ã€‚ç ”ç©¶ä¸­å‘ç°äº†æ–¹å‘é€€åŒ–ç°è±¡ï¼Œå¯¼è‡´ç¿»è¯‘è´¨é‡ä¸‹é™ï¼Œå› æ­¤æå‡ºäº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚LMTçš„å‘å¸ƒå°†ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›å¼ºæœ‰åŠ›çš„åŸºå‡†ï¼Œæ¨åŠ¨é«˜è´¨é‡çš„å¤šè¯­è¨€ç¿»è¯‘å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.07080",
            "title": "Wasm: A Pipeline for Constructing Structured Arabic Interleaved\n  Multimodal Corpora",
            "url": "https://huggingface.co/papers/2511.07080",
            "abstract": "A pipeline for processing the Common Crawl dataset to create a new Arabic multimodal dataset that preserves document structure and supports both text-only and multimodal pre-training.  \t\t\t\t\tAI-generated summary \t\t\t\t The performance of large language models (LLMs) and large multimodal models (LMMs) depends heavily on the quality and scale of their pre-training datasets. Recent research shows that large multimodal models trained on natural documents where images and text are interleaved outperform those trained only on image-text pairs across a wide range of benchmarks, leveraging advanced pre- trained models to enforce semantic alignment, image-sequence consistency, and textual coherence. For Arabic, however, the lack of high-quality multimodal datasets that preserve document structure has limited progress. In this paper, we present our pipeline Wasm for processing the Common Crawl dataset to create a new Arabic multimodal dataset that uniquely provides markdown output. Unlike existing Arabic corpora that focus solely on text extraction, our approach preserves the structural integrity of web content while maintaining flexibility for both text-only and multimodal pre-training scenarios. We provide a comprehensive comparative analysis of our data processing pipeline against those used for major existing datasets, highlighting the convergences in filtering strategies and justifying our specific design choices. To support future research, we publicly release a representative dataset dump along with the multimodal processing pipeline for Arabic.",
            "score": 31,
            "issue_id": 1,
            "pub_date": "2025-11-10",
            "pub_date_card": {
                "ru": "10 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 10",
                "zh": "11æœˆ10æ—¥"
            },
            "hash": "22c66b82a539c9c4",
            "authors": [
                "Khalil Hennara",
                "Ahmad Bastati",
                "Muhammad Hreden",
                "Mohamed Motasim Hamed",
                "Zeina Aldallal",
                "Sara Chrouf",
                "Safwan AlModhayan"
            ],
            "affiliations": [
                "Khobar, Saudi Arabia"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.07080.jpg",
            "data": {
                "categories": [
                    "#low_resource",
                    "#open_source",
                    "#multimodal",
                    "#data",
                    "#dataset",
                    "#multilingual",
                    "#synthetic"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Wasm Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¾Ğ³Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Common Crawl, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ²ĞµĞ±-Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ markdown. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ… Ñ Ñ‡ĞµÑ€ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ÑÑ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ¿Ğ°Ñ€Ğ°Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ñ‚ĞµĞºÑÑ‚. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ°Ğº Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… LLM, Ñ‚Ğ°Ğº Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ² Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¸Ñ… ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ¾Ğ², ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ¾Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ğ¾ Ğ²Ñ‹Ğ¿ÑƒÑĞºĞ°ÑÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¾Ğ¹ LLM Ğ¸ LMM."
                },
                "en": {
                    "title": "Enhancing Arabic Multimodal Learning with Structured Data",
                    "desc": "This paper introduces a new pipeline called Wasm for processing the Common Crawl dataset to create a multimodal dataset specifically for the Arabic language. The dataset preserves the structure of documents, allowing for both text-only and multimodal pre-training, which is crucial for enhancing the performance of large language and multimodal models. The authors highlight the limitations of existing Arabic datasets that focus only on text extraction and demonstrate how their approach maintains the integrity of web content. Additionally, they provide a detailed comparison of their processing methods with existing datasets and release their dataset and pipeline for future research."
                },
                "zh": {
                    "title": "æ„å»ºé˜¿æ‹‰ä¼¯å¤šæ¨¡æ€æ•°æ®é›†ï¼Œæå‡æ¨¡å‹æ€§èƒ½",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§å¤„ç†Common Crawlæ•°æ®é›†çš„ç®¡é“ï¼Œæ—¨åœ¨åˆ›å»ºä¸€ä¸ªæ–°çš„é˜¿æ‹‰ä¼¯å¤šæ¨¡æ€æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†ä¿ç•™äº†æ–‡æ¡£ç»“æ„ï¼Œæ”¯æŒæ–‡æœ¬å’Œå¤šæ¨¡æ€çš„é¢„è®­ç»ƒã€‚ç ”ç©¶è¡¨æ˜ï¼Œè®­ç»ƒåœ¨è‡ªç„¶æ–‡æ¡£ä¸Šçš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºä»…ä½¿ç”¨å›¾åƒ-æ–‡æœ¬å¯¹çš„æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…å…³æ³¨æ–‡æœ¬æå–ï¼Œè¿˜ä¿æŒäº†ç½‘é¡µå†…å®¹çš„ç»“æ„å®Œæ•´æ€§ï¼Œæä¾›äº†çµæ´»çš„é¢„è®­ç»ƒé€‰é¡¹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.06281",
            "title": "VideoSSR: Video Self-Supervised Reinforcement Learning",
            "url": "https://huggingface.co/papers/2511.06281",
            "abstract": "A novel video self-supervised reinforcement learning framework, VideoSSR, enhances MLLM performance across various video understanding tasks by leveraging intrinsic video information.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) has substantially advanced the video understanding capabilities of Multimodal Large Language Models (MLLMs). However, the rapid progress of MLLMs is outpacing the complexity of existing video datasets, while the manual annotation of new, high-quality data remains prohibitively expensive. This work investigates a pivotal question: Can the rich, intrinsic information within videos be harnessed to self-generate high-quality, verifiable training data? To investigate this, we introduce three self-supervised pretext tasks: Anomaly Grounding, Object Counting, and Temporal Jigsaw. We construct the Video Intrinsic Understanding Benchmark (VIUBench) to validate their difficulty, revealing that current state-of-the-art MLLMs struggle significantly on these tasks. Building upon these pretext tasks, we develop the VideoSSR-30K dataset and propose VideoSSR, a novel video self-supervised reinforcement learning framework for RLVR. Extensive experiments across 17 benchmarks, spanning four major video domains (General Video QA, Long Video QA, Temporal Grounding, and Complex Reasoning), demonstrate that VideoSSR consistently enhances model performance, yielding an average improvement of over 5\\%. These results establish VideoSSR as a potent foundational framework for developing more advanced video understanding in MLLMs. The code is available at https://github.com/lcqysl/VideoSSR.",
            "score": 24,
            "issue_id": 1,
            "pub_date": "2025-11-09",
            "pub_date_card": {
                "ru": "9 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 9",
                "zh": "11æœˆ9æ—¥"
            },
            "hash": "2c35c4aa0f09b95f",
            "authors": [
                "Zefeng He",
                "Xiaoye Qu",
                "Yafu Li",
                "Siyuan Huang",
                "Daizong Liu",
                "Yu Cheng"
            ],
            "affiliations": [
                "Nanjing University",
                "Shanghai Artificial Intelligence Laboratory",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong",
                "Wuhan University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.06281.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#video",
                    "#multimodal",
                    "#dataset",
                    "#rl"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ VideoSSR â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ñ‚Ñ€Ğ¸ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹, Ğ¿Ğ¾Ğ´ÑÑ‡Ñ‘Ñ‚ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¿ĞµÑ€ĞµÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºÑƒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ±ĞµĞ· Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸. Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº VIUBench, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ ÑÑ‚Ğ¸Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ Ğ¸Ñ… Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ½Ğ° 17 Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ½Ğ° 5% Ğ²Ğ¾ Ğ²ÑĞµÑ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Harnessing Video Intrinsic Information for Enhanced Learning",
                    "desc": "The paper introduces VideoSSR, a self-supervised reinforcement learning framework designed to improve the performance of Multimodal Large Language Models (MLLMs) in video understanding tasks. It addresses the challenge of limited high-quality video datasets by utilizing intrinsic information from videos to create verifiable training data. The authors propose three self-supervised pretext tasks and develop the Video Intrinsic Understanding Benchmark (VIUBench) to assess their complexity. Experimental results show that VideoSSR significantly enhances MLLM performance across various benchmarks, demonstrating its effectiveness in advancing video understanding capabilities."
                },
                "zh": {
                    "title": "åˆ©ç”¨è§†é¢‘å†…åœ¨ä¿¡æ¯æå‡æ¨¡å‹æ€§èƒ½çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è§†é¢‘è‡ªç›‘ç£å¼ºåŒ–å­¦ä¹ æ¡†æ¶VideoSSRï¼Œæ—¨åœ¨é€šè¿‡åˆ©ç”¨è§†é¢‘å†…åœ¨ä¿¡æ¯æ¥æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„æ€§èƒ½ã€‚ç ”ç©¶ä¸­å¼•å…¥äº†ä¸‰ä¸ªè‡ªç›‘ç£çš„é¢„è®­ç»ƒä»»åŠ¡ï¼šå¼‚å¸¸å®šä½ã€ç‰©ä½“è®¡æ•°å’Œæ—¶é—´æ‹¼å›¾ï¼Œä»¥ç”Ÿæˆé«˜è´¨é‡çš„å¯éªŒè¯è®­ç»ƒæ•°æ®ã€‚é€šè¿‡æ„å»ºè§†é¢‘å†…åœ¨ç†è§£åŸºå‡†ï¼ˆVIUBenchï¼‰ï¼ŒéªŒè¯äº†è¿™äº›ä»»åŠ¡çš„éš¾åº¦ï¼Œå¹¶å‘ç°ç°æœ‰çš„æœ€å…ˆè¿›çš„MLLMåœ¨è¿™äº›ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVideoSSRåœ¨17ä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡æ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ï¼Œå¹³å‡æé«˜è¶…è¿‡5%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.07587",
            "title": "Beyond Fact Retrieval: Episodic Memory for RAG with Generative Semantic Workspaces",
            "url": "https://huggingface.co/papers/2511.07587",
            "abstract": "The Generative Semantic Workspace (GSW) enhances LLMs' long-context reasoning by creating structured, interpretable representations of evolving situations, outperforming existing methods on the Episodic Memory Benchmark and reducing inference time.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) face fundamental challenges in long-context reasoning: many documents exceed their finite context windows, while performance on texts that do fit degrades with sequence length, necessitating their augmentation with external memory frameworks. Current solutions, which have evolved from retrieval using semantic embeddings to more sophisticated structured knowledge graphs representations for improved sense-making and associativity, are tailored for fact-based retrieval and fail to build the space-time-anchored narrative representations required for tracking entities through episodic events. To bridge this gap, we propose the Generative Semantic Workspace (GSW), a neuro-inspired generative memory framework that builds structured, interpretable representations of evolving situations, enabling LLMs to reason over evolving roles, actions, and spatiotemporal contexts. Our framework comprises an Operator, which maps incoming observations to intermediate semantic structures, and a Reconciler, which integrates these into a persistent workspace that enforces temporal, spatial, and logical coherence. On the Episodic Memory Benchmark (EpBench) huet_episodic_2025 comprising corpora ranging from 100k to 1M tokens in length, GSW outperforms existing RAG based baselines by up to 20\\%. Furthermore, GSW is highly efficient, reducing query-time context tokens by 51\\% compared to the next most token-efficient baseline, reducing inference time costs considerably. More broadly, GSW offers a concrete blueprint for endowing LLMs with human-like episodic memory, paving the way for more capable agents that can reason over long horizons.",
            "score": 8,
            "issue_id": 1,
            "pub_date": "2025-11-10",
            "pub_date_card": {
                "ru": "10 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 10",
                "zh": "11æœˆ10æ—¥"
            },
            "hash": "26a754ddb1bf04c0",
            "authors": [
                "Shreyas Rajesh",
                "Pavan Holur",
                "Chenda Duan",
                "David Chong",
                "Vwani Roychowdhury"
            ],
            "affiliations": [
                "University of California, Los Angeles"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.07587.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#benchmark",
                    "#interpretability",
                    "#agents",
                    "#long_context",
                    "#inference",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Generative Semantic Workspace (GSW) â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸. GSW ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ñ…ÑÑ ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸Ğ¹, Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ñ€Ğ¾Ğ»ĞµĞ¹, Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹. ĞšĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ Operator Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹, Ğ° Reconciler Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ñ… Ğ² Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‡ĞµĞµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ, Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ. ĞĞ° Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ EpBench GSW Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ RAG Ğ½Ğ° 20% Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° 51%, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Empowering LLMs with Human-like Episodic Memory",
                    "desc": "The Generative Semantic Workspace (GSW) is a new framework designed to improve the long-context reasoning abilities of Large Language Models (LLMs). It creates structured and interpretable representations of situations, allowing LLMs to better track entities and events over time. GSW outperforms existing methods on the Episodic Memory Benchmark by enhancing coherence and reducing inference time significantly. This approach not only boosts performance but also provides a model for developing LLMs with human-like episodic memory capabilities."
                },
                "zh": {
                    "title": "ç”Ÿæˆè¯­ä¹‰å·¥ä½œåŒºï¼šæå‡é•¿ä¸Šä¸‹æ–‡æ¨ç†èƒ½åŠ›çš„å…³é”®",
                    "desc": "ç”Ÿæˆè¯­ä¹‰å·¥ä½œåŒºï¼ˆGSWï¼‰é€šè¿‡åˆ›å»ºç»“æ„åŒ–å’Œå¯è§£é‡Šçš„æ¼”å˜æƒ…å†µè¡¨ç¤ºï¼Œå¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é•¿ä¸Šä¸‹æ–‡æ¨ç†ä¸­çš„èƒ½åŠ›ã€‚è¯¥æ–¹æ³•åœ¨æƒ…èŠ‚è®°å¿†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶æ˜¾è‘—å‡å°‘äº†æ¨ç†æ—¶é—´ã€‚GSWåŒ…æ‹¬ä¸€ä¸ªæ“ä½œå™¨ï¼Œå°†è¾“å…¥è§‚å¯Ÿæ˜ å°„åˆ°ä¸­é—´è¯­ä¹‰ç»“æ„ï¼Œä»¥åŠä¸€ä¸ªè°ƒå’Œå™¨ï¼Œå°†è¿™äº›ç»“æ„æ•´åˆåˆ°ä¸€ä¸ªæŒä¹…çš„å·¥ä½œåŒºä¸­ï¼Œä»¥ç¡®ä¿æ—¶é—´ã€ç©ºé—´å’Œé€»è¾‘çš„ä¸€è‡´æ€§ã€‚æ€»ä½“è€Œè¨€ï¼ŒGSWä¸ºèµ‹äºˆLLMsäººç±»èˆ¬çš„æƒ…èŠ‚è®°å¿†æä¾›äº†å…·ä½“è“å›¾ï¼Œæ¨åŠ¨äº†æ›´å¼ºå¤§çš„æ™ºèƒ½ä½“çš„å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.07885",
            "title": "Intelligence per Watt: Measuring Intelligence Efficiency of Local AI",
            "url": "https://huggingface.co/papers/2511.07885",
            "abstract": "Local inference using small language models on accelerators can accurately handle many real-world queries, significantly reducing demand on centralized cloud infrastructure, as measured by intelligence per watt.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language model (LLM) queries are predominantly processed by frontier models in centralized cloud infrastructure. Rapidly growing demand strains this paradigm, and cloud providers struggle to scale infrastructure at pace. Two advances enable us to rethink this paradigm: small LMs (<=20B active parameters) now achieve competitive performance to frontier models on many tasks, and local accelerators (e.g., Apple M4 Max) run these models at interactive latencies. This raises the question: can local inference viably redistribute demand from centralized infrastructure? Answering this requires measuring whether local LMs can accurately answer real-world queries and whether they can do so efficiently enough to be practical on power-constrained devices (i.e., laptops). We propose intelligence per watt (IPW), task accuracy divided by unit of power, as a metric for assessing capability and efficiency of local inference across model-accelerator pairs. We conduct a large-scale empirical study across 20+ state-of-the-art local LMs, 8 accelerators, and a representative subset of LLM traffic: 1M real-world single-turn chat and reasoning queries. For each query, we measure accuracy, energy, latency, and power. Our analysis reveals 3 findings. First, local LMs can accurately answer 88.7% of single-turn chat and reasoning queries with accuracy varying by domain. Second, from 2023-2025, IPW improved 5.3x and local query coverage rose from 23.2% to 71.3%. Third, local accelerators achieve at least 1.4x lower IPW than cloud accelerators running identical models, revealing significant headroom for optimization. These findings demonstrate that local inference can meaningfully redistribute demand from centralized infrastructure, with IPW serving as the critical metric for tracking this transition. We release our IPW profiling harness for systematic intelligence-per-watt benchmarking.",
            "score": 6,
            "issue_id": 1,
            "pub_date": "2025-11-11",
            "pub_date_card": {
                "ru": "11 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 11",
                "zh": "11æœˆ11æ—¥"
            },
            "hash": "1966e52e54e6ac4d",
            "authors": [
                "Jon Saad-Falcon",
                "Avanika Narayan",
                "Hakki Orhun Akengin",
                "J. Wes Griffin",
                "Herumb Shandilya",
                "Adrian Gamarra Lafuente",
                "Medhya Goel",
                "Rebecca Joseph",
                "Shlok Natarajan",
                "Etash Kumar Guha",
                "Shang Zhu",
                "Ben Athiwaratkun",
                "John Hennessy",
                "Azalia Mirhoseini",
                "Christopher RÃ©"
            ],
            "affiliations": [
                "Stanford University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.07885.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#small_models",
                    "#open_source",
                    "#optimization",
                    "#inference"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "ĞÑ‚ Ğ¾Ğ±Ğ»Ğ°ĞºĞ° Ğº ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ñƒ: Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ²Ğ¾Ğ´ Ğ¼Ğ°Ğ»Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ°Ğº Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ° Ğ¾Ğ±Ğ»Ğ°Ñ‡Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸ÑĞ¼",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ° Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (Ğ´Ğ¾ 20 Ğ¼Ğ»Ñ€Ğ´ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²) Ğ½Ğ° Ğ¿Ğ¾Ñ€Ñ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ĞµĞ»ÑÑ… Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¾Ğ±Ğ»Ğ°Ñ‡Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ 'Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ Ğ½Ğ° Ğ²Ğ°Ñ‚Ñ‚' (IPW) Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ ÑĞ½ĞµÑ€Ğ³Ğ¾ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ñ 20+ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ 8 ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¸Ñ‚ÑŒ Ğ½Ğ° 88.7% Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ĞµĞ»ĞµĞ¹ Ğ½Ğ° 1.4x Ğ²Ñ‹ÑˆĞµ Ğ¾Ğ±Ğ»Ğ°Ñ‡Ğ½Ñ‹Ñ…. Ğ—Ğ° Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´ 2023-2025 Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ° IPW ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ»Ğ°ÑÑŒ Ğ² 5.3 Ñ€Ğ°Ğ·Ğ°, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ¿ĞµÑ€ĞµÑ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ñ Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°Ñ‡Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ½Ğ° Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°."
                },
                "en": {
                    "title": "Empowering Local Inference: Reducing Cloud Dependency with Small LMs",
                    "desc": "This paper explores the potential of using small language models (LMs) for local inference on devices like laptops, which can efficiently handle real-world queries. It introduces a new metric called intelligence per watt (IPW) to evaluate the performance and energy efficiency of these models compared to traditional cloud-based systems. The study shows that local LMs can accurately respond to 88.7% of queries and that their efficiency has significantly improved over time. The findings suggest that local inference can effectively reduce reliance on centralized cloud infrastructure, making it a viable alternative for processing language tasks."
                },
                "zh": {
                    "title": "æœ¬åœ°æ¨ç†ï¼šæ™ºèƒ½ä¸æ•ˆç‡çš„æ–°å¹³è¡¡",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†ä½¿ç”¨å°å‹è¯­è¨€æ¨¡å‹åœ¨æœ¬åœ°åŠ é€Ÿå™¨ä¸Šè¿›è¡Œæ¨ç†çš„èƒ½åŠ›ï¼Œèƒ½å¤Ÿå‡†ç¡®å¤„ç†è®¸å¤šç°å®ä¸–ç•Œçš„æŸ¥è¯¢ï¼Œä»è€Œæ˜¾è‘—å‡å°‘å¯¹é›†ä¸­å¼äº‘åŸºç¡€è®¾æ–½çš„éœ€æ±‚ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå°å‹è¯­è¨€æ¨¡å‹åœ¨è®¸å¤šä»»åŠ¡ä¸Šä¸å‰æ²¿æ¨¡å‹çš„æ€§èƒ½ç›¸å½“ï¼Œå¹¶ä¸”æœ¬åœ°åŠ é€Ÿå™¨èƒ½å¤Ÿä»¥äº¤äº’å»¶è¿Ÿè¿è¡Œè¿™äº›æ¨¡å‹ã€‚é€šè¿‡å¼•å…¥æ¯ç“¦ç‰¹æ™ºèƒ½ï¼ˆIPWï¼‰ä½œä¸ºè¯„ä¼°æœ¬åœ°æ¨ç†èƒ½åŠ›å’Œæ•ˆç‡çš„æŒ‡æ ‡ï¼Œç ”ç©¶å‘ç°æœ¬åœ°æ¨¡å‹åœ¨å¤„ç†æŸ¥è¯¢æ—¶çš„å‡†ç¡®ç‡é«˜è¾¾88.7%ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜è¡¨æ˜æœ¬åœ°æ¨ç†å¯ä»¥æœ‰æ•ˆåœ°é‡æ–°åˆ†é…å¯¹é›†ä¸­åŸºç¡€è®¾æ–½çš„éœ€æ±‚ï¼Œæä¾›äº†ä¼˜åŒ–çš„ç©ºé—´ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.08043",
            "title": "DynaAct: Large Language Model Reasoning with Dynamic Action Spaces",
            "url": "https://huggingface.co/papers/2511.08043",
            "abstract": "A new framework, DynaAct, automatically constructs a compact action space using large language models and submodular functions to enhance sequential reasoning in complex problem-solving scenarios, improving performance while maintaining efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t In modern sequential decision-making systems, the construction of an optimal candidate action space is critical to efficient inference. However, existing approaches either rely on manually defined action spaces that lack scalability or utilize unstructured spaces that render exhaustive search computationally prohibitive. In this paper, we propose a novel framework named DynaAct for automatically constructing a compact action space to enhance sequential reasoning in complex problem-solving scenarios. Our method first estimates a proxy for the complete action space by extracting general sketches observed in a corpus covering diverse complex reasoning problems using large language models. We then formulate a submodular function that jointly evaluates candidate actions based on their utility to the current state and their diversity, and employ a greedy algorithm to select an optimal candidate set. Extensive experiments on six diverse standard benchmarks demonstrate that our approach significantly improves overall performance, while maintaining efficient inference without introducing substantial latency. The implementation is available at https://github.com/zhaoxlpku/DynaAct.",
            "score": 5,
            "issue_id": 1,
            "pub_date": "2025-11-11",
            "pub_date_card": {
                "ru": "11 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 11",
                "zh": "11æœˆ11æ—¥"
            },
            "hash": "868a7bfc42d8fb76",
            "authors": [
                "Xueliang Zhao",
                "Wei Wu",
                "Jian Guan",
                "Qintong Li",
                "Lingpeng Kong"
            ],
            "affiliations": [
                "Ant Group",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.08043.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#open_source",
                    "#optimization",
                    "#agents",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº DynaAct Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ¾Ğ±Ñ‰Ğ¸Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸Ğ· ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑƒĞ±Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ€Ğ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ² Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ñ… Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ. Ğ–Ğ°Ğ´Ğ½Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ° ÑˆĞµÑÑ‚Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "DynaAct: Smart Action Space for Efficient Decision-Making",
                    "desc": "The paper introduces DynaAct, a new framework designed to automatically create a compact action space for sequential decision-making. By leveraging large language models, DynaAct estimates a proxy for the complete action space, capturing essential patterns from a wide range of complex reasoning problems. It employs a submodular function to evaluate candidate actions based on their usefulness and diversity, using a greedy algorithm to select the best options. The results show that DynaAct enhances performance in problem-solving tasks while ensuring efficient inference with minimal latency."
                },
                "zh": {
                    "title": "DynaActï¼šé«˜æ•ˆæ„å»ºç´§å‡‘åŠ¨ä½œç©ºé—´çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "DynaActæ˜¯ä¸€ä¸ªæ–°æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œå­æ¨¡å—å‡½æ•°è‡ªåŠ¨æ„å»ºç´§å‡‘çš„åŠ¨ä½œç©ºé—´ï¼Œä»¥å¢å¼ºå¤æ‚é—®é¢˜è§£å†³ä¸­çš„é¡ºåºæ¨ç†èƒ½åŠ›ã€‚è¯¥æ–¹æ³•é¦–å…ˆé€šè¿‡æå–åœ¨å¤šæ ·åŒ–å¤æ‚æ¨ç†é—®é¢˜ä¸­è§‚å¯Ÿåˆ°çš„ä¸€èˆ¬è‰å›¾ï¼Œæ¥ä¼°è®¡å®Œæ•´åŠ¨ä½œç©ºé—´çš„ä»£ç†ã€‚ç„¶åï¼Œåˆ©ç”¨å­æ¨¡å—å‡½æ•°è¯„ä¼°å€™é€‰åŠ¨ä½œçš„æ•ˆç”¨å’Œå¤šæ ·æ€§ï¼Œå¹¶é‡‡ç”¨è´ªå¿ƒç®—æ³•é€‰æ‹©æœ€ä½³å€™é€‰é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDynaActæ˜¾è‘—æé«˜äº†æ•´ä½“æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†é«˜æ•ˆæ¨ç†ï¼Œä¸”æ²¡æœ‰å¼•å…¥æ˜¾è‘—çš„å»¶è¿Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.06428",
            "title": "Walking the Tightrope of LLMs for Software Development: A Practitioners' Perspective",
            "url": "https://huggingface.co/papers/2511.06428",
            "abstract": "LLMs impact software development by offering benefits like maintaining workflow and fostering entrepreneurship, but also pose risks to developers' well-being and reputation, necessitating careful management and adoption strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t Background: Large Language Models emerged with the potential of provoking a revolution in software development (e.g., automating processes, workforce transformation). Although studies have started to investigate the perceived impact of LLMs for software development, there is a need for empirical studies to comprehend how to balance forward and backward effects of using LLMs. Objective: We investigated how LLMs impact software development and how to manage the impact from a software developer's perspective. Method: We conducted 22 interviews with software practitioners across 3 rounds of data collection and analysis, between October (2024) and September (2025). We employed socio-technical grounded theory (STGT) for data analysis to rigorously analyse interview participants' responses. Results: We identified the benefits (e.g., maintain software development flow, improve developers' mental model, and foster entrepreneurship) and disadvantages (e.g., negative impact on developers' personality and damage to developers' reputation) of using LLMs at individual, team, organisation, and society levels; as well as best practices on how to adopt LLMs. Conclusion: Critically, we present the trade-offs that software practitioners, teams, and organisations face in working with LLMs. Our findings are particularly useful for software team leaders and IT managers to assess the viability of LLMs within their specific context.",
            "score": 4,
            "issue_id": 1,
            "pub_date": "2025-11-09",
            "pub_date_card": {
                "ru": "9 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 9",
                "zh": "11æœˆ9æ—¥"
            },
            "hash": "fd5c9d9c51b94cfb",
            "authors": [
                "Samuel Ferino",
                "Rashina Hoda",
                "John Grundy",
                "Christoph Treude"
            ],
            "affiliations": [
                "Faculty of Information Technology, Monash University",
                "School of Computing and Information Systems, Singapore Management University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.06428.jpg",
            "data": {
                "categories": [],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "Ğ‘Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ² Ğ¸ Ñ€Ğ¸ÑĞºĞ¾Ğ²: ĞºĞ°Ğº ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸ĞµĞ¼ LLM Ğ½Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ²",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·ÑƒÑ‡Ğ°ĞµÑ‚ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¸Ğ½Ñ‚ĞµÑ€Ğ²ÑŒÑ Ñ 22 Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ ĞºĞ°Ğº Ğ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ñ‹ (Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‡ĞµĞ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°, ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ², Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°), Ñ‚Ğ°Ğº Ğ¸ Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑÑ‚Ğ²Ğ¸Ñ (Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ½Ğ° Ğ»Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ°, Ñ€ĞµĞ¿ÑƒÑ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ñ€Ğ¸ÑĞºĞ¸). Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¾ÑÑŒ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ¾Ñ†Ğ¸Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼, ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ½Ğ¾Ğ¼, Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ğ¸ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½ÑÑ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸ĞµĞ¼ LLM Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ»Ğ¸Ğ´ĞµÑ€Ğ¾Ğ² ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´ Ğ¸ IT-Ğ¼ĞµĞ½ĞµĞ´Ğ¶ĞµÑ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "Balancing Benefits and Risks of LLMs in Software Development",
                    "desc": "This paper explores the dual impact of Large Language Models (LLMs) on software development, highlighting both their advantages and disadvantages. The study reveals that LLMs can enhance workflow, improve developers' understanding, and encourage entrepreneurship, but they also pose risks to developers' mental health and professional reputation. Through interviews with software practitioners, the research identifies best practices for managing the integration of LLMs in development processes. Ultimately, the findings emphasize the need for careful consideration of the trade-offs involved in adopting LLMs in software teams and organizations."
                },
                "zh": {
                    "title": "å¹³è¡¡å¤§å‹è¯­è¨€æ¨¡å‹çš„åˆ©å¼Šï¼ŒåŠ©åŠ›è½¯ä»¶å¼€å‘",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è½¯ä»¶å¼€å‘ä¸­å¸¦æ¥äº†æ˜¾è‘—çš„å½±å“ï¼Œæ—¢æœ‰åŠ©äºç»´æŠ¤å¼€å‘æµç¨‹å’Œä¿ƒè¿›åˆ›ä¸šï¼Œä¹Ÿå¯èƒ½å¯¹å¼€å‘è€…çš„å¿ƒç†å¥åº·å’Œå£°èª‰é€ æˆé£é™©ã€‚å› æ­¤ï¼Œå¼€å‘è€…éœ€è¦è°¨æ…ç®¡ç†å’Œé‡‡ç”¨è¿™äº›æŠ€æœ¯ã€‚æˆ‘ä»¬é€šè¿‡å¯¹22ä½è½¯ä»¶ä»ä¸šè€…çš„è®¿è°ˆï¼Œåˆ†æäº†LLMsçš„ä¼˜ç¼ºç‚¹ï¼ŒåŒ…æ‹¬å¯¹ä¸ªäººã€å›¢é˜Ÿã€ç»„ç»‡å’Œç¤¾ä¼šå±‚é¢çš„å½±å“ã€‚ç ”ç©¶ç»“æœä¸ºè½¯ä»¶å›¢é˜Ÿé¢†å¯¼å’ŒITç»ç†æä¾›äº†åœ¨ç‰¹å®šç¯å¢ƒä¸­è¯„ä¼°LLMså¯è¡Œæ€§çš„æœ‰ä»·å€¼è§è§£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.05650",
            "title": "Optimizing Diversity and Quality through Base-Aligned Model Collaboration",
            "url": "https://huggingface.co/papers/2511.05650",
            "abstract": "BACo, a token-level collaboration framework for LLMs, enhances output diversity and quality through dynamic routing without degrading performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Alignment has greatly improved large language models (LLMs)' output quality at the cost of diversity, yielding highly similar outputs across generations. We propose Base-Aligned Model Collaboration (BACo), an inference-time token-level model collaboration framework that dynamically combines a base LLM with its aligned counterpart to optimize diversity and quality. Inspired by prior work (Fei et al., 2025), BACo employs routing strategies that determine, at each token, from which model to decode based on next-token prediction uncertainty and predicted contents' semantic role. Prior diversity-promoting methods, such as retraining, prompt engineering, and multi-sampling methods, improve diversity but often degrade quality or require costly decoding or post-training. In contrast, BACo achieves both high diversity and quality post hoc within a single pass, while offering strong controllability. We explore a family of routing strategies, across three open-ended generation tasks and 13 metrics covering diversity and quality, BACo consistently surpasses state-of-the-art inference-time baselines. With our best router, BACo achieves a 21.3% joint improvement in diversity and quality. Human evaluations also mirror these improvements. The results suggest that collaboration between base and aligned models can optimize and control diversity and quality.",
            "score": 4,
            "issue_id": 1,
            "pub_date": "2025-11-07",
            "pub_date_card": {
                "ru": "7 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 7",
                "zh": "11æœˆ7æ—¥"
            },
            "hash": "10bfab1d0dd4dc6b",
            "authors": [
                "Yichen Wang",
                "Chenghao Yang",
                "Tenghao Huang",
                "Muhao Chen",
                "Jonathan May",
                "Mina Lee"
            ],
            "affiliations": [
                "University of California, Davis",
                "University of Chicago",
                "University of Southern California"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.05650.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#alignment",
                    "#architecture",
                    "#training",
                    "#inference"
                ],
                "emoji": "ğŸ¤",
                "ru": {
                    "title": "Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ BACo â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ LLM-Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ ĞµÑ‘ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ²ĞµÑ€ÑĞ¸ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ÑÑ‚, Ğ¸Ğ· ĞºĞ°ĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ñ€Ğ°Ñ‚ÑŒ ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½, Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹Ğ²Ğ°ÑÑÑŒ Ğ½Ğ° Ğ½ĞµÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ€Ğ¾Ğ»Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² (Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, prompt engineering, Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºÑ€Ğ°Ñ‚Ğ½Ğ°Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ°), BACo Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´ Ğ±ĞµĞ· ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ½Ğ° Ñ‚Ñ€Ñ‘Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° 21,3% ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ² Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğ¸ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "BACo: Boosting Diversity and Quality in LLMs Through Dynamic Collaboration",
                    "desc": "BACo is a novel framework designed for large language models (LLMs) that enhances the diversity and quality of generated outputs. It operates at the token level, dynamically routing between a base model and its aligned counterpart based on prediction uncertainty and semantic roles. Unlike traditional methods that often compromise quality for diversity, BACo achieves both simultaneously in a single pass. The framework has shown significant improvements in various generation tasks, outperforming existing methods in both diversity and quality metrics."
                },
                "zh": {
                    "title": "BACoï¼šæå‡å¤šæ ·æ€§ä¸è´¨é‡çš„åä½œæ¡†æ¶",
                    "desc": "BACoæ˜¯ä¸€ç§é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ä»¤ç‰Œçº§åä½œæ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€è·¯ç”±æé«˜è¾“å‡ºçš„å¤šæ ·æ€§å’Œè´¨é‡ï¼Œè€Œä¸é™ä½æ€§èƒ½ã€‚è¯¥æ¡†æ¶åœ¨æ¨ç†æ—¶åŠ¨æ€ç»“åˆåŸºç¡€LLMå’Œå…¶å¯¹é½æ¨¡å‹ï¼Œä»¥ä¼˜åŒ–ç”Ÿæˆå†…å®¹çš„å¤šæ ·æ€§å’Œè´¨é‡ã€‚BACoé‡‡ç”¨è·¯ç”±ç­–ç•¥ï¼Œæ ¹æ®æ¯ä¸ªä»¤ç‰Œçš„é¢„æµ‹ä¸ç¡®å®šæ€§å’Œè¯­ä¹‰è§’è‰²ï¼Œå†³å®šä»å“ªä¸ªæ¨¡å‹è§£ç ã€‚ä¸ä»¥å¾€çš„å¤šæ ·æ€§æå‡æ–¹æ³•ç›¸æ¯”ï¼ŒBACoåœ¨å•æ¬¡æ¨ç†ä¸­å®ç°äº†é«˜å¤šæ ·æ€§å’Œé«˜è´¨é‡çš„å¹³è¡¡ï¼Œå¹¶ä¸”æä¾›äº†å¼ºå¤§çš„å¯æ§æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.08029",
            "title": "BiCA: Effective Biomedical Dense Retrieval with Citation-Aware Hard Negatives",
            "url": "https://huggingface.co/papers/2511.08029",
            "abstract": "BiCA uses citation links to improve biomedical retrieval models by providing effective hard negatives, enhancing zero-shot and long-tailed performance with minimal fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Hard negatives are essential for training effective retrieval models. Hard-negative mining typically relies on ranking documents using cross-encoders or static embedding models based on similarity metrics such as cosine distance. Hard negative mining becomes challenging for biomedical and scientific domains due to the difficulty in distinguishing between source and hard negative documents. However, referenced documents naturally share contextual relevance with the source document but are not duplicates, making them well-suited as hard negatives. In this work, we propose BiCA: Biomedical Dense Retrieval with Citation-Aware Hard Negatives, an approach for hard-negative mining by utilizing citation links in 20,000 PubMed articles for improving a domain-specific small dense retriever. We fine-tune the GTE_small and GTE_Base models using these citation-informed negatives and observe consistent improvements in zero-shot dense retrieval using nDCG@10 for both in-domain and out-of-domain tasks on BEIR and outperform baselines on long-tailed topics in LoTTE using Success@5. Our findings highlight the potential of leveraging document link structure to generate highly informative negatives, enabling state-of-the-art performance with minimal fine-tuning and demonstrating a path towards highly data-efficient domain adaptation.",
            "score": 2,
            "issue_id": 1,
            "pub_date": "2025-11-11",
            "pub_date_card": {
                "ru": "11 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 11",
                "zh": "11æœˆ11æ—¥"
            },
            "hash": "dd8b545b8edb6041",
            "authors": [
                "Aarush Sinha",
                "Pavan Kumar S",
                "Roshan Balaji",
                "Nirav Pravinbhai Bhatt"
            ],
            "affiliations": [
                "BioSystems Engineering and Control (BiSECt) Lab, Department of Biotechnology and Wadhwani School of Data Science and AI, Indian Institute of Technology (IIT) Madras, Tamil Nadu India",
                "The Centre for Integrative Biology and Systems medicinE (IBSE), IIT Madras, Chennai, Tamil Nadu, India",
                "Vellore Institute of Technology (VIT) Chennai, India"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.08029.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#benchmark",
                    "#small_models",
                    "#optimization",
                    "#training",
                    "#dataset",
                    "#science"
                ],
                "emoji": "ğŸ“š",
                "ru": {
                    "title": "Ğ¦Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ğº Ğ·Ğ¾Ğ»Ğ¾Ñ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ±Ğ¾Ñ€Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ¾Ğ² Ğ² Ğ¿Ğ¾Ğ¸ÑĞºĞµ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ BiCA â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ±Ğ¸Ğ¾Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑÑÑ‹Ğ»Ğ¾Ğº Ñ†Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ñ†Ğ¸Ñ‚Ğ°Ñ‚Ñ‹ Ğ¸Ğ· 20,000 ÑÑ‚Ğ°Ñ‚ĞµĞ¹ PubMed ĞºĞ°Ğº Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ½ÑƒĞ»ĞµĞ²Ğ¾Ğ¹ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ñ…Ğ²Ğ¾ÑÑ‚Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ GTE_small Ğ¸ GTE_Base. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ± Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ğ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ ÑĞ²ÑĞ·ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Harnessing Citations for Better Biomedical Retrieval",
                    "desc": "BiCA introduces a novel method for enhancing biomedical retrieval models by utilizing citation links to generate effective hard negatives. This approach addresses the challenges of hard-negative mining in the biomedical field, where distinguishing relevant documents can be difficult. By leveraging citation information from 20,000 PubMed articles, BiCA improves the performance of small dense retrievers with minimal fine-tuning. The results show significant advancements in zero-shot retrieval and long-tailed topic performance, demonstrating the effectiveness of citation-aware hard negatives in domain-specific applications."
                },
                "zh": {
                    "title": "åˆ©ç”¨å¼•ç”¨é“¾æ¥æå‡ç”Ÿç‰©åŒ»å­¦æ£€ç´¢æ€§èƒ½",
                    "desc": "BiCAæ˜¯ä¸€ç§åˆ©ç”¨å¼•ç”¨é“¾æ¥æ¥æ”¹è¿›ç”Ÿç‰©åŒ»å­¦æ£€ç´¢æ¨¡å‹çš„æ–¹æ³•ã€‚å®ƒé€šè¿‡æä¾›æœ‰æ•ˆçš„å›°éš¾è´Ÿæ ·æœ¬ï¼Œå¢å¼ºäº†é›¶æ ·æœ¬å’Œé•¿å°¾æ€§èƒ½ï¼Œä¸”åªéœ€æœ€å°çš„å¾®è°ƒã€‚è¯¥æ–¹æ³•åˆ©ç”¨äº†20,000ç¯‡PubMedæ–‡ç« ä¸­çš„å¼•ç”¨ä¿¡æ¯ï¼Œå¸®åŠ©è®­ç»ƒé¢†åŸŸç‰¹å®šçš„å°å‹å¯†é›†æ£€ç´¢å™¨ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåˆ©ç”¨æ–‡æ¡£é“¾æ¥ç»“æ„ç”Ÿæˆé«˜ä¿¡æ¯é‡çš„è´Ÿæ ·æœ¬ï¼Œå¯ä»¥å®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸”æ•°æ®æ•ˆç‡é«˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.05489",
            "title": "TimeSearch-R: Adaptive Temporal Search for Long-Form Video Understanding via Self-Verification Reinforcement Learning",
            "url": "https://huggingface.co/papers/2511.05489",
            "abstract": "TimeSearch-R uses interleaved text-video thinking with GRPO-CSV to optimize temporal search in videos, improving performance on long-form video understanding benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Temporal search aims to identify a minimal set of relevant frames from tens of thousands based on a given query, serving as a foundation for accurate long-form video understanding. Existing works attempt to progressively narrow the search space. However, these approaches typically rely on a hand-crafted search process, lacking end-to-end optimization for learning optimal search strategies. In this paper, we propose TimeSearch-R, which reformulates temporal search as interleaved text-video thinking, seamlessly integrating searching video clips into the reasoning process through reinforcement learning (RL). However, applying RL training methods, such as Group Relative Policy Optimization (GRPO), to video reasoning can result in unsupervised intermediate search decisions. This leads to insufficient exploration of the video content and inconsistent logical reasoning. To address these issues, we introduce GRPO with Completeness Self-Verification (GRPO-CSV), which gathers searched video frames from the interleaved reasoning process and utilizes the same policy model to verify the adequacy of searched frames, thereby improving the completeness of video reasoning. Additionally, we construct datasets specifically designed for the SFT cold-start and RL training of GRPO-CSV, filtering out samples with weak temporal dependencies to enhance task difficulty and improve temporal search capabilities. Extensive experiments demonstrate that TimeSearch-R achieves significant improvements on temporal search benchmarks such as Haystack-LVBench and Haystack-Ego4D, as well as long-form video understanding benchmarks like VideoMME and MLVU. Notably, TimeSearch-R establishes a new state-of-the-art on LongVideoBench with 4.1% improvement over the base model Qwen2.5-VL and 2.0% over the advanced video reasoning model Video-R1. Our code is available at https://github.com/Time-Search/TimeSearch-R.",
            "score": 2,
            "issue_id": 1,
            "pub_date": "2025-11-07",
            "pub_date_card": {
                "ru": "7 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 7",
                "zh": "11æœˆ7æ—¥"
            },
            "hash": "76aa50eba07d81af",
            "authors": [
                "Junwen Pan",
                "Qizhe Zhang",
                "Rui Zhang",
                "Ming Lu",
                "Xin Wan",
                "Yuan Zhang",
                "Chang Liu",
                "Qi She"
            ],
            "affiliations": [
                "ByteDance",
                "School of Computer Science, Peking University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.05489.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#open_source",
                    "#optimization",
                    "#video",
                    "#long_context",
                    "#multimodal",
                    "#training",
                    "#dataset",
                    "#rl",
                    "#reasoning"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ñ‹Ğ¹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ñ‚ĞµÑ€Ğ»Ğ¸Ğ²Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "TimeSearch-R Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ² Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° ĞºĞ°Ğº Ğ¸Ğ½Ñ‚ĞµÑ€Ğ»Ğ¸Ğ²Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (GRPO) Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° end-to-end, Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¿Ğ¾Ğ¸ÑĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ GRPO-CSV â€” Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ GRPO Ñ ÑĞ°Ğ¼Ğ¾Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ ÑĞ¾Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ½Ñ‹Ğµ ĞºĞ°Ğ´Ñ€Ñ‹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ Ğ¸Ñ… Ğ°Ğ´ĞµĞºĞ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ğ² Ğ½Ğ¾Ğ²Ñ‹Ğ¹ state-of-the-art Ğ½Ğ° LongVideoBench."
                },
                "en": {
                    "title": "Optimizing Video Search with Interleaved Text-Video Reasoning",
                    "desc": "TimeSearch-R is a novel approach that enhances temporal search in videos by integrating text and video reasoning through reinforcement learning. It addresses the limitations of traditional methods that rely on hand-crafted search processes by employing Group Relative Policy Optimization with Completeness Self-Verification (GRPO-CSV). This method ensures that the search for relevant video frames is optimized and verified, leading to better logical reasoning and exploration of video content. The results show significant improvements in various benchmarks, establishing new state-of-the-art performance in long-form video understanding tasks."
                },
                "zh": {
                    "title": "ä¼˜åŒ–è§†é¢‘ç†è§£çš„æ—¶é—´æœç´¢æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºTimeSearch-Rçš„æ–¹æ³•ï¼Œæ—¨åœ¨ä¼˜åŒ–è§†é¢‘ä¸­çš„æ—¶é—´æœç´¢ã€‚é€šè¿‡å°†æ–‡æœ¬å’Œè§†é¢‘çš„æ€ç»´äº¤é”™ç»“åˆï¼ŒTimeSearch-Råˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥æå‡è§†é¢‘ç†è§£çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†å¸¦æœ‰å®Œæ•´æ€§è‡ªæˆ‘éªŒè¯çš„ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPO-CSVï¼‰ï¼Œä»¥ç¡®ä¿æœç´¢åˆ°çš„è§†é¢‘å¸§çš„å……åˆ†æ€§ï¼Œä»è€Œæ”¹å–„è§†é¢‘æ¨ç†çš„å®Œæ•´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTimeSearch-Råœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†æ—¶é—´æœç´¢å’Œé•¿è§†é¢‘ç†è§£çš„æ€§èƒ½ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-11-11.html",
    "link_next": "2025-11-13.html",
    "link_month": "2025-11.html",
    "short_date_prev": {
        "ru": "11.11",
        "en": "11/11",
        "zh": "11æœˆ11æ—¥"
    },
    "short_date_next": {
        "ru": "13.11",
        "en": "11/13",
        "zh": "11æœˆ13æ—¥"
    },
    "categories": {
        "#dataset": 6,
        "#data": 1,
        "#benchmark": 7,
        "#agents": 3,
        "#cv": 0,
        "#rl": 3,
        "#rlhf": 1,
        "#rag": 2,
        "#plp": 0,
        "#inference": 3,
        "#3d": 0,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 4,
        "#math": 1,
        "#multilingual": 2,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 8,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 2,
        "#reasoning": 6,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 8,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 2,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 7,
        "#small_models": 4,
        "#science": 1,
        "#low_resource": 2,
        "#translation": 1
    }
}