{
    "date": {
        "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
        "en": "February 11",
        "zh": "2æœˆ11æ—¥"
    },
    "time_utc": "2026-02-11 18:54",
    "weekday": 2,
    "issue_id": 1010,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2602.05400",
            "title": "OPUS: Towards Efficient and Principled Data Selection in Large Language Model Pre-training in Every Iteration",
            "url": "https://huggingface.co/papers/2602.05400",
            "abstract": "OPUS is a dynamic data selection framework that improves pre-training efficiency by scoring data candidates based on optimizer-induced update projections in a stable proxy-derived target space, achieving superior performance with reduced computational overhead.  \t\t\t\t\tAI-generated summary \t\t\t\t As high-quality public text approaches exhaustion, a phenomenon known as the Data Wall, pre-training is shifting from more tokens to better tokens. However, existing methods either rely on heuristic static filters that ignore training dynamics, or use dynamic yet optimizer-agnostic criteria based on raw gradients. We propose OPUS (Optimizer-induced Projected Utility Selection), a dynamic data selection framework that defines utility in the optimizer-induced update space. OPUS scores candidates by projecting their effective updates, shaped by modern optimizers, onto a target direction derived from a stable, in-distribution proxy. To ensure scalability, we employ Ghost technique with CountSketch for computational efficiency, and Boltzmann sampling for data diversity, incurring only 4.7\\% additional compute overhead. OPUS achieves remarkable results across diverse corpora, quality tiers, optimizers, and model scales. In pre-training of GPT-2 Large/XL on FineWeb and FineWeb-Edu with 30B tokens, OPUS outperforms industrial-level baselines and even full 200B-token training. Moreover, when combined with industrial-level static filters, OPUS further improves pre-training efficiency, even with lower-quality data. Furthermore, in continued pre-training of Qwen3-8B-Base on SciencePedia, OPUS achieves superior performance using only 0.5B tokens compared to full training with 3B tokens, demonstrating significant data efficiency gains in specialized domains.",
            "score": 274,
            "issue_id": 999,
            "pub_date": "2026-02-05",
            "pub_date_card": {
                "ru": "5 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 5",
                "zh": "2æœˆ5æ—¥"
            },
            "hash": "02cf6344cccb5e9f",
            "authors": [
                "Shaobo Wang",
                "Xuan Ouyang",
                "Tianyi Xu",
                "Yuzheng Hu",
                "Jialin Liu",
                "Guo Chen",
                "Tianyu Zhang",
                "Junhao Zheng",
                "Kexin Yang",
                "Xingzhang Ren",
                "Dayiheng Liu",
                "Linfeng Zhang"
            ],
            "affiliations": [
                "EPIC Lab, SJTU",
                "Mila - Quebec AI Institute",
                "Qwen Team, Alibaba Group",
                "UIUC",
                "UWMadison"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.05400.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#training"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ",
                    "desc": "OPUS â€” ÑÑ‚Ğ¾ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ğ¹ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ, Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼Ğ¾Ğ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾ĞºÑĞ¸ Ğ¸Ğ· Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ghost Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Ñ CountSketch Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ, Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑÑ Ğ²ÑĞµĞ³Ğ¾ 4,7% Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹. OPUS Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ°Ñ… Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¾Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ñ…."
                },
                "en": {
                    "title": "Optimize Data Selection for Efficient Pre-Training",
                    "desc": "OPUS is a novel framework designed to enhance the efficiency of pre-training machine learning models by selecting the most effective data. It scores data candidates based on how well they align with the updates produced by modern optimizers, focusing on a stable target space. This approach allows OPUS to outperform traditional methods that either use static filters or ignore the dynamics of training. By combining advanced techniques for computational efficiency and data diversity, OPUS achieves significant performance improvements while minimizing additional computational costs."
                },
                "zh": {
                    "title": "OPUSï¼šæå‡é¢„è®­ç»ƒæ•ˆç‡çš„åŠ¨æ€æ•°æ®é€‰æ‹©æ¡†æ¶",
                    "desc": "OPUSæ˜¯ä¸€ç§åŠ¨æ€æ•°æ®é€‰æ‹©æ¡†æ¶ï¼Œé€šè¿‡åœ¨ç¨³å®šçš„ä»£ç†ç›®æ ‡ç©ºé—´ä¸­åŸºäºä¼˜åŒ–å™¨å¼•èµ·çš„æ›´æ–°æŠ•å½±æ¥è¯„åˆ†æ•°æ®å€™é€‰ï¼Œä»è€Œæé«˜é¢„è®­ç»ƒæ•ˆç‡ã€‚éšç€é«˜è´¨é‡å…¬å…±æ–‡æœ¬çš„é€æ¸æ¯ç«­ï¼Œé¢„è®­ç»ƒçš„é‡ç‚¹ä»æ›´å¤šçš„æ ‡è®°è½¬å‘æ›´å¥½çš„æ ‡è®°ã€‚OPUSé€šè¿‡å°†æœ‰æ•ˆæ›´æ–°æŠ•å½±åˆ°ç›®æ ‡æ–¹å‘æ¥å®šä¹‰æ•ˆç”¨ï¼Œç¡®ä¿äº†è®¡ç®—æ•ˆç‡å’Œæ•°æ®å¤šæ ·æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOPUSåœ¨å¤šç§è¯­æ–™åº“å’Œæ¨¡å‹è§„æ¨¡ä¸Šå‡è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—æé«˜äº†é¢„è®­ç»ƒçš„æ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.09856",
            "title": "Code2World: A GUI World Model via Renderable Code Generation",
            "url": "https://huggingface.co/papers/2602.09856",
            "abstract": "Code2World enables autonomous GUI agents to predict next visual states through renderable code generation, achieving high visual fidelity and structural controllability while improving navigation performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Autonomous GUI agents interact with environments by perceiving interfaces and executing actions. As a virtual sandbox, the GUI World model empowers agents with human-like foresight by enabling action-conditioned prediction. However, existing text- and pixel-based approaches struggle to simultaneously achieve high visual fidelity and fine-grained structural controllability. To this end, we propose Code2World, a vision-language coder that simulates the next visual state via renderable code generation. Specifically, to address the data scarcity problem, we construct AndroidCode by translating GUI trajectories into high-fidelity HTML and refining synthesized code through a visual-feedback revision mechanism, yielding a corpus of over 80K high-quality screen-action pairs. To adapt existing VLMs into code prediction, we first perform SFT as a cold start for format layout following, then further apply Render-Aware Reinforcement Learning which uses rendered outcome as the reward signal by enforcing visual semantic fidelity and action consistency. Extensive experiments demonstrate that Code2World-8B achieves the top-performing next UI prediction, rivaling the competitive GPT-5 and Gemini-3-Pro-Image. Notably, Code2World significantly enhances downstream navigation success rates in a flexible manner, boosting Gemini-2.5-Flash by +9.5% on AndroidWorld navigation. The code is available at https://github.com/AMAP-ML/Code2World.",
            "score": 166,
            "issue_id": 998,
            "pub_date": "2026-02-10",
            "pub_date_card": {
                "ru": "10 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 10",
                "zh": "2æœˆ10æ—¥"
            },
            "hash": "8f82b21c9235c8f8",
            "authors": [
                "Yuhao Zheng",
                "Li'an Zhong",
                "Yi Wang",
                "Rui Dai",
                "Kaikui Liu",
                "Xiangxiang Chu",
                "Linyuan Lv",
                "Philip Torr",
                "Kevin Qinghong Lin"
            ],
            "affiliations": [
                "AMAP, Alibaba Group",
                "Sun Yat-sen University",
                "University of Oxford",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.09856.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#dataset",
                    "#rlhf",
                    "#multimodal",
                    "#training"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ´Ğ°",
                    "desc": "Code2World â€” ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ vision-language, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ GUI Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ AndroidCode Ñ 80K+ Ğ¿Ğ°Ñ€ ÑĞºÑ€Ğ°Ğ½-Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, Ğ¿ĞµÑ€ĞµĞ²ĞµĞ´Ñ GUI Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ² HTML Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ² Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ supervised fine-tuning Ğ´Ğ»Ñ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ° ĞºĞ¾Ğ´Ğ°, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ÑÑ Ñ‡ĞµÑ€ĞµĞ· Render-Aware Reinforcement Learning, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ ĞºĞ°Ğº ÑĞ¸Ğ³Ğ½Ğ°Ğ» Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹. Code2World-8B Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¸ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ UI Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° 9.5%."
                },
                "en": {
                    "title": "Code2World: Predicting UI States with Renderable Code Generation",
                    "desc": "Code2World is a machine learning framework that enables autonomous GUI agents to predict future visual states by generating renderable code. It addresses the challenges of achieving both high visual fidelity and structural controllability, which are often difficult for traditional text- and pixel-based methods. By creating a dataset called AndroidCode, which consists of high-quality screen-action pairs, the model improves its predictions through a visual-feedback revision mechanism. The results show that Code2World outperforms existing models like GPT-5 and Gemini-3-Pro-Image in UI prediction and enhances navigation success rates significantly."
                },
                "zh": {
                    "title": "Code2Worldï¼šæå‡GUIä»£ç†çš„è§†è§‰é¢„æµ‹èƒ½åŠ›",
                    "desc": "Code2World æ˜¯ä¸€ç§è‡ªä¸»å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†ï¼Œèƒ½å¤Ÿé€šè¿‡å¯æ¸²æŸ“ä»£ç ç”Ÿæˆæ¥é¢„æµ‹ä¸‹ä¸€ä¸ªè§†è§‰çŠ¶æ€ï¼Œä»è€Œå®ç°é«˜è§†è§‰ä¿çœŸåº¦å’Œç»“æ„å¯æ§æ€§ï¼ŒåŒæ—¶æé«˜å¯¼èˆªæ€§èƒ½ã€‚è¯¥æ–¹æ³•é€šè¿‡æ„å»º AndroidCodeï¼Œå°† GUI è½¨è¿¹è½¬æ¢ä¸ºé«˜ä¿çœŸçš„ HTMLï¼Œå¹¶é€šè¿‡è§†è§‰åé¦ˆä¿®è®¢æœºåˆ¶ä¼˜åŒ–åˆæˆä»£ç ï¼Œç”Ÿæˆè¶…è¿‡ 8 ä¸‡å¯¹é«˜è´¨é‡çš„å±å¹•-åŠ¨ä½œå¯¹ã€‚ä¸ºäº†å°†ç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰é€‚åº”äºä»£ç é¢„æµ‹ï¼Œç ”ç©¶è€…ä»¬é‡‡ç”¨äº†å†·å¯åŠ¨çš„æ ¼å¼å¸ƒå±€è·Ÿéšå’ŒåŸºäºæ¸²æŸ“ç»“æœçš„å¼ºåŒ–å­¦ä¹ ï¼Œç¡®ä¿è§†è§‰è¯­ä¹‰çš„ä¿çœŸåº¦å’ŒåŠ¨ä½œçš„ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCode2World-8B åœ¨ä¸‹ä¸€ä¸ªç”¨æˆ·ç•Œé¢é¢„æµ‹ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—æå‡äº†ä¸‹æ¸¸å¯¼èˆªçš„æˆåŠŸç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.09082",
            "title": "UI-Venus-1.5 Technical Report",
            "url": "https://huggingface.co/papers/2602.09082",
            "abstract": "UI-Venus-1.5 is a unified GUI agent with improved performance through mid-training stages, online reinforcement learning, and model merging techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t GUI agents have emerged as a powerful paradigm for automating interactions in digital environments, yet achieving both broad generality and consistently strong task performance remains challenging.In this report, we present UI-Venus-1.5, a unified, end-to-end GUI Agent designed for robust real-world applications.The proposed model family comprises two dense variants (2B and 8B) and one mixture-of-experts variant (30B-A3B) to meet various downstream application scenarios.Compared to our previous version, UI-Venus-1.5 introduces three key technical advances: (1) a comprehensive Mid-Training stage leveraging 10 billion tokens across 30+ datasets to establish foundational GUI semantics; (2) Online Reinforcement Learning with full-trajectory rollouts, aligning training objectives with long-horizon, dynamic navigation in large-scale environments; and (3) a single unified GUI Agent constructed via Model Merging, which synthesizes domain-specific models (grounding, web, and mobile) into one cohesive checkpoint. Extensive evaluations demonstrate that UI-Venus-1.5 establishes new state-of-the-art performance on benchmarks such as ScreenSpot-Pro (69.6%), VenusBench-GD (75.0%), and AndroidWorld (77.6%), significantly outperforming previous strong baselines. In addition, UI-Venus-1.5 demonstrates robust navigation capabilities across a variety of Chinese mobile apps, effectively executing user instructions in real-world scenarios. Code: https://github.com/inclusionAI/UI-Venus; Model: https://huggingface.co/collections/inclusionAI/ui-venus",
            "score": 143,
            "issue_id": 998,
            "pub_date": "2026-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "10aa747a4555ce0a",
            "authors": [
                "Veuns-Team",
                ":",
                "Changlong Gao",
                "Zhangxuan Gu",
                "Yulin Liu",
                "Xinyu Qiu",
                "Shuheng Shen",
                "Yue Wen",
                "Tianyu Xia",
                "Zhenyu Xu",
                "Zhengwen Zeng",
                "Beitong Zhou",
                "Xingran Zhou",
                "Weizhi Chen",
                "Sunhao Dai",
                "Jingya Dou",
                "Yichen Gong",
                "Yuan Guo",
                "Zhenlin Guo",
                "Feng Li",
                "Qian Li",
                "Jinzhen Lin",
                "Yuqi Zhou",
                "Linchao Zhu",
                "Liang Chen",
                "Zhenyu Guo",
                "Changhua Meng",
                "Weiqiang Wang"
            ],
            "affiliations": [
                "Ant Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.09082.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#agents",
                    "#benchmark",
                    "#multimodal",
                    "#training"
                ],
                "emoji": "ğŸ–±ï¸",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ° Ñ‡ĞµÑ€ĞµĞ· ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "UI-Venus-1.5 â€” ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¾ÑˆĞ»Ğ° Ñ‚Ñ€Ñ‘Ñ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° 10 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸Ğ· 30+ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾ÑĞ²Ğ¾ĞµĞ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸ GUI, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ°Ñ… Ğ¸, Ğ½Ğ°ĞºĞ¾Ğ½ĞµÑ†, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ÑÑ‚Ğ¸Ğ¼ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ°Ğ¼ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ 69.6% Ğ½Ğ° ScreenSpot-Pro Ğ¸ 77.6% Ğ½Ğ° AndroidWorld. UI-Venus-1.5 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ, ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ½Ğ° ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ… Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Revolutionizing GUI Interaction with UI-Venus-1.5",
                    "desc": "UI-Venus-1.5 is an advanced GUI agent that enhances performance through innovative techniques like mid-training, online reinforcement learning, and model merging. It utilizes a comprehensive training phase with billions of tokens from diverse datasets to build a strong understanding of GUI semantics. The model features different variants to cater to various applications, ensuring flexibility and robustness in real-world tasks. Extensive testing shows that UI-Venus-1.5 achieves state-of-the-art results on multiple benchmarks, demonstrating its superior navigation and task execution capabilities in complex environments."
                },
                "zh": {
                    "title": "ç»Ÿä¸€çš„GUIä»£ç†ï¼Œæ€§èƒ½å†åˆ›æ–°é«˜",
                    "desc": "UI-Venus-1.5 æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†ï¼Œé‡‡ç”¨äº†ä¸­æœŸè®­ç»ƒã€åœ¨çº¿å¼ºåŒ–å­¦ä¹ å’Œæ¨¡å‹åˆå¹¶ç­‰æŠ€æœ¯ï¼Œæ˜¾è‘—æå‡äº†æ€§èƒ½ã€‚è¯¥æ¨¡å‹åŒ…æ‹¬ä¸¤ç§å¯†é›†å˜ä½“ï¼ˆ2B å’Œ 8Bï¼‰ä»¥åŠä¸€ç§ä¸“å®¶æ··åˆå˜ä½“ï¼ˆ30B-A3Bï¼‰ï¼Œä»¥é€‚åº”ä¸åŒçš„åº”ç”¨åœºæ™¯ã€‚é€šè¿‡åˆ©ç”¨è¶…è¿‡ 100 äº¿ä¸ªæ ‡è®°å’Œ 30 å¤šä¸ªæ•°æ®é›†çš„ä¸­æœŸè®­ç»ƒé˜¶æ®µï¼ŒUI-Venus-1.5 å»ºç«‹äº†åŸºç¡€çš„ GUI è¯­ä¹‰ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›æ€§èƒ½ï¼Œå°¤å…¶åœ¨ä¸­å›½ç§»åŠ¨åº”ç”¨ä¸­å±•ç°äº†å¼ºå¤§çš„å¯¼èˆªèƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.10063",
            "title": "Chain of Mindset: Reasoning with Adaptive Cognitive Modes",
            "url": "https://huggingface.co/papers/2602.10063",
            "abstract": "A novel training-free framework called Chain of Mindset enables step-level adaptive mindset orchestration for large language models by integrating spatial, convergent, divergent, and algorithmic reasoning approaches.  \t\t\t\t\tAI-generated summary \t\t\t\t Human problem-solving is never the repetition of a single mindset, by which we mean a distinct mode of cognitive processing. When tackling a specific task, we do not rely on a single mindset; instead, we integrate multiple mindsets within the single solution process. However, existing LLM reasoning methods fall into a common trap: they apply the same fixed mindset across all steps, overlooking that different stages of solving the same problem require fundamentally different mindsets. This single-minded assumption prevents models from reaching the next level of intelligence. To address this limitation, we propose Chain of Mindset (CoM), a training-free agentic framework that enables step-level adaptive mindset orchestration. CoM decomposes reasoning into four functionally heterogeneous mindsets: Spatial, Convergent, Divergent, and Algorithmic. A Meta-Agent dynamically selects the optimal mindset based on the evolving reasoning state, while a bidirectional Context Gate filters cross-module information flow to maintain effectiveness and efficiency. Experiments across six challenging benchmarks spanning mathematics, code generation, scientific QA, and spatial reasoning demonstrate that CoM achieves state-of-the-art performance, outperforming the strongest baseline by 4.96\\% and 4.72\\% in overall accuracy on Qwen3-VL-32B-Instruct and Gemini-2.0-Flash, while balancing reasoning efficiency. Our code is publicly available at https://github.com/QuantaAlpha/chain-of-mindset{https://github.com/QuantaAlpha/chain-of-mindset}.",
            "score": 62,
            "issue_id": 998,
            "pub_date": "2026-02-10",
            "pub_date_card": {
                "ru": "10 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 10",
                "zh": "2æœˆ10æ—¥"
            },
            "hash": "4cffca8a7d14d351",
            "authors": [
                "Tianyi Jiang",
                "Arctanx An",
                "Hengyi Feng",
                "Naixin Zhai",
                "Haodong Li",
                "Xiaomin Yu",
                "Jiahui Liu",
                "Hanwen Du",
                "Shuo Zhang",
                "Zhi Yang",
                "Jie Huang",
                "Yuhua Li",
                "Yongxin Ni",
                "Huacan Wang",
                "Ronghao Chen"
            ],
            "affiliations": [
                "BJTU",
                "NUS",
                "PKU",
                "QuantaAlpha",
                "SUFE",
                "StepFun"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.10063.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#agents",
                    "#architecture"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ¾Ğ² Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑƒĞ¼Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² LLM",
                    "desc": "ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºĞ° Chain of Mindset, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑÑ‚Ğ¸Ğ»ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° ĞºĞ¾ Ğ²ÑĞµĞ¼Ñƒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑƒ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‡ĞµÑ‚Ñ‹Ñ€ÑŒĞ¼Ñ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸: Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼, ĞºĞ¾Ğ½Ğ²ĞµÑ€Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼, Ğ´Ğ¸Ğ²ĞµÑ€Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼ Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ¡Ğ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Meta-Agent ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğ¼ ÑÑ‚Ğ¸Ğ»Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ñ‚ĞµĞºÑƒÑ‰ĞµĞ³Ğ¾ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, Ğ° Ğ´Ğ²ÑƒÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½Ğ¸Ğ¹ Context Gate ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ‚Ğ¾Ğº Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑĞ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´Ğ¸ĞºĞ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 4,72-4,96% Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ğ° Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Adaptive Mindset Orchestration for Enhanced AI Reasoning",
                    "desc": "The paper introduces a new framework called Chain of Mindset (CoM) that enhances the reasoning capabilities of large language models (LLMs) by allowing them to adapt their cognitive processing styles at each step of problem-solving. Unlike traditional methods that use a single fixed mindset, CoM incorporates four distinct reasoning approaches: Spatial, Convergent, Divergent, and Algorithmic. A Meta-Agent within the framework intelligently selects the most suitable mindset based on the current reasoning context, while a Context Gate ensures efficient information flow between different reasoning modules. Experimental results show that CoM significantly improves performance on various benchmarks, achieving state-of-the-art accuracy and demonstrating the importance of adaptive reasoning in AI."
                },
                "zh": {
                    "title": "æ€ç»´é“¾ï¼šæ™ºèƒ½æ¨ç†çš„æ–°æ–¹å¼",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ— è®­ç»ƒæ¡†æ¶ï¼Œç§°ä¸ºæ€ç»´é“¾ï¼ˆChain of Mindsetï¼‰ï¼Œæ—¨åœ¨ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹æä¾›é€æ­¥é€‚åº”çš„æ€ç»´æ¨¡å¼åè°ƒã€‚è¯¥æ¡†æ¶å°†æ¨ç†è¿‡ç¨‹åˆ†è§£ä¸ºå››ç§åŠŸèƒ½å¼‚è´¨çš„æ€ç»´æ¨¡å¼ï¼šç©ºé—´ã€æ”¶æ•›ã€å‘æ•£å’Œç®—æ³•ã€‚é€šè¿‡åŠ¨æ€é€‰æ‹©æœ€ä½³æ€ç»´æ¨¡å¼ï¼Œæ€ç»´é“¾èƒ½å¤Ÿæ ¹æ®æ¨ç†çŠ¶æ€çš„å˜åŒ–è¿›è¡Œè°ƒæ•´ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ™ºèƒ½æ°´å¹³ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ€ç»´é“¾åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ•´ä½“å‡†ç¡®ç‡è¶…è¿‡äº†æœ€å¼ºåŸºçº¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.08234",
            "title": "SkillRL: Evolving Agents via Recursive Skill-Augmented Reinforcement Learning",
            "url": "https://huggingface.co/papers/2602.08234",
            "abstract": "SkillRL enables LLM agents to improve through hierarchical skill discovery and recursive policy evolution, achieving superior performance on complex tasks while reducing computational overhead.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Model (LLM) agents have shown stunning results in complex tasks, yet they often operate in isolation, failing to learn from past experiences. Existing memory-based methods primarily store raw trajectories, which are often redundant and noise-heavy. This prevents agents from extracting high-level, reusable behavioral patterns that are essential for generalization. In this paper, we propose SkillRL, a framework that bridges the gap between raw experience and policy improvement through automatic skill discovery and recursive evolution. Our approach introduces an experience-based distillation mechanism to build a hierarchical skill library SkillBank, an adaptive retrieval strategy for general and task-specific heuristics, and a recursive evolution mechanism that allows the skill library to co-evolve with the agent's policy during reinforcement learning. These innovations significantly reduce the token footprint while enhancing reasoning utility. Experimental results on ALFWorld, WebShop and seven search-augmented tasks demonstrate that SkillRL achieves state-of-the-art performance, outperforming strong baselines over 15.3% and maintaining robustness as task complexity increases. Code is available at this https://github.com/aiming-lab/SkillRL.",
            "score": 55,
            "issue_id": 998,
            "pub_date": "2026-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "99b55b5d68cbbed5",
            "authors": [
                "Peng Xia",
                "Jianwen Chen",
                "Hanyang Wang",
                "Jiaqi Liu",
                "Kaide Zeng",
                "Yu Wang",
                "Siwei Han",
                "Yiyang Zhou",
                "Xujiang Zhao",
                "Haifeng Chen",
                "Zeyu Zheng",
                "Cihang Xie",
                "Huaxiu Yao"
            ],
            "affiliations": [
                "NEC Labs America",
                "UNC-Chapel Hill",
                "University of California Berkeley",
                "University of California San Diego",
                "University of California Santa Cruz",
                "University of Chicago"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.08234.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#open_source",
                    "#agents",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞÑ‚ Ğ¾Ğ¿Ñ‹Ñ‚Ğ° Ğº Ğ½Ğ°Ğ²Ñ‹ĞºĞ°Ğ¼: Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸ĞµĞ¹",
                    "desc": "SkillRL â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒ ÑĞ²Ğ¾Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ². Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑÑ‹Ñ€Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¾Ğ¿Ñ‹Ñ‚Ğ°, ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºÑƒ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² (SkillBank) Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¾Ğ¿Ñ‹Ñ‚Ğ°. ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ğº Ğ¾Ğ±Ñ‰Ğ¸Ğµ, Ñ‚Ğ°Ğº Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸. Ğ ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğµ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ¸ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "Empowering LLMs with Skill Discovery for Enhanced Learning",
                    "desc": "SkillRL is a novel framework designed for Large Language Model (LLM) agents to enhance their learning through hierarchical skill discovery and recursive policy evolution. It addresses the limitations of traditional memory-based methods by introducing an experience-based distillation mechanism that creates a SkillBank, a library of reusable skills. This framework allows agents to adaptively retrieve both general and task-specific heuristics, improving their ability to generalize across complex tasks. Experimental results show that SkillRL significantly outperforms existing methods, achieving over 15.3% better performance while reducing computational costs."
                },
                "zh": {
                    "title": "SkillRLï¼šæå‡æ™ºèƒ½ä½“æ€§èƒ½çš„æŠ€èƒ½å‘ç°ä¸æ¼”åŒ–",
                    "desc": "SkillRL æ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡è‡ªåŠ¨æŠ€èƒ½å‘ç°å’Œé€’å½’ç­–ç•¥æ¼”åŒ–æ¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†çš„æ€§èƒ½ã€‚å®ƒé€šè¿‡ç»éªŒè’¸é¦æœºåˆ¶æ„å»ºä¸€ä¸ªå±‚æ¬¡åŒ–çš„æŠ€èƒ½åº“ï¼ˆSkillBankï¼‰ï¼Œå¹¶é‡‡ç”¨è‡ªé€‚åº”æ£€ç´¢ç­–ç•¥æ¥è·å–é€šç”¨å’Œç‰¹å®šä»»åŠ¡çš„å¯å‘å¼æ–¹æ³•ã€‚è¯¥æ¡†æ¶çš„é€’å½’æ¼”åŒ–æœºåˆ¶ä½¿å¾—æŠ€èƒ½åº“èƒ½å¤Ÿä¸ä»£ç†çš„ç­–ç•¥å…±åŒæ¼”åŒ–ï¼Œä»è€Œæé«˜å­¦ä¹ æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSkillRL åœ¨å¤šä¸ªå¤æ‚ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†å¼ºåŸºçº¿è¶…è¿‡ 15.3%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.09443",
            "title": "P1-VL: Bridging Visual Perception and Scientific Reasoning in Physics Olympiads",
            "url": "https://huggingface.co/papers/2602.09443",
            "abstract": "Physics-oriented vision-language models leverage curriculum reinforcement learning and agentic augmentation to achieve state-of-the-art scientific reasoning performance while maintaining physical consistency through multimodal perception.  \t\t\t\t\tAI-generated summary \t\t\t\t The transition from symbolic manipulation to science-grade reasoning represents a pivotal frontier for Large Language Models (LLMs), with physics serving as the critical test anchor for binding abstract logic to physical reality. Physics demands that a model maintain physical consistency with the laws governing the universe, a task that fundamentally requires multimodal perception to ground abstract logic in reality. At the Olympiad level, diagrams are often constitutive rather than illustrative, containing essential constraints, such as boundary conditions and spatial symmetries, that are absent from the text. To bridge this visual-logical gap, we introduce P1-VL, a family of open-source vision-language models engineered for advanced scientific reasoning. Our method harmonizes Curriculum Reinforcement Learning, which employs progressive difficulty expansion to stabilize post-training, with Agentic Augmentation, enabling iterative self-verification at inference. Evaluated on HiPhO, a rigorous benchmark of 13 exams from 2024-2025, our flagship P1-VL-235B-A22B becomes the first open-source Vision-Language Model (VLM) to secure 12 gold medals and achieves the state-of-the-art performance in the open-source models. Our agent-augmented system achieves the No.2 overall rank globally, trailing only Gemini-3-Pro. Beyond physics, P1-VL demonstrates remarkable scientific reasoning capacity and generalizability, establishing significant leads over base models in STEM benchmarks. By open-sourcing P1-VL, we provide a foundational step toward general-purpose physical intelligence to better align visual perceptions with abstract physical laws for machine scientific discovery.",
            "score": 51,
            "issue_id": 1000,
            "pub_date": "2026-02-10",
            "pub_date_card": {
                "ru": "10 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 10",
                "zh": "2æœˆ10æ—¥"
            },
            "hash": "5200fdac8866bf48",
            "authors": [
                "Yun Luo",
                "Futing Wang",
                "Qianjia Cheng",
                "Fangchen Yu",
                "Haodi Lei",
                "Jianhao Yan",
                "Chenxi Li",
                "Jiacheng Chen",
                "Yufeng Zhao",
                "Haiyuan Wan",
                "Yuchen Zhang",
                "Shenghe Zheng",
                "Junchi Yao",
                "Qingyang Zhang",
                "Haonan He",
                "Wenxuan Zeng",
                "Li Sheng",
                "Chengxing Xie",
                "Yuxin Zuo",
                "Yizhuo Li",
                "Yulun Wu",
                "Rui Huang",
                "Dongzhan Zhou",
                "Kai Chen",
                "Yu Qiao",
                "Lei Bai",
                "Yu Cheng",
                "Ning Ding",
                "Bowen Zhou",
                "Peng Ye",
                "Ganqu Cui"
            ],
            "affiliations": [
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.09443.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#rl",
                    "#agents",
                    "#multimodal",
                    "#training",
                    "#open_source",
                    "#cv",
                    "#science",
                    "#optimization"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "Ğ¤Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°ĞºĞ¾Ğ½Ñ‹ Ñ‡ĞµÑ€ĞµĞ· Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ: VLM Ğ´Ğ»Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞµĞ¼ÑŒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ P1-VL, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‚ Ğ·Ñ€ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ·Ñ‹Ğº Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾ ÑƒÑ‡ĞµĞ±Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğµ, Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰ĞµĞµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑÑ‚ÑŒ ÑĞ²Ğ¾Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒÑ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²ÑƒÑ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ²Ğ°Ğ¶Ğ½ÑƒÑ Ğ¿Ñ€Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¾Ğ»Ğ¸Ğ¼Ğ¿Ğ¸Ğ°Ğ´, Ğ³Ğ´Ğµ Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ P1-VL-235B-A22B ÑÑ‚Ğ°Ğ»Ğ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¹ VLM, Ğ·Ğ°Ğ²Ğ¾ĞµĞ²Ğ°Ğ²ÑˆĞµĞ¹ 12 Ğ·Ğ¾Ğ»Ğ¾Ñ‚Ñ‹Ñ… Ğ¼ĞµĞ´Ğ°Ğ»ĞµĞ¹ Ğ½Ğ° Ğ¼ĞµĞ¶Ğ´ÑƒĞ½Ğ°Ñ€Ğ¾Ğ´Ğ½Ñ‹Ñ… Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ»Ğ¸Ğ¼Ğ¿Ğ¸Ğ°Ğ´Ğ°Ñ… Ğ¸ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ²ÑˆĞµĞ¹ Ğ²Ñ‹Ğ´Ğ°ÑÑ‰Ğ¸ĞµÑÑ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… STEM-Ñ‚ĞµÑÑ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "P1-VL: Bridging Vision and Physics for Superior Scientific Reasoning",
                    "desc": "This paper presents P1-VL, a series of vision-language models designed to enhance scientific reasoning in physics by integrating multimodal perception. The models utilize Curriculum Reinforcement Learning to progressively increase task difficulty, ensuring stability after training, and Agentic Augmentation for self-verification during inference. P1-VL has achieved outstanding results on the HiPhO benchmark, winning 12 gold medals and ranking second globally among open-source models. By making P1-VL open-source, the authors aim to advance the development of physical intelligence in machines, aligning visual understanding with the principles of physics."
                },
                "zh": {
                    "title": "å¼€åˆ›ç§‘å­¦æ¨ç†çš„æ–°çºªå…ƒï¼šP1-VLæ¨¡å‹",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºP1-VLçš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡å¤šæ¨¡æ€æ„ŸçŸ¥å®ç°ç§‘å­¦æ¨ç†çš„ç‰©ç†ä¸€è‡´æ€§ã€‚è¯¥æ¨¡å‹ç»“åˆäº†è¯¾ç¨‹å¼ºåŒ–å­¦ä¹ å’Œä»£ç†å¢å¼ºæŠ€æœ¯ï¼Œä»¥æé«˜æ¨ç†èƒ½åŠ›å¹¶ç¨³å®šè®­ç»ƒåçš„è¡¨ç°ã€‚P1-VLåœ¨2024-2025å¹´çš„HiPhOåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè·å¾—äº†12æšé‡‘ç‰Œï¼Œæˆä¸ºé¦–ä¸ªå¼€æºè§†è§‰-è¯­è¨€æ¨¡å‹ã€‚é€šè¿‡å¼€æºP1-VLï¼Œç ”ç©¶è€…ä»¬å¸Œæœ›ä¸ºé€šç”¨ç‰©ç†æ™ºèƒ½çš„å®ç°å¥ å®šåŸºç¡€ï¼Œä½¿æœºå™¨èƒ½å¤Ÿæ›´å¥½åœ°å°†è§†è§‰æ„ŸçŸ¥ä¸æŠ½è±¡ç‰©ç†æ³•åˆ™ç›¸ç»“åˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.10090",
            "title": "Agent World Model: Infinity Synthetic Environments for Agentic Reinforcement Learning",
            "url": "https://huggingface.co/papers/2602.10090",
            "abstract": "Large language model agents trained in synthetic environments with code-driven simulations and database-backed state transitions demonstrate superior out-of-distribution generalization compared to traditional benchmark-specific approaches.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language model (LLM) have empowered autonomous agents to perform complex tasks that require multi-turn interactions with tools and environments. However, scaling such agent training is limited by the lack of diverse and reliable environments. In this paper, we propose Agent World Model (AWM), a fully synthetic environment generation pipeline. Using this pipeline, we scale to 1,000 environments covering everyday scenarios, in which agents can interact with rich toolsets (35 tools per environment on average) and obtain high-quality observations. Notably, these environments are code-driven and backed by databases, providing more reliable and consistent state transitions than environments simulated by LLMs. Moreover, they enable more efficient agent interaction compared with collecting trajectories from realistic environments. To demonstrate the effectiveness of this resource, we perform large-scale reinforcement learning for multi-turn tool-use agents. Thanks to the fully executable environments and accessible database states, we can also design reliable reward functions. Experiments on three benchmarks show that training exclusively in synthetic environments, rather than benchmark-specific ones, yields strong out-of-distribution generalization. The code is available at https://github.com/Snowflake-Labs/agent-world-model.",
            "score": 38,
            "issue_id": 999,
            "pub_date": "2026-02-10",
            "pub_date_card": {
                "ru": "10 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 10",
                "zh": "2æœˆ10æ—¥"
            },
            "hash": "3ad75457ca45a724",
            "authors": [
                "Zhaoyang Wang",
                "Canwen Xu",
                "Boyi Liu",
                "Yite Wang",
                "Siwei Han",
                "Zhewei Yao",
                "Huaxiu Yao",
                "Yuxiong He"
            ],
            "affiliations": [
                "Snowflake",
                "University of North Carolina at Chapel Hill"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.10090.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#open_source",
                    "#synthetic",
                    "#rl",
                    "#agents",
                    "#benchmark"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼Ğ¸Ñ€Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Agent World Model (AWM) â€” ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ´Ğ¾ 1000 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ² Ğ¿Ğ¾Ğ²ÑĞµĞ´Ğ½ĞµĞ²Ğ½Ğ¾Ğ¹ Ğ¶Ğ¸Ğ·Ğ½Ğ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¹, ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğµ ÑÑ€ĞµĞ´Ñ‹ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ğ¾Ğ¼ ĞºĞ¾Ğ´Ğµ Ğ¸ Ğ±Ğ°Ğ·Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ğµ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ñ‹ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ñ… Ñ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² (Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ 35 Ğ½Ğ° Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ), Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ñƒ Ğº ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ÑĞ¼ Ğ±Ğ°Ğ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ñ‚Ñ€Ñ‘Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‘Ñ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ… Ğ²Ğ½Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸, ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ²."
                },
                "en": {
                    "title": "Empowering Agents with Synthetic Environments for Better Generalization",
                    "desc": "This paper introduces the Agent World Model (AWM), a novel pipeline for generating fully synthetic environments to train large language model agents. By creating 1,000 diverse environments with rich toolsets, the AWM allows agents to interact in complex scenarios while ensuring reliable state transitions through code-driven simulations. The study demonstrates that training agents in these synthetic environments leads to superior out-of-distribution generalization compared to traditional methods that rely on specific benchmarks. The findings highlight the potential of synthetic environments in enhancing the efficiency and effectiveness of reinforcement learning for multi-turn tool-use agents."
                },
                "zh": {
                    "title": "åˆæˆç¯å¢ƒæå‡ä»£ç†çš„æ³›åŒ–èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºAgent World Modelï¼ˆAWMï¼‰çš„å…¨æ–°åˆæˆç¯å¢ƒç”Ÿæˆç®¡é“ï¼Œæ—¨åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†çš„è®­ç»ƒæ•ˆæœã€‚é€šè¿‡åˆ›å»º1000ä¸ªæ¶µç›–æ—¥å¸¸åœºæ™¯çš„ç¯å¢ƒï¼Œä»£ç†å¯ä»¥ä¸ä¸°å¯Œçš„å·¥å…·é›†è¿›è¡Œäº¤äº’ï¼Œä»è€Œè·å¾—é«˜è´¨é‡çš„è§‚å¯Ÿæ•°æ®ã€‚è¿™äº›ç¯å¢ƒæ˜¯åŸºäºä»£ç é©±åŠ¨çš„ï¼Œå¹¶ç”±æ•°æ®åº“æ”¯æŒï¼Œæä¾›æ¯”ä¼ ç»Ÿæ¨¡æ‹Ÿç¯å¢ƒæ›´å¯é çš„çŠ¶æ€è½¬æ¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨åˆæˆç¯å¢ƒä¸­è®­ç»ƒçš„ä»£ç†åœ¨å¤„ç†æœªè§è¿‡çš„æ•°æ®æ—¶è¡¨ç°ä¼˜äºåœ¨ç‰¹å®šåŸºå‡†ç¯å¢ƒä¸­è®­ç»ƒçš„ä»£ç†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.08426",
            "title": "Prism: Spectral-Aware Block-Sparse Attention",
            "url": "https://huggingface.co/papers/2602.08426",
            "abstract": "Prism addresses inefficiencies in block-sparse attention for long-context LLM pre-filling by using a spectral-aware approach that improves block selection accuracy through energy-based temperature calibration.  \t\t\t\t\tAI-generated summary \t\t\t\t Block-sparse attention is promising for accelerating long-context LLM pre-filling, yet identifying relevant blocks efficiently remains a bottleneck. Existing methods typically employ coarse-grained attention as a proxy for block importance estimation, but often resort to expensive token-level searching or scoring, resulting in significant selection overhead. In this work, we trace the inaccuracy of standard coarse-grained attention via mean pooling to a theoretical root cause: the interaction between mean pooling and Rotary Positional Embeddings (RoPE). We prove that mean pooling acts as a low-pass filter that induces destructive interference in high-frequency dimensions, effectively creating a \"blind spot\" for local positional information (e.g., slash patterns). To address this, we introduce Prism, a training-free spectral-aware approach that decomposes block selection into high-frequency and low-frequency branches. By applying energy-based temperature calibration, Prism restores the attenuated positional signals directly from pooled representations, enabling block importance estimation using purely block-level operations, thereby improving efficiency. Extensive evaluations confirm that Prism maintains accuracy parity with full attention while delivering up to 5.1times speedup.",
            "score": 30,
            "issue_id": 998,
            "pub_date": "2026-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "682c7054289ddfb0",
            "authors": [
                "Xinghao Wang",
                "Pengyu Wang",
                "Xiaoran Liu",
                "Fangxu Liu",
                "Jason Chu",
                "Kai Song",
                "Xipeng Qiu"
            ],
            "affiliations": [
                "ByteDance Inc.",
                "Fudan University",
                "OpenMOSS Team",
                "Shanghai Innovation Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.08426.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#inference",
                    "#long_context",
                    "#architecture",
                    "#training"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ¡Ğ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑĞ»ĞµĞ¿Ğ¾Ñ‚Ñ‹: ĞºĞ°Ğº Prism ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ LLM Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ²",
                    "desc": "Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ±Ğ»Ğ¾Ñ‡Ğ½Ğ¾-Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ (pre-filling) LLM Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ: ÑƒÑÑ€ĞµĞ´Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ±Ğ»Ğ¾ĞºĞ¾Ğ² Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ ĞºĞ°Ğº Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€ Ğ½Ğ¸Ğ·ĞºĞ¸Ñ… Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚, Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ÑÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸ÑÑ… Ğ¸Ğ·-Ğ·Ğ° Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ°Ğ¼Ğ¸ RoPE. ĞŸÑ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Prism, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ±Ğ»Ğ¾ĞºĞ¾Ğ² Ğ½Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½ÑƒÑ Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½ÑƒÑ Ğ²ĞµÑ‚Ğ²Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºÑƒ Ñ‚ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ½ĞµÑ€Ğ³Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ñ‚ÑƒÑ…ÑˆĞ¸Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ 5.1 Ñ€Ğ°Ğ· Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ±Ğ»Ğ¾ĞºĞ¾Ğ² Ğ±ĞµĞ· Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²."
                },
                "en": {
                    "title": "Prism: Accelerating Block-Sparse Attention with Spectral Awareness",
                    "desc": "Prism is a novel approach designed to enhance block-sparse attention in long-context language model (LLM) pre-filling. It addresses the inefficiencies of existing methods that rely on coarse-grained attention for block importance estimation, which can lead to high computational costs. By identifying the limitations of mean pooling in conjunction with Rotary Positional Embeddings, Prism introduces a spectral-aware method that separates block selection into high-frequency and low-frequency components. This allows for more accurate block importance estimation while significantly improving processing speed, achieving up to 5.1 times faster performance without sacrificing accuracy."
                },
                "zh": {
                    "title": "Prismï¼šæå‡å—ç¨€ç–æ³¨æ„åŠ›æ•ˆç‡çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "Prism æ˜¯ä¸€ç§é’ˆå¯¹é•¿ä¸Šä¸‹æ–‡ LLM é¢„å¡«å……ä¸­å—ç¨€ç–æ³¨æ„åŠ›ä½æ•ˆé—®é¢˜çš„è§£å†³æ–¹æ¡ˆã€‚å®ƒé€šè¿‡èƒ½é‡åŸºç¡€çš„æ¸©åº¦æ ¡å‡†ï¼Œæå‡äº†å—é€‰æ‹©çš„å‡†ç¡®æ€§ï¼Œè§£å†³äº†ä¼ ç»Ÿç²—ç²’åº¦æ³¨æ„åŠ›æ–¹æ³•çš„ä¸è¶³ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå‡å€¼æ± åŒ–ä¸æ—‹è½¬ä½ç½®åµŒå…¥ï¼ˆRoPEï¼‰ä¹‹é—´çš„ç›¸äº’ä½œç”¨å¯¼è‡´äº†ä¿¡æ¯æŸå¤±ï¼Œå½¢æˆäº†å¯¹å±€éƒ¨ä½ç½®ä¿¡æ¯çš„â€œç›²ç‚¹â€ã€‚Prism é€šè¿‡å°†å—é€‰æ‹©åˆ†è§£ä¸ºé«˜é¢‘å’Œä½é¢‘åˆ†æ”¯ï¼Œèƒ½å¤Ÿåœ¨ä¸å¢åŠ è®¡ç®—æˆæœ¬çš„æƒ…å†µä¸‹ï¼Œä¿æŒä¸å…¨æ³¨æ„åŠ›ç›¸åŒçš„å‡†ç¡®æ€§ï¼ŒåŒæ—¶å®ç°é«˜è¾¾ 5.1 å€çš„åŠ é€Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.07035",
            "title": "DLLM-Searcher: Adapting Diffusion Large Language Model for Search Agents",
            "url": "https://huggingface.co/papers/2602.07035",
            "abstract": "Diffusion Large Language Models are optimized for search agents through enhanced reasoning capabilities and reduced latency via a parallel reasoning paradigm.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, Diffusion Large Language Models (dLLMs) have demonstrated unique efficiency advantages, enabled by their inherently parallel decoding mechanism and flexible generation paradigm. Meanwhile, despite the rapid advancement of Search Agents, their practical deployment is constrained by a fundamental limitation, termed as 1) Latency Challenge: the serial execution of multi-round reasoning, tool calling, and tool response waiting under the ReAct agent paradigm induces severe end-to-end latency. Intuitively, dLLMs can leverage their distinctive strengths to optimize the operational efficiency of agents under the ReAct agent paradigm. Practically, existing dLLM backbones face the 2) Agent Ability Challenge. That is, existing dLLMs exhibit remarkably weak reasoning and tool-calling capabilities, preventing these advantages from being effectively realized in practice. In this paper, we propose DLLM-Searcher, an optimization framework for dLLM-based Search Agents. To solve the Agent Ability Challenge, we design a two-stage post-training pipeline encompassing Agentic Supervised Fine-Tuning (Agentic SFT) and Agentic Variance-Reduced Preference Optimization Agentic VRPO, which enhances the backbone dLLM's information seeking and reasoning capabilities. To mitigate the Latency Challenge, we leverage the flexible generation mechanism of dLLMs and propose a novel agent paradigm termed Parallel-Reasoning and Acting P-ReAct. P-ReAct guides the model to prioritize decoding tool_call instructions, thereby allowing the model to keep thinking while waiting for the tool's return. Experimental results demonstrate that DLLM-Searcher achieves performance comparable to mainstream LLM-based search agents and P-ReAct delivers approximately 15% inference acceleration. Our code is available at https://anonymous.4open.science/r/DLLM-Searcher-553C",
            "score": 25,
            "issue_id": 998,
            "pub_date": "2026-02-03",
            "pub_date_card": {
                "ru": "3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 3",
                "zh": "2æœˆ3æ—¥"
            },
            "hash": "701aa08fdd4212e1",
            "authors": [
                "Jiahao Zhao",
                "Shaoxuan Xu",
                "Zhongxiang Sun",
                "Fengqi Zhu",
                "Jingyang Ou",
                "Yuling Shi",
                "Chongxuan Li",
                "Xiao Zhang",
                "Jun Xu"
            ],
            "affiliations": [
                "Renmin University of China",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.07035.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#agents",
                    "#optimization",
                    "#inference",
                    "#architecture",
                    "#training",
                    "#reasoning",
                    "#diffusion"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "ĞŸĞ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº DLLM-Searcher Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ´Ğ²Ğµ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹: ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Agentic SFT Ğ¸ Agentic VRPO. Ğ”Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Parallel-Reasoning and Acting (P-ReAct), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ°Ñ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ¹ Ñ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸, Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ½Ğ° 15%."
                },
                "en": {
                    "title": "Optimizing Search Agents with Fast and Smart dLLMs",
                    "desc": "This paper introduces DLLM-Searcher, a framework designed to enhance the efficiency of Diffusion Large Language Models (dLLMs) for search agents. It addresses two main challenges: the Latency Challenge, which arises from slow multi-round reasoning processes, and the Agent Ability Challenge, where existing dLLMs struggle with reasoning and tool-calling tasks. The proposed solution includes a two-stage post-training process that improves the model's reasoning capabilities and a new agent paradigm called Parallel-Reasoning and Acting (P-ReAct) that reduces latency by allowing simultaneous processing. Experimental results show that DLLM-Searcher performs on par with current leading search agents while achieving a 15% increase in inference speed."
                },
                "zh": {
                    "title": "æå‡æœç´¢ä»£ç†æ•ˆç‡çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºDLLM-Searcherçš„ä¼˜åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨æå‡åŸºäºæ‰©æ•£å¤§è¯­è¨€æ¨¡å‹ï¼ˆdLLMï¼‰çš„æœç´¢ä»£ç†çš„æ€§èƒ½ã€‚é€šè¿‡è®¾è®¡ä¸€ä¸ªä¸¤é˜¶æ®µçš„åè®­ç»ƒæµç¨‹ï¼ŒåŒ…æ‹¬ä»£ç†ç›‘ç£å¾®è°ƒï¼ˆAgentic SFTï¼‰å’Œä»£ç†æ–¹å·®å‡å°‘åå¥½ä¼˜åŒ–ï¼ˆAgentic VRPOï¼‰ï¼Œå¢å¼ºäº†dLLMçš„ä¿¡æ¯è·å–å’Œæ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³å»¶è¿ŸæŒ‘æˆ˜ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ç§æ–°çš„ä»£ç†èŒƒå¼â€”â€”å¹¶è¡Œæ¨ç†ä¸è¡ŒåŠ¨ï¼ˆP-ReActï¼‰ï¼Œä½¿æ¨¡å‹åœ¨ç­‰å¾…å·¥å…·è¿”å›æ—¶èƒ½å¤Ÿç»§ç»­æ€è€ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDLLM-Searcherçš„æ€§èƒ½ä¸ä¸»æµLLMæœç´¢ä»£ç†ç›¸å½“ï¼Œå¹¶ä¸”P-ReActå®ç°äº†çº¦15%çš„æ¨ç†åŠ é€Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.10104",
            "title": "Olaf-World: Orienting Latent Actions for Video World Modeling",
            "url": "https://huggingface.co/papers/2602.10104",
            "abstract": "Sequence-level control-effect alignment enables structured latent action space learning for zero-shot action transfer in video world models.  \t\t\t\t\tAI-generated summary \t\t\t\t Scaling action-controllable world models is limited by the scarcity of action labels. While latent action learning promises to extract control interfaces from unlabeled video, learned latents often fail to transfer across contexts: they entangle scene-specific cues and lack a shared coordinate system. This occurs because standard objectives operate only within each clip, providing no mechanism to align action semantics across contexts. Our key insight is that although actions are unobserved, their semantic effects are observable and can serve as a shared reference. We introduce SeqÎ”-REPA, a sequence-level control-effect alignment objective that anchors integrated latent action to temporal feature differences from a frozen, self-supervised video encoder. Building on this, we present Olaf-World, a pipeline that pretrains action-conditioned video world models from large-scale passive video. Extensive experiments demonstrate that our method learns a more structured latent action space, leading to stronger zero-shot action transfer and more data-efficient adaptation to new control interfaces than state-of-the-art baselines.",
            "score": 21,
            "issue_id": 998,
            "pub_date": "2026-02-10",
            "pub_date_card": {
                "ru": "10 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 10",
                "zh": "2æœˆ10æ—¥"
            },
            "hash": "dbc0a4e38007e9f5",
            "authors": [
                "Yuxin Jiang",
                "Yuchao Gu",
                "Ivor W. Tsang",
                "Mike Zheng Shou"
            ],
            "affiliations": [
                "CFAR & IHPC, Agency for Science, Technology and Research (A*STAR), Singapore",
                "Show Lab, National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.10104.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#video",
                    "#architecture"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ñ‹Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ñ‹ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ° ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¼Ğ¸Ñ€Ğ° Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ñ‹Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ñ‹ ÑÑ‚Ğ¸Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ SeqÎ”-REPA â€” Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğº Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸ÑĞ¼ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸Ğ· Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ¾Ğ³Ğ¾ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Olaf-World Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¸Ñ€Ğ° Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¾Ğ±ÑŠĞµĞ¼Ğ°Ñ… Ğ¿Ğ°ÑÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ² zero-shot Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°Ğ¼ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Aligning Action Effects for Better Zero-Shot Learning in Video Models",
                    "desc": "This paper addresses the challenge of learning action-controllable world models from unlabeled video data, which is often hindered by the lack of action labels. The authors propose a novel approach called SeqÎ”-REPA, which aligns the effects of actions across different contexts by using observable semantic effects as a reference. This method allows for the creation of a structured latent action space that improves the transfer of learned actions to new situations without requiring additional labeled data. The results show that their approach, implemented in the Olaf-World pipeline, outperforms existing methods in zero-shot action transfer and adapts more efficiently to new control interfaces."
                },
                "zh": {
                    "title": "é€šè¿‡æ§åˆ¶æ•ˆæœå¯¹é½å®ç°é›¶-shotåŠ¨ä½œè½¬ç§»",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åºåˆ—çº§æ§åˆ¶æ•ˆæœå¯¹é½çš„æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³è§†é¢‘ä¸–ç•Œæ¨¡å‹ä¸­åŠ¨ä½œæ ‡ç­¾ç¨€ç¼ºçš„é—®é¢˜ã€‚é€šè¿‡è§‚å¯ŸåŠ¨ä½œçš„è¯­ä¹‰æ•ˆæœï¼Œæˆ‘ä»¬å¯ä»¥ä¸ºæœªè§‚å¯Ÿåˆ°çš„åŠ¨ä½œæä¾›ä¸€ä¸ªå…±äº«çš„å‚è€ƒæ¡†æ¶ã€‚æˆ‘ä»¬å¼•å…¥äº†SeqÎ”-REPAç›®æ ‡ï¼Œä½¿å¾—é›†æˆçš„æ½œåœ¨åŠ¨ä½œä¸å†»ç»“çš„è‡ªç›‘ç£è§†é¢‘ç¼–ç å™¨çš„æ—¶é—´ç‰¹å¾å·®å¼‚å¯¹é½ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå­¦ä¹ æ›´ç»“æ„åŒ–çš„æ½œåœ¨åŠ¨ä½œç©ºé—´ï¼Œä»è€Œåœ¨é›¶-shotåŠ¨ä½œè½¬ç§»å’Œæ–°æ§åˆ¶æ¥å£çš„é€‚åº”æ€§ä¸Šä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.07022",
            "title": "Condition Errors Refinement in Autoregressive Image Generation with Diffusion Loss",
            "url": "https://huggingface.co/papers/2602.07022",
            "abstract": "Autoregressive models with diffusion loss outperform traditional diffusion models by effectively mitigating condition errors through patch denoising optimization and condition refinement using Optimal Transport theory.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent studies have explored autoregressive models for image generation, with promising results, and have combined diffusion models with autoregressive frameworks to optimize image generation via diffusion losses. In this study, we present a theoretical analysis of diffusion and autoregressive models with diffusion loss, highlighting the latter's advantages. We present a theoretical comparison of conditional diffusion and autoregressive diffusion with diffusion loss, demonstrating that patch denoising optimization in autoregressive models effectively mitigates condition errors and leads to a stable condition distribution. Our analysis also reveals that autoregressive condition generation refines the condition, causing the condition error influence to decay exponentially. In addition, we introduce a novel condition refinement approach based on Optimal Transport (OT) theory to address ``condition inconsistency''. We theoretically demonstrate that formulating condition refinement as a Wasserstein Gradient Flow ensures convergence toward the ideal condition distribution, effectively mitigating condition inconsistency. Experiments demonstrate the superiority of our method over diffusion and autoregressive models with diffusion loss methods.",
            "score": 18,
            "issue_id": 1000,
            "pub_date": "2026-02-02",
            "pub_date_card": {
                "ru": "2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 2",
                "zh": "2æœˆ2æ—¥"
            },
            "hash": "97647adf90403558",
            "authors": [
                "Yucheng Zhou",
                "Hao Li",
                "Jianbing Shen"
            ],
            "affiliations": [
                "SKL-IOTSC, CIS, University of Macau"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.07022.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#optimization"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚Ğ¾Ğ¼ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ĞµĞ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ°Ñ‚Ñ‡-Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ğ½Ğ³ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ° Ñ‚ĞµĞ¾Ñ€Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚Ğ°, Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ĞºĞ°Ğº Ğ¿Ğ¾Ñ‚Ğ¾Ğº Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ° Ğ’Ğ°ÑÑĞµÑ€ÑˆÑ‚ĞµĞ¹Ğ½Ğ° Ğ´Ğ»Ñ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğº Ğ¸Ğ´ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Image Generation with Autoregressive Models and Optimal Transport",
                    "desc": "This paper discusses how autoregressive models combined with diffusion loss can improve image generation by reducing errors in conditions. It highlights the use of patch denoising optimization to stabilize the condition distribution and refine the conditions effectively. The authors introduce a new method based on Optimal Transport theory to solve issues related to condition inconsistency. Their experiments show that this approach outperforms traditional diffusion models and other autoregressive methods with diffusion loss."
                },
                "zh": {
                    "title": "è‡ªå›å½’æ¨¡å‹ä¸æ‰©æ•£æŸå¤±çš„ç»“åˆï¼Œæå‡å›¾åƒç”Ÿæˆè´¨é‡",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†è‡ªå›å½’æ¨¡å‹ä¸æ‰©æ•£æŸå¤±çš„ç»“åˆï¼Œå±•ç¤ºäº†å…¶åœ¨å›¾åƒç”Ÿæˆä¸­çš„ä¼˜åŠ¿ã€‚é€šè¿‡è¡¥ä¸å»å™ªä¼˜åŒ–å’Œæ¡ä»¶ç²¾ç‚¼ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå‡å°‘æ¡ä»¶é”™è¯¯ï¼Œæå‡ç”Ÿæˆå›¾åƒçš„è´¨é‡ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†åŸºäºæœ€ä¼˜ä¼ è¾“ç†è®ºçš„æ–°æ¡ä»¶ç²¾ç‚¼æ–¹æ³•ï¼Œç¡®ä¿æ¡ä»¶åˆ†å¸ƒçš„æ”¶æ•›æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨æ€§èƒ½ä¸Šä¼˜äºä¼ ç»Ÿçš„æ‰©æ•£æ¨¡å‹å’Œè‡ªå›å½’æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.09084",
            "title": "Agent Banana: High-Fidelity Image Editing with Agentic Thinking and Tooling",
            "url": "https://huggingface.co/papers/2602.09084",
            "abstract": "Agent Banana addresses challenges in instruction-based image editing through a hierarchical framework with context folding and image layer decomposition for high-fidelity, multi-turn editing at ultra-high resolution.  \t\t\t\t\tAI-generated summary \t\t\t\t We study instruction-based image editing under professional workflows and identify three persistent challenges: (i) editors often over-edit, modifying content beyond the user's intent; (ii) existing models are largely single-turn, while multi-turn edits can alter object faithfulness; and (iii) evaluation at around 1K resolution is misaligned with real workflows that often operate on ultra high-definition images (e.g., 4K). We propose Agent Banana, a hierarchical agentic planner-executor framework for high-fidelity, object-aware, deliberative editing. Agent Banana introduces two key mechanisms: (1) Context Folding, which compresses long interaction histories into structured memory for stable long-horizon control; and (2) Image Layer Decomposition, which performs localized layer-based edits to preserve non-target regions while enabling native-resolution outputs. To support rigorous evaluation, we build HDD-Bench, a high-definition, dialogue-based benchmark featuring verifiable stepwise targets and native 4K images (11.8M pixels) for diagnosing long-horizon failures. On HDD-Bench, Agent Banana achieves the best multi-turn consistency and background fidelity (e.g., IC 0.871, SSIM-OM 0.84, LPIPS-OM 0.12) while remaining competitive on instruction following, and also attains strong performance on standard single-turn editing benchmarks. We hope this work advances reliable, professional-grade agentic image editing and its integration into real workflows.",
            "score": 17,
            "issue_id": 998,
            "pub_date": "2026-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "ebae9e98aa8a63f3",
            "authors": [
                "Ruijie Ye",
                "Jiayi Zhang",
                "Zhuoxin Liu",
                "Zihao Zhu",
                "Siyuan Yang",
                "Li Li",
                "Tianfu Fu",
                "Franck Dernoncourt",
                "Yue Zhao",
                "Jiacheng Zhu",
                "Ryan Rossi",
                "Wenhao Chai",
                "Zhengzhong Tu"
            ],
            "affiliations": [
                "Adobe Research",
                "Brown University",
                "Meta AI",
                "Princeton University",
                "TAMU",
                "UCSD",
                "USC",
                "UW-Madison",
                "xAI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.09084.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#dataset",
                    "#cv",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ Ğ¸ Ğ¿Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ğ¾Ğ¹ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸ĞµĞ¹",
                    "desc": "Agent Banana Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼, Ñ€ĞµÑˆĞ°ÑÑ‰ÑƒÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ°Ğ¼Ğ¸ ÑĞ²Ğ»ÑÑÑ‚ÑÑ Context Folding Ğ´Ğ»Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ² ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ¸ Image Layer Decomposition Ğ´Ğ»Ñ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ñ‘Ğ² Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾ÑÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ HDD-Bench â€” Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ñ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹ Ğ¸ Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼ 4K Ğ´Ğ»Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Agent Banana Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ„Ğ¾Ğ½Ğ°, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹."
                },
                "en": {
                    "title": "Revolutionizing Image Editing with Agent Banana",
                    "desc": "Agent Banana is a new framework designed to improve instruction-based image editing by addressing common issues faced by editors. It uses a hierarchical approach that includes Context Folding to manage long editing histories and Image Layer Decomposition to allow precise edits without affecting the entire image. This method enables high-fidelity, multi-turn editing at ultra-high resolutions, which is crucial for professional workflows. The framework is evaluated using HDD-Bench, a benchmark that tests its performance on high-definition images, showing superior consistency and fidelity in edits compared to existing models."
                },
                "zh": {
                    "title": "Agent Bananaï¼šé«˜ä¿çœŸå›¾åƒç¼–è¾‘çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†Agent Bananaï¼Œä¸€ä¸ªç”¨äºåŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘çš„åˆ†å±‚æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¸“ä¸šå·¥ä½œæµç¨‹ä¸­çš„ä¸‰ä¸ªä¸»è¦æŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œç¼–è¾‘è€…å¸¸å¸¸è¿‡åº¦ç¼–è¾‘ï¼Œè¶…å‡ºç”¨æˆ·çš„æ„å›¾ï¼›å…¶æ¬¡ï¼Œç°æœ‰æ¨¡å‹ä¸»è¦æ˜¯å•è½®ç¼–è¾‘ï¼Œè€Œå¤šè½®ç¼–è¾‘å¯èƒ½ä¼šå½±å“å¯¹è±¡çš„çœŸå®æ€§ï¼›æœ€åï¼Œç°æœ‰è¯„ä¼°é€šå¸¸åœ¨1Kåˆ†è¾¨ç‡ä¸‹è¿›è¡Œï¼Œè€Œå®é™…å·¥ä½œæµç¨‹å¸¸åœ¨è¶…é«˜æ¸…å›¾åƒï¼ˆå¦‚4Kï¼‰ä¸Šè¿›è¡Œã€‚Agent Bananaå¼•å…¥äº†ä¸Šä¸‹æ–‡æŠ˜å å’Œå›¾åƒå±‚åˆ†è§£ä¸¤ä¸ªå…³é”®æœºåˆ¶ï¼Œä»¥å®ç°é«˜ä¿çœŸã€å¯¹è±¡æ„ŸçŸ¥çš„æ·±æ€ç†Ÿè™‘ç¼–è¾‘ï¼Œå¹¶åœ¨HDD-BenchåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.04208",
            "title": "SCALE: Self-uncertainty Conditioned Adaptive Looking and Execution for Vision-Language-Action Models",
            "url": "https://huggingface.co/papers/2602.04208",
            "abstract": "SCALE is a novel inference strategy for Vision-Language-Action models that jointly modulates visual perception and action based on self-uncertainty, improving robustness without additional training or multiple forward passes.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language-Action (VLA) models have emerged as a promising paradigm for general-purpose robotic control, with test-time scaling (TTS) gaining attention to enhance robustness beyond training. However, existing TTS methods for VLAs require additional training, verifiers, and multiple forward passes, making them impractical for deployment. Moreover, they intervene only at action decoding while keeping visual representations fixed-insufficient under perceptual ambiguity, where reconsidering how to perceive is as important as deciding what to do. To address these limitations, we propose SCALE, a simple inference strategy that jointly modulates visual perception and action based on 'self-uncertainty', inspired by uncertainty-driven exploration in Active Inference theory-requiring no additional training, no verifier, and only a single forward pass. SCALE broadens exploration in both perception and action under high uncertainty, while focusing on exploitation when confident-enabling adaptive execution across varying conditions. Experiments on simulated and real-world benchmarks demonstrate that SCALE improves state-of-the-art VLAs and outperforms existing TTS methods while maintaining single-pass efficiency.",
            "score": 17,
            "issue_id": 998,
            "pub_date": "2026-02-04",
            "pub_date_card": {
                "ru": "4 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 4",
                "zh": "2æœˆ4æ—¥"
            },
            "hash": "33aa8c2ca8f8555f",
            "authors": [
                "Hyeonbeom Choi",
                "Daechul Ahn",
                "Youhan Lee",
                "Taewook Kang",
                "Seongwon Cho",
                "Jonghyun Choi"
            ],
            "affiliations": [
                "ECE, IPAI and ASRI in Seoul National University",
                "Seoul National University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.04208.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#robotics",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ¼ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ SCALE Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Vision-Language-Action Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ°Ğ¼Ğ¾Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»Ñ‘Ğ½ Ñ‚ĞµĞ¾Ñ€Ğ¸ĞµĞ¹ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¸ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¸Ğ»Ğ¸ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ. SCALE Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ°Ğº Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ, Ñ‚Ğ°Ğº Ğ¸ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ° Ğ¿Ñ€Ğ¸ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ°Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… VLA Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°."
                },
                "en": {
                    "title": "SCALE: Enhancing VLA Robustness with Self-Uncertainty",
                    "desc": "SCALE is an innovative inference strategy designed for Vision-Language-Action (VLA) models that enhances their robustness by addressing self-uncertainty during decision-making. Unlike traditional test-time scaling methods, SCALE does not require extra training or multiple forward passes, making it more efficient for real-world applications. It dynamically adjusts both visual perception and action based on the model's confidence, allowing for better adaptability in uncertain environments. Experimental results show that SCALE outperforms existing methods while maintaining a single-pass execution, thus improving the overall performance of VLA models."
                },
                "zh": {
                    "title": "SCALEï¼šæå‡è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹é²æ£’æ€§çš„åˆ›æ–°ç­–ç•¥",
                    "desc": "SCALEæ˜¯ä¸€ç§æ–°é¢–çš„æ¨ç†ç­–ç•¥ï¼Œæ—¨åœ¨æ”¹å–„è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹çš„é²æ£’æ€§ã€‚å®ƒé€šè¿‡è‡ªæˆ‘ä¸ç¡®å®šæ€§æ¥å…±åŒè°ƒèŠ‚è§†è§‰æ„ŸçŸ¥å’ŒåŠ¨ä½œï¼Œè€Œæ— éœ€é¢å¤–çš„è®­ç»ƒæˆ–å¤šæ¬¡å‰å‘ä¼ é€’ã€‚ä¸ç°æœ‰çš„æ–¹æ³•ç›¸æ¯”ï¼ŒSCALEåœ¨é«˜ä¸ç¡®å®šæ€§ä¸‹æ‰©å±•äº†æ„ŸçŸ¥å’ŒåŠ¨ä½œçš„æ¢ç´¢ï¼ŒåŒæ—¶åœ¨è‡ªä¿¡æ—¶ä¸“æ³¨äºåˆ©ç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSCALEåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰çš„TTSæ–¹æ³•ï¼Œä¿æŒäº†å•æ¬¡ä¼ é€’çš„é«˜æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.09849",
            "title": "BagelVLA: Enhancing Long-Horizon Manipulation via Interleaved Vision-Language-Action Generation",
            "url": "https://huggingface.co/papers/2602.09849",
            "abstract": "BagelVLA is a unified Vision-Language-Action model that integrates linguistic planning, visual forecasting, and action generation through residual flow guidance for improved manipulation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Equipping embodied agents with the ability to reason about tasks, foresee physical outcomes, and generate precise actions is essential for general-purpose manipulation. While recent Vision-Language-Action (VLA) models have leveraged pre-trained foundation models, they typically focus on either linguistic planning or visual forecasting in isolation. These methods rarely integrate both capabilities simultaneously to guide action generation, leading to suboptimal performance in complex, long-horizon manipulation tasks. To bridge this gap, we propose BagelVLA, a unified model that integrates linguistic planning, visual forecasting, and action generation within a single framework. Initialized from a pretrained unified understanding and generative model, BagelVLA is trained to interleave textual reasoning and visual prediction directly into the action execution loop. To efficiently couple these modalities, we introduce Residual Flow Guidance (RFG), which initializes from current observation and leverages single-step denoising to extract predictive visual features, guiding action generation with minimal latency. Extensive experiments demonstrate that BagelVLA outperforms existing baselines by a significant margin on multiple simulated and real-world benchmarks, particularly in tasks requiring multi-stage reasoning.",
            "score": 13,
            "issue_id": 998,
            "pub_date": "2026-02-10",
            "pub_date_card": {
                "ru": "10 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 10",
                "zh": "2æœˆ10æ—¥"
            },
            "hash": "3ce6ad384612c6ee",
            "authors": [
                "Yucheng Hu",
                "Jianke Zhang",
                "Yuanfei Luo",
                "Yanjiang Guo",
                "Xiaoyu Chen",
                "Xinshu Sun",
                "Kun Feng",
                "Qingzhou Lu",
                "Sheng Chen",
                "Yangang Zhang",
                "Wei Li",
                "Jianyu Chen"
            ],
            "affiliations": [
                "ByteDance",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.09849.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#architecture",
                    "#multimodal",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ°, Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ¼Ğ½Ğ¾Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°",
                    "desc": "BagelVLA â€” ÑÑ‚Ğ¾ ĞµĞ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Vision-Language-Action, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. Ğ”Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Residual Flow Guidance (RFG) â€” Ğ¼ĞµÑ‚Ğ¾Ğ´, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ´Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ BagelVLA Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğµ Ñ‚ĞµÑÑ‚Ğ¾Ğ², Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Unified Vision-Language-Action for Enhanced Manipulation",
                    "desc": "BagelVLA is a comprehensive model that combines language understanding, visual prediction, and action execution to enhance manipulation tasks. It addresses the limitations of previous Vision-Language-Action models that often treated linguistic and visual components separately. By integrating these elements into a single framework, BagelVLA allows for more effective reasoning and action generation. The introduction of Residual Flow Guidance enables the model to efficiently utilize visual features for real-time action planning, resulting in superior performance in complex tasks."
                },
                "zh": {
                    "title": "BagelVLAï¼šç»Ÿä¸€çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹",
                    "desc": "BagelVLAæ˜¯ä¸€ç§ç»Ÿä¸€çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡æ®‹å·®æµå¼•å¯¼æ•´åˆè¯­è¨€è§„åˆ’ã€è§†è§‰é¢„æµ‹å’ŒåŠ¨ä½œç”Ÿæˆï¼Œä»¥æé«˜æ“ä½œä»»åŠ¡çš„è¡¨ç°ã€‚è¯¥æ¨¡å‹è§£å†³äº†ç°æœ‰è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹é€šå¸¸åªå…³æ³¨è¯­è¨€è§„åˆ’æˆ–è§†è§‰é¢„æµ‹çš„é—®é¢˜ï¼Œç¼ºä¹åŒæ—¶æ•´åˆè¿™ä¸¤ç§èƒ½åŠ›çš„èƒ½åŠ›ã€‚BagelVLAé€šè¿‡å°†æ–‡æœ¬æ¨ç†å’Œè§†è§‰é¢„æµ‹ç›´æ¥èå…¥åŠ¨ä½œæ‰§è¡Œå¾ªç¯ï¼Œæå‡äº†å¤æ‚é•¿æ—¶é—´æ“ä½œä»»åŠ¡çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBagelVLAåœ¨å¤šä¸ªæ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„åŸºçº¿æ¨¡å‹ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¤šé˜¶æ®µæ¨ç†çš„ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.00268",
            "title": "TokenTrim: Inference-Time Token Pruning for Autoregressive Long Video Generation",
            "url": "https://huggingface.co/papers/2602.00268",
            "abstract": "Auto-regressive video generation suffers from temporal drift due to error accumulation in latent conditioning tokens, which is addressed by identifying and removing unstable tokens during inference to improve long-horizon consistency.  \t\t\t\t\tAI-generated summary \t\t\t\t Auto-regressive video generation enables long video synthesis by iteratively conditioning each new batch of frames on previously generated content. However, recent work has shown that such pipelines suffer from severe temporal drift, where errors accumulate and amplify over long horizons. We hypothesize that this drift does not primarily stem from insufficient model capacity, but rather from inference-time error propagation. Specifically, we contend that drift arises from the uncontrolled reuse of corrupted latent conditioning tokens during auto-regressive inference. To correct this accumulation of errors, we propose a simple, inference-time method that mitigates temporal drift by identifying and removing unstable latent tokens before they are reused for conditioning. For this purpose, we define unstable tokens as latent tokens whose representations deviate significantly from those of the previously generated batch, indicating potential corruption or semantic drift. By explicitly removing corrupted latent tokens from the auto-regressive context, rather than modifying entire spatial regions or model parameters, our method prevents unreliable latent information from influencing future generation steps. As a result, it significantly improves long-horizon temporal consistency without modifying the model architecture, training procedure, or leaving latent space.",
            "score": 13,
            "issue_id": 1005,
            "pub_date": "2026-01-30",
            "pub_date_card": {
                "ru": "30 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 30",
                "zh": "1æœˆ30æ—¥"
            },
            "hash": "13664096e306225d",
            "authors": [
                "Ariel Shaulov",
                "Eitan Shaar",
                "Amit Edenzon",
                "Lior Wolf"
            ],
            "affiliations": [
                "Independent Researcher",
                "School of Computer Science Tel Aviv University, Israel",
                "School of Mathematics Bar-Ilan University, Israel"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.00268.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#inference"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ĞµÑ‚ Ğ¸Ğ·-Ğ·Ğ° Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ñ… ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ¸ ÑƒĞ´Ğ°Ğ»ÑĞµÑ‚ Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ â€” Ñ‚Ğµ, Ñ‡ÑŒĞ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½ÑÑÑ‚ÑÑ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰ĞµĞ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ´Ñ€Ğ¾Ğ². ĞŸÑƒÑ‚ĞµĞ¼ Ğ¸ÑĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ñ€Ñ€ÑƒĞ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸Ğ· Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ½Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğµ ÑˆĞ°Ğ³Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½ÑƒÑ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Stabilizing Video Generation by Removing Unstable Tokens",
                    "desc": "This paper addresses the problem of temporal drift in auto-regressive video generation, where errors accumulate over time, leading to inconsistencies in long videos. The authors propose that this drift is caused by the reuse of unstable latent conditioning tokens during inference, rather than a lack of model capacity. To combat this issue, they introduce a method that identifies and removes these unstable tokens before they can affect future frame generation. By doing so, the approach enhances long-horizon consistency without altering the model's architecture or training process."
                },
                "zh": {
                    "title": "æ¶ˆé™¤æ—¶é—´æ¼‚ç§»ï¼Œæå‡è§†é¢‘ç”Ÿæˆä¸€è‡´æ€§",
                    "desc": "è‡ªå›å½’è§†é¢‘ç”Ÿæˆåœ¨ç”Ÿæˆé•¿è§†é¢‘æ—¶ä¼šå‡ºç°æ—¶é—´æ¼‚ç§»é—®é¢˜ï¼Œè¿™æ˜¯ç”±äºæ½œåœ¨æ¡ä»¶ä»¤ç‰Œçš„é”™è¯¯ç´¯ç§¯é€ æˆçš„ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œé€šè¿‡åœ¨æ¨ç†è¿‡ç¨‹ä¸­è¯†åˆ«å¹¶ç§»é™¤ä¸ç¨³å®šçš„ä»¤ç‰Œï¼Œæ¥æ”¹å–„é•¿æ—¶é—´æ®µçš„ä¸€è‡´æ€§ã€‚æˆ‘ä»¬å®šä¹‰ä¸ç¨³å®šä»¤ç‰Œä¸ºä¸ä¹‹å‰ç”Ÿæˆçš„æ‰¹æ¬¡è¡¨ç¤ºæ˜¾è‘—åç¦»çš„æ½œåœ¨ä»¤ç‰Œï¼Œè¿™è¡¨æ˜å®ƒä»¬å¯èƒ½å·²ç»è¢«æŸåæˆ–å‘ç”Ÿäº†è¯­ä¹‰æ¼‚ç§»ã€‚é€šè¿‡æ˜¾å¼ç§»é™¤è¿™äº›ä¸å¯é çš„æ½œåœ¨ä¿¡æ¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æé«˜äº†é•¿æ—¶é—´æ®µçš„æ—¶é—´ä¸€è‡´æ€§ï¼Œè€Œæ— éœ€ä¿®æ”¹æ¨¡å‹æ¶æ„æˆ–è®­ç»ƒè¿‡ç¨‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.10098",
            "title": "VLA-JEPA: Enhancing Vision-Language-Action Model with Latent World Model",
            "url": "https://huggingface.co/papers/2602.10098",
            "abstract": "VLA-JEPA is a JEPA-style pretraining framework that improves vision-language-action policy learning by using leakage-free state prediction in latent space, enhancing generalization and robustness in manipulation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Pretraining Vision-Language-Action (VLA) policies on internet-scale video is appealing, yet current latent-action objectives often learn the wrong thing: they remain anchored to pixel variation rather than action-relevant state transitions, making them vulnerable to appearance bias, nuisance motion, and information leakage. We introduce VLA-JEPA, a JEPA-style pretraining framework that sidesteps these pitfalls by design. The key idea is leakage-free state prediction: a target encoder produces latent representations from future frames, while the student pathway sees only the current observation -- future information is used solely as supervision targets, never as input. By predicting in latent space rather than pixel space, VLA-JEPA learns dynamics abstractions that are robust to camera motion and irrelevant background changes. This yields a simple two-stage recipe -- JEPA pretraining followed by action-head fine-tuning -- without the multi-stage complexity of prior latent-action pipelines. Experiments on LIBERO, LIBERO-Plus, SimplerEnv and real-world manipulation tasks show that VLA-JEPA achieves consistent gains in generalization and robustness over existing methods.",
            "score": 12,
            "issue_id": 998,
            "pub_date": "2026-02-10",
            "pub_date_card": {
                "ru": "10 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 10",
                "zh": "2æœˆ10æ—¥"
            },
            "hash": "2b0f8debdedc7c01",
            "authors": [
                "Jingwen Sun",
                "Wenyao Zhang",
                "Zekun Qi",
                "Shaojie Ren",
                "Zezhi Liu",
                "Hanxin Zhu",
                "Guangzhong Sun",
                "Xin Jin",
                "Zhibo Chen"
            ],
            "affiliations": [
                "Eastern Institute of Technology, Ningbo",
                "Nankai University",
                "Shanghai Jiao Tong University",
                "Tsinghua University",
                "University of Chinese Academy of Sciences",
                "University of Science and Technology of China",
                "Zhongguancun Academy, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.10098.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#robotics",
                    "#architecture",
                    "#cv"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ±ĞµĞ· ÑƒÑ‚ĞµÑ‡ĞµĞº: ĞºĞ°Ğº Ğ½Ğ°ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ° Ğ²Ğ¸Ğ´ĞµÑ‚ÑŒ ÑÑƒÑ‚ÑŒ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹",
                    "desc": "VLA-JEPA Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ JEPA, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ·Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾-Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ² ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ±ĞµĞ· ÑƒÑ‚ĞµÑ‡ĞºĞ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ², Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚ Ğ²Ğ¸Ğ´Ğ¸Ñ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğµ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ ÑƒÑ‚ĞµÑ‡ĞºÑƒ Ğ¸ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¹ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹. ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ñ‹ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸, ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ñ‹Ğµ Ğº Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑĞ¼ Ñ„Ğ¾Ğ½Ğ°. Ğ”Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, ÑĞ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ğ¹ Ğ¸Ğ· JEPA Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ action-head, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Enhancing Vision-Language-Action Learning with VLA-JEPA",
                    "desc": "VLA-JEPA is a new framework designed to improve how machines learn to perform tasks that involve both vision and action. It addresses common problems in existing methods, such as being overly influenced by irrelevant visual details, by using a technique called leakage-free state prediction. This means that the model learns to predict future states based only on current observations, avoiding the pitfalls of using future information directly. As a result, VLA-JEPA enhances the model's ability to generalize and perform robustly in various manipulation tasks, making it simpler and more effective than previous approaches."
                },
                "zh": {
                    "title": "VLA-JEPAï¼šæå‡è§†è§‰-è¯­è¨€-åŠ¨ä½œå­¦ä¹ çš„é²æ£’æ€§ä¸æ³›åŒ–èƒ½åŠ›",
                    "desc": "VLA-JEPAæ˜¯ä¸€ç§JEPAé£æ ¼çš„é¢„è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åœ¨æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œæ— æ³„æ¼çŠ¶æ€é¢„æµ‹æ¥æ”¹å–„è§†è§‰-è¯­è¨€-åŠ¨ä½œç­–ç•¥å­¦ä¹ ã€‚è¿™ç§æ–¹æ³•å¢å¼ºäº†åœ¨æ“ä½œä»»åŠ¡ä¸­çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚VLA-JEPAé€šè¿‡è®¾è®¡é¿å…äº†å½“å‰æ½œåœ¨åŠ¨ä½œç›®æ ‡çš„ç¼ºé™·ï¼Œç¡®ä¿æœªæ¥ä¿¡æ¯ä»…ä½œä¸ºç›‘ç£ç›®æ ‡ï¼Œè€Œä¸æ˜¯è¾“å…¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVLA-JEPAåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ç›¸è¾ƒäºç°æœ‰æ–¹æ³•å–å¾—äº†ä¸€è‡´çš„æ€§èƒ½æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.00462",
            "title": "LatentLens: Revealing Highly Interpretable Visual Tokens in LLMs",
            "url": "https://huggingface.co/papers/2602.00462",
            "abstract": "LatentLens enables interpretation of visual token representations in vision-language models by comparing them to contextualized textual representations, revealing that visual tokens are more interpretable than previously thought.  \t\t\t\t\tAI-generated summary \t\t\t\t Transforming a large language model (LLM) into a Vision-Language Model (VLM) can be achieved by mapping the visual tokens from a vision encoder into the embedding space of an LLM. Intriguingly, this mapping can be as simple as a shallow MLP transformation. To understand why LLMs can so readily process visual tokens, we need interpretability methods that reveal what is encoded in the visual token representations at every layer of LLM processing. In this work, we introduce LatentLens, a novel approach for mapping latent representations to descriptions in natural language. LatentLens works by encoding a large text corpus and storing contextualized token representations for each token in that corpus. Visual token representations are then compared to their contextualized textual representations, with the top-k nearest neighbor representations providing descriptions of the visual token. We evaluate this method on 10 different VLMs, showing that commonly used methods, such as LogitLens, substantially underestimate the interpretability of visual tokens. With LatentLens instead, the majority of visual tokens are interpretable across all studied models and all layers. Qualitatively, we show that the descriptions produced by LatentLens are semantically meaningful and provide more fine-grained interpretations for humans compared to individual tokens. More broadly, our findings contribute new evidence on the alignment between vision and language representations, opening up new directions for analyzing latent representations.",
            "score": 12,
            "issue_id": 1006,
            "pub_date": "2026-01-31",
            "pub_date_card": {
                "ru": "31 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 31",
                "zh": "1æœˆ31æ—¥"
            },
            "hash": "5876f318f2b0dc4e",
            "authors": [
                "Benno Krojer",
                "Shravan Nayak",
                "Oscar MaÃ±as",
                "Vaibhav Adlakha",
                "Desmond Elliott",
                "Siva Reddy",
                "Marius Mosbach"
            ],
            "affiliations": [
                "Canada CIFAR AI Chair",
                "McGill University",
                "Mila Quebec AI Institute",
                "University of Copenhagen",
                "UniversitÃ© de MontrÃ©al"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.00462.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#interpretability",
                    "#multimodal"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°Ñ ÑĞ¼Ñ‹ÑĞ» Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ LatentLens â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ° Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¸Ñ… ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ÑƒĞ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞº k Ğ±Ğ»Ğ¸Ğ¶Ğ°Ğ¹ÑˆĞ¸Ñ… ÑĞ¾ÑĞµĞ´ĞµĞ¹ Ğ´Ğ»Ñ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° 10 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… VLM Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº LogitLens, ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ½ĞµĞ´Ğ¾Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ğ»Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ½Ğ° Ğ²ÑĞµÑ… ÑĞ»Ğ¾ÑÑ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Unlocking the Interpretability of Visual Tokens with LatentLens",
                    "desc": "LatentLens is a new method that helps us understand visual token representations in vision-language models by comparing them to textual representations. It shows that visual tokens are easier to interpret than we previously thought. By using a simple mapping technique, LatentLens allows us to find meaningful descriptions for visual tokens based on their closest textual counterparts. This research reveals that many visual tokens are interpretable across various models and layers, enhancing our understanding of how vision and language are connected."
                },
                "zh": {
                    "title": "æå‡è§†è§‰æ ‡è®°å¯è§£é‡Šæ€§çš„LatentLens",
                    "desc": "LatentLens æ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œç”¨äºè§£é‡Šè§†è§‰-è¯­è¨€æ¨¡å‹ä¸­çš„è§†è§‰æ ‡è®°è¡¨ç¤ºã€‚å®ƒé€šè¿‡å°†è§†è§‰æ ‡è®°ä¸ä¸Šä¸‹æ–‡åŒ–çš„æ–‡æœ¬è¡¨ç¤ºè¿›è¡Œæ¯”è¾ƒï¼Œæ­ç¤ºäº†è§†è§‰æ ‡è®°çš„å¯è§£é‡Šæ€§æ¯”ä¹‹å‰è®¤ä¸ºçš„æ›´é«˜ã€‚è¯¥æ–¹æ³•é€šè¿‡ç¼–ç å¤§é‡æ–‡æœ¬è¯­æ–™åº“ï¼Œå­˜å‚¨æ¯ä¸ªæ ‡è®°çš„ä¸Šä¸‹æ–‡åŒ–è¡¨ç¤ºï¼Œå¹¶åˆ©ç”¨æœ€è¿‘é‚»æ–¹æ³•ä¸ºè§†è§‰æ ‡è®°æä¾›æè¿°ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼ŒLatentLens èƒ½å¤Ÿåœ¨å¤šä¸ªè§†è§‰-è¯­è¨€æ¨¡å‹ä¸­æœ‰æ•ˆåœ°æé«˜è§†è§‰æ ‡è®°çš„å¯è§£é‡Šæ€§ï¼Œæ¨åŠ¨äº†è§†è§‰ä¸è¯­è¨€è¡¨ç¤ºä¹‹é—´çš„å¯¹é½ç ”ç©¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.08847",
            "title": "Dr. MAS: Stable Reinforcement Learning for Multi-Agent LLM Systems",
            "url": "https://huggingface.co/papers/2602.08847",
            "abstract": "Multi-agent large language model systems face training instability in reinforcement learning due to global normalization mismatches, which is addressed by Dr. MAS through agent-specific advantage normalization and enhanced training stability.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-agent LLM systems enable advanced reasoning and tool use via role specialization, yet reliable reinforcement learning (RL) post-training for such systems remains difficult. In this work, we theoretically pinpoint a key reason for training instability when extending group-based RL to multi-agent LLM systems. We show that under GRPO-style optimization, a global normalization baseline may deviate from diverse agents' reward distributions, which ultimately leads to gradient-norm instability. Based on this finding, we propose Dr. MAS, a simple and stable RL training recipe for multi-agent LLM systems. Dr. MAS uses an agent-wise remedy: normalizing advantages per agent using each agent's own reward statistics, which calibrates gradient scales and dramatically stabilizes training, both theoretically and empirically. Beyond the algorithm, Dr. MAS provides an end-to-end RL training framework for multi-agent LLM systems, supporting scalable orchestration, flexible per-agent LLM serving and optimization configs, and shared resource scheduling of LLM actor backends. We evaluate Dr. MAS on multi-agent math reasoning and multi-turn search benchmarks using Qwen2.5 and Qwen3 series models. Dr. MAS achieves clear gains over vanilla GRPO (e.g., +5.6\\% avg@16 and +4.6\\% pass@16 on math, and +15.2\\% avg@16 and +13.1\\% pass@16 on search) while largely eliminating gradient spikes. Moreover, it remains highly effective under heterogeneous agent-model assignments while improving efficiency.",
            "score": 11,
            "issue_id": 999,
            "pub_date": "2026-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "bb95bc2a1e953137",
            "authors": [
                "Lang Feng",
                "Longtao Zheng",
                "Shuo He",
                "Fuxiang Zhang",
                "Bo An"
            ],
            "affiliations": [
                "Nanyang Technological University, Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.08847.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#alignment",
                    "#training",
                    "#rl",
                    "#agents",
                    "#math",
                    "#optimization"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… LLM Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ³ĞµĞ½Ñ‚-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½ÑƒÑ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ»Ğ¸Ğ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ GRPO-ÑÑ‚Ğ¸Ğ»Ñ Ñ€Ğ°ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Dr. MAS Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½ÑƒÑ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ², ĞºĞ°Ğ»Ğ¸Ğ±Ñ€ÑƒÑ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ñ‹ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ ÑĞºĞ°Ñ‡ĞºĞ¾Ğ² Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Stabilizing Multi-Agent Learning with Dr. MAS",
                    "desc": "This paper addresses the challenges of training multi-agent large language models (LLMs) using reinforcement learning (RL), particularly focusing on the instability caused by global normalization mismatches. The authors introduce Dr. MAS, a novel approach that normalizes advantages for each agent based on their individual reward statistics, which helps stabilize the training process. By employing this agent-specific normalization, Dr. MAS significantly reduces gradient-norm instability and enhances overall training performance. The framework is evaluated on various benchmarks, demonstrating substantial improvements over traditional methods while maintaining efficiency across diverse agent configurations."
                },
                "zh": {
                    "title": "æå‡å¤šæ™ºèƒ½ä½“ç³»ç»Ÿè®­ç»ƒç¨³å®šæ€§çš„å…³é”®æ–¹æ³•",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†å¤šæ™ºèƒ½ä½“å¤§è¯­è¨€æ¨¡å‹ç³»ç»Ÿåœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„è®­ç»ƒä¸ç¨³å®šæ€§é—®é¢˜ï¼Œä¸»è¦åŸå› æ˜¯å…¨å±€å½’ä¸€åŒ–ä¸ä¸åŒæ™ºèƒ½ä½“çš„å¥–åŠ±åˆ†å¸ƒä¸åŒ¹é…ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†Dr. MASæ–¹æ³•ï¼Œé€šè¿‡å¯¹æ¯ä¸ªæ™ºèƒ½ä½“çš„å¥–åŠ±ç»Ÿè®¡è¿›è¡Œä¼˜åŠ¿å½’ä¸€åŒ–ï¼Œæ˜¾è‘—æé«˜äº†è®­ç»ƒçš„ç¨³å®šæ€§ã€‚è¯¥æ–¹æ³•ä¸ä»…æä¾›äº†ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¡†æ¶ï¼Œè¿˜æ”¯æŒå¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„å¯æ‰©å±•ç¼–æ’å’Œçµæ´»é…ç½®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDr. MASåœ¨æ•°å­¦æ¨ç†å’Œå¤šè½®æœç´¢åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—æå‡äº†æ€§èƒ½å¹¶å‡å°‘äº†æ¢¯åº¦æ³¢åŠ¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.06820",
            "title": "ScaleEnv: Scaling Environment Synthesis from Scratch for Generalist Interactive Tool-Use Agent Training",
            "url": "https://huggingface.co/papers/2602.06820",
            "abstract": "ScaleEnv framework generates interactive environments from scratch to improve agent generalization through diverse domain scaling and verified task completion.  \t\t\t\t\tAI-generated summary \t\t\t\t Training generalist agents capable of adapting to diverse scenarios requires interactive environments for self-exploration. However, interactive environments remain critically scarce, and existing synthesis methods suffer from significant limitations regarding environmental diversity and scalability. To address these challenges, we introduce ScaleEnv, a framework that constructs fully interactive environments and verifiable tasks entirely from scratch. Specifically, ScaleEnv ensures environment reliability through procedural testing, and guarantees task completeness and solvability via tool dependency graph expansion and executable action verification. By enabling agents to learn through exploration within ScaleEnv, we demonstrate significant performance improvements on unseen, multi-turn tool-use benchmarks such as Ï„^2-Bench and VitaBench, highlighting strong generalization capabilities. Furthermore, we investigate the relationship between increasing number of domains and model generalization performance, providing empirical evidence that scaling environmental diversity is critical for robust agent learning.",
            "score": 11,
            "issue_id": 1000,
            "pub_date": "2026-02-06",
            "pub_date_card": {
                "ru": "6 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 6",
                "zh": "2æœˆ6æ—¥"
            },
            "hash": "c24b62f0ace09935",
            "authors": [
                "Dunwei Tu",
                "Hongyan Hao",
                "Hansi Yang",
                "Yihao Chen",
                "Yi-Kai Zhang",
                "Zhikang Xia",
                "Yu Yang",
                "Yueqing Sun",
                "Xingchen Liu",
                "Furao Shen",
                "Qi Gu",
                "Hui Su",
                "Xunliang Cai"
            ],
            "affiliations": [
                "Institute of Computer Science and Technology, Harbin Institute of Technology (Shenzhen), Shenzhen, China",
                "Meituan, Beijing, China",
                "National Key Laboratory for Novel Software Technology, Nanjing University",
                "School of Artificial Intelligence, Nanjing University, Nanjing, China",
                "School of Statistics, East China Normal University, Shanghai, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.06820.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ—ï¸",
                "ru": {
                    "title": "ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "ScaleEnv â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ÑÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ñ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞµĞ¹ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒÑ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğ½Ğ¾Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ñƒ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³Ñ€Ğ°Ñ„Ğ° Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞĞ³ĞµĞ½Ñ‚Ñ‹ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ÑÑ… Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ï„Â²-Bench Ğ¸ VitaBench. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸, Ñ‡Ñ‚Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ² ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸."
                },
                "en": {
                    "title": "Empowering Agents with Diverse Interactive Environments",
                    "desc": "The ScaleEnv framework creates interactive environments from the ground up to enhance the generalization abilities of agents in machine learning. It addresses the lack of diverse and scalable environments by generating fully interactive settings and verifiable tasks. The framework employs procedural testing to ensure reliability and uses tool dependency graphs to confirm task completeness and solvability. By allowing agents to explore these environments, ScaleEnv significantly boosts their performance on complex benchmarks, demonstrating the importance of diverse domains for effective learning."
                },
                "zh": {
                    "title": "ScaleEnvï¼šæå‡æ™ºèƒ½ä½“æ³›åŒ–èƒ½åŠ›çš„äº’åŠ¨ç¯å¢ƒç”Ÿæˆæ¡†æ¶",
                    "desc": "ScaleEnvæ¡†æ¶é€šè¿‡ä»é›¶å¼€å§‹ç”Ÿæˆäº’åŠ¨ç¯å¢ƒï¼Œæ—¨åœ¨æé«˜æ™ºèƒ½ä½“çš„æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ¡†æ¶è§£å†³äº†ç°æœ‰ç¯å¢ƒåˆæˆæ–¹æ³•åœ¨å¤šæ ·æ€§å’Œå¯æ‰©å±•æ€§æ–¹é¢çš„ä¸è¶³ã€‚é€šè¿‡ç¨‹åºæµ‹è¯•ç¡®ä¿ç¯å¢ƒçš„å¯é æ€§ï¼Œå¹¶é€šè¿‡å·¥å…·ä¾èµ–å›¾æ‰©å±•å’Œå¯æ‰§è¡ŒåŠ¨ä½œéªŒè¯æ¥ä¿è¯ä»»åŠ¡çš„å®Œæ•´æ€§å’Œå¯è§£æ€§ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨ScaleEnvè¿›è¡Œæ¢ç´¢å­¦ä¹ çš„æ™ºèƒ½ä½“åœ¨æœªè§è¿‡çš„å¤šè½®å·¥å…·ä½¿ç”¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°æ˜¾è‘—æå‡ï¼Œæ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.10102",
            "title": "VideoWorld 2: Learning Transferable Knowledge from Real-world Videos",
            "url": "https://huggingface.co/papers/2602.10102",
            "abstract": "VideoWorld 2 enables transferable knowledge learning from raw videos through a dynamic-enhanced Latent Dynamics Model that decouples action dynamics from visual appearance, achieving improved task performance and long-horizon reasoning in real-world applications.  \t\t\t\t\tAI-generated summary \t\t\t\t Learning transferable knowledge from unlabeled video data and applying it in new environments is a fundamental capability of intelligent agents. This work presents VideoWorld 2, which extends VideoWorld and offers the first investigation into learning transferable knowledge directly from raw real-world videos. At its core, VideoWorld 2 introduces a dynamic-enhanced Latent Dynamics Model (dLDM) that decouples action dynamics from visual appearance: a pretrained video diffusion model handles visual appearance modeling, enabling the dLDM to learn latent codes that focus on compact and meaningful task-related dynamics. These latent codes are then modeled autoregressively to learn task policies and support long-horizon reasoning. We evaluate VideoWorld 2 on challenging real-world handcraft making tasks, where prior video generation and latent-dynamics models struggle to operate reliably. Remarkably, VideoWorld 2 achieves up to 70% improvement in task success rate and produces coherent long execution videos. In robotics, we show that VideoWorld 2 can acquire effective manipulation knowledge from the Open-X dataset, which substantially improves task performance on CALVIN. This study reveals the potential of learning transferable world knowledge directly from raw videos, with all code, data, and models to be open-sourced for further research.",
            "score": 10,
            "issue_id": 998,
            "pub_date": "2026-02-10",
            "pub_date_card": {
                "ru": "10 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 10",
                "zh": "2æœˆ10æ—¥"
            },
            "hash": "16776e7a28305864",
            "authors": [
                "Zhongwei Ren",
                "Yunchao Wei",
                "Xiao Yu",
                "Guixun Luo",
                "Yao Zhao",
                "Bingyi Kang",
                "Jiashi Feng",
                "Xiaojie Jin"
            ],
            "affiliations": [
                "Beijing Jiaotong University",
                "ByteDance Seed"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.10102.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#agents",
                    "#reasoning",
                    "#video",
                    "#robotics",
                    "#training",
                    "#transfer_learning",
                    "#diffusion"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğº Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼: Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ¸ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ° Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ",
                    "desc": "VideoWorld 2 â€” ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¼ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼ Ğ¸Ğ· ÑÑ‹Ñ€Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ (dLDM). ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ: Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ dLDM ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ¾Ñ‡Ğ¸Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ñ… Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ñ… ĞºĞ¾Ğ´Ğ¾Ğ², ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ˜Ğ·ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ ĞºĞ¾Ğ´Ñ‹ Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ°Ğ¼ Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ… â€” Ğ½Ğ° 70% Ğ²Ñ‹ÑˆĞµ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ."
                },
                "en": {
                    "title": "Learning from Raw Videos: Transferable Knowledge Unleashed!",
                    "desc": "VideoWorld 2 is a machine learning framework that learns useful knowledge from raw videos without needing labels. It uses a dynamic-enhanced Latent Dynamics Model (dLDM) to separate the understanding of actions from how things look in the videos. This allows the model to focus on important task-related dynamics while a pretrained video diffusion model takes care of visual details. The results show significant improvements in task performance, especially in complex real-world scenarios like handcraft making and robotics."
                },
                "zh": {
                    "title": "ä»åŸå§‹è§†é¢‘ä¸­å­¦ä¹ å¯è½¬ç§»çŸ¥è¯†çš„æ½œåŠ›",
                    "desc": "VideoWorld 2 æ˜¯ä¸€ä¸ªæ–°æ¨¡å‹ï¼Œèƒ½å¤Ÿä»åŸå§‹è§†é¢‘ä¸­å­¦ä¹ å¯è½¬ç§»çš„çŸ¥è¯†ã€‚å®ƒé€šè¿‡åŠ¨æ€å¢å¼ºçš„æ½œåœ¨åŠ¨æ€æ¨¡å‹ï¼ˆdLDMï¼‰å°†åŠ¨ä½œåŠ¨æ€ä¸è§†è§‰å¤–è§‚åˆ†ç¦»ï¼Œä»è€Œæé«˜ä»»åŠ¡æ€§èƒ½å’Œé•¿æ—¶é—´æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œçš„æ‰‹å·¥åˆ¶ä½œä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼ŒæˆåŠŸç‡æé«˜äº†70%ã€‚è¿™é¡¹ç ”ç©¶å±•ç¤ºäº†ç›´æ¥ä»åŸå§‹è§†é¢‘ä¸­å­¦ä¹ å¯è½¬ç§»ä¸–ç•ŒçŸ¥è¯†çš„æ½œåŠ›ï¼Œå¹¶å°†æ‰€æœ‰ä»£ç ã€æ•°æ®å’Œæ¨¡å‹å¼€æºä»¥ä¾›è¿›ä¸€æ­¥ç ”ç©¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.01244",
            "title": "Large-Scale Terminal Agentic Trajectory Generation from Dockerized Environments",
            "url": "https://huggingface.co/papers/2602.01244",
            "abstract": "A scalable pipeline called TerminalTraj addresses challenges in creating high-quality terminal trajectories for training agentic models by filtering repositories, generating Docker-aligned task instances, and synthesizing executable agent trajectories across multiple domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Training agentic models for terminal-based tasks critically depends on high-quality terminal trajectories that capture realistic long-horizon interactions across diverse domains. However, constructing such data at scale remains challenging due to two key requirements: \\emph{Executability}, since each instance requires a suitable and often distinct Docker environment; and \\emph{Verifiability}, because heterogeneous task outputs preclude unified, standardized verification. To address these challenges, we propose TerminalTraj, a scalable pipeline that (i) filters high-quality repositories to construct Dockerized execution environments, (ii) generates Docker-aligned task instances, and (iii) synthesizes agent trajectories with executable validation code. Using TerminalTraj, we curate 32K Docker images and generate 50,733 verified terminal trajectories across eight domains. Models trained on this data with the Qwen2.5-Coder backbone achieve consistent performance improvements on TerminalBench (TB), with gains of up to 20\\% on TB~1.0 and 10\\% on TB~2.0 over their respective backbones. Notably, TerminalTraj-32B achieves strong performance among models with fewer than 100B parameters, reaching 35.30\\% on TB~1.0 and 22.00\\% on TB~2.0, and demonstrates improved test-time scaling behavior. All code and data are available at https://github.com/Wusiwei0410/TerminalTraj.",
            "score": 10,
            "issue_id": 1006,
            "pub_date": "2026-02-01",
            "pub_date_card": {
                "ru": "1 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 1",
                "zh": "2æœˆ1æ—¥"
            },
            "hash": "b5329b55c2bf1556",
            "authors": [
                "Siwei Wu",
                "Yizhi Li",
                "Yuyang Song",
                "Wei Zhang",
                "Yang Wang",
                "Riza Batista-Navarro",
                "Xian Yang",
                "Mingjie Tang",
                "Bryan Dai",
                "Jian Yang",
                "Chenghua Lin"
            ],
            "affiliations": [
                "Beihang University",
                "IQuest Research",
                "Multimodal",
                "Projection Research Community",
                "Sichuan University",
                "University of Manchester"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.01244.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ³",
                "ru": {
                    "title": "ĞÑ‚ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ñƒ: ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹",
                    "desc": "TerminalTraj â€” ÑÑ‚Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ñ Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ°Ğ»Ğ¾Ğ¼, Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ agentic Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ´Ğ²Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹: Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ² Ğ¸Ğ·Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Docker ÑÑ€ĞµĞ´Ğµ Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ñ‚Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸ĞµĞ² Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ 50,733 Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ½Ğ° 32K Docker Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ°Ñ… Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ»Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° ÑÑ‚Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° TerminalTraj, Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ°Ñ…, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚Ğ° Ğ´Ğ¾ 20% Ğ½Ğ° TerminalBench~1.0."
                },
                "en": {
                    "title": "Streamlining Agentic Model Training with TerminalTraj",
                    "desc": "The paper introduces TerminalTraj, a scalable pipeline designed to create high-quality terminal trajectories for training agentic models. It addresses the challenges of executability and verifiability by filtering repositories to build Docker environments, generating task instances aligned with Docker, and synthesizing executable agent trajectories. The pipeline successfully curates 32,000 Docker images and produces over 50,000 verified terminal trajectories across eight different domains. Models trained using this data show significant performance improvements, achieving up to 20% gains on benchmark tasks, demonstrating the effectiveness of the TerminalTraj approach."
                },
                "zh": {
                    "title": "é«˜æ•ˆç”Ÿæˆç»ˆç«¯è½¨è¿¹çš„å¯æ‰©å±•ç®¡é“",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºTerminalTrajçš„å¯æ‰©å±•ç®¡é“ï¼Œæ—¨åœ¨è§£å†³ä¸ºè®­ç»ƒæ™ºèƒ½æ¨¡å‹åˆ›å»ºé«˜è´¨é‡ç»ˆç«¯è½¨è¿¹çš„æŒ‘æˆ˜ã€‚è¯¥ç®¡é“é€šè¿‡è¿‡æ»¤é«˜è´¨é‡çš„ä»£ç åº“ã€ç”Ÿæˆä¸Dockerå¯¹é½çš„ä»»åŠ¡å®ä¾‹ï¼Œå¹¶åœ¨å¤šä¸ªé¢†åŸŸåˆæˆå¯æ‰§è¡Œçš„æ™ºèƒ½ä½“è½¨è¿¹æ¥å®ç°ã€‚TerminalTrajèƒ½å¤Ÿæ„å»ºDockeråŒ–çš„æ‰§è¡Œç¯å¢ƒï¼Œå¹¶ç”Ÿæˆç»è¿‡éªŒè¯çš„ç»ˆç«¯è½¨è¿¹ï¼Œæœ€ç»ˆç”Ÿæˆäº†32,000ä¸ªDockeré•œåƒå’Œ50,733ä¸ªéªŒè¯è¿‡çš„ç»ˆç«¯è½¨è¿¹ã€‚ä½¿ç”¨è¿™äº›æ•°æ®è®­ç»ƒçš„æ¨¡å‹åœ¨TerminalBenchä¸Šè¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.09439",
            "title": "Fine-T2I: An Open, Large-Scale, and Diverse Dataset for High-Quality T2I Fine-Tuning",
            "url": "https://huggingface.co/papers/2602.09439",
            "abstract": "A large-scale, high-quality, and fully open dataset for text-to-image fine-tuning is presented, featuring over 6 million text-image pairs with rigorous filtering for alignment and quality across multiple task combinations and visual styles.  \t\t\t\t\tAI-generated summary \t\t\t\t High-quality and open datasets remain a major bottleneck for text-to-image (T2I) fine-tuning. Despite rapid progress in model architectures and training pipelines, most publicly available fine-tuning datasets suffer from low resolution, poor text-image alignment, or limited diversity, resulting in a clear performance gap between open research models and enterprise-grade models. In this work, we present Fine-T2I, a large-scale, high-quality, and fully open dataset for T2I fine-tuning. Fine-T2I spans 10 task combinations, 32 prompt categories, 11 visual styles, and 5 prompt templates, and combines synthetic images generated by strong modern models with carefully curated real images from professional photographers. All samples are rigorously filtered for text-image alignment, visual fidelity, and prompt quality, with over 95% of initial candidates removed. The final dataset contains over 6 million text-image pairs, around 2 TB on disk, approaching the scale of pretraining datasets while maintaining fine-tuning-level quality. Across a diverse set of pretrained diffusion and autoregressive models, fine-tuning on Fine-T2I consistently improves both generation quality and instruction adherence, as validated by human evaluation, visual comparison, and automatic metrics. We release Fine-T2I under an open license to help close the data gap in T2I fine-tuning in the open community.",
            "score": 9,
            "issue_id": 999,
            "pub_date": "2026-02-10",
            "pub_date_card": {
                "ru": "10 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 10",
                "zh": "2æœˆ10æ—¥"
            },
            "hash": "2956b71525931922",
            "authors": [
                "Xu Ma",
                "Yitian Zhang",
                "Qihua Dong",
                "Yun Fu"
            ],
            "affiliations": [
                "Department of Electrical & Computer Engineering, Northeastern University, Boston"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.09439.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#open_source",
                    "#synthetic"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ’Ñ‹ÑĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ñ„Ğ°Ğ¹Ğ½-Ñ‚ÑĞ½Ğ¸Ğ½Ğ³Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‚ĞµĞºÑÑ‚-Ğ²-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Fine-T2I Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 6 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€ Ñ‚ĞµĞºÑÑ‚-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ Ñ„Ğ°Ğ¹Ğ½-Ñ‚ÑĞ½Ğ¸Ğ½Ğ³Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ„Ğ¾Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»Ğ¾Ğ², Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğµ 10 ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡, 32 ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ¸ 11 Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ‚Ğ¸Ğ»ĞµĞ¹. Ğ’ÑĞµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¿Ğ¾Ğ´Ğ²ĞµÑ€Ğ³Ğ½ÑƒÑ‚Ñ‹ ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾Ğ¹ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‡ĞµÑ‚ĞºĞ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²ĞµĞ»Ğ¾ Ğº ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ 95% Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ². Ğ¤Ğ°Ğ¹Ğ½-Ñ‚ÑĞ½Ğ¸Ğ½Ğ³ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ğº ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ½Ğ¾ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼."
                },
                "en": {
                    "title": "Unlocking High-Quality Text-to-Image Fine-Tuning with Fine-T2I",
                    "desc": "This paper introduces Fine-T2I, a comprehensive dataset designed for text-to-image (T2I) fine-tuning, consisting of over 6 million carefully curated text-image pairs. The dataset addresses common issues in existing datasets, such as low resolution and poor alignment, by ensuring high quality and diversity across various tasks and visual styles. By combining synthetic images from advanced models with real images from professional photographers, Fine-T2I achieves a balance between scale and quality, making it suitable for fine-tuning. The results show that models fine-tuned on this dataset exhibit significant improvements in generation quality and adherence to instructions, demonstrating its potential to enhance T2I applications in the research community."
                },
                "zh": {
                    "title": "é«˜è´¨é‡å¼€æ”¾æ•°æ®é›†åŠ©åŠ›æ–‡æœ¬åˆ°å›¾åƒå¾®è°ƒ",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªå¤§è§„æ¨¡ã€é«˜è´¨é‡ä¸”å®Œå…¨å¼€æ”¾çš„æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰å¾®è°ƒæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡600ä¸‡ä¸ªæ–‡æœ¬-å›¾åƒå¯¹ï¼Œç»è¿‡ä¸¥æ ¼ç­›é€‰ä»¥ç¡®ä¿å¯¹é½å’Œè´¨é‡ã€‚ç°æœ‰çš„å…¬å¼€å¾®è°ƒæ•°æ®é›†é€šå¸¸å­˜åœ¨ä½åˆ†è¾¨ç‡ã€æ–‡æœ¬-å›¾åƒå¯¹é½å·®å·®æˆ–å¤šæ ·æ€§ä¸è¶³çš„é—®é¢˜ï¼Œå¯¼è‡´å¼€æ”¾ç ”ç©¶æ¨¡å‹ä¸ä¼ä¸šçº§æ¨¡å‹ä¹‹é—´å­˜åœ¨æ˜æ˜¾çš„æ€§èƒ½å·®è·ã€‚Fine-T2Iæ•°æ®é›†æ¶µç›–10ç§ä»»åŠ¡ç»„åˆã€32ä¸ªæç¤ºç±»åˆ«ã€11ç§è§†è§‰é£æ ¼å’Œ5ç§æç¤ºæ¨¡æ¿ï¼Œç»“åˆäº†ç°ä»£å¼ºå¤§æ¨¡å‹ç”Ÿæˆçš„åˆæˆå›¾åƒå’Œä¸“ä¸šæ‘„å½±å¸ˆç²¾å¿ƒç­–åˆ’çš„çœŸå®å›¾åƒã€‚é€šè¿‡åœ¨Fine-T2Iä¸Šè¿›è¡Œå¾®è°ƒï¼Œé¢„è®­ç»ƒçš„æ‰©æ•£å’Œè‡ªå›å½’æ¨¡å‹åœ¨ç”Ÿæˆè´¨é‡å’ŒæŒ‡ä»¤éµå¾ªæ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ï¼Œä¸”è¯¥æ•°æ®é›†ä»¥å¼€æ”¾è®¸å¯è¯å‘å¸ƒï¼Œæ—¨åœ¨å¸®åŠ©ç¼©å°å¼€æ”¾ç¤¾åŒºä¸­T2Iå¾®è°ƒçš„æ•°æ®å·®è·ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.09017",
            "title": "Contact-Anchored Policies: Contact Conditioning Creates Strong Robot Utility Models",
            "url": "https://huggingface.co/papers/2602.09017",
            "abstract": "Contact-Anchored Policies replace language conditioning with physical contact points and use modular utility models for robust manipulation, achieving superior zero-shot performance with minimal demonstration data through real-to-sim iteration cycles.  \t\t\t\t\tAI-generated summary \t\t\t\t The prevalent paradigm in robot learning attempts to generalize across environments, embodiments, and tasks with language prompts at runtime. A fundamental tension limits this approach: language is often too abstract to guide the concrete physical understanding required for robust manipulation. In this work, we introduce Contact-Anchored Policies (CAP), which replace language conditioning with points of physical contact in space. Simultaneously, we structure CAP as a library of modular utility models rather than a monolithic generalist policy. This factorization allows us to implement a real-to-sim iteration cycle: we build EgoGym, a lightweight simulation benchmark, to rapidly identify failure modes and refine our models and datasets prior to real-world deployment. We show that by conditioning on contact and iterating via simulation, CAP generalizes to novel environments and embodiments out of the box on three fundamental manipulation skills while using only 23 hours of demonstration data, and outperforms large, state-of-the-art VLAs in zero-shot evaluations by 56%. All model checkpoints, codebase, hardware, simulation, and datasets will be open-sourced. Project page: https://cap-policy.github.io/",
            "score": 9,
            "issue_id": 1007,
            "pub_date": "2026-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "9cca690eda42c150",
            "authors": [
                "Zichen Jeff Cui",
                "Omar Rayyan",
                "Haritheja Etukuru",
                "Bowen Tan",
                "Zavier Andrianarivo",
                "Zicheng Teng",
                "Yihang Zhou",
                "Krish Mehta",
                "Nicholas Wojno",
                "Kevin Yuanbo Wu",
                "Manan H Anjaria",
                "Ziyuan Wu",
                "Manrong Mao",
                "Guangxun Zhang",
                "Binit Shah",
                "Yejin Kim",
                "Soumith Chintala",
                "Lerrel Pinto",
                "Nur Muhammad Mahi Shafiullah"
            ],
            "affiliations": [
                "Ai2",
                "Hello Robot Inc.",
                "New York University",
                "University of California, Berkeley",
                "University of California, Los Angeles",
                "University of Waterloo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.09017.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#open_source",
                    "#dataset",
                    "#benchmark",
                    "#robotics"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞÑ‚ ÑĞ·Ñ‹ĞºĞ° Ğº ĞºĞ°ÑĞ°Ğ½Ğ¸Ñ: Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ° Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ñ‚Ğ¾Ñ‡ĞºĞ¸",
                    "desc": "Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Contact-Anchored Policies (CAP) â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ·Ğ°Ğ¼ĞµĞ½ÑĞµÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğµ ĞºĞ¾Ğ½Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ‚Ğ¾Ñ‡ĞºĞ¸ ĞºĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ğ° Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ½Ğ¾Ğ»Ğ¸Ñ‚Ğ½Ğ¾Ğ¹ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€ÑƒÑÑ‚ CAP ĞºĞ°Ğº Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºÑƒ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ñ… ÑƒÑ‚Ğ¸Ğ»Ğ¸Ñ‚-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº EgoGym Ğ´Ğ»Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¸ ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿ĞµÑ€ĞµĞ´ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ½ÑƒĞ»ĞµĞ²Ñ‹Ñ… Ğ²Ñ‹ÑÑ‚Ñ€ĞµĞ»Ğ°Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ 23 Ñ‡Ğ°ÑĞ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ VLA-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° 56% Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼Ğ¾Ñ€Ñ„Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Robot Learning with Contact Points",
                    "desc": "This paper presents Contact-Anchored Policies (CAP), a new approach to robot learning that uses physical contact points instead of language prompts for guiding manipulation tasks. By structuring CAP as a collection of modular utility models, the authors enable more effective generalization across different environments and tasks. The method incorporates a real-to-sim iteration cycle using a simulation benchmark called EgoGym, which helps identify and address failure modes before real-world application. The results demonstrate that CAP achieves superior performance in zero-shot scenarios with minimal demonstration data, outperforming existing state-of-the-art models significantly."
                },
                "zh": {
                    "title": "æ¥è§¦é”šå®šç­–ç•¥ï¼šè¶…è¶Šè¯­è¨€çš„æœºå™¨äººæ“ä½œæ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æœºå™¨äººå­¦ä¹ æ–¹æ³•ï¼Œç§°ä¸ºæ¥è§¦é”šå®šç­–ç•¥ï¼ˆContact-Anchored Policiesï¼ŒCAPï¼‰ï¼Œå®ƒç”¨ç‰©ç†æ¥è§¦ç‚¹æ›¿ä»£äº†è¯­è¨€æç¤ºï¼Œä»¥å®ç°æ›´ç¨³å¥çš„æ“ä½œã€‚é€šè¿‡å°†CAPæ„å»ºä¸ºæ¨¡å—åŒ–çš„æ•ˆç”¨æ¨¡å‹åº“ï¼Œè€Œä¸æ˜¯å•ä¸€çš„é€šç”¨ç­–ç•¥ï¼Œç ”ç©¶è€…èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°è¿›è¡ŒçœŸå®ä¸æ¨¡æ‹Ÿçš„è¿­ä»£å¾ªç¯ã€‚è¯¥æ–¹æ³•åœ¨ä»…ä½¿ç”¨23å°æ—¶çš„æ¼”ç¤ºæ•°æ®çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿåœ¨ä¸‰ç§åŸºæœ¬æ“ä½œæŠ€èƒ½ä¸Šå®ç°é›¶-shotæ€§èƒ½çš„æ˜¾è‘—æå‡ï¼Œè¶…è¶Šäº†ç°æœ‰çš„å…ˆè¿›è§†è§‰è¯­è¨€æ¨¡å‹ã€‚æ‰€æœ‰æ¨¡å‹æ£€æŸ¥ç‚¹ã€ä»£ç åº“ã€ç¡¬ä»¶ã€æ¨¡æ‹Ÿå’Œæ•°æ®é›†éƒ½å°†å¼€æºï¼Œä¿ƒè¿›è¿›ä¸€æ­¥çš„ç ”ç©¶å’Œåº”ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.08382",
            "title": "Dynamic Long Context Reasoning over Compressed Memory via End-to-End Reinforcement Learning",
            "url": "https://huggingface.co/papers/2602.08382",
            "abstract": "A cognitive-inspired framework for long-context language modeling uses chunk-wise compression and selective memory recall to improve efficiency and performance over traditional approaches.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) face significant challenges in long-context processing, including quadratic computational costs, information forgetting, and the context fragmentation inherent in retrieval-augmented generation (RAG). We propose a cognitively inspired framework for efficient long-context inference based on chunk-wise compression and selective memory recall, rather than processing all raw tokens. The framework segments long inputs into chunks and encodes each chunk into compressed memory representations using a learned compressor. A gating module dynamically selects relevant memory blocks, which are then iteratively processed by a reasoning module with an evolving working memory to solve downstream tasks. The compressor and reasoner are jointly optimized via end-to-end reinforcement learning, while the gating module is trained separately as a classifier. Experimental results show that the proposed method achieves competitive accuracy on multi-hop reasoning benchmarks such as RULER-HQA, extrapolates context length from 7K to 1.75M tokens, and offers a favorable accuracy-efficiency trade-off compared to strong long-context baselines. In particular, it achieves up to a 2 times reduction in peak GPU memory usage and a 6 times inference speedup over MemAgent.",
            "score": 9,
            "issue_id": 1000,
            "pub_date": "2026-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "a9f1e207df12b840",
            "authors": [
                "Zhuoen Chen",
                "Dongfang Li",
                "Meishan Zhang",
                "Baotian Hu",
                "Min Zhang"
            ],
            "affiliations": [
                "Research Institute of Computing and Intelligence Harbin Institute of Technology, Shenzhen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.08382.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#benchmark",
                    "#inference",
                    "#rl",
                    "#reasoning",
                    "#optimization",
                    "#rag"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞšĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑÑ‹Ñ€Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²: ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾-Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° ÑĞ¶Ğ°Ñ‚Ğ¸Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ±Ğ»Ğ¾ĞºĞ°Ğ¼ Ğ¸ ÑĞµĞ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²ÑĞµÑ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ, Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ½Ğ° Ñ‡Ğ°ÑÑ‚Ğ¸, ĞºĞ°Ğ¶Ğ´Ğ°Ñ Ğ¸Ğ· ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ² ÑĞ¶Ğ°Ñ‚Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¾Ñ€Ğ°. ĞœĞ¾Ğ´ÑƒĞ»ÑŒ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²ĞµĞ½Ñ‚Ğ¸Ğ»ÑĞ¼Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ±Ğ»Ğ¾ĞºĞ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ÑÑ Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‰ĞµĞ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‡ĞµĞ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞºÑÑ‚Ñ€Ğ°Ğ¿Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ»Ğ¸Ğ½Ñƒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ¾ 1,75 Ğ¼Ğ»Ğ½ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ² 2 Ñ€Ğ°Ğ·Ğ° Ğ¸ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸ĞµĞ¼ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ² 6 Ñ€Ğ°Ğ· Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Cognitive-Inspired Efficiency for Long-Context Language Models",
                    "desc": "This paper presents a new framework for long-context language modeling that mimics cognitive processes to enhance efficiency and performance. It introduces chunk-wise compression to break down long inputs into manageable pieces, which are then encoded into compact memory representations. A selective memory recall mechanism is employed to dynamically choose relevant chunks for processing, allowing for effective reasoning on large contexts. The framework is optimized through reinforcement learning, resulting in significant improvements in memory usage and inference speed while maintaining competitive accuracy on reasoning tasks."
                },
                "zh": {
                    "title": "è®¤çŸ¥å¯å‘çš„é«˜æ•ˆé•¿ä¸Šä¸‹æ–‡å»ºæ¨¡",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§å—è®¤çŸ¥å¯å‘çš„é•¿ä¸Šä¸‹æ–‡è¯­è¨€å»ºæ¨¡æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜æ•ˆç‡å’Œæ€§èƒ½ã€‚è¯¥æ¡†æ¶é€šè¿‡åˆ†å—å‹ç¼©å’Œé€‰æ‹©æ€§è®°å¿†å›å¿†æ¥å¤„ç†é•¿è¾“å…¥ï¼Œè€Œä¸æ˜¯å¤„ç†æ‰€æœ‰åŸå§‹æ ‡è®°ã€‚å®ƒå°†é•¿è¾“å…¥åˆ†å‰²æˆå¤šä¸ªå—ï¼Œå¹¶ä½¿ç”¨å­¦ä¹ åˆ°çš„å‹ç¼©å™¨å°†æ¯ä¸ªå—ç¼–ç ä¸ºå‹ç¼©çš„è®°å¿†è¡¨ç¤ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šè·³æ¨ç†åŸºå‡†ä¸Šè¡¨ç°å‡ºç«äº‰åŠ›ï¼Œå¹¶åœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡ä¹‹é—´å®ç°äº†è‰¯å¥½çš„å¹³è¡¡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.09268",
            "title": "Rethinking Global Text Conditioning in Diffusion Transformers",
            "url": "https://huggingface.co/papers/2602.09268",
            "abstract": "Modulation-based text conditioning in diffusion transformers provides performance benefits when used as guidance for controllable generation rather than just as attention mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion transformers typically incorporate textual information via attention layers and a modulation mechanism using a pooled text embedding. Nevertheless, recent approaches discard modulation-based text conditioning and rely exclusively on attention. In this paper, we address whether modulation-based text conditioning is necessary and whether it can provide any performance advantage. Our analysis shows that, in its conventional usage, the pooled embedding contributes little to overall performance, suggesting that attention alone is generally sufficient for faithfully propagating prompt information. However, we reveal that the pooled embedding can provide significant gains when used from a different perspective-serving as guidance and enabling controllable shifts toward more desirable properties. This approach is training-free, simple to implement, incurs negligible runtime overhead, and can be applied to various diffusion models, bringing improvements across diverse tasks, including text-to-image/video generation and image editing.",
            "score": 8,
            "issue_id": 1002,
            "pub_date": "2026-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "30d52ae8f01b5aa7",
            "authors": [
                "Nikita Starodubcev",
                "Daniil Pakhomov",
                "Zongze Wu",
                "Ilya Drobyshevskiy",
                "Yuchen Liu",
                "Zhonghao Wang",
                "Yuqian Zhou",
                "Zhe Lin",
                "Dmitry Baranchuk"
            ],
            "affiliations": [
                "Adobe Research",
                "Yandex Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.09268.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#architecture",
                    "#multimodal",
                    "#training"
                ],
                "emoji": "ğŸ›ï¸",
                "ru": {
                    "title": "ĞŸĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ†Ğ¸Ğ¸: Ğ¾Ñ‚ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğº ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ embedding Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ… Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»ĞµĞ½Ğ° ĞºĞ°Ğº Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ° Ğ½Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ ĞºĞ°Ğº Ñ‡Ğ°ÑÑ‚ÑŒ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½Ñ‘Ğ½Ğ½Ñ‹Ğ¹ embedding Ğ²Ğ½Ğ¾ÑĞ¸Ñ‚ Ğ½ĞµĞ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ²ĞºĞ»Ğ°Ğ´ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾, Ğ¿Ğ¾ÑĞºĞ¾Ğ»ÑŒĞºÑƒ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº. ĞĞ´Ğ½Ğ°ĞºĞ¾, ĞºĞ¾Ğ³Ğ´Ğ° ÑÑ‚Ğ° Ğ¶Ğµ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ ĞºĞ°Ğº guidance Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¾Ğ½Ğ° Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ñ€Ğ¾ÑÑ‚ Ğ² Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ñ‘Ğ½ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Generation Control with Modulation in Diffusion Transformers",
                    "desc": "This paper explores the role of modulation-based text conditioning in diffusion transformers, which are models used for generating content. It finds that while traditional methods rely heavily on attention mechanisms, the pooled text embedding can actually enhance performance when used as guidance for generating specific outputs. The authors demonstrate that this approach is not only effective but also easy to implement and does not significantly slow down the model. Overall, the study suggests that using modulation for controllable generation can lead to better results in tasks like text-to-image generation and image editing."
                },
                "zh": {
                    "title": "è°ƒåˆ¶æ–‡æœ¬æ¡ä»¶åŒ–ï¼šæå‡ç”Ÿæˆæ§åˆ¶çš„å…³é”®",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†åœ¨æ‰©æ•£å˜æ¢å™¨ä¸­ä½¿ç”¨åŸºäºè°ƒåˆ¶çš„æ–‡æœ¬æ¡ä»¶åŒ–çš„å¿…è¦æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä¼ ç»Ÿçš„æ–‡æœ¬åµŒå…¥å¯¹æ•´ä½“æ€§èƒ½è´¡çŒ®ä¸å¤§ï¼Œå•é æ³¨æ„åŠ›æœºåˆ¶é€šå¸¸è¶³ä»¥ä¼ é€’æç¤ºä¿¡æ¯ã€‚ç„¶è€Œï¼Œå½“ä»ä¸åŒçš„è§’åº¦ä½¿ç”¨æ–‡æœ¬åµŒå…¥ä½œä¸ºæŒ‡å¯¼æ—¶ï¼Œå¯ä»¥æ˜¾è‘—æé«˜ç”Ÿæˆçš„å¯æ§æ€§ã€‚è¯¥æ–¹æ³•ç®€å•æ˜“è¡Œï¼Œå‡ ä¹ä¸å¢åŠ è¿è¡Œæ—¶é—´å¼€é”€ï¼Œé€‚ç”¨äºå¤šç§æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨æ–‡æœ¬åˆ°å›¾åƒ/è§†é¢‘ç”Ÿæˆå’Œå›¾åƒç¼–è¾‘ç­‰ä»»åŠ¡ä¸­å¸¦æ¥æ”¹è¿›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.09823",
            "title": "Covo-Audio Technical Report",
            "url": "https://huggingface.co/papers/2602.09823",
            "abstract": "Covo-Audio is a 7B-parameter end-to-end large audio language model that processes continuous audio inputs and generates audio outputs, achieving state-of-the-art performance across speech-text modeling, spoken dialogue, and full-duplex voice interaction tasks through large-scale pretraining and post-training techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t In this work, we present Covo-Audio, a 7B-parameter end-to-end LALM that directly processes continuous audio inputs and generates audio outputs within a single unified architecture. Through large-scale curated pretraining and targeted post-training, Covo-Audio achieves state-of-the-art or competitive performance among models of comparable scale across a broad spectrum of tasks, including speech-text modeling, spoken dialogue, speech understanding, audio understanding, and full-duplex voice interaction. Extensive evaluations demonstrate that the pretrained foundation model exhibits strong speech-text comprehension and semantic reasoning capabilities on multiple benchmarks, outperforming representative open-source models of comparable scale. Furthermore, Covo-Audio-Chat, the dialogue-oriented variant, demonstrates strong spoken conversational abilities, including understanding, contextual reasoning, instruction following, and generating contextually appropriate and empathetic responses, validating its applicability to real-world conversational assistant scenarios. Covo-Audio-Chat-FD, the evolved full-duplex model, achieves substantially superior performance on both spoken dialogue capabilities and full-duplex interaction behaviors, demonstrating its competence in practical robustness. To mitigate the high cost of deploying end-to-end LALMs for natural conversational systems, we propose an intelligence-speaker decoupling strategy that separates dialogue intelligence from voice rendering, enabling flexible voice customization with minimal text-to-speech (TTS) data while preserving dialogue performance. Overall, our results highlight the strong potential of 7B-scale models to integrate sophisticated audio intelligence with high-level semantic reasoning, and suggest a scalable path toward more capable and versatile LALMs.",
            "score": 7,
            "issue_id": 998,
            "pub_date": "2026-02-10",
            "pub_date_card": {
                "ru": "10 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 10",
                "zh": "2æœˆ10æ—¥"
            },
            "hash": "e82ef2485adbca7a",
            "authors": [
                "Wenfu Wang",
                "Chenxing Li",
                "Liqiang Zhang",
                "Yiyang Zhao",
                "Yuxiang Zou",
                "Hanzhao Li",
                "Mingyu Cui",
                "Hao Zhang",
                "Kun Wei",
                "Le Xu",
                "Zikang Huang",
                "Jiajun Xu",
                "Jiliang Hu",
                "Xiang He",
                "Zeyu Xie",
                "Jiawen Kang",
                "Youjun Chen",
                "Meng Yu",
                "Dong Yu",
                "Rilin Chen",
                "Linlin Di",
                "Shulin Feng",
                "Na Hu",
                "Yang Liu",
                "Bang Wang",
                "Shan Yang"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2602.09823.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#audio",
                    "#benchmark",
                    "#multimodal",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "ğŸ™ï¸",
                "ru": {
                    "title": "ĞšĞ¾Ğ½ĞµÑ† Ñ€ĞµÑ‡ĞµĞ²Ğ¾Ğ¼Ñƒ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ñƒ: Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ´ÑƒĞ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Covo-Audio â€” Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ñ 7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğ¹ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ²Ñ…Ğ¾Ğ´ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ²Ñ‹Ñ…Ğ¾Ğ´ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸-Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ° Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ´ÑƒĞ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ¾Ñ‚ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµÑ‡Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ°Ñ Ğ³Ğ¸Ğ±ĞºĞ¾ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ³Ğ¾Ğ»Ğ¾Ñ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ° 7B Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼."
                },
                "en": {
                    "title": "Covo-Audio: Revolutionizing Audio Interaction with 7B Parameters",
                    "desc": "Covo-Audio is a large audio language model with 7 billion parameters that processes audio inputs and generates audio outputs in a single framework. It achieves top performance in various tasks like speech-to-text, dialogue understanding, and full-duplex interactions through extensive pretraining and post-training methods. The model shows strong capabilities in speech comprehension and reasoning, outperforming similar models on multiple benchmarks. Additionally, its dialogue variant, Covo-Audio-Chat, excels in conversational tasks, while a new strategy allows for flexible voice customization without sacrificing performance."
                },
                "zh": {
                    "title": "éŸ³é¢‘æ™ºèƒ½ä¸è¯­ä¹‰æ¨ç†çš„å®Œç¾ç»“åˆ",
                    "desc": "Covo-Audioæ˜¯ä¸€ä¸ªæ‹¥æœ‰70äº¿å‚æ•°çš„ç«¯åˆ°ç«¯å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿå¤„ç†è¿ç»­çš„éŸ³é¢‘è¾“å…¥å¹¶ç”ŸæˆéŸ³é¢‘è¾“å‡ºã€‚é€šè¿‡å¤§è§„æ¨¡çš„é¢„è®­ç»ƒå’Œåè®­ç»ƒæŠ€æœ¯ï¼ŒCovo-Audioåœ¨è¯­éŸ³æ–‡æœ¬å»ºæ¨¡ã€å¯¹è¯ç†è§£å’Œå…¨åŒå·¥è¯­éŸ³äº¤äº’ç­‰ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¯¥æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å±•ç°å‡ºå¼ºå¤§çš„è¯­éŸ³ç†è§£å’Œè¯­ä¹‰æ¨ç†èƒ½åŠ›ï¼Œè¶…è¶Šäº†åŒè§„æ¨¡çš„å¼€æºæ¨¡å‹ã€‚æ­¤å¤–ï¼ŒCovo-Audio-Chatä½œä¸ºå¯¹è¯å¯¼å‘çš„å˜ä½“ï¼Œå±•ç¤ºäº†å‡ºè‰²çš„å£è¯­å¯¹è¯èƒ½åŠ›ï¼Œé€‚ç”¨äºå®é™…çš„å¯¹è¯åŠ©æ‰‹åœºæ™¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.07276",
            "title": "Steer2Adapt: Dynamically Composing Steering Vectors Elicits Efficient Adaptation of LLMs",
            "url": "https://huggingface.co/papers/2602.07276",
            "abstract": "STEER2ADAPT is a lightweight framework that adapts large language models by composing steering vectors from reusable semantic prior subspaces, enabling efficient and flexible task adaptation through linear combinations of basis vectors.  \t\t\t\t\tAI-generated summary \t\t\t\t Activation steering has emerged as a promising approach for efficiently adapting large language models (LLMs) to downstream behaviors. However, most existing steering methods rely on a single static direction per task or concept, making them inflexible under task variation and inadequate for complex tasks that require multiple coordinated capabilities. To address this limitation, we propose STEER2ADAPT, a lightweight framework that adapts LLMs by composing steering vectors rather than learning new ones from scratch. In many domains (e.g., reasoning or safety), tasks share a small set of underlying concept dimensions. STEER2ADAPT captures these dimensions as a reusable, low-dimensional semantic prior subspace, and adapts to new tasks by dynamically discovering a linear combination of basis vectors from only a handful of examples. Experiments across 9 tasks and 3 models in both reasoning and safety domains demonstrate the effectiveness of STEER2ADAPT, achieving an average improvement of 8.2%. Extensive analyses further show that STEER2ADAPT is a data-efficient, stable, and transparent inference-time adaptation method for LLMs.",
            "score": 7,
            "issue_id": 1001,
            "pub_date": "2026-02-07",
            "pub_date_card": {
                "ru": "7 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 7",
                "zh": "2æœˆ7æ—¥"
            },
            "hash": "ae74518aaab9e469",
            "authors": [
                "Pengrui Han",
                "Xueqiang Xu",
                "Keyang Xuan",
                "Peiyang Song",
                "Siru Ouyang",
                "Runchu Tian",
                "Yuqing Jiang",
                "Cheng Qian",
                "Pengcheng Jiang",
                "Jiashuo Sun",
                "Junxia Cui",
                "Ming Zhong",
                "Ge Liu",
                "Jiawei Han",
                "Jiaxuan You"
            ],
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.07276.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training"
                ],
                "emoji": "ğŸ§­",
                "ru": {
                    "title": "Ğ“Ğ¸Ğ±ĞºĞ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ LLM Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ñ… Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ²",
                    "desc": "STEER2ADAPT â€” ÑÑ‚Ğ¾ Ğ»Ñ‘Ğ³ĞºĞ¸Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ· Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ² ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚Ğ¾Ğ². Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ Ğ½ÑƒĞ»Ñ, Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ñ‹ÑƒÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ´Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹, Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸Ğ»Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ). ĞĞ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ°Ğ·Ğ¸ÑĞ½Ñ‹Ñ… Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ²ÑĞµĞ³Ğ¾ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ½Ğ° 9 Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ 3 Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 8.2% Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ° Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼, ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Efficient Task Adaptation for LLMs with STEER2ADAPT",
                    "desc": "STEER2ADAPT is a novel framework designed to enhance the adaptability of large language models (LLMs) by utilizing steering vectors derived from reusable semantic prior subspaces. This approach allows for efficient task adaptation through linear combinations of basis vectors, rather than requiring the model to learn new directions for each task. By capturing shared underlying concept dimensions, STEER2ADAPT enables the model to dynamically adjust to new tasks with minimal examples. Experimental results indicate that this method significantly improves performance across various tasks, demonstrating its data efficiency and stability during inference."
                },
                "zh": {
                    "title": "çµæ´»é€‚åº”å¤§å‹è¯­è¨€æ¨¡å‹çš„STEER2ADAPTæ¡†æ¶",
                    "desc": "STEER2ADAPTæ˜¯ä¸€ä¸ªè½»é‡çº§æ¡†æ¶ï¼Œé€šè¿‡ç»„åˆå¯é‡ç”¨çš„è¯­ä¹‰å…ˆéªŒå­ç©ºé—´ä¸­çš„å¼•å¯¼å‘é‡ï¼Œæ¥é€‚åº”å¤§å‹è¯­è¨€æ¨¡å‹ã€‚è¯¥æ–¹æ³•è§£å†³äº†ç°æœ‰å¼•å¯¼æ–¹æ³•åœ¨ä»»åŠ¡å˜åŒ–æ—¶çš„çµæ´»æ€§ä¸è¶³é—®é¢˜ï¼Œèƒ½å¤Ÿé€šè¿‡çº¿æ€§ç»„åˆåŸºç¡€å‘é‡æ¥é«˜æ•ˆé€‚åº”æ–°ä»»åŠ¡ã€‚STEER2ADAPTæ•æ‰å…±äº«çš„æ¦‚å¿µç»´åº¦ï¼Œå¹¶é€šè¿‡å°‘é‡ç¤ºä¾‹åŠ¨æ€å‘ç°è¿™äº›ç»´åº¦çš„ç»„åˆï¼Œä»è€Œå®ç°ä»»åŠ¡é€‚åº”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSTEER2ADAPTåœ¨æ¨ç†å’Œå®‰å…¨é¢†åŸŸçš„å¤šä¸ªä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹³å‡æå‡è¾¾8.2%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.09276",
            "title": "Effective Reasoning Chains Reduce Intrinsic Dimensionality",
            "url": "https://huggingface.co/papers/2602.09276",
            "abstract": "Effective chain-of-thought reasoning strategies reduce intrinsic dimensionality, leading to better generalization by requiring fewer model parameters to achieve given accuracy thresholds.  \t\t\t\t\tAI-generated summary \t\t\t\t Chain-of-thought (CoT) reasoning and its variants have substantially improved the performance of language models on complex reasoning tasks, yet the precise mechanisms by which different strategies facilitate generalization remain poorly understood. While current explanations often point to increased test-time computation or structural guidance, establishing a consistent, quantifiable link between these factors and generalization remains challenging. In this work, we identify intrinsic dimensionality as a quantitative measure for characterizing the effectiveness of reasoning chains. Intrinsic dimensionality quantifies the minimum number of model dimensions needed to reach a given accuracy threshold on a given task. By keeping the model architecture fixed and varying the task formulation through different reasoning strategies, we demonstrate that effective reasoning strategies consistently reduce the intrinsic dimensionality of the task. Validating this on GSM8K with Gemma-3 1B and 4B, we observe a strong inverse correlation between the intrinsic dimensionality of a reasoning strategy and its generalization performance on both in-distribution and out-of-distribution data. Our findings suggest that effective reasoning chains facilitate learning by better compressing the task using fewer parameters, offering a new quantitative metric for analyzing reasoning processes.",
            "score": 6,
            "issue_id": 1007,
            "pub_date": "2026-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "b0d3f3cc0a0d1514",
            "authors": [
                "Archiki Prasad",
                "Mandar Joshi",
                "Kenton Lee",
                "Mohit Bansal",
                "Peter Shaw"
            ],
            "affiliations": [
                "Google DeepMind",
                "UNC Chapel Hill"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.09276.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#reasoning",
                    "#interpretability",
                    "#math",
                    "#small_models"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ ÑĞ¶Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ: Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ»ÑƒÑ‡ÑˆĞµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ¼Ñ‹ÑĞ»ĞµĞ¹ (Chain-of-Thought) Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ğ·Ğ¼Ñƒ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ ÑĞ½Ğ¸Ğ¶Ğ°ÑÑ‚ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ÑÑ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ, Ñ‚Ğ¾ ĞµÑÑ‚ÑŒ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾Ğµ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ (GSM8K) Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ°Ñ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ°Ğº Ğ½Ğ° Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ÑÑ…."
                },
                "en": {
                    "title": "Reducing Dimensionality for Better Generalization in Language Models",
                    "desc": "This paper explores how effective chain-of-thought (CoT) reasoning strategies can improve the performance of language models by reducing intrinsic dimensionality. Intrinsic dimensionality measures the minimum number of dimensions needed for a model to achieve a certain level of accuracy on a task. The authors show that by using different reasoning strategies while keeping the model architecture constant, the intrinsic dimensionality decreases, leading to better generalization. Their findings indicate that effective reasoning chains allow models to learn more efficiently, requiring fewer parameters to perform well on various tasks."
                },
                "zh": {
                    "title": "æœ‰æ•ˆæ¨ç†é“¾é™ä½å†…åœ¨ç»´åº¦ï¼Œæå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†ç­–ç•¥å¦‚ä½•é€šè¿‡é™ä½å†…åœ¨ç»´åº¦æ¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚å†…åœ¨ç»´åº¦æ˜¯æŒ‡å®Œæˆç‰¹å®šä»»åŠ¡æ‰€éœ€çš„æœ€å°æ¨¡å‹ç»´åº¦ã€‚æˆ‘ä»¬é€šè¿‡å›ºå®šæ¨¡å‹æ¶æ„å¹¶æ”¹å˜ä»»åŠ¡è¡¨è¿°ï¼Œå‘ç°æœ‰æ•ˆçš„æ¨ç†ç­–ç•¥èƒ½å¤Ÿæ˜¾è‘—å‡å°‘ä»»åŠ¡çš„å†…åœ¨ç»´åº¦ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ¨ç†ç­–ç•¥çš„å†…åœ¨ç»´åº¦ä¸å…¶åœ¨æ•°æ®é›†ä¸Šçš„æ³›åŒ–æ€§èƒ½å‘ˆå¼ºè´Ÿç›¸å…³ï¼Œè¡¨æ˜æ›´æœ‰æ•ˆçš„æ¨ç†é“¾å¯ä»¥ç”¨æ›´å°‘çš„å‚æ•°æ›´å¥½åœ°å‹ç¼©ä»»åŠ¡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.09662",
            "title": "TreeCUA: Efficiently Scaling GUI Automation with Tree-Structured Verifiable Evolution",
            "url": "https://huggingface.co/papers/2602.09662",
            "abstract": "TreeCUA enables efficient GUI automation scaling through tree-structured trajectory organization and multi-agent collaboration, improving GUI planning capabilities via adaptive exploration and trajectory verification.  \t\t\t\t\tAI-generated summary \t\t\t\t Effectively scaling GUI automation is essential for computer-use agents (CUAs); however, existing work primarily focuses on scaling GUI grounding rather than the more crucial GUI planning, which requires more sophisticated data collection. In reality, the exploration process of a CUA across apps/desktops/web pages typically follows a tree structure, with earlier functional entry points often being explored more frequently. Thus, organizing large-scale trajectories into tree structures can reduce data cost and streamline the data scaling of GUI planning. In this work, we propose TreeCUA to efficiently scale GUI automation with tree-structured verifiable evolution. We propose a multi-agent collaborative framework to explore the environment, verify actions, summarize trajectories, and evaluate quality to generate high-quality and scalable GUI trajectories. To improve efficiency, we devise a novel tree-based topology to store and replay duplicate exploration nodes, and design an adaptive exploration algorithm to balance the depth (i.e., trajectory difficulty) and breadth (i.e., trajectory diversity). Moreover, we develop world knowledge guidance and global memory backtracking to avoid low-quality generation. Finally, we naturally extend and propose the TreeCUA-DPO method from abundant tree node information, improving GUI planning capability by referring to the branch information of adjacent trajectories. Experimental results show that TreeCUA and TreeCUA-DPO offer significant improvements, and out-of-domain (OOD) studies further demonstrate strong generalization. All trajectory node information and code will be available at https://github.com/UITron-hub/TreeCUA.",
            "score": 5,
            "issue_id": 1002,
            "pub_date": "2026-02-10",
            "pub_date_card": {
                "ru": "10 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 10",
                "zh": "2æœˆ10æ—¥"
            },
            "hash": "cfc064c4eefa52ac",
            "authors": [
                "Deyang Jiang",
                "Jing Huang",
                "Xuanle Zhao",
                "Lei Chen",
                "Liming Zheng",
                "Fanfan Liu",
                "Haibo Qiu",
                "Peng Shi",
                "Zhixiong Zeng"
            ],
            "affiliations": [
                "Meituan, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.09662.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#rlhf",
                    "#agents"
                ],
                "emoji": "ğŸŒ³",
                "ru": {
                    "title": "Ğ”Ñ€ĞµĞ²Ğ¾Ğ²Ğ¸Ğ´Ğ½Ğ°Ñ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ²",
                    "desc": "TreeCUA â€” ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·ÑƒĞµÑ‚ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ² Ğ´Ñ€ĞµĞ²Ğ¾Ğ²Ğ¸Ğ´Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ, Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹, Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒÑ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñƒ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ. Ğ”Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸Ğ·Ğ±ĞµĞ³Ğ°ÑÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ TreeCUA-DPO Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ ÑĞ¾ÑĞµĞ´Ğ½Ğ¸Ñ… Ğ²ĞµÑ‚Ğ²ĞµĞ¹ Ğ´ĞµÑ€ĞµĞ²Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¿Ñ€Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ GUI."
                },
                "en": {
                    "title": "TreeCUA: Scaling GUI Automation with Tree Structures and Collaboration",
                    "desc": "TreeCUA is a novel framework designed to enhance the efficiency of GUI automation by organizing exploration trajectories into tree structures. This approach allows for better data management and reduces costs associated with GUI planning, which is often overlooked in existing methods. By employing a multi-agent system, TreeCUA facilitates collaborative exploration and verification of actions, leading to the generation of high-quality trajectories. Additionally, the framework incorporates adaptive exploration techniques and memory backtracking to ensure diverse and effective trajectory generation, demonstrating strong performance in experimental evaluations."
                },
                "zh": {
                    "title": "TreeCUAï¼šé«˜æ•ˆæ‰©å±•GUIè‡ªåŠ¨åŒ–çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "TreeCUAæ˜¯ä¸€ç§é«˜æ•ˆçš„å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰è‡ªåŠ¨åŒ–æ‰©å±•æ–¹æ³•ï¼Œé€šè¿‡æ ‘çŠ¶è½¨è¿¹ç»„ç»‡å’Œå¤šæ™ºèƒ½ä½“åä½œæ¥æå‡GUIè§„åˆ’èƒ½åŠ›ã€‚è¯¥æ–¹æ³•åˆ©ç”¨è‡ªé€‚åº”æ¢ç´¢å’Œè½¨è¿¹éªŒè¯ï¼Œä¼˜åŒ–äº†æ•°æ®æ”¶é›†è¿‡ç¨‹ï¼Œé™ä½äº†æ•°æ®æˆæœ¬ã€‚é€šè¿‡å°†å¤§è§„æ¨¡è½¨è¿¹ç»„ç»‡æˆæ ‘ç»“æ„ï¼ŒTreeCUAèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ¢ç´¢ç¯å¢ƒå¹¶ç”Ÿæˆé«˜è´¨é‡çš„GUIè½¨è¿¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTreeCUAåœ¨æ‰©å±•æ€§å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.07153",
            "title": "ANCHOR: Branch-Point Data Generation for GUI Agents",
            "url": "https://huggingface.co/papers/2602.07153",
            "abstract": "A trajectory expansion framework called Anchor bootstraps scalable desktop supervision from seed demonstrations by identifying branch points and generating new trajectories through state-grounded task variants.  \t\t\t\t\tAI-generated summary \t\t\t\t End-to-end GUI agents for real desktop environments require large amounts of high-quality interaction data, yet collecting human demonstrations is expensive and existing synthetic pipelines often suffer from limited task diversity or noisy, goal-drifting trajectories. We present a trajectory expansion framework Anchor that bootstraps scalable desktop supervision from a small set of verified seed demonstrations. Starting from each seed, we identify branch points that correspond to meaningful state changes and propose new, state-grounded task variants conditioned on the current GUI context. An executing agent then follows the proposed instructions to generate new trajectories, while a verifier enforces task completion via state-aware checks and trajectory-level consistency. To improve supervision quality, we further apply task-conditioned step-level filtering to remove ungrounded actions and denoise post-branch segments to maintain coherent intent. Experiments on standard desktop benchmarks, OSWorld and WindowsAgentArena, show that models fine-tuned on our expanded corpus achieve consistent improvements over zero-shot agents and representative synthesis baselines, and generalize across applications and operating systems.",
            "score": 5,
            "issue_id": 998,
            "pub_date": "2026-02-06",
            "pub_date_card": {
                "ru": "6 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 6",
                "zh": "2æœˆ6æ—¥"
            },
            "hash": "80bf5f8d39c1170a",
            "authors": [
                "Jinbiao Wei",
                "Yilun Zhao",
                "Kangqi Ni",
                "Arman Cohan"
            ],
            "affiliations": [
                "University of North Carolina at Chapel Hill"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.07153.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸŒ³",
                "ru": {
                    "title": "ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´ĞµÑĞºÑ‚Ğ¾Ğ¿Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Anchor Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ²ĞµÑ‚Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑÑ… Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ñ‹ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğ¼ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°. Ğ”Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€, Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑÑÑ‰Ğ¸Ğ¹ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¸ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¾Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ğ¾Ğ¼ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Expanding AI Learning with Smart Trajectories",
                    "desc": "The paper introduces a framework called Anchor that enhances the collection of interaction data for desktop AI agents by expanding on a small number of initial demonstrations. It identifies key points in the task where new variations can be generated, allowing for the creation of diverse and contextually relevant trajectories. The framework includes a verification process to ensure that the generated actions are coherent and meet the task requirements. Experiments demonstrate that models trained with this expanded dataset perform better than those relying solely on traditional methods, showing improved generalization across different applications."
                },
                "zh": {
                    "title": "ä»ç§å­æ¼”ç¤ºåˆ°å¯æ‰©å±•æ¡Œé¢ç›‘ç£çš„è½¨è¿¹æ‰©å±•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºAnchorçš„è½¨è¿¹æ‰©å±•æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡è¯†åˆ«åˆ†æ”¯ç‚¹å’Œç”Ÿæˆæ–°çš„è½¨è¿¹æ¥ä»å°‘é‡ç§å­æ¼”ç¤ºä¸­å¼•å¯¼å¯æ‰©å±•çš„æ¡Œé¢ç›‘ç£ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨å½“å‰å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä¸Šä¸‹æ–‡ä¸­ç”Ÿæˆæ–°çš„ä»»åŠ¡å˜ä½“ï¼Œä»è€Œæé«˜äº¤äº’æ•°æ®çš„å¤šæ ·æ€§å’Œè´¨é‡ã€‚æ‰§è¡Œä»£ç†æ ¹æ®æå‡ºçš„æŒ‡ä»¤ç”Ÿæˆæ–°è½¨è¿¹ï¼ŒåŒæ—¶éªŒè¯å™¨é€šè¿‡çŠ¶æ€æ„ŸçŸ¥æ£€æŸ¥ç¡®ä¿ä»»åŠ¡å®Œæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºæ‰©å±•æ•°æ®é›†å¾®è°ƒçš„æ¨¡å‹åœ¨å¤šä¸ªæ¡Œé¢åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºé›¶-shotä»£ç†å’ŒåˆæˆåŸºçº¿ï¼Œä¸”åœ¨ä¸åŒåº”ç”¨å’Œæ“ä½œç³»ç»Ÿä¸­å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.10116",
            "title": "SAGE: Scalable Agentic 3D Scene Generation for Embodied AI",
            "url": "https://huggingface.co/papers/2602.10116",
            "abstract": "SAGE is an agentic framework that automatically generates simulation-ready 3D environments for embodied AI by combining layout and object composition generators with evaluative critics for semantic plausibility and physical stability.  \t\t\t\t\tAI-generated summary \t\t\t\t Real-world data collection for embodied agents remains costly and unsafe, calling for scalable, realistic, and simulator-ready 3D environments. However, existing scene-generation systems often rely on rule-based or task-specific pipelines, yielding artifacts and physically invalid scenes. We present SAGE, an agentic framework that, given a user-specified embodied task (e.g., \"pick up a bowl and place it on the table\"), understands the intent and automatically generates simulation-ready environments at scale. The agent couples multiple generators for layout and object composition with critics that evaluate semantic plausibility, visual realism, and physical stability. Through iterative reasoning and adaptive tool selection, it self-refines the scenes until meeting user intent and physical validity. The resulting environments are realistic, diverse, and directly deployable in modern simulators for policy training. Policies trained purely on this data exhibit clear scaling trends and generalize to unseen objects and layouts, demonstrating the promise of simulation-driven scaling for embodied AI. Code, demos, and the SAGE-10k dataset can be found on the project page here: https://nvlabs.github.io/sage.",
            "score": 4,
            "issue_id": 998,
            "pub_date": "2026-02-10",
            "pub_date_card": {
                "ru": "10 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 10",
                "zh": "2æœˆ10æ—¥"
            },
            "hash": "6ebb28cc4eb6e2d6",
            "authors": [
                "Hongchi Xia",
                "Xuan Li",
                "Zhaoshuo Li",
                "Qianli Ma",
                "Jiashu Xu",
                "Ming-Yu Liu",
                "Yin Cui",
                "Tsung-Yi Lin",
                "Wei-Chiu Ma",
                "Shenlong Wang",
                "Shuran Song",
                "Fangyin Wei"
            ],
            "affiliations": [
                "Cornell University",
                "NVIDIA",
                "Stanford University",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.10116.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#agents",
                    "#dataset",
                    "#synthetic",
                    "#robotics",
                    "#3d",
                    "#reasoning"
                ],
                "emoji": "ğŸ ",
                "ru": {
                    "title": "ĞĞ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… 3D ÑÑ†ĞµĞ½ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "SAGE â€” ÑÑ‚Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½, Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹Ñ… Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ embodied AI Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ ÑÑ†ĞµĞ½Ñ‹ Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ°Ğ¼Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¼ Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ. ĞĞ³ĞµĞ½Ñ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²ÑƒĞµÑ‚ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ñ‹ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ²Ñ‹Ğ²Ğ¾Ğ´, Ğ¿Ğ¾ĞºĞ° Ğ½Ğµ Ğ±ÑƒĞ´ĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ½ÑƒÑ‚Ğ° ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑ†ĞµĞ½Ñ‹. ĞŸĞ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ñ‚Ğ°ĞºĞ¸Ñ… ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ½ĞµĞ²Ğ¸Ğ´Ğ¸Ğ¼Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸."
                },
                "en": {
                    "title": "SAGE: Automating Realistic 3D Environment Generation for AI Training",
                    "desc": "SAGE is a framework designed to create realistic 3D environments for embodied AI, which are essential for training AI agents in simulations. It combines layout and object composition generators with evaluative critics to ensure that the generated scenes are semantically plausible and physically stable. By understanding user-specified tasks, SAGE can automatically generate diverse and scalable environments that are ready for simulation. This approach allows for effective policy training, as the AI can learn from these environments and generalize to new scenarios."
                },
                "zh": {
                    "title": "æ™ºèƒ½ç”ŸæˆçœŸå®3Dç¯å¢ƒçš„æ¡†æ¶SAGE",
                    "desc": "SAGEæ˜¯ä¸€ä¸ªæ™ºèƒ½æ¡†æ¶ï¼Œèƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆé€‚åˆäºå…·èº«äººå·¥æ™ºèƒ½çš„3Dç¯å¢ƒã€‚å®ƒç»“åˆäº†å¸ƒå±€å’Œç‰©ä½“ç»„åˆç”Ÿæˆå™¨ï¼Œä»¥åŠè¯„ä¼°è¯­ä¹‰åˆç†æ€§å’Œç‰©ç†ç¨³å®šæ€§çš„è¯„ä¼°å™¨ã€‚ç”¨æˆ·åªéœ€æŒ‡å®šä¸€ä¸ªå…·èº«ä»»åŠ¡ï¼ŒSAGEå°±èƒ½ç†è§£æ„å›¾å¹¶å¤§è§„æ¨¡ç”Ÿæˆç¬¦åˆè¦æ±‚çš„æ¨¡æ‹Ÿç¯å¢ƒã€‚ç”Ÿæˆçš„ç¯å¢ƒçœŸå®å¤šæ ·ï¼Œé€‚åˆåœ¨ç°ä»£æ¨¡æ‹Ÿå™¨ä¸­è¿›è¡Œç­–ç•¥è®­ç»ƒï¼Œå±•ç¤ºäº†åŸºäºæ¨¡æ‹Ÿçš„å…·èº«AIæ‰©å±•æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.09024",
            "title": "Autoregressive Image Generation with Masked Bit Modeling",
            "url": "https://huggingface.co/papers/2602.09024",
            "abstract": "Discrete tokenizers can match or exceed continuous methods when properly scaled, and a new masked Bit AutoRegressive modeling approach achieves state-of-the-art results with reduced computational costs.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper challenges the dominance of continuous pipelines in visual generation. We systematically investigate the performance gap between discrete and continuous methods. Contrary to the belief that discrete tokenizers are intrinsically inferior, we demonstrate that the disparity arises primarily from the total number of bits allocated in the latent space (i.e., the compression ratio). We show that scaling up the codebook size effectively bridges this gap, allowing discrete tokenizers to match or surpass their continuous counterparts. However, existing discrete generation methods struggle to capitalize on this insight, suffering from performance degradation or prohibitive training costs with scaled codebook. To address this, we propose masked Bit AutoRegressive modeling (BAR), a scalable framework that supports arbitrary codebook sizes. By equipping an autoregressive transformer with a masked bit modeling head, BAR predicts discrete tokens through progressively generating their constituent bits. BAR achieves a new state-of-the-art gFID of 0.99 on ImageNet-256, outperforming leading methods across both continuous and discrete paradigms, while significantly reducing sampling costs and converging faster than prior continuous approaches. Project page is available at https://bar-gen.github.io/",
            "score": 4,
            "issue_id": 998,
            "pub_date": "2026-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "1c2120d35b663d1b",
            "authors": [
                "Qihang Yu",
                "Qihao Liu",
                "Ju He",
                "Xinyang Zhang",
                "Yang Liu",
                "Liang-Chieh Chen",
                "Xi Chen"
            ],
            "affiliations": [
                "Amazon FAR (Frontier AI & Robotics)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.09024.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#cv"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ”Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ñ‹ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ñ‚Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ°Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑÑ€Ğ°Ğ²Ğ½ÑÑ‚ÑŒÑÑ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ñ‚Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°Ğ·Ğ½Ğ¸Ñ†Ğ° Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ÑÑ Ğ² Ğ¿ĞµÑ€Ğ²ÑƒÑ Ğ¾Ñ‡ĞµÑ€ĞµĞ´ÑŒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ±Ğ¸Ñ‚Ğ¾Ğ² Ğ² ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ, Ğ° Ğ½Ğµ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ½ĞµĞ¿Ğ¾Ğ»Ğ½Ğ¾Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ². Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Masked Bit AutoRegressive (BAR), Ğ³Ğ´Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ñ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¸Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ñ… ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ñ…. BAR Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ñ gFID 0.99 Ğ½Ğ° ImageNet-256, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºÑƒ Ğ¸ ÑƒÑĞºĞ¾Ñ€ÑÑ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Scaling Discrete Tokenizers to Surpass Continuous Methods",
                    "desc": "This paper explores the effectiveness of discrete tokenizers in visual generation, showing that they can perform as well as or better than continuous methods when properly scaled. The authors identify that the performance gap is mainly due to the number of bits used in the latent space, suggesting that increasing the codebook size can enhance discrete tokenizers. They introduce a new approach called masked Bit AutoRegressive modeling (BAR), which allows for flexible codebook sizes and improves the generation of discrete tokens. BAR achieves state-of-the-art results on ImageNet-256 while reducing computational costs and training time compared to existing methods."
                },
                "zh": {
                    "title": "ç¦»æ•£æ ‡è®°å™¨çš„å´›èµ·ï¼šè¶…è¶Šè¿ç»­æ–¹æ³•çš„åˆ›æ–°",
                    "desc": "è¿™ç¯‡è®ºæ–‡æŒ‘æˆ˜äº†è¿ç»­ç”Ÿæˆæ–¹æ³•åœ¨è§†è§‰ç”Ÿæˆä¸­çš„ä¸»å¯¼åœ°ä½ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°ç ”ç©¶äº†ç¦»æ•£å’Œè¿ç»­æ–¹æ³•ä¹‹é—´çš„æ€§èƒ½å·®è·ï¼Œå‘ç°ç¦»æ•£æ ‡è®°å™¨å¹¶éå¤©ç”ŸåŠ£åŠ¿ï¼Œè€Œæ˜¯ç”±äºæ½œåœ¨ç©ºé—´ä¸­åˆ†é…çš„æ¯”ç‰¹æ€»æ•°ï¼ˆå³å‹ç¼©æ¯”ï¼‰é€ æˆçš„ã€‚é€šè¿‡æ‰©å¤§ä»£ç æœ¬çš„å¤§å°ï¼Œæˆ‘ä»¬è¯æ˜ç¦»æ•£æ ‡è®°å™¨å¯ä»¥ä¸è¿ç»­æ–¹æ³•ç›¸åŒ¹æ•Œæˆ–è¶…è¶Šå®ƒä»¬ã€‚ä¸ºäº†è§£å†³ç°æœ‰ç¦»æ•£ç”Ÿæˆæ–¹æ³•åœ¨æ‰©å±•ä»£ç æœ¬æ—¶çš„æ€§èƒ½ä¸‹é™æˆ–è®­ç»ƒæˆæœ¬è¿‡é«˜çš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ©ç æ¯”ç‰¹è‡ªå›å½’å»ºæ¨¡ï¼ˆBARï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ”¯æŒä»»æ„ä»£ç æœ¬å¤§å°çš„å¯æ‰©å±•æ¡†æ¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.08344",
            "title": "OPE: Overcoming Information Saturation in Parallel Thinking via Outline-Guided Path Exploration",
            "url": "https://huggingface.co/papers/2602.08344",
            "abstract": "Reinforcement learning with verifiable rewards is used to enhance parallel thinking in large reasoning models through outline-guided path exploration that reduces information redundancy and improves solution discovery.  \t\t\t\t\tAI-generated summary \t\t\t\t Parallel thinking has emerged as a new paradigm for large reasoning models (LRMs) in tackling complex problems. Recent methods leverage Reinforcement Learning (RL) to enhance parallel thinking, aiming to address the limitations in computational resources and effectiveness encountered with supervised fine-tuning. However, most existing studies primarily focus on optimizing the aggregation phase, with limited attention to the path exploration stage. In this paper, we theoretically analyze the optimization of parallel thinking under the Reinforcement Learning with Verifiable Rewards (RLVR) setting, and identify that the mutual information bottleneck among exploration paths fundamentally restricts overall performance. To address this, we propose Outline-Guided Path Exploration (OPE), which explicitly partitions the solution space by generating diverse reasoning outlines prior to parallel path reasoning, thereby reducing information redundancy and improving the diversity of information captured across exploration paths. We implement OPE with an iterative RL strategy that optimizes outline planning and outline-guided reasoning independently. Extensive experiments across multiple challenging mathematical benchmarks demonstrate that OPE effectively improves reasoning performance in different aggregation strategies, enabling LRMs to more reliably discover correct solutions.",
            "score": 4,
            "issue_id": 998,
            "pub_date": "2026-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "f6f1601f72235fed",
            "authors": [
                "Qi Guo",
                "Jianing Wang",
                "Deyang Kong",
                "Xiangyu Xi",
                "Jianfei Zhang",
                "Yi Lu",
                "Jingang Wang",
                "Wei Wang",
                "Shikun Zhang",
                "Wei Ye"
            ],
            "affiliations": [
                "Meituan Group, Beijing, China",
                "National Engineering Research Center for Software Engineering, Peking University, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.08344.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#optimization",
                    "#training",
                    "#reasoning",
                    "#math"
                ],
                "emoji": "ğŸ§­",
                "ru": {
                    "title": "Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ½Ğ°Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿ÑƒÑ‚ÑĞ¼Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑÑ ÑƒĞ·ĞºĞ¸Ğ¼ Ğ¼ĞµÑÑ‚Ğ¾Ğ¼, Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¼ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Outline-Guided Path Exploration ÑĞ²Ğ½Ğ¾ Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ÑÑ…ĞµĞ¼Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿ĞµÑ€ĞµĞ´ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼, Ñ‡Ñ‚Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ˜Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ…ĞµĞ¼ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğµ Ğ¸Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ¸ÑĞºĞ° ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "Enhancing Reasoning in AI with Outline-Guided Exploration",
                    "desc": "This paper discusses a method to improve large reasoning models (LRMs) using Reinforcement Learning with Verifiable Rewards (RLVR). It introduces Outline-Guided Path Exploration (OPE), which helps in organizing the reasoning process by creating diverse outlines before exploring solutions. This approach reduces redundancy in information and enhances the overall performance of the models. The authors demonstrate that OPE leads to better reasoning outcomes across various mathematical challenges by optimizing both outline planning and reasoning strategies."
                },
                "zh": {
                    "title": "é€šè¿‡å¤§çº²å¼•å¯¼æå‡å¹³è¡Œæ€ç»´çš„å¼ºåŒ–å­¦ä¹ ",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•é€šè¿‡å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ æ¥å¢å¼ºå¤§å‹æ¨ç†æ¨¡å‹ä¸­çš„å¹³è¡Œæ€ç»´ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºå¤§çº²å¼•å¯¼è·¯å¾„æ¢ç´¢ï¼ˆOPEï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨å‡å°‘ä¿¡æ¯å†—ä½™å¹¶æé«˜è§£å†³æ–¹æ¡ˆçš„å‘ç°èƒ½åŠ›ã€‚OPEé€šè¿‡åœ¨å¹³è¡Œè·¯å¾„æ¨ç†ä¹‹å‰ç”Ÿæˆå¤šæ ·åŒ–çš„æ¨ç†å¤§çº²ï¼Œæ˜ç¡®åˆ’åˆ†äº†è§£å†³æ–¹æ¡ˆç©ºé—´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOPEåœ¨ä¸åŒçš„èšåˆç­–ç•¥ä¸‹æœ‰æ•ˆæå‡äº†æ¨ç†æ€§èƒ½ï¼Œä½¿å¤§å‹æ¨ç†æ¨¡å‹èƒ½å¤Ÿæ›´å¯é åœ°å‘ç°æ­£ç¡®çš„è§£å†³æ–¹æ¡ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.07839",
            "title": "TodoEvolve: Learning to Architect Agent Planning Systems",
            "url": "https://huggingface.co/papers/2602.07839",
            "abstract": "TodoEvolve enables autonomous synthesis and revision of task-specific planning architectures through a modular design space and multi-objective reinforcement learning optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Planning has become a central capability for contemporary agent systems in navigating complex, long-horizon tasks, yet existing approaches predominantly rely on fixed, hand-crafted planning structures that lack the flexibility to adapt to the structural diversity of open-ended problems. To address this limitation, we introduce TodoEvolve, a meta-planning paradigm that autonomously synthesizes and dynamically revises task-specific planning architectures. Specifically, we first construct PlanFactory, a modular design space that standardizes diverse planning paradigms within a unified codebase encompassing topology, initialization, adaptation, and navigation, thereby providing a common interface for heterogeneous planning patterns. Leveraging PlanFactory, we collect high-quality planning trajectories and train Todo-14B via Impedance-Guided Preference Optimization (IGPO), a multi-objective reinforcement learning objective that encourages the generation of planning systems that are performant, stable, and token-efficient across arbitrary tasks and agent backbones. Empirical evaluations on five agentic benchmarks demonstrate that TodoEvolve consistently surpasses carefully engineered planning modules while maintaining economical API costs and runtime overhead.",
            "score": 4,
            "issue_id": 1001,
            "pub_date": "2026-02-08",
            "pub_date_card": {
                "ru": "8 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 8",
                "zh": "2æœˆ8æ—¥"
            },
            "hash": "a7d1be5c5c833b86",
            "authors": [
                "Jiaxi Liu",
                "Yanzuo Jiang",
                "Guibin Zhang",
                "Zihan Zhang",
                "Heng Chang",
                "Zhenfei Yin",
                "Qibing Ren",
                "Junchi Yan"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2602.07839.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#training",
                    "#agents",
                    "#architecture",
                    "#rl"
                ],
                "emoji": "ğŸ—ï¸",
                "ru": {
                    "title": "Ğ­Ğ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ†ĞµĞ»ĞµĞ²ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ",
                    "desc": "TodoEvolve Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¼ĞµÑ‚Ğ°-Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ PlanFactory â€” Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğº Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Todo-14B Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸Ñ (IGPO), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ‚ĞµÑÑ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ€ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "Empowering AI with Self-Improving Planning Systems",
                    "desc": "TodoEvolve is a new approach that helps AI systems create and improve their own planning methods for complex tasks. It uses a modular design called PlanFactory, which organizes different planning strategies into a single framework, making it easier to adapt to various problems. By applying multi-objective reinforcement learning through Impedance-Guided Preference Optimization (IGPO), TodoEvolve trains AI to develop planning systems that are efficient and effective. Tests show that TodoEvolve outperforms traditional planning methods while being cost-effective and quick to run."
                },
                "zh": {
                    "title": "TodoEvolveï¼šè‡ªä¸»è§„åˆ’çš„æœªæ¥",
                    "desc": "TodoEvolve æ˜¯ä¸€ç§å…ƒè§„åˆ’èŒƒå¼ï¼Œèƒ½å¤Ÿè‡ªä¸»åˆæˆå’ŒåŠ¨æ€ä¿®è®¢ç‰¹å®šä»»åŠ¡çš„è§„åˆ’æ¶æ„ã€‚å®ƒé€šè¿‡æ„å»ºä¸€ä¸ªæ¨¡å—åŒ–è®¾è®¡ç©ºé—´ PlanFactoryï¼Œæ ‡å‡†åŒ–äº†å¤šç§è§„åˆ’èŒƒå¼ï¼Œä½¿å…¶åœ¨ç»Ÿä¸€çš„ä»£ç åº“ä¸­å®ç°ã€‚åˆ©ç”¨å¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ–¹æ³• Impedance-Guided Preference Optimization (IGPO)ï¼ŒTodoEvolve ç”Ÿæˆé«˜æ•ˆã€ç¨³å®šä¸”èµ„æºèŠ‚çœçš„è§„åˆ’ç³»ç»Ÿã€‚å®éªŒè¯æ˜ï¼ŒTodoEvolve åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„æ‰‹å·¥è®¾è®¡è§„åˆ’æ¨¡å—ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.07422",
            "title": "Secure Code Generation via Online Reinforcement Learning with Vulnerability Reward Model",
            "url": "https://huggingface.co/papers/2602.07422",
            "abstract": "SecCoderX uses online reinforcement learning to align large language models for secure code generation while preserving functionality, addressing the functionality-security trade-off through vulnerability detection integration and reward modeling.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are increasingly used in software development, yet their tendency to generate insecure code remains a major barrier to real-world deployment. Existing secure code alignment methods often suffer from a functionality--security paradox, improving security at the cost of substantial utility degradation. We propose SecCoderX, an online reinforcement learning framework for functionality-preserving secure code generation. SecCoderX first bridges vulnerability detection and secure code generation by repurposing mature detection resources in two ways: (i) synthesizing diverse, reality-grounded vulnerability-inducing coding tasks for online RL rollouts, and (ii) training a reasoning-based vulnerability reward model that provides scalable and reliable security supervision. Together, these components are unified in an online RL loop to align code LLMs to generate secure and functional code. Extensive experiments demonstrate that SecCoderX achieves state-of-the-art performance, improving Effective Safety Rate (ESR) by approximately 10% over unaligned models, whereas prior methods often degrade ESR by 14-54%. We release our code, dataset and model checkpoints at https://github.com/AndrewWTY/SecCoderX.",
            "score": 3,
            "issue_id": 1007,
            "pub_date": "2026-02-07",
            "pub_date_card": {
                "ru": "7 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 7",
                "zh": "2æœˆ7æ—¥"
            },
            "hash": "ea94db6f15b811ce",
            "authors": [
                "Tianyi Wu",
                "Mingzhe Du",
                "Yue Liu",
                "Chengran Yang",
                "Terry Yue Zhuo",
                "Jiaheng Zhang",
                "See-Kiong Ng"
            ],
            "affiliations": [
                "CSIROs Data61",
                "Monash University",
                "Nanyang Technological University",
                "National University of Singapore",
                "Singapore Management University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.07422.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#security",
                    "#open_source",
                    "#alignment",
                    "#plp",
                    "#optimization"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ Ğ±ĞµĞ· ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑĞ¾Ğ²: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ LLM-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ°",
                    "desc": "SecCoderX â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ñ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ²ÑƒĞ¼Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ°Ğ¼Ğ¸: ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¾ĞºÑ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸-Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°Ğ½ÑŒÑˆĞµ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ğ» Ğº Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼Ñƒ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ĞºĞ¾Ğ´Ğ° Ğ¿Ñ€Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SecCoderX ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 10% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ñ‚Ğ¾Ğ³Ğ´Ğ° ĞºĞ°Ğº Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ‡Ğ°ÑÑ‚Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°Ğ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° 14-54%."
                },
                "en": {
                    "title": "SecCoderX: Balancing Security and Functionality in Code Generation",
                    "desc": "SecCoderX is a novel framework that utilizes online reinforcement learning to enhance the security of code generated by large language models (LLMs) while maintaining their functionality. It addresses the common issue where improving code security often leads to a loss in utility, known as the functionality-security trade-off. By integrating vulnerability detection with secure code generation, SecCoderX employs a reasoning-based reward model to guide the learning process effectively. Experimental results show that SecCoderX significantly improves the Effective Safety Rate (ESR) of generated code compared to previous methods, making it a promising solution for secure software development."
                },
                "zh": {
                    "title": "SecCoderXï¼šå®‰å…¨ä¸åŠŸèƒ½å…¼å¾—çš„ä»£ç ç”Ÿæˆ",
                    "desc": "SecCoderX æ˜¯ä¸€ç§åœ¨çº¿å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨ç”Ÿæˆå®‰å…¨ä¸”åŠŸèƒ½å®Œå¤‡çš„ä»£ç ã€‚å®ƒé€šè¿‡å°†æ¼æ´æ£€æµ‹ä¸å®‰å…¨ä»£ç ç”Ÿæˆç›¸ç»“åˆï¼Œè§£å†³äº†åŠŸèƒ½æ€§ä¸å®‰å…¨æ€§ä¹‹é—´çš„çŸ›ç›¾ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æˆç†Ÿçš„æ£€æµ‹èµ„æºï¼Œåˆæˆå¤šæ ·åŒ–çš„ç¼–ç ä»»åŠ¡ï¼Œå¹¶è®­ç»ƒåŸºäºæ¨ç†çš„æ¼æ´å¥–åŠ±æ¨¡å‹ï¼Œä»¥æä¾›å¯æ‰©å±•çš„å®‰å…¨ç›‘ç£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSecCoderX åœ¨æœ‰æ•ˆå®‰å…¨ç‡ï¼ˆESRï¼‰ä¸Šæ¯”æœªå¯¹é½æ¨¡å‹æé«˜äº†çº¦ 10%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.06161",
            "title": "Stop the Flip-Flop: Context-Preserving Verification for Fast Revocable Diffusion Decoding",
            "url": "https://huggingface.co/papers/2602.06161",
            "abstract": "COVER enables efficient parallel decoding for diffusion language models by implementing cache override verification that reduces unnecessary revisions and maintains output quality through stable drafting and attention view construction.  \t\t\t\t\tAI-generated summary \t\t\t\t Parallel diffusion decoding can accelerate diffusion language model inference by unmasking multiple tokens per step, but aggressive parallelism often harms quality. Revocable decoding mitigates this by rechecking earlier tokens, yet we observe that existing verification schemes frequently trigger flip-flop oscillations, where tokens are remasked and later restored unchanged. This behaviour slows inference in two ways: remasking verified positions weakens the conditioning context for parallel drafting, and repeated remask cycles consume the revision budget with little net progress. We propose COVER (Cache Override Verification for Efficient Revision), which performs leave-one-out verification and stable drafting within a single forward pass. COVER constructs two attention views via KV cache override: selected seeds are masked for verification, while their cached key value states are injected for all other queries to preserve contextual information, with a closed form diagonal correction preventing self leakage at the seed positions. COVER further prioritises seeds using a stability aware score that balances uncertainty, downstream influence, and cache drift, and it adapts the number of verified seeds per step. Across benchmarks, COVER markedly reduces unnecessary revisions and yields faster decoding while preserving output quality.",
            "score": 3,
            "issue_id": 1002,
            "pub_date": "2026-02-05",
            "pub_date_card": {
                "ru": "5 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 5",
                "zh": "2æœˆ5æ—¥"
            },
            "hash": "c62534e67c2e230f",
            "authors": [
                "Yanzheng Xiang",
                "Lan Wei",
                "Yizhen Yao",
                "Qinglin Zhu",
                "Hanqi Yan",
                "Chen Jin",
                "Philip Alexander Teare",
                "Dandan Zhang",
                "Lin Gui",
                "Amrutha Saseendran",
                "Yulan He"
            ],
            "affiliations": [
                "Centre for AI, Data Science & Artificial Intelligence, BioPharmaceuticals R&D, AstraZeneca, UK",
                "Imperial College London, UK",
                "Kings College London, UK",
                "The Alan Turing Institute, UK"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.06161.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#optimization"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· ÑƒĞ¼Ğ½ÑƒÑ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ ĞºÑÑˆĞ°",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ COVER Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸ Ğ°Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‚ ĞºĞ¾Ğ»ĞµĞ±Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ğ³Ğ´Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾ Ğ¿ĞµÑ€ĞµĞ´ĞµĞ»Ñ‹Ğ²Ğ°ÑÑ‚ÑÑ Ğ¸ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ, Ñ‡Ñ‚Ğ¾ Ğ·Ğ°Ğ¼ĞµĞ´Ğ»ÑĞµÑ‚ Ğ²Ñ‹Ğ²Ğ¾Ğ´ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. COVER Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Ğ¿ĞµÑ€ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ KV-ĞºÑÑˆĞ° Ğ´Ğ»Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¼ forward pass, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°Ñ ÑĞ°Ğ¼Ğ¾Ñ€Ğ°Ğ·Ñ€ÑƒÑˆĞµĞ½Ğ¸Ğµ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºÑƒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰ĞµĞ¹ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ½ĞµĞ½ÑƒĞ¶Ğ½Ñ‹Ğµ Ğ¿ĞµÑ€ĞµÑÑ‡Ñ‘Ñ‚Ñ‹ Ğ¸ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "COVER: Efficient Parallel Decoding with Quality Preservation",
                    "desc": "This paper introduces COVER, a method designed to enhance the efficiency of parallel decoding in diffusion language models. It addresses the issue of quality degradation caused by aggressive parallelism, which can lead to unnecessary remasking of tokens. By implementing cache override verification, COVER allows for stable drafting and effective verification of tokens in a single forward pass. The method improves inference speed while maintaining high output quality by optimizing the selection and verification of tokens based on their stability and contextual relevance."
                },
                "zh": {
                    "title": "COVERï¼šé«˜æ•ˆçš„æ‰©æ•£è¯­è¨€æ¨¡å‹è§£ç ",
                    "desc": "COVERæ˜¯ä¸€ç§é«˜æ•ˆçš„å¹¶è¡Œè§£ç æ–¹æ³•ï¼Œä¸“ä¸ºæ‰©æ•£è¯­è¨€æ¨¡å‹è®¾è®¡ã€‚å®ƒé€šè¿‡å®æ–½ç¼“å­˜è¦†ç›–éªŒè¯ï¼Œå‡å°‘ä¸å¿…è¦çš„ä¿®è®¢ï¼ŒåŒæ—¶ä¿æŒè¾“å‡ºè´¨é‡ã€‚è¯¥æ–¹æ³•åœ¨å•æ¬¡å‰å‘ä¼ é€’ä¸­è¿›è¡ŒéªŒè¯å’Œç¨³å®šè‰æ‹Ÿï¼Œæ„å»ºäº†ä¸¤ä¸ªæ³¨æ„åŠ›è§†å›¾ï¼Œä»¥ä¿ç•™ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚COVERæ˜¾è‘—é™ä½äº†ä¸å¿…è¦çš„ä¿®è®¢ï¼Œæé«˜äº†è§£ç é€Ÿåº¦ï¼ŒåŒæ—¶ä¿æŒäº†è¾“å‡ºçš„è´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.05435",
            "title": "Stable Velocity: A Variance Perspective on Flow Matching",
            "url": "https://huggingface.co/papers/2602.05435",
            "abstract": "Stable Velocity framework addresses high-variance training in flow matching by identifying low-variance regimes and proposing variance-reduction techniques for improved training efficiency and sampling speed.  \t\t\t\t\tAI-generated summary \t\t\t\t While flow matching is elegant, its reliance on single-sample conditional velocities leads to high-variance training targets that destabilize optimization and slow convergence. By explicitly characterizing this variance, we identify 1) a high-variance regime near the prior, where optimization is challenging, and 2) a low-variance regime near the data distribution, where conditional and marginal velocities nearly coincide. Leveraging this insight, we propose Stable Velocity, a unified framework that improves both training and sampling. For training, we introduce Stable Velocity Matching (StableVM), an unbiased variance-reduction objective, along with Variance-Aware Representation Alignment (VA-REPA), which adaptively strengthen auxiliary supervision in the low-variance regime. For inference, we show that dynamics in the low-variance regime admit closed-form simplifications, enabling Stable Velocity Sampling (StableVS), a finetuning-free acceleration. Extensive experiments on ImageNet 256times256 and large pretrained text-to-image and text-to-video models, including SD3.5, Flux, Qwen-Image, and Wan2.2, demonstrate consistent improvements in training efficiency and more than 2times faster sampling within the low-variance regime without degrading sample quality. Our code is available at https://github.com/linYDTHU/StableVelocity.",
            "score": 3,
            "issue_id": 1000,
            "pub_date": "2026-02-05",
            "pub_date_card": {
                "ru": "5 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 5",
                "zh": "2æœˆ5æ—¥"
            },
            "hash": "468b5496236d552e",
            "authors": [
                "Donglin Yang",
                "Yongxing Zhang",
                "Xin Yu",
                "Liang Hou",
                "Xin Tao",
                "Pengfei Wan",
                "Xiaojuan Qi",
                "Renjie Liao"
            ],
            "affiliations": [
                "CIFAR",
                "Kling Team, Kuaishou Technology",
                "University of British Columbia",
                "University of Hong Kong",
                "Vector Institute for AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.05435.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#multimodal",
                    "#diffusion",
                    "#open_source",
                    "#video",
                    "#optimization"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ flow matching Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ñ… ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Stable Velocity Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ flow matching. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ´Ğ²Ğ° Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ¶Ğ¸Ğ¼ Ğ²Ğ±Ğ»Ğ¸Ğ·Ğ¸ Ğ°Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ¶Ğ¸Ğ¼ Ğ²Ğ±Ğ»Ğ¸Ğ·Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ”Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ¼ĞµÑ‚Ğ¾Ğ´ StableVM Ñ Ñ€ĞµĞ´ÑƒĞºÑ†Ğ¸ĞµĞ¹ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ğ¸ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ° VA-REPA Ğ´Ğ»Ñ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ñ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ”Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ StableVS, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑƒĞ¿Ñ€Ğ¾Ñ‰Ñ‘Ğ½Ğ½ÑƒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ² Ğ½Ğ¸Ğ·ĞºĞ¾Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ Ğ¸ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ ÑĞµĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ² Ğ´Ğ²Ğ° Ñ€Ğ°Ğ·Ğ° Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°."
                },
                "en": {
                    "title": "Harnessing Low-Variance for Efficient Flow Matching",
                    "desc": "The Stable Velocity framework tackles the problem of high-variance training in flow matching by pinpointing low-variance areas and introducing techniques to reduce variance for better training efficiency and faster sampling. It identifies two regimes: a challenging high-variance regime near the prior and a more stable low-variance regime near the data distribution. The framework includes Stable Velocity Matching (StableVM), which provides an unbiased variance-reduction objective, and Variance-Aware Representation Alignment (VA-REPA) to enhance supervision in low-variance conditions. Experimental results show that this approach significantly improves training efficiency and accelerates sampling by over two times without compromising the quality of the generated samples."
                },
                "zh": {
                    "title": "ç¨³å®šé€Ÿåº¦æ¡†æ¶ï¼šæå‡è®­ç»ƒä¸é‡‡æ ·æ•ˆç‡çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "ç¨³å®šé€Ÿåº¦æ¡†æ¶é€šè¿‡è¯†åˆ«ä½æ–¹å·®åŒºåŸŸå¹¶æå‡ºæ–¹å·®å‡å°‘æŠ€æœ¯ï¼Œè§£å†³äº†æµåŒ¹é…ä¸­çš„é«˜æ–¹å·®è®­ç»ƒé—®é¢˜ï¼Œä»è€Œæé«˜äº†è®­ç»ƒæ•ˆç‡å’Œé‡‡æ ·é€Ÿåº¦ã€‚æµåŒ¹é…è™½ç„¶ä¼˜é›…ï¼Œä½†ä¾èµ–å•æ ·æœ¬æ¡ä»¶é€Ÿåº¦å¯¼è‡´é«˜æ–¹å·®è®­ç»ƒç›®æ ‡ï¼Œè¿›è€Œä½¿ä¼˜åŒ–ä¸ç¨³å®šä¸”æ”¶æ•›ç¼“æ…¢ã€‚æˆ‘ä»¬æ˜ç¡®è¡¨å¾äº†è¿™ç§æ–¹å·®ï¼Œè¯†åˆ«å‡ºé«˜æ–¹å·®åŒºåŸŸå’Œä½æ–¹å·®åŒºåŸŸï¼Œå¹¶æå‡ºäº†ç¨³å®šé€Ÿåº¦åŒ¹é…ï¼ˆStableVMï¼‰å’Œæ–¹å·®æ„ŸçŸ¥è¡¨ç¤ºå¯¹é½ï¼ˆVA-REPAï¼‰ç­‰æ–¹æ³•æ¥æ”¹å–„è®­ç»ƒã€‚é€šè¿‡åœ¨ä½æ–¹å·®åŒºåŸŸçš„åŠ¨æ€ç®€åŒ–ï¼Œæˆ‘ä»¬å®ç°äº†ç¨³å®šé€Ÿåº¦é‡‡æ ·ï¼ˆStableVSï¼‰ï¼Œåœ¨ä¸é™ä½æ ·æœ¬è´¨é‡çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—åŠ å¿«äº†é‡‡æ ·é€Ÿåº¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.02464",
            "title": "From Directions to Regions: Decomposing Activations in Language Models via Local Geometry",
            "url": "https://huggingface.co/papers/2602.02464",
            "abstract": "Mixture of Factor Analyzers (MFA) provides a scalable, unsupervised approach for discovering complex, nonlinear structures in language model activation spaces by modeling local geometric structures through Gaussian regions and their covariance properties.  \t\t\t\t\tAI-generated summary \t\t\t\t Activation decomposition methods in language models are tightly coupled to geometric assumptions on how concepts are realized in activation space. Existing approaches search for individual global directions, implicitly assuming linear separability, which overlooks concepts with nonlinear or multi-dimensional structure. In this work, we leverage Mixture of Factor Analyzers (MFA) as a scalable, unsupervised alternative that models the activation space as a collection of Gaussian regions with their local covariance structure. MFA decomposes activations into two compositional geometric objects: the region's centroid in activation space, and the local variation from the centroid. We train large-scale MFAs for Llama-3.1-8B and Gemma-2-2B, and show they capture complex, nonlinear structures in activation space. Moreover, evaluations on localization and steering benchmarks show that MFA outperforms unsupervised baselines, is competitive with supervised localization methods, and often achieves stronger steering performance than sparse autoencoders. Together, our findings position local geometry, expressed through subspaces, as a promising unit of analysis for scalable concept discovery and model control, accounting for complex structures that isolated directions fail to capture.",
            "score": 3,
            "issue_id": 1000,
            "pub_date": "2026-02-02",
            "pub_date_card": {
                "ru": "2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 2",
                "zh": "2æœˆ2æ—¥"
            },
            "hash": "9022a520ea4c5580",
            "authors": [
                "Or Shafran",
                "Shaked Ronen",
                "Omri Fahn",
                "Shauli Ravfogel",
                "Atticus Geiger",
                "Mor Geva"
            ],
            "affiliations": [
                "Blavatnik School of Computer Science and AI, Tel Aviv University",
                "Goodfire",
                "New York University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.02464.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#benchmark",
                    "#training",
                    "#architecture"
                ],
                "emoji": "ğŸ§²",
                "ru": {
                    "title": "Ğ›Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹: Ğ¾Ñ‚ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğº Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ´Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°Ğ¼",
                    "desc": "Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Mixture of Factor Analyzers (MFA) ĞºĞ°Ğº Ğ½ĞµĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ½ĞµĞ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°Ñ… Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸Ñ‰ÑƒÑ‚ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½ÑƒÑ Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ, MFA Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ ĞºĞ°Ğº ÑĞ¾Ğ²Ğ¾ĞºÑƒĞ¿Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ĞºĞ¾Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¾Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ·Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ´Ğ²Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹: Ñ†ĞµĞ½Ñ‚Ñ€Ğ¾Ğ¸Ğ´ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ° Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚ Ñ†ĞµĞ½Ñ‚Ñ€Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MFA Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ½ĞµĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚ĞµĞ½ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¸ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ."
                },
                "en": {
                    "title": "Unlocking Nonlinear Structures in Language Models with MFA",
                    "desc": "This paper introduces Mixture of Factor Analyzers (MFA) as a method for uncovering complex, nonlinear structures in the activation spaces of language models. Unlike traditional approaches that assume linear separability, MFA models the activation space using Gaussian regions, allowing for a more nuanced understanding of local geometric structures. The method decomposes activations into centroids and their local variations, enabling the capture of intricate relationships between concepts. The results demonstrate that MFA not only outperforms existing unsupervised methods but also competes well with supervised techniques, highlighting its effectiveness in scalable concept discovery and model control."
                },
                "zh": {
                    "title": "åˆ©ç”¨æ··åˆå› å­åˆ†æå™¨å‘ç°è¯­è¨€æ¨¡å‹ä¸­çš„å¤æ‚ç»“æ„",
                    "desc": "æ··åˆå› å­åˆ†æå™¨ï¼ˆMFAï¼‰æä¾›äº†ä¸€ç§å¯æ‰©å±•çš„æ— ç›‘ç£æ–¹æ³•ï¼Œç”¨äºå‘ç°è¯­è¨€æ¨¡å‹æ¿€æ´»ç©ºé—´ä¸­çš„å¤æ‚éçº¿æ€§ç»“æ„ã€‚è¯¥æ–¹æ³•é€šè¿‡é«˜æ–¯åŒºåŸŸåŠå…¶åæ–¹å·®ç‰¹æ€§æ¥å»ºæ¨¡å±€éƒ¨å‡ ä½•ç»“æ„ï¼Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•å¯¹çº¿æ€§å¯åˆ†æ€§çš„éšå«å‡è®¾ã€‚MFAå°†æ¿€æ´»åˆ†è§£ä¸ºä¸¤ä¸ªå‡ ä½•å¯¹è±¡ï¼šæ¿€æ´»ç©ºé—´ä¸­åŒºåŸŸçš„è´¨å¿ƒå’Œç›¸å¯¹äºè´¨å¿ƒçš„å±€éƒ¨å˜åŒ–ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒMFAåœ¨æ•æ‰å¤æ‚ç»“æ„æ–¹é¢ä¼˜äºæ— ç›‘ç£åŸºçº¿ï¼Œå¹¶åœ¨æ¨¡å‹æ§åˆ¶ä¸­è¡¨ç°å‡ºè‰²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.09591",
            "title": "On the Optimal Reasoning Length for RL-Trained Language Models",
            "url": "https://huggingface.co/papers/2602.09591",
            "abstract": "Length control methods in reinforcement learning-trained language models affect reasoning performance and computational efficiency, with optimal output lengths balancing these factors.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning substantially improves reasoning in large language models, but it also tends to lengthen chain of thought outputs and increase computational cost during both training and inference. Though length control methods have been proposed, it remains unclear what the optimal output length is for balancing efficiency and performance. In this work, we compare several length control methods on two models, Qwen3-1.7B Base and DeepSeek-R1-Distill-Qwen-1.5B. Our results indicate that length penalties may hinder reasoning acquisition, while properly tuned length control can improve efficiency for models with strong prior reasoning. By extending prior work to RL trained policies, we identify two failure modes, 1) long outputs increase dispersion, and 2) short outputs lead to under-thinking.",
            "score": 2,
            "issue_id": 1006,
            "pub_date": "2026-02-10",
            "pub_date_card": {
                "ru": "10 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 10",
                "zh": "2æœˆ10æ—¥"
            },
            "hash": "5c4a66da764e61ee",
            "authors": [
                "Daisuke Nohara",
                "Taishi Nakamura",
                "Rio Yokota"
            ],
            "affiliations": [
                "Institute of Science Tokyo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.09591.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#rl",
                    "#optimization",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ: ĞºĞ°Ğº ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ reinforcement learning. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğº Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ½Ğ° Ğ´Ğ²ÑƒÑ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑˆÑ‚Ñ€Ğ°Ñ„Ñ‹ Ğ·Ğ° Ğ´Ğ»Ğ¸Ğ½Ñƒ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ñ€ĞµĞ¿ÑÑ‚ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ´Ğ»Ğ¸Ğ½Ğ° Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒÑÑ‰Ğ°Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ´Ğ²Ğµ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹: ÑĞ»Ğ¸ÑˆĞºĞ¾Ğ¼ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ñ‹ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‚ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², Ğ° Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ğ¾ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´ÑÑ‚ Ğº Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°."
                },
                "en": {
                    "title": "Balancing Output Length for Efficient Reasoning in RL Models",
                    "desc": "This paper investigates how controlling the length of outputs in reinforcement learning (RL) trained language models impacts their reasoning abilities and computational efficiency. It highlights that while RL enhances reasoning, it often results in longer outputs that can increase computational costs. The authors evaluate various length control methods on two specific models to find an optimal balance between output length, reasoning performance, and efficiency. Their findings suggest that while length penalties can negatively affect reasoning, well-tuned length control can enhance efficiency without sacrificing reasoning quality."
                },
                "zh": {
                    "title": "ä¼˜åŒ–è¾“å‡ºé•¿åº¦ï¼Œæå‡æ¨ç†ä¸æ•ˆç‡çš„å¹³è¡¡",
                    "desc": "åœ¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ä¸­ï¼Œé•¿åº¦æ§åˆ¶æ–¹æ³•å¯¹æ¨ç†æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡æœ‰é‡è¦å½±å“ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæœ€ä½³çš„è¾“å‡ºé•¿åº¦å¯ä»¥åœ¨è¿™ä¸¤è€…ä¹‹é—´å–å¾—å¹³è¡¡ã€‚æˆ‘ä»¬æ¯”è¾ƒäº†å‡ ç§é•¿åº¦æ§åˆ¶æ–¹æ³•åœ¨ä¸åŒæ¨¡å‹ä¸Šçš„æ•ˆæœï¼Œå‘ç°é•¿åº¦æƒ©ç½šå¯èƒ½ä¼šå¦¨ç¢æ¨ç†èƒ½åŠ›çš„è·å–ã€‚é€‚å½“è°ƒæ•´çš„é•¿åº¦æ§åˆ¶å¯ä»¥æé«˜å…·æœ‰å¼ºæ¨ç†èƒ½åŠ›æ¨¡å‹çš„æ•ˆç‡ï¼ŒåŒæ—¶ä¹Ÿè¯†åˆ«äº†ä¸¤ç§å¤±è´¥æ¨¡å¼ï¼šè¿‡é•¿çš„è¾“å‡ºä¼šå¢åŠ åˆ†æ•£æ€§ï¼Œè¿‡çŸ­çš„è¾“å‡ºåˆ™ä¼šå¯¼è‡´æ€è€ƒä¸è¶³ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.09000",
            "title": "iGRPO: Self-Feedback-Driven LLM Reasoning",
            "url": "https://huggingface.co/papers/2602.09000",
            "abstract": "Iterative Group Relative Policy Optimization enhances mathematical reasoning in large language models through a two-stage process combining exploratory drafting and refined iterations, achieving state-of-the-art results on competitive benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have shown promise in solving complex mathematical problems, yet they still fall short of producing accurate and consistent solutions. Reinforcement Learning (RL) is a framework for aligning these models with task-specific rewards, improving overall quality and reliability. Group Relative Policy Optimization (GRPO) is an efficient, value-function-free alternative to Proximal Policy Optimization (PPO) that leverages group-relative reward normalization. We introduce Iterative Group Relative Policy Optimization (iGRPO), a two-stage extension of GRPO that adds dynamic self-conditioning through model-generated drafts. In Stage 1, iGRPO samples multiple exploratory drafts and selects the highest-reward draft using the same scalar reward signal used for optimization. In Stage 2, it appends this best draft to the original prompt and applies a GRPO-style update on draft-conditioned refinements, training the policy to improve beyond its strongest prior attempt. Under matched rollout budgets, iGRPO consistently outperforms GRPO across base models (e.g., Nemotron-H-8B-Base-8K and DeepSeek-R1 Distilled), validating its effectiveness on diverse reasoning benchmarks. Moreover, applying iGRPO to OpenReasoning-Nemotron-7B trained on AceReason-Math achieves new state-of-the-art results of 85.62\\% and 79.64\\% on AIME24 and AIME25, respectively. Ablations further show that the refinement wrapper generalizes beyond GRPO variants, benefits from a generative judge, and alters learning dynamics by delaying entropy collapse. These results underscore the potential of iterative, self-feedback-based RL for advancing verifiable mathematical reasoning.",
            "score": 2,
            "issue_id": 1008,
            "pub_date": "2026-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "f2205cc1a0e37bbe",
            "authors": [
                "Ali Hatamizadeh",
                "Shrimai Prabhumoye",
                "Igor Gitman",
                "Ximing Lu",
                "Seungju Han",
                "Wei Ping",
                "Yejin Choi",
                "Jan Kautz"
            ],
            "affiliations": [
                "NVIDIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.09000.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#rl",
                    "#math",
                    "#alignment",
                    "#rlhf",
                    "#reasoning"
                ],
                "emoji": "ğŸ§®",
                "ru": {
                    "title": "Ğ˜Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ñ‡ĞµÑ€ĞµĞ· ÑĞ°Ğ¼Ğ¾Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ñ‡ĞµÑ€Ğ½Ğ¾Ğ²Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Iterative Group Relative Policy Optimization (iGRPO), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ¾Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¹ Ğ¸Ğ· Ğ½Ğ¸Ñ… Ğ¿Ğ¾ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğµ, Ğ½Ğ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¾Ğ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ğ¾ Ğ»ÑƒÑ‡ÑˆĞµĞµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ğº ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğµ Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Group Relative Policy Optimization (GRPO) Ğ¸ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞµĞ³Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ñ‡ĞµĞ¼ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¹ PPO. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ iGRPO Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğ° Ğ½Ğ° ĞºĞ¾Ğ½ĞºÑƒÑ€ÑĞ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ 85.62% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° AIME24."
                },
                "en": {
                    "title": "Enhancing Mathematical Reasoning in LLMs with iGRPO",
                    "desc": "This paper presents Iterative Group Relative Policy Optimization (iGRPO), a method that improves the mathematical reasoning capabilities of large language models (LLMs). It employs a two-stage process where the model first generates multiple drafts of a solution and selects the best one based on a reward signal. In the second stage, this selected draft is refined further, enhancing the model's performance beyond its previous attempts. The results demonstrate that iGRPO achieves state-of-the-art performance on various reasoning benchmarks, showcasing the effectiveness of reinforcement learning in improving LLMs' accuracy and reliability."
                },
                "zh": {
                    "title": "è¿­ä»£ä¼˜åŒ–ï¼Œæå‡æ•°å­¦æ¨ç†èƒ½åŠ›ï¼",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºè¿­ä»£ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆiGRPOï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†æ–¹é¢çš„è¡¨ç°ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸¤ä¸ªé˜¶æ®µçš„è¿‡ç¨‹ï¼Œé¦–å…ˆç”Ÿæˆå¤šä¸ªæ¢ç´¢æ€§è‰ç¨¿ï¼Œç„¶åé€‰æ‹©æœ€ä½³è‰ç¨¿è¿›è¡Œè¿›ä¸€æ­¥ä¼˜åŒ–ã€‚iGRPOåˆ©ç”¨å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€è‡ªæˆ‘è°ƒèŠ‚æ¥æå‡æ¨¡å‹çš„å‡†ç¡®æ€§å’Œä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒiGRPOåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æŠ€æœ¯ï¼Œå±•ç¤ºäº†åŸºäºè¿­ä»£è‡ªåé¦ˆçš„å¼ºåŒ–å­¦ä¹ åœ¨æ•°å­¦æ¨ç†ä¸­çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.08503",
            "title": "Learning Self-Correction in Vision-Language Models via Rollout Augmentation",
            "url": "https://huggingface.co/papers/2602.08503",
            "abstract": "Octopus, an RL rollout augmentation framework, enables efficient self-correction learning in vision-language models through synthetic example generation and response masking strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t Self-correction is essential for solving complex reasoning problems in vision-language models (VLMs). However, existing reinforcement learning (RL) methods struggle to learn it, as effective self-correction behaviors emerge only rarely, making learning signals extremely sparse. To address this challenge, we propose correction-specific rollouts (Octopus), an RL rollout augmentation framework that synthesizes dense self-correction examples by recombining existing rollouts. This augmentation simultaneously improves sample efficiency due to rollout reuse and stabilizes RL optimization through balanced supervision. Furthermore, we introduce a response-masking strategy that decouples self-correction from direct reasoning, avoiding signal conflicts and enabling both behaviors to be learned effectively. Building on this, we introduce Octopus-8B, a reasoning VLM with controllable self-correction capability. Across 7 benchmarks, it achieves SoTA performance among open-source VLMs, outperforming the best RLVR baseline by 1.0 score while requiring only 0.72times training time per step.",
            "score": 2,
            "issue_id": 1000,
            "pub_date": "2026-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "9c803b2b79ca8254",
            "authors": [
                "Yi Ding",
                "Ziliang Qiu",
                "Bolian Li",
                "Ruqi Zhang"
            ],
            "affiliations": [
                "Department of Computer Science, Purdue University, West Lafayette, USA",
                "School of Information Sciences, University of Illinois Urbana-Champaign, Champaign, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.08503.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#rl",
                    "#training",
                    "#multimodal",
                    "#open_source",
                    "#cv",
                    "#synthetic",
                    "#optimization"
                ],
                "emoji": "ğŸ™",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ñ Ğ² vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‡ĞµÑ€ĞµĞ· Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Octopus Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸ Ğ² vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¿ĞµÑ€ĞµkomĞ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ (rollouts), Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ° ÑÑ‡Ñ‘Ñ‚ Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ñ Ğ¸ Ğ¿Ñ€ÑĞ¼Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ, Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°Ñ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ñ‹ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ². ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Octopus-8B, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ÑÑ€ĞµĞ´Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Octopus: Enhancing Self-Correction in Vision-Language Models",
                    "desc": "The paper presents Octopus, a framework designed to enhance self-correction learning in vision-language models (VLMs) using reinforcement learning (RL). It addresses the challenge of sparse learning signals by generating synthetic examples through correction-specific rollouts, which improves sample efficiency and stabilizes the RL optimization process. Additionally, a response-masking strategy is introduced to separate self-correction from direct reasoning, allowing both to be learned without interference. The results show that Octopus-8B achieves state-of-the-art performance on multiple benchmarks while being more efficient in training time compared to existing methods."
                },
                "zh": {
                    "title": "Octopusï¼šæå‡è§†è§‰è¯­è¨€æ¨¡å‹è‡ªæˆ‘çº æ­£èƒ½åŠ›çš„æ¡†æ¶",
                    "desc": "Octopusæ˜¯ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å›æ»šå¢å¼ºæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åˆæˆç¤ºä¾‹ç”Ÿæˆå’Œå“åº”æ©è”½ç­–ç•¥æ¥æé«˜è§†è§‰è¯­è¨€æ¨¡å‹çš„è‡ªæˆ‘çº æ­£å­¦ä¹ æ•ˆç‡ã€‚è‡ªæˆ‘çº æ­£å¯¹äºè§£å†³è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„å¤æ‚æ¨ç†é—®é¢˜è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å­¦ä¹ è‡ªæˆ‘çº æ­£è¡Œä¸ºæ—¶é¢ä¸´æŒ‘æˆ˜ã€‚Octopusé€šè¿‡é‡æ–°ç»„åˆç°æœ‰å›æ»šï¼Œåˆæˆå¯†é›†çš„è‡ªæˆ‘çº æ­£ç¤ºä¾‹ï¼Œä»è€Œæé«˜æ ·æœ¬æ•ˆç‡å¹¶ç¨³å®šRLä¼˜åŒ–ã€‚æ­¤å¤–ï¼Œå“åº”æ©è”½ç­–ç•¥ä½¿è‡ªæˆ‘çº æ­£ä¸ç›´æ¥æ¨ç†è§£è€¦ï¼Œé¿å…ä¿¡å·å†²çªï¼Œæœ‰æ•ˆåœ°å­¦ä¹ è¿™ä¸¤ç§è¡Œä¸ºã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.07755",
            "title": "Learning to Continually Learn via Meta-learning Agentic Memory Designs",
            "url": "https://huggingface.co/papers/2602.07755",
            "abstract": "ALMA is a framework that uses meta-learning to automatically discover memory designs for agentic systems, enabling continual learning without human engineering across diverse domains.  \t\t\t\t\tAI-generated summary \t\t\t\t The statelessness of foundation models bottlenecks agentic systems' ability to continually learn, a core capability for long-horizon reasoning and adaptation. To address this limitation, agentic systems commonly incorporate memory modules to retain and reuse past experience, aiming for continual learning during test time. However, most existing memory designs are human-crafted and fixed, which limits their ability to adapt to the diversity and non-stationarity of real-world tasks. In this paper, we introduce ALMA (Automated meta-Learning of Memory designs for Agentic systems), a framework that meta-learns memory designs to replace hand-engineered memory designs, therefore minimizing human effort and enabling agentic systems to be continual learners across diverse domains. Our approach employs a Meta Agent that searches over memory designs expressed as executable code in an open-ended manner, theoretically allowing the discovery of arbitrary memory designs, including database schemas as well as their retrieval and update mechanisms. Extensive experiments across four sequential decision-making domains demonstrate that the learned memory designs enable more effective and efficient learning from experience than state-of-the-art human-crafted memory designs on all benchmarks. When developed and deployed safely, ALMA represents a step toward self-improving AI systems that learn to be adaptive, continual learners.",
            "score": 2,
            "issue_id": 1003,
            "pub_date": "2026-02-08",
            "pub_date_card": {
                "ru": "8 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 8",
                "zh": "2æœˆ8æ—¥"
            },
            "hash": "c2f25eb338c19894",
            "authors": [
                "Yiming Xiong",
                "Shengran Hu",
                "Jeff Clune"
            ],
            "affiliations": [
                "Canada CIFAR AI Chair",
                "University of British Columbia",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.07755.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#reasoning",
                    "#agents",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ°ÑÑÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ: Ğ¾Ñ‚ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "ALMA â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¼ĞµÑ‚Ğ°Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ² Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…. ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¾Ñ€Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğµ Ğ¸Ğ¼ĞµÑÑ‚ Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº continual learning Ğ¸ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¼Ñƒ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ¡ÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ñ€ÑƒÑ‡Ğ½ÑƒÑ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ½Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğº Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑÑ…ĞµĞ¼Ñ‹ Ğ±Ğ°Ğ·Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ¸Ñ… Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ hand-engineered Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "ALMA: Automating Memory Design for Adaptive Learning",
                    "desc": "ALMA is a framework that leverages meta-learning to automatically create memory designs for agentic systems, enhancing their ability to learn continuously without manual intervention. Traditional memory designs are often fixed and human-engineered, which restricts their adaptability to changing environments. By using a Meta Agent, ALMA explores various memory architectures, allowing for the discovery of innovative designs that can better handle diverse tasks. Experimental results show that these learned memory designs outperform existing human-crafted solutions, paving the way for more autonomous and adaptive AI systems."
                },
                "zh": {
                    "title": "ALMAï¼šè‡ªåŠ¨åŒ–è®°å¿†è®¾è®¡çš„å…ƒå­¦ä¹ æ¡†æ¶",
                    "desc": "ALMAæ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œåˆ©ç”¨å…ƒå­¦ä¹ è‡ªåŠ¨å‘ç°ä»£ç†ç³»ç»Ÿçš„è®°å¿†è®¾è®¡ï¼Œä»è€Œå®ç°æŒç»­å­¦ä¹ ã€‚ä¼ ç»Ÿçš„è®°å¿†æ¨¡å—é€šå¸¸æ˜¯äººå·¥è®¾è®¡çš„ï¼Œé™åˆ¶äº†ç³»ç»Ÿåœ¨å¤šæ ·åŒ–å’Œéå¹³ç¨³ä»»åŠ¡ä¸­çš„é€‚åº”èƒ½åŠ›ã€‚ALMAé€šè¿‡ä¸€ä¸ªå…ƒä»£ç†ï¼Œæ¢ç´¢å¯æ‰§è¡Œä»£ç å½¢å¼çš„è®°å¿†è®¾è®¡ï¼Œèƒ½å¤Ÿå‘ç°ä»»æ„çš„è®°å¿†ç»“æ„ã€‚å®éªŒè¡¨æ˜ï¼ŒALMAå­¦ä¹ çš„è®°å¿†è®¾è®¡åœ¨å¤šä¸ªå†³ç­–é¢†åŸŸä¸­æ¯”ä¼ ç»Ÿçš„äººå·¥è®¾è®¡æ›´æœ‰æ•ˆç‡ï¼Œæ¨åŠ¨äº†è‡ªæˆ‘æ”¹è¿›çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.05892",
            "title": "ContextBench: A Benchmark for Context Retrieval in Coding Agents",
            "url": "https://huggingface.co/papers/2602.05892",
            "abstract": "ContextBench evaluates context retrieval in coding agents through detailed process analysis, revealing that advanced agent designs provide limited improvements in context usage while highlighting gaps between explored and utilized information.  \t\t\t\t\tAI-generated summary \t\t\t\t LLM-based coding agents have shown strong performance on automated issue resolution benchmarks, yet existing evaluations largely focus on final task success, providing limited insight into how agents retrieve and use code context during problem solving. We introduce ContextBench, a process-oriented evaluation of context retrieval in coding agents. ContextBench consists of 1,136 issue-resolution tasks from 66 repositories across eight programming languages, each augmented with human-annotated gold contexts. We further implement an automated evaluation framework that tracks agent trajectories and measures context recall, precision, and efficiency throughout issue resolution. Using ContextBench, we evaluate four frontier LLMs and five coding agents. Our results show that sophisticated agent scaffolding yields only marginal gains in context retrieval (\"The Bitter Lesson\" of coding agents), LLMs consistently favor recall over precision, and substantial gaps exist between explored and utilized context. ContextBench augments existing end-to-end benchmarks with intermediate gold-context metrics that unbox the issue-resolution process. These contexts offer valuable intermediate signals for guiding LLM reasoning in software tasks.",
            "score": 2,
            "issue_id": 1007,
            "pub_date": "2026-02-05",
            "pub_date_card": {
                "ru": "5 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 5",
                "zh": "2æœˆ5æ—¥"
            },
            "hash": "690f03fb06800d0c",
            "authors": [
                "Han Li",
                "Letian Zhu",
                "Bohan Zhang",
                "Rili Feng",
                "Jiaming Wang",
                "Yue Pan",
                "Earl T. Barr",
                "Sarro Federica",
                "Zhaoyang Chu",
                "He Ye"
            ],
            "affiliations": [
                "Nanjing University",
                "University College London"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.05892.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#plp",
                    "#agents"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² ĞºĞ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ñ…: Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ContextBench â€” Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‚Ğ¾Ğ³Ğ¾, ĞºĞ°Ğº Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ÑÑ‚ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ ĞºĞ¾Ğ´Ğ° Ğ¿Ñ€Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° 1136 Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸Ğ· 66 Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸ĞµĞ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ğ·Ğ¾Ğ»Ğ¾Ñ‚Ñ‹Ğµ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ñ‹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ°ÑÑ‚ Ğ»Ğ¸ÑˆÑŒ Ğ½ĞµĞ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ğ¸Ñ‚Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ñƒ Ğ½Ğ°Ğ´ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€ÑƒÑ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚, Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€ÑƒÑ Ğ¾Ğ½Ğ¸ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¿Ñ€Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡."
                },
                "en": {
                    "title": "ContextBench: Unpacking Context Retrieval in Coding Agents",
                    "desc": "This paper introduces ContextBench, a new evaluation framework for assessing how coding agents retrieve and utilize context during problem-solving tasks. It analyzes 1,136 issue-resolution tasks across various programming languages, providing insights into the effectiveness of different coding agents and large language models (LLMs). The findings reveal that while advanced agent designs show slight improvements in context usage, there are significant gaps between the context that agents explore and what they actually utilize. ContextBench enhances traditional evaluations by focusing on intermediate metrics, which can help improve LLM reasoning in software development tasks."
                },
                "zh": {
                    "title": "ContextBenchï¼šæ­ç¤ºç¼–ç ä»£ç†ä¸Šä¸‹æ–‡æ£€ç´¢çš„çœŸç›¸",
                    "desc": "ContextBench æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°ç¼–ç ä»£ç†åœ¨ä¸Šä¸‹æ–‡æ£€ç´¢æ–¹é¢çš„å·¥å…·ã€‚å®ƒé€šè¿‡åˆ†æç¼–ç ä»£ç†åœ¨è§£å†³é—®é¢˜æ—¶å¦‚ä½•æ£€ç´¢å’Œä½¿ç”¨ä»£ç ä¸Šä¸‹æ–‡ï¼Œæ­ç¤ºäº†é«˜çº§ä»£ç†è®¾è®¡åœ¨ä¸Šä¸‹æ–‡ä½¿ç”¨ä¸Šçš„æœ‰é™æ”¹è¿›ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡å¤æ‚çš„ä»£ç†æ¡†æ¶å¸¦æ¥äº†è¾¹é™…æ”¶ç›Šï¼Œä½†åœ¨æ£€ç´¢ä¸Šä¸‹æ–‡æ—¶ï¼ŒLLM æ›´å€¾å‘äºå¬å›è€Œéç²¾ç¡®åº¦ã€‚æ­¤å¤–ï¼Œæ¢ç´¢çš„ä¸Šä¸‹æ–‡ä¸å®é™…ä½¿ç”¨çš„ä¸Šä¸‹æ–‡ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®è·ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.10099",
            "title": "Learning on the Manifold: Unlocking Standard Diffusion Transformers with Representation Encoders",
            "url": "https://huggingface.co/papers/2602.10099",
            "abstract": "Geometric interference in standard diffusion transformers prevents convergence on representation encoders, which is resolved through Riemannian flow matching with jacobi regularization enabling effective training without width scaling.  \t\t\t\t\tAI-generated summary \t\t\t\t Leveraging representation encoders for generative modeling offers a path for efficient, high-fidelity synthesis. However, standard diffusion transformers fail to converge on these representations directly. While recent work attributes this to a capacity bottleneck proposing computationally expensive width scaling of diffusion transformers we demonstrate that the failure is fundamentally geometric. We identify Geometric Interference as the root cause: standard Euclidean flow matching forces probability paths through the low-density interior of the hyperspherical feature space of representation encoders, rather than following the manifold surface. To resolve this, we propose Riemannian Flow Matching with Jacobi Regularization (RJF). By constraining the generative process to the manifold geodesics and correcting for curvature-induced error propagation, RJF enables standard Diffusion Transformer architectures to converge without width scaling. Our method RJF enables the standard DiT-B architecture (131M parameters) to converge effectively, achieving an FID of 3.37 where prior methods fail to converge. Code: https://github.com/amandpkr/RJF",
            "score": 1,
            "issue_id": 1003,
            "pub_date": "2026-02-10",
            "pub_date_card": {
                "ru": "10 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 10",
                "zh": "2æœˆ10æ—¥"
            },
            "hash": "220d226794862a3d",
            "authors": [
                "Amandeep Kumar",
                "Vishal M. Patel"
            ],
            "affiliations": [
                "Johns Hopkins University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.10099.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#math",
                    "#optimization",
                    "#open_source",
                    "#diffusion",
                    "#architecture"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ“ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ: Ğ¿Ğ¾Ñ‚Ğ¾Ğº Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ĞµĞ²ĞºĞ»Ğ¸Ğ´Ğ¾Ğ²Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ½Ğ° Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑÑ…, Ğ·Ğ°ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¹ ĞµĞ²ĞºĞ»Ğ¸Ğ´Ğ¾Ğ² flow matching Ğ·Ğ°ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğµ Ğ¿ÑƒÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ³Ğ¸Ğ¿ĞµÑ€ÑÑ„ĞµÑ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ¿ÑÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Riemannian Flow Matching Ñ Jacobi Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ³ĞµĞ¾Ğ´ĞµĞ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ»Ğ¸Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸, Ğ²Ñ‹Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ĞºÑ€Ğ¸Ğ²Ğ¸Ğ·Ğ½Ğ¾Ğ¹. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ğ¼ Diffusion Transformer Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑˆĞ¸Ñ€Ğ¸Ğ½Ñ‹ ÑĞµÑ‚Ğ¸."
                },
                "en": {
                    "title": "Overcoming Geometric Interference for Better Diffusion Transformer Training",
                    "desc": "This paper addresses the issue of convergence in standard diffusion transformers when using representation encoders for generative modeling. The authors identify a problem called Geometric Interference, which arises from the way probability paths are matched in the feature space. They propose a new method called Riemannian Flow Matching with Jacobi Regularization (RJF) that allows the generative process to follow the correct geometric paths on the manifold. By using RJF, the authors demonstrate that diffusion transformers can effectively train without the need for width scaling, achieving significant improvements in performance."
                },
                "zh": {
                    "title": "è§£å†³å‡ ä½•å¹²æ‰°ï¼Œå®ç°æœ‰æ•ˆæ”¶æ•›",
                    "desc": "åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†æ ‡å‡†æ‰©æ•£å˜æ¢å™¨åœ¨è¡¨ç¤ºç¼–ç å™¨ä¸Šçš„æ”¶æ•›é—®é¢˜ï¼Œå‘ç°å…¶æ ¹æœ¬åŸå› æ˜¯å‡ ä½•å¹²æ‰°ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºé»æ›¼æµåŒ¹é…ä¸é›…å¯æ¯”æ­£åˆ™åŒ–ï¼ˆRJFï¼‰ï¼Œå¯ä»¥æœ‰æ•ˆè§£å†³è¿™ä¸€é—®é¢˜ã€‚é€šè¿‡å°†ç”Ÿæˆè¿‡ç¨‹é™åˆ¶åœ¨æµå½¢æµ‹åœ°çº¿å¹¶ä¿®æ­£æ›²ç‡å¼•èµ·çš„è¯¯å·®ä¼ æ’­ï¼ŒRJFä½¿å¾—æ ‡å‡†æ‰©æ•£å˜æ¢å™¨èƒ½å¤Ÿåœ¨ä¸æ‰©å±•å®½åº¦çš„æƒ…å†µä¸‹æ”¶æ•›ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒRJFæ–¹æ³•åœ¨æ ‡å‡†DiT-Bæ¶æ„ä¸Šå®ç°äº†æ˜¾è‘—çš„æ”¶æ•›æ•ˆæœï¼ŒFIDå€¼è¾¾åˆ°3.37ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.09924",
            "title": "LLMs Encode Their Failures: Predicting Success from Pre-Generation Activations",
            "url": "https://huggingface.co/papers/2602.09924",
            "abstract": "LLMs' internal representations can predict problem difficulty and enable efficient inference routing that reduces costs while maintaining performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Running LLMs with extended reasoning on every problem is expensive, but determining which inputs actually require additional compute remains challenging. We investigate whether their own likelihood of success is recoverable from their internal representations before generation, and if this signal can guide more efficient inference. We train linear probes on pre-generation activations to predict policy-specific success on math and coding tasks, substantially outperforming surface features such as question length and TF-IDF. Using E2H-AMC, which provides both human and model performance on identical problems, we show that models encode a model-specific notion of difficulty that is distinct from human difficulty, and that this distinction increases with extended reasoning. Leveraging these probes, we demonstrate that routing queries across a pool of models can exceed the best-performing model whilst reducing inference cost by up to 70\\% on MATH, showing that internal representations enable practical efficiency gains even when they diverge from human intuitions about difficulty. Our code is available at: https://github.com/KabakaWilliam/llms_know_difficulty",
            "score": 1,
            "issue_id": 1002,
            "pub_date": "2026-02-10",
            "pub_date_card": {
                "ru": "10 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 10",
                "zh": "2æœˆ10æ—¥"
            },
            "hash": "11fccc31662bc946",
            "authors": [
                "William Lugoloobi",
                "Thomas Foster",
                "William Bankes",
                "Chris Russell"
            ],
            "affiliations": [
                "Department of Computer Science, University College London",
                "FLAIR, University of Oxford",
                "Oxford Internet Institute, University of Oxford"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.09924.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#math",
                    "#inference"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞœĞ¾Ğ´ĞµĞ»Ğ¸ ÑĞ°Ğ¼Ğ¸ Ğ·Ğ½Ğ°ÑÑ‚, Ğ½Ğ°ÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ğ° Ğ´Ğ»Ñ Ğ½Ğ¸Ñ…",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ LLM ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ² ÑĞ²Ğ¾Ğ¸Ñ… Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑÑ… Ğ´Ğ¾ Ñ‚Ğ¾Ğ³Ğ¾, ĞºĞ°Ğº Ğ½Ğ°Ñ‡Ğ½ÑƒÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚. ĞĞ½Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğµ Ğ·Ğ¾Ğ½Ğ´Ñ‹ Ğ½Ğ° Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸ÑÑ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ ÑƒÑĞ¿ĞµÑ…Ğ° Ğ½Ğ° Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ²Ñ€Ğ¾Ğ´Ğµ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ÑÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸Ğ½Ğ°Ñ‡Ğµ, Ñ‡ĞµĞ¼ Ğ»ÑĞ´Ğ¸, Ğ¸ ÑÑ‚Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¸Ñ†Ğ° Ñ€Ğ°ÑÑ‚Ñ‘Ñ‚ Ğ¿Ñ€Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ğ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ÑÑ‚Ğ¾Ğ¼Ñƒ Ğ¾Ğ½Ğ¸ ÑĞ¼Ğ¾Ğ³Ğ»Ğ¸ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ Ğ½Ğ° 70% Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Unlocking Efficiency: Predicting Problem Difficulty with LLMs",
                    "desc": "This paper explores how the internal representations of large language models (LLMs) can predict the difficulty of problems, allowing for more efficient use of computational resources. By training linear probes on the models' pre-generation activations, the authors demonstrate that these internal signals can outperform traditional features like question length in predicting success on math and coding tasks. The study reveals that LLMs have a unique understanding of problem difficulty that differs from human perceptions, especially when extended reasoning is involved. Ultimately, the research shows that intelligently routing queries based on these internal representations can significantly reduce inference costs while maintaining or even improving performance."
                },
                "zh": {
                    "title": "åˆ©ç”¨å†…éƒ¨è¡¨ç¤ºæå‡æ¨ç†æ•ˆç‡",
                    "desc": "æœ¬æ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å†…éƒ¨è¡¨ç¤ºå¦‚ä½•é¢„æµ‹é—®é¢˜çš„éš¾åº¦ï¼Œå¹¶é€šè¿‡é«˜æ•ˆçš„æ¨ç†è·¯ç”±æ¥é™ä½è®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒæ€§èƒ½ã€‚æˆ‘ä»¬å‘ç°ï¼Œæ¨¡å‹åœ¨ç”Ÿæˆä¹‹å‰çš„å†…éƒ¨è¡¨ç¤ºèƒ½å¤Ÿåæ˜ å…¶æˆåŠŸçš„å¯èƒ½æ€§ï¼Œä»è€ŒæŒ‡å¯¼æ›´é«˜æ•ˆçš„æ¨ç†ã€‚é€šè¿‡è®­ç»ƒçº¿æ€§æ¢é’ˆï¼Œæˆ‘ä»¬åœ¨æ•°å­¦å’Œç¼–ç¨‹ä»»åŠ¡ä¸Šé¢„æµ‹ç‰¹å®šç­–ç•¥çš„æˆåŠŸç‡ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„è¡¨é¢ç‰¹å¾ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬å±•ç¤ºäº†é€šè¿‡åœ¨æ¨¡å‹æ± ä¸­è·¯ç”±æŸ¥è¯¢ï¼Œå¯ä»¥åœ¨é™ä½æ¨ç†æˆæœ¬çš„åŒæ—¶è¶…è¶Šæœ€ä½³æ¨¡å‹çš„è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.08519",
            "title": "Bridging Academia and Industry: A Comprehensive Benchmark for Attributed Graph Clustering",
            "url": "https://huggingface.co/papers/2602.08519",
            "abstract": "PyAGC presents a production-ready benchmark and library for attributed graph clustering that addresses limitations of current research through scalable, memory-efficient implementations and comprehensive evaluation protocols.  \t\t\t\t\tAI-generated summary \t\t\t\t Attributed Graph Clustering (AGC) is a fundamental unsupervised task that integrates structural topology and node attributes to uncover latent patterns in graph-structured data. Despite its significance in industrial applications such as fraud detection and user segmentation, a significant chasm persists between academic research and real-world deployment. Current evaluation protocols suffer from the small-scale, high-homophily citation datasets, non-scalable full-batch training paradigms, and a reliance on supervised metrics that fail to reflect performance in label-scarce environments. To bridge these gaps, we present PyAGC, a comprehensive, production-ready benchmark and library designed to stress-test AGC methods across diverse scales and structural properties. We unify existing methodologies into a modular Encode-Cluster-Optimize framework and, for the first time, provide memory-efficient, mini-batch implementations for a wide array of state-of-the-art AGC algorithms. Our benchmark curates 12 diverse datasets, ranging from 2.7K to 111M nodes, specifically incorporating industrial graphs with complex tabular features and low homophily. Furthermore, we advocate for a holistic evaluation protocol that mandates unsupervised structural metrics and efficiency profiling alongside traditional supervised metrics. Battle-tested in high-stakes industrial workflows at Ant Group, this benchmark offers the community a robust, reproducible, and scalable platform to advance AGC research towards realistic deployment. The code and resources are publicly available via GitHub (https://github.com/Cloudy1225/PyAGC), PyPI (https://pypi.org/project/pyagc), and Documentation (https://pyagc.readthedocs.io).",
            "score": 1,
            "issue_id": 1004,
            "pub_date": "2026-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "a651a5c312507d66",
            "authors": [
                "Yunhui Liu",
                "Pengyu Qiu",
                "Yu Xing",
                "Yongchao Liu",
                "Peng Du",
                "Chuntao Hong",
                "Jiajun Zheng",
                "Tao Zheng",
                "Tieke He"
            ],
            "affiliations": [
                "Ant Group",
                "Nanjing University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.08519.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#dataset",
                    "#benchmark",
                    "#open_source",
                    "#graphs"
                ],
                "emoji": "ğŸ”—",
                "ru": {
                    "title": "ĞÑ‚ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğº Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞµ: Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¸Ğ½Ğ´ÑƒÑÑ‚Ñ€Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "PyAGC Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ², Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ñ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ ÑƒĞ·Ğ»Ğ¾Ğ² Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ². Ğ‘Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ° Ñ€ĞµÑˆĞ°ĞµÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸-Ğ±Ğ°Ñ‚Ñ‡ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ². Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 12 Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ¾Ñ‚ 2.7K Ğ´Ğ¾ 111M ÑƒĞ·Ğ»Ğ¾Ğ², ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ ÑÑ„Ğ¾ĞºÑƒÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ñ„Ğ°Ñ… Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğ³Ğ¾Ğ¼Ğ¾Ñ„Ğ¸Ğ»Ğ¸ĞµĞ¹ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ Ğ½ĞµĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ°Ğ¿Ñ€Ğ¾Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ€Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ñ… Ant Group."
                },
                "en": {
                    "title": "Bridging Research and Real-World AGC with PyAGC",
                    "desc": "PyAGC is a new benchmark and library for attributed graph clustering (AGC) that aims to improve the transition from research to real-world applications. It addresses the limitations of existing methods by providing scalable and memory-efficient implementations, along with a comprehensive evaluation framework. The library includes a modular Encode-Cluster-Optimize structure and supports mini-batch processing for various AGC algorithms. With 12 diverse datasets and a focus on unsupervised metrics, PyAGC is designed to enhance the robustness and scalability of AGC research in practical scenarios."
                },
                "zh": {
                    "title": "PyAGCï¼šæ¨åŠ¨å±æ€§å›¾èšç±»çš„ç°å®åº”ç”¨",
                    "desc": "PyAGCæ˜¯ä¸€ä¸ªä¸ºå±æ€§å›¾èšç±»ï¼ˆAGCï¼‰æä¾›çš„ç”Ÿäº§çº§åŸºå‡†å’Œåº“ï¼Œæ—¨åœ¨è§£å†³å½“å‰ç ”ç©¶çš„å±€é™æ€§ã€‚å®ƒé€šè¿‡å¯æ‰©å±•å’Œå†…å­˜é«˜æ•ˆçš„å®ç°ï¼Œç»“åˆå…¨é¢çš„è¯„ä¼°åè®®ï¼Œæ¥æ”¯æŒAGCæ–¹æ³•çš„æµ‹è¯•ã€‚è¯¥åº“æ•´åˆäº†ç°æœ‰çš„æ–¹æ³•ï¼Œé‡‡ç”¨æ¨¡å—åŒ–çš„ç¼–ç -èšç±»-ä¼˜åŒ–æ¡†æ¶ï¼Œå¹¶é¦–æ¬¡æä¾›äº†å¤šç§æœ€å…ˆè¿›AGCç®—æ³•çš„å†…å­˜é«˜æ•ˆå°æ‰¹é‡å®ç°ã€‚PyAGCçš„åŸºå‡†åŒ…å«12ä¸ªå¤šæ ·åŒ–çš„æ•°æ®é›†ï¼Œç‰¹åˆ«é€‚ç”¨äºå·¥ä¸šå›¾å½¢ï¼Œæ¨åŠ¨AGCç ”ç©¶å‘ç°å®éƒ¨ç½²è¿ˆè¿›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.07670",
            "title": "Surprisal-Guided Selection: Compute-Optimal Test-Time Strategies for Execution-Grounded Code Generation",
            "url": "https://huggingface.co/papers/2602.07670",
            "abstract": "Test-time training fails in verification-grounded tasks due to over-sharpening, while surprisal-guided selection improves performance by favoring diverse, low-confidence samples.  \t\t\t\t\tAI-generated summary \t\t\t\t Test-time training (TTT) adapts language models through gradient-based updates at inference. But is adaptation the right strategy? We study compute-optimal test-time strategies for verifiable execution-grounded (VEG) tasks, domains like GPU kernel optimization where a deterministic evaluator provides dense, continuous reward signals. Using KernelBench as our testbed and a 120B-parameter model (GPT-OSS-120B with LoRA adaptation), we find that search outperforms minimal adaptation (1-5 gradient steps): Best-of-N sampling achieves 90% task success (18/20 tasks) at K=64 across the full KernelBench L1 eval set while TTT's best checkpoint reaches only 30.6% (3-seed mean), with TTT's \"equivalent K\" falling below 1, worse than single-sample inference. The failure mode is over-sharpening: gradient updates collapse diversity toward mediocre solutions rather than discovering optimal ones. Our main contribution is surprisal-guided selection: selecting the highest-surprisal (lowest-confidence) correct sample yields 80% success vs. 50% for most-confident selection, a 30% improvement. Extending to surprisal-guided-top3 matches oracle performance at 100%. This zero-cost strategy, validated through length-controlled analysis, recovers oracle performance. For dense-reward VEG tasks, compute should be allocated to sample diversity and intelligent selection rather than gradient adaptation. The surprisal-guided selection principle may generalize to other execution-grounded domains where optimal solutions occupy the distribution tail.",
            "score": 1,
            "issue_id": 1007,
            "pub_date": "2026-02-07",
            "pub_date_card": {
                "ru": "7 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 7",
                "zh": "2æœˆ7æ—¥"
            },
            "hash": "4afcf342a1f684e7",
            "authors": [
                "Jarrod Barnes"
            ],
            "affiliations": [
                "Arc Intelligence"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.07670.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#inference"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ: surprisal-guided selection Ğ´Ğ»Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼, Ğ³Ğ´Ğµ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° (TTT) Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ¿ĞµÑ€ĞµĞ¾Ğ±Ğ¾ÑÑ‚Ñ€ĞµĞ½Ğ¸Ñ â€” ĞºĞ¾Ğ»Ğ»Ğ°Ğ¿ÑÑƒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ² ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñƒ Ğ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ surprisal-guided selection Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ñ‹ Ñ Ğ½Ğ°Ğ¸Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¹ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑÑ€ĞµĞ´Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° 30% Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ¾Ñ€Ğ°ĞºÑƒĞ»ÑŒĞ½Ğ¾Ğ¼Ñƒ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑÑƒÑ€ÑÑ‹ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¿Ğ¾Ñ‚Ñ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ±Ğ¾Ñ€, Ñ‡ĞµĞ¼ Ğ½Ğ° Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Surprisal-Guided Selection: A Better Approach for Test-Time Training",
                    "desc": "This paper investigates the effectiveness of test-time training (TTT) in verification-grounded tasks, particularly in scenarios where a deterministic evaluator provides continuous rewards. The authors find that TTT often leads to over-sharpening, which reduces diversity and results in suboptimal solutions. Instead, they propose a method called surprisal-guided selection, which focuses on choosing low-confidence samples to improve performance significantly. Their experiments show that this approach can achieve near-optimal results in dense-reward tasks, suggesting that prioritizing sample diversity over gradient updates is more beneficial for these types of problems."
                },
                "zh": {
                    "title": "é€‰æ‹©å¤šæ ·æ€§ï¼Œè¶…è¶Šé€‚åº”æ€§ï¼",
                    "desc": "æœ¬æ–‡ç ”ç©¶äº†åœ¨éªŒè¯åŸºç¡€ä»»åŠ¡ä¸­ï¼Œæµ‹è¯•æ—¶è®­ç»ƒï¼ˆTTTï¼‰ç”±äºè¿‡åº¦é”åŒ–è€Œå¯¼è‡´çš„å¤±è´¥ï¼Œä»¥åŠå¦‚ä½•é€šè¿‡æƒŠè®¶å¼•å¯¼é€‰æ‹©æ¥æé«˜æ€§èƒ½ã€‚æˆ‘ä»¬å‘ç°ï¼Œåœ¨GPUå†…æ ¸ä¼˜åŒ–ç­‰å¯éªŒè¯æ‰§è¡ŒåŸºç¡€ä»»åŠ¡ä¸­ï¼Œæœç´¢ç­–ç•¥ä¼˜äºæœ€å°é€‚åº”ï¼ˆ1-5ä¸ªæ¢¯åº¦æ­¥éª¤ï¼‰ã€‚é€šè¿‡é€‰æ‹©æœ€é«˜æƒŠè®¶ï¼ˆæœ€ä½ç½®ä¿¡åº¦ï¼‰çš„æ ·æœ¬ï¼Œæˆ‘ä»¬çš„æˆåŠŸç‡è¾¾åˆ°äº†80%ï¼Œè€Œé€‰æ‹©æœ€æœ‰ä¿¡å¿ƒçš„æ ·æœ¬ä»…ä¸º50%ã€‚å› æ­¤ï¼Œå¯¹äºå¯†é›†å¥–åŠ±çš„å¯éªŒè¯æ‰§è¡Œä»»åŠ¡ï¼Œè®¡ç®—èµ„æºåº”æ›´å¤šåœ°ç”¨äºæ ·æœ¬å¤šæ ·æ€§å’Œæ™ºèƒ½é€‰æ‹©ï¼Œè€Œéæ¢¯åº¦é€‚åº”ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.04802",
            "title": "VISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text?",
            "url": "https://huggingface.co/papers/2602.04802",
            "abstract": "VISTA-Bench evaluates vision-language models' ability to understand visualized text versus pure-text queries, revealing significant performance gaps and sensitivity to rendering variations.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language Models (VLMs) have achieved impressive performance in cross-modal understanding across textual and visual inputs, yet existing benchmarks predominantly focus on pure-text queries. In real-world scenarios, language also frequently appears as visualized text embedded in images, raising the question of whether current VLMs handle such input requests comparably. We introduce VISTA-Bench, a systematic benchmark from multimodal perception, reasoning, to unimodal understanding domains. It evaluates visualized text understanding by contrasting pure-text and visualized-text questions under controlled rendering conditions. Extensive evaluation of over 20 representative VLMs reveals a pronounced modality gap: models that perform well on pure-text queries often degrade substantially when equivalent semantic content is presented as visualized text. This gap is further amplified by increased perceptual difficulty, highlighting sensitivity to rendering variations despite unchanged semantics. Overall, VISTA-Bench provides a principled evaluation framework to diagnose this limitation and to guide progress toward more unified language representations across tokenized text and pixels. The source dataset is available at https://github.com/QingAnLiu/VISTA-Bench.",
            "score": 1,
            "issue_id": 1005,
            "pub_date": "2026-02-04",
            "pub_date_card": {
                "ru": "4 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 4",
                "zh": "2æœˆ4æ—¥"
            },
            "hash": "a26a5b083789d08e",
            "authors": [
                "Qing'an Liu",
                "Juntong Feng",
                "Yuhao Wang",
                "Xinzhe Han",
                "Yujie Cheng",
                "Yue Zhu",
                "Haiwen Diao",
                "Yunzhi Zhuge",
                "Huchuan Lu"
            ],
            "affiliations": [
                "S-Lab, Nanyang Technological University, Singapore",
                "School of Artificial Intelligence, Dalian University of Technology, Dalian, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.04802.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#multimodal",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "ğŸ“–",
                "ru": {
                    "title": "ĞœĞ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²: ĞºĞ°Ğº Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‚ĞµÑ€ÑÑÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ VISTA-Bench â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ñ‚ĞµĞºÑÑ‚, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…, Ğ² ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸: Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ğµ Ñ Ñ‡Ğ¸ÑÑ‚Ñ‹Ğ¼ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼, Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ ÑĞºĞ²Ğ¸Ğ²Ğ°Ğ»ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚ Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ»Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ 20 Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… VLM Ğ¸ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑÑ‚Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸ÑĞ¼ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ°, Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ½ĞµĞ¸Ğ·Ğ¼ĞµĞ½Ğ½ÑƒÑ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºÑƒ. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¾Ñ‚ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ ÑƒĞ½Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ°."
                },
                "en": {
                    "title": "Bridging the Gap: Evaluating Vision-Language Models with Visualized Text",
                    "desc": "VISTA-Bench is a new benchmark designed to evaluate how well vision-language models (VLMs) understand visualized text compared to pure-text queries. The study shows that while VLMs perform well with text alone, their performance drops significantly when the same information is presented as visualized text. This performance gap is influenced by the difficulty of interpreting the visualized text, which varies with rendering conditions. VISTA-Bench aims to identify these weaknesses in VLMs and promote the development of models that can better integrate and understand both text and visual information."
                },
                "zh": {
                    "title": "VISTA-Benchï¼šæ­ç¤ºè§†è§‰è¯­è¨€æ¨¡å‹çš„æ€§èƒ½å·®è·",
                    "desc": "VISTA-Bench æ˜¯ä¸€ä¸ªè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ç†è§£å¯è§†åŒ–æ–‡æœ¬ä¸çº¯æ–‡æœ¬æŸ¥è¯¢èƒ½åŠ›çš„åŸºå‡†ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰çš„ VLMs åœ¨å¤„ç†çº¯æ–‡æœ¬æŸ¥è¯¢æ—¶è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨é¢å¯¹å¯è§†åŒ–æ–‡æœ¬æ—¶æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚è¯¥åŸºå‡†é€šè¿‡å¯¹æ¯”çº¯æ–‡æœ¬å’Œå¯è§†åŒ–æ–‡æœ¬é—®é¢˜ï¼Œæ­ç¤ºäº†æ¨¡å‹å¯¹æ¸²æŸ“å˜åŒ–çš„æ•æ„Ÿæ€§ã€‚VISTA-Bench ä¸ºè¯Šæ–­è¿™ä¸€å±€é™æ€§æä¾›äº†ç³»ç»Ÿçš„è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨æ¨åŠ¨æ›´ç»Ÿä¸€çš„è¯­è¨€è¡¨ç¤ºã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.04521",
            "title": "C-Î”Î˜: Circuit-Restricted Weight Arithmetic for Selective Refusal",
            "url": "https://huggingface.co/papers/2602.04521",
            "abstract": "Offline selective refusal in large language models is achieved through circuit-restricted weight updates that eliminate runtime intervention costs while maintaining performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern deployments require LLMs to enforce safety policies at scale, yet many controls rely on inference-time interventions that add recurring compute cost and serving complexity. Activation steering is widely used, but it requires runtime hooks and scales cost with the number of generations; conditional variants improve selectivity by gating when steering is applied but still retain an inference-time control path. We ask whether selective refusal can be moved entirely offline: can a mechanistic understanding of category-specific refusal be distilled into a circuit-restricted weight update that deploys as a standard checkpoint? We propose C-Î”Î¸: Circuit Restricted Weight Arithmetic, which (i) localizes refusal-causal computation as a sparse circuit using EAP-IG and (ii) computes a constrained weight update Î”Î¸C supported only on that circuit (typically <5% of parameters). Applying Î”Î¸C yields a drop-in edited checkpoint with no inference-time hooks, shifting cost from per-request intervention to a one-time offline update. We evaluate category-targeted selectivity and capability retention on refusal and utility benchmarks.",
            "score": 1,
            "issue_id": 1005,
            "pub_date": "2026-02-04",
            "pub_date_card": {
                "ru": "4 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 4",
                "zh": "2æœˆ4æ—¥"
            },
            "hash": "d058732a6d35a9cc",
            "authors": [
                "Aditya Kasliwal",
                "Pratinav Seth",
                "Vinay Kumar Sankarapu"
            ],
            "affiliations": [
                "Lexsi Labs"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.04521.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#inference"
                ],
                "emoji": "ğŸ”§",
                "ru": {
                    "title": "Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ±ĞµĞ· Ñ†ĞµĞ½Ñ‹: Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½-Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ offline-Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚ĞºĞ°Ğ·Ğ° Ğ² Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿ĞµĞ¹ (circuits), Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ° Ğ¾Ñ‚ĞºĞ°Ğ·, Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·ÑƒÑÑ‚ Ğ¸Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° EAP-IG. Ğ—Ğ°Ñ‚ĞµĞ¼ Ğ¾Ğ½Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ÑÑÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ²ĞµÑĞ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ğ³Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼ĞµĞ½ĞµĞµ 5% Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ²Ğ½ĞµĞ´Ñ€ÑĞµÑ‚ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ² ÑÑ‚Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ checkpoint Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒÑÑ ĞºĞ°Ğº ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ²ĞµĞ½Ñ†Ğ¸ÑÑ… Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ¿ĞµÑ€ĞµĞ¼ĞµÑ‰Ğ°Ñ Ğ²ÑĞµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾ĞºÑ€Ğ°Ñ‚Ğ½Ñ‹Ğ¹ offline Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ."
                },
                "en": {
                    "title": "Efficient Offline Selective Refusal for Large Language Models",
                    "desc": "This paper introduces a method called C-Î”Î¸, which allows large language models (LLMs) to refuse certain outputs without needing to intervene during runtime. By using circuit-restricted weight updates, the authors eliminate the need for costly inference-time controls, making the process more efficient. The approach focuses on refining the model's parameters offline, which means that the model can be updated once and then used without additional computational costs during operation. The results show that this method maintains the model's performance while effectively implementing selective refusal for specific categories."
                },
                "zh": {
                    "title": "ç¦»çº¿é€‰æ‹©æ€§æ‹’ç»ï¼šé™ä½æˆæœ¬ï¼Œæå‡æ€§èƒ½",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­å®ç°ç¦»çº¿é€‰æ‹©æ€§æ‹’ç»ï¼Œé€šè¿‡ç”µè·¯é™åˆ¶çš„æƒé‡æ›´æ–°æ¥æ¶ˆé™¤è¿è¡Œæ—¶å¹²é¢„æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒæ€§èƒ½ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºC-Î”Î¸çš„æœºåˆ¶ï¼Œå®ƒå°†æ‹’ç»ç›¸å…³çš„è®¡ç®—å±€éƒ¨åŒ–ä¸ºç¨€ç–ç”µè·¯ï¼Œå¹¶è®¡ç®—ä»…åœ¨è¯¥ç”µè·¯ä¸Šæ”¯æŒçš„çº¦æŸæƒé‡æ›´æ–°ã€‚è¿™æ ·å¯ä»¥å°†æˆæœ¬ä»æ¯æ¬¡è¯·æ±‚çš„å¹²é¢„è½¬ç§»åˆ°ä¸€æ¬¡æ€§çš„ç¦»çº¿æ›´æ–°ã€‚æˆ‘ä»¬çš„è¯„ä¼°è¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•åœ¨æ‹’ç»å’Œæ•ˆç”¨åŸºå‡†æµ‹è¯•ä¸­ä¿æŒäº†ç±»åˆ«é’ˆå¯¹æ€§çš„é€‰æ‹©æ€§å’Œèƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.04908",
            "title": "Temporal Pair Consistency for Variance-Reduced Flow Matching",
            "url": "https://huggingface.co/papers/2602.04908",
            "abstract": "Temporal Pair Consistency reduces variance in continuous-time generative models by coupling velocity predictions at paired timesteps, improving sample quality and efficiency without altering model architecture or training procedures.  \t\t\t\t\tAI-generated summary \t\t\t\t Continuous-time generative models, such as diffusion models, flow matching, and rectified flow, learn time-dependent vector fields but are typically trained with objectives that treat timesteps independently, leading to high estimator variance and inefficient sampling. Prior approaches mitigate this via explicit smoothness penalties, trajectory regularization, or modified probability paths and solvers. We introduce Temporal Pair Consistency (TPC), a lightweight variance-reduction principle that couples velocity predictions at paired timesteps along the same probability path, operating entirely at the estimator level without modifying the model architecture, probability path, or solver. We provide a theoretical analysis showing that TPC induces a quadratic, trajectory-coupled regularization that provably reduces gradient variance while preserving the underlying flow-matching objective. Instantiated within flow matching, TPC improves sample quality and efficiency across CIFAR-10 and ImageNet at multiple resolutions, achieving lower FID at identical or lower computational cost than prior methods, and extends seamlessly to modern SOTA-style pipelines with noise-augmented training, score-based denoising, and rectified flow.",
            "score": 1,
            "issue_id": 998,
            "pub_date": "2026-02-04",
            "pub_date_card": {
                "ru": "4 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 4",
                "zh": "2æœˆ4æ—¥"
            },
            "hash": "7af03117fefc8c2a",
            "authors": [
                "Chika Maduabuchi",
                "Jindong Wang"
            ],
            "affiliations": [
                "William & Mary"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.04908.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#training",
                    "#optimization",
                    "#architecture"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "ĞŸĞ°Ñ€Ğ½Ğ°Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ°",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Temporal Pair Consistency Ğ´Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ğ¸ Ğ² Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ flow matching. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ĞµĞ¹ Ğ² Ğ¿Ğ°Ñ€Ğ°Ñ… Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ğ²Ğ´Ğ¾Ğ»ÑŒ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸ĞºĞ° Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸, Ñ‡Ñ‚Ğ¾ TPC Ğ¸Ğ½Ğ´ÑƒÑ†Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½ÑƒÑ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½ÑƒÑ Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ñ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ objective Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° CIFAR-10 Ğ¸ ImageNet Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ¸Ğ·ĞºĞ¸Ğ¼ FID Ğ¿Ñ€Ğ¸ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ñ‹Ñ… Ğ¸Ğ»Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "Enhancing Sample Quality with Temporal Pair Consistency",
                    "desc": "This paper introduces Temporal Pair Consistency (TPC), a method designed to reduce variance in continuous-time generative models by linking velocity predictions at paired timesteps. By doing so, TPC enhances the quality of generated samples and improves sampling efficiency without changing the model's architecture or training methods. The authors provide a theoretical framework demonstrating that TPC effectively reduces gradient variance while maintaining the original flow-matching objective. The results show that TPC outperforms previous techniques in terms of sample quality on datasets like CIFAR-10 and ImageNet, achieving better performance at similar or lower computational costs."
                },
                "zh": {
                    "title": "æ—¶é—´å¯¹å¶ä¸€è‡´æ€§ï¼šæå‡ç”Ÿæˆæ¨¡å‹çš„æ ·æœ¬è´¨é‡ä¸æ•ˆç‡",
                    "desc": "æ—¶é—´å¯¹å¶ä¸€è‡´æ€§ï¼ˆTemporal Pair Consistency, TPCï¼‰é€šè¿‡åœ¨é…å¯¹æ—¶é—´æ­¥é•¿ä¸Šè€¦åˆé€Ÿåº¦é¢„æµ‹ï¼Œå‡å°‘äº†è¿ç»­æ—¶é—´ç”Ÿæˆæ¨¡å‹ä¸­çš„æ–¹å·®ï¼Œä»è€Œæé«˜äº†æ ·æœ¬è´¨é‡å’Œæ•ˆç‡ã€‚ä¼ ç»Ÿçš„ç”Ÿæˆæ¨¡å‹é€šå¸¸ç‹¬ç«‹å¤„ç†æ—¶é—´æ­¥é•¿ï¼Œå¯¼è‡´é«˜ä¼°è®¡æ–¹å·®å’Œä½æ•ˆé‡‡æ ·ã€‚TPCæ˜¯ä¸€ç§è½»é‡çº§çš„æ–¹å·®å‡å°‘åŸåˆ™ï¼Œå®Œå…¨åœ¨ä¼°è®¡å™¨å±‚é¢æ“ä½œï¼Œä¸éœ€è¦ä¿®æ”¹æ¨¡å‹æ¶æ„æˆ–è®­ç»ƒè¿‡ç¨‹ã€‚é€šè¿‡ç†è®ºåˆ†æï¼Œæˆ‘ä»¬è¯æ˜TPCèƒ½å¤Ÿé™ä½æ¢¯åº¦æ–¹å·®ï¼ŒåŒæ—¶ä¿æŒæµåŒ¹é…çš„åŸºæœ¬ç›®æ ‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.01725",
            "title": "SafePred: A Predictive Guardrail for Computer-Using Agents via World Models",
            "url": "https://huggingface.co/papers/2602.01725",
            "abstract": "SafePred is a predictive guardrail framework for computer-using agents that uses risk prediction and decision optimization to prevent both immediate and delayed high-risk consequences in complex environments.  \t\t\t\t\tAI-generated summary \t\t\t\t With the widespread deployment of Computer-using Agents (CUAs) in complex real-world environments, prevalent long-term risks often lead to severe and irreversible consequences. Most existing guardrails for CUAs adopt a reactive approach, constraining agent behavior only within the current observation space. While these guardrails can prevent immediate short-term risks (e.g., clicking on a phishing link), they cannot proactively avoid long-term risks: seemingly reasonable actions can lead to high-risk consequences that emerge with a delay (e.g., cleaning logs leads to future audits being untraceable), which reactive guardrails cannot identify within the current observation space. To address these limitations, we propose a predictive guardrail approach, with the core idea of aligning predicted future risks with current decisions. Based on this approach, we present SafePred, a predictive guardrail framework for CUAs that establishes a risk-to-decision loop to ensure safe agent behavior. SafePred supports two key abilities: (1) Short- and long-term risk prediction: by using safety policies as the basis for risk prediction, SafePred leverages the prediction capability of the world model to generate semantic representations of both short-term and long-term risks, thereby identifying and pruning actions that lead to high-risk states; (2) Decision optimization: translating predicted risks into actionable safe decision guidances through step-level interventions and task-level re-planning. Extensive experiments show that SafePred significantly reduces high-risk behaviors, achieving over 97.6% safety performance and improving task utility by up to 21.4% compared with reactive baselines.",
            "score": 1,
            "issue_id": 1000,
            "pub_date": "2026-02-02",
            "pub_date_card": {
                "ru": "2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 2",
                "zh": "2æœˆ2æ—¥"
            },
            "hash": "af08c39afe68d468",
            "authors": [
                "Yurun Chen",
                "Zeyi Liao",
                "Ping Yin",
                "Taotao Xie",
                "Keting Yin",
                "Shengyu Zhang"
            ],
            "affiliations": [
                "Inspur Cloud, China",
                "The Ohio State University, USA",
                "Zhejiang University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.01725.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#alignment"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ¸ÑĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "SafePred â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¸ĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ¸ÑĞºĞ¾Ğ² Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ€ĞµĞ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°ÑÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½ĞµĞ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ñ€Ğ¸ÑĞºĞ¸ Ğ² Ñ‚ĞµĞºÑƒÑ‰ĞµĞ¼ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ğ¾Ğ¼ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¸, SafePred Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑÑ‚Ğ²Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼Ğ¸Ñ€Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ñ€Ğ¸ÑĞºĞ¾Ğ². Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ€ĞµĞ°Ğ»Ğ¸Ğ·ÑƒĞµÑ‚ Ñ†Ğ¸ĞºĞ» Â«Ñ€Ğ¸ÑĞº-Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµÂ», ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ»ÑĞµÑ‚ Ğ²Ğ¼ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ¾ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ğ¸ Ğ¿ĞµÑ€ĞµĞ¿Ğ»Ğ°Ğ½ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğº Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SafePred Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 97.6% ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° 21.4% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ€ĞµĞ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Proactive Safety for Computer-Using Agents with SafePred",
                    "desc": "SafePred is a framework designed to enhance the safety of computer-using agents (CUAs) by predicting risks and optimizing decisions. Unlike traditional guardrails that react to immediate threats, SafePred proactively identifies both short-term and long-term risks associated with agent actions. It utilizes a risk-to-decision loop, where predicted risks inform current decision-making, allowing agents to avoid actions that could lead to future high-risk situations. Experimental results demonstrate that SafePred significantly improves safety performance and task efficiency compared to existing reactive methods."
                },
                "zh": {
                    "title": "SafePredï¼šä¸»åŠ¨é˜²èŒƒé£é™©çš„æ™ºèƒ½ä»£ç†æ¡†æ¶",
                    "desc": "SafePredæ˜¯ä¸€ä¸ªä¸ºè®¡ç®—æœºä½¿ç”¨ä»£ç†è®¾è®¡çš„é¢„æµ‹æ€§ä¿æŠ¤æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡é£é™©é¢„æµ‹å’Œå†³ç­–ä¼˜åŒ–æ¥é˜²æ­¢å¤æ‚ç¯å¢ƒä¸­çš„é«˜é£é™©åæœã€‚ä¸ä¼ ç»Ÿçš„ååº”æ€§ä¿æŠ¤æªæ–½ä¸åŒï¼ŒSafePredèƒ½å¤Ÿä¸»åŠ¨è¯†åˆ«å’Œé¿å…çŸ­æœŸå’Œé•¿æœŸé£é™©ï¼Œç¡®ä¿ä»£ç†çš„å®‰å…¨è¡Œä¸ºã€‚è¯¥æ¡†æ¶é€šè¿‡å»ºç«‹é£é™©ä¸å†³ç­–çš„å¾ªç¯ï¼Œåˆ©ç”¨ä¸–ç•Œæ¨¡å‹çš„é¢„æµ‹èƒ½åŠ›ç”Ÿæˆé£é™©çš„è¯­ä¹‰è¡¨ç¤ºï¼Œä»è€Œè¯†åˆ«å’Œä¿®å‰ªå¯¼è‡´é«˜é£é™©çŠ¶æ€çš„è¡Œä¸ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSafePredæ˜¾è‘—é™ä½äº†é«˜é£é™©è¡Œä¸ºï¼Œå®‰å…¨æ€§èƒ½è¶…è¿‡97.6%ï¼Œä»»åŠ¡æ•ˆç”¨æé«˜äº†21.4%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.21235",
            "title": "SHARP: Social Harm Analysis via Risk Profiles for Measuring Inequities in Large Language Models",
            "url": "https://huggingface.co/papers/2601.21235",
            "abstract": "Large language models exhibit varying levels of social risk across multiple dimensions, with significant differences in worst-case behavior that cannot be captured by traditional scalar evaluation metrics.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are increasingly deployed in high-stakes domains, where rare but severe failures can result in irreversible harm. However, prevailing evaluation benchmarks often reduce complex social risk to mean-centered scalar scores, thereby obscuring distributional structure, cross-dimensional interactions, and worst-case behavior. This paper introduces Social Harm Analysis via Risk Profiles (SHARP), a framework for multidimensional, distribution-aware evaluation of social harm. SHARP models harm as a multivariate random variable and integrates explicit decomposition into bias, fairness, ethics, and epistemic reliability with a union-of-failures aggregation reparameterized as additive cumulative log-risk. The framework further employs risk-sensitive distributional statistics, with Conditional Value at Risk (CVaR95) as a primary metric, to characterize worst-case model behavior. Application of SHARP to eleven frontier LLMs, evaluated on a fixed corpus of n=901 socially sensitive prompts, reveals that models with similar average risk can exhibit more than twofold differences in tail exposure and volatility. Across models, dimension-wise marginal tail behavior varies systematically across harm dimensions, with bias exhibiting the strongest tail severities, epistemic and fairness risks occupying intermediate regimes, and ethical misalignment consistently lower; together, these patterns reveal heterogeneous, model-dependent failure structures that scalar benchmarks conflate. These findings indicate that responsible evaluation and governance of LLMs require moving beyond scalar averages toward multidimensional, tail-sensitive risk profiling.",
            "score": 1,
            "issue_id": 998,
            "pub_date": "2026-01-29",
            "pub_date_card": {
                "ru": "29 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 29",
                "zh": "1æœˆ29æ—¥"
            },
            "hash": "870a7fad02adf8df",
            "authors": [
                "Alok Abhishek",
                "Tushar Bandopadhyay",
                "Lisa Erickson"
            ],
            "affiliations": [
                "Boston, USA",
                "San Francisco, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.21235.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#ethics",
                    "#benchmark"
                ],
                "emoji": "âš ï¸",
                "ru": {
                    "title": "ĞÑ‚ ÑÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ñ€Ğ¸ÑĞºĞ° Ğº Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ ĞºĞ°Ñ‚Ğ°ÑÑ‚Ñ€Ğ¾Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ²: Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ²Ñ€ĞµĞ´Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ SHARP Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ´Ğ° Ğ¾Ñ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ·Ğ° Ñ€Ğ°Ğ¼ĞºĞ¸ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞºĞ°Ğ»ÑÑ€Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ¸ÑĞºĞ¾Ğ² Ğ¿Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸ÑĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ñ€ĞµĞ´ ĞºĞ°Ğº Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½ÑƒÑ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½ÑƒÑ Ğ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ½Ñƒ, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ (bias), ÑĞ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ (fairness), ÑÑ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¿Ğ¸ÑÑ‚ĞµĞ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¾Ğ¹ Conditional Value at Risk Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ°Ğ¸Ñ…ÑƒĞ´ÑˆĞ¸Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ². ĞŸÑ€Ğ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¾Ğ´Ğ¸Ğ½Ğ½Ğ°Ğ´Ñ†Ğ°Ñ‚Ğ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… LLM-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° 901 ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğµ Ğ²Ñ‹ÑÑĞ½Ğ¸Ğ»Ğ¾ÑÑŒ, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ğ¼ ÑÑ€ĞµĞ´Ğ½Ğ¸Ğ¼ Ñ€Ğ¸ÑĞºĞ¾Ğ¼ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°Ñ‚ÑŒÑÑ Ğ² Ğ´Ğ²Ğ° Ñ€Ğ°Ğ·Ğ° Ğ¿Ğ¾ Ñ…Ğ²Ğ¾ÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ñ€Ğ¸ÑĞºÑƒ Ğ¸ Ğ²Ğ¾Ğ»Ğ°Ñ‚Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ LLM Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ° Ğ¾Ñ‚ ÑƒÑÑ€ĞµĞ´Ğ½ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğº Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¼Ñƒ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ, Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğº ÑĞºÑÑ‚Ñ€ĞµĞ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑĞ»ÑƒÑ‡Ğ°ÑĞ¼."
                },
                "en": {
                    "title": "Beyond Averages: Evaluating Social Risks in Language Models",
                    "desc": "This paper discusses the limitations of traditional evaluation metrics for large language models (LLMs) in assessing social risks. It introduces a new framework called Social Harm Analysis via Risk Profiles (SHARP), which evaluates social harm in a multidimensional way. SHARP considers various factors like bias, fairness, ethics, and reliability, focusing on worst-case scenarios rather than just average performance. The study shows that LLMs with similar average risks can have vastly different behaviors in extreme cases, highlighting the need for more nuanced evaluation methods."
                },
                "zh": {
                    "title": "è¶…è¶Šæ ‡é‡è¯„ä¼°ï¼Œæ·±å…¥ç†è§£ç¤¾ä¼šé£é™©",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šä¸ªç»´åº¦ä¸Šè¡¨ç°å‡ºä¸åŒç¨‹åº¦çš„ç¤¾ä¼šé£é™©ï¼Œä¼ ç»Ÿçš„æ ‡é‡è¯„ä¼°æŒ‡æ ‡æ— æ³•æ•æ‰åˆ°æœ€åæƒ…å†µä¸‹çš„è¡Œä¸ºå·®å¼‚ã€‚æœ¬æ–‡æå‡ºäº†ç¤¾ä¼šå±å®³åˆ†ææ¡†æ¶ï¼ˆSHARPï¼‰ï¼Œç”¨äºå¤šç»´åº¦ã€åˆ†å¸ƒæ„ŸçŸ¥çš„ç¤¾ä¼šå±å®³è¯„ä¼°ã€‚SHARPå°†å±å®³å»ºæ¨¡ä¸ºå¤šå…ƒéšæœºå˜é‡ï¼Œå¹¶å°†åè§ã€å…¬å¹³æ€§ã€ä¼¦ç†å’Œè®¤çŸ¥å¯é æ€§è¿›è¡Œæ˜ç¡®åˆ†è§£ï¼Œé‡‡ç”¨åŠ æ³•ç´¯ç§¯å¯¹æ•°é£é™©çš„è”åˆå¤±æ•ˆèšåˆã€‚ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡æ¨¡å‹çš„å¹³å‡é£é™©ç›¸ä¼¼ï¼Œä½†åœ¨å°¾éƒ¨æš´éœ²å’Œæ³¢åŠ¨æ€§æ–¹é¢å¯èƒ½å­˜åœ¨è¶…è¿‡ä¸¤å€çš„å·®å¼‚ï¼Œå¼ºè°ƒäº†éœ€è¦è¶…è¶Šæ ‡é‡å¹³å‡å€¼è¿›è¡Œå¤šç»´åº¦çš„é£é™©åˆ†æã€‚"
                }
            }
        }
    ],
    "link_prev": "2026-02-10.html",
    "link_next": "2026-02-12.html",
    "link_month": "2026-02.html",
    "short_date_prev": {
        "ru": "10.02",
        "en": "02/10",
        "zh": "2æœˆ10æ—¥"
    },
    "short_date_next": {
        "ru": "12.02",
        "en": "02/12",
        "zh": "2æœˆ12æ—¥"
    },
    "categories": {
        "#dataset": 9,
        "#data": 1,
        "#benchmark": 15,
        "#agents": 15,
        "#cv": 6,
        "#rl": 12,
        "#rlhf": 3,
        "#rag": 1,
        "#plp": 2,
        "#inference": 8,
        "#3d": 1,
        "#audio": 1,
        "#video": 4,
        "#multimodal": 13,
        "#math": 6,
        "#multilingual": 0,
        "#architecture": 14,
        "#healthcare": 0,
        "#training": 31,
        "#robotics": 6,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 4,
        "#reasoning": 15,
        "#transfer_learning": 1,
        "#graphs": 1,
        "#ethics": 1,
        "#security": 2,
        "#optimization": 17,
        "#survey": 0,
        "#diffusion": 9,
        "#alignment": 4,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 2,
        "#synthetic": 4,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 14,
        "#small_models": 2,
        "#science": 1,
        "#low_resource": 0
    }
}