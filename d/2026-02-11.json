{
    "date": {
        "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
        "en": "February 11",
        "zh": "2æœˆ11æ—¥"
    },
    "time_utc": "2026-02-11 01:29",
    "weekday": 2,
    "issue_id": 997,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2602.08222",
            "title": "Weak-Driven Learning: How Weak Agents make Strong Agents Stronger",
            "url": "https://huggingface.co/papers/2602.08222",
            "abstract": "WMSS is a post-training paradigm that uses weak model checkpoints to identify and fill learning gaps, enabling continued improvement beyond conventional saturation points in large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t As post-training optimization becomes central to improving large language models, we observe a persistent saturation bottleneck: once models grow highly confident, further training yields diminishing returns. While existing methods continue to reinforce target predictions, we find that informative supervision signals remain latent in models' own historical weak states. Motivated by this observation, we propose WMSS (Weak Agents Can Make Strong Agents Stronger), a post-training paradigm that leverages weak checkpoints to guide continued optimization. By identifying recoverable learning gaps via entropy dynamics and reinforcing them through compensatory learning, WMSS enables strong agents to improve beyond conventional post-training saturation. Experiments on mathematical reasoning and code generation datasets show that agents trained with our approach achieve effective performance improvements, while incurring zero additional inference cost.",
            "score": 150,
            "issue_id": 982,
            "pub_date": "2026-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "5de7bf3e511c0cff",
            "authors": [
                "Zehao Chen",
                "Gongxun Li",
                "Tianxiang Ai",
                "Yifei Li",
                "Zixuan Huang",
                "Wang Zhou",
                "Fuzhen Zhuang",
                "Xianglong Liu",
                "Jianxin Li",
                "Deqing Wang",
                "Yikun Ban"
            ],
            "affiliations": [
                "Beihang University",
                "China Teleco"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.08222.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#optimization"
                ],
                "emoji": "ğŸ“ˆ",
                "ru": {
                    "title": "Ğ¡Ğ»Ğ°Ğ±Ñ‹Ğµ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾Ñ‡ĞºĞ¸ ĞºĞ°Ğº Ğ¿ÑƒÑ‚ÑŒ Ğº ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ WMSS â€” Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½Ğ°ÑÑ‹Ñ‰ĞµĞ½Ğ¸Ñ, Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‰ÑƒÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ»Ğ°Ğ±Ñ‹Ğµ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ğ¾Ğ² Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿ĞµĞ½ÑĞ°Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ»Ğ°Ğ±Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ¸ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ»Ğ°Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Unlocking Hidden Potential in Language Models with WMSS",
                    "desc": "WMSS is a new method for improving large language models after their initial training. It focuses on using weaker versions of the model, called weak checkpoints, to find areas where the model can still learn and grow. By analyzing the model's past performance and identifying gaps in its knowledge, WMSS helps the model to continue improving even when it seems to have reached its peak. Tests show that models using WMSS perform better in tasks like math reasoning and code generation without increasing the cost of using the model."
                },
                "zh": {
                    "title": "å¼±æ¨¡å‹åŠ©åŠ›å¼ºæ¨¡å‹æ›´å¼º",
                    "desc": "WMSSæ˜¯ä¸€ç§åè®­ç»ƒèŒƒå¼ï¼Œåˆ©ç”¨å¼±æ¨¡å‹æ£€æŸ¥ç‚¹æ¥è¯†åˆ«å’Œå¡«è¡¥å­¦ä¹ ç©ºç™½ï¼Œä»è€Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¼ ç»Ÿé¥±å’Œç‚¹ä¹‹å¤–ç»§ç»­æ”¹è¿›ã€‚æˆ‘ä»¬å‘ç°ï¼Œå°½ç®¡æ¨¡å‹åœ¨è®­ç»ƒä¸­å˜å¾—é«˜åº¦è‡ªä¿¡ï¼Œä½†è¿›ä¸€æ­¥çš„è®­ç»ƒæ•ˆæœé€’å‡ã€‚WMSSé€šè¿‡è¯†åˆ«å¯æ¢å¤çš„å­¦ä¹ å·®è·ï¼Œå¹¶é€šè¿‡è¡¥å¿å­¦ä¹ æ¥å¼ºåŒ–è¿™äº›å·®è·ï¼Œä½¿å¼ºæ¨¡å‹èƒ½å¤Ÿè¶…è¶Šä¼ ç»Ÿçš„åè®­ç»ƒé¥±å’Œã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨è¯¥æ–¹æ³•è®­ç»ƒçš„æ¨¡å‹åœ¨æ•°å­¦æ¨ç†å’Œä»£ç ç”Ÿæˆæ•°æ®é›†ä¸Šå–å¾—äº†æœ‰æ•ˆçš„æ€§èƒ½æå‡ï¼ŒåŒæ—¶æ²¡æœ‰å¢åŠ é¢å¤–çš„æ¨ç†æˆæœ¬ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.07085",
            "title": "QuantaAlpha: An Evolutionary Framework for LLM-Driven Alpha Mining",
            "url": "https://huggingface.co/papers/2602.07085",
            "abstract": "Financial markets are noisy and non-stationary, making alpha mining highly sensitive to noise in backtesting results and sudden market regime shifts. While recent agentic frameworks improve alpha mining automation, they often lack controllable multi-round search and reliable reuse of validated experience. To address these challenges, we propose QuantaAlpha, an evolutionary alpha mining framework that treats each end-to-end mining run as a trajectory and improves factors through trajectory-level mutation and crossover operations. QuantaAlpha localizes suboptimal steps in each trajectory for targeted revision and recombines complementary high-reward segments to reuse effective patterns, enabling structured exploration and refinement across mining iterations. During factor generation, QuantaAlpha enforces semantic consistency across the hypothesis, factor expression, and executable code, while constraining the complexity and redundancy of the generated factor to mitigate crowding. Extensive experiments on the China Securities Index 300 (CSI 300) demonstrate consistent gains over strong baseline models and prior agentic systems. When utilizing GPT-5.2, QuantaAlpha achieves an Information Coefficient (IC) of 0.1501, with an Annualized Rate of Return (ARR) of 27.75% and a Maximum Drawdown (MDD) of 7.98%. Moreover, factors mined on CSI 300 transfer effectively to the China Securities Index 500 (CSI 500) and the Standard & Poor's 500 Index (S&P 500), delivering 160% and 137% cumulative excess return over four years, respectively, which indicates strong robustness of QuantaAlpha under market distribution shifts.",
            "score": 142,
            "issue_id": 981,
            "pub_date": "2026-02-06",
            "pub_date_card": {
                "ru": "6 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 6",
                "zh": "2æœˆ6æ—¥"
            },
            "hash": "10b950e0be093e2a",
            "authors": [
                "Jun Han",
                "Shuo Zhang",
                "Wei Li",
                "Zhi Yang",
                "Yifan Dong",
                "Tu Hu",
                "Jialuo Yuan",
                "Xiaomin Yu",
                "Yumo Zhu",
                "Fangqi Lou",
                "Xin Guo",
                "Zhaowei Liu",
                "Tianyi Jiang",
                "Ruichuan An",
                "Jingping Liu",
                "Biao Wu",
                "Rongze Chen",
                "Kunyi Wang",
                "Yifan Wang",
                "Sen Hu",
                "Xinbing Kong",
                "Liwen Zhang",
                "Ronghao Chen",
                "Huacan Wang"
            ],
            "affiliations": [
                "PKU",
                "QuantaAlpha",
                "SEU",
                "SUFE",
                "SYSU",
                "Stanford"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.07085.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ’¹",
                "ru": {
                    "title": "Ğ­Ğ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ñ‚Ğ¾Ñ€Ğ³Ğ¾Ğ²Ñ‹Ñ… Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹",
                    "desc": "QuantaAlpha Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ°Ğ»ÑŒÑ„Ğ°-Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ½Ğ° Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ñ€Ñ‹Ğ½ĞºĞ°Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ñ†Ğ¸ĞºĞ» Ğ¼Ğ°Ğ¹Ğ½Ğ¸Ğ½Ğ³Ğ° ĞºĞ°Ğº Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¿Ğ¾Ğ¸ÑĞºĞ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ĞºÑ€Ğ¾ÑÑĞ¾Ğ²ĞµÑ€Ğ° Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ², Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·ÑƒÑ ÑÑƒĞ±Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑˆĞ°Ğ³Ğ¸ Ğ¸ Ñ€ĞµĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒÑ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ´Ğ¾Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ². Ğ’Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·Ğ¾Ğ¹, Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼, Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ ÑĞ¼ÑĞ³Ñ‡ĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ° Ğ¿ĞµÑ€ĞµĞ½Ğ°ÑÑ‹Ñ‰ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğº ÑĞ´Ğ²Ğ¸Ğ³Ğ°Ğ¼ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ñ€Ñ‹Ğ½ĞºĞ°, Ñ ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ 0.1501 Ğ¸ Ğ³Ğ¾Ğ´Ğ¾Ğ²Ğ¾Ğ¹ Ğ´Ğ¾Ñ…Ğ¾Ğ´Ğ½Ğ¾ÑÑ‚ÑŒÑ 27.75% Ğ¿Ñ€Ğ¸ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾ÑĞ°Ğ´ĞºĞµ 7.98% Ğ½Ğ° Ğ¸Ğ½Ğ´ĞµĞºÑĞµ CSI 300."
                },
                "en": {
                    "title": "QuantaAlpha: Evolving Alpha Mining for Robust Financial Insights",
                    "desc": "QuantaAlpha is an innovative framework designed for alpha mining in financial markets, which are often unpredictable and changeable. It enhances the process by treating each mining attempt as a trajectory, allowing for targeted improvements through mutation and crossover techniques. This method not only revises suboptimal steps but also combines successful segments to optimize the mining process. The framework ensures that the generated factors maintain semantic consistency and reduces complexity, leading to significant performance improvements over existing models, as demonstrated by its strong results on various market indices."
                },
                "zh": {
                    "title": "QuantaAlphaï¼šæ™ºèƒ½åŒ–çš„é˜¿å°”æ³•æŒ–æ˜æ¡†æ¶",
                    "desc": "é‡‘èå¸‚åœºå™ªå£°å¤§ä¸”éå¹³ç¨³ï¼Œä½¿å¾—é˜¿å°”æ³•æŒ–æ˜å¯¹å›æµ‹ç»“æœä¸­çš„å™ªå£°å’Œå¸‚åœºçŠ¶æ€çªå˜éå¸¸æ•æ„Ÿã€‚æˆ‘ä»¬æå‡ºäº†QuantaAlphaï¼Œè¿™æ˜¯ä¸€ç§è¿›åŒ–çš„é˜¿å°”æ³•æŒ–æ˜æ¡†æ¶ï¼Œé€šè¿‡è½¨è¿¹çº§çš„å˜å¼‚å’Œäº¤å‰æ“ä½œæ¥æ”¹è¿›æŒ–æ˜è¿‡ç¨‹ã€‚QuantaAlphaèƒ½å¤Ÿå®šä½æ¯ä¸ªè½¨è¿¹ä¸­çš„æ¬¡ä¼˜æ­¥éª¤è¿›è¡Œé’ˆå¯¹æ€§ä¿®æ­£ï¼Œå¹¶é‡æ–°ç»„åˆé«˜æ”¶ç›Šçš„äº’è¡¥ç‰‡æ®µï¼Œä»¥ä¾¿åœ¨æŒ–æ˜è¿­ä»£ä¸­å®ç°ç»“æ„åŒ–æ¢ç´¢å’Œä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒQuantaAlphaåœ¨ä¸­å›½è¯åˆ¸æŒ‡æ•°300ï¼ˆCSI 300ï¼‰ä¸Šè¡¨ç°ä¼˜äºå¼ºåŸºçº¿æ¨¡å‹å’Œä¹‹å‰çš„æ™ºèƒ½ç³»ç»Ÿï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å¸‚åœºåˆ†å¸ƒå˜åŒ–ä¸‹çš„å¼ºå¤§é²æ£’æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.08794",
            "title": "MOVA: Towards Scalable and Synchronized Video-Audio Generation",
            "url": "https://huggingface.co/papers/2602.08794",
            "abstract": "MOVA is an open-source model that generates synchronized audio-visual content using a Mixture-of-Experts architecture with 32 billion parameters, supporting image-text to video-audio generation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Audio is indispensable for real-world video, yet generation models have largely overlooked audio components. Current approaches to producing audio-visual content often rely on cascaded pipelines, which increase cost, accumulate errors, and degrade overall quality. While systems such as Veo 3 and Sora 2 emphasize the value of simultaneous generation, joint multimodal modeling introduces unique challenges in architecture, data, and training. Moreover, the closed-source nature of existing systems limits progress in the field. In this work, we introduce MOVA (MOSS Video and Audio), an open-source model capable of generating high-quality, synchronized audio-visual content, including realistic lip-synced speech, environment-aware sound effects, and content-aligned music. MOVA employs a Mixture-of-Experts (MoE) architecture, with a total of 32B parameters, of which 18B are active during inference. It supports IT2VA (Image-Text to Video-Audio) generation task. By releasing the model weights and code, we aim to advance research and foster a vibrant community of creators. The released codebase features comprehensive support for efficient inference, LoRA fine-tuning, and prompt enhancement.",
            "score": 131,
            "issue_id": 981,
            "pub_date": "2026-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "4f8806b98c1c0d7e",
            "authors": [
                "SII-OpenMOSS Team",
                ":",
                "Donghua Yu",
                "Mingshu Chen",
                "Qi Chen",
                "Qi Luo",
                "Qianyi Wu",
                "Qinyuan Cheng",
                "Ruixiao Li",
                "Tianyi Liang",
                "Wenbo Zhang",
                "Wenming Tu",
                "Xiangyu Peng",
                "Yang Gao",
                "Yanru Huo",
                "Ying Zhu",
                "Yinze Luo",
                "Yiyang Zhang",
                "Yuerong Song",
                "Zhe Xu",
                "Zhiyu Zhang",
                "Chenchen Yang",
                "Cheng Chang",
                "Chushu Zhou",
                "Hanfu Chen",
                "Hongnan Ma",
                "Jiaxi Li",
                "Jingqi Tong",
                "Junxi Liu",
                "Ke Chen",
                "Shimin Li",
                "Songlin Wang",
                "Wei Jiang",
                "Zhaoye Fei",
                "Zhiyuan Ning",
                "Chunguo Li",
                "Chenhui Li",
                "Ziwei He",
                "Zengfeng Huang",
                "Xie Chen",
                "Xipeng Qiu"
            ],
            "affiliations": [
                "SII-OpenMOSS Team"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.08794.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#video",
                    "#audio",
                    "#dataset",
                    "#architecture",
                    "#open_source",
                    "#multimodal"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ¡Ğ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ",
                    "desc": "MOVA â€” ÑÑ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ Mixture-of-Experts, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ°Ñ 32 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºĞ°ÑĞºĞ°Ğ´Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ½Ğ°ĞºĞ°Ğ¿Ğ»Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. MOVA Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Image-Text to Video-Audio generation Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ€ĞµÑ‡ÑŒÑ, Ğ·Ğ²ÑƒĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¾Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºÑƒÑÑ‚ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ Ğ¸ Ğ²ĞµÑĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ñ‡ĞµÑ€ĞµĞ· LoRA Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°."
                },
                "en": {
                    "title": "MOVA: Revolutionizing Synchronized Audio-Visual Generation",
                    "desc": "MOVA is an innovative open-source model designed to generate synchronized audio-visual content by utilizing a Mixture-of-Experts (MoE) architecture with 32 billion parameters. This model addresses the common oversight of audio in existing video generation systems, which often rely on inefficient cascaded pipelines that can lead to quality degradation. By enabling simultaneous generation of audio and video, MOVA enhances the realism of outputs, including lip-synced speech and environment-aware sound effects. The release of MOVA's model weights and code aims to promote collaboration and progress in the field of audio-visual content creation."
                },
                "zh": {
                    "title": "MOVAï¼šå¼€æºéŸ³é¢‘-è§†è§‰ç”Ÿæˆçš„æ–°çºªå…ƒ",
                    "desc": "MOVAæ˜¯ä¸€ä¸ªå¼€æºæ¨¡å‹ï¼Œä½¿ç”¨æ··åˆä¸“å®¶æ¶æ„ç”ŸæˆåŒæ­¥çš„éŸ³é¢‘-è§†è§‰å†…å®¹ï¼Œå…·æœ‰320äº¿ä¸ªå‚æ•°ï¼Œæ”¯æŒå›¾åƒ-æ–‡æœ¬åˆ°è§†é¢‘-éŸ³é¢‘çš„ç”Ÿæˆä»»åŠ¡ã€‚è¯¥æ¨¡å‹è§£å†³äº†ç°æœ‰ç”Ÿæˆæ¨¡å‹å¿½è§†éŸ³é¢‘ç»„ä»¶çš„é—®é¢˜ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„åŒæ­¥éŸ³é¢‘-è§†è§‰å†…å®¹ï¼ŒåŒ…æ‹¬é€¼çœŸçš„å£å‹åŒæ­¥è¯­éŸ³å’Œç¯å¢ƒæ„ŸçŸ¥éŸ³æ•ˆã€‚MOVAçš„è®¾è®¡æ—¨åœ¨å…‹æœçº§è”ç®¡é“å¸¦æ¥çš„æˆæœ¬å’Œé”™è¯¯ç´¯ç§¯é—®é¢˜ï¼ŒåŒæ—¶æä¾›é«˜æ•ˆçš„æ¨ç†å’Œå¾®è°ƒæ”¯æŒã€‚é€šè¿‡å‘å¸ƒæ¨¡å‹æƒé‡å’Œä»£ç ï¼Œæˆ‘ä»¬å¸Œæœ›æ¨åŠ¨ç ”ç©¶è¿›å±•ï¼Œå¹¶ä¿ƒè¿›åˆ›ä½œè€…ç¤¾åŒºçš„æ´»è·ƒå‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.07026",
            "title": "Modality Gap-Driven Subspace Alignment Training Paradigm For Multimodal Large Language Models",
            "url": "https://huggingface.co/papers/2602.07026",
            "abstract": "Researchers address the modality gap in multimodal learning by proposing a fixed-frame theory and a training-free alignment method that enables efficient scaling of multimodal models using unpaired data.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite the success of multimodal contrastive learning in aligning visual and linguistic representations, a persistent geometric anomaly, the Modality Gap, remains: embeddings of distinct modalities expressing identical semantics occupy systematically offset regions. Prior approaches to bridge this gap are largely limited by oversimplified isotropic assumptions, hindering their application in large-scale scenarios. In this paper, we address these limitations by precisely characterizing the geometric shape of the modality gap and leveraging it for efficient model scaling. First, we propose the Fixed-frame Modality Gap Theory, which decomposes the modality gap within a frozen reference frame into stable biases and anisotropic residuals. Guided by this precise modeling, we introduce ReAlign, a training-free modality alignment strategy. Utilizing statistics from massive unpaired data, ReAlign aligns text representation into the image representation distribution via a three-step process comprising Anchor, Trace, and Centroid Alignment, thereby explicitly rectifying geometric misalignment. Building on ReAlign, we propose ReVision, a scalable training paradigm for Multimodal Large Language Models (MLLMs). ReVision integrates ReAlign into the pretraining stage, enabling the model to learn the distribution of visual representations from unpaired text before visual instruction tuning, without the need for large-scale, high-quality image-text pairs. Our framework demonstrates that statistically aligned unpaired data can effectively substitute for expensive image-text pairs, offering a robust path for the efficient scaling of MLLMs.",
            "score": 121,
            "issue_id": 980,
            "pub_date": "2026-02-02",
            "pub_date_card": {
                "ru": "2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 2",
                "zh": "2æœˆ2æ—¥"
            },
            "hash": "56c55d551e82f8ed",
            "authors": [
                "Xiaomin Yu",
                "Yi Xin",
                "Wenjie Zhang",
                "Chonghan Liu",
                "Hanzhen Zhao",
                "Xiaoxing Hu",
                "Xinlei Yu",
                "Ziyue Qiao",
                "Hao Tang",
                "Xue Yang",
                "Xiaobin Hu",
                "Chengwei Qin",
                "Hui Xiong",
                "Yu Qiao",
                "Shuicheng Yan"
            ],
            "affiliations": [
                "GBU",
                "HKUST(GZ)",
                "NUS",
                "PKU",
                "SII",
                "SJTU",
                "UCLA",
                "sh AILab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.07026.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#multimodal",
                    "#architecture"
                ],
                "emoji": "ğŸ”—",
                "ru": {
                    "title": "Ğ’Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€ĞµÑˆĞ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸, Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ² Ñ‚ĞµĞ¾Ñ€Ğ¸Ñ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ ReAlign, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºÑƒ Ğ¸Ğ· Ğ½ĞµĞ¿Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ ÑĞºĞ¾Ñ€Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡ĞºĞ¸, Ñ‚Ñ€Ğ°ÑÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ†ĞµĞ½Ñ‚Ñ€Ğ¾Ğ¸Ğ´Ğ¾Ğ². ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ReAlign Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° ReVision Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµĞ¿Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ñ… Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ñ‚ĞµĞºÑÑ‚. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ñ‹Ñ€Ğ¾Ğ²Ğ½ĞµĞ½Ğ½Ñ‹Ğµ Ğ½ĞµĞ¿Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ·Ğ°Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Bridging the Modality Gap for Efficient Multimodal Learning",
                    "desc": "This paper tackles the challenge of the Modality Gap in multimodal learning, which occurs when different types of data (like images and text) that mean the same thing are not aligned properly in their representation spaces. The authors introduce a new theory called Fixed-frame Modality Gap Theory, which helps to understand and decompose the geometric misalignment between these modalities. They propose a method called ReAlign that aligns text and image representations without needing extensive training, using unpaired data instead. Finally, they present ReVision, a scalable approach for training Multimodal Large Language Models that leverages this alignment to improve learning efficiency without relying on large sets of paired data."
                },
                "zh": {
                    "title": "è§£å†³å¤šæ¨¡æ€å­¦ä¹ ä¸­çš„æ¨¡æ€å·®è·",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å›ºå®šæ¡†æ¶ç†è®ºå’Œæ— è®­ç»ƒå¯¹é½æ–¹æ³•ï¼Œä»¥è§£å†³å¤šæ¨¡æ€å­¦ä¹ ä¸­çš„æ¨¡æ€å·®è·é—®é¢˜ã€‚æ¨¡æ€å·®è·æ˜¯æŒ‡ä¸åŒæ¨¡æ€çš„åµŒå…¥åœ¨å‡ ä½•ä¸Šå­˜åœ¨ç³»ç»Ÿæ€§åç§»ï¼Œå¯¼è‡´ç›¸åŒè¯­ä¹‰çš„è¡¨ç¤ºä¸åœ¨åŒä¸€åŒºåŸŸã€‚æˆ‘ä»¬é€šè¿‡ç²¾ç¡®æè¿°æ¨¡æ€å·®è·çš„å‡ ä½•å½¢çŠ¶ï¼Œæå‡ºäº†ReAlignæ–¹æ³•ï¼Œåˆ©ç”¨å¤§é‡æœªé…å¯¹æ•°æ®è¿›è¡Œæ¨¡æ€å¯¹é½ã€‚åŸºäºReAlignï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ReVisionè®­ç»ƒèŒƒå¼ï¼Œèƒ½å¤Ÿåœ¨æ²¡æœ‰å¤§è§„æ¨¡é«˜è´¨é‡å›¾åƒ-æ–‡æœ¬å¯¹çš„æƒ…å†µä¸‹ï¼Œæœ‰æ•ˆæ‰©å±•å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.06855",
            "title": "AIRS-Bench: a Suite of Tasks for Frontier AI Research Science Agents",
            "url": "https://huggingface.co/papers/2602.06855",
            "abstract": "AIRS-Bench presents a comprehensive benchmark suite for evaluating LLM agents across diverse scientific domains, demonstrating current limitations while providing open-source resources for advancing autonomous scientific research.  \t\t\t\t\tAI-generated summary \t\t\t\t LLM agents hold significant promise for advancing scientific research. To accelerate this progress, we introduce AIRS-Bench (the AI Research Science Benchmark), a suite of 20 tasks sourced from state-of-the-art machine learning papers. These tasks span diverse domains, including language modeling, mathematics, bioinformatics, and time series forecasting. AIRS-Bench tasks assess agentic capabilities over the full research lifecycle -- including idea generation, experiment analysis and iterative refinement -- without providing baseline code. The AIRS-Bench task format is versatile, enabling easy integration of new tasks and rigorous comparison across different agentic frameworks. We establish baselines using frontier models paired with both sequential and parallel scaffolds. Our results show that agents exceed human SOTA in four tasks but fail to match it in sixteen others. Even when agents surpass human benchmarks, they do not reach the theoretical performance ceiling for the underlying tasks. These findings indicate that AIRS-Bench is far from saturated and offers substantial room for improvement. We open-source the AIRS-Bench task definitions and evaluation code to catalyze further development in autonomous scientific research.",
            "score": 55,
            "issue_id": 986,
            "pub_date": "2026-02-06",
            "pub_date_card": {
                "ru": "6 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 6",
                "zh": "2æœˆ6æ—¥"
            },
            "hash": "b3c674bbefd84cd7",
            "authors": [
                "Alisia Lupidi",
                "Bhavul Gauri",
                "Thomas Simon Foster",
                "Bassel Al Omari",
                "Despoina Magka",
                "Alberto Pepe",
                "Alexis Audran-Reiss",
                "Muna Aghamelu",
                "Nicolas Baldwin",
                "Lucia Cipolina-Kun",
                "Jean-Christophe Gagnon-Audet",
                "Chee Hau Leow",
                "Sandra Lefdal",
                "Hossam Mossalam",
                "Abhinav Moudgil",
                "Saba Nazir",
                "Emanuel Tewolde",
                "Isabel Urrego",
                "Jordi Armengol Estape",
                "Amar Budhiraja",
                "Gaurav Chaurasia",
                "Abhishek Charnalia",
                "Derek Dunfield",
                "Karen Hambardzumyan",
                "Daniel Izcovich",
                "Martin Josifoski",
                "Ishita Mediratta",
                "Kelvin Niu",
                "Parth Pathak",
                "Michael Shvartsman",
                "Edan Toledo",
                "Anton Protopopov",
                "Roberta Raileanu",
                "Alexander Miller",
                "Tatiana Shavrina",
                "Jakob Foerster",
                "Yoram Bachrach"
            ],
            "affiliations": [
                "FAIR at Meta",
                "University College London",
                "University of Oxford"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.06855.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#open_source",
                    "#survey",
                    "#science",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "ğŸ§ª",
                "ru": {
                    "title": "ĞœĞµÑ€Ğ¸Ğ»Ğ¾ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ AIRS-Bench â€” ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ LLM Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑÑ…. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 20 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºÑƒ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ñ€ÑĞ´Ñ‹, Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ²ÑĞµÑ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ³Ğ¾ Ñ†Ğ¸ĞºĞ»Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ»Ğ¸ÑˆÑŒ Ğ² Ñ‡ĞµÑ‚Ñ‹Ñ€Ñ‘Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ½Ğ¾ Ğ½Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾Ğ»ĞºĞ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ° Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Unlocking the Future of Autonomous Scientific Research with AIRS-Bench",
                    "desc": "AIRS-Bench is a benchmark suite designed to evaluate large language model (LLM) agents across various scientific fields. It includes 20 tasks derived from leading machine learning research, covering areas like language modeling and bioinformatics. The benchmark assesses the capabilities of LLM agents throughout the research process, from generating ideas to analyzing experiments. Results indicate that while some agents outperform human benchmarks in specific tasks, they generally do not reach the maximum potential for these tasks, highlighting opportunities for further advancements in autonomous scientific research."
                },
                "zh": {
                    "title": "AIRS-Benchï¼šæ¨åŠ¨è‡ªä¸»ç§‘å­¦ç ”ç©¶çš„åŸºå‡†å·¥å…·",
                    "desc": "AIRS-Benchæ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†å¥—ä»¶ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸åŒç§‘å­¦é¢†åŸŸçš„è¡¨ç°ã€‚å®ƒåŒ…å«20ä¸ªä»»åŠ¡ï¼Œæ¶µç›–è¯­è¨€å»ºæ¨¡ã€æ•°å­¦ã€ç”Ÿç‰©ä¿¡æ¯å­¦å’Œæ—¶é—´åºåˆ—é¢„æµ‹ç­‰å¤šä¸ªé¢†åŸŸï¼Œæ—¨åœ¨è¯„ä¼°æ™ºèƒ½ä½“åœ¨æ•´ä¸ªç ”ç©¶ç”Ÿå‘½å‘¨æœŸä¸­çš„èƒ½åŠ›ã€‚å°½ç®¡ä¸€äº›æ™ºèƒ½ä½“åœ¨å››ä¸ªä»»åŠ¡ä¸­è¶…è¶Šäº†äººç±»çš„æœ€ä½³è¡¨ç°ï¼Œä½†åœ¨å…¶ä»–åå…­ä¸ªä»»åŠ¡ä¸­ä»æœªè¾¾åˆ°äººç±»æ°´å¹³ã€‚è¿™è¡¨æ˜AIRS-Benchä»æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ï¼Œå¹¶ä¸ºè‡ªä¸»ç§‘å­¦ç ”ç©¶çš„å‘å±•æä¾›äº†å¼€æ”¾æºä»£ç èµ„æºã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.07845",
            "title": "Recurrent-Depth VLA: Implicit Test-Time Compute Scaling of Vision-Language-Action Models via Latent Iterative Reasoning",
            "url": "https://huggingface.co/papers/2602.07845",
            "abstract": "RD-VLA introduces a recurrent architecture for vision-language-action models that adapts computational depth through latent iterative refinement, achieving constant memory usage and improved task success rates.  \t\t\t\t\tAI-generated summary \t\t\t\t Current Vision-Language-Action (VLA) models rely on fixed computational depth, expending the same amount of compute on simple adjustments and complex multi-step manipulation. While Chain-of-Thought (CoT) prompting enables variable computation, it scales memory linearly and is ill-suited for continuous action spaces. We introduce Recurrent-Depth VLA (RD-VLA), an architecture that achieves computational adaptivity via latent iterative refinement rather than explicit token generation. RD-VLA employs a recurrent, weight-tied action head that supports arbitrary inference depth with a constant memory footprint. The model is trained using truncated backpropagation through time (TBPTT) to efficiently supervise the refinement process. At inference, RD-VLA dynamically allocates compute using an adaptive stopping criterion based on latent convergence. Experiments on challenging manipulation tasks show that recurrent depth is critical: tasks that fail entirely (0 percent success) with single-iteration inference exceed 90 percent success with four iterations, while simpler tasks saturate rapidly. RD-VLA provides a scalable path to test-time compute in robotics, replacing token-based reasoning with latent reasoning to achieve constant memory usage and up to 80x inference speedup over prior reasoning-based VLA models. Project page: https://rd-vla.github.io/",
            "score": 49,
            "issue_id": 981,
            "pub_date": "2026-02-08",
            "pub_date_card": {
                "ru": "8 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 8",
                "zh": "2æœˆ8æ—¥"
            },
            "hash": "22a085d4fed206a9",
            "authors": [
                "Yalcin Tur",
                "Jalal Naghiyev",
                "Haoquan Fang",
                "Wei-Chuan Tsai",
                "Jiafei Duan",
                "Dieter Fox",
                "Ranjay Krishna"
            ],
            "affiliations": [
                "Allen Institute for Artificial Intelligence",
                "Stanford University",
                "Technical University of Munich",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.07845.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#training",
                    "#robotics",
                    "#architecture",
                    "#multimodal"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹: ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğµ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ robotics",
                    "desc": "RD-VLA Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ vision-language-action, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñƒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ Ğ² ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… VLA-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ¾Ğ¹, Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²ĞµÑ-ÑĞ²ÑĞ·Ğ°Ğ½Ğ½ÑƒÑ Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºÑƒ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰ÑƒÑ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ truncated backpropagation through time Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ, Ğ° Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ğ°Ñ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ° ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ° Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡: Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ½Ğµ Ñ€ĞµÑˆĞ°Ğ»Ğ¸ÑÑŒ Ğ¿Ñ€Ğ¸ ĞµĞ´Ğ¸Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ 90% ÑƒÑĞ¿ĞµÑ…Ğ° Ğ¿Ñ€Ğ¸ Ñ‡ĞµÑ‚Ñ‹Ñ€Ñ‘Ñ… Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸ÑÑ…, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ² 80 Ñ€Ğ°Ğ· Ğ¸ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Adaptive Depth for Efficient Vision-Language-Action Models",
                    "desc": "The RD-VLA paper presents a new architecture for vision-language-action models that allows for flexible computational depth through a process called latent iterative refinement. Unlike traditional models that use a fixed amount of computation, RD-VLA adapts its resource usage based on the complexity of the task, maintaining a constant memory footprint. This is achieved by employing a recurrent action head and using truncated backpropagation through time for training, which enhances the model's ability to handle complex tasks efficiently. Experimental results demonstrate that RD-VLA significantly improves task success rates, especially in challenging scenarios, by dynamically adjusting its inference depth."
                },
                "zh": {
                    "title": "é€’å½’æ·±åº¦ï¼Œæ™ºèƒ½å†³ç­–çš„æ–°è·¯å¾„",
                    "desc": "RD-VLAæå‡ºäº†ä¸€ç§é€’å½’æ¶æ„ï¼Œç”¨äºè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œé€šè¿‡æ½œåœ¨çš„è¿­ä»£ä¼˜åŒ–æ¥é€‚åº”è®¡ç®—æ·±åº¦ï¼Œä»è€Œå®ç°æ’å®šçš„å†…å­˜ä½¿ç”¨å’Œæé«˜ä»»åŠ¡æˆåŠŸç‡ã€‚ä¸ä¼ ç»Ÿçš„å›ºå®šè®¡ç®—æ·±åº¦æ¨¡å‹ä¸åŒï¼ŒRD-VLAèƒ½å¤Ÿæ ¹æ®ä»»åŠ¡å¤æ‚æ€§åŠ¨æ€è°ƒæ•´è®¡ç®—èµ„æºï¼Œé¿å…äº†åœ¨ç®€å•å’Œå¤æ‚ä»»åŠ¡ä¸Šéƒ½æ¶ˆè€—ç›¸åŒè®¡ç®—é‡çš„é—®é¢˜ã€‚è¯¥æ¨¡å‹é‡‡ç”¨äº†é€’å½’çš„ã€æƒé‡ç»‘å®šçš„åŠ¨ä½œå¤´ï¼Œæ”¯æŒä»»æ„æ¨ç†æ·±åº¦ï¼ŒåŒæ—¶ä¿æŒæ’å®šçš„å†…å­˜å ç”¨ã€‚å®éªŒè¡¨æ˜ï¼Œé€’å½’æ·±åº¦å¯¹ä»»åŠ¡æˆåŠŸç‡è‡³å…³é‡è¦ï¼ŒRD-VLAåœ¨æœºå™¨äººé¢†åŸŸæä¾›äº†ä¸€ç§å¯æ‰©å±•çš„æµ‹è¯•æ—¶è®¡ç®—è·¯å¾„ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.08676",
            "title": "LLaDA2.1: Speeding Up Text Diffusion via Token Editing",
            "url": "https://huggingface.co/papers/2602.08676",
            "abstract": "LLaDA2.1 introduces a novel token-to-token editing approach with speed and quality modes, enhanced through reinforcement learning for improved reasoning and instruction following in large language diffusion models.  \t\t\t\t\tAI-generated summary \t\t\t\t While LLaDA2.0 showcased the scaling potential of 100B-level block-diffusion models and their inherent parallelization, the delicate equilibrium between decoding speed and generation quality has remained an elusive frontier. Today, we unveil LLaDA2.1, a paradigm shift designed to transcend this trade-off. By seamlessly weaving Token-to-Token (T2T) editing into the conventional Mask-to-Token (M2T) scheme, we introduce a joint, configurable threshold-decoding scheme. This structural innovation gives rise to two distinct personas: the Speedy Mode (S Mode), which audaciously lowers the M2T threshold to bypass traditional constraints while relying on T2T to refine the output; and the Quality Mode (Q Mode), which leans into conservative thresholds to secure superior benchmark performances with manageable efficiency degrade. Furthering this evolution, underpinned by an expansive context window, we implement the first large-scale Reinforcement Learning (RL) framework specifically tailored for dLLMs, anchored by specialized techniques for stable gradient estimation. This alignment not only sharpens reasoning precision but also elevates instruction-following fidelity, bridging the chasm between diffusion dynamics and complex human intent. We culminate this work by releasing LLaDA2.1-Mini (16B) and LLaDA2.1-Flash (100B). Across 33 rigorous benchmarks, LLaDA2.1 delivers strong task performance and lightning-fast decoding speed. Despite its 100B volume, on coding tasks it attains an astounding 892 TPS on HumanEval+, 801 TPS on BigCodeBench, and 663 TPS on LiveCodeBench.",
            "score": 42,
            "issue_id": 981,
            "pub_date": "2026-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "fcabdb36b8d8199a",
            "authors": [
                "Tiwei Bie",
                "Maosong Cao",
                "Xiang Cao",
                "Bingsen Chen",
                "Fuyuan Chen",
                "Kun Chen",
                "Lun Du",
                "Daozhuo Feng",
                "Haibo Feng",
                "Mingliang Gong",
                "Zhuocheng Gong",
                "Yanmei Gu",
                "Jian Guan",
                "Kaiyuan Guan",
                "Hongliang He",
                "Zenan Huang",
                "Juyong Jiang",
                "Zhonghui Jiang",
                "Zhenzhong Lan",
                "Chengxi Li",
                "Jianguo Li",
                "Zehuan Li",
                "Huabin Liu",
                "Lin Liu",
                "Guoshan Lu",
                "Yuan Lu",
                "Yuxin Ma",
                "Xingyu Mou",
                "Zhenxuan Pan",
                "Kaida Qiu",
                "Yuji Ren",
                "Jianfeng Tan",
                "Yiding Tian",
                "Zian Wang",
                "Lanning Wei",
                "Tao Wu",
                "Yipeng Xing",
                "Wentao Ye",
                "Liangyu Zha",
                "Tianze Zhang",
                "Xiaolu Zhang",
                "Junbo Zhao",
                "Da Zheng",
                "Hao Zhong",
                "Wanli Zhong",
                "Jun Zhou",
                "Junlin Zhou",
                "Liwang Zhu",
                "Muzhi Zhu",
                "Yihong Zhuang"
            ],
            "affiliations": [
                "Ant Group",
                "Southern University of Science and Technology",
                "Westlake University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.08676.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#training",
                    "#optimization",
                    "#alignment",
                    "#benchmark",
                    "#architecture",
                    "#open_source",
                    "#reasoning",
                    "#diffusion",
                    "#plp"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ‘Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ° ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‡ĞµÑ€ĞµĞ· T2T Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "LLaDA2.1 Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ°Ğ¼Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ³Ğ¸Ğ±ĞºĞ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Mask-to-Token Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ Token-to-Token Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². Ğ”Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ²Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ° Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ’Ñ‹Ğ¿ÑƒÑ‰ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ LLaDA2.1-Mini (16B) Ğ¸ LLaDA2.1-Flash (100B) Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° 33 Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ñ€ĞµĞºĞ¾Ñ€Ğ´Ğ½Ğ¾Ğ¹ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ â€” Ğ±Ğ¾Ğ»ĞµĞµ 800 Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² ÑĞµĞºÑƒĞ½Ğ´Ñƒ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Balancing Speed and Quality in Language Models with LLaDA2.1",
                    "desc": "LLaDA2.1 presents a new method for editing tokens in large language models, balancing speed and quality through innovative techniques. It introduces a Token-to-Token (T2T) editing approach alongside the existing Mask-to-Token (M2T) method, allowing for configurable decoding modes. The Speedy Mode prioritizes fast output generation, while the Quality Mode focuses on achieving high performance with a slight efficiency trade-off. Additionally, it employs a large-scale Reinforcement Learning framework to enhance reasoning and instruction adherence, resulting in impressive performance across various benchmarks."
                },
                "zh": {
                    "title": "çªç ´é€Ÿåº¦ä¸è´¨é‡çš„å¹³è¡¡ï¼ŒLLaDA2.1å¼•é¢†æ–°æ½®æµ",
                    "desc": "LLaDA2.1æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä»¤ç‰Œåˆ°ä»¤ç‰Œç¼–è¾‘æ–¹æ³•ï¼Œç»“åˆäº†é€Ÿåº¦å’Œè´¨é‡æ¨¡å¼ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ æå‡äº†å¤§å‹è¯­è¨€æ‰©æ•£æ¨¡å‹çš„æ¨ç†å’ŒæŒ‡ä»¤è·Ÿéšèƒ½åŠ›ã€‚è¯¥æ–¹æ³•åœ¨ä¼ ç»Ÿçš„æ©ç åˆ°ä»¤ç‰Œæ–¹æ¡ˆä¸­æ— ç¼èå…¥äº†ä»¤ç‰Œåˆ°ä»¤ç‰Œç¼–è¾‘ï¼Œå½¢æˆäº†ä¸€ä¸ªå¯é…ç½®çš„é˜ˆå€¼è§£ç æ–¹æ¡ˆã€‚LLaDA2.1å…·æœ‰ä¸¤ç§æ¨¡å¼ï¼šå¿«é€Ÿæ¨¡å¼é€šè¿‡é™ä½é˜ˆå€¼æ¥æé«˜é€Ÿåº¦ï¼Œè€Œè´¨é‡æ¨¡å¼åˆ™é€šè¿‡ä¿å®ˆçš„é˜ˆå€¼ç¡®ä¿æ›´é«˜çš„æ€§èƒ½ã€‚é€šè¿‡å¤§è§„æ¨¡çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼ŒLLaDA2.1åœ¨æ¨ç†ç²¾åº¦å’ŒæŒ‡ä»¤éµå¾ªæ–¹é¢å–å¾—äº†æ˜¾è‘—æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.06422",
            "title": "Alleviating Sparse Rewards by Modeling Step-Wise and Long-Term Sampling Effects in Flow-Based GRPO",
            "url": "https://huggingface.co/papers/2602.06422",
            "abstract": "TP-GRPO addresses reward sparsity in flow matching models by introducing step-level incremental rewards and identifying turning points to capture long-term effects in denoising trajectories.  \t\t\t\t\tAI-generated summary \t\t\t\t Deploying GRPO on Flow Matching models has proven effective for text-to-image generation. However, existing paradigms typically propagate an outcome-based reward to all preceding denoising steps without distinguishing the local effect of each step. Moreover, current group-wise ranking mainly compares trajectories at matched timesteps and ignores within-trajectory dependencies, where certain early denoising actions can affect later states via delayed, implicit interactions. We propose TurningPoint-GRPO (TP-GRPO), a GRPO framework that alleviates step-wise reward sparsity and explicitly models long-term effects within the denoising trajectory. TP-GRPO makes two key innovations: (i) it replaces outcome-based rewards with step-level incremental rewards, providing a dense, step-aware learning signal that better isolates each denoising action's \"pure\" effect, and (ii) it identifies turning points-steps that flip the local reward trend and make subsequent reward evolution consistent with the overall trajectory trend-and assigns these actions an aggregated long-term reward to capture their delayed impact. Turning points are detected solely via sign changes in incremental rewards, making TP-GRPO efficient and hyperparameter-free. Extensive experiments also demonstrate that TP-GRPO exploits reward signals more effectively and consistently improves generation. Demo code is available at https://github.com/YunzeTong/TurningPoint-GRPO.",
            "score": 37,
            "issue_id": 980,
            "pub_date": "2026-02-06",
            "pub_date_card": {
                "ru": "6 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 6",
                "zh": "2æœˆ6æ—¥"
            },
            "hash": "92898f90dd66cb60",
            "authors": [
                "Yunze Tong",
                "Mushui Liu",
                "Canyu Zhao",
                "Wanggui He",
                "Shiyi Zhang",
                "Hongwei Zhang",
                "Peng Zhang",
                "Jinlong Liu",
                "Ju Huang",
                "Jiamang Wang",
                "Hao Jiang",
                "Pipei Huang"
            ],
            "affiliations": [
                "Alibaba Group, Hangzhou, China",
                "Tsinghua University, Beijing, China",
                "Zhejiang University, Hangzhou, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.06422.jpg",
            "data": {
                "categories": [
                    "#optimization"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğµ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "TP-GRPO Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… flow matching Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ²Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ½ĞºÑ€ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²ÑĞµĞ¹ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ğ½Ğ³Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ² Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ â€” Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ñ‹, Ğ³Ğ´Ğµ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚Ñ€ĞµĞ½Ğ´ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ñ€Ğ°Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ â€” Ğ¸ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡Ğ°ĞµÑ‚ Ğ¸Ğ¼ Ğ°Ğ³Ñ€ĞµĞ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ¾Ñ‚Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… ÑÑ„Ñ„ĞµĞºÑ‚Ğ¾Ğ². Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ³Ğ½Ğ°Ğ» Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¸Ğ·Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑˆĞ°Ğ³Ğ° Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ğ½Ğ³Ğ° Ğ¸ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ TP-GRPO ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°."
                },
                "en": {
                    "title": "Enhancing Reward Signals for Better Denoising in Flow Matching",
                    "desc": "TP-GRPO is a novel framework designed to improve reward mechanisms in flow matching models, particularly for text-to-image generation tasks. It introduces step-level incremental rewards that provide a more detailed learning signal, allowing for better isolation of the effects of individual denoising actions. Additionally, TP-GRPO identifies turning points in the reward trend, which are critical steps that influence future rewards, thus capturing long-term dependencies in the denoising process. This approach enhances the efficiency of reward utilization and leads to consistent improvements in generation quality."
                },
                "zh": {
                    "title": "TP-GRPOï¼šæå‡å»å™ªæ¨¡å‹çš„å¥–åŠ±åˆ©ç”¨æ•ˆç‡",
                    "desc": "TP-GRPOæ˜¯ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æµåŒ¹é…æ¨¡å‹ä¸­çš„å¥–åŠ±ç¨€ç–é—®é¢˜ã€‚å®ƒé€šè¿‡å¼•å…¥é€æ­¥å¢é‡å¥–åŠ±å’Œè¯†åˆ«è½¬æŠ˜ç‚¹æ¥æ•æ‰å»å™ªè½¨è¿¹ä¸­çš„é•¿æœŸå½±å“ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒTP-GRPOä¸ºæ¯ä¸ªå»å™ªæ­¥éª¤æä¾›äº†æ›´å¯†é›†çš„å­¦ä¹ ä¿¡å·ï¼Œå¹¶ä¸”èƒ½å¤Ÿæœ‰æ•ˆåœ°è¯†åˆ«é‚£äº›å½±å“åç»­å¥–åŠ±çš„å…³é”®æ­¥éª¤ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTP-GRPOåœ¨ç”Ÿæˆä»»åŠ¡ä¸­èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨å¥–åŠ±ä¿¡å·ï¼Œæ˜¾è‘—æé«˜ç”Ÿæˆè´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.09007",
            "title": "GEBench: Benchmarking Image Generation Models as GUI Environments",
            "url": "https://huggingface.co/papers/2602.09007",
            "abstract": "A new benchmark and evaluation metric are introduced for assessing temporal coherence and dynamic interaction in GUI generation models, revealing significant challenges in maintaining consistency over extended interaction sequences.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in image generation models have enabled the prediction of future Graphical User Interface (GUI) states based on user instructions. However, existing benchmarks primarily focus on general domain visual fidelity, leaving the evaluation of state transitions and temporal coherence in GUI-specific contexts underexplored. To address this gap, we introduce GEBench, a comprehensive benchmark for evaluating dynamic interaction and temporal coherence in GUI generation. GEBench comprises 700 carefully curated samples spanning five task categories, covering both single-step interactions and multi-step trajectories across real-world and fictional scenarios, as well as grounding point localization. To support systematic evaluation, we propose GE-Score, a novel five-dimensional metric that assesses Goal Achievement, Interaction Logic, Content Consistency, UI Plausibility, and Visual Quality. Extensive evaluations on current models indicate that while they perform well on single-step transitions, they struggle significantly with maintaining temporal coherence and spatial grounding over longer interaction sequences. Our findings identify icon interpretation, text rendering, and localization precision as critical bottlenecks. This work provides a foundation for systematic assessment and suggests promising directions for future research toward building high-fidelity generative GUI environments. The code is available at: https://github.com/stepfun-ai/GEBench.",
            "score": 33,
            "issue_id": 982,
            "pub_date": "2026-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "5b641f3312e9b596",
            "authors": [
                "Haodong Li",
                "Jingwei Wu",
                "Quan Sun",
                "Guopeng Li",
                "Juanxi Tian",
                "Huanyu Zhang",
                "Yanlin Lai",
                "Ruichuan An",
                "Hongbo Peng",
                "Yuhong Dai",
                "Chenxi Li",
                "Chunmei Qing",
                "Jia Wang",
                "Ziyang Meng",
                "Zheng Ge",
                "Xiangyu Zhang",
                "Daxin Jiang"
            ],
            "affiliations": [
                "Peking University",
                "South China University of Technology",
                "StepFun"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.09007.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#cv",
                    "#dataset"
                ],
                "emoji": "ğŸ–¥ï¸",
                "ru": {
                    "title": "ĞÑ†ĞµĞ½ĞºĞ° Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ³ĞµÑ€ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ²",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº GEBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ², ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 700 Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ñ Ğ¾Ğ´Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ GE-Score Ñ Ğ¿ÑÑ‚ÑŒÑ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸ÑĞ¼Ğ¸: Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ñ†ĞµĞ»Ğ¸, Ğ»Ğ¾Ğ³Ğ¸ĞºĞ° Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ, ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°, Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ° Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ¾Ğ´Ğ½Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸, Ğ½Ğ¾ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ° ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑƒĞ·ĞºĞ¸Ğµ Ğ¼ĞµÑÑ‚Ğ° Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ĞºĞ¾Ğ½Ğ¾Ğº, Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ´Ğ»Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ GUI."
                },
                "en": {
                    "title": "GEBench: Elevating GUI Generation with Temporal Coherence Metrics",
                    "desc": "This paper introduces GEBench, a new benchmark designed to evaluate the temporal coherence and dynamic interaction capabilities of GUI generation models. It highlights the limitations of existing benchmarks that focus mainly on visual fidelity, neglecting the assessment of state transitions in GUI contexts. The authors propose a novel metric called GE-Score, which evaluates five dimensions: Goal Achievement, Interaction Logic, Content Consistency, UI Plausibility, and Visual Quality. The findings reveal that while current models excel in single-step interactions, they face significant challenges in maintaining coherence and accuracy over longer sequences of user interactions."
                },
                "zh": {
                    "title": "æå‡GUIç”Ÿæˆæ¨¡å‹çš„æ—¶é—´ä¸€è‡´æ€§ä¸åŠ¨æ€äº¤äº’è¯„ä¼°",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„åŸºå‡†æµ‹è¯•å’Œè¯„ä¼°æŒ‡æ ‡ï¼Œç”¨äºè¯„ä¼°å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ç”Ÿæˆæ¨¡å‹ä¸­çš„æ—¶é—´ä¸€è‡´æ€§å’ŒåŠ¨æ€äº¤äº’ã€‚ç°æœ‰çš„è¯„ä¼°ä¸»è¦å…³æ³¨è§†è§‰è´¨é‡ï¼Œè€Œå¯¹çŠ¶æ€è½¬æ¢å’Œæ—¶é—´ä¸€è‡´æ€§çš„è¯„ä¼°åˆ™ç›¸å¯¹ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†GEBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†ï¼Œæ¶µç›–äº†700ä¸ªæ ·æœ¬ï¼Œæ¶‰åŠå•æ­¥å’Œå¤šæ­¥äº¤äº’çš„ä»»åŠ¡ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†GE-Scoreï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„äº”ç»´æŒ‡æ ‡ï¼Œç”¨äºç³»ç»Ÿè¯„ä¼°ç”Ÿæˆçš„GUIçš„å„ä¸ªæ–¹é¢ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.08439",
            "title": "Demo-ICL: In-Context Learning for Procedural Video Knowledge Acquisition",
            "url": "https://huggingface.co/papers/2602.08439",
            "abstract": "Researchers introduce a new video understanding task and benchmark that evaluates models' ability to learn from few-shot demonstrations, along with a specialized MLLM architecture trained using a two-stage approach combining video supervision and preference optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite the growing video understanding capabilities of recent Multimodal Large Language Models (MLLMs), existing video benchmarks primarily assess understanding based on models' static, internal knowledge, rather than their ability to learn and adapt from dynamic, novel contexts from few examples. To bridge this gap, we present Demo-driven Video In-Context Learning, a novel task focused on learning from in-context demonstrations to answer questions about the target videos. Alongside this, we propose Demo-ICL-Bench, a challenging benchmark designed to evaluate demo-driven video in-context learning capabilities. Demo-ICL-Bench is constructed from 1200 instructional YouTube videos with associated questions, from which two types of demonstrations are derived: (i) summarizing video subtitles for text demonstration; and (ii) corresponding instructional videos as video demonstrations. To effectively tackle this new challenge, we develop Demo-ICL, an MLLM with a two-stage training strategy: video-supervised fine-tuning and information-assisted direct preference optimization, jointly enhancing the model's ability to learn from in-context examples. Extensive experiments with state-of-the-art MLLMs confirm the difficulty of Demo-ICL-Bench, demonstrate the effectiveness of Demo-ICL, and thereby unveil future research directions.",
            "score": 28,
            "issue_id": 981,
            "pub_date": "2026-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "faa968b3b5bd841c",
            "authors": [
                "Yuhao Dong",
                "Shulin Tian",
                "Shuai Liu",
                "Shuangrui Ding",
                "Yuhang Zang",
                "Xiaoyi Dong",
                "Yuhang Cao",
                "Jiaqi Wang",
                "Ziwei Liu"
            ],
            "affiliations": [
                "CUHK-MMLab",
                "S-Lab, Nanyang Technological University",
                "Shanghai AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.08439.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#video",
                    "#benchmark",
                    "#dataset",
                    "#multimodal",
                    "#rlhf"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ½ĞµĞ¼Ğ½Ğ¾Ğ³Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ… Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ Ñ‡Ğ¸ÑĞ»Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (MLLM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ¼, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ¾ÑÑƒĞ¿ĞµÑ€Ğ²Ğ¸Ğ·Ğ¸Ñ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Demo-ICL-Bench ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ 1200 Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ YouTube Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ²Ğ° Ñ‚Ğ¸Ğ¿Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¹: Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑĞ¼Ğµ ÑÑƒĞ±Ñ‚Ğ¸Ñ‚Ñ€Ğ¾Ğ² Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Demo-ICL Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°, Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… MLLM Ğ² Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼."
                },
                "en": {
                    "title": "Learning from Few Examples in Video Understanding",
                    "desc": "This paper introduces a new task called Demo-driven Video In-Context Learning, which focuses on how models can learn from a few examples in dynamic video contexts. The authors present a benchmark, Demo-ICL-Bench, that evaluates this learning ability using 1200 instructional YouTube videos and associated questions. They propose a specialized Multimodal Large Language Model (MLLM) called Demo-ICL, which is trained in two stages: first through video supervision and then through preference optimization. The results show that Demo-ICL effectively addresses the challenges of the new benchmark, highlighting the potential for future advancements in video understanding."
                },
                "zh": {
                    "title": "ç¤ºä¾‹é©±åŠ¨çš„è§†é¢‘ç†è§£æ–°æŒ‘æˆ˜",
                    "desc": "ç ”ç©¶äººå‘˜æå‡ºäº†ä¸€é¡¹æ–°çš„è§†é¢‘ç†è§£ä»»åŠ¡å’ŒåŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹ä»å°‘é‡ç¤ºä¾‹ä¸­å­¦ä¹ çš„èƒ½åŠ›ã€‚è¯¥ä»»åŠ¡ç§°ä¸ºç¤ºä¾‹é©±åŠ¨çš„è§†é¢‘ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œé‡ç‚¹åœ¨äºé€šè¿‡ä¸Šä¸‹æ–‡ç¤ºä¾‹å›ç­”å…³äºç›®æ ‡è§†é¢‘çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†Demo-ICLï¼Œä¸€ä¸ªé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œç»“åˆè§†é¢‘ç›‘ç£å’Œåå¥½ä¼˜åŒ–ã€‚é€šè¿‡å¤§é‡å®éªŒï¼ŒéªŒè¯äº†Demo-ICL-Benchçš„æŒ‘æˆ˜æ€§ï¼Œå¹¶å±•ç¤ºäº†Demo-ICLçš„æœ‰æ•ˆæ€§ï¼Œæ­ç¤ºäº†æœªæ¥çš„ç ”ç©¶æ–¹å‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.06025",
            "title": "Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory",
            "url": "https://huggingface.co/papers/2602.06025",
            "abstract": "BudgetMem is a runtime memory framework for LLM agents that uses modular components with three budget tiers and a neural policy router to optimize performance-cost trade-offs in memory usage.  \t\t\t\t\tAI-generated summary \t\t\t\t Memory is increasingly central to Large Language Model (LLM) agents operating beyond a single context window, yet most existing systems rely on offline, query-agnostic memory construction that can be inefficient and may discard query-critical information. Although runtime memory utilization is a natural alternative, prior work often incurs substantial overhead and offers limited explicit control over the performance-cost trade-off. In this work, we present BudgetMem, a runtime agent memory framework for explicit, query-aware performance-cost control. BudgetMem structures memory processing as a set of memory modules, each offered in three budget tiers (i.e., Low/Mid/High). A lightweight router performs budget-tier routing across modules to balance task performance and memory construction cost, which is implemented as a compact neural policy trained with reinforcement learning. Using BudgetMem as a unified testbed, we study three complementary strategies for realizing budget tiers: implementation (method complexity), reasoning (inference behavior), and capacity (module model size). Across LoCoMo, LongMemEval, and HotpotQA, BudgetMem surpasses strong baselines when performance is prioritized (i.e., high-budget setting), and delivers better accuracy-cost frontiers under tighter budgets. Moreover, our analysis disentangles the strengths and weaknesses of different tiering strategies, clarifying when each axis delivers the most favorable trade-offs under varying budget regimes.",
            "score": 27,
            "issue_id": 982,
            "pub_date": "2026-02-05",
            "pub_date_card": {
                "ru": "5 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 5",
                "zh": "2æœˆ5æ—¥"
            },
            "hash": "a764ee2002f6dbd6",
            "authors": [
                "Haozhen Zhang",
                "Haodong Yue",
                "Tao Feng",
                "Quanyu Long",
                "Jianzhu Bao",
                "Bowen Jin",
                "Weizhi Zhang",
                "Xiao Li",
                "Jiaxuan You",
                "Chengwei Qin",
                "Wenya Wang"
            ],
            "affiliations": [
                "Nanyang Technological University",
                "Sun Yat-sen University",
                "The Hong Kong University of Science and Technology (Guangzhou)",
                "Tsinghua University",
                "University of Illinois Chicago",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.06025.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#rl",
                    "#agents",
                    "#long_context",
                    "#training"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "Ğ˜Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "BudgetMem â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ Ñ‚Ñ€ĞµĞ¼Ñ ÑƒÑ€Ğ¾Ğ²Ğ½ÑĞ¼Ğ¸ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ° Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ğ¾Ğ¹ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ° Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ Ñ‚Ñ€Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¹ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ°: ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°, Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… LoCoMo, LongMemEval Ğ¸ HotpotQA Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ BudgetMem Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Optimizing Memory Management in LLMs with BudgetMem",
                    "desc": "BudgetMem is a novel framework designed to enhance memory management in Large Language Model (LLM) agents by providing a structured approach to runtime memory usage. It introduces three budget tiersâ€”Low, Mid, and Highâ€”allowing for flexible performance-cost trade-offs based on the task requirements. A neural policy router intelligently directs memory requests to the appropriate budget tier, optimizing both efficiency and effectiveness. Through extensive testing, BudgetMem demonstrates superior performance in high-budget scenarios and improved accuracy under constrained budgets, offering valuable insights into memory management strategies."
                },
                "zh": {
                    "title": "ä¼˜åŒ–å†…å­˜ä½¿ç”¨çš„æ™ºèƒ½æ¡†æ¶",
                    "desc": "BudgetMem æ˜¯ä¸€ä¸ªé’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†çš„è¿è¡Œæ—¶å†…å­˜æ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜åŒ–å†…å­˜ä½¿ç”¨ä¸­çš„æ€§èƒ½ä¸æˆæœ¬ä¹‹é—´çš„æƒè¡¡ã€‚è¯¥æ¡†æ¶é‡‡ç”¨æ¨¡å—åŒ–ç»„ä»¶ï¼Œæä¾›ä¸‰ç§é¢„ç®—å±‚çº§ï¼ˆä½ã€ä¸­ã€é«˜ï¼‰ï¼Œå¹¶é€šè¿‡ç¥ç»ç­–ç•¥è·¯ç”±å™¨è¿›è¡Œå†…å­˜å¤„ç†ã€‚ä¸ä¼ ç»Ÿçš„ç¦»çº¿å†…å­˜æ„å»ºæ–¹æ³•ç›¸æ¯”ï¼ŒBudgetMem å…è®¸æ›´çµæ´»çš„æŸ¥è¯¢æ„ŸçŸ¥æ§åˆ¶ï¼Œå‡å°‘äº†ä¸å¿…è¦çš„ä¿¡æ¯ä¸¢å¤±ã€‚é€šè¿‡åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒBudgetMem æ˜¾ç¤ºäº†åœ¨ä¸åŒé¢„ç®—æ¡ä»¶ä¸‹å®ç°æœ€ä½³æ€§èƒ½çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.07962",
            "title": "LOCA-bench: Benchmarking Language Agents Under Controllable and Extreme Context Growth",
            "url": "https://huggingface.co/papers/2602.07962",
            "abstract": "LOCA-bench is introduced as a benchmark for evaluating language agents in long-context, agentic scenarios with controlled environment state management.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are increasingly capable of carrying out long-running, real-world tasks. However, as the amount of context grows, their reliability often deteriorates, a phenomenon known as \"context rot\". Existing long-context benchmarks primarily focus on single-step settings that evaluate a model's ability to retrieve information from a long snippet. In realistic scenarios, however, LLMs often need to act as agents that explore environments, follow instructions and plans, extract useful information, and predict correct actions under a dynamically growing context. To assess language agents in such settings, we introduce LOCA-bench (a benchmark for LOng-Context Agents). Given a task prompt, LOCA-bench leverages automated and scalable control of environment states to regulate the agent's context length. This design enables LOCA-bench to extend the context length potentially to infinity in a controlled way while keeping the underlying task semantics fixed. LOCA-bench evaluates language agents as a combination of models and scaffolds, including various context management strategies. While agent performance generally degrades as the environment states grow more complex, advanced context management techniques can substantially improve the overall success rate. We open-source LOCA-bench to provide a platform for evaluating models and scaffolds in long-context, agentic scenarios: https://github.com/hkust-nlp/LOCA-bench",
            "score": 23,
            "issue_id": 980,
            "pub_date": "2026-02-08",
            "pub_date_card": {
                "ru": "8 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 8",
                "zh": "2æœˆ8æ—¥"
            },
            "hash": "483ad401f8702de9",
            "authors": [
                "Weihao Zeng",
                "Yuzhen Huang",
                "Junxian He"
            ],
            "affiliations": [
                "HKUST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.07962.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#long_context",
                    "#open_source",
                    "#benchmark"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ñ… Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ’Ğ²ĞµĞ´ĞµĞ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº LOCA-bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ĞµĞ¼ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾ Ğ¼ĞµÑ€Ğµ Ñ€Ğ¾ÑÑ‚Ğ° ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ LLM ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ÑÑ (ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Â«ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ°Ğ´Ğ°Â»). Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ², Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ…ÑÑ Ğ½Ğ° Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, LOCA-bench Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ³Ğ´Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ, ÑĞ»ĞµĞ´ÑƒÑÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ¸ Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°ÑÑ‚ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€Ğ°ÑÑ‚ÑƒÑ‰ĞµĞ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ ÑĞ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Evaluating Language Agents in Dynamic Long-Context Scenarios",
                    "desc": "LOCA-bench is a new benchmark designed to evaluate language agents in complex, long-context scenarios where they must manage their environment effectively. It addresses the issue of 'context rot', where the performance of large language models declines as the context length increases. Unlike existing benchmarks that focus on single-step tasks, LOCA-bench allows for dynamic context management, enabling agents to handle tasks that require exploration and decision-making over extended interactions. By using advanced context management strategies, LOCA-bench aims to improve the reliability and success rates of language agents in real-world applications."
                },
                "zh": {
                    "title": "LOCA-benchï¼šé•¿ä¸Šä¸‹æ–‡ä»£ç†çš„è¯„ä¼°æ–°æ ‡å‡†",
                    "desc": "LOCA-benchæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°è¯­è¨€ä»£ç†åœ¨é•¿ä¸Šä¸‹æ–‡å’Œä»£ç†åœºæ™¯ä¸­çš„åŸºå‡†æµ‹è¯•å·¥å…·ã€‚å®ƒé€šè¿‡è‡ªåŠ¨åŒ–å’Œå¯æ‰©å±•çš„ç¯å¢ƒçŠ¶æ€æ§åˆ¶ï¼Œè°ƒèŠ‚ä»£ç†çš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œä»¥åº”å¯¹å¤æ‚çš„ä»»åŠ¡ã€‚å°½ç®¡ç¯å¢ƒçŠ¶æ€çš„å¤æ‚æ€§ä¼šå¯¼è‡´ä»£ç†æ€§èƒ½ä¸‹é™ï¼Œä½†å…ˆè¿›çš„ä¸Šä¸‹æ–‡ç®¡ç†æŠ€æœ¯å¯ä»¥æ˜¾è‘—æé«˜æˆåŠŸç‡ã€‚LOCA-benchä¸ºç ”ç©¶äººå‘˜æä¾›äº†ä¸€ä¸ªå¼€æ”¾çš„å¹³å°ï¼Œä»¥è¯„ä¼°åœ¨é•¿ä¸Šä¸‹æ–‡ä»£ç†åœºæ™¯ä¸­çš„æ¨¡å‹å’Œæ”¯æ¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.00169",
            "title": "Towards Agentic Intelligence for Materials Science",
            "url": "https://huggingface.co/papers/2602.00169",
            "abstract": "AI-driven materials science integrates large language models across discovery pipelines from data curation to agent-based experimentation, emphasizing system-level optimization and autonomous goal pursuit.  \t\t\t\t\tAI-generated summary \t\t\t\t The convergence of artificial intelligence and materials science presents a transformative opportunity, but achieving true acceleration in discovery requires moving beyond task-isolated, fine-tuned models toward agentic systems that plan, act, and learn across the full discovery loop. This survey advances a unique pipeline-centric view that spans from corpus curation and pretraining, through domain adaptation and instruction tuning, to goal-conditioned agents interfacing with simulation and experimental platforms. Unlike prior reviews, we treat the entire process as an end-to-end system to be optimized for tangible discovery outcomes rather than proxy benchmarks. This perspective allows us to trace how upstream design choices-such as data curation and training objectives-can be aligned with downstream experimental success through effective credit assignment.   To bridge communities and establish a shared frame of reference, we first present an integrated lens that aligns terminology, evaluation, and workflow stages across AI and materials science. We then analyze the field through two focused lenses: From the AI perspective, the survey details LLM strengths in pattern recognition, predictive analytics, and natural language processing for literature mining, materials characterization, and property prediction; from the materials science perspective, it highlights applications in materials design, process optimization, and the acceleration of computational workflows via integration with external tools (e.g., DFT, robotic labs). Finally, we contrast passive, reactive approaches with agentic design, cataloging current contributions while motivating systems that pursue long-horizon goals with autonomy, memory, and tool use. This survey charts a practical roadmap towards autonomous, safety-aware LLM agents aimed at discovering novel and useful materials.",
            "score": 20,
            "issue_id": 990,
            "pub_date": "2026-01-29",
            "pub_date_card": {
                "ru": "29 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 29",
                "zh": "1æœˆ29æ—¥"
            },
            "hash": "51d3fde62f46bbea",
            "authors": [
                "Huan Zhang",
                "Yizhan Li",
                "Wenhao Huang",
                "Ziyu Hou",
                "Yu Song",
                "Xuye Liu",
                "Farshid Effaty",
                "Jinya Jiang",
                "Sifan Wu",
                "Qianggang Ding",
                "Izumi Takahara",
                "Leonard R. MacGillivray",
                "Teruyasu Mizoguchi",
                "Tianshu Yu",
                "Lizi Liao",
                "Yuyu Luo",
                "Yu Rong",
                "Jia Li",
                "Ying Diao",
                "Heng Ji",
                "Bang Liu"
            ],
            "affiliations": [
                "Alibaba DAMO Academy",
                "Canada CIFAR AI Chair",
                "DIRO & Institut Courtois, UniversitÃ© de MontrÃ©al",
                "Mila Quebec AI Institute",
                "Singapore Management University",
                "The Chinese University of Hong Kong, Shenzhen",
                "The Hong Kong University of Science and Technology, Guangzhou",
                "The University of Tokyo, Institute of Industrial Science",
                "University of California, San Diego",
                "University of Illinois Urbana-Champaign",
                "University of Waterloo",
                "UniversitÃ© de Sherbrooke"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.00169.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#training",
                    "#survey",
                    "#science",
                    "#optimization",
                    "#agents",
                    "#data"
                ],
                "emoji": "ğŸ§ª",
                "ru": {
                    "title": "ĞÑ‚ Ğ¸Ğ·Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğ¼ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼: LLM Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ²",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ Ñ†Ğ¸ĞºĞ» Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ², Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²ĞµÑÑŒ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğº ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ğ´Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ†ĞµĞ»ĞµĞ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸ÑĞ¼Ğ¸ Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ¼Ğ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ¾Ğ², Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ ĞºĞ°Ğº ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğ¸ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ², Ğ° Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹ ĞºĞ°Ğº AI, Ñ‚Ğ°Ğº Ğ¸ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ, Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñ‹, Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² Ğ¸ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ° Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼, Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸ĞµĞ¼."
                },
                "en": {
                    "title": "Empowering Materials Discovery with Autonomous AI Agents",
                    "desc": "This paper discusses how artificial intelligence, particularly large language models (LLMs), can enhance materials science by creating a comprehensive discovery pipeline. It emphasizes the need for systems that can autonomously plan, act, and learn throughout the entire discovery process, rather than just focusing on isolated tasks. The authors propose a pipeline-centric approach that connects data curation, model training, and experimental execution to optimize outcomes in materials discovery. By aligning AI capabilities with materials science applications, the paper aims to foster the development of autonomous agents that can effectively discover new materials."
                },
                "zh": {
                    "title": "æ™ºèƒ½ææ–™å‘ç°çš„è‡ªä¸»ç³»ç»Ÿä¼˜åŒ–",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†äººå·¥æ™ºèƒ½ä¸ææ–™ç§‘å­¦çš„ç»“åˆï¼Œå¼ºè°ƒäº†åœ¨ææ–™å‘ç°è¿‡ç¨‹ä¸­éœ€è¦ç³»ç»Ÿçº§çš„ä¼˜åŒ–å’Œè‡ªä¸»ç›®æ ‡è¿½æ±‚ã€‚ä½œè€…æå‡ºäº†ä¸€ç§ç‹¬ç‰¹çš„ç®¡é“ä¸­å¿ƒè§†è§’ï¼Œæ¶µç›–äº†ä»æ•°æ®æ•´ç†åˆ°å®éªŒå¹³å°çš„æ•´ä¸ªè¿‡ç¨‹ï¼Œæ—¨åœ¨ä¼˜åŒ–å®é™…çš„å‘ç°ç»“æœã€‚ä¸ä»¥å¾€çš„ç ”ç©¶ä¸åŒï¼Œè¿™ç¯‡è®ºæ–‡å°†æ•´ä¸ªè¿‡ç¨‹è§†ä¸ºä¸€ä¸ªç«¯åˆ°ç«¯çš„ç³»ç»Ÿï¼Œè€Œä¸æ˜¯å­¤ç«‹çš„ä»»åŠ¡æ¨¡å‹ã€‚æœ€åï¼Œè®ºæ–‡è¿˜åˆ†æäº†ä¸»åŠ¨è®¾è®¡ä¸è¢«åŠ¨ååº”æ–¹æ³•çš„å¯¹æ¯”ï¼Œæ¨åŠ¨äº†è‡ªä¸»ã€æ™ºèƒ½çš„ææ–™å‘ç°ä»£ç†ç³»ç»Ÿçš„å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.08990",
            "title": "InternAgent-1.5: A Unified Agentic Framework for Long-Horizon Autonomous Scientific Discovery",
            "url": "https://huggingface.co/papers/2602.08990",
            "abstract": "InternAgent-1.5 is a unified system for autonomous scientific discovery that integrates computational modeling and experimental research through coordinated subsystems for generation, verification, and evolution.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce InternAgent-1.5, a unified system designed for end-to-end scientific discovery across computational and empirical domains. The system is built on a structured architecture composed of three coordinated subsystems for generation, verification, and evolution. These subsystems are supported by foundational capabilities for deep research, solution optimization, and long horizon memory. The architecture allows InternAgent-1.5 to operate continuously across extended discovery cycles while maintaining coherent and improving behavior. It also enables the system to coordinate computational modeling and laboratory experimentation within a single unified system. We evaluate InternAgent-1.5 on scientific reasoning benchmarks such as GAIA, HLE, GPQA, and FrontierScience, and the system achieves leading performance that demonstrates strong foundational capabilities. Beyond these benchmarks, we further assess two categories of discovery tasks. In algorithm discovery tasks, InternAgent-1.5 autonomously designs competitive methods for core machine learning problems. In empirical discovery tasks, it executes complete computational or wet lab experiments and produces scientific findings in earth, life, biological, and physical domains. Overall, these results show that InternAgent-1.5 provides a general and scalable framework for autonomous scientific discovery.",
            "score": 19,
            "issue_id": 981,
            "pub_date": "2026-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "838f93e075c62ada",
            "authors": [
                "Shiyang Feng",
                "Runmin Ma",
                "Xiangchao Yan",
                "Yue Fan",
                "Yusong Hu",
                "Songtao Huang",
                "Shuaiyu Zhang",
                "Zongsheng Cao",
                "Tianshuo Peng",
                "Jiakang Yuan",
                "Zijie Guo",
                "Zhijie Zhong",
                "Shangheng Du",
                "Weida Wang",
                "Jinxin Shi",
                "Yuhao Zhou",
                "Xiaohan He",
                "Zhiyin Yu",
                "Fangchen Yu",
                "Qihao Zheng",
                "Jiamin Wu",
                "Mianxin Liu",
                "Chi Zhang",
                "Shaowei Hou",
                "Shuya Li",
                "Yankai Jiang",
                "Wenjie Lou",
                "Lilong Wang",
                "Zifu Wang",
                "Jiong Wang",
                "Wanghan Xu",
                "Yue Deng",
                "Dongrui Liu",
                "Yiheng Wang",
                "Wenlong Zhang",
                "Fenghua Ling",
                "Shufei Zhang",
                "Xiaosong Wang",
                "Shuangjia Zheng",
                "Xun Huang",
                "Siqi Sun",
                "Shuyue Hu",
                "Peng Ye",
                "Chunfeng Song",
                "Bin Wang",
                "Conghui He",
                "Yihao Liu",
                "Xin Li",
                "Qibin Hou",
                "Tao Chen",
                "Xiangyu Yue",
                "Bin Wang",
                "Liang He",
                "Dahua Lin",
                "Bowen Zhou",
                "Bo Zhang",
                "Lei Bai"
            ],
            "affiliations": [
                "Shanghai Artificial Intelligence Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.08990.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#reasoning",
                    "#benchmark",
                    "#open_source",
                    "#agents"
                ],
                "emoji": "ğŸ§¬",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ°Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "InternAgent-1.5 â€” ÑÑ‚Ğ¾ ĞµĞ´Ğ¸Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ñ‚Ñ€Ğ¸ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹: Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞµĞ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾ Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğµ Ñ†Ğ¸ĞºĞ»Ñ‹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ. InternAgent-1.5 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹Ğ´Ğ°ÑÑ‰Ğ¸ĞµÑÑ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ (GAIA, HLE, GPQA, FrontierScience) Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ğ»Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ‚Ğ¾Ñ€Ğ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹, Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ğ¾Ñ‚ Ğ·ĞµĞ¼Ğ½Ñ‹Ñ… Ğ´Ğ¾ Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ½Ğ°ÑƒĞº."
                },
                "en": {
                    "title": "Empowering Autonomous Scientific Discovery with InternAgent-1.5",
                    "desc": "InternAgent-1.5 is a comprehensive system that facilitates autonomous scientific discovery by integrating computational modeling with experimental research. It consists of three main subsystems: generation, verification, and evolution, which work together to enhance the discovery process. The system is capable of performing both algorithm discovery and empirical experiments, producing significant scientific insights across various domains. Evaluated on multiple benchmarks, InternAgent-1.5 demonstrates superior performance and offers a scalable framework for ongoing scientific exploration."
                },
                "zh": {
                    "title": "è‡ªä¸»ç§‘å­¦å‘ç°çš„ç»Ÿä¸€æ¡†æ¶",
                    "desc": "InternAgent-1.5 æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„ç³»ç»Ÿï¼Œæ—¨åœ¨é€šè¿‡åè°ƒçš„å­ç³»ç»Ÿå®ç°è‡ªä¸»ç§‘å­¦å‘ç°ï¼Œæ•´åˆè®¡ç®—å»ºæ¨¡å’Œå®éªŒç ”ç©¶ã€‚è¯¥ç³»ç»Ÿç”±ä¸‰ä¸ªåè°ƒçš„å­ç³»ç»Ÿç»„æˆï¼Œåˆ†åˆ«ç”¨äºç”Ÿæˆã€éªŒè¯å’Œæ¼”åŒ–ï¼Œæ”¯æŒæ·±åº¦ç ”ç©¶ã€è§£å†³æ–¹æ¡ˆä¼˜åŒ–å’Œé•¿æœŸè®°å¿†ç­‰åŸºç¡€èƒ½åŠ›ã€‚InternAgent-1.5 èƒ½å¤Ÿåœ¨å»¶ç»­çš„å‘ç°å‘¨æœŸä¸­æŒç»­è¿è¡Œï¼ŒåŒæ—¶ä¿æŒä¸€è‡´æ€§å’Œæ”¹è¿›è¡Œä¸ºã€‚é€šè¿‡åœ¨ç§‘å­¦æ¨ç†åŸºå‡†ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œè¯¥ç³»ç»Ÿåœ¨ç®—æ³•å‘ç°å’Œå®è¯å‘ç°ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå±•ç¤ºäº†å…¶å¼ºå¤§çš„åŸºç¡€èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.08543",
            "title": "GISA: A Benchmark for General Information-Seeking Assistant",
            "url": "https://huggingface.co/papers/2602.08543",
            "abstract": "A new benchmark called GISA is introduced for evaluating information-seeking assistants, featuring human-crafted queries with structured answer formats and live updates to prevent memorization.  \t\t\t\t\tAI-generated summary \t\t\t\t The advancement of large language models (LLMs) has significantly accelerated the development of search agents capable of autonomously gathering information through multi-turn web interactions. Various benchmarks have been proposed to evaluate such agents. However, existing benchmarks often construct queries backward from answers, producing unnatural tasks misaligned with real-world needs. Moreover, these benchmarks tend to focus on either locating specific information or aggregating information from multiple sources, while relying on static answer sets prone to data contamination. To bridge these gaps, we introduce GISA, a benchmark for General Information-Seeking Assistants comprising 373 human-crafted queries that reflect authentic information-seeking scenarios. GISA features four structured answer formats (item, set, list, and table), enabling deterministic evaluation. It integrates both deep reasoning and broad information aggregation within unified tasks, and includes a live subset with periodically updated answers to resist memorization. Notably, GISA provides complete human search trajectories for every query, offering gold-standard references for process-level supervision and imitation learning. Experiments on mainstream LLMs and commercial search products reveal that even the best-performing model achieves only 19.30\\% exact match score, with performance notably degrading on tasks requiring complex planning and comprehensive information gathering. These findings highlight substantial room for future improvement.",
            "score": 18,
            "issue_id": 981,
            "pub_date": "2026-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "bba14fa5a052b07d",
            "authors": [
                "Yutao Zhu",
                "Xingshuo Zhang",
                "Maosen Zhang",
                "Jiajie Jin",
                "Liancheng Zhang",
                "Xiaoshuai Song",
                "Kangzhi Zhao",
                "Wencong Zeng",
                "Ruiming Tang",
                "Han Li",
                "Ji-Rong Wen",
                "Zhicheng Dou"
            ],
            "affiliations": [
                "Kuaishou Technology",
                "Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.08543.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#dataset",
                    "#survey",
                    "#agents"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "GISA: Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¶Ğ¸Ğ²Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº GISA Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾-Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ñ… Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ¾Ğ², ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 373 ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°, ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸, Ğ° Ğ½Ğµ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸Ğ· Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ° Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ğº Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² Ğ² ĞµĞ´Ğ¸Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ”Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ½ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğµ Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ Ñ Ğ¶Ğ¸Ğ²Ñ‹Ğ¼Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼Ğ¸. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°."
                },
                "en": {
                    "title": "GISA: A New Benchmark for Real-World Information-Seeking Evaluation",
                    "desc": "The paper introduces GISA, a new benchmark designed to evaluate information-seeking assistants, particularly those powered by large language models. It features 373 human-crafted queries that mimic real-world information-seeking scenarios and includes structured answer formats like lists and tables for clear evaluation. GISA also incorporates live updates to answers, which helps prevent models from simply memorizing responses. The results show that even top-performing models struggle with complex tasks, indicating significant opportunities for enhancement in this area."
                },
                "zh": {
                    "title": "GISAï¼šè¯„ä¼°ä¿¡æ¯æ£€ç´¢åŠ©æ‰‹çš„æ–°åŸºå‡†",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„åŸºå‡†æµ‹è¯•GISAï¼Œç”¨äºè¯„ä¼°ä¿¡æ¯æ£€ç´¢åŠ©æ‰‹ã€‚GISAåŒ…å«373ä¸ªç”±äººç±»è®¾è®¡çš„æŸ¥è¯¢ï¼Œåæ˜ çœŸå®çš„ä¿¡æ¯æ£€ç´¢åœºæ™¯ï¼Œå¹¶æä¾›å››ç§ç»“æ„åŒ–ç­”æ¡ˆæ ¼å¼ã€‚è¯¥åŸºå‡†æµ‹è¯•ç»“åˆäº†æ·±åº¦æ¨ç†å’Œå¹¿æ³›çš„ä¿¡æ¯èšåˆï¼Œä¸”åŒ…å«å®æ—¶æ›´æ–°çš„ç­”æ¡ˆï¼Œä»¥é˜²æ­¢è®°å¿†åŒ–ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯è¡¨ç°æœ€å¥½çš„æ¨¡å‹ï¼Œå…¶å‡†ç¡®åŒ¹é…ç‡ä¹Ÿä»…ä¸º19.30%ï¼Œè¡¨æ˜åœ¨å¤æ‚è§„åˆ’å’Œä¿¡æ¯æ”¶é›†ä»»åŠ¡ä¸Šä»æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.09022",
            "title": "WorldCompass: Reinforcement Learning for Long-Horizon World Models",
            "url": "https://huggingface.co/papers/2602.09022",
            "abstract": "WorldCompass enhances long-horizon video-based world models through reinforcement learning post-training with clip-level rollouts, complementary rewards, and efficient RL algorithms.  \t\t\t\t\tAI-generated summary \t\t\t\t This work presents WorldCompass, a novel Reinforcement Learning (RL) post-training framework for the long-horizon, interactive video-based world models, enabling them to explore the world more accurately and consistently based on interaction signals. To effectively \"steer\" the world model's exploration, we introduce three core innovations tailored to the autoregressive video generation paradigm: 1) Clip-level rollout Strategy: We generate and evaluate multiple samples at a single target clip, which significantly boosts rollout efficiency and provides fine-grained reward signals. 2) Complementary Reward Functions: We design reward functions for both interaction-following accuracy and visual quality, which provide direct supervision and effectively suppress reward-hacking behaviors. 3) Efficient RL Algorithm: We employ the negative-aware fine-tuning strategy coupled with various efficiency optimizations to efficiently and effectively enhance model capacity. Evaluations on the SoTA open-source world model, WorldPlay, demonstrate that WorldCompass significantly improves interaction accuracy and visual fidelity across various scenarios.",
            "score": 17,
            "issue_id": 981,
            "pub_date": "2026-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "977be04170016363",
            "authors": [
                "Zehan Wang",
                "Tengfei Wang",
                "Haiyu Zhang",
                "Xuhui Zuo",
                "Junta Wu",
                "Haoyuan Wang",
                "Wenqiang Sun",
                "Zhenwei Wang",
                "Chenjie Cao",
                "Hengshuang Zhao",
                "Chunchao Guo",
                "Zhou Zhao"
            ],
            "affiliations": [
                "Tencent Hunyuan",
                "The University of Hong Kong",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.09022.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#video",
                    "#rl"
                ],
                "emoji": "ğŸ§­",
                "ru": {
                    "title": "ĞĞ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¸Ñ€Ğ°: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "WorldCompass Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ¸Ñ€Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¸: ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ»Ğ¸Ğ¿Ğ¾Ğ² Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ±, Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¼Ğ¸Ñ€Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ, Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ÑÑ Ğ½ĞµĞ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Enhancing Video World Models with Efficient Reinforcement Learning",
                    "desc": "WorldCompass is a new framework that improves long-horizon video-based world models using reinforcement learning (RL) after their initial training. It introduces a clip-level rollout strategy that allows the model to generate and assess multiple video samples at once, enhancing efficiency and providing detailed reward feedback. Additionally, it incorporates complementary reward functions that focus on both the accuracy of interactions and the quality of visuals, helping to prevent the model from exploiting loopholes in the reward system. Finally, the framework uses an efficient RL algorithm that optimizes the model's performance, leading to better interaction accuracy and visual quality in various scenarios."
                },
                "zh": {
                    "title": "WorldCompassï¼šæå‡è§†é¢‘ä¸–ç•Œæ¨¡å‹çš„æ¢ç´¢èƒ½åŠ›",
                    "desc": "WorldCompass æ˜¯ä¸€ç§æ–°é¢–çš„å¼ºåŒ–å­¦ä¹ åè®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨æå‡åŸºäºè§†é¢‘çš„é•¿æ—¶é—´äº¤äº’ä¸–ç•Œæ¨¡å‹çš„æ¢ç´¢èƒ½åŠ›ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥å‰ªè¾‘çº§å›æ”¾ç­–ç•¥ã€äº’è¡¥å¥–åŠ±å‡½æ•°å’Œé«˜æ•ˆçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œæ¥ä¼˜åŒ–æ¨¡å‹çš„æ¢ç´¢è¿‡ç¨‹ã€‚å‰ªè¾‘çº§å›æ”¾ç­–ç•¥å¯ä»¥åœ¨å•ä¸ªç›®æ ‡å‰ªè¾‘ä¸­ç”Ÿæˆå’Œè¯„ä¼°å¤šä¸ªæ ·æœ¬ï¼Œä»è€Œæé«˜å›æ”¾æ•ˆç‡ã€‚é€šè¿‡è¿™äº›åˆ›æ–°ï¼ŒWorldCompass æ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨ä¸åŒåœºæ™¯ä¸‹çš„äº¤äº’å‡†ç¡®æ€§å’Œè§†è§‰è´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.07055",
            "title": "Theory of Space: Can Foundation Models Construct Spatial Beliefs through Active Exploration?",
            "url": "https://huggingface.co/papers/2602.07055",
            "abstract": "Current multimodal foundation models show limitations in maintaining coherent spatial beliefs during active exploration, exhibiting gaps between active and passive performance, inefficient exploration strategies, and difficulties in updating outdated spatial knowledge.  \t\t\t\t\tAI-generated summary \t\t\t\t Spatial embodied intelligence requires agents to act to acquire information under partial observability. While multimodal foundation models excel at passive perception, their capacity for active, self-directed exploration remains understudied. We propose Theory of Space, defined as an agent's ability to actively acquire information through self-directed, active exploration and to construct, revise, and exploit a spatial belief from sequential, partial observations. We evaluate this through a benchmark where the goal is curiosity-driven exploration to build an accurate cognitive map. A key innovation is spatial belief probing, which prompts models to reveal their internal spatial representations at each step. Our evaluation of state-of-the-art models reveals several critical bottlenecks. First, we identify an Active-Passive Gap, where performance drops significantly when agents must autonomously gather information. Second, we find high inefficiency, as models explore unsystematically compared to program-based proxies. Through belief probing, we diagnose that while perception is an initial bottleneck, global beliefs suffer from instability that causes spatial knowledge to degrade over time. Finally, using a false belief paradigm, we uncover Belief Inertia, where agents fail to update obsolete priors with new evidence. This issue is present in text-based agents but is particularly severe in vision-based models. Our findings suggest that current foundation models struggle to maintain coherent, revisable spatial beliefs during active exploration.",
            "score": 17,
            "issue_id": 982,
            "pub_date": "2026-02-04",
            "pub_date_card": {
                "ru": "4 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 4",
                "zh": "2æœˆ4æ—¥"
            },
            "hash": "e9848ab21f78fd95",
            "authors": [
                "Pingyue Zhang",
                "Zihan Huang",
                "Yue Wang",
                "Jieyu Zhang",
                "Letian Xue",
                "Zihan Wang",
                "Qineng Wang",
                "Keshigeyan Chandrasegaran",
                "Ruohan Zhang",
                "Yejin Choi",
                "Ranjay Krishna",
                "Jiajun Wu",
                "Li Fei-Fei",
                "Manling Li"
            ],
            "affiliations": [
                "Cornell University",
                "Northwestern University",
                "Stanford University",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.07055.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#agents",
                    "#multimodal",
                    "#robotics"
                ],
                "emoji": "ğŸ—ºï¸",
                "ru": {
                    "title": "ĞĞ³ĞµĞ½Ñ‚Ñ‹ Ğ½Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾: Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ° ÑĞ»Ğ°Ğ±Ğ¾ÑÑ‚ĞµĞ¹ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² foundation models",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ foundation models Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ñ‹ Ğ¸ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ 'Theory of Space' â€” ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸ ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ, Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑÑ‚ÑŒ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºÑƒ 'spatial belief probing' Ğ´Ğ»Ñ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ²Ñ‹ÑĞ²Ğ¸Ğ² ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹: Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¸ Ğ¿Ğ°ÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸ĞµĞ¼, Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ¸ Ğ¸Ğ½ĞµÑ€Ñ†Ğ¸Ñ ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ğ³Ğ´Ğ° Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ½Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑÑÑ‚ ÑƒÑÑ‚Ğ°Ñ€ĞµĞ²ÑˆĞ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ current foundation models Ğ² Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ñ‹Ñ… Ğ¸ Ğ¿ĞµÑ€ĞµÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµĞ¼Ñ‹Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ¸Ñ€Ğ°."
                },
                "en": {
                    "title": "Enhancing Spatial Intelligence in Active Exploration",
                    "desc": "This paper addresses the limitations of current multimodal foundation models in maintaining coherent spatial beliefs during active exploration. It introduces the Theory of Space, which emphasizes the importance of self-directed exploration for agents to gather information and update their spatial knowledge. The study reveals significant performance drops when agents transition from passive perception to active information gathering, highlighting an Active-Passive Gap. Additionally, it identifies issues like inefficiency in exploration strategies and Belief Inertia, where agents struggle to revise outdated spatial beliefs, particularly in vision-based models."
                },
                "zh": {
                    "title": "ä¸»åŠ¨æ¢ç´¢ä¸­çš„ç©ºé—´ä¿¡å¿µæŒ‘æˆ˜",
                    "desc": "å½“å‰çš„å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹åœ¨ä¸»åŠ¨æ¢ç´¢è¿‡ç¨‹ä¸­å­˜åœ¨ä¿æŒä¸€è‡´ç©ºé—´ä¿¡å¿µçš„å±€é™æ€§ï¼Œè¡¨ç°å‡ºä¸»åŠ¨ä¸è¢«åŠ¨æ€§èƒ½ä¹‹é—´çš„å·®è·ã€‚æˆ‘ä»¬æå‡ºäº†ç©ºé—´ç†è®ºï¼Œå®šä¹‰ä¸ºä»£ç†é€šè¿‡è‡ªæˆ‘å¯¼å‘çš„ä¸»åŠ¨æ¢ç´¢æ¥ä¸»åŠ¨è·å–ä¿¡æ¯çš„èƒ½åŠ›ï¼Œå¹¶ä»åºåˆ—çš„éƒ¨åˆ†è§‚å¯Ÿä¸­æ„å»ºã€ä¿®è®¢å’Œåˆ©ç”¨ç©ºé—´ä¿¡å¿µã€‚é€šè¿‡åŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬å‘ç°æ¨¡å‹åœ¨è‡ªä¸»æ”¶é›†ä¿¡æ¯æ—¶æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œå¹¶ä¸”æ¢ç´¢ç­–ç•¥æ•ˆç‡ä½ä¸‹ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œå½“å‰çš„åŸºç¡€æ¨¡å‹åœ¨ä¸»åŠ¨æ¢ç´¢ä¸­éš¾ä»¥ç»´æŒä¸€è‡´å’Œå¯ä¿®è®¢çš„ç©ºé—´ä¿¡å¿µã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.07075",
            "title": "LatentChem: From Textual CoT to Latent Thinking in Chemical Reasoning",
            "url": "https://huggingface.co/papers/2602.07075",
            "abstract": "LatentChem enables chemical reasoning through continuous latent space computations instead of discrete textual tokens, achieving superior performance and efficiency compared to traditional chain-of-thought approaches.  \t\t\t\t\tAI-generated summary \t\t\t\t Chemical large language models (LLMs) predominantly rely on explicit Chain-of-Thought (CoT) in natural language to perform complex reasoning. However, chemical reasoning is inherently continuous and structural, and forcing it into discrete linguistic tokens introduces a fundamental representation mismatch that constrains both efficiency and performance. We introduce LatentChem, a latent reasoning interface that decouples chemical computation from textual generation, enabling models to perform multi-step reasoning directly in continuous latent space while emitting language only for final outputs. Remarkably, we observe a consistent emergent behavior: when optimized solely for task success, models spontaneously internalize reasoning, progressively abandoning verbose textual derivations in favor of implicit latent computation. This shift is not merely stylistic but computationally advantageous. Across diverse chemical reasoning benchmarks, LatentChem achieves a 59.88\\% non-tie win rate over strong CoT-based baselines on ChemCoTBench, while delivering a 10.84times average inference speedup. Our results provide empirical evidence that chemical reasoning is more naturally and effectively realized as continuous latent dynamics rather than discretized linguistic trajectories.",
            "score": 15,
            "issue_id": 980,
            "pub_date": "2026-02-06",
            "pub_date_card": {
                "ru": "6 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 6",
                "zh": "2æœˆ6æ—¥"
            },
            "hash": "8c5ebd4aa9539a12",
            "authors": [
                "Xinwu Ye",
                "Yicheng Mao",
                "Jia Zhang",
                "Yimeng Liu",
                "Li Hao",
                "Fang Wu",
                "Zhiwei Li",
                "Yuxuan Liao",
                "Zehong Wang",
                "Zhiyuan Liu",
                "Zhenfei Yin",
                "Li Yuan",
                "Philip Torr",
                "Huan Sun",
                "Xiangxiang Zeng",
                "Mengdi Wang",
                "Le Cong",
                "Shenghua Gao",
                "Xiangru Tang"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2602.07075.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#reasoning"
                ],
                "emoji": "âš—ï¸",
                "ru": {
                    "title": "Ğ¥Ğ¸Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ğ¼Ñ‹ÑĞ»Ğ¸",
                    "desc": "LatentChem Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ…Ğ¸Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ² ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ…Ğ¸Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ LLM Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒÑÑ‚ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğµ Ñ…Ğ¸Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ Ğ² Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ¿Ğ¾Ñ‚ĞµÑ€Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ, Ğ²Ñ‹Ğ´Ğ°Ğ²Ğ°Ñ Ñ‚ĞµĞºÑÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ»Ñ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ 59.88% Ğ¿Ğ¾Ğ±ĞµĞ´Ğ½Ñ‹Ñ… ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ°Ğ´ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ğ¼Ñ‹ÑĞ»Ğ¸. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ 10.84-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ, Ñ‡Ñ‚Ğ¾ Ñ…Ğ¸Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½ĞµĞµ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·ÑƒĞµÑ‚ÑÑ ĞºĞ°Ğº Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ°Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ° Ğ² ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ."
                },
                "en": {
                    "title": "Revolutionizing Chemical Reasoning with Continuous Latent Dynamics",
                    "desc": "LatentChem is a novel approach that enhances chemical reasoning by utilizing continuous latent space computations instead of relying on discrete textual tokens. Traditional methods, which use Chain-of-Thought (CoT) reasoning, often struggle with the inherent continuous nature of chemical data, leading to inefficiencies. By allowing models to perform multi-step reasoning directly in latent space, LatentChem improves both performance and speed, achieving a significant win rate over existing CoT-based methods. This research demonstrates that chemical reasoning benefits from a continuous representation, resulting in faster and more effective computations."
                },
                "zh": {
                    "title": "åŒ–å­¦æ¨ç†çš„æ–°æ–¹æ³•ï¼šæ½œåœ¨ç©ºé—´è®¡ç®—",
                    "desc": "LatentChem æ˜¯ä¸€ç§æ–°çš„åŒ–å­¦æ¨ç†æ–¹æ³•ï¼Œå®ƒé€šè¿‡è¿ç»­çš„æ½œåœ¨ç©ºé—´è®¡ç®—æ¥å®ç°ï¼Œè€Œä¸æ˜¯ä¾èµ–äºç¦»æ•£çš„æ–‡æœ¬ç¬¦å·ã€‚è¿™ç§æ–¹æ³•è§£å†³äº†ä¼ ç»Ÿé“¾å¼æ€ç»´åœ¨åŒ–å­¦æ¨ç†ä¸­å­˜åœ¨çš„è¡¨ç¤ºä¸åŒ¹é…é—®é¢˜ï¼Œä»è€Œæé«˜äº†æ•ˆç‡å’Œæ€§èƒ½ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå½“æ¨¡å‹ä»…ä¼˜åŒ–ä»»åŠ¡æˆåŠŸæ—¶ï¼Œå®ƒä»¬ä¼šè‡ªå‘åœ°å†…åŒ–æ¨ç†ï¼Œé€æ¸æ”¾å¼ƒå†—é•¿çš„æ–‡æœ¬æ¨å¯¼ï¼Œè½¬è€Œä½¿ç”¨éšå¼çš„æ½œåœ¨è®¡ç®—ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLatentChem åœ¨åŒ–å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå…·æœ‰æ˜¾è‘—çš„æ¨ç†é€Ÿåº¦æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.03784",
            "title": "Context Compression via Explicit Information Transmission",
            "url": "https://huggingface.co/papers/2602.03784",
            "abstract": "ComprExIT introduces a novel approach to long-context inference in LLMs by using explicit information transmission over frozen hidden states, improving compression efficiency through depth-wise and width-wise transmission mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t Long-context inference with Large Language Models (LLMs) is costly due to quadratic attention and growing key-value caches, motivating context compression. In this work, we study soft context compression, where a long context is condensed into a small set of continuous representations. Existing methods typically re-purpose the LLM itself as a trainable compressor, relying on layer-by-layer self-attention to iteratively aggregate information. We argue that this paradigm suffers from two structural limitations: (i) progressive representation overwriting across layers (ii) uncoordinated allocation of compression capacity across tokens. We propose ComprExIT (Context Compression via Explicit Information Transmission), a lightweight framework that formulates soft compression into a new paradigm: explicit information transmission over frozen LLM hidden states. This decouples compression from the model's internal self-attention dynamics. ComprExIT performs (i) depth-wise transmission to selectively transmit multi-layer information into token anchors, mitigating progressive overwriting, and (ii) width-wise transmission to aggregate anchors into a small number of slots via a globally optimized transmission plan, ensuring coordinated allocation of information. Across six question-answering benchmarks, ComprExIT consistently outperforms state-of-the-art context compression methods while introducing only ~1% additional parameters, demonstrating that explicit and coordinated information transmission enables more effective and robust long-context compression.",
            "score": 13,
            "issue_id": 988,
            "pub_date": "2026-02-03",
            "pub_date_card": {
                "ru": "3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 3",
                "zh": "2æœˆ3æ—¥"
            },
            "hash": "d4567a9e2ddd4d92",
            "authors": [
                "Jiangnan Ye",
                "Hanqi Yan",
                "Zhenyi Shen",
                "Heng Chang",
                "Ye Mao",
                "Yulan He"
            ],
            "affiliations": [
                "Imperial College London, UK",
                "Kings College London, UK",
                "The Alan Turing Institute, UK",
                "Tsinghua University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.03784.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#long_context",
                    "#inference",
                    "#benchmark"
                ],
                "emoji": "ğŸ“¦",
                "ru": {
                    "title": "Ğ¯Ğ²Ğ½Ğ°Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°",
                    "desc": "ComprExIT Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¶Ğ°Ñ‚Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ ÑĞ²Ğ½ÑƒÑ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ñƒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğµ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑ‚ÑƒÑ‰ĞµĞ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° ĞºÑÑˆĞµĞ¹ ĞºĞ»ÑÑ‡-Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ ÑĞ°Ğ¼Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ĞºĞ°Ğº Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¾Ñ€, ComprExIT Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²ÑƒÑ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ñƒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸: Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ½ÑƒÑ (Ñ‡ĞµÑ€ĞµĞ· ÑĞ»Ğ¾Ğ¸) Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ·Ğ°Ğ¿Ğ¸ÑĞ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑˆĞ¸Ñ€Ğ¸Ğ½Ğ½ÑƒÑ (Ğ¿Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼) Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ½Ğ° ÑˆĞµÑÑ‚Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ñ€Ğ¸ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 1% Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "Efficient Long-Context Inference with ComprExIT",
                    "desc": "ComprExIT presents a new method for improving long-context inference in Large Language Models (LLMs) by using explicit information transmission over fixed hidden states. This approach enhances compression efficiency through two mechanisms: depth-wise transmission, which prevents overwriting of information across layers, and width-wise transmission, which optimally aggregates information into fewer slots. Unlike traditional methods that rely on the model's self-attention, ComprExIT decouples compression from these dynamics, allowing for better coordination of information allocation. The results show that ComprExIT outperforms existing context compression techniques while adding minimal parameters, making it a promising solution for efficient long-context processing."
                },
                "zh": {
                    "title": "æ˜¾å¼ä¿¡æ¯ä¼ è¾“ï¼Œæå‡é•¿ä¸Šä¸‹æ–‡æ¨ç†æ•ˆç‡",
                    "desc": "ComprExITæå‡ºäº†ä¸€ç§æ–°é¢–çš„é•¿ä¸Šä¸‹æ–‡æ¨ç†æ–¹æ³•ï¼Œé€šè¿‡åœ¨å†»ç»“çš„éšè—çŠ¶æ€ä¸Šè¿›è¡Œæ˜¾å¼ä¿¡æ¯ä¼ è¾“ï¼Œæé«˜äº†å‹ç¼©æ•ˆç‡ã€‚è¯¥æ–¹æ³•é‡‡ç”¨æ·±åº¦å’Œå®½åº¦ä¼ è¾“æœºåˆ¶ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨å±‚é—´ä¿¡æ¯èšåˆæ—¶çš„ç»“æ„æ€§é™åˆ¶ã€‚é€šè¿‡é€‰æ‹©æ€§åœ°å°†å¤šå±‚ä¿¡æ¯ä¼ è¾“åˆ°æ ‡è®°é”šç‚¹ï¼ŒComprExITå‡è½»äº†é€å±‚è¦†ç›–çš„é—®é¢˜ï¼Œå¹¶é€šè¿‡å…¨å±€ä¼˜åŒ–çš„ä¼ è¾“è®¡åˆ’å°†é”šç‚¹èšåˆåˆ°å°‘é‡æ§½ä¸­ï¼Œç¡®ä¿ä¿¡æ¯çš„åè°ƒåˆ†é…ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒComprExITåœ¨å…­ä¸ªé—®ç­”åŸºå‡†æµ‹è¯•ä¸­å§‹ç»ˆä¼˜äºæœ€å…ˆè¿›çš„ä¸Šä¸‹æ–‡å‹ç¼©æ–¹æ³•ï¼ŒåŒæ—¶ä»…å¢åŠ çº¦1%çš„å‚æ•°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.08658",
            "title": "Fundamental Reasoning Paradigms Induce Out-of-Domain Generalization in Language Models",
            "url": "https://huggingface.co/papers/2602.08658",
            "abstract": "Research investigates how fundamental reasoning paradigms influence large language model generalization through targeted training approaches and evaluation on real-world tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Deduction, induction, and abduction are fundamental reasoning paradigms, core for human logical thinking. Although improving Large Language Model (LLM) reasoning has attracted significant research efforts, the extent to which the fundamental paradigms induce generalization has yet to be systematically explored. In this study, we shed light on how the interplay between these core paradigms influences LLMs' reasoning behavior. To this end, we first collect a new dataset of reasoning trajectories from symbolic tasks, each targeting one of the three fundamental paradigms, to abstract from concrete world knowledge. Then, we investigate effective ways for inducing these skills into LLMs. We experiment with a battery of methods including simple fine-tuning, and more complex approaches to increase model depth, or transform a dense model to a mixture-of-experts. We comprehensively evaluate induced models on realistic out-of-domain tasks, that are entirely formulated in natural language and contain real-world knowledge. Our results reveal that our approach yields strong generalizability with substantial performance gains (up to 14.60) across realistic tasks.",
            "score": 12,
            "issue_id": 987,
            "pub_date": "2026-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "afe3437380b2e1d1",
            "authors": [
                "Mingzi Cao",
                "Xingwei Tan",
                "Mahmud Akhter",
                "Marco Valentino",
                "Maria Liakata",
                "Xi Wang",
                "Nikolaos Aletras"
            ],
            "affiliations": [
                "School of Computer Science, University of Sheffield",
                "School of Electronic Engineering and Computer Science, Queen Mary University of London",
                "The Alan Turing Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.08658.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#training",
                    "#reasoning",
                    "#synthetic",
                    "#dataset"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¢Ñ€Ğ¸ Ñ‚Ğ¸Ğ¿Ğ° Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¸ â€” Ğ¾Ğ´Ğ¸Ğ½ ĞºĞ»ÑÑ‡ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ LLM",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¼ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚, ĞºĞ°Ğº Ñ‚Ñ€Ğ¸ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ° Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ â€” Ğ´ĞµĞ´ÑƒĞºÑ†Ğ¸Ñ, Ğ¸Ğ½Ğ´ÑƒĞºÑ†Ğ¸Ñ Ğ¸ Ğ°Ğ±Ğ´ÑƒĞºÑ†Ğ¸Ñ â€” Ğ²Ğ»Ğ¸ÑÑÑ‚ Ğ½Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ fine-tuning Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¸Ğ¿Ğ° mixture-of-experts. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ñ‹Ğ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾ Ğ¾Ñ†ĞµĞ½ĞµĞ½Ñ‹ Ğ½Ğ° Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ½Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¾ 14.60 Ğ¿ÑƒĞ½ĞºÑ‚Ğ° Ğ½Ğ° Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "Enhancing LLMs: The Power of Reasoning Paradigms",
                    "desc": "This paper explores how different reasoning methods, like deduction, induction, and abduction, affect the ability of large language models (LLMs) to generalize their knowledge. The authors create a new dataset that focuses on these reasoning methods through symbolic tasks, allowing them to study how well LLMs can learn these skills. They test various training techniques, including fine-tuning and modifying model architectures, to enhance the reasoning capabilities of LLMs. The findings show that their approach significantly improves the performance of LLMs on real-world tasks, demonstrating better generalization abilities."
                },
                "zh": {
                    "title": "æ¨ç†èŒƒå¼æå‡è¯­è¨€æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†åŸºæœ¬æ¨ç†èŒƒå¼å¦‚ä½•é€šè¿‡é’ˆå¯¹æ€§çš„è®­ç»ƒæ–¹æ³•å½±å“å¤§å‹è¯­è¨€æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚æ¨ç†çš„åŸºæœ¬èŒƒå¼åŒ…æ‹¬æ¼”ç»ã€å½’çº³å’Œæº¯å› ï¼Œè¿™äº›æ˜¯äººç±»é€»è¾‘æ€ç»´çš„æ ¸å¿ƒã€‚æˆ‘ä»¬æ”¶é›†äº†ä¸€ä¸ªæ–°çš„æ¨ç†è½¨è¿¹æ•°æ®é›†ï¼Œé’ˆå¯¹è¿™ä¸‰ç§åŸºæœ¬èŒƒå¼è¿›è¡Œå®éªŒï¼Œä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç°å®ä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œæ€§èƒ½æå‡å¯è¾¾14.60ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.06540",
            "title": "AgentCPM-Report: Interleaving Drafting and Deepening for Open-Ended Deep Research",
            "url": "https://huggingface.co/papers/2602.06540",
            "abstract": "AgentCPM-Report presents a lightweight local solution for deep research report generation using a Writing As Reasoning Policy framework and multi-stage agentic training to enhance small models' reasoning and outline evolution capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating deep research reports requires large-scale information acquisition and the synthesis of insight-driven analysis, posing a significant challenge for current language models. Most existing approaches follow a plan-then-write paradigm, whose performance heavily depends on the quality of the initial outline. However, constructing a comprehensive outline itself demands strong reasoning ability, causing current deep research systems to rely almost exclusively on closed-source or online large models. This reliance raises practical barriers to deployment and introduces safety and privacy concerns for user-authored data. In this work, we present AgentCPM-Report, a lightweight yet high-performing local solution composed of a framework that mirrors the human writing process and an 8B-parameter deep research agent. Our framework uses a Writing As Reasoning Policy (WARP), which enables models to dynamically revise outlines during report generation. Under this policy, the agent alternates between Evidence-Based Drafting and Reasoning-Driven Deepening, jointly supporting information acquisition, knowledge refinement, and iterative outline evolution. To effectively equip small models with this capability, we introduce a Multi-Stage Agentic Training strategy, consisting of cold-start, atomic skill RL, and holistic pipeline RL. Experiments on DeepResearch Bench, DeepConsult, and DeepResearch Gym demonstrate that AgentCPM-Report outperforms leading closed-source systems, with substantial gains in Insight.",
            "score": 12,
            "issue_id": 980,
            "pub_date": "2026-02-06",
            "pub_date_card": {
                "ru": "6 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 6",
                "zh": "2æœˆ6æ—¥"
            },
            "hash": "99c9091242c48388",
            "authors": [
                "Yishan Li",
                "Wentong Chen",
                "Yukun Yan",
                "Mingwei Li",
                "Sen Mei",
                "Xiaorong Wang",
                "Kunpeng Liu",
                "Xin Cong",
                "Shuo Wang",
                "Zhong Zhang",
                "Yaxi Lu",
                "Zhenghao Liu",
                "Yankai Lin",
                "Zhiyuan Liu",
                "Maosong Sun"
            ],
            "affiliations": [
                "OpenBMB"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.06540.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#small_models",
                    "#open_source",
                    "#rl",
                    "#agents",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "ğŸ“",
                "ru": {
                    "title": "Ğ Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¿Ğ¸ÑÑŒĞ¼Ğ°: Ğ¼Ğ°Ğ»Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°",
                    "desc": "AgentCPM-Report Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ»Ñ‘Ğ³ĞºĞ¾Ğµ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Writing As Reasoning Policy Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ€ĞµĞ²Ğ¸Ğ·Ğ¸Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ° Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ 8-Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ñ‡ĞµÑ€ĞµĞ´ÑƒĞµÑ‚ ÑÑ‚Ğ°Ğ¿Ñ‹ ÑĞ±Ğ¾Ñ€Ğ° Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ² Ğ¸ ÑƒĞ³Ğ»ÑƒĞ±Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¸ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ğ°. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ RL, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ°Ñ Ñ…Ğ¾Ğ»Ğ¾Ğ´Ğ½Ñ‹Ğ¹ ÑÑ‚Ğ°Ñ€Ñ‚, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ½Ğ°Ğ²Ñ‹ĞºĞ°Ğ¼ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸Ğ½ÑĞ°Ğ¹Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Empowering Small Models for Insightful Research Reports",
                    "desc": "AgentCPM-Report introduces a novel approach for generating deep research reports using a lightweight local model that enhances reasoning and outline development. It employs a Writing As Reasoning Policy (WARP) that allows the model to iteratively refine its outline while drafting the report. This method addresses the limitations of traditional plan-then-write strategies by integrating evidence-based drafting with reasoning-driven enhancements. The Multi-Stage Agentic Training strategy further empowers smaller models to achieve performance levels comparable to larger, closed-source systems, while ensuring user data privacy and safety."
                },
                "zh": {
                    "title": "è½»é‡çº§æœ¬åœ°è§£å†³æ–¹æ¡ˆï¼Œæå‡æ·±åº¦ç ”ç©¶æŠ¥å‘Šç”Ÿæˆèƒ½åŠ›",
                    "desc": "AgentCPM-Report æ˜¯ä¸€ç§è½»é‡çº§çš„æœ¬åœ°è§£å†³æ–¹æ¡ˆï¼Œæ—¨åœ¨é€šè¿‡å†™ä½œä½œä¸ºæ¨ç†æ”¿ç­–æ¡†æ¶å’Œå¤šé˜¶æ®µä»£ç†è®­ç»ƒæ¥ç”Ÿæˆæ·±åº¦ç ”ç©¶æŠ¥å‘Šã€‚è¯¥æ–¹æ³•è§£å†³äº†ç°æœ‰è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆæ·±åº¦ç ”ç©¶æŠ¥å‘Šæ—¶é¢ä¸´çš„ä¿¡æ¯è·å–å’Œåˆ†æåˆæˆçš„æŒ‘æˆ˜ã€‚é€šè¿‡åŠ¨æ€ä¿®è®¢å¤§çº²ï¼ŒAgentCPM-Report æé«˜äº†å°æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œå¤§çº²æ¼”å˜èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†é¢†å…ˆçš„é—­æºç³»ç»Ÿï¼Œæ˜¾è‘—æå‡äº†æ´å¯ŸåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.06454",
            "title": "RelayGen: Intra-Generation Model Switching for Efficient Reasoning",
            "url": "https://huggingface.co/papers/2602.06454",
            "abstract": "RelayGen is a training-free framework that dynamically switches between large and small models during reasoning by identifying difficulty transitions at the segment level, achieving faster inference with minimal accuracy loss.  \t\t\t\t\tAI-generated summary \t\t\t\t Large reasoning models (LRMs) achieve strong performance on complex reasoning tasks by generating long, multi-step reasoning trajectories, but inference-time scaling incurs substantial deployment cost. A key challenge is that generation difficulty varies within a single output, whereas existing efficiency-oriented approaches either ignore this intra-generation variation or rely on supervised token-level routing with high system complexity. We present RelayGen, a training-free, segment-level runtime model switching framework that exploits difficulty variation in long-form reasoning. Through offline analysis of generation uncertainty using token probability margins, we show that coarse-grained segment-level control is sufficient to capture difficulty transitions within a reasoning trajectory. RelayGen identifies model-specific switch cues that signal transitions to lower-difficulty segments and dynamically delegates their continuation to a smaller model, while preserving high-difficulty reasoning on the large model. Across multiple reasoning benchmarks, RelayGen substantially reduces inference latency while preserving most of the accuracy of large models. When combined with speculative decoding, RelayGen achieves up to 2.2times end-to-end speedup with less than 2\\% accuracy degradation, without requiring additional training or learned routing components.",
            "score": 11,
            "issue_id": 980,
            "pub_date": "2026-02-06",
            "pub_date_card": {
                "ru": "6 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 6",
                "zh": "2æœˆ6æ—¥"
            },
            "hash": "5323131100597ece",
            "authors": [
                "Jiwon Song",
                "Yoongon Kim",
                "Jae-Joon Kim"
            ],
            "affiliations": [
                "Seoul National University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.06454.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#training",
                    "#small_models",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹",
                    "desc": "RelayGen â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¹ Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğµ Ğ¼Ğ°Ñ€Ğ¶Ğ¸Ğ½Ñ‹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ° Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° (Ğ´Ğ¾ 2.2 Ñ€Ğ°Ğ·) Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒÑ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚-ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğµ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ¾ ÑĞ¿ĞµĞºÑƒĞ»ÑÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼. RelayGen Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ³Ñ€ÑƒĞ±Ğ¾Ğ·ĞµÑ€Ğ½Ğ¸ÑÑ‚Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑƒÑ‚Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Dynamic Model Switching for Efficient Reasoning",
                    "desc": "RelayGen is a novel framework designed to enhance the efficiency of large reasoning models (LRMs) by dynamically switching between large and small models based on the difficulty of reasoning tasks. It identifies transitions in difficulty at the segment level, allowing for faster inference without significant loss in accuracy. Unlike traditional methods that either overlook difficulty variations or require complex supervised routing, RelayGen operates without additional training, making it simpler and more effective. By leveraging token probability margins to analyze generation uncertainty, RelayGen achieves substantial speed improvements while maintaining high performance across various reasoning benchmarks."
                },
                "zh": {
                    "title": "åŠ¨æ€åˆ‡æ¢æ¨¡å‹ï¼Œå®ç°é«˜æ•ˆæ¨ç†",
                    "desc": "RelayGen æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„æ¡†æ¶ï¼Œé€šè¿‡åœ¨æ¨ç†è¿‡ç¨‹ä¸­åŠ¨æ€åˆ‡æ¢å¤§æ¨¡å‹å’Œå°æ¨¡å‹ï¼Œæ¥è¯†åˆ«æ®µçº§åˆ«çš„éš¾åº¦å˜åŒ–ï¼Œä»è€Œå®ç°æ›´å¿«çš„æ¨ç†é€Ÿåº¦ä¸”å‡ ä¹ä¸æŸå¤±å‡†ç¡®æ€§ã€‚å¤§å‹æ¨ç†æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†æ¨ç†æ—¶çš„æ‰©å±•ä¼šå¸¦æ¥é«˜æ˜‚çš„éƒ¨ç½²æˆæœ¬ã€‚RelayGen é€šè¿‡ç¦»çº¿åˆ†æç”Ÿæˆä¸ç¡®å®šæ€§ï¼Œåˆ©ç”¨ç²—ç²’åº¦çš„æ®µçº§æ§åˆ¶æ¥æ•æ‰æ¨ç†è½¨è¿¹ä¸­çš„éš¾åº¦å˜åŒ–ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿè¯†åˆ«æ¨¡å‹ç‰¹å®šçš„åˆ‡æ¢ä¿¡å·ï¼Œå°†ä½éš¾åº¦æ®µçš„æ¨ç†äº¤ç»™å°æ¨¡å‹å¤„ç†ï¼ŒåŒæ—¶ä¿æŒé«˜éš¾åº¦æ¨ç†åœ¨å¤§æ¨¡å‹ä¸Šè¿›è¡Œã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.08808",
            "title": "How2Everything: Mining the Web for How-To Procedures to Evaluate and Improve LLMs",
            "url": "https://huggingface.co/papers/2602.08808",
            "abstract": "A scalable framework for evaluating and improving goal-conditioned procedure generation using large-scale web mining, automated scoring, and reinforcement learning to enhance step-by-step instruction quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating step-by-step \"how-to\" procedures is a key LLM capability: how-to advice is commonly requested in chatbots, and step-by-step planning is critical for reasoning over complex tasks. Yet, measuring and improving procedural validity at scale on real-world tasks remains challenging and understudied. To address this, we introduce How2Everything, a scalable framework to evaluate and improve goal-conditioned procedure generation. Our framework includes How2Mine, which mines 351K procedures from 980K web pages across 14 topics and readily scales to larger corpora. From this pool we build How2Bench, a 7K-example evaluation set balanced across topics. To reliably score model outputs, we develop How2Score, an evaluation protocol that uses an LLM judge to detect whether a generation contains any critical failure that would prevent achieving the goal. For low-cost, reproducible evaluation, we distill a frontier model into an open 8B model, achieving 80.5% agreement with human annotators. How2Bench reveals clear scaling trends across model sizes and training stages, providing signal early in pretraining. Finally, RL using How2Score as a reward improves performance on How2Bench by >10 points across three models without systematic regressions on standard benchmarks, with gains robust to superficial source-document memorization or format compliance. Taken together, How2Everything shows how pretraining web data can support a closed loop of capability evaluation and improvement at scale.",
            "score": 6,
            "issue_id": 981,
            "pub_date": "2026-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "7cb6296b3849734d",
            "authors": [
                "Yapei Chang",
                "Kyle Lo",
                "Mohit Iyyer",
                "Luca Soldaini"
            ],
            "affiliations": [
                "Allen Institute for AI",
                "University of Maryland",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.08808.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#training",
                    "#optimization",
                    "#synthetic",
                    "#dataset",
                    "#benchmark",
                    "#open_source",
                    "#reasoning"
                ],
                "emoji": "ğŸ“‹",
                "ru": {
                    "title": "Ğ—Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ñ†Ğ¸ĞºĞ» Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° How2Everything - Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°ĞµĞ¼Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¾Ğ±Ñ€Ğ°Ğ»Ğ¸ 351 Ñ‚Ñ‹ÑÑÑ‡Ñƒ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€ Ğ¸Ğ· Ğ²ĞµĞ±-ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ† (How2Mine), ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 7 Ñ‚Ñ‹ÑÑÑ‡ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² (How2Bench) Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ LLM-ÑÑƒĞ´ÑŒÑ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ (How2Score). Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸, Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ»Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ½Ğ° 10 Ğ¿ÑƒĞ½ĞºÑ‚Ğ¾Ğ² Ğ±ĞµĞ· Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¸ Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, ĞºĞ°Ğº Ğ²ĞµĞ±-Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ğ·Ğ°Ğ¼ĞºĞ½ÑƒÑ‚Ñ‹Ğ¹ Ñ†Ğ¸ĞºĞ» Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ² Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğµ."
                },
                "en": {
                    "title": "Enhancing Instruction Quality with Scalable Evaluation Frameworks",
                    "desc": "This paper presents How2Everything, a framework designed to evaluate and enhance the generation of goal-oriented procedures using large-scale web data. It introduces How2Mine, which collects a vast number of procedures from the internet, and How2Bench, a balanced evaluation set for assessing the quality of these procedures. The framework employs How2Score, an evaluation method that utilizes a language model to identify critical failures in generated instructions. Additionally, reinforcement learning is applied to improve the performance of models based on the scoring system, demonstrating significant advancements in procedural generation without compromising on standard benchmarks."
                },
                "zh": {
                    "title": "æå‡ç›®æ ‡å¯¼å‘ç¨‹åºç”Ÿæˆçš„å¯æ‰©å±•æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºHow2Everythingçš„æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°å’Œæ”¹è¿›åŸºäºç›®æ ‡çš„ç¨‹åºç”Ÿæˆã€‚è¯¥æ¡†æ¶é€šè¿‡ä»ç½‘ç»œä¸ŠæŒ–æ˜351Kä¸ªç¨‹åºï¼Œæ„å»ºäº†ä¸€ä¸ªåŒ…å«7000ä¸ªç¤ºä¾‹çš„è¯„ä¼°é›†How2Benchï¼Œä»¥ä¾¿äºå¤§è§„æ¨¡è¯„ä¼°ã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†How2Scoreè¯„ä¼°åè®®ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åˆ¤æ–­ç”Ÿæˆå†…å®¹æ˜¯å¦å­˜åœ¨å…³é”®æ€§é”™è¯¯ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼Œä½¿ç”¨How2Scoreä½œä¸ºå¥–åŠ±ï¼Œæ¨¡å‹åœ¨How2Benchä¸Šçš„è¡¨ç°æé«˜äº†è¶…è¿‡10åˆ†ï¼Œå±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨ç½‘ç»œæ•°æ®è¿›è¡Œèƒ½åŠ›è¯„ä¼°å’Œæ”¹è¿›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.08236",
            "title": "When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning",
            "url": "https://huggingface.co/papers/2602.08236",
            "abstract": "Adaptive test-time framework with world models enables selective visual imagination for spatial reasoning, improving efficiency and reliability by determining when imagination is necessary.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite rapid progress in Multimodal Large Language Models (MLLMs), visual spatial reasoning remains unreliable when correct answers depend on how a scene would appear under unseen or alternative viewpoints. Recent work addresses this by augmenting reasoning with world models for visual imagination, but questions such as when imagination is actually necessary, how much of it is beneficial, and when it becomes harmful, remain poorly understood. In practice, indiscriminate imagination can increase computation and even degrade performance by introducing misleading evidence. In this work, we present an in-depth analysis of test-time visual imagination as a controllable resource for spatial reasoning. We study when static visual evidence is sufficient, when imagination improves reasoning, and how excessive or unnecessary imagination affects accuracy and efficiency. To support this analysis, we introduce AVIC, an adaptive test-time framework with world models that explicitly reasons about the sufficiency of current visual evidence before selectively invoking and scaling visual imagination. Across spatial reasoning benchmarks (SAT, MMSI) and an embodied navigation benchmark (R2R), our results reveal clear scenarios where imagination is critical, marginal, or detrimental, and show that selective control can match or outperform fixed imagination strategies with substantially fewer world-model calls and language tokens. Overall, our findings highlight the importance of analyzing and controlling test-time imagination for efficient and reliable spatial reasoning.",
            "score": 6,
            "issue_id": 980,
            "pub_date": "2026-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "93306b7f7aeb8f16",
            "authors": [
                "Shoubin Yu",
                "Yue Zhang",
                "Zun Wang",
                "Jaehong Yoon",
                "Huaxiu Yao",
                "Mingyu Ding",
                "Mohit Bansal"
            ],
            "affiliations": [
                "Department of Computer Science, University of North Carolina, Chapel Hill",
                "Nanyang Technological University, Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.08236.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#multimodal",
                    "#cv",
                    "#benchmark"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ˜Ğ·Ğ±Ğ¸Ñ€Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ: ĞºĞ¾Ğ³Ğ´Ğ° Ğ²Ğ¸Ğ´ĞµÑ‚ÑŒ Ğ½ÑƒĞ¶Ğ½Ğ¾, Ğ° ĞºĞ¾Ğ³Ğ´Ğ° Ğ½ĞµÑ‚",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¾ÑˆĞ¸Ğ±Ğ°ÑÑ‚ÑÑ Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ÑŒ ÑÑ†ĞµĞ½Ñƒ Ñ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº Ğ·Ñ€ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ AVIC â€” Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ world models, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚, ĞºĞ¾Ğ³Ğ´Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ñ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¸ Ğ´Ğ°Ğ¶Ğµ ÑƒÑ…ÑƒĞ´ÑˆĞ¸Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾, Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ¼Ñƒ Ğ½ÑƒĞ¶ĞµĞ½ ÑĞµĞ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğ¹ Ğº world models."
                },
                "en": {
                    "title": "Selective Imagination for Smarter Spatial Reasoning",
                    "desc": "This paper introduces an adaptive framework called AVIC that enhances spatial reasoning by selectively using visual imagination based on the sufficiency of current visual evidence. It addresses the challenges of indiscriminate imagination, which can lead to increased computation and degraded performance. The study analyzes when visual imagination is beneficial and when it can be harmful, providing insights into optimizing its use. The results demonstrate that controlled imagination can improve efficiency and accuracy in spatial reasoning tasks compared to fixed strategies."
                },
                "zh": {
                    "title": "è‡ªé€‚åº”è§†è§‰æƒ³è±¡æå‡ç©ºé—´æ¨ç†æ•ˆç‡",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§è‡ªé€‚åº”æµ‹è¯•æ—¶æ¡†æ¶ï¼Œåˆ©ç”¨ä¸–ç•Œæ¨¡å‹æ¥é€‰æ‹©æ€§åœ°è¿›è¡Œè§†è§‰æƒ³è±¡ï¼Œä»¥æé«˜ç©ºé—´æ¨ç†çš„æ•ˆç‡å’Œå¯é æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œé™æ€è§†è§‰è¯æ®å·²ç»è¶³å¤Ÿï¼Œè€Œåœ¨å…¶ä»–æƒ…å†µä¸‹ï¼Œæƒ³è±¡å¯ä»¥æ”¹å–„æ¨ç†æ•ˆæœï¼Œä½†è¿‡åº¦çš„æƒ³è±¡å¯èƒ½ä¼šå¯¼è‡´å‡†ç¡®æ€§å’Œæ•ˆç‡ä¸‹é™ã€‚é€šè¿‡å¼•å…¥AVICæ¡†æ¶ï¼Œè®ºæ–‡åˆ†æäº†ä½•æ—¶éœ€è¦æƒ³è±¡ä»¥åŠæƒ³è±¡çš„é€‚åº¦ç¨‹åº¦ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œé€‰æ‹©æ€§æ§åˆ¶çš„æƒ³è±¡ç­–ç•¥åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºå›ºå®šçš„æƒ³è±¡ç­–ç•¥ï¼Œå‡å°‘äº†ä¸–ç•Œæ¨¡å‹çš„è°ƒç”¨å’Œè¯­è¨€æ ‡è®°çš„ä½¿ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.07775",
            "title": "Rolling Sink: Bridging Limited-Horizon Training and Open-Ended Testing in Autoregressive Video Diffusion",
            "url": "https://huggingface.co/papers/2602.07775",
            "abstract": "Autoregressive video diffusion models suffer from train-test gaps when generating long videos, but a training-free approach called Rolling Sink addresses this by maintaining AR cache and enabling ultra-long video synthesis.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, autoregressive (AR) video diffusion models has achieved remarkable performance. However, due to their limited training durations, a train-test gap emerges when testing at longer horizons, leading to rapid visual degradations. Following Self Forcing, which studies the train-test gap within the training duration, this work studies the train-test gap beyond the training duration, i.e., the gap between the limited horizons during training and open-ended horizons during testing. Since open-ended testing can extend beyond any finite training window, and long-video training is computationally expensive, we pursue a training-free solution to bridge this gap. To explore a training-free solution, we conduct a systematic analysis of AR cache maintenance. These insights lead to Rolling Sink. Built on Self Forcing (trained on only 5s clips), Rolling Sink effectively scales the AR video synthesis to ultra-long durations (e.g., 5-30 minutes at 16 FPS) at test time, with consistent subjects, stable colors, coherent structures, and smooth motions. As demonstrated by extensive experiments, Rolling Sink achieves superior long-horizon visual fidelity and temporal consistency compared to SOTA baselines. Project page: https://rolling-sink.github.io/",
            "score": 6,
            "issue_id": 981,
            "pub_date": "2026-02-08",
            "pub_date_card": {
                "ru": "8 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 8",
                "zh": "2æœˆ8æ—¥"
            },
            "hash": "b20640068525b746",
            "authors": [
                "Haodong Li",
                "Shaoteng Liu",
                "Zhe Lin",
                "Manmohan Chandraker"
            ],
            "affiliations": [
                "Adobe Research",
                "UC San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.07775.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#training",
                    "#video",
                    "#long_context",
                    "#diffusion"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ‘ĞµÑĞ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ°Ğ²Ñ‚Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¾ ÑĞ²ĞµÑ€Ñ…Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ² Ğ°Ğ²Ñ‚Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ Ğ½Ğ° ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ»Ğ¸Ğ¿Ğ°Ñ…, Ğ½Ğ¾ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Rolling Sink, Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ ÑÑ‚Ñƒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºÑÑˆĞµĞ¼ Ğ°Ğ²Ñ‚Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ ÑĞ²ĞµÑ€Ñ…Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ° (Ğ´Ğ¾ 30 Ğ¼Ğ¸Ğ½ÑƒÑ‚) Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹, ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ†Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ Ğ¿Ğ»Ğ°Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Rolling Sink Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ³ĞµÑ€ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹."
                },
                "en": {
                    "title": "Bridging the Gap: Ultra-Long Video Synthesis with Rolling Sink",
                    "desc": "This paper introduces Rolling Sink, a training-free method designed to improve the performance of autoregressive video diffusion models when generating long videos. Traditional models face a train-test gap, where the quality of generated videos degrades significantly when tested beyond their training duration. Rolling Sink addresses this issue by maintaining an autoregressive cache, allowing for the synthesis of ultra-long videos without the need for extensive training. The method demonstrates superior visual fidelity and temporal consistency, enabling the generation of coherent and stable videos lasting from 5 to 30 minutes."
                },
                "zh": {
                    "title": "æ— è®­ç»ƒçš„è¶…é•¿è§†é¢‘åˆæˆæ–°æ–¹æ³•",
                    "desc": "è‡ªå›å½’è§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé•¿è§†é¢‘æ—¶å­˜åœ¨è®­ç»ƒä¸æµ‹è¯•ä¹‹é—´çš„å·®è·ï¼Œå¯¼è‡´è§†è§‰è´¨é‡ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ— è®­ç»ƒçš„æ–¹æ³•ï¼Œç§°ä¸ºRolling Sinkï¼Œå®ƒé€šè¿‡ç»´æŠ¤è‡ªå›å½’ç¼“å­˜æ¥å®ç°è¶…é•¿è§†é¢‘åˆæˆã€‚è¯¥æ–¹æ³•åŸºäºSelf Forcingçš„ç ”ç©¶ï¼Œä¸“æ³¨äºè®­ç»ƒæ—¶é™ä¸æµ‹è¯•æ—¶é™ä¹‹é—´çš„å·®è·ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRolling Sinkåœ¨é•¿æ—¶é—´è§†é¢‘åˆæˆä¸­è¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰ä¸€è‡´çš„ä¸»é¢˜ã€ç¨³å®šçš„è‰²å½©å’Œæµç•…çš„è¿åŠ¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.06694",
            "title": "NanoQuant: Efficient Sub-1-Bit Quantization of Large Language Models",
            "url": "https://huggingface.co/papers/2602.06694",
            "abstract": "NanoQuant enables efficient post-training quantization of large language models to binary and sub-1-bit levels using low-rank binary factorization and ADMM optimization, achieving state-of-the-art accuracy while reducing memory requirements for consumer hardware deployment.  \t\t\t\t\tAI-generated summary \t\t\t\t Weight-only quantization has become a standard approach for efficiently serving large language models (LLMs). However, existing methods fail to efficiently compress models to binary (1-bit) levels, as they either require large amounts of data and compute or incur additional storage. In this work, we propose NanoQuant, the first post-training quantization (PTQ) method to compress LLMs to both binary and sub-1-bit levels. NanoQuant formulates quantization as a low-rank binary factorization problem, and compresses full-precision weights to low-rank binary matrices and scales. Specifically, it utilizes an efficient alternating direction method of multipliers (ADMM) method to precisely initialize latent binary matrices and scales, and then tune the initialized parameters through a block and model reconstruction process. Consequently, NanoQuant establishes a new Pareto frontier in low-memory post-training quantization, achieving state-of-the-art accuracy even at sub-1-bit compression rates. NanoQuant makes large-scale deployment feasible on consumer hardware. For example, it compresses Llama2-70B by 25.8times in just 13 hours on a single H100, enabling a 70B model to operate on a consumer 8 GB GPU.",
            "score": 6,
            "issue_id": 980,
            "pub_date": "2026-02-06",
            "pub_date_card": {
                "ru": "6 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 6",
                "zh": "2æœˆ6æ—¥"
            },
            "hash": "164cf8bc03ab7d1b",
            "authors": [
                "Hyochan Chong",
                "Dongkyu Kim",
                "Changdong Kim",
                "Minseop Choi"
            ],
            "affiliations": [
                "Samsung Research, Seoul, Korea"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.06694.jpg",
            "data": {
                "categories": [
                    "#optimization"
                ],
                "emoji": "âš™ï¸",
                "ru": {
                    "title": "Ğ­ĞºÑÑ‚Ñ€ĞµĞ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ¿ÑƒÑ‚ÑŒ Ğº Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…",
                    "desc": "NanoQuant â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ³Ğ¾ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¶Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ğ¾Ğ³Ğ¾ (1-Ğ±Ğ¸Ñ‚) Ğ¸ ÑÑƒĞ±Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ²ÑƒÑ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½ÑƒÑ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ğº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ, Ñ€ĞµÑˆĞ°ĞµĞ¼ÑƒÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ñ‡ĞµÑ€ĞµĞ´ÑƒÑÑ‰Ğ¸Ñ…ÑÑ Ğ¼Ğ½Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ĞµĞ¹ Ğ›Ğ°Ğ³Ñ€Ğ°Ğ½Ğ¶Ğ° (ADMM). ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ñ‹, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ñ‡ĞµÑ€ĞµĞ· Ğ±Ğ»Ğ¾Ñ‡Ğ½ÑƒÑ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒĞ½ÑƒÑ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞºÑÑ‚Ñ€ĞµĞ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ÑĞ¶Ğ°Ñ‚Ğ¸Ğ¸ Ğ¸ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ½Ğ° Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¼ Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸."
                },
                "en": {
                    "title": "NanoQuant: Revolutionizing Model Compression for Consumer Hardware",
                    "desc": "NanoQuant is a novel method for post-training quantization of large language models (LLMs) that allows them to be compressed to binary and sub-1-bit levels. It uses low-rank binary factorization to convert full-precision weights into low-rank binary matrices, significantly reducing memory usage. The method employs an efficient alternating direction method of multipliers (ADMM) for precise initialization and tuning of these binary matrices. As a result, NanoQuant achieves state-of-the-art accuracy while making it possible to deploy large models on consumer hardware, such as compressing Llama2-70B by 25.8 times."
                },
                "zh": {
                    "title": "NanoQuantï¼šé«˜æ•ˆçš„åè®­ç»ƒé‡åŒ–æ–°æ–¹æ³•",
                    "desc": "NanoQuantæ˜¯ä¸€ç§é«˜æ•ˆçš„åè®­ç»ƒé‡åŒ–æ–¹æ³•ï¼Œèƒ½å¤Ÿå°†å¤§å‹è¯­è¨€æ¨¡å‹å‹ç¼©åˆ°äºŒè¿›åˆ¶å’Œä½äº1ä½çš„æ°´å¹³ã€‚å®ƒé€šè¿‡ä½ç§©äºŒè¿›åˆ¶åˆ†è§£å’Œäº¤æ›¿æ–¹å‘ä¹˜å­æ³•ï¼ˆADMMï¼‰ä¼˜åŒ–ï¼Œå®ç°äº†åœ¨å‡å°‘å†…å­˜éœ€æ±‚çš„åŒæ—¶ä¿æŒæœ€å…ˆè¿›çš„å‡†ç¡®æ€§ã€‚NanoQuantå°†é‡åŒ–é—®é¢˜å½¢å¼åŒ–ä¸ºä½ç§©äºŒè¿›åˆ¶åˆ†è§£ï¼Œèƒ½å¤Ÿåœ¨æ¶ˆè´¹è€…ç¡¬ä»¶ä¸Šå®ç°å¤§è§„æ¨¡éƒ¨ç½²ã€‚è¯¥æ–¹æ³•åœ¨ä»…ç”¨13å°æ—¶å†…å°†Llama2-70Bæ¨¡å‹å‹ç¼©äº†25.8å€ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨8GBçš„æ¶ˆè´¹çº§GPUä¸Šè¿è¡Œã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.07796",
            "title": "Thinking Makes LLM Agents Introverted: How Mandatory Thinking Can Backfire in User-Engaged Agents",
            "url": "https://huggingface.co/papers/2602.07796",
            "abstract": "Explicit reasoning in LLM agents can degrade performance in user-engaged scenarios by reducing information disclosure and weakening agent-user communication, with transparency-aware prompting showing better results.  \t\t\t\t\tAI-generated summary \t\t\t\t Eliciting reasoning has emerged as a powerful technique for improving the performance of large language models (LLMs) on complex tasks by inducing thinking. However, their effectiveness in realistic user-engaged agent scenarios remains unclear. In this paper, we conduct a comprehensive study on the effect of explicit thinking in user-engaged LLM agents. Our experiments span across seven models, three benchmarks, and two thinking instantiations, and we evaluate them through both a quantitative response taxonomy analysis and qualitative failure propagation case studies. Contrary to expectations, we find that mandatory thinking often backfires on agents in user-engaged settings, causing anomalous performance degradation across various LLMs. Our key finding reveals that thinking makes agents more ``introverted'' by shortening responses and reducing information disclosure to users, which weakens agent-user information exchange and leads to downstream task failures. Furthermore, we demonstrate that explicitly prompting for information disclosure reliably improves performance across diverse model families, suggesting that proactive transparency is a vital lever for agent optimization. Overall, our study suggests that information transparency awareness is a crucial yet underexplored perspective for the future design of reasoning agents in real-world scenarios. Our code is available at https://github.com/deeplearning-wisc/Thinking-Agent.",
            "score": 5,
            "issue_id": 982,
            "pub_date": "2026-02-08",
            "pub_date_card": {
                "ru": "8 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 8",
                "zh": "2æœˆ8æ—¥"
            },
            "hash": "43b8400b761765b6",
            "authors": [
                "Jiatong Li",
                "Changdae Oh",
                "Hyeong Kyu Choi",
                "Jindong Wang",
                "Sharon Li"
            ],
            "affiliations": [
                "University of Wisconsin-Madison",
                "William & Mary"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.07796.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#agents",
                    "#reasoning",
                    "#benchmark",
                    "#open_source",
                    "#training"
                ],
                "emoji": "ğŸ¤",
                "ru": {
                    "title": "ĞœĞ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ¾Ğ»Ğ¾Ñ‚Ğ¾? ĞŸĞ¾Ñ‡ĞµĞ¼Ñƒ ÑĞ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¾ÑĞ»Ğ°Ğ±Ğ»ÑĞµÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ ÑĞ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² LLM Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ñ…, Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼Ğ¸, Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ¼Ñ‹ÑĞ»ĞµĞ¹ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ·Ğ°ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°Ñ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ¸ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°Ñ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¾Ğ±Ğ¼ĞµĞ½ Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ ÑĞ²Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ€Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡Ñ‘Ñ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ¼ĞµĞ½Ğ° Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°ÑÑ‰Ğ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Transparency Over Thinking: Enhancing LLM Performance in User Engagement",
                    "desc": "This paper investigates how explicit reasoning in large language models (LLMs) affects their performance in user-engaged scenarios. The authors find that requiring LLMs to think explicitly can actually harm their ability to communicate effectively with users, leading to shorter and less informative responses. Through extensive experiments, they show that this 'introverted' behavior results in degraded performance on various tasks. The study highlights the importance of transparency in agent interactions, suggesting that encouraging information disclosure can enhance LLM performance in real-world applications."
                },
                "zh": {
                    "title": "ä¿¡æ¯é€æ˜åº¦æ˜¯æå‡ä»£ç†æ€§èƒ½çš„å…³é”®",
                    "desc": "æœ¬æ–‡ç ”ç©¶äº†åœ¨ç”¨æˆ·äº¤äº’åœºæ™¯ä¸­ï¼Œæ˜¾å¼æ¨ç†å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†æ€§èƒ½çš„å½±å“ã€‚ç ”ç©¶å‘ç°ï¼Œå¼ºåˆ¶æ¨ç†å¾€å¾€å¯¼è‡´ä»£ç†çš„è¡¨ç°ä¸‹é™ï¼Œå› ä¸ºå®ƒä½¿å¾—ä»£ç†çš„å›ç­”å˜å¾—æ›´ç®€çŸ­ï¼Œå‡å°‘äº†ä¸ç”¨æˆ·çš„ä¿¡æ¯äº¤æµã€‚é€šè¿‡å¯¹ä¸ƒä¸ªæ¨¡å‹å’Œå¤šä¸ªåŸºå‡†çš„å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œä¸»åŠ¨çš„ä¿¡æ¯é€æ˜åº¦å¯ä»¥æ˜¾è‘—æå‡ä»£ç†çš„æ€§èƒ½ã€‚æ€»çš„æ¥è¯´ï¼Œä¿¡æ¯é€æ˜åº¦æ„è¯†æ˜¯æœªæ¥æ¨ç†ä»£ç†è®¾è®¡ä¸­ä¸€ä¸ªé‡è¦ä½†æœªè¢«å……åˆ†æ¢ç´¢çš„è§†è§’ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.08145",
            "title": "Reliable and Responsible Foundation Models: A Comprehensive Survey",
            "url": "https://huggingface.co/papers/2602.08145",
            "abstract": "Foundation models including LLMs, MLLMs, and generative models require reliable and responsible development addressing bias, security, explainability, and other critical issues for trustworthy deployment across multiple domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Foundation models, including Large Language Models (LLMs), Multimodal Large Language Models (MLLMs), Image Generative Models (i.e, Text-to-Image Models and Image-Editing Models), and Video Generative Models, have become essential tools with broad applications across various domains such as law, medicine, education, finance, science, and beyond. As these models see increasing real-world deployment, ensuring their reliability and responsibility has become critical for academia, industry, and government. This survey addresses the reliable and responsible development of foundation models. We explore critical issues, including bias and fairness, security and privacy, uncertainty, explainability, and distribution shift. Our research also covers model limitations, such as hallucinations, as well as methods like alignment and Artificial Intelligence-Generated Content (AIGC) detection. For each area, we review the current state of the field and outline concrete future research directions. Additionally, we discuss the intersections between these areas, highlighting their connections and shared challenges. We hope our survey fosters the development of foundation models that are not only powerful but also ethical, trustworthy, reliable, and socially responsible.",
            "score": 5,
            "issue_id": 981,
            "pub_date": "2026-02-04",
            "pub_date_card": {
                "ru": "4 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 4",
                "zh": "2æœˆ4æ—¥"
            },
            "hash": "bc07f9e6a92f2e07",
            "authors": [
                "Xinyu Yang",
                "Junlin Han",
                "Rishi Bommasani",
                "Jinqi Luo",
                "Wenjie Qu",
                "Wangchunshu Zhou",
                "Adel Bibi",
                "Xiyao Wang",
                "Jaehong Yoon",
                "Elias Stengel-Eskin",
                "Shengbang Tong",
                "Lingfeng Shen",
                "Rafael Rafailov",
                "Runjia Li",
                "Zhaoyang Wang",
                "Yiyang Zhou",
                "Chenhang Cui",
                "Yu Wang",
                "Wenhao Zheng",
                "Huichi Zhou",
                "Jindong Gu",
                "Zhaorun Chen",
                "Peng Xia",
                "Tony Lee",
                "Thomas Zollo",
                "Vikash Sehwag",
                "Jixuan Leng",
                "Jiuhai Chen",
                "Yuxin Wen",
                "Huan Zhang",
                "Zhun Deng",
                "Linjun Zhang",
                "Pavel Izmailov",
                "Pang Wei Koh",
                "Yulia Tsvetkov",
                "Andrew Wilson",
                "Jiaheng Zhang",
                "James Zou",
                "Cihang Xie",
                "Hao Wang",
                "Philip Torr",
                "Julian McAuley",
                "David Alvarez-Melis",
                "Florian TramÃ¨r",
                "Kaidi Xu",
                "Suman Jana",
                "Chris Callison-Burch",
                "Rene Vidal",
                "Filippos Kokkinos",
                "Mohit Bansal",
                "Beidi Chen",
                "Huaxiu Yao"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "Columbia University",
                "Drexel University",
                "ETH Zurich",
                "Harvard University",
                "Imperial College London",
                "Johns Hopkins University",
                "Mila",
                "National University of Singapore",
                "New York University",
                "Princeton University",
                "Rutgers University",
                "Stanford University",
                "University College London",
                "University of California, San Diego",
                "University of California, Santa Cruz",
                "University of Chicago",
                "University of Maryland",
                "University of Montreal",
                "University of North Carolina at Chapel Hill",
                "University of Oxford",
                "University of Pennsylvania",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.08145.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#alignment",
                    "#ethics",
                    "#hallucinations",
                    "#survey",
                    "#interpretability"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "ĞŸÑƒÑ‚ÑŒ Ğº Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğ¼ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¼ Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ¹ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ LLM Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ñ‹: Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ, Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¾Ğ±ÑŠÑÑĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹. Ğ¦ĞµĞ»ÑŒÑ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ÑĞ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ñ…, ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¸ Ğ·Ğ°ÑĞ»ÑƒĞ¶Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ¾Ğ²ĞµÑ€Ğ¸Ñ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…."
                },
                "en": {
                    "title": "Building Trustworthy Foundation Models for a Responsible AI Future",
                    "desc": "This paper discusses the development of foundation models, which include Large Language Models (LLMs) and generative models, emphasizing the need for their responsible and reliable use. It highlights critical issues such as bias, security, explainability, and the challenges of deploying these models in real-world applications. The authors review current limitations of these models, including hallucinations, and propose methods for improving alignment and detecting AI-generated content. The goal is to encourage the creation of powerful yet ethical models that can be trusted across various sectors."
                },
                "zh": {
                    "title": "æ¨åŠ¨åŸºç¡€æ¨¡å‹çš„å¯é ä¸è´£ä»»å‘å±•",
                    "desc": "åŸºç¡€æ¨¡å‹ï¼ŒåŒ…æ‹¬å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å’Œç”Ÿæˆæ¨¡å‹ï¼Œå·²æˆä¸ºå„ä¸ªé¢†åŸŸçš„é‡è¦å·¥å…·ã€‚éšç€è¿™äº›æ¨¡å‹åœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨æ—¥ç›Šå¢åŠ ï¼Œç¡®ä¿å…¶å¯é æ€§å’Œè´£ä»»æ€§å˜å¾—è‡³å…³é‡è¦ã€‚æœ¬æ–‡æ¢è®¨äº†åè§ä¸å…¬å¹³æ€§ã€å®‰å…¨ä¸éšç§ã€ä¸ç¡®å®šæ€§ã€å¯è§£é‡Šæ€§å’Œåˆ†å¸ƒå˜åŒ–ç­‰å…³é”®é—®é¢˜ï¼Œå¹¶æå‡ºäº†æœªæ¥çš„ç ”ç©¶æ–¹å‘ã€‚æˆ‘ä»¬å¸Œæœ›æ¨åŠ¨åŸºç¡€æ¨¡å‹çš„å‘å±•ï¼Œä½¿å…¶ä¸ä»…å¼ºå¤§ï¼Œè€Œä¸”ç¬¦åˆä¼¦ç†ã€å€¼å¾—ä¿¡èµ–ã€å¯é å’Œå…·æœ‰ç¤¾ä¼šè´£ä»»æ„Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.21363",
            "title": "Towards Bridging the Gap between Large-Scale Pretraining and Efficient Finetuning for Humanoid Control",
            "url": "https://huggingface.co/papers/2601.21363",
            "abstract": "Off-policy Soft Actor-Critic with large-batch updates enables efficient humanoid locomotion policy pretraining, while model-based methods facilitate safe adaptation through deterministic data collection and stochastic exploration within physics-informed world models.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) is widely used for humanoid control, with on-policy methods such as Proximal Policy Optimization (PPO) enabling robust training via large-scale parallel simulation and, in some cases, zero-shot deployment to real robots. However, the low sample efficiency of on-policy algorithms limits safe adaptation to new environments. Although off-policy RL and model-based RL have shown improved sample efficiency, the gap between large-scale pretraining and efficient finetuning on humanoids still exists. In this paper, we find that off-policy Soft Actor-Critic (SAC), with large-batch update and a high Update-To-Data (UTD) ratio, reliably supports large-scale pretraining of humanoid locomotion policies, achieving zero-shot deployment on real robots. For adaptation, we demonstrate that these SAC-pretrained policies can be finetuned in new environments and out-of-distribution tasks using model-based methods. Data collection in the new environment executes a deterministic policy while stochastic exploration is instead confined to a physics-informed world model. This separation mitigates the risks of random exploration during adaptation while preserving exploratory coverage for improvement. Overall, the approach couples the wall-clock efficiency of large-scale simulation during pretraining with the sample efficiency of model-based learning during fine-tuning.",
            "score": 4,
            "issue_id": 980,
            "pub_date": "2026-01-29",
            "pub_date_card": {
                "ru": "29 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 29",
                "zh": "1æœˆ29æ—¥"
            },
            "hash": "2f5a5c620860610c",
            "authors": [
                "Weidong Huang",
                "Zhehan Li",
                "Hangxin Liu",
                "Biao Hou",
                "Yao Su",
                "Jingwen Zhang"
            ],
            "affiliations": [
                "School of Artificial Intelligence, Xidian University",
                "State Key Laboratory of General Artificial Intelligence, BIGAI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.21363.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#robotics",
                    "#rl"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ³ÑƒĞ¼Ğ°Ğ½Ğ¾Ğ¸Ğ´Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· off-policy Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Soft Actor-Critic Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ Ğ±Ğ°Ñ‚Ñ‡Ğ° Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¼ ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ (UTD) ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³ÑƒĞ¼Ğ°Ğ½Ğ¾Ğ¸Ğ´Ğ¾Ğ¼ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğµ. ĞŸĞ¾ÑĞ»Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ÑĞ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ³Ğ´Ğµ ÑĞ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾, Ğ° ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°, ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ñ€Ğ¸ÑĞºĞ¸ Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼. ĞšĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ñ‡Ğ½Ğ¾-ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "Efficient Humanoid Control: Bridging Pretraining and Adaptation",
                    "desc": "This paper presents an approach that combines off-policy Soft Actor-Critic (SAC) with large-batch updates to enhance the pretraining of humanoid locomotion policies. By leveraging high Update-To-Data (UTD) ratios, the method achieves efficient zero-shot deployment of these policies on real robots. For adapting to new environments, the authors utilize model-based techniques that allow for safe data collection through deterministic policies while maintaining stochastic exploration in a physics-informed world model. This strategy effectively bridges the gap between large-scale pretraining and efficient fine-tuning, ensuring both safety and performance in humanoid control tasks."
                },
                "zh": {
                    "title": "é«˜æ•ˆç±»äººæ­¥æ€ç­–ç•¥é¢„è®­ç»ƒä¸å®‰å…¨é€‚åº”",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºç¦»çº¿ç­–ç•¥çš„è½¯æ¼”å‘˜è¯„è®ºå®¶ï¼ˆSACï¼‰æ–¹æ³•ï¼Œç»“åˆå¤§æ‰¹é‡æ›´æ–°ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°è¿›è¡Œç±»äººæ­¥æ€ç­–ç•¥çš„é¢„è®­ç»ƒã€‚é€šè¿‡æ¨¡å‹é©±åŠ¨çš„æ–¹æ³•ï¼Œå®‰å…¨é€‚åº”æ–°ç¯å¢ƒï¼Œé‡‡ç”¨ç¡®å®šæ€§æ•°æ®æ”¶é›†å’Œç‰©ç†çŸ¥è¯†é©±åŠ¨çš„ä¸–ç•Œæ¨¡å‹è¿›è¡Œéšæœºæ¢ç´¢ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç»è¿‡é¢„è®­ç»ƒçš„SACç­–ç•¥å¯ä»¥åœ¨æ–°ç¯å¢ƒå’Œåˆ†å¸ƒå¤–ä»»åŠ¡ä¸­è¿›è¡Œå¾®è°ƒï¼Œç¡®ä¿é€‚åº”è¿‡ç¨‹çš„å®‰å…¨æ€§ã€‚æ•´ä½“æ–¹æ³•å°†å¤§è§„æ¨¡ä»¿çœŸé¢„è®­ç»ƒçš„æ—¶é—´æ•ˆç‡ä¸æ¨¡å‹é©±åŠ¨å­¦ä¹ çš„æ ·æœ¬æ•ˆç‡ç›¸ç»“åˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.09003",
            "title": "Data Science and Technology Towards AGI Part I: Tiered Data Management",
            "url": "https://huggingface.co/papers/2602.09003",
            "abstract": "Large language models are increasingly guiding data management processes through a tiered framework that optimizes data quality, cost, and training efficiency across different stages of model development.  \t\t\t\t\tAI-generated summary \t\t\t\t The development of artificial intelligence can be viewed as an evolution of data-driven learning paradigms, with successive shifts in data organization and utilization continuously driving advances in model capability. Current LLM research is dominated by a paradigm that relies heavily on unidirectional scaling of data size, increasingly encountering bottlenecks in data availability, acquisition cost, and training efficiency. In this work, we argue that the development of AGI is entering a new phase of data-model co-evolution, in which models actively guide data management while high-quality data, in turn, amplifies model capabilities. To implement this vision, we propose a tiered data management framework, designed to support the full LLM training lifecycle across heterogeneous learning objectives and cost constraints. Specifically, we introduce an L0-L4 tiered data management framework, ranging from raw uncurated resources to organized and verifiable knowledge. Importantly, LLMs are fully used in data management processes, such as quality scoring and content editing, to refine data across tiers. Each tier is characterized by distinct data properties, management strategies, and training roles, enabling data to be strategically allocated across LLM training stages, including pre-training, mid-training, and alignment. The framework balances data quality, acquisition cost, and marginal training benefit, providing a systematic approach to scalable and sustainable data management. We validate the effectiveness of the proposed framework through empirical studies, in which tiered datasets are constructed from raw corpora and used across multiple training phases. Experimental results demonstrate that tier-aware data utilization significantly improves training efficiency and model performance. To facilitate further research, we release our tiered datasets and processing tools to the community.",
            "score": 3,
            "issue_id": 986,
            "pub_date": "2026-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "7dba9ed1f1f3b13b",
            "authors": [
                "Yudong Wang",
                "Zixuan Fu",
                "Hengyu Zhao",
                "Chen Zhao",
                "Chuyue Zhou",
                "Xinle Lin",
                "Hongya Lyu",
                "Shuaikang Xue",
                "Yi Yi",
                "Yingjiao Wang",
                "Zhi Zheng",
                "Yuzhou Zhang",
                "Jie Zhou",
                "Chaojun Xiao",
                "Xu Han",
                "Zhiyuan Liu",
                "Maosong Sun"
            ],
            "affiliations": [
                "Beijing Institute of Technology",
                "ModelBest Inc.",
                "South China Agricultural University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.09003.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#open_source",
                    "#alignment",
                    "#agi",
                    "#optimization",
                    "#data"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°ÑÑ‚ÑƒÑ‚ Ğ²Ğ¼ĞµÑÑ‚Ğµ: Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ AGI",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¹ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ³Ğ´Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ğ¼Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€Ñ‘Ñ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ (L0-L4), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·ÑƒĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚ ÑÑ‹Ñ€Ñ‹Ñ… Ğ½ĞµÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ´Ğ¾ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ LLM Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¹ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑƒÑ€Ğ°Ğ²Ğ½Ğ¾Ğ²ĞµÑˆĞ¸Ğ²Ğ°Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾, ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾ÑĞ¾Ğ·Ğ½Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Optimizing Data Management for Enhanced Model Training Efficiency",
                    "desc": "This paper discusses a new approach to managing data for training large language models (LLMs) by introducing a tiered framework. The framework categorizes data into different levels, from raw to organized, allowing models to guide data management processes effectively. By optimizing data quality and training efficiency, the framework addresses challenges related to data availability and acquisition costs. Empirical studies show that using tiered datasets enhances model performance and training efficiency, paving the way for more sustainable AI development."
                },
                "zh": {
                    "title": "åˆ†å±‚æ•°æ®ç®¡ç†ï¼Œæå‡æ¨¡å‹è®­ç»ƒæ•ˆç‡",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ•°æ®ç®¡ç†è¿‡ç¨‹ä¸­çš„æ–°æ–¹æ³•ï¼Œæå‡ºäº†ä¸€ç§åˆ†å±‚çš„æ•°æ®ç®¡ç†æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ—¨åœ¨ä¼˜åŒ–æ•°æ®è´¨é‡ã€æˆæœ¬å’Œè®­ç»ƒæ•ˆç‡ï¼Œæ”¯æŒæ¨¡å‹å¼€å‘çš„å„ä¸ªé˜¶æ®µã€‚é€šè¿‡å¼•å…¥L0-L4çš„åˆ†å±‚ç®¡ç†ï¼Œæ¨¡å‹èƒ½å¤Ÿä¸»åŠ¨æŒ‡å¯¼æ•°æ®ç®¡ç†ï¼ŒåŒæ—¶é«˜è´¨é‡çš„æ•°æ®åˆèƒ½å¢å¼ºæ¨¡å‹çš„èƒ½åŠ›ã€‚å®éªŒè¯æ˜ï¼Œé‡‡ç”¨åˆ†å±‚æ•°æ®åˆ©ç”¨æ˜¾è‘—æé«˜äº†è®­ç»ƒæ•ˆç‡å’Œæ¨¡å‹æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.08961",
            "title": "MotionCrafter: Dense Geometry and Motion Reconstruction with a 4D VAE",
            "url": "https://huggingface.co/papers/2602.08961",
            "abstract": "MotionCrafter is a video diffusion framework that jointly reconstructs 4D geometry and estimates dense motion using a novel joint representation and 4D VAE architecture.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce MotionCrafter, a video diffusion-based framework that jointly reconstructs 4D geometry and estimates dense motion from a monocular video. The core of our method is a novel joint representation of dense 3D point maps and 3D scene flows in a shared coordinate system, and a novel 4D VAE to effectively learn this representation. Unlike prior work that forces the 3D value and latents to align strictly with RGB VAE latents-despite their fundamentally different distributions-we show that such alignment is unnecessary and leads to suboptimal performance. Instead, we introduce a new data normalization and VAE training strategy that better transfers diffusion priors and greatly improves reconstruction quality. Extensive experiments across multiple datasets demonstrate that MotionCrafter achieves state-of-the-art performance in both geometry reconstruction and dense scene flow estimation, delivering 38.64% and 25.0% improvements in geometry and motion reconstruction, respectively, all without any post-optimization. Project page: https://ruijiezhu94.github.io/MotionCrafter_Page",
            "score": 3,
            "issue_id": 982,
            "pub_date": "2026-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "a03c379e4c6f9864",
            "authors": [
                "Ruijie Zhu",
                "Jiahao Lu",
                "Wenbo Hu",
                "Xiaoguang Han",
                "Jianfei Cai",
                "Ying Shan",
                "Chuanxia Zheng"
            ],
            "affiliations": [
                "ARC Lab, Tencent PCG",
                "CUHK(SZ)",
                "HKUST",
                "Monash University",
                "NTU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.08961.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#video",
                    "#architecture",
                    "#3d",
                    "#training"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ¡Ğ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ°Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· 4D Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ",
                    "desc": "MotionCrafter â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€ÑƒĞµÑ‚ 4D Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ ÑÑ†ĞµĞ½Ñ‹ Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸Ğ· Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° â€” Ğ½Ğ¾Ğ²Ğ¾Ğµ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ 3D ĞºĞ°Ñ€Ñ‚ Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¸ 3D Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ² ÑÑ†ĞµĞ½Ñ‹ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ 4D VAE Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¼Ñƒ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ RGB VAE Ğ½ĞµĞ¾Ğ±ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¸ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾, Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ¼Ñƒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ VAE Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² â€” Ğ½Ğ° 38.64% Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¸ Ğ½Ğ° 25.0% Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ÑÑ†ĞµĞ½Ñ‹ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸."
                },
                "en": {
                    "title": "Revolutionizing 4D Geometry and Motion Estimation with MotionCrafter",
                    "desc": "MotionCrafter is a cutting-edge framework that utilizes video diffusion techniques to reconstruct 4D geometry and estimate dense motion from single-camera video inputs. It introduces a unique joint representation that combines dense 3D point maps with 3D scene flows, allowing for a more cohesive understanding of motion and structure in a scene. The framework employs a novel 4D Variational Autoencoder (VAE) that enhances the learning of this representation without the need for strict alignment with RGB VAE latents, which can hinder performance. Through innovative data normalization and training strategies, MotionCrafter significantly improves the quality of both geometry reconstruction and motion estimation, achieving remarkable performance gains over previous methods."
                },
                "zh": {
                    "title": "MotionCrafterï¼šè§†é¢‘é‡å»ºä¸è¿åŠ¨ä¼°è®¡çš„æ–°çªç ´",
                    "desc": "MotionCrafter æ˜¯ä¸€ä¸ªåŸºäºè§†é¢‘æ‰©æ•£çš„æ¡†æ¶ï¼Œèƒ½å¤ŸåŒæ—¶é‡å»ºå››ç»´å‡ ä½•ç»“æ„å¹¶ä¼°è®¡å¯†é›†è¿åŠ¨ã€‚è¯¥æ–¹æ³•é‡‡ç”¨äº†ä¸€ç§æ–°é¢–çš„è”åˆè¡¨ç¤ºï¼Œå°†å¯†é›†çš„ä¸‰ç»´ç‚¹å›¾å’Œä¸‰ç»´åœºæ™¯æµåœ¨å…±äº«åæ ‡ç³»ç»Ÿä¸­ç»“åˆã€‚ä¸ä»¥å¾€çš„ç ”ç©¶ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸å†å¼ºåˆ¶ä¸‰ç»´å€¼ä¸ RGB VAE æ½œå˜é‡ä¸¥æ ¼å¯¹é½ï¼Œè€Œæ˜¯é€šè¿‡æ–°çš„æ•°æ®å½’ä¸€åŒ–å’Œ VAE è®­ç»ƒç­–ç•¥ï¼Œæ˜¾è‘—æé«˜äº†é‡å»ºè´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMotionCrafter åœ¨å‡ ä½•é‡å»ºå’Œå¯†é›†åœºæ™¯æµä¼°è®¡æ–¹é¢å‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåˆ†åˆ«æé«˜äº† 38.64% å’Œ 25.0%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.08829",
            "title": "WildReward: Learning Reward Models from In-the-Wild Human Interactions",
            "url": "https://huggingface.co/papers/2602.08829",
            "abstract": "WildReward demonstrates that reward models can be effectively trained from in-the-wild user interactions using ordinal regression, achieving performance comparable to traditional methods while benefiting from user diversity.  \t\t\t\t\tAI-generated summary \t\t\t\t Reward models (RMs) are crucial for the training of large language models (LLMs), yet they typically rely on large-scale human-annotated preference pairs. With the widespread deployment of LLMs, in-the-wild interactions have emerged as a rich source of implicit reward signals. This raises the question: Can we develop reward models directly from in-the-wild interactions? In this work, we explore this possibility by adopting WildChat as an interaction source and proposing a pipeline to extract reliable human feedback, yielding 186k high-quality instances for training WildReward via ordinal regression directly on user feedback without preference pairs. Extensive experiments demonstrate that WildReward achieves comparable or even superior performance compared to conventional reward models, with improved calibration and cross-sample consistency. We also observe that WildReward benefits directly from user diversity, where more users yield stronger reward models. Finally, we apply WildReward to online DPO training and observe significant improvements across various tasks. Code and data are released at https://github.com/THU-KEG/WildReward.",
            "score": 3,
            "issue_id": 981,
            "pub_date": "2026-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "d398683bec2a3790",
            "authors": [
                "Hao Peng",
                "Yunjia Qi",
                "Xiaozhi Wang",
                "Zijun Yao",
                "Lei Hou",
                "Juanzi Li"
            ],
            "affiliations": [
                "Department of Computer Science and Technology, Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.08829.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#rlhf",
                    "#data"
                ],
                "emoji": "ğŸ†",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑÑ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ WildReward â€” Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑÑ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¸. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ÑÑ‚ Ğ½ĞµÑĞ²Ğ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¸Ğ· 186 Ñ‚Ñ‹ÑÑÑ‡ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼ÑƒÑ Ğ¸Ğ»Ğ¸ Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, Ñ‡ĞµĞ¼ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¾Ğ¹ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ²Ñ‹Ğ¸Ğ³Ñ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ñ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ² Ğ¼ĞµÑ‚Ğ¾Ğ´Ğµ DPO Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Harnessing User Diversity for Superior Reward Models",
                    "desc": "WildReward introduces a novel approach to training reward models using ordinal regression based on real user interactions, rather than relying on traditional human-annotated preference pairs. This method leverages the implicit feedback from diverse users, resulting in a rich dataset of 186,000 high-quality instances for training. The experiments show that WildReward not only matches but can outperform conventional reward models in terms of performance, calibration, and consistency across samples. Additionally, the model benefits from user diversity, indicating that a broader user base leads to stronger reward model performance."
                },
                "zh": {
                    "title": "åˆ©ç”¨ç”¨æˆ·äº¤äº’æå‡å¥–åŠ±æ¨¡å‹çš„å¤šæ ·æ€§ä¸æ€§èƒ½",
                    "desc": "WildRewardå±•ç¤ºäº†å¦‚ä½•é€šè¿‡ç”¨æˆ·çš„è‡ªç„¶äº¤äº’æ¥æœ‰æ•ˆè®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œä½¿ç”¨äº†åºæ•°å›å½’çš„æ–¹æ³•ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒWildRewardåœ¨æ€§èƒ½ä¸Šç›¸å½“ï¼ŒåŒæ—¶ä¹Ÿåˆ©ç”¨äº†ç”¨æˆ·çš„å¤šæ ·æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç›´æ¥ä»ç”¨æˆ·åé¦ˆä¸­æå–çš„é«˜è´¨é‡å®ä¾‹å¯ä»¥æ˜¾è‘—æå‡å¥–åŠ±æ¨¡å‹çš„æ•ˆæœã€‚æœ€ç»ˆï¼ŒWildRewardåœ¨åœ¨çº¿DPOè®­ç»ƒä¸­åº”ç”¨ï¼Œæ˜¾ç¤ºå‡ºåœ¨å¤šä¸ªä»»åŠ¡ä¸Šçš„æ˜¾è‘—æ”¹è¿›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.06445",
            "title": "ECO: Energy-Constrained Optimization with Reinforcement Learning for Humanoid Walking",
            "url": "https://huggingface.co/papers/2602.06445",
            "abstract": "Energy-constrained optimization framework separates energy metrics from rewards using Lagrangian method to achieve stable, energy-efficient humanoid robot locomotion with reduced hyperparameter tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Achieving stable and energy-efficient locomotion is essential for humanoid robots to operate continuously in real-world applications. Existing MPC and RL approaches often rely on energy-related metrics embedded within a multi-objective optimization framework, which require extensive hyperparameter tuning and often result in suboptimal policies. To address these challenges, we propose ECO (Energy-Constrained Optimization), a constrained RL framework that separates energy-related metrics from rewards, reformulating them as explicit inequality constraints. This method provides a clear and interpretable physical representation of energy costs, enabling more efficient and intuitive hyperparameter tuning for improved energy efficiency. ECO introduces dedicated constraints for energy consumption and reference motion, enforced by the Lagrangian method, to achieve stable, symmetric, and energy-efficient walking for humanoid robots. We evaluated ECO against MPC, standard RL with reward shaping, and four state-of-the-art constrained RL methods. Experiments, including sim-to-sim and sim-to-real transfers on the kid-sized humanoid robot BRUCE, demonstrate that ECO significantly reduces energy consumption compared to baselines while maintaining robust walking performance. These results highlight a substantial advancement in energy-efficient humanoid locomotion. All experimental demonstrations can be found on the project website: https://sites.google.com/view/eco-humanoid.",
            "score": 3,
            "issue_id": 980,
            "pub_date": "2026-02-06",
            "pub_date_card": {
                "ru": "6 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 6",
                "zh": "2æœˆ6æ—¥"
            },
            "hash": "a859fde68e6d19ae",
            "authors": [
                "Weidong Huang",
                "Jingwen Zhang",
                "Jiongye Li",
                "Shibowen Zhang",
                "Jiayang Wu",
                "Jiayi Wang",
                "Hangxin Liu",
                "Yaodong Yang",
                "Yao Su"
            ],
            "affiliations": [
                "Department of Automation, Tsinghua University",
                "Department of Automation, University of Science and Technology of China",
                "Department of Computer Science, Harbin Institute of Technology",
                "Institute for Artificial Intelligence and School of Artificial Intelligence, Peking University",
                "State Key Laboratory of General Artificial Intelligence, Beijing Institute for General Artificial Intelligence (BIGAI)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.06445.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#robotics",
                    "#rl"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ½ĞµÑ€Ğ³Ğ¾Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ÑĞ²Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑˆÑ‚Ñ€Ğ°Ñ„Ğ¾Ğ² Ğ² Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğµ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ECO (Energy-Constrained Optimization) â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ñ‚Ğ´ĞµĞ»ÑĞµÑ‚ ÑĞ½ĞµÑ€Ğ³ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¾Ñ‚ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ-Ğ½ĞµÑ€Ğ°Ğ²ĞµĞ½ÑÑ‚Ğ²Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ»Ğ°Ğ³Ñ€Ğ°Ğ½Ğ¶ĞµĞ² Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ ÑĞ²Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ ÑĞ½ĞµÑ€Ğ³Ğ¾Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ³ÑƒĞ¼Ğ°Ğ½Ğ¾Ğ¸Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ñ‚Ñ€ĞµĞ±ÑƒĞµĞ¼ÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğµ BRUCE Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ECO Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ÑĞ½ĞµÑ€Ğ³Ğ¾ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ñ…Ğ¾Ğ´ÑŒĞ±Ğµ."
                },
                "en": {
                    "title": "ECO: Revolutionizing Energy-Efficient Humanoid Robot Locomotion",
                    "desc": "The paper presents a new framework called Energy-Constrained Optimization (ECO) for improving the locomotion of humanoid robots. It separates energy metrics from rewards using a Lagrangian method, allowing for clearer constraints on energy consumption. This approach reduces the need for extensive hyperparameter tuning, leading to more efficient and stable walking patterns. Experimental results show that ECO significantly lowers energy use while maintaining robust performance compared to existing methods."
                },
                "zh": {
                    "title": "èƒ½é‡çº¦æŸä¼˜åŒ–ï¼šæå‡ç±»äººæœºå™¨äººè¡Œèµ°æ•ˆç‡çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§èƒ½é‡çº¦æŸä¼˜åŒ–æ¡†æ¶ï¼ˆECOï¼‰ï¼Œæ—¨åœ¨æé«˜ç±»äººæœºå™¨äººåœ¨è¡Œèµ°æ—¶çš„èƒ½é‡æ•ˆç‡å’Œç¨³å®šæ€§ã€‚é€šè¿‡å°†èƒ½é‡ç›¸å…³æŒ‡æ ‡ä¸å¥–åŠ±åˆ†ç¦»ï¼Œå¹¶å°†å…¶é‡æ–°è¡¨è¿°ä¸ºæ˜ç¡®çš„ä¸ç­‰å¼çº¦æŸï¼ŒECOç®€åŒ–äº†è¶…å‚æ•°è°ƒä¼˜çš„è¿‡ç¨‹ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ‹‰æ ¼æœ—æ—¥æ–¹æ³•å¼•å…¥ä¸“é—¨çš„èƒ½é‡æ¶ˆè€—å’Œå‚è€ƒè¿åŠ¨çº¦æŸï¼Œä»è€Œå®ç°å¯¹ç±»äººæœºå™¨äººç¨³å®šã€å¯¹ç§°ä¸”é«˜æ•ˆçš„è¡Œèµ°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒECOåœ¨èƒ½é‡æ¶ˆè€—ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„å¤šç›®æ ‡ä¼˜åŒ–æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒäº†è‰¯å¥½çš„è¡Œèµ°æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.07803",
            "title": "SoulX-Singer: Towards High-Quality Zero-Shot Singing Voice Synthesis",
            "url": "https://huggingface.co/papers/2602.07803",
            "abstract": "A high-quality open-source singing voice synthesis system is presented with support for multiple languages and controllable generation, along with a dedicated benchmark for evaluating zero-shot performance.  \t\t\t\t\tAI-generated summary \t\t\t\t While recent years have witnessed rapid progress in speech synthesis, open-source singing voice synthesis (SVS) systems still face significant barriers to industrial deployment, particularly in terms of robustness and zero-shot generalization. In this report, we introduce SoulX-Singer, a high-quality open-source SVS system designed with practical deployment considerations in mind. SoulX-Singer supports controllable singing generation conditioned on either symbolic musical scores (MIDI) or melodic representations, enabling flexible and expressive control in real-world production workflows. Trained on more than 42,000 hours of vocal data, the system supports Mandarin Chinese, English, and Cantonese and consistently achieves state-of-the-art synthesis quality across languages under diverse musical conditions. Furthermore, to enable reliable evaluation of zero-shot SVS performance in practical scenarios, we construct SoulX-Singer-Eval, a dedicated benchmark with strict training-test disentanglement, facilitating systematic assessment in zero-shot settings.",
            "score": 2,
            "issue_id": 981,
            "pub_date": "2026-02-08",
            "pub_date_card": {
                "ru": "8 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 8",
                "zh": "2æœˆ8æ—¥"
            },
            "hash": "89a41e234847ede2",
            "authors": [
                "Jiale Qian",
                "Hao Meng",
                "Tian Zheng",
                "Pengcheng Zhu",
                "Haopeng Lin",
                "Yuhang Dai",
                "Hanke Xie",
                "Wenxiao Cao",
                "Ruixuan Shang",
                "Jun Wu",
                "Hongmei Liu",
                "Hanlin Wen",
                "Jian Zhao",
                "Zhonglin Jiang",
                "Yong Chen",
                "Shunshun Yin",
                "Ming Tao",
                "Jianguo Wei",
                "Lei Xie",
                "Xinsheng Wang"
            ],
            "affiliations": [
                "AI Center, Geely Automobile Research Institute (Ningbo) Co., Ltd., Ningbo, China",
                "Audio, Speech and Language Processing Group (ASLP@NPU), Northwestern Polytechnical University, Xian, China",
                "Audio-Visual Cognitive Computing Team, Tianjin University, Tianjin, China",
                "Soul AI Lab, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.07803.jpg",
            "data": {
                "categories": [
                    "#low_resource",
                    "#audio",
                    "#dataset",
                    "#benchmark",
                    "#open_source",
                    "#multilingual"
                ],
                "emoji": "ğŸ¤",
                "ru": {
                    "title": "ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¿ĞµĞ²Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ³Ğ¾Ğ»Ğ¾ÑĞ° Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¸ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° SoulX-Singer â€” Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¿ĞµĞ²Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ³Ğ¾Ğ»Ğ¾ÑĞ° Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼, Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ÑÑ‰Ğ°Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ² (Ğ¼Ğ°Ğ½Ğ´Ğ°Ñ€Ğ¸Ğ½ÑĞºĞ¸Ğ¹, Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ğ¹ Ğ¸ ĞºĞ°Ğ½Ñ‚Ğ¾Ğ½ÑĞºĞ¸Ğ¹) Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ MIDI-Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ¸Ğ»Ğ¸ Ğ¼ĞµĞ»Ğ¾Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 42 000 Ñ‡Ğ°ÑĞ°Ñ… Ğ²Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµÑ‡Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº SoulX-Singer-Eval Ğ´Ğ»Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ Ğ½ÑƒĞ»ĞµĞ²Ğ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ (zero-shot) Ñ Ñ‡Ñ‘Ñ‚ĞºĞ¾Ğ¹ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ¹ Ğ¸ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğº. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ±Ğ°Ñ€ÑŒĞµÑ€Ñ‹ Ğº Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¿ĞµĞ²Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ³Ğ¾Ğ»Ğ¾ÑĞ°."
                },
                "en": {
                    "title": "SoulX-Singer: Revolutionizing Singing Voice Synthesis with Flexibility and Quality",
                    "desc": "This paper presents SoulX-Singer, an advanced open-source singing voice synthesis (SVS) system that enhances the quality and flexibility of AI-generated singing. It allows for controllable singing generation based on musical scores or melodies, making it suitable for various production needs. The system is trained on a vast dataset of over 42,000 hours of vocal recordings, supporting multiple languages including Mandarin, English, and Cantonese, while achieving high synthesis quality. Additionally, the authors introduce SoulX-Singer-Eval, a benchmark designed to evaluate the system's zero-shot performance, ensuring reliable assessments in real-world applications."
                },
                "zh": {
                    "title": "é«˜è´¨é‡å¼€æºå”±æ­Œåˆæˆç³»ç»ŸSoulX-Singer",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é«˜è´¨é‡çš„å¼€æºå”±æ­Œå£°éŸ³åˆæˆç³»ç»ŸSoulX-Singerï¼Œæ”¯æŒå¤šç§è¯­è¨€å’Œå¯æ§ç”Ÿæˆã€‚è¯¥ç³»ç»Ÿèƒ½å¤Ÿæ ¹æ®ç¬¦å·éŸ³ä¹ä¹è°±ï¼ˆMIDIï¼‰æˆ–æ—‹å¾‹è¡¨ç¤ºè¿›è¡Œçµæ´»çš„å”±æ­Œç”Ÿæˆï¼Œé€‚ç”¨äºå®é™…ç”Ÿäº§å·¥ä½œæµç¨‹ã€‚SoulX-Singeråœ¨è¶…è¿‡42,000å°æ—¶çš„å£°ä¹æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæ”¯æŒæ™®é€šè¯ã€è‹±è¯­å’Œç²¤è¯­ï¼Œå¹¶åœ¨ä¸åŒéŸ³ä¹æ¡ä»¶ä¸‹å§‹ç»ˆå®ç°æœ€å…ˆè¿›çš„åˆæˆè´¨é‡ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ„å»ºäº†SoulX-Singer-EvalåŸºå‡†ï¼Œä»¥ä¾¿åœ¨å®é™…åœºæ™¯ä¸­å¯é åœ°è¯„ä¼°é›¶-shotåˆæˆæ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.07150",
            "title": "On Randomness in Agentic Evals",
            "url": "https://huggingface.co/papers/2602.07150",
            "abstract": "Analysis of agentic system evaluation reveals significant variance in single-run performance estimates, necessitating multiple runs and advanced metrics for reliable assessment.  \t\t\t\t\tAI-generated summary \t\t\t\t Agentic systems are evaluated on benchmarks where agents interact with environments to solve tasks. Most papers report a pass@1 score computed from a single run per task, assuming this gives a reliable performance estimate. We test this assumption by collecting 60,000 agentic trajectories on SWE-Bench-Verified, spanning three models and two scaffolds. We find substantial variance: single-run pass@1 estimates vary by 2.2 to 6.0 percentage points depending on which run is selected, with standard deviations exceeding 1.5 percentage points even at temperature 0. This variance has critical implications: reported improvements of 2--3 percentage points may reflect evaluation noise rather than genuine algorithmic progress. Through token-level analysis, we show that trajectories diverge early, often within the first few percent of tokens, and that these small differences cascade into different solution strategies. To enable reliable evaluation of agentic systems, we recommend three concrete practices: (1) estimate pass@1 from multiple independent runs per task, especially when measuring small improvements, (2) use statistical power analysis to determine the number of runs needed to detect expected effect sizes, and (3) consider metrics like pass@k (optimistic bound) and pass^k (pessimistic bound) with k>1 to better characterize the full performance envelope. While these practices increase evaluation cost, they are essential for distinguishing genuine scientific progress from statistical noise.",
            "score": 2,
            "issue_id": 985,
            "pub_date": "2026-02-06",
            "pub_date_card": {
                "ru": "6 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 6",
                "zh": "2æœˆ6æ—¥"
            },
            "hash": "62f6c355ea98810a",
            "authors": [
                "Bjarni Haukur Bjarnason",
                "AndrÃ© Silva",
                "Martin Monperrus"
            ],
            "affiliations": [
                "KTH Royal Institute of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.07150.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#benchmark"
                ],
                "emoji": "ğŸ²",
                "ru": {
                    "title": "ĞœĞ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ³Ğ¾Ğ½Ñ‹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾: Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒÑÑ‚ Ñ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ½Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ñ Ğ² Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ pass@1, Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ñ€Ğ¾Ğ³Ğ¾Ğ½Ñƒ: Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ°Ñ€ÑŒĞ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ½Ğ° 2,2-6,0 Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿ÑƒĞ½ĞºÑ‚Ğ° Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ğ¾Ğ½Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ€Ğ°ÑÑ…Ğ¾Ğ´ÑÑ‚ÑÑ Ğ½Ğ° Ñ€Ğ°Ğ½Ğ½Ğ¸Ñ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ…, Ğ¸ Ğ¼Ğ°Ğ»Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ñ… Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´ÑÑ‚ Ğº Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ÑĞ¼ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. ĞĞ½Ğ¸ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ³Ğ¾Ğ½Ğ¾Ğ², ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ²Ñ€Ğ¾Ğ´Ğµ pass@k Ğ¸ pass^k Ğ´Ğ»Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ° Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼."
                },
                "en": {
                    "title": "Enhancing Evaluation Reliability in Agentic Systems",
                    "desc": "This paper analyzes the evaluation methods used for agentic systems, which are AI agents that interact with environments to complete tasks. It highlights that relying on a single run to report performance scores can lead to misleading results due to significant variance in outcomes. The authors conducted extensive testing, revealing that single-run performance estimates can vary by several percentage points, indicating that reported improvements may not reflect true advancements. To improve evaluation reliability, they recommend conducting multiple runs, using statistical analysis to determine necessary run counts, and employing alternative metrics to capture a broader performance range."
                },
                "zh": {
                    "title": "æé«˜ä»£ç†ç³»ç»Ÿè¯„ä¼°çš„å¯é æ€§",
                    "desc": "æœ¬è®ºæ–‡åˆ†æäº†ä»£ç†ç³»ç»Ÿè¯„ä¼°ä¸­çš„æ˜¾è‘—æ–¹å·®ï¼ŒæŒ‡å‡ºå•æ¬¡è¿è¡Œçš„æ€§èƒ½ä¼°è®¡ä¸å¯é ï¼Œå› æ­¤éœ€è¦å¤šæ¬¡è¿è¡Œå’Œå…ˆè¿›çš„è¯„ä¼°æŒ‡æ ‡ã€‚ç ”ç©¶å‘ç°ï¼Œå•æ¬¡è¿è¡Œçš„é€šè¿‡ç‡ï¼ˆpass@1ï¼‰åœ¨ä¸åŒè¿è¡Œä¹‹é—´çš„å·®å¼‚å¯è¾¾2.2åˆ°6.0ä¸ªç™¾åˆ†ç‚¹ï¼Œæ ‡å‡†å·®è¶…è¿‡1.5ä¸ªç™¾åˆ†ç‚¹ï¼Œè¿™è¡¨æ˜æŠ¥å‘Šçš„æ”¹è¿›å¯èƒ½åªæ˜¯è¯„ä¼°å™ªå£°ï¼Œè€ŒéçœŸæ­£çš„ç®—æ³•è¿›æ­¥ã€‚é€šè¿‡å¯¹ä»£ç†è½¨è¿¹çš„åˆ†æï¼Œå‘ç°è¿™äº›è½¨è¿¹åœ¨æœ€åˆçš„å‡ ä¸ªtokenä¸­å°±å¼€å§‹åˆ†æ­§ï¼Œè¿™äº›å¾®å°çš„å·®å¼‚ä¼šå¯¼è‡´ä¸åŒçš„è§£å†³ç­–ç•¥ã€‚ä¸ºç¡®ä¿ä»£ç†ç³»ç»Ÿçš„å¯é è¯„ä¼°ï¼Œå»ºè®®é‡‡ç”¨å¤šæ¬¡ç‹¬ç«‹è¿è¡Œã€ç»Ÿè®¡åŠŸæ•ˆåˆ†æä»¥åŠä½¿ç”¨æ›´å…¨é¢çš„è¯„ä¼°æŒ‡æ ‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.06942",
            "title": "Optimal Turkish Subword Strategies at Scale: Systematic Evaluation of Data, Vocabulary, Morphology Interplay",
            "url": "https://huggingface.co/papers/2602.06942",
            "abstract": "A comprehensive study of Turkish subword tokenization systematically investigates the relationship between vocabulary size, training corpus, and tokenizer performance across multiple linguistic tasks and diagnostics.  \t\t\t\t\tAI-generated summary \t\t\t\t Tokenization is a pivotal design choice for neural language modeling in morphologically rich languages (MRLs) such as Turkish, where productive agglutination challenges both vocabulary efficiency and morphological fidelity. Prior studies have explored tokenizer families and vocabulary sizes but typically (i) vary vocabulary without systematically controlling the tokenizer's training corpus, (ii) provide limited intrinsic diagnostics, and (iii) evaluate a narrow slice of downstream tasks. We present the first comprehensive, principled study of Turkish subword tokenization; a \"subwords manifest\", that jointly varies vocabulary size and tokenizer training corpus size (data and vocabulary coupling), compares multiple tokenizer families under matched parameter budgets (WordPiece, morphology level, and character baselines), and evaluates across semantic (NLI, STS, sentiment analysis, NER), syntactic (POS, dependency parsing), and morphology-sensitive probes. To explain why tokenizers succeed or fail, we introduce a morphology-aware diagnostic toolkit that goes beyond coarse aggregates to boundary-level micro/macro F1, decoupled lemma atomicity vs. surface boundary hits, over/under-segmentation indices, character/word edit distances (CER/WER), continuation rates, and affix-type coverage and token-level atomicity. Our contributions are fourfold: (i) a systematic investigation of the vocabulary-corpus-success triad; (ii) a unified, morphology-aware evaluation framework linking intrinsic diagnostics to extrinsic outcomes; (iii) controlled comparisons identifying when character-level and morphology-level tokenization pay off; and (iv) an open-source release of evaluation code, tokenizer pipelines, and models. As the first work of its kind, this \"subwords manifest\" delivers actionable guidance for building effective tokenizers in MRLs and establishes a reproducible foundation for future research.",
            "score": 2,
            "issue_id": 988,
            "pub_date": "2026-02-06",
            "pub_date_card": {
                "ru": "6 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 6",
                "zh": "2æœˆ6æ—¥"
            },
            "hash": "530f77884820f18e",
            "authors": [
                "Duygu Altinok"
            ],
            "affiliations": [
                "Independent Researcher, Berlin, Germany"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.06942.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#interpretability",
                    "#multilingual",
                    "#low_resource",
                    "#data",
                    "#benchmark"
                ],
                "emoji": "ğŸ”¤",
                "ru": {
                    "title": "Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑÑƒĞ±ÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ¾Ñ€Ñ„Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑƒĞ±ÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ‚ÑƒÑ€ĞµÑ†ĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ÑÑ Ğº Ğ¼Ğ¾Ñ€Ñ„Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ğ¼ ÑĞ·Ñ‹ĞºĞ°Ğ¼ Ñ Ğ°Ğ³Ğ³Ğ»ÑÑ‚Ğ¸Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¾Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ°Ñ€ÑŒĞ¸Ñ€ÑƒÑÑ‚ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ², ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² (WordPiece, Ğ¼Ğ¾Ñ€Ñ„Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ, ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ) Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¸Ñ… Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡: ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… (NLI, STS, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğ¹, NER), ÑĞ¸Ğ½Ñ‚Ğ°ĞºÑĞ¸Ñ‡ĞµÑĞºĞ¸Ñ… (POS, ÑĞ¸Ğ½Ñ‚Ğ°ĞºÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ñ€Ğ°Ğ·Ğ±Ğ¾Ñ€) Ğ¸ Ğ¼Ğ¾Ñ€Ñ„Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ…. Ğ”Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ²Ğ²ĞµĞ´ĞµĞ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ¼Ğ¾Ñ€Ñ„ĞµĞ¼ Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ¼Ğ¸ĞºÑ€Ğ¾/Ğ¼Ğ°ĞºÑ€Ğ¾ F1, Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ğ¼Ğ¸ Ğ½ĞµĞ´Ğ¾- Ğ¸ Ğ¿ĞµÑ€Ğµ-ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑĞ¼Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¼Ğ¾Ñ€Ñ„Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ ĞºĞ¾Ğ´ Ğ´Ğ»Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Unlocking Effective Tokenization for Turkish Language Models",
                    "desc": "This paper presents a detailed study on subword tokenization for Turkish, a morphologically rich language. It explores how vocabulary size and the training corpus affect the performance of different tokenizers across various linguistic tasks. The authors introduce a new diagnostic toolkit that evaluates tokenizers at a granular level, providing insights into their strengths and weaknesses. This research aims to improve tokenizer design in Turkish and offers a framework for future studies in similar languages."
                },
                "zh": {
                    "title": "åœŸè€³å…¶è¯­å­è¯åˆ†è¯çš„å…¨é¢ç ”ç©¶",
                    "desc": "è¿™ç¯‡è®ºæ–‡å¯¹åœŸè€³å…¶è¯­çš„å­è¯åˆ†è¯è¿›è¡Œäº†å…¨é¢ç ”ç©¶ï¼Œæ¢è®¨äº†è¯æ±‡å¤§å°ã€è®­ç»ƒè¯­æ–™å’Œåˆ†è¯å™¨æ€§èƒ½ä¹‹é—´çš„å…³ç³»ã€‚ç ”ç©¶è¡¨æ˜ï¼Œåœ¨å½¢æ€ä¸°å¯Œçš„è¯­è¨€ä¸­ï¼Œåˆ†è¯è®¾è®¡å¯¹ç¥ç»è¯­è¨€æ¨¡å‹è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†è¯æ±‡æ•ˆç‡å’Œå½¢æ€å¿ å®æ€§æ—¶ã€‚ä½œè€…æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æ¡†æ¶ï¼Œç»“åˆäº†å¤šç§åˆ†è¯å™¨ï¼Œå¹¶åœ¨å¤šä¸ªè¯­è¨€ä»»åŠ¡ä¸Šè¿›è¡Œäº†æ¯”è¾ƒï¼Œä»¥è¯„ä¼°å…¶æ•ˆæœã€‚è¯¥ç ”ç©¶ä¸ºæ„å»ºæœ‰æ•ˆçš„åˆ†è¯å™¨æä¾›äº†å®ç”¨æŒ‡å¯¼ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶å¥ å®šäº†å¯é‡å¤çš„åŸºç¡€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.06600",
            "title": "Echoes as Anchors: Probabilistic Costs and Attention Refocusing in LLM Reasoning",
            "url": "https://huggingface.co/papers/2602.06600",
            "abstract": "Large reasoning models exhibit spontaneous question repetition patterns that can be formalized and leveraged to improve computational efficiency and accuracy through echo-aware training and prompting techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Test-time compute allocation in large reasoning models (LRMs) is widely used and has applications in mathematical problem solving, code synthesis, and planning. Recent work has addressed this problem by scaling self-consistency and parallel thinking, adding generic ``thinking tokens'' and prompting models to re-read the question before answering. Unfortunately, these approaches either inject task-agnostic tokens or mandate heuristics that do not explain -- and often ignore -- the spontaneous repetition that many LRMs exhibit at the head of their internal chains. In contrast, we analyze and harness the model's tendency to restate the question, which we term the Echo of Prompt (EOP), as a front-loaded, compute-shaping mechanism. We formalize its probabilistic cost by casting echo removal as rejection-based conditioning and defining the Echo Likelihood Gap Î”L as a computable proxy. This provides the missing theoretical link that links early repetition to likelihood gains and downstream accuracy. However, it does not by itself specify how to exploit EOP. Consequently, we develop Echo-Distilled SFT (ED-SFT) to instill an ``echo-then-reason'' pattern through supervised finetuning, and Echoic Prompting (EP) to re-ground the model mid-trace without training. While promising, quantifying benefits beyond verbosity is non-trivial. Therefore, we conduct length and suffix-controlled likelihood analyses together with layer-wise attention studies, showing that EOP increases answer to answer-prefix attention in middle layers, consistent with an attention refocusing mechanism. We evaluate on GSM8K, MathQA, Hendrycks-MATH, AIME24, and MATH-500 under identical decoding settings and budgets, and find consistent gains over baselines. Code is available at https://github.com/hhh2210/echoes-as-anchors.",
            "score": 2,
            "issue_id": 986,
            "pub_date": "2026-02-06",
            "pub_date_card": {
                "ru": "6 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 6",
                "zh": "2æœˆ6æ—¥"
            },
            "hash": "7852ce687779c2fe",
            "authors": [
                "Zhuoyuan Hao",
                "Zhuo Li",
                "Wu Li",
                "Fangming Liu",
                "Min Zhang",
                "Jing Li"
            ],
            "affiliations": [
                "Harbin Institute of Technology, Shenzhen, China",
                "Huazhong University of Science and Technology, China",
                "Pengcheng Laboratory, Shenzhen, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.06600.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#training",
                    "#reasoning",
                    "#open_source",
                    "#inference",
                    "#interpretability",
                    "#optimization"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ Ğ¸Ğ½Ñ‚ÑƒĞ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ² ÑĞºĞ¾Ñ€ÑŒ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¾Ğ½Ñ‚Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ° Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Â«Ğ­Ñ…Ğ¾ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸Â». ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒÑÑ‚ ÑÑ‚Ğ¾ ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¸ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Gap Ğ’ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ­Ñ…Ğ¾ Ğ´Ğ»Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ°. Ğ”Ğ»Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ´Ğ²Ğ° Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°: Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ ED-SFT Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ° Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ EP, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ° ÑÑ‡Ñ‘Ñ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ¿ĞµÑ€ĞµĞ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑĞ»Ğ¾ÑÑ… ÑĞµÑ‚Ğ¸."
                },
                "en": {
                    "title": "Harnessing Echoes for Smarter AI Reasoning",
                    "desc": "This paper explores how large reasoning models (LRMs) often repeat questions during their processing, a behavior termed the Echo of Prompt (EOP). The authors propose that this repetition can be utilized to enhance the models' computational efficiency and accuracy through new training and prompting methods. They introduce Echo-Distilled SFT (ED-SFT) for supervised fine-tuning and Echoic Prompting (EP) to improve model performance without additional training. Their experiments demonstrate that leveraging EOP leads to better attention mechanisms and improved results on various mathematical problem-solving benchmarks."
                },
                "zh": {
                    "title": "åˆ©ç”¨å›å£°æå‡æ¨ç†æ¨¡å‹çš„æ•ˆç‡ä¸å‡†ç¡®æ€§",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤§å‹æ¨ç†æ¨¡å‹åœ¨å›ç­”é—®é¢˜æ—¶è‡ªå‘é‡å¤é—®é¢˜çš„ç°è±¡ï¼Œå¹¶æå‡ºäº†åˆ©ç”¨è¿™ç§ç°è±¡æ¥æé«˜è®¡ç®—æ•ˆç‡å’Œå‡†ç¡®æ€§çš„æ–¹æ³•ã€‚ç ”ç©¶è€…ä»¬å®šä¹‰äº†â€œæç¤ºçš„å›å£°â€ï¼ˆEcho of Prompt, EOPï¼‰ï¼Œå¹¶å°†å…¶è§†ä¸ºä¸€ç§è®¡ç®—ä¼˜åŒ–æœºåˆ¶ã€‚é€šè¿‡å›å£°å»é™¤çš„æ¦‚ç‡æˆæœ¬åˆ†æï¼Œè®ºæ–‡å»ºç«‹äº†æ—©æœŸé‡å¤ä¸å‡†ç¡®æ€§æå‡ä¹‹é—´çš„ç†è®ºè”ç³»ã€‚æœ€åï¼Œä½œè€…æå‡ºäº†å›å£°è’¸é¦çš„ç›‘ç£å¾®è°ƒæ–¹æ³•ï¼ˆED-SFTï¼‰å’Œå›å£°æç¤ºï¼ˆEPï¼‰ï¼Œä»¥ä¼˜åŒ–æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.08818",
            "title": "FlexMoRE: A Flexible Mixture of Rank-heterogeneous Experts for Efficient Federatedly-trained Large Language Models",
            "url": "https://huggingface.co/papers/2602.08818",
            "abstract": "FlexMoRE demonstrates that low-rank adapters can replace full-sized experts in mixture-of-experts architectures, achieving better performance with significantly fewer parameters.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in mixture-of-experts architectures have shown that individual experts models can be trained federatedly, i.e., in isolation from other experts by using a common base model to facilitate coordination. However, we hypothesize that full-sized experts may not be necessary for all domains and that instead low-rank adapters may be sufficient. Here, we introduce FlexMoRE, a Flexible Mixture of Rank-heterogenous Experts, which may be either full-sized experts or adapters of a suitable rank. We systematically investigate the trade-off between expert rank and downstream task performance by evaluating 6 experts with ranks 2^0 to 2^{14} resulting in experiments covering 150 mixtures (96 with 2 experts, 54 with 7 experts) that are evaluated across 120 tasks. For our experiments, we build on FlexOlmo and turn its pre-trained experts into low-rank versions. Our regression analysis from expert rank to downstream task performance reveals that the best-performing rank is substantially higher for reasoning-heavy benchmarks than for knowledge-heavy benchmarks. These findings on rank sensitivity come with direct implications for memory efficiency: Using optimal ranks, FlexMoRE yields improved downstream task performance (average score 47.18) compared to the baseline FlexOlmo-style mixture of full-sized experts (average score 45.46) at less than one third the parameters (10.75B for FlexMoRE vs. 33.27B for FlexOlmo). All code will be made available.",
            "score": 1,
            "issue_id": 984,
            "pub_date": "2026-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "9e6c58cd1bfae43d",
            "authors": [
                "Annemette Brok Pirchert",
                "Jacob Nielsen",
                "Mogens Henrik From",
                "Lukas Galke Poech",
                "Peter Schneider-Kamp"
            ],
            "affiliations": [
                "Ordbogen A/S",
                "University of Southern Denmark"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.08818.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#open_source",
                    "#reasoning"
                ],
                "emoji": "ğŸ›ï¸",
                "ru": {
                    "title": "ĞĞ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ²Ñ‹Ğµ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ñ‹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ² ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° FlexMoRE, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ²Ñ‹Ğµ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ñ‹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ² ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ¼ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ° Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ¾Ñ†ĞµĞ½Ğ¸Ğ² 150 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğ¹ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ½Ğ° 120 Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ½Ğ³ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ÑÑ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ñ‡ĞµĞ¼ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµĞ½ĞµĞµ Ñ‚Ñ€ĞµÑ‚Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "FlexMoRE: Efficient Mixture of Experts with Low-Rank Adapters",
                    "desc": "FlexMoRE introduces a novel approach to mixture-of-experts architectures by utilizing low-rank adapters instead of full-sized experts, leading to enhanced performance with fewer parameters. The study investigates the relationship between the rank of experts and their effectiveness across various tasks, revealing that lower ranks can suffice in many scenarios. Through extensive experiments, FlexMoRE demonstrates that optimal ranks can significantly improve task performance while reducing memory usage. This research highlights the potential for more efficient models in machine learning by leveraging rank-heterogeneous experts."
                },
                "zh": {
                    "title": "ä½ç§©é€‚é…å™¨ï¼Œä¸“å®¶æ¶æ„çš„æ–°é€‰æ‹©",
                    "desc": "FlexMoREå±•ç¤ºäº†ä½ç§©é€‚é…å™¨å¯ä»¥æ›¿ä»£å…¨å°ºå¯¸ä¸“å®¶åœ¨æ··åˆä¸“å®¶æ¶æ„ä¸­çš„ä½œç”¨ï¼Œå¹¶åœ¨å‚æ•°æ˜¾è‘—å‡å°‘çš„æƒ…å†µä¸‹å®ç°æ›´å¥½çš„æ€§èƒ½ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå¹¶éæ‰€æœ‰é¢†åŸŸéƒ½éœ€è¦å…¨å°ºå¯¸ä¸“å®¶ï¼Œä½ç§©é€‚é…å™¨åœ¨è®¸å¤šæƒ…å†µä¸‹è¶³å¤Ÿä½¿ç”¨ã€‚é€šè¿‡å¯¹ä¸åŒç§©çš„ä¸“å®¶è¿›è¡Œç³»ç»Ÿè¯„ä¼°ï¼Œå‘ç°æ¨ç†å¯†é›†å‹åŸºå‡†çš„æœ€ä½³ç§©æ˜¾è‘—é«˜äºçŸ¥è¯†å¯†é›†å‹åŸºå‡†ã€‚æœ€ç»ˆï¼ŒFlexMoREåœ¨å†…å­˜æ•ˆç‡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½¿ç”¨æœ€ä¼˜ç§©çš„æƒ…å†µä¸‹ï¼Œå…¶ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ä¼˜äºä¼ ç»Ÿçš„å…¨å°ºå¯¸ä¸“å®¶æ¶æ„ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.07970",
            "title": "Learning-guided Kansa collocation for forward and inverse PDEs beyond linearity",
            "url": "https://huggingface.co/papers/2602.07970",
            "abstract": "Research explores PDE solvers including neural frameworks for scientific simulations, examining forward solutions, inverse problems, and equation discovery across multi-variable and non-linear systems.  \t\t\t\t\tAI-generated summary \t\t\t\t Partial Differential Equations are precise in modelling the physical, biological and graphical phenomena. However, the numerical methods suffer from the curse of dimensionality, high computation costs and domain-specific discretization. We aim to explore pros and cons of different PDE solvers, and apply them to specific scientific simulation problems, including forwarding solution, inverse problems and equations discovery. In particular, we extend the recent CNF (NeurIPS 2023) framework solver to multi-dependent-variable and non-linear settings, together with down-stream applications. The outcomes include implementation of selected methods, self-tuning techniques, evaluation on benchmark problems and a comprehensive survey of neural PDE solvers and scientific simulation applications.",
            "score": 1,
            "issue_id": 982,
            "pub_date": "2026-02-08",
            "pub_date_card": {
                "ru": "8 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 8",
                "zh": "2æœˆ8æ—¥"
            },
            "hash": "11607ec65d9805f1",
            "authors": [
                "Zheyuan Hu",
                "Weitao Chen",
                "Cengiz Ã–ztireli",
                "Chenliang Zhou",
                "Fangcheng Zhong"
            ],
            "affiliations": [
                "University of Cambridge, UK"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.07970.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ§®",
                "ru": {
                    "title": "ĞĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ñ‹Ğµ Ñ€ĞµÑˆĞ°Ñ‚ĞµĞ»Ğ¸ ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ‡Ğ°ÑÑ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ‡Ğ°ÑÑ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ½Ñ‹Ñ… (Ğ£Ğ§ĞŸ) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ´Ğ»Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€ÑĞ¼Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¸ Ğ½ĞµĞ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…. ĞĞ½Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑÑ‚ Ğ½ĞµĞ´Ğ°Ğ²Ğ½ÑÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ CNF Ğ½Ğ° ÑĞ»ÑƒÑ‡Ğ°Ğ¸ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ğ¼Ğ¸ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ½ĞµĞ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ğ¼Ğ¸. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ ÑĞ°Ğ¼Ğ¾Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ñ‹Ñ… Ñ€ĞµÑˆĞ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ£Ğ§ĞŸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ² Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Harnessing Neural Networks for Advanced PDE Solutions",
                    "desc": "This paper investigates the use of neural network frameworks for solving Partial Differential Equations (PDEs) in scientific simulations. It addresses challenges such as high computational costs and the curse of dimensionality that traditional numerical methods face. The research extends the recent Continuous Normalizing Flow (CNF) framework to handle multi-variable and non-linear PDEs, focusing on applications like forward solutions, inverse problems, and equation discovery. The findings include the implementation of various methods, self-tuning techniques, and a thorough evaluation of neural PDE solvers in practical scenarios."
                },
                "zh": {
                    "title": "æ¢ç´¢åå¾®åˆ†æ–¹ç¨‹æ±‚è§£å™¨çš„æœªæ¥",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†åå¾®åˆ†æ–¹ç¨‹ï¼ˆPDEï¼‰æ±‚è§£å™¨ï¼ŒåŒ…æ‹¬ç”¨äºç§‘å­¦æ¨¡æ‹Ÿçš„ç¥ç»ç½‘ç»œæ¡†æ¶ã€‚æˆ‘ä»¬åˆ†æäº†ä¸åŒPDEæ±‚è§£å™¨çš„ä¼˜ç¼ºç‚¹ï¼Œå¹¶å°†å…¶åº”ç”¨äºç‰¹å®šçš„ç§‘å­¦æ¨¡æ‹Ÿé—®é¢˜ï¼Œå¦‚æ­£å‘è§£ã€é€†é—®é¢˜å’Œæ–¹ç¨‹å‘ç°ã€‚ç‰¹åˆ«åœ°ï¼Œæˆ‘ä»¬æ‰©å±•äº†æœ€è¿‘çš„CNFæ¡†æ¶æ±‚è§£å™¨ï¼Œä»¥é€‚åº”å¤šå˜é‡å’Œéçº¿æ€§è®¾ç½®ï¼Œå¹¶è¿›è¡Œä¸‹æ¸¸åº”ç”¨ã€‚ç ”ç©¶ç»“æœåŒ…æ‹¬æ‰€é€‰æ–¹æ³•çš„å®ç°ã€è‡ªè°ƒæŠ€æœ¯ã€åŸºå‡†é—®é¢˜çš„è¯„ä¼°ä»¥åŠå¯¹ç¥ç»PDEæ±‚è§£å™¨å’Œç§‘å­¦æ¨¡æ‹Ÿåº”ç”¨çš„å…¨é¢è°ƒæŸ¥ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.07491",
            "title": "GraphAgents: Knowledge Graph-Guided Agentic AI for Cross-Domain Materials Design",
            "url": "https://huggingface.co/papers/2602.07491",
            "abstract": "A multi-agent framework guided by knowledge graphs addresses materials science challenges by integrating specialized agents for problem decomposition, evidence retrieval, and graph traversal to discover sustainable PFAS alternatives.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) promise to accelerate discovery by reasoning across the expanding scientific landscape. Yet, the challenge is no longer access to information but connecting it in meaningful, domain-spanning ways. In materials science, where innovation demands integrating concepts from molecular chemistry to mechanical performance, this is especially acute. Neither humans nor single-agent LLMs can fully contend with this torrent of information, with the latter often prone to hallucinations. To address this bottleneck, we introduce a multi-agent framework guided by large-scale knowledge graphs to find sustainable substitutes for per- and polyfluoroalkyl substances (PFAS)-chemicals currently under intense regulatory scrutiny. Agents in the framework specialize in problem decomposition, evidence retrieval, design parameter extraction, and graph traversal, uncovering latent connections across distinct knowledge pockets to support hypothesis generation. Ablation studies show that the full multi-agent pipeline outperforms single-shot prompting, underscoring the value of distributed specialization and relational reasoning. We demonstrate that by tailoring graph traversal strategies, the system alternates between exploitative searches focusing on domain-critical outcomes and exploratory searches surfacing emergent cross-connections. Illustrated through the exemplar of biomedical tubing, the framework generates sustainable PFAS-free alternatives that balance tribological performance, thermal stability, chemical resistance, and biocompatibility. This work establishes a framework combining knowledge graphs with multi-agent reasoning to expand the materials design space, showcasing several initial design candidates to demonstrate the approach.",
            "score": 1,
            "issue_id": 984,
            "pub_date": "2026-02-07",
            "pub_date_card": {
                "ru": "7 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 7",
                "zh": "2æœˆ7æ—¥"
            },
            "hash": "aaf4c7d64d18aaf8",
            "authors": [
                "Isabella A. Stewart",
                "Tarjei Paule Hage",
                "Yu-Chuan Hsu",
                "Markus J. Buehler"
            ],
            "affiliations": [
                "Massachusetts Institute of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.07491.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#science",
                    "#reasoning"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "ĞœĞ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ³Ñ€Ğ°Ñ„Ğ°Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ²",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ°Ñ Ğ³Ñ€Ğ°Ñ„Ğ°Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼, Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ² Ğ¸ Ğ¾Ğ±Ñ…Ğ¾Ğ´Ğ° Ğ³Ñ€Ğ°Ñ„Ğ° Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ñ Ñ†ĞµĞ»ÑŒÑ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ñ‹Ñ… Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ² PFAS. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğµ Ğ¾Ğ´Ğ½Ğ¾ĞºÑ€Ğ°Ñ‚Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾Ğ¹ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ñ‡ĞµÑ€ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¸ Ñ€Ğ°Ğ·Ğ²ĞµĞ´Ñ‹Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¸Ğ¾ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ñ‹Ğµ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ñ‹ Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¿Ğ¾ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ñƒ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "Harnessing Multi-Agent Intelligence for Sustainable Materials Discovery",
                    "desc": "This paper presents a multi-agent framework that utilizes knowledge graphs to tackle challenges in materials science, specifically in finding sustainable alternatives to PFAS chemicals. The framework consists of specialized agents that perform tasks such as problem decomposition, evidence retrieval, and graph traversal, enabling them to uncover hidden connections in diverse knowledge areas. By employing a combination of exploitative and exploratory search strategies, the system enhances hypothesis generation and improves the discovery process. The results indicate that this approach outperforms traditional single-agent methods, highlighting the importance of distributed specialization and relational reasoning in materials design."
                },
                "zh": {
                    "title": "å¤šæ™ºèƒ½ä½“æ¡†æ¶åŠ©åŠ›ææ–™ç§‘å­¦åˆ›æ–°",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œåˆ©ç”¨çŸ¥è¯†å›¾è°±æ¥è§£å†³ææ–™ç§‘å­¦ä¸­çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸“é—¨åŒ–çš„æ™ºèƒ½ä½“è¿›è¡Œé—®é¢˜åˆ†è§£ã€è¯æ®æ£€ç´¢å’Œå›¾éå†ï¼Œä»¥å‘ç°å¯æŒç»­çš„PFASæ›¿ä»£å“ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè¿™ç§å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨å¤„ç†ä¿¡æ¯æ—¶ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°è¿›è¡Œå…³ç³»æ¨ç†å’Œåˆ†å¸ƒå¼ä¸“ä¸šåŒ–ï¼Œä»è€Œè¶…è¶Šå•ä¸€æ™ºèƒ½ä½“çš„å±€é™ã€‚é€šè¿‡å…·ä½“æ¡ˆä¾‹ï¼Œå±•ç¤ºäº†è¯¥æ¡†æ¶åœ¨ç”Ÿç‰©åŒ»å­¦ç®¡é“è®¾è®¡ä¸­çš„åº”ç”¨ï¼Œç”Ÿæˆäº†åœ¨æ‘©æ“¦æ€§èƒ½ã€çƒ­ç¨³å®šæ€§ã€åŒ–å­¦æŠ—æ€§å’Œç”Ÿç‰©ç›¸å®¹æ€§æ–¹é¢çš„å¯æŒç»­æ›¿ä»£å“ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.07120",
            "title": "Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model",
            "url": "https://huggingface.co/papers/2602.07120",
            "abstract": "Anchor decoding suppresses verbatim copying in language models while maintaining fluency and factual accuracy through constrained generation that balances risk and utility.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern language models (LMs) tend to memorize portions of their training data and emit verbatim spans. When the underlying sources are sensitive or copyright-protected, such reproduction raises issues of consent and compensation for creators and compliance risks for developers. We propose Anchored Decoding, a plug-and-play inference-time method for suppressing verbatim copying: it enables decoding from any risky LM trained on mixed-license data by keeping generation in bounded proximity to a permissively trained safe LM. Anchored Decoding adaptively allocates a user-chosen information budget over the generation trajectory and enforces per-step constraints that yield a sequence-level guarantee, enabling a tunable risk-utility trade-off. To make Anchored Decoding practically useful, we introduce a new permissively trained safe model (TinyComma 1.8B), as well as Anchored_{Byte} Decoding, a byte-level variant of our method that enables cross-vocabulary fusion via the ByteSampler framework (Hayase et al., 2025). We evaluate our methods across six model pairs on long-form evaluations of copyright risk and utility. Anchored and Anchored_{Byte} Decoding define a new Pareto frontier, preserving near-original fluency and factuality while eliminating up to 75% of the measurable copying gap (averaged over six copying metrics) between the risky baseline and a safe reference, at a modest inference overhead.",
            "score": 1,
            "issue_id": 991,
            "pub_date": "2026-02-06",
            "pub_date_card": {
                "ru": "6 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 6",
                "zh": "2æœˆ6æ—¥"
            },
            "hash": "2d163f9f900c054d",
            "authors": [
                "Jacqueline He",
                "Jonathan Hayase",
                "Wen-tau Yih",
                "Sewoong Oh",
                "Luke Zettlemoyer",
                "Pang Wei Koh"
            ],
            "affiliations": [
                "Allen Institute for Artificial Intelligence",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.07120.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#security",
                    "#leakage"
                ],
                "emoji": "ğŸ”’",
                "ru": {
                    "title": "Ğ¯ĞºĞ¾Ñ€Ğ½Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ: ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ±ĞµĞ· Ğ½Ğ°Ñ€ÑƒÑˆĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ°Ğ²",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Anchored Decoding, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğµ ĞºĞ¾Ğ¿Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ· Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ, ÑƒĞ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°Ñ ĞµÑ‘ Ğ±Ğ»Ğ¸Ğ·ĞºĞ¾ Ğº Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞ°ÑÑ‰ĞµĞ¹ Ğ»Ğ¸Ñ†ĞµĞ½Ğ·Ğ¸ĞµĞ¹. ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ±ÑĞ´Ğ¶ĞµÑ‚ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ½Ğ° Ğ²ÑĞµĞ¹ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ñ€Ğ¸ÑĞºĞ° ĞºĞ¾Ğ¿Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑƒÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ´Ğ¾ 75% Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ´Ğ¾ÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ¿Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ„Ğ°ĞºÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°."
                },
                "en": {
                    "title": "Anchored Decoding: Balancing Safety and Fluency in Language Models",
                    "desc": "This paper introduces Anchored Decoding, a method designed to reduce verbatim copying in language models while ensuring fluency and factual accuracy. It works by constraining the generation process to stay close to a safely trained model, allowing for a balance between risk and utility. The method allows users to set an information budget, which helps manage how much sensitive content can be generated. The authors also present a new safe model, TinyComma 1.8B, and a byte-level variant, Anchored_{Byte} Decoding, which together significantly decrease the risk of copyright infringement while maintaining high-quality output."
                },
                "zh": {
                    "title": "é”šå®šè§£ç ï¼šå¹³è¡¡é£é™©ä¸æ•ˆç”¨çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºé”šå®šè§£ç ï¼ˆAnchored Decodingï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨æŠ‘åˆ¶è¯­è¨€æ¨¡å‹ä¸­çš„é€å­—å¤åˆ¶ï¼ŒåŒæ—¶ä¿æŒç”Ÿæˆæ–‡æœ¬çš„æµç•…æ€§å’Œäº‹å®å‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ä¿æŒä¸å®‰å…¨æ¨¡å‹çš„æ¥è¿‘ï¼Œæ¥å¹³è¡¡é£é™©å’Œæ•ˆç”¨ï¼Œé€‚ç”¨äºæ··åˆè®¸å¯æ•°æ®è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ã€‚é”šå®šè§£ç å…è®¸ç”¨æˆ·é€‰æ‹©ä¿¡æ¯é¢„ç®—ï¼Œå¹¶åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­æ–½åŠ é€æ­¥çº¦æŸï¼Œä»è€Œå®ç°å¯è°ƒçš„é£é™©-æ•ˆç”¨æƒè¡¡ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ–°çš„å®‰å…¨æ¨¡å‹ï¼ˆTinyComma 1.8Bï¼‰å’Œå­—èŠ‚çº§å˜ä½“ï¼ˆAnchored_{Byte} Decodingï¼‰ï¼Œå¹¶åœ¨å¤šä¸ªæ¨¡å‹å¯¹ä¸Šè¯„ä¼°äº†å…¶åœ¨ç‰ˆæƒé£é™©å’Œæ•ˆç”¨æ–¹é¢çš„è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.07090",
            "title": "Concept-Aware Privacy Mechanisms for Defending Embedding Inversion Attacks",
            "url": "https://huggingface.co/papers/2602.07090",
            "abstract": "SPARSE is a user-centric framework that protects text embeddings from privacy leaks by selectively perturbing sensitive dimensions using differentiable masking and Mahalanobis noise calibration.  \t\t\t\t\tAI-generated summary \t\t\t\t Text embeddings enable numerous NLP applications but face severe privacy risks from embedding inversion attacks, which can expose sensitive attributes or reconstruct raw text. Existing differential privacy defenses assume uniform sensitivity across embedding dimensions, leading to excessive noise and degraded utility. We propose SPARSE, a user-centric framework for concept-specific privacy protection in text embeddings. SPARSE combines (1) differentiable mask learning to identify privacy-sensitive dimensions for user-defined concepts, and (2) the Mahalanobis mechanism that applies elliptical noise calibrated by dimension sensitivity. Unlike traditional spherical noise injection, SPARSE selectively perturbs privacy-sensitive dimensions while preserving non-sensitive semantics. Evaluated across six datasets with three embedding models and attack scenarios, SPARSE consistently reduces privacy leakage while achieving superior downstream performance compared to state-of-the-art DP methods.",
            "score": 1,
            "issue_id": 981,
            "pub_date": "2026-02-06",
            "pub_date_card": {
                "ru": "6 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 6",
                "zh": "2æœˆ6æ—¥"
            },
            "hash": "988a2e9f31f78d97",
            "authors": [
                "Yu-Che Tsai",
                "Hsiang Hsiao",
                "Kuan-Yu Chen",
                "Shou-De Lin"
            ],
            "affiliations": [
                "Department of Computer Science and Information Engineering, National Taiwan University",
                "National Taiwan University AI Center of Research Excellence"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.07090.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#leakage"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ°Ñ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ° ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ²: Ğ¸Ğ·Ğ±Ğ¸Ñ€Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ ÑˆÑƒĞ¼ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑĞ»ĞµĞ¿Ğ¾Ğ³Ğ¾",
                    "desc": "SPARSE â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, Ğ·Ğ°Ñ‰Ğ¸Ñ‰Ğ°ÑÑ‰Ğ¸Ğ¹ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ¾Ñ‚ ÑƒÑ‚ĞµÑ‡ĞµĞº Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¸Ğ·Ğ±Ğ¸Ñ€Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸Ñ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²ĞºÑƒ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ĞµĞ¹, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ñ… ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğµ, Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ĞœĞ°Ñ…Ğ°Ğ»Ğ°Ğ½Ğ¾Ğ±Ğ¸ÑĞ° Ğ´Ğ»Ñ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¸ ÑĞ»Ğ»Ğ¸Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑˆÑƒĞ¼Ğ°. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑÑ„ĞµÑ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑˆÑƒĞ¼Ğ°, SPARSE ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½ĞµĞ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ñ‹Ñ… ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ñ€Ğ¸ÑĞº Ğ°Ñ‚Ğ°Ğº Ğ½Ğ° Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ², Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ½Ğ° Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "SPARSE: Tailored Privacy for Text Embeddings",
                    "desc": "SPARSE is a framework designed to enhance the privacy of text embeddings by focusing on user-defined sensitive dimensions. It uses differentiable masking to identify which dimensions of the embeddings are sensitive and need protection. Additionally, it employs Mahalanobis noise calibration to apply targeted noise to these sensitive dimensions, rather than uniformly across all dimensions. This approach minimizes privacy leakage while maintaining the utility of the embeddings for various natural language processing tasks."
                },
                "zh": {
                    "title": "SPARSEï¼šä¿æŠ¤æ–‡æœ¬åµŒå…¥éšç§çš„æ–°æ–¹æ³•",
                    "desc": "SPARSEæ˜¯ä¸€ä¸ªä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒçš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡é€‰æ‹©æ€§åœ°æ‰°åŠ¨æ•æ„Ÿç»´åº¦æ¥ä¿æŠ¤æ–‡æœ¬åµŒå…¥çš„éšç§ã€‚å®ƒç»“åˆäº†å¯å¾®åˆ†æ©ç å­¦ä¹ å’Œé©¬å“ˆæ‹‰è¯ºæ¯”æ–¯å™ªå£°æ ¡å‡†ï¼Œèƒ½å¤Ÿè¯†åˆ«ç”¨æˆ·å®šä¹‰æ¦‚å¿µçš„éšç§æ•æ„Ÿç»´åº¦ã€‚ä¸ä¼ ç»Ÿçš„å‡åŒ€æ•æ„Ÿæ€§å‡è®¾ä¸åŒï¼ŒSPARSEåœ¨ä¿æŒéæ•æ„Ÿè¯­ä¹‰çš„åŒæ—¶ï¼Œä¸“æ³¨äºæ‰°åŠ¨éšç§æ•æ„Ÿçš„ç»´åº¦ã€‚ç»è¿‡åœ¨å…­ä¸ªæ•°æ®é›†å’Œä¸‰ç§åµŒå…¥æ¨¡å‹çš„è¯„ä¼°ï¼ŒSPARSEåœ¨å‡å°‘éšç§æ³„éœ²çš„åŒæ—¶ï¼Œè¡¨ç°å‡ºä¼˜äºç°æœ‰å·®åˆ†éšç§æ–¹æ³•çš„ä¸‹æ¸¸æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.07080",
            "title": "CodeCircuit: Toward Inferring LLM-Generated Code Correctness via Attribution Graphs",
            "url": "https://huggingface.co/papers/2602.07080",
            "abstract": "LLM code verification can be achieved through internal neural dynamics analysis, identifying structural signatures that distinguish correct reasoning from logical failures in computational circuits.  \t\t\t\t\tAI-generated summary \t\t\t\t Current paradigms for code verification rely heavily on external mechanisms-such as execution-based unit tests or auxiliary LLM judges-which are often labor-intensive or limited by the judging model's own capabilities. This raises a fundamental, yet unexplored question: Can an LLM's functional correctness be assessed purely from its internal computational structure? Our primary objective is to investigate whether the model's neural dynamics encode internally decodable signals that are predictive of logical validity during code generation. Inspired by mechanistic interpretability, we propose to treat code verification as a mechanistic diagnostic task, mapping the model's explicit algorithmic trajectory into line-level attribution graphs. By decomposing complex residual flows, we aim to identify the structural signatures that distinguish sound reasoning from logical failure within the model's internal circuits. Analysis across Python, C++, and Java confirms that intrinsic correctness signals are robust across diverse syntaxes. Topological features from these internal graphs predict correctness more reliably than surface heuristics and enable targeted causal interventions to fix erroneous logic. These findings establish internal introspection as a decodable property for verifying generated code. Our code is at https:// github.com/bruno686/CodeCircuit.",
            "score": 1,
            "issue_id": 990,
            "pub_date": "2026-02-06",
            "pub_date_card": {
                "ru": "6 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 6",
                "zh": "2æœˆ6æ—¥"
            },
            "hash": "6f09f58d4ce2b656",
            "authors": [
                "Yicheng He",
                "Zheng Zhao",
                "Zhou Kaiyu",
                "Bryan Dai",
                "Jie Fu",
                "Yonghui Yang"
            ],
            "affiliations": [
                "IQuest Research",
                "Nanyang Technological University",
                "National University of Singapore",
                "University of Edinburgh",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.07080.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ’Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ ÑÑ…ĞµĞ¼Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ ĞºĞ¾Ğ´Ğ°",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ´Ğ°, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¸Ñ… Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞ¹ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ², Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº ÑĞ½Ğ¸Ñ‚-Ñ‚ĞµÑÑ‚Ñ‹ Ğ¸Ğ»Ğ¸ ÑÑƒĞ´ĞµĞ¹ÑĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸, Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶Ğ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ³Ñ€Ğ°Ñ„Ñ‹ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑÑ‚Ñ€Ğ¾Ğº ĞºĞ¾Ğ´Ğ°, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ñ‚ÑŒ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ñ‚ÑƒÑ€Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°ÑÑ‚ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ² Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ÑÑ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ²Ğ¾ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… ÑÑ…ĞµĞ¼Ğ°Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¾ÑÑ‚Ğ°ÑÑ‚ÑÑ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ñ‹Ğ¼Ğ¸ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸ÑĞ¼ ÑĞ¸Ğ½Ñ‚Ğ°ĞºÑĞ¸ÑĞ° ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ¢Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ ÑÑ‚Ğ¸Ñ… Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾, Ñ‡ĞµĞ¼ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğµ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸, Ğ¸ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ²ĞµĞ½Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¸."
                },
                "en": {
                    "title": "Unlocking Code Verification Through Neural Dynamics",
                    "desc": "This paper explores a new method for verifying the correctness of code generated by large language models (LLMs) by analyzing their internal neural dynamics. Instead of relying on external tests or judges, the authors propose that the model's internal structure can reveal signals that indicate whether the reasoning is correct or flawed. They use mechanistic interpretability to create line-level attribution graphs that map the model's decision-making process, allowing them to identify patterns that correlate with logical validity. Their findings show that these internal signals are consistent across different programming languages and can be used to improve the accuracy of code generation."
                },
                "zh": {
                    "title": "é€šè¿‡å†…éƒ¨åŠ¨æ€åˆ†æå®ç°ä»£ç éªŒè¯",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†é€šè¿‡åˆ†æå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å†…éƒ¨ç¥ç»åŠ¨æ€æ¥éªŒè¯ä»£ç çš„æ­£ç¡®æ€§ã€‚æˆ‘ä»¬æå‡ºï¼Œæ¨¡å‹çš„å†…éƒ¨è®¡ç®—ç»“æ„ä¸­å¯èƒ½ç¼–ç äº†å¯è§£ç çš„ä¿¡å·ï¼Œè¿™äº›ä¿¡å·å¯ä»¥é¢„æµ‹ä»£ç ç”Ÿæˆè¿‡ç¨‹ä¸­çš„é€»è¾‘æœ‰æ•ˆæ€§ã€‚é€šè¿‡å°†ä»£ç éªŒè¯è§†ä¸ºä¸€ç§æœºæ¢°è¯Šæ–­ä»»åŠ¡ï¼Œæˆ‘ä»¬èƒ½å¤Ÿè¯†åˆ«å‡ºåŒºåˆ†æ­£ç¡®æ¨ç†ä¸é€»è¾‘å¤±è´¥çš„ç»“æ„ç‰¹å¾ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè¿™äº›å†…éƒ¨å›¾çš„æ‹“æ‰‘ç‰¹å¾æ¯”è¡¨é¢å¯å‘å¼æ–¹æ³•æ›´å¯é åœ°é¢„æµ‹æ­£ç¡®æ€§ï¼Œå¹¶èƒ½å¤Ÿé’ˆå¯¹æ€§åœ°ä¿®å¤é”™è¯¯é€»è¾‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.05929",
            "title": "KV-CoRE: Benchmarking Data-Dependent Low-Rank Compressibility of KV-Caches in LLMs",
            "url": "https://huggingface.co/papers/2602.05929",
            "abstract": "KV-CoRE method evaluates kv-cache compressibility through SVD-based low-rank approximation, revealing patterns linking compressibility to model architecture and training data across multiple languages and domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models rely on kv-caches to avoid redundant computation during autoregressive decoding, but as context length grows, reading and writing the cache can quickly saturate GPU memory bandwidth. Recent work has explored KV-cache compression, yet most approaches neglect the data-dependent nature of kv-caches and their variation across layers. We introduce KV-CoRE KV-cache Compressibility by Rank Evaluation), an SVD-based method for quantifying the data-dependent low-rank compressibility of kv-caches. KV-CoRE computes the optimal low-rank approximation under the Frobenius norm and, being gradient-free and incremental, enables efficient dataset-level, layer-wise evaluation. Using this method, we analyze multiple models and datasets spanning five English domains and sixteen languages, uncovering systematic patterns that link compressibility to model architecture, training data, and language coverage. As part of this analysis, we employ the Normalized Effective Rank as a metric of compressibility and show that it correlates strongly with performance degradation under compression. Our study establishes a principled evaluation framework and the first large-scale benchmark of kv-cache compressibility in LLMs, offering insights for dynamic, data-aware compression and data-centric model development.",
            "score": 1,
            "issue_id": 988,
            "pub_date": "2026-02-05",
            "pub_date_card": {
                "ru": "5 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 5",
                "zh": "2æœˆ5æ—¥"
            },
            "hash": "bad70d01da936f4f",
            "authors": [
                "Jian Chen",
                "Zhuoran Wang",
                "Jiayu Qin",
                "Ming Li",
                "Meng Wang",
                "Changyou Chen",
                "Yin Chen",
                "Qizhen Weng",
                "Yirui Liu"
            ],
            "affiliations": [
                "ByteDance",
                "Delft University of Technology",
                "Dolby Laboratories",
                "Institute of Artificial Intelligence (TeleAI), China Telecom",
                "University at Buffalo",
                "University of Maryland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.05929.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#optimization",
                    "#long_context",
                    "#multilingual",
                    "#low_resource",
                    "#benchmark"
                ],
                "emoji": "ğŸ“¦",
                "ru": {
                    "title": "ĞÑ†ĞµĞ½ĞºĞ° Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»Ğ° KV-ĞºÑÑˆĞ° Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ KV-CoRE Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¶Ğ¸Ğ¼Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸ KV-ĞºÑÑˆĞ° Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‡ĞµÑ€ĞµĞ· SVD-Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ½Ğ³Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ğ¾-ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ğ½Ñ‹Ğ¼ Ğ¸ Ğ¸Ğ½ĞºÑ€ĞµĞ¼ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¶Ğ¸Ğ¼Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ¸ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ»Ğ¾Ñ‘Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞŸÑ€Ğ¾Ğ²ĞµĞ´Ñ‘Ğ½ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¿ÑÑ‚Ğ¸ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ñ… Ğ¸ ÑˆĞµÑÑ‚Ğ½Ğ°Ğ´Ñ†Ğ°Ñ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ°Ñ…, Ğ²Ñ‹ÑĞ²Ğ¸Ğ²ÑˆĞ¸Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¶Ğ¸Ğ¼Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ’Ğ²ĞµĞ´ĞµĞ½Ğ° Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ° ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ Ğ°Ğ½Ğ³Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ñ Ñ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸ĞµĞ¹ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¸ ÑĞ¶Ğ°Ñ‚Ğ¸Ğ¸, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ ĞºÑÑˆĞ° Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Unlocking KV-Cache Efficiency with KV-CoRE",
                    "desc": "The KV-CoRE method introduces a new way to evaluate the compressibility of kv-caches in large language models using singular value decomposition (SVD) for low-rank approximation. This approach highlights how compressibility varies depending on the model architecture and the training data used, which is often overlooked in previous studies. By analyzing multiple models across different languages and domains, KV-CoRE reveals systematic patterns that connect compressibility to these factors. The method also provides a framework for understanding how compressibility impacts model performance, paving the way for more efficient and data-aware compression techniques."
                },
                "zh": {
                    "title": "KV-CoREï¼šè¯„ä¼°kv-cacheå‹ç¼©èƒ½åŠ›çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºKV-CoREçš„æ–¹æ³•ï¼Œç”¨äºè¯„ä¼°kv-cacheçš„å‹ç¼©èƒ½åŠ›ã€‚è¯¥æ–¹æ³•åŸºäºå¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰è¿›è¡Œä½ç§©è¿‘ä¼¼ï¼Œæ­ç¤ºäº†å‹ç¼©èƒ½åŠ›ä¸æ¨¡å‹æ¶æ„å’Œè®­ç»ƒæ•°æ®ä¹‹é—´çš„å…³ç³»ã€‚é€šè¿‡å¯¹å¤šä¸ªæ¨¡å‹å’Œæ•°æ®é›†çš„åˆ†æï¼Œå‘ç°å‹ç¼©èƒ½åŠ›ä¸æ¨¡å‹æ€§èƒ½ä¸‹é™ä¹‹é—´å­˜åœ¨æ˜¾è‘—ç›¸å…³æ€§ã€‚KV-CoREä¸ºå¤§è§„æ¨¡è¯„ä¼°kv-cacheçš„å‹ç¼©æ€§æä¾›äº†ä¸€ä¸ªç³»ç»ŸåŒ–çš„æ¡†æ¶ï¼Œä¿ƒè¿›äº†æ•°æ®é©±åŠ¨çš„å‹ç¼©å’Œæ¨¡å‹å¼€å‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.05708",
            "title": "Cost-Efficient RAG for Entity Matching with LLMs: A Blocking-based Exploration",
            "url": "https://huggingface.co/papers/2602.05708",
            "abstract": "CE-RAG4EM reduces computational overhead in large-scale entity matching by implementing blocking-based batch retrieval and generation while maintaining competitive matching quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Retrieval-augmented generation (RAG) enhances LLM reasoning in knowledge-intensive tasks, but existing RAG pipelines incur substantial retrieval and generation overhead when applied to large-scale entity matching. To address this limitation, we introduce CE-RAG4EM, a cost-efficient RAG architecture that reduces computation through blocking-based batch retrieval and generation. We also present a unified framework for analyzing and evaluating RAG systems for entity matching, focusing on blocking-aware optimizations and retrieval granularity. Extensive experiments suggest that CE-RAG4EM can achieve comparable or improved matching quality while substantially reducing end-to-end runtime relative to strong baselines. Our analysis further reveals that key configuration parameters introduce an inherent trade-off between performance and overhead, offering practical guidance for designing efficient and scalable RAG systems for entity matching and data integration.",
            "score": 1,
            "issue_id": 988,
            "pub_date": "2026-02-05",
            "pub_date_card": {
                "ru": "5 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 5",
                "zh": "2æœˆ5æ—¥"
            },
            "hash": "70ecc0f0d0f8ce3e",
            "authors": [
                "Chuangtao Ma",
                "Zeyu Zhang",
                "Arijit Khan",
                "Sebastian Schelter",
                "Paul Groth"
            ],
            "affiliations": [
                "Aalborg University",
                "BIFOLD & TU Berlin",
                "Bowling Green State University",
                "TU Berlin",
                "University of Amsterdam"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.05708.jpg",
            "data": {
                "categories": [],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ RAG Ğ´Ğ»Ñ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ CE-RAG4EM, Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ñ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ (RAG), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ»Ğ¾ĞºĞ¸Ñ€ÑƒÑÑ‰ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¸ Ğ¿Ğ°ĞºĞµÑ‚Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ LLM Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ĞµĞ´Ğ¸Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ RAG-ÑĞ¸ÑÑ‚ĞµĞ¼, ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ¾Ñ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸ÑÑ… Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ Ğ±Ğ»Ğ¾ĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ CE-RAG4EM Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ Ğ¸Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼."
                },
                "en": {
                    "title": "Efficient Entity Matching with CE-RAG4EM",
                    "desc": "CE-RAG4EM is a novel architecture designed to improve the efficiency of entity matching tasks by minimizing computational costs. It employs a blocking-based approach for batch retrieval and generation, which helps in managing large datasets effectively. The paper also introduces a comprehensive framework for evaluating retrieval-augmented generation (RAG) systems, emphasizing optimizations that consider blocking and retrieval granularity. Experimental results demonstrate that CE-RAG4EM maintains or enhances matching quality while significantly reducing runtime compared to existing methods."
                },
                "zh": {
                    "title": "é«˜æ•ˆå®ä½“åŒ¹é…çš„æ™ºèƒ½è§£å†³æ–¹æ¡ˆ",
                    "desc": "CE-RAG4EMæ˜¯ä¸€ç§é«˜æ•ˆçš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¶æ„ï¼Œæ—¨åœ¨é™ä½å¤§è§„æ¨¡å®ä½“åŒ¹é…ä¸­çš„è®¡ç®—å¼€é”€ã€‚å®ƒé€šè¿‡åŸºäºé˜»å¡çš„æ‰¹é‡æ£€ç´¢å’Œç”Ÿæˆæ–¹æ³•ï¼Œä¿æŒäº†åŒ¹é…è´¨é‡çš„ç«äº‰åŠ›ã€‚è¯¥è®ºæ–‡è¿˜æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œç”¨äºåˆ†æå’Œè¯„ä¼°å®ä½“åŒ¹é…çš„RAGç³»ç»Ÿï¼Œé‡ç‚¹å…³æ³¨é˜»å¡æ„ŸçŸ¥ä¼˜åŒ–å’Œæ£€ç´¢ç²’åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCE-RAG4EMåœ¨å‡å°‘ç«¯åˆ°ç«¯è¿è¡Œæ—¶é—´çš„åŒæ—¶ï¼Œèƒ½å¤Ÿå®ç°ä¸å¼ºåŸºçº¿ç›¸å½“æˆ–æ›´å¥½çš„åŒ¹é…è´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.07054",
            "title": "AVERE: Improving Audiovisual Emotion Reasoning with Preference Optimization",
            "url": "https://huggingface.co/papers/2602.07054",
            "abstract": "A benchmark and optimization technique are presented to improve multimodal large language models' emotion understanding by addressing spurious associations and hallucinations in audiovisual cues.  \t\t\t\t\tAI-generated summary \t\t\t\t Emotion understanding is essential for building socially intelligent agents. Although recent multimodal large language models have shown strong performance on this task, two key challenges remain - spurious associations between emotions and irrelevant audiovisual cues, and hallucinations of audiovisual cues driven by text priors in the language model backbone. To quantify and understand these issues, we introduce EmoReAlM, a benchmark designed to evaluate MLLMs for cue-emotion associations, hallucinations and modality agreement. We then propose AVEm-DPO, a preference optimization technique that aligns model responses with both audiovisual inputs and emotion-centric queries. Specifically, we construct preferences over responses exhibiting spurious associations or hallucinations, and audiovisual input pairs guided by textual prompts. We also include a regularization term that penalizes reliance on text priors, thereby mitigating modality-specific cue hallucinations. Experimental results on DFEW, RAVDESS and EMER demonstrate that our method significantly improves the performance of the reference baseline models with 6-19% of relative performance gains in zero-shot settings. By providing both a rigorous benchmark and a robust optimization framework, this work enables principled evaluation and improvement of MLLMs for emotion understanding and social AI. Code, models and benchmark will be released at https://avere-iclr.github.io.",
            "score": 1,
            "issue_id": 982,
            "pub_date": "2026-02-04",
            "pub_date_card": {
                "ru": "4 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 4",
                "zh": "2æœˆ4æ—¥"
            },
            "hash": "9f87bc5a2487213a",
            "authors": [
                "Ashutosh Chaubey",
                "Jiacheng Pang",
                "Maksim Siniukov",
                "Mohammad Soleymani"
            ],
            "affiliations": [
                "Institute for Creative Technologies, University of Southern California"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.07054.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#multimodal",
                    "#video",
                    "#hallucinations",
                    "#audio",
                    "#benchmark",
                    "#rlhf",
                    "#open_source",
                    "#training"
                ],
                "emoji": "ğŸ˜Š",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹: Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ°ÑÑĞ¾Ñ†Ğ¸Ğ°Ñ†Ğ¸Ğ¹",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº EmoReAlM Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ AVEm-DPO Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¼Ğ¾Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ½ĞµÑ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°Ğ¼Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‚ Ñ‚Ğ°ĞºĞ¸Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ preference optimization Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¸ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 6-19% Ğ² Ğ½ÑƒĞ»ĞµĞ²Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… DFEW, RAVDESS Ğ¸ EMER."
                },
                "en": {
                    "title": "Enhancing Emotion Understanding in AI with Robust Benchmarking and Optimization",
                    "desc": "This paper introduces EmoReAlM, a benchmark designed to evaluate multimodal large language models (MLLMs) on their ability to understand emotions through audiovisual cues. It addresses two main challenges: spurious associations between emotions and irrelevant cues, and hallucinations where the model generates incorrect audiovisual information based on text. The authors propose AVEm-DPO, an optimization technique that aligns model outputs with both audiovisual inputs and emotion-focused queries, while penalizing over-reliance on text priors. Experimental results show significant performance improvements in emotion understanding tasks, demonstrating the effectiveness of the proposed methods."
                },
                "zh": {
                    "title": "æå‡æƒ…æ„Ÿç†è§£çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¼˜åŒ–æŠ€æœ¯",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºå‡†å’Œä¼˜åŒ–æŠ€æœ¯ï¼Œä»¥æé«˜å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨æƒ…æ„Ÿç†è§£æ–¹é¢çš„è¡¨ç°ã€‚ç ”ç©¶ä¸­è¯†åˆ«äº†æƒ…æ„Ÿä¸æ— å…³è§†å¬çº¿ç´¢ä¹‹é—´çš„è™šå‡å…³è”å’Œç”±æ–‡æœ¬å…ˆéªŒå¼•èµ·çš„è§†å¬çº¿ç´¢å¹»è§‰è¿™ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ã€‚æˆ‘ä»¬å¼•å…¥äº†EmoReAlMåŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨æƒ…æ„Ÿçº¿ç´¢å…³è”ã€å¹»è§‰å’Œæ¨¡æ€ä¸€è‡´æ€§æ–¹é¢çš„è¡¨ç°ã€‚é€šè¿‡æå‡ºAVEm-DPOä¼˜åŒ–æŠ€æœ¯ï¼Œæˆ‘ä»¬èƒ½å¤Ÿæ›´å¥½åœ°å¯¹é½æ¨¡å‹å“åº”ä¸è§†å¬è¾“å…¥å’Œæƒ…æ„Ÿä¸­å¿ƒæŸ¥è¯¢ï¼Œä»è€Œæ˜¾è‘—æå‡æ¨¡å‹çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.07040",
            "title": "Aster: Autonomous Scientific Discovery over 20x Faster Than Existing Methods",
            "url": "https://huggingface.co/papers/2602.07040",
            "abstract": "Aster is an AI agent that accelerates scientific discovery by iteratively improving programs, achieving state-of-the-art results across multiple domains including mathematics, biology, and machine learning with significantly reduced computational requirements.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Aster, an AI agent for autonomous scientific discovery capable of operating over 20 times faster than existing frameworks. Given a task, an initial program, and a script to evaluate the performance of the program, Aster iteratively improves the program, often leading to new state-of-the-art performances. Aster's significant reduction in the number of iterations required for novel discovery expands the domain of tractable problems to include tasks with long evaluation durations, such as multi-hour machine learning training runs.   We applied Aster to problems in mathematics, GPU kernel engineering, biology, neuroscience, and language model training. More specifically: the Erdos minimum overlap problem, optimizing the TriMul kernel, a single-cell analysis denoising problem, training a neural activity prediction model to perform well on ZAPBench, and the NanoGPT Speedrun Competition. Aster attains SOTA results in every task, except for ZAPBench, where it matches the performance of the best human solution with less than 1/190th of the compute.   Aster is accessible via a web interface and API at asterlab.ai.",
            "score": 1,
            "issue_id": 981,
            "pub_date": "2026-02-03",
            "pub_date_card": {
                "ru": "3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 3",
                "zh": "2æœˆ3æ—¥"
            },
            "hash": "c0356800a9ac2db2",
            "authors": [
                "Emmett Bicker"
            ],
            "affiliations": [
                "Aster AI Labs Inc., San Francisco, CA, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.07040.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#training",
                    "#optimization",
                    "#open_source",
                    "#agents",
                    "#plp"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼",
                    "desc": "Aster â€” ÑÑ‚Ğ¾ AI-Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹ Ğ² 20 Ñ€Ğ°Ğ· Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ, Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñƒ Ğ¸ ÑĞºÑ€Ğ¸Ğ¿Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ. Aster Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…: Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞµ, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ GPU-ÑĞ´ĞµÑ€, Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ—Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½ĞµĞ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ‡Ğ°ÑĞ¾Ğ²Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹."
                },
                "en": {
                    "title": "Aster: Revolutionizing Scientific Discovery with Speed and Efficiency",
                    "desc": "Aster is an advanced AI agent designed to enhance scientific discovery by iteratively refining existing programs. It operates over 20 times faster than traditional frameworks, allowing for quicker improvements and achieving state-of-the-art results in various fields such as mathematics and biology. By significantly reducing the number of iterations needed for program enhancement, Aster can tackle complex tasks that require long evaluation times, like extensive machine learning training. Its effectiveness is demonstrated across multiple applications, achieving top results while using far less computational power than conventional methods."
                },
                "zh": {
                    "title": "Asterï¼šåŠ é€Ÿç§‘å­¦å‘ç°çš„æ™ºèƒ½ä»£ç†",
                    "desc": "Asteræ˜¯ä¸€ä¸ªè‡ªä¸»ç§‘å­¦å‘ç°çš„äººå·¥æ™ºèƒ½ä»£ç†ï¼Œèƒ½å¤Ÿä»¥è¶…è¿‡ç°æœ‰æ¡†æ¶20å€çš„é€Ÿåº¦è¿è¡Œã€‚å®ƒé€šè¿‡è¿­ä»£æ”¹è¿›ç¨‹åºï¼Œèƒ½å¤Ÿåœ¨å¤šä¸ªé¢†åŸŸï¼ˆå¦‚æ•°å­¦ã€ç”Ÿç‰©å­¦å’Œæœºå™¨å­¦ä¹ ï¼‰ä¸­å®ç°æœ€å…ˆè¿›çš„ç»“æœï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘è®¡ç®—éœ€æ±‚ã€‚Asterçš„è¿­ä»£æ¬¡æ•°å¤§å¹…å‡å°‘ï¼Œä½¿å¾—å¤„ç†é•¿æ—¶é—´è¯„ä¼°çš„ä»»åŠ¡æˆä¸ºå¯èƒ½ï¼Œä¾‹å¦‚å¤šå°æ—¶çš„æœºå™¨å­¦ä¹ è®­ç»ƒã€‚Asteråœ¨å¤šä¸ªä»»åŠ¡ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œå±•ç¤ºäº†å…¶åœ¨ç§‘å­¦ç ”ç©¶ä¸­çš„å·¨å¤§æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.02827",
            "title": "Col-Bandit: Zero-Shot Query-Time Pruning for Late-Interaction Retrieval",
            "url": "https://huggingface.co/papers/2602.02827",
            "abstract": "Col-Bandit reduces computational costs in multi-vector late-interaction retrieval by adaptively pruning token-level interactions during query processing while maintaining ranking accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-vector late-interaction retrievers such as ColBERT achieve state-of-the-art retrieval quality, but their query-time cost is dominated by exhaustively computing token-level MaxSim interactions for every candidate document. While approximating late interaction with single-vector representations reduces cost, it often incurs substantial accuracy loss. We introduce Col-Bandit, a query-time pruning algorithm that reduces this computational burden by casting reranking as a finite-population Top-K identification problem. Col-Bandit maintains uncertainty-aware bounds over partially observed document scores and adaptively reveals only the (document, query token) MaxSim entries needed to determine the top results under statistical decision bounds with a tunable relaxation. Unlike coarse-grained approaches that prune entire documents or tokens offline, Col-Bandit sparsifies the interaction matrix on the fly. It operates as a zero-shot, drop-in layer over standard multi-vector systems, requiring no index modifications, offline preprocessing, or model retraining. Experiments on textual (BEIR) and multimodal (REAL-MM-RAG) benchmarks show that Col-Bandit preserves ranking fidelity while reducing MaxSim FLOPs by up to 5times, indicating that dense late-interaction scoring contains substantial redundancy that can be identified and pruned efficiently at query time.",
            "score": 1,
            "issue_id": 992,
            "pub_date": "2026-02-02",
            "pub_date_card": {
                "ru": "2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 2",
                "zh": "2æœˆ2æ—¥"
            },
            "hash": "e7df11f1cc7e30a0",
            "authors": [
                "Roi Pony",
                "Adi Raz",
                "Oshri Naparstek",
                "Idan Friedman",
                "Udi Barzelay"
            ],
            "affiliations": [
                "IBM Research Israel"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.02827.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#optimization",
                    "#rag",
                    "#benchmark"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ° Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°",
                    "desc": "Col-Bandit â€” ÑÑ‚Ğ¾ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ·Ğ´Ğ½Ğ¸Ğ¼ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ĞµĞ¼, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº ColBERT. ĞœĞµÑ‚Ğ¾Ğ´ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿ĞµÑ€ĞµÑ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ğº Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ğ¿-K ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ²Ğ¾ĞºÑƒĞ¿Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ñ‹Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ ÑĞµĞ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ MaxSim Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ¼ Ğ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ»ĞµÑ‚Ñƒ, Ñ€Ğ°Ğ·Ñ€ĞµĞ¶Ğ¸Ğ²Ğ°Ñ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñƒ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°. Col-Bandit Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ ĞºĞ°Ğº Ğ½ÑƒĞ»ĞµĞ²Ğ°Ñ Ğ¿Ğ¾ÑÑ‚-Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ»Ğ¸ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°, ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¾ 5 Ñ€Ğ°Ğ· Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Efficient Retrieval with Adaptive Pruning",
                    "desc": "Col-Bandit is a novel algorithm designed to enhance the efficiency of multi-vector late-interaction retrieval systems by adaptively reducing unnecessary token-level interactions during query processing. It addresses the high computational costs associated with calculating MaxSim interactions for each candidate document while ensuring that ranking accuracy is preserved. By framing the reranking process as a Top-K identification problem, Col-Bandit intelligently prunes interactions based on uncertainty-aware bounds, revealing only the necessary entries to determine top results. This approach allows it to function as a seamless addition to existing systems without requiring any changes to the index or model retraining, significantly lowering computational demands while maintaining performance."
                },
                "zh": {
                    "title": "Col-Banditï¼šé«˜æ•ˆçš„æŸ¥è¯¢å¤„ç†ä¸å‡†ç¡®æ’å",
                    "desc": "Col-Banditæ˜¯ä¸€ç§åœ¨å¤šå‘é‡å»¶è¿Ÿäº¤äº’æ£€ç´¢ä¸­å‡å°‘è®¡ç®—æˆæœ¬çš„ç®—æ³•ã€‚å®ƒé€šè¿‡åœ¨æŸ¥è¯¢å¤„ç†è¿‡ç¨‹ä¸­è‡ªé€‚åº”åœ°ä¿®å‰ªä»¤ç‰Œçº§äº¤äº’ï¼Œæ¥ä¿æŒæ’åçš„å‡†ç¡®æ€§ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒCol-Banditåœ¨æŸ¥è¯¢æ—¶åŠ¨æ€ç¨€ç–åŒ–äº¤äº’çŸ©é˜µï¼Œè€Œä¸æ˜¯ç¦»çº¿ä¿®å‰ªæ•´ä¸ªæ–‡æ¡£æˆ–ä»¤ç‰Œã€‚å®éªŒè¡¨æ˜ï¼ŒCol-Banditåœ¨ä¿æŒæ’åå‡†ç¡®æ€§çš„åŒæ—¶ï¼Œå¯ä»¥å°†MaxSimè®¡ç®—é‡å‡å°‘å¤šè¾¾5å€ï¼Œæ˜¾ç¤ºå‡ºå¯†é›†çš„å»¶è¿Ÿäº¤äº’è¯„åˆ†ä¸­å­˜åœ¨æ˜¾è‘—çš„å†—ä½™ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.08629",
            "title": "CauScale: Neural Causal Discovery at Scale",
            "url": "https://huggingface.co/papers/2602.08629",
            "abstract": "CauScale is a neural architecture that enables efficient causal discovery on large graphs through compressed embeddings and tied attention weights, achieving high accuracy and significant speedups over previous methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Causal discovery is essential for advancing data-driven fields such as scientific AI and data analysis, yet existing approaches face significant time- and space-efficiency bottlenecks when scaling to large graphs. To address this challenge, we present CauScale, a neural architecture designed for efficient causal discovery that scales inference to graphs with up to 1000 nodes. CauScale improves time efficiency via a reduction unit that compresses data embeddings and improves space efficiency by adopting tied attention weights to avoid maintaining axis-specific attention maps. To keep high causal discovery accuracy, CauScale adopts a two-stream design: a data stream extracts relational evidence from high-dimensional observations, while a graph stream integrates statistical graph priors and preserves key structural signals. CauScale successfully scales to 500-node graphs during training, where prior work fails due to space limitations. Across testing data with varying graph scales and causal mechanisms, CauScale achieves 99.6% mAP on in-distribution data and 84.4% on out-of-distribution data, while delivering 4-13,000 times inference speedups over prior methods. Our project page is at https://github.com/OpenCausaLab/CauScale.",
            "score": 0,
            "issue_id": 988,
            "pub_date": "2026-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "f39b9790a43da6ea",
            "authors": [
                "Bo Peng",
                "Sirui Chen",
                "Jiaguo Tian",
                "Yu Qiao",
                "Chaochao Lu"
            ],
            "affiliations": [
                "Shanghai Artificial Intelligence Laboratory",
                "Shanghai Innovation Institute",
                "Shanghai Jiao Tong University",
                "Tongji University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.08629.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#graphs",
                    "#open_source"
                ],
                "emoji": "ğŸ”—",
                "ru": {
                    "title": "ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ³Ñ€Ğ°Ñ„Ğ°Ñ… Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ",
                    "desc": "CauScale Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ²ÑĞ·ĞµĞ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ³Ñ€Ğ°Ñ„Ğ°Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¶Ğ°Ñ‚Ñ‹Ğµ embeddings Ğ¸ ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²ĞµÑĞ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ´Ğ²Ğ° Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ°: Ğ¿Ğ¾Ñ‚Ğ¾Ğº Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑĞ²Ğ¸Ğ´ĞµÑ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ğ¸Ğ· Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹, Ğ° Ğ¿Ğ¾Ñ‚Ğ¾Ğº Ğ³Ñ€Ğ°Ñ„Ğ° Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ñ‹ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ (99.6% Ğ½Ğ° in-distribution Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…) Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ² 4-13,000 Ñ€Ğ°Ğ· Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "CauScale: Fast and Accurate Causal Discovery for Large Graphs",
                    "desc": "CauScale is a novel neural architecture designed for efficient causal discovery in large graphs, addressing the limitations of existing methods in terms of time and space efficiency. It utilizes compressed embeddings to reduce data size and tied attention weights to streamline the attention mechanism, allowing it to handle graphs with up to 1000 nodes. The architecture features a two-stream design that combines a data stream for extracting relational evidence and a graph stream for integrating statistical priors, ensuring high accuracy in causal inference. With impressive performance metrics, CauScale achieves significant speed improvements, making it a powerful tool for data-driven fields requiring causal analysis."
                },
                "zh": {
                    "title": "é«˜æ•ˆå› æœå‘ç°çš„æ–°æ–¹æ³•ï¼šCauScale",
                    "desc": "CauScaleæ˜¯ä¸€ç§ç¥ç»ç½‘ç»œæ¶æ„ï¼Œæ—¨åœ¨é«˜æ•ˆåœ°è¿›è¡Œå¤§å›¾çš„å› æœå‘ç°ã€‚å®ƒé€šè¿‡å‹ç¼©åµŒå…¥å’Œç»‘å®šæ³¨æ„åŠ›æƒé‡æ¥æé«˜æ—¶é—´å’Œç©ºé—´æ•ˆç‡ï¼Œèƒ½å¤Ÿå¤„ç†å¤šè¾¾1000ä¸ªèŠ‚ç‚¹çš„å›¾ã€‚CauScaleé‡‡ç”¨åŒæµè®¾è®¡ï¼Œæ•°æ®æµæå–é«˜ç»´è§‚å¯Ÿä¸­çš„å…³ç³»è¯æ®ï¼Œå›¾æµæ•´åˆç»Ÿè®¡å›¾å…ˆéªŒå¹¶ä¿ç•™å…³é”®ç»“æ„ä¿¡å·ã€‚è¯¥æ–¹æ³•åœ¨è®­ç»ƒæ—¶æˆåŠŸæ‰©å±•åˆ°500èŠ‚ç‚¹çš„å›¾ï¼Œå¹¶åœ¨ä¸åŒè§„æ¨¡å’Œå› æœæœºåˆ¶çš„æµ‹è¯•æ•°æ®ä¸Šå®ç°äº†æ˜¾è‘—çš„å‡†ç¡®æ€§å’Œé€Ÿåº¦æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.08004",
            "title": "Agent Skills: A Data-Driven Analysis of Claude Skills for Extending Large Language Model Functionality",
            "url": "https://huggingface.co/papers/2602.08004",
            "abstract": "Agent skills extend large language model (LLM) agents with reusable, program-like modules that define triggering conditions, procedural logic, and tool interactions. As these skills proliferate in public marketplaces, it is unclear what types are available, how users adopt them, and what risks they pose. To answer these questions, we conduct a large-scale, data-driven analysis of 40,285 publicly listed skills from a major marketplace. Our results show that skill publication tends to occur in short bursts that track shifts in community attention. We also find that skill content is highly concentrated in software engineering workflows, while information retrieval and content creation account for a substantial share of adoption. Beyond content trends, we uncover a pronounced supply-demand imbalance across categories, and we show that most skills remain within typical prompt budgets despite a heavy-tailed length distribution. Finally, we observe strong ecosystem homogeneity, with widespread intent-level redundancy, and we identify non-trivial safety risks, including skills that enable state-changing or system-level actions. Overall, our findings provide a quantitative snapshot of agent skills as an emerging infrastructure layer for agents and inform future work on skill reuse, standardization, and safety-aware design.",
            "score": 0,
            "issue_id": 989,
            "pub_date": "2026-02-08",
            "pub_date_card": {
                "ru": "8 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 8",
                "zh": "2æœˆ8æ—¥"
            },
            "hash": "c94a8b551a60d94a",
            "authors": [
                "George Ling",
                "Shanshan Zhong",
                "Richard Huang"
            ],
            "affiliations": [
                "Bosch Research",
                "Carnegie Mellon University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.08004.jpg",
            "data": {
                "categories": [
                    "#ethics",
                    "#security"
                ],
                "emoji": "ğŸ› ï¸",
                "ru": {
                    "title": "ĞœĞ°Ñ€ĞºĞµÑ‚Ğ¿Ğ»ĞµĞ¹Ñ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ´Ğ»Ñ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²: Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑĞºĞ¾ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹, Ñ€Ğ¸ÑĞºĞ¾Ğ² Ğ¸ Ñ‚ĞµĞ½Ğ´ĞµĞ½Ñ†Ğ¸Ğ¹ ÑĞ¿Ñ€Ğ¾ÑĞ°",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· 40 285 Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ğ¾ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² (skills) Ğ´Ğ»Ñ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸Ğ· Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°Ñ€ĞºĞµÑ‚Ğ¿Ğ»ĞµĞ¹ÑĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚, ĞºĞ°Ğº ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ñ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑĞ¼Ğ¸ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°, Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¾Ğ¹ Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑÑ‚ÑÑ ÑÑ€ĞµĞ´Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹, Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°ÑÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ² Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞµ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ´Ğ¸ÑĞ±Ğ°Ğ»Ğ°Ğ½Ñ ÑĞ¿Ñ€Ğ¾ÑĞ° Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸, Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¾Ğ´Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞºĞ¾ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ñ Ğ´ÑƒĞ±Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ¸ÑĞºĞ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ÑÑ‰ĞµĞ¹ÑÑ Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ´Ğ°ĞµÑ‚ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğ¼Ñƒ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Unlocking the Potential of Agent Skills in LLMs",
                    "desc": "This paper analyzes a large dataset of 40,285 agent skills from a major marketplace, focusing on their characteristics and usage patterns. It reveals that the publication of these skills often coincides with shifts in community interest, indicating a dynamic ecosystem. The study highlights a concentration of skills in software engineering, with significant adoption in information retrieval and content creation. Additionally, it identifies safety risks associated with certain skills and emphasizes the need for standardization and safety-aware design in the development of agent skills."
                },
                "zh": {
                    "title": "æŠ€èƒ½æ‰©å±•ï¼šå¤§å‹è¯­è¨€æ¨¡å‹çš„æœªæ¥åŸºç¡€è®¾æ–½",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†çš„æŠ€èƒ½æ‰©å±•ï¼Œè¿™äº›æŠ€èƒ½æ˜¯å¯é‡ç”¨çš„æ¨¡å—ï¼Œå®šä¹‰äº†è§¦å‘æ¡ä»¶ã€ç¨‹åºé€»è¾‘å’Œå·¥å…·äº¤äº’ã€‚æˆ‘ä»¬å¯¹ä¸€ä¸ªä¸»è¦å¸‚åœºä¸­40,285ä¸ªå…¬å¼€åˆ—å‡ºçš„æŠ€èƒ½è¿›è¡Œäº†å¤§è§„æ¨¡çš„æ•°æ®é©±åŠ¨åˆ†æï¼Œå‘ç°æŠ€èƒ½å‘å¸ƒé€šå¸¸åœ¨ç¤¾åŒºå…³æ³¨åº¦å˜åŒ–æ—¶å‡ºç°çŸ­æš‚çš„é«˜å³°ã€‚åˆ†æç»“æœæ˜¾ç¤ºï¼ŒæŠ€èƒ½å†…å®¹ä¸»è¦é›†ä¸­åœ¨è½¯ä»¶å·¥ç¨‹å·¥ä½œæµç¨‹ä¸­ï¼Œè€Œä¿¡æ¯æ£€ç´¢å’Œå†…å®¹åˆ›ä½œåˆ™å æ®äº†ç›¸å½“å¤§çš„é‡‡ç”¨æ¯”ä¾‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å‘ç°ä¸åŒç±»åˆ«ä¹‹é—´å­˜åœ¨æ˜æ˜¾çš„ä¾›éœ€ä¸å¹³è¡¡ï¼Œå¹¶è¯†åˆ«å‡ºä¸€äº›æ½œåœ¨çš„å®‰å…¨é£é™©ï¼ŒåŒ…æ‹¬èƒ½å¤Ÿè¿›è¡ŒçŠ¶æ€æ›´æ”¹æˆ–ç³»ç»Ÿçº§æ“ä½œçš„æŠ€èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.07948",
            "title": "dewi-kadita: A Python Library for Idealized Fish Schooling Simulation with Entropy-Based Diagnostics",
            "url": "https://huggingface.co/papers/2602.07948",
            "abstract": "Collective motion in fish schools exemplifies emergent self-organization in active matter systems, yet computational tools for simulating and analyzing these dynamics remain fragmented across research groups. We present dewi-kadita, an open-source Python library implementing the three-dimensional Couzin zone-based model with comprehensive entropy diagnostics tailored for marine collective behavior research. The library introduces seven information-theoretic metrics -- school cohesion entropy, polarization entropy, depth stratification entropy, angular momentum entropy, nearest-neighbor entropy, velocity correlation entropy, and school shape entropy -- that characterize distinct organizational features inaccessible to classical order parameters. These metrics combine into an Oceanic Schooling Index (OSI) providing a single scalar measure of collective disorder. Validation across four canonical configurations (swarm, torus, dynamic parallel, highly parallel) confirms correct reproduction of known phase behaviors: the swarm maintains disorder with polarization P < 0.1 and OSI approx 0.71, while the highly parallel state achieves P = 0.998 with OSI = 0.24 and velocity correlation entropy vanishing to zero. The entropy framework successfully discriminates the torus and dynamic parallel configurations that exhibit comparable order parameter magnitudes through different organizational mechanisms. Numba just-in-time (JIT) compilation accelerates pairwise interaction calculations by 10--100times, enabling simulations of 150--250 agents over 1000--2000 time steps within five minutes on standard workstation hardware. NetCDF4 output ensures interoperability with oceanographic analysis tools. The library addresses the need for standardized, reproducible infrastructure in collective behavior modeling analogous to established molecular dynamics codes.",
            "score": 0,
            "issue_id": 987,
            "pub_date": "2026-02-08",
            "pub_date_card": {
                "ru": "8 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 8",
                "zh": "2æœˆ8æ—¥"
            },
            "hash": "49a6a78c36e28b77",
            "authors": [
                "Sandy H. S. Herho",
                "Iwan P. Anwar",
                "Faruq Khadami",
                "Alfita P. Handayani",
                "Karina A. Sujatmiko",
                "Kamaluddin Kasim",
                "Rusmawan Suwarman",
                "Dasapta E. Irawan"
            ],
            "affiliations": [
                "Bandung Institute of Technology",
                "Ministry of Maritime Affairs and Fisheries",
                "Samudera Sains Teknologi Ltd.",
                "State University of New York, Binghamton",
                "University of California, Riverside"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.07948.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸŸ",
                "ru": {
                    "title": "Ğ­Ğ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ ĞºĞ°Ğº Ğ¼Ğ¾ÑÑ‚ Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ»Ğ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ° Ğ² Ğ¿Ñ€Ğ¸Ñ€Ğ¾Ğ´Ğµ",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ dewi-kadita â€” Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ÑƒÑ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºÑƒ Ğ½Ğ° Python Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ»Ğ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ñ€Ñ‹Ğ± Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ·Ğ¾Ğ½Ñ‹-Ğ±Ğ°Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Couzin Ğ² Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. Ğ‘Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ° Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ ÑĞµĞ¼ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾-Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸Ğ·ÑƒÑÑ‚ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ñ‹Ğ±Ğ½Ñ‹Ñ… ÑˆĞºĞ¾Ğ», Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğµ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ°, Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¸Ñ… Ğ² ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ğ´ĞµĞºÑ OSI. Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€Ñ‘Ñ… ĞºĞ°Ğ½Ğ¾Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸ÑÑ… Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ñ… Ñ„Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¾Ğ² Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°Ñ‚ÑŒ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ°. ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· JIT-ĞºĞ¾Ğ¼Ğ¿Ğ¸Ğ»ÑÑ†Ğ¸Ñ Numba Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ² 10-100 Ñ€Ğ°Ğ· Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¾Ñ‚Ğ½Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ·Ğ° Ğ¿Ñ€Ğ¸ĞµĞ¼Ğ»ĞµĞ¼Ğ¾Ğµ Ğ²Ñ€ĞµĞ¼Ñ, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚."
                },
                "en": {
                    "title": "Revolutionizing Fish School Simulations with Dewi-Kadita",
                    "desc": "The paper introduces dewi-kadita, an open-source Python library designed for simulating and analyzing collective motion in fish schools using a three-dimensional Couzin zone-based model. It features seven unique information-theoretic metrics that quantify various aspects of school organization, which are not captured by traditional order parameters. These metrics are combined into an Oceanic Schooling Index (OSI) that provides a single measure of collective disorder, enhancing the understanding of emergent behaviors in active matter systems. The library also improves computational efficiency through Numba JIT compilation, allowing for rapid simulations of large groups of agents, thus facilitating standardized research in marine collective behavior."
                },
                "zh": {
                    "title": "é±¼ç¾¤è¿åŠ¨çš„æ ‡å‡†åŒ–æ¨¡æ‹Ÿå·¥å…·",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†dewi-kaditaï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æºçš„Pythonåº“ï¼Œç”¨äºæ¨¡æ‹Ÿå’Œåˆ†æé±¼ç¾¤çš„é›†ä½“è¿åŠ¨ã€‚è¯¥åº“å®ç°äº†ä¸‰ç»´çš„CouzinåŒºåŸŸæ¨¡å‹ï¼Œå¹¶æä¾›äº†ä¸ƒç§ä¿¡æ¯è®ºåº¦é‡ï¼Œèƒ½å¤Ÿæè¿°é±¼ç¾¤çš„ä¸åŒç»„ç»‡ç‰¹å¾ã€‚é€šè¿‡è¿™äº›åº¦é‡ï¼Œç ”ç©¶è€…å¯ä»¥è®¡ç®—å‡ºä¸€ä¸ªç§°ä¸ºæµ·æ´‹å­¦æ ¡æŒ‡æ•°ï¼ˆOSIï¼‰çš„å•ä¸€æ ‡é‡ï¼Œæ¥è¡¡é‡é›†ä½“æ— åºç¨‹åº¦ã€‚è¯¥åº“çš„è®¾è®¡æ—¨åœ¨ä¸ºé›†ä½“è¡Œä¸ºå»ºæ¨¡æä¾›æ ‡å‡†åŒ–å’Œå¯é‡å¤çš„åŸºç¡€è®¾æ–½ï¼Œä¿ƒè¿›ç›¸å…³ç ”ç©¶çš„è¿›å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.07125",
            "title": "Reasoning-Augmented Representations for Multimodal Retrieval",
            "url": "https://huggingface.co/papers/2602.07125",
            "abstract": "UMR systems face challenges with latent reasoning tasks, which the proposed framework addresses by decoupling reasoning from retrieval through enhanced visual and textual representations.  \t\t\t\t\tAI-generated summary \t\t\t\t Universal Multimodal Retrieval (UMR) seeks any-to-any search across text and vision, yet modern embedding models remain brittle when queries require latent reasoning (e.g., resolving underspecified references or matching compositional constraints). We argue this brittleness is often data-induced: when images carry \"silent\" evidence and queries leave key semantics implicit, a single embedding pass must both reason and compress, encouraging spurious feature matching. We propose a data-centric framework that decouples these roles by externalizing reasoning before retrieval. Using a strong Vision--Language Model, we make implicit semantics explicit by densely captioning visual evidence in corpus entries, resolving ambiguous multimodal references in queries, and rewriting verbose instructions into concise retrieval constraints. Inference-time enhancement alone is insufficient; the retriever must be trained on these semantically dense representations to avoid distribution shift and fully exploit the added signal. Across M-BEIR, our reasoning-augmented training method yields consistent gains over strong baselines, with ablations showing that corpus enhancement chiefly benefits knowledge-intensive queries while query enhancement is critical for compositional modification requests. We publicly release our code at https://github.com/AugmentedRetrieval/ReasoningAugmentedRetrieval.",
            "score": 0,
            "issue_id": 995,
            "pub_date": "2026-02-06",
            "pub_date_card": {
                "ru": "6 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 6",
                "zh": "2æœˆ6æ—¥"
            },
            "hash": "846962cae06e2251",
            "authors": [
                "Jianrui Zhang",
                "Anirudh Sundara Rajan",
                "Brandon Han",
                "Soochahn Lee",
                "Sukanta Ganguly",
                "Yong Jae Lee"
            ],
            "affiliations": [
                "Kookmin University",
                "NetApp, Inc.",
                "University of Wisconsin-Madison"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.07125.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#rag",
                    "#data",
                    "#open_source",
                    "#dataset",
                    "#reasoning"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°: Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²",
                    "desc": "Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…, ĞºĞ¾Ğ³Ğ´Ğ° Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‚ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ñ‹Ñ… ÑÑÑ‹Ğ»Ğ¾Ğº Ğ¸ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ½Ñ‹Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿ÑƒÑ‚Ñ‘Ğ¼ ÑĞ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½ĞµÑĞ²Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾Ğ¹ Vision-Language Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ²ĞºĞ»Ğ°Ğ´ â€” Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ retriever Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ¾Ğ³Ğ°Ñ‰Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑÑ… Ğ´Ğ»Ñ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ M-BEIR Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ: Ğ¾Ğ±Ğ¾Ğ³Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ° Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ´Ğ»Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸ĞµÑ‘Ğ¼ĞºĞ¸Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ñ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Decoupling Reasoning from Retrieval for Better Multimodal Search",
                    "desc": "This paper addresses the challenges faced by Universal Multimodal Retrieval (UMR) systems when dealing with latent reasoning tasks. The authors propose a framework that separates the reasoning process from the retrieval process by improving visual and textual representations. By enhancing the data with explicit semantics and dense captions, the framework helps resolve ambiguities in queries and improves the matching of complex constraints. The results show that this reasoning-augmented approach leads to better performance on knowledge-intensive and compositional queries compared to existing methods."
                },
                "zh": {
                    "title": "è§£è€¦æ¨ç†ä¸æ£€ç´¢ï¼Œæå‡å¤šæ¨¡æ€æ£€ç´¢èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶æ¥è§£å†³é€šç”¨å¤šæ¨¡æ€æ£€ç´¢ï¼ˆUMRï¼‰ç³»ç»Ÿåœ¨æ½œåœ¨æ¨ç†ä»»åŠ¡ä¸­çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å¢å¼ºè§†è§‰å’Œæ–‡æœ¬è¡¨ç¤ºï¼Œå°†æ¨ç†ä¸æ£€ç´¢è§£è€¦ï¼Œä»è€Œæé«˜äº†æ¨¡å‹çš„é²æ£’æ€§ã€‚æˆ‘ä»¬é€šè¿‡å¯†é›†æ ‡æ³¨è§†è§‰è¯æ®ï¼Œä½¿éšå«è¯­ä¹‰æ˜¾æ€§åŒ–ï¼Œè§£å†³äº†å¤šæ¨¡æ€æŸ¥è¯¢ä¸­çš„æ¨¡ç³Šå¼•ç”¨é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¢å¼ºçš„è®­ç»ƒæ–¹æ³•åœ¨çŸ¥è¯†å¯†é›†å‹æŸ¥è¯¢å’Œç»„åˆä¿®æ”¹è¯·æ±‚ä¸­å‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.05946",
            "title": "f-GRPO and Beyond: Divergence-Based Reinforcement Learning Algorithms for General LLM Alignment",
            "url": "https://huggingface.co/papers/2602.05946",
            "abstract": "Preference alignment objectives are extended to general alignment settings using f-divergence variational representations, introducing novel on-policy and hybrid policy optimization methods for LLM alignment with theoretical and empirical validation.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent research shows that Preference Alignment (PA) objectives act as divergence estimators between aligned (chosen) and unaligned (rejected) response distributions. In this work, we extend this divergence-based perspective to general alignment settings, such as reinforcement learning with verifiable rewards (RLVR), where only environmental rewards are available. Within this unified framework, we propose f-Group Relative Policy Optimization (f-GRPO), a class of on-policy reinforcement learning, and f-Hybrid Alignment Loss (f-HAL), a hybrid on/off policy objectives, for general LLM alignment based on variational representation of f-divergences. We provide theoretical guarantees that these classes of objectives improve the average reward after alignment. Empirically, we validate our framework on both RLVR (Math Reasoning) and PA tasks (Safety Alignment), demonstrating superior performance and flexibility compared to current methods.",
            "score": 0,
            "issue_id": 988,
            "pub_date": "2026-02-05",
            "pub_date_card": {
                "ru": "5 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 5",
                "zh": "2æœˆ5æ—¥"
            },
            "hash": "0804cdbd3b49cfc2",
            "authors": [
                "Rajdeep Haldar",
                "Lantao Mei",
                "Guang Lin",
                "Yue Xing",
                "Qifan Song"
            ],
            "affiliations": [
                "Department of Statistics, Michigan State University",
                "Department of Statistics, Purdue University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.05946.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#alignment",
                    "#reasoning"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ñ€Ğ°Ğ¼ĞºĞ° Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· f-Ğ´Ğ¸Ğ²ĞµÑ€Ğ³ĞµĞ½Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ LLM, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ ĞºĞ°Ğº Ğ´Ğ¸Ğ²ĞµÑ€Ğ³ĞµĞ½Ñ†Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ Ğ½ĞµĞ²Ñ‹Ñ€Ğ°Ğ²Ğ½ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ f-Ğ´Ğ¸Ğ²ĞµÑ€Ğ³ĞµĞ½Ñ†Ğ¸Ğ¹, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ´Ğ²Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°: f-GRPO Ğ´Ğ»Ñ on-policy Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ f-HAL Ğ´Ğ»Ñ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ñ… on/off-policy Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹. Ğ¢ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ ÑÑ€ĞµĞ´Ğ½ĞµĞµ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ»Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing LLM Alignment with Divergence-Based Optimization",
                    "desc": "This paper extends Preference Alignment (PA) objectives to broader alignment scenarios using f-divergence variational representations. It introduces new methods for policy optimization, specifically f-Group Relative Policy Optimization (f-GRPO) for on-policy reinforcement learning and f-Hybrid Alignment Loss (f-HAL) for hybrid objectives. The authors provide theoretical guarantees that these methods enhance average rewards after alignment. Empirical results show that the proposed framework outperforms existing techniques in both reinforcement learning with verifiable rewards and safety alignment tasks."
                },
                "zh": {
                    "title": "æ‰©å±•åå¥½å¯¹é½ï¼Œæå‡æ¨¡å‹æ€§èƒ½",
                    "desc": "æœ¬ç ”ç©¶æ‰©å±•äº†åå¥½å¯¹é½ï¼ˆPAï¼‰ç›®æ ‡åˆ°ä¸€èˆ¬å¯¹é½è®¾ç½®ï¼Œä½¿ç”¨f-æ•£åº¦å˜åˆ†è¡¨ç¤ºã€‚æˆ‘ä»¬æå‡ºäº†f-ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆf-GRPOï¼‰å’Œf-æ··åˆå¯¹é½æŸå¤±ï¼ˆf-HALï¼‰ï¼Œè¿™ä¸¤ç§æ–¹æ³•åˆ†åˆ«ç”¨äºåœ¨ç­–ç•¥å­¦ä¹ å’Œæ··åˆç­–ç•¥å­¦ä¹ ä¸­è¿›è¡Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¯¹é½ã€‚ç†è®ºä¸Šï¼Œæˆ‘ä»¬è¯æ˜äº†è¿™äº›ç›®æ ‡ç±»åœ¨å¯¹é½åèƒ½æé«˜å¹³å‡å¥–åŠ±ã€‚é€šè¿‡åœ¨å¼ºåŒ–å­¦ä¹ å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰å’Œåå¥½å¯¹é½ä»»åŠ¡ä¸Šè¿›è¡Œå®è¯éªŒè¯ï¼Œæˆ‘ä»¬å±•ç¤ºäº†è¯¥æ¡†æ¶çš„ä¼˜è¶Šæ€§èƒ½å’Œçµæ´»æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.02285",
            "title": "Statistical Learning Theory in Lean 4: Empirical Processes from Scratch",
            "url": "https://huggingface.co/papers/2602.02285",
            "abstract": "A comprehensive formalization of statistical learning theory in Lean 4 addresses gaps in mathematical libraries and demonstrates human-AI collaboration for verified machine learning theory foundations.  \t\t\t\t\tAI-generated summary \t\t\t\t We present the first comprehensive Lean 4 formalization of statistical learning theory (SLT) grounded in empirical process theory. Our end-to-end formal infrastructure implement the missing contents in latest Lean 4 Mathlib library, including a complete development of Gaussian Lipschitz concentration, the first formalization of Dudley's entropy integral theorem for sub-Gaussian processes, and an application to least-squares (sparse) regression with a sharp rate. The project was carried out using a human-AI collaborative workflow, in which humans design proof strategies and AI agents execute tactical proof construction, leading to the human-verified Lean 4 toolbox for SLT. Beyond implementation, the formalization process exposes and resolves implicit assumptions and missing details in standard SLT textbooks, enforcing a granular, line-by-line understanding of the theory. This work establishes a reusable formal foundation and opens the door for future developments in machine learning theory. The code is available at https://github.com/YuanheZ/lean-stat-learning-theory",
            "score": 0,
            "issue_id": 987,
            "pub_date": "2026-02-02",
            "pub_date_card": {
                "ru": "2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 2",
                "zh": "2æœˆ2æ—¥"
            },
            "hash": "d31ab3a15b4e5c6a",
            "authors": [
                "Yuanhe Zhang",
                "Jason D. Lee",
                "Fanghui Liu"
            ],
            "affiliations": [
                "Shanghai Jiao Tong University",
                "University of California, Berkeley",
                "University of Warwick"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.02285.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#science"
                ],
                "emoji": "âœ“",
                "ru": {
                    "title": "ĞœĞ°ÑˆĞ¸Ğ½Ğ½Ğ°Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ°: Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ñ‚ĞµĞ¾Ñ€Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾-AI ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¿ĞµÑ€Ğ²Ğ°Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ°Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Lean 4, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°ÑÑ‰Ğ¸Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ¸ Mathlib, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñƒ ĞºĞ¾Ğ½Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ›Ğ¸Ğ¿ÑˆĞ¸Ñ†Ğ° Ğ¸ Ñ‚ĞµĞ¾Ñ€ĞµĞ¼Ñƒ Ğ¾Ğ± Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ğ»Ğ°Ğ¸ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ğ”Ğ°Ğ´Ğ»Ğ¸ Ğ´Ğ»Ñ ÑÑƒĞ±-Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ². ĞŸÑ€Ğ¾ĞµĞºÑ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞ»ÑÑ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‡ĞµĞ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ AI, Ğ³Ğ´Ğµ Ğ»ÑĞ´Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ğ»Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ², Ğ° AI-Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ñ‚Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°. Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ° Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞ¸Ğ»Ğ° Ğ½ĞµÑĞ²Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ² ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… ÑƒÑ‡ĞµĞ±Ğ½Ğ¸ĞºĞ°Ñ…, ÑĞ¾Ğ·Ğ´Ğ°Ğ² Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğ¹ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Building a Verified Foundation for Statistical Learning Theory",
                    "desc": "This paper presents a detailed formalization of statistical learning theory (SLT) using Lean 4, addressing gaps in existing mathematical libraries. It includes significant developments such as Gaussian Lipschitz concentration and Dudley's entropy integral theorem, which are crucial for understanding sub-Gaussian processes. The research showcases a collaborative approach where humans design proof strategies while AI assists in executing these proofs, resulting in a verified Lean 4 toolbox for SLT. This work not only implements theoretical concepts but also clarifies assumptions in standard SLT literature, paving the way for future advancements in machine learning theory."
                },
                "zh": {
                    "title": "äººæœºåä½œï¼Œæ„å»ºç»Ÿè®¡å­¦ä¹ ç†è®ºçš„åŸºç¡€",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†åœ¨Lean 4ä¸­å¯¹ç»Ÿè®¡å­¦ä¹ ç†è®ºçš„å…¨é¢å½¢å¼åŒ–ï¼Œå¡«è¡¥äº†æ•°å­¦åº“ä¸­çš„ç©ºç™½ï¼Œå¹¶å±•ç¤ºäº†äººç±»ä¸äººå·¥æ™ºèƒ½çš„åä½œã€‚æˆ‘ä»¬å®ç°äº†æœ€æ–°Lean 4 Mathlibåº“ä¸­ç¼ºå¤±çš„å†…å®¹ï¼ŒåŒ…æ‹¬é«˜æ–¯Lipschitzé›†ä¸­å’ŒDudleyç†µç§¯åˆ†å®šç†çš„é¦–æ¬¡å½¢å¼åŒ–ã€‚è¯¥é¡¹ç›®é‡‡ç”¨äººæœºåä½œçš„å·¥ä½œæµç¨‹ï¼Œç”±äººç±»è®¾è®¡è¯æ˜ç­–ç•¥ï¼Œäººå·¥æ™ºèƒ½æ‰§è¡Œè¯æ˜æ„å»ºï¼Œæœ€ç»ˆå½¢æˆäº†ç»è¿‡äººç±»éªŒè¯çš„Lean 4å·¥å…·ç®±ã€‚æ­¤å·¥ä½œä¸ºæœºå™¨å­¦ä¹ ç†è®ºå»ºç«‹äº†å¯é‡ç”¨çš„å½¢å¼åŸºç¡€ï¼Œå¹¶ä¸ºæœªæ¥çš„å‘å±•é“ºå¹³äº†é“è·¯ã€‚"
                }
            }
        }
    ],
    "link_prev": "2026-02-10.html",
    "link_next": "2026-02-12.html",
    "link_month": "2026-02.html",
    "short_date_prev": {
        "ru": "10.02",
        "en": "02/10",
        "zh": "2æœˆ10æ—¥"
    },
    "short_date_next": {
        "ru": "12.02",
        "en": "02/12",
        "zh": "2æœˆ12æ—¥"
    },
    "categories": {
        "#dataset": 10,
        "#data": 5,
        "#benchmark": 21,
        "#agents": 11,
        "#cv": 2,
        "#rl": 7,
        "#rlhf": 3,
        "#rag": 2,
        "#plp": 2,
        "#inference": 9,
        "#3d": 1,
        "#audio": 3,
        "#video": 6,
        "#multimodal": 8,
        "#math": 1,
        "#multilingual": 3,
        "#architecture": 5,
        "#healthcare": 0,
        "#training": 20,
        "#robotics": 4,
        "#agi": 1,
        "#games": 0,
        "#interpretability": 3,
        "#reasoning": 15,
        "#transfer_learning": 0,
        "#graphs": 1,
        "#ethics": 2,
        "#security": 4,
        "#optimization": 18,
        "#survey": 4,
        "#diffusion": 3,
        "#alignment": 5,
        "#story_generation": 0,
        "#hallucinations": 3,
        "#long_context": 5,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 2,
        "#open_source": 19,
        "#small_models": 2,
        "#science": 8,
        "#low_resource": 3
    }
}