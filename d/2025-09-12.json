{
    "date": {
        "ru": "12 сентября",
        "en": "September 12",
        "zh": "9月12日"
    },
    "time_utc": "2025-09-12 02:14",
    "weekday": 4,
    "issue_id": 5852,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2509.09265",
            "title": "Harnessing Uncertainty: Entropy-Modulated Policy Gradients for\n  Long-Horizon LLM Agents",
            "url": "https://huggingface.co/papers/2509.09265",
            "abstract": "Entropy-Modulated Policy Gradients (EMPG) addresses learning dynamics issues in LLMs by recalibrating policy gradients based on uncertainty and task outcomes, leading to improved performance in long-horizon tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t In long-horizon tasks, recent agents based on Large Language Models (LLMs) face a significant challenge that sparse, outcome-based rewards make it difficult to assign credit to intermediate steps. Previous methods mainly focus on creating dense reward signals to guide learning, either through traditional reinforcement learning techniques like inverse reinforcement learning or by using Process Reward Models for step-by-step feedback. In this paper, we identify a fundamental problem in the learning dynamics of LLMs: the magnitude of policy gradients is inherently coupled with the entropy, which leads to inefficient small updates for confident correct actions and potentially destabilizes large updates for uncertain ones. To resolve this, we propose Entropy-Modulated Policy Gradients (EMPG), a framework that re-calibrates the learning signal based on step-wise uncertainty and the final task outcome. EMPG amplifies updates for confident correct actions, penalizes confident errors, and attenuates updates from uncertain steps to stabilize exploration. We further introduce a bonus term for future clarity that encourages agents to find more predictable solution paths. Through comprehensive experiments on three challenging agent tasks, WebShop, ALFWorld, and Deep Search, we demonstrate that EMPG achieves substantial performance gains and significantly outperforms strong policy gradient baselines. Project page is at https://empgseed-seed.github.io/",
            "score": 3,
            "issue_id": 5852,
            "pub_date": "2025-09-11",
            "pub_date_card": {
                "ru": "11 сентября",
                "en": "September 11",
                "zh": "9月11日"
            },
            "hash": "7850d32271ef8349",
            "authors": [
                "Jiawei Wang",
                "Jiacai Liu",
                "Yuqian Fu",
                "Yingru Li",
                "Xintao Wang",
                "Yuan Lin",
                "Yu Yue",
                "Lin Zhang",
                "Yang Wang",
                "Ke Wang"
            ],
            "affiliations": [
                "ByteDance Seed"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.09265.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#optimization",
                    "#rl",
                    "#training",
                    "#rlhf"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Улучшение обучения языковых моделей через энтропийную модуляцию градиентов",
                    "desc": "Метод EMPG (Entropy-Modulated Policy Gradients) решает проблемы динамики обучения в больших языковых моделях (LLM) при выполнении долгосрочных задач. Он перекалибрует градиенты политики на основе неопределенности и результатов задачи, что приводит к улучшению производительности. EMPG усиливает обновления для уверенных правильных действий, штрафует уверенные ошибки и ослабляет обновления от неопределенных шагов для стабилизации исследования. Эксперименты на трех сложных задачах показали, что EMPG значительно превосходит базовые методы градиента политики."
                },
                "en": {
                    "title": "Boosting Learning with Entropy Awareness",
                    "desc": "Entropy-Modulated Policy Gradients (EMPG) is a new approach that improves learning in Large Language Models (LLMs) by adjusting policy gradients based on uncertainty and task results. In long-horizon tasks, LLMs struggle with sparse rewards, making it hard to credit intermediate actions. EMPG addresses this by recalibrating the learning signal, enhancing updates for confident actions while reducing the impact of uncertain ones. This method leads to better performance in complex tasks, as shown in experiments with various challenging agent environments."
                },
                "zh": {
                    "title": "熵调制策略梯度：提升长时间任务学习效率的关键",
                    "desc": "本文提出了一种名为熵调制策略梯度（EMPG）的方法，旨在解决大型语言模型（LLMs）在长时间任务中的学习动态问题。通过根据不确定性和任务结果重新校准策略梯度，EMPG能够提高在稀疏奖励环境中的学习效率。该方法放大了对正确自信动作的更新，惩罚自信错误，并减弱来自不确定步骤的更新，从而稳定探索过程。实验结果表明，EMPG在多个复杂任务中显著提升了性能，超越了传统的策略梯度基线。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.09680",
            "title": "FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning\n  Dataset and Comprehensive Benchmark",
            "url": "https://huggingface.co/papers/2509.09680",
            "abstract": "FLUX-Reason-6M and PRISM-Bench address the lack of reasoning-focused datasets and benchmarks for text-to-image models, providing a large-scale dataset and evaluation standard to improve model performance.  \t\t\t\t\tAI-generated summary \t\t\t\t The advancement of open-source text-to-image (T2I) models has been hindered by the absence of large-scale, reasoning-focused datasets and comprehensive evaluation benchmarks, resulting in a performance gap compared to leading closed-source systems. To address this challenge, We introduce FLUX-Reason-6M and PRISM-Bench (Precise and Robust Image Synthesis Measurement Benchmark). FLUX-Reason-6M is a massive dataset consisting of 6 million high-quality FLUX-generated images and 20 million bilingual (English and Chinese) descriptions specifically designed to teach complex reasoning. The image are organized according to six key characteristics: Imagination, Entity, Text rendering, Style, Affection, and Composition, and design explicit Generation Chain-of-Thought (GCoT) to provide detailed breakdowns of image generation steps. The whole data curation takes 15,000 A100 GPU days, providing the community with a resource previously unattainable outside of large industrial labs. PRISM-Bench offers a novel evaluation standard with seven distinct tracks, including a formidable Long Text challenge using GCoT. Through carefully designed prompts, it utilizes advanced vision-language models for nuanced human-aligned assessment of prompt-image alignment and image aesthetics. Our extensive evaluation of 19 leading models on PRISM-Bench reveals critical performance gaps and highlights specific areas requiring improvement. Our dataset, benchmark, and evaluation code are released to catalyze the next wave of reasoning-oriented T2I generation. Project page: https://flux-reason-6m.github.io/ .",
            "score": 2,
            "issue_id": 5852,
            "pub_date": "2025-09-11",
            "pub_date_card": {
                "ru": "11 сентября",
                "en": "September 11",
                "zh": "9月11日"
            },
            "hash": "60acc7b8f0e01329",
            "authors": [
                "Rongyao Fang",
                "Aldrich Yu",
                "Chengqi Duan",
                "Linjiang Huang",
                "Shuai Bai",
                "Yuxuan Cai",
                "Kun Wang",
                "Si Liu",
                "Xihui Liu",
                "Hongsheng Li"
            ],
            "affiliations": [
                "Alibaba",
                "BUAA",
                "CUHK",
                "HKU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.09680.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#benchmark",
                    "#open_source",
                    "#long_context",
                    "#dataset",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Революция в обучении и оценке моделей текст-изображение",
                    "desc": "FLUX-Reason-6M и PRISM-Bench решают проблему отсутствия наборов данных и эталонов для оценки моделей преобразования текста в изображение, ориентированных на рассуждения. FLUX-Reason-6M представляет собой массивный датасет из 6 миллионов высококачественных изображений с 20 миллионами двуязычных описаний, специально разработанных для обучения сложным рассуждениям. PRISM-Bench предлагает новый стандарт оценки с семью различными направлениями, включая сложную задачу Long Text с использованием Generation Chain-of-Thought (GCoT). Обширная оценка 19 ведущих моделей на PRISM-Bench выявляет критические пробелы в производительности и подчеркивает конкретные области, требующие улучшения."
                },
                "en": {
                    "title": "Empowering Text-to-Image Models with Reasoning Datasets and Benchmarks",
                    "desc": "FLUX-Reason-6M and PRISM-Bench are initiatives aimed at enhancing text-to-image (T2I) models by providing a large-scale dataset and a robust evaluation framework. FLUX-Reason-6M includes 6 million images and 20 million bilingual descriptions that focus on teaching complex reasoning through structured characteristics. The dataset is meticulously curated using extensive computational resources, making it a valuable asset for researchers. PRISM-Bench introduces a new evaluation standard with multiple tracks to assess model performance, revealing significant gaps and guiding future improvements in T2I generation."
                },
                "zh": {
                    "title": "推动推理导向的文本到图像生成",
                    "desc": "FLUX-Reason-6M和PRISM-Bench旨在解决文本到图像模型缺乏以推理为重点的数据集和基准的问题。FLUX-Reason-6M是一个包含600万张高质量图像和2000万条双语描述的大型数据集，专门设计用于教授复杂的推理能力。PRISM-Bench提供了一个新的评估标准，包含七个不同的评估轨道，特别是一个使用生成链思维（GCoT）的长文本挑战。通过这些资源，我们希望推动推理导向的文本到图像生成的下一波发展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.09674",
            "title": "SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning",
            "url": "https://huggingface.co/papers/2509.09674",
            "abstract": "SimpleVLA-RL, an RL framework for VLA models, enhances long-horizon action planning, achieves state-of-the-art performance, and discovers novel patterns during training.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language-Action (VLA) models have recently emerged as a powerful paradigm for robotic manipulation. Despite substantial progress enabled by large-scale pretraining and supervised fine-tuning (SFT), these models face two fundamental challenges: (i) the scarcity and high cost of large-scale human-operated robotic trajectories required for SFT scaling, and (ii) limited generalization to tasks involving distribution shift. Recent breakthroughs in Large Reasoning Models (LRMs) demonstrate that reinforcement learning (RL) can dramatically enhance step-by-step reasoning capabilities, raising a natural question: Can RL similarly improve the long-horizon step-by-step action planning of VLA? In this work, we introduce SimpleVLA-RL, an efficient RL framework tailored for VLA models. Building upon veRL, we introduce VLA-specific trajectory sampling, scalable parallelization, multi-environment rendering, and optimized loss computation. When applied to OpenVLA-OFT, SimpleVLA-RL achieves SoTA performance on LIBERO and even outperforms pi_0 on RoboTwin 1.0\\&2.0 with the exploration-enhancing strategies we introduce. SimpleVLA-RL not only reduces dependence on large-scale data and enables robust generalization, but also remarkably surpasses SFT in real-world tasks. Moreover, we identify a novel phenomenon ``pushcut'' during RL training, wherein the policy discovers previously unseen patterns beyond those seen in the previous training process. Github: https://github.com/PRIME-RL/SimpleVLA-RL",
            "score": 2,
            "issue_id": 5852,
            "pub_date": "2025-09-11",
            "pub_date_card": {
                "ru": "11 сентября",
                "en": "September 11",
                "zh": "9月11日"
            },
            "hash": "36851aee36c7e5a0",
            "authors": [
                "Haozhan Li",
                "Yuxin Zuo",
                "Jiale Yu",
                "Yuhao Zhang",
                "Zhaohui Yang",
                "Kaiyan Zhang",
                "Xuekai Zhu",
                "Yuchen Zhang",
                "Tianxing Chen",
                "Ganqu Cui",
                "Dehui Wang",
                "Dingxiang Luo",
                "Yuchen Fan",
                "Youbang Sun",
                "Jia Zeng",
                "Jiangmiao Pang",
                "Shanghang Zhang",
                "Yu Wang",
                "Yao Mu",
                "Bowen Zhou",
                "Ning Ding"
            ],
            "affiliations": [
                "Peking University",
                "Shanghai AI Lab",
                "Shanghai Jiao Tong University",
                "The University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.09674.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#optimization",
                    "#rl",
                    "#robotics",
                    "#reasoning"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Обучение с подкреплением открывает новые горизонты для роботов",
                    "desc": "SimpleVLA-RL - это фреймворк для обучения с подкреплением, разработанный для моделей типа Vision-Language-Action (VLA). Он улучшает планирование действий на длительных горизонтах и достигает наилучших результатов в ряде задач робототехники. SimpleVLA-RL снижает зависимость от больших объемов данных и обеспечивает надежную генерализацию. Во время обучения с подкреплением модель обнаруживает новые паттерны поведения, не встречавшиеся в предыдущем процессе обучения."
                },
                "en": {
                    "title": "Revolutionizing Robotic Action Planning with SimpleVLA-RL",
                    "desc": "SimpleVLA-RL is a reinforcement learning framework designed to improve Vision-Language-Action (VLA) models for robotic manipulation. It addresses challenges like the need for extensive human-operated robotic trajectories and the difficulty in generalizing to new tasks. By implementing techniques such as VLA-specific trajectory sampling and multi-environment rendering, SimpleVLA-RL enhances long-horizon action planning and achieves state-of-the-art performance. Additionally, it uncovers new patterns during training, demonstrating its ability to go beyond traditional supervised fine-tuning methods."
                },
                "zh": {
                    "title": "SimpleVLA-RL：提升视觉-语言-动作模型的长时间规划能力",
                    "desc": "SimpleVLA-RL是一个针对视觉-语言-动作（VLA）模型的强化学习框架，旨在增强长时间跨度的动作规划能力。该框架通过引入特定的轨迹采样、可扩展的并行处理和多环境渲染等技术，显著提高了模型的性能。SimpleVLA-RL在LIBERO数据集上达到了最先进的表现，并在RoboTwin 1.0和2.0上超越了现有的基准。更重要的是，该框架在训练过程中发现了一种新现象“pushcut”，揭示了模型在学习过程中能够识别出新的模式。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.09614",
            "title": "LoCoBench: A Benchmark for Long-Context Large Language Models in Complex\n  Software Engineering",
            "url": "https://huggingface.co/papers/2509.09614",
            "abstract": "LoCoBench evaluates long-context language models in complex software development scenarios, addressing the gap in understanding entire codebases and maintaining architectural consistency across large-scale systems.  \t\t\t\t\tAI-generated summary \t\t\t\t The emergence of long-context language models with context windows extending to millions of tokens has created new opportunities for sophisticated code understanding and software development evaluation. We propose LoCoBench, a comprehensive benchmark specifically designed to evaluate long-context LLMs in realistic, complex software development scenarios. Unlike existing code evaluation benchmarks that focus on single-function completion or short-context tasks, LoCoBench addresses the critical evaluation gap for long-context capabilities that require understanding entire codebases, reasoning across multiple files, and maintaining architectural consistency across large-scale software systems. Our benchmark provides 8,000 evaluation scenarios systematically generated across 10 programming languages, with context lengths spanning 10K to 1M tokens, a 100x variation that enables precise assessment of long-context performance degradation in realistic software development settings. LoCoBench introduces 8 task categories that capture essential long-context capabilities: architectural understanding, cross-file refactoring, multi-session development, bug investigation, feature implementation, code comprehension, integration testing, and security analysis. Through a 5-phase pipeline, we create diverse, high-quality scenarios that challenge LLMs to reason about complex codebases at unprecedented scale. We introduce a comprehensive evaluation framework with 17 metrics across 4 dimensions, including 8 new evaluation metrics, combined in a LoCoBench Score (LCBS). Our evaluation of state-of-the-art long-context models reveals substantial performance gaps, demonstrating that long-context understanding in complex software development represents a significant unsolved challenge that demands more attention. LoCoBench is released at: https://github.com/SalesforceAIResearch/LoCoBench.",
            "score": 1,
            "issue_id": 5852,
            "pub_date": "2025-09-11",
            "pub_date_card": {
                "ru": "11 сентября",
                "en": "September 11",
                "zh": "9月11日"
            },
            "hash": "15a23d38c535fc1c",
            "authors": [
                "Jielin Qiu",
                "Zuxin Liu",
                "Zhiwei Liu",
                "Rithesh Murthy",
                "Jianguo Zhang",
                "Haolin Chen",
                "Shiyu Wang",
                "Ming Zhu",
                "Liangwei Yang",
                "Juntao Tan",
                "Zhepeng Cen",
                "Cheng Qian",
                "Shelby Heinecke",
                "Weiran Yao",
                "Silvio Savarese",
                "Caiming Xiong",
                "Huan Wang"
            ],
            "affiliations": [
                "Salesforce AI Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.09614.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#long_context"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "LoCoBench: новый рубеж в оценке ИИ для разработки ПО",
                    "desc": "LoCoBench - это новый бенчмарк для оценки языковых моделей с длинным контекстом в сложных сценариях разработки программного обеспечения. Он включает 8000 сценариев оценки на 10 языках программирования с контекстом от 10 тыс. до 1 млн токенов. LoCoBench вводит 8 категорий задач, охватывающих ключевые возможности работы с длинным контекстом, такие как понимание архитектуры, рефакторинг между файлами и анализ безопасности. Оценка современных моделей с длинным контекстом выявила значительные пробелы в производительности, показывая, что понимание длинного контекста в сложной разработке ПО остается нерешенной проблемой."
                },
                "en": {
                    "title": "Evaluating Long-Context LLMs for Complex Software Development",
                    "desc": "LoCoBench is a new benchmark designed to evaluate long-context language models (LLMs) in complex software development tasks. It focuses on understanding entire codebases and maintaining architectural consistency, which is crucial for large-scale systems. The benchmark includes 8,000 scenarios across 10 programming languages, with context lengths ranging from 10,000 to 1,000,000 tokens, allowing for a thorough assessment of LLM performance. By introducing 8 task categories and a comprehensive evaluation framework, LoCoBench highlights significant performance gaps in current models, emphasizing the need for improved long-context understanding in software development."
                },
                "zh": {
                    "title": "评估长上下文模型的全新基准",
                    "desc": "LoCoBench是一个专门评估长上下文语言模型在复杂软件开发场景中的基准测试工具。它填补了对整个代码库理解和在大规模系统中保持架构一致性的评估空白。该基准提供了8000个评估场景，涵盖10种编程语言，能够精确评估长上下文性能的下降。通过引入多种任务类别和评估指标，LoCoBench为长上下文理解在软件开发中的挑战提供了全面的评估框架。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.09595",
            "title": "Kling-Avatar: Grounding Multimodal Instructions for Cascaded\n  Long-Duration Avatar Animation Synthesis",
            "url": "https://huggingface.co/papers/2509.09595",
            "abstract": "Kling-Avatar, a cascaded framework, enhances audio-driven avatar video generation by integrating multimodal instruction understanding with photorealistic portrait generation, resulting in high-fidelity, semantically grounded videos.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in audio-driven avatar video generation have significantly enhanced audio-visual realism. However, existing methods treat instruction conditioning merely as low-level tracking driven by acoustic or visual cues, without modeling the communicative purpose conveyed by the instructions. This limitation compromises their narrative coherence and character expressiveness. To bridge this gap, we introduce Kling-Avatar, a novel cascaded framework that unifies multimodal instruction understanding with photorealistic portrait generation. Our approach adopts a two-stage pipeline. In the first stage, we design a multimodal large language model (MLLM) director that produces a blueprint video conditioned on diverse instruction signals, thereby governing high-level semantics such as character motion and emotions. In the second stage, guided by blueprint keyframes, we generate multiple sub-clips in parallel using a first-last frame strategy. This global-to-local framework preserves fine-grained details while faithfully encoding the high-level intent behind multimodal instructions. Our parallel architecture also enables fast and stable generation of long-duration videos, making it suitable for real-world applications such as digital human livestreaming and vlogging. To comprehensively evaluate our method, we construct a benchmark of 375 curated samples covering diverse instructions and challenging scenarios. Extensive experiments demonstrate that Kling-Avatar is capable of generating vivid, fluent, long-duration videos at up to 1080p and 48 fps, achieving superior performance in lip synchronization accuracy, emotion and dynamic expressiveness, instruction controllability, identity preservation, and cross-domain generalization. These results establish Kling-Avatar as a new benchmark for semantically grounded, high-fidelity audio-driven avatar synthesis.",
            "score": 1,
            "issue_id": 5852,
            "pub_date": "2025-09-11",
            "pub_date_card": {
                "ru": "11 сентября",
                "en": "September 11",
                "zh": "9月11日"
            },
            "hash": "9128c939612e1d3e",
            "authors": [
                "Yikang Ding",
                "Jiwen Liu",
                "Wenyuan Zhang",
                "Zekun Wang",
                "Wentao Hu",
                "Liyuan Cui",
                "Mingming Lao",
                "Yingchao Shao",
                "Hui Liu",
                "Xiaohan Li",
                "Ming Chen",
                "Xiaoqiang Liu",
                "Yu-Shen Liu",
                "Pengfei Wan"
            ],
            "affiliations": [
                "Kuaishou Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.09595.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#story_generation",
                    "#video",
                    "#games"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Семантически обоснованные аватары с высокой детализацией",
                    "desc": "Kling-Avatar - это новая каскадная архитектура для генерации видео с аватарами на основе аудио. Она объединяет понимание мультимодальных инструкций с фотореалистичной генерацией портретов. На первом этапе мультимодальная языковая модель создает видео-макет, управляющий семантикой высокого уровня. На втором этапе генерируются отдельные фрагменты видео с сохранением мелких деталей и общего замысла."
                },
                "en": {
                    "title": "Kling-Avatar: Bridging Audio and Visual Realism in Avatar Generation",
                    "desc": "Kling-Avatar is a new framework designed to improve the generation of avatar videos driven by audio instructions. It combines understanding of multimodal instructions with the creation of realistic portraits, resulting in videos that are both visually appealing and semantically meaningful. The framework operates in two stages: first, it uses a large language model to create a blueprint video that captures high-level semantics like character emotions and movements. Then, it generates detailed sub-clips based on this blueprint, allowing for fast and stable production of long videos while maintaining high fidelity and expressiveness."
                },
                "zh": {
                    "title": "Kling-Avatar：音频驱动虚拟形象生成的新标杆",
                    "desc": "Kling-Avatar是一个级联框架，旨在提升音频驱动的虚拟形象视频生成。它通过整合多模态指令理解与逼真的肖像生成，生成高保真且语义明确的视频。该方法采用两阶段流程，首先利用多模态大语言模型生成蓝图视频，然后根据蓝图关键帧并行生成多个子片段。实验表明，Kling-Avatar在视频生成的清晰度、情感表达和指令控制等方面表现优异，适用于数字人直播和视频博客等实际应用。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.09676",
            "title": "SpatialVID: A Large-Scale Video Dataset with Spatial Annotations",
            "url": "https://huggingface.co/papers/2509.09676",
            "abstract": "SpatialVID, a large-scale dataset with diverse videos and dense 3D annotations, enhances model generalization and performance in video and 3D vision research.  \t\t\t\t\tAI-generated summary \t\t\t\t Significant progress has been made in spatial intelligence, spanning both spatial reconstruction and world exploration. However, the scalability and real-world fidelity of current models remain severely constrained by the scarcity of large-scale, high-quality training data. While several datasets provide camera pose information, they are typically limited in scale, diversity, and annotation richness, particularly for real-world dynamic scenes with ground-truth camera motion. To this end, we collect SpatialVID, a dataset consists of a large corpus of in-the-wild videos with diverse scenes, camera movements and dense 3D annotations such as per-frame camera poses, depth, and motion instructions. Specifically, we collect more than 21,000 hours of raw video, and process them into 2.7 million clips through a hierarchical filtering pipeline, totaling 7,089 hours of dynamic content. A subsequent annotation pipeline enriches these clips with detailed spatial and semantic information, including camera poses, depth maps, dynamic masks, structured captions, and serialized motion instructions. Analysis of SpatialVID's data statistics reveals a richness and diversity that directly foster improved model generalization and performance, establishing it as a key asset for the video and 3D vision research community.",
            "score": 0,
            "issue_id": 5852,
            "pub_date": "2025-09-11",
            "pub_date_card": {
                "ru": "11 сентября",
                "en": "September 11",
                "zh": "9月11日"
            },
            "hash": "b2d981674edaecf5",
            "authors": [
                "Jiahao Wang",
                "Yufeng Yuan",
                "Rujie Zheng",
                "Youtian Lin",
                "Jian Gao",
                "Lin-Zhuo Chen",
                "Yajie Bao",
                "Yi Zhang",
                "Chang Zeng",
                "Yanxi Zhou",
                "Xiaoxiao Long",
                "Hao Zhu",
                "Zhaoxiang Zhang",
                "Xun Cao",
                "Yao Yao"
            ],
            "affiliations": [
                "Institute of Automation, Chinese Academy of Science",
                "Nanjing University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.09676.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#dataset",
                    "#video"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "SpatialVID: Большие данные для прорыва в пространственном интеллекте",
                    "desc": "SpatialVID - это масштабный набор данных, содержащий разнообразные видео с плотными 3D-аннотациями. Он включает более 21 000 часов необработанного видео, обработанного в 2,7 миллиона клипов общей продолжительностью 7 089 часов динамического контента. Датасет обогащен детальной пространственной и семантической информацией, включая позы камеры, карты глубины, динамические маски, структурированные подписи и сериализованные инструкции по движению. SpatialVID способствует улучшению обобщения и производительности моделей в исследованиях компьютерного зрения и 3D-реконструкции."
                },
                "en": {
                    "title": "Unlocking 3D Vision with SpatialVID: A New Era of Video Datasets",
                    "desc": "SpatialVID is a comprehensive dataset designed to improve machine learning models in video and 3D vision tasks. It contains over 21,000 hours of diverse, real-world video footage, which has been meticulously processed into 2.7 million clips. Each clip is enriched with dense 3D annotations, including camera poses, depth maps, and motion instructions, providing a rich source of training data. This dataset addresses the limitations of existing datasets by offering high-quality, large-scale data that enhances model generalization and performance in spatial intelligence applications."
                },
                "zh": {
                    "title": "SpatialVID：提升视频与3D视觉研究的关键数据集",
                    "desc": "SpatialVID是一个大规模的数据集，包含多样化的视频和密集的3D注释，旨在提升视频和3D视觉研究中的模型泛化能力和性能。该数据集收集了超过21,000小时的原始视频，并通过分层过滤管道处理成270万段视频片段，提供了丰富的动态内容。每个片段都附有详细的空间和语义信息，包括相机位姿、深度图、动态掩码、结构化标题和序列化运动指令。SpatialVID的数据统计分析显示出其丰富性和多样性，直接促进了模型的泛化和性能提升，成为视频和3D视觉研究领域的重要资产。"
                }
            }
        }
    ],
    "link_prev": "2025-09-11.html",
    "link_next": "2025-09-15.html",
    "link_month": "2025-09.html",
    "short_date_prev": {
        "ru": "11.09",
        "en": "09/11",
        "zh": "9月11日"
    },
    "short_date_next": {
        "ru": "15.09",
        "en": "09/15",
        "zh": "9月15日"
    },
    "categories": {
        "#dataset": 3,
        "#data": 0,
        "#benchmark": 3,
        "#agents": 2,
        "#cv": 0,
        "#rl": 2,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 1,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 1,
        "#hallucinations": 0,
        "#long_context": 2,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}