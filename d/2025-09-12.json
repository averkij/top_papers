{
    "date": {
        "ru": "12 сентября",
        "en": "September 12",
        "zh": "9月12日"
    },
    "time_utc": "2025-09-12 03:21",
    "weekday": 4,
    "issue_id": 5853,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2509.09674",
            "title": "SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning",
            "url": "https://huggingface.co/papers/2509.09674",
            "abstract": "SimpleVLA-RL, an RL framework for VLA models, enhances long-horizon action planning, achieves state-of-the-art performance, and discovers novel patterns during training.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language-Action (VLA) models have recently emerged as a powerful paradigm for robotic manipulation. Despite substantial progress enabled by large-scale pretraining and supervised fine-tuning (SFT), these models face two fundamental challenges: (i) the scarcity and high cost of large-scale human-operated robotic trajectories required for SFT scaling, and (ii) limited generalization to tasks involving distribution shift. Recent breakthroughs in Large Reasoning Models (LRMs) demonstrate that reinforcement learning (RL) can dramatically enhance step-by-step reasoning capabilities, raising a natural question: Can RL similarly improve the long-horizon step-by-step action planning of VLA? In this work, we introduce SimpleVLA-RL, an efficient RL framework tailored for VLA models. Building upon veRL, we introduce VLA-specific trajectory sampling, scalable parallelization, multi-environment rendering, and optimized loss computation. When applied to OpenVLA-OFT, SimpleVLA-RL achieves SoTA performance on LIBERO and even outperforms pi_0 on RoboTwin 1.0\\&2.0 with the exploration-enhancing strategies we introduce. SimpleVLA-RL not only reduces dependence on large-scale data and enables robust generalization, but also remarkably surpasses SFT in real-world tasks. Moreover, we identify a novel phenomenon ``pushcut'' during RL training, wherein the policy discovers previously unseen patterns beyond those seen in the previous training process. Github: https://github.com/PRIME-RL/SimpleVLA-RL",
            "score": 25,
            "issue_id": 5852,
            "pub_date": "2025-09-11",
            "pub_date_card": {
                "ru": "11 сентября",
                "en": "September 11",
                "zh": "9月11日"
            },
            "hash": "36851aee36c7e5a0",
            "authors": [
                "Haozhan Li",
                "Yuxin Zuo",
                "Jiale Yu",
                "Yuhao Zhang",
                "Zhaohui Yang",
                "Kaiyan Zhang",
                "Xuekai Zhu",
                "Yuchen Zhang",
                "Tianxing Chen",
                "Ganqu Cui",
                "Dehui Wang",
                "Dingxiang Luo",
                "Yuchen Fan",
                "Youbang Sun",
                "Jia Zeng",
                "Jiangmiao Pang",
                "Shanghang Zhang",
                "Yu Wang",
                "Yao Mu",
                "Bowen Zhou",
                "Ning Ding"
            ],
            "affiliations": [
                "Peking University",
                "Shanghai AI Lab",
                "Shanghai Jiao Tong University",
                "The University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.09674.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#optimization",
                    "#rl",
                    "#robotics",
                    "#reasoning"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Обучение с подкреплением открывает новые горизонты для роботов",
                    "desc": "SimpleVLA-RL - это фреймворк для обучения с подкреплением, разработанный для моделей типа Vision-Language-Action (VLA). Он улучшает планирование действий на длительных горизонтах и достигает наилучших результатов в ряде задач робототехники. SimpleVLA-RL снижает зависимость от больших объемов данных и обеспечивает надежную генерализацию. Во время обучения с подкреплением модель обнаруживает новые паттерны поведения, не встречавшиеся в предыдущем процессе обучения."
                },
                "en": {
                    "title": "Revolutionizing Robotic Action Planning with SimpleVLA-RL",
                    "desc": "SimpleVLA-RL is a reinforcement learning framework designed to improve Vision-Language-Action (VLA) models for robotic manipulation. It addresses challenges like the need for extensive human-operated robotic trajectories and the difficulty in generalizing to new tasks. By implementing techniques such as VLA-specific trajectory sampling and multi-environment rendering, SimpleVLA-RL enhances long-horizon action planning and achieves state-of-the-art performance. Additionally, it uncovers new patterns during training, demonstrating its ability to go beyond traditional supervised fine-tuning methods."
                },
                "zh": {
                    "title": "SimpleVLA-RL：提升视觉-语言-动作模型的长时间规划能力",
                    "desc": "SimpleVLA-RL是一个针对视觉-语言-动作（VLA）模型的强化学习框架，旨在增强长时间跨度的动作规划能力。该框架通过引入特定的轨迹采样、可扩展的并行处理和多环境渲染等技术，显著提高了模型的性能。SimpleVLA-RL在LIBERO数据集上达到了最先进的表现，并在RoboTwin 1.0和2.0上超越了现有的基准。更重要的是，该框架在训练过程中发现了一种新现象“pushcut”，揭示了模型在学习过程中能够识别出新的模式。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.09174",
            "title": "EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for\n  Speech-to-Speech LLMs",
            "url": "https://huggingface.co/papers/2509.09174",
            "abstract": "EchoX, a speech-to-speech large language model, addresses the acoustic-semantic gap by integrating semantic representations, preserving reasoning abilities, and achieving advanced performance on knowledge-based benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Speech-to-speech large language models (SLLMs) are attracting increasing attention. Derived from text-based large language models (LLMs), SLLMs often exhibit degradation in knowledge and reasoning capabilities. We hypothesize that this limitation arises because current training paradigms for SLLMs fail to bridge the acoustic-semantic gap in the feature representation space. To address this issue, we propose EchoX, which leverages semantic representations and dynamically generates speech training targets. This approach integrates both acoustic and semantic learning, enabling EchoX to preserve strong reasoning abilities as a speech LLM. Experimental results demonstrate that EchoX, with about six thousand hours of training data, achieves advanced performance on multiple knowledge-based question-answering benchmarks. The project is available at https://github.com/FreedomIntelligence/EchoX.",
            "score": 25,
            "issue_id": 5853,
            "pub_date": "2025-09-11",
            "pub_date_card": {
                "ru": "11 сентября",
                "en": "September 11",
                "zh": "9月11日"
            },
            "hash": "b6e2cc4088bce9ac",
            "authors": [
                "Yuhao Zhang",
                "Yuhao Du",
                "Zhanchen Dai",
                "Xiangnan Ma",
                "Kaiqi Kou",
                "Benyou Wang",
                "Haizhou Li"
            ],
            "affiliations": [
                "The Chinese University of Hong Kong, Shenzhen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.09174.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#reasoning",
                    "#benchmark",
                    "#audio",
                    "#dataset"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "Преодоление разрыва между звуком и смыслом в речевых ИИ-моделях",
                    "desc": "EchoX - это речевая большая языковая модель, которая решает проблему акустико-семантического разрыва. Она интегрирует семантические представления и сохраняет способности к рассуждению. EchoX использует динамическую генерацию речевых целей обучения. Модель достигает продвинутых результатов на тестах, основанных на знаниях, используя всего около 6000 часов обучающих данных."
                },
                "en": {
                    "title": "Bridging the Acoustic-Semantic Gap with EchoX",
                    "desc": "EchoX is a speech-to-speech large language model (SLLM) designed to overcome the challenges of the acoustic-semantic gap in speech processing. By integrating semantic representations into its training, EchoX maintains strong reasoning capabilities that are often lost in traditional SLLMs. This model dynamically generates speech training targets, allowing it to effectively learn from both acoustic and semantic features. As a result, EchoX demonstrates superior performance on various knowledge-based benchmarks, showcasing its advanced capabilities in understanding and generating speech."
                },
                "zh": {
                    "title": "EchoX：打破声学与语义的壁垒",
                    "desc": "EchoX是一种语音到语音的大型语言模型，旨在解决声学与语义之间的差距。它通过整合语义表示，保持推理能力，从而在知识基础的基准测试中取得了优异的表现。当前的语音到语音模型在知识和推理能力上常常表现不佳，EchoX通过动态生成语音训练目标来克服这一限制。实验结果表明，EchoX在约六千小时的训练数据下，在多个知识问答基准上表现出色。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.09680",
            "title": "FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning\n  Dataset and Comprehensive Benchmark",
            "url": "https://huggingface.co/papers/2509.09680",
            "abstract": "FLUX-Reason-6M and PRISM-Bench address the lack of reasoning-focused datasets and benchmarks for text-to-image models, providing a large-scale dataset and evaluation standard to improve model performance.  \t\t\t\t\tAI-generated summary \t\t\t\t The advancement of open-source text-to-image (T2I) models has been hindered by the absence of large-scale, reasoning-focused datasets and comprehensive evaluation benchmarks, resulting in a performance gap compared to leading closed-source systems. To address this challenge, We introduce FLUX-Reason-6M and PRISM-Bench (Precise and Robust Image Synthesis Measurement Benchmark). FLUX-Reason-6M is a massive dataset consisting of 6 million high-quality FLUX-generated images and 20 million bilingual (English and Chinese) descriptions specifically designed to teach complex reasoning. The image are organized according to six key characteristics: Imagination, Entity, Text rendering, Style, Affection, and Composition, and design explicit Generation Chain-of-Thought (GCoT) to provide detailed breakdowns of image generation steps. The whole data curation takes 15,000 A100 GPU days, providing the community with a resource previously unattainable outside of large industrial labs. PRISM-Bench offers a novel evaluation standard with seven distinct tracks, including a formidable Long Text challenge using GCoT. Through carefully designed prompts, it utilizes advanced vision-language models for nuanced human-aligned assessment of prompt-image alignment and image aesthetics. Our extensive evaluation of 19 leading models on PRISM-Bench reveals critical performance gaps and highlights specific areas requiring improvement. Our dataset, benchmark, and evaluation code are released to catalyze the next wave of reasoning-oriented T2I generation. Project page: https://flux-reason-6m.github.io/ .",
            "score": 9,
            "issue_id": 5852,
            "pub_date": "2025-09-11",
            "pub_date_card": {
                "ru": "11 сентября",
                "en": "September 11",
                "zh": "9月11日"
            },
            "hash": "60acc7b8f0e01329",
            "authors": [
                "Rongyao Fang",
                "Aldrich Yu",
                "Chengqi Duan",
                "Linjiang Huang",
                "Shuai Bai",
                "Yuxuan Cai",
                "Kun Wang",
                "Si Liu",
                "Xihui Liu",
                "Hongsheng Li"
            ],
            "affiliations": [
                "Alibaba",
                "BUAA",
                "CUHK",
                "HKU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.09680.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#benchmark",
                    "#open_source",
                    "#long_context",
                    "#dataset",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Революция в обучении и оценке моделей текст-изображение",
                    "desc": "FLUX-Reason-6M и PRISM-Bench решают проблему отсутствия наборов данных и эталонов для оценки моделей преобразования текста в изображение, ориентированных на рассуждения. FLUX-Reason-6M представляет собой массивный датасет из 6 миллионов высококачественных изображений с 20 миллионами двуязычных описаний, специально разработанных для обучения сложным рассуждениям. PRISM-Bench предлагает новый стандарт оценки с семью различными направлениями, включая сложную задачу Long Text с использованием Generation Chain-of-Thought (GCoT). Обширная оценка 19 ведущих моделей на PRISM-Bench выявляет критические пробелы в производительности и подчеркивает конкретные области, требующие улучшения."
                },
                "en": {
                    "title": "Empowering Text-to-Image Models with Reasoning Datasets and Benchmarks",
                    "desc": "FLUX-Reason-6M and PRISM-Bench are initiatives aimed at enhancing text-to-image (T2I) models by providing a large-scale dataset and a robust evaluation framework. FLUX-Reason-6M includes 6 million images and 20 million bilingual descriptions that focus on teaching complex reasoning through structured characteristics. The dataset is meticulously curated using extensive computational resources, making it a valuable asset for researchers. PRISM-Bench introduces a new evaluation standard with multiple tracks to assess model performance, revealing significant gaps and guiding future improvements in T2I generation."
                },
                "zh": {
                    "title": "推动推理导向的文本到图像生成",
                    "desc": "FLUX-Reason-6M和PRISM-Bench旨在解决文本到图像模型缺乏以推理为重点的数据集和基准的问题。FLUX-Reason-6M是一个包含600万张高质量图像和2000万条双语描述的大型数据集，专门设计用于教授复杂的推理能力。PRISM-Bench提供了一个新的评估标准，包含七个不同的评估轨道，特别是一个使用生成链思维（GCoT）的长文本挑战。通过这些资源，我们希望推动推理导向的文本到图像生成的下一波发展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.09265",
            "title": "Harnessing Uncertainty: Entropy-Modulated Policy Gradients for\n  Long-Horizon LLM Agents",
            "url": "https://huggingface.co/papers/2509.09265",
            "abstract": "Entropy-Modulated Policy Gradients (EMPG) addresses learning dynamics issues in LLMs by recalibrating policy gradients based on uncertainty and task outcomes, leading to improved performance in long-horizon tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t In long-horizon tasks, recent agents based on Large Language Models (LLMs) face a significant challenge that sparse, outcome-based rewards make it difficult to assign credit to intermediate steps. Previous methods mainly focus on creating dense reward signals to guide learning, either through traditional reinforcement learning techniques like inverse reinforcement learning or by using Process Reward Models for step-by-step feedback. In this paper, we identify a fundamental problem in the learning dynamics of LLMs: the magnitude of policy gradients is inherently coupled with the entropy, which leads to inefficient small updates for confident correct actions and potentially destabilizes large updates for uncertain ones. To resolve this, we propose Entropy-Modulated Policy Gradients (EMPG), a framework that re-calibrates the learning signal based on step-wise uncertainty and the final task outcome. EMPG amplifies updates for confident correct actions, penalizes confident errors, and attenuates updates from uncertain steps to stabilize exploration. We further introduce a bonus term for future clarity that encourages agents to find more predictable solution paths. Through comprehensive experiments on three challenging agent tasks, WebShop, ALFWorld, and Deep Search, we demonstrate that EMPG achieves substantial performance gains and significantly outperforms strong policy gradient baselines. Project page is at https://empgseed-seed.github.io/",
            "score": 9,
            "issue_id": 5852,
            "pub_date": "2025-09-11",
            "pub_date_card": {
                "ru": "11 сентября",
                "en": "September 11",
                "zh": "9月11日"
            },
            "hash": "7850d32271ef8349",
            "authors": [
                "Jiawei Wang",
                "Jiacai Liu",
                "Yuqian Fu",
                "Yingru Li",
                "Xintao Wang",
                "Yuan Lin",
                "Yu Yue",
                "Lin Zhang",
                "Yang Wang",
                "Ke Wang"
            ],
            "affiliations": [
                "ByteDance Seed"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.09265.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#optimization",
                    "#rl",
                    "#training",
                    "#rlhf"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Улучшение обучения языковых моделей через энтропийную модуляцию градиентов",
                    "desc": "Метод EMPG (Entropy-Modulated Policy Gradients) решает проблемы динамики обучения в больших языковых моделях (LLM) при выполнении долгосрочных задач. Он перекалибрует градиенты политики на основе неопределенности и результатов задачи, что приводит к улучшению производительности. EMPG усиливает обновления для уверенных правильных действий, штрафует уверенные ошибки и ослабляет обновления от неопределенных шагов для стабилизации исследования. Эксперименты на трех сложных задачах показали, что EMPG значительно превосходит базовые методы градиента политики."
                },
                "en": {
                    "title": "Boosting Learning with Entropy Awareness",
                    "desc": "Entropy-Modulated Policy Gradients (EMPG) is a new approach that improves learning in Large Language Models (LLMs) by adjusting policy gradients based on uncertainty and task results. In long-horizon tasks, LLMs struggle with sparse rewards, making it hard to credit intermediate actions. EMPG addresses this by recalibrating the learning signal, enhancing updates for confident actions while reducing the impact of uncertain ones. This method leads to better performance in complex tasks, as shown in experiments with various challenging agent environments."
                },
                "zh": {
                    "title": "熵调制策略梯度：提升长时间任务学习效率的关键",
                    "desc": "本文提出了一种名为熵调制策略梯度（EMPG）的方法，旨在解决大型语言模型（LLMs）在长时间任务中的学习动态问题。通过根据不确定性和任务结果重新校准策略梯度，EMPG能够提高在稀疏奖励环境中的学习效率。该方法放大了对正确自信动作的更新，惩罚自信错误，并减弱来自不确定步骤的更新，从而稳定探索过程。实验结果表明，EMPG在多个复杂任务中显著提升了性能，超越了传统的策略梯度基线。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.09676",
            "title": "SpatialVID: A Large-Scale Video Dataset with Spatial Annotations",
            "url": "https://huggingface.co/papers/2509.09676",
            "abstract": "SpatialVID, a large-scale dataset with diverse videos and dense 3D annotations, enhances model generalization and performance in video and 3D vision research.  \t\t\t\t\tAI-generated summary \t\t\t\t Significant progress has been made in spatial intelligence, spanning both spatial reconstruction and world exploration. However, the scalability and real-world fidelity of current models remain severely constrained by the scarcity of large-scale, high-quality training data. While several datasets provide camera pose information, they are typically limited in scale, diversity, and annotation richness, particularly for real-world dynamic scenes with ground-truth camera motion. To this end, we collect SpatialVID, a dataset consists of a large corpus of in-the-wild videos with diverse scenes, camera movements and dense 3D annotations such as per-frame camera poses, depth, and motion instructions. Specifically, we collect more than 21,000 hours of raw video, and process them into 2.7 million clips through a hierarchical filtering pipeline, totaling 7,089 hours of dynamic content. A subsequent annotation pipeline enriches these clips with detailed spatial and semantic information, including camera poses, depth maps, dynamic masks, structured captions, and serialized motion instructions. Analysis of SpatialVID's data statistics reveals a richness and diversity that directly foster improved model generalization and performance, establishing it as a key asset for the video and 3D vision research community.",
            "score": 3,
            "issue_id": 5852,
            "pub_date": "2025-09-11",
            "pub_date_card": {
                "ru": "11 сентября",
                "en": "September 11",
                "zh": "9月11日"
            },
            "hash": "b2d981674edaecf5",
            "authors": [
                "Jiahao Wang",
                "Yufeng Yuan",
                "Rujie Zheng",
                "Youtian Lin",
                "Jian Gao",
                "Lin-Zhuo Chen",
                "Yajie Bao",
                "Yi Zhang",
                "Chang Zeng",
                "Yanxi Zhou",
                "Xiaoxiao Long",
                "Hao Zhu",
                "Zhaoxiang Zhang",
                "Xun Cao",
                "Yao Yao"
            ],
            "affiliations": [
                "Institute of Automation, Chinese Academy of Science",
                "Nanjing University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.09676.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#dataset",
                    "#video"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "SpatialVID: Большие данные для прорыва в пространственном интеллекте",
                    "desc": "SpatialVID - это масштабный набор данных, содержащий разнообразные видео с плотными 3D-аннотациями. Он включает более 21 000 часов необработанного видео, обработанного в 2,7 миллиона клипов общей продолжительностью 7 089 часов динамического контента. Датасет обогащен детальной пространственной и семантической информацией, включая позы камеры, карты глубины, динамические маски, структурированные подписи и сериализованные инструкции по движению. SpatialVID способствует улучшению обобщения и производительности моделей в исследованиях компьютерного зрения и 3D-реконструкции."
                },
                "en": {
                    "title": "Unlocking 3D Vision with SpatialVID: A New Era of Video Datasets",
                    "desc": "SpatialVID is a comprehensive dataset designed to improve machine learning models in video and 3D vision tasks. It contains over 21,000 hours of diverse, real-world video footage, which has been meticulously processed into 2.7 million clips. Each clip is enriched with dense 3D annotations, including camera poses, depth maps, and motion instructions, providing a rich source of training data. This dataset addresses the limitations of existing datasets by offering high-quality, large-scale data that enhances model generalization and performance in spatial intelligence applications."
                },
                "zh": {
                    "title": "SpatialVID：提升视频与3D视觉研究的关键数据集",
                    "desc": "SpatialVID是一个大规模的数据集，包含多样化的视频和密集的3D注释，旨在提升视频和3D视觉研究中的模型泛化能力和性能。该数据集收集了超过21,000小时的原始视频，并通过分层过滤管道处理成270万段视频片段，提供了丰富的动态内容。每个片段都附有详细的空间和语义信息，包括相机位姿、深度图、动态掩码、结构化标题和序列化运动指令。SpatialVID的数据统计分析显示出其丰富性和多样性，直接促进了模型的泛化和性能提升，成为视频和3D视觉研究领域的重要资产。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.09595",
            "title": "Kling-Avatar: Grounding Multimodal Instructions for Cascaded\n  Long-Duration Avatar Animation Synthesis",
            "url": "https://huggingface.co/papers/2509.09595",
            "abstract": "Kling-Avatar, a cascaded framework, enhances audio-driven avatar video generation by integrating multimodal instruction understanding with photorealistic portrait generation, resulting in high-fidelity, semantically grounded videos.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in audio-driven avatar video generation have significantly enhanced audio-visual realism. However, existing methods treat instruction conditioning merely as low-level tracking driven by acoustic or visual cues, without modeling the communicative purpose conveyed by the instructions. This limitation compromises their narrative coherence and character expressiveness. To bridge this gap, we introduce Kling-Avatar, a novel cascaded framework that unifies multimodal instruction understanding with photorealistic portrait generation. Our approach adopts a two-stage pipeline. In the first stage, we design a multimodal large language model (MLLM) director that produces a blueprint video conditioned on diverse instruction signals, thereby governing high-level semantics such as character motion and emotions. In the second stage, guided by blueprint keyframes, we generate multiple sub-clips in parallel using a first-last frame strategy. This global-to-local framework preserves fine-grained details while faithfully encoding the high-level intent behind multimodal instructions. Our parallel architecture also enables fast and stable generation of long-duration videos, making it suitable for real-world applications such as digital human livestreaming and vlogging. To comprehensively evaluate our method, we construct a benchmark of 375 curated samples covering diverse instructions and challenging scenarios. Extensive experiments demonstrate that Kling-Avatar is capable of generating vivid, fluent, long-duration videos at up to 1080p and 48 fps, achieving superior performance in lip synchronization accuracy, emotion and dynamic expressiveness, instruction controllability, identity preservation, and cross-domain generalization. These results establish Kling-Avatar as a new benchmark for semantically grounded, high-fidelity audio-driven avatar synthesis.",
            "score": 3,
            "issue_id": 5852,
            "pub_date": "2025-09-11",
            "pub_date_card": {
                "ru": "11 сентября",
                "en": "September 11",
                "zh": "9月11日"
            },
            "hash": "9128c939612e1d3e",
            "authors": [
                "Yikang Ding",
                "Jiwen Liu",
                "Wenyuan Zhang",
                "Zekun Wang",
                "Wentao Hu",
                "Liyuan Cui",
                "Mingming Lao",
                "Yingchao Shao",
                "Hui Liu",
                "Xiaohan Li",
                "Ming Chen",
                "Xiaoqiang Liu",
                "Yu-Shen Liu",
                "Pengfei Wan"
            ],
            "affiliations": [
                "Kuaishou Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.09595.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#story_generation",
                    "#video",
                    "#games"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Семантически обоснованные аватары с высокой детализацией",
                    "desc": "Kling-Avatar - это новая каскадная архитектура для генерации видео с аватарами на основе аудио. Она объединяет понимание мультимодальных инструкций с фотореалистичной генерацией портретов. На первом этапе мультимодальная языковая модель создает видео-макет, управляющий семантикой высокого уровня. На втором этапе генерируются отдельные фрагменты видео с сохранением мелких деталей и общего замысла."
                },
                "en": {
                    "title": "Kling-Avatar: Bridging Audio and Visual Realism in Avatar Generation",
                    "desc": "Kling-Avatar is a new framework designed to improve the generation of avatar videos driven by audio instructions. It combines understanding of multimodal instructions with the creation of realistic portraits, resulting in videos that are both visually appealing and semantically meaningful. The framework operates in two stages: first, it uses a large language model to create a blueprint video that captures high-level semantics like character emotions and movements. Then, it generates detailed sub-clips based on this blueprint, allowing for fast and stable production of long videos while maintaining high fidelity and expressiveness."
                },
                "zh": {
                    "title": "Kling-Avatar：音频驱动虚拟形象生成的新标杆",
                    "desc": "Kling-Avatar是一个级联框架，旨在提升音频驱动的虚拟形象视频生成。它通过整合多模态指令理解与逼真的肖像生成，生成高保真且语义明确的视频。该方法采用两阶段流程，首先利用多模态大语言模型生成蓝图视频，然后根据蓝图关键帧并行生成多个子片段。实验表明，Kling-Avatar在视频生成的清晰度、情感表达和指令控制等方面表现优异，适用于数字人直播和视频博客等实际应用。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.09118",
            "title": "Gradient-Attention Guided Dual-Masking Synergetic Framework for Robust\n  Text-based Person Retrieval",
            "url": "https://huggingface.co/papers/2509.09118",
            "abstract": "GA-DMS framework enhances CLIP for person representation learning by improving data quality and model architecture, achieving state-of-the-art performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Although Contrastive Language-Image Pre-training (CLIP) exhibits strong performance across diverse vision tasks, its application to person representation learning faces two critical challenges: (i) the scarcity of large-scale annotated vision-language data focused on person-centric images, and (ii) the inherent limitations of global contrastive learning, which struggles to maintain discriminative local features crucial for fine-grained matching while remaining vulnerable to noisy text tokens. This work advances CLIP for person representation learning through synergistic improvements in data curation and model architecture. First, we develop a noise-resistant data construction pipeline that leverages the in-context learning capabilities of MLLMs to automatically filter and caption web-sourced images. This yields WebPerson, a large-scale dataset of 5M high-quality person-centric image-text pairs. Second, we introduce the GA-DMS (Gradient-Attention Guided Dual-Masking Synergetic) framework, which improves cross-modal alignment by adaptively masking noisy textual tokens based on the gradient-attention similarity score. Additionally, we incorporate masked token prediction objectives that compel the model to predict informative text tokens, enhancing fine-grained semantic representation learning. Extensive experiments show that GA-DMS achieves state-of-the-art performance across multiple benchmarks.",
            "score": 3,
            "issue_id": 5853,
            "pub_date": "2025-09-11",
            "pub_date_card": {
                "ru": "11 сентября",
                "en": "September 11",
                "zh": "9月11日"
            },
            "hash": "7693d45e6980cf3f",
            "authors": [
                "Tianlu Zheng",
                "Yifan Zhang",
                "Xiang An",
                "Ziyong Feng",
                "Kaicheng Yang",
                "Qichuan Ding"
            ],
            "affiliations": [
                "DeepGlint",
                "Northeastern University",
                "South China University of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.09118.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#optimization",
                    "#benchmark",
                    "#transfer_learning",
                    "#dataset",
                    "#architecture"
                ],
                "emoji": "👤",
                "ru": {
                    "title": "Улучшение CLIP для точного распознавания людей",
                    "desc": "Статья представляет GA-DMS фреймворк для улучшения CLIP в задаче обучения представлений людей. Авторы разработали конвейер для создания высококачественного датасета WebPerson из 5 миллионов пар изображение-текст. GA-DMS использует адаптивное маскирование шумных текстовых токенов на основе оценки градиентно-аттенционного сходства. Дополнительно вводятся цели предсказания замаскированных токенов для улучшения обучения семантических представлений."
                },
                "en": {
                    "title": "Enhancing CLIP for Superior Person Representation Learning",
                    "desc": "The GA-DMS framework enhances the CLIP model for person representation learning by addressing data quality and model architecture challenges. It introduces a new data construction pipeline that uses MLLMs to create a large dataset of 5 million high-quality person-centric image-text pairs, called WebPerson. The framework also employs a dual-masking technique that adapts to noisy text tokens, improving the model's ability to align visual and textual information. As a result, GA-DMS achieves state-of-the-art performance in various benchmarks for person representation tasks."
                },
                "zh": {
                    "title": "GA-DMS：提升CLIP的人物表示学习",
                    "desc": "GA-DMS框架通过改进数据质量和模型架构，增强了CLIP在人物表示学习中的表现。该研究解决了人物中心图像的标注数据稀缺和全局对比学习的局限性。我们开发了一种抗噪声的数据构建流程，生成了一个包含500万高质量人物图像-文本对的大型数据集WebPerson。GA-DMS框架通过基于梯度注意力相似度分数自适应地屏蔽噪声文本标记，显著提高了跨模态对齐能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.09614",
            "title": "LoCoBench: A Benchmark for Long-Context Large Language Models in Complex\n  Software Engineering",
            "url": "https://huggingface.co/papers/2509.09614",
            "abstract": "LoCoBench evaluates long-context language models in complex software development scenarios, addressing the gap in understanding entire codebases and maintaining architectural consistency across large-scale systems.  \t\t\t\t\tAI-generated summary \t\t\t\t The emergence of long-context language models with context windows extending to millions of tokens has created new opportunities for sophisticated code understanding and software development evaluation. We propose LoCoBench, a comprehensive benchmark specifically designed to evaluate long-context LLMs in realistic, complex software development scenarios. Unlike existing code evaluation benchmarks that focus on single-function completion or short-context tasks, LoCoBench addresses the critical evaluation gap for long-context capabilities that require understanding entire codebases, reasoning across multiple files, and maintaining architectural consistency across large-scale software systems. Our benchmark provides 8,000 evaluation scenarios systematically generated across 10 programming languages, with context lengths spanning 10K to 1M tokens, a 100x variation that enables precise assessment of long-context performance degradation in realistic software development settings. LoCoBench introduces 8 task categories that capture essential long-context capabilities: architectural understanding, cross-file refactoring, multi-session development, bug investigation, feature implementation, code comprehension, integration testing, and security analysis. Through a 5-phase pipeline, we create diverse, high-quality scenarios that challenge LLMs to reason about complex codebases at unprecedented scale. We introduce a comprehensive evaluation framework with 17 metrics across 4 dimensions, including 8 new evaluation metrics, combined in a LoCoBench Score (LCBS). Our evaluation of state-of-the-art long-context models reveals substantial performance gaps, demonstrating that long-context understanding in complex software development represents a significant unsolved challenge that demands more attention. LoCoBench is released at: https://github.com/SalesforceAIResearch/LoCoBench.",
            "score": 2,
            "issue_id": 5852,
            "pub_date": "2025-09-11",
            "pub_date_card": {
                "ru": "11 сентября",
                "en": "September 11",
                "zh": "9月11日"
            },
            "hash": "15a23d38c535fc1c",
            "authors": [
                "Jielin Qiu",
                "Zuxin Liu",
                "Zhiwei Liu",
                "Rithesh Murthy",
                "Jianguo Zhang",
                "Haolin Chen",
                "Shiyu Wang",
                "Ming Zhu",
                "Liangwei Yang",
                "Juntao Tan",
                "Zhepeng Cen",
                "Cheng Qian",
                "Shelby Heinecke",
                "Weiran Yao",
                "Silvio Savarese",
                "Caiming Xiong",
                "Huan Wang"
            ],
            "affiliations": [
                "Salesforce AI Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.09614.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#long_context"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "LoCoBench: новый рубеж в оценке ИИ для разработки ПО",
                    "desc": "LoCoBench - это новый бенчмарк для оценки языковых моделей с длинным контекстом в сложных сценариях разработки программного обеспечения. Он включает 8000 сценариев оценки на 10 языках программирования с контекстом от 10 тыс. до 1 млн токенов. LoCoBench вводит 8 категорий задач, охватывающих ключевые возможности работы с длинным контекстом, такие как понимание архитектуры, рефакторинг между файлами и анализ безопасности. Оценка современных моделей с длинным контекстом выявила значительные пробелы в производительности, показывая, что понимание длинного контекста в сложной разработке ПО остается нерешенной проблемой."
                },
                "en": {
                    "title": "Evaluating Long-Context LLMs for Complex Software Development",
                    "desc": "LoCoBench is a new benchmark designed to evaluate long-context language models (LLMs) in complex software development tasks. It focuses on understanding entire codebases and maintaining architectural consistency, which is crucial for large-scale systems. The benchmark includes 8,000 scenarios across 10 programming languages, with context lengths ranging from 10,000 to 1,000,000 tokens, allowing for a thorough assessment of LLM performance. By introducing 8 task categories and a comprehensive evaluation framework, LoCoBench highlights significant performance gaps in current models, emphasizing the need for improved long-context understanding in software development."
                },
                "zh": {
                    "title": "评估长上下文模型的全新基准",
                    "desc": "LoCoBench是一个专门评估长上下文语言模型在复杂软件开发场景中的基准测试工具。它填补了对整个代码库理解和在大规模系统中保持架构一致性的评估空白。该基准提供了8000个评估场景，涵盖10种编程语言，能够精确评估长上下文性能的下降。通过引入多种任务类别和评估指标，LoCoBench为长上下文理解在软件开发中的挑战提供了全面的评估框架。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.09286",
            "title": "Visual Programmability: A Guide for Code-as-Thought in Chart\n  Understanding",
            "url": "https://huggingface.co/papers/2509.09286",
            "abstract": "VLMs are enhanced with an adaptive framework that selects between code-based and direct visual reasoning for chart understanding, improving performance and robustness.  \t\t\t\t\tAI-generated summary \t\t\t\t Chart understanding presents a critical test to the reasoning capabilities of Vision-Language Models (VLMs). Prior approaches face critical limitations: some rely on external tools, making them brittle and constrained by a predefined toolkit, while others fine-tune specialist models that often adopt a single reasoning strategy, such as text-based chain-of-thought (CoT). The intermediate steps of text-based reasoning are difficult to verify, which complicates the use of reinforcement-learning signals that reward factual accuracy. To address this, we propose a Code-as-Thought (CaT) approach to represent the visual information of a chart in a verifiable, symbolic format. Our key insight is that this strategy must be adaptive: a fixed, code-only implementation consistently fails on complex charts where symbolic representation is unsuitable. This finding leads us to introduce Visual Programmability: a learnable property that determines if a chart-question pair is better solved with code or direct visual analysis. We implement this concept in an adaptive framework where a VLM learns to choose between the CaT pathway and a direct visual reasoning pathway. The selection policy of the model is trained with reinforcement learning using a novel dual-reward system. This system combines a data-accuracy reward to ground the model in facts and prevent numerical hallucination, with a decision reward that teaches the model when to use each strategy, preventing it from defaulting to a single reasoning mode. Experiments demonstrate strong and robust performance across diverse chart-understanding benchmarks. Our work shows that VLMs can be taught not only to reason but also how to reason, dynamically selecting the optimal reasoning pathway for each task.",
            "score": 1,
            "issue_id": 5853,
            "pub_date": "2025-09-11",
            "pub_date_card": {
                "ru": "11 сентября",
                "en": "September 11",
                "zh": "9月11日"
            },
            "hash": "73de225642b07635",
            "authors": [
                "Bohao Tang",
                "Yan Ma",
                "Fei Zhang",
                "Jiadi Su",
                "Ethan Chern",
                "Zhulin Hu",
                "Zhixin Wang",
                "Pengfei Liu",
                "Ya Zhang"
            ],
            "affiliations": [
                "Fudan University",
                "Shanghai Innovation Institute",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.09286.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#rl",
                    "#benchmark",
                    "#multimodal",
                    "#training",
                    "#hallucinations"
                ],
                "emoji": "📊",
                "ru": {
                    "title": "Адаптивное рассуждение VLM для улучшенного понимания графиков",
                    "desc": "Статья представляет адаптивный фреймворк для улучшения понимания графиков визуально-языковыми моделями (VLM). Предложен подход Code-as-Thought (CaT), который представляет визуальную информацию графика в символьном формате. Введено понятие визуальной программируемости - обучаемого свойства, определяющего оптимальный метод решения для пары график-вопрос. Фреймворк обучается выбирать между CaT и прямым визуальным анализом с помощью обучения с подкреплением, используя двойную систему вознаграждений."
                },
                "en": {
                    "title": "Adaptive Reasoning for Enhanced Chart Understanding in VLMs",
                    "desc": "This paper introduces an adaptive framework for Vision-Language Models (VLMs) that enhances their ability to understand charts by selecting between code-based reasoning and direct visual analysis. The authors highlight the limitations of previous methods, which either rely on rigid external tools or single reasoning strategies that are hard to verify. They propose a novel approach called Code-as-Thought (CaT) that represents visual information in a symbolic format, allowing for better verification and accuracy. By implementing Visual Programmability, the model learns to dynamically choose the most effective reasoning pathway for each chart-question pair, leading to improved performance across various benchmarks."
                },
                "zh": {
                    "title": "动态选择最佳推理路径的视觉语言模型",
                    "desc": "本文提出了一种增强视觉语言模型（VLMs）的方法，通过自适应框架在代码基础推理和直接视觉推理之间进行选择，以提高图表理解的性能和鲁棒性。以往的方法存在局限性，依赖外部工具或单一推理策略，导致在复杂图表上表现不佳。我们引入了代码作为思维（CaT）的方法，将图表的视觉信息以可验证的符号格式表示，并提出视觉可编程性，允许模型根据图表和问题的特性选择最佳的推理方式。通过强化学习训练模型的选择策略，结合数据准确性奖励和决策奖励，确保模型在不同任务中动态选择最优推理路径。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.09332",
            "title": "OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and\n  Embodiment-aware Reasoning",
            "url": "https://huggingface.co/papers/2509.09332",
            "abstract": "OmniEVA addresses spatial and embodiment gaps in multimodal large language models for embodied intelligence through a task-adaptive 3D grounding mechanism and an embodiment-aware reasoning framework, achieving state-of-the-art performance across diverse tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in multimodal large language models (MLLMs) have opened new opportunities for embodied intelligence, enabling multimodal understanding, reasoning, and interaction, as well as continuous spatial decision-making. Nevertheless, current MLLM-based embodied systems face two critical limitations. First, Geometric Adaptability Gap: models trained solely on 2D inputs or with hard-coded 3D geometry injection suffer from either insufficient spatial information or restricted 2D generalization, leading to poor adaptability across tasks with diverse spatial demands. Second, Embodiment Constraint Gap: prior work often neglects the physical constraints and capacities of real robots, resulting in task plans that are theoretically valid but practically infeasible.To address these gaps, we introduce OmniEVA -- an embodied versatile planner that enables advanced embodied reasoning and task planning through two pivotal innovations: (1) a Task-Adaptive 3D Grounding mechanism, which introduces a gated router to perform explicit selective regulation of 3D fusion based on contextual requirements, enabling context-aware 3D grounding for diverse embodied tasks. (2) an Embodiment-Aware Reasoning framework that jointly incorporates task goals and embodiment constraints into the reasoning loop, resulting in planning decisions that are both goal-directed and executable. Extensive experimental results demonstrate that OmniEVA not only achieves state-of-the-art general embodied reasoning performance, but also exhibits a strong ability across a wide range of downstream scenarios. Evaluations of a suite of proposed embodied benchmarks, including both primitive and composite tasks, confirm its robust and versatile planning capabilities. Project page: https://omnieva.github.io",
            "score": 0,
            "issue_id": 5853,
            "pub_date": "2025-09-11",
            "pub_date_card": {
                "ru": "11 сентября",
                "en": "September 11",
                "zh": "9月11日"
            },
            "hash": "65192793a84b5158",
            "authors": [
                "Yuecheng Liu",
                "Dafeng Chi",
                "Shiguang Wu",
                "Zhanguang Zhang",
                "Yuzheng Zhuang",
                "Bowen Yang",
                "He Zhu",
                "Lingfeng Zhang",
                "Pengwei Xie",
                "David Gamaliel Arcos Bravo",
                "Yingxue Zhang",
                "Jianye Hao",
                "Xingyue Quan"
            ],
            "affiliations": [
                "Huawei Noahs Ark Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.09332.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#reasoning",
                    "#benchmark",
                    "#multimodal",
                    "#games",
                    "#3d"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "OmniEVA: Универсальный планировщик для воплощенного ИИ",
                    "desc": "OmniEVA - это новая система для воплощенного искусственного интеллекта, которая решает проблемы пространственной адаптации и учета физических ограничений роботов. Она использует механизм адаптивной 3D-привязки к задаче и фреймворк рассуждений с учетом воплощения. OmniEVA демонстрирует передовую производительность в различных задачах воплощенного интеллекта. Система успешно применяется как для простых, так и для составных задач робототехники."
                },
                "en": {
                    "title": "Bridging Gaps for Smarter Embodied Intelligence",
                    "desc": "OmniEVA is a new approach that improves how large language models understand and interact with the physical world. It tackles two main problems: the Geometric Adaptability Gap, which limits models trained on 2D data from effectively handling 3D tasks, and the Embodiment Constraint Gap, where models fail to consider the real-world limitations of robots. The paper introduces a Task-Adaptive 3D Grounding mechanism that allows the model to adjust its understanding of 3D space based on the task at hand. Additionally, it presents an Embodiment-Aware Reasoning framework that ensures planning decisions are practical and achievable, leading to superior performance in various embodied tasks."
                },
                "zh": {
                    "title": "OmniEVA：提升具身智能的多模态推理能力",
                    "desc": "OmniEVA 是一种新型的多模态大语言模型，旨在解决在具身智能中的空间和具身性差距。它通过任务自适应的三维定位机制和具身感知推理框架，提升了模型在多样化任务中的表现。该模型能够根据上下文需求进行三维信息的选择性融合，从而实现更好的空间理解和决策。实验结果表明，OmniEVA 在多种下游任务中展现了卓越的推理能力和灵活的规划能力。"
                }
            }
        }
    ],
    "link_prev": "2025-09-11.html",
    "link_next": "2025-09-15.html",
    "link_month": "2025-09.html",
    "short_date_prev": {
        "ru": "11.09",
        "en": "09/11",
        "zh": "9月11日"
    },
    "short_date_next": {
        "ru": "15.09",
        "en": "09/15",
        "zh": "9月15日"
    },
    "categories": {
        "#dataset": 5,
        "#data": 1,
        "#benchmark": 7,
        "#agents": 3,
        "#cv": 0,
        "#rl": 3,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 2,
        "#audio": 1,
        "#video": 2,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 1,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 0,
        "#reasoning": 5,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 1,
        "#hallucinations": 1,
        "#long_context": 2,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    }
}