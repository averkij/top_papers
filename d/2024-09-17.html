
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 15 papers. September 17.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #0989eacf;
            --secondary-color: #fff;
            --background-color: #f5f5f5;
            --text-color: #333333;
            --header-color: #0989eacf;
            --body-color: #f5f5f5;
            --menu-color: #002370;
        }        
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 0;
            line-height: 1;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 2em;
            padding: 10px 0 20px 0;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.tags {
            color: #555;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);
            transition: background-color 0.2s ease;
            display: flex;
            flex-direction: column;
            position: relative;
        }
        .article-content {
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
        }
        body.dark-theme>div>main>article {
            background-color: #444;
        }
        body.light-theme>div>main>article {
            background-color: #fff;
        }
        body.dark-theme>div>main>article:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
        }
        .pub-date {
            font-size: 0.9em;
            margin-bottom: 0.8em;
            font-weight: 300;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 1em;
            position: absolute;
            bottom: 10px;
            font-weight: 300;
            font-family: 'Roboto Slab';
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 80px;
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        a:hover {
            color: #e73838;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            padding: 1em 0;
            margin-top: 2em;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 0px;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }
        
        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">17 сентября</span> | <span id="title-articles-count">15 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item" id="nav-prev"><a href="/d/2024-09-16.html">⬅️ <span id="prev-date">16.09</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-09-18.html">➡️ <span id="next-date">18.09</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2024-09.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '17 сентября', 'en': 'September 17', 'zh': '9月17日'};
        let feedDateNext = {'ru': '18.09', 'en': '09/18', 'zh': '9月18日'};
        let feedDatePrev = {'ru': '16.09', 'en': '09/16', 'zh': '9月16日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'Статья от ', 'en': 'Published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2409.09214', 'title': 'Seed-Music: A Unified Framework for High Quality and Controlled Music Generation', 'url': 'https://huggingface.co/papers/2409.09214', 'abstract': 'We introduce Seed-Music, a suite of music generation systems capable of producing high-quality music with fine-grained style control. Our unified framework leverages both auto-regressive language modeling and diffusion approaches to support two key music creation workflows: controlled music generation and post-production editing. For controlled music generation, our system enables vocal music generation with performance controls from multi-modal inputs, including style descriptions, audio references, musical scores, and voice prompts. For post-production editing, it offers interactive tools for editing lyrics and vocal melodies directly in the generated audio.   We encourage readers to listen to demo audio examples at https://team.doubao.com/seed-music .', 'score': 46, 'issue_id': 1, 'pub_date': '2024-09-13', 'pub_date_card': {'ru': '13 сентября', 'en': 'September 13', 'zh': '9月13日'}, 'hash': 'ece3dd3f1fb1597d', 'data': {'categories': ['#audio', '#games', '#diffusion', '#architecture', '#multimodal'], 'emoji': '🎵', 'ru': {'title': 'Seed-Music: ИИ-композитор с точным контролем стиля', 'desc': 'Статья представляет Seed-Music - набор систем для генерации музыки с точным контролем стиля. Фреймворк использует авторегрессионное языковое моделирование и диффузионные подходы для двух основных сценариев: контролируемая генерация музыки и постобработка. Система позволяет генерировать вокальную музыку с контролем исполнения на основе многомодальных входных данных. Также предоставляются интерактивные инструменты для редактирования текстов и вокальных мелодий непосредственно в сгенерированном аудио.'}, 'en': {'title': 'Create and Edit Music with Style Control!', 'desc': 'Seed-Music is a music generation system that creates high-quality music while allowing users to control the style of the output. It combines auto-regressive language modeling with diffusion techniques to facilitate two main workflows: generating music with specific controls and editing existing music. Users can generate vocal music by providing various inputs such as style descriptions and audio references, enabling a tailored music creation experience. Additionally, the system includes interactive tools for editing lyrics and melodies, enhancing the post-production process.'}, 'zh': {'title': '高质量音乐生成与风格控制的创新系统', 'desc': 'Seed-Music 是一个音乐生成系统，能够生成高质量的音乐并实现细致的风格控制。该框架结合了自回归语言建模和扩散方法，支持两种主要的音乐创作工作流程：受控音乐生成和后期制作编辑。对于受控音乐生成，我们的系统可以根据多模态输入（如风格描述、音频参考、乐谱和语音提示）生成声乐音乐。在后期制作编辑方面，它提供了交互式工具，可以直接在生成的音频中编辑歌词和声乐旋律。'}}}, {'id': 'https://huggingface.co/papers/2409.10594', 'title': 'Kolmogorov-Arnold Transformer', 'url': 'https://huggingface.co/papers/2409.10594', 'abstract': 'Transformers stand as the cornerstone of mordern deep learning. Traditionally, these models rely on multi-layer perceptron (MLP) layers to mix the information between channels. In this paper, we introduce the Kolmogorov-Arnold Transformer (KAT), a novel architecture that replaces MLP layers with Kolmogorov-Arnold Network (KAN) layers to enhance the expressiveness and performance of the model. Integrating KANs into transformers, however, is no easy feat, especially when scaled up. Specifically, we identify three key challenges: (C1) Base function. The standard B-spline function used in KANs is not optimized for parallel computing on modern hardware, resulting in slower inference speeds. (C2) Parameter and Computation Inefficiency. KAN requires a unique function for each input-output pair, making the computation extremely large. (C3) Weight initialization. The initialization of weights in KANs is particularly challenging due to their learnable activation functions, which are critical for achieving convergence in deep neural networks. To overcome the aforementioned challenges, we propose three key solutions: (S1) Rational basis. We replace B-spline functions with rational functions to improve compatibility with modern GPUs. By implementing this in CUDA, we achieve faster computations. (S2) Group KAN. We share the activation weights through a group of neurons, to reduce the computational load without sacrificing performance. (S3) Variance-preserving initialization. We carefully initialize the activation weights to make sure that the activation variance is maintained across layers. With these designs, KAT scales effectively and readily outperforms traditional MLP-based transformers.', 'score': 38, 'issue_id': 1, 'pub_date': '2024-09-16', 'pub_date_card': {'ru': '16 сентября', 'en': 'September 16', 'zh': '9月16日'}, 'hash': '95aae0e4700cae7e', 'data': {'categories': ['#training', '#inference', '#optimization', '#transformers', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'КАТ: Революция в архитектуре трансформеров', 'desc': 'В статье представлен Колмогоров-Арнольд Трансформер (КАТ) - новая архитектура, заменяющая слои MLP в трансформерах на слои сети Колмогорова-Арнольда (KAN) для повышения выразительности и производительности модели. Авторы выявили три ключевые проблемы при интеграции KAN в трансформеры: базовая функция, неэффективность параметров и вычислений, а также инициализация весов. Для решения этих проблем предложены три ключевых решения: рациональный базис, групповой KAN и инициализация с сохранением дисперсии. Благодаря этим улучшениям КАТ эффективно масштабируется и превосходит традиционные трансформеры на основе MLP.'}, 'en': {'title': 'Revolutionizing Transformers with Kolmogorov-Arnold Networks', 'desc': 'This paper presents the Kolmogorov-Arnold Transformer (KAT), a new type of transformer model that replaces traditional multi-layer perceptron (MLP) layers with Kolmogorov-Arnold Network (KAN) layers. The authors address three main challenges in integrating KANs: optimizing the base function for parallel computing, managing the large computation requirements, and effectively initializing weights. To tackle these issues, they propose using rational functions for better GPU compatibility, sharing activation weights among neurons to reduce computation, and implementing a variance-preserving weight initialization strategy. The results show that KAT significantly improves performance compared to conventional MLP-based transformers.'}, 'zh': {'title': 'KAT：提升变换器性能的新架构', 'desc': '本文介绍了一种新的变换器架构，称为Kolmogorov-Arnold变换器（KAT），它用Kolmogorov-Arnold网络（KAN）层替代了传统的多层感知器（MLP）层，以提高模型的表现力和性能。我们识别了在将KAN集成到变换器中时面临的三个主要挑战，包括基础函数的选择、参数和计算效率低下以及权重初始化的困难。为了解决这些问题，我们提出了三种关键解决方案：使用有理函数替代B样条函数以提高计算速度，采用组KAN来减少计算负担，以及通过方差保持初始化来确保激活方差在层间保持。通过这些设计，KAT在规模扩展方面表现出色，并且在性能上超越了传统的基于MLP的变换器。'}}}, {'id': 'https://huggingface.co/papers/2409.10516', 'title': 'RetrievalAttention: Accelerating Long-Context LLM Inference via Vector Retrieval', 'url': 'https://huggingface.co/papers/2409.10516', 'abstract': 'Transformer-based large Language Models (LLMs) become increasingly important in various domains. However, the quadratic time complexity of attention operation poses a significant challenge for scaling to longer contexts due to the extremely high inference latency and GPU memory consumption for caching key-value (KV) vectors. This paper proposes RetrievalAttention, a training-free approach to accelerate attention computation. To leverage the dynamic sparse property of attention, RetrievalAttention builds approximate nearest neighbor search (ANNS) indexes upon KV vectors in CPU memory and retrieves the most relevant ones via vector search during generation. Due to the out-of-distribution (OOD) between query vectors and key vectors, off-the-shelf ANNS indexes still need to scan O(N) (usually 30% of all keys) data for accurate retrieval, which fails to exploit the high sparsity. RetrievalAttention first identifies the OOD challenge of ANNS-based attention, and addresses it via an attention-aware vector search algorithm that can adapt to queries and only access 1--3% of data, thus achieving a sub-linear time complexity. RetrievalAttention greatly reduces the inference cost of long-context LLM with much lower GPU memory requirements while maintaining the model accuracy. Especially, RetrievalAttention only needs 16GB GPU memory for serving 128K tokens in LLMs with 8B parameters, which is capable of generating one token in 0.188 seconds on a single NVIDIA RTX4090 (24GB).', 'score': 37, 'issue_id': 1, 'pub_date': '2024-09-16', 'pub_date_card': {'ru': '16 сентября', 'en': 'September 16', 'zh': '9月16日'}, 'hash': '9119afe59a2800b3', 'data': {'categories': ['#long_context', '#rag', '#inference', '#optimization', '#architecture'], 'emoji': '🚀', 'ru': {'title': 'Эффективное внимание для длинных контекстов в больших языковых моделях', 'desc': 'Эта статья представляет RetrievalAttention - подход для ускорения вычислений внимания в трансформерных моделях без дополнительного обучения. Метод использует приближенный поиск ближайших соседей для извлечения наиболее релевантных ключевых векторов из CPU-памяти во время генерации. Авторы разработали алгоритм поиска векторов, учитывающий особенности механизма внимания, который обращается только к 1-3% данных, достигая сублинейной временной сложности. RetrievalAttention значительно снижает вычислительные затраты и требования к GPU-памяти при работе с длинными контекстами, сохраняя точность модели.'}, 'en': {'title': 'Accelerating Attention with RetrievalAttention for Efficient LLMs', 'desc': 'This paper introduces RetrievalAttention, a novel method designed to improve the efficiency of attention computation in large language models (LLMs). It addresses the challenge of high time complexity and memory usage associated with traditional attention mechanisms, especially for long contexts. By utilizing approximate nearest neighbor search (ANNS) to dynamically retrieve relevant key-value vectors, RetrievalAttention significantly reduces the amount of data accessed during inference. This approach not only lowers GPU memory requirements but also maintains model accuracy, allowing for faster token generation in LLMs with large parameter sizes.'}, 'zh': {'title': '加速长文本处理的RetrievalAttention方法', 'desc': '本文提出了一种名为RetrievalAttention的方法，旨在加速基于Transformer的大型语言模型的注意力计算。传统的注意力机制在处理长文本时面临时间复杂度高和内存消耗大的问题。RetrievalAttention通过在CPU内存中构建近似最近邻搜索索引，动态检索与查询向量最相关的键值对，从而降低计算成本。该方法显著减少了推理时的GPU内存需求，同时保持了模型的准确性，能够在较低的延迟下处理长上下文。'}}}, {'id': 'https://huggingface.co/papers/2409.10173', 'title': 'jina-embeddings-v3: Multilingual Embeddings With Task LoRA', 'url': 'https://huggingface.co/papers/2409.10173', 'abstract': 'We introduce jina-embeddings-v3, a novel text embedding model with 570 million parameters, achieves state-of-the-art performance on multilingual data and long-context retrieval tasks, supporting context lengths of up to 8192 tokens. The model includes a set of task-specific Low-Rank Adaptation (LoRA) adapters to generate high-quality embeddings for query-document retrieval, clustering, classification, and text matching. Additionally, Matryoshka Representation Learning is integrated into the training process, allowing flexible truncation of embedding dimensions without compromising performance. Evaluation on the MTEB benchmark shows that jina-embeddings-v3 outperforms the latest proprietary embeddings from OpenAI and Cohere on English tasks, while achieving superior performance compared to multilingual-e5-large-instruct across all multilingual tasks.', 'score': 23, 'issue_id': 1, 'pub_date': '2024-09-16', 'pub_date_card': {'ru': '16 сентября', 'en': 'September 16', 'zh': '9月16日'}, 'hash': '885ccda1522de531', 'data': {'categories': ['#dataset', '#long_context', '#multilingual', '#training', '#transfer_learning', '#benchmark', '#small_models', '#architecture', '#synthetic'], 'emoji': '🌐', 'ru': {'title': 'Универсальные многоязычные эмбеддинги для длинных текстов', 'desc': 'Модель jina-embeddings-v3 - это новая модель текстовых эмбеддингов с 570 миллионами параметров, достигающая передовых результатов в многоязычных задачах и задачах поиска с длинным контекстом. Она включает набор специализированных Low-Rank Adaptation (LoRA) адаптеров для генерации высококачественных эмбеддингов для различных задач. В процесс обучения интегрировано Matryoshka Representation Learning, позволяющее гибко уменьшать размерность эмбеддингов без потери производительности. Оценка на бенчмарке MTEB показывает, что jina-embeddings-v3 превосходит последние проприетарные эмбеддинги от OpenAI и Cohere на английских задачах, а также превосходит multilingual-e5-large-instruct на всех многоязычных задачах.'}, 'en': {'title': 'Revolutionizing Text Embeddings with Jina-Embeddings-V3', 'desc': 'The paper presents jina-embeddings-v3, a powerful text embedding model with 570 million parameters designed for multilingual and long-context retrieval tasks. It utilizes Low-Rank Adaptation (LoRA) adapters to enhance the quality of embeddings for various applications like query-document retrieval and classification. The model also incorporates Matryoshka Representation Learning, which allows for flexible adjustment of embedding dimensions while maintaining high performance. Evaluation results indicate that jina-embeddings-v3 surpasses existing proprietary models on English tasks and excels in multilingual scenarios.'}, 'zh': {'title': 'jina-embeddings-v3：多语言文本嵌入的新标杆', 'desc': '我们介绍了jina-embeddings-v3，这是一种新型的文本嵌入模型，具有5.7亿个参数，在多语言数据和长上下文检索任务中表现出色，支持最长8192个标记的上下文长度。该模型包含一组特定任务的低秩适配器（LoRA），用于生成高质量的查询-文档检索、聚类、分类和文本匹配的嵌入。通过集成马特里奥什卡表示学习，模型在训练过程中可以灵活截断嵌入维度，而不影响性能。评估结果显示，jina-embeddings-v3在MTEB基准测试中超越了OpenAI和Cohere的最新专有嵌入，在英语任务上表现优异，并在所有多语言任务中优于multilingual-e5-large-instruct。'}}}, {'id': 'https://huggingface.co/papers/2409.09502', 'title': 'One missing piece in Vision and Language: A Survey on Comics Understanding', 'url': 'https://huggingface.co/papers/2409.09502', 'abstract': 'Vision-language models have recently evolved into versatile systems capable of high performance across a range of tasks, such as document understanding, visual question answering, and grounding, often in zero-shot settings. Comics Understanding, a complex and multifaceted field, stands to greatly benefit from these advances. Comics, as a medium, combine rich visual and textual narratives, challenging AI models with tasks that span image classification, object detection, instance segmentation, and deeper narrative comprehension through sequential panels. However, the unique structure of comics -- characterized by creative variations in style, reading order, and non-linear storytelling -- presents a set of challenges distinct from those in other visual-language domains. In this survey, we present a comprehensive review of Comics Understanding from both dataset and task perspectives. Our contributions are fivefold: (1) We analyze the structure of the comics medium, detailing its distinctive compositional elements; (2) We survey the widely used datasets and tasks in comics research, emphasizing their role in advancing the field; (3) We introduce the Layer of Comics Understanding (LoCU) framework, a novel taxonomy that redefines vision-language tasks within comics and lays the foundation for future work; (4) We provide a detailed review and categorization of existing methods following the LoCU framework; (5) Finally, we highlight current research challenges and propose directions for future exploration, particularly in the context of vision-language models applied to comics. This survey is the first to propose a task-oriented framework for comics intelligence and aims to guide future research by addressing critical gaps in data availability and task definition. A project associated with this survey is available at https://github.com/emanuelevivoli/awesome-comics-understanding.', 'score': 23, 'issue_id': 1, 'pub_date': '2024-09-14', 'pub_date_card': {'ru': '14 сентября', 'en': 'September 14', 'zh': '9月14日'}, 'hash': 'a9ad5cf3e4c1277e', 'data': {'categories': ['#survey', '#dataset', '#cv', '#benchmark', '#games', '#open_source', '#architecture', '#multimodal'], 'emoji': '🦸', 'ru': {'title': 'Новый взгляд на ИИ для комиксов: от пикселей к сюжетам', 'desc': 'Эта статья представляет собой обзор области понимания комиксов искусственным интеллектом. Авторы анализируют структуру комиксов, существующие наборы данных и задачи, а также предлагают новую таксономию задач понимания комиксов (LoCU). В работе рассматриваются текущие методы в соответствии с этой таксономией и обсуждаются проблемы и направления будущих исследований. Особое внимание уделяется применению мультимодальных моделей, объединяющих зрение и язык, к задаче понимания комиксов.'}, 'en': {'title': 'Unlocking the Narrative: Advancing AI in Comics Understanding', 'desc': 'This paper reviews the emerging field of Comics Understanding, which leverages vision-language models to interpret the unique structure of comics. It highlights the challenges posed by comics, such as their non-linear storytelling and diverse artistic styles, which require advanced AI techniques for tasks like image classification and narrative comprehension. The authors introduce the Layer of Comics Understanding (LoCU) framework, which categorizes tasks and datasets specific to comics, aiming to standardize research efforts in this area. Additionally, the paper identifies current challenges and suggests future research directions to enhance the capabilities of AI in understanding comics.'}, 'zh': {'title': '漫画理解：视觉语言模型的新挑战', 'desc': '本文探讨了视觉语言模型在漫画理解领域的应用。漫画结合了丰富的视觉和文本叙事，给人工智能模型带来了图像分类、物体检测和叙事理解等多重挑战。我们提出了漫画理解的层次框架（LoCU），重新定义了漫画中的视觉语言任务，并为未来的研究奠定基础。最后，我们强调了当前研究中的挑战，并提出了未来探索的方向。'}}}, {'id': 'https://huggingface.co/papers/2409.06277', 'title': 'Ferret: Federated Full-Parameter Tuning at Scale for Large Language Models', 'url': 'https://huggingface.co/papers/2409.06277', 'abstract': 'Large Language Models (LLMs) have become indispensable in numerous real-world applications. Unfortunately, fine-tuning these models at scale, especially in federated settings where data privacy and communication efficiency are critical, presents significant challenges. Existing methods often resort to parameter-efficient fine-tuning (PEFT) to mitigate communication overhead, but this typically comes at the cost of model accuracy. To address these limitations, we propose federated full-parameter tuning at scale for LLMs (Ferret), the first first-order method with shared randomness to enable scalable full-parameter tuning of LLMs across decentralized data sources while maintaining competitive model accuracy. Ferret accomplishes this through three aspects: (1) it employs widely applied first-order methods for efficient local updates; (2) it projects these updates into a low-dimensional space to considerably reduce communication overhead; and (3) it reconstructs local updates from this low-dimensional space with shared randomness to facilitate effective full-parameter global aggregation, ensuring fast convergence and competitive final performance. Our rigorous theoretical analyses and insights along with extensive experiments, show that Ferret significantly enhances the scalability of existing federated full-parameter tuning approaches by achieving high computational efficiency, reduced communication overhead, and fast convergence, all while maintaining competitive model accuracy. Our implementation is available at https://github.com/allen4747/Ferret.', 'score': 14, 'issue_id': 1, 'pub_date': '2024-09-10', 'pub_date_card': {'ru': '10 сентября', 'en': 'September 10', 'zh': '9月10日'}, 'hash': '6799f6d8602fb00e', 'data': {'categories': ['#training', '#optimization', '#data', '#benchmark', '#open_source'], 'emoji': '🦡', 'ru': {'title': 'Эффективное федеративное обучение LLM без компромиссов в точности', 'desc': 'Статья представляет метод Ferret для федеративного обучения больших языковых моделей (LLM) в полнопараметрическом режиме. Ferret использует методы первого порядка и проекцию обновлений в низкоразмерное пространство для уменьшения накладных расходов на коммуникацию. Метод реконструирует локальные обновления с помощью общей случайности для эффективной глобальной агрегации полных параметров. Теоретический анализ и эксперименты показывают, что Ferret значительно повышает масштабируемость существующих подходов к федеративному обучению LLM.'}, 'en': {'title': 'Ferret: Scalable Fine-Tuning for LLMs in Federated Learning', 'desc': 'This paper introduces Ferret, a novel approach for fine-tuning Large Language Models (LLMs) in federated settings while addressing data privacy and communication efficiency. Ferret utilizes first-order methods for efficient local updates and projects these updates into a low-dimensional space to minimize communication overhead. It also employs shared randomness to reconstruct local updates, enabling effective global aggregation of model parameters. The results demonstrate that Ferret achieves high computational efficiency and fast convergence without sacrificing model accuracy, making it a significant advancement in federated learning for LLMs.'}, 'zh': {'title': '联邦全参数微调：提升大型语言模型的效率与准确性', 'desc': '大型语言模型（LLMs）在许多实际应用中变得不可或缺。然而，在联邦设置中对这些模型进行大规模微调，尤其是在数据隐私和通信效率至关重要的情况下，面临重大挑战。现有方法通常采用参数高效微调（PEFT）来减少通信开销，但这通常会影响模型的准确性。为了解决这些问题，我们提出了联邦全参数微调（Ferret），这是首个使用共享随机性的一级方法，能够在去中心化数据源上实现可扩展的全参数微调，同时保持竞争力的模型准确性。'}}}, {'id': 'https://huggingface.co/papers/2409.10038', 'title': 'On the Diagram of Thought', 'url': 'https://huggingface.co/papers/2409.10038', 'abstract': 'We introduce Diagram of Thought (DoT), a framework that models iterative reasoning in large language models (LLMs) as the construction of a directed acyclic graph (DAG) within a single model. Unlike traditional approaches that represent reasoning as linear chains or trees, DoT organizes propositions, critiques, refinements, and verifications into a cohesive DAG structure, allowing the model to explore complex reasoning pathways while maintaining logical consistency. Each node in the diagram corresponds to a proposition that has been proposed, critiqued, refined, or verified, enabling the LLM to iteratively improve its reasoning through natural language feedback. By leveraging auto-regressive next-token prediction with role-specific tokens, DoT facilitates seamless transitions between proposing ideas and critically evaluating them, providing richer feedback than binary signals. Furthermore, we formalize the DoT framework using Topos Theory, providing a mathematical foundation that ensures logical consistency and soundness in the reasoning process. This approach enhances both the training and inference processes within a single LLM, eliminating the need for multiple models or external control mechanisms. DoT offers a conceptual framework for designing next-generation reasoning-specialized models, emphasizing training efficiency, robust reasoning capabilities, and theoretical grounding. The code is available at https://github.com/diagram-of-thought/diagram-of-thought.', 'score': 11, 'issue_id': 1, 'pub_date': '2024-09-16', 'pub_date_card': {'ru': '16 сентября', 'en': 'September 16', 'zh': '9月16日'}, 'hash': '8fd0d604f6d30463', 'data': {'categories': ['#reasoning', '#training', '#math', '#graphs', '#inference', '#open_source', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Граф мыслей: новый подход к моделированию рассуждений в ИИ', 'desc': 'Статья представляет новый фреймворк под названием Diagram of Thought (DoT) для моделирования итеративных рассуждений в больших языковых моделях (LLM). DoT организует процесс рассуждения в виде направленного ациклического графа (DAG), позволяя модели исследовать сложные пути рассуждений, сохраняя логическую согласованность. Фреймворк использует авторегрессивное предсказание следующего токена с ролевыми токенами для перехода между предложением идей и их критической оценкой. DoT формализован с использованием теории топосов, что обеспечивает математическую основу для логической согласованности процесса рассуждений.'}, 'en': {'title': 'Revolutionizing Reasoning with Directed Acyclic Graphs', 'desc': 'The Diagram of Thought (DoT) framework presents a novel way to model iterative reasoning in large language models (LLMs) by using a directed acyclic graph (DAG) structure. This approach allows for a more complex organization of reasoning processes, where each node represents a different stage of reasoning, such as proposing or critiquing ideas. By utilizing auto-regressive next-token prediction with role-specific tokens, DoT enables the model to fluidly transition between generating ideas and evaluating them, enhancing the feedback quality. The framework is mathematically grounded in Topos Theory, ensuring logical consistency and soundness, which improves both training and inference in LLMs without needing multiple models.'}, 'zh': {'title': '思维图：提升推理能力的新框架', 'desc': '本文介绍了思维图（DoT）框架，该框架将大型语言模型（LLMs）中的迭代推理建模为一个有向无环图（DAG）的构建。与传统的线性链或树形结构不同，DoT将命题、批评、改进和验证组织成一个统一的DAG结构，使模型能够在保持逻辑一致性的同时探索复杂的推理路径。每个节点对应一个已提出、批评、改进或验证的命题，使LLM能够通过自然语言反馈迭代改进推理。通过利用自回归的下一个标记预测和角色特定的标记，DoT实现了从提出想法到批判性评估的无缝过渡，提供比二元信号更丰富的反馈。'}}}, {'id': 'https://huggingface.co/papers/2409.09213', 'title': 'ReCLAP: Improving Zero Shot Audio Classification by Describing Sounds', 'url': 'https://huggingface.co/papers/2409.09213', 'abstract': "Open-vocabulary audio-language models, like CLAP, offer a promising approach for zero-shot audio classification (ZSAC) by enabling classification with any arbitrary set of categories specified with natural language prompts. In this paper, we propose a simple but effective method to improve ZSAC with CLAP. Specifically, we shift from the conventional method of using prompts with abstract category labels (e.g., Sound of an organ) to prompts that describe sounds using their inherent descriptive features in a diverse context (e.g.,The organ's deep and resonant tones filled the cathedral.). To achieve this, we first propose ReCLAP, a CLAP model trained with rewritten audio captions for improved understanding of sounds in the wild. These rewritten captions describe each sound event in the original caption using their unique discriminative characteristics. ReCLAP outperforms all baselines on both multi-modal audio-text retrieval and ZSAC. Next, to improve zero-shot audio classification with ReCLAP, we propose prompt augmentation. In contrast to the traditional method of employing hand-written template prompts, we generate custom prompts for each unique label in the dataset. These custom prompts first describe the sound event in the label and then employ them in diverse scenes. Our proposed method improves ReCLAP's performance on ZSAC by 1%-18% and outperforms all baselines by 1% - 55%.", 'score': 11, 'issue_id': 1, 'pub_date': '2024-09-13', 'pub_date_card': {'ru': '13 сентября', 'en': 'September 13', 'zh': '9月13日'}, 'hash': 'c74e142f5a60bdb1', 'data': {'categories': ['#audio', '#training', '#transfer_learning', '#open_source', '#architecture', '#synthetic', '#multimodal'], 'emoji': '🔊', 'ru': {'title': 'Улучшение нулевой классификации аудио через контекстные описания звуков', 'desc': 'Статья представляет новый метод улучшения нулевой классификации аудио с помощью модели CLAP. Авторы предлагают использовать промпты, описывающие звуки через их характерные особенности в разнообразном контексте, вместо абстрактных категорий. Они разработали модель ReCLAP, обученную на переписанных аудио-подписях для лучшего понимания реальных звуков. Метод включает генерацию уникальных промптов для каждой метки в датасете, что значительно повышает производительность в задаче нулевой классификации аудио.'}, 'en': {'title': 'Enhancing Zero-Shot Audio Classification with Descriptive Prompts', 'desc': 'This paper introduces a method to enhance zero-shot audio classification (ZSAC) using the CLAP model, which can classify sounds based on natural language prompts. The authors propose a new approach called ReCLAP, which involves training the model with rewritten audio captions that focus on the unique characteristics of sounds. Additionally, they suggest prompt augmentation, where custom prompts are generated for each sound label, describing the sound in various contexts. The results show that ReCLAP significantly improves ZSAC performance, outperforming existing methods by a notable margin.'}, 'zh': {'title': '提升零样本音频分类的有效方法', 'desc': '本文提出了一种改进开放词汇音频语言模型CLAP的方法，以提高零样本音频分类（ZSAC）的性能。我们通过使用描述声音固有特征的提示，替代传统的抽象类别标签，来增强模型对声音的理解。我们首先提出了ReCLAP模型，通过重写音频标题来训练，以更好地捕捉声音事件的独特特征。接着，我们通过生成自定义提示来进一步提升ReCLAP在ZSAC上的表现，最终实现了1%-55%的性能提升。'}}}, {'id': 'https://huggingface.co/papers/2409.09269', 'title': 'Guiding Vision-Language Model Selection for Visual Question-Answering Across Tasks, Domains, and Knowledge Types', 'url': 'https://huggingface.co/papers/2409.09269', 'abstract': 'Visual Question-Answering (VQA) has become a key use-case in several applications to aid user experience, particularly after Vision-Language Models (VLMs) achieving good results in zero-shot inference. But evaluating different VLMs for an application requirement using a standardized framework in practical settings is still challenging. This paper introduces a comprehensive framework for evaluating VLMs tailored to VQA tasks in practical settings. We present a novel dataset derived from established VQA benchmarks, annotated with task types, application domains, and knowledge types, three key practical aspects on which tasks can vary. We also introduce GoEval, a multimodal evaluation metric developed using GPT-4o, achieving a correlation factor of 56.71% with human judgments. Our experiments with ten state-of-the-art VLMs reveals that no single model excelling universally, making appropriate selection a key design decision. Proprietary models such as Gemini-1.5-Pro and GPT-4o-mini generally outperform others, though open-source models like InternVL-2-8B and CogVLM-2-Llama-3-19B demonstrate competitive strengths in specific contexts, while providing additional advantages. This study guides the selection of VLMs based on specific task requirements and resource constraints, and can also be extended to other vision-language tasks.', 'score': 7, 'issue_id': 1, 'pub_date': '2024-09-14', 'pub_date_card': {'ru': '14 сентября', 'en': 'September 14', 'zh': '9月14日'}, 'hash': '336c2172f89b1d8d', 'data': {'categories': ['#dataset', '#cv', '#interpretability', '#benchmark', '#games', '#open_source', '#small_models', '#architecture', '#multimodal'], 'emoji': '🔍', 'ru': {'title': 'Комплексная оценка VLM для практического применения в задачах VQA', 'desc': 'Эта статья представляет комплексную систему для оценки моделей визуального-языкового понимания (VLM) в задачах визуальных вопросов и ответов (VQA). Авторы создали новый набор данных, аннотированный по типам задач, областям применения и видам знаний. Они также разработали метрику оценки GoEval, использующую GPT-4o и имеющую корреляцию 56.71% с оценками людей. Эксперименты с десятью современными VLM показали, что ни одна модель не превосходит универсально, что делает выбор модели ключевым решением при разработке.'}, 'en': {'title': 'Evaluating VLMs for Effective Visual Question-Answering', 'desc': 'This paper addresses the challenges of evaluating Vision-Language Models (VLMs) specifically for Visual Question-Answering (VQA) tasks. It introduces a new framework that includes a dataset with annotations for task types, application domains, and knowledge types, which are crucial for practical evaluations. The authors also present GoEval, a multimodal evaluation metric that correlates well with human judgments, providing a standardized way to assess VLM performance. The findings indicate that while proprietary models often perform better overall, open-source models can excel in certain scenarios, emphasizing the importance of selecting the right model based on specific needs.'}, 'zh': {'title': '选择合适的视觉语言模型，提升视觉问答效果', 'desc': '本文介绍了一种用于视觉问答（VQA）任务的评估框架，旨在解决不同视觉语言模型（VLMs）在实际应用中的评估挑战。我们构建了一个新的数据集，涵盖任务类型、应用领域和知识类型等关键方面，以便更好地评估模型的表现。通过引入GoEval这一多模态评估指标，我们的实验表明，没有单一模型在所有任务中表现最佳，因此选择合适的模型至关重要。研究结果还显示，尽管一些专有模型表现优越，但开源模型在特定场景下也展现出竞争力，提供了额外的优势。'}}}, {'id': 'https://huggingface.co/papers/2409.06957', 'title': 'Policy Filtration in RLHF to Fine-Tune LLM for Code Generation', 'url': 'https://huggingface.co/papers/2409.06957', 'abstract': 'Reinforcement learning from human feedback (RLHF) is one of the key techniques that helps large language models (LLMs) to follow instructions and provide helpful and harmless responses. While direct policy optimization methods exist, state-of-the-art LLMs adopt RL-based methods (usually PPO) in RLHF to train the policy to generate good responses guided by a reward model learned from preference data. The main challenge of these methods is the inaccuracy of the intermediate reward model, especially in code generation tasks that require long and complex reasoning to score a response. We find that the reliability of the reward model varies across responses assigned with different rewards. This motivates us to filter the samples whose rewards may be unreliable to improve signal-to-noise ratio during policy learning, resulting in Policy Filtration for Proximal Policy Optimization (PF-PPO). To choose a proper policy filtration strategy for a given reward model, the coefficient of determination (R^2) between rewards and actual scores on filtered samples serves as a good metrics and helps us find several promising strategies. We provide extensive experiments to validate the effectiveness of PF-PPO in code generation tasks, and find that some variants of PF-PPO are highly effective and achieve new state-of-the-art performance across 7-billion-parameter models on HumanEval, MBPP, and a new and more challenging LeetCode Contest benchmark.', 'score': 5, 'issue_id': 1, 'pub_date': '2024-09-11', 'pub_date_card': {'ru': '11 сентября', 'en': 'September 11', 'zh': '9月11日'}, 'hash': 'a16e56fefdb70e47', 'data': {'categories': ['#reasoning', '#rl', '#optimization', '#plp', '#benchmark', '#alignment', '#rlhf', '#small_models'], 'emoji': '🧠', 'ru': {'title': 'Фильтрация для точности: новый подход к RLHF в генерации кода', 'desc': 'Эта статья представляет новый метод обучения с подкреплением на основе обратной связи от человека (RLHF) для улучшения генерации кода большими языковыми моделями. Авторы предлагают технику фильтрации политики для проксимального оптимизация политики (PF-PPO), которая отбирает наиболее надежные образцы для обучения. Метод использует коэффициент детерминации (R^2) для выбора оптимальной стратегии фильтрации. Эксперименты показывают, что PF-PPO достигает нового уровня производительности для 7-миллиардных моделей на нескольких бенчмарках генерации кода.'}, 'en': {'title': 'Enhancing RLHF with Policy Filtration for Better Code Generation', 'desc': 'This paper discusses a method called Policy Filtration for Proximal Policy Optimization (PF-PPO) that enhances reinforcement learning from human feedback (RLHF) for large language models (LLMs). The authors identify that the reward model used to guide the training can be inaccurate, particularly in complex tasks like code generation. By filtering out samples with unreliable rewards, they improve the quality of the training signal, leading to better policy learning. Extensive experiments demonstrate that PF-PPO achieves state-of-the-art performance on various coding benchmarks, indicating its effectiveness in refining LLM responses.'}, 'zh': {'title': '提升奖励模型可靠性的策略过滤', 'desc': '强化学习从人类反馈（RLHF）是帮助大型语言模型（LLMs）遵循指令并提供有用和无害响应的关键技术之一。尽管存在直接的策略优化方法，但最先进的LLMs通常采用基于强化学习的方法（通常是PPO）来训练生成良好响应的策略，这些响应由从偏好数据中学习的奖励模型指导。本文的主要挑战在于中间奖励模型的不准确性，尤其是在需要长时间和复杂推理的代码生成任务中。我们提出了一种策略过滤方法（PF-PPO），通过过滤可能不可靠的奖励样本来提高策略学习中的信噪比，从而在代码生成任务中取得了新的最先进性能。'}}}, {'id': 'https://huggingface.co/papers/2409.08831', 'title': 'Breaking reCAPTCHAv2', 'url': 'https://huggingface.co/papers/2409.08831', 'abstract': "Our work examines the efficacy of employing advanced machine learning methods to solve captchas from Google's reCAPTCHAv2 system. We evaluate the effectiveness of automated systems in solving captchas by utilizing advanced YOLO models for image segmentation and classification. Our main result is that we can solve 100% of the captchas, while previous work only solved 68-71%. Furthermore, our findings suggest that there is no significant difference in the number of challenges humans and bots must solve to pass the captchas in reCAPTCHAv2. This implies that current AI technologies can exploit advanced image-based captchas. We also look under the hood of reCAPTCHAv2, and find evidence that reCAPTCHAv2 is heavily based on cookie and browser history data when evaluating whether a user is human or not. The code is provided alongside this paper.", 'score': 4, 'issue_id': 1, 'pub_date': '2024-09-13', 'pub_date_card': {'ru': '13 сентября', 'en': 'September 13', 'zh': '9月13日'}, 'hash': 'fd6b8afece4a0b97', 'data': {'categories': ['#cv', '#security', '#data', '#benchmark', '#open_source'], 'emoji': '🤖', 'ru': {'title': 'ИИ превзошёл человека в решении капч reCAPTCHAv2', 'desc': 'Исследование посвящено применению продвинутых методов машинного обучения для решения капч системы reCAPTCHAv2 от Google. Авторы использовали усовершенствованные модели YOLO для сегментации и классификации изображений, достигнув 100% эффективности решения капч, что значительно превосходит предыдущие результаты в 68-71%. Исследование показало, что нет существенной разницы в количестве проверок, которые должны пройти люди и боты для решения капч в reCAPTCHAv2. Также обнаружено, что reCAPTCHAv2 в значительной степени опирается на данные о cookie и историю браузера при определении, является ли пользователь человеком или нет.'}, 'en': {'title': 'Breaking Barriers: AI Triumphs Over reCAPTCHAv2', 'desc': "This paper investigates how effective advanced machine learning techniques are at solving Google's reCAPTCHAv2 captchas. By using sophisticated YOLO models for image segmentation and classification, the authors achieved a 100% success rate in solving these captchas, surpassing previous attempts that only managed 68-71%. The study reveals that both humans and bots face a similar number of challenges to pass the captchas, indicating that current AI can effectively bypass these security measures. Additionally, the research uncovers that reCAPTCHAv2 relies significantly on cookie and browser history data to determine if a user is human."}, 'zh': {'title': '利用先进机器学习技术破解验证码的突破', 'desc': '本研究探讨了使用先进的机器学习方法解决谷歌reCAPTCHA v2系统中的验证码的有效性。我们利用先进的YOLO模型进行图像分割和分类，评估自动化系统在解决验证码方面的效果。我们的主要结果是，我们能够100%解决验证码，而之前的研究仅能解决68-71%。此外，我们的发现表明，人类和机器人在通过reCAPTCHA v2时需要解决的挑战数量没有显著差异，这意味着当前的人工智能技术可以利用基于图像的高级验证码。'}}}, {'id': 'https://huggingface.co/papers/2409.08199', 'title': 'AudioBERT: Audio Knowledge Augmented Language Model', 'url': 'https://huggingface.co/papers/2409.08199', 'abstract': 'Recent studies have identified that language models, pretrained on text-only datasets, often lack elementary visual knowledge, e.g., colors of everyday objects. Motivated by this observation, we ask whether a similar shortcoming exists in terms of the auditory knowledge. To answer this question, we construct a new dataset called AuditoryBench, which consists of two novel tasks for evaluating auditory knowledge. Based on our analysis using the benchmark, we find that language models also suffer from a severe lack of auditory knowledge. To address this limitation, we propose AudioBERT, a novel method to augment the auditory knowledge of BERT through a retrieval-based approach. First, we detect auditory knowledge spans in prompts to query our retrieval model efficiently. Then, we inject audio knowledge into BERT and switch on low-rank adaptation for effective adaptation when audio knowledge is required. Our experiments demonstrate that AudioBERT is quite effective, achieving superior performance on the AuditoryBench. The dataset and code are available at https://github.com/HJ-Ok/AudioBERT.', 'score': 4, 'issue_id': 1, 'pub_date': '2024-09-12', 'pub_date_card': {'ru': '12 сентября', 'en': 'September 12', 'zh': '9月12日'}, 'hash': '46254d47da49f69d', 'data': {'categories': ['#audio', '#dataset', '#training', '#rag', '#interpretability', '#optimization', '#benchmark', '#open_source', '#architecture'], 'emoji': '🎵', 'ru': {'title': 'Обогащение языковых моделей звуковыми знаниями', 'desc': 'Исследователи создали набор данных AuditoryBench для оценки звуковых знаний языковых моделей. Они обнаружили, что модели, обученные только на текстовых данных, имеют серьезный недостаток в области аудиальных знаний. Для решения этой проблемы была разработана модель AudioBERT, которая использует подход на основе извлечения информации для обогащения BERT звуковыми знаниями. Эксперименты показали, что AudioBERT значительно улучшает производительность на AuditoryBench.'}, 'en': {'title': 'Enhancing Language Models with Auditory Knowledge through AudioBERT', 'desc': 'This paper investigates the auditory knowledge of language models, which have previously been shown to lack basic visual knowledge. The authors introduce a new dataset called AuditoryBench, designed to evaluate the auditory understanding of these models. They propose a method called AudioBERT, which enhances the auditory knowledge of BERT by using a retrieval-based approach to incorporate relevant audio information. Experimental results indicate that AudioBERT significantly improves performance on the AuditoryBench tasks, highlighting the importance of integrating auditory knowledge into language models.'}, 'zh': {'title': '提升语言模型的听觉知识', 'desc': '最近的研究发现，基于文本数据集预训练的语言模型通常缺乏基本的视觉知识，例如日常物体的颜色。受到这一观察的启发，我们探讨了语言模型在听觉知识方面是否也存在类似的不足。为此，我们构建了一个新的数据集AuditoryBench，包含两个新任务来评估听觉知识。我们提出了AudioBERT，通过检索方法增强BERT的听觉知识，实验结果表明AudioBERT在AuditoryBench上表现优异。'}}}, {'id': 'https://huggingface.co/papers/2409.08554', 'title': 'LLM-Powered Grapheme-to-Phoneme Conversion: Benchmark and Case Study', 'url': 'https://huggingface.co/papers/2409.08554', 'abstract': 'Grapheme-to-phoneme (G2P) conversion is critical in speech processing, particularly for applications like speech synthesis. G2P systems must possess linguistic understanding and contextual awareness of languages with polyphone words and context-dependent phonemes. Large language models (LLMs) have recently demonstrated significant potential in various language tasks, suggesting that their phonetic knowledge could be leveraged for G2P. In this paper, we evaluate the performance of LLMs in G2P conversion and introduce prompting and post-processing methods that enhance LLM outputs without additional training or labeled data. We also present a benchmarking dataset designed to assess G2P performance on sentence-level phonetic challenges of the Persian language. Our results show that by applying the proposed methods, LLMs can outperform traditional G2P tools, even in an underrepresented language like Persian, highlighting the potential of developing LLM-aided G2P systems.', 'score': 3, 'issue_id': 1, 'pub_date': '2024-09-13', 'pub_date_card': {'ru': '13 сентября', 'en': 'September 13', 'zh': '9月13日'}, 'hash': '906778ce172ee87d', 'data': {'categories': ['#audio', '#dataset', '#multilingual', '#transfer_learning', '#benchmark', '#low_resource'], 'emoji': '🗣️', 'ru': {'title': 'LLM открывают новые горизонты в преобразовании текста в речь', 'desc': 'Статья исследует применение больших языковых моделей (LLM) для преобразования графем в фонемы (G2P) в обработке речи. Авторы оценивают эффективность LLM в задаче G2P и предлагают методы промптинга и постобработки для улучшения результатов без дополнительного обучения. Они также представляют набор данных для оценки G2P на уровне предложений в персидском языке. Результаты показывают, что LLM с предложенными методами превосходят традиционные инструменты G2P даже для малоресурсных языков.'}, 'en': {'title': 'Leveraging LLMs for Enhanced Grapheme-to-Phoneme Conversion', 'desc': 'This paper focuses on converting written words into their spoken forms, known as grapheme-to-phoneme (G2P) conversion, which is essential for speech synthesis. It highlights the challenges posed by languages with complex phonetic rules and how large language models (LLMs) can be utilized to improve G2P performance. The authors propose innovative prompting and post-processing techniques that enhance the outputs of LLMs without needing extra training or labeled data. Their findings indicate that these methods allow LLMs to surpass traditional G2P systems, particularly in the context of the Persian language, showcasing the effectiveness of LLMs in this area.'}, 'zh': {'title': '利用大型语言模型提升字音转换性能', 'desc': '本论文探讨了字音转换（G2P）在语音处理中的重要性，尤其是在语音合成应用中。我们评估了大型语言模型（LLMs）在G2P转换中的表现，并提出了增强LLM输出的提示和后处理方法，这些方法无需额外训练或标注数据。我们还介绍了一个基准数据集，用于评估波斯语句子级别的G2P性能挑战。研究结果表明，应用这些方法后，LLMs在G2P任务中超越了传统工具，展示了LLM辅助G2P系统的潜力。'}}}, {'id': 'https://huggingface.co/papers/2409.07012', 'title': "Towards Predicting Temporal Changes in a Patient's Chest X-ray Images based on Electronic Health Records", 'url': 'https://huggingface.co/papers/2409.07012', 'abstract': 'Chest X-ray imaging (CXR) is an important diagnostic tool used in hospitals to assess patient conditions and monitor changes over time. Generative models, specifically diffusion-based models, have shown promise in generating realistic synthetic X-rays. However, these models mainly focus on conditional generation using single-time-point data, i.e., typically CXRs taken at a specific time with their corresponding reports, limiting their clinical utility, particularly for capturing temporal changes. To address this limitation, we propose a novel framework, EHRXDiff, which predicts future CXR images by integrating previous CXRs with subsequent medical events, e.g., prescriptions, lab measures, etc. Our framework dynamically tracks and predicts disease progression based on a latent diffusion model, conditioned on the previous CXR image and a history of medical events. We comprehensively evaluate the performance of our framework across three key aspects, including clinical consistency, demographic consistency, and visual realism. We demonstrate that our framework generates high-quality, realistic future images that capture potential temporal changes, suggesting its potential for further development as a clinical simulation tool. This could offer valuable insights for patient monitoring and treatment planning in the medical field.', 'score': 3, 'issue_id': 1, 'pub_date': '2024-09-11', 'pub_date_card': {'ru': '11 сентября', 'en': 'September 11', 'zh': '9月11日'}, 'hash': '91dd45168fabc255', 'data': {'categories': ['#science', '#cv', '#healthcare', '#diffusion', '#architecture', '#synthetic', '#3d'], 'emoji': '\U0001fa7b', 'ru': {'title': 'Предсказание будущих рентгеновских снимков с помощью ИИ', 'desc': 'Исследователи представили EHRXDiff - новую модель для предсказания будущих рентгеновских снимков грудной клетки на основе предыдущих снимков и медицинских данных пациента. Модель использует латентную диффузионную архитектуру для генерации реалистичных изображений, отражающих возможные изменения состояния пациента со временем. Авторы провели комплексную оценку модели по трем ключевым аспектам: клиническая согласованность, демографическая согласованность и визуальный реализм. Результаты показывают потенциал EHRXDiff как инструмента для моделирования прогрессирования заболеваний и планирования лечения.'}, 'en': {'title': 'Predicting Future Chest X-rays with EHRXDiff', 'desc': "This paper introduces EHRXDiff, a novel framework that enhances the generation of chest X-ray images by predicting future images based on past X-rays and subsequent medical events. Unlike traditional models that only generate images from single-time-point data, EHRXDiff utilizes a latent diffusion model to track disease progression over time. The framework is evaluated on clinical consistency, demographic consistency, and visual realism, showing its ability to produce high-quality synthetic X-rays that reflect potential changes in a patient's condition. This advancement could significantly improve patient monitoring and treatment planning in healthcare settings."}, 'zh': {'title': '预测未来X光图像的创新框架', 'desc': '胸部X光成像（CXR）是医院中重要的诊断工具，用于评估患者状况和监测变化。本文提出了一种新框架EHRXDiff，通过整合先前的CXR图像和后续的医疗事件（如处方和实验室测量）来预测未来的CXR图像。该框架基于潜在扩散模型，动态跟踪和预测疾病进展，能够捕捉潜在的时间变化。我们的评估表明，该框架生成的高质量未来图像在临床一致性、人口统计一致性和视觉真实感方面表现出色，具有作为临床模拟工具的潜力。'}}}, {'id': 'https://huggingface.co/papers/2409.10309', 'title': 'beeFormer: Bridging the Gap Between Semantic and Interaction Similarity in Recommender Systems', 'url': 'https://huggingface.co/papers/2409.10309', 'abstract': 'Recommender systems often use text-side information to improve their predictions, especially in cold-start or zero-shot recommendation scenarios, where traditional collaborative filtering approaches cannot be used. Many approaches to text-mining side information for recommender systems have been proposed over recent years, with sentence Transformers being the most prominent one. However, these models are trained to predict semantic similarity without utilizing interaction data with hidden patterns specific to recommender systems. In this paper, we propose beeFormer, a framework for training sentence Transformer models with interaction data. We demonstrate that our models trained with beeFormer can transfer knowledge between datasets while outperforming not only semantic similarity sentence Transformers but also traditional collaborative filtering methods. We also show that training on multiple datasets from different domains accumulates knowledge in a single model, unlocking the possibility of training universal, domain-agnostic sentence Transformer models to mine text representations for recommender systems. We release the source code, trained models, and additional details allowing replication of our experiments at https://github.com/recombee/beeformer.', 'score': 2, 'issue_id': 1, 'pub_date': '2024-09-16', 'pub_date_card': {'ru': '16 сентября', 'en': 'September 16', 'zh': '9月16日'}, 'hash': '89bd2f6e3847302d', 'data': {'categories': ['#dataset', '#training', '#data', '#transfer_learning', '#benchmark', '#open_source', '#architecture', '#recommender_systems'], 'emoji': '🐝', 'ru': {'title': 'beeFormer: универсальные текстовые представления для рекомендательных систем', 'desc': 'В статье представлен beeFormer - фреймворк для обучения моделей sentence Transformer с использованием данных о взаимодействиях пользователей. Авторы показывают, что модели, обученные с помощью beeFormer, превосходят как традиционные методы коллаборативной фильтрации, так и модели семантического сходства. Фреймворк позволяет переносить знания между датасетами и аккумулировать знания из разных доменов в единой модели. Это открывает возможность обучения универсальных, доменно-агностичных моделей для извлечения текстовых представлений в рекомендательных системах.'}, 'en': {'title': 'Unlocking Universal Recommendations with beeFormer', 'desc': 'This paper introduces beeFormer, a novel framework that enhances sentence Transformer models by incorporating interaction data from recommender systems. Traditional models focus on semantic similarity but often overlook valuable user interaction patterns. By training on multiple datasets, beeFormer enables the development of universal models that can effectively mine text representations across different domains. The results show that beeFormer outperforms both semantic similarity models and standard collaborative filtering techniques, making it a significant advancement in recommender system technology.'}, 'zh': {'title': '利用交互数据提升推荐系统的文本挖掘能力', 'desc': '推荐系统通常利用文本信息来提高预测准确性，尤其是在冷启动或零样本推荐场景中。近年来，许多方法被提出用于挖掘推荐系统的文本信息，其中句子变换器模型最为突出。本文提出了beeFormer框架，通过交互数据训练句子变换器模型，克服了传统模型不利用交互数据的局限。我们的实验表明，使用beeFormer训练的模型在多个数据集间能够迁移知识，并且在性能上超越了语义相似度句子变换器和传统的协同过滤方法。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents', '#agi', '#alignment (1)', '#architecture (11)', '#audio (4)', '#benchmark (9)', '#cv (4)', '#data (3)', '#dataset (6)', '#diffusion (2)', '#ethics', '#games (3)', '#graphs (1)', '#hallucinations', '#healthcare (1)', '#inference (3)', '#interpretability (2)', '#leakage', '#long_context (2)', '#low_resource (1)', '#machine_translation', '#math (1)', '#multilingual (2)', '#multimodal (4)', '#open_source (8)', '#optimization (5)', '#plp (1)', '#rag (2)', '#reasoning (2)', '#recommender_systems', '#rl (1)', '#rlhf (1)', '#robotics', '#science (1)', '#security (1)', '#small_models (3)', '#story_generation', '#survey (1)', '#synthetic (3)', '#training (7)', '#transfer_learning (4)', '#transformers', '#video'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].join(" ");
                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="background-digit">${index + 1}</div>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <h2>${item['data']['emoji']} ${item['title']}</h2>
                            <p class="meta"><svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path transform="translate(0, 2)" fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> ${item['score']}. ${title}</p>
                            <p class="pub-date">📝 ${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>
                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>
                            <p class="tags">${cats}</p>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2024-09-17 09:00',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-09-17 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-09-17 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    