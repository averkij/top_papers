{
    "date": {
        "ru": "2 января",
        "en": "January 2",
        "zh": "1月2日"
    },
    "time_utc": "2025-01-02 07:10",
    "weekday": 3,
    "issue_id": 1454,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.19638",
            "title": "Xmodel-2 Technical Report",
            "url": "https://huggingface.co/papers/2412.19638",
            "abstract": "Xmodel-2 is a 1.2-billion-parameter large language model designed specifically for reasoning tasks. Its architecture enables different model scales to share a unified set of hyperparameters, allowing for extensive experimentation on smaller models and seamless transfer of optimal configurations to larger models. To maximize training efficiency and stability, Xmodel-2 employs the WSD learning rate scheduler from MiniCPM. Pretrained on 1.5 trillion tokens from diverse sources, Xmodel-2 achieves state-of-the-art performance in complex reasoning and agent-based tasks, while maintaining low training costs. These results highlight the potential of efficient model design and training strategies in advancing reasoning capabilities. Model checkpoints and code are publicly available on GitHub at https://github.com/XiaoduoAILab/Xmodel-2",
            "score": 5,
            "issue_id": 1453,
            "pub_date": "2025-12-27",
            "pub_date_card": {
                "ru": "27 декабря",
                "en": "December 27",
                "zh": "12月27日"
            },
            "hash": "4707dc8ac5a87e66",
            "authors": [
                "Wang Qun",
                "Liu Yang",
                "Lin Qingquan",
                "Qu Zhijiu",
                "Jiang Ling"
            ],
            "affiliations": [
                "AI Lab, Xiaodu Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.19638.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#small_models",
                    "#reasoning",
                    "#open_source",
                    "#architecture"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Эффективное рассуждение с Xmodel-2: мощь в компактности",
                    "desc": "Xmodel-2 - это языковая модель с 1,2 миллиардами параметров, специализирующаяся на задачах рассуждения. Её архитектура позволяет разным масштабам модели использовать единый набор гиперпараметров, что облегчает эксперименты и перенос оптимальных конфигураций. Модель использует планировщик скорости обучения WSD из MiniCPM для повышения эффективности и стабильности. Предобученная на 1,5 триллионах токенов, Xmodel-2 достигает передовых результатов в сложных задачах рассуждения, сохраняя низкие затраты на обучение."
                },
                "en": {
                    "title": "Unlocking Reasoning Power with Efficient Model Design",
                    "desc": "Xmodel-2 is a large language model with 1.2 billion parameters, specifically built for reasoning tasks. It features a flexible architecture that allows different model sizes to use the same hyperparameters, facilitating experimentation and optimization across scales. The model utilizes the WSD learning rate scheduler to enhance training efficiency and stability. With pretraining on 1.5 trillion tokens, Xmodel-2 demonstrates superior performance in complex reasoning tasks while keeping training costs low, showcasing the benefits of efficient model design."
                },
                "zh": {
                    "title": "高效推理能力的模型设计与训练策略",
                    "desc": "Xmodel-2 是一个拥有 12 亿参数的大型语言模型，专门设计用于推理任务。它的架构允许不同规模的模型共享统一的超参数，从而可以在较小的模型上进行广泛实验，并将最佳配置无缝转移到更大的模型上。为了最大化训练效率和稳定性，Xmodel-2 采用了 MiniCPM 的 WSD 学习率调度器。经过在 1.5 万亿个来自多样化来源的标记上进行预训练，Xmodel-2 在复杂推理和基于代理的任务中达到了最先进的性能，同时保持了较低的训练成本。"
                }
            }
        }
    ],
    "link_prev": "2025-01-01.html",
    "link_next": "2025-01-03.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "01.01",
        "en": "01/01",
        "zh": "1月1日"
    },
    "short_date_next": {
        "ru": "03.01",
        "en": "01/03",
        "zh": "1月3日"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章讨论了计算机视觉（CV）尚未完全实现自然语言处理（NLP）中观察到的零样本任务泛化。作者提出，CV 使用离散和术语化的任务定义（如“图像分割”），这可能是零样本任务泛化的主要障碍。为了验证这一点，作者引入了解释性指令，通过详细的语言转换从输入图像到输出来定义 CV 任务目标。他们创建了一个包含 1200 万个“图像输入到解释性指令到输出”三元组的大规模数据集，并训练了一个自回归视觉-语言模型（AR-based VLM），该模型以图像和解释性指令作为输入。通过学习遵循这些指令，AR-based VLM 实现了指令级别的零样本能力，并在未见过的 CV 任务上展示了强大的零样本泛化。代码和数据集将在 GitHub 存储库中公开。",
        "title": "Explanatory Instructions: Towards Unified Vision Tasks Understanding and Zero-shot Generalization",
        "pinyin": "这篇文章讨论了计算机视觉（CV）尚未完全实现自然语言处理（NLP）中观察到的零样本任务泛化。\nZhè piān wénzhāng tǎolùn le jìsuànjī shìjué (CV) shàng wèi quánxiàn shíxiàn zìrán yǔyán chǔlǐ (NLP) zhōng guānchá dào de líng yàngbǎn rènwù fànhuà.\n\n作者提出，CV 使用离散和术语化的任务定义（如“图像分割”），这可能是零样本任务泛化的主要障碍。\nZuòzhě tíchū, CV shǐyòng lí sàn hé shùyǔ huà de rènwù dìngyì (rú “túxiàng fēngē”), zhè kěnéng shì líng yàngbǎn rènwù fànhuà de zhǔyào zhàng'ài.\n\n为了验证这一点，作者引入了解释性指令，通过详细的语言转换从输入图像到输出来定义 CV 任务目标。\nWèile yànzhèng zhè yī diǎn, zuòzhě yǐnrù le jiěshì xìng zhǐlìng, tōngguò xiángxì de yǔyán zhuǎnhuàn cóng shūrù túxiàng dào shūchū lái dìngyì CV rènwù mùbiāo.\n\n他们创建了一个包含 1200 万个“图像输入到解释性指令到输出”三元组的大规模数据集，并训练了一个自回归视觉-语言模型（AR-based VLM），该模型以图像和解释性指令作为输入。\nTāmen chuàngjiàn le yīgè bāohán 1200 wàn gè “túxiàng shūrù dào jiěshì xìng zhǐlìng dào shūchū” sānyuánzǔ de dà guīmó shùjùjí, bìng xùnliàn le yīgè zì huíguī shìjué-yǔyán móxíng (AR-based VLM), gāi móxíng yǐ túxiàng hé jiěshì xìng zhǐlìng zuòwéi shūrù.\n\n通过学习遵循这些指令，AR-based VLM 实现了指令级别的零样本能力，并在未见过的 CV 任务上展示了强大的零样本泛化。\nTōngguò xuéxí zūnxún zhèxiē zhǐlìng, AR-based VLM shíxiàn le zhǐlìng jíbié de líng yàngbǎn nénglì, bìng zài wèi jiànguò de CV rènwù shàng zhǎnshì le qiángdà de líng yàngbǎn fànhuà.\n\n代码和数据集将在 GitHub 存储库中公开。\nDàimǎ hé shùjùjí jiāng zài GitHub cúnchūkù zhōng gōngkāi.",
        "vocab": "[\n    {\"word\": \"讨论\", \"pinyin\": \"tǎo lùn\", \"trans\": \"discuss\"},\n    {\"word\": \"计算机视觉\", \"pinyin\": \"jì suàn jī shì jué\", \"trans\": \"computer vision\"},\n    {\"word\": \"自然语言处理\", \"pinyin\": \"zì rán yǔ yán chǔ lǐ\", \"trans\": \"natural language processing\"},\n    {\"word\": \"观察\", \"pinyin\": \"guān chá\", \"trans\": \"observe\"},\n    {\"word\": \"零样本任务泛化\", \"pinyin\": \"líng yàng běn rèn wù fàn huà\", \"trans\": \"zero-shot task generalization\"},\n    {\"word\": \"提出\", \"pinyin\": \"tí chū\", \"trans\": \"propose\"},\n    {\"word\": \"离散\", \"pinyin\": \"lí sàn\", \"trans\": \"discrete\"},\n    {\"word\": \"术语化\", \"pinyin\": \"shù yǔ huà\", \"trans\": \"terminological\"},\n    {\"word\": \"任务定义\", \"pinyin\": \"rèn wù dìng yì\", \"trans\": \"task definition\"},\n    {\"word\": \"图像分割\", \"pinyin\": \"tú xiàng fēn gē\", \"trans\": \"image segmentation\"},\n    {\"word\": \"障碍\", \"pinyin\": \"zhàng ài\", \"trans\": \"obstacle\"},\n    {\"word\": \"验证\", \"pinyin\": \"yàn zhèng\", \"trans\": \"verify\"},\n    {\"word\": \"引入\", \"pinyin\": \"yǐn rù\", \"trans\": \"introduce\"},\n    {\"word\": \"解释性指令\", \"pinyin\": \"jiě shì xìng zhǐ lǐng\", \"trans\": \"explanatory instructions\"},\n    {\"word\": \"语言转换\", \"pinyin\": \"yǔ yán zhuǎn huàn\", \"trans\": \"language transformation\"},\n    {\"word\": \"输入图像\", \"pinyin\": \"shū rù tú xiàng\", \"trans\": \"input image\"},\n    {\"word\": \"输出\", \"pinyin\": \"shū chū\", \"trans\": \"output\"},\n    {\"word\": \"三元组\", \"pinyin\": \"sān yuán zǔ\", \"trans\": \"triplet\"},\n    {\"word\": \"大规模数据集\", \"pinyin\": \"dà guī mó shù jù\", \"trans\": \"large-scale dataset\"},\n    {\"word\": \"自回归视觉-语言模型\", \"pinyin\": \"zì huí guī shì jué yǔ yán mó xíng\", \"trans\": \"autoregressive vision-language model\"},\n    {\"word\": \"遵循\", \"pinyin\": \"zūn zhòng\", \"trans\": \"follow\"},\n    {\"word\": \"指令级别\", \"pinyin\": \"zhǐ lǐng jí bié\", \"trans\": \"instruction level\"},\n    {\"word\": \"零样本能力\", \"pinyin\": \"líng yàng běn néng lì\", \"trans\": \"zero-shot capability\"},\n    {\"word\": \"展示\", \"pinyin\": \"zhǎn shì\", \"trans\": \"demonstrate\"},\n    {\"word\": \"未见过的\", \"pinyin\": \"wèi jiàn guò de\", \"trans\": \"unseen\"},\n    {\"word\": \"存储库\", \"pinyin\": \"cún chǔ kù\", \"trans\": \"repository\"}\n]",
        "trans": "This article discusses the fact that computer vision (CV) has not yet fully achieved the zero-shot task generalization observed in natural language processing (NLP). The authors suggest that CV's use of discrete and terminological task definitions (such as \"image segmentation\") may be a major obstacle to zero-shot task generalization. To validate this, the authors introduce explanatory instructions, defining CV task objectives through detailed language transformations from input images to outputs. They created a large-scale dataset containing 12 million \"image input to explanatory instruction to output\" triplets and trained an autoregressive vision-language model (AR-based VLM) that takes images and explanatory instructions as inputs. By learning to follow these instructions, the AR-based VLM achieved instruction-level zero-shot capabilities and demonstrated strong zero-shot generalization on unseen CV tasks. The code and dataset will be made publicly available on a GitHub repository.",
        "update_ts": "2025-01-01 09:10"
    }
}