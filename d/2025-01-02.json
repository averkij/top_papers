{
    "date": {
        "ru": "2 ÑĞ½Ğ²Ğ°Ñ€Ñ",
        "en": "January 2",
        "zh": "1æœˆ2æ—¥"
    },
    "time_utc": "2025-01-02 07:10",
    "weekday": 3,
    "issue_id": 1454,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.19638",
            "title": "Xmodel-2 Technical Report",
            "url": "https://huggingface.co/papers/2412.19638",
            "abstract": "Xmodel-2 is a 1.2-billion-parameter large language model designed specifically for reasoning tasks. Its architecture enables different model scales to share a unified set of hyperparameters, allowing for extensive experimentation on smaller models and seamless transfer of optimal configurations to larger models. To maximize training efficiency and stability, Xmodel-2 employs the WSD learning rate scheduler from MiniCPM. Pretrained on 1.5 trillion tokens from diverse sources, Xmodel-2 achieves state-of-the-art performance in complex reasoning and agent-based tasks, while maintaining low training costs. These results highlight the potential of efficient model design and training strategies in advancing reasoning capabilities. Model checkpoints and code are publicly available on GitHub at https://github.com/XiaoduoAILab/Xmodel-2",
            "score": 5,
            "issue_id": 1453,
            "pub_date": "2025-12-27",
            "pub_date_card": {
                "ru": "27 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 27",
                "zh": "12æœˆ27æ—¥"
            },
            "hash": "4707dc8ac5a87e66",
            "authors": [
                "Wang Qun",
                "Liu Yang",
                "Lin Qingquan",
                "Qu Zhijiu",
                "Jiang Ling"
            ],
            "affiliations": [
                "AI Lab, Xiaodu Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.19638.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#small_models",
                    "#reasoning",
                    "#open_source",
                    "#architecture"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ Xmodel-2: Ğ¼Ğ¾Ñ‰ÑŒ Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Xmodel-2 - ÑÑ‚Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 1,2 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰Ğ°ÑÑÑ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ•Ñ‘ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±Ğ»ĞµĞ³Ñ‡Ğ°ĞµÑ‚ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸ Ğ¿ĞµÑ€ĞµĞ½Ğ¾Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ WSD Ğ¸Ğ· MiniCPM Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞŸÑ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° 1,5 Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Xmodel-2 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ½Ğ¸Ğ·ĞºĞ¸Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ."
                },
                "en": {
                    "title": "Unlocking Reasoning Power with Efficient Model Design",
                    "desc": "Xmodel-2 is a large language model with 1.2 billion parameters, specifically built for reasoning tasks. It features a flexible architecture that allows different model sizes to use the same hyperparameters, facilitating experimentation and optimization across scales. The model utilizes the WSD learning rate scheduler to enhance training efficiency and stability. With pretraining on 1.5 trillion tokens, Xmodel-2 demonstrates superior performance in complex reasoning tasks while keeping training costs low, showcasing the benefits of efficient model design."
                },
                "zh": {
                    "title": "é«˜æ•ˆæ¨ç†èƒ½åŠ›çš„æ¨¡å‹è®¾è®¡ä¸è®­ç»ƒç­–ç•¥",
                    "desc": "Xmodel-2 æ˜¯ä¸€ä¸ªæ‹¥æœ‰ 12 äº¿å‚æ•°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä¸“é—¨è®¾è®¡ç”¨äºæ¨ç†ä»»åŠ¡ã€‚å®ƒçš„æ¶æ„å…è®¸ä¸åŒè§„æ¨¡çš„æ¨¡å‹å…±äº«ç»Ÿä¸€çš„è¶…å‚æ•°ï¼Œä»è€Œå¯ä»¥åœ¨è¾ƒå°çš„æ¨¡å‹ä¸Šè¿›è¡Œå¹¿æ³›å®éªŒï¼Œå¹¶å°†æœ€ä½³é…ç½®æ— ç¼è½¬ç§»åˆ°æ›´å¤§çš„æ¨¡å‹ä¸Šã€‚ä¸ºäº†æœ€å¤§åŒ–è®­ç»ƒæ•ˆç‡å’Œç¨³å®šæ€§ï¼ŒXmodel-2 é‡‡ç”¨äº† MiniCPM çš„ WSD å­¦ä¹ ç‡è°ƒåº¦å™¨ã€‚ç»è¿‡åœ¨ 1.5 ä¸‡äº¿ä¸ªæ¥è‡ªå¤šæ ·åŒ–æ¥æºçš„æ ‡è®°ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼ŒXmodel-2 åœ¨å¤æ‚æ¨ç†å’ŒåŸºäºä»£ç†çš„ä»»åŠ¡ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†è¾ƒä½çš„è®­ç»ƒæˆæœ¬ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-01-01.html",
    "link_next": "2025-01-03.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "01.01",
        "en": "01/01",
        "zh": "1æœˆ1æ—¥"
    },
    "short_date_next": {
        "ru": "03.01",
        "en": "01/03",
        "zh": "1æœˆ3æ—¥"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†è®¡ç®—æœºè§†è§‰ï¼ˆCVï¼‰å°šæœªå®Œå…¨å®ç°è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­è§‚å¯Ÿåˆ°çš„é›¶æ ·æœ¬ä»»åŠ¡æ³›åŒ–ã€‚ä½œè€…æå‡ºï¼ŒCV ä½¿ç”¨ç¦»æ•£å’Œæœ¯è¯­åŒ–çš„ä»»åŠ¡å®šä¹‰ï¼ˆå¦‚â€œå›¾åƒåˆ†å‰²â€ï¼‰ï¼Œè¿™å¯èƒ½æ˜¯é›¶æ ·æœ¬ä»»åŠ¡æ³›åŒ–çš„ä¸»è¦éšœç¢ã€‚ä¸ºäº†éªŒè¯è¿™ä¸€ç‚¹ï¼Œä½œè€…å¼•å…¥äº†è§£é‡Šæ€§æŒ‡ä»¤ï¼Œé€šè¿‡è¯¦ç»†çš„è¯­è¨€è½¬æ¢ä»è¾“å…¥å›¾åƒåˆ°è¾“å‡ºæ¥å®šä¹‰ CV ä»»åŠ¡ç›®æ ‡ã€‚ä»–ä»¬åˆ›å»ºäº†ä¸€ä¸ªåŒ…å« 1200 ä¸‡ä¸ªâ€œå›¾åƒè¾“å…¥åˆ°è§£é‡Šæ€§æŒ‡ä»¤åˆ°è¾“å‡ºâ€ä¸‰å…ƒç»„çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå¹¶è®­ç»ƒäº†ä¸€ä¸ªè‡ªå›å½’è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆAR-based VLMï¼‰ï¼Œè¯¥æ¨¡å‹ä»¥å›¾åƒå’Œè§£é‡Šæ€§æŒ‡ä»¤ä½œä¸ºè¾“å…¥ã€‚é€šè¿‡å­¦ä¹ éµå¾ªè¿™äº›æŒ‡ä»¤ï¼ŒAR-based VLM å®ç°äº†æŒ‡ä»¤çº§åˆ«çš„é›¶æ ·æœ¬èƒ½åŠ›ï¼Œå¹¶åœ¨æœªè§è¿‡çš„ CV ä»»åŠ¡ä¸Šå±•ç¤ºäº†å¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–ã€‚ä»£ç å’Œæ•°æ®é›†å°†åœ¨ GitHub å­˜å‚¨åº“ä¸­å…¬å¼€ã€‚",
        "title": "Explanatory Instructions: Towards Unified Vision Tasks Understanding and Zero-shot Generalization",
        "pinyin": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†è®¡ç®—æœºè§†è§‰ï¼ˆCVï¼‰å°šæœªå®Œå…¨å®ç°è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­è§‚å¯Ÿåˆ°çš„é›¶æ ·æœ¬ä»»åŠ¡æ³›åŒ–ã€‚\nZhÃ¨ piÄn wÃ©nzhÄng tÇolÃ¹n le jÃ¬suÃ njÄ« shÃ¬juÃ© (CV) shÃ ng wÃ¨i quÃ¡nxiÃ n shÃ­xiÃ n zÃ¬rÃ¡n yÇ”yÃ¡n chÇ”lÇ (NLP) zhÅng guÄnchÃ¡ dÃ o de lÃ­ng yÃ ngbÇn rÃ¨nwÃ¹ fÃ nhuÃ .\n\nä½œè€…æå‡ºï¼ŒCV ä½¿ç”¨ç¦»æ•£å’Œæœ¯è¯­åŒ–çš„ä»»åŠ¡å®šä¹‰ï¼ˆå¦‚â€œå›¾åƒåˆ†å‰²â€ï¼‰ï¼Œè¿™å¯èƒ½æ˜¯é›¶æ ·æœ¬ä»»åŠ¡æ³›åŒ–çš„ä¸»è¦éšœç¢ã€‚\nZuÃ²zhÄ› tÃ­chÅ«, CV shÇyÃ²ng lÃ­ sÃ n hÃ© shÃ¹yÇ” huÃ  de rÃ¨nwÃ¹ dÃ¬ngyÃ¬ (rÃº â€œtÃºxiÃ ng fÄ“ngÄ“â€), zhÃ¨ kÄ›nÃ©ng shÃ¬ lÃ­ng yÃ ngbÇn rÃ¨nwÃ¹ fÃ nhuÃ  de zhÇ”yÃ o zhÃ ng'Ã i.\n\nä¸ºäº†éªŒè¯è¿™ä¸€ç‚¹ï¼Œä½œè€…å¼•å…¥äº†è§£é‡Šæ€§æŒ‡ä»¤ï¼Œé€šè¿‡è¯¦ç»†çš„è¯­è¨€è½¬æ¢ä»è¾“å…¥å›¾åƒåˆ°è¾“å‡ºæ¥å®šä¹‰ CV ä»»åŠ¡ç›®æ ‡ã€‚\nWÃ¨ile yÃ nzhÃ¨ng zhÃ¨ yÄ« diÇn, zuÃ²zhÄ› yÇnrÃ¹ le jiÄ›shÃ¬ xÃ¬ng zhÇlÃ¬ng, tÅngguÃ² xiÃ¡ngxÃ¬ de yÇ”yÃ¡n zhuÇnhuÃ n cÃ³ng shÅ«rÃ¹ tÃºxiÃ ng dÃ o shÅ«chÅ« lÃ¡i dÃ¬ngyÃ¬ CV rÃ¨nwÃ¹ mÃ¹biÄo.\n\nä»–ä»¬åˆ›å»ºäº†ä¸€ä¸ªåŒ…å« 1200 ä¸‡ä¸ªâ€œå›¾åƒè¾“å…¥åˆ°è§£é‡Šæ€§æŒ‡ä»¤åˆ°è¾“å‡ºâ€ä¸‰å…ƒç»„çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå¹¶è®­ç»ƒäº†ä¸€ä¸ªè‡ªå›å½’è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆAR-based VLMï¼‰ï¼Œè¯¥æ¨¡å‹ä»¥å›¾åƒå’Œè§£é‡Šæ€§æŒ‡ä»¤ä½œä¸ºè¾“å…¥ã€‚\nTÄmen chuÃ ngjiÃ n le yÄ«gÃ¨ bÄohÃ¡n 1200 wÃ n gÃ¨ â€œtÃºxiÃ ng shÅ«rÃ¹ dÃ o jiÄ›shÃ¬ xÃ¬ng zhÇlÃ¬ng dÃ o shÅ«chÅ«â€ sÄnyuÃ¡nzÇ” de dÃ  guÄ«mÃ³ shÃ¹jÃ¹jÃ­, bÃ¬ng xÃ¹nliÃ n le yÄ«gÃ¨ zÃ¬ huÃ­guÄ« shÃ¬juÃ©-yÇ”yÃ¡n mÃ³xÃ­ng (AR-based VLM), gÄi mÃ³xÃ­ng yÇ tÃºxiÃ ng hÃ© jiÄ›shÃ¬ xÃ¬ng zhÇlÃ¬ng zuÃ²wÃ©i shÅ«rÃ¹.\n\né€šè¿‡å­¦ä¹ éµå¾ªè¿™äº›æŒ‡ä»¤ï¼ŒAR-based VLM å®ç°äº†æŒ‡ä»¤çº§åˆ«çš„é›¶æ ·æœ¬èƒ½åŠ›ï¼Œå¹¶åœ¨æœªè§è¿‡çš„ CV ä»»åŠ¡ä¸Šå±•ç¤ºäº†å¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–ã€‚\nTÅngguÃ² xuÃ©xÃ­ zÅ«nxÃºn zhÃ¨xiÄ“ zhÇlÃ¬ng, AR-based VLM shÃ­xiÃ n le zhÇlÃ¬ng jÃ­biÃ© de lÃ­ng yÃ ngbÇn nÃ©nglÃ¬, bÃ¬ng zÃ i wÃ¨i jiÃ nguÃ² de CV rÃ¨nwÃ¹ shÃ ng zhÇnshÃ¬ le qiÃ¡ngdÃ  de lÃ­ng yÃ ngbÇn fÃ nhuÃ .\n\nä»£ç å’Œæ•°æ®é›†å°†åœ¨ GitHub å­˜å‚¨åº“ä¸­å…¬å¼€ã€‚\nDÃ imÇ hÃ© shÃ¹jÃ¹jÃ­ jiÄng zÃ i GitHub cÃºnchÅ«kÃ¹ zhÅng gÅngkÄi.",
        "vocab": "[\n    {\"word\": \"è®¨è®º\", \"pinyin\": \"tÇo lÃ¹n\", \"trans\": \"discuss\"},\n    {\"word\": \"è®¡ç®—æœºè§†è§‰\", \"pinyin\": \"jÃ¬ suÃ n jÄ« shÃ¬ juÃ©\", \"trans\": \"computer vision\"},\n    {\"word\": \"è‡ªç„¶è¯­è¨€å¤„ç†\", \"pinyin\": \"zÃ¬ rÃ¡n yÇ” yÃ¡n chÇ” lÇ\", \"trans\": \"natural language processing\"},\n    {\"word\": \"è§‚å¯Ÿ\", \"pinyin\": \"guÄn chÃ¡\", \"trans\": \"observe\"},\n    {\"word\": \"é›¶æ ·æœ¬ä»»åŠ¡æ³›åŒ–\", \"pinyin\": \"lÃ­ng yÃ ng bÄ›n rÃ¨n wÃ¹ fÃ n huÃ \", \"trans\": \"zero-shot task generalization\"},\n    {\"word\": \"æå‡º\", \"pinyin\": \"tÃ­ chÅ«\", \"trans\": \"propose\"},\n    {\"word\": \"ç¦»æ•£\", \"pinyin\": \"lÃ­ sÃ n\", \"trans\": \"discrete\"},\n    {\"word\": \"æœ¯è¯­åŒ–\", \"pinyin\": \"shÃ¹ yÇ” huÃ \", \"trans\": \"terminological\"},\n    {\"word\": \"ä»»åŠ¡å®šä¹‰\", \"pinyin\": \"rÃ¨n wÃ¹ dÃ¬ng yÃ¬\", \"trans\": \"task definition\"},\n    {\"word\": \"å›¾åƒåˆ†å‰²\", \"pinyin\": \"tÃº xiÃ ng fÄ“n gÄ“\", \"trans\": \"image segmentation\"},\n    {\"word\": \"éšœç¢\", \"pinyin\": \"zhÃ ng Ã i\", \"trans\": \"obstacle\"},\n    {\"word\": \"éªŒè¯\", \"pinyin\": \"yÃ n zhÃ¨ng\", \"trans\": \"verify\"},\n    {\"word\": \"å¼•å…¥\", \"pinyin\": \"yÇn rÃ¹\", \"trans\": \"introduce\"},\n    {\"word\": \"è§£é‡Šæ€§æŒ‡ä»¤\", \"pinyin\": \"jiÄ› shÃ¬ xÃ¬ng zhÇ lÇng\", \"trans\": \"explanatory instructions\"},\n    {\"word\": \"è¯­è¨€è½¬æ¢\", \"pinyin\": \"yÇ” yÃ¡n zhuÇn huÃ n\", \"trans\": \"language transformation\"},\n    {\"word\": \"è¾“å…¥å›¾åƒ\", \"pinyin\": \"shÅ« rÃ¹ tÃº xiÃ ng\", \"trans\": \"input image\"},\n    {\"word\": \"è¾“å‡º\", \"pinyin\": \"shÅ« chÅ«\", \"trans\": \"output\"},\n    {\"word\": \"ä¸‰å…ƒç»„\", \"pinyin\": \"sÄn yuÃ¡n zÇ”\", \"trans\": \"triplet\"},\n    {\"word\": \"å¤§è§„æ¨¡æ•°æ®é›†\", \"pinyin\": \"dÃ  guÄ« mÃ³ shÃ¹ jÃ¹\", \"trans\": \"large-scale dataset\"},\n    {\"word\": \"è‡ªå›å½’è§†è§‰-è¯­è¨€æ¨¡å‹\", \"pinyin\": \"zÃ¬ huÃ­ guÄ« shÃ¬ juÃ© yÇ” yÃ¡n mÃ³ xÃ­ng\", \"trans\": \"autoregressive vision-language model\"},\n    {\"word\": \"éµå¾ª\", \"pinyin\": \"zÅ«n zhÃ²ng\", \"trans\": \"follow\"},\n    {\"word\": \"æŒ‡ä»¤çº§åˆ«\", \"pinyin\": \"zhÇ lÇng jÃ­ biÃ©\", \"trans\": \"instruction level\"},\n    {\"word\": \"é›¶æ ·æœ¬èƒ½åŠ›\", \"pinyin\": \"lÃ­ng yÃ ng bÄ›n nÃ©ng lÃ¬\", \"trans\": \"zero-shot capability\"},\n    {\"word\": \"å±•ç¤º\", \"pinyin\": \"zhÇn shÃ¬\", \"trans\": \"demonstrate\"},\n    {\"word\": \"æœªè§è¿‡çš„\", \"pinyin\": \"wÃ¨i jiÃ n guÃ² de\", \"trans\": \"unseen\"},\n    {\"word\": \"å­˜å‚¨åº“\", \"pinyin\": \"cÃºn chÇ” kÃ¹\", \"trans\": \"repository\"}\n]",
        "trans": "This article discusses the fact that computer vision (CV) has not yet fully achieved the zero-shot task generalization observed in natural language processing (NLP). The authors suggest that CV's use of discrete and terminological task definitions (such as \"image segmentation\") may be a major obstacle to zero-shot task generalization. To validate this, the authors introduce explanatory instructions, defining CV task objectives through detailed language transformations from input images to outputs. They created a large-scale dataset containing 12 million \"image input to explanatory instruction to output\" triplets and trained an autoregressive vision-language model (AR-based VLM) that takes images and explanatory instructions as inputs. By learning to follow these instructions, the AR-based VLM achieved instruction-level zero-shot capabilities and demonstrated strong zero-shot generalization on unseen CV tasks. The code and dataset will be made publicly available on a GitHub repository.",
        "update_ts": "2025-01-01 09:10"
    }
}