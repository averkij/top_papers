
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 16 papers. September 16.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">16 сентября</span> | <span id="title-articles-count">16 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-09-15.html">⬅️ <span id="prev-date">15.09</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-09-17.html">➡️ <span id="next-date">17.09</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-09.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '16 сентября', 'en': 'September 16', 'zh': '9月16日'};
        let feedDateNext = {'ru': '17.09', 'en': '09/17', 'zh': '9月17日'};
        let feedDatePrev = {'ru': '15.09', 'en': '09/15', 'zh': '9月15日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2509.12201', 'title': 'OmniWorld: A Multi-Domain and Multi-Modal Dataset for 4D World Modeling', 'url': 'https://huggingface.co/papers/2509.12201', 'abstract': "OmniWorld, a large-scale, multi-domain, multi-modal dataset, addresses the limitations of existing 4D world modeling datasets and benchmarks, enabling significant performance improvements in 4D reconstruction and video generation.  \t\t\t\t\tAI-generated summary \t\t\t\t The field of 4D world modeling - aiming to jointly capture spatial geometry and temporal dynamics - has witnessed remarkable progress in recent years, driven by advances in large-scale generative models and multimodal learning. However, the development of truly general 4D world models remains fundamentally constrained by the availability of high-quality data. Existing datasets and benchmarks often lack the dynamic complexity, multi-domain diversity, and spatial-temporal annotations required to support key tasks such as 4D geometric reconstruction, future prediction, and camera-control video generation. To address this gap, we introduce OmniWorld, a large-scale, multi-domain, multi-modal dataset specifically designed for 4D world modeling. OmniWorld consists of a newly collected OmniWorld-Game dataset and several curated public datasets spanning diverse domains. Compared with existing synthetic datasets, OmniWorld-Game provides richer modality coverage, larger scale, and more realistic dynamic interactions. Based on this dataset, we establish a challenging benchmark that exposes the limitations of current state-of-the-art (SOTA) approaches in modeling complex 4D environments. Moreover, fine-tuning existing SOTA methods on OmniWorld leads to significant performance gains across 4D reconstruction and video generation tasks, strongly validating OmniWorld as a powerful resource for training and evaluation. We envision OmniWorld as a catalyst for accelerating the development of general-purpose 4D world models, ultimately advancing machines' holistic understanding of the physical world.", 'score': 71, 'issue_id': 5907, 'pub_date': '2025-09-15', 'pub_date_card': {'ru': '15 сентября', 'en': 'September 15', 'zh': '9月15日'}, 'hash': '6236e5a1b21f6911', 'authors': ['Yang Zhou', 'Yifan Wang', 'Jianjun Zhou', 'Wenzheng Chang', 'Haoyu Guo', 'Zizun Li', 'Kaijing Ma', 'Xinyue Li', 'Yating Wang', 'Haoyi Zhu', 'Mingyu Liu', 'Dingning Liu', 'Jiange Yang', 'Zhoujie Fu', 'Junyi Chen', 'Chunhua Shen', 'Jiangmiao Pang', 'Kaipeng Zhang', 'Tong He'], 'affiliations': ['Shanghai AI Lab', 'ZJU'], 'pdf_title_img': 'assets/pdf/title_img/2509.12201.jpg', 'data': {'categories': ['#games', '#multimodal', '#dataset', '#video', '#synthetic', '#benchmark'], 'emoji': '🌐', 'ru': {'title': 'OmniWorld: прорыв в моделировании 4D-миров', 'desc': 'OmniWorld - это крупномасштабный многодоменный мультимодальный датасет для моделирования 4D-миров. Он преодолевает ограничения существующих наборов данных, предоставляя более богатое покрытие модальностей, больший масштаб и более реалистичные динамические взаимодействия. OmniWorld включает новый набор данных OmniWorld-Game и несколько курированных публичных датасетов из разных доменов. Использование OmniWorld позволяет значительно улучшить производительность в задачах 4D-реконструкции и генерации видео.'}, 'en': {'title': 'OmniWorld: Revolutionizing 4D World Modeling with Rich Data', 'desc': 'OmniWorld is a comprehensive dataset designed to enhance 4D world modeling, which combines spatial and temporal data. It addresses the shortcomings of current datasets by providing diverse, high-quality data that includes dynamic interactions and multi-domain scenarios. The dataset supports critical tasks like 4D geometric reconstruction and video generation, allowing for better training of machine learning models. By benchmarking existing state-of-the-art methods on OmniWorld, significant performance improvements are observed, showcasing its potential to advance the field of 4D modeling.'}, 'zh': {'title': 'OmniWorld：推动4D世界建模的新动力', 'desc': 'OmniWorld是一个大规模的多领域多模态数据集，旨在解决现有4D世界建模数据集的局限性。它支持4D几何重建和视频生成等关键任务，提供了丰富的动态复杂性和多样性。通过引入OmniWorld-Game数据集，OmniWorld在模态覆盖、规模和动态交互方面优于现有合成数据集。我们希望OmniWorld能加速通用4D世界模型的发展，提升机器对物理世界的整体理解。'}}}, {'id': 'https://huggingface.co/papers/2509.11543', 'title': 'UI-S1: Advancing GUI Automation via Semi-online Reinforcement Learning', 'url': 'https://huggingface.co/papers/2509.11543', 'abstract': 'Semi-online Reinforcement Learning addresses the limitations of offline and online RL by simulating online RL on offline trajectories, achieving state-of-the-art performance in dynamic benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Graphical User Interface (GUI) agents have demonstrated remarkable progress in automating complex user interface interactions through reinforcement learning. However, current approaches face a fundamental dilemma: offline RL enables stable training on pre-collected trajectories, but struggles with multi-step task execution for lack of trajectory-level reward signals; online RL captures these signals through environment interaction, but suffers from sparse rewards and prohibitive deployment costs. To address it, we present Semi-online Reinforcement Learning, a novel paradigm that simulates online RL on offline trajectories. During each rollout process, we preserve the original model output within the multi-turn dialogue, where a Patch Module adaptively recovers the divergence between rollout and expert trajectories. To capture long-term training signals, Semi-online RL introduces discounted future returns into the reward computation and optimizes the policy with weighted step-level and episode-level advantages. We further introduce Semi-Online Performance (SOP), a metric that aligns better with true online performance, serving as a practical and effective proxy for real-world evaluation. Experiments show that ours Semi-online RL achieves SOTA performance among 7B models across four dynamic benchmarks, with significant gains over the base model (e.g., +12.0% on AndroidWorld, +23.8% on AITW), demonstrating significant progress in bridging the gap between offline training efficiency and online multi-turn reasoning. The code is available at https://github.com/X-PLUG/MobileAgent/tree/main/UI-S1.', 'score': 33, 'issue_id': 5907, 'pub_date': '2025-09-15', 'pub_date_card': {'ru': '15 сентября', 'en': 'September 15', 'zh': '9月15日'}, 'hash': 'dcbb6b99868e170d', 'authors': ['Zhengxi Lu', 'Jiabo Ye', 'Fei Tang', 'Yongliang Shen', 'Haiyang Xu', 'Ziwei Zheng', 'Weiming Lu', 'Ming Yan', 'Fei Huang', 'Jun Xiao', 'Yueting Zhuang'], 'affiliations': ['Tongyi Lab, Alibaba Group', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2509.11543.jpg', 'data': {'categories': ['#rl', '#games', '#agents', '#reasoning', '#optimization', '#rlhf', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'Полуонлайновое обучение с подкреплением: лучшее из двух миров', 'desc': 'Статья представляет новую парадигму обучения с подкреплением - полуонлайновое обучение (Semi-online Reinforcement Learning). Этот метод симулирует онлайн-обучение на офлайн-траекториях, сочетая преимущества офлайн- и онлайн-подходов. Авторы вводят модуль Patch для адаптивного восстановления расхождений между симулируемыми и экспертными траекториями. Эксперименты показывают, что предложенный метод достигает наилучших результатов среди моделей размером 7 миллиардов параметров на четырех динамических бенчмарках.'}, 'en': {'title': 'Bridging Offline and Online RL for Superior Performance', 'desc': "Semi-online Reinforcement Learning (RL) is a new approach that combines the strengths of offline and online RL to improve performance in dynamic environments. It simulates online RL using pre-collected offline trajectories, allowing for stable training while addressing the challenges of multi-step task execution. The method incorporates a Patch Module to align the model's outputs with expert trajectories and uses discounted future returns to enhance reward computation. Experiments show that this approach significantly outperforms existing models, achieving state-of-the-art results in various benchmarks."}, 'zh': {'title': '半在线强化学习：连接离线效率与在线推理的桥梁', 'desc': '半在线强化学习（Semi-online Reinforcement Learning）解决了离线和在线强化学习的局限性，通过在离线轨迹上模拟在线强化学习，达到了动态基准测试中的最先进性能。该方法在每次回合过程中保留了多轮对话中的原始模型输出，并通过补丁模块自适应地恢复回合与专家轨迹之间的差异。为了捕捉长期训练信号，半在线强化学习在奖励计算中引入了折扣未来收益，并使用加权的步骤级和回合级优势来优化策略。实验结果表明，半在线强化学习在四个动态基准测试中相较于基础模型有显著提升，成功缩小了离线训练效率与在线多轮推理之间的差距。'}}}, {'id': 'https://huggingface.co/papers/2509.10813', 'title': 'InternScenes: A Large-scale Simulatable Indoor Scene Dataset with\n  Realistic Layouts', 'url': 'https://huggingface.co/papers/2509.10813', 'abstract': 'InternScenes is a large-scale, diverse, and realistic indoor scene dataset that addresses limitations in existing datasets, enabling better scene layout generation and point-goal navigation.  \t\t\t\t\tAI-generated summary \t\t\t\t The advancement of Embodied AI heavily relies on large-scale, simulatable 3D scene datasets characterized by scene diversity and realistic layouts. However, existing datasets typically suffer from limitations in data scale or diversity, sanitized layouts lacking small items, and severe object collisions. To address these shortcomings, we introduce InternScenes, a novel large-scale simulatable indoor scene dataset comprising approximately 40,000 diverse scenes by integrating three disparate scene sources, real-world scans, procedurally generated scenes, and designer-created scenes, including 1.96M 3D objects and covering 15 common scene types and 288 object classes. We particularly preserve massive small items in the scenes, resulting in realistic and complex layouts with an average of 41.5 objects per region. Our comprehensive data processing pipeline ensures simulatability by creating real-to-sim replicas for real-world scans, enhances interactivity by incorporating interactive objects into these scenes, and resolves object collisions by physical simulations. We demonstrate the value of InternScenes with two benchmark applications: scene layout generation and point-goal navigation. Both show the new challenges posed by the complex and realistic layouts. More importantly, InternScenes paves the way for scaling up the model training for both tasks, making the generation and navigation in such complex scenes possible. We commit to open-sourcing the data, models, and benchmarks to benefit the whole community.', 'score': 22, 'issue_id': 5907, 'pub_date': '2025-09-13', 'pub_date_card': {'ru': '13 сентября', 'en': 'September 13', 'zh': '9月13日'}, 'hash': 'b7614d46b6b62960', 'authors': ['Weipeng Zhong', 'Peizhou Cao', 'Yichen Jin', 'Li Luo', 'Wenzhe Cai', 'Jingli Lin', 'Hanqing Wang', 'Zhaoyang Lyu', 'Tai Wang', 'Bo Dai', 'Xudong Xu', 'Jiangmiao Pang'], 'affiliations': ['Beihang University', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2509.10813.jpg', 'data': {'categories': ['#3d', '#data', '#games', '#dataset', '#training', '#open_source', '#benchmark'], 'emoji': '🏠', 'ru': {'title': 'Реалистичные интерьеры для прорыва в Embodied AI', 'desc': 'InternScenes - это новый масштабный набор данных для симуляции интерьеров, содержащий около 40 000 разнообразных сцен. Он объединяет реальные сканы, процедурно сгенерированные и созданные дизайнерами сцены, включая 1,96 млн 3D-объектов. Особое внимание уделено сохранению мелких предметов, что делает макеты более реалистичными и сложными. Набор данных решает проблемы существующих датасетов и открывает новые возможности для обучения моделей генерации макетов и навигации.'}, 'en': {'title': 'InternScenes: A Game-Changer for Indoor Scene Understanding', 'desc': 'InternScenes is a new dataset designed to improve the training of AI in understanding and navigating indoor environments. It includes around 40,000 diverse scenes with realistic layouts, featuring a wide variety of small objects to enhance complexity. The dataset addresses common issues in existing datasets, such as lack of diversity and object collisions, by using a combination of real-world scans, procedural generation, and designer input. By providing a rich and interactive environment, InternScenes supports advancements in scene layout generation and point-goal navigation tasks for Embodied AI.'}, 'zh': {'title': 'InternScenes：推动室内场景理解的未来', 'desc': 'InternScenes是一个大规模、多样化且真实的室内场景数据集，旨在解决现有数据集的局限性，从而改善场景布局生成和目标导航。该数据集包含约40,000个多样化场景，整合了真实世界扫描、程序生成场景和设计师创建的场景，涵盖了1.96百万个3D物体和15种常见场景类型。我们特别保留了大量小物品，使得场景布局更加真实和复杂，平均每个区域有41.5个物体。InternScenes为模型训练提供了新的挑战，并承诺开源数据、模型和基准，以造福整个社区。'}}}, {'id': 'https://huggingface.co/papers/2509.10708', 'title': 'SearchInstruct: Enhancing Domain Adaptation via Retrieval-Based\n  Instruction Dataset Creation', 'url': 'https://huggingface.co/papers/2509.10708', 'abstract': 'SearchInstruct enhances supervised fine-tuning datasets for large language models by expanding domain-specific questions and retrieving accurate answers, improving model performance and enabling efficient model editing.  \t\t\t\t\tAI-generated summary \t\t\t\t Supervised Fine-Tuning (SFT) is essential for training large language models (LLMs), significantly enhancing critical capabilities such as instruction following and in-context learning. Nevertheless, creating suitable training datasets tailored for specific domains remains challenging due to unique domain constraints and data scarcity. In this paper, we propose SearchInstruct, an innovative method explicitly designed to construct high quality instruction datasets for SFT. Our approach begins with a limited set of domain specific, human generated questions, which are systematically expanded using a large language model. Subsequently, domain relevant resources are dynamically retrieved to generate accurate and contextually appropriate answers for each augmented question. Experimental evaluation demonstrates that SearchInstruct enhances both the diversity and quality of SFT datasets, leading to measurable improvements in LLM performance within specialized domains. Additionally, we show that beyond dataset generation, the proposed method can also effectively facilitate tasks such as model editing, enabling efficient updates to existing models. To facilitate reproducibility and community adoption, we provide full implementation details, the complete set of generated instruction response pairs, and the source code in a publicly accessible Git repository: [https://github.com/mostafaamiri/SearchInstruct](https://github.com/mostafaamiri/SearchInstruct)', 'score': 9, 'issue_id': 5917, 'pub_date': '2025-09-12', 'pub_date_card': {'ru': '12 сентября', 'en': 'September 12', 'zh': '9月12日'}, 'hash': 'dbe5e6e435863e63', 'authors': ['Iman Barati', 'Mostafa Amiri', 'Heshaam Faili'], 'affiliations': ['Iran University of Science and Technology', 'University of Tehran'], 'pdf_title_img': 'assets/pdf/title_img/2509.10708.jpg', 'data': {'categories': ['#dataset', '#optimization', '#training', '#open_source', '#data'], 'emoji': '🔍', 'ru': {'title': 'SearchInstruct: Умное расширение данных для точной настройки языковых моделей', 'desc': 'SearchInstruct - это новый метод создания качественных наборов данных для обучения больших языковых моделей (LLM) в специфических доменах. Он расширяет начальный набор вопросов с помощью LLM и динамически извлекает релевантную информацию для генерации точных ответов. Эксперименты показывают, что SearchInstruct улучшает разнообразие и качество наборов данных, повышая производительность LLM в специализированных областях. Метод также эффективен для редактирования существующих моделей.'}, 'en': {'title': 'Enhancing Fine-Tuning Datasets for Better Language Model Performance', 'desc': 'This paper introduces SearchInstruct, a method that improves supervised fine-tuning datasets for large language models (LLMs) by expanding domain-specific questions and retrieving accurate answers. The process starts with a small set of human-generated questions, which are then expanded using a large language model to create a more diverse dataset. By dynamically retrieving relevant resources, the method ensures that the answers generated are both accurate and contextually appropriate. The results show that SearchInstruct not only enhances dataset quality but also aids in efficient model editing, allowing for better performance in specialized domains.'}, 'zh': {'title': 'SearchInstruct：提升语言模型的微调数据集质量', 'desc': '本论文提出了一种名为SearchInstruct的方法，用于增强大型语言模型的监督微调数据集。该方法通过扩展特定领域的问题，并动态检索相关资源，生成准确且符合上下文的答案，从而提高模型的性能。SearchInstruct不仅提升了数据集的多样性和质量，还有效支持模型编辑，便于对现有模型进行高效更新。实验结果表明，该方法在特定领域的应用中显著改善了大型语言模型的表现。'}}}, {'id': 'https://huggingface.co/papers/2509.12203', 'title': 'LazyDrag: Enabling Stable Drag-Based Editing on Multi-Modal Diffusion\n  Transformers via Explicit Correspondence', 'url': 'https://huggingface.co/papers/2509.12203', 'abstract': "LazyDrag, a drag-based image editing method for Multi-Modal Diffusion Transformers, eliminates implicit point matching, enabling precise geometric control and text guidance without test-time optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t The reliance on implicit point matching via attention has become a core bottleneck in drag-based editing, resulting in a fundamental compromise on weakened inversion strength and costly test-time optimization (TTO). This compromise severely limits the generative capabilities of diffusion models, suppressing high-fidelity inpainting and text-guided creation. In this paper, we introduce LazyDrag, the first drag-based image editing method for Multi-Modal Diffusion Transformers, which directly eliminates the reliance on implicit point matching. In concrete terms, our method generates an explicit correspondence map from user drag inputs as a reliable reference to boost the attention control. This reliable reference opens the potential for a stable full-strength inversion process, which is the first in the drag-based editing task. It obviates the necessity for TTO and unlocks the generative capability of models. Therefore, LazyDrag naturally unifies precise geometric control with text guidance, enabling complex edits that were previously out of reach: opening the mouth of a dog and inpainting its interior, generating new objects like a ``tennis ball'', or for ambiguous drags, making context-aware changes like moving a hand into a pocket. Additionally, LazyDrag supports multi-round workflows with simultaneous move and scale operations. Evaluated on the DragBench, our method outperforms baselines in drag accuracy and perceptual quality, as validated by VIEScore and human evaluation. LazyDrag not only establishes new state-of-the-art performance, but also paves a new way to editing paradigms.", 'score': 8, 'issue_id': 5907, 'pub_date': '2025-09-15', 'pub_date_card': {'ru': '15 сентября', 'en': 'September 15', 'zh': '9月15日'}, 'hash': '5c030994bd1fdc14', 'authors': ['Zixin Yin', 'Xili Dai', 'Duomin Wang', 'Xianfang Zeng', 'Lionel M. Ni', 'Gang Yu', 'Heung-Yeung Shum'], 'affiliations': ['StepFun', 'The Hong Kong University of Science and Technology', 'The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2509.12203.jpg', 'data': {'categories': ['#multimodal', '#cv', '#diffusion', '#benchmark'], 'emoji': '🖼️', 'ru': {'title': 'LazyDrag: Революция в редактировании изображений с помощью ИИ', 'desc': 'LazyDrag - это новый метод редактирования изображений для мультимодальных диффузионных трансформеров. Он устраняет необходимость в неявном сопоставлении точек, что позволяет осуществлять точный геометрический контроль и текстовое управление без оптимизации во время тестирования. LazyDrag генерирует явную карту соответствия на основе пользовательских перетаскиваний, что обеспечивает надежную опору для управления вниманием модели. Это позволяет объединить точный геометрический контроль с текстовым управлением, открывая возможности для сложных правок изображений.'}, 'en': {'title': 'LazyDrag: Revolutionizing Image Editing with Precision and Control', 'desc': 'LazyDrag is a novel drag-based image editing technique designed for Multi-Modal Diffusion Transformers that removes the need for implicit point matching, which has been a major limitation in previous methods. By generating an explicit correspondence map from user inputs, LazyDrag enhances attention control and allows for a more robust inversion process without the need for test-time optimization. This advancement enables users to perform complex edits, such as inpainting and generating new objects, with greater precision and flexibility. The method has been shown to outperform existing techniques in terms of drag accuracy and perceptual quality, setting a new standard in the field of image editing.'}, 'zh': {'title': 'LazyDrag：精确控制与文本指导的图像编辑新方法', 'desc': 'LazyDrag是一种基于拖动的图像编辑方法，专为多模态扩散变换器设计。它消除了对隐式点匹配的依赖，从而实现了精确的几何控制和文本指导，而无需在测试时进行优化。通过生成用户拖动输入的显式对应图，LazyDrag提高了注意力控制的可靠性，支持复杂的编辑任务。该方法在DragBench上的评估显示，LazyDrag在拖动精度和感知质量方面超越了现有基线，确立了新的最先进性能。'}}}, {'id': 'https://huggingface.co/papers/2509.09672', 'title': 'Locality in Image Diffusion Models Emerges from Data Statistics', 'url': 'https://huggingface.co/papers/2509.09672', 'abstract': 'Research shows that locality in deep diffusion models is a statistical property of image datasets rather than an inductive bias of convolutional neural networks, leading to the development of a more accurate analytical denoiser.  \t\t\t\t\tAI-generated summary \t\t\t\t Among generative models, diffusion models are uniquely intriguing due to the existence of a closed-form optimal minimizer of their training objective, often referred to as the optimal denoiser. However, diffusion using this optimal denoiser merely reproduces images in the training set and hence fails to capture the behavior of deep diffusion models. Recent work has attempted to characterize this gap between the optimal denoiser and deep diffusion models, proposing analytical, training-free models that can generate images that resemble those generated by a trained UNet. The best-performing method hypothesizes that shift equivariance and locality inductive biases of convolutional neural networks are the cause of the performance gap, hence incorporating these assumptions into its analytical model. In this work, we present evidence that the locality in deep diffusion models emerges as a statistical property of the image dataset, not due to the inductive bias of convolutional neural networks. Specifically, we demonstrate that an optimal parametric linear denoiser exhibits similar locality properties to the deep neural denoisers. We further show, both theoretically and experimentally, that this locality arises directly from the pixel correlations present in natural image datasets. Finally, we use these insights to craft an analytical denoiser that better matches scores predicted by a deep diffusion model than the prior expert-crafted alternative.', 'score': 7, 'issue_id': 5908, 'pub_date': '2025-09-11', 'pub_date_card': {'ru': '11 сентября', 'en': 'September 11', 'zh': '9月11日'}, 'hash': '80381c11a7c56ebb', 'authors': ['Artem Lukoianov', 'Chenyang Yuan', 'Justin Solomon', 'Vincent Sitzmann'], 'affiliations': ['Massachusetts Institute of Technology', 'Toyota Research Institute'], 'pdf_title_img': 'assets/pdf/title_img/2509.09672.jpg', 'data': {'categories': ['#optimization', '#dataset', '#cv', '#diffusion', '#data'], 'emoji': '🔍', 'ru': {'title': 'Локальность в диффузионных моделях: свойство данных, а не нейросетей', 'desc': 'Исследование показывает, что локальность в глубоких диффузионных моделях является статистическим свойством наборов данных изображений, а не индуктивным смещением сверточных нейронных сетей. Авторы демонстрируют, что оптимальный параметрический линейный денойзер проявляет свойства локальности, схожие с глубокими нейронными денойзерами. Теоретически и экспериментально показано, что эта локальность возникает непосредственно из корреляций пикселей в наборах данных естественных изображений. На основе этих выводов разработан аналитический денойзер, который лучше соответствует оценкам, предсказанным глубокой диффузионной моделью.'}, 'en': {'title': 'Locality in Deep Diffusion: A Dataset Property, Not a Network Bias!', 'desc': 'This paper investigates the relationship between locality in deep diffusion models and the statistical properties of image datasets. It argues that the observed locality is not a result of the inductive biases inherent in convolutional neural networks, but rather a characteristic of the datasets themselves. The authors demonstrate that an optimal linear denoiser can replicate the locality properties found in deep neural denoisers, suggesting that pixel correlations in natural images drive this phenomenon. By leveraging these findings, they develop a new analytical denoiser that outperforms previous models in aligning with the scores from deep diffusion models.'}, 'zh': {'title': '揭示深度扩散模型的局部性本质', 'desc': '本研究表明，深度扩散模型中的局部性是图像数据集的统计特性，而不是卷积神经网络的归纳偏置。这一发现促使我们开发出一种更准确的分析去噪器。我们证明了最优参数线性去噪器与深度神经去噪器具有相似的局部性特征，并且这种局部性直接源于自然图像数据集中的像素相关性。最终，我们利用这些见解设计了一种分析去噪器，其性能优于之前的专家设计模型。'}}}, {'id': 'https://huggingface.co/papers/2509.09658', 'title': 'Measuring Epistemic Humility in Multimodal Large Language Models', 'url': 'https://huggingface.co/papers/2509.09658', 'abstract': 'HumbleBench evaluates multimodal large language models\' ability to reject incorrect answers, addressing the issue of hallucinations in visual question answering and decision-making.  \t\t\t\t\tAI-generated summary \t\t\t\t Hallucinations in multimodal large language models (MLLMs) -- where the model generates content inconsistent with the input image -- pose significant risks in real-world applications, from misinformation in visual question answering to unsafe errors in decision-making. Existing benchmarks primarily test recognition accuracy, i.e., evaluating whether models can select the correct answer among distractors. This overlooks an equally critical capability for trustworthy AI: recognizing when none of the provided options are correct, a behavior reflecting epistemic humility. We present HumbleBench, a new hallucination benchmark designed to evaluate MLLMs\' ability to reject plausible but incorrect answers across three hallucination types: object, relation, and attribute. Built from a panoptic scene graph dataset, we leverage fine-grained scene graph annotations to extract ground-truth entities and relations, and prompt GPT-4-Turbo to generate multiple-choice questions, followed by a rigorous manual filtering process. Each question includes a "None of the above" option, requiring models not only to recognize correct visual information but also to identify when no provided answer is valid. We evaluate a variety of state-of-the-art MLLMs -- including both general-purpose and specialized reasoning models -- on HumbleBench and share valuable findings and insights with the community. By incorporating explicit false-option rejection, HumbleBench fills a key gap in current evaluation suites, providing a more realistic measure of MLLM reliability in safety-critical settings. Our code and dataset are released publicly and can be accessed at https://github.com/maifoundations/HumbleBench.', 'score': 5, 'issue_id': 5907, 'pub_date': '2025-09-11', 'pub_date_card': {'ru': '11 сентября', 'en': 'September 11', 'zh': '9月11日'}, 'hash': 'f1250c153f3e9659', 'authors': ['Bingkui Tong', 'Jiaer Xia', 'Sifeng Shang', 'Kaiyang Zhou'], 'affiliations': ['Hong Kong Baptist University, Hong Kong', 'Mohamed bin Zayed University of Artificial Intelligence, United Arab Emirates'], 'pdf_title_img': 'assets/pdf/title_img/2509.09658.jpg', 'data': {'categories': ['#hallucinations', '#multimodal', '#dataset', '#open_source', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Проверка эпистемической скромности MLLM', 'desc': "HumbleBench - это новый бенчмарк для оценки способности мультимодальных больших языковых моделей (MLLM) отвергать неправильные ответы. Он фокусируется на трех типах галлюцинаций: объектных, реляционных и атрибутивных. Бенчмарк использует аннотации графов сцен для создания вопросов с множественным выбором, включая опцию 'Ничего из вышеперечисленного'. HumbleBench оценивает не только способность моделей распознавать правильную визуальную информацию, но и идентифицировать ситуации, когда ни один из предложенных ответов не является верным."}, 'en': {'title': "HumbleBench: Evaluating AI's Ability to Say 'None of the Above'", 'desc': 'HumbleBench is a new benchmark designed to assess the ability of multimodal large language models (MLLMs) to reject incorrect answers, addressing the problem of hallucinations in visual question answering. Hallucinations occur when models generate responses that do not align with the input image, which can lead to misinformation and unsafe decisions. Unlike existing benchmarks that focus solely on recognizing correct answers, HumbleBench evaluates models on their capacity to identify when none of the provided options are correct, promoting epistemic humility. The benchmark includes a variety of question types and is built from a detailed scene graph dataset, allowing for a comprehensive evaluation of MLLMs in real-world applications.'}, 'zh': {'title': 'HumbleBench：提升多模态模型的可靠性', 'desc': 'HumbleBench 是一个新的基准测试，旨在评估多模态大型语言模型（MLLMs）拒绝错误答案的能力。该研究关注模型在视觉问答和决策过程中产生的幻觉问题，即生成与输入图像不一致的内容。HumbleBench 通过三种幻觉类型（对象、关系和属性）来测试模型的能力，确保模型不仅能识别正确的信息，还能判断提供的选项中没有有效答案。该基准测试为当前评估工具填补了关键空白，提供了更真实的 MLLM 可靠性测量，尤其在安全关键的应用场景中。'}}}, {'id': 'https://huggingface.co/papers/2509.11986', 'title': 'Lost in Embeddings: Information Loss in Vision-Language Models', 'url': 'https://huggingface.co/papers/2509.11986', 'abstract': "Two approaches are introduced to analyze and quantify information loss in vision-language models during the projection of visual inputs into the language model's embedding space, revealing significant distortions and their impact on model performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision--language models (VLMs) often process visual inputs through a pretrained vision encoder, followed by a projection into the language model's embedding space via a connector component. While crucial for modality fusion, the potential information loss induced by this projection step and its direct impact on model capabilities remain understudied. We introduce two complementary approaches to examine and quantify this loss by analyzing the latent representation space. First, we evaluate semantic information preservation by analyzing changes in k-nearest neighbor relationships between image representations, before and after projection. Second, we directly measure information loss by reconstructing visual embeddings from the projected representation, localizing loss at an image patch level. Experiments reveal that connectors substantially distort the local geometry of visual representations, with k-nearest neighbors diverging by 40--60\\% post-projection, correlating with degradation in retrieval performance. The patch-level embedding reconstruction provides interpretable insights for model behavior on visually grounded question-answering tasks, finding that areas of high information loss reliably predict instances where models struggle.", 'score': 4, 'issue_id': 5907, 'pub_date': '2025-09-15', 'pub_date_card': {'ru': '15 сентября', 'en': 'September 15', 'zh': '9月15日'}, 'hash': '51ff80bb55b3f8d8', 'authors': ['Wenyan Li', 'Raphael Tang', 'Chengzu Li', 'Caiqi Zhang', 'Ivan Vulić', 'Anders Søgaard'], 'affiliations': ['Microsoft', 'University of Cambridge', 'University of Copenhagen'], 'pdf_title_img': 'assets/pdf/title_img/2509.11986.jpg', 'data': {'categories': ['#multimodal', '#interpretability', '#cv', '#games'], 'emoji': '🔍', 'ru': {'title': 'Раскрытие информационных потерь в мультимодальных моделях', 'desc': 'В статье представлены два подхода к анализу и количественной оценке потери информации в мультимодальных моделях, объединяющих зрение и язык. Исследователи изучают искажения, возникающие при проекции визуальных входных данных в пространство эмбеддингов языковой модели. Первый метод оценивает сохранение семантической информации путем анализа изменений в отношениях k-ближайших соседей между представлениями изображений до и после проекции. Второй подход измеряет потерю информации путем реконструкции визуальных эмбеддингов из спроецированного представления, локализуя потери на уровне патчей изображения.'}, 'en': {'title': 'Quantifying Information Loss in Vision-Language Models', 'desc': "This paper investigates how vision-language models (VLMs) lose information when converting visual inputs into a language model's embedding space. It introduces two methods to analyze this information loss: one examines how well the semantic relationships between images are preserved after projection, and the other reconstructs visual embeddings to identify loss at a detailed level. The findings show that the projection process significantly distorts the spatial relationships of visual data, leading to a 40-60% divergence in k-nearest neighbor relationships, which negatively affects model performance. Additionally, the study highlights that areas with high information loss can predict where models may struggle in tasks like visually grounded question-answering."}, 'zh': {'title': '揭示视觉-语言模型中的信息损失', 'desc': '本文介绍了两种方法来分析和量化视觉-语言模型在将视觉输入投影到语言模型嵌入空间时的信息损失。这种投影步骤可能导致显著的失真，并直接影响模型的性能。我们通过分析潜在表示空间来评估语义信息的保留情况，并测量信息损失。实验结果表明，连接器显著扭曲了视觉表示的局部几何结构，导致k近邻关系在投影后发生40-60%的偏离，且与检索性能的下降相关联。'}}}, {'id': 'https://huggingface.co/papers/2509.10884', 'title': 'Nav-R1: Reasoning and Navigation in Embodied Scenes', 'url': 'https://huggingface.co/papers/2509.10884', 'abstract': 'Nav-R1, an embodied foundation model, enhances navigation by integrating structured reasoning and decoupled control mechanisms, outperforming existing approaches on benchmarks and in real-world scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Embodied navigation requires agents to integrate perception, reasoning, and action for robust interaction in complex 3D environments. Existing approaches often suffer from incoherent and unstable reasoning traces that hinder generalization across diverse environments, and difficulty balancing long-horizon semantic reasoning with low-latency control for real-time navigation. To address these challenges, we propose Nav-R1, an embodied foundation model that unifies reasoning in embodied environments. We first construct Nav-CoT-110K, a large-scale dataset of step-by-step Chains-of-Thought (CoT) for embodied tasks, which enables cold-start initialization with structured reasoning. Building on this foundation, we design a GRPO-based reinforcement learning framework with three complementary rewards: format, understanding, and navigation, to improve structural adherence, semantic grounding, and path fidelity. Furthermore, we introduce a Fast-in-Slow reasoning paradigm, decoupling deliberate semantic reasoning from low-latency reactive control for efficient yet coherent navigation. Extensive evaluations on embodied AI benchmarks demonstrate that Nav-R1 consistently outperforms strong baselines, with over 8% average improvement in reasoning and navigation performance. Real-world deployment on a mobile robot further validates its robustness under limited onboard resources. Code: https://github.com/AIGeeksGroup/Nav-R1. Website: https://aigeeksgroup.github.io/Nav-R1.', 'score': 4, 'issue_id': 5907, 'pub_date': '2025-09-13', 'pub_date_card': {'ru': '13 сентября', 'en': 'September 13', 'zh': '9月13日'}, 'hash': 'c1cc764cd16892f2', 'authors': ['Qingxiang Liu', 'Ting Huang', 'Zeyu Zhang', 'Hao Tang'], 'affiliations': ['Peking University', 'Shanghai University of Engineering Science'], 'pdf_title_img': 'assets/pdf/title_img/2509.10884.jpg', 'data': {'categories': ['#3d', '#rl', '#agents', '#reasoning', '#dataset', '#optimization', '#training'], 'emoji': '🧭', 'ru': {'title': 'Nav-R1: ИИ-навигатор нового поколения с продвинутым рассуждением', 'desc': 'Nav-R1 - это модель искусственного интеллекта для навигации в трехмерном пространстве, объединяющая восприятие, рассуждение и действие. Модель использует структурированные цепочки рассуждений и разделяет семантическое планирование и низкоуровневое управление. Nav-R1 превосходит существующие подходы на стандартных тестах и в реальных сценариях. Модель обучается с помощью специального набора данных и системы вознаграждений для улучшения рассуждений и навигации.'}, 'en': {'title': 'Revolutionizing Navigation with Structured Reasoning and Decoupled Control', 'desc': 'Nav-R1 is an advanced embodied foundation model designed to improve navigation in complex 3D environments by combining structured reasoning with decoupled control mechanisms. It addresses common issues in existing navigation systems, such as unstable reasoning and the challenge of balancing long-term planning with quick responses. The model utilizes a large dataset called Nav-CoT-110K, which provides step-by-step reasoning examples for better initialization and learning. By implementing a unique Fast-in-Slow reasoning approach and a GRPO-based reinforcement learning framework, Nav-R1 achieves significant performance improvements in both simulated benchmarks and real-world applications.'}, 'zh': {'title': 'Nav-R1：智能导航的新纪元', 'desc': 'Nav-R1是一种具身基础模型，通过整合结构化推理和解耦控制机制来增强导航能力。它解决了现有方法在复杂3D环境中推理不连贯和不稳定的问题，提升了在多样化环境中的泛化能力。该模型使用了Nav-CoT-110K数据集，支持基于步骤的推理，并采用了基于GRPO的强化学习框架，结合了格式、理解和导航三种奖励机制。通过快速与慢速推理的解耦，Nav-R1实现了高效且连贯的导航，在基准测试和实际应用中均表现优异。'}}}, {'id': 'https://huggingface.co/papers/2509.11444', 'title': 'CognitiveSky: Scalable Sentiment and Narrative Analysis for\n  Decentralized Social Media', 'url': 'https://huggingface.co/papers/2509.11444', 'abstract': "CognitiveSky, a transformer-based framework, analyzes sentiment, emotion, and narratives on Bluesky, providing insights through a dynamic dashboard and supporting various applications in computational social science.  \t\t\t\t\tAI-generated summary \t\t\t\t The emergence of decentralized social media platforms presents new opportunities and challenges for real-time analysis of public discourse. This study introduces CognitiveSky, an open-source and scalable framework designed for sentiment, emotion, and narrative analysis on Bluesky, a federated Twitter or X.com alternative. By ingesting data through Bluesky's Application Programming Interface (API), CognitiveSky applies transformer-based models to annotate large-scale user-generated content and produces structured and analyzable outputs. These summaries drive a dynamic dashboard that visualizes evolving patterns in emotion, activity, and conversation topics. Built entirely on free-tier infrastructure, CognitiveSky achieves both low operational cost and high accessibility. While demonstrated here for monitoring mental health discourse, its modular design enables applications across domains such as disinformation detection, crisis response, and civic sentiment analysis. By bridging large language models with decentralized networks, CognitiveSky offers a transparent, extensible tool for computational social science in an era of shifting digital ecosystems.", 'score': 3, 'issue_id': 5906, 'pub_date': '2025-09-14', 'pub_date_card': {'ru': '14 сентября', 'en': 'September 14', 'zh': '9月14日'}, 'hash': 'a5f0549d84adb9a5', 'authors': ['Gaurab Chhetri', 'Anandi Dutta', 'Subasish Das'], 'affiliations': ['Department of Computer Science Texas State University San Marcos, Texas, USA', 'Ingram School of Engineering Texas State University San Marcos, Texas, USA'], 'pdf_title_img': 'assets/pdf/title_img/2509.11444.jpg', 'data': {'categories': ['#data', '#healthcare', '#open_source', '#dataset', '#multimodal', '#science'], 'emoji': '🧠', 'ru': {'title': 'Анализ настроений в децентрализованных соцсетях с помощью ИИ', 'desc': 'CognitiveSky - это фреймворк на основе трансформеров для анализа настроений, эмоций и нарративов в социальной сети Bluesky. Система использует API Bluesky для сбора данных и применяет модели глубокого обучения для аннотации контента пользователей. CognitiveSky предоставляет интерактивную панель мониторинга для визуализации паттернов эмоций, активности и тем обсуждений. Фреймворк имеет модульную архитектуру и может применяться в различных областях вычислительной социологии.'}, 'en': {'title': 'CognitiveSky: Transforming Social Media Insights with AI', 'desc': "CognitiveSky is a transformer-based framework that analyzes sentiment, emotion, and narratives on the decentralized social media platform Bluesky. It utilizes Bluesky's API to gather user-generated content and applies advanced machine learning models to extract meaningful insights. The framework features a dynamic dashboard that visualizes trends in public discourse, making it useful for various applications in computational social science. Its open-source nature and low operational costs enhance accessibility for researchers and practitioners in fields like mental health monitoring and disinformation detection."}, 'zh': {'title': 'CognitiveSky：解读Bluesky的情感与叙事', 'desc': 'CognitiveSky是一个基于变换器的框架，专注于分析Bluesky平台上的情感、情绪和叙事。它通过Bluesky的API获取数据，利用变换器模型对用户生成的内容进行标注，并生成结构化的可分析输出。该框架提供一个动态仪表板，实时可视化情感、活动和话题的变化模式。CognitiveSky的模块化设计使其能够广泛应用于虚假信息检测、危机响应和公民情感分析等领域。'}}}, {'id': 'https://huggingface.co/papers/2509.12132', 'title': 'Look Again, Think Slowly: Enhancing Visual Reflection in Vision-Language\n  Models', 'url': 'https://huggingface.co/papers/2509.12132', 'abstract': 'Reflection-V enhances visual reasoning by constructing vision-centered data and using a visual attention reward model, improving reliance on visual information.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in text-only "slow-thinking" reasoning have prompted efforts to transfer this capability to vision-language models (VLMs), for training visual reasoning models (VRMs). owever, such transfer faces critical challenges: Effective "slow thinking" in VRMs requires visual reflection, the ability to check the reasoning process based on visual information. Through quantitative analysis, we observe that current VRMs exhibit limited visual reflection, as their attention to visual information diminishes rapidly with longer generated responses. To address this challenge, we propose a new VRM Reflection-V, which enhances visual reflection based on reasoning data construction for cold-start and reward design for reinforcement learning (RL). Firstly, we construct vision-centered reasoning data by leveraging an agent that interacts between VLMs and reasoning LLMs, enabling cold-start learning of visual reflection patterns. Secondly, a visual attention based reward model is employed during RL to encourage reasoning based on visual information. Therefore, Reflection-V demonstrates significant improvements across multiple visual reasoning benchmarks. Furthermore, Reflection-V maintains a stronger and more consistent reliance on visual information during visual reasoning, indicating effective enhancement in visual reflection capabilities.', 'score': 2, 'issue_id': 5913, 'pub_date': '2025-09-15', 'pub_date_card': {'ru': '15 сентября', 'en': 'September 15', 'zh': '9月15日'}, 'hash': 'e932da8242289ed6', 'authors': ['Pu Jian', 'Junhong Wu', 'Wei Sun', 'Chen Wang', 'Shuo Ren', 'Jiajun Zhang'], 'affiliations': ['Institute of Automation, Chinese Academy of Sciences', 'School of Artificial Intelligence, University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2509.12132.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#cv', '#agents', '#dataset', '#games', '#rl'], 'emoji': '🧠', 'ru': {'title': 'Усиление визуального мышления ИИ через отражение и внимание', 'desc': 'Статья представляет модель Reflection-V, которая улучшает визуальное мышление в задачах компьютерного зрения. Модель использует специально сконструированные данные, ориентированные на визуальную информацию, для начального обучения. Затем применяется обучение с подкреплением с использованием модели вознаграждения, основанной на визуальном внимании. Reflection-V демонстрирует значительные улучшения в различных задачах визуального мышления и сохраняет более сильную опору на визуальную информацию в процессе рассуждений.'}, 'en': {'title': 'Enhancing Visual Reasoning with Reflection-V', 'desc': "Reflection-V is a new model designed to improve visual reasoning in AI by focusing on how visual information is used during reasoning processes. It addresses the challenge of 'slow thinking' in visual reasoning models (VRMs) by enhancing their ability to reflect on visual data. The model constructs vision-centered reasoning data and employs a visual attention reward system to reinforce the use of visual information. As a result, Reflection-V shows significant improvements in performance on various visual reasoning tasks, demonstrating a stronger reliance on visual cues throughout the reasoning process."}, 'zh': {'title': '增强视觉推理的Reflection-V模型', 'desc': 'Reflection-V 是一种增强视觉推理的模型，通过构建以视觉为中心的数据和使用视觉注意力奖励模型，提升了对视觉信息的依赖性。该模型解决了当前视觉推理模型在长生成响应时对视觉信息关注度迅速下降的问题。通过量化分析，发现现有模型的视觉反思能力有限，因此我们提出了 Reflection-V，以改进视觉反思能力。该模型在多个视觉推理基准测试中表现出显著的提升，表明其在视觉推理过程中对视觉信息的依赖更加稳定和一致。'}}}, {'id': 'https://huggingface.co/papers/2509.11866', 'title': 'Dr.V: A Hierarchical Perception-Temporal-Cognition Framework to Diagnose\n  Video Hallucination by Fine-grained Spatial-Temporal Grounding', 'url': 'https://huggingface.co/papers/2509.11866', 'abstract': 'Dr.V, a hierarchical framework with Dr.V-Bench and Dr.V-Agent, addresses video hallucinations through fine-grained spatial-temporal grounding and cognitive reasoning, enhancing video understanding.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in large video models (LVMs) have significantly enhance video understanding. However, these models continue to suffer from hallucinations, producing content that conflicts with input videos. To address this issue, we propose Dr.V, a hierarchical framework covering perceptive, temporal, and cognitive levels to diagnose video hallucination by fine-grained spatial-temporal grounding. Dr.V comprises of two key components: a benchmark dataset Dr.V-Bench and a satellite video agent Dr.V-Agent. Dr.V-Bench includes 10k instances drawn from 4,974 videos spanning diverse tasks, each enriched with detailed spatial-temporal annotation. Dr.V-Agent detects hallucinations in LVMs by systematically applying fine-grained spatial-temporal grounding at the perceptive and temporal levels, followed by cognitive level reasoning. This step-by-step pipeline mirrors human-like video comprehension and effectively identifies hallucinations. Extensive experiments demonstrate that Dr.V-Agent is effective in diagnosing hallucination while enhancing interpretability and reliability, offering a practical blueprint for robust video understanding in real-world scenarios. All our data and code are available at https://github.com/Eurekaleo/Dr.V.', 'score': 1, 'issue_id': 5916, 'pub_date': '2025-09-15', 'pub_date_card': {'ru': '15 сентября', 'en': 'September 15', 'zh': '9月15日'}, 'hash': 'cc3f1e4f666199c3', 'authors': ['Meng Luo', 'Shengqiong Wu', 'Liqiang Jing', 'Tianjie Ju', 'Li Zheng', 'Jinxiang Lai', 'Tianlong Wu', 'Xinya Du', 'Jian Li', 'Siyuan Yan', 'Jiebo Luo', 'William Yang Wang', 'Hao Fei', 'Mong-Li Lee', 'Wynne Hsu'], 'affiliations': ['HKUST', 'Monash', 'NJU', 'NUS', 'UCSB', 'UR', 'UTD', 'WHU'], 'pdf_title_img': 'assets/pdf/title_img/2509.11866.jpg', 'data': {'categories': ['#video', '#hallucinations', '#interpretability', '#dataset', '#reasoning', '#benchmark'], 'emoji': '🎥', 'ru': {'title': 'Dr.V: Лечим галлюцинации видеомоделей', 'desc': 'Dr.V - это иерархическая система для выявления галлюцинаций в крупных видеомоделях (LVM). Она включает в себя набор данных Dr.V-Bench с 10 тысячами аннотированных примеров и агента Dr.V-Agent для детального пространственно-временного анализа видео. Dr.V-Agent применяет многоуровневый подход, имитирующий человеческое понимание видео. Эксперименты показывают эффективность Dr.V в диагностике галлюцинаций и повышении интерпретируемости видеомоделей.'}, 'en': {'title': 'Dr.V: Enhancing Video Understanding by Tackling Hallucinations', 'desc': 'The paper introduces Dr.V, a hierarchical framework designed to tackle video hallucinations in large video models (LVMs). It features two main components: Dr.V-Bench, a benchmark dataset with 10,000 instances from nearly 5,000 videos, and Dr.V-Agent, which employs fine-grained spatial-temporal grounding and cognitive reasoning to identify hallucinations. By mimicking human-like video comprehension, Dr.V-Agent enhances the interpretability and reliability of video understanding. The framework aims to provide a practical solution for improving the robustness of video analysis in real-world applications.'}, 'zh': {'title': 'Dr.V：提升视频理解的分层框架', 'desc': 'Dr.V是一个分层框架，旨在通过细粒度的时空定位和认知推理来解决视频幻觉问题，从而增强视频理解能力。该框架包括两个主要组件：Dr.V-Bench和Dr.V-Agent。Dr.V-Bench是一个基准数据集，包含来自4974个视频的1万实例，提供详细的时空注释。Dr.V-Agent通过在感知和时间层面上系统地应用细粒度时空定位，结合认知层推理，有效地识别视频中的幻觉。'}}}, {'id': 'https://huggingface.co/papers/2509.11648', 'title': 'EthicsMH: A Pilot Benchmark for Ethical Reasoning in Mental Health AI', 'url': 'https://huggingface.co/papers/2509.11648', 'abstract': "EthicsMH is a dataset of 125 scenarios designed to evaluate AI systems' ethical reasoning in mental health contexts, focusing on decision accuracy, explanation quality, and alignment with professional norms.  \t\t\t\t\tAI-generated summary \t\t\t\t The deployment of large language models (LLMs) in mental health and other sensitive domains raises urgent questions about ethical reasoning, fairness, and responsible alignment. Yet, existing benchmarks for moral and clinical decision-making do not adequately capture the unique ethical dilemmas encountered in mental health practice, where confidentiality, autonomy, beneficence, and bias frequently intersect. To address this gap, we introduce Ethical Reasoning in Mental Health (EthicsMH), a pilot dataset of 125 scenarios designed to evaluate how AI systems navigate ethically charged situations in therapeutic and psychiatric contexts. Each scenario is enriched with structured fields, including multiple decision options, expert-aligned reasoning, expected model behavior, real-world impact, and multi-stakeholder viewpoints. This structure enables evaluation not only of decision accuracy but also of explanation quality and alignment with professional norms. Although modest in scale and developed with model-assisted generation, EthicsMH establishes a task framework that bridges AI ethics and mental health decision-making. By releasing this dataset, we aim to provide a seed resource that can be expanded through community and expert contributions, fostering the development of AI systems capable of responsibly handling some of society's most delicate decisions.", 'score': 1, 'issue_id': 5906, 'pub_date': '2025-09-15', 'pub_date_card': {'ru': '15 сентября', 'en': 'September 15', 'zh': '9月15日'}, 'hash': 'ab3984446a10aa88', 'authors': ['Sai Kartheek Reddy Kasu'], 'affiliations': ['IIIT Dharwad, India'], 'pdf_title_img': 'assets/pdf/title_img/2509.11648.jpg', 'data': {'categories': ['#benchmark', '#ethics', '#healthcare', '#alignment', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Этичный ИИ в психическом здоровье: новый стандарт оценки', 'desc': 'Этот документ представляет собой датасет EthicsMH, содержащий 125 сценариев для оценки этического рассуждения систем искусственного интеллекта в контексте психического здоровья. Датасет фокусируется на точности принятия решений, качестве объяснений и соответствии профессиональным нормам. EthicsMH предназначен для оценки способности ИИ-систем ориентироваться в этически сложных ситуациях в терапевтических и психиатрических контекстах. Авторы стремятся создать ресурс, который может быть расширен с помощью вклада сообщества и экспертов, способствуя развитию ИИ-систем, способных ответственно принимать деликатные решения.'}, 'en': {'title': 'Evaluating AI Ethics in Mental Health with EthicsMH', 'desc': 'EthicsMH is a dataset consisting of 125 scenarios aimed at assessing the ethical reasoning capabilities of AI systems in mental health settings. It focuses on key aspects such as decision accuracy, explanation quality, and adherence to professional ethical standards. The dataset includes structured elements like decision options and expert reasoning to facilitate comprehensive evaluation. By addressing the unique ethical challenges in mental health, EthicsMH serves as a foundational resource for developing AI systems that can make responsible decisions in sensitive contexts.'}, 'zh': {'title': '推动AI在心理健康领域的伦理决策能力', 'desc': 'EthicsMH是一个包含125个场景的数据集，旨在评估人工智能系统在心理健康领域的伦理推理能力。该数据集关注决策准确性、解释质量和与专业规范的一致性，特别是在涉及保密性、自主性和偏见等伦理困境时。每个场景都包含多个决策选项、专家对齐的推理、预期模型行为和多方利益相关者的观点。这一数据集为AI伦理与心理健康决策提供了一个框架，旨在促进AI系统在处理社会敏感决策时的责任感。'}}}, {'id': 'https://huggingface.co/papers/2509.11452', 'title': 'Learning to Optimize Multi-Objective Alignment Through Dynamic Reward\n  Weighting', 'url': 'https://huggingface.co/papers/2509.11452', 'abstract': 'Dynamic reward weighting in multi-objective reinforcement learning adaptively adjusts weights during training to explore Pareto fronts effectively, outperforming fixed-weight scalarization methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Prior works in multi-objective reinforcement learning typically use linear reward scalarization with fixed weights, which provably fail to capture non-convex Pareto fronts and thus yield suboptimal results. This limitation becomes especially critical in online preference alignment for large language models. Here, stochastic trajectories generated by parameterized policies create highly non-linear and non-convex mappings from parameters to objectives that no single static weighting scheme can find optimal trade-offs. We address this limitation by introducing dynamic reward weighting, which adaptively adjusts reward weights during the online reinforcement learning process. Unlike existing approaches that rely on fixed-weight interpolation, our dynamic weighting continuously balances and prioritizes objectives in training, facilitating effective exploration of Pareto fronts in objective space. We introduce two approaches of increasing sophistication and generalizability: (1) hypervolume-guided weight adaptation and (2) gradient-based weight optimization, offering a versatile toolkit for online multi-objective alignment. Our extensive experiments demonstrate their compatibility with commonly used online reinforcement learning algorithms (including GRPO, REINFORCE, and RLOO), effectiveness across multiple mathematical reasoning datasets, and applicability to different model families, consistently achieving Pareto dominant solutions with fewer training steps than fixed-weight linear scalarization baselines.', 'score': 1, 'issue_id': 5906, 'pub_date': '2025-09-14', 'pub_date_card': {'ru': '14 сентября', 'en': 'September 14', 'zh': '9月14日'}, 'hash': '5e0e1351e67bf729', 'authors': ['Yining Lu', 'Zilong Wang', 'Shiyang Li', 'Xin Liu', 'Changlong Yu', 'Qingyu Yin', 'Zhan Shi', 'Zixuan Zhang', 'Meng Jiang'], 'affiliations': ['Amazon', 'University of Notre Dame'], 'pdf_title_img': 'assets/pdf/title_img/2509.11452.jpg', 'data': {'categories': ['#rl', '#rlhf', '#reasoning', '#training', '#optimization', '#alignment'], 'emoji': '⚖️', 'ru': {'title': 'Динамическое взвешивание для оптимального баланса целей в обучении с подкреплением', 'desc': 'Статья представляет новый подход к многоцелевому обучению с подкреплением - динамическое взвешивание наград. В отличие от традиционных методов с фиксированными весами, этот метод адаптивно корректирует веса во время обучения. Это позволяет эффективно исследовать фронт Парето в пространстве целей. Авторы предлагают два варианта реализации: адаптация весов на основе гиперобъема и оптимизация весов на основе градиентов. Эксперименты показывают, что метод превосходит базовые подходы с фиксированными весами на различных задачах и моделях.'}, 'en': {'title': 'Dynamic Reward Weighting: Optimizing Multi-Objective Learning', 'desc': 'This paper presents a method called dynamic reward weighting for multi-objective reinforcement learning, which adjusts the importance of different objectives during training. Traditional methods use fixed weights, which can lead to poor performance when dealing with complex, non-linear relationships between objectives. The proposed approach allows for better exploration of the Pareto front, leading to more optimal solutions. The authors demonstrate that their method outperforms existing techniques across various datasets and reinforcement learning algorithms, achieving better results with fewer training steps.'}, 'zh': {'title': '动态奖励加权：优化多目标强化学习的利器', 'desc': '在多目标强化学习中，动态奖励加权通过在训练过程中自适应调整权重，有效探索帕累托前沿，优于固定权重的标量化方法。以往的研究通常使用固定权重的线性奖励标量化，这无法捕捉非凸的帕累托前沿，导致次优结果。我们提出的动态奖励加权方法，能够在在线强化学习过程中持续平衡和优先考虑目标，从而更好地探索目标空间中的帕累托前沿。实验结果表明，该方法与常用的在线强化学习算法兼容，并在多个数学推理数据集上表现出色，能够以更少的训练步骤实现帕累托主导解。'}}}, {'id': 'https://huggingface.co/papers/2509.11362', 'title': 'PersonaX: Multimodal Datasets with LLM-Inferred Behavior Traits', 'url': 'https://huggingface.co/papers/2509.11362', 'abstract': 'PersonaX, a multimodal dataset, combines behavioral traits, facial imagery, and biographical information to enable comprehensive analysis and causal reasoning using large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Understanding human behavior traits is central to applications in human-computer interaction, computational social science, and personalized AI systems. Such understanding often requires integrating multiple modalities to capture nuanced patterns and relationships. However, existing resources rarely provide datasets that combine behavioral descriptors with complementary modalities such as facial attributes and biographical information. To address this gap, we present PersonaX, a curated collection of multimodal datasets designed to enable comprehensive analysis of public traits across modalities. PersonaX consists of (1) CelebPersona, featuring 9444 public figures from diverse occupations, and (2) AthlePersona, covering 4181 professional athletes across 7 major sports leagues. Each dataset includes behavioral trait assessments inferred by three high-performing large language models, alongside facial imagery and structured biographical features. We analyze PersonaX at two complementary levels. First, we abstract high-level trait scores from text descriptions and apply five statistical independence tests to examine their relationships with other modalities. Second, we introduce a novel causal representation learning (CRL) framework tailored to multimodal and multi-measurement data, providing theoretical identifiability guarantees. Experiments on both synthetic and real-world data demonstrate the effectiveness of our approach. By unifying structured and unstructured analysis, PersonaX establishes a foundation for studying LLM-inferred behavioral traits in conjunction with visual and biographical attributes, advancing multimodal trait analysis and causal reasoning.', 'score': 1, 'issue_id': 5907, 'pub_date': '2025-09-14', 'pub_date_card': {'ru': '14 сентября', 'en': 'September 14', 'zh': '9月14日'}, 'hash': '90082309548dd76f', 'authors': ['Loka Li', 'Wong Yu Kang', 'Minghao Fu', 'Guangyi Chen', 'Zhenhao Chen', 'Gongxu Luo', 'Yuewen Sun', 'Salman Khan', 'Peter Spirtes', 'Kun Zhang'], 'affiliations': ['Australian National University', 'Carnegie Mellon University', 'Mohamed bin Zayed University of Artificial Intelligence', 'University of California San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2509.11362.jpg', 'data': {'categories': ['#interpretability', '#reasoning', '#multimodal', '#dataset', '#synthetic'], 'emoji': '🧠', 'ru': {'title': 'PersonaX: мультимодальный анализ личности с помощью ИИ', 'desc': 'PersonaX - это мультимодальный набор данных, объединяющий поведенческие черты, изображения лиц и биографическую информацию для комплексного анализа с помощью больших языковых моделей. Он состоит из двух частей: CelebPersona с 9444 публичными фигурами и AthlePersona с 4181 профессиональным спортсменом. Данные включают оценки поведенческих черт, сделанные тремя высокопроизводительными языковыми моделями, а также изображения лиц и структурированные биографические характеристики. Авторы применяют статистические тесты и новую структуру обучения причинно-следственным представлениям для анализа взаимосвязей между модальностями.'}, 'en': {'title': 'Unlocking Human Behavior Through Multimodal Analysis with PersonaX', 'desc': 'PersonaX is a new multimodal dataset that combines behavioral traits, facial images, and biographical data to enhance the analysis of human behavior using large language models. It includes two main components: CelebPersona, which features public figures, and AthlePersona, which focuses on professional athletes, both enriched with behavioral assessments from advanced language models. The dataset allows researchers to explore relationships between different types of data through statistical tests and introduces a causal representation learning framework for better understanding these connections. By integrating various modalities, PersonaX aims to improve the study of behavioral traits and their implications in AI systems and human-computer interaction.'}, 'zh': {'title': 'PersonaX：多模态特征分析的新基础', 'desc': 'PersonaX是一个多模态数据集，结合了行为特征、面部图像和传记信息，以便使用大型语言模型进行全面分析和因果推理。该数据集包括CelebPersona和AthlePersona，涵盖了来自不同职业的公共人物和专业运动员。每个数据集都包含由高性能大型语言模型推断的行为特征评估，以及面部图像和结构化的传记特征。通过引入新的因果表示学习框架，PersonaX为多模态和多测量数据的分析奠定了基础，推动了多模态特征分析和因果推理的发展。'}}}, {'id': 'https://huggingface.co/papers/2509.10844', 'title': 'GAPrune: Gradient-Alignment Pruning for Domain-Aware Embeddings', 'url': 'https://huggingface.co/papers/2509.10844', 'abstract': 'GAPrune, a pruning framework that considers domain importance and general linguistic foundation, effectively compresses models while maintaining and enhancing domain-specific performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Domain-specific embedding models have shown promise for applications that require specialized semantic understanding, such as coding agents and financial retrieval systems, often achieving higher performance gains than general models. However, state-of-the-art embedding models are typically based on LLMs, which contain billions of parameters, making deployment challenging in resource-constrained environments. Model compression through pruning offers a promising solution, but existing pruning methods treat all parameters uniformly, failing to distinguish between general semantic representations and domain-specific patterns, leading to suboptimal pruning decisions. Thus, we propose GAPrune, a pruning framework that addresses this challenge by considering both domain importance and preserving general linguistic foundation. Our method uses Fisher Information to measure importance and general-domain gradient alignment to assess parameter behavior, then combines these signals using our Domain Alignment Importance (DAI) scoring. Lower DAI scores indicate that the parameter is either less important for the domain task or creates conflicts between domain and general objectives. Experiments on two domain benchmarks, FinMTEB and ChemTEB, show that GAPrune maintains performance within 2.5% of dense models in one-shot pruning at 50% sparsity, while outperforming all baselines. With retraining in 100 steps, GAPrune achieves +4.51% improvement on FinMTEB and +1.73% on ChemTEB, demonstrating that our pruning strategy not only preserves but enhances domain-specific capabilities. Our findings demonstrate that principled pruning strategies can achieve model compression and enhanced domain specialization, providing the research community with a new approach for development.', 'score': 1, 'issue_id': 5907, 'pub_date': '2025-09-13', 'pub_date_card': {'ru': '13 сентября', 'en': 'September 13', 'zh': '9月13日'}, 'hash': '018cc042df405787', 'authors': ['Yixuan Tang', 'Yi Yang'], 'affiliations': ['The Hong Kong University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2509.10844.jpg', 'data': {'categories': ['#training', '#inference', '#optimization'], 'emoji': '✂️', 'ru': {'title': 'Умное сжатие моделей: сохраняем важное, отсекаем лишнее', 'desc': 'GAPrune - это новый фреймворк для прунинга моделей машинного обучения, который учитывает важность доменных знаний и общелингвистической основы. Он эффективно сжимает модели, сохраняя и улучшая их производительность в специфических доменах. GAPrune использует информацию Фишера для измерения важности параметров и выравнивание градиентов в общем домене для оценки их поведения. Эксперименты показали, что GAPrune превосходит базовые методы и может даже улучшать специализированные возможности модели при сжатии.'}, 'en': {'title': 'GAPrune: Smart Pruning for Enhanced Domain Performance', 'desc': 'GAPrune is a novel pruning framework designed to compress machine learning models while enhancing their performance in specific domains. It distinguishes between general semantic representations and domain-specific patterns, allowing for more effective pruning decisions. By utilizing Fisher Information and Domain Alignment Importance (DAI) scoring, GAPrune identifies which parameters are crucial for domain tasks and which can be pruned without losing performance. Experiments show that GAPrune not only maintains high accuracy but also improves domain-specific capabilities after retraining, making it a valuable tool for deploying models in resource-limited environments.'}, 'zh': {'title': 'GAPrune：智能剪枝，提升领域性能', 'desc': 'GAPrune是一种剪枝框架，考虑了领域重要性和通用语言基础，有效地压缩模型，同时保持和增强领域特定的性能。该方法通过使用Fisher信息来衡量参数的重要性，并结合通用领域梯度对齐来评估参数行为，从而实现更优的剪枝决策。实验结果表明，GAPrune在FinMTEB和ChemTEB两个领域基准上表现出色，能够在50%稀疏度下保持与密集模型相近的性能，并在重新训练后进一步提升领域特定能力。我们的研究表明，合理的剪枝策略不仅可以实现模型压缩，还能增强领域专业化。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (3)', '#agi', '#alignment (2)', '#architecture', '#audio', '#benchmark (8)', '#cv (4)', '#data (4)', '#dataset (11)', '#diffusion (2)', '#ethics (1)', '#games (5)', '#graphs', '#hallucinations (2)', '#healthcare (2)', '#inference (1)', '#interpretability (3)', '#leakage', '#long_context', '#low_resource', '#machine_translation', '#math', '#multilingual', '#multimodal (6)', '#open_source (4)', '#optimization (6)', '#plp', '#rag', '#reasoning (6)', '#rl (4)', '#rlhf (2)', '#robotics', '#science (1)', '#security', '#small_models', '#story_generation', '#survey', '#synthetic (2)', '#training (5)', '#transfer_learning', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-09-16 16:13',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-09-16 16:13')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-09-16 16:13')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    