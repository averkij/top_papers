
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 18 papers. July 2.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">2 июля</span> | <span id="title-articles-count">18 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-07-01.html">⬅️ <span id="prev-date">01.07</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-07-03.html">➡️ <span id="next-date">03.07</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-07.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '2 июля', 'en': 'July 2', 'zh': '7月2日'};
        let feedDateNext = {'ru': '03.07', 'en': '07/03', 'zh': '7月3日'};
        let feedDatePrev = {'ru': '01.07', 'en': '07/01', 'zh': '7月1日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2507.01006', 'title': 'GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2507.01006', 'abstract': 'A vision-language model, GLM-4.1V-Thinking, enhances general-purpose multimodal reasoning through large-scale pre-training and reinforcement learning, achieving state-of-the-art performance across various tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t We present GLM-4.1V-Thinking, a vision-language model (VLM) designed to advance general-purpose multimodal reasoning. In this report, we share our key findings in the development of the reasoning-centric training framework. We first develop a capable vision foundation model with significant potential through large-scale pre-training, which arguably sets the upper bound for the final performance. Reinforcement Learning with Curriculum Sampling (RLCS) then unlocks the full potential of the model, leading to comprehensive capability enhancement across a diverse range of tasks, including STEM problem solving, video understanding, content recognition, coding, grounding, GUI-based agents, and long document understanding, among others. To facilitate research in this field, we open-source GLM-4.1V-9B-Thinking, which achieves state-of-the-art performance among models of comparable size. In a comprehensive evaluation across 28 public benchmarks, our model outperforms Qwen2.5-VL-7B on nearly all tasks and achieves comparable or even superior performance on 18 benchmarks relative to the significantly larger Qwen2.5-VL-72B. Notably, GLM-4.1V-9B-Thinking also demonstrates competitive or superior performance compared to closed-source models such as GPT-4o on challenging tasks including long document understanding and STEM reasoning, further underscoring its strong capabilities. Code, models and more information are released at https://github.com/THUDM/GLM-4.1V-Thinking.', 'score': 135, 'issue_id': 4595, 'pub_date': '2025-07-01', 'pub_date_card': {'ru': '1 июля', 'en': 'July 1', 'zh': '7月1日'}, 'hash': '174c869b64ed9ae7', 'authors': ['Wenyi Hong', 'Wenmeng Yu', 'Xiaotao Gu', 'Guo Wang', 'Guobing Gan', 'Haomiao Tang', 'Jiale Cheng', 'Ji Qi', 'Junhui Ji', 'Lihang Pan', 'Shuaiqi Duan', 'Weihan Wang', 'Yan Wang', 'Yean Cheng', 'Zehai He', 'Zhe Su', 'Zhen Yang', 'Ziyang Pan', 'Aohan Zeng', 'Baoxu Wang', 'Boyan Shi', 'Changyu Pang', 'Chenhui Zhang', 'Da Yin', 'Fan Yang', 'Guoqing Chen', 'Jiazheng Xu', 'Jiali Chen', 'Jing Chen', 'Jinhao Chen', 'Jinghao Lin', 'Jinjiang Wang', 'Junjie Chen', 'Leqi Lei', 'Leyi Pan', 'Mingzhi Zhang', 'Qinkai Zheng', 'Sheng Yang', 'Shi Zhong', 'Shiyu Huang', 'Shuyuan Zhao', 'Siyan Xue', 'Shangqin Tu', 'Shengbiao Meng', 'Tianshu Zhang', 'Tianwei Luo', 'Tianxiang Hao', 'Tianle Gong', 'Wenkai Li', 'Wei Jia', 'Xin Lyu', 'Xuancheng Huang', 'Yanling Wang', 'Yadong Xue', 'Yanfeng Wang', 'Yifan An', 'Yifan Du', 'Yiming Shi', 'Yiheng Huang', 'Yilin Niu', 'Yuan Wang', 'Yuanchang Yue', 'Yuchen Li', 'Yutao Zhang', 'Yuxuan Zhang', 'Zhanxiao Du', 'Zhenyu Hou', 'Zhao Xue', 'Zhengxiao Du', 'Zihan Wang', 'Peng Zhang', 'Debing Liu', 'Bin Xu', 'Juanzi Li', 'Minlie Huang', 'Yuxiao Dong', 'Jie Tang'], 'affiliations': ['Tsinghua University', 'Zhipu AI'], 'pdf_title_img': 'assets/pdf/title_img/2507.01006.jpg', 'data': {'categories': ['#open_source', '#rl', '#architecture', '#reasoning', '#multimodal', '#benchmark', '#training'], 'emoji': '🧠', 'ru': {'title': 'Мультимодальное рассуждение на новом уровне', 'desc': 'Представлена модель GLM-4.1V-Thinking - мультимодальная модель для обработки текста и изображений. Она использует масштабное предобучение и обучение с подкреплением для улучшения рассуждений на различных задачах. Модель достигает передовых результатов на 28 публичных бенчмарках, превосходя более крупные модели. GLM-4.1V-9B-Thinking демонстрирует конкурентоспособные результаты даже по сравнению с закрытыми моделями вроде GPT-4 на сложных задачах.'}, 'en': {'title': 'Unlocking Multimodal Reasoning with GLM-4.1V-Thinking', 'desc': 'GLM-4.1V-Thinking is a vision-language model that enhances multimodal reasoning through extensive pre-training and reinforcement learning. The model is built on a strong vision foundation, which is crucial for achieving high performance across various tasks. By employing Reinforcement Learning with Curriculum Sampling, it maximizes its capabilities in areas like STEM problem solving and video understanding. The model has been open-sourced and shows superior performance compared to other models of similar size, making it a significant contribution to the field of AI.'}, 'zh': {'title': 'GLM-4.1V-Thinking：多模态推理的新高度', 'desc': 'GLM-4.1V-Thinking 是一种视觉-语言模型，旨在提升通用多模态推理能力。通过大规模预训练和强化学习，该模型在多个任务上实现了最先进的性能。我们开发了一个强大的视觉基础模型，并通过课程采样的强化学习方法进一步提升了模型的能力。该模型在28个公共基准测试中表现优异，超越了许多同类模型，展示了其在长文档理解和STEM推理等复杂任务中的竞争力。'}}}, {'id': 'https://huggingface.co/papers/2507.01001', 'title': 'SciArena: An Open Evaluation Platform for Foundation Models in\n  Scientific Literature Tasks', 'url': 'https://huggingface.co/papers/2507.01001', 'abstract': "SciArena is a community-driven platform for evaluating foundation models on scientific literature tasks, using collective voter judgments to rank models and address the need for reliable automated evaluation.  \t\t\t\t\tAI-generated summary \t\t\t\t We present SciArena, an open and collaborative platform for evaluating foundation models on scientific literature tasks. Unlike traditional benchmarks for scientific literature understanding and synthesis, SciArena engages the research community directly, following the Chatbot Arena evaluation approach of community voting on model comparisons. By leveraging collective intelligence, SciArena offers a community-driven evaluation of model performance on open-ended scientific tasks that demand literature-grounded, long-form responses. The platform currently supports 23 open-source and proprietary foundation models and has collected over 13,000 votes from trusted researchers across diverse scientific domains. We analyze the data collected so far and confirm that the submitted questions are diverse, aligned with real-world literature needs, and that participating researchers demonstrate strong self-consistency and inter-annotator agreement in their evaluations. We discuss the results and insights based on the model ranking leaderboard. To further promote research in building model-based automated evaluation systems for literature tasks, we release SciArena-Eval, a meta-evaluation benchmark based on our collected preference data. The benchmark measures the accuracy of models in judging answer quality by comparing their pairwise assessments with human votes. Our experiments highlight the benchmark's challenges and emphasize the need for more reliable automated evaluation methods.", 'score': 32, 'issue_id': 4593, 'pub_date': '2025-07-01', 'pub_date_card': {'ru': '1 июля', 'en': 'July 1', 'zh': '7月1日'}, 'hash': 'f3c20682e2dcf410', 'authors': ['Yilun Zhao', 'Kaiyan Zhang', 'Tiansheng Hu', 'Sihong Wu', 'Ronan Le Bras', 'Taira Anderson', 'Jonathan Bragg', 'Joseph Chee Chang', 'Jesse Dodge', 'Matt Latzke', 'Yixin Liu', 'Charles McGrady', 'Xiangru Tang', 'Zihang Wang', 'Chen Zhao', 'Hannaneh Hajishirzi', 'Doug Downey', 'Arman Cohan'], 'affiliations': ['Allen Institute for AI', 'New York University', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2507.01001.jpg', 'data': {'categories': ['#open_source', '#science', '#benchmark', '#survey', '#dataset'], 'emoji': '🧪', 'ru': {'title': 'Коллективный разум в оценке ИИ для научной литературы', 'desc': 'SciArena - это платформа для оценки фундаментальных моделей в задачах работы с научной литературой, использующая коллективные суждения оценщиков для ранжирования моделей. Платформа поддерживает 23 модели с открытым исходным кодом и проприетарные модели, собрав более 13 000 голосов от доверенных исследователей из различных научных областей. Анализ собранных данных подтверждает разнообразие вопросов, их соответствие реальным потребностям литературы и высокую согласованность оценок участвующих исследователей. На основе собранных данных о предпочтениях авторы также выпустили бенчмарк SciArena-Eval для оценки точности моделей в определении качества ответов.'}, 'en': {'title': 'Empowering Scientific Model Evaluation through Community Collaboration', 'desc': 'SciArena is a collaborative platform designed to evaluate foundation models specifically for tasks related to scientific literature. It utilizes community voting to rank models, moving away from traditional benchmarks and fostering direct engagement from researchers. The platform supports a variety of models and has gathered extensive voting data, demonstrating strong agreement among participants in their evaluations. Additionally, SciArena introduces a meta-evaluation benchmark, SciArena-Eval, to enhance automated evaluation systems by comparing model assessments with human judgments.'}, 'zh': {'title': 'SciArena：科学文献任务的社区评估平台', 'desc': 'SciArena是一个社区驱动的平台，用于评估基础模型在科学文献任务上的表现。与传统的科学文献理解基准不同，SciArena通过社区投票的方式直接参与研究者，利用集体智慧对模型性能进行评估。该平台支持23个开源和专有的基础模型，并收集了来自不同科学领域的研究者的超过13,000个投票。我们还推出了SciArena-Eval，一个基于收集的偏好数据的元评估基准，旨在促进文献任务的自动评估系统的研究。'}}}, {'id': 'https://huggingface.co/papers/2506.23115', 'title': 'MoCa: Modality-aware Continual Pre-training Makes Better Bidirectional\n  Multimodal Embeddings', 'url': 'https://huggingface.co/papers/2506.23115', 'abstract': 'MoCa, a two-stage framework, enhances pre-trained causal vision-language models for multimodal embedding by introducing bidirectional attention, scaling with unlabeled data, and diverse training objectives.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal embedding models, built upon causal Vision Language Models (VLMs), have shown promise in various tasks. However, current approaches face three key limitations: the use of causal attention in VLM backbones is suboptimal for embedding tasks; scalability issues due to reliance on high-quality labeled paired data for contrastive learning; and limited diversity in training objectives and data. To address these issues, we propose MoCa, a two-stage framework for transforming pre-trained VLMs into effective bidirectional multimodal embedding models. The first stage, Modality-aware Continual Pre-training, introduces a joint reconstruction objective that simultaneously denoises interleaved text and image inputs, enhancing bidirectional context-aware reasoning. The second stage, Heterogeneous Contrastive Fine-tuning, leverages diverse, semantically rich multimodal data beyond simple image-caption pairs to enhance generalization and alignment. Our method addresses the stated limitations by introducing bidirectional attention through continual pre-training, scaling effectively with massive unlabeled datasets via joint reconstruction objectives, and utilizing diverse multimodal data for enhanced representation robustness. Experiments demonstrate that MoCa consistently improves performance across MMEB and ViDoRe-v2 benchmarks, achieving new state-of-the-art results, and exhibits strong scalability with both model size and training data on MMEB.', 'score': 30, 'issue_id': 4592, 'pub_date': '2025-06-29', 'pub_date_card': {'ru': '29 июня', 'en': 'June 29', 'zh': '6月29日'}, 'hash': 'd7fecdae218ccf8e', 'authors': ['Haonan Chen', 'Hong Liu', 'Yuping Luo', 'Liang Wang', 'Nan Yang', 'Furu Wei', 'Zhicheng Dou'], 'affiliations': ['Gaoling School of Artificial Intelligence, Renmin University of China', 'Microsoft Corporation', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2506.23115.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#training', '#optimization', '#multimodal', '#alignment', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'MoCa: революция в мультимодальном встраивании', 'desc': 'MoCa - это двухэтапная система для улучшения предобученных причинно-следственных визуально-языковых моделей для мультимодального встраивания. Она вводит двунаправленное внимание, масштабирование с помощью неразмеченных данных и разнообразные цели обучения. Первый этап включает совместную реконструкцию для улучшения двунаправленных рассуждений с учетом контекста. Второй этап использует разнообразные семантически богатые мультимодальные данные для улучшения обобщения и выравнивания.'}, 'en': {'title': 'MoCa: Enhancing Multimodal Embedding with Bidirectional Attention', 'desc': 'MoCa is a two-stage framework designed to improve pre-trained causal vision-language models for better multimodal embedding. It addresses limitations in current models, such as the inefficiency of causal attention and the need for high-quality labeled data. The first stage focuses on modality-aware continual pre-training, which enhances understanding by denoising both text and image inputs. The second stage employs heterogeneous contrastive fine-tuning, using diverse multimodal data to improve model generalization and alignment, leading to state-of-the-art performance in various benchmarks.'}, 'zh': {'title': 'MoCa：双向多模态嵌入的创新框架', 'desc': 'MoCa是一个两阶段框架，旨在增强预训练的因果视觉语言模型在多模态嵌入中的表现。它通过引入双向注意力机制、利用未标记数据进行扩展以及多样化的训练目标来解决现有方法的局限性。第一阶段通过联合重建目标来提高文本和图像输入的去噪能力，增强双向上下文感知推理。第二阶段则利用丰富的多模态数据进行异构对比微调，从而提高模型的泛化能力和对齐效果。'}}}, {'id': 'https://huggingface.co/papers/2507.00432', 'title': 'Does Math Reasoning Improve General LLM Capabilities? Understanding\n  Transferability of LLM Reasoning', 'url': 'https://huggingface.co/papers/2507.00432', 'abstract': 'Reinforcement learning-tuned models outperform supervised fine-tuned models in generalizing mathematical problem-solving abilities to other domains, indicating a need to re-evaluate training methods for reasoning models.  \t\t\t\t\tAI-generated summary \t\t\t\t Math reasoning has become the poster child of progress in large language models (LLMs), with new models rapidly surpassing human-level performance on benchmarks like MATH and AIME. But as math leaderboards improve week by week, it is worth asking: do these gains reflect broader problem-solving ability or just narrow overfitting? To answer this question, we evaluate over 20 open-weight reasoning-tuned models across a broad suite of tasks, including math, scientific QA, agent planning, coding, and standard instruction-following. We surprisingly find that most models that succeed in math fail to transfer their gains to other domains. To rigorously study this phenomenon, we conduct controlled experiments on Qwen3-14B models using math-only data but different tuning methods. We find that reinforcement learning (RL)-tuned models generalize well across domains, while supervised fine-tuning (SFT)-tuned models often forget general capabilities. Latent-space representation and token-space distribution shift analyses reveal that SFT induces substantial representation and output drift, while RL preserves general-domain structure. Our results suggest a need to rethink standard post-training recipes, particularly the reliance on SFT-distilled data for advancing reasoning models.', 'score': 29, 'issue_id': 4592, 'pub_date': '2025-07-01', 'pub_date_card': {'ru': '1 июля', 'en': 'July 1', 'zh': '7月1日'}, 'hash': 'c4a7e4dd11865858', 'authors': ['Maggie Huan', 'Yuetai Li', 'Tuney Zheng', 'Xiaoyu Xu', 'Seungone Kim', 'Minxin Du', 'Radha Poovendran', 'Graham Neubig', 'Xiang Yue'], 'affiliations': ['Carnegie Mellon University', 'M-A-P', 'The Hong Kong Polytechnic University', 'University of Pennsylvania', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2507.00432.jpg', 'data': {'categories': ['#reasoning', '#training', '#rl', '#transfer_learning', '#optimization', '#math'], 'emoji': '🧠', 'ru': {'title': 'Обучение с подкреплением превосходит обучение с учителем в обобщении навыков решения задач', 'desc': 'Исследование показывает, что модели, обученные с помощью обучения с подкреплением (RL), лучше обобщают навыки решения математических задач на другие области, чем модели, настроенные с помощью обучения с учителем (SFT). Анализ латентного пространства и распределения токенов выявил, что SFT вызывает значительный сдвиг в представлении и выводе, в то время как RL сохраняет общую структуру. Эксперименты проводились на моделях Qwen3-14B с использованием только математических данных, но разных методов обучения. Результаты указывают на необходимость пересмотра стандартных подходов к обучению моделей рассуждения, особенно в отношении использования данных, полученных с помощью SFT.'}, 'en': {'title': 'Rethinking Training: Reinforcement Learning for Better Generalization', 'desc': 'This paper investigates the effectiveness of different training methods for reasoning models, particularly in the context of mathematical problem-solving. It finds that models trained with reinforcement learning (RL) outperform those fine-tuned with supervised learning (SFT) when applied to a variety of tasks beyond mathematics. The study reveals that while SFT models excel in math, they struggle to generalize their skills to other domains due to significant representation drift. In contrast, RL-tuned models maintain their general problem-solving abilities, suggesting a need to reconsider current training approaches for reasoning models.'}, 'zh': {'title': '重新思考推理模型的训练方法', 'desc': '这篇论文探讨了强化学习（RL）调优模型在数学问题解决能力上的表现，发现其在其他领域的泛化能力优于监督微调（SFT）模型。研究表明，虽然许多模型在数学任务上表现出色，但它们在其他任务上的迁移能力却较差。通过对Qwen3-14B模型的实验，发现RL调优模型能够在多个领域中保持良好的泛化能力，而SFT模型则容易遗忘其通用能力。结果提示我们需要重新审视现有的训练方法，尤其是对SFT数据的依赖。'}}}, {'id': 'https://huggingface.co/papers/2506.19852', 'title': 'Radial Attention: O(nlog n) Sparse Attention with Energy Decay for\n  Long Video Generation', 'url': 'https://huggingface.co/papers/2506.19852', 'abstract': 'Radial Attention, a scalable sparse attention mechanism, improves efficiency and preserves video quality in diffusion models by leveraging spatiotemporal energy decay.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in diffusion models have enabled high-quality video generation, but the additional temporal dimension significantly increases computational costs, making training and inference on long videos prohibitively expensive. In this paper, we identify a phenomenon we term Spatiotemporal Energy Decay in video diffusion models: post-softmax attention scores diminish as spatial and temporal distance between tokens increase, akin to the physical decay of signal or waves over space and time in nature. Motivated by this, we propose Radial Attention, a scalable sparse attention mechanism with O(n log n) complexity that translates energy decay into exponentially decaying compute density, which is significantly more efficient than standard O(n^2) dense attention and more expressive than linear attention. Specifically, Radial Attention employs a simple, static attention mask where each token attends to spatially nearby tokens, with the attention window size shrinking with temporal distance. Moreover, it allows pre-trained video diffusion models to extend their generation length with efficient LoRA-based fine-tuning. Extensive experiments show that Radial Attention maintains video quality across Wan2.1-14B, HunyuanVideo, and Mochi 1, achieving up to a 1.9times speedup over the original dense attention. With minimal tuning, it enables video generation up to 4times longer while reducing training costs by up to 4.4times compared to direct fine-tuning and accelerating inference by up to 3.7times compared to dense attention inference.', 'score': 24, 'issue_id': 4594, 'pub_date': '2025-06-24', 'pub_date_card': {'ru': '24 июня', 'en': 'June 24', 'zh': '6月24日'}, 'hash': 'c195d9c32370fcf1', 'authors': ['Xingyang Li', 'Muyang Li', 'Tianle Cai', 'Haocheng Xi', 'Shuo Yang', 'Yujun Lin', 'Lvmin Zhang', 'Songlin Yang', 'Jinbo Hu', 'Kelly Peng', 'Maneesh Agrawala', 'Ion Stoica', 'Kurt Keutzer', 'Song Han'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2506.19852.jpg', 'data': {'categories': ['#video', '#optimization', '#diffusion', '#architecture', '#inference', '#training'], 'emoji': '🎥', 'ru': {'title': 'Радиальное внимание: эффективная генерация видео с сохранением качества', 'desc': "Статья представляет новый механизм внимания под названием 'Радиальное внимание' для моделей диффузии видео. Этот метод использует явление пространственно-временного затухания энергии, что позволяет значительно повысить эффективность вычислений при сохранении качества генерируемого видео. Радиальное внимание имеет сложность O(n log n) и позволяет моделям генерировать более длинные видео с меньшими вычислительными затратами. Эксперименты показывают, что этот подход ускоряет обучение и инференс в несколько раз по сравнению с стандартным плотным вниманием."}, 'en': {'title': 'Radial Attention: Efficient Video Generation with Sparse Attention', 'desc': 'This paper introduces Radial Attention, a new sparse attention mechanism designed to enhance the efficiency of video generation in diffusion models. It leverages the concept of Spatiotemporal Energy Decay, which explains how attention scores decrease as the distance between tokens increases, similar to how signals weaken over distance. By using a static attention mask that focuses on nearby tokens and reduces the attention window with temporal distance, Radial Attention achieves a computational complexity of O(n log n), making it much faster than traditional O(n^2) dense attention. The results show that this method not only speeds up training and inference but also maintains high video quality, allowing for longer video generation with reduced costs.'}, 'zh': {'title': '径向注意力：高效视频生成的新机制', 'desc': '本论文提出了一种名为径向注意力（Radial Attention）的稀疏注意力机制，旨在提高扩散模型在视频生成中的效率，同时保持视频质量。我们发现视频扩散模型中存在一种现象，称为时空能量衰减，随着空间和时间距离的增加，注意力得分会减小。径向注意力通过将能量衰减转化为指数衰减的计算密度，显著提高了计算效率，复杂度为O(n log n)，远优于传统的O(n^2)密集注意力。实验结果表明，径向注意力在多个视频生成模型中保持了视频质量，并实现了训练和推理速度的显著提升。'}}}, {'id': 'https://huggingface.co/papers/2506.20639', 'title': 'DiffuCoder: Understanding and Improving Masked Diffusion Models for Code\n  Generation', 'url': 'https://huggingface.co/papers/2506.20639', 'abstract': "Diffusion large language models are applied to code generation, revealing their unique denoising processes and benefiting from a novel reinforcement learning sampling scheme.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion large language models (dLLMs) are compelling alternatives to autoregressive (AR) models because their denoising models operate over the entire sequence. The global planning and iterative refinement features of dLLMs are particularly useful for code generation. However, current training and inference mechanisms for dLLMs in coding are still under-explored. To demystify the decoding behavior of dLLMs and unlock their potential for coding, we systematically investigate their denoising processes and reinforcement learning (RL) methods. We train a 7B dLLM, DiffuCoder, on 130B tokens of code. Using this model as a testbed, we analyze its decoding behavior, revealing how it differs from that of AR models: (1) dLLMs can decide how causal their generation should be without relying on semi-AR decoding, and (2) increasing the sampling temperature diversifies not only token choices but also their generation order. This diversity creates a rich search space for RL rollouts. For RL training, to reduce the variance of token log-likelihood estimates and maintain training efficiency, we propose coupled-GRPO, a novel sampling scheme that constructs complementary mask noise for completions used in training. In our experiments, coupled-GRPO significantly improves DiffuCoder's performance on code generation benchmarks (+4.4\\% on EvalPlus) and reduces reliance on AR causal during decoding. Our work provides deeper insight into the machinery of dLLM generation and offers an effective, diffusion-native RL training framework. https://github.com/apple/ml-diffucoder.", 'score': 15, 'issue_id': 4593, 'pub_date': '2025-06-25', 'pub_date_card': {'ru': '25 июня', 'en': 'June 25', 'zh': '6月25日'}, 'hash': '20d886d0a4cd5bb6', 'authors': ['Shansan Gong', 'Ruixiang Zhang', 'Huangjie Zheng', 'Jiatao Gu', 'Navdeep Jaitly', 'Lingpeng Kong', 'Yizhe Zhang'], 'affiliations': ['Apple', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2506.20639.jpg', 'data': {'categories': ['#training', '#rl', '#architecture', '#optimization', '#dataset', '#diffusion'], 'emoji': '🧠', 'ru': {'title': 'Раскрытие потенциала диффузионных языковых моделей для генерации кода', 'desc': 'Статья исследует применение диффузионных языковых моделей (dLLM) для генерации кода. Авторы анализируют уникальные процессы шумоподавления в dLLM и их отличия от авторегрессивных моделей. Они предлагают новый метод обучения с подкреплением под названием coupled-GRPO для улучшения производительности. Эксперименты показывают значительное повышение эффективности генерации кода с использованием предложенного подхода.'}, 'en': {'title': 'Unlocking Code Generation with Diffusion Models', 'desc': 'This paper explores the use of diffusion large language models (dLLMs) for code generation, highlighting their unique denoising processes compared to traditional autoregressive models. The authors introduce a novel reinforcement learning sampling scheme called coupled-GRPO, which enhances the training efficiency and performance of the dLLM named DiffuCoder. By analyzing the decoding behavior of DiffuCoder, the study reveals that dLLMs can flexibly adjust their generation strategies and improve diversity in output. The findings demonstrate that dLLMs have significant potential for coding tasks, providing a new framework for effective code generation.'}, 'zh': {'title': '扩散大语言模型：代码生成的新选择', 'desc': '本文探讨了扩散大语言模型（dLLMs）在代码生成中的应用，揭示了其独特的去噪过程。dLLMs与自回归模型相比，能够在整个序列上进行去噪，具有全球规划和迭代优化的特点，特别适合代码生成。我们提出了一种新颖的强化学习采样方案coupled-GRPO，以提高训练效率并减少标记对数估计的方差。实验结果表明，coupled-GRPO显著提升了DiffuCoder在代码生成基准上的表现，并减少了对自回归因果解码的依赖。'}}}, {'id': 'https://huggingface.co/papers/2506.21277', 'title': 'HumanOmniV2: From Understanding to Omni-Modal Reasoning with Context', 'url': 'https://huggingface.co/papers/2506.21277', 'abstract': 'A reinforcement learning-based approach enhances multimodal reasoning by addressing context understanding and shortcut problems, using context, format, accuracy, and logical rewards, and achieving superior performance on the IntentBench benchmark.  \t\t\t\t\tAI-generated summary \t\t\t\t With the rapid evolution of multimodal large language models, the capacity to deeply understand and interpret human intentions has emerged as a critical capability, which demands detailed and thoughtful reasoning. In recent studies, Reinforcement Learning (RL) has demonstrated potential in enhancing the reasoning capabilities of Large Language Models (LLMs). Nonetheless, the challenges associated with adapting RL to multimodal data and formats remain largely unaddressed. In this paper, we identify two issues in existing multimodal reasoning models: insufficient global context understanding and shortcut problems. Insufficient context understanding can happen when a model misinterprets multimodal context, resulting in incorrect answers. The shortcut problem occurs when the model overlooks crucial clues in multimodal inputs, directly addressing the query without considering the multimodal information. To tackle these issues, we emphasize the necessity for the model to reason with a clear understanding of the global context within multimodal inputs. This global context understanding can effectively prevent the model from overlooking key multimodal cues and ensure a thorough reasoning process. To ensure the accurate interpretation of multimodal context information, we implement a context reward judged by a large language model, alongside format and accuracy rewards. Additionally, to improve complex reasoning capability, we employ the LLM to assess the logical reward, determining whether the reasoning process successfully integrates multimodal information with logical methods. We also introduce a reasoning omni-modal benchmark, IntentBench, aimed at evaluating models in understanding complex human intentions and emotions. Our proposed method demonstrates advanced performance across multiple omni-modal benchmarks compared to other open-source omni-modal models.', 'score': 10, 'issue_id': 4592, 'pub_date': '2025-06-26', 'pub_date_card': {'ru': '26 июня', 'en': 'June 26', 'zh': '6月26日'}, 'hash': '15a38ef84e7820fa', 'authors': ['Qize Yang', 'Shimin Yao', 'Weixuan Chen', 'Shenghao Fu', 'Detao Bai', 'Jiaxing Zhao', 'Boyuan Sun', 'Bowen Yin', 'Xihan Wei', 'Jingren Zhou'], 'affiliations': ['Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2506.21277.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#games', '#rl', '#multimodal', '#survey'], 'emoji': '🧠', 'ru': {'title': 'Усиление мультимодального рассуждения с помощью обучения с подкреплением', 'desc': 'Предложен подход на основе обучения с подкреплением для улучшения мультимодального рассуждения в больших языковых моделях. Метод решает проблемы понимания контекста и упрощения, используя контекстные, форматные, точностные и логические награды. Авторы вводят новый бенчмарк IntentBench для оценки понимания сложных человеческих намерений и эмоций. Предложенный метод показывает превосходную производительность на IntentBench по сравнению с другими открытыми мультимодальными моделями.'}, 'en': {'title': 'Enhancing Multimodal Reasoning with Reinforcement Learning', 'desc': 'This paper presents a reinforcement learning approach to improve multimodal reasoning in large language models. It addresses two main challenges: insufficient understanding of global context and the shortcut problem, where models fail to consider important multimodal cues. By implementing context, format, accuracy, and logical rewards, the model enhances its reasoning capabilities and interprets multimodal inputs more effectively. The proposed method outperforms existing models on the IntentBench benchmark, showcasing its ability to understand complex human intentions and emotions.'}, 'zh': {'title': '强化学习提升多模态推理能力', 'desc': '本论文提出了一种基于强化学习的方法，以增强多模态推理能力，解决上下文理解和捷径问题。我们发现现有多模态推理模型存在全球上下文理解不足和捷径问题，这会导致模型错误解读多模态信息。为了解决这些问题，我们强调模型需要在多模态输入中清晰理解全球上下文，并通过上下文奖励、格式奖励和准确性奖励来确保对多模态信息的准确解读。我们的研究在IntentBench基准测试中表现优异，展示了在理解复杂人类意图和情感方面的先进性能。'}}}, {'id': 'https://huggingface.co/papers/2507.00951', 'title': 'Thinking Beyond Tokens: From Brain-Inspired Intelligence to Cognitive\n  Foundations for Artificial General Intelligence and its Societal Impact', 'url': 'https://huggingface.co/papers/2507.00951', 'abstract': 'The paper synthesizes the interdisciplinary approach to achieving Artificial General Intelligence, emphasizing modular reasoning, memory, multi-agent coordination, and the integration of neurosymbolic systems and reinforcement learning to overcome current model limitations.  \t\t\t\t\tAI-generated summary \t\t\t\t Can machines truly think, reason and act in domains like humans? This enduring question continues to shape the pursuit of Artificial General Intelligence (AGI). Despite the growing capabilities of models such as GPT-4.5, DeepSeek, Claude 3.5 Sonnet, Phi-4, and Grok 3, which exhibit multimodal fluency and partial reasoning, these systems remain fundamentally limited by their reliance on token-level prediction and lack of grounded agency. This paper offers a cross-disciplinary synthesis of AGI development, spanning artificial intelligence, cognitive neuroscience, psychology, generative models, and agent-based systems. We analyze the architectural and cognitive foundations of general intelligence, highlighting the role of modular reasoning, persistent memory, and multi-agent coordination. In particular, we emphasize the rise of Agentic RAG frameworks that combine retrieval, planning, and dynamic tool use to enable more adaptive behavior. We discuss generalization strategies, including information compression, test-time adaptation, and training-free methods, as critical pathways toward flexible, domain-agnostic intelligence. Vision-Language Models (VLMs) are reexamined not just as perception modules but as evolving interfaces for embodied understanding and collaborative task completion. We also argue that true intelligence arises not from scale alone but from the integration of memory and reasoning: an orchestration of modular, interactive, and self-improving components where compression enables adaptive behavior. Drawing on advances in neurosymbolic systems, reinforcement learning, and cognitive scaffolding, we explore how recent architectures begin to bridge the gap between statistical learning and goal-directed cognition. Finally, we identify key scientific, technical, and ethical challenges on the path to AGI.', 'score': 8, 'issue_id': 4593, 'pub_date': '2025-07-01', 'pub_date_card': {'ru': '1 июля', 'en': 'July 1', 'zh': '7月1日'}, 'hash': '056b6a5007ee5fc2', 'authors': ['Rizwan Qureshi', 'Ranjan Sapkota', 'Abbas Shah', 'Amgad Muneer', 'Anas Zafar', 'Ashmal Vayani', 'Maged Shoman', 'Abdelrahman B. M. Eldaly', 'Kai Zhang', 'Ferhat Sadak', 'Shaina Raza', 'Xinqi Fan', 'Ravid Shwartz-Ziv', 'Hong Yan', 'Vinjia Jain', 'Aman Chadha', 'Manoj Karkee', 'Jia Wu', 'Philip Torr', 'Seyedali Mirjalili'], 'affiliations': ['Amazon Research (Work done outside Amazon)', 'Center for Data Science, New York University, NYU, NY, USA', 'Center for research in Computer Vision, University of Central Florida, Orlando, FL, USA', 'Centre for Artificial Intelligence Research and Optimization, Torrens University Australia, Fortitude Valley, Brisbane, QLD 4006, Australia', 'Cornell University, Department of Biological and Environmental Engineering, Ithaca, NY 14853, USA', 'Department of Electrical Engineering, City University of Hong Kong, SAR China', 'Department of Electronics Engineering, Mehran University of Engineering & Technology, Jamshoro, Sindh, Pakistan', 'Department of Engineering Science, University of Oxford, UK', 'Department of Imaging Physics, The University of Texas MD Anderson Cancer Center, Houston, TX, USA', 'Department of Mechanical Engineering, Bartin University, Bartin Turkey', 'Intelligent Transportation Systems, University of Tennessee, Oakridge, TN, USA', 'Manchester Metropolitan University, Manchester, UK', 'Meta Research (Work done outside Meta)', 'University Research and Innovation Center, Obuda University, 1034 Budapest, Hungary', 'Vector Institute, Toronto Canada'], 'pdf_title_img': 'assets/pdf/title_img/2507.00951.jpg', 'data': {'categories': ['#agents', '#agi', '#rl', '#architecture', '#multimodal', '#rag', '#ethics', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Путь к AGI: объединяя модульность, память и мультиагентность', 'desc': 'Статья представляет междисциплинарный подход к достижению искусственного общего интеллекта (AGI). Авторы подчеркивают важность модульного рассуждения, памяти и координации между несколькими агентами. Особое внимание уделяется интеграции нейросимволических систем и обучения с подкреплением для преодоления ограничений современных моделей. В работе также рассматриваются стратегии генерализации и роль мультимодальных моделей в развитии AGI.'}, 'en': {'title': 'Towards True Intelligence: Integrating Memory, Reasoning, and Adaptation for AGI', 'desc': 'This paper explores the interdisciplinary approach to achieving Artificial General Intelligence (AGI) by integrating concepts from various fields such as cognitive neuroscience and psychology. It emphasizes the importance of modular reasoning, persistent memory, and multi-agent coordination in developing intelligent systems. The authors propose that true intelligence is not just about scaling models but involves the orchestration of memory and reasoning capabilities. They also highlight the potential of neurosymbolic systems and reinforcement learning to create more adaptive and flexible AI agents.'}, 'zh': {'title': '跨学科推动人工通用智能的实现', 'desc': '这篇论文探讨了实现人工通用智能（AGI）的跨学科方法，强调了模块化推理、持久记忆和多智能体协调的重要性。论文分析了通用智能的架构和认知基础，提出了结合检索、规划和动态工具使用的Agentic RAG框架，以实现更灵活的行为。我们还讨论了信息压缩、测试时适应和无训练方法等泛化策略，作为实现灵活、领域无关智能的关键路径。最后，论文指出了在实现AGI过程中面临的科学、技术和伦理挑战。'}}}, {'id': 'https://huggingface.co/papers/2507.00339', 'title': 'Training for X-Ray Vision: Amodal Segmentation, Amodal Content\n  Completion, and View-Invariant Object Representation from Multi-Camera Video', 'url': 'https://huggingface.co/papers/2507.00339', 'abstract': 'Amodal segmentation and amodal content completion require using object priors to estimate occluded masks and features of objects in complex scenes. Until now, no data has provided an additional dimension for object context: the possibility of multiple cameras sharing a view of a scene. We introduce MOVi-MC-AC: Multiple Object Video with Multi-Cameras and Amodal Content, the largest amodal segmentation and first amodal content dataset to date. Cluttered scenes of generic household objects are simulated in multi-camera video. MOVi-MC-AC contributes to the growing literature of object detection, tracking, and segmentation by including two new contributions to the deep learning for computer vision world. Multiple Camera (MC) settings where objects can be identified and tracked between various unique camera perspectives are rare in both synthetic and real-world video. We introduce a new complexity to synthetic video by providing consistent object ids for detections and segmentations between both frames and multiple cameras each with unique features and motion patterns on a single scene. Amodal Content (AC) is a reconstructive task in which models predict the appearance of target objects through occlusions. In the amodal segmentation literature, some datasets have been released with amodal detection, tracking, and segmentation labels. While other methods rely on slow cut-and-paste schemes to generate amodal content pseudo-labels, they do not account for natural occlusions present in the modal masks. MOVi-MC-AC provides labels for ~5.8 million object instances, setting a new maximum in the amodal dataset literature, along with being the first to provide ground-truth amodal content. The full dataset is available at https://huggingface.co/datasets/Amar-S/MOVi-MC-AC ,', 'score': 8, 'issue_id': 4607, 'pub_date': '2025-07-01', 'pub_date_card': {'ru': '1 июля', 'en': 'July 1', 'zh': '7月1日'}, 'hash': 'a0d52ad7a093d23d', 'authors': ['Alexander Moore', 'Amar Saini', 'Kylie Cancilla', 'Doug Poland', 'Carmen Carrano'], 'affiliations': ['Lawrence Livermore National Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2507.00339.jpg', 'data': {'categories': ['#dataset', '#cv'], 'emoji': '📹', 'ru': {'title': 'Многокамерное видео для амодальной сегментации и восстановления контента', 'desc': 'MOVi-MC-AC - это новый набор данных для амодальной сегментации и восстановления контента, содержащий видео с нескольких камер и сложными сценами бытовых предметов. Он включает уникальные идентификаторы объектов для отслеживания между кадрами и камерами, а также разметку для амодальной сегментации и восстановления скрытых частей объектов. Датасет содержит около 5,8 миллионов размеченных экземпляров объектов, что делает его крупнейшим в своей области. MOVi-MC-AC предоставляет новые возможности для задач обнаружения, отслеживания и сегментации объектов с использованием глубокого обучения.'}, 'en': {'title': 'Revolutionizing Amodal Segmentation with Multi-Camera Insights', 'desc': 'This paper presents MOVi-MC-AC, a groundbreaking dataset for amodal segmentation and content completion in complex scenes. It introduces the concept of using multiple cameras to capture a scene, allowing for better tracking and identification of objects from different perspectives. The dataset includes approximately 5.8 million labeled object instances, providing a rich resource for training deep learning models in computer vision. By offering ground-truth amodal content, it addresses limitations in previous datasets and enhances the understanding of occluded objects in cluttered environments.'}, 'zh': {'title': '多摄像头下的无模态分割新突破', 'desc': '本论文介绍了MOVi-MC-AC数据集，这是迄今为止最大的无模态分割和无模态内容数据集。该数据集模拟了多摄像头视频中的杂乱场景，提供了对象的遮挡掩码和特征。通过在多个独特摄像头视角之间识别和跟踪对象，MOVi-MC-AC为深度学习在计算机视觉领域的研究做出了重要贡献。数据集中包含约580万个对象实例，并提供了真实的无模态内容标签。'}}}, {'id': 'https://huggingface.co/papers/2506.21545', 'title': 'Data Efficacy for Language Model Training', 'url': 'https://huggingface.co/papers/2506.21545', 'abstract': 'DELT, a paradigm for enhancing language model performance through data efficacy, consists of data scoring, selection, and ordering, demonstrating significant improvements without increasing data scale or model size.  \t\t\t\t\tAI-generated summary \t\t\t\t Data is fundamental to the training of language models (LM). Recent research has been dedicated to data efficiency, which aims to maximize performance by selecting a minimal or optimal subset of training data. Techniques such as data filtering, sampling, and selection play a crucial role in this area. To complement it, we define Data Efficacy, which focuses on maximizing performance by optimizing the organization of training data and remains relatively underexplored. This work introduces a general paradigm, DELT, for considering data efficacy in LM training, which highlights the significance of training data organization. DELT comprises three components: Data Scoring, Data Selection, and Data Ordering. Among these components, we design Learnability-Quality Scoring (LQS), as a new instance of Data Scoring, which considers both the learnability and quality of each data sample from the gradient consistency perspective. We also devise Folding Ordering (FO), as a novel instance of Data Ordering, which addresses issues such as model forgetting and data distribution bias. Comprehensive experiments validate the data efficacy in LM training, which demonstrates the following: Firstly, various instances of the proposed DELT enhance LM performance to varying degrees without increasing the data scale and model size. Secondly, among these instances, the combination of our proposed LQS for data scoring and Folding for data ordering achieves the most significant improvement. Lastly, data efficacy can be achieved together with data efficiency by applying data selection. Therefore, we believe that data efficacy is a promising foundational area in LM training.', 'score': 6, 'issue_id': 4596, 'pub_date': '2025-06-26', 'pub_date_card': {'ru': '26 июня', 'en': 'June 26', 'zh': '6月26日'}, 'hash': 'b19a54a5dd4e35c8', 'authors': ['Yalun Dai', 'Yangyu Huang', 'Xin Zhang', 'Wenshan Wu', 'Chong Li', 'Wenhui Lu', 'Shijie Cao', 'Li Dong', 'Scarlett Li'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2506.21545.jpg', 'data': {'categories': ['#optimization', '#training', '#data'], 'emoji': '🧠', 'ru': {'title': 'Эффективность данных - ключ к улучшению языковых моделей', 'desc': 'DELT - это новая парадигма для улучшения производительности языковых моделей путем оптимизации организации обучающих данных. Она включает три компонента: оценку данных, отбор данных и упорядочивание данных. Авторы предлагают новый метод оценки данных LQS, учитывающий обучаемость и качество каждого образца, а также метод упорядочивания данных FO для решения проблем забывания модели и смещения распределения данных. Эксперименты показывают, что DELT позволяет значительно улучшить производительность языковых моделей без увеличения объема данных или размера модели.'}, 'en': {'title': 'Maximizing Language Model Performance through Data Efficacy', 'desc': 'The paper introduces DELT, a new approach to improve language model performance by focusing on data efficacy, which is about how well training data is organized. It consists of three main components: Data Scoring, Data Selection, and Data Ordering, which work together to enhance model training without needing more data or larger models. A key innovation is the Learnability-Quality Scoring (LQS), which evaluates data samples based on their learnability and quality. The results show that using DELT can significantly boost performance, especially when combining LQS with Folding Ordering, while also achieving data efficiency.'}, 'zh': {'title': '提升语言模型性能的新方法：DELT', 'desc': 'DELT是一种提高语言模型性能的新方法，专注于数据的有效性。它包括数据评分、选择和排序三个部分，旨在优化训练数据的组织方式。通过设计学习质量评分（LQS）和折叠排序（FO），DELT能够在不增加数据规模或模型大小的情况下显著提升模型性能。实验结果表明，数据有效性与数据效率可以结合使用，从而为语言模型训练提供新的思路。'}}}, {'id': 'https://huggingface.co/papers/2507.00162', 'title': 'FreeLong++: Training-Free Long Video Generation via Multi-band\n  SpectralFusion', 'url': 'https://huggingface.co/papers/2507.00162', 'abstract': 'Recent advances in video generation models have enabled high-quality short video generation from text prompts. However, extending these models to longer videos remains a significant challenge, primarily due to degraded temporal consistency and visual fidelity. Our preliminary observations show that naively applying short-video generation models to longer sequences leads to noticeable quality degradation. Further analysis identifies a systematic trend where high-frequency components become increasingly distorted as video length grows, an issue we term high-frequency distortion. To address this, we propose FreeLong, a training-free framework designed to balance the frequency distribution of long video features during the denoising process. FreeLong achieves this by blending global low-frequency features, which capture holistic semantics across the full video, with local high-frequency features extracted from short temporal windows to preserve fine details. Building on this, FreeLong++ extends FreeLong dual-branch design into a multi-branch architecture with multiple attention branches, each operating at a distinct temporal scale. By arranging multiple window sizes from global to local, FreeLong++ enables multi-band frequency fusion from low to high frequencies, ensuring both semantic continuity and fine-grained motion dynamics across longer video sequences. Without any additional training, FreeLong++ can be plugged into existing video generation models (e.g. Wan2.1 and LTX-Video) to produce longer videos with substantially improved temporal consistency and visual fidelity. We demonstrate that our approach outperforms previous methods on longer video generation tasks (e.g. 4x and 8x of native length). It also supports coherent multi-prompt video generation with smooth scene transitions and enables controllable video generation using long depth or pose sequences.', 'score': 4, 'issue_id': 4600, 'pub_date': '2025-06-30', 'pub_date_card': {'ru': '30 июня', 'en': 'June 30', 'zh': '6月30日'}, 'hash': '2e49553aab1de98a', 'authors': ['Yu Lu', 'Yi Yang'], 'affiliations': ['ReLER, CCAI, Zhejiang University, Hangzhou, 310027, China'], 'pdf_title_img': 'assets/pdf/title_img/2507.00162.jpg', 'data': {'categories': ['#games', '#optimization', '#video'], 'emoji': '🎬', 'ru': {'title': 'FreeLong: Прорыв в генерации длинных видео с сохранением качества', 'desc': 'Статья представляет FreeLong и FreeLong++, новые подходы к генерации длинных видео с использованием моделей машинного обучения. Авторы выявили проблему искажения высокочастотных компонентов при увеличении длины видео и предложили решение, балансирующее распределение частот во время процесса денойзинга. FreeLong++ использует многоветвенную архитектуру с несколькими ветвями внимания, работающими на разных временных масштабах. Эти методы позволяют генерировать более длинные видео с улучшенной временной согласованностью и визуальной точностью без дополнительного обучения моделей.'}, 'en': {'title': 'Enhancing Long Video Generation with FreeLong Framework', 'desc': 'This paper introduces FreeLong, a novel framework aimed at improving the generation of longer videos from text prompts while maintaining high visual quality and temporal consistency. The authors identify a problem called high-frequency distortion, which occurs when short-video generation models are applied to longer sequences, leading to degraded video quality. FreeLong addresses this by blending low-frequency features that capture overall video semantics with high-frequency features that preserve fine details. The enhanced version, FreeLong++, expands this concept into a multi-branch architecture, allowing for better frequency fusion and improved performance in generating longer videos with coherent transitions and controllable elements.'}, 'zh': {'title': '长视频生成的新突破', 'desc': '最近视频生成模型的进展使得从文本提示生成高质量短视频成为可能。然而，将这些模型扩展到更长的视频仍然是一个重大挑战，主要是由于时间一致性和视觉保真度的下降。我们的研究表明，简单地将短视频生成模型应用于较长序列会导致明显的质量下降。为了解决这个问题，我们提出了FreeLong框架，通过在去噪过程中平衡长视频特征的频率分布，结合全局低频特征和局部高频特征，从而保持细节和语义的一致性。'}}}, {'id': 'https://huggingface.co/papers/2506.23329', 'title': 'IR3D-Bench: Evaluating Vision-Language Model Scene Understanding as\n  Agentic Inverse Rendering', 'url': 'https://huggingface.co/papers/2506.23329', 'abstract': 'Vision-language models (VLMs) excel at descriptive tasks, but whether they truly understand scenes from visual observations remains uncertain. We introduce IR3D-Bench, a benchmark challenging VLMs to demonstrate understanding through active creation rather than passive recognition. Grounded in the analysis-by-synthesis paradigm, IR3D-Bench tasks Vision-Language Agents (VLAs) with actively using programming and rendering tools to recreate the underlying 3D structure of an input image, achieving agentic inverse rendering through tool use. This "understanding-by-creating" approach probes the tool-using generative capacity of VLAs, moving beyond the descriptive or conversational capacity measured by traditional scene understanding benchmarks. We provide a comprehensive suite of metrics to evaluate geometric accuracy, spatial relations, appearance attributes, and overall plausibility. Initial experiments on agentic inverse rendering powered by various state-of-the-art VLMs highlight current limitations, particularly in visual precision rather than basic tool usage. IR3D-Bench, including data and evaluation protocols, is released to facilitate systematic study and development of tool-using VLAs towards genuine scene understanding by creating.', 'score': 4, 'issue_id': 4604, 'pub_date': '2025-06-29', 'pub_date_card': {'ru': '29 июня', 'en': 'June 29', 'zh': '6月29日'}, 'hash': 'a1469fc6d15316f8', 'authors': ['Parker Liu', 'Chenxin Li', 'Zhengxin Li', 'Yipeng Wu', 'Wuyang Li', 'Zhiqin Yang', 'Zhenyuan Zhang', 'Yunlong Lin', 'Sirui Han', 'Brandon Y. Feng'], 'affiliations': ['CUHK', 'EPFL', 'HKUST', 'MIT', 'TJU', 'XMU'], 'pdf_title_img': 'assets/pdf/title_img/2506.23329.jpg', 'data': {'categories': ['#games', '#benchmark', '#multimodal', '#cv', '#interpretability'], 'emoji': '🎨', 'ru': {'title': 'Понимание сцены через её активное воссоздание', 'desc': "Статья представляет IR3D-Bench - новый бенчмарк для оценки понимания сцен моделями компьютерного зрения и языка (VLM). В отличие от традиционных тестов на распознавание, IR3D-Bench требует от моделей активного воссоздания 3D-структуры изображения с помощью инструментов программирования и рендеринга. Этот подход 'понимание через создание' позволяет оценить генеративные способности моделей и их умение использовать инструменты. Бенчмарк включает набор метрик для оценки геометрической точности, пространственных отношений и правдоподобности воссозданных сцен."}, 'en': {'title': 'Understanding Scenes by Creating, Not Just Recognizing', 'desc': "This paper introduces IR3D-Bench, a new benchmark designed to test the understanding capabilities of Vision-Language Models (VLMs) through active creation rather than just recognition. It employs the analysis-by-synthesis approach, where Vision-Language Agents (VLAs) use programming and rendering tools to reconstruct the 3D structure of images. This method emphasizes 'understanding-by-creating', which assesses the generative abilities of VLAs beyond traditional descriptive tasks. The study also presents metrics for evaluating various aspects of the generated scenes, revealing current limitations in visual precision among state-of-the-art VLMs."}, 'zh': {'title': '通过创造理解场景的能力', 'desc': '本文介绍了IR3D-Bench，这是一个新的基准测试，旨在评估视觉语言模型（VLMs）在理解场景方面的能力。与传统的被动识别不同，IR3D-Bench要求视觉语言代理（VLAs）通过编程和渲染工具主动创建输入图像的3D结构。该方法基于分析-合成范式，强调通过创造来理解，而不仅仅是描述。初步实验显示，尽管当前的VLM在工具使用上表现良好，但在视觉精度方面仍存在局限。'}}}, {'id': 'https://huggingface.co/papers/2506.23009', 'title': 'MusiXQA: Advancing Visual Music Understanding in Multimodal Large\n  Language Models', 'url': 'https://huggingface.co/papers/2506.23009', 'abstract': 'Multimodal Large Language Models (MLLMs) have achieved remarkable visual reasoning abilities in natural images, text-rich documents, and graphic designs. However, their ability to interpret music sheets remains underexplored. To bridge this gap, we introduce MusiXQA, the first comprehensive dataset for evaluating and advancing MLLMs in music sheet understanding. MusiXQA features high-quality synthetic music sheets generated via MusiXTeX, with structured annotations covering note pitch and duration, chords, clefs, key/time signatures, and text, enabling diverse visual QA tasks. Through extensive evaluations, we reveal significant limitations of current state-of-the-art MLLMs in this domain. Beyond benchmarking, we developed Phi-3-MusiX, an MLLM fine-tuned on our dataset, achieving significant performance gains over GPT-based methods. The proposed dataset and model establish a foundation for future advances in MLLMs for music sheet understanding. Code, data, and model will be released upon acceptance.', 'score': 4, 'issue_id': 4610, 'pub_date': '2025-06-28', 'pub_date_card': {'ru': '28 июня', 'en': 'June 28', 'zh': '6月28日'}, 'hash': 'd5d1bf85d7b72633', 'authors': ['Jian Chen', 'Wenye Ma', 'Penghang Liu', 'Wei Wang', 'Tengwei Song', 'Ming Li', 'Chenguang Wang', 'Ruiyi Zhang', 'Changyou Chen'], 'affiliations': ['Duke University', 'King Abdullah University of Science and Technology', 'Mohamed bin Zayed University of Artificial Intelligence', 'University at Buffalo', 'University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2506.23009.jpg', 'data': {'categories': ['#training', '#benchmark', '#synthetic', '#games', '#dataset', '#multimodal'], 'emoji': '🎼', 'ru': {'title': 'MusiXQA: новый рубеж в понимании нотных листов искусственным интеллектом', 'desc': 'Статья представляет MusiXQA - первый всеобъемлющий датасет для оценки и улучшения мультимодальных больших языковых моделей (MLLM) в понимании нотных листов. Датасет содержит синтетические нотные листы с аннотациями, охватывающими различные аспекты музыкальной нотации. Авторы выявили значительные ограничения современных MLLM в этой области. Они также разработали модель Phi-3-MusiX, специально настроенную на этом датасете, которая показала существенное улучшение производительности по сравнению с методами на основе GPT.'}, 'en': {'title': 'Unlocking Music Sheet Understanding for MLLMs', 'desc': 'This paper introduces MusiXQA, a new dataset designed to improve the understanding of music sheets by Multimodal Large Language Models (MLLMs). The dataset includes high-quality synthetic music sheets with detailed annotations, allowing MLLMs to perform various visual question-answering tasks related to music notation. The authors demonstrate that current MLLMs struggle with music sheet interpretation, highlighting the need for specialized training. They also present Phi-3-MusiX, an MLLM fine-tuned on MusiXQA, which shows improved performance compared to existing models like GPT.'}, 'zh': {'title': '乐谱理解的新突破：MusiXQA与Phi-3-MusiX', 'desc': '多模态大型语言模型（MLLMs）在自然图像、文本丰富的文档和图形设计方面表现出色，但在乐谱理解方面的能力仍然未被充分探索。为了解决这个问题，我们推出了MusiXQA，这是第一个全面评估和推动MLLMs在乐谱理解方面进展的数据集。MusiXQA包含高质量的合成乐谱，配有结构化注释，涵盖音符音高和时值、和弦、谱号、调号/拍号和文本，支持多样的视觉问答任务。通过广泛的评估，我们揭示了当前最先进的MLLMs在这一领域的显著局限性，并开发了Phi-3-MusiX，一个在我们的数据集上微调的MLLM，显著提升了性能。'}}}, {'id': 'https://huggingface.co/papers/2506.22960', 'title': 'Peccavi: Visual Paraphrase Attack Safe and Distortion Free Image\n  Watermarking Technique for AI-Generated Images', 'url': 'https://huggingface.co/papers/2506.22960', 'abstract': 'PECCAVI is a robust image watermarking technique that is resistant to visual paraphrase attacks and distortions, utilizing NMPs and multi-channel frequency domain watermarking.  \t\t\t\t\tAI-generated summary \t\t\t\t A report by the European Union Law Enforcement Agency predicts that by 2026, up to 90 percent of online content could be synthetically generated, raising concerns among policymakers, who cautioned that "Generative AI could act as a force multiplier for political disinformation. The combined effect of generative text, images, videos, and audio may surpass the influence of any single modality." In response, California\'s Bill AB 3211 mandates the watermarking of AI-generated images, videos, and audio. However, concerns remain regarding the vulnerability of invisible watermarking techniques to tampering and the potential for malicious actors to bypass them entirely. Generative AI-powered de-watermarking attacks, especially the newly introduced visual paraphrase attack, have shown an ability to fully remove watermarks, resulting in a paraphrase of the original image. This paper introduces PECCAVI, the first visual paraphrase attack-safe and distortion-free image watermarking technique. In visual paraphrase attacks, an image is altered while preserving its core semantic regions, termed Non-Melting Points (NMPs). PECCAVI strategically embeds watermarks within these NMPs and employs multi-channel frequency domain watermarking. It also incorporates noisy burnishing to counter reverse-engineering efforts aimed at locating NMPs to disrupt the embedded watermark, thereby enhancing durability. PECCAVI is model-agnostic. All relevant resources and codes will be open-sourced.', 'score': 4, 'issue_id': 4593, 'pub_date': '2025-06-28', 'pub_date_card': {'ru': '28 июня', 'en': 'June 28', 'zh': '6月28日'}, 'hash': 'c946e3ac9bf6133a', 'authors': ['Shreyas Dixit', 'Ashhar Aziz', 'Shashwat Bajpai', 'Vasu Sharma', 'Aman Chadha', 'Vinija Jain', 'Amitava Das'], 'affiliations': ['AI Institute, University of South Carolina, USA', 'Amazon GenAI, USA', 'BITS Pilani Hyderabad, India', 'IIIT Delhi, India', 'Meta AI, USA', 'Stanford University, USA', 'VIIT Pune, India'], 'pdf_title_img': 'assets/pdf/title_img/2506.22960.jpg', 'data': {'categories': ['#security', '#synthetic', '#data', '#open_source', '#multimodal', '#cv'], 'emoji': '🔐', 'ru': {'title': 'Непобедимые водяные знаки для эпохи генеративного ИИ', 'desc': 'PECCAVI - это устойчивая техника водяных знаков для изображений, которая противостоит атакам визуального перефразирования и искажениям. Она использует неизменяемые точки (NMP) и многоканальное встраивание водяных знаков в частотной области. PECCAVI стратегически размещает водяные знаки в NMP и применяет шумовую полировку для противодействия обратному инжинирингу. Эта техника является моделенезависимой и обещает быть открытым исходным кодом.'}, 'en': {'title': 'PECCAVI: Watermarking Resilience Against Visual Paraphrase Attacks', 'desc': 'PECCAVI is a novel image watermarking technique designed to withstand visual paraphrase attacks and various distortions. It utilizes Non-Melting Points (NMPs) to strategically embed watermarks, ensuring that the essential features of the image remain intact. The method employs multi-channel frequency domain watermarking and incorporates noisy burnishing to protect against reverse-engineering attempts. This approach is model-agnostic, making it applicable across different systems, and all resources will be made available to the public.'}, 'zh': {'title': 'PECCAVI：抵御视觉改写的水印新技术', 'desc': 'PECCAVI是一种强大的图像水印技术，能够抵御视觉改写攻击和失真。它利用非熔化点（NMPs）和多通道频域水印技术，将水印嵌入图像的核心语义区域。该技术还采用了噪声烧灼方法，以防止逆向工程攻击，增强水印的耐久性。PECCAVI不依赖于特定模型，所有相关资源和代码将开源。'}}}, {'id': 'https://huggingface.co/papers/2507.00606', 'title': 'Mixture of Reasonings: Teach Large Language Models to Reason with\n  Adaptive Strategies', 'url': 'https://huggingface.co/papers/2507.00606', 'abstract': 'Large language models (LLMs) excel in complex tasks through advanced prompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT), but their reliance on manually crafted, task-specific prompts limits adaptability and efficiency. We introduce Mixture of Reasoning (MoR), a training framework that embeds diverse reasoning strategies into LLMs for autonomous, task-adaptive reasoning without external prompt engineering. MoR has two phases: Thought Generation, creating reasoning chain templates with models like GPT-4o, and SFT Dataset Construction, pairing templates with benchmark datasets for supervised fine-tuning.Our experiments show that MoR significantly enhances performance, with MoR150 achieving 0.730 (2.2% improvement) using CoT prompting and 0.734 (13.5% improvement) compared to baselines. MoR eliminates the need for task-specific prompts, offering a generalizable solution for robust reasoning across diverse tasks.', 'score': 2, 'issue_id': 4607, 'pub_date': '2025-07-01', 'pub_date_card': {'ru': '1 июля', 'en': 'July 1', 'zh': '7月1日'}, 'hash': 'a9828e151d8e7eb8', 'authors': ['Tao Xiong', 'Xavier Hu', 'Wenyan Fan', 'Shengyu Zhang'], 'affiliations': ['Dalian University of Technology, Dalian, LiaoNing, China', 'Independent, Hangzhou, ZheJiang, China', 'Zhejiang University, Hangzhou, ZheJiang, China'], 'pdf_title_img': 'assets/pdf/title_img/2507.00606.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#training'], 'emoji': '🧠', 'ru': {'title': 'Автономное рассуждение для языковых моделей без промптов', 'desc': 'Статья представляет новый метод обучения больших языковых моделей (LLM) под названием Mixture of Reasoning (MoR). MoR позволяет моделям автономно применять различные стратегии рассуждений без необходимости ручной разработки промптов. Метод включает две фазы: генерацию шаблонов цепочек рассуждений и создание датасета для дообучения модели. Эксперименты показывают значительное улучшение производительности модели по сравнению с базовыми методами.'}, 'en': {'title': 'Empowering LLMs with Autonomous Reasoning through Mixture of Reasoning', 'desc': "This paper presents a new framework called Mixture of Reasoning (MoR) that improves the performance of large language models (LLMs) by integrating various reasoning strategies. Unlike traditional methods that depend on specific prompts for each task, MoR allows LLMs to adapt their reasoning autonomously. The framework consists of two main phases: Thought Generation, which creates templates for reasoning chains, and SFT Dataset Construction, which pairs these templates with datasets for fine-tuning. Experimental results demonstrate that MoR enhances the model's performance significantly, making it a versatile solution for a wide range of tasks without the need for manual prompt design."}, 'zh': {'title': '推理混合：无提示自适应推理的新方法', 'desc': '大型语言模型（LLMs）在复杂任务中表现出色，得益于先进的提示技术，如思维链（CoT）和思维树（ToT），但它们对手动设计的特定任务提示的依赖限制了适应性和效率。我们提出了推理混合（MoR）框架，将多样的推理策略嵌入LLMs中，实现自主、任务自适应的推理，而无需外部提示工程。MoR包括两个阶段：思维生成，使用像GPT-4o这样的模型创建推理链模板，以及SFT数据集构建，将模板与基准数据集配对进行监督微调。实验表明，MoR显著提升了性能，MoR150在使用CoT提示时达到了0.730（提高2.2%），与基线相比提高了13.5%，提供了一种可推广的解决方案，以实现跨多样任务的稳健推理。'}}}, {'id': 'https://huggingface.co/papers/2507.00476', 'title': 'FreNBRDF: A Frequency-Rectified Neural Material Representation', 'url': 'https://huggingface.co/papers/2507.00476', 'abstract': 'Accurate material modeling is crucial for achieving photorealistic rendering, bridging the gap between computer-generated imagery and real-world photographs. While traditional approaches rely on tabulated BRDF data, recent work has shifted towards implicit neural representations, which offer compact and flexible frameworks for a range of tasks. However, their behavior in the frequency domain remains poorly understood. To address this, we introduce FreNBRDF, a frequency-rectified neural material representation. By leveraging spherical harmonics, we integrate frequency-domain considerations into neural BRDF modeling. We propose a novel frequency-rectified loss, derived from a frequency analysis of neural materials, and incorporate it into a generalizable and adaptive reconstruction and editing pipeline. This framework enhances fidelity, adaptability, and efficiency. Extensive experiments demonstrate that \\ours improves the accuracy and robustness of material appearance reconstruction and editing compared to state-of-the-art baselines, enabling more structured and interpretable downstream tasks and applications.', 'score': 1, 'issue_id': 4608, 'pub_date': '2025-07-01', 'pub_date_card': {'ru': '1 июля', 'en': 'July 1', 'zh': '7月1日'}, 'hash': 'c539d168bcfbab61', 'authors': ['Chenliang Zhou', 'Zheyuan Hu', 'Cengiz Oztireli'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2507.00476.jpg', 'data': {'categories': ['#cv', '#3d'], 'emoji': '🎨', 'ru': {'title': 'Частотно-оптимизированные нейронные BRDF для фотореалистичного рендеринга', 'desc': 'Статья представляет FreNBRDF - новый подход к нейронному моделированию материалов с учетом частотной области. Авторы используют сферические гармоники для интеграции частотного анализа в нейронные BRDF. Предложен частотно-корректированный loss и адаптивный пайплайн для реконструкции и редактирования материалов. Эксперименты показывают, что FreNBRDF превосходит современные методы по точности и надежности моделирования внешнего вида материалов.'}, 'en': {'title': 'Enhancing Material Modeling with Frequency-Rectified Neural Representations', 'desc': 'This paper presents FreNBRDF, a new method for modeling materials in computer graphics using neural networks. It focuses on improving the accuracy of material representation by incorporating frequency-domain analysis through spherical harmonics. The authors introduce a frequency-rectified loss function that enhances the training of neural BRDF models, making them more adaptable and efficient. Experimental results show that this approach significantly outperforms existing methods in reconstructing and editing material appearances, leading to better photorealistic rendering.'}, 'zh': {'title': '频率校正，提升材料建模的准确性', 'desc': '准确的材料建模对于实现逼真的渲染至关重要，能够缩小计算机生成图像与真实照片之间的差距。传统方法依赖于表格化的BRDF数据，而最近的研究则转向隐式神经表示，提供了紧凑且灵活的框架。我们提出了FreNBRDF，这是一种频率校正的神经材料表示，通过利用球谐函数将频域考虑整合到神经BRDF建模中。我们的框架在材料外观重建和编辑方面提高了准确性和鲁棒性，支持更结构化和可解释的下游任务和应用。'}}}, {'id': 'https://huggingface.co/papers/2506.24019', 'title': 'Ella: Embodied Social Agents with Lifelong Memory', 'url': 'https://huggingface.co/papers/2506.24019', 'abstract': "We introduce Ella, an embodied social agent capable of lifelong learning within a community in a 3D open world, where agents accumulate experiences and acquire knowledge through everyday visual observations and social interactions. At the core of Ella's capabilities is a structured, long-term multimodal memory system that stores, updates, and retrieves information effectively. It consists of a name-centric semantic memory for organizing acquired knowledge and a spatiotemporal episodic memory for capturing multimodal experiences. By integrating this lifelong memory system with foundation models, Ella retrieves relevant information for decision-making, plans daily activities, builds social relationships, and evolves autonomously while coexisting with other intelligent beings in the open world. We conduct capability-oriented evaluations in a dynamic 3D open world where 15 agents engage in social activities for days and are assessed with a suite of unseen controlled evaluations. Experimental results show that Ella can influence, lead, and cooperate with other agents well to achieve goals, showcasing its ability to learn effectively through observation and social interaction. Our findings highlight the transformative potential of combining structured memory systems with foundation models for advancing embodied intelligence. More videos can be found at https://umass-embodied-agi.github.io/Ella/.", 'score': 1, 'issue_id': 4612, 'pub_date': '2025-06-30', 'pub_date_card': {'ru': '30 июня', 'en': 'June 30', 'zh': '6月30日'}, 'hash': 'e3bbc5a001cf1269', 'authors': ['Hongxin Zhang', 'Zheyuan Zhang', 'Zeyuan Wang', 'Zunzhe Zhang', 'Lixing Fang', 'Qinhong Zhou', 'Chuang Gan'], 'affiliations': ['Johns Hopkins University', 'Tsinghua University', 'University of Massachusetts Amherst'], 'pdf_title_img': 'assets/pdf/title_img/2506.24019.jpg', 'data': {'categories': ['#3d', '#agents', '#multimodal', '#agi'], 'emoji': '🤖', 'ru': {'title': 'Элла: воплощенный ИИ с непрерывным обучением в социальном 3D-мире', 'desc': 'Статья представляет Эллу - воплощенного социального агента, способного к непрерывному обучению в 3D-мире. Ключевой особенностью Эллы является структурированная долговременная мультимодальная система памяти, включающая семантическую и эпизодическую составляющие. Интеграция этой системы памяти с фундаментальными моделями позволяет Элле принимать решения, планировать деятельность и развивать социальные отношения. Эксперименты показали эффективность Эллы в достижении целей через наблюдение и социальное взаимодействие в динамичном 3D-мире.'}, 'en': {'title': 'Ella: Lifelong Learning in a Social 3D World', 'desc': "The paper presents Ella, an embodied social agent designed for lifelong learning in a 3D open world. Ella utilizes a structured multimodal memory system that includes semantic memory for knowledge organization and episodic memory for capturing experiences. This system allows Ella to make informed decisions, plan activities, and build social relationships through interactions with other agents. The results demonstrate Ella's effectiveness in influencing and cooperating with peers, emphasizing the benefits of integrating structured memory with foundation models for enhanced embodied intelligence."}, 'zh': {'title': 'Ella：终身学习的具身社交代理', 'desc': '本文介绍了Ella，一个能够在3D开放世界中进行终身学习的具身社交代理。Ella的核心能力是一个结构化的长期多模态记忆系统，能够有效地存储、更新和检索信息。该系统包括以名称为中心的语义记忆和捕捉多模态经验的时空情节记忆。通过将这一终身记忆系统与基础模型结合，Ella能够在与其他智能生物共存的环境中进行决策、规划日常活动和建立社交关系。'}}}, {'id': 'https://huggingface.co/papers/2506.22973', 'title': 'Confident Splatting: Confidence-Based Compression of 3D Gaussian\n  Splatting via Learnable Beta Distributions', 'url': 'https://huggingface.co/papers/2506.22973', 'abstract': "A novel lossy compression method using learnable confidence scores improves storage and computational efficiency in 3D Gaussian Splatting without sacrificing visual quality.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D Gaussian Splatting enables high-quality real-time rendering but often produces millions of splats, resulting in excessive storage and computational overhead. We propose a novel lossy compression method based on learnable confidence scores modeled as Beta distributions. Each splat's confidence is optimized through reconstruction-aware losses, enabling pruning of low-confidence splats while preserving visual fidelity. The proposed approach is architecture-agnostic and can be applied to any Gaussian Splatting variant. In addition, the average confidence values serve as a new metric to assess the quality of the scene. Extensive experiments demonstrate favorable trade-offs between compression and fidelity compared to prior work. Our code and data are publicly available at https://github.com/amirhossein-razlighi/Confident-Splatting", 'score': 0, 'issue_id': 4606, 'pub_date': '2025-06-28', 'pub_date_card': {'ru': '28 июня', 'en': 'June 28', 'zh': '6月28日'}, 'hash': '20ec91d26e253f96', 'authors': ['AmirHossein Naghi Razlighi', 'Elaheh Badali Golezani', 'Shohreh Kasaei'], 'affiliations': ['Sharif University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2506.22973.jpg', 'data': {'categories': ['#3d', '#inference', '#open_source', '#optimization'], 'emoji': '🎨', 'ru': {'title': 'Эффективное сжатие 3D-сцен без потери качества', 'desc': 'Представлен новый метод сжатия с потерями для 3D Gaussian Splatting, использующий обучаемые оценки достоверности на основе бета-распределения. Метод позволяет уменьшить объем хранения и вычислительные затраты без ущерба для визуального качества. Подход оптимизирует достоверность каждого сплата с помощью функций потерь, учитывающих реконструкцию, что позволяет отбрасывать малозначимые сплаты. Метод применим к любым вариантам Gaussian Splatting и предоставляет новую метрику оценки качества сцены.'}, 'en': {'title': 'Optimizing 3D Rendering with Learnable Confidence Scores', 'desc': 'This paper introduces a new method for compressing 3D Gaussian Splatting data using learnable confidence scores, which enhances both storage and computational efficiency. The method employs Beta distributions to model the confidence of each splat, allowing for the removal of less important splats without losing visual quality. By optimizing these confidence scores through reconstruction-aware losses, the approach effectively balances compression and fidelity. The technique is versatile and can be integrated with various Gaussian Splatting architectures, providing a new metric for scene quality assessment.'}, 'zh': {'title': '提升3D高斯点云压缩效率的新方法', 'desc': '本文提出了一种新颖的有损压缩方法，利用可学习的置信度评分来提高3D高斯点云的存储和计算效率，同时不牺牲视觉质量。3D高斯点云渲染通常会产生大量的点，导致存储和计算开销过大。我们的方法通过重建感知损失优化每个点的置信度，从而在保留视觉保真度的同时，去除低置信度的点。该方法不依赖于特定架构，适用于任何高斯点云变体，并且通过平均置信度值提供了一种新的场景质量评估指标。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (3)', '#agents (2)', '#agi (2)', '#alignment (1)', '#architecture (5)', '#audio', '#benchmark (7)', '#cv (4)', '#data (2)', '#dataset (5)', '#diffusion (2)', '#ethics (1)', '#games (4)', '#graphs', '#hallucinations', '#healthcare', '#inference (2)', '#interpretability (1)', '#leakage', '#long_context', '#low_resource', '#machine_translation', '#math (1)', '#multilingual', '#multimodal (8)', '#open_source (4)', '#optimization (7)', '#plp', '#rag (1)', '#reasoning (5)', '#rl (5)', '#rlhf', '#robotics', '#science (1)', '#security (1)', '#small_models', '#story_generation', '#survey (2)', '#synthetic (2)', '#training (8)', '#transfer_learning (1)', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-07-02 22:11',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-07-02 22:11')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-07-02 22:11')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    