{
    "date": {
        "ru": "1 октября",
        "en": "October 1",
        "zh": "10月1日"
    },
    "time_utc": "2025-10-01 02:30",
    "weekday": 2,
    "issue_id": 6175,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2509.25182",
            "title": "DC-VideoGen: Efficient Video Generation with Deep Compression Video\n  Autoencoder",
            "url": "https://huggingface.co/papers/2509.25182",
            "abstract": "DC-VideoGen accelerates video generation by adapting pre-trained diffusion models to a deep compression latent space, reducing inference latency and enabling high-resolution video generation.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce DC-VideoGen, a post-training acceleration framework for efficient video generation. DC-VideoGen can be applied to any pre-trained video diffusion model, improving efficiency by adapting it to a deep compression latent space with lightweight fine-tuning. The framework builds on two key innovations: (i) a Deep Compression Video Autoencoder with a novel chunk-causal temporal design that achieves 32x/64x spatial and 4x temporal compression while preserving reconstruction quality and generalization to longer videos; and (ii) AE-Adapt-V, a robust adaptation strategy that enables rapid and stable transfer of pre-trained models into the new latent space. Adapting the pre-trained Wan-2.1-14B model with DC-VideoGen requires only 10 GPU days on the NVIDIA H100 GPU. The accelerated models achieve up to 14.8x lower inference latency than their base counterparts without compromising quality, and further enable 2160x3840 video generation on a single GPU. Code: https://github.com/dc-ai-projects/DC-VideoGen.",
            "score": 14,
            "issue_id": 6175,
            "pub_date": "2025-09-29",
            "pub_date_card": {
                "ru": "29 сентября",
                "en": "September 29",
                "zh": "9月29日"
            },
            "hash": "7de7ea8b15ae7048",
            "authors": [
                "Junyu Chen",
                "Wenkun He",
                "Yuchao Gu",
                "Yuyang Zhao",
                "Jincheng Yu",
                "Junsong Chen",
                "Dongyun Zou",
                "Yujun Lin",
                "Zhekai Zhang",
                "Muyang Li",
                "Haocheng Xi",
                "Ligeng Zhu",
                "Enze Xie",
                "Song Han",
                "Han Cai"
            ],
            "affiliations": [
                "NVIDIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.25182.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#video",
                    "#optimization",
                    "#diffusion",
                    "#architecture",
                    "#training"
                ],
                "emoji": "⚡",
                "ru": {
                    "title": "Ускорение генерации видео через глубокое сжатие латентного пространства",
                    "desc": "DC-VideoGen — это фреймворк для ускорения генерации видео, который адаптирует предобученные диффузионные модели к глубоко сжатому латентному пространству. Ключевые инновации включают Deep Compression Video Autoencoder со сжатием 32-64x по пространству и 4x по времени, а также метод AE-Adapt-V для быстрой адаптации моделей. Адаптация модели Wan-2.1-14B требует всего 10 GPU-дней на NVIDIA H100, что обеспечивает ускорение инференса в 14.8 раз без потери качества. Технология позволяет генерировать видео разрешением 2160x3840 на одной GPU, значительно снижая вычислительные требования для высококачественной генерации видео."
                },
                "en": {
                    "title": "Accelerating Video Generation with Deep Compression",
                    "desc": "DC-VideoGen is a framework designed to speed up video generation by modifying existing diffusion models to work in a compressed latent space. This approach allows for significant reductions in inference time while still producing high-quality, high-resolution videos. The framework utilizes a Deep Compression Video Autoencoder that efficiently compresses video data and an adaptation strategy called AE-Adapt-V for seamless integration of pre-trained models. As a result, DC-VideoGen can generate videos much faster, achieving up to 14.8 times lower latency compared to traditional methods."
                },
                "zh": {
                    "title": "高效视频生成的新突破",
                    "desc": "DC-VideoGen 是一个加速视频生成的框架，它通过将预训练的扩散模型适应到深度压缩的潜在空间来减少推理延迟，从而实现高分辨率视频生成。该框架可以应用于任何预训练的视频扩散模型，通过轻量级微调提高效率。它的两个关键创新包括：一种具有新颖块因果时间设计的深度压缩视频自编码器，能够在保持重建质量的同时实现32倍/64倍的空间压缩和4倍的时间压缩；以及AE-Adapt-V，一种稳健的适应策略，能够快速稳定地将预训练模型转移到新的潜在空间。使用DC-VideoGen对预训练的Wan-2.1-14B模型进行适应只需10天的GPU时间，且加速后的模型在推理延迟上比基础模型低14.8倍，且能够在单个GPU上生成2160x3840的视频。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.26490",
            "title": "VitaBench: Benchmarking LLM Agents with Versatile Interactive Tasks in\n  Real-world Applications",
            "url": "https://huggingface.co/papers/2509.26490",
            "abstract": "VitaBench is a benchmark for evaluating LLM-based agents in complex, real-world interactive tasks using a diverse set of tools and scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t As LLM-based agents are increasingly deployed in real-life scenarios, existing benchmarks fail to capture their inherent complexity of handling extensive information, leveraging diverse resources, and managing dynamic user interactions. To address this gap, we introduce VitaBench, a challenging benchmark that evaluates agents on versatile interactive tasks grounded in real-world settings. Drawing from daily applications in food delivery, in-store consumption, and online travel services, VitaBench presents agents with the most complex life-serving simulation environment to date, comprising 66 tools. Through a framework that eliminates domain-specific policies, we enable flexible composition of these scenarios and tools, yielding 100 cross-scenario tasks (main results) and 300 single-scenario tasks. Each task is derived from multiple real user requests and requires agents to reason across temporal and spatial dimensions, utilize complex tool sets, proactively clarify ambiguous instructions, and track shifting user intent throughout multi-turn conversations. Moreover, we propose a rubric-based sliding window evaluator, enabling robust assessment of diverse solution pathways in complex environments and stochastic interactions. Our comprehensive evaluation reveals that even the most advanced models achieve only 30% success rate on cross-scenario tasks, and less than 50% success rate on others. Overall, we believe VitaBench will serve as a valuable resource for advancing the development of AI agents in practical real-world applications. The code, dataset, and leaderboard are available at https://vitabench.github.io/",
            "score": 5,
            "issue_id": 6175,
            "pub_date": "2025-09-30",
            "pub_date_card": {
                "ru": "30 сентября",
                "en": "September 30",
                "zh": "9月30日"
            },
            "hash": "3ee3a7d4e39bff94",
            "authors": [
                "Wei He",
                "Yueqing Sun",
                "Hongyan Hao",
                "Xueyuan Hao",
                "Zhikang Xia",
                "Qi Gu",
                "Chengcheng Han",
                "Dengchang Zhao",
                "Hui Su",
                "Kefeng Zhang",
                "Man Gao",
                "Xi Su",
                "Xiaodong Cai",
                "Xunliang Cai",
                "Yu Yang",
                "Yunke Zhao"
            ],
            "affiliations": [
                "Meituan LongCat Team"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.26490.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#agents",
                    "#benchmark",
                    "#games",
                    "#survey"
                ],
                "emoji": "🧭",
                "ru": {
                    "title": "VitaBench: жизненный экзамен для AI-агентов в реальных сценариях",
                    "desc": "VitaBench — это новый бенчмарк для оценки LLM-агентов в сложных интерактивных задачах, приближенных к реальной жизни. Он включает 66 инструментов и 400 задач из сфер доставки еды, ресторанного обслуживания и онлайн-туризма, требующих рассуждений в пространстве и времени, работы со сложными наборами инструментов и многоходовых диалогов с пользователями. Для оценки предложен метод на основе рубрик со скользящим окном, учитывающий множество возможных путей решения. Даже самые продвинутые модели достигают лишь 30% успеха в кросс-сценарных задачах, что показывает значительный разрыв между текущими возможностями AI-агентов и требованиями реального мира."
                },
                "en": {
                    "title": "VitaBench: Advancing AI Agents in Real-World Complexity",
                    "desc": "VitaBench is a new benchmark designed to test large language model (LLM)-based agents in complex, real-world tasks that require interaction with various tools. It addresses the limitations of existing benchmarks by providing a diverse set of scenarios that reflect daily applications, such as food delivery and travel services. The benchmark includes 66 tools and offers 100 cross-scenario tasks, challenging agents to manage dynamic user interactions and reason through complex instructions. The evaluation shows that even advanced models struggle, achieving only a 30% success rate on cross-scenario tasks, highlighting the need for further development in AI agents for practical use."
                },
                "zh": {
                    "title": "VitaBench：评估复杂互动任务的基准测试",
                    "desc": "VitaBench是一个用于评估基于大型语言模型（LLM）代理在复杂现实互动任务中的基准测试。它解决了现有基准无法捕捉代理处理大量信息、利用多样资源和管理动态用户交互的复杂性的问题。VitaBench提供了66种工具和多种场景，设计了100个跨场景任务和300个单场景任务，要求代理在多轮对话中推理时间和空间维度，使用复杂工具集，并主动澄清模糊指令。我们的评估显示，即使是最先进的模型在跨场景任务上的成功率也仅为30%，这表明VitaBench将推动AI代理在实际应用中的发展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.26488",
            "title": "dParallel: Learnable Parallel Decoding for dLLMs",
            "url": "https://huggingface.co/papers/2509.26488",
            "abstract": "dParallel is a method that enhances the parallel decoding of diffusion large language models, significantly reducing decoding steps without compromising performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion large language models (dLLMs) have recently drawn considerable attention within the research community as a promising alternative to autoregressive generation, offering parallel token prediction and lower inference latency. Yet, their parallel decoding potential remains largely underexplored, as existing open-source models still require nearly token-length decoding steps to ensure performance. To address this, we introduce dParallel, a simple and effective method that unlocks the inherent parallelism of dLLMs for fast sampling. We identify that the key bottleneck to parallel decoding arises from the sequential certainty convergence for masked tokens. Building on this insight, we introduce the core of our approach: certainty-forcing distillation, a novel training strategy that distills the model to follow its original sampling trajectories while enforcing it to achieve high certainty on masked tokens more rapidly and in parallel. Extensive experiments across various benchmarks demonstrate that our method can dramatically reduce the number of decoding steps while maintaining performance. When applied to the LLaDA-8B-Instruct model, dParallel reduces decoding steps from 256 to 30 on GSM8K, achieving an 8.5x speedup without performance degradation. On the MBPP benchmark, it cuts decoding steps from 256 to 24, resulting in a 10.5x speedup while maintaining accuracy. Our code is available at https://github.com/czg1225/dParallel",
            "score": 4,
            "issue_id": 6175,
            "pub_date": "2025-09-30",
            "pub_date_card": {
                "ru": "30 сентября",
                "en": "September 30",
                "zh": "9月30日"
            },
            "hash": "50b8e2e379343971",
            "authors": [
                "Zigeng Chen",
                "Gongfan Fang",
                "Xinyin Ma",
                "Ruonan Yu",
                "Xinchao Wang"
            ],
            "affiliations": [
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.26488.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#optimization",
                    "#benchmark",
                    "#diffusion",
                    "#open_source",
                    "#training"
                ],
                "emoji": "⚡",
                "ru": {
                    "title": "Параллельное декодирование диффузионных LLM с 10-кратным ускорением",
                    "desc": "Статья представляет dParallel — метод для ускорения параллельного декодирования в диффузионных языковых моделях (dLLMs). Авторы обнаружили, что основное узкое место при параллельном декодировании связано с последовательной сходимостью уверенности для замаскированных токенов. Предложенная техника certainty-forcing distillation обучает модель быстрее достигать высокой уверенности при предсказании токенов параллельно. В результате метод сокращает количество шагов декодирования с 256 до 24-30 на бенчмарках GSM8K и MBPP, обеспечивая ускорение в 8-10 раз без потери качества."
                },
                "en": {
                    "title": "Unlocking Fast Parallel Decoding in Diffusion Models",
                    "desc": "dParallel is a novel method designed to improve the efficiency of parallel decoding in diffusion large language models (dLLMs). It addresses the challenge of sequential certainty convergence for masked tokens, which has limited the speed of parallel decoding. By introducing certainty-forcing distillation, dParallel trains the model to quickly achieve high certainty on masked tokens while maintaining its original sampling paths. Experimental results show that dParallel significantly reduces decoding steps, achieving up to 10.5 times faster inference without sacrificing performance."
                },
                "zh": {
                    "title": "dParallel：加速扩散模型的并行解码",
                    "desc": "dParallel是一种增强扩散大语言模型（dLLMs）并行解码的方法，显著减少了解码步骤而不影响性能。该方法利用了dLLMs的并行性，解决了现有模型在解码时需要接近令牌长度的步骤的问题。通过引入确定性强制蒸馏的训练策略，dParallel能够更快地并行处理被遮蔽的令牌。实验结果表明，dParallel在多个基准测试中显著减少了解码步骤，同时保持了模型的准确性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.26628",
            "title": "Attention as a Compass: Efficient Exploration for Process-Supervised RL\n  in Reasoning Models",
            "url": "https://huggingface.co/papers/2509.26628",
            "abstract": "A novel PSRL framework (AttnRL) enhances exploration efficiency in reasoning models by branching from high attention positions and using an adaptive sampling strategy, outperforming prior methods in mathematical reasoning benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning (RL) has shown remarkable success in enhancing the reasoning capabilities of Large Language Models (LLMs). Process-Supervised RL (PSRL) has emerged as a more effective paradigm compared to outcome-based RL. However, existing PSRL approaches suffer from limited exploration efficiency, both in terms of branching positions and sampling. In this paper, we introduce a novel PSRL framework (AttnRL), which enables efficient exploration for reasoning models. Motivated by preliminary observations that steps exhibiting high attention scores correlate with reasoning behaviors, we propose to branch from positions with high values. Furthermore, we develop an adaptive sampling strategy that accounts for problem difficulty and historical batch size, ensuring that the whole training batch maintains non-zero advantage values. To further improve sampling efficiency, we design a one-step off-policy training pipeline for PSRL. Extensive experiments on multiple challenging mathematical reasoning benchmarks demonstrate that our method consistently outperforms prior approaches in terms of performance and sampling and training efficiency.",
            "score": 3,
            "issue_id": 6175,
            "pub_date": "2025-09-30",
            "pub_date_card": {
                "ru": "30 сентября",
                "en": "September 30",
                "zh": "9月30日"
            },
            "hash": "ac8005b1bfc91f64",
            "authors": [
                "Runze Liu",
                "Jiakang Wang",
                "Yuling Shi",
                "Zhihui Xie",
                "Chenxin An",
                "Kaiyan Zhang",
                "Jian Zhao",
                "Xiaodong Gu",
                "Lei Lin",
                "Wenping Hu",
                "Xiu Li",
                "Fuzheng Zhang",
                "Guorui Zhou",
                "Kun Gai"
            ],
            "affiliations": [
                "Beijing University of Posts and Telecommunications",
                "Kuaishou Technology",
                "Shanghai Jiao Tong University",
                "The University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.26628.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#optimization",
                    "#reasoning",
                    "#training",
                    "#rl"
                ],
                "emoji": "🌳",
                "ru": {
                    "title": "Умное ветвление через внимание для обучения рассуждениям",
                    "desc": "Исследователи предложили новый подход AttnRL для улучшения Process-Supervised Reinforcement Learning при обучении LLM математическим рассуждениям. Ключевая идея заключается в том, чтобы создавать ветвления решения в позициях с высокими значениями attention scores, которые коррелируют с важными шагами рассуждения. Метод использует адаптивную стратегию сэмплирования, учитывающую сложность задачи, и применяет one-step off-policy обучение для повышения эффективности. Эксперименты показывают значительное превосходство над предыдущими методами как по качеству решений, так и по эффективности обучения."
                },
                "en": {
                    "title": "Enhancing Reasoning with Efficient Exploration in AttnRL",
                    "desc": "The paper presents a new framework called AttnRL that improves exploration efficiency in reasoning models using Process-Supervised Reinforcement Learning (PSRL). It focuses on branching from positions in the model that have high attention scores, which are linked to better reasoning performance. Additionally, the authors introduce an adaptive sampling strategy that adjusts based on the difficulty of problems and the size of previous training batches. Experiments show that AttnRL outperforms existing methods in mathematical reasoning tasks, enhancing both performance and training efficiency."
                },
                "zh": {
                    "title": "提升推理模型探索效率的新框架",
                    "desc": "本文提出了一种新的过程监督强化学习框架（AttnRL），旨在提高推理模型的探索效率。该框架通过从高注意力位置分支，并采用自适应采样策略，克服了现有方法在数学推理基准测试中的局限性。研究表明，高注意力分数的步骤与推理行为相关，因此我们选择从这些位置进行分支。此外，我们设计了一种一步离线策略训练管道，以进一步提高采样效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.26625",
            "title": "Learning to See Before Seeing: Demystifying LLM Visual Priors from\n  Language Pre-training",
            "url": "https://huggingface.co/papers/2509.26625",
            "abstract": "LLMs develop visual priors during language pre-training, which can be leveraged for vision tasks with minimal additional data, and these priors are composed of separable perception and reasoning components.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs), despite being trained on text alone, surprisingly develop rich visual priors. These priors allow latent visual capabilities to be unlocked for vision tasks with a relatively small amount of multimodal data, and in some cases, to perform visual tasks without ever having seen an image. Through systematic analysis, we reveal that visual priors-the implicit, emergent knowledge about the visual world acquired during language pre-training-are composed of separable perception and reasoning priors with unique scaling trends and origins. We show that an LLM's latent visual reasoning ability is predominantly developed by pre-training on reasoning-centric data (e.g., code, math, academia) and scales progressively. This reasoning prior acquired from language pre-training is transferable and universally applicable to visual reasoning. In contrast, a perception prior emerges more diffusely from broad corpora, and perception ability is more sensitive to the vision encoder and visual instruction tuning data. In parallel, text describing the visual world proves crucial, though its performance impact saturates rapidly. Leveraging these insights, we propose a data-centric recipe for pre-training vision-aware LLMs and verify it in 1T token scale pre-training. Our findings are grounded in over 100 controlled experiments consuming 500,000 GPU-hours, spanning the full MLLM construction pipeline-from LLM pre-training to visual alignment and supervised multimodal fine-tuning-across five model scales, a wide range of data categories and mixtures, and multiple adaptation setups. Along with our main findings, we propose and investigate several hypotheses, and introduce the Multi-Level Existence Bench (MLE-Bench). Together, this work provides a new way of deliberately cultivating visual priors from language pre-training, paving the way for the next generation of multimodal LLMs.",
            "score": 2,
            "issue_id": 6175,
            "pub_date": "2025-09-30",
            "pub_date_card": {
                "ru": "30 сентября",
                "en": "September 30",
                "zh": "9月30日"
            },
            "hash": "4ecd034d7f6a8060",
            "authors": [
                "Junlin Han",
                "Shengbang Tong",
                "David Fan",
                "Yufan Ren",
                "Koustuv Sinha",
                "Philip Torr",
                "Filippos Kokkinos"
            ],
            "affiliations": [
                "Meta Superintelligence Labs",
                "University of Oxford"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.26625.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#reasoning",
                    "#benchmark",
                    "#alignment",
                    "#dataset",
                    "#transfer_learning"
                ],
                "emoji": "👁️",
                "ru": {
                    "title": "Визуальные приоры из текста: как LLM учатся видеть без изображений",
                    "desc": "Исследование показывает, что большие языковые модели (LLM) неожиданно развивают визуальные представления во время обучения только на текстовых данных. Эти визуальные приоры состоят из двух отдельных компонентов: перцептивного и рассуждающего, которые имеют разные источники происхождения и закономерности масштабирования. Способность к визуальным рассуждениям формируется преимущественно на данных с кодом, математикой и научными текстами, в то время как перцептивные способности возникают из более широкого корпуса и зависят от vision encoder. На основе более 100 контролируемых экспериментов авторы предлагают рецепт предобучения мультимодальных LLM и вводят новый бенчмарк MLE-Bench для оценки визуальных способностей."
                },
                "en": {
                    "title": "Unlocking Visual Understanding in Language Models",
                    "desc": "This paper explores how Large Language Models (LLMs) can develop visual understanding during their training on text data alone. It reveals that these models create visual priors, which are essential for performing vision tasks with minimal additional data. The study identifies two main components of these priors: perception and reasoning, each with distinct characteristics and scaling behaviors. By analyzing extensive experiments, the authors propose a method for enhancing LLMs with visual capabilities, setting a foundation for future multimodal AI systems."
                },
                "zh": {
                    "title": "从语言预训练中培养视觉先验的全新方法",
                    "desc": "大型语言模型（LLMs）在语言预训练过程中意外地发展出丰富的视觉先验。这些视觉先验使得在视觉任务中能够以相对较少的多模态数据解锁潜在的视觉能力。研究表明，视觉先验由可分离的感知和推理组件组成，且这两者在规模和来源上具有独特的趋势。通过系统分析，我们提出了一种以数据为中心的预训练方法，旨在培养视觉感知能力，从而推动下一代多模态LLMs的发展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.25760",
            "title": "TruthRL: Incentivizing Truthful LLMs via Reinforcement Learning",
            "url": "https://huggingface.co/papers/2509.25760",
            "abstract": "TruthRL, a reinforcement learning framework, enhances the truthfulness of large language models by balancing accuracy and abstention, significantly reducing hallucinations and improving performance across benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t While large language models (LLMs) have demonstrated strong performance on factoid question answering, they are still prone to hallucination and untruthful responses, particularly when tasks demand information outside their parametric knowledge. Indeed, truthfulness requires more than accuracy -- models must also recognize uncertainty and abstain when unsure to avoid hallucinations. This presents a fundamental challenge for existing methods: approaches that optimize for accuracy often amplify hallucinations, while those that encourage abstention can become overly conservative, sacrificing correct answers. Both extremes ultimately compromise truthfulness. In this work, we present TruthRL, a general reinforcement learning (RL) framework that directly optimizes the truthfulness of LLMs. Specifically, we implement TruthRL using GRPO with a simple yet effective ternary reward that distinguishes correct answers, hallucinations, and abstentions. It incentivizes models to reduce hallucinations not only by providing correct responses, but also by enabling abstention when uncertain, thereby improving truthfulness. Extensive experiments across four knowledge-intensive benchmarks show that, compared to vanilla RL, TruthRL significantly reduces hallucinations by 28.9% and improves truthfulness by 21.1%, with consistent gains across various backbone models (e.g., Qwen, Llama) under both retrieval and non-retrieval setups. In-depth ablation study demonstrates that vanilla accuracy-driven methods, such as supervised fine-tuning or RL with a binary reward, struggle to balance factual correctness and uncertainty. In contrast, our proposed truthfulness-driven TruthRL achieves strong performance in both accuracy and truthfulness, underscoring the importance of learning objective design for developing truthful LLMs.",
            "score": 2,
            "issue_id": 6175,
            "pub_date": "2025-09-30",
            "pub_date_card": {
                "ru": "30 сентября",
                "en": "September 30",
                "zh": "9月30日"
            },
            "hash": "58cf56a1a824c556",
            "authors": [
                "Zhepei Wei",
                "Xiao Yang",
                "Kai Sun",
                "Jiaqi Wang",
                "Rulin Shao",
                "Sean Chen",
                "Mohammad Kachuee",
                "Teja Gollapudi",
                "Tony Liao",
                "Nicolas Scheffer",
                "Rakesh Wanga",
                "Anuj Kumar",
                "Yu Meng",
                "Wen-tau Yih",
                "Xin Luna Dong"
            ],
            "affiliations": [
                "FAIR at Meta",
                "Meta Reality Labs",
                "University of Virginia",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.25760.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#hallucinations",
                    "#reasoning",
                    "#optimization",
                    "#training",
                    "#rl"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Обучение LLM говорить правду через воздержание от ответа",
                    "desc": "В статье представлен TruthRL — фреймворк на основе reinforcement learning для повышения правдивости больших языковых моделей. Ключевая идея заключается в использовании тернарной системы наград, которая различает правильные ответы, галлюцинации и воздержание от ответа. Традиционные методы оптимизации точности часто усиливают галлюцинации, а методы, поощряющие воздержание, становятся чрезмерно консервативными. TruthRL достигает баланса между точностью и способностью признавать неопределённость, снижая галлюцинации на 28.9% и улучшая правдивость на 21.1% на различных бенчмарках."
                },
                "en": {
                    "title": "TruthRL: Balancing Accuracy and Abstention for Truthful AI",
                    "desc": "TruthRL is a novel reinforcement learning framework designed to enhance the truthfulness of large language models (LLMs) by effectively balancing accuracy and the ability to abstain from answering when uncertain. Traditional methods often lead to increased hallucinations or overly conservative responses, compromising the model's truthfulness. TruthRL addresses this by using a ternary reward system that differentiates between correct answers, hallucinations, and abstentions, encouraging models to provide accurate responses while also recognizing when to refrain from answering. Experimental results show that TruthRL significantly reduces hallucinations and improves overall truthfulness across various benchmarks and model architectures."
                },
                "zh": {
                    "title": "TruthRL：提升语言模型真实性的强化学习框架",
                    "desc": "TruthRL是一种强化学习框架，旨在提高大型语言模型的真实性。它通过平衡准确性和放弃来显著减少幻觉现象，并在多个基准测试中提升性能。该框架使用简单有效的三元奖励机制，鼓励模型在不确定时选择放弃，从而避免错误回答。实验结果表明，TruthRL相比传统的强化学习方法，减少了28.9%的幻觉现象，并提高了21.1%的真实性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.25758",
            "title": "Thinking Sparks!: Emergent Attention Heads in Reasoning Models During\n  Post Training",
            "url": "https://huggingface.co/papers/2509.25758",
            "abstract": "Post-training techniques like supervised fine-tuning and reinforcement learning lead to the emergence of specialized attention heads that support structured reasoning, with different training regimes affecting their evolution and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t The remarkable capabilities of modern large reasoning models are largely unlocked through post-training techniques such as supervised fine-tuning and reinforcement learning. However, the architectural mechanisms behind such improvements remain largely opaque. In this work, we use circuit analysis to demonstrate that post-training for complex reasoning sparks the emergence of novel, functionally specialized attention heads. These heads collectively support structured reasoning and computation. Our comparative analysis across Qwen families and DeepSeek-distilled model reveals that these emergent heads evolve differently under different training regimes. Distillation and SFT foster a cumulative addition of stable reasoning heads. In contrast, group relative policy optimization operates in a dynamic search mode: relatively few attention heads are iteratively activated, evaluated, and pruned, with their survival closely tracking fluctuations in the task reward signal. Furthermore, we find that controllable think on/off models do not possess dedicated thinking heads. Instead, turning off explicit reasoning triggers a broader-but less efficient-set of compensatory heads. Through ablation and qualitative analyses, we connect these circuit-level dynamics to a crucial performance trade-off: strengthened heads enable sophisticated problem-solving strategies for difficult problems but can also introduce over-thinking failure modes, such as calculation errors or logical loops on simpler tasks. These findings connect circuit-level dynamics to macro-level performance, identifying an inherent tension where complex reasoning comes at the cost of elementary computations. More broadly, our work points to future directions for training policy design, emphasizing the need to balance the development of effective reasoning strategies with the assurance of reliable, flawless execution.",
            "score": 2,
            "issue_id": 6175,
            "pub_date": "2025-09-30",
            "pub_date_card": {
                "ru": "30 сентября",
                "en": "September 30",
                "zh": "9月30日"
            },
            "hash": "f7f61c1e3b1bdf7d",
            "authors": [
                "Yein Park",
                "Minbyul Jeong",
                "Jaewoo Kang"
            ],
            "affiliations": [
                "AIGEN Sciences",
                "Korea University",
                "Upstage AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.25758.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#optimization",
                    "#reasoning",
                    "#architecture",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Специализированные головы внимания: ключ к рассуждениям LLM",
                    "desc": "Исследование показывает, что post-training техники, такие как supervised fine-tuning и reinforcement learning, приводят к появлению специализированных attention heads, которые поддерживают структурированное рассуждение в больших языковых моделях. Разные методы обучения влияют на эволюцию этих голов по-разному: distillation и SFT создают стабильные reasoning heads, в то время как group relative policy optimization работает в режиме динамического поиска с итеративной активацией и отбором голов. Модели с возможностью включения и выключения явного рассуждения не имеют выделенных thinking heads, а вместо этого активируют более широкий, но менее эффективный набор компенсаторных механизмов. Анализ выявляет важный компромисс: усиленные головы внимания улучшают решение сложных задач, но могут приводить к ошибкам на простых задачах из-за избыточного рассуждения."
                },
                "en": {
                    "title": "Unlocking Reasoning: The Power of Specialized Attention Heads",
                    "desc": "This paper explores how post-training techniques like supervised fine-tuning and reinforcement learning enhance the performance of large reasoning models. It reveals that these techniques lead to the emergence of specialized attention heads that facilitate structured reasoning. The study shows that different training methods influence the evolution and effectiveness of these attention heads, with some fostering stable reasoning capabilities while others operate in a dynamic, adaptive manner. Ultimately, the research highlights a trade-off between advanced reasoning abilities and the risk of errors in simpler tasks, suggesting a need for careful design in training policies."
                },
                "zh": {
                    "title": "后训练技术助力结构化推理的演变",
                    "desc": "本研究探讨了后训练技术如何促进专门化注意力头的出现，这些注意力头支持结构化推理。通过电路分析，我们发现不同的训练方式会影响这些注意力头的演变和性能。特别是，蒸馏和监督微调促进了稳定推理头的累积，而相对策略优化则在动态搜索模式下工作。我们的研究揭示了复杂推理与基本计算之间的内在张力，强调了在训练策略设计中平衡有效推理与可靠执行的重要性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.23610",
            "title": "Efficient Audio-Visual Speech Separation with Discrete Lip Semantics and\n  Multi-Scale Global-Local Attention",
            "url": "https://huggingface.co/papers/2509.23610",
            "abstract": "Dolphin, an efficient AVSS method, uses a dual-path lightweight video encoder and a lightweight encoder-decoder separator with global-local attention blocks to achieve high separation quality and significant computational efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Audio-visual speech separation (AVSS) methods leverage visual cues to extract target speech and have demonstrated strong separation quality in noisy acoustic environments. However, these methods usually involve a large number of parameters and require high computational cost, which is unacceptable in many applications where speech separation serves as only a preprocessing step for further speech processing. To address this issue, we propose an efficient AVSS method, named Dolphin. For visual feature extraction, we develop DP-LipCoder, a dual-path lightweight video encoder that transforms lip-motion into discrete audio-aligned semantic tokens. For audio separation, we construct a lightweight encoder-decoder separator, in which each layer incorporates a global-local attention (GLA) block to efficiently capture multi-scale dependencies. Experiments on three benchmark datasets showed that Dolphin not only surpassed the current state-of-the-art (SOTA) model in separation quality but also achieved remarkable improvements in efficiency: over 50% fewer parameters, more than 2.4x reduction in MACs, and over 6x faster GPU inference speed. These results indicate that Dolphin offers a practical and deployable solution for high-performance AVSS in real-world scenarios. Our code and demo page are publicly available at http://cslikai.cn/Dolphin/.",
            "score": 2,
            "issue_id": 6175,
            "pub_date": "2025-09-28",
            "pub_date_card": {
                "ru": "28 сентября",
                "en": "September 28",
                "zh": "9月28日"
            },
            "hash": "d324759166979416",
            "authors": [
                "Kai Li",
                "Kejun Gao",
                "Xiaolin Hu"
            ],
            "affiliations": [
                "Chinese Institute for Brain Research (CIBR), Beijing 100010, China",
                "Department of Computer Science and Technology, Institute for AI, BNRist, Tsinghua University, Beijing 100084, China",
                "Tsinghua Laboratory of Brain and Intelligence (THBI), IDG/McGovern Institute for Brain Research, Tsinghua University, Beijing 100084, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.23610.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#benchmark",
                    "#audio",
                    "#inference"
                ],
                "emoji": "🐬",
                "ru": {
                    "title": "Dolphin: Быстрая и эффективная сепарация речи с помощью визуальных подсказок",
                    "desc": "Dolphin - это эффективный метод аудио-визуальной сепарации речи (AVSS), который использует визуальные подсказки для извлечения целевой речи из зашумленной акустической среды. Метод включает двухпутевой легковесный видео-энкодер DP-LipCoder, преобразующий движения губ в дискретные аудио-выровненные семантические токены, и легковесный encoder-decoder сепаратор с блоками глобально-локального внимания для эффективного захвата многомасштабных зависимостей. Dolphin превосходит современные SOTA модели по качеству сепарации, при этом имея на 50% меньше параметров, в 2.4 раза меньше вычислительных операций и в 6 раз более высокую скорость inference на GPU. Это делает метод практичным решением для реального применения, где сепарация речи является лишь этапом предобработки для дальнейшей обработки аудио."
                },
                "en": {
                    "title": "Dolphin: Efficient AVSS with Dual-Path Encoding and Global-Local Attention",
                    "desc": "Dolphin is a novel audio-visual speech separation (AVSS) method that enhances speech extraction by utilizing visual cues from lip movements. It features a dual-path lightweight video encoder called DP-LipCoder, which converts lip motion into audio-aligned semantic tokens, improving the quality of speech separation. Additionally, Dolphin employs a lightweight encoder-decoder architecture with global-local attention blocks to efficiently manage multi-scale dependencies while significantly reducing computational costs. Experimental results demonstrate that Dolphin outperforms existing state-of-the-art models in both separation quality and efficiency, making it suitable for practical applications in noisy environments."
                },
                "zh": {
                    "title": "Dolphin：高效的音视频语音分离新方法",
                    "desc": "Dolphin是一种高效的音视频语音分离（AVSS）方法，采用双路径轻量级视频编码器和轻量级编码-解码分离器，结合全局-局部注意力模块，以实现高质量的分离效果和显著的计算效率。该方法通过DP-LipCoder提取视觉特征，将唇部运动转化为与音频对齐的语义标记。实验结果表明，Dolphin在分离质量上超越了当前的最先进模型，同时在参数数量上减少了50%以上，MACs减少了2.4倍，GPU推理速度提高了6倍以上。Dolphin为实际应用中的高性能音视频语音分离提供了一个可行的解决方案。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.26542",
            "title": "Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced\n  Performance Gap",
            "url": "https://huggingface.co/papers/2509.26542",
            "abstract": "VERA is a benchmark for evaluating reasoning ability in voice-interactive systems, revealing significant performance gaps compared to text models and highlighting challenges in real-time interaction.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Voice Evaluation of Reasoning Ability (VERA), a benchmark for evaluating reasoning ability in voice-interactive systems under real-time conversational constraints. VERA comprises 2,931 voice-native episodes derived from established text benchmarks and organized into five tracks (Math, Web, Science, Long-Context, Factual). Each item is adapted for speech interaction while preserving reasoning difficulty. VERA enables direct text-voice comparison within model families and supports analysis of how architectural choices affect reliability. We assess 12 contemporary voice systems alongside strong text baselines and observe large, consistent modality gaps: on competition mathematics a leading text model attains 74.8% accuracy while its voice counterpart reaches 6.1%; macro-averaged across tracks the best text models achieve 54.0% versus 11.3% for voice. Latency-accuracy analyses reveal a low-latency plateau, where fast voice systems cluster around ~10% accuracy, while approaching text performance requires sacrificing real-time interaction. Diagnostic experiments indicate that common mitigations are insufficient. Increasing \"thinking time\" yields negligible gains; a decoupled cascade that separates reasoning from narration improves accuracy but still falls well short of text and introduces characteristic grounding/consistency errors. Failure analyses further show distinct error signatures across native streaming, end-to-end, and cascade designs. VERA provides a reproducible testbed and targeted diagnostics for architectures that decouple thinking from speaking, offering a principled way to measure progress toward real-time voice assistants that are both fluent and reliably reasoned.",
            "score": 1,
            "issue_id": 6175,
            "pub_date": "2025-09-30",
            "pub_date_card": {
                "ru": "30 сентября",
                "en": "September 30",
                "zh": "9月30日"
            },
            "hash": "94e739b649ffcc6e",
            "authors": [
                "Yueqian Lin",
                "Zhengmian Hu",
                "Qinsi Wang",
                "Yudong Liu",
                "Hengfan Zhang",
                "Jayakumar Subramanian",
                "Nikos Vlassis",
                "Hai Helen Li",
                "Yiran Chen"
            ],
            "affiliations": [
                "Adobe, San Jose, CA, USA",
                "Duke University, Durham, NC, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.26542.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#reasoning",
                    "#benchmark",
                    "#architecture",
                    "#long_context"
                ],
                "emoji": "🎤",
                "ru": {
                    "title": "Голосовые AI-помощники сильно отстают в способности рассуждать",
                    "desc": "VERA — это бенчмарк для оценки способности к рассуждению в голосовых интерактивных системах в условиях реального времени. Исследование показывает огромный разрыв в производительности между текстовыми и голосовыми моделями: на математических задачах текстовая модель достигает 74.8% точности, а её голосовая версия — всего 6.1%. Увеличение времени на «размышление» почти не помогает, а отделение процесса рассуждения от озвучивания улучшает результаты, но всё равно сильно уступает тексту. Бенчмарк включает 2,931 голосовой эпизод по пяти категориям и позволяет систематически изучать, как архитектурные решения влияют на надёжность голосовых ассистентов."
                },
                "en": {
                    "title": "Bridging the Gap: Evaluating Voice Reasoning with VERA",
                    "desc": "VERA is a benchmark designed to evaluate the reasoning capabilities of voice-interactive systems, highlighting the performance differences between voice and text models. It includes 2,931 voice-native episodes adapted from existing text benchmarks, organized into five distinct tracks. The study reveals significant accuracy gaps, with text models outperforming voice models in reasoning tasks, particularly in mathematics and factual contexts. VERA serves as a tool for analyzing how different architectural choices impact the reliability of voice systems, aiming to improve real-time interaction without sacrificing reasoning quality."
                },
                "zh": {
                    "title": "VERA：语音交互推理能力的评估基准",
                    "desc": "VERA是一个用于评估语音交互系统推理能力的基准，揭示了与文本模型相比的显著性能差距，并强调了实时交互中的挑战。该基准包含2931个语音原生的案例，涵盖数学、网络、科学、长上下文和事实五个领域，适应语音交互的同时保持推理难度。通过对12个现代语音系统与强大的文本基线进行评估，发现语音系统在准确性上存在较大的差距。VERA为解耦思考与表达的架构提供了可重复的测试平台和针对性的诊断，帮助衡量实时语音助手在流畅性和可靠推理方面的进展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.26539",
            "title": "Ferret-UI Lite: Lessons from Building Small On-Device GUI Agents",
            "url": "https://huggingface.co/papers/2509.26539",
            "abstract": "Ferret-UI Lite, a compact end-to-end GUI agent, achieves competitive performance across diverse platforms using chain-of-thought reasoning, visual tool-use, and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Developing autonomous agents that effectively interact with Graphic User Interfaces (GUIs) remains a challenging open problem, especially for small on-device models. In this paper, we present Ferret-UI Lite, a compact, end-to-end GUI agent that operates across diverse platforms, including mobile, web, and desktop. Utilizing techniques optimized for developing small models, we build our 3B Ferret-UI Lite agent through curating a diverse GUI data mixture from real and synthetic sources, strengthening inference-time performance through chain-of-thought reasoning and visual tool-use, and reinforcement learning with designed rewards. Ferret-UI Lite achieves competitive performance with other small-scale GUI agents. In GUI grounding, Ferret-UI Lite attains scores of 91.6%, 53.3%, and 61.2% on the ScreenSpot-V2, ScreenSpot-Pro, and OSWorld-G benchmarks, respectively. For GUI navigation, Ferret-UI Lite achieves success rates of 28.0% on AndroidWorld and 19.8% on OSWorld. We share our methods and lessons learned from developing compact, on-device GUI agents.",
            "score": 1,
            "issue_id": 6175,
            "pub_date": "2025-09-30",
            "pub_date_card": {
                "ru": "30 сентября",
                "en": "September 30",
                "zh": "9月30日"
            },
            "hash": "4f42c990da9b21fe",
            "authors": [
                "Zhen Yang",
                "Zi-Yi Dou",
                "Di Feng",
                "Forrest Huang",
                "Anh Nguyen",
                "Keen You",
                "Omar Attia",
                "Yuhao Yang",
                "Michael Feng",
                "Haotian Zhang",
                "Ram Ramrakhya",
                "Chao Jia",
                "Jeffrey Nichols",
                "Alexander Toshev",
                "Yinfei Yang",
                "Zhe Gan"
            ],
            "affiliations": [
                "Apple"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.26539.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#agents",
                    "#reasoning",
                    "#small_models",
                    "#synthetic",
                    "#data",
                    "#dataset",
                    "#rl"
                ],
                "emoji": "📱",
                "ru": {
                    "title": "Компактный AI-агент для управления интерфейсами на устройстве",
                    "desc": "Ferret-UI Lite — это компактная модель размером 3B параметров для автономного взаимодействия с графическими интерфейсами на мобильных, веб и десктоп платформах. Модель обучена на смеси реальных и синтетических данных с использованием chain-of-thought рассуждений, визуальных инструментов и reinforcement learning со специально разработанными наградами. На бенчмарках для определения элементов интерфейса модель достигает точности до 91.6% на ScreenSpot-V2, а для навигации показывает успешность 28% на AndroidWorld. Это демонстрирует возможность создания эффективных GUI-агентов малого размера для работы непосредственно на устройствах пользователей."
                },
                "en": {
                    "title": "Compact GUI Agent with Competitive Performance",
                    "desc": "Ferret-UI Lite is a small, end-to-end agent designed to interact with Graphic User Interfaces (GUIs) across various platforms like mobile and desktop. It employs chain-of-thought reasoning and visual tool-use to enhance its performance, making it effective even with limited resources. The agent is trained using a mix of real and synthetic GUI data, and it utilizes reinforcement learning to optimize its actions based on specific rewards. Overall, Ferret-UI Lite demonstrates competitive results compared to other small-scale GUI agents, showcasing its potential for on-device applications."
                },
                "zh": {
                    "title": "紧凑高效的GUI代理：Ferret-UI Lite",
                    "desc": "Ferret-UI Lite 是一种紧凑的端到端图形用户界面（GUI）代理，能够在多种平台上实现竞争力的性能。该模型采用了链式思维推理、视觉工具使用和强化学习等技术，专为小型设备优化。通过从真实和合成来源中策划多样化的GUI数据，Ferret-UI Lite 在推理时的表现得到了增强。实验结果显示，Ferret-UI Lite 在多个基准测试中表现优异，成功率与其他小型GUI代理相当。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.25541",
            "title": "Vision-Zero: Scalable VLM Self-Improvement via Strategic Gamified\n  Self-Play",
            "url": "https://huggingface.co/papers/2509.25541",
            "abstract": "Vision-Zero is a domain-agnostic framework that enhances vision-language models through self-improvement in competitive visual games, using Iterative Self-Play Policy Optimization and achieving state-of-the-art performance without human annotation.  \t\t\t\t\tAI-generated summary \t\t\t\t Although reinforcement learning (RL) can effectively enhance the reasoning capabilities of vision-language models (VLMs), current methods remain heavily dependent on labor-intensive datasets that require extensive manual construction and verification, leading to extremely high training costs and consequently constraining the practical deployment of VLMs. To address this challenge, we propose Vision-Zero, a domain-agnostic framework enabling VLM self-improvement through competitive visual games generated from arbitrary image pairs. Specifically, Vision-Zero encompasses three main attributes: (1) Strategic Self-Play Framework: Vision-Zero trains VLMs in \"Who Is the Spy\"-style games, where the models engage in strategic reasoning and actions across multiple roles. Through interactive gameplay, models autonomously generate their training data without human annotation. (2) Gameplay from Arbitrary Images: Unlike existing gamified frameworks, Vision-Zero can generate games from arbitrary images, thereby enhancing the model's reasoning ability across diverse domains and showing strong generalization to different tasks. We demonstrate this versatility using three distinct types of image datasets: CLEVR-based synthetic scenes, charts, and real-world images. (3) Sustainable Performance Gain: We introduce Iterative Self-Play Policy Optimization (Iterative-SPO), a novel training algorithm that alternates between Self-Play and reinforcement learning with verifiable rewards (RLVR), mitigating the performance plateau often seen in self-play-only training and achieving sustained long-term improvements. Despite using label-free data, Vision-Zero achieves state-of-the-art performance on reasoning, chart question answering, and vision-centric understanding tasks, surpassing other annotation-based methods. Models and code has been released at https://github.com/wangqinsi1/Vision-Zero.",
            "score": 1,
            "issue_id": 6175,
            "pub_date": "2025-09-29",
            "pub_date_card": {
                "ru": "29 сентября",
                "en": "September 29",
                "zh": "9月29日"
            },
            "hash": "1e4232d439e827c1",
            "authors": [
                "Qinsi Wang",
                "Bo Liu",
                "Tianyi Zhou",
                "Jing Shi",
                "Yueqian Lin",
                "Yiran Chen",
                "Hai Helen Li",
                "Kun Wan",
                "Wentian Zhao"
            ],
            "affiliations": [
                "Adobe Inc.",
                "Duke University",
                "National University of Singapore",
                "University of Maryland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.25541.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#reasoning",
                    "#optimization",
                    "#games",
                    "#cv",
                    "#training",
                    "#rl"
                ],
                "emoji": "🎮",
                "ru": {
                    "title": "Самообучение VLM через визуальные игры без разметки",
                    "desc": "Vision-Zero — это фреймворк для самосовершенствования vision-language моделей через соревновательные визуальные игры, созданные из произвольных пар изображений. Модели обучаются играя в игры типа «Кто шпион», где они выполняют разные роли и генерируют тренировочные данные автоматически, без участия людей. Алгоритм Iterative Self-Play Policy Optimization чередует самоигру с reinforcement learning, что позволяет избежать плато в производительности и обеспечивает стабильный рост качества. Подход достигает state-of-the-art результатов на задачах reasoning, понимания графиков и визуального анализа, используя только данные без разметки."
                },
                "en": {
                    "title": "Empowering Vision-Language Models through Self-Play Games",
                    "desc": "Vision-Zero is a new framework that improves vision-language models (VLMs) by allowing them to learn from playing competitive visual games without needing human-created datasets. It uses a method called Iterative Self-Play Policy Optimization, which helps models generate their own training data through gameplay, enhancing their reasoning skills. The framework can create games from any image, making it versatile across different domains and tasks. As a result, Vision-Zero achieves top performance in various reasoning tasks while avoiding the high costs of manual data annotation."
                },
                "zh": {
                    "title": "Vision-Zero：无标注自我提升的视觉语言模型框架",
                    "desc": "Vision-Zero是一个领域无关的框架，通过在竞争性视觉游戏中自我提升，增强视觉语言模型（VLM）。该框架采用迭代自我游戏策略优化（Iterative-SPO），使模型能够在没有人工标注的情况下生成训练数据。Vision-Zero能够从任意图像对生成游戏，提升模型在不同领域的推理能力，并展示出强大的泛化能力。最终，Vision-Zero在推理、图表问答和视觉理解任务上达到了最先进的性能，超越了其他基于标注的方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.25339",
            "title": "VisualOverload: Probing Visual Understanding of VLMs in Really Dense\n  Scenes",
            "url": "https://huggingface.co/papers/2509.25339",
            "abstract": "VisualOverload is a VQA benchmark that challenges models with simple vision tasks in densely populated scenes, revealing gaps in current VLMs' performance and offering insights into their failure modes.  \t\t\t\t\tAI-generated summary \t\t\t\t Is basic visual understanding really solved in state-of-the-art VLMs? We present VisualOverload, a slightly different visual question answering (VQA) benchmark comprising 2,720 question-answer pairs, with privately held ground-truth responses. Unlike prior VQA datasets that typically focus on near global image understanding, VisualOverload challenges models to perform simple, knowledge-free vision tasks in densely populated (or, overloaded) scenes. Our dataset consists of high-resolution scans of public-domain paintings that are populated with multiple figures, actions, and unfolding subplots set against elaborately detailed backdrops. We manually annotated these images with questions across six task categories to probe for a thorough understanding of the scene. We hypothesize that current benchmarks overestimate the performance of VLMs, and encoding and reasoning over details is still a challenging task for them, especially if they are confronted with densely populated scenes. Indeed, we observe that even the best model (o3) out of 37 tested models only achieves 19.6% accuracy on our hardest test split and overall 69.5% accuracy on all questions. Beyond a thorough evaluation, we complement our benchmark with an error analysis that reveals multiple failure modes, including a lack of counting skills, failure in OCR, and striking logical inconsistencies under complex tasks. Altogether, VisualOverload exposes a critical gap in current vision models and offers a crucial resource for the community to develop better models.   Benchmark: http://paulgavrikov.github.io/visualoverload",
            "score": 1,
            "issue_id": 6175,
            "pub_date": "2025-09-29",
            "pub_date_card": {
                "ru": "29 сентября",
                "en": "September 29",
                "zh": "9月29日"
            },
            "hash": "be3aca0a0807cc94",
            "authors": [
                "Paul Gavrikov",
                "Wei Lin",
                "M. Jehanzeb Mirza",
                "Soumya Jahagirdar",
                "Muhammad Huzaifa",
                "Sivan Doveh",
                "Serena Yeung-Levy",
                "James Glass",
                "Hilde Kuehne"
            ],
            "affiliations": [
                "Independent Researcher",
                "JKU Linz",
                "MIT CSAIL",
                "MIT-IBM Watson AI Lab",
                "Stanford",
                "Tübingen AI Center"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.25339.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#reasoning",
                    "#benchmark",
                    "#dataset",
                    "#cv"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "VisualOverload: когда сложные сцены ставят VLM в тупик",
                    "desc": "Исследователи представили бенчмарк VisualOverload для оценки vision-language моделей на задачах визуального понимания в перегруженных деталями сценах. Датасет содержит 2720 вопросов к высокодетализированным изображениям классических картин с множеством персонажей и элементов. Даже лучшая модель (o3) достигает только 69.5% точности, а на самом сложном тестовом сплите — всего 19.6%, что выявляет критические проблемы современных VLM. Анализ ошибок показал слабости моделей в подсчёте объектов, распознавании текста (OCR) и логической последовательности при работе со сложными визуальными задачами."
                },
                "en": {
                    "title": "Unveiling Gaps in Visual Understanding with VisualOverload",
                    "desc": "VisualOverload is a new benchmark for visual question answering (VQA) that tests the capabilities of vision-language models (VLMs) in complex scenes filled with details. It includes 2,720 question-answer pairs based on high-resolution images of public-domain paintings, focusing on simple tasks that require understanding of crowded environments. The study reveals that existing VLMs struggle with basic visual comprehension, achieving only 19.6% accuracy on the most challenging questions. This benchmark not only highlights the limitations of current models but also provides a resource for improving their performance through targeted error analysis."
                },
                "zh": {
                    "title": "揭示视觉模型的关键缺陷",
                    "desc": "VisualOverload是一个视觉问答（VQA）基准，旨在通过密集场景中的简单视觉任务来挑战模型，揭示当前视觉语言模型（VLMs）性能的不足。该基准包含2720个问答对，主要关注在复杂背景下的图像理解能力。研究表明，现有基准可能高估了VLMs的性能，尤其是在处理密集场景时，模型在细节编码和推理方面仍面临挑战。通过错误分析，VisualOverload揭示了多个失败模式，包括计数能力不足、光学字符识别失败和复杂任务下的逻辑不一致性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.22613",
            "title": "Benefits and Pitfalls of Reinforcement Learning for Language Model\n  Planning: A Theoretical Perspective",
            "url": "https://huggingface.co/papers/2509.22613",
            "abstract": "Theoretical analysis of reinforcement learning methods in enhancing LLM planning reveals that while RL improves generalization through exploration, policy gradient suffers from diversity collapse, whereas Q-learning maintains diversity and requires careful reward design.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent reinforcement learning (RL) methods have substantially enhanced the planning capabilities of Large Language Models (LLMs), yet the theoretical basis for their effectiveness remains elusive. In this work, we investigate RL's benefits and limitations through a tractable graph-based abstraction, focusing on policy gradient (PG) and Q-learning methods. Our theoretical analyses reveal that supervised fine-tuning (SFT) may introduce co-occurrence-based spurious solutions, whereas RL achieves correct planning primarily through exploration, underscoring exploration's role in enabling better generalization. However, we also show that PG suffers from diversity collapse, where output diversity decreases during training and persists even after perfect accuracy is attained. By contrast, Q-learning provides two key advantages: off-policy learning and diversity preservation at convergence. We further demonstrate that careful reward design is necessary to prevent reward hacking in Q-learning. Finally, applying our framework to the real-world planning benchmark Blocksworld, we confirm that these behaviors manifest in practice.",
            "score": 1,
            "issue_id": 6175,
            "pub_date": "2025-09-26",
            "pub_date_card": {
                "ru": "26 сентября",
                "en": "September 26",
                "zh": "9月26日"
            },
            "hash": "8899a479ee5856c0",
            "authors": [
                "Siwei Wang",
                "Yifei Shen",
                "Haoran Sun",
                "Shi Feng",
                "Shang-Hua Teng",
                "Li Dong",
                "Yaru Hao",
                "Wei Chen"
            ],
            "affiliations": [
                "Harvard University",
                "Microsoft Research Asia",
                "Peking University",
                "University of Southern California"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.22613.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#benchmark",
                    "#training",
                    "#rl"
                ],
                "emoji": "🗺️",
                "ru": {
                    "title": "Почему Q-learning лучше policy gradient для планирования в LLM",
                    "desc": "Исследование теоретически анализирует, как методы reinforcement learning улучшают способности LLM к планированию на графовой абстракции задач. Supervised fine-tuning может приводить к ложным решениям на основе совместной встречаемости, тогда как RL достигает правильного планирования через исследование среды. Policy gradient методы страдают от коллапса разнообразия выходов во время обучения, в то время как Q-learning сохраняет разнообразие и позволяет учиться off-policy. Авторы также показывают важность тщательного дизайна функции награды для предотвращения reward hacking в Q-learning."
                },
                "en": {
                    "title": "Exploration Enhances Planning: Balancing Diversity in RL for LLMs",
                    "desc": "This paper analyzes how reinforcement learning (RL) methods can improve the planning abilities of Large Language Models (LLMs). It highlights that while RL enhances generalization through exploration, policy gradient methods face a problem called diversity collapse, where the variety of outputs decreases over time. In contrast, Q-learning maintains output diversity and allows for off-policy learning, but it requires careful design of rewards to avoid issues like reward hacking. The findings are validated through experiments on the Blocksworld planning benchmark, demonstrating the practical implications of these theoretical insights."
                },
                "zh": {
                    "title": "强化学习提升语言模型规划能力的理论分析",
                    "desc": "本研究分析了强化学习（RL）在提升大型语言模型（LLM）规划能力中的作用。我们发现，虽然RL通过探索提高了模型的泛化能力，但策略梯度方法在训练过程中会出现多样性崩溃的问题。相比之下，Q学习方法能够保持多样性，并且在收敛时具有离线学习的优势。我们还强调了奖励设计的重要性，以防止Q学习中的奖励操控问题。"
                }
            }
        }
    ],
    "link_prev": "2025-09-30.html",
    "link_next": "2025-10-02.html",
    "link_month": "2025-10.html",
    "short_date_prev": {
        "ru": "30.09",
        "en": "09/30",
        "zh": "9月30日"
    },
    "short_date_next": {
        "ru": "02.10",
        "en": "10/02",
        "zh": "10月2日"
    },
    "categories": {
        "#dataset": 3,
        "#data": 1,
        "#benchmark": 7,
        "#agents": 2,
        "#cv": 2,
        "#rl": 5,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 4,
        "#3d": 0,
        "#audio": 1,
        "#video": 2,
        "#multimodal": 3,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 7,
        "#robotics": 0,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 2,
        "#reasoning": 10,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 7,
        "#survey": 1,
        "#diffusion": 2,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    }
}