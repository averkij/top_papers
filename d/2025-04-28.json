{
    "date": {
        "ru": "28 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
        "en": "April 28",
        "zh": "4æœˆ28æ—¥"
    },
    "time_utc": "2025-04-28 02:29",
    "weekday": 0,
    "issue_id": 3457,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.15376",
            "title": "Towards Understanding Camera Motions in Any Video",
            "url": "https://huggingface.co/papers/2504.15376",
            "abstract": "We introduce CameraBench, a large-scale dataset and benchmark designed to assess and improve camera motion understanding. CameraBench consists of ~3,000 diverse internet videos, annotated by experts through a rigorous multi-stage quality control process. One of our contributions is a taxonomy of camera motion primitives, designed in collaboration with cinematographers. We find, for example, that some motions like \"follow\" (or tracking) require understanding scene content like moving subjects. We conduct a large-scale human study to quantify human annotation performance, revealing that domain expertise and tutorial-based training can significantly enhance accuracy. For example, a novice may confuse zoom-in (a change of intrinsics) with translating forward (a change of extrinsics), but can be trained to differentiate the two. Using CameraBench, we evaluate Structure-from-Motion (SfM) and Video-Language Models (VLMs), finding that SfM models struggle to capture semantic primitives that depend on scene content, while VLMs struggle to capture geometric primitives that require precise estimation of trajectories. We then fine-tune a generative VLM on CameraBench to achieve the best of both worlds and showcase its applications, including motion-augmented captioning, video question answering, and video-text retrieval. We hope our taxonomy, benchmark, and tutorials will drive future efforts towards the ultimate goal of understanding camera motions in any video.",
            "score": 67,
            "issue_id": 3457,
            "pub_date": "2025-04-21",
            "pub_date_card": {
                "ru": "21 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 21",
                "zh": "4æœˆ21æ—¥"
            },
            "hash": "fb835f3848fe977a",
            "authors": [
                "Zhiqiu Lin",
                "Siyuan Cen",
                "Daniel Jiang",
                "Jay Karhade",
                "Hewei Wang",
                "Chancharik Mitra",
                "Tiffany Ling",
                "Yuhan Huang",
                "Sifan Liu",
                "Mingyu Chen",
                "Rushikesh Zawar",
                "Xue Bai",
                "Yilun Du",
                "Chuang Gan",
                "Deva Ramanan"
            ],
            "affiliations": [
                "Adobe",
                "CMU",
                "Emerson",
                "Harvard",
                "MIT-IBM",
                "UMass Amherst",
                "USC"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.15376.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#cv",
                    "#multimodal"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "CameraBench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ¾ĞºĞ¾Ğ»Ğ¾ 3000 Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚-Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ğ¾Ğ² Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ñ ĞºĞ¸Ğ½Ğ¾Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ CameraBench, Ğ±Ñ‹Ğ»Ğ¸ Ğ¾Ñ†ĞµĞ½ĞµĞ½Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Structure-from-Motion (SfM) Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (VLM), Ğ²Ñ‹ÑĞ²Ğ¸Ğ² Ğ¸Ñ… ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ ÑĞ»Ğ°Ğ±Ñ‹Ğµ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñ‹ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ² Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹."
                },
                "en": {
                    "title": "Unlocking Camera Motion Understanding with CameraBench",
                    "desc": "CameraBench is a comprehensive dataset and benchmark aimed at enhancing the understanding of camera motion in videos. It includes around 3,000 videos that have been meticulously annotated by experts, ensuring high-quality data for training and evaluation. The paper introduces a taxonomy of camera motion primitives, which helps in categorizing different types of camera movements, such as tracking and zooming. The study reveals that while traditional Structure-from-Motion models struggle with semantic understanding, fine-tuning a generative Video-Language Model on CameraBench can effectively bridge the gap between geometric and semantic motion understanding."
                },
                "zh": {
                    "title": "CameraBenchï¼šæå‡ç›¸æœºè¿åŠ¨ç†è§£çš„åŸºå‡†ä¸æ•°æ®é›†",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†CameraBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„æ•°æ®é›†å’ŒåŸºå‡†ï¼Œç”¨äºè¯„ä¼°å’Œæ”¹å–„å¯¹ç›¸æœºè¿åŠ¨çš„ç†è§£ã€‚CameraBenchåŒ…å«çº¦3000ä¸ªå¤šæ ·åŒ–çš„äº’è”ç½‘è§†é¢‘ï¼Œç»è¿‡ä¸“å®¶çš„ä¸¥æ ¼å¤šé˜¶æ®µè´¨é‡æ§åˆ¶è¿›è¡Œæ ‡æ³¨ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç›¸æœºè¿åŠ¨åŸè¯­çš„åˆ†ç±»æ³•ï¼Œå¹¶ä¸ç”µå½±æ‘„å½±å¸ˆåˆä½œè®¾è®¡ã€‚ç ”ç©¶å‘ç°ï¼ŒæŸäº›è¿åŠ¨å¦‚â€œè·Ÿéšâ€éœ€è¦ç†è§£åœºæ™¯å†…å®¹ï¼Œè€Œé€šè¿‡CameraBenchè¯„ä¼°çš„æ¨¡å‹åœ¨æ•æ‰è¯­ä¹‰åŸè¯­å’Œå‡ ä½•åŸè¯­æ—¶å­˜åœ¨å›°éš¾ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.16427",
            "title": "Can Large Language Models Help Multimodal Language Analysis? MMLA: A\n  Comprehensive Benchmark",
            "url": "https://huggingface.co/papers/2504.16427",
            "abstract": "Multimodal language analysis is a rapidly evolving field that leverages multiple modalities to enhance the understanding of high-level semantics underlying human conversational utterances. Despite its significance, little research has investigated the capability of multimodal large language models (MLLMs) to comprehend cognitive-level semantics. In this paper, we introduce MMLA, a comprehensive benchmark specifically designed to address this gap. MMLA comprises over 61K multimodal utterances drawn from both staged and real-world scenarios, covering six core dimensions of multimodal semantics: intent, emotion, dialogue act, sentiment, speaking style, and communication behavior. We evaluate eight mainstream branches of LLMs and MLLMs using three methods: zero-shot inference, supervised fine-tuning, and instruction tuning. Extensive experiments reveal that even fine-tuned models achieve only about 60%~70% accuracy, underscoring the limitations of current MLLMs in understanding complex human language. We believe that MMLA will serve as a solid foundation for exploring the potential of large language models in multimodal language analysis and provide valuable resources to advance this field. The datasets and code are open-sourced at https://github.com/thuiar/MMLA.",
            "score": 2,
            "issue_id": 3457,
            "pub_date": "2025-04-23",
            "pub_date_card": {
                "ru": "23 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 23",
                "zh": "4æœˆ23æ—¥"
            },
            "hash": "1845c93e3bc64dd1",
            "authors": [
                "Hanlei Zhang",
                "Zhuohang Li",
                "Yeshuang Zhu",
                "Hua Xu",
                "Peiwu Wang",
                "Haige Zhu",
                "Jie Zhou",
                "Jinchao Zhang"
            ],
            "affiliations": [
                "Department of Computer Science and Technology, Tsinghua University",
                "Kennesaw State University",
                "Pattern Recognition Center, WeChat AI, Tencent Inc, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.16427.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#open_source",
                    "#multimodal"
                ],
                "emoji": "ğŸ—£ï¸",
                "ru": {
                    "title": "MMLA: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MMLA - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºÑƒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ñ€ĞµÑ‡Ğ¸. MMLA Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 61 Ñ‚Ñ‹ÑÑÑ‡ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ 6 ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ² ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ, Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ğ¾ÑĞ»Ğµ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ»Ğ¸ÑˆÑŒ 60-70%, Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¯Ğœ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ñ€ĞµÑ‡Ğ¸."
                },
                "en": {
                    "title": "Unlocking Multimodal Understanding in Language Models",
                    "desc": "This paper introduces MMLA, a new benchmark for evaluating multimodal large language models (MLLMs) in understanding complex human conversations. It includes over 61,000 multimodal utterances that assess six key aspects of semantics: intent, emotion, dialogue act, sentiment, speaking style, and communication behavior. The study evaluates various LLMs and MLLMs using different methods, revealing that even the best fine-tuned models only achieve 60-70% accuracy in understanding these nuances. The authors aim for MMLA to be a foundational resource for advancing research in multimodal language analysis."
                },
                "zh": {
                    "title": "å¤šæ¨¡æ€è¯­è¨€åˆ†æçš„æ–°åŸºå‡†ï¼šMMLA",
                    "desc": "å¤šæ¨¡æ€è¯­è¨€åˆ†ææ˜¯ä¸€ä¸ªå¿«é€Ÿå‘å±•çš„é¢†åŸŸï¼Œåˆ©ç”¨å¤šç§æ¨¡æ€æ¥å¢å¼ºå¯¹äººç±»å¯¹è¯è¯­ä¹‰çš„ç†è§£ã€‚å°½ç®¡å…¶é‡è¦æ€§æ˜¾è‘—ï¼Œä½†å…³äºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ç†è§£è®¤çŸ¥å±‚é¢è¯­ä¹‰çš„ç ”ç©¶ä»ç„¶è¾ƒå°‘ã€‚æœ¬æ–‡ä»‹ç»äº†MMLAï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨è®¾è®¡çš„åŸºå‡†ï¼Œæ—¨åœ¨å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼ŒåŒ…å«è¶…è¿‡61Kä¸ªæ¥è‡ªä¸åŒåœºæ™¯çš„å¤šæ¨¡æ€è¯è¯­ã€‚é€šè¿‡å¯¹å…«ç§ä¸»æµLLMså’ŒMLLMsçš„è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºå³ä½¿ç»è¿‡å¾®è°ƒçš„æ¨¡å‹åœ¨ç†è§£å¤æ‚äººç±»è¯­è¨€æ–¹é¢çš„å‡†ç¡®ç‡ä¹Ÿä»…ä¸º60%~70%ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-04-25.html",
    "link_next": "2025-04-29.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "25.04",
        "en": "04/25",
        "zh": "4æœˆ25æ—¥"
    },
    "short_date_next": {
        "ru": "29.04",
        "en": "04/29",
        "zh": "4æœˆ29æ—¥"
    },
    "categories": {
        "#dataset": 2,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 0,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "å°½ç®¡æœºå™¨å­¦ä¹ ç ”ç©¶è¿…é€Ÿå‘å±•ï¼Œä½†ç›¸åº”çš„ä»£ç å®ç°å¸¸å¸¸ç¼ºå¤±ï¼Œä½¿å¾—ç ”ç©¶äººå‘˜éš¾ä»¥å¤ç°ç»“æœå¹¶åœ¨å‰äººå·¥ä½œåŸºç¡€ä¸Šæ„å»ºã€‚ä¸æ­¤åŒæ—¶ï¼Œæœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ“…é•¿ç†è§£ç§‘å­¦æ–‡æ¡£å¹¶ç”Ÿæˆé«˜è´¨é‡ä»£ç ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†PaperCoderï¼Œä¸€ä¸ªå°†æœºå™¨å­¦ä¹ è®ºæ–‡è½¬åŒ–ä¸ºåŠŸèƒ½ä»£ç ä»“åº“çš„å¤šä»£ç†LLMæ¡†æ¶ã€‚PaperCoderåˆ†ä¸‰ä¸ªé˜¶æ®µè¿è¡Œï¼šè§„åˆ’ã€åˆ†æå’Œç”Ÿæˆã€‚æ¯ä¸ªé˜¶æ®µç”±ä¸€ç»„ä¸“é—¨è®¾è®¡çš„ä»£ç†å®ç°ï¼Œä»¥ç¡®ä¿é«˜æ•ˆåä½œã€‚æˆ‘ä»¬çš„ç»“æœè¯æ˜PaperCoderèƒ½å¤Ÿåˆ›å»ºé«˜è´¨é‡ã€å¿ å®çš„å®ç°ã€‚",
        "title": "Paper2Code: Automating Code Generation from Scientific Papers in Machine\n  Learning",
        "pinyin": "JÇnguÇn jÄ«qi xuÃ©xÃ­ yÃ¡njiÅ« xÃ¹nsÃ¹ fÄzhÇn, dÃ n xiÃ ngyÃ¬ng de dÃ imÇ shÃ­xiÃ n chÃ¡ngchÃ¡ng quÄ“shÄ«, shÇ de yÃ¡njiÅ« rÃ©nyuÃ¡n nÃ¡n yÇ fÃ¹xiÃ n jiÃ©guÇ’ bÃ¬ng zÃ i qiÃ¡nrÃ©n gÅngzuÃ² jÄ«chÇ” shÃ ng gÃ²ujiÃ n. YÇ” cÇ tÃ³ngshÃ­, zuÃ¬xÄ«n de dÃ xÃ­ng yÇ”yÃ¡n mÃ³xÃ­ng (LLMs) shÃ nchÃ¡ng lÇjiÄ› kÄ“xuÃ© wÃ©njiÃ n bÃ¬ng shÄ“ngchÃ©ng gÄo zhÃ¬liÃ ng dÃ imÇ. ShÃ²u cÇ qÇfÃ , wÇ’men yÇn rÃ¹ le PaperCoder, yÄ«gÃ¨ jiÄng jÄ«qi xuÃ©xÃ­ lÃ¹nwÃ©n zhuÇnhuÃ  wÃ©i gÅngnÃ©ng dÃ imÇ cÃ¡ngkÃ¹ de duÅ dÃ ilÇ LLM kuÃ ngjiÃ . PaperCoder fÄ“n sÄn gÃ¨ jiÄ“duÃ n yÃ¹nxÃ­ng: guÄ«huÃ , fÄ“nxi hÃ© shÄ“ngchÃ©ng. MÄ›i gÃ¨ jiÄ“duÃ n yÃ³u yÄ« zÇ” zhuÄnmÃ©n shÃ¨jÃ¬ de dÃ ilÇ shÃ­xiÃ n, yÇ quÃ¨bÇo gÄoxiÃ o xiÃ©zuÃ². WÇ’men de jiÃ©guÇ’ zhÃ¨ngmÃ­ng PaperCoder nÃ©nggÃ²u chuÃ ngjiÃ n gÄo zhÃ¬liÃ ng, zhÅngshÃ­ de shÃ­xiÃ n.",
        "vocab": "[\n    {\"word\": \"å°½ç®¡\", \"pinyin\": \"jÇn guÇn\", \"trans\": \"although\"},\n    {\"word\": \"è¿…é€Ÿ\", \"pinyin\": \"xÃ¹n sÃ¹\", \"trans\": \"rapidly\"},\n    {\"word\": \"ç¼ºå¤±\", \"pinyin\": \"quÄ“ shÄ«\", \"trans\": \"lack\"},\n    {\"word\": \"å¤ç°\", \"pinyin\": \"fÃ¹ xiÃ n\", \"trans\": \"reproduce\"},\n    {\"word\": \"æ“…é•¿\", \"pinyin\": \"shÃ n chÃ¡ng\", \"trans\": \"be good at\"},\n    {\"word\": \"å¯å‘\", \"pinyin\": \"qÇ fÄ\", \"trans\": \"inspire\"},\n    {\"word\": \"å¼•å…¥\", \"pinyin\": \"yÇn rÃ¹\", \"trans\": \"introduce\"},\n    {\"word\": \"è½¬åŒ–\", \"pinyin\": \"zhuÇn huÃ \", \"trans\": \"transform\"},\n    {\"word\": \"åŠŸèƒ½\", \"pinyin\": \"gÅng nÃ©ng\", \"trans\": \"functional\"},\n    {\"word\": \"ä»£ç†\", \"pinyin\": \"dÃ i lÇ\", \"trans\": \"agent\"},\n    {\"word\": \"ç¡®ä¿\", \"pinyin\": \"quÃ¨ bÇo\", \"trans\": \"ensure\"},\n    {\"word\": \"åä½œ\", \"pinyin\": \"xiÃ© zuÃ²\", \"trans\": \"collaborate\"},\n    {\"word\": \"å¿ å®\", \"pinyin\": \"zhÅng shÃ­\", \"trans\": \"faithful\"}\n]",
        "trans": "Despite the rapid advancement of machine learning research, corresponding code implementations are often lacking, making it difficult for researchers to reproduce results and build upon previous work. Meanwhile, the latest large language models (LLMs) excel at understanding scientific documents and generating high-quality code. Inspired by this, we introduce PaperCoder, a multi-agent LLM framework that converts machine learning papers into functional code repositories. PaperCoder operates in three stages: planning, analysis, and generation. Each stage is implemented by a set of specifically designed agents to ensure efficient collaboration. Our results demonstrate that PaperCoder can create high-quality, faithful implementations.",
        "update_ts": "2025-04-27 12:42"
    }
}