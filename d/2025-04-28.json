{
    "date": {
        "ru": "28 апреля",
        "en": "April 28",
        "zh": "4月28日"
    },
    "time_utc": "2025-04-28 04:15",
    "weekday": 0,
    "issue_id": 3459,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.15376",
            "title": "Towards Understanding Camera Motions in Any Video",
            "url": "https://huggingface.co/papers/2504.15376",
            "abstract": "We introduce CameraBench, a large-scale dataset and benchmark designed to assess and improve camera motion understanding. CameraBench consists of ~3,000 diverse internet videos, annotated by experts through a rigorous multi-stage quality control process. One of our contributions is a taxonomy of camera motion primitives, designed in collaboration with cinematographers. We find, for example, that some motions like \"follow\" (or tracking) require understanding scene content like moving subjects. We conduct a large-scale human study to quantify human annotation performance, revealing that domain expertise and tutorial-based training can significantly enhance accuracy. For example, a novice may confuse zoom-in (a change of intrinsics) with translating forward (a change of extrinsics), but can be trained to differentiate the two. Using CameraBench, we evaluate Structure-from-Motion (SfM) and Video-Language Models (VLMs), finding that SfM models struggle to capture semantic primitives that depend on scene content, while VLMs struggle to capture geometric primitives that require precise estimation of trajectories. We then fine-tune a generative VLM on CameraBench to achieve the best of both worlds and showcase its applications, including motion-augmented captioning, video question answering, and video-text retrieval. We hope our taxonomy, benchmark, and tutorials will drive future efforts towards the ultimate goal of understanding camera motions in any video.",
            "score": 69,
            "issue_id": 3457,
            "pub_date": "2025-04-21",
            "pub_date_card": {
                "ru": "21 апреля",
                "en": "April 21",
                "zh": "4月21日"
            },
            "hash": "fb835f3848fe977a",
            "authors": [
                "Zhiqiu Lin",
                "Siyuan Cen",
                "Daniel Jiang",
                "Jay Karhade",
                "Hewei Wang",
                "Chancharik Mitra",
                "Tiffany Ling",
                "Yuhan Huang",
                "Sifan Liu",
                "Mingyu Chen",
                "Rushikesh Zawar",
                "Xue Bai",
                "Yilun Du",
                "Chuang Gan",
                "Deva Ramanan"
            ],
            "affiliations": [
                "Adobe",
                "CMU",
                "Emerson",
                "Harvard",
                "MIT-IBM",
                "UMass Amherst",
                "USC"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.15376.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#cv",
                    "#multimodal"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Новый взгляд на понимание движения камеры в видео",
                    "desc": "CameraBench - это новый крупномасштабный датасет и бенчмарк для оценки и улучшения понимания движения камеры в видео. Датасет содержит около 3000 разнообразных интернет-видео, размеченных экспертами с использованием многоступенчатого контроля качества. Авторы разработали таксономию примитивов движения камеры совместно с кинооператорами. Используя CameraBench, были оценены модели Structure-from-Motion (SfM) и видео-языковые модели (VLM), выявив их сильные и слабые стороны в понимании различных аспектов движения камеры."
                },
                "en": {
                    "title": "Unlocking Camera Motion Understanding with CameraBench",
                    "desc": "CameraBench is a comprehensive dataset and benchmark aimed at enhancing the understanding of camera motion in videos. It includes around 3,000 videos that have been meticulously annotated by experts, ensuring high-quality data for training and evaluation. The paper introduces a taxonomy of camera motion primitives, which helps in categorizing different types of camera movements, such as tracking and zooming. The study reveals that while traditional Structure-from-Motion models struggle with semantic understanding, fine-tuning a generative Video-Language Model on CameraBench can effectively bridge the gap between geometric and semantic motion understanding."
                },
                "zh": {
                    "title": "CameraBench：提升相机运动理解的基准与数据集",
                    "desc": "本文介绍了CameraBench，这是一个大规模的数据集和基准，用于评估和改善对相机运动的理解。CameraBench包含约3000个多样化的互联网视频，经过专家的严格多阶段质量控制进行标注。我们提出了一种相机运动原语的分类法，并与电影摄影师合作设计。研究发现，某些运动如“跟随”需要理解场景内容，而通过CameraBench评估的模型在捕捉语义原语和几何原语时存在困难。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.16427",
            "title": "Can Large Language Models Help Multimodal Language Analysis? MMLA: A\n  Comprehensive Benchmark",
            "url": "https://huggingface.co/papers/2504.16427",
            "abstract": "Multimodal language analysis is a rapidly evolving field that leverages multiple modalities to enhance the understanding of high-level semantics underlying human conversational utterances. Despite its significance, little research has investigated the capability of multimodal large language models (MLLMs) to comprehend cognitive-level semantics. In this paper, we introduce MMLA, a comprehensive benchmark specifically designed to address this gap. MMLA comprises over 61K multimodal utterances drawn from both staged and real-world scenarios, covering six core dimensions of multimodal semantics: intent, emotion, dialogue act, sentiment, speaking style, and communication behavior. We evaluate eight mainstream branches of LLMs and MLLMs using three methods: zero-shot inference, supervised fine-tuning, and instruction tuning. Extensive experiments reveal that even fine-tuned models achieve only about 60%~70% accuracy, underscoring the limitations of current MLLMs in understanding complex human language. We believe that MMLA will serve as a solid foundation for exploring the potential of large language models in multimodal language analysis and provide valuable resources to advance this field. The datasets and code are open-sourced at https://github.com/thuiar/MMLA.",
            "score": 4,
            "issue_id": 3457,
            "pub_date": "2025-04-23",
            "pub_date_card": {
                "ru": "23 апреля",
                "en": "April 23",
                "zh": "4月23日"
            },
            "hash": "1845c93e3bc64dd1",
            "authors": [
                "Hanlei Zhang",
                "Zhuohang Li",
                "Yeshuang Zhu",
                "Hua Xu",
                "Peiwu Wang",
                "Haige Zhu",
                "Jie Zhou",
                "Jinchao Zhang"
            ],
            "affiliations": [
                "Department of Computer Science and Technology, Tsinghua University",
                "Kennesaw State University",
                "Pattern Recognition Center, WeChat AI, Tencent Inc, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.16427.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#open_source",
                    "#multimodal"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "MMLA: новый бенчмарк для оценки понимания мультимодальной семантики языковыми моделями",
                    "desc": "Статья представляет MMLA - новый набор данных для оценки способности мультимодальных языковых моделей понимать семантику человеческой речи. MMLA включает более 61 тысяч мультимодальных высказываний и охватывает 6 ключевых аспектов семантики. Авторы провели обширные эксперименты с различными языковыми моделями, используя методы обучения без учителя, тонкой настройки и инструктирования. Результаты показали, что даже после дообучения точность моделей составляет лишь 60-70%, что указывает на ограничения современных мультимодальных ЯМ в понимании сложной человеческой речи."
                },
                "en": {
                    "title": "Unlocking Multimodal Understanding in Language Models",
                    "desc": "This paper introduces MMLA, a new benchmark for evaluating multimodal large language models (MLLMs) in understanding complex human conversations. It includes over 61,000 multimodal utterances that assess six key aspects of semantics: intent, emotion, dialogue act, sentiment, speaking style, and communication behavior. The study evaluates various LLMs and MLLMs using different methods, revealing that even the best fine-tuned models only achieve 60-70% accuracy in understanding these nuances. The authors aim for MMLA to be a foundational resource for advancing research in multimodal language analysis."
                },
                "zh": {
                    "title": "多模态语言分析的新基准：MMLA",
                    "desc": "多模态语言分析是一个快速发展的领域，利用多种模态来增强对人类对话语义的理解。尽管其重要性显著，但关于多模态大语言模型（MLLMs）理解认知层面语义的研究仍然较少。本文介绍了MMLA，这是一个专门设计的基准，旨在填补这一空白，包含超过61K个来自不同场景的多模态话语。通过对八种主流LLMs和MLLMs的评估，结果显示即使经过微调的模型在理解复杂人类语言方面的准确率也仅为60%~70%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.18415",
            "title": "BitNet v2: Native 4-bit Activations with Hadamard Transformation for\n  1-bit LLMs",
            "url": "https://huggingface.co/papers/2504.18415",
            "abstract": "Efficient deployment of 1-bit Large Language Models (LLMs) is hindered by activation outliers, which complicate quantization to low bit-widths. We introduce BitNet v2, a novel framework enabling native 4-bit activation quantization for 1-bit LLMs. To tackle outliers in attention and feed-forward network activations, we propose H-BitLinear, a module applying an online Hadamard transformation prior to activation quantization. This transformation smooths sharp activation distributions into more Gaussian-like forms, suitable for low-bit representation. Experiments show BitNet v2 trained from scratch with 8-bit activations matches BitNet b1.58 performance. Crucially, BitNet v2 achieves minimal performance degradation when trained with native 4-bit activations, significantly reducing memory footprint and computational cost for batched inference.",
            "score": 2,
            "issue_id": 3458,
            "pub_date": "2025-04-25",
            "pub_date_card": {
                "ru": "25 апреля",
                "en": "April 25",
                "zh": "4月25日"
            },
            "hash": "1b18f5117da74852",
            "authors": [
                "Hongyu Wang",
                "Shuming Ma",
                "Furu Wei"
            ],
            "affiliations": [
                "Microsoft Research",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.18415.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#optimization",
                    "#inference",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Эффективное сжатие больших языковых моделей без потери качества",
                    "desc": "BitNet v2 - это новая технология для эффективного развертывания 1-битных больших языковых моделей (LLM). Она решает проблему выбросов в активациях, которые затрудняют квантование до малого числа бит. Ключевой компонент - модуль H-BitLinear, применяющий онлайн-преобразование Адамара перед квантованием активаций. Эксперименты показывают, что BitNet v2 с 4-битными активациями достигает производительности BitNet b1.58, значительно снижая потребление памяти и вычислительные затраты."
                },
                "en": {
                    "title": "Revolutionizing 1-bit LLMs with Efficient 4-bit Quantization",
                    "desc": "This paper presents BitNet v2, a new framework designed to improve the deployment of 1-bit Large Language Models (LLMs) by enabling efficient 4-bit activation quantization. The challenge of activation outliers, which complicate the quantization process, is addressed through a novel module called H-BitLinear that applies an online Hadamard transformation. This transformation helps to convert sharp activation distributions into smoother, Gaussian-like forms, making them more suitable for low-bit representation. The results demonstrate that BitNet v2 can be trained with 4-bit activations with minimal performance loss, significantly lowering memory usage and computational costs during inference."
                },
                "zh": {
                    "title": "高效量化，轻松部署1位语言模型",
                    "desc": "本文介绍了一种名为BitNet v2的新框架，旨在高效部署1位大型语言模型（LLMs）。该框架通过引入H-BitLinear模块，解决了在注意力和前馈网络激活中出现的异常值问题，从而实现原生4位激活量化。H-BitLinear模块通过在线Hadamard变换，将激活分布平滑为更接近高斯分布的形式，适合低位表示。实验表明，使用原生4位激活训练的BitNet v2在性能上几乎没有下降，同时显著减少了内存占用和计算成本。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.12080",
            "title": "DC-SAM: In-Context Segment Anything in Images and Videos via Dual\n  Consistency",
            "url": "https://huggingface.co/papers/2504.12080",
            "abstract": "Given a single labeled example, in-context segmentation aims to segment corresponding objects. This setting, known as one-shot segmentation in few-shot learning, explores the segmentation model's generalization ability and has been applied to various vision tasks, including scene understanding and image/video editing. While recent Segment Anything Models have achieved state-of-the-art results in interactive segmentation, these approaches are not directly applicable to in-context segmentation. In this work, we propose the Dual Consistency SAM (DC-SAM) method based on prompt-tuning to adapt SAM and SAM2 for in-context segmentation of both images and videos. Our key insights are to enhance the features of the SAM's prompt encoder in segmentation by providing high-quality visual prompts. When generating a mask prior, we fuse the SAM features to better align the prompt encoder. Then, we design a cycle-consistent cross-attention on fused features and initial visual prompts. Next, a dual-branch design is provided by using the discriminative positive and negative prompts in the prompt encoder. Furthermore, we design a simple mask-tube training strategy to adopt our proposed dual consistency method into the mask tube. Although the proposed DC-SAM is primarily designed for images, it can be seamlessly extended to the video domain with the support of SAM2. Given the absence of in-context segmentation in the video domain, we manually curate and construct the first benchmark from existing video segmentation datasets, named In-Context Video Object Segmentation (IC-VOS), to better assess the in-context capability of the model. Extensive experiments demonstrate that our method achieves 55.5 (+1.4) mIoU on COCO-20i, 73.0 (+1.1) mIoU on PASCAL-5i, and a J&F score of 71.52 on the proposed IC-VOS benchmark. Our source code and benchmark are available at https://github.com/zaplm/DC-SAM.",
            "score": 0,
            "issue_id": 3459,
            "pub_date": "2025-04-16",
            "pub_date_card": {
                "ru": "16 апреля",
                "en": "April 16",
                "zh": "4月16日"
            },
            "hash": "e91663786d7f873e",
            "authors": [
                "Mengshi Qi",
                "Pengfei Zhu",
                "Xiangtai Li",
                "Xiaoyang Bi",
                "Lu Qi",
                "Huadong Ma",
                "Ming-Hsuan Yang"
            ],
            "affiliations": [
                "Nanyang Technological University, Singapore",
                "State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, China",
                "UC Merced, US"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.12080.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#transfer_learning",
                    "#benchmark",
                    "#cv",
                    "#games"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "DC-SAM: Продвинутая сегментация в контексте для изображений и видео",
                    "desc": "Эта статья представляет метод Dual Consistency SAM (DC-SAM) для адаптации моделей SAM и SAM2 к задаче сегментации в контексте как для изображений, так и для видео. Авторы предлагают улучшить функции энкодера подсказок SAM, используя высококачественные визуальные подсказки и циклически согласованное кросс-внимание. Метод включает двухветвевую архитектуру с дискриминативными положительными и отрицательными подсказками, а также стратегию обучения mask-tube для видео. Эксперименты показывают улучшение результатов на нескольких бенчмарках, включая новый набор данных IC-VOS для оценки способности модели к сегментации видео в контексте."
                },
                "en": {
                    "title": "Enhancing One-Shot Segmentation with Dual Consistency",
                    "desc": "This paper introduces the Dual Consistency SAM (DC-SAM) method for one-shot segmentation, which allows a model to segment objects using just one labeled example. It enhances the Segment Anything Model (SAM) by improving the prompt encoder's features through high-quality visual prompts and a cycle-consistent cross-attention mechanism. The authors also present a new benchmark for in-context video object segmentation, named IC-VOS, to evaluate their method's performance in video tasks. Experimental results show that DC-SAM outperforms existing models on several datasets, demonstrating its effectiveness in both image and video segmentation tasks."
                },
                "zh": {
                    "title": "双一致性SAM：提升上下文分割能力的创新方法",
                    "desc": "本文提出了一种名为双一致性SAM（DC-SAM）的方法，旨在解决单个标记示例下的上下文分割问题。该方法通过提示调优来适应现有的分割模型，特别是针对图像和视频的上下文分割。我们通过提供高质量的视觉提示来增强分割模型的特征，并设计了循环一致的交叉注意机制来优化提示编码器。实验结果表明，DC-SAM在多个数据集上表现出色，展示了其在上下文分割任务中的有效性。"
                }
            }
        }
    ],
    "link_prev": "2025-04-25.html",
    "link_next": "2025-04-29.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "25.04",
        "en": "04/25",
        "zh": "4月25日"
    },
    "short_date_next": {
        "ru": "29.04",
        "en": "04/29",
        "zh": "4月29日"
    },
    "categories": {
        "#dataset": 3,
        "#data": 0,
        "#benchmark": 3,
        "#agents": 0,
        "#cv": 2,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "尽管机器学习研究迅速发展，但相应的代码实现常常缺失，使得研究人员难以复现结果并在前人工作基础上构建。与此同时，最新的大型语言模型（LLMs）擅长理解科学文档并生成高质量代码。受此启发，我们引入了PaperCoder，一个将机器学习论文转化为功能代码仓库的多代理LLM框架。PaperCoder分三个阶段运行：规划、分析和生成。每个阶段由一组专门设计的代理实现，以确保高效协作。我们的结果证明PaperCoder能够创建高质量、忠实的实现。",
        "title": "Paper2Code: Automating Code Generation from Scientific Papers in Machine\n  Learning",
        "pinyin": "Jǐnguǎn jīqi xuéxí yánjiū xùnsù fāzhǎn, dàn xiàngyìng de dàimǎ shíxiàn chángcháng quēshī, shǐ de yánjiū rényuán nán yǐ fùxiàn jiéguǒ bìng zài qiánrén gōngzuò jīchǔ shàng gòujiàn. Yǔ cǐ tóngshí, zuìxīn de dàxíng yǔyán móxíng (LLMs) shàncháng lǐjiě kēxué wénjiàn bìng shēngchéng gāo zhìliàng dàimǎ. Shòu cǐ qǐfà, wǒmen yǐn rù le PaperCoder, yīgè jiāng jīqi xuéxí lùnwén zhuǎnhuà wéi gōngnéng dàimǎ cángkù de duō dàilǐ LLM kuàngjià. PaperCoder fēn sān gè jiēduàn yùnxíng: guīhuà, fēnxi hé shēngchéng. Měi gè jiēduàn yóu yī zǔ zhuānmén shèjì de dàilǐ shíxiàn, yǐ quèbǎo gāoxiào xiézuò. Wǒmen de jiéguǒ zhèngmíng PaperCoder nénggòu chuàngjiàn gāo zhìliàng, zhōngshí de shíxiàn.",
        "vocab": "[\n    {\"word\": \"尽管\", \"pinyin\": \"jǐn guǎn\", \"trans\": \"although\"},\n    {\"word\": \"迅速\", \"pinyin\": \"xùn sù\", \"trans\": \"rapidly\"},\n    {\"word\": \"缺失\", \"pinyin\": \"quē shī\", \"trans\": \"lack\"},\n    {\"word\": \"复现\", \"pinyin\": \"fù xiàn\", \"trans\": \"reproduce\"},\n    {\"word\": \"擅长\", \"pinyin\": \"shàn cháng\", \"trans\": \"be good at\"},\n    {\"word\": \"启发\", \"pinyin\": \"qǐ fā\", \"trans\": \"inspire\"},\n    {\"word\": \"引入\", \"pinyin\": \"yǐn rù\", \"trans\": \"introduce\"},\n    {\"word\": \"转化\", \"pinyin\": \"zhuǎn huà\", \"trans\": \"transform\"},\n    {\"word\": \"功能\", \"pinyin\": \"gōng néng\", \"trans\": \"functional\"},\n    {\"word\": \"代理\", \"pinyin\": \"dài lǐ\", \"trans\": \"agent\"},\n    {\"word\": \"确保\", \"pinyin\": \"què bǎo\", \"trans\": \"ensure\"},\n    {\"word\": \"协作\", \"pinyin\": \"xié zuò\", \"trans\": \"collaborate\"},\n    {\"word\": \"忠实\", \"pinyin\": \"zhōng shí\", \"trans\": \"faithful\"}\n]",
        "trans": "Despite the rapid advancement of machine learning research, corresponding code implementations are often lacking, making it difficult for researchers to reproduce results and build upon previous work. Meanwhile, the latest large language models (LLMs) excel at understanding scientific documents and generating high-quality code. Inspired by this, we introduce PaperCoder, a multi-agent LLM framework that converts machine learning papers into functional code repositories. PaperCoder operates in three stages: planning, analysis, and generation. Each stage is implemented by a set of specifically designed agents to ensure efficient collaboration. Our results demonstrate that PaperCoder can create high-quality, faithful implementations.",
        "update_ts": "2025-04-27 12:42"
    }
}