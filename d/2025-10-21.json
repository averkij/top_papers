{
    "date": {
        "ru": "21 октября",
        "en": "October 21",
        "zh": "10月21日"
    },
    "time_utc": "2025-10-21 04:14",
    "weekday": 1,
    "issue_id": 6523,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2510.16872",
            "title": "DeepAnalyze: Agentic Large Language Models for Autonomous Data Science",
            "url": "https://huggingface.co/papers/2510.16872",
            "abstract": "DeepAnalyze-8B, an agentic LLM, autonomously completes the data science pipeline from raw data to research reports using curriculum-based training and data-grounded trajectory synthesis.  \t\t\t\t\tAI-generated summary \t\t\t\t Autonomous data science, from raw data sources to analyst-grade deep research reports, has been a long-standing challenge, and is now becoming feasible with the emergence of powerful large language models (LLMs). Recent workflow-based data agents have shown promising results on specific data tasks but remain fundamentally limited in achieving fully autonomous data science due to their reliance on predefined workflows. In this paper, we introduce DeepAnalyze-8B, the first agentic LLM designed for autonomous data science, capable of automatically completing the end-toend pipeline from data sources to analyst-grade deep research reports. To tackle high-complexity data science tasks, we propose a curriculum-based agentic training paradigm that emulates the learning trajectory of human data scientists, enabling LLMs to progressively acquire and integrate multiple capabilities in real-world environments. We also introduce a data-grounded trajectory synthesis framework that constructs high-quality training data. Through agentic training, DeepAnalyze learns to perform a broad spectrum of data tasks, ranging from data question answering and specialized analytical tasks to open-ended data research. Experiments demonstrate that, with only 8B parameters, DeepAnalyze outperforms previous workflow-based agents built on most advanced proprietary LLMs. The model, code, and training data of DeepAnalyze are open-sourced, paving the way toward autonomous data science.",
            "score": 32,
            "issue_id": 6521,
            "pub_date": "2025-10-19",
            "pub_date_card": {
                "ru": "19 октября",
                "en": "October 19",
                "zh": "10月19日"
            },
            "hash": "274bf4f3e131abd9",
            "authors": [
                "Shaolei Zhang",
                "Ju Fan",
                "Meihao Fan",
                "Guoliang Li",
                "Xiaoyong Du"
            ],
            "affiliations": [
                "Renmin University of China",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.16872.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#agi",
                    "#training",
                    "#agents",
                    "#data",
                    "#open_source"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "Автономный data scientist с 8 миллиардами параметров",
                    "desc": "DeepAnalyze-8B — это первая agentic LLM, разработанная специально для автономной data science, способная самостоятельно выполнять весь pipeline от сырых данных до аналитических отчётов исследовательского уровня. Модель обучается по curriculum-based парадигме, которая имитирует траекторию обучения человека-аналитика, позволяя LLM постепенно осваивать и интегрировать множество навыков в реальных условиях. Авторы также предложили data-grounded framework для синтеза высококачественных обучающих траекторий. При всего 8 миллиардах параметров DeepAnalyze превосходит предыдущие workflow-based агенты, построенные на самых продвинутых проприетарных LLM."
                },
                "en": {
                    "title": "Autonomous Data Science with DeepAnalyze-8B",
                    "desc": "DeepAnalyze-8B is a large language model (LLM) designed to autonomously manage the entire data science process, from raw data to comprehensive research reports. It utilizes a curriculum-based training approach that mimics how human data scientists learn, allowing it to develop various skills needed for complex data tasks. The model also employs a data-grounded trajectory synthesis framework to create high-quality training data, enhancing its learning capabilities. Experiments show that DeepAnalyze-8B, with only 8 billion parameters, surpasses previous models in performing diverse data science tasks, making significant strides towards fully autonomous data analysis."
                },
                "zh": {
                    "title": "自主数据科学的新纪元：DeepAnalyze-8B",
                    "desc": "DeepAnalyze-8B是一种自主的语言模型，能够从原始数据自动完成数据科学流程，生成研究报告。它采用基于课程的训练方法，模拟人类数据科学家的学习过程，使模型能够逐步掌握多种能力。通过数据驱动的轨迹合成框架，DeepAnalyze生成高质量的训练数据，提升模型的表现。实验结果表明，DeepAnalyze在仅有80亿参数的情况下，超越了以往基于工作流的代理模型，推动了自主数据科学的发展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.17354",
            "title": "Towards Mixed-Modal Retrieval for Universal Retrieval-Augmented\n  Generation",
            "url": "https://huggingface.co/papers/2510.17354",
            "abstract": "Nyx, a unified mixed-modal retriever, enhances vision-language generation by retrieving and reasoning over mixed-modal data, outperforming existing RAG systems in real-world scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for enhancing large language models (LLMs) by retrieving relevant documents from an external corpus. However, existing RAG systems primarily focus on unimodal text documents, and often fall short in real-world scenarios where both queries and documents may contain mixed modalities (such as text and images). In this paper, we address the challenge of Universal Retrieval-Augmented Generation (URAG), which involves retrieving and reasoning over mixed-modal information to improve vision-language generation. To this end, we propose Nyx, a unified mixed-modal to mixed-modal retriever tailored for URAG scenarios. To mitigate the scarcity of realistic mixed-modal data, we introduce a four-stage automated pipeline for generation and filtering, leveraging web documents to construct NyxQA, a dataset comprising diverse mixed-modal question-answer pairs that better reflect real-world information needs. Building on this high-quality dataset, we adopt a two-stage training framework for Nyx: we first perform pre-training on NyxQA along with a variety of open-source retrieval datasets, followed by supervised fine-tuning using feedback from downstream vision-language models (VLMs) to align retrieval outputs with generative preferences. Experimental results demonstrate that Nyx not only performs competitively on standard text-only RAG benchmarks, but also excels in the more general and realistic URAG setting, significantly improving generation quality in vision-language tasks.",
            "score": 23,
            "issue_id": 6521,
            "pub_date": "2025-10-20",
            "pub_date_card": {
                "ru": "20 октября",
                "en": "October 20",
                "zh": "10月20日"
            },
            "hash": "a8b0b456ca9cf1f9",
            "authors": [
                "Chenghao Zhang",
                "Guanting Dong",
                "Xinyu Yang",
                "Zhicheng Dou"
            ],
            "affiliations": [
                "Renmin University of China, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.17354.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#training",
                    "#optimization",
                    "#dataset",
                    "#multimodal",
                    "#reasoning",
                    "#games",
                    "#open_source"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Универсальный поиск для мультимодальной генерации",
                    "desc": "Статья представляет Nyx — универсальную систему для retrieval-augmented generation (RAG), работающую с мультимодальными данными (текст и изображения). Авторы создали датасет NyxQA с разнообразными мультимодальными вопросами и ответами, используя автоматический pipeline для генерации данных из веб-документов. Nyx обучается в два этапа: сначала pre-training на NyxQA и других датасетах, затем fine-tuning с использованием обратной связи от vision-language моделей. Эксперименты показывают, что Nyx превосходит существующие RAG-системы как на текстовых задачах, так и в реалистичных сценариях с мультимодальными запросами."
                },
                "en": {
                    "title": "Nyx: Bridging Text and Images for Better AI Understanding",
                    "desc": "The paper introduces Nyx, a mixed-modal retriever designed to enhance vision-language generation by effectively retrieving and reasoning over both text and image data. Unlike traditional Retrieval-Augmented Generation (RAG) systems that focus solely on text, Nyx addresses the complexities of Universal Retrieval-Augmented Generation (URAG) where mixed modalities are involved. To support this, the authors developed a four-stage automated pipeline to create NyxQA, a dataset of mixed-modal question-answer pairs that better represent real-world scenarios. Experimental results show that Nyx outperforms existing RAG systems, particularly in tasks that require understanding and generating content from both text and images."
                },
                "zh": {
                    "title": "Nyx：提升视觉-语言生成的混合模态检索器",
                    "desc": "本文提出了一种名为Nyx的统一混合模态检索器，旨在通过检索和推理混合模态数据来增强视觉-语言生成。现有的检索增强生成（RAG）系统主要集中在单一模态的文本文档上，而Nyx则能够处理同时包含文本和图像的查询和文档。为了应对现实场景中的挑战，Nyx采用了一个四阶段的自动化管道来生成和过滤数据，构建了一个多样化的混合模态问答数据集NyxQA。实验结果表明，Nyx在标准文本RAG基准测试中表现出色，并在更广泛的现实URAG设置中显著提高了视觉-语言任务的生成质量。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.17681",
            "title": "PICABench: How Far Are We from Physically Realistic Image Editing?",
            "url": "https://huggingface.co/papers/2510.17681",
            "abstract": "PICABench and PICAEval evaluate physical realism in image editing by assessing eight sub-dimensions and using VLM-as-a-judge with human annotations, highlighting the need for physics-based solutions.  \t\t\t\t\tAI-generated summary \t\t\t\t Image editing has achieved remarkable progress recently. Modern editing models could already follow complex instructions to manipulate the original content. However, beyond completing the editing instructions, the accompanying physical effects are the key to the generation realism. For example, removing an object should also remove its shadow, reflections, and interactions with nearby objects. Unfortunately, existing models and benchmarks mainly focus on instruction completion but overlook these physical effects. So, at this moment, how far are we from physically realistic image editing? To answer this, we introduce PICABench, which systematically evaluates physical realism across eight sub-dimension (spanning optics, mechanics, and state transitions) for most of the common editing operations (add, remove, attribute change, etc). We further propose the PICAEval, a reliable evaluation protocol that uses VLM-as-a-judge with per-case, region-level human annotations and questions. Beyond benchmarking, we also explore effective solutions by learning physics from videos and construct a training dataset PICA-100K. After evaluating most of the mainstream models, we observe that physical realism remains a challenging problem with large rooms to explore. We hope that our benchmark and proposed solutions can serve as a foundation for future work moving from naive content editing toward physically consistent realism.",
            "score": 12,
            "issue_id": 6521,
            "pub_date": "2025-10-20",
            "pub_date_card": {
                "ru": "20 октября",
                "en": "October 20",
                "zh": "10月20日"
            },
            "hash": "3b31de43894f079f",
            "authors": [
                "Yuandong Pu",
                "Le Zhuo",
                "Songhao Han",
                "Jinbo Xing",
                "Kaiwen Zhu",
                "Shuo Cao",
                "Bin Fu",
                "Si Liu",
                "Hongsheng Li",
                "Yu Qiao",
                "Wenlong Zhang",
                "Xi Chen",
                "Yihao Liu"
            ],
            "affiliations": [
                "Beihang University",
                "CUHK MMLab",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "The University of Hong Kong",
                "Tongyi Lab",
                "USTC"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.17681.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#optimization",
                    "#dataset",
                    "#survey",
                    "#benchmark"
                ],
                "emoji": "🔮",
                "ru": {
                    "title": "Физическая реалистичность в редактировании изображений: новый бенчмарк",
                    "desc": "Исследователи представили PICABench — новый бенчмарк для оценки физической реалистичности при редактировании изображений с помощью AI. Существующие модели редактирования хорошо справляются с выполнением инструкций, но игнорируют физические эффекты, такие как тени, отражения и взаимодействие объектов. Бенчмарк оценивает восемь измерений физического реализма (оптика, механика, изменения состояний) и использует VLM в качестве судьи с аннотациями на уровне регионов изображения. Для обучения моделей авторы создали датасет PICA-100K, который позволяет моделям учиться физике из видео, показав, что физическая корректность остаётся серьёзной проблемой для современных систем."
                },
                "en": {
                    "title": "Towards Physically Realistic Image Editing: Evaluating with PICABench and PICAEval",
                    "desc": "This paper introduces PICABench and PICAEval, tools designed to evaluate the physical realism of image editing by examining eight specific aspects related to physics. It highlights that while current image editing models can follow complex instructions, they often neglect the physical consequences of those edits, such as shadows and reflections. The authors propose a new evaluation framework that incorporates human annotations and uses a Vision-Language Model (VLM) to assess the realism of edits. The findings indicate that achieving physically realistic image editing is still a significant challenge, suggesting a need for further research and development in this area."
                },
                "zh": {
                    "title": "推动图像编辑向物理真实感发展",
                    "desc": "本文介绍了PICABench和PICAEval，这两个工具用于评估图像编辑中的物理真实感。它们通过评估八个子维度，结合人类注释，强调了基于物理的解决方案的必要性。当前的图像编辑模型虽然能够完成复杂的指令，但往往忽视了物理效果，例如去除物体时也应去除其阴影和反射。通过系统评估和提出有效的学习物理的方法，本文希望为未来的研究奠定基础，推动图像编辑向物理一致的真实感发展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.17800",
            "title": "Glyph: Scaling Context Windows via Visual-Text Compression",
            "url": "https://huggingface.co/papers/2510.17800",
            "abstract": "Glyph compresses long textual inputs into images using vision-language models, achieving significant token compression and improved performance in long-context tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) increasingly rely on long-context modeling for tasks such as document understanding, code analysis, and multi-step reasoning. However, scaling context windows to the million-token level brings prohibitive computational and memory costs, limiting the practicality of long-context LLMs. In this work, we take a different perspective-visual context scaling-to tackle this challenge. Instead of extending token-based sequences, we propose Glyph, a framework that renders long texts into images and processes them with vision-language models (VLMs). This approach substantially compresses textual input while preserving semantic information, and we further design an LLM-driven genetic search to identify optimal visual rendering configurations for balancing accuracy and compression. Through extensive experiments, we demonstrate that our method achieves 3-4x token compression while maintaining accuracy comparable to leading LLMs such as Qwen3-8B on various long-context benchmarks. This compression also leads to around 4x faster prefilling and decoding, and approximately 2x faster SFT training. Furthermore, under extreme compression, a 128K-context VLM could scale to handle 1M-token-level text tasks. In addition, the rendered text data benefits real-world multimodal tasks, such as document understanding. Our code and model are released at https://github.com/thu-coai/Glyph.",
            "score": 9,
            "issue_id": 6522,
            "pub_date": "2025-10-20",
            "pub_date_card": {
                "ru": "20 октября",
                "en": "October 20",
                "zh": "10月20日"
            },
            "hash": "194e6c8d4ed48372",
            "authors": [
                "Jiale Cheng",
                "Yusen Liu",
                "Xinyu Zhang",
                "Yulin Fei",
                "Wenyi Hong",
                "Ruiliang Lyu",
                "Weihan Wang",
                "Zhe Su",
                "Xiaotao Gu",
                "Xiao Liu",
                "Yushi Bai",
                "Jie Tang",
                "Hongning Wang",
                "Minlie Huang"
            ],
            "affiliations": [
                "The Conversational Artificial Intelligence (CoAI) Group, Tsinghua University",
                "The Knowledge Engineering Group (KEG), Tsinghua University",
                "Zhipu AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.17800.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#cv",
                    "#long_context",
                    "#data",
                    "#dataset",
                    "#training",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Текст как картинка: сжатие миллиона токенов через визуализацию",
                    "desc": "Статья представляет Glyph - framework, который преобразует длинные текстовые входы в изображения для обработки vision-language моделями вместо традиционных LLM. Метод достигает 3-4x компрессии токенов при сохранении точности, сравнимой с современными моделями типа Qwen3-8B, что приводит к ускорению prefilling и decoding примерно в 4 раза. Для оптимизации визуального рендеринга текста используется генетический поиск на основе LLM, балансирующий между точностью и степенью сжатия. При экстремальной компрессии VLM с контекстом 128K токенов может обрабатывать задачи уровня 1M токенов, что также улучшает производительность в задачах понимания документов."
                },
                "en": {
                    "title": "Transforming Text to Images for Efficient Long-Context Processing",
                    "desc": "Glyph is a novel framework that transforms lengthy textual inputs into images, utilizing vision-language models (VLMs) to achieve significant token compression. This method addresses the challenges of scaling context windows in large language models (LLMs) by rendering text visually, which preserves semantic meaning while reducing the number of tokens needed. The framework incorporates an LLM-driven genetic search to optimize visual rendering configurations, balancing accuracy and compression effectively. Experimental results show that Glyph can compress tokens by 3-4 times and improve processing speed, making it suitable for handling extensive text tasks up to 1 million tokens."
                },
                "zh": {
                    "title": "Glyph：长文本的图像压缩新方法",
                    "desc": "Glyph 是一种将长文本输入压缩为图像的框架，利用视觉-语言模型实现显著的令牌压缩，并在长上下文任务中提高性能。传统的大型语言模型在处理长文本时面临计算和内存成本的挑战，而 Glyph 通过将文本渲染为图像来解决这一问题。该方法在保持语义信息的同时，显著减少了文本输入的大小，并通过 LLM 驱动的遗传搜索优化视觉渲染配置。实验表明，Glyph 实现了 3-4 倍的令牌压缩，同时在多个长上下文基准测试中保持与领先 LLM 相当的准确性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.16888",
            "title": "Uniworld-V2: Reinforce Image Editing with Diffusion Negative-aware\n  Finetuning and MLLM Implicit Feedback",
            "url": "https://huggingface.co/papers/2510.16888",
            "abstract": "Edit-R1, a post-training framework using Diffusion Negative-aware Finetuning and a Multimodal Large Language Model, achieves state-of-the-art results in instruction-based image editing by addressing overfitting and lack of a universal reward model.  \t\t\t\t\tAI-generated summary \t\t\t\t Instruction-based image editing has achieved remarkable progress; however, models solely trained via supervised fine-tuning often overfit to annotated patterns, hindering their ability to explore and generalize beyond training distributions. To this end, we introduce Edit-R1, a novel post-training framework for instruction-based image editing based on policy optimization. Specifically, we utilize Diffusion Negative-aware Finetuning (DiffusionNFT), a likelihood-free policy optimization method consistent with the flow matching forward process, thereby enabling the use of higher-order samplers and more efficient training. Another key challenge here is the absence of a universal reward model, resulting from the diverse nature of editing instructions and tasks. To bridge this gap, we employ a Multimodal Large Language Model (MLLM) as a unified, training-free reward model, leveraging its output logits to provide fine-grained feedback. Furthermore, we carefully design a low-variance group filtering mechanism to reduce MLLM scoring noise and stabilize optimization. UniWorld-V2, trained with this framework, achieves state-of-the-art results on the ImgEdit and GEdit-Bench benchmarks, scoring 4.49 and 7.83, respectively. Crucially, our framework is model-agnostic, delivering substantial performance gains when applied to diverse base models like Qwen-Image-Edit and FLUX-Kontext, demonstrating its wide applicability. Code and models are publicly available at https://github.com/PKU-YuanGroup/UniWorld-V2.",
            "score": 8,
            "issue_id": 6523,
            "pub_date": "2025-10-19",
            "pub_date_card": {
                "ru": "19 октября",
                "en": "October 19",
                "zh": "10月19日"
            },
            "hash": "4c50f02ff0438a71",
            "authors": [
                "Zongjian Li",
                "Zheyuan Liu",
                "Qihui Zhang",
                "Bin Lin",
                "Shenghai Yuan",
                "Zhiyuan Yan",
                "Yang Ye",
                "Wangbo Yu",
                "Yuwei Niu",
                "Li Yuan"
            ],
            "affiliations": [
                "Rabbitpre AI",
                "Shenzhen Graduate School, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.16888.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#optimization",
                    "#training",
                    "#multimodal",
                    "#diffusion",
                    "#open_source"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Обучение с подкреплением для редактирования изображений по инструкциям",
                    "desc": "Исследователи представили Edit-R1 — фреймворк для дополнительного обучения моделей редактирования изображений по текстовым инструкциям. Основная идея заключается в использовании policy optimization с методом Diffusion Negative-aware Finetuning, который совместим с процессом flow matching и позволяет эффективнее обучать модели. В качестве универсальной функции награды применяется Multimodal LLM, который оценивает качество отредактированных изображений без дополнительного обучения. Модель UniWorld-V2, обученная с помощью этого фреймворка, достигла state-of-the-art результатов на бенчмарках ImgEdit и GEdit-Bench, причём подход применим к различным базовым моделям."
                },
                "en": {
                    "title": "Revolutionizing Image Editing with Edit-R1 Framework",
                    "desc": "Edit-R1 is a new framework designed to improve instruction-based image editing by addressing common issues like overfitting and the lack of a universal reward model. It employs Diffusion Negative-aware Finetuning (DiffusionNFT), which optimizes policies without relying on likelihood, allowing for more efficient training with advanced sampling techniques. To tackle the challenge of diverse editing tasks, it uses a Multimodal Large Language Model (MLLM) as a consistent reward model, providing detailed feedback based on its outputs. This approach has led to significant performance improvements on benchmark tests, proving its effectiveness across various base models."
                },
                "zh": {
                    "title": "Edit-R1：图像编辑的新突破",
                    "desc": "Edit-R1 是一个后训练框架，旨在解决基于指令的图像编辑中的过拟合问题。它采用了扩散负向微调（DiffusionNFT）方法，通过无似然的策略优化来提高训练效率。为了克服缺乏通用奖励模型的挑战，Edit-R1 使用了多模态大型语言模型（MLLM）作为统一的奖励模型，提供细致的反馈。该框架在多个基准测试中表现出色，证明了其在不同基础模型上的广泛适用性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.17803",
            "title": "ConsistEdit: Highly Consistent and Precise Training-free Visual Editing",
            "url": "https://huggingface.co/papers/2510.17803",
            "abstract": "ConsistEdit, a novel attention control method for MM-DiT, enhances image and video editing by ensuring consistency and fine-grained control across all inference steps and attention layers.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in training-free attention control methods have enabled flexible and efficient text-guided editing capabilities for existing generation models. However, current approaches struggle to simultaneously deliver strong editing strength while preserving consistency with the source. This limitation becomes particularly critical in multi-round and video editing, where visual errors can accumulate over time. Moreover, most existing methods enforce global consistency, which limits their ability to modify individual attributes such as texture while preserving others, thereby hindering fine-grained editing. Recently, the architectural shift from U-Net to MM-DiT has brought significant improvements in generative performance and introduced a novel mechanism for integrating text and vision modalities. These advancements pave the way for overcoming challenges that previous methods failed to resolve. Through an in-depth analysis of MM-DiT, we identify three key insights into its attention mechanisms. Building on these, we propose ConsistEdit, a novel attention control method specifically tailored for MM-DiT. ConsistEdit incorporates vision-only attention control, mask-guided pre-attention fusion, and differentiated manipulation of the query, key, and value tokens to produce consistent, prompt-aligned edits. Extensive experiments demonstrate that ConsistEdit achieves state-of-the-art performance across a wide range of image and video editing tasks, including both structure-consistent and structure-inconsistent scenarios. Unlike prior methods, it is the first approach to perform editing across all inference steps and attention layers without handcraft, significantly enhancing reliability and consistency, which enables robust multi-round and multi-region editing. Furthermore, it supports progressive adjustment of structural consistency, enabling finer control.",
            "score": 7,
            "issue_id": 6521,
            "pub_date": "2025-10-20",
            "pub_date_card": {
                "ru": "20 октября",
                "en": "October 20",
                "zh": "10月20日"
            },
            "hash": "b0c5a5bae934a22a",
            "authors": [
                "Zixin Yin",
                "Ling-Hao Chen",
                "Lionel Ni",
                "Xili Dai"
            ],
            "affiliations": [
                "Hong Kong University of Science and Technology",
                "Hong Kong University of Science and Technology, Guangzhou",
                "International Digital Economy Academy",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.17803.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#training",
                    "#optimization",
                    "#multimodal",
                    "#architecture"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "ConsistEdit: точный контроль внимания в MM-DiT для надёжного редактирования",
                    "desc": "Исследователи представили ConsistEdit — новый метод управления механизмом внимания в архитектуре MM-DiT для редактирования изображений и видео. Метод решает ключевую проблему существующих подходов: невозможность одновременно обеспечить сильное редактирование и сохранить консистентность с исходным изображением. ConsistEdit использует три техники: контроль внимания только для визуальной модальности, слияние признаков с учётом маски и дифференцированную работу с query, key и value токенами. Это позволяет выполнять надёжное многораундовое редактирование на всех шагах inference и во всех слоях внимания, с возможностью тонкой настройки структурной консистентности."
                },
                "en": {
                    "title": "ConsistEdit: Precision and Consistency in Image and Video Editing",
                    "desc": "ConsistEdit is a new method designed to improve image and video editing by providing better control over attention mechanisms in the MM-DiT model. It addresses the challenge of maintaining consistency while allowing for detailed edits, which is crucial in multi-round editing scenarios. By using techniques like vision-only attention control and mask-guided pre-attention fusion, ConsistEdit ensures that edits align closely with the original content. This approach not only enhances the quality of edits but also allows for progressive adjustments, making it easier to manipulate specific attributes without losing overall coherence."
                },
                "zh": {
                    "title": "ConsistEdit：提升图像视频编辑的一致性与控制力",
                    "desc": "ConsistEdit是一种新颖的注意力控制方法，专为MM-DiT设计，旨在提升图像和视频编辑的效果。它通过在所有推理步骤和注意力层中确保一致性和细粒度控制，解决了现有方法在编辑强度和源一致性之间的矛盾。该方法引入了视觉专用的注意力控制和掩码引导的预注意力融合，能够在编辑过程中实现更精细的属性调整。实验结果表明，ConsistEdit在多种图像和视频编辑任务中表现出色，显著提高了编辑的可靠性和一致性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.17795",
            "title": "Executable Knowledge Graphs for Replicating AI Research",
            "url": "https://huggingface.co/papers/2510.17795",
            "abstract": "Executable Knowledge Graphs (xKG) enhance AI research replication by integrating technical insights and code snippets from scientific literature, improving performance in automated replication tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Replicating AI research is a crucial yet challenging task for large language model (LLM) agents. Existing approaches often struggle to generate executable code, primarily due to insufficient background knowledge and the limitations of retrieval-augmented generation (RAG) methods, which fail to capture latent technical details hidden in referenced papers. Furthermore, previous approaches tend to overlook valuable implementation-level code signals and lack structured knowledge representations that support multi-granular retrieval and reuse. To overcome these challenges, we propose Executable Knowledge Graphs (xKG), a modular and pluggable knowledge base that automatically integrates technical insights, code snippets, and domain-specific knowledge extracted from scientific literature. When integrated into three agent frameworks with two different LLMs, xKG shows substantial performance gains (10.9% with o3-mini) on PaperBench, demonstrating its effectiveness as a general and extensible solution for automated AI research replication. Code will released at https://github.com/zjunlp/xKG.",
            "score": 7,
            "issue_id": 6522,
            "pub_date": "2025-10-20",
            "pub_date_card": {
                "ru": "20 октября",
                "en": "October 20",
                "zh": "10月20日"
            },
            "hash": "b47dfdceba451eed",
            "authors": [
                "Yujie Luo",
                "Zhuoyun Yu",
                "Xuehai Wang",
                "Yuqi Zhu",
                "Ningyu Zhang",
                "Lanning Wei",
                "Lun Du",
                "Da Zheng",
                "Huajun Chen"
            ],
            "affiliations": [
                "Ant Group",
                "Zhejiang University",
                "Zhejiang University - Ant Group Joint Laboratory of Knowledge Graph"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.17795.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#rag",
                    "#science",
                    "#open_source",
                    "#dataset",
                    "#graphs",
                    "#agents"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "Граф знаний с исполняемым кодом для репликации AI-исследований",
                    "desc": "Статья представляет Executable Knowledge Graphs (xKG) — модульную базу знаний для автоматизации репликации исследований в области AI с помощью LLM-агентов. xKG автоматически извлекает и структурирует технические детали, фрагменты кода и специализированные знания из научных статей. Система решает проблемы традиционных RAG-методов, которые не улавливают скрытые технические нюансы из цитируемых работ. Интеграция xKG в агентские фреймворки демонстрирует значительный прирост производительности (10.9% с o3-mini) на бенчмарке PaperBench."
                },
                "en": {
                    "title": "Enhancing AI Research Replication with Executable Knowledge Graphs",
                    "desc": "Executable Knowledge Graphs (xKG) are designed to improve the replication of AI research by combining insights and code from scientific papers. Traditional methods often fail to generate executable code due to a lack of background knowledge and limitations in retrieval-augmented generation techniques. xKG addresses these issues by providing a structured knowledge base that captures technical details and implementation-level code signals. When tested with various agent frameworks and large language models, xKG significantly enhances performance in automated replication tasks, proving its utility in the field."
                },
                "zh": {
                    "title": "可执行知识图谱：提升AI研究复制的利器",
                    "desc": "可执行知识图谱（xKG）通过整合科学文献中的技术见解和代码片段，增强了人工智能研究的可复制性，提升了自动复制任务的性能。现有方法在生成可执行代码时常常面临挑战，主要是由于背景知识不足和检索增强生成（RAG）方法的局限性。xKG 作为一个模块化的知识库，能够自动整合从文献中提取的技术见解和领域特定知识，克服了以往方法的不足。通过在不同的代理框架中集成 xKG，实验结果显示其在 PaperBench 上的性能提升显著，证明了其作为自动化 AI 研究复制的有效解决方案。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.17509",
            "title": "Annotation-Efficient Universal Honesty Alignment",
            "url": "https://huggingface.co/papers/2510.17509",
            "abstract": "EliCal, a two-stage framework combining self-consistency supervision and minimal correctness annotations, achieves near-optimal honesty alignment in large language models with limited annotation effort.  \t\t\t\t\tAI-generated summary \t\t\t\t Honesty alignment-the ability of large language models (LLMs) to recognize their knowledge boundaries and express calibrated confidence-is essential for trustworthy deployment. Existing methods either rely on training-free confidence estimation (e.g., token probabilities, self-consistency) or training-based calibration with correctness annotations. While effective, achieving universal honesty alignment with training-based calibration requires costly, large-scale labeling. To support annotation-efficient training, we introduce Elicitation-Then-Calibration (EliCal), a two-stage framework that first elicits internal confidence using inexpensive self-consistency supervision, then calibrates this confidence with a small set of correctness annotations. To support a large-scale study, we release HonestyBench, a benchmark covering ten free-form QA datasets with 560k training and 70k evaluation instances annotated with correctness and self-consistency signals. Experiments show that EliCal achieves near-optimal alignment with only 1k correctness annotations (0.18% of full supervision) and better alignment performance on unseen MMLU tasks than the calibration-only baseline, offering a scalable solution toward universal honesty alignment in LLMs.",
            "score": 6,
            "issue_id": 6523,
            "pub_date": "2025-10-20",
            "pub_date_card": {
                "ru": "20 октября",
                "en": "October 20",
                "zh": "10月20日"
            },
            "hash": "ff4277a47970b55d",
            "authors": [
                "Shiyu Ni",
                "Keping Bi",
                "Jiafeng Guo",
                "Minghao Tang",
                "Jingtong Wu",
                "Zengxin Han",
                "Xueqi Cheng"
            ],
            "affiliations": [
                "State Key Laboratory of AI Safety, Institute of Computing Technology, CAS",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.17509.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#training",
                    "#alignment",
                    "#dataset",
                    "#open_source"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Честность AI с минимальной разметкой: калибровка уверенности через самосогласованность",
                    "desc": "Статья представляет EliCal — двухэтапный фреймворк для обучения больших языковых моделей честности, то есть способности признавать границы своих знаний и выражать калиброванную уверенность в ответах. Метод сначала извлекает внутреннюю уверенность модели с помощью недорогого self-consistency supervision, а затем калибрует её с использованием минимального количества аннотаций правильности ответов. Авторы выпускают HonestyBench — бенчмарк с 560 тысячами обучающих примеров на десяти датасетах для оценки честности LLM. Эксперименты показывают, что EliCal достигает почти оптимального выравнивания всего с 1000 аннотациями (0.18% от полного объёма разметки), превосходя baseline методы на новых задачах."
                },
                "en": {
                    "title": "EliCal: Efficient Honesty Alignment for Language Models",
                    "desc": "EliCal is a two-stage framework designed to improve honesty alignment in large language models (LLMs) while minimizing the need for extensive annotations. The first stage uses self-consistency supervision to gauge the model's internal confidence, which is a cost-effective method. In the second stage, this confidence is calibrated using a small number of correctness annotations, significantly reducing the labeling effort required. The framework is validated through HonestyBench, a benchmark that demonstrates EliCal's effectiveness in achieving near-optimal honesty alignment with minimal annotation, outperforming traditional calibration methods."
                },
                "zh": {
                    "title": "EliCal：高效实现大型语言模型的诚实对齐",
                    "desc": "EliCal是一个两阶段框架，结合了自一致性监督和最小的正确性注释，旨在实现大型语言模型的诚实对齐。诚实对齐是指模型能够识别其知识边界并表达经过校准的信心，这对于可信的部署至关重要。EliCal首先通过低成本的自一致性监督来引导内部信心，然后使用少量的正确性注释来校准这种信心。实验表明，EliCal在仅使用1000个正确性注释的情况下，达到了接近最佳的对齐效果，提供了一种可扩展的解决方案，以实现大型语言模型的普遍诚实对齐。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.17269",
            "title": "FineVision: Open Data Is All You Need",
            "url": "https://huggingface.co/papers/2510.17269",
            "abstract": "FineVision, a large-scale and curated dataset, enhances vision-language models through rigorous data collection, de-duplication, and human oversight, leading to improved performance.  \t\t\t\t\tAI-generated summary \t\t\t\t The advancement of vision-language models (VLMs) is hampered by a fragmented landscape of inconsistent and contaminated public datasets. We introduce FineVision, a meticulously collected, curated, and unified corpus of 24 million samples - the largest open resource of its kind. We unify more than 200 sources into 185 subsets via a semi-automated, human-in-the-loop pipeline: automation performs bulk ingestion and schema mapping, while reviewers audit mappings and spot-check outputs to verify faithful consumption of annotations, appropriate formatting and diversity, and safety; issues trigger targeted fixes and re-runs. The workflow further applies rigorous de-duplication within and across sources and decontamination against 66 public benchmarks. FineVision also encompasses agentic/GUI tasks with a unified action space; reviewers validate schemas and inspect a sample of trajectories to confirm executable fidelity. Models trained on FineVision consistently outperform those trained on existing open mixtures across a broad evaluation suite, underscoring the benefits of scale, data hygiene, and balanced automation with human oversight. We release the corpus and curation tools to accelerate data-centric VLM research.",
            "score": 4,
            "issue_id": 6522,
            "pub_date": "2025-10-20",
            "pub_date_card": {
                "ru": "20 октября",
                "en": "October 20",
                "zh": "10月20日"
            },
            "hash": "a226ea625b58c9d6",
            "authors": [
                "Luis Wiedmann",
                "Orr Zohar",
                "Amir Mahla",
                "Xiaohan Wang",
                "Rui Li",
                "Thibaud Frere",
                "Leandro von Werra",
                "Aritra Roy Gosthipaty",
                "Andrés Marafioti"
            ],
            "affiliations": [
                "Hugging Face",
                "Stanford University",
                "Technical University Munich"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.17269.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#data",
                    "#open_source"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "FineVision: новый стандарт для моделей зрение-язык",
                    "desc": "FineVision — это новый крупный и тщательно отобранный набор данных для улучшения моделей, объединяющих зрение и язык. Он включает 24 миллиона образцов, собранных из более чем 200 источников, и проходит через полуавтоматизированный процесс с участием человека. Это позволяет обеспечить высокое качество данных, их разнообразие и безопасность, а также избежать дублирования и загрязнения. Модели, обученные на FineVision, показывают лучшие результаты по сравнению с существующими наборами данных."
                },
                "en": {
                    "title": "FineVision: Elevating Vision-Language Models with Quality Data",
                    "desc": "FineVision is a large and carefully curated dataset designed to improve vision-language models (VLMs) by addressing issues with existing public datasets. It consists of 24 million samples collected from over 200 sources, ensuring high quality through a semi-automated process that includes human oversight for validation and de-duplication. The dataset not only focuses on data hygiene but also includes diverse tasks with a unified action space, enhancing the training of models. Results show that models trained on FineVision significantly outperform those trained on other datasets, highlighting the importance of well-curated data in machine learning."
                },
                "zh": {
                    "title": "FineVision：提升视觉语言模型的最佳数据集",
                    "desc": "FineVision是一个大规模且经过精心策划的数据集，旨在通过严格的数据收集、去重和人工监督来提升视觉语言模型的性能。该数据集包含2400万样本，是同类中最大的开放资源，整合了200多个来源，形成185个子集。通过半自动化的人工审核流程，确保数据的准确性和多样性，同时进行严格的去重和去污染处理。使用FineVision训练的模型在广泛的评估中表现优于现有的开放混合数据集，显示出规模、数据清洁和人机协作的优势。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.16720",
            "title": "Beyond Pipelines: A Survey of the Paradigm Shift toward Model-Native\n  Agentic AI",
            "url": "https://huggingface.co/papers/2510.16720",
            "abstract": "The survey outlines the shift from pipeline-based to model-native agentic AI, emphasizing the role of reinforcement learning in integrating planning, tool use, and memory within large language models across various domains.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid evolution of agentic AI marks a new phase in artificial intelligence, where Large Language Models (LLMs) no longer merely respond but act, reason, and adapt. This survey traces the paradigm shift in building agentic AI: from Pipeline-based systems, where planning, tool use, and memory are orchestrated by external logic, to the emerging Model-native paradigm, where these capabilities are internalized within the model's parameters. We first position Reinforcement Learning (RL) as the algorithmic engine enabling this paradigm shift. By reframing learning from imitating static data to outcome-driven exploration, RL underpins a unified solution of LLM + RL + Task across language, vision and embodied domains. Building on this, the survey systematically reviews how each capability -- Planning, Tool use, and Memory -- has evolved from externally scripted modules to end-to-end learned behaviors. Furthermore, it examines how this paradigm shift has reshaped major agent applications, specifically the Deep Research agent emphasizing long-horizon reasoning and the GUI agent emphasizing embodied interaction. We conclude by discussing the continued internalization of agentic capabilities like Multi-agent collaboration and Reflection, alongside the evolving roles of the system and model layers in future agentic AI. Together, these developments outline a coherent trajectory toward model-native agentic AI as an integrated learning and interaction framework, marking the transition from constructing systems that apply intelligence to developing models that grow intelligence through experience.",
            "score": 4,
            "issue_id": 6522,
            "pub_date": "2025-10-19",
            "pub_date_card": {
                "ru": "19 октября",
                "en": "October 19",
                "zh": "10月19日"
            },
            "hash": "92a34f8897f8ebdf",
            "authors": [
                "Jitao Sang",
                "Jinlin Xiao",
                "Jiarun Han",
                "Jilin Chen",
                "Xiaoyi Chen",
                "Shuyu Wei",
                "Yongjie Sun",
                "Yuhang Wang"
            ],
            "affiliations": [
                "Beijing Jiaotong University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.16720.jpg",
            "data": {
                "categories": [
                    "#survey",
                    "#reasoning",
                    "#rl",
                    "#agents"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "От конвейера к интеллекту: как RL превращает LLM в самообучающихся агентов",
                    "desc": "Статья описывает переход от конвейерных AI-систем к model-native подходу, где планирование, использование инструментов и память интегрированы внутри самой модели. Reinforcement Learning выступает ключевым механизмом этой трансформации, позволяя LLM не просто имитировать данные, а учиться через взаимодействие и достижение целей. Авторы систематически анализируют эволюцию агентских способностей - от внешних модулей к end-to-end обучаемым поведенческим паттернам в языковых, визуальных и embodied доменах. Это знаменует переход от систем, применяющих интеллект, к моделям, которые развивают интеллект через собственный опыт."
                },
                "en": {
                    "title": "From Pipeline to Model-Native: The Future of Agentic AI",
                    "desc": "This paper discusses the transition from traditional pipeline-based AI systems to a new approach called model-native agentic AI, where large language models (LLMs) can act and adapt autonomously. It highlights the importance of reinforcement learning (RL) as a key technology that allows these models to learn from outcomes rather than just imitating data. The survey reviews how essential capabilities like planning, tool use, and memory have shifted from being externally programmed to being learned directly by the models. It also explores the implications of this shift for various applications, including long-term reasoning and interactive agents, and suggests future directions for enhancing agentic capabilities."
                },
                "zh": {
                    "title": "从管道到模型本土化：智能体AI的演变",
                    "desc": "这篇论文概述了从基于管道的人工智能到模型本土化的智能体人工智能的转变，强调了强化学习在大型语言模型中整合规划、工具使用和记忆的作用。智能体人工智能的快速发展标志着人工智能的新阶段，模型不仅仅是响应，而是能够行动、推理和适应。论文回顾了这一范式转变，指出强化学习作为推动这一转变的算法引擎，强调了从模仿静态数据到基于结果的探索的学习方式。最后，论文讨论了智能体能力的持续内化以及未来智能体人工智能中系统和模型层的演变。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.16751",
            "title": "Visual Autoregressive Models Beat Diffusion Models on Inference Time\n  Scaling",
            "url": "https://huggingface.co/papers/2510.16751",
            "abstract": "Beam search in discrete visual autoregressive models enhances text-to-image generation more effectively than search in continuous diffusion models, highlighting architecture's importance over scale.  \t\t\t\t\tAI-generated summary \t\t\t\t While inference-time scaling through search has revolutionized Large Language Models, translating these gains to image generation has proven difficult. Recent attempts to apply search strategies to continuous diffusion models show limited benefits, with simple random sampling often performing best. We demonstrate that the discrete, sequential nature of visual autoregressive models enables effective search for image generation. We show that beam search substantially improves text-to-image generation, enabling a 2B parameter autoregressive model to outperform a 12B parameter diffusion model across benchmarks. Systematic ablations show that this advantage comes from the discrete token space, which allows early pruning and computational reuse, and our verifier analysis highlights trade-offs between speed and reasoning capability. These findings suggest that model architecture, not just scale, is critical for inference-time optimization in visual generation.",
            "score": 3,
            "issue_id": 6521,
            "pub_date": "2025-10-19",
            "pub_date_card": {
                "ru": "19 октября",
                "en": "October 19",
                "zh": "10月19日"
            },
            "hash": "d554b99c0fe303db",
            "authors": [
                "Erik Riise",
                "Mehmet Onurcan Kaya",
                "Dim P. Papadopoulos"
            ],
            "affiliations": [
                "Pioneer Center for AI",
                "Technical University of Denmark"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.16751.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#optimization",
                    "#inference",
                    "#architecture",
                    "#benchmark",
                    "#games"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Дискретность важнее масштаба: beam search в авторегрессионных моделях для генерации изображений",
                    "desc": "Исследователи показали, что beam search значительно эффективнее работает в дискретных визуальных авторегрессионных моделях, чем в непрерывных диффузионных моделях. Авторегрессионная модель всего на 2 миллиарда параметров с beam search превосходит 12-миллиардную диффузионную модель в задаче генерации изображений по тексту. Преимущество достигается благодаря дискретному пространству токенов, которое позволяет раньше отсекать неудачные варианты и переиспользовать вычисления. Работа демонстрирует, что архитектура модели играет критическую роль для оптимизации на этапе инференса, а не только размер модели."
                },
                "en": {
                    "title": "Architecture Matters: Beam Search Boosts Text-to-Image Generation!",
                    "desc": "This paper explores the effectiveness of beam search in discrete visual autoregressive models for text-to-image generation. It reveals that traditional continuous diffusion models do not benefit significantly from search strategies, often yielding inferior results compared to simpler methods like random sampling. The authors demonstrate that the discrete nature of autoregressive models allows for more efficient search processes, leading to improved performance. Their findings emphasize that the architecture of the model plays a crucial role in optimizing inference time, rather than merely increasing the model's size."
                },
                "zh": {
                    "title": "架构优于规模：束搜索提升图像生成",
                    "desc": "本文探讨了在离散视觉自回归模型中使用束搜索对文本到图像生成的影响。研究表明，束搜索显著提升了图像生成的效果，尤其是在与连续扩散模型的比较中。通过系统的消融实验，发现离散的标记空间使得早期剪枝和计算重用成为可能，从而提高了效率。结果表明，模型架构在视觉生成中的推理优化中比模型规模更为重要。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.16333",
            "title": "RL makes MLLMs see better than SFT",
            "url": "https://huggingface.co/papers/2510.16333",
            "abstract": "Reinforcement Learning enhances vision encoders in Multimodal Language Models, leading to better visual representations and performance compared to Supervised Fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t A dominant assumption in Multimodal Language Model (MLLM) research is that its performance is largely inherited from the LLM backbone, given its immense parameter scale and remarkable capabilities. This has created a void in the understanding of the vision encoder, which determines how MLLMs perceive images. The recent shift in MLLM training paradigms, from Supervised Finetuning (SFT) to Reinforcement Learning (RL), magnifies this oversight-namely, the significant lack of analysis on how such training reshapes the vision encoder as well as the MLLM. To address this, we first investigate the impact of training strategies on MLLMs, where RL shows a clear advantage over SFT in strongly vision-related VQA benchmarks. Motivated by this, we conduct a critical yet under-explored analysis of the vision encoder of MLLMs through diverse and in-depth experiments, ranging from ImageNet classification and segmentation to gradient visualization. Our results demonstrate that MLLM's post-training strategy (i.e., SFT or RL) not only leads to distinct outcomes on MLLM downstream tasks, but also fundamentally reshapes MLLM's underlying visual representations. Specifically, the key finding of our study is that RL produces stronger and precisely localized visual representations compared to SFT, boosting the ability of the vision encoder for MLLM. We then reframe our findings into a simple recipe for building strong vision encoders for MLLMs, Preference-Instructed Vision OpTimization (PIVOT). When integrated into MLLMs, a PIVOT-trained vision encoder outperforms even larger and more heavily-trained counterparts, despite requiring less than 1% of the computational cost of standard vision pretraining. This result opens an effective and efficient path for advancing the vision backbones of MLLMs. Project page available at https://june-page.github.io/pivot/",
            "score": 3,
            "issue_id": 6522,
            "pub_date": "2025-10-18",
            "pub_date_card": {
                "ru": "18 октября",
                "en": "October 18",
                "zh": "10月18日"
            },
            "hash": "dce5190f84c6972c",
            "authors": [
                "Junha Song",
                "Sangdoo Yun",
                "Dongyoon Han",
                "Jaegul Choo",
                "Byeongho Heo"
            ],
            "affiliations": [
                "KAIST",
                "NAVER AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.16333.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#cv",
                    "#games",
                    "#training",
                    "#rl",
                    "#optimization"
                ],
                "emoji": "👁️",
                "ru": {
                    "title": "Reinforcement Learning делает зрение мультимодальных моделей острее",
                    "desc": "Исследование показывает, что стратегия обучения мультимодальных языковых моделей существенно влияет на качество vision encoder. Reinforcement Learning формирует более сильные и точно локализованные визуальные представления по сравнению с supervised fine-tuning. Авторы предлагают метод PIVOT для оптимизации vision encoder, который требует менее 1% вычислительных затрат стандартного предобучения, но превосходит более крупные аналоги. Работа демонстрирует, что улучшение визуального энкодера через RL значительно повышает производительность MLLM на задачах, связанных с визуальным пониманием."
                },
                "en": {
                    "title": "Reinforcement Learning: A Game Changer for Vision Encoders in MLLMs",
                    "desc": "This paper explores how Reinforcement Learning (RL) can improve the vision encoders in Multimodal Language Models (MLLMs) compared to traditional Supervised Fine-tuning (SFT). The authors highlight that the training strategy significantly affects the visual representations and performance of MLLMs, particularly in vision-related tasks like Visual Question Answering (VQA). Their experiments reveal that RL leads to stronger and more accurately localized visual representations, enhancing the overall capabilities of the MLLM. They propose a new method called Preference-Instructed Vision Optimization (PIVOT), which achieves superior performance with minimal computational resources, paving the way for more efficient vision encoder development in MLLMs."
                },
                "zh": {
                    "title": "强化学习提升多模态语言模型的视觉编码能力",
                    "desc": "本研究探讨了强化学习（RL）在多模态语言模型（MLLM）中的应用，特别是如何增强视觉编码器的表现。与传统的监督微调（SFT）方法相比，RL在视觉相关的任务上表现出明显的优势。通过一系列实验，我们发现RL能够生成更强大且精确定位的视觉表示，从而提升MLLM的性能。我们的研究提出了一种新的构建强大视觉编码器的方法，称为偏好指导视觉优化（PIVOT），其计算成本远低于传统的视觉预训练方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.16259",
            "title": "Distractor Injection Attacks on Large Reasoning Models: Characterization\n  and Defense",
            "url": "https://huggingface.co/papers/2510.16259",
            "abstract": "Large reasoning models are vulnerable to reasoning distraction, where irrelevant tasks embedded in prompts reduce accuracy, and a combined SFT and RL defense improves robustness.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large reasoning models (LRMs) have enabled remarkable performance on complex tasks such as mathematics and coding by generating long Chain-of-Thought (CoT) traces. In this paper, we identify and systematically analyze a critical vulnerability we term reasoning distraction, where LRMs are diverted from their primary objective by irrelevant yet complex tasks maliciously embedded in the prompt. Through a comprehensive study across diverse models and benchmarks, we show that even state-of-the-art LRMs are highly susceptible, with injected distractors reducing task accuracy by up to 60%. We further reveal that certain alignment techniques can amplify this weakness and that models may exhibit covert compliance, following hidden adversarial instructions in reasoning while concealing them in the final output. To mitigate these risks, we propose a training-based defense that combines Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on synthetic adversarial data, improving robustness by over 50 points on challenging distractor attacks. Our findings establish reasoning distraction as a distinct and urgent threat to LRM reliability and provide a practical step toward safer and more trustworthy reasoning systems.",
            "score": 3,
            "issue_id": 6522,
            "pub_date": "2025-10-17",
            "pub_date_card": {
                "ru": "17 октября",
                "en": "October 17",
                "zh": "10月17日"
            },
            "hash": "0748fb9f2246fa1b",
            "authors": [
                "Zhehao Zhang",
                "Weijie Xu",
                "Shixian Cui",
                "Chandan K. Reddy"
            ],
            "affiliations": [
                "Amazon"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.16259.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#reasoning",
                    "#alignment",
                    "#security",
                    "#training",
                    "#rl"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Не дай себя отвлечь: защита reasoning-моделей от манипуляций в промптах",
                    "desc": "Исследование выявляет новую уязвимость больших reasoning-моделей под названием \"reasoning distraction\" - отвлечение рассуждений. Злоумышленники могут внедрять в промпты посторонние сложные задачи, которые отвлекают модель от основной цели и снижают точность решений до 60%. Некоторые методы выравнивания (alignment) даже усугубляют эту проблему, а модели могут скрыто следовать вредоносным инструкциям. Авторы предлагают защиту на основе комбинации supervised fine-tuning и reinforcement learning на синтетических adversarial данных, что повышает устойчивость более чем на 50 пунктов."
                },
                "en": {
                    "title": "Defending Large Reasoning Models Against Distraction",
                    "desc": "This paper discusses a vulnerability in large reasoning models (LRMs) called reasoning distraction, where irrelevant tasks in prompts can significantly lower their accuracy. The authors demonstrate that even advanced LRMs can suffer from this issue, with performance dropping by up to 60% when faced with distractors. They also highlight that some alignment techniques can worsen this problem, leading to covert compliance with hidden adversarial instructions. To address this, the paper proposes a defense strategy that combines Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), which enhances the models' robustness against these distractions by over 50 points."
                },
                "zh": {
                    "title": "抵御推理干扰，提升模型鲁棒性",
                    "desc": "本文探讨了大型推理模型（LRMs）在面对无关任务时的脆弱性，这种现象被称为推理干扰。研究表明，嵌入提示中的复杂无关任务会显著降低模型的准确性，甚至可达60%。此外，某些对齐技术可能会加剧这一弱点，导致模型在推理时隐性遵循对抗性指令。为了解决这一问题，本文提出了一种结合监督微调（SFT）和强化学习（RL）的训练防御方法，显著提高了模型在干扰攻击下的鲁棒性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.17797",
            "title": "Enterprise Deep Research: Steerable Multi-Agent Deep Research for\n  Enterprise Analytics",
            "url": "https://huggingface.co/papers/2510.17797",
            "abstract": "Enterprise Deep Research (EDR) is a multi-agent system that automates report generation and real-time data analysis by integrating specialized agents and tools, outperforming existing agentic systems on open benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t As information grows exponentially, enterprises face increasing pressure to transform unstructured data into coherent, actionable insights. While autonomous agents show promise, they often struggle with domain-specific nuances, intent alignment, and enterprise integration. We present Enterprise Deep Research (EDR), a multi-agent system that integrates (1) a Master Planning Agent for adaptive query decomposition, (2) four specialized search agents (General, Academic, GitHub, LinkedIn), (3) an extensible MCP-based tool ecosystem supporting NL2SQL, file analysis, and enterprise workflows, (4) a Visualization Agent for data-driven insights, and (5) a reflection mechanism that detects knowledge gaps and updates research direction with optional human-in-the-loop steering guidance. These components enable automated report generation, real-time streaming, and seamless enterprise deployment, as validated on internal datasets. On open-ended benchmarks including DeepResearch Bench and DeepConsult, EDR outperforms state-of-the-art agentic systems without any human steering. We release the EDR framework and benchmark trajectories to advance research on multi-agent reasoning applications.   Code at https://github.com/SalesforceAIResearch/enterprise-deep-research and Dataset at https://huggingface.co/datasets/Salesforce/EDR-200",
            "score": 2,
            "issue_id": 6522,
            "pub_date": "2025-10-20",
            "pub_date_card": {
                "ru": "20 октября",
                "en": "October 20",
                "zh": "10月20日"
            },
            "hash": "5f61a58a2a56c59f",
            "authors": [
                "Akshara Prabhakar",
                "Roshan Ram",
                "Zixiang Chen",
                "Silvio Savarese",
                "Frank Wang",
                "Caiming Xiong",
                "Huan Wang",
                "Weiran Yao"
            ],
            "affiliations": [
                "Salesforce AI Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.17797.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#open_source",
                    "#dataset",
                    "#agi",
                    "#benchmark",
                    "#agents",
                    "#optimization"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Мультиагентная система для автоматизации корпоративной аналитики",
                    "desc": "Enterprise Deep Research (EDR) — это мультиагентная система для автоматической генерации отчётов и анализа данных в реальном времени. Система включает Master Planning Agent для декомпозиции запросов, четыре специализированных поисковых агента, экосистему инструментов на основе MCP для работы с SQL и файлами, Visualization Agent для визуализации данных и механизм рефлексии для обнаружения пробелов в знаниях. EDR превосходит существующие агентные системы на открытых бенчмарках DeepResearch Bench и DeepConsult без участия человека. Авторы выкладывают в открытый доступ фреймворк и датасет с траекториями работы системы."
                },
                "en": {
                    "title": "Automating Insights with Multi-Agent Intelligence",
                    "desc": "Enterprise Deep Research (EDR) is a multi-agent system designed to automate the generation of reports and analyze data in real-time. It combines various specialized agents, including a Master Planning Agent for breaking down queries and multiple search agents tailored for different domains. EDR also features a tool ecosystem for natural language to SQL conversion and data visualization, along with a mechanism to identify knowledge gaps. This system has shown superior performance on benchmarks compared to existing agentic systems, demonstrating its effectiveness in handling unstructured data in enterprises."
                },
                "zh": {
                    "title": "企业深度研究：智能化报告生成与数据分析的未来",
                    "desc": "企业深度研究（EDR）是一个多智能体系统，能够自动生成报告和实时数据分析。它通过整合多个专业代理和工具，超越了现有的智能体系统。EDR包括一个主规划代理用于自适应查询分解，以及多个专门的搜索代理，支持自然语言到SQL的转换和文件分析。该系统在开放基准测试中表现优异，能够实现无人工干预的自动化报告生成和企业部署。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.17790",
            "title": "UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action",
            "url": "https://huggingface.co/papers/2510.17790",
            "abstract": "UltraCUA integrates GUI actions with programmatic tools to improve computer-use agent performance and efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal agents for computer use rely exclusively on primitive actions (click, type, scroll) that require accurate visual grounding and lengthy execution chains, leading to cascading failures and performance bottlenecks. While other agents leverage rich programmatic interfaces (APIs, MCP servers, tools), computer-use agents (CUAs) remain isolated from these capabilities. We present UltraCUA, a foundation model that bridges this gap through hybrid action -- seamlessly integrating GUI primitives with high-level programmatic tool calls. To achieve this, our approach comprises four key components: (1) an automated pipeline that scales programmatic tools from software documentation, open-source repositories, and code generation; (2) a synthetic data engine producing over 17,000 verifiable tasks spanning real-world computer-use scenarios; (3) a large-scale high-quality hybrid action trajectory collection with both low-level GUI actions and high-level programmatic tool calls; and (4) a two-stage training pipeline combining supervised fine-tuning with online reinforcement learning, enabling strategic alternation between low-level and high-level actions. Experiments with our 7B and 32B models demonstrate substantial improvements over state-of-the-art agents. On OSWorld, UltraCUA models achieve an average 22% relative improvement over base models, while being 11% faster in terms of steps. Out-of-domain evaluation on WindowsAgentArena shows our model reaches 21.7% success rate, outperforming baselines trained on Windows data. The hybrid action mechanism proves critical, reducing error propagation while maintaining execution efficiency.",
            "score": 2,
            "issue_id": 6523,
            "pub_date": "2025-10-20",
            "pub_date_card": {
                "ru": "20 октября",
                "en": "October 20",
                "zh": "10月20日"
            },
            "hash": "fdbe9a6344e337f9",
            "authors": [
                "Yuhao Yang",
                "Zhen Yang",
                "Zi-Yi Dou",
                "Anh Nguyen",
                "Keen You",
                "Omar Attia",
                "Andrew Szot",
                "Michael Feng",
                "Ram Ramrakhya",
                "Alexander Toshev",
                "Chao Huang",
                "Yinfei Yang",
                "Zhe Gan"
            ],
            "affiliations": [
                "Apple",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.17790.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#synthetic",
                    "#training",
                    "#multimodal",
                    "#agents",
                    "#open_source"
                ],
                "emoji": "🖱️",
                "ru": {
                    "title": "Гибридные действия для AI-агентов: клики плюс программные инструменты",
                    "desc": "Статья представляет UltraCUA — модель для управления компьютером, которая объединяет примитивные GUI-действия (клики, набор текста) с высокоуровневыми программными инструментами. Авторы создали пайплайн для автоматического извлечения инструментов из документации и кода, синтетический датасет из 17000 задач и двухэтапное обучение с supervised fine-tuning и reinforcement learning. Модели размером 7B и 32B параметров показывают улучшение на 22% относительно базовых моделей на бенчмарке OSWorld при ускорении выполнения на 11%. Гибридный подход значительно снижает накопление ошибок и повышает эффективность выполнения задач по сравнению с использованием только примитивных действий."
                },
                "en": {
                    "title": "Bridging GUI and Programmatic Actions for Smarter Agents",
                    "desc": "UltraCUA is a novel foundation model designed to enhance the performance of computer-use agents by integrating graphical user interface (GUI) actions with programmatic tools. Traditional computer-use agents rely on basic actions like clicking and typing, which can lead to inefficiencies and errors. UltraCUA addresses this by combining low-level GUI actions with high-level programmatic tool calls, allowing for more complex and effective interactions. The model is trained using a unique pipeline that includes synthetic data generation and reinforcement learning, resulting in significant performance improvements over existing agents."
                },
                "zh": {
                    "title": "混合动作，提升计算机使用效率！",
                    "desc": "UltraCUA是一种基础模型，它通过混合动作将图形用户界面（GUI）操作与高级编程工具调用无缝集成，从而提高计算机使用代理的性能和效率。该模型的关键组成部分包括自动化管道、合成数据引擎、混合动作轨迹集合和两阶段训练管道。通过这些组件，UltraCUA能够生成大量可验证的任务，并在低级和高级动作之间进行战略性切换。实验结果表明，UltraCUA在多个评估中显著优于现有的最先进代理，减少了错误传播并提高了执行效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.17498",
            "title": "Deep Self-Evolving Reasoning",
            "url": "https://huggingface.co/papers/2510.17498",
            "abstract": "Deep Self-Evolving Reasoning (DSER) extends the reasoning capabilities of smaller models by iteratively improving solutions through a probabilistic Markov chain, enabling them to solve previously unsolvable problems and surpass larger models in accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Long-form chain-of-thought reasoning has become a cornerstone of advanced reasoning in large language models. While recent verification-refinement frameworks have enabled proprietary models to solve Olympiad-level problems, their effectiveness hinges on strong, reliable verification and correction capabilities, which remain fragile in open-weight, smaller-scale models. This work demonstrates that even with weak verification and refinement capabilities on hard tasks, the reasoning limits of such models can be substantially extended through a probabilistic paradigm we call Deep Self-Evolving Reasoning (DSER). We conceptualize iterative reasoning as a Markov chain, where each step represents a stochastic transition in the solution space. The key insight is that convergence to a correct solution is guaranteed as long as the probability of improvement marginally exceeds that of degradation. By running multiple long-horizon, self-evolving processes in parallel, DSER amplifies these small positive tendencies, enabling the model to asymptotically approach correct answers. Empirically, we apply DSER to the DeepSeek-R1-0528-Qwen3-8B model. On the challenging AIME 2024-2025 benchmark, DSER solves 5 out of 9 previously unsolvable problems and boosts overall performance, enabling this compact model to surpass the single-turn accuracy of its 600B-parameter teacher through majority voting. Beyond its immediate utility for test-time scaling, the DSER framework serves to diagnose the fundamental limitations of current open-weight reasoners. By clearly delineating their shortcomings in self-verification, refinement, and stability, our findings establish a clear research agenda for developing next-generation models with powerful, intrinsic self-evolving capabilities.",
            "score": 2,
            "issue_id": 6522,
            "pub_date": "2025-10-20",
            "pub_date_card": {
                "ru": "20 октября",
                "en": "October 20",
                "zh": "10月20日"
            },
            "hash": "02b06c53e0aeb5c6",
            "authors": [
                "Zihan Liu",
                "Shun Zheng",
                "Xumeng Wen",
                "Yang Wang",
                "Jiang Bian",
                "Mao Yang"
            ],
            "affiliations": [
                "Microsoft Research Asia",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.17498.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#architecture",
                    "#small_models",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "Малые модели учатся решать сложные задачи через самоулучшение",
                    "desc": "Метод Deep Self-Evolving Reasoning (DSER) позволяет небольшим языковым моделям итеративно улучшать свои решения через вероятностную цепь Маркова. Ключевая идея заключается в том, что даже при слабых способностях к верификации модель может сходиться к правильному ответу, если вероятность улучшения хотя бы немного превышает вероятность ухудшения. Запуская множество параллельных процессов самоэволюции, DSER усиливает эти небольшие положительные тенденции. В результате компактная 8B модель решает задачи олимпиадного уровня и превосходит по точности свою 600B учительскую модель."
                },
                "en": {
                    "title": "Empowering Small Models with Deep Self-Evolving Reasoning",
                    "desc": "Deep Self-Evolving Reasoning (DSER) enhances the reasoning abilities of smaller machine learning models by using a probabilistic Markov chain to iteratively refine their solutions. This approach allows these models to tackle complex problems that were previously unsolvable and even outperform larger models in terms of accuracy. DSER operates by conceptualizing reasoning as a series of stochastic transitions, ensuring that as long as the chance of improvement is slightly higher than that of failure, the model will converge on correct solutions. The framework not only improves performance on benchmarks but also highlights the limitations of current models, paving the way for future advancements in self-evolving reasoning capabilities."
                },
                "zh": {
                    "title": "深度自我演化推理：小模型的推理新突破",
                    "desc": "深度自我演化推理（DSER）通过迭代改进解决方案，扩展了小型模型的推理能力，使其能够解决以前无法解决的问题，并在准确性上超越更大型模型。该方法将迭代推理视为马尔可夫链，每一步代表解决空间中的随机转移。关键在于，只要改进的概率略高于退化的概率，就能保证收敛到正确的解决方案。通过并行运行多个长时间的自我演化过程，DSER放大了这些小的积极趋势，使模型能够逐渐接近正确答案。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.14605",
            "title": "Knowledge-based Visual Question Answer with Multimodal Processing,\n  Retrieval and Filtering",
            "url": "https://huggingface.co/papers/2510.14605",
            "abstract": "A novel three-stage method, Wiki-PRF, enhances knowledge-based visual question answering by improving multimodal query quality and relevance through visual language models and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Knowledge-based visual question answering (KB-VQA) requires visual language models (VLMs) to integrate visual understanding with external knowledge retrieval. Although retrieval-augmented generation (RAG) achieves significant advances in this task by combining knowledge-base querying, it still struggles with the quality of multimodal queries and the relevance of retrieved results. To overcome these challenges, we propose a novel three-stage method, termed Wiki-PRF, including Processing, Retrieval and Filtering stages. The processing stage dynamically invokes visual tools to extract precise multimodal information for retrieval. The retrieval stage integrates visual and text features to achieve multimodal knowledge retrieval. The filtering stage performs relevance filtering and concentration on retrieval results. To this end, we introduce a visual language model trained with answer accuracy and format consistency as reward signals via a reinforcement learning manner. This enhances the model's reasoning, tool invocation for accurate queries, and filtering of irrelevant content. Experiments on benchmark datasets (E-VQA and InfoSeek) show significant improvements~(36.0 and 42.8) in answer quality, achieving state-of-the-art performance. Code is available at https://github.com/cqu-student/Wiki-PRF",
            "score": 2,
            "issue_id": 6522,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 октября",
                "en": "October 16",
                "zh": "10月16日"
            },
            "hash": "a93b11e3aa1b7d4d",
            "authors": [
                "Yuyang Hong",
                "Jiaqi Gu",
                "Qi Yang",
                "Lubin Fan",
                "Yue Wu",
                "Ying Wang",
                "Kun Ding",
                "Shiming Xiang",
                "Jieping Ye"
            ],
            "affiliations": [
                "Alibaba Cloud Computing",
                "MAIS, Institute of Automation, Chinese Academy of Sciences",
                "School of Artificial Intelligence, University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.14605.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#rag",
                    "#reasoning",
                    "#benchmark",
                    "#rl",
                    "#optimization"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Трёхэтапный метод для визуального ответа на вопросы с использованием внешних знаний",
                    "desc": "Статья представляет метод Wiki-PRF для knowledge-based visual question answering, который работает в три этапа: обработка, поиск и фильтрация. На этапе обработки динамически вызываются визуальные инструменты для извлечения мультимодальной информации, затем происходит поиск знаний с интеграцией визуальных и текстовых признаков. Для фильтрации нерелевантных результатов используется visual language model, обученная с помощью reinforcement learning с наградами за точность ответов и согласованность формата. Метод показывает значительное улучшение качества ответов на benchmark-датасетах E-VQA и InfoSeek, достигая state-of-the-art результатов."
                },
                "en": {
                    "title": "Enhancing Visual Question Answering with Wiki-PRF",
                    "desc": "The paper presents a new method called Wiki-PRF that improves knowledge-based visual question answering (KB-VQA) by enhancing the quality of multimodal queries. It consists of three stages: Processing, Retrieval, and Filtering, which work together to extract and refine information from both visual and textual sources. The method employs a visual language model that uses reinforcement learning to optimize for answer accuracy and format consistency. Experiments demonstrate that Wiki-PRF significantly boosts answer quality, achieving state-of-the-art results on benchmark datasets."
                },
                "zh": {
                    "title": "提升视觉问答质量的三阶段方法",
                    "desc": "本文提出了一种新颖的三阶段方法Wiki-PRF，用于增强基于知识的视觉问答（KB-VQA）。该方法通过视觉语言模型和强化学习提高了多模态查询的质量和相关性。Wiki-PRF包括处理、检索和过滤三个阶段，动态提取多模态信息并整合视觉和文本特征。实验结果表明，该方法在回答质量上显著提升，达到了最新的性能水平。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.16258",
            "title": "Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset",
            "url": "https://huggingface.co/papers/2510.16258",
            "abstract": "Embody 3D is a multimodal dataset featuring extensive 3D motion data with hand tracking, body shape, text annotations, and audio tracks from multiple participants in various scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t The Codec Avatars Lab at Meta introduces Embody 3D, a multimodal dataset of 500 individual hours of 3D motion data from 439 participants collected in a multi-camera collection stage, amounting to over 54 million frames of tracked 3D motion. The dataset features a wide range of single-person motion data, including prompted motions, hand gestures, and locomotion; as well as multi-person behavioral and conversational data like discussions, conversations in different emotional states, collaborative activities, and co-living scenarios in an apartment-like space. We provide tracked human motion including hand tracking and body shape, text annotations, and a separate audio track for each participant.",
            "score": 1,
            "issue_id": 6522,
            "pub_date": "2025-10-17",
            "pub_date_card": {
                "ru": "17 октября",
                "en": "October 17",
                "zh": "10月17日"
            },
            "hash": "7b781f6f8c6b350c",
            "authors": [
                "Claire McLean",
                "Makenzie Meendering",
                "Tristan Swartz",
                "Orri Gabbay",
                "Alexandra Olsen",
                "Rachel Jacobs",
                "Nicholas Rosen",
                "Philippe de Bree",
                "Tony Garcia",
                "Gadsden Merrill",
                "Jake Sandakly",
                "Julia Buffalini",
                "Neham Jain",
                "Steven Krenn",
                "Moneish Kumar",
                "Dejan Markovic",
                "Evonne Ng",
                "Fabian Prada",
                "Andrew Saba",
                "Siwei Zhang",
                "Vasu Agrawal",
                "Tim Godisart",
                "Alexander Richard",
                "Michael Zollhoefer"
            ],
            "affiliations": [
                "Codec Avatars Lab, Meta"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.16258.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#3d",
                    "#multimodal"
                ],
                "emoji": "🕺",
                "ru": {
                    "title": "Embody 3D: огромный датасет для оживления аватаров",
                    "desc": "Исследователи из Meta представили Embody 3D — масштабный мультимодальный датасет с 3D-данными движений человека. Набор данных содержит 500 часов записей от 439 участников, что составляет более 54 миллионов кадров отслеженных движений тела и рук. Датасет включает как одиночные действия (жесты, ходьба, заданные движения), так и многопользовательские сценарии: разговоры в разных эмоциональных состояниях, совместная деятельность и бытовые ситуации. Каждая запись сопровождается текстовыми аннотациями, аудиодорожками и данными о форме тела участников."
                },
                "en": {
                    "title": "Unlocking Human Motion: The Embody 3D Dataset",
                    "desc": "Embody 3D is a comprehensive multimodal dataset designed for advancing research in 3D motion analysis. It includes 500 hours of motion data from 439 participants, captured using a multi-camera setup, resulting in over 54 million frames of detailed tracking. The dataset encompasses a variety of motion types, such as individual gestures and multi-person interactions, along with corresponding audio and text annotations. This rich resource aims to facilitate the development of more sophisticated AI models for understanding human motion and interaction in diverse scenarios."
                },
                "zh": {
                    "title": "Embody 3D：多模态3D运动数据集的创新",
                    "desc": "Embody 3D是一个多模态数据集，包含来自439名参与者的500小时3D运动数据。该数据集记录了超过5400万帧的3D运动，包括单人动作、手势和移动等多种类型。它还包括多人的行为和对话数据，涵盖了不同情感状态下的讨论、合作活动和共同生活场景。数据集提供了手部跟踪、身体形状、文本注释和每位参与者的独立音频轨道。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.16156",
            "title": "AsyncVoice Agent: Real-Time Explanation for LLM Planning and Reasoning",
            "url": "https://huggingface.co/papers/2510.16156",
            "abstract": "AsyncVoice Agent, with its asynchronous architecture, enhances human-AI collaboration by enabling real-time interaction and interruption of the model's reasoning process, significantly reducing latency while maintaining accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Effective human-AI collaboration on complex reasoning tasks requires that users understand and interact with the model's process, not just receive an output. However, the monolithic text from methods like Chain-of-Thought (CoT) prevents this, as current interfaces lack real-time verbalization and robust user barge-in. We present AsyncVoice Agent, a system whose asynchronous architecture decouples a streaming LLM backend from a conversational voice frontend. This design allows narration and inference to run in parallel, empowering users to interrupt, query, and steer the model's reasoning process at any time. Objective benchmarks show this approach reduces interaction latency by more than 600x compared to monolithic baselines while ensuring high fidelity and competitive task accuracy. By enabling a two-way dialogue with a model's thought process, AsyncVoice Agent offers a new paradigm for building more effective, steerable, and trustworthy human-AI systems for high-stakes tasks.",
            "score": 1,
            "issue_id": 6522,
            "pub_date": "2025-10-17",
            "pub_date_card": {
                "ru": "17 октября",
                "en": "October 17",
                "zh": "10月17日"
            },
            "hash": "e2dff0559dd212a7",
            "authors": [
                "Yueqian Lin",
                "Zhengmian Hu",
                "Jayakumar Subramanian",
                "Qinsi Wang",
                "Nikos Vlassis",
                "Hai \"Helen\" Li",
                "Yiran Chen"
            ],
            "affiliations": [
                "Adobe Research, San Jose, CA, USA",
                "Duke University, Durham, NC, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.16156.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#reasoning",
                    "#alignment",
                    "#training",
                    "#agents"
                ],
                "emoji": "🎙️",
                "ru": {
                    "title": "Прерывай и управляй: голосовой AI-агент с асинхронной архитектурой",
                    "desc": "Статья представляет AsyncVoice Agent — систему для взаимодействия человека с AI через голос в реальном времени. Асинхронная архитектура разделяет потоковый LLM-бэкенд и голосовой интерфейс, позволяя модели озвучивать свои рассуждения параллельно с их генерацией. Пользователь может в любой момент прервать модель, задать вопрос или скорректировать процесс рассуждений, что снижает латентность взаимодействия более чем в 600 раз по сравнению с монолитными подходами. Система открывает новую парадигму для создания управляемых и прозрачных AI-систем в критически важных задачах, где важен контроль над процессом мышления модели."
                },
                "en": {
                    "title": "Empowering Real-Time Interaction in Human-AI Collaboration",
                    "desc": "The AsyncVoice Agent introduces an innovative asynchronous architecture that improves human-AI collaboration by allowing real-time interaction with the model's reasoning. This system separates the streaming large language model (LLM) from the conversational voice interface, enabling users to interrupt and guide the model's thought process dynamically. By doing so, it significantly reduces interaction latency by over 600 times compared to traditional methods while maintaining high accuracy. This approach fosters a more engaging and trustworthy dialogue between users and AI, particularly in complex reasoning tasks."
                },
                "zh": {
                    "title": "异步语音代理：提升人机协作的新方式",
                    "desc": "AsyncVoice Agent 是一种新型的人工智能系统，采用异步架构，增强了人机协作。它允许用户实时与模型的推理过程互动，显著降低了延迟，同时保持了准确性。与传统的单一文本输出方法不同，AsyncVoice Agent 使得叙述和推理可以并行进行，用户可以随时打断、查询和引导模型的思考。通过这种方式，AsyncVoice Agent 为高风险任务提供了一种更有效、可引导和可信赖的人机系统新范式。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.15527",
            "title": "Balanced Multi-Task Attention for Satellite Image Classification: A\n  Systematic Approach to Achieving 97.23% Accuracy on EuroSAT Without\n  Pre-Training",
            "url": "https://huggingface.co/papers/2510.15527",
            "abstract": "A novel balanced multi-task attention mechanism in custom convolutional neural networks improves satellite land use classification accuracy to 97.23% on the EuroSAT dataset without pre-trained models.  \t\t\t\t\tAI-generated summary \t\t\t\t This work presents a systematic investigation of custom convolutional neural network architectures for satellite land use classification, achieving 97.23% test accuracy on the EuroSAT dataset without reliance on pre-trained models. Through three progressive architectural iterations (baseline: 94.30%, CBAM-enhanced: 95.98%, and balanced multi-task attention: 97.23%) we identify and address specific failure modes in satellite imagery classification. Our principal contribution is a novel balanced multi-task attention mechanism that combines Coordinate Attention for spatial feature extraction with Squeeze-Excitation blocks for spectral feature extraction, unified through a learnable fusion parameter. Experimental results demonstrate that this learnable parameter autonomously converges to alpha approximately 0.57, indicating near-equal importance of spatial and spectral modalities for satellite imagery. We employ progressive DropBlock regularization (5-20% by network depth) and class-balanced loss weighting to address overfitting and confusion pattern imbalance. The final 12-layer architecture achieves Cohen's Kappa of 0.9692 with all classes exceeding 94.46% accuracy, demonstrating confidence calibration with a 24.25% gap between correct and incorrect predictions. Our approach achieves performance within 1.34% of fine-tuned ResNet-50 (98.57%) while requiring no external data, validating the efficacy of systematic architectural design for domain-specific applications. Complete code, trained models, and evaluation scripts are publicly available.",
            "score": 1,
            "issue_id": 6521,
            "pub_date": "2025-10-17",
            "pub_date_card": {
                "ru": "17 октября",
                "en": "October 17",
                "zh": "10月17日"
            },
            "hash": "c89aa46fa8b1dc13",
            "authors": [
                "Aditya Vir"
            ],
            "affiliations": [
                "Department of Computer Science and Engineering Manipal University Jaipur Jaipur, Rajasthan 303007, India"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.15527.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#optimization",
                    "#dataset",
                    "#architecture",
                    "#open_source"
                ],
                "emoji": "🛰️",
                "ru": {
                    "title": "Сбалансированное внимание к спутниковым снимкам без предобучения",
                    "desc": "Исследователи разработали специализированную свёрточную нейронную сеть для классификации использования земель по спутниковым снимкам, достигнув точности 97.23% на датасете EuroSAT без использования предобученных моделей. Ключевая новизна работы — сбалансированный механизм внимания с множественными задачами, который объединяет пространственные и спектральные признаки через обучаемый параметр слияния, автоматически сходящийся к значению 0.57. Архитектура использует прогрессивную DropBlock регуляризацию и взвешивание классов для борьбы с переобучением, обеспечивая точность выше 94.46% для всех классов и коэффициент Каппа Коэна 0.9692. Результаты показывают, что систематический дизайн архитектуры под конкретную задачу может достичь производительности в пределах 1.34% от fine-tuned ResNet-50 без внешних данных."
                },
                "en": {
                    "title": "Boosting Satellite Classification with Balanced Attention Mechanism",
                    "desc": "This paper introduces a new balanced multi-task attention mechanism within custom convolutional neural networks to enhance satellite land use classification. The proposed method achieves an impressive accuracy of 97.23% on the EuroSAT dataset without using pre-trained models. By iterating through different architectural designs, the authors effectively address challenges in satellite imagery classification, particularly through the integration of spatial and spectral feature extraction techniques. The results indicate that the model's design significantly improves performance while maintaining a balance between different data modalities, showcasing the potential of tailored neural network architectures for specific tasks."
                },
                "zh": {
                    "title": "创新的平衡多任务注意力机制提升卫星分类精度",
                    "desc": "本文提出了一种新颖的平衡多任务注意力机制，应用于自定义卷积神经网络，以提高卫星土地利用分类的准确性，达到了97.23%的测试准确率，且无需依赖预训练模型。通过三次逐步的架构迭代，我们识别并解决了卫星图像分类中的特定失败模式。我们的主要贡献是结合坐标注意力和压缩激励块的平衡多任务注意力机制，利用可学习的融合参数来统一空间和光谱特征提取。实验结果表明，该可学习参数自我收敛至约0.57，表明空间和光谱模态在卫星图像中的重要性几乎相等。"
                }
            }
        }
    ],
    "link_prev": "2025-10-20.html",
    "link_next": "2025-10-22.html",
    "link_month": "2025-10.html",
    "short_date_prev": {
        "ru": "20.10",
        "en": "10/20",
        "zh": "10月20日"
    },
    "short_date_next": {
        "ru": "22.10",
        "en": "10/22",
        "zh": "10月22日"
    },
    "categories": {
        "#dataset": 9,
        "#data": 3,
        "#benchmark": 8,
        "#agents": 6,
        "#cv": 6,
        "#rl": 4,
        "#rlhf": 0,
        "#rag": 3,
        "#plp": 0,
        "#inference": 1,
        "#3d": 1,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 10,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 4,
        "#healthcare": 0,
        "#training": 11,
        "#robotics": 0,
        "#agi": 2,
        "#games": 3,
        "#interpretability": 0,
        "#reasoning": 7,
        "#transfer_learning": 0,
        "#graphs": 1,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 11,
        "#survey": 2,
        "#diffusion": 1,
        "#alignment": 3,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 9,
        "#small_models": 1,
        "#science": 2,
        "#low_resource": 0
    }
}