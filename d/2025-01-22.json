{
    "date": {
        "ru": "22 ÑĞ½Ğ²Ğ°Ñ€Ñ",
        "en": "January 22",
        "zh": "1æœˆ22æ—¥"
    },
    "time_utc": "2025-01-22 00:45",
    "weekday": 2,
    "issue_id": 1793,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.08325",
            "title": "GameFactory: Creating New Games with Generative Interactive Videos",
            "url": "https://huggingface.co/papers/2501.08325",
            "abstract": "Generative game engines have the potential to revolutionize game development by autonomously creating new content and reducing manual workload. However, existing video-based game generation methods fail to address the critical challenge of scene generalization, limiting their applicability to existing games with fixed styles and scenes. In this paper, we present GameFactory, a framework focused on exploring scene generalization in game video generation. To enable the creation of entirely new and diverse games, we leverage pre-trained video diffusion models trained on open-domain video data. To bridge the domain gap between open-domain priors and small-scale game dataset, we propose a multi-phase training strategy that decouples game style learning from action control, preserving open-domain generalization while achieving action controllability. Using Minecraft as our data source, we release GF-Minecraft, a high-quality and diversity action-annotated video dataset for research. Furthermore, we extend our framework to enable autoregressive action-controllable game video generation, allowing the production of unlimited-length interactive game videos. Experimental results demonstrate that GameFactory effectively generates open-domain, diverse, and action-controllable game videos, representing a significant step forward in AI-driven game generation. Our dataset and project page are publicly available at https://vvictoryuki.github.io/gamefactory/.",
            "score": 47,
            "issue_id": 1773,
            "pub_date": "2025-01-14",
            "pub_date_card": {
                "ru": "14 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 14",
                "zh": "1æœˆ14æ—¥"
            },
            "hash": "0331c9576ced4090",
            "authors": [
                "Jiwen Yu",
                "Yiran Qin",
                "Xintao Wang",
                "Pengfei Wan",
                "Di Zhang",
                "Xihui Liu"
            ],
            "affiliations": [
                "Kuaishou Technology",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.08325.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#video",
                    "#open_source",
                    "#diffusion",
                    "#games",
                    "#training",
                    "#multimodal"
                ],
                "emoji": "ğŸ®",
                "ru": {
                    "title": "GameFactory: Ğ˜Ğ˜-Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¸Ğ³Ñ€",
                    "desc": "GameFactory - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ³Ñ€ Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ñ‹. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¸Ğ³Ñ€Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ¸Ğ»Ñ Ğ¸Ğ³Ñ€Ñ‹ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ³Ñ€ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ½ĞµĞ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ñ‹."
                },
                "en": {
                    "title": "Revolutionizing Game Development with Scene Generalization",
                    "desc": "This paper introduces GameFactory, a novel framework aimed at enhancing scene generalization in game video generation. It addresses the limitations of current methods that struggle with fixed styles and scenes by utilizing pre-trained video diffusion models on diverse video data. The authors propose a multi-phase training strategy that separates game style learning from action control, allowing for better generalization and controllability. The framework is validated using a new dataset, GF-Minecraft, which supports the generation of diverse and interactive game videos, marking a significant advancement in AI-driven game development."
                },
                "zh": {
                    "title": "GameFactoryï¼šé©å‘½æ€§çš„æ¸¸æˆè§†é¢‘ç”Ÿæˆæ¡†æ¶",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†GameFactoryæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ¸¸æˆè§†é¢‘ç”Ÿæˆä¸­çš„åœºæ™¯æ³›åŒ–é—®é¢˜ã€‚ç°æœ‰çš„è§†é¢‘ç”Ÿæˆæ–¹æ³•æ— æ³•é€‚åº”ä¸åŒé£æ ¼å’Œåœºæ™¯çš„æ¸¸æˆï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚æˆ‘ä»¬åˆ©ç”¨é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œå¹¶æå‡ºå¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œä»¥å®ç°æ¸¸æˆé£æ ¼å­¦ä¹ ä¸åŠ¨ä½œæ§åˆ¶çš„è§£è€¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGameFactoryèƒ½å¤Ÿæœ‰æ•ˆç”Ÿæˆå¼€æ”¾åŸŸã€å¤šæ ·åŒ–ä¸”å¯æ§çš„æ¸¸æˆè§†é¢‘ï¼Œæ¨åŠ¨äº†AIé©±åŠ¨çš„æ¸¸æˆç”ŸæˆæŠ€æœ¯çš„å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.09781",
            "title": "VideoWorld: Exploring Knowledge Learning from Unlabeled Videos",
            "url": "https://huggingface.co/papers/2501.09781",
            "abstract": "This work explores whether a deep generative model can learn complex knowledge solely from visual input, in contrast to the prevalent focus on text-based models like large language models (LLMs). We develop VideoWorld, an auto-regressive video generation model trained on unlabeled video data, and test its knowledge acquisition abilities in video-based Go and robotic control tasks. Our experiments reveal two key findings: (1) video-only training provides sufficient information for learning knowledge, including rules, reasoning and planning capabilities, and (2) the representation of visual change is crucial for knowledge acquisition. To improve both the efficiency and efficacy of this process, we introduce the Latent Dynamics Model (LDM) as a key component of VideoWorld. Remarkably, VideoWorld reaches a 5-dan professional level in the Video-GoBench with just a 300-million-parameter model, without relying on search algorithms or reward mechanisms typical in reinforcement learning. In robotic tasks, VideoWorld effectively learns diverse control operations and generalizes across environments, approaching the performance of oracle models in CALVIN and RLBench. This study opens new avenues for knowledge acquisition from visual data, with all code, data, and models open-sourced for further research.",
            "score": 7,
            "issue_id": 1779,
            "pub_date": "2025-01-16",
            "pub_date_card": {
                "ru": "16 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 16",
                "zh": "1æœˆ16æ—¥"
            },
            "hash": "db65df971ed9f199",
            "authors": [
                "Zhongwei Ren",
                "Yunchao Wei",
                "Xun Guo",
                "Yao Zhao",
                "Bingyi Kang",
                "Jiashi Feng",
                "Xiaojie Jin"
            ],
            "affiliations": [
                "Beijing Jiaotong University",
                "ByteDance Seed",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.09781.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#video",
                    "#open_source",
                    "#small_models",
                    "#rl",
                    "#games",
                    "#optimization"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ: Ğ¾Ñ‚ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğº Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğ¼ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼ Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ VideoWorld, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ½ĞµĞ¼Ğ°Ñ€ĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸Ğ³Ñ€Ñ‹ Ğ² Ğ³Ğ¾ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ñ‹: Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ¾Ğ±Ñ€ĞµÑ‚ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ°, Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ° Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»Ğ° 5 Ğ´Ğ°Ğ½Ğ° Ğ² Ğ³Ğ¾ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…."
                },
                "en": {
                    "title": "Learning Knowledge from Visuals: VideoWorld's Breakthrough",
                    "desc": "This paper investigates the ability of a deep generative model to learn complex knowledge from visual inputs, rather than relying on text-based models. The authors introduce VideoWorld, an auto-regressive model that generates videos and learns from unlabeled video data, demonstrating its effectiveness in tasks like video-based Go and robotic control. Key findings indicate that training solely on video data is sufficient for acquiring knowledge such as rules and reasoning, and that understanding visual changes is essential for this learning process. The introduction of the Latent Dynamics Model enhances the efficiency of knowledge acquisition, allowing VideoWorld to achieve high performance in various tasks without traditional reinforcement learning techniques."
                },
                "zh": {
                    "title": "ä»è§†è§‰æ•°æ®ä¸­è·å–çŸ¥è¯†çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†æ·±åº¦ç”Ÿæˆæ¨¡å‹æ˜¯å¦å¯ä»¥ä»…é€šè¿‡è§†è§‰è¾“å…¥å­¦ä¹ å¤æ‚çŸ¥è¯†ï¼Œè€Œä¸æ˜¯ä¾èµ–äºæ–‡æœ¬æ¨¡å‹ã€‚æˆ‘ä»¬å¼€å‘äº†VideoWorldï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºè‡ªå›å½’çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œè®­ç»ƒäºæœªæ ‡è®°çš„è§†é¢‘æ•°æ®ï¼Œå¹¶æµ‹è¯•å…¶åœ¨è§†é¢‘å›´æ£‹å’Œæœºå™¨äººæ§åˆ¶ä»»åŠ¡ä¸­çš„çŸ¥è¯†è·å–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè§†é¢‘è®­ç»ƒæä¾›äº†è¶³å¤Ÿçš„ä¿¡æ¯æ¥å­¦ä¹ è§„åˆ™ã€æ¨ç†å’Œè§„åˆ’èƒ½åŠ›ï¼Œè§†è§‰å˜åŒ–çš„è¡¨ç¤ºå¯¹çŸ¥è¯†è·å–è‡³å…³é‡è¦ã€‚é€šè¿‡å¼•å…¥æ½œåœ¨åŠ¨æ€æ¨¡å‹ï¼ˆLDMï¼‰ï¼ŒVideoWorldåœ¨è§†é¢‘å›´æ£‹åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†5æ®µä¸“ä¸šæ°´å¹³ï¼Œä¸”åœ¨æœºå™¨äººä»»åŠ¡ä¸­æœ‰æ•ˆå­¦ä¹ äº†å¤šç§æ§åˆ¶æ“ä½œã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.09284",
            "title": "SEAL: Entangled White-box Watermarks on Low-Rank Adaptation",
            "url": "https://huggingface.co/papers/2501.09284",
            "abstract": "Recently, LoRA and its variants have become the de facto strategy for training and sharing task-specific versions of large pretrained models, thanks to their efficiency and simplicity. However, the issue of copyright protection for LoRA weights, especially through watermark-based techniques, remains underexplored. To address this gap, we propose SEAL (SEcure wAtermarking on LoRA weights), the universal whitebox watermarking for LoRA. SEAL embeds a secret, non-trainable matrix between trainable LoRA weights, serving as a passport to claim ownership. SEAL then entangles the passport with the LoRA weights through training, without extra loss for entanglement, and distributes the finetuned weights after hiding the passport. When applying SEAL, we observed no performance degradation across commonsense reasoning, textual/visual instruction tuning, and text-to-image synthesis tasks. We demonstrate that SEAL is robust against a variety of known attacks: removal, obfuscation, and ambiguity attacks.",
            "score": 2,
            "issue_id": 1782,
            "pub_date": "2025-01-16",
            "pub_date_card": {
                "ru": "16 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 16",
                "zh": "1æœˆ16æ—¥"
            },
            "hash": "3c8f91b49b49bdd2",
            "authors": [
                "Giyeong Oh",
                "Saejin Kim",
                "Woohyun Cho",
                "Sangkyu Lee",
                "Jiwan Chung",
                "Dokyung Song",
                "Youngjae Yu"
            ],
            "affiliations": [
                "Department of Artificial Intelligence, Yonsei University, Seoul, Republic of Korea",
                "Department of Computer Science and Engineering, Yonsei University, Seoul, Republic of Korea"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.09284.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#security"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "SEAL: Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ°Ğ²Ñ‚Ğ¾Ñ€ÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ°Ğ² Ğ½Ğ° LoRA-Ğ²ĞµÑĞ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²Ğ¾Ğ´ÑĞ½Ñ‹Ñ… Ğ·Ğ½Ğ°ĞºĞ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SEAL - ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ğ¾Ğ´ÑĞ½Ñ‹Ñ… Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ²ĞµÑĞ¾Ğ² LoRA. SEAL Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞµĞºÑ€ĞµÑ‚Ğ½ÑƒÑ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñƒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ²ĞµÑĞ°Ğ¼Ğ¸ LoRA, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ Ğ¿Ğ°ÑĞ¿Ğ¾Ñ€Ñ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ÑÑ‚Ğ²Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ½Ğµ ÑƒÑ…ÑƒĞ´ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. SEAL Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ğ¼ Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼ Ğ½Ğ° Ğ²Ğ¾Ğ´ÑĞ½Ñ‹Ğµ Ğ·Ğ½Ğ°ĞºĞ¸, Ñ‚Ğ°ĞºĞ¸Ğ¼ ĞºĞ°Ğº ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ, Ğ¾Ğ±Ñ„ÑƒÑĞºĞ°Ñ†Ğ¸Ñ Ğ¸ Ğ°Ñ‚Ğ°ĞºĞ¸ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "SEAL: Safeguarding LoRA Weights with Robust Watermarking",
                    "desc": "This paper introduces SEAL, a watermarking technique designed to protect LoRA weights used in machine learning. SEAL embeds a secret matrix within the trainable weights, allowing owners to claim their models without affecting performance. The method ensures that the watermark is integrated during training, maintaining the model's effectiveness across various tasks. Additionally, SEAL demonstrates resilience against common attacks aimed at removing or obscuring the watermark."
                },
                "zh": {
                    "title": "ä¿æŠ¤LoRAæƒé‡çš„æ°´å°æŠ€æœ¯",
                    "desc": "æœ€è¿‘ï¼ŒLoRAåŠå…¶å˜ä½“æˆä¸ºè®­ç»ƒå’Œå…±äº«ç‰¹å®šä»»åŠ¡çš„å¤§å‹é¢„è®­ç»ƒæ¨¡å‹çš„ä¸»è¦ç­–ç•¥ï¼Œå› å…¶é«˜æ•ˆå’Œç®€å•ã€‚ç„¶è€Œï¼ŒLoRAæƒé‡çš„ç‰ˆæƒä¿æŠ¤é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åŸºäºæ°´å°çš„æŠ€æœ¯ï¼Œä»ç„¶æœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SEALï¼ˆLoRAæƒé‡çš„å®‰å…¨æ°´å°ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é€šç”¨çš„ç™½ç›’æ°´å°æŠ€æœ¯ã€‚SEALåœ¨å¯è®­ç»ƒçš„LoRAæƒé‡ä¹‹é—´åµŒå…¥ä¸€ä¸ªç§˜å¯†çš„ã€ä¸å¯è®­ç»ƒçš„çŸ©é˜µï¼Œä½œä¸ºæ‰€æœ‰æƒçš„å‡­è¯ï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å°†å…¶ä¸LoRAæƒé‡çº ç¼ ï¼Œç¡®ä¿æ€§èƒ½ä¸ä¸‹é™ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-01-21.html",
    "link_next": "2025-01-23.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "21.01",
        "en": "01/21",
        "zh": "1æœˆ21æ—¥"
    },
    "short_date_next": {
        "ru": "23.01",
        "en": "01/23",
        "zh": "1æœˆ23æ—¥"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 0,
        "#agents": 1,
        "#cv": 0,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§åä¸ºGameFactoryçš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ç”Ÿæˆæ¸¸æˆå¼•æ“æ¥é©æ–°æ¸¸æˆå¼€å‘ã€‚å®ƒä½¿ç”¨é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿåˆ›å»ºå…¨æ–°ä¸”å¤šæ ·åŒ–çš„æ¸¸æˆã€‚ä¸ºäº†è§£å†³ç°æœ‰æ–¹æ³•åœ¨åœºæ™¯ç”Ÿæˆä¸Šçš„å±€é™ï¼Œä½œè€…æå‡ºäº†ä¸€ç§å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ã€‚ä»–ä»¬è¿˜å‘å¸ƒäº†ä¸€ä¸ªåŸºäºMinecraftçš„é«˜è´¨é‡è§†é¢‘æ•°æ®é›†ï¼Œå¹¶å±•ç¤ºäº†æ¡†æ¶èƒ½å¤Ÿç”Ÿæˆå¼€æ”¾åŸŸã€å¤šæ ·åŒ–å’Œå¯æ§çš„æ¸¸æˆè§†é¢‘ã€‚",
        "title": "GameFactory: Creating New Games with Generative Interactive Videos",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§åä¸ºGameFactoryçš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ç”Ÿæˆæ¸¸æˆå¼•æ“æ¥é©æ–°æ¸¸æˆå¼€å‘ã€‚å®ƒä½¿ç”¨é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿåˆ›å»ºå…¨æ–°ä¸”å¤šæ ·åŒ–çš„æ¸¸æˆã€‚ä¸ºäº†è§£å†³ç°æœ‰æ–¹æ³•åœ¨åœºæ™¯ç”Ÿæˆä¸Šçš„å±€é™ï¼Œä½œè€…æå‡ºäº†ä¸€ç§å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ã€‚ä»–ä»¬è¿˜å‘å¸ƒäº†ä¸€ä¸ªåŸºäºMinecraftçš„é«˜è´¨é‡è§†é¢‘æ•°æ®é›†ï¼Œå¹¶å±•ç¤ºäº†æ¡†æ¶èƒ½å¤Ÿç”Ÿæˆå¼€æ”¾åŸŸã€å¤šæ ·åŒ–å’Œå¯æ§çš„æ¸¸æˆè§†é¢‘ã€‚\n\nzhÃ¨ piÄn wÃ©n zhÄng jiÃ¨ shÃ o le yÄ« zhÇ’ng mÃ­ng wÃ¨i GameFactory de kuÃ ng jiÃ , zhÇ zÃ i tÅng guÃ² shÄ“ng chÃ©ng yÃ²u xÃ­ yÇn qÃ­ng lÃ¡i gÃ© xÄ«n yÃ²u xÃ­ kÄi fÄ. tÄ shÇ yÃ²ng yÃ¹ xÃ¹n liÃ n de shÃ¬ pÃ­n kuÃ² sÃ n mÃ³ xÃ­ng, nÃ©ng gÃ²u chuÃ ng jiÃ n quÃ¡n xÄ«n qiÄ› duÅ yÃ ng huÃ  de yÃ²u xÃ­. wÃ¨i le jiÄ› juÃ© xiÃ n yÇ’u fÄng fÇ zÃ i chÇng jÄ«ng shÄ“ng chÃ©ng shÃ ng de jÃº xiÃ n, zuÃ² zhÄ› tÃ­ chÅ« le yÄ« zhÇ’ng duÅ jiÄ“ duÃ n xÃ¹n liÃ n cÃ¨ lÃ¼Ã¨. tÄ men hÃ¡i fÄ bÃ¹ le yÄ« gÃ¨ jÄ« yÃº Minecraft de gÄo zhÃ¬ liÃ ng shÃ¬ pÃ­n shÃ¹ jÃ¹ jÃ­, bÃ¬ng zhÃ n shÃ¬ le kuÃ ng jiÃ  nÃ©ng gÃ²u shÄ“ng chÃ©ng kÄi fÃ ng yÃ¹, duÅ yÃ ng huÃ  hÃ© kÄ› kÃ²ng de yÃ²u xÃ­ shÃ¬ pÃ­n.",
        "vocab": "[{'word': 'æ¡†æ¶', 'pinyin': 'kuÃ ngjiÃ ', 'trans': 'framework'},\n{'word': 'æ—¨åœ¨', 'pinyin': 'zhÇzÃ i', 'trans': 'aim to'},\n{'word': 'é©æ–°', 'pinyin': 'gÃ©xÄ«n', 'trans': 'innovate'},\n{'word': 'å¼•æ“', 'pinyin': 'yÇnqÃ­ng', 'trans': 'engine'},\n{'word': 'é¢„è®­ç»ƒ', 'pinyin': 'yÃ¹ xÃ¹nliÃ n', 'trans': 'pre-trained'},\n{'word': 'æ‰©æ•£', 'pinyin': 'kuÃ²sÃ n', 'trans': 'diffusion'},\n{'word': 'å¤šæ ·åŒ–', 'pinyin': 'duÅyÃ nghuÃ ', 'trans': 'diversified'},\n{'word': 'å±€é™', 'pinyin': 'jÃºxiÃ n', 'trans': 'limitation'},\n{'word': 'æå‡º', 'pinyin': 'tÃ­chÅ«', 'trans': 'propose'},\n{'word': 'ç­–ç•¥', 'pinyin': 'cÃ¨lÃ¼Ã¨', 'trans': 'strategy'},\n{'word': 'åŸºäº', 'pinyin': 'jÄ«yÃº', 'trans': 'based on'},\n{'word': 'é«˜è´¨é‡', 'pinyin': 'gÄo zhÃ¬liÃ ng', 'trans': 'high quality'},\n{'word': 'æ•°æ®é›†', 'pinyin': 'shÃ¹jÃ¹ jÃ­', 'trans': 'dataset'},\n{'word': 'å±•ç¤º', 'pinyin': 'zhÇnshÃ¬', 'trans': 'demonstrate'},\n{'word': 'å¼€æ”¾åŸŸ', 'pinyin': 'kÄifÃ ng yÃ¹', 'trans': 'open domain'},\n{'word': 'å¯æ§', 'pinyin': 'kÄ›kÃ²ng', 'trans': 'controllable'}]",
        "trans": "This article introduces a framework called GameFactory, which aims to revolutionize game development by generating game engines. It utilizes pre-trained video diffusion models to create novel and diverse games. To address the limitations of existing methods in scene generation, the authors propose a multi-stage training strategy. They also release a high-quality video dataset based on Minecraft and demonstrate that the framework can generate open-domain, diverse, and controllable game videos.",
        "update_ts": "2025-01-21 09:10"
    }
}