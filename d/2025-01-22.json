{
    "date": {
        "ru": "22 января",
        "en": "January 22",
        "zh": "1月22日"
    },
    "time_utc": "2025-01-22 00:45",
    "weekday": 2,
    "issue_id": 1793,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.08325",
            "title": "GameFactory: Creating New Games with Generative Interactive Videos",
            "url": "https://huggingface.co/papers/2501.08325",
            "abstract": "Generative game engines have the potential to revolutionize game development by autonomously creating new content and reducing manual workload. However, existing video-based game generation methods fail to address the critical challenge of scene generalization, limiting their applicability to existing games with fixed styles and scenes. In this paper, we present GameFactory, a framework focused on exploring scene generalization in game video generation. To enable the creation of entirely new and diverse games, we leverage pre-trained video diffusion models trained on open-domain video data. To bridge the domain gap between open-domain priors and small-scale game dataset, we propose a multi-phase training strategy that decouples game style learning from action control, preserving open-domain generalization while achieving action controllability. Using Minecraft as our data source, we release GF-Minecraft, a high-quality and diversity action-annotated video dataset for research. Furthermore, we extend our framework to enable autoregressive action-controllable game video generation, allowing the production of unlimited-length interactive game videos. Experimental results demonstrate that GameFactory effectively generates open-domain, diverse, and action-controllable game videos, representing a significant step forward in AI-driven game generation. Our dataset and project page are publicly available at https://vvictoryuki.github.io/gamefactory/.",
            "score": 47,
            "issue_id": 1773,
            "pub_date": "2025-01-14",
            "pub_date_card": {
                "ru": "14 января",
                "en": "January 14",
                "zh": "1月14日"
            },
            "hash": "0331c9576ced4090",
            "authors": [
                "Jiwen Yu",
                "Yiran Qin",
                "Xintao Wang",
                "Pengfei Wan",
                "Di Zhang",
                "Xihui Liu"
            ],
            "affiliations": [
                "Kuaishou Technology",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.08325.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#video",
                    "#open_source",
                    "#diffusion",
                    "#games",
                    "#training",
                    "#multimodal"
                ],
                "emoji": "🎮",
                "ru": {
                    "title": "GameFactory: ИИ-революция в создании видеоигр",
                    "desc": "GameFactory - это новая система для генерации видео игр с возможностью обобщения на различные сцены. Она использует предобученные модели диффузии видео на общих данных, что позволяет создавать разнообразные новые игры. Авторы предлагают многоэтапную стратегию обучения, которая разделяет изучение стиля игры и контроль действий. Система также поддерживает авторегрессивную генерацию видео игр с контролем действий неограниченной длины."
                },
                "en": {
                    "title": "Revolutionizing Game Development with Scene Generalization",
                    "desc": "This paper introduces GameFactory, a novel framework aimed at enhancing scene generalization in game video generation. It addresses the limitations of current methods that struggle with fixed styles and scenes by utilizing pre-trained video diffusion models on diverse video data. The authors propose a multi-phase training strategy that separates game style learning from action control, allowing for better generalization and controllability. The framework is validated using a new dataset, GF-Minecraft, which supports the generation of diverse and interactive game videos, marking a significant advancement in AI-driven game development."
                },
                "zh": {
                    "title": "GameFactory：革命性的游戏视频生成框架",
                    "desc": "本论文介绍了GameFactory框架，旨在解决游戏视频生成中的场景泛化问题。现有的视频生成方法无法适应不同风格和场景的游戏，限制了其应用。我们利用预训练的视频扩散模型，并提出多阶段训练策略，以实现游戏风格学习与动作控制的解耦。实验结果表明，GameFactory能够有效生成开放域、多样化且可控的游戏视频，推动了AI驱动的游戏生成技术的发展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.09781",
            "title": "VideoWorld: Exploring Knowledge Learning from Unlabeled Videos",
            "url": "https://huggingface.co/papers/2501.09781",
            "abstract": "This work explores whether a deep generative model can learn complex knowledge solely from visual input, in contrast to the prevalent focus on text-based models like large language models (LLMs). We develop VideoWorld, an auto-regressive video generation model trained on unlabeled video data, and test its knowledge acquisition abilities in video-based Go and robotic control tasks. Our experiments reveal two key findings: (1) video-only training provides sufficient information for learning knowledge, including rules, reasoning and planning capabilities, and (2) the representation of visual change is crucial for knowledge acquisition. To improve both the efficiency and efficacy of this process, we introduce the Latent Dynamics Model (LDM) as a key component of VideoWorld. Remarkably, VideoWorld reaches a 5-dan professional level in the Video-GoBench with just a 300-million-parameter model, without relying on search algorithms or reward mechanisms typical in reinforcement learning. In robotic tasks, VideoWorld effectively learns diverse control operations and generalizes across environments, approaching the performance of oracle models in CALVIN and RLBench. This study opens new avenues for knowledge acquisition from visual data, with all code, data, and models open-sourced for further research.",
            "score": 7,
            "issue_id": 1779,
            "pub_date": "2025-01-16",
            "pub_date_card": {
                "ru": "16 января",
                "en": "January 16",
                "zh": "1月16日"
            },
            "hash": "db65df971ed9f199",
            "authors": [
                "Zhongwei Ren",
                "Yunchao Wei",
                "Xun Guo",
                "Yao Zhao",
                "Bingyi Kang",
                "Jiashi Feng",
                "Xiaojie Jin"
            ],
            "affiliations": [
                "Beijing Jiaotong University",
                "ByteDance Seed",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.09781.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#video",
                    "#open_source",
                    "#small_models",
                    "#rl",
                    "#games",
                    "#optimization"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Визуальное обучение: от видео к глубоким знаниям",
                    "desc": "Исследование посвящено обучению глубокой генеративной модели сложным знаниям исключительно на основе визуальных данных. Разработана модель VideoWorld, обученная на немаркированных видеоданных, которая тестируется на задачах игры в го и управления роботами. Ключевые выводы: визуальное обучение достаточно для приобретения знаний, включая правила, рассуждения и планирование, а представление визуальных изменений критично для этого процесса. Модель достигает уровня профессионала 5 дана в го и эффективно обучается управлению роботами в различных средах."
                },
                "en": {
                    "title": "Learning Knowledge from Visuals: VideoWorld's Breakthrough",
                    "desc": "This paper investigates the ability of a deep generative model to learn complex knowledge from visual inputs, rather than relying on text-based models. The authors introduce VideoWorld, an auto-regressive model that generates videos and learns from unlabeled video data, demonstrating its effectiveness in tasks like video-based Go and robotic control. Key findings indicate that training solely on video data is sufficient for acquiring knowledge such as rules and reasoning, and that understanding visual changes is essential for this learning process. The introduction of the Latent Dynamics Model enhances the efficiency of knowledge acquisition, allowing VideoWorld to achieve high performance in various tasks without traditional reinforcement learning techniques."
                },
                "zh": {
                    "title": "从视觉数据中获取知识的新方法",
                    "desc": "本研究探讨了深度生成模型是否可以仅通过视觉输入学习复杂知识，而不是依赖于文本模型。我们开发了VideoWorld，这是一个基于自回归的视频生成模型，训练于未标记的视频数据，并测试其在视频围棋和机器人控制任务中的知识获取能力。实验结果表明，视频训练提供了足够的信息来学习规则、推理和规划能力，视觉变化的表示对知识获取至关重要。通过引入潜在动态模型（LDM），VideoWorld在视频围棋基准测试中达到了5段专业水平，且在机器人任务中有效学习了多种控制操作。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.09284",
            "title": "SEAL: Entangled White-box Watermarks on Low-Rank Adaptation",
            "url": "https://huggingface.co/papers/2501.09284",
            "abstract": "Recently, LoRA and its variants have become the de facto strategy for training and sharing task-specific versions of large pretrained models, thanks to their efficiency and simplicity. However, the issue of copyright protection for LoRA weights, especially through watermark-based techniques, remains underexplored. To address this gap, we propose SEAL (SEcure wAtermarking on LoRA weights), the universal whitebox watermarking for LoRA. SEAL embeds a secret, non-trainable matrix between trainable LoRA weights, serving as a passport to claim ownership. SEAL then entangles the passport with the LoRA weights through training, without extra loss for entanglement, and distributes the finetuned weights after hiding the passport. When applying SEAL, we observed no performance degradation across commonsense reasoning, textual/visual instruction tuning, and text-to-image synthesis tasks. We demonstrate that SEAL is robust against a variety of known attacks: removal, obfuscation, and ambiguity attacks.",
            "score": 2,
            "issue_id": 1782,
            "pub_date": "2025-01-16",
            "pub_date_card": {
                "ru": "16 января",
                "en": "January 16",
                "zh": "1月16日"
            },
            "hash": "3c8f91b49b49bdd2",
            "authors": [
                "Giyeong Oh",
                "Saejin Kim",
                "Woohyun Cho",
                "Sangkyu Lee",
                "Jiwan Chung",
                "Dokyung Song",
                "Youngjae Yu"
            ],
            "affiliations": [
                "Department of Artificial Intelligence, Yonsei University, Seoul, Republic of Korea",
                "Department of Computer Science and Engineering, Yonsei University, Seoul, Republic of Korea"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.09284.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#security"
                ],
                "emoji": "🔐",
                "ru": {
                    "title": "SEAL: Защита авторских прав на LoRA-веса с помощью водяных знаков",
                    "desc": "Статья представляет SEAL - универсальный метод водяных знаков для весов LoRA. SEAL встраивает секретную матрицу между обучаемыми весами LoRA, которая служит паспортом для подтверждения авторства. Метод не ухудшает производительность модели на различных задачах обработки естественного языка и компьютерного зрения. SEAL демонстрирует устойчивость к известным атакам на водяные знаки, таким как удаление, обфускация и атаки неоднозначности."
                },
                "en": {
                    "title": "SEAL: Safeguarding LoRA Weights with Robust Watermarking",
                    "desc": "This paper introduces SEAL, a watermarking technique designed to protect LoRA weights used in machine learning. SEAL embeds a secret matrix within the trainable weights, allowing owners to claim their models without affecting performance. The method ensures that the watermark is integrated during training, maintaining the model's effectiveness across various tasks. Additionally, SEAL demonstrates resilience against common attacks aimed at removing or obscuring the watermark."
                },
                "zh": {
                    "title": "保护LoRA权重的水印技术",
                    "desc": "最近，LoRA及其变体成为训练和共享特定任务的大型预训练模型的主要策略，因其高效和简单。然而，LoRA权重的版权保护问题，特别是基于水印的技术，仍然未得到充分研究。为了解决这个问题，我们提出了SEAL（LoRA权重的安全水印），这是一种通用的白盒水印技术。SEAL在可训练的LoRA权重之间嵌入一个秘密的、不可训练的矩阵，作为所有权的凭证，并在训练过程中将其与LoRA权重纠缠，确保性能不下降。"
                }
            }
        }
    ],
    "link_prev": "2025-01-21.html",
    "link_next": "2025-01-23.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "21.01",
        "en": "01/21",
        "zh": "1月21日"
    },
    "short_date_next": {
        "ru": "23.01",
        "en": "01/23",
        "zh": "1月23日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 0,
        "#agents": 1,
        "#cv": 0,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章介绍了一种名为GameFactory的框架，旨在通过生成游戏引擎来革新游戏开发。它使用预训练的视频扩散模型，能够创建全新且多样化的游戏。为了解决现有方法在场景生成上的局限，作者提出了一种多阶段训练策略。他们还发布了一个基于Minecraft的高质量视频数据集，并展示了框架能够生成开放域、多样化和可控的游戏视频。",
        "title": "GameFactory: Creating New Games with Generative Interactive Videos",
        "pinyin": "这篇文章介绍了一种名为GameFactory的框架，旨在通过生成游戏引擎来革新游戏开发。它使用预训练的视频扩散模型，能够创建全新且多样化的游戏。为了解决现有方法在场景生成上的局限，作者提出了一种多阶段训练策略。他们还发布了一个基于Minecraft的高质量视频数据集，并展示了框架能够生成开放域、多样化和可控的游戏视频。\n\nzhè piān wén zhāng jiè shào le yī zhǒng míng wèi GameFactory de kuàng jià, zhǐ zài tōng guò shēng chéng yòu xí yǐn qíng lái gé xīn yòu xí kāi fā. tā shǐ yòng yù xùn liàn de shì pín kuò sàn mó xíng, néng gòu chuàng jiàn quán xīn qiě duō yàng huà de yòu xí. wèi le jiě jué xiàn yǒu fāng fǎ zài chǎng jīng shēng chéng shàng de jú xiàn, zuò zhě tí chū le yī zhǒng duō jiē duàn xùn liàn cè lüè. tā men hái fā bù le yī gè jī yú Minecraft de gāo zhì liàng shì pín shù jù jí, bìng zhàn shì le kuàng jià néng gòu shēng chéng kāi fàng yù, duō yàng huà hé kě kòng de yòu xí shì pín.",
        "vocab": "[{'word': '框架', 'pinyin': 'kuàngjià', 'trans': 'framework'},\n{'word': '旨在', 'pinyin': 'zhǐzài', 'trans': 'aim to'},\n{'word': '革新', 'pinyin': 'géxīn', 'trans': 'innovate'},\n{'word': '引擎', 'pinyin': 'yǐnqíng', 'trans': 'engine'},\n{'word': '预训练', 'pinyin': 'yù xùnliàn', 'trans': 'pre-trained'},\n{'word': '扩散', 'pinyin': 'kuòsàn', 'trans': 'diffusion'},\n{'word': '多样化', 'pinyin': 'duōyànghuà', 'trans': 'diversified'},\n{'word': '局限', 'pinyin': 'júxiàn', 'trans': 'limitation'},\n{'word': '提出', 'pinyin': 'tíchū', 'trans': 'propose'},\n{'word': '策略', 'pinyin': 'cèlüè', 'trans': 'strategy'},\n{'word': '基于', 'pinyin': 'jīyú', 'trans': 'based on'},\n{'word': '高质量', 'pinyin': 'gāo zhìliàng', 'trans': 'high quality'},\n{'word': '数据集', 'pinyin': 'shùjù jí', 'trans': 'dataset'},\n{'word': '展示', 'pinyin': 'zhǎnshì', 'trans': 'demonstrate'},\n{'word': '开放域', 'pinyin': 'kāifàng yù', 'trans': 'open domain'},\n{'word': '可控', 'pinyin': 'kěkòng', 'trans': 'controllable'}]",
        "trans": "This article introduces a framework called GameFactory, which aims to revolutionize game development by generating game engines. It utilizes pre-trained video diffusion models to create novel and diverse games. To address the limitations of existing methods in scene generation, the authors propose a multi-stage training strategy. They also release a high-quality video dataset based on Minecraft and demonstrate that the framework can generate open-domain, diverse, and controllable game videos.",
        "update_ts": "2025-01-21 09:10"
    }
}