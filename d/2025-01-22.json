{
    "date": {
        "ru": "22 ÑĞ½Ğ²Ğ°Ñ€Ñ",
        "en": "January 22",
        "zh": "1æœˆ22æ—¥"
    },
    "time_utc": "2025-01-22 04:12",
    "weekday": 2,
    "issue_id": 1796,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.12273",
            "title": "Condor: Enhance LLM Alignment with Knowledge-Driven Data Synthesis and Refinement",
            "url": "https://huggingface.co/papers/2501.12273",
            "abstract": "The quality of Supervised Fine-Tuning (SFT) data plays a critical role in enhancing the conversational capabilities of Large Language Models (LLMs). However, as LLMs become more advanced, the availability of high-quality human-annotated SFT data has become a significant bottleneck, necessitating a greater reliance on synthetic training data. In this work, we introduce Condor, a novel two-stage synthetic data generation framework that incorporates World Knowledge Tree and Self-Reflection Refinement to produce high-quality SFT data at scale. Our experimental results demonstrate that a base model fine-tuned on only 20K Condor-generated samples achieves superior performance compared to counterparts. The additional refinement stage in Condor further enables iterative self-improvement for LLMs at various scales (up to 72B), validating the effectiveness of our approach. Furthermore, our investigation into the scaling for synthetic data in post-training reveals substantial unexplored potential for performance improvements, opening promising avenues for future research.",
            "score": 5,
            "issue_id": 1796,
            "pub_date": "2025-01-21",
            "pub_date_card": {
                "ru": "21 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 21",
                "zh": "1æœˆ21æ—¥"
            },
            "hash": "10499c8b820d5368",
            "authors": [
                "Maosong Cao",
                "Taolin Zhang",
                "Mo Li",
                "Chuyu Zhang",
                "Yunxin Liu",
                "Haodong Duan",
                "Songyang Zhang",
                "Kai Chen"
            ],
            "affiliations": [
                "Shanghai AI Laboratory",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.12273.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#synthetic",
                    "#data",
                    "#dataset",
                    "#training"
                ],
                "emoji": "ğŸ¦…",
                "ru": {
                    "title": "Condor: Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Condor - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´ĞµÑ€ĞµĞ²Ğ¾ Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ ÑĞ°Ğ¼Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° 20 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ñ… ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Condor Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ LLM Ğ¿Ñ€Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Unlocking LLM Potential with Synthetic Data Generation",
                    "desc": "This paper addresses the challenge of obtaining high-quality Supervised Fine-Tuning (SFT) data for Large Language Models (LLMs). It presents Condor, a two-stage framework that generates synthetic training data using World Knowledge Tree and Self-Reflection Refinement techniques. The results show that models fine-tuned with just 20,000 samples from Condor outperform those trained with traditional methods. Additionally, the framework allows for iterative self-improvement, suggesting significant potential for enhancing LLM performance through synthetic data."
                },
                "zh": {
                    "title": "åˆæˆæ•°æ®ç”Ÿæˆï¼Œæå‡å¯¹è¯èƒ½åŠ›çš„å…³é”®",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ•°æ®çš„è´¨é‡å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹è¯èƒ½åŠ›çš„é‡è¦æ€§ã€‚éšç€LLMsçš„è¿›æ­¥ï¼Œé«˜è´¨é‡çš„äººç±»æ ‡æ³¨SFTæ•°æ®å˜å¾—ç¨€ç¼ºï¼Œå› æ­¤éœ€è¦æ›´å¤šä¾èµ–åˆæˆè®­ç»ƒæ•°æ®ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºCondorçš„ä¸¤é˜¶æ®µåˆæˆæ•°æ®ç”Ÿæˆæ¡†æ¶ï¼Œç»“åˆäº†ä¸–ç•ŒçŸ¥è¯†æ ‘å’Œè‡ªæˆ‘åæ€ç²¾ç‚¼ï¼Œä»¥å¤§è§„æ¨¡ç”Ÿæˆé«˜è´¨é‡çš„SFTæ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä»…ç”¨20Kä¸ªCondorç”Ÿæˆçš„æ ·æœ¬å¾®è°ƒçš„åŸºç¡€æ¨¡å‹ï¼Œå…¶æ€§èƒ½ä¼˜äºå…¶ä»–æ¨¡å‹ï¼ŒéªŒè¯äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-01-21.html",
    "link_next": "2025-01-23.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "21.01",
        "en": "01/21",
        "zh": "1æœˆ21æ—¥"
    },
    "short_date_next": {
        "ru": "23.01",
        "en": "01/23",
        "zh": "1æœˆ23æ—¥"
    },
    "categories": {
        "#dataset": 1,
        "#data": 1,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§åä¸ºGameFactoryçš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ç”Ÿæˆæ¸¸æˆå¼•æ“æ¥é©æ–°æ¸¸æˆå¼€å‘ã€‚å®ƒä½¿ç”¨é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿåˆ›å»ºå…¨æ–°ä¸”å¤šæ ·åŒ–çš„æ¸¸æˆã€‚ä¸ºäº†è§£å†³ç°æœ‰æ–¹æ³•åœ¨åœºæ™¯ç”Ÿæˆä¸Šçš„å±€é™ï¼Œä½œè€…æå‡ºäº†ä¸€ç§å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ã€‚ä»–ä»¬è¿˜å‘å¸ƒäº†ä¸€ä¸ªåŸºäºMinecraftçš„é«˜è´¨é‡è§†é¢‘æ•°æ®é›†ï¼Œå¹¶å±•ç¤ºäº†æ¡†æ¶èƒ½å¤Ÿç”Ÿæˆå¼€æ”¾åŸŸã€å¤šæ ·åŒ–å’Œå¯æ§çš„æ¸¸æˆè§†é¢‘ã€‚",
        "title": "GameFactory: Creating New Games with Generative Interactive Videos",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§åä¸ºGameFactoryçš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ç”Ÿæˆæ¸¸æˆå¼•æ“æ¥é©æ–°æ¸¸æˆå¼€å‘ã€‚å®ƒä½¿ç”¨é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿåˆ›å»ºå…¨æ–°ä¸”å¤šæ ·åŒ–çš„æ¸¸æˆã€‚ä¸ºäº†è§£å†³ç°æœ‰æ–¹æ³•åœ¨åœºæ™¯ç”Ÿæˆä¸Šçš„å±€é™ï¼Œä½œè€…æå‡ºäº†ä¸€ç§å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ã€‚ä»–ä»¬è¿˜å‘å¸ƒäº†ä¸€ä¸ªåŸºäºMinecraftçš„é«˜è´¨é‡è§†é¢‘æ•°æ®é›†ï¼Œå¹¶å±•ç¤ºäº†æ¡†æ¶èƒ½å¤Ÿç”Ÿæˆå¼€æ”¾åŸŸã€å¤šæ ·åŒ–å’Œå¯æ§çš„æ¸¸æˆè§†é¢‘ã€‚\n\nzhÃ¨ piÄn wÃ©n zhÄng jiÃ¨ shÃ o le yÄ« zhÇ’ng mÃ­ng wÃ¨i GameFactory de kuÃ ng jiÃ , zhÇ zÃ i tÅng guÃ² shÄ“ng chÃ©ng yÃ²u xÃ­ yÇn qÃ­ng lÃ¡i gÃ© xÄ«n yÃ²u xÃ­ kÄi fÄ. tÄ shÇ yÃ²ng yÃ¹ xÃ¹n liÃ n de shÃ¬ pÃ­n kuÃ² sÃ n mÃ³ xÃ­ng, nÃ©ng gÃ²u chuÃ ng jiÃ n quÃ¡n xÄ«n qiÄ› duÅ yÃ ng huÃ  de yÃ²u xÃ­. wÃ¨i le jiÄ› juÃ© xiÃ n yÇ’u fÄng fÇ zÃ i chÇng jÄ«ng shÄ“ng chÃ©ng shÃ ng de jÃº xiÃ n, zuÃ² zhÄ› tÃ­ chÅ« le yÄ« zhÇ’ng duÅ jiÄ“ duÃ n xÃ¹n liÃ n cÃ¨ lÃ¼Ã¨. tÄ men hÃ¡i fÄ bÃ¹ le yÄ« gÃ¨ jÄ« yÃº Minecraft de gÄo zhÃ¬ liÃ ng shÃ¬ pÃ­n shÃ¹ jÃ¹ jÃ­, bÃ¬ng zhÃ n shÃ¬ le kuÃ ng jiÃ  nÃ©ng gÃ²u shÄ“ng chÃ©ng kÄi fÃ ng yÃ¹, duÅ yÃ ng huÃ  hÃ© kÄ› kÃ²ng de yÃ²u xÃ­ shÃ¬ pÃ­n.",
        "vocab": "[{'word': 'æ¡†æ¶', 'pinyin': 'kuÃ ngjiÃ ', 'trans': 'framework'},\n{'word': 'æ—¨åœ¨', 'pinyin': 'zhÇzÃ i', 'trans': 'aim to'},\n{'word': 'é©æ–°', 'pinyin': 'gÃ©xÄ«n', 'trans': 'innovate'},\n{'word': 'å¼•æ“', 'pinyin': 'yÇnqÃ­ng', 'trans': 'engine'},\n{'word': 'é¢„è®­ç»ƒ', 'pinyin': 'yÃ¹ xÃ¹nliÃ n', 'trans': 'pre-trained'},\n{'word': 'æ‰©æ•£', 'pinyin': 'kuÃ²sÃ n', 'trans': 'diffusion'},\n{'word': 'å¤šæ ·åŒ–', 'pinyin': 'duÅyÃ nghuÃ ', 'trans': 'diversified'},\n{'word': 'å±€é™', 'pinyin': 'jÃºxiÃ n', 'trans': 'limitation'},\n{'word': 'æå‡º', 'pinyin': 'tÃ­chÅ«', 'trans': 'propose'},\n{'word': 'ç­–ç•¥', 'pinyin': 'cÃ¨lÃ¼Ã¨', 'trans': 'strategy'},\n{'word': 'åŸºäº', 'pinyin': 'jÄ«yÃº', 'trans': 'based on'},\n{'word': 'é«˜è´¨é‡', 'pinyin': 'gÄo zhÃ¬liÃ ng', 'trans': 'high quality'},\n{'word': 'æ•°æ®é›†', 'pinyin': 'shÃ¹jÃ¹ jÃ­', 'trans': 'dataset'},\n{'word': 'å±•ç¤º', 'pinyin': 'zhÇnshÃ¬', 'trans': 'demonstrate'},\n{'word': 'å¼€æ”¾åŸŸ', 'pinyin': 'kÄifÃ ng yÃ¹', 'trans': 'open domain'},\n{'word': 'å¯æ§', 'pinyin': 'kÄ›kÃ²ng', 'trans': 'controllable'}]",
        "trans": "This article introduces a framework called GameFactory, which aims to revolutionize game development by generating game engines. It utilizes pre-trained video diffusion models to create novel and diverse games. To address the limitations of existing methods in scene generation, the authors propose a multi-stage training strategy. They also release a high-quality video dataset based on Minecraft and demonstrate that the framework can generate open-domain, diverse, and controllable game videos.",
        "update_ts": "2025-01-21 09:10"
    }
}