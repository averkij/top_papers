{
    "date": {
        "ru": "22 ÑĞ½Ğ²Ğ°Ñ€Ñ",
        "en": "January 22",
        "zh": "1æœˆ22æ—¥"
    },
    "time_utc": "2025-01-22 08:13",
    "weekday": 2,
    "issue_id": 1800,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.11425",
            "title": "Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training",
            "url": "https://huggingface.co/papers/2501.11425",
            "abstract": "Large Language Models (LLMs) agents are increasingly pivotal for addressing complex tasks in interactive environments. Existing work mainly focuses on enhancing performance through behavior cloning from stronger experts, yet such approaches often falter in real-world applications, mainly due to the inability to recover from errors. However, step-level critique data is difficult and expensive to collect. Automating and dynamically constructing self-critique datasets is thus crucial to empowering models with intelligent agent capabilities. In this work, we propose an iterative self-training framework, Agent-R, that enables language Agent to Reflect on the fly. Unlike traditional methods that reward or penalize actions based on correctness, Agent-R leverages MCTS to construct training data that recover correct trajectories from erroneous ones. A key challenge of agent reflection lies in the necessity for timely revision rather than waiting until the end of a rollout. To address this, we introduce a model-guided critique construction mechanism: the actor model identifies the first error step (within its current capability) in a failed trajectory. Starting from it, we splice it with the adjacent correct path, which shares the same parent node in the tree. This strategy enables the model to learn reflection based on its current policy, therefore yielding better learning efficiency. To further explore the scalability of this self-improvement paradigm, we investigate iterative refinement of both error correction capabilities and dataset construction. Our findings demonstrate that Agent-R continuously improves the model's ability to recover from errors and enables timely error correction. Experiments on three interactive environments show that Agent-R effectively equips agents to correct erroneous actions while avoiding loops, achieving superior performance compared to baseline methods (+5.59%).",
            "score": 31,
            "issue_id": 1798,
            "pub_date": "2025-01-20",
            "pub_date_card": {
                "ru": "20 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 20",
                "zh": "1æœˆ20æ—¥"
            },
            "hash": "96d073b4606b0493",
            "authors": [
                "Siyu Yuan",
                "Zehui Chen",
                "Zhiheng Xi",
                "Junjie Ye",
                "Zhengyin Du",
                "Jiecao Chen"
            ],
            "affiliations": [
                "ByteDance",
                "Fudan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.11425.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#optimization",
                    "#agents",
                    "#training",
                    "#agi"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸ĞµÑÑ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹: Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ½Ğ° Ğ»ĞµÑ‚Ñƒ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Agent-R. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ°Ğ¼Ğ¾ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºÑƒ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡. Agent-R Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ĞœĞ¾Ğ½Ñ‚Ğµ-ĞšĞ°Ñ€Ğ»Ğ¾ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ğ´ĞµÑ€ĞµĞ²Ğ° Ğ¿Ğ¾Ğ¸ÑĞºĞ° (MCTS) Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ÑÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ñƒ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Agent-R Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Empowering Language Agents with Real-Time Self-Critique",
                    "desc": "This paper introduces Agent-R, an iterative self-training framework designed to enhance the performance of Large Language Models (LLMs) in interactive environments. Unlike traditional methods that rely on static feedback, Agent-R utilizes Monte Carlo Tree Search (MCTS) to dynamically create training data that helps models recover from mistakes in real-time. The framework focuses on timely error correction by identifying the first error in a trajectory and splicing it with a correct path, allowing the model to learn from its current policy. Experimental results show that Agent-R significantly improves the model's error recovery capabilities and overall performance, outperforming baseline methods by 5.59%."
                },
                "zh": {
                    "title": "Agent-Rï¼šå®æ—¶åæ€ï¼Œæå‡å­¦ä¹ æ•ˆç‡",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚ä»»åŠ¡çš„äº¤äº’ç¯å¢ƒä¸­å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚ç°æœ‰ç ”ç©¶ä¸»è¦é€šè¿‡æ¨¡ä»¿æ›´å¼ºä¸“å®¶çš„è¡Œä¸ºæ¥æå‡æ€§èƒ½ï¼Œä½†è¿™ç§æ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­å¸¸å¸¸å¤±è´¥ï¼Œä¸»è¦æ˜¯å› ä¸ºæ— æ³•ä»é”™è¯¯ä¸­æ¢å¤ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è¿­ä»£è‡ªæˆ‘è®­ç»ƒæ¡†æ¶Agent-Rï¼Œä½¿è¯­è¨€ä»£ç†èƒ½å¤Ÿå®æ—¶åæ€ã€‚Agent-Ré€šè¿‡æ„å»ºè®­ç»ƒæ•°æ®æ¥çº æ­£é”™è¯¯è½¨è¿¹ï¼Œä»è€Œæé«˜æ¨¡å‹çš„å­¦ä¹ æ•ˆç‡å’Œé”™è¯¯æ¢å¤èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.12380",
            "title": "MMVU: Measuring Expert-Level Multi-Discipline Video Understanding",
            "url": "https://huggingface.co/papers/2501.12380",
            "abstract": "We introduce MMVU, a comprehensive expert-level, multi-discipline benchmark for evaluating foundation models in video understanding. MMVU includes 3,000 expert-annotated questions spanning 27 subjects across four core disciplines: Science, Healthcare, Humanities & Social Sciences, and Engineering. Compared to prior benchmarks, MMVU features three key advancements. First, it challenges models to apply domain-specific knowledge and perform expert-level reasoning to analyze specialized-domain videos, moving beyond the basic visual perception typically assessed in current video benchmarks. Second, each example is annotated by human experts from scratch. We implement strict data quality controls to ensure the high quality of the dataset. Finally, each example is enriched with expert-annotated reasoning rationals and relevant domain knowledge, facilitating in-depth analysis. We conduct an extensive evaluation of 32 frontier multimodal foundation models on MMVU. The latest System-2-capable models, o1 and Gemini 2.0 Flash Thinking, achieve the highest performance among the tested models. However, they still fall short of matching human expertise. Through in-depth error analyses and case studies, we offer actionable insights for future advancements in expert-level, knowledge-intensive video understanding for specialized domains.",
            "score": 29,
            "issue_id": 1797,
            "pub_date": "2025-01-21",
            "pub_date_card": {
                "ru": "21 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 21",
                "zh": "1æœˆ21æ—¥"
            },
            "hash": "dcb04aaca349cc32",
            "authors": [
                "Yilun Zhao",
                "Lujing Xie",
                "Haowei Zhang",
                "Guo Gan",
                "Yitao Long",
                "Zhiyuan Hu",
                "Tongyan Hu",
                "Weiyuan Chen",
                "Chuhan Li",
                "Junyang Song",
                "Zhijian Xu",
                "Chengye Wang",
                "Weifeng Pan",
                "Ziyao Shangguan",
                "Xiangru Tang",
                "Zhenwen Liang",
                "Yixin Liu",
                "Chen Zhao",
                "Arman Cohan"
            ],
            "affiliations": [
                "Yale NLP"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.12380.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#science",
                    "#benchmark",
                    "#video",
                    "#healthcare",
                    "#reasoning"
                ],
                "emoji": "ğŸ“",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾: Ğ¾Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğº ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ğ¾Ğ¼Ñƒ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MMVU - Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ğ¹ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. MMVU Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 3000 Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¿Ğ¾ 27 Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ°Ğ¼ Ğ² Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ÑÑ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸, Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°. ĞÑ†ĞµĞ½ĞºĞ° 32 Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° MMVU Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ° Ğ½Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°-ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ° Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ."
                },
                "en": {
                    "title": "MMVU: Elevating Video Understanding to Expert Levels",
                    "desc": "The paper presents MMVU, a new benchmark designed to evaluate foundation models specifically in video understanding across various expert domains. It includes 3,000 questions that require advanced reasoning and domain-specific knowledge, moving beyond simple visual recognition tasks. Each question is meticulously annotated by human experts, ensuring high data quality and providing reasoning rationales to enhance analysis. The evaluation of 32 advanced multimodal models reveals that while some perform well, they still do not reach the level of human expertise, highlighting areas for future improvement in this field."
                },
                "zh": {
                    "title": "MMVUï¼šè§†é¢‘ç†è§£çš„æ–°æ ‡å‡†",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†MMVUï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„ä¸“å®¶çº§å¤šå­¦ç§‘åŸºå‡†ï¼Œç”¨äºè¯„ä¼°åŸºç¡€æ¨¡å‹åœ¨è§†é¢‘ç†è§£æ–¹é¢çš„è¡¨ç°ã€‚MMVUåŒ…å«3000ä¸ªä¸“å®¶æ³¨é‡Šçš„é—®é¢˜ï¼Œæ¶µç›–ç§‘å­¦ã€åŒ»ç–—ã€äººæ–‡å­¦ç§‘ä¸ç¤¾ä¼šç§‘å­¦å’Œå·¥ç¨‹å››ä¸ªæ ¸å¿ƒå­¦ç§‘ã€‚ä¸ä¹‹å‰çš„åŸºå‡†ç›¸æ¯”ï¼ŒMMVUåœ¨ä¸‰ä¸ªå…³é”®æ–¹é¢æœ‰æ‰€æ”¹è¿›ï¼ŒåŒ…æ‹¬è¦æ±‚æ¨¡å‹åº”ç”¨é¢†åŸŸç‰¹å®šçŸ¥è¯†è¿›è¡Œä¸“å®¶çº§æ¨ç†ï¼Œç¡®ä¿æ•°æ®é›†çš„é«˜è´¨é‡ï¼Œä»¥åŠä¸ºæ¯ä¸ªç¤ºä¾‹æä¾›ä¸“å®¶æ³¨é‡Šçš„æ¨ç†ä¾æ®å’Œç›¸å…³é¢†åŸŸçŸ¥è¯†ã€‚æˆ‘ä»¬å¯¹32ä¸ªå‰æ²¿å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹åœ¨MMVUä¸Šçš„è¡¨ç°è¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œå‘ç°æœ€æ–°çš„ç³»ç»Ÿ2èƒ½åŠ›æ¨¡å‹o1å’ŒGemini 2.0 Flash Thinkingåœ¨æµ‹è¯•æ¨¡å‹ä¸­è¡¨ç°æœ€ä½³ï¼Œä½†ä»æœªèƒ½è¾¾åˆ°äººç±»ä¸“å®¶çš„æ°´å¹³ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.11873",
            "title": "Demons in the Detail: On Implementing Load Balancing Loss for Training Specialized Mixture-of-Expert Models",
            "url": "https://huggingface.co/papers/2501.11873",
            "abstract": "This paper revisits the implementation of Load-balancing Loss (LBL) when training Mixture-of-Experts (MoEs) models. Specifically, LBL for MoEs is defined as N_E sum_{i=1}^{N_E} f_i p_i, where N_E is the total number of experts, f_i represents the frequency of expert i being selected, and p_i denotes the average gating score of the expert i. Existing MoE training frameworks usually employ the parallel training strategy so that f_i and the LBL are calculated within a micro-batch and then averaged across parallel groups. In essence, a micro-batch for training billion-scale LLMs normally contains very few sequences. So, the micro-batch LBL is almost at the sequence level, and the router is pushed to distribute the token evenly within each sequence. Under this strict constraint, even tokens from a domain-specific sequence (e.g., code) are uniformly routed to all experts, thereby inhibiting expert specialization. In this work, we propose calculating LBL using a global-batch to loose this constraint. Because a global-batch contains much more diverse sequences than a micro-batch, which will encourage load balance at the corpus level. Specifically, we introduce an extra communication step to synchronize f_i across micro-batches and then use it to calculate the LBL. Through experiments on training MoEs-based LLMs (up to 42.8B total parameters and 400B tokens), we surprisingly find that the global-batch LBL strategy yields excellent performance gains in both pre-training perplexity and downstream tasks. Our analysis reveals that the global-batch LBL also greatly improves the domain specialization of MoE experts.",
            "score": 27,
            "issue_id": 1797,
            "pub_date": "2025-01-21",
            "pub_date_card": {
                "ru": "21 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 21",
                "zh": "1æœˆ21æ—¥"
            },
            "hash": "370d057fec504963",
            "authors": [
                "Zihan Qiu",
                "Zeyu Huang",
                "Bo Zheng",
                "Kaiyue Wen",
                "Zekun Wang",
                "Rui Men",
                "Ivan Titov",
                "Dayiheng Liu",
                "Jingren Zhou",
                "Junyang Lin"
            ],
            "affiliations": [
                "Qwen Team, Alibaba Group",
                "Stanford University",
                "University of Edinburgh"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.11873.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#training"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "Ğ“Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞµ Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ² MoE Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ (LBL) Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Mixture-of-Experts (MoE). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ÑÑ‚ÑŒ LBL Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ±Ğ°Ñ‚Ñ‡Ğ°, Ğ° Ğ½Ğµ Ğ¼Ğ¸ĞºÑ€Ğ¾-Ğ±Ğ°Ñ‚Ñ‡Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾ÑĞ»Ğ°Ğ±Ğ¸Ñ‚ÑŒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿ĞµÑ€Ğ¿Ğ»ĞµĞºÑĞ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… downstream. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ğ¼."
                },
                "en": {
                    "title": "Enhancing Expert Specialization with Global-Batch Load-Balancing",
                    "desc": "This paper focuses on improving the Load-balancing Loss (LBL) in training Mixture-of-Experts (MoEs) models. The authors highlight that traditional methods use micro-batches, which limit the diversity of sequences and hinder expert specialization. They propose a new approach that utilizes global-batches, allowing for a broader range of sequences and better load balancing across the entire dataset. Experimental results show that this global-batch LBL method significantly enhances model performance and expert specialization in large language models."
                },
                "zh": {
                    "title": "å…¨å±€æ‰¹æ¬¡æå‡æ··åˆä¸“å®¶æ¨¡å‹çš„è´Ÿè½½å‡è¡¡ä¸ä¸“ä¸šåŒ–",
                    "desc": "æœ¬æ–‡é‡æ–°å®¡è§†äº†åœ¨è®­ç»ƒæ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMoEsï¼‰æ—¶çš„è´Ÿè½½å‡è¡¡æŸå¤±ï¼ˆLBLï¼‰å®ç°ã€‚æˆ‘ä»¬æå‡ºä½¿ç”¨å…¨å±€æ‰¹æ¬¡æ¥è®¡ç®—LBLï¼Œä»¥æ‰“ç ´å¾®æ‰¹æ¬¡çš„ä¸¥æ ¼çº¦æŸï¼Œä»è€Œåœ¨è¯­æ–™åº“å±‚é¢ä¸Šä¿ƒè¿›è´Ÿè½½å‡è¡¡ã€‚é€šè¿‡åœ¨è®­ç»ƒä¸­å¼•å…¥é¢å¤–çš„é€šä¿¡æ­¥éª¤æ¥åŒæ­¥ä¸“å®¶é€‰æ‹©é¢‘ç‡ï¼Œå®éªŒç»“æœæ˜¾ç¤ºå…¨å±€æ‰¹æ¬¡LBLç­–ç•¥åœ¨é¢„è®­ç»ƒå›°æƒ‘åº¦å’Œä¸‹æ¸¸ä»»åŠ¡ä¸­å‡æ˜¾è‘—æå‡äº†æ€§èƒ½ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œå…¨å±€æ‰¹æ¬¡LBLè¿˜å¤§å¤§æ”¹å–„äº†MoEä¸“å®¶çš„é¢†åŸŸä¸“ä¸šåŒ–ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.11733",
            "title": "Mobile-Agent-E: Self-Evolving Mobile Assistant for Complex Tasks",
            "url": "https://huggingface.co/papers/2501.11733",
            "abstract": "Smartphones have become indispensable in modern life, yet navigating complex tasks on mobile devices often remains frustrating. Recent advancements in large multimodal model (LMM)-based mobile agents have demonstrated the ability to perceive and act in mobile environments. However, current approaches face significant limitations: they fall short in addressing real-world human needs, struggle with reasoning-intensive and long-horizon tasks, and lack mechanisms to learn and improve from prior experiences. To overcome these challenges, we introduce Mobile-Agent-E, a hierarchical multi-agent framework capable of self-evolution through past experience. By hierarchical, we mean an explicit separation of high-level planning and low-level action execution. The framework comprises a Manager, responsible for devising overall plans by breaking down complex tasks into subgoals, and four subordinate agents--Perceptor, Operator, Action Reflector, and Notetaker--which handle fine-grained visual perception, immediate action execution, error verification, and information aggregation, respectively. Mobile-Agent-E also features a novel self-evolution module which maintains a persistent long-term memory comprising Tips and Shortcuts. Tips are general guidance and lessons learned from prior tasks on how to effectively interact with the environment. Shortcuts are reusable, executable sequences of atomic operations tailored for specific subroutines. The inclusion of Tips and Shortcuts facilitates continuous refinement in performance and efficiency. Alongside this framework, we introduce Mobile-Eval-E, a new benchmark featuring complex mobile tasks requiring long-horizon, multi-app interactions. Empirical results show that Mobile-Agent-E achieves a 22% absolute improvement over previous state-of-the-art approaches across three foundation model backbones. Project page: https://x-plug.github.io/MobileAgent.",
            "score": 12,
            "issue_id": 1798,
            "pub_date": "2025-01-20",
            "pub_date_card": {
                "ru": "20 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 20",
                "zh": "1æœˆ20æ—¥"
            },
            "hash": "a9cddb8786536def",
            "authors": [
                "Zhenhailong Wang",
                "Haiyang Xu",
                "Junyang Wang",
                "Xi Zhang",
                "Ming Yan",
                "Ji Zhang",
                "Fei Huang",
                "Heng Ji"
            ],
            "affiliations": [
                "Alibaba Group",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.11733.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#optimization",
                    "#agents",
                    "#multimodal",
                    "#long_context"
                ],
                "emoji": "ğŸ“±",
                "ru": {
                    "title": "ĞœĞ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ˜Ğ˜-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚ Ñ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Mobile-Agent-E - Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ĞœĞµĞ½ĞµĞ´Ğ¶ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ğ¿Ğ¾Ğ´Ñ‡Ğ¸Ğ½ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ, Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¸ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒÑ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ ÑĞ°Ğ¼Ğ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰ĞµĞ¹ ĞŸĞ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸ Ğ¸ Ğ¯Ñ€Ğ»Ñ‹ĞºĞ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ Mobile-Eval-E."
                },
                "en": {
                    "title": "Empowering Mobile Agents with Self-Evolution for Enhanced Task Performance",
                    "desc": "This paper presents Mobile-Agent-E, a hierarchical multi-agent framework designed to enhance mobile task performance by learning from past experiences. The framework separates high-level planning from low-level execution, utilizing a Manager for task decomposition and four specialized agents for perception, action, error checking, and information management. A key feature is the self-evolution module, which incorporates a long-term memory of Tips and Shortcuts to improve task efficiency and effectiveness. Experimental results demonstrate that Mobile-Agent-E significantly outperforms existing methods, achieving a 22% improvement in complex mobile tasks."
                },
                "zh": {
                    "title": "æ™ºèƒ½æ‰‹æœºä»»åŠ¡æ‰§è¡Œçš„æ–°çªç ´",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºMobile-Agent-Eçš„å±‚æ¬¡åŒ–å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨æå‡æ™ºèƒ½æ‰‹æœºä¸Šçš„ä»»åŠ¡æ‰§è¡Œèƒ½åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†é«˜å±‚è§„åˆ’ä¸ä½å±‚æ‰§è¡Œæ˜ç¡®åˆ†ç¦»ï¼ŒåŒ…å«ä¸€ä¸ªç®¡ç†è€…å’Œå››ä¸ªå­ä»£ç†ï¼Œåˆ†åˆ«è´Ÿè´£è§†è§‰æ„ŸçŸ¥ã€åŠ¨ä½œæ‰§è¡Œã€é”™è¯¯éªŒè¯å’Œä¿¡æ¯èšåˆã€‚Mobile-Agent-Eè¿˜å¼•å…¥äº†è‡ªæˆ‘è¿›åŒ–æ¨¡å—ï¼Œåˆ©ç”¨é•¿æœŸè®°å¿†ä¸­çš„æç¤ºå’Œæ·å¾„æ¥ä¸æ–­ä¼˜åŒ–æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤æ‚ç§»åŠ¨ä»»åŠ¡ä¸­ç›¸è¾ƒäºç°æœ‰æ–¹æ³•æœ‰22%çš„ç»å¯¹æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.12326",
            "title": "UI-TARS: Pioneering Automated GUI Interaction with Native Agents",
            "url": "https://huggingface.co/papers/2501.12326",
            "abstract": "This paper introduces UI-TARS, a native GUI agent model that solely perceives the screenshots as input and performs human-like interactions (e.g., keyboard and mouse operations). Unlike prevailing agent frameworks that depend on heavily wrapped commercial models (e.g., GPT-4o) with expert-crafted prompts and workflows, UI-TARS is an end-to-end model that outperforms these sophisticated frameworks. Experiments demonstrate its superior performance: UI-TARS achieves SOTA performance in 10+ GUI agent benchmarks evaluating perception, grounding, and GUI task execution. Notably, in the OSWorld benchmark, UI-TARS achieves scores of 24.6 with 50 steps and 22.7 with 15 steps, outperforming Claude (22.0 and 14.9 respectively). In AndroidWorld, UI-TARS achieves 46.6, surpassing GPT-4o (34.5). UI-TARS incorporates several key innovations: (1) Enhanced Perception: leveraging a large-scale dataset of GUI screenshots for context-aware understanding of UI elements and precise captioning; (2) Unified Action Modeling, which standardizes actions into a unified space across platforms and achieves precise grounding and interaction through large-scale action traces; (3) System-2 Reasoning, which incorporates deliberate reasoning into multi-step decision making, involving multiple reasoning patterns such as task decomposition, reflection thinking, milestone recognition, etc. (4) Iterative Training with Reflective Online Traces, which addresses the data bottleneck by automatically collecting, filtering, and reflectively refining new interaction traces on hundreds of virtual machines. Through iterative training and reflection tuning, UI-TARS continuously learns from its mistakes and adapts to unforeseen situations with minimal human intervention. We also analyze the evolution path of GUI agents to guide the further development of this domain.",
            "score": 9,
            "issue_id": 1797,
            "pub_date": "2025-01-21",
            "pub_date_card": {
                "ru": "21 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 21",
                "zh": "1æœˆ21æ—¥"
            },
            "hash": "1f98d8f49b073983",
            "authors": [
                "Yujia Qin",
                "Yining Ye",
                "Junjie Fang",
                "Haoming Wang",
                "Shihao Liang",
                "Shizuo Tian",
                "Junda Zhang",
                "Jiahao Li",
                "Yunxin Li",
                "Shijue Huang",
                "Wanjun Zhong",
                "Kuanye Li",
                "Jiale Yang",
                "Yu Miao",
                "Woyu Lin",
                "Longxiang Liu",
                "Xu Jiang",
                "Qianli Ma",
                "Jingyu Li",
                "Xiaojun Xiao",
                "Kai Cai",
                "Chuang Li",
                "Yaowei Zheng",
                "Chaolin Jin",
                "Chen Li",
                "Xiao Zhou",
                "Minchao Wang",
                "Haoli Chen",
                "Zhaojian Li",
                "Haihua Yang",
                "Haifeng Liu",
                "Feng Lin",
                "Tao Peng",
                "Xin Liu",
                "Guang Shi"
            ],
            "affiliations": [
                "ByteDance Seed",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.12326.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#dataset",
                    "#agents",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "ğŸ–¥ï¸",
                "ru": {
                    "title": "UI-TARS: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¼Ğ¸Ñ€Ğµ GUI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ UI-TARS - Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑĞºÑ€Ğ¸Ğ½ÑˆĞ¾Ñ‚Ñ‹ Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼. UI-TARS Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 10 Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ´Ğ»Ñ GUI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¹: ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ, ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ-2 Ğ¸ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ñ‚Ñ€Ğ°ÑÑĞ°Ğ¼Ğ¸. UI-TARS Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾ ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ğ½Ğ° ÑĞ²Ğ¾Ğ¸Ñ… Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ñ… Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğº Ğ½ĞµĞ¿Ñ€ĞµĞ´Ğ²Ğ¸Ğ´ĞµĞ½Ğ½Ñ‹Ğ¼ ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸ÑĞ¼ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ğ¼ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ¾Ğ¼ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°."
                },
                "en": {
                    "title": "Revolutionizing GUI Interaction with UI-TARS: The End-to-End Agent Model",
                    "desc": "UI-TARS is a novel GUI agent model that processes screenshots to perform tasks like a human would, using keyboard and mouse actions. Unlike existing models that rely on complex commercial frameworks and pre-defined prompts, UI-TARS operates end-to-end and shows superior performance in various benchmarks. It achieves state-of-the-art results in GUI task execution by utilizing enhanced perception, unified action modeling, and system-2 reasoning for better decision-making. Additionally, its iterative training approach allows it to learn from past interactions, improving its adaptability with minimal human input."
                },
                "zh": {
                    "title": "UI-TARSï¼šé©æ–°å›¾å½¢ç”¨æˆ·ç•Œé¢ä»£ç†çš„å…¨æ–°æ¨¡å‹",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†UI-TARSï¼Œè¿™æ˜¯ä¸€ç§åŸç”Ÿçš„å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†æ¨¡å‹ï¼Œèƒ½å¤Ÿä»…é€šè¿‡å±å¹•æˆªå›¾è¿›è¡Œäººç±»èˆ¬çš„äº¤äº’ã€‚ä¸ä¾èµ–å¤æ‚å•†ä¸šæ¨¡å‹çš„ç°æœ‰ä»£ç†æ¡†æ¶ä¸åŒï¼ŒUI-TARSæ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„æ¨¡å‹ï¼Œåœ¨å¤šä¸ªGUIä»£ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨æ„ŸçŸ¥ã€å®šä½å’Œä»»åŠ¡æ‰§è¡Œæ–¹é¢ã€‚UI-TARSé€šè¿‡å¢å¼ºæ„ŸçŸ¥ã€ç»Ÿä¸€åŠ¨ä½œå»ºæ¨¡ã€ç³»ç»Ÿ-2æ¨ç†å’Œåæ€åœ¨çº¿è¿½è¸ªç­‰åˆ›æ–°ï¼Œæ˜¾è‘—æé«˜äº†å…¶æ€§èƒ½ã€‚é€šè¿‡è¿­ä»£è®­ç»ƒå’Œåæ€è°ƒä¼˜ï¼ŒUI-TARSèƒ½å¤Ÿä¸æ–­å­¦ä¹ å¹¶é€‚åº”æ–°çš„æƒ…å†µï¼Œå‡å°‘å¯¹äººç±»å¹²é¢„çš„éœ€æ±‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.12273",
            "title": "Condor: Enhance LLM Alignment with Knowledge-Driven Data Synthesis and Refinement",
            "url": "https://huggingface.co/papers/2501.12273",
            "abstract": "The quality of Supervised Fine-Tuning (SFT) data plays a critical role in enhancing the conversational capabilities of Large Language Models (LLMs). However, as LLMs become more advanced, the availability of high-quality human-annotated SFT data has become a significant bottleneck, necessitating a greater reliance on synthetic training data. In this work, we introduce Condor, a novel two-stage synthetic data generation framework that incorporates World Knowledge Tree and Self-Reflection Refinement to produce high-quality SFT data at scale. Our experimental results demonstrate that a base model fine-tuned on only 20K Condor-generated samples achieves superior performance compared to counterparts. The additional refinement stage in Condor further enables iterative self-improvement for LLMs at various scales (up to 72B), validating the effectiveness of our approach. Furthermore, our investigation into the scaling for synthetic data in post-training reveals substantial unexplored potential for performance improvements, opening promising avenues for future research.",
            "score": 9,
            "issue_id": 1796,
            "pub_date": "2025-01-21",
            "pub_date_card": {
                "ru": "21 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 21",
                "zh": "1æœˆ21æ—¥"
            },
            "hash": "10499c8b820d5368",
            "authors": [
                "Maosong Cao",
                "Taolin Zhang",
                "Mo Li",
                "Chuyu Zhang",
                "Yunxin Liu",
                "Haodong Duan",
                "Songyang Zhang",
                "Kai Chen"
            ],
            "affiliations": [
                "Shanghai AI Laboratory",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.12273.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#synthetic",
                    "#data",
                    "#dataset",
                    "#training"
                ],
                "emoji": "ğŸ¦…",
                "ru": {
                    "title": "Condor: Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Condor - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´ĞµÑ€ĞµĞ²Ğ¾ Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ ÑĞ°Ğ¼Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° 20 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ñ… ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Condor Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ LLM Ğ¿Ñ€Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Unlocking LLM Potential with Synthetic Data Generation",
                    "desc": "This paper addresses the challenge of obtaining high-quality Supervised Fine-Tuning (SFT) data for Large Language Models (LLMs). It presents Condor, a two-stage framework that generates synthetic training data using World Knowledge Tree and Self-Reflection Refinement techniques. The results show that models fine-tuned with just 20,000 samples from Condor outperform those trained with traditional methods. Additionally, the framework allows for iterative self-improvement, suggesting significant potential for enhancing LLM performance through synthetic data."
                },
                "zh": {
                    "title": "åˆæˆæ•°æ®ç”Ÿæˆï¼Œæå‡å¯¹è¯èƒ½åŠ›çš„å…³é”®",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ•°æ®çš„è´¨é‡å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹è¯èƒ½åŠ›çš„é‡è¦æ€§ã€‚éšç€LLMsçš„è¿›æ­¥ï¼Œé«˜è´¨é‡çš„äººç±»æ ‡æ³¨SFTæ•°æ®å˜å¾—ç¨€ç¼ºï¼Œå› æ­¤éœ€è¦æ›´å¤šä¾èµ–åˆæˆè®­ç»ƒæ•°æ®ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºCondorçš„ä¸¤é˜¶æ®µåˆæˆæ•°æ®ç”Ÿæˆæ¡†æ¶ï¼Œç»“åˆäº†ä¸–ç•ŒçŸ¥è¯†æ ‘å’Œè‡ªæˆ‘åæ€ç²¾ç‚¼ï¼Œä»¥å¤§è§„æ¨¡ç”Ÿæˆé«˜è´¨é‡çš„SFTæ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä»…ç”¨20Kä¸ªCondorç”Ÿæˆçš„æ ·æœ¬å¾®è°ƒçš„åŸºç¡€æ¨¡å‹ï¼Œå…¶æ€§èƒ½ä¼˜äºå…¶ä»–æ¨¡å‹ï¼ŒéªŒè¯äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.12202",
            "title": "Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D Assets Generation",
            "url": "https://huggingface.co/papers/2501.12202",
            "abstract": "We present Hunyuan3D 2.0, an advanced large-scale 3D synthesis system for generating high-resolution textured 3D assets. This system includes two foundation components: a large-scale shape generation model -- Hunyuan3D-DiT, and a large-scale texture synthesis model -- Hunyuan3D-Paint. The shape generative model, built on a scalable flow-based diffusion transformer, aims to create geometry that properly aligns with a given condition image, laying a solid foundation for downstream applications. The texture synthesis model, benefiting from strong geometric and diffusion priors, produces high-resolution and vibrant texture maps for either generated or hand-crafted meshes. Furthermore, we build Hunyuan3D-Studio -- a versatile, user-friendly production platform that simplifies the re-creation process of 3D assets. It allows both professional and amateur users to manipulate or even animate their meshes efficiently. We systematically evaluate our models, showing that Hunyuan3D 2.0 outperforms previous state-of-the-art models, including the open-source models and closed-source models in geometry details, condition alignment, texture quality, and etc. Hunyuan3D 2.0 is publicly released in order to fill the gaps in the open-source 3D community for large-scale foundation generative models. The code and pre-trained weights of our models are available at: https://github.com/Tencent/Hunyuan3D-2",
            "score": 8,
            "issue_id": 1798,
            "pub_date": "2025-01-21",
            "pub_date_card": {
                "ru": "21 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 21",
                "zh": "1æœˆ21æ—¥"
            },
            "hash": "f95f069cba0bd83e",
            "authors": [
                "Zibo Zhao",
                "Zeqiang Lai",
                "Qingxiang Lin",
                "Yunfei Zhao",
                "Haolin Liu",
                "Shuhui Yang",
                "Yifei Feng",
                "Mingxin Yang",
                "Sheng Zhang",
                "Xianghui Yang",
                "Huiwen Shi",
                "Sicong Liu",
                "Junta Wu",
                "Yihang Lian",
                "Fan Yang",
                "Ruining Tang",
                "Zebin He",
                "Xinzhou Wang",
                "Jian Liu",
                "Xuhui Zuo",
                "Zhuo Chen",
                "Biwen Lei",
                "Haohan Weng",
                "Jing Xu",
                "Yiling Zhu",
                "Xinhai Liu",
                "Lixin Xu",
                "Changrong Hu",
                "Tianyu Huang",
                "Lifu Wang",
                "Jihong Zhang",
                "Meng Chen",
                "Liang Dong",
                "Yiwen Jia",
                "Yulin Cai",
                "Jiaao Yu",
                "Yixuan Tang",
                "Hao Zhang",
                "Zheng Ye",
                "Peng He",
                "Runzhou Wu",
                "Chao Zhang",
                "Yonghao Tan",
                "Jie Xiao",
                "Yangyu Tao",
                "Jianchen Zhu",
                "Jinbao Xue",
                "Kai Liu",
                "Chongqing Zhao",
                "Xinming Wu",
                "Zhichao Hu",
                "Lei Qin",
                "Jianbing Peng",
                "Zhan Li",
                "Minghui Chen",
                "Xipeng Zhang",
                "Lin Niu",
                "Paige Wang",
                "Yingkai Wang",
                "Haozhao Kuang",
                "Zhongyi Fan",
                "Xu Zheng",
                "Weihao Zhuang",
                "YingPing He",
                "Tian Liu",
                "Yong Yang",
                "Di Wang",
                "Yuhong Liu",
                "Jie Jiang",
                "Jingwei Huang",
                "Chunchao Guo"
            ],
            "affiliations": [
                "Tencent"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.12202.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#open_source",
                    "#3d"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² 3D-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸: Ğ¾Ñ‚ Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Ğº Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ğµ",
                    "desc": "Hunyuan3D 2.0 - ÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. ĞĞ½Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²: Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ„Ğ¾Ñ€Ğ¼ Hunyuan3D-DiT Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ‚ĞµĞºÑÑ‚ÑƒÑ€ Hunyuan3D-Paint. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ„Ğ¾Ñ€Ğ¼ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğµ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ, ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ‚ĞµĞºÑÑ‚ÑƒÑ€, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ°Ğ¹Ğ¼Ñ‹, ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ´Ğ»Ñ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ Ğ¼ĞµÑˆĞµĞ¹."
                },
                "en": {
                    "title": "Revolutionizing 3D Asset Creation with Hunyuan3D 2.0",
                    "desc": "Hunyuan3D 2.0 is a sophisticated system designed for creating high-quality 3D models with detailed textures. It consists of two main components: Hunyuan3D-DiT for generating 3D shapes and Hunyuan3D-Paint for applying textures. The shape model uses a flow-based diffusion transformer to ensure that the generated geometry matches the input conditions, while the texture model leverages geometric and diffusion principles to create vibrant textures. This system not only enhances the quality of 3D assets but also provides an accessible platform for users to create and animate their models easily."
                },
                "zh": {
                    "title": "Hunyuan3D 2.0ï¼šé«˜æ•ˆç”Ÿæˆé«˜è´¨é‡3Dèµ„äº§çš„ç³»ç»Ÿ",
                    "desc": "Hunyuan3D 2.0 æ˜¯ä¸€ä¸ªå…ˆè¿›çš„å¤§è§„æ¨¡ 3D åˆæˆç³»ç»Ÿï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜åˆ†è¾¨ç‡çš„çº¹ç† 3D èµ„äº§ã€‚è¯¥ç³»ç»ŸåŒ…å«ä¸¤ä¸ªåŸºç¡€ç»„ä»¶ï¼šHunyuan3D-DiT å½¢çŠ¶ç”Ÿæˆæ¨¡å‹å’Œ Hunyuan3D-Paint çº¹ç†åˆæˆæ¨¡å‹ã€‚å½¢çŠ¶ç”Ÿæˆæ¨¡å‹åŸºäºå¯æ‰©å±•çš„æµå¼æ‰©æ•£å˜æ¢å™¨ï¼Œæ—¨åœ¨åˆ›å»ºä¸ç»™å®šæ¡ä»¶å›¾åƒç›¸åŒ¹é…çš„å‡ ä½•å½¢çŠ¶ã€‚çº¹ç†åˆæˆæ¨¡å‹åˆ™åˆ©ç”¨å¼ºå¤§çš„å‡ ä½•å’Œæ‰©æ•£å…ˆéªŒï¼Œä¸ºç”Ÿæˆæˆ–æ‰‹å·¥åˆ¶ä½œçš„ç½‘æ ¼ç”Ÿæˆé«˜åˆ†è¾¨ç‡çš„ç”ŸåŠ¨çº¹ç†å›¾ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.10687",
            "title": "EMO2: End-Effector Guided Audio-Driven Avatar Video Generation",
            "url": "https://huggingface.co/papers/2501.10687",
            "abstract": "In this paper, we propose a novel audio-driven talking head method capable of simultaneously generating highly expressive facial expressions and hand gestures. Unlike existing methods that focus on generating full-body or half-body poses, we investigate the challenges of co-speech gesture generation and identify the weak correspondence between audio features and full-body gestures as a key limitation. To address this, we redefine the task as a two-stage process. In the first stage, we generate hand poses directly from audio input, leveraging the strong correlation between audio signals and hand movements. In the second stage, we employ a diffusion model to synthesize video frames, incorporating the hand poses generated in the first stage to produce realistic facial expressions and body movements. Our experimental results demonstrate that the proposed method outperforms state-of-the-art approaches, such as CyberHost and Vlogger, in terms of both visual quality and synchronization accuracy. This work provides a new perspective on audio-driven gesture generation and a robust framework for creating expressive and natural talking head animations.",
            "score": 7,
            "issue_id": 1798,
            "pub_date": "2025-01-18",
            "pub_date_card": {
                "ru": "18 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 18",
                "zh": "1æœˆ18æ—¥"
            },
            "hash": "13c0931101eb51eb",
            "authors": [
                "Linrui Tian",
                "Siqi Hu",
                "Qi Wang",
                "Bang Zhang",
                "Liefeng Bo"
            ],
            "affiliations": [
                "Institute for Intelligent Computing, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.10687.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#audio",
                    "#video",
                    "#games",
                    "#diffusion"
                ],
                "emoji": "ğŸ—£ï¸",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸: Ğ¾Ñ‚ Ğ·Ğ²ÑƒĞºĞ° Ğº Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ¶ĞµÑÑ‚Ğ°Ğ¼",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰ĞµĞ¹ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğ¹ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¸Ğ¼Ğ¸ĞºÑƒ Ğ¸ Ğ¶ĞµÑÑ‚Ñ‹ Ñ€ÑƒĞº. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ĞºĞ°Ğº Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ¿Ğ¾Ğ·Ñ‹ Ñ€ÑƒĞº Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¸Ğ· Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ²Ñ…Ğ¾Ğ´Ğ°, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ°Ğ´Ñ€Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¶ĞµÑÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½ÑƒÑ Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¹ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰ĞµĞ¹ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹."
                },
                "en": {
                    "title": "Expressive Talking Heads: Bridging Audio and Gesture Generation",
                    "desc": "This paper introduces a new method for creating talking head animations that are driven by audio. It focuses on generating both facial expressions and hand gestures, addressing the limitations of previous methods that often overlook the connection between audio and gestures. The approach is divided into two stages: first, it generates hand poses from audio signals, and then it uses a diffusion model to create video frames that combine these hand poses with realistic facial movements. The results show that this method is more effective than existing techniques, providing better visual quality and synchronization with the audio."
                },
                "zh": {
                    "title": "éŸ³é¢‘é©±åŠ¨çš„ç”ŸåŠ¨è¡¨æƒ…ä¸æ‰‹åŠ¿ç”Ÿæˆæ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„éŸ³é¢‘é©±åŠ¨çš„è¯´è¯å¤´æ–¹æ³•ï¼Œèƒ½å¤ŸåŒæ—¶ç”Ÿæˆé«˜åº¦è¡¨ç°åŠ›çš„é¢éƒ¨è¡¨æƒ…å’Œæ‰‹åŠ¿ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬å…³æ³¨äºå…±è¯­æ‰‹åŠ¿ç”Ÿæˆçš„æŒ‘æˆ˜ï¼Œå¹¶è¯†åˆ«éŸ³é¢‘ç‰¹å¾ä¸å…¨èº«æ‰‹åŠ¿ä¹‹é—´çš„å¼±å¯¹åº”å…³ç³»ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å°†ä»»åŠ¡é‡æ–°å®šä¹‰ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µç›´æ¥ä»éŸ³é¢‘è¾“å…¥ç”Ÿæˆæ‰‹åŠ¿ï¼Œç¬¬äºŒé˜¶æ®µä½¿ç”¨æ‰©æ•£æ¨¡å‹åˆæˆè§†é¢‘å¸§ï¼Œç»“åˆç¬¬ä¸€é˜¶æ®µç”Ÿæˆçš„æ‰‹åŠ¿ï¼Œäº§ç”Ÿé€¼çœŸçš„é¢éƒ¨è¡¨æƒ…å’Œèº«ä½“åŠ¨ä½œã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è§†è§‰è´¨é‡å’ŒåŒæ­¥ç²¾åº¦æ–¹é¢ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.11223",
            "title": "Reasoning Language Models: A Blueprint",
            "url": "https://huggingface.co/papers/2501.11223",
            "abstract": "Reasoning language models (RLMs), also known as Large Reasoning Models (LRMs), such as OpenAI's o1 and o3, DeepSeek-V3, and Alibaba's QwQ, have redefined AI's problem-solving capabilities by extending large language models (LLMs) with advanced reasoning mechanisms. Yet, their high costs, proprietary nature, and complex architectures - uniquely combining Reinforcement Learning (RL), search heuristics, and LLMs - present accessibility and scalability challenges. To address these, we propose a comprehensive blueprint that organizes RLM components into a modular framework, based on a survey and analysis of all RLM works. This blueprint incorporates diverse reasoning structures (chains, trees, graphs, and nested forms), reasoning strategies (e.g., Monte Carlo Tree Search, Beam Search), RL concepts (policy, value models and others), and supervision schemes (Output-Based and Process-Based Supervision). We also provide detailed mathematical formulations and algorithmic specifications to simplify RLM implementation. By showing how schemes like LLaMA-Berry, QwQ, Journey Learning, and Graph of Thoughts fit as special cases, we demonstrate the blueprint's versatility and unifying potential. To illustrate its utility, we introduce x1, a modular implementation for rapid RLM prototyping and experimentation. Using x1 and a literature review, we provide key insights, such as multi-phase training for policy and value models, and the importance of familiar training distributions. Finally, we outline how RLMs can integrate with a broader LLM ecosystem, including tools and databases. Our work demystifies RLM construction, democratizes advanced reasoning capabilities, and fosters innovation, aiming to mitigate the gap between \"rich AI\" and \"poor AI\" by lowering barriers to RLM development and experimentation.",
            "score": 7,
            "issue_id": 1797,
            "pub_date": "2025-01-20",
            "pub_date_card": {
                "ru": "20 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 20",
                "zh": "1æœˆ20æ—¥"
            },
            "hash": "f554416ad9af3344",
            "authors": [
                "Maciej Besta",
                "Julia Barth",
                "Eric Schreiber",
                "Ales Kubicek",
                "Afonso Catarino",
                "Robert Gerstenberger",
                "Piotr Nyczyk",
                "Patrick Iff",
                "Yueling Li",
                "Sam Houliston",
                "Tomasz Sternal",
                "Marcin Copik",
                "Grzegorz KwaÅ›niewski",
                "JÃ¼rgen MÃ¼ller",
                "Åukasz Flis",
                "Hannes Eberhard",
                "Hubert Niewiadomski",
                "Torsten Hoefler"
            ],
            "affiliations": [
                "BASF SE",
                "Cledar",
                "Cyfronet AGH",
                "ETH Zurich"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.11223.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#math",
                    "#training",
                    "#survey",
                    "#reasoning",
                    "#architecture"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ”ĞµĞ¼Ğ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°: Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (RLM), Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ÑƒÑ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ ÑÑ…ĞµĞ¼Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ x1 - Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½ÑƒÑ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾Ñ‚Ğ¸Ğ¿Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ RLM. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¾ Ğ½Ğ° Ğ´ĞµĞ¼Ğ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ˜Ğ˜ Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ±Ğ°Ñ€ÑŒĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ RLM."
                },
                "en": {
                    "title": "Democratizing Advanced Reasoning in AI",
                    "desc": "This paper introduces a modular framework for Reasoning Language Models (RLMs), which enhance traditional Large Language Models (LLMs) with advanced reasoning capabilities. The authors address the challenges of high costs and complex architectures by organizing RLM components into a comprehensive blueprint that includes various reasoning structures and strategies. They provide mathematical formulations and algorithmic specifications to facilitate easier implementation of RLMs. Additionally, the paper presents x1, a tool for rapid prototyping, and discusses how RLMs can be integrated into the larger LLM ecosystem to promote accessibility and innovation in AI development."
                },
                "zh": {
                    "title": "ç®€åŒ–æ¨ç†è¯­è¨€æ¨¡å‹ï¼Œä¿ƒè¿›AIåˆ›æ–°",
                    "desc": "æ¨ç†è¯­è¨€æ¨¡å‹ï¼ˆRLMsï¼‰é€šè¿‡ç»“åˆå¼ºåŒ–å­¦ä¹ ã€æœç´¢å¯å‘å¼å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œé‡æ–°å®šä¹‰äº†äººå·¥æ™ºèƒ½çš„è§£å†³é—®é¢˜èƒ½åŠ›ã€‚å°½ç®¡å®ƒä»¬å…·æœ‰å¼ºå¤§çš„æ¨ç†æœºåˆ¶ï¼Œä½†é«˜æˆæœ¬å’Œå¤æ‚æ¶æ„ä½¿å¾—å…¶å¯è®¿é—®æ€§å’Œå¯æ‰©å±•æ€§é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ¨¡å—åŒ–æ¡†æ¶ï¼Œç»„ç»‡RLMç»„ä»¶ï¼Œå¹¶æä¾›è¯¦ç»†çš„æ•°å­¦å…¬å¼å’Œç®—æ³•è§„èŒƒï¼Œä»¥ç®€åŒ–RLMçš„å®ç°ã€‚æˆ‘ä»¬çš„å·¥ä½œæ—¨åœ¨é™ä½RLMå¼€å‘å’Œå®éªŒçš„é—¨æ§›ï¼Œä¿ƒè¿›åˆ›æ–°ï¼Œç¼©å°â€œå¯Œæœ‰AIâ€å’Œâ€œè´«ç©·AIâ€ä¹‹é—´çš„å·®è·ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.08331",
            "title": "Go-with-the-Flow: Motion-Controllable Video Diffusion Models Using Real-Time Warped Noise",
            "url": "https://huggingface.co/papers/2501.08331",
            "abstract": "Generative modeling aims to transform random noise into structured outputs. In this work, we enhance video diffusion models by allowing motion control via structured latent noise sampling. This is achieved by just a change in data: we pre-process training videos to yield structured noise. Consequently, our method is agnostic to diffusion model design, requiring no changes to model architectures or training pipelines. Specifically, we propose a novel noise warping algorithm, fast enough to run in real time, that replaces random temporal Gaussianity with correlated warped noise derived from optical flow fields, while preserving the spatial Gaussianity. The efficiency of our algorithm enables us to fine-tune modern video diffusion base models using warped noise with minimal overhead, and provide a one-stop solution for a wide range of user-friendly motion control: local object motion control, global camera movement control, and motion transfer. The harmonization between temporal coherence and spatial Gaussianity in our warped noise leads to effective motion control while maintaining per-frame pixel quality. Extensive experiments and user studies demonstrate the advantages of our method, making it a robust and scalable approach for controlling motion in video diffusion models. Video results are available on our webpage: https://vgenai-netflix-eyeline-research.github.io/Go-with-the-Flow. Source code and model checkpoints are available on GitHub: https://github.com/VGenAI-Netflix-Eyeline-Research/Go-with-the-Flow.",
            "score": 4,
            "issue_id": 1798,
            "pub_date": "2025-01-14",
            "pub_date_card": {
                "ru": "14 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 14",
                "zh": "1æœˆ14æ—¥"
            },
            "hash": "c48e19ef08e8d758",
            "authors": [
                "Ryan Burgert",
                "Yuancheng Xu",
                "Wenqi Xian",
                "Oliver Pilarski",
                "Pascal Clausen",
                "Mingming He",
                "Li Ma",
                "Yitong Deng",
                "Lingxiao Li",
                "Mohsen Mousavi",
                "Michael Ryoo",
                "Paul Debevec",
                "Ning Yu"
            ],
            "affiliations": [
                "Eyeline Studios",
                "Netflix",
                "Stanford University",
                "Stony Brook University",
                "University of Maryland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.08331.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#video",
                    "#data"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞšĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ÑˆÑƒĞ¼",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ ÑˆÑƒĞ¼Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ñ ÑˆÑƒĞ¼Ğ° Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ğ¾ÑÑ‚ÑŒ, Ğ½Ğ¾ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¾Ğ¼ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Transforming Noise into Motion: Enhanced Control in Video Diffusion Models",
                    "desc": "This paper presents an improvement in video diffusion models by introducing a method for controlling motion through structured latent noise sampling. The authors propose a novel noise warping algorithm that modifies the training data to replace random noise with correlated noise based on optical flow, enhancing temporal coherence while maintaining spatial quality. This approach allows for real-time processing and fine-tuning of existing video diffusion models without altering their architecture or training methods. The results show that this method effectively enables various motion control tasks, making it a versatile tool for video generation applications."
                },
                "zh": {
                    "title": "è¿åŠ¨æ§åˆ¶çš„æ–°æ–¹æ³•ï¼šæ‰­æ›²å™ªå£°çš„åŠ›é‡",
                    "desc": "ç”Ÿæˆå»ºæ¨¡çš„ç›®æ ‡æ˜¯å°†éšæœºå™ªå£°è½¬åŒ–ä¸ºç»“æ„åŒ–è¾“å‡ºã€‚æœ¬æ–‡é€šè¿‡ç»“æ„åŒ–æ½œåœ¨å™ªå£°é‡‡æ ·å¢å¼ºè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œå®ç°äº†è¿åŠ¨æ§åˆ¶ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„å™ªå£°æ‰­æ›²ç®—æ³•ï¼Œèƒ½å¤Ÿå®æ—¶è¿è¡Œï¼Œå¹¶ç”¨å…‰æµåœºå¯¼å‡ºçš„ç›¸å…³æ‰­æ›²å™ªå£°æ›¿ä»£éšæœºæ—¶é—´é«˜æ–¯å™ªå£°ï¼ŒåŒæ—¶ä¿æŒç©ºé—´é«˜æ–¯æ€§ã€‚æˆ‘ä»¬çš„ç®—æ³•é«˜æ•ˆæ€§ä½¿å¾—åœ¨ç°ä»£è§†é¢‘æ‰©æ•£åŸºç¡€æ¨¡å‹ä¸­ä½¿ç”¨æ‰­æ›²å™ªå£°è¿›è¡Œå¾®è°ƒæˆä¸ºå¯èƒ½ï¼Œæä¾›äº†ç”¨æˆ·å‹å¥½çš„è¿åŠ¨æ§åˆ¶è§£å†³æ–¹æ¡ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.12375",
            "title": "Video Depth Anything: Consistent Depth Estimation for Super-Long Videos",
            "url": "https://huggingface.co/papers/2501.12375",
            "abstract": "Depth Anything has achieved remarkable success in monocular depth estimation with strong generalization ability. However, it suffers from temporal inconsistency in videos, hindering its practical applications. Various methods have been proposed to alleviate this issue by leveraging video generation models or introducing priors from optical flow and camera poses. Nonetheless, these methods are only applicable to short videos (< 10 seconds) and require a trade-off between quality and computational efficiency. We propose Video Depth Anything for high-quality, consistent depth estimation in super-long videos (over several minutes) without sacrificing efficiency. We base our model on Depth Anything V2 and replace its head with an efficient spatial-temporal head. We design a straightforward yet effective temporal consistency loss by constraining the temporal depth gradient, eliminating the need for additional geometric priors. The model is trained on a joint dataset of video depth and unlabeled images, similar to Depth Anything V2. Moreover, a novel key-frame-based strategy is developed for long video inference. Experiments show that our model can be applied to arbitrarily long videos without compromising quality, consistency, or generalization ability. Comprehensive evaluations on multiple video benchmarks demonstrate that our approach sets a new state-of-the-art in zero-shot video depth estimation. We offer models of different scales to support a range of scenarios, with our smallest model capable of real-time performance at 30 FPS.",
            "score": 4,
            "issue_id": 1798,
            "pub_date": "2025-01-21",
            "pub_date_card": {
                "ru": "21 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 21",
                "zh": "1æœˆ21æ—¥"
            },
            "hash": "00640fb6adcf39e3",
            "authors": [
                "Sili Chen",
                "Hengkai Guo",
                "Shengnan Zhu",
                "Feihu Zhang",
                "Zilong Huang",
                "Jiashi Feng",
                "Bingyi Kang"
            ],
            "affiliations": [
                "ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.12375.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#optimization",
                    "#small_models",
                    "#video",
                    "#cv",
                    "#training"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ¡Ğ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ´Ğ»Ñ ÑĞ²ĞµÑ€Ñ…Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Video Depth Anything Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ² ÑĞ²ĞµÑ€Ñ…Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Depth Anything V2 Ñ Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğ¹ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±ĞµĞ· ÑƒÑ‰ĞµÑ€Ğ±Ğ° Ğ´Ğ»Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ zero-shot Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…."
                },
                "en": {
                    "title": "Achieving Consistent Depth Estimation in Long Videos",
                    "desc": "This paper introduces Video Depth Anything, a model designed for accurate depth estimation in long videos, overcoming the limitations of previous methods that struggled with temporal consistency. The model builds on Depth Anything V2, enhancing it with a spatial-temporal head and a novel temporal consistency loss that focuses on the depth gradient over time. By training on a combined dataset of video depth and unlabeled images, the model achieves high-quality depth estimation without the need for complex geometric priors. The results demonstrate that Video Depth Anything can handle videos of any length while maintaining efficiency and setting new benchmarks in zero-shot video depth estimation."
                },
                "zh": {
                    "title": "è¶…é•¿è§†é¢‘æ·±åº¦ä¼°è®¡çš„æ–°çªç ´",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºVideo Depth Anythingçš„æ–°æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³å•ç›®æ·±åº¦ä¼°è®¡åœ¨è§†é¢‘ä¸­çš„æ—¶é—´ä¸€è‡´æ€§é—®é¢˜ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨è¶…é•¿è§†é¢‘ï¼ˆè¶…è¿‡å‡ åˆ†é’Ÿï¼‰ä¸­å®ç°é«˜è´¨é‡å’Œä¸€è‡´æ€§çš„æ·±åº¦ä¼°è®¡ï¼Œè€Œä¸ç‰ºç‰²è®¡ç®—æ•ˆç‡ã€‚æˆ‘ä»¬é€šè¿‡è®¾è®¡ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„æ—¶é—´ä¸€è‡´æ€§æŸå¤±ï¼Œæ¥çº¦æŸæ—¶é—´æ·±åº¦æ¢¯åº¦ï¼Œä»è€Œé¿å…äº†é¢å¤–å‡ ä½•å…ˆéªŒçš„éœ€æ±‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¤šä¸ªè§†é¢‘åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè®¾å®šäº†é›¶-shotè§†é¢‘æ·±åº¦ä¼°è®¡çš„æ–°çŠ¶æ€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.12390",
            "title": "GPS as a Control Signal for Image Generation",
            "url": "https://huggingface.co/papers/2501.12390",
            "abstract": "We show that the GPS tags contained in photo metadata provide a useful control signal for image generation. We train GPS-to-image models and use them for tasks that require a fine-grained understanding of how images vary within a city. In particular, we train a diffusion model to generate images conditioned on both GPS and text. The learned model generates images that capture the distinctive appearance of different neighborhoods, parks, and landmarks. We also extract 3D models from 2D GPS-to-image models through score distillation sampling, using GPS conditioning to constrain the appearance of the reconstruction from each viewpoint. Our evaluations suggest that our GPS-conditioned models successfully learn to generate images that vary based on location, and that GPS conditioning improves estimated 3D structure.",
            "score": 4,
            "issue_id": 1797,
            "pub_date": "2025-01-21",
            "pub_date_card": {
                "ru": "21 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 21",
                "zh": "1æœˆ21æ—¥"
            },
            "hash": "11d289e8a895bedd",
            "authors": [
                "Chao Feng",
                "Ziyang Chen",
                "Aleksander Holynski",
                "Alexei A. Efros",
                "Andrew Owens"
            ],
            "affiliations": [
                "UC Berkeley",
                "University of Michigan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.12390.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#cv",
                    "#multimodal",
                    "#dataset",
                    "#diffusion",
                    "#3d"
                ],
                "emoji": "ğŸ—ºï¸",
                "ru": {
                    "title": "GPS-Ğ¼ĞµÑ‚ĞºĞ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ñ‹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, ĞºĞ°Ğº GPS-Ğ¼ĞµÑ‚ĞºĞ¸ Ğ² Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ„Ğ¾Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ğ¹ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ GPS-ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶Ğ°Ñ‚ÑŒ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ¹Ğ¾Ğ½Ğ¾Ğ² Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ‡Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ÑÑ‚ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ· 2D GPS-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºĞ¸ score distillation sampling. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ GPS-Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ²Ğ°Ñ€ÑŒĞ¸Ñ€ÑƒÑÑ‰Ğ¸ĞµÑÑ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ¼ĞµÑÑ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ, Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ğ¾Ñ†ĞµĞ½ĞºÑƒ 3D-ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹."
                },
                "en": {
                    "title": "Harnessing GPS Data for Location-Aware Image Generation",
                    "desc": "This paper explores the use of GPS data embedded in photo metadata as a control signal for generating images. The authors develop GPS-to-image models, particularly a diffusion model, that can create images based on both GPS coordinates and textual descriptions. The model effectively captures the unique characteristics of various urban environments, such as neighborhoods and landmarks. Additionally, they demonstrate the ability to extract 3D models from these images, enhancing the accuracy of 3D reconstructions by using GPS information to guide the process."
                },
                "zh": {
                    "title": "åˆ©ç”¨GPSæ ‡ç­¾ç”ŸæˆåŸå¸‚å›¾åƒçš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡å±•ç¤ºäº†ç…§ç‰‡å…ƒæ•°æ®ä¸­çš„GPSæ ‡ç­¾å¯ä»¥ä½œä¸ºå›¾åƒç”Ÿæˆçš„æœ‰ç”¨æ§åˆ¶ä¿¡å·ã€‚æˆ‘ä»¬è®­ç»ƒäº†GPSåˆ°å›¾åƒçš„æ¨¡å‹ï¼Œå¹¶å°†å…¶åº”ç”¨äºéœ€è¦ç»†è‡´ç†è§£åŸå¸‚ä¸­å›¾åƒå˜åŒ–çš„ä»»åŠ¡ã€‚ç‰¹åˆ«åœ°ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªæ‰©æ•£æ¨¡å‹ï¼Œç”ŸæˆåŒæ—¶ä¾èµ–äºGPSå’Œæ–‡æœ¬çš„å›¾åƒã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„GPSæ¡ä»¶æ¨¡å‹æˆåŠŸå­¦ä¹ äº†åŸºäºä½ç½®ç”Ÿæˆå˜åŒ–å›¾åƒï¼Œå¹¶ä¸”GPSæ¡ä»¶æ”¹å–„äº†ä¼°è®¡çš„3Dç»“æ„ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.10893",
            "title": "Learn-by-interact: A Data-Centric Framework for Self-Adaptive Agents in Realistic Environments",
            "url": "https://huggingface.co/papers/2501.10893",
            "abstract": "Autonomous agents powered by large language models (LLMs) have the potential to enhance human capabilities, assisting with digital tasks from sending emails to performing data analysis. The abilities of existing LLMs at such tasks are often hindered by the lack of high-quality agent data from the corresponding environments they interact with. We propose Learn-by-interact, a data-centric framework to adapt LLM agents to any given environments without human annotations. Learn-by-interact synthesizes trajectories of agent-environment interactions based on documentations, and constructs instructions by summarizing or abstracting the interaction histories, a process called backward construction. We assess the quality of our synthetic data by using them in both training-based scenarios and training-free in-context learning (ICL), where we craft innovative retrieval approaches optimized for agents. Extensive experiments on SWE-bench, WebArena, OSWorld and Spider2-V spanning across realistic coding, web, and desktop environments show the effectiveness of Learn-by-interact in various downstream agentic tasks -- baseline results are improved by up to 12.2\\% for ICL with Claude-3.5 and 19.5\\% for training with Codestral-22B. We further demonstrate the critical role of backward construction, which provides up to 14.0\\% improvement for training. Our ablation studies demonstrate the efficiency provided by our synthesized data in ICL and the superiority of our retrieval pipeline over alternative approaches like conventional retrieval-augmented generation (RAG). We expect that Learn-by-interact will serve as a foundation for agent data synthesis as LLMs are increasingly deployed at real-world environments.",
            "score": 3,
            "issue_id": 1798,
            "pub_date": "2025-01-18",
            "pub_date_card": {
                "ru": "18 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 18",
                "zh": "1æœˆ18æ—¥"
            },
            "hash": "b6ab4c9ac3809941",
            "authors": [
                "Hongjin Su",
                "Ruoxi Sun",
                "Jinsung Yoon",
                "Pengcheng Yin",
                "Tao Yu",
                "Sercan Ã–. ArÄ±k"
            ],
            "affiliations": [
                "Google",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.10893.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#agents",
                    "#synthetic",
                    "#training",
                    "#data",
                    "#rag",
                    "#dataset"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Learn-by-interact - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ ÑÑ€ĞµĞ´Ğ°Ğ¼ Ğ±ĞµĞ· Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° ÑĞ¾ ÑÑ€ĞµĞ´Ğ¾Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´Ğ¾ 19.5% Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ€Ğ¾Ğ»ÑŒ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Empowering LLM Agents through Synthetic Interaction Data",
                    "desc": "This paper introduces Learn-by-interact, a framework designed to enhance the performance of large language model (LLM) agents in various environments without needing human-generated data. The framework generates synthetic data by simulating interactions between agents and their environments, using documentation to guide the process. A key innovation is the backward construction method, which summarizes interaction histories to create effective instructions for the agents. Experimental results show significant improvements in agent performance across multiple tasks, highlighting the framework's potential for real-world applications."
                },
                "zh": {
                    "title": "é€šè¿‡äº¤äº’å­¦ä¹ ï¼Œæå‡æ™ºèƒ½ä»£ç†èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºLearn-by-interactçš„æ•°æ®ä¸­å¿ƒæ¡†æ¶ï¼Œæ—¨åœ¨ä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å¤Ÿé€‚åº”ä¸åŒçš„ç¯å¢ƒï¼Œè€Œæ— éœ€äººå·¥æ ‡æ³¨ã€‚è¯¥æ¡†æ¶é€šè¿‡æ–‡æ¡£ç”Ÿæˆä»£ç†ä¸ç¯å¢ƒäº¤äº’çš„è½¨è¿¹ï¼Œå¹¶é€šè¿‡æ€»ç»“æˆ–æŠ½è±¡äº¤äº’å†å²æ¥æ„å»ºæŒ‡ä»¤ï¼Œè¿™ä¸€è¿‡ç¨‹ç§°ä¸ºåå‘æ„å»ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLearn-by-interactåœ¨å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨æ— ç›‘ç£å­¦ä¹ å’Œè®­ç»ƒåœºæ™¯ä¸­ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†åå‘æ„å»ºåœ¨è®­ç»ƒä¸­çš„é‡è¦æ€§ï¼Œè¿›ä¸€æ­¥éªŒè¯äº†åˆæˆæ•°æ®çš„æœ‰æ•ˆæ€§å’Œæ£€ç´¢ç®¡é“çš„ä¼˜è¶Šæ€§ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-01-21.html",
    "link_next": "2025-01-23.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "21.01",
        "en": "01/21",
        "zh": "1æœˆ21æ—¥"
    },
    "short_date_next": {
        "ru": "23.01",
        "en": "01/23",
        "zh": "1æœˆ23æ—¥"
    },
    "categories": {
        "#dataset": 4,
        "#data": 3,
        "#benchmark": 3,
        "#agents": 4,
        "#cv": 2,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 0,
        "#3d": 2,
        "#audio": 1,
        "#video": 4,
        "#multimodal": 4,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 1,
        "#training": 7,
        "#robotics": 0,
        "#agi": 1,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 5,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 7,
        "#survey": 1,
        "#diffusion": 4,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 3,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 1,
        "#science": 1,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§åä¸ºGameFactoryçš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ç”Ÿæˆæ¸¸æˆå¼•æ“æ¥é©æ–°æ¸¸æˆå¼€å‘ã€‚å®ƒä½¿ç”¨é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿåˆ›å»ºå…¨æ–°ä¸”å¤šæ ·åŒ–çš„æ¸¸æˆã€‚ä¸ºäº†è§£å†³ç°æœ‰æ–¹æ³•åœ¨åœºæ™¯ç”Ÿæˆä¸Šçš„å±€é™ï¼Œä½œè€…æå‡ºäº†ä¸€ç§å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ã€‚ä»–ä»¬è¿˜å‘å¸ƒäº†ä¸€ä¸ªåŸºäºMinecraftçš„é«˜è´¨é‡è§†é¢‘æ•°æ®é›†ï¼Œå¹¶å±•ç¤ºäº†æ¡†æ¶èƒ½å¤Ÿç”Ÿæˆå¼€æ”¾åŸŸã€å¤šæ ·åŒ–å’Œå¯æ§çš„æ¸¸æˆè§†é¢‘ã€‚",
        "title": "GameFactory: Creating New Games with Generative Interactive Videos",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§åä¸ºGameFactoryçš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ç”Ÿæˆæ¸¸æˆå¼•æ“æ¥é©æ–°æ¸¸æˆå¼€å‘ã€‚å®ƒä½¿ç”¨é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿåˆ›å»ºå…¨æ–°ä¸”å¤šæ ·åŒ–çš„æ¸¸æˆã€‚ä¸ºäº†è§£å†³ç°æœ‰æ–¹æ³•åœ¨åœºæ™¯ç”Ÿæˆä¸Šçš„å±€é™ï¼Œä½œè€…æå‡ºäº†ä¸€ç§å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ã€‚ä»–ä»¬è¿˜å‘å¸ƒäº†ä¸€ä¸ªåŸºäºMinecraftçš„é«˜è´¨é‡è§†é¢‘æ•°æ®é›†ï¼Œå¹¶å±•ç¤ºäº†æ¡†æ¶èƒ½å¤Ÿç”Ÿæˆå¼€æ”¾åŸŸã€å¤šæ ·åŒ–å’Œå¯æ§çš„æ¸¸æˆè§†é¢‘ã€‚\n\nzhÃ¨ piÄn wÃ©n zhÄng jiÃ¨ shÃ o le yÄ« zhÇ’ng mÃ­ng wÃ¨i GameFactory de kuÃ ng jiÃ , zhÇ zÃ i tÅng guÃ² shÄ“ng chÃ©ng yÃ²u xÃ­ yÇn qÃ­ng lÃ¡i gÃ© xÄ«n yÃ²u xÃ­ kÄi fÄ. tÄ shÇ yÃ²ng yÃ¹ xÃ¹n liÃ n de shÃ¬ pÃ­n kuÃ² sÃ n mÃ³ xÃ­ng, nÃ©ng gÃ²u chuÃ ng jiÃ n quÃ¡n xÄ«n qiÄ› duÅ yÃ ng huÃ  de yÃ²u xÃ­. wÃ¨i le jiÄ› juÃ© xiÃ n yÇ’u fÄng fÇ zÃ i chÇng jÄ«ng shÄ“ng chÃ©ng shÃ ng de jÃº xiÃ n, zuÃ² zhÄ› tÃ­ chÅ« le yÄ« zhÇ’ng duÅ jiÄ“ duÃ n xÃ¹n liÃ n cÃ¨ lÃ¼Ã¨. tÄ men hÃ¡i fÄ bÃ¹ le yÄ« gÃ¨ jÄ« yÃº Minecraft de gÄo zhÃ¬ liÃ ng shÃ¬ pÃ­n shÃ¹ jÃ¹ jÃ­, bÃ¬ng zhÃ n shÃ¬ le kuÃ ng jiÃ  nÃ©ng gÃ²u shÄ“ng chÃ©ng kÄi fÃ ng yÃ¹, duÅ yÃ ng huÃ  hÃ© kÄ› kÃ²ng de yÃ²u xÃ­ shÃ¬ pÃ­n.",
        "vocab": "[{'word': 'æ¡†æ¶', 'pinyin': 'kuÃ ngjiÃ ', 'trans': 'framework'},\n{'word': 'æ—¨åœ¨', 'pinyin': 'zhÇzÃ i', 'trans': 'aim to'},\n{'word': 'é©æ–°', 'pinyin': 'gÃ©xÄ«n', 'trans': 'innovate'},\n{'word': 'å¼•æ“', 'pinyin': 'yÇnqÃ­ng', 'trans': 'engine'},\n{'word': 'é¢„è®­ç»ƒ', 'pinyin': 'yÃ¹ xÃ¹nliÃ n', 'trans': 'pre-trained'},\n{'word': 'æ‰©æ•£', 'pinyin': 'kuÃ²sÃ n', 'trans': 'diffusion'},\n{'word': 'å¤šæ ·åŒ–', 'pinyin': 'duÅyÃ nghuÃ ', 'trans': 'diversified'},\n{'word': 'å±€é™', 'pinyin': 'jÃºxiÃ n', 'trans': 'limitation'},\n{'word': 'æå‡º', 'pinyin': 'tÃ­chÅ«', 'trans': 'propose'},\n{'word': 'ç­–ç•¥', 'pinyin': 'cÃ¨lÃ¼Ã¨', 'trans': 'strategy'},\n{'word': 'åŸºäº', 'pinyin': 'jÄ«yÃº', 'trans': 'based on'},\n{'word': 'é«˜è´¨é‡', 'pinyin': 'gÄo zhÃ¬liÃ ng', 'trans': 'high quality'},\n{'word': 'æ•°æ®é›†', 'pinyin': 'shÃ¹jÃ¹ jÃ­', 'trans': 'dataset'},\n{'word': 'å±•ç¤º', 'pinyin': 'zhÇnshÃ¬', 'trans': 'demonstrate'},\n{'word': 'å¼€æ”¾åŸŸ', 'pinyin': 'kÄifÃ ng yÃ¹', 'trans': 'open domain'},\n{'word': 'å¯æ§', 'pinyin': 'kÄ›kÃ²ng', 'trans': 'controllable'}]",
        "trans": "This article introduces a framework called GameFactory, which aims to revolutionize game development by generating game engines. It utilizes pre-trained video diffusion models to create novel and diverse games. To address the limitations of existing methods in scene generation, the authors propose a multi-stage training strategy. They also release a high-quality video dataset based on Minecraft and demonstrate that the framework can generate open-domain, diverse, and controllable game videos.",
        "update_ts": "2025-01-21 09:10"
    }
}