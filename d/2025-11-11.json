{
    "date": {
        "ru": "11 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
        "en": "November 11",
        "zh": "11æœˆ11æ—¥"
    },
    "time_utc": "2025-11-11 09:00",
    "weekday": 1,
    "issue_id": 1,
    "home_page_url": "https://huggingface.co/papers?date=2025-11-11",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2511.03506",
            "title": "HaluMem: Evaluating Hallucinations in Memory Systems of Agents",
            "url": "https://huggingface.co/papers/2511.03506",
            "abstract": "HaluMem, a benchmark for evaluating memory hallucinations in AI systems, identifies and analyzes hallucinations across memory extraction, updating, and question answering stages using large-scale human-AI interaction datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Memory systems are key components that enable AI systems such as LLMs and AI agents to achieve long-term learning and sustained interaction. However, during memory storage and retrieval, these systems frequently exhibit memory hallucinations, including fabrication, errors, conflicts, and omissions. Existing evaluations of memory hallucinations are primarily end-to-end question answering, which makes it difficult to localize the operational stage within the memory system where hallucinations arise. To address this, we introduce the Hallucination in Memory Benchmark (HaluMem), the first operation level hallucination evaluation benchmark tailored to memory systems. HaluMem defines three evaluation tasks (memory extraction, memory updating, and memory question answering) to comprehensively reveal hallucination behaviors across different operational stages of interaction. To support evaluation, we construct user-centric, multi-turn human-AI interaction datasets, HaluMem-Medium and HaluMem-Long. Both include about 15k memory points and 3.5k multi-type questions. The average dialogue length per user reaches 1.5k and 2.6k turns, with context lengths exceeding 1M tokens, enabling evaluation of hallucinations across different context scales and task complexities. Empirical studies based on HaluMem show that existing memory systems tend to generate and accumulate hallucinations during the extraction and updating stages, which subsequently propagate errors to the question answering stage. Future research should focus on developing interpretable and constrained memory operation mechanisms that systematically suppress hallucinations and improve memory reliability.",
            "score": 93,
            "issue_id": 1,
            "pub_date": "2025-11-05",
            "pub_date_card": {
                "ru": "5 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 5",
                "zh": "11æœˆ5æ—¥"
            },
            "hash": "05b93ffabaaec001",
            "authors": [
                "Ding Chen",
                "Simin Niu",
                "Kehang Li",
                "Peng Liu",
                "Xiangping Zheng",
                "Bo Tang",
                "Xinchi Li",
                "Feiyu Xiong",
                "Zhiyu Li"
            ],
            "affiliations": [
                "China Telecom Research Institute",
                "Harbin Engineering University",
                "MemTensor (Shanghai) Technology"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.03506.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#hallucinations",
                    "#agents",
                    "#long_context",
                    "#dataset"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ’Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ AI Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ HaluMem â€” Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ AI, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‚ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ, Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ²Ğ° ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ AI ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ñ… Ğ±Ğ¾Ğ»ĞµĞµ 15 Ñ‚Ñ‹ÑÑÑ‡ Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼, Ğ¿Ñ€ĞµĞ²Ñ‹ÑˆĞ°ÑÑ‰Ğ¸Ğ¼ 1 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¸ Ğ½Ğ°ĞºĞ°Ğ¿Ğ»Ğ¸Ğ²Ğ°ÑÑ‚ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¼ Ğ½Ğ° Ñ€Ğ°Ğ½Ğ½Ğ¸Ñ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… (Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ·Ğ°Ñ‚ĞµĞ¼ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑÑ‚ÑÑ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ."
                },
                "en": {
                    "title": "HaluMem: Unveiling Memory Hallucinations in AI Systems",
                    "desc": "HaluMem is a new benchmark designed to evaluate memory hallucinations in AI systems, particularly focusing on large language models (LLMs) and AI agents. It identifies and analyzes hallucinations during three key stages: memory extraction, memory updating, and question answering. By using extensive human-AI interaction datasets, HaluMem allows researchers to pinpoint where in the memory process these hallucinations occur, rather than just assessing the final output. The findings indicate that hallucinations often arise during the initial stages and can lead to errors in the final responses, highlighting the need for improved memory management techniques."
                },
                "zh": {
                    "title": "HaluMemï¼šæ­ç¤ºAIè®°å¿†å¹»è§‰çš„å…¨æ–°åŸºå‡†",
                    "desc": "HaluMemæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°äººå·¥æ™ºèƒ½ç³»ç»Ÿä¸­è®°å¿†å¹»è§‰çš„åŸºå‡†ã€‚å®ƒé€šè¿‡åˆ†æè®°å¿†æå–ã€æ›´æ–°å’Œé—®ç­”é˜¶æ®µçš„å¹»è§‰ï¼Œåˆ©ç”¨å¤§è§„æ¨¡çš„äººæœºäº¤äº’æ•°æ®é›†æ¥è¯†åˆ«å’Œåˆ†æè¿™äº›é—®é¢˜ã€‚ç°æœ‰çš„è¯„ä¼°æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨ç«¯åˆ°ç«¯çš„é—®ç­”ä¸Šï¼Œéš¾ä»¥å®šä½å¹»è§‰å‘ç”Ÿçš„å…·ä½“æ“ä½œé˜¶æ®µã€‚HaluMemå®šä¹‰äº†ä¸‰ä¸ªè¯„ä¼°ä»»åŠ¡ï¼Œæ—¨åœ¨å…¨é¢æ­ç¤ºä¸åŒæ“ä½œé˜¶æ®µçš„å¹»è§‰è¡Œä¸ºï¼Œå¹¶æ„å»ºäº†ç”¨æˆ·ä¸­å¿ƒçš„å¤šè½®äººæœºäº¤äº’æ•°æ®é›†ä»¥æ”¯æŒè¯„ä¼°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.07327",
            "title": "IterResearch: Rethinking Long-Horizon Agents via Markovian State\n  Reconstruction",
            "url": "https://huggingface.co/papers/2511.07327",
            "abstract": "IterResearch, an iterative deep-research paradigm, improves long-horizon reasoning by reformulating it as a Markov Decision Process with strategic workspace reconstruction and Efficiency-Aware Policy Optimization, achieving better performance and interaction scaling compared to existing agents.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in deep-research agents have shown promise for autonomous knowledge construction through dynamic reasoning over external sources. However, existing approaches rely on a mono-contextual paradigm that accumulates all information in a single, expanding context window, leading to context suffocation and noise contamination that limit their effectiveness on long-horizon tasks. We introduce IterResearch, a novel iterative deep-research paradigm that reformulates long-horizon research as a Markov Decision Process with strategic workspace reconstruction. By maintaining an evolving report as memory and periodically synthesizing insights, our approach preserves consistent reasoning capacity across arbitrary exploration depths. We further develop Efficiency-Aware Policy Optimization (EAPO), a reinforcement learning framework that incentivizes efficient exploration through geometric reward discounting and enables stable distributed training via adaptive downsampling. Extensive experiments demonstrate that IterResearch achieves substantial improvements over existing open-source agents with average +14.5pp across six benchmarks and narrows the gap with frontier proprietary systems. Remarkably, our paradigm exhibits unprecedented interaction scaling, extending to 2048 interactions with dramatic performance gains (from 3.5\\% to 42.5\\%), and serves as an effective prompting strategy, improving frontier models by up to 19.2pp over ReAct on long-horizon tasks. These findings position IterResearch as a versatile solution for long-horizon reasoning, effective both as a trained agent and as a prompting paradigm for frontier models.",
            "score": 74,
            "issue_id": 1,
            "pub_date": "2025-11-10",
            "pub_date_card": {
                "ru": "10 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 10",
                "zh": "11æœˆ10æ—¥"
            },
            "hash": "635698d160c24d3f",
            "authors": [
                "Guoxin Chen",
                "Zile Qiao",
                "Xuanzhong Chen",
                "Donglei Yu",
                "Haotian Xu",
                "Wayne Xin Zhao",
                "Ruihua Song",
                "Wenbiao Yin",
                "Huifeng Yin",
                "Liwen Zhang",
                "Kuan Li",
                "Minpeng Liao",
                "Yong Jiang",
                "Pengjun Xie",
                "Fei Huang",
                "Jingren Zhou"
            ],
            "affiliations": [
                "Gaoling School of Artificial Intelligence, Renmin University of China",
                "OpenRLHF",
                "Tongyi Lab, Alibaba Group"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.07327.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#optimization",
                    "#agents",
                    "#long_context",
                    "#rl",
                    "#reasoning"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ˜Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿ĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ĞœĞ°Ñ€ĞºĞ¾Ğ²ÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ",
                    "desc": "IterResearch Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ´Ğ»Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğº ĞœĞ°Ñ€ĞºĞ¾Ğ²ÑĞºĞ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ĞµĞ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‡ĞµĞ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Efficiency-Aware Policy Optimization (EAPO), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ´Ğ¸ÑĞºĞ¾Ğ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Â«ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ½Ğ°ÑÑ‹Ñ‰ĞµĞ½Ğ¸ÑÂ» Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ğ° ĞºĞ°Ğº Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ IterResearch Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° 14.5 Ğ¿ÑƒĞ½ĞºÑ‚Ğ° Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±ĞµÑĞ¿Ñ€ĞµÑ†ĞµĞ´ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ¾ 2048 Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ 3.5% Ğ´Ğ¾ 42.5%."
                },
                "en": {
                    "title": "IterResearch: Revolutionizing Long-Horizon Reasoning in AI",
                    "desc": "The paper introduces IterResearch, a new approach to enhance long-horizon reasoning in AI by treating it as a Markov Decision Process. It addresses the limitations of existing methods that suffer from context suffocation by using strategic workspace reconstruction and maintaining an evolving memory report. The authors also present Efficiency-Aware Policy Optimization (EAPO), which encourages efficient exploration and supports stable training through adaptive downsampling. Experimental results show that IterResearch significantly outperforms current agents, achieving better interaction scaling and improved performance on long-horizon tasks."
                },
                "zh": {
                    "title": "è¿­ä»£æ·±åº¦ç ”ç©¶ï¼šé•¿æ—¶é—´æ¨ç†çš„æ–°èŒƒå¼",
                    "desc": "IterResearchæ˜¯ä¸€ç§æ–°çš„è¿­ä»£æ·±åº¦ç ”ç©¶èŒƒå¼ï¼Œé€šè¿‡å°†é•¿æ—¶é—´æ¨ç†é‡æ–°æ„å»ºä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œæ¥æé«˜æ¨ç†èƒ½åŠ›ã€‚å®ƒé‡‡ç”¨æˆ˜ç•¥å·¥ä½œç©ºé—´é‡å»ºçš„æ–¹æ³•ï¼Œä¿æŒä¸€ä¸ªä¸æ–­æ¼”å˜çš„æŠ¥å‘Šä½œä¸ºè®°å¿†ï¼Œå¹¶å®šæœŸåˆæˆè§è§£ï¼Œä»è€Œåœ¨ä»»æ„æ¢ç´¢æ·±åº¦ä¸Šä¿æŒä¸€è‡´çš„æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæ•ˆç‡æ„ŸçŸ¥ç­–ç•¥ä¼˜åŒ–ï¼ˆEAPOï¼‰æ¡†æ¶é€šè¿‡å‡ ä½•å¥–åŠ±æŠ˜æ‰£æ¿€åŠ±é«˜æ•ˆæ¢ç´¢ï¼Œæ”¯æŒç¨³å®šçš„åˆ†å¸ƒå¼è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIterResearchåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰çš„å¼€æºä»£ç†ï¼Œå±•ç°å‡ºå‰æ‰€æœªæœ‰çš„äº¤äº’æ‰©å±•èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.06307",
            "title": "DRIVE: Data Curation Best Practices for Reinforcement Learning with\n  Verifiable Reward in Competitive Code Generation",
            "url": "https://huggingface.co/papers/2511.06307",
            "abstract": "The study presents a two-stage reinforcement learning approach for competitive-programming code generation, achieving state-of-the-art performance using Group Relative Policy Optimization and a hard-focus curriculum.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent reasoning-first models (e.g., OpenAI o1, DeepSeek R1) have spurred a resurgence of interest in RLVR. Nevertheless, advances are dominated by mathematics (e.g., AIME), with competitive-programming code generation underexplored and data curation receiving less attention than RL algorithm design. We investigate how to construct RLVR datasets (i.e., RL prompts) and present practical training techniques that yield strong performance on competitive-programming code generation. Our pipeline begins with supervised fine-tuning (SFT) distilled from strong open-source models, augmented with general-purpose and reasoning-intensive data. RL then follows a two-stage process with executable, testcase-driven rewards: first, training on a large, uniformly distributed set of competitive-programming problems using Group Relative Policy Optimization (GRPO) with 8 rollouts per prompt and a relatively short response-generation window (e.g., 32k during SFT and 24k in this stage) to expand entropy and mitigate repetition and truncation; second, we perform Pre-GRPO: updating on a small, high-quality set of challenging problems with a large rollout budget (64 rollouts per prompt) under a hard-focus curriculum that continuously retains the most difficult instances throughout training. We implement our method on Qwen2.5-32B and evaluate on LeetCode and Codeforces weekly contests to avoid data leakage. The resulting model achieves state-of-the-art performance among models of similar scale and is comparable to leading systems such as DeepSeek v3.1 and Doubao-1.5-Thinking. We also examine scaling trends and observe strong RL scaling on an internal large-scale MoE model. Our study distills concise best practices for data curation, entropy expansion, and curriculum design in RLVR for competitive-programming code generation.",
            "score": 50,
            "issue_id": 1,
            "pub_date": "2025-11-09",
            "pub_date_card": {
                "ru": "9 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 9",
                "zh": "11æœˆ9æ—¥"
            },
            "hash": "4a5459d116b8bd5e",
            "authors": [
                "Speed Zhu",
                "Jianwei Cai",
                "Guang Chen",
                "Lulu Wu",
                "Saiyong Yang",
                "Wiggin Zhou"
            ],
            "affiliations": [
                "Hunyuan Team, Tencent"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.06307.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#plp",
                    "#data",
                    "#training",
                    "#leakage",
                    "#rl",
                    "#reasoning"
                ],
                "emoji": "ğŸ’»",
                "ru": {
                    "title": "Ğ”Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ² ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸. ĞĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Group Relative Policy Optimization (GRPO) Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ GRPO Ñ Ğ¶Ñ‘ÑÑ‚ĞºĞ¾Ğ¹ ÑƒÑ‡ĞµĞ±Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¾Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ°Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ… Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ¼ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… LeetCode Ğ¸ Codeforces, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ñ… Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸ Ñ‚Ğ¸Ğ¿Ğ° DeepSeek v3.1."
                },
                "en": {
                    "title": "Revolutionizing Code Generation with Two-Stage Reinforcement Learning",
                    "desc": "This paper introduces a two-stage reinforcement learning (RL) method specifically designed for generating code in competitive programming. It utilizes Group Relative Policy Optimization (GRPO) to enhance the model's performance by focusing on both a broad range of problems and a curated set of challenging tasks. The approach begins with supervised fine-tuning using strong existing models and then transitions to RL with a focus on executable rewards from test cases. The results demonstrate that this method achieves state-of-the-art performance, outperforming similar models and providing insights into effective data curation and training strategies in RL for code generation."
                },
                "zh": {
                    "title": "ç«äº‰ç¼–ç¨‹ä»£ç ç”Ÿæˆçš„å¼ºåŒ–å­¦ä¹ æ–°æ–¹æ³•",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºç«äº‰ç¼–ç¨‹ä»£ç ç”Ÿæˆï¼Œé‡‡ç”¨äº†ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–å’Œä¸¥æ ¼çš„è¯¾ç¨‹è®¾è®¡ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬æ¢è®¨äº†å¦‚ä½•æ„å»ºå¼ºåŒ–å­¦ä¹ è§†è§‰è¯†åˆ«æ•°æ®é›†ï¼Œå¹¶æå‡ºäº†æœ‰æ•ˆçš„è®­ç»ƒæŠ€æœ¯ï¼Œä»¥åœ¨ç«äº‰ç¼–ç¨‹ä»£ç ç”Ÿæˆä¸­è·å¾—å¼ºå¤§çš„è¡¨ç°ã€‚è¯¥æ–¹æ³•é¦–å…ˆé€šè¿‡ç›‘ç£å¾®è°ƒä»å¼ºå¤§çš„å¼€æºæ¨¡å‹ä¸­æå–çŸ¥è¯†ï¼Œç„¶ååœ¨å¤§è§„æ¨¡çš„ç«äº‰ç¼–ç¨‹é—®é¢˜ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæœ€ååœ¨é«˜è´¨é‡çš„æŒ‘æˆ˜æ€§é—®é¢˜ä¸Šè¿›è¡Œæ›´æ–°ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨LeetCodeå’ŒCodeforcesçš„å‘¨èµ›ä¸­è¯„ä¼°ï¼Œè¡¨ç°ä¼˜äºåŒè§„æ¨¡çš„å…¶ä»–æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.06309",
            "title": "The Station: An Open-World Environment for AI-Driven Discovery",
            "url": "https://huggingface.co/papers/2511.06309",
            "abstract": "AI agents in the STATION environment achieve state-of-the-art performance across various benchmarks through autonomous scientific discovery and emergent behavior.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce the STATION, an open-world multi-agent environment that models a miniature scientific ecosystem. Leveraging their extended context windows, agents in the Station can engage in long scientific journeys that include reading papers from peers, formulating hypotheses, submitting code, performing analyses, and publishing results. Importantly, there is no centralized system coordinating their activities - agents are free to choose their own actions and develop their own narratives within the Station. Experiments demonstrate that AI agents in the Station achieve new state-of-the-art performance on a wide range of benchmarks, spanning from mathematics to computational biology to machine learning, notably surpassing AlphaEvolve in circle packing. A rich tapestry of narratives emerges as agents pursue independent research, interact with peers, and build upon a cumulative history. From these emergent narratives, novel methods arise organically, such as a new density-adaptive algorithm for scRNA-seq batch integration. The Station marks a first step towards autonomous scientific discovery driven by emergent behavior in an open-world environment, representing a new paradigm that moves beyond rigid optimization.",
            "score": 35,
            "issue_id": 1,
            "pub_date": "2025-11-09",
            "pub_date_card": {
                "ru": "9 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 9",
                "zh": "11æœˆ9æ—¥"
            },
            "hash": "7be14a71aa38479f",
            "authors": [
                "Stephen Chung",
                "Wenyu Du"
            ],
            "affiliations": [
                "DualverseAI",
                "University of Cambridge",
                "University of Hong Kong"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.06309.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#open_source",
                    "#science"
                ],
                "emoji": "ğŸ§ª",
                "ru": {
                    "title": "Ğ­Ğ¼ĞµÑ€Ğ´Ğ¶ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ±ĞµĞ· Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° STATION â€” Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑÑ€ĞµĞ´Ğ°, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑÑ‰Ğ°Ñ Ğ½Ğ°ÑƒÑ‡Ğ½ÑƒÑ ÑĞºĞ¾ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ, Ğ³Ğ´Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ· Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸. ĞĞ³ĞµĞ½Ñ‚Ñ‹ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒÑÑ‚ Ñ‡ĞµÑ€ĞµĞ· Ñ‡Ñ‚ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚ ĞºĞ¾Ğ»Ğ»ĞµĞ³, Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·, Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ´Ğ° Ğ¸ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğµ Ğ¾ĞºĞ½Ğ° Ğ´Ğ»Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¸Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¿ÑƒÑ‚ĞµÑˆĞµÑÑ‚Ğ²Ğ¸Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¾Ñ‚ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¸ Ğ´Ğ¾ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹, Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‰Ğ¸Ğµ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸Ğ· ÑĞ¼ĞµÑ€Ğ´Ğ¶ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° ÑĞ¼ĞµÑ€Ğ´Ğ¶ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğ¸ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ, Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¶ĞµÑÑ‚ĞºĞ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Empowering AI Agents for Autonomous Scientific Discovery in STATION",
                    "desc": "The paper presents STATION, a multi-agent environment designed to simulate a scientific ecosystem where AI agents can autonomously conduct research. These agents utilize extended context windows to engage in complex tasks such as reading scientific papers, forming hypotheses, and publishing their findings without centralized control. The results show that these agents achieve state-of-the-art performance across various scientific benchmarks, including advancements in mathematics and computational biology. This work highlights the potential for emergent behavior in AI to foster innovative research methods and represents a significant shift towards autonomous scientific discovery."
                },
                "zh": {
                    "title": "STATIONï¼šè‡ªä¸»ç§‘å­¦å‘ç°çš„æ–°çºªå…ƒ",
                    "desc": "STATIONæ˜¯ä¸€ä¸ªå¼€æ”¾ä¸–ç•Œçš„å¤šæ™ºèƒ½ä½“ç¯å¢ƒï¼Œæ¨¡æ‹Ÿäº†ä¸€ä¸ªå¾®å‹ç§‘å­¦ç”Ÿæ€ç³»ç»Ÿã€‚åœ¨è¿™ä¸ªç¯å¢ƒä¸­ï¼Œæ™ºèƒ½ä½“å¯ä»¥è‡ªä¸»è¿›è¡Œç§‘å­¦æ¢ç´¢ï¼ŒåŒ…æ‹¬é˜…è¯»åŒè¡Œçš„è®ºæ–‡ã€æå‡ºå‡è®¾ã€æäº¤ä»£ç ã€è¿›è¡Œåˆ†æå’Œå‘å¸ƒç»“æœã€‚æ™ºèƒ½ä½“ä¹‹é—´æ²¡æœ‰ä¸­å¤®ç³»ç»Ÿåè°ƒï¼Œå®ƒä»¬å¯ä»¥è‡ªç”±é€‰æ‹©è¡ŒåŠ¨å¹¶å‘å±•è‡ªå·±çš„å™äº‹ã€‚å®éªŒè¡¨æ˜ï¼ŒSTATIONä¸­çš„æ™ºèƒ½ä½“åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†AlphaEvolveï¼Œå±•ç¤ºäº†è‡ªä¸»ç§‘å­¦å‘ç°çš„æ–°èŒƒå¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.07416",
            "title": "Robot Learning from a Physical World Model",
            "url": "https://huggingface.co/papers/2511.07416",
            "abstract": "PhysWorld integrates video generation and physical world modeling to enable accurate robotic manipulation from visual demonstrations without real robot data.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce PhysWorld, a framework that enables robot learning from video generation through physical world modeling. Recent video generation models can synthesize photorealistic visual demonstrations from language commands and images, offering a powerful yet underexplored source of training signals for robotics. However, directly retargeting pixel motions from generated videos to robots neglects physics, often resulting in inaccurate manipulations. PhysWorld addresses this limitation by coupling video generation with physical world reconstruction. Given a single image and a task command, our method generates task-conditioned videos and reconstructs the underlying physical world from the videos, and the generated video motions are grounded into physically accurate actions through object-centric residual reinforcement learning with the physical world model. This synergy transforms implicit visual guidance into physically executable robotic trajectories, eliminating the need for real robot data collection and enabling zero-shot generalizable robotic manipulation. Experiments on diverse real-world tasks demonstrate that PhysWorld substantially improves manipulation accuracy compared to previous approaches. Visit https://pointscoder.github.io/PhysWorld_Web/{the project webpage} for details.",
            "score": 30,
            "issue_id": 1,
            "pub_date": "2025-11-10",
            "pub_date_card": {
                "ru": "10 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 10",
                "zh": "11æœˆ10æ—¥"
            },
            "hash": "ad6cc657d01abe15",
            "authors": [
                "Jiageng Mao",
                "Sicheng He",
                "Hao-Ning Wu",
                "Yang You",
                "Shuyang Sun",
                "Zhicheng Wang",
                "Yanan Bao",
                "Huizhong Chen",
                "Leonidas Guibas",
                "Vitor Guizilini",
                "Howard Zhou",
                "Yue Wang"
            ],
            "affiliations": [
                "DeepMind",
                "Google",
                "Stanford",
                "Toyota Research Institute",
                "USC"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.07416.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#rl",
                    "#3d",
                    "#video"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞÑ‚ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğº Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ Ñ€Ğ¾Ğ±Ğ¾-Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼ Ğ±ĞµĞ· Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "PhysWorld â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ ÑÑ†ĞµĞ½Ñ‹ Ğ¸Ğ· ÑÑ‚Ğ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ¡Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒÑÑ‚ÑÑ Ğ² Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ object-centric Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°. Ğ”Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ½ÑƒĞ»ĞµĞ²Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°."
                },
                "en": {
                    "title": "Transforming Visual Guidance into Accurate Robotic Actions",
                    "desc": "PhysWorld is a novel framework that combines video generation with physical world modeling to enhance robotic manipulation. It leverages advanced video generation techniques to create realistic visual demonstrations based on task commands and images. By integrating physical world reconstruction, PhysWorld ensures that the generated video motions correspond to accurate physical actions, using object-centric residual reinforcement learning. This approach allows robots to learn effective manipulation strategies without needing real robot data, achieving better accuracy in diverse tasks."
                },
                "zh": {
                    "title": "PhysWorldï¼šä»è§†é¢‘ç”Ÿæˆåˆ°ç‰©ç†æ‰§è¡Œçš„æœºå™¨äººå­¦ä¹ ",
                    "desc": "PhysWorldæ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œç»“åˆè§†é¢‘ç”Ÿæˆå’Œç‰©ç†ä¸–ç•Œå»ºæ¨¡ï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿä»è§†è§‰æ¼”ç¤ºä¸­å­¦ä¹ ã€‚å®ƒåˆ©ç”¨æœ€æ–°çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œä»è¯­è¨€æŒ‡ä»¤å’Œå›¾åƒä¸­åˆæˆé€¼çœŸçš„è§†è§‰æ¼”ç¤ºï¼Œä¸ºæœºå™¨äººæä¾›å¼ºå¤§çš„è®­ç»ƒä¿¡å·ã€‚é€šè¿‡å°†è§†é¢‘ç”Ÿæˆä¸ç‰©ç†ä¸–ç•Œé‡å»ºç›¸ç»“åˆï¼ŒPhysWorldèƒ½å¤Ÿç”Ÿæˆä¸ä»»åŠ¡ç›¸å…³çš„è§†é¢‘ï¼Œå¹¶å°†è§†é¢‘ä¸­çš„è¿åŠ¨è½¬åŒ–ä¸ºç‰©ç†ä¸Šå‡†ç¡®çš„æœºå™¨äººåŠ¨ä½œã€‚å®éªŒè¡¨æ˜ï¼ŒPhysWorldåœ¨å¤šç§çœŸå®ä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†æœºå™¨äººçš„æ“ä½œç²¾åº¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.06876",
            "title": "Generating an Image From 1,000 Words: Enhancing Text-to-Image With\n  Structured Captions",
            "url": "https://huggingface.co/papers/2511.06876",
            "abstract": "A text-to-image model trained on long structured captions with DimFusion fusion mechanism and TaBR evaluation protocol achieves state-of-the-art prompt alignment and improved controllability.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-image models have rapidly evolved from casual creative tools to professional-grade systems, achieving unprecedented levels of image quality and realism. Yet, most models are trained to map short prompts into detailed images, creating a gap between sparse textual input and rich visual outputs. This mismatch reduces controllability, as models often fill in missing details arbitrarily, biasing toward average user preferences and limiting precision for professional use. We address this limitation by training the first open-source text-to-image model on long structured captions, where every training sample is annotated with the same set of fine-grained attributes. This design maximizes expressive coverage and enables disentangled control over visual factors. To process long captions efficiently, we propose DimFusion, a fusion mechanism that integrates intermediate tokens from a lightweight LLM without increasing token length. We also introduce the Text-as-a-Bottleneck Reconstruction (TaBR) evaluation protocol. By assessing how well real images can be reconstructed through a captioning-generation loop, TaBR directly measures controllability and expressiveness, even for very long captions where existing evaluation methods fail. Finally, we demonstrate our contributions by training the large-scale model FIBO, achieving state-of-the-art prompt alignment among open-source models. Model weights are publicly available at https://huggingface.co/briaai/FIBO",
            "score": 26,
            "issue_id": 1,
            "pub_date": "2025-11-10",
            "pub_date_card": {
                "ru": "10 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 10",
                "zh": "11æœˆ10æ—¥"
            },
            "hash": "f7ff852adc4d5527",
            "authors": [
                "Eyal Gutflaish",
                "Eliran Kachlon",
                "Hezi Zisman",
                "Tal Hacham",
                "Nimrod Sarid",
                "Alexander Visheratin",
                "Saar Huberman",
                "Gal Davidi",
                "Guy Bukchin",
                "Kfir Goldberg",
                "Ron Mokady"
            ],
            "affiliations": [
                "Bria AI"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.06876.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#open_source",
                    "#architecture",
                    "#cv",
                    "#multimodal",
                    "#training",
                    "#dataset"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ”Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ°Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¿ĞµÑ€Ğ²Ğ°Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑÑ… Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ DimFusion, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ’Ğ²ĞµĞ´Ñ‘Ğ½ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» Ğ¾Ñ†ĞµĞ½ĞºĞ¸ TaBR, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ñ†Ğ¸ĞºĞ» Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ¸Ğ·Ğ¼ĞµÑ€ÑÑ‚ÑŒ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ FIBO Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ ÑÑ€ĞµĞ´Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Bridging the Gap: Long Captions for Precise Image Generation",
                    "desc": "This paper presents a novel text-to-image model that utilizes long structured captions to enhance image generation quality and controllability. By employing the DimFusion fusion mechanism, the model efficiently processes these lengthy captions without increasing token length, allowing for better alignment between text prompts and generated images. The introduction of the TaBR evaluation protocol enables a more accurate assessment of the model's performance in reconstructing images from captions, particularly for complex and detailed inputs. The resulting model, FIBO, achieves state-of-the-art results in prompt alignment, making it a significant advancement in the field of AI-generated imagery."
                },
                "zh": {
                    "title": "é•¿ç»“æ„åŒ–æ ‡é¢˜æå‡æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„å¯æ§æ€§",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ï¼Œä½¿ç”¨é•¿ç»“æ„åŒ–æ ‡é¢˜è¿›è¡Œè®­ç»ƒï¼Œé‡‡ç”¨DimFusionèåˆæœºåˆ¶å’ŒTaBRè¯„ä¼°åè®®ã€‚è¯¥æ¨¡å‹è§£å†³äº†çŸ­æç¤ºä¸è¯¦ç»†å›¾åƒä¹‹é—´çš„å·®è·ï¼Œæé«˜äº†å¯æ§æ€§å’Œè¡¨è¾¾èƒ½åŠ›ã€‚é€šè¿‡å¯¹æ¯ä¸ªè®­ç»ƒæ ·æœ¬è¿›è¡Œç»†ç²’åº¦å±æ€§æ ‡æ³¨ï¼Œæ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°æ§åˆ¶è§†è§‰å› ç´ ã€‚æœ€ç»ˆï¼Œè®­ç»ƒçš„å¤§è§„æ¨¡æ¨¡å‹FIBOåœ¨å¼€æºæ¨¡å‹ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æç¤ºå¯¹é½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.07419",
            "title": "Routing Manifold Alignment Improves Generalization of Mixture-of-Experts\n  LLMs",
            "url": "https://huggingface.co/papers/2511.07419",
            "abstract": "Aligning routing weights with task embeddings in Sparse Mixture-of-Experts (MoE) models improves generalization and reduces performance gaps in large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Sparse Mixture-of-Experts (MoE) have been widely adopted in recent large language models since it can efficiently scale up the model capability without increasing the inference cost. However, evaluations on broad downstream tasks reveal a consistent suboptimality of the routers in existing MoE LLMs, which results in a severe performance gap (e.g., 10-20% in accuracy) to the optimal routing. In this paper, we show that aligning the manifold of routing weights with that of task embedding can effectively reduce the gap and improve MoE LLMs' generalization performance. Our method, \"Routing Manifold Alignment (RoMA)\", introduces an additional manifold regularization term in the post-training objective and only requires lightweight finetuning of routers (with other parameters frozen). Specifically, the regularization encourages the routing weights of each sample to be close to those of its successful neighbors (whose routing weights lead to correct answers) in a task embedding space. Consequently, samples targeting similar tasks will share similar expert choices across layers. Building such bindings between tasks and experts over different samples is essential to achieve better generalization. Moreover, RoMA demonstrates the advantage of unifying the task understanding (by embedding models) with solution generation (by MoE LLMs). In experiments, we finetune routers in OLMoE, DeepSeekMoE, and Qwen3-MoE using RoMA. Evaluations on diverse benchmarks and extensive comparisons with baselines show the substantial improvement brought by RoMA.",
            "score": 25,
            "issue_id": 1,
            "pub_date": "2025-11-10",
            "pub_date_card": {
                "ru": "10 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 10",
                "zh": "11æœˆ10æ—¥"
            },
            "hash": "0ddf3df037bc9395",
            "authors": [
                "Zhongyang Li",
                "Ziyue Li",
                "Tianyi Zhou"
            ],
            "affiliations": [
                "Johns Hopkins University",
                "University of Maryland, College Park"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.07419.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#architecture",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸ”€",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ MoE Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ RoMA (Routing Manifold Alignment) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Mixture-of-Experts Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ²ĞµÑĞ¾Ğ² Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸ĞµĞ¼ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ°Ğ¼ Ñ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ñ‹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ‡Ğ»ĞµĞ½ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¸ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ğ¾Ğ¹ Ğ´Ğ¾Ñ‡ĞµÑ€Ğ½ĞµĞ¹ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… LLM Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¿Ğ°ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚ĞµĞºÑƒÑ‰ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹, Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 10-20%."
                },
                "en": {
                    "title": "Aligning Routing for Better Generalization in MoE Models",
                    "desc": "This paper presents a method called Routing Manifold Alignment (RoMA) to enhance Sparse Mixture-of-Experts (MoE) models in large language tasks. The authors identify that existing routing mechanisms in MoE models often lead to significant performance gaps, which can be as high as 20% in accuracy. RoMA aligns routing weights with task embeddings, allowing the model to better generalize by ensuring that similar tasks share expert choices. The approach requires only lightweight finetuning of the routers, leading to improved performance across various benchmarks without the need to retrain the entire model."
                },
                "zh": {
                    "title": "ä¼˜åŒ–è·¯ç”±æƒé‡ï¼Œæå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›",
                    "desc": "åœ¨ç¨€ç–ä¸“å®¶æ··åˆæ¨¡å‹ï¼ˆMoEï¼‰ä¸­ï¼Œå°†è·¯ç”±æƒé‡ä¸ä»»åŠ¡åµŒå…¥å¯¹é½å¯ä»¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶å‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½å·®è·ã€‚ç°æœ‰çš„MoEæ¨¡å‹åœ¨å¹¿æ³›çš„ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºè·¯ç”±å™¨çš„æ¬¡ä¼˜æ€§ï¼Œå¯¼è‡´å‡†ç¡®ç‡ä¸Šå­˜åœ¨10-20%çš„æ˜¾è‘—å·®è·ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºâ€œè·¯ç”±æµå½¢å¯¹é½ï¼ˆRoMAï¼‰â€çš„æ–¹æ³•ï¼Œé€šè¿‡åœ¨åè®­ç»ƒç›®æ ‡ä¸­å¼•å…¥é¢å¤–çš„æµå½¢æ­£åˆ™åŒ–é¡¹ï¼Œä»…éœ€å¯¹è·¯ç”±å™¨è¿›è¡Œè½»é‡çº§å¾®è°ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRoMAåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ï¼Œè¯æ˜äº†ä»»åŠ¡ç†è§£ä¸è§£å†³æ–¹æ¡ˆç”Ÿæˆçš„ç»Ÿä¸€æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.07070",
            "title": "RedOne 2.0: Rethinking Domain-specific LLM Post-Training in Social\n  Networking Services",
            "url": "https://huggingface.co/papers/2511.07070",
            "abstract": "RedOne 2.0, a social networking service-oriented LLM, uses a progressive, RL-prioritized post-training paradigm to achieve rapid and stable adaptation, delivering improvements over larger baselines with less data.  \t\t\t\t\tAI-generated summary \t\t\t\t As a key medium for human interaction and information exchange, social networking services (SNS) pose unique challenges for large language models (LLMs): heterogeneous workloads, fast-shifting norms and slang, and multilingual, culturally diverse corpora that induce sharp distribution shift. Supervised fine-tuning (SFT) can specialize models but often triggers a ``seesaw'' between in-distribution gains and out-of-distribution robustness, especially for smaller models. To address these challenges, we introduce RedOne 2.0, an SNS-oriented LLM trained with a progressive, RL-prioritized post-training paradigm designed for rapid and stable adaptation. The pipeline consist in three stages: (1) Exploratory Learning on curated SNS corpora to establish initial alignment and identify systematic weaknesses; (2) Targeted Fine-Tuning that selectively applies SFT to the diagnosed gaps while mixing a small fraction of general data to mitigate forgetting; and (3) Refinement Learning that re-applies RL with SNS-centric signals to consolidate improvements and harmonize trade-offs across tasks. Across various tasks spanning three categories, our 4B scale model delivers an average improvements about 2.41 over the 7B sub-optimal baseline. Additionally, RedOne 2.0 achieves average performance lift about 8.74 from the base model with less than half the data required by SFT-centric method RedOne, evidencing superior data efficiency and stability at compact scales. Overall, RedOne 2.0 establishes a competitive, cost-effective baseline for domain-specific LLMs in SNS scenario, advancing capability without sacrificing robustness.",
            "score": 18,
            "issue_id": 1,
            "pub_date": "2025-11-10",
            "pub_date_card": {
                "ru": "10 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 10",
                "zh": "11æœˆ10æ—¥"
            },
            "hash": "a2e8e21aa7d3d87d",
            "authors": [
                "Fei Zhao",
                "Chonggang Lu",
                "Haofu Qian",
                "Fangcheng Shi",
                "Zijie Meng",
                "Jianzhao Huang",
                "Xu Tang",
                "Zheyong Xie",
                "Zheyu Ye",
                "Zhe Xu",
                "Yao Hu",
                "Shaosheng Cao"
            ],
            "affiliations": [
                "NLP Team, Xiaohongshu Inc. Huangpu District, Shanghai, China"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.07070.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#multilingual",
                    "#small_models",
                    "#training"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "RedOne 2.0 â€” ÑÑ‚Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ñ€Ğ¸ ÑÑ‚Ğ°Ğ¿Ğ°: Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ°Ñ… ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹, Ñ†ĞµĞ»ĞµĞ²ÑƒÑ Ğ´Ğ¾Ğ²Ğ¾Ğ´ĞºÑƒ Ñ Ğ¸Ğ·Ğ±Ğ¸Ñ€Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼ ÑÑƒĞ¿ĞµÑ€Ğ²Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞšĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ 4-Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ 7-Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ½Ğ¾Ğ¹ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ»ÑƒÑ‡ÑˆÑƒÑ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Â«ĞºĞ°Ñ‡ĞµĞ»Ğ¸Â» Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ Ğº ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸ĞºĞµ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ğ±Ñ‰ĞµĞ¹ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "RedOne 2.0: Efficient Adaptation for Social Networking LLMs",
                    "desc": "RedOne 2.0 is a large language model (LLM) specifically designed for social networking services (SNS), addressing unique challenges such as diverse languages and rapidly changing social norms. It employs a progressive, reinforcement learning (RL)-prioritized post-training approach that allows for quick and stable adaptation, outperforming larger models while using less data. The training process consists of three stages: exploratory learning to identify weaknesses, targeted fine-tuning to address those gaps, and refinement learning to enhance performance using SNS-specific signals. This model demonstrates significant improvements in efficiency and robustness, making it a strong candidate for domain-specific applications in social media contexts."
                },
                "zh": {
                    "title": "RedOne 2.0ï¼šç¤¾äº¤ç½‘ç»œæœåŠ¡çš„é«˜æ•ˆè¯­è¨€æ¨¡å‹",
                    "desc": "RedOne 2.0 æ˜¯ä¸€ç§é¢å‘ç¤¾äº¤ç½‘ç»œæœåŠ¡çš„è¯­è¨€æ¨¡å‹ï¼Œé‡‡ç”¨æ¸è¿›å¼çš„å¼ºåŒ–å­¦ä¹ ä¼˜å…ˆåè®­ç»ƒæ–¹æ³•ï¼Œä»¥å®ç°å¿«é€Ÿå’Œç¨³å®šçš„é€‚åº”ã€‚è¯¥æ¨¡å‹é€šè¿‡ä¸‰ä¸ªé˜¶æ®µçš„è®­ç»ƒæµç¨‹ï¼Œé¦–å…ˆåœ¨ç²¾é€‰çš„ç¤¾äº¤ç½‘ç»œè¯­æ–™ä¸Šè¿›è¡Œæ¢ç´¢æ€§å­¦ä¹ ï¼Œç„¶åé’ˆå¯¹è¯†åˆ«å‡ºçš„å¼±ç‚¹è¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„å¾®è°ƒï¼Œæœ€åé€šè¿‡å¼ºåŒ–å­¦ä¹ å·©å›ºæ”¹è¿›ã€‚ä¸ä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒæ–¹æ³•ç›¸æ¯”ï¼ŒRedOne 2.0 åœ¨æ•°æ®æ•ˆç‡å’Œç¨³å®šæ€§ä¸Šè¡¨ç°æ›´ä½³ï¼Œèƒ½å¤Ÿåœ¨è¾ƒå°çš„æ•°æ®é›†ä¸Šå®ç°æ›´å¤§çš„æ€§èƒ½æå‡ã€‚æ€»ä½“è€Œè¨€ï¼ŒRedOne 2.0 ä¸ºç¤¾äº¤ç½‘ç»œåœºæ™¯ä¸­çš„é¢†åŸŸç‰¹å®šè¯­è¨€æ¨¡å‹å»ºç«‹äº†ä¸€ä¸ªå…·æœ‰ç«äº‰åŠ›çš„åŸºå‡†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.07250",
            "title": "MVU-Eval: Towards Multi-Video Understanding Evaluation for Multimodal\n  LLMs",
            "url": "https://huggingface.co/papers/2511.07250",
            "abstract": "MVU-Eval is a comprehensive benchmark for evaluating multi-video understanding in Multimodal Large Language Models, addressing gaps in existing single-video benchmarks and highlighting performance discrepancies in real-world applications.  \t\t\t\t\tAI-generated summary \t\t\t\t The advent of Multimodal Large Language Models (MLLMs) has expanded AI capabilities to visual modalities, yet existing evaluation benchmarks remain limited to single-video understanding, overlooking the critical need for multi-video understanding in real-world scenarios (e.g., sports analytics and autonomous driving). To address this significant gap, we introduce MVU-Eval, the first comprehensive benchmark for evaluating Multi-Video Understanding for MLLMs. Specifically, our MVU-Eval mainly assesses eight core competencies through 1,824 meticulously curated question-answer pairs spanning 4,959 videos from diverse domains, addressing both fundamental perception tasks and high-order reasoning tasks. These capabilities are rigorously aligned with real-world applications such as multi-sensor synthesis in autonomous systems and cross-angle sports analytics. Through extensive evaluation of state-of-the-art open-source and closed-source models, we reveal significant performance discrepancies and limitations in current MLLMs' ability to perform understanding across multiple videos. The benchmark will be made publicly available to foster future research.",
            "score": 17,
            "issue_id": 1,
            "pub_date": "2025-11-10",
            "pub_date_card": {
                "ru": "10 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 10",
                "zh": "11æœˆ10æ—¥"
            },
            "hash": "581a9ddc3ec5d36a",
            "authors": [
                "Tianhao Peng",
                "Haochen Wang",
                "Yuanxing Zhang",
                "Zekun Wang",
                "Zili Wang",
                "Gavin Chang",
                "Jian Yang",
                "Shihao Li",
                "Yanghai Wang",
                "Xintao Wang",
                "Houyi Li",
                "Wei Ji",
                "Pengfei Wan",
                "Steven Huang",
                "Zhaoxiang Zhang",
                "Jiaheng Liu"
            ],
            "affiliations": [
                "CASIA",
                "Kuaishou Technology",
                "M-A-P",
                "Nanjing University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.07250.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#open_source",
                    "#video",
                    "#multimodal",
                    "#dataset",
                    "#reasoning"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞÑ†ĞµĞ½ĞºĞ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ MVU-Eval â€” Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 1824 Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ñ‹ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ 4959 Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¾ÑĞµĞ¼ÑŒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿ĞµÑ‚ĞµĞ½Ñ†Ğ¸Ğ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¸Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ°Ğ¼Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ±ÑƒĞ´ĞµÑ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½ Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑĞ¼, Ñ‚Ğ°ĞºĞ¸Ğ¼ ĞºĞ°Ğº Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ¸ ÑĞ¿Ğ¾Ñ€Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ° Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑƒĞ³Ğ»Ğ¾Ğ²."
                },
                "en": {
                    "title": "MVU-Eval: Bridging the Gap in Multi-Video Understanding for MLLMs",
                    "desc": "MVU-Eval is a new benchmark designed to evaluate how well Multimodal Large Language Models (MLLMs) understand multiple videos, which is important for real-world tasks like sports analysis and self-driving cars. Unlike previous benchmarks that only focused on single videos, MVU-Eval includes a wide range of scenarios with 1,824 question-answer pairs from nearly 5,000 videos. It tests eight key skills, including basic perception and complex reasoning, to see how well these models can handle multi-video information. The findings show that current MLLMs have significant gaps in their performance when it comes to understanding multiple videos, highlighting the need for improvement in this area."
                },
                "zh": {
                    "title": "å¤šè§†é¢‘ç†è§£çš„è¯„ä¼°æ–°æ ‡å‡†",
                    "desc": "MVU-Evalæ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤šè§†é¢‘ç†è§£åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„è¡¨ç°ã€‚ç°æœ‰çš„å•è§†é¢‘åŸºå‡†æ— æ³•æ»¡è¶³ç°å®åº”ç”¨ä¸­å¯¹å¤šè§†é¢‘ç†è§£çš„éœ€æ±‚ï¼Œä¾‹å¦‚ä½“è‚²åˆ†æå’Œè‡ªåŠ¨é©¾é©¶ã€‚MVU-Evalé€šè¿‡1824ä¸ªç²¾å¿ƒç­–åˆ’çš„é—®é¢˜-ç­”æ¡ˆå¯¹ï¼Œè¯„ä¼°äº†æ¥è‡ª4959ä¸ªè§†é¢‘çš„å…«é¡¹æ ¸å¿ƒèƒ½åŠ›ï¼Œæ¶µç›–åŸºæœ¬æ„ŸçŸ¥ä»»åŠ¡å’Œé«˜é˜¶æ¨ç†ä»»åŠ¡ã€‚é€šè¿‡å¯¹å½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹è¿›è¡Œå¹¿æ³›è¯„ä¼°ï¼Œæˆ‘ä»¬æ­ç¤ºäº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šè§†é¢‘ç†è§£æ–¹é¢çš„æ˜¾è‘—æ€§èƒ½å·®å¼‚å’Œå±€é™æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.06209",
            "title": "Reasoning with Confidence: Efficient Verification of LLM Reasoning Steps\n  via Uncertainty Heads",
            "url": "https://huggingface.co/papers/2511.06209",
            "abstract": "Transformer-based uncertainty quantification heads improve step-level reasoning verification in LLMs by estimating uncertainty from internal states, offering a lightweight and scalable alternative to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Solving complex tasks usually requires LLMs to generate long multi-step reasoning chains. Previous work has shown that verifying the correctness of individual reasoning steps can further improve the performance and efficiency of LLMs on such tasks and enhance solution interpretability. However, existing verification approaches, such as Process Reward Models (PRMs), are either computationally expensive, limited to specific domains, or require large-scale human or model-generated annotations. Thus, we propose a lightweight alternative for step-level reasoning verification based on data-driven uncertainty scores. We train transformer-based uncertainty quantification heads (UHeads) that use the internal states of a frozen LLM to estimate the uncertainty of its reasoning steps during generation. The approach is fully automatic: target labels are generated either by another larger LLM (e.g., DeepSeek R1) or in a self-supervised manner by the original model itself. UHeads are both effective and lightweight, containing less than 10M parameters. Across multiple domains, including mathematics, planning, and general knowledge question answering, they match or even surpass the performance of PRMs that are up to 810x larger. Our findings suggest that the internal states of LLMs encode their uncertainty and can serve as reliable signals for reasoning verification, offering a promising direction toward scalable and generalizable introspective LLMs.",
            "score": 17,
            "issue_id": 1,
            "pub_date": "2025-11-09",
            "pub_date_card": {
                "ru": "9 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 9",
                "zh": "11æœˆ9æ—¥"
            },
            "hash": "1df27bb09e636af9",
            "authors": [
                "Jingwei Ni",
                "Ekaterina Fadeeva",
                "Tianyi Wu",
                "Mubashara Akhtar",
                "Jiaheng Zhang",
                "Elliott Ash",
                "Markus Leippold",
                "Timothy Baldwin",
                "See-Kiong Ng",
                "Artem Shelmanov",
                "Mrinmaya Sachan"
            ],
            "affiliations": [
                "ETH Zurich",
                "MBZUAI",
                "National University of Singapore",
                "The University of Melbourne",
                "University of Zurich"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.06209.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#optimization",
                    "#interpretability",
                    "#architecture",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ›Ñ‘Ğ³ĞºĞ¸Ğµ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹ Ğ½ĞµÑƒĞ²ĞµÑ€Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸: Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² LLM",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½ĞµÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€-Ğ½Ñ‹Ñ… Ğ³Ğ¾Ğ»Ğ¾Ğ² Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ (UHeads). Ğ­Ñ‚Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ Ğ¼ĞµĞ½ĞµĞµ 10 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ñ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ¹ LLM Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ¦ĞµĞ»ĞµĞ²Ñ‹Ğµ Ğ¼ĞµÑ‚ĞºĞ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ñ‹ Ğ»Ğ¸Ğ±Ğ¾ Ğ¾Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ»Ğ¸Ğ±Ğ¾ ÑĞ°Ğ¼Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ UHeads Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ°Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ (PRM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¸, Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Lightweight Uncertainty Heads for Efficient Reasoning Verification in LLMs",
                    "desc": "This paper introduces a new method for verifying the reasoning steps of large language models (LLMs) using transformer-based uncertainty quantification heads (UHeads). These UHeads estimate the uncertainty of reasoning steps by analyzing the internal states of a frozen LLM, providing a lightweight and efficient alternative to traditional verification methods like Process Reward Models (PRMs). The approach is fully automatic, generating target labels through either a larger LLM or self-supervised techniques, and it requires significantly fewer parameters than existing models. The results demonstrate that UHeads can effectively match or exceed the performance of much larger PRMs across various domains, indicating that LLMs' internal states can be valuable for assessing reasoning accuracy."
                },
                "zh": {
                    "title": "è½»é‡çº§ä¸ç¡®å®šæ€§é‡åŒ–ï¼Œæå‡æ¨ç†éªŒè¯æ•ˆç‡",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå˜æ¢å™¨çš„æ¨¡å‹ä¸ç¡®å®šæ€§é‡åŒ–å¤´ï¼ˆUHeadsï¼‰ï¼Œç”¨äºæ”¹è¿›å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é€æ­¥æ¨ç†éªŒè¯ä¸­çš„è¡¨ç°ã€‚é€šè¿‡åˆ©ç”¨LLMçš„å†…éƒ¨çŠ¶æ€ï¼ŒUHeadsèƒ½å¤Ÿè‡ªåŠ¨ä¼°è®¡æ¨ç†æ­¥éª¤çš„ä¸ç¡®å®šæ€§ï¼Œä»è€Œæä¾›äº†ä¸€ç§è½»é‡çº§ä¸”å¯æ‰©å±•çš„éªŒè¯æ–¹æ³•ã€‚ä¸ç°æœ‰çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰ç›¸æ¯”ï¼ŒUHeadsåœ¨å¤šä¸ªé¢†åŸŸçš„è¡¨ç°ç›¸å½“æˆ–æ›´ä¼˜ï¼Œä¸”å‚æ•°é‡å°‘äº1000ä¸‡ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒLLMçš„å†…éƒ¨çŠ¶æ€èƒ½å¤Ÿæœ‰æ•ˆç¼–ç ä¸ç¡®å®šæ€§ï¼Œä¸ºæ¨ç†éªŒè¯æä¾›å¯é ä¿¡å·ï¼Œæ¨åŠ¨å¯æ‰©å±•å’Œé€šç”¨çš„è‡ªçœå‹LLMçš„å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.07384",
            "title": "Teaching Pretrained Language Models to Think Deeper with Retrofitted\n  Recurrence",
            "url": "https://huggingface.co/papers/2511.07384",
            "abstract": "Converting pretrained non-recurrent language models to depth-recurrent models improves performance at a given compute budget using a curriculum of recurrences.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in depth-recurrent language models show that recurrence can decouple train-time compute and parameter count from test-time compute. In this work, we study how to convert existing pretrained non-recurrent language models into depth-recurrent models. We find that using a curriculum of recurrences to increase the effective depth of the model over the course of training preserves performance while reducing total computational cost. In our experiments, on mathematics, we observe that converting pretrained models to recurrent ones results in better performance at a given compute budget than simply post-training the original non-recurrent language model.",
            "score": 16,
            "issue_id": 1,
            "pub_date": "2025-11-10",
            "pub_date_card": {
                "ru": "10 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 10",
                "zh": "11æœˆ10æ—¥"
            },
            "hash": "5f882be3bb030bd3",
            "authors": [
                "Sean McLeish",
                "Ang Li",
                "John Kirchenbauer",
                "Dayal Singh Kalra",
                "Brian R. Bartoldson",
                "Bhavya Kailkhura",
                "Avi Schwarzschild",
                "Jonas Geiping",
                "Tom Goldstein",
                "Micah Goldblum"
            ],
            "affiliations": [
                "Columbia University",
                "ELLIS Institute TÃ¼bingen",
                "Lawrence Livermore National Laboratory",
                "Max Planck Institute for Intelligent Systems",
                "New York University",
                "TÃ¼bingen AI Center",
                "University of Maryland",
                "University of North Carolina"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.07384.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "ĞÑ‚ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğº Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹: Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ² Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğµ Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ñ€ĞµĞºÑƒÑ€ÑĞ¸Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ (curriculum learning), Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ½Ğ¸ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑÑƒÑ€ÑÑ‹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ñ€ĞµĞ¼ĞµĞ½ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½ĞµĞ¼ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. ĞĞ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğµ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ½ĞµÑ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Language Models: From Non-Recurrent to Depth-Recurrent for Better Efficiency",
                    "desc": "This paper explores the transformation of pretrained non-recurrent language models into depth-recurrent models to enhance their performance while managing computational resources. The authors introduce a curriculum of recurrences that gradually increases the model's effective depth during training, which helps maintain performance levels. Their findings indicate that this approach allows for better utilization of compute budgets compared to merely post-training the original non-recurrent models. The experiments conducted, particularly in the domain of mathematics, demonstrate that the converted recurrent models outperform their non-recurrent counterparts under the same computational constraints."
                },
                "zh": {
                    "title": "å°†éé€’å½’æ¨¡å‹è½¬ä¸ºæ·±åº¦é€’å½’æ¨¡å‹ï¼Œæå‡æ€§èƒ½ä¸æ•ˆç‡",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†å¦‚ä½•å°†é¢„è®­ç»ƒçš„éé€’å½’è¯­è¨€æ¨¡å‹è½¬æ¢ä¸ºæ·±åº¦é€’å½’æ¨¡å‹ã€‚é€šè¿‡ä½¿ç”¨é€’å½’çš„è¯¾ç¨‹å­¦ä¹ æ–¹æ³•ï¼Œé€æ­¥å¢åŠ æ¨¡å‹çš„æœ‰æ•ˆæ·±åº¦ï¼Œå¯ä»¥åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶é™ä½æ€»è®¡ç®—æˆæœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æ•°å­¦ä»»åŠ¡ä¸Šï¼Œå°†é¢„è®­ç»ƒæ¨¡å‹è½¬æ¢ä¸ºé€’å½’æ¨¡å‹åœ¨ç»™å®šçš„è®¡ç®—é¢„ç®—ä¸‹è¡¨ç°æ›´å¥½ã€‚æ­¤æ–¹æ³•æœ‰æ•ˆåœ°è§£è€¦äº†è®­ç»ƒæ—¶çš„è®¡ç®—ä¸æµ‹è¯•æ—¶çš„è®¡ç®—ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.06411",
            "title": "SofT-GRPO: Surpassing Discrete-Token LLM Reinforcement Learning via\n  Gumbel-Reparameterized Soft-Thinking Policy Optimization",
            "url": "https://huggingface.co/papers/2511.06411",
            "abstract": "A novel policy optimization algorithm, SofT-GRPO, enhances soft-thinking in Large Language Models by integrating Gumbel noise and the Gumbel-Softmax technique, leading to improved performance over discrete-token methods.  \t\t\t\t\tAI-generated summary \t\t\t\t The soft-thinking paradigm for Large Language Model (LLM) reasoning can outperform the conventional discrete-token Chain-of-Thought (CoT) reasoning in some scenarios, underscoring its research and application value. However, while the discrete-token CoT reasoning pattern can be reinforced through policy optimization algorithms such as group relative policy optimization (GRPO), extending the soft-thinking pattern with Reinforcement Learning (RL) remains challenging. This difficulty stems from the complexities of injecting stochasticity into soft-thinking tokens and updating soft-thinking policies accordingly. As a result, previous attempts to combine soft-thinking with GRPO typically underperform their discrete-token GRPO counterparts. To fully unlock the potential of soft-thinking, this paper presents a novel policy optimization algorithm, SofT-GRPO, to reinforce LLMs under the soft-thinking reasoning pattern. SofT-GRPO injects the Gumbel noise into logits, employs the Gumbel-Softmax technique to avoid soft-thinking tokens outside the pre-trained embedding space, and leverages the reparameterization trick in policy gradient. We conduct experiments across base LLMs ranging from 1.5B to 7B parameters, and results demonstrate that SofT-GRPO enables soft-thinking LLMs to slightly outperform discrete-token GRPO on Pass@1 (+0.13% on average accuracy), while exhibiting a substantial uplift on Pass@32 (+2.19% on average accuracy). Codes and weights are available on https://github.com/zz1358m/SofT-GRPO-master",
            "score": 16,
            "issue_id": 1,
            "pub_date": "2025-11-09",
            "pub_date_card": {
                "ru": "9 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 9",
                "zh": "11æœˆ9æ—¥"
            },
            "hash": "15e102fa44767987",
            "authors": [
                "Zhi Zheng",
                "Wee Sun Lee"
            ],
            "affiliations": [
                "School of Computing, National University of Singapore"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.06411.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#optimization",
                    "#architecture",
                    "#training",
                    "#rl",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞœÑĞ³ĞºĞ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ LLM Ñ‡ĞµÑ€ĞµĞ· ÑˆÑƒĞ¼ Ğ“ÑƒĞ¼Ğ±ĞµĞ»Ñ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ SofT-GRPO, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¼ÑĞ³ĞºĞ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ ÑˆÑƒĞ¼Ğ° Ğ“ÑƒĞ¼Ğ±ĞµĞ»Ñ Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Gumbel-Softmax. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ reinforcement learning Ğº Ğ¼ÑĞ³ĞºĞ¸Ğ¼ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ€Ğ°Ğ½ĞµĞµ Ğ±Ñ‹Ğ»Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ²Ñ€Ğ¾Ğ´Ğµ GRPO. SofT-GRPO Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€ÑĞº Ñ€ĞµĞ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ° Ğ¼ÑĞ³ĞºĞ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ·Ğ° Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‹ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½ĞµĞ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚ĞºĞ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Unlocking Soft-Thinking Potential in LLMs with SofT-GRPO",
                    "desc": "This paper introduces SofT-GRPO, a new policy optimization algorithm designed to enhance soft-thinking in Large Language Models (LLMs). By integrating Gumbel noise and the Gumbel-Softmax technique, SofT-GRPO improves the performance of LLMs compared to traditional discrete-token methods. The algorithm addresses the challenges of incorporating stochasticity into soft-thinking tokens and effectively updates soft-thinking policies. Experimental results show that SofT-GRPO achieves better accuracy on various tasks, demonstrating its potential to advance soft-thinking reasoning in LLMs."
                },
                "zh": {
                    "title": "SofT-GRPOï¼šæå‡è½¯æ€ç»´çš„ç­–ç•¥ä¼˜åŒ–æ–°ç®—æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç­–ç•¥ä¼˜åŒ–ç®—æ³•SofT-GRPOï¼Œæ—¨åœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„è½¯æ€ç»´èƒ½åŠ›ã€‚é€šè¿‡å¼•å…¥Gumbelå™ªå£°å’ŒGumbel-SoftmaxæŠ€æœ¯ï¼ŒSofT-GRPOåœ¨æ€§èƒ½ä¸Šä¼˜äºä¼ ç»Ÿçš„ç¦»æ•£æ ‡è®°æ–¹æ³•ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè½¯æ€ç»´æ¨ç†åœ¨æŸäº›åœºæ™¯ä¸‹èƒ½å¤Ÿè¶…è¶Šå¸¸è§„çš„ç¦»æ•£æ ‡è®°æ¨ç†ï¼Œæ˜¾ç¤ºå‡ºå…¶ç ”ç©¶å’Œåº”ç”¨çš„ä»·å€¼ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSofT-GRPOä½¿å¾—è½¯æ€ç»´æ¨¡å‹åœ¨å‡†ç¡®æ€§ä¸Šç•¥å¾®è¶…è¶Šç¦»æ•£æ ‡è®°çš„GRPOï¼Œå°¤å…¶åœ¨Pass@32ä¸Šè¡¨ç°å‡ºæ˜¾è‘—æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.07317",
            "title": "RLVE: Scaling Up Reinforcement Learning for Language Models with\n  Adaptive Verifiable Environments",
            "url": "https://huggingface.co/papers/2511.07317",
            "abstract": "Reinforcement Learning with Adaptive Verifiable Environments (RLVE) improves language model reasoning by dynamically adjusting problem difficulty, outperforming static environments and traditional RL training.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Reinforcement Learning (RL) with Adaptive Verifiable Environments (RLVE), an approach using verifiable environments that procedurally generate problems and provide algorithmically verifiable rewards, to scale up RL for language models (LMs). RLVE enables each verifiable environment to dynamically adapt its problem difficulty distribution to the policy model's capabilities as training progresses. In contrast, static data distributions often lead to vanishing learning signals when problems are either too easy or too hard for the policy. To implement RLVE, we create RLVE-Gym, a large-scale suite of 400 verifiable environments carefully developed through manual environment engineering. Using RLVE-Gym, we show that environment scaling, i.e., expanding the collection of training environments, consistently improves generalizable reasoning capabilities. RLVE with joint training across all 400 environments in RLVE-Gym yields a 3.37% absolute average improvement across six reasoning benchmarks, starting from one of the strongest 1.5B reasoning LMs. By comparison, continuing this LM's original RL training yields only a 0.49% average absolute gain despite using over 3x more compute. We release our code publicly.",
            "score": 13,
            "issue_id": 1,
            "pub_date": "2025-11-10",
            "pub_date_card": {
                "ru": "10 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 10",
                "zh": "11æœˆ10æ—¥"
            },
            "hash": "f0d01635ac23233d",
            "authors": [
                "Zhiyuan Zeng",
                "Hamish Ivison",
                "Yiping Wang",
                "Lifan Yuan",
                "Shuyue Stella Li",
                "Zhuorui Ye",
                "Siting Li",
                "Jacqueline He",
                "Runlong Zhou",
                "Tong Chen",
                "Chenyang Zhao",
                "Yulia Tsvetkov",
                "Simon Shaolei Du",
                "Natasha Jaques",
                "Hao Peng",
                "Pang Wei Koh",
                "Hannaneh Hajishirzi"
            ],
            "affiliations": [
                "Allen Institute for AI",
                "Carnegie Mellon University",
                "Tsinghua University",
                "University of Washington"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.07317.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#small_models",
                    "#open_source",
                    "#optimization",
                    "#training",
                    "#rl",
                    "#reasoning"
                ],
                "emoji": "ğŸ“",
                "ru": {
                    "title": "ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Reinforcement Learning Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ (RLVE), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€ĞµĞ³ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ RLVE-Gym â€” Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ· 400 Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‚Ğ°Ğº ĞºĞ°Ğº Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ¸ÑÑ‡ĞµĞ·Ğ½Ğ¾Ğ²ĞµĞ½Ğ¸Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ğ³Ğ´Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ÑÑ‚ÑÑ ÑĞ»Ğ¸ÑˆĞºĞ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğ¼Ğ¸ Ğ¸Ğ»Ğ¸ ÑĞ»Ğ¸ÑˆĞºĞ¾Ğ¼ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ²ÑĞµÑ… 400 Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ÑÑ… Ğ´Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 3.37% Ğ½Ğ° ÑˆĞµÑÑ‚Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ RL."
                },
                "en": {
                    "title": "Dynamic Difficulty for Smarter Language Models",
                    "desc": "Reinforcement Learning with Adaptive Verifiable Environments (RLVE) enhances the reasoning abilities of language models by adjusting the difficulty of problems during training. This method uses verifiable environments that generate problems and provide rewards that can be confirmed algorithmically, allowing for scalable reinforcement learning. Unlike traditional static environments, RLVE adapts to the model's learning progress, preventing issues with problems being too easy or too hard. The implementation of RLVE-Gym, a suite of 400 environments, demonstrates significant improvements in reasoning capabilities, achieving a 3.37% average gain across benchmarks compared to traditional methods."
                },
                "zh": {
                    "title": "åŠ¨æ€è°ƒæ•´éš¾åº¦ï¼Œæå‡æ¨ç†èƒ½åŠ›ï¼",
                    "desc": "å¼ºåŒ–å­¦ä¹ ä¸è‡ªé€‚åº”å¯éªŒè¯ç¯å¢ƒï¼ˆRLVEï¼‰é€šè¿‡åŠ¨æ€è°ƒæ•´é—®é¢˜éš¾åº¦æ¥æå‡è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œè¶…è¶Šäº†é™æ€ç¯å¢ƒå’Œä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒã€‚RLVEä½¿ç”¨å¯éªŒè¯çš„ç¯å¢ƒï¼Œç¨‹åºæ€§ç”Ÿæˆé—®é¢˜å¹¶æä¾›ç®—æ³•å¯éªŒè¯çš„å¥–åŠ±ï¼Œä»è€Œæ‰©å±•äº†è¯­è¨€æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ ã€‚ä¸é™æ€æ•°æ®åˆ†å¸ƒç›¸æ¯”ï¼ŒRLVEèƒ½å¤Ÿæ ¹æ®ç­–ç•¥æ¨¡å‹çš„èƒ½åŠ›åŠ¨æ€è°ƒæ•´é—®é¢˜éš¾åº¦åˆ†å¸ƒï¼Œé¿å…äº†å­¦ä¹ ä¿¡å·æ¶ˆå¤±çš„é—®é¢˜ã€‚é€šè¿‡RLVE-Gymï¼Œæˆ‘ä»¬å±•ç¤ºäº†ç¯å¢ƒæ‰©å±•èƒ½å¤ŸæŒç»­æ”¹å–„å¯æ¨å¹¿çš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶åœ¨å¤šä¸ªæ¨ç†åŸºå‡†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.07025",
            "title": "Llama-Embed-Nemotron-8B: A Universal Text Embedding Model for\n  Multilingual and Cross-Lingual Tasks",
            "url": "https://huggingface.co/papers/2511.07025",
            "abstract": "A fully open-source text embedding model achieves state-of-the-art performance across embedding tasks, particularly in multilingual scenarios, through a novel data mix and detailed ablation studies.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce llama-embed-nemotron-8b, an open-weights text embedding model that achieves state-of-the-art performance on the Multilingual Massive Text Embedding Benchmark (MMTEB) leaderboard as of October 21, 2025. While recent models show strong performance, their training data or methodologies are often not fully disclosed. We aim to address this by developing a fully open-source model, publicly releasing its weights and detailed ablation studies, and planning to share the curated training datasets. Our model demonstrates superior performance across all major embedding tasks -- including retrieval, classification and semantic textual similarity (STS) -- and excels in challenging multilingual scenarios, such as low-resource languages and cross-lingual setups. This state-of-the-art performance is driven by a novel data mix of 16.1 million query-document pairs, split between 7.7 million samples from public datasets and 8.4 million synthetically generated examples from various open-weight LLMs. One of our key contributions is a detailed ablation study analyzing core design choices, including a comparison of contrastive loss implementations, an evaluation of synthetic data generation (SDG) strategies, and the impact of model merging. The llama-embed-nemotron-8b is an instruction-aware model, supporting user-defined instructions to enhance performance for specific use-cases. This combination of top-tier performance, broad applicability, and user-driven flexibility enables it to serve as a universal text embedding solution.",
            "score": 11,
            "issue_id": 1,
            "pub_date": "2025-11-10",
            "pub_date_card": {
                "ru": "10 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 10",
                "zh": "11æœˆ10æ—¥"
            },
            "hash": "0eba4b504cf8021b",
            "authors": [
                "Yauhen Babakhin",
                "Radek Osmulski",
                "Ronay Ak",
                "Gabriel Moreira",
                "Mengyao Xu",
                "Benedikt Schifferer",
                "Bo Liu",
                "Even Oldridge"
            ],
            "affiliations": [
                "NVIDIA"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.07025.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#open_source",
                    "#low_resource",
                    "#data",
                    "#training",
                    "#dataset",
                    "#multilingual",
                    "#synthetic"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ², Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² llama-embed-nemotron-8b, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ MMTEB Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼Ñƒ ÑĞ¾ÑÑ‚Ğ°Ğ²Ñƒ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 16.1 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ğ¿Ğ°Ñ€ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ-Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ²ÑĞµÑ… Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ğ¾Ğ¸ÑĞº Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºÑƒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑÑ…Ğ¾Ğ¶ĞµÑÑ‚Ğ¸, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€ĞµÑÑƒÑ€ÑĞ½Ñ‹Ñ… Ğ¸ ĞºÑ€Ğ¾ÑÑ-Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ, ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞŸĞ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ¾Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²ĞµÑĞ°Ğ¼Ğ¸, ĞºĞ¾Ğ´Ğ¾Ğ¼ Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Unlocking Multilingual Mastery with Open-Source Embeddings",
                    "desc": "The paper presents llama-embed-nemotron-8b, a fully open-source text embedding model that sets new records in performance on the Multilingual Massive Text Embedding Benchmark (MMTEB). It utilizes a unique data mix of 16.1 million query-document pairs, combining public datasets and synthetically generated examples to enhance its multilingual capabilities. The authors conduct detailed ablation studies to evaluate various design choices, including loss functions and data generation strategies, which contribute to the model's effectiveness. This model not only excels in traditional embedding tasks like retrieval and classification but also adapts to user-defined instructions, making it a versatile tool for diverse applications."
                },
                "zh": {
                    "title": "å¼€æºæ–‡æœ¬åµŒå…¥æ¨¡å‹ï¼Œæ€§èƒ½å“è¶Šï¼",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§å®Œå…¨å¼€æºçš„æ–‡æœ¬åµŒå…¥æ¨¡å‹ llama-embed-nemotron-8bï¼Œè¯¥æ¨¡å‹åœ¨å¤šè¯­è¨€å¤§è§„æ¨¡æ–‡æœ¬åµŒå…¥åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ã€‚é€šè¿‡åˆ›æ–°çš„æ•°æ®æ··åˆæ–¹æ³•å’Œè¯¦ç»†çš„æ¶ˆèç ”ç©¶ï¼Œè¯¥æ¨¡å‹åœ¨æ£€ç´¢ã€åˆ†ç±»å’Œè¯­ä¹‰æ–‡æœ¬ç›¸ä¼¼æ€§ç­‰ä¸»è¦åµŒå…¥ä»»åŠ¡ä¸­å‡è¡¨ç°ä¼˜å¼‚ã€‚æˆ‘ä»¬å…¬å¼€äº†æ¨¡å‹çš„æƒé‡å’Œè®­ç»ƒæ•°æ®é›†ï¼Œç¡®ä¿é€æ˜æ€§å’Œå¯é‡å¤æ€§ã€‚è¯¥æ¨¡å‹æ”¯æŒç”¨æˆ·å®šä¹‰çš„æŒ‡ä»¤ï¼Œå¢å¼ºäº†åœ¨ç‰¹å®šç”¨ä¾‹ä¸­çš„æ€§èƒ½ï¼Œæˆä¸ºä¸€ç§é€šç”¨çš„æ–‡æœ¬åµŒå…¥è§£å†³æ–¹æ¡ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.06449",
            "title": "FLEX: Continuous Agent Evolution via Forward Learning from Experience",
            "url": "https://huggingface.co/papers/2511.06449",
            "abstract": "FLEX, a gradient-free learning paradigm, enables Large Language Model agents to continuously evolve through experience, improving performance in tasks like mathematical reasoning, chemical retrosynthesis, and protein fitness prediction.  \t\t\t\t\tAI-generated summary \t\t\t\t Autonomous agents driven by Large Language Models (LLMs) have revolutionized reasoning and problem-solving but remain static after training, unable to grow with experience as intelligent beings do during deployment. We introduce Forward Learning with EXperience (FLEX), a gradient-free learning paradigm that enables LLM agents to continuously evolve through accumulated experience. Specifically, FLEX cultivates scalable and inheritable evolution by constructing a structured experience library through continual reflection on successes and failures during interaction with the environment. FLEX delivers substantial improvements on mathematical reasoning, chemical retrosynthesis, and protein fitness prediction (up to 23% on AIME25, 10% on USPTO50k, and 14% on ProteinGym). We further identify a clear scaling law of experiential growth and the phenomenon of experience inheritance across agents, marking a step toward scalable and inheritable continuous agent evolution. Project Page: https://flex-gensi-thuair.github.io.",
            "score": 11,
            "issue_id": 1,
            "pub_date": "2025-11-09",
            "pub_date_card": {
                "ru": "9 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 9",
                "zh": "11æœˆ9æ—¥"
            },
            "hash": "c9a6f0c59dafe929",
            "authors": [
                "Zhicheng Cai",
                "Xinyuan Guo",
                "Yu Pei",
                "Jiangtao Feng",
                "Jinsong Su",
                "Jiangjie Chen",
                "Ya-Qin Zhang",
                "Wei-Ying Ma",
                "Mingxuan Wang",
                "Hao Zhou"
            ],
            "affiliations": [
                "ByteDance Seed",
                "Institute for AI Industry Research (AIR), Tsinghua University",
                "SIA-Lab of Tsinghua AIR and ByteDance Seed",
                "School of Informatics, Xiamen University",
                "Shanghai Artificial Intelligence Laboratory",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.06449.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#benchmark",
                    "#optimization",
                    "#agents",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ³ĞµĞ½Ñ‚Ñ‹ LLM ÑƒÑ‡Ğ°Ñ‚ÑÑ Ğ±ĞµĞ· Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğµ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ¿Ñ‹Ñ‚Ğ°",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° FLEX (Forward Learning with EXperience) â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒÑÑ Ñ‡ĞµÑ€ĞµĞ· Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¾Ğ¿Ñ‹Ñ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ğ¾Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºÑƒ Ğ¾Ğ¿Ñ‹Ñ‚Ğ° Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ÑƒÑĞ¿ĞµÑ…Ğ¾Ğ² Ğ¸ Ğ½ĞµÑƒĞ´Ğ°Ñ‡, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ…Ğ¸Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² Ğ±ĞµĞ»ĞºĞ¾Ğ² (Ğ´Ğ¾ 23% Ğ½Ğ° AIME25, 10% Ğ½Ğ° USPTO50k Ğ¸ 14% Ğ½Ğ° ProteinGym). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ¾ÑÑ‚Ğ° Ğ¸ Ñ„ĞµĞ½Ğ¾Ğ¼ĞµĞ½ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ Ğ¾Ğ¿Ñ‹Ñ‚Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹ Ğ´Ğ»Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ¹ Ğ¸ Ğ½Ğ°ÑĞ»ĞµĞ´ÑƒĞµĞ¼Ğ¾Ğ¹ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM."
                },
                "en": {
                    "title": "Empowering AI Agents to Learn and Evolve Continuously with FLEX",
                    "desc": "FLEX is a new learning method that allows Large Language Model (LLM) agents to improve their skills over time without needing traditional gradient-based training. This approach focuses on building a library of experiences, where agents learn from their successes and failures during interactions. By using FLEX, these agents can enhance their performance in complex tasks such as mathematical reasoning and chemical synthesis. The research shows that this method leads to significant performance gains and establishes a framework for continuous learning and evolution in AI agents."
                },
                "zh": {
                    "title": "FLEXï¼šè®©æ™ºèƒ½ä½“é€šè¿‡ç»éªŒä¸æ–­è¿›åŒ–",
                    "desc": "FLEXæ˜¯ä¸€ç§æ— æ¢¯åº¦å­¦ä¹ èŒƒå¼ï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†èƒ½å¤Ÿé€šè¿‡ç»éªŒä¸æ–­è¿›åŒ–ã€‚ä¸ä¼ ç»Ÿçš„é™æ€è®­ç»ƒä¸åŒï¼ŒFLEXé€šè¿‡æ„å»ºç»“æ„åŒ–çš„ç»éªŒåº“ï¼Œä¿ƒè¿›ä»£ç†åœ¨ä¸ç¯å¢ƒäº’åŠ¨ä¸­åæ€æˆåŠŸä¸å¤±è´¥ï¼Œä»è€Œå®ç°å¯æ‰©å±•å’Œå¯ç»§æ‰¿çš„è¿›åŒ–ã€‚è¯¥æ–¹æ³•åœ¨æ•°å­¦æ¨ç†ã€åŒ–å­¦é€†åˆæˆå’Œè›‹ç™½è´¨é€‚åº”æ€§é¢„æµ‹ç­‰ä»»åŠ¡ä¸Šæ˜¾è‘—æé«˜äº†æ€§èƒ½ã€‚FLEXè¿˜æ­ç¤ºäº†ç»éªŒå¢é•¿çš„æ˜ç¡®è§„æ¨¡æ³•åˆ™å’Œä»£ç†ä¹‹é—´çš„ç»éªŒç»§æ‰¿ç°è±¡ï¼Œæ ‡å¿—ç€å‘å¯æ‰©å±•å’Œå¯ç»§æ‰¿çš„è¿ç»­ä»£ç†è¿›åŒ–è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.06194",
            "title": "NURBGen: High-Fidelity Text-to-CAD Generation through LLM-Driven NURBS\n  Modeling",
            "url": "https://huggingface.co/papers/2511.06194",
            "abstract": "NURBGen generates high-fidelity 3D CAD models from text using Non-Uniform Rational B-Splines, outperforming existing methods in geometric fidelity and dimensional accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating editable 3D CAD models from natural language remains challenging, as existing text-to-CAD systems either produce meshes or rely on scarce design-history data. We present NURBGen, the first framework to generate high-fidelity 3D CAD models directly from text using Non-Uniform Rational B-Splines (NURBS). To achieve this, we fine-tune a large language model (LLM) to translate free-form texts into JSON representations containing NURBS surface parameters (i.e, control points, knot vectors, degrees, and rational weights) which can be directly converted into BRep format using Python. We further propose a hybrid representation that combines untrimmed NURBS with analytic primitives to handle trimmed surfaces and degenerate regions more robustly, while reducing token complexity. Additionally, we introduce partABC, a curated subset of the ABC dataset consisting of individual CAD components, annotated with detailed captions using an automated annotation pipeline. NURBGen demonstrates strong performance on diverse prompts, surpassing prior methods in geometric fidelity and dimensional accuracy, as confirmed by expert evaluations. Code and dataset will be released publicly.",
            "score": 11,
            "issue_id": 1,
            "pub_date": "2025-11-09",
            "pub_date_card": {
                "ru": "9 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 9",
                "zh": "11æœˆ9æ—¥"
            },
            "hash": "eae8f3eb93752e5b",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#dataset",
                    "#3d",
                    "#training"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "ĞÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ° Ğº ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸: NURBS-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ CAD-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° NURBGen, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ CAD-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ B-ÑĞ¿Ğ»Ğ°Ğ¹Ğ½Ñ‹ (NURBS). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ² JSON-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² NURBS-Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚ĞµĞ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾Ñ‡ĞºĞ¸, Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ñ‹ ÑƒĞ·Ğ»Ğ¾Ğ² Ğ¸ Ñ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²ĞµÑĞ°. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ°Ñ Ğ½ĞµĞ¿Ğ¾Ğ´Ñ€ĞµĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ NURBS Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ€Ğ¾Ğ±ÑƒÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ğ¾Ğ´Ñ€ĞµĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¾ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¾Ğ¹."
                },
                "en": {
                    "title": "Transforming Text to Precise 3D CAD Models with NURBGen",
                    "desc": "NURBGen is a novel framework that generates high-quality 3D CAD models from text descriptions using Non-Uniform Rational B-Splines (NURBS). It fine-tunes a large language model to convert natural language into JSON representations that specify NURBS parameters, enabling direct conversion to BRep format. The framework also introduces a hybrid representation that effectively combines NURBS with analytic primitives to improve handling of complex surfaces. NURBGen outperforms existing text-to-CAD systems in terms of geometric fidelity and dimensional accuracy, as validated by expert assessments."
                },
                "zh": {
                    "title": "ä»æ–‡æœ¬ç”Ÿæˆé«˜ä¿çœŸ 3D CAD æ¨¡å‹çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "NURBGen æ˜¯ä¸€ä¸ªæ–°æ¡†æ¶ï¼Œå¯ä»¥ä»æ–‡æœ¬ç”Ÿæˆé«˜ä¿çœŸåº¦çš„ 3D CAD æ¨¡å‹ï¼Œä½¿ç”¨éå‡åŒ€æœ‰ç† B æ ·æ¡ï¼ˆNURBSï¼‰æŠ€æœ¯ã€‚ä¸ç°æœ‰çš„æ–‡æœ¬åˆ° CAD ç³»ç»Ÿä¸åŒï¼ŒNURBGen ç›´æ¥å°†è‡ªç„¶è¯­è¨€è½¬æ¢ä¸ºåŒ…å« NURBS è¡¨é¢å‚æ•°çš„ JSON è¡¨ç¤ºã€‚è¯¥æ–¹æ³•é€šè¿‡å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹æ¥å®ç°ï¼Œèƒ½å¤Ÿç”Ÿæˆå¯ç¼–è¾‘çš„ CAD æ¨¡å‹ï¼Œå…·æœ‰æ›´é«˜çš„å‡ ä½•ä¿çœŸåº¦å’Œå°ºå¯¸å‡†ç¡®æ€§ã€‚NURBGen åœ¨å¤šç§æç¤ºä¸‹è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„æ–¹æ³•ï¼Œä»£ç å’Œæ•°æ®é›†å°†å…¬å¼€å‘å¸ƒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.05933",
            "title": "Reinforcement Learning Improves Traversal of Hierarchical Knowledge in LLMs",
            "url": "https://huggingface.co/papers/2511.05933",
            "abstract": "Reinforcement learning enhances language models' ability to recall hierarchical knowledge without degrading memorized facts, as evidenced by improved performance on structured prompting and deep-retrieval tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) is often credited with improving language model reasoning and generalization at the expense of degrading memorized knowledge. We challenge this narrative by observing that RL-enhanced models consistently outperform their base and supervised fine-tuned (SFT) counterparts on pure knowledge recall tasks, particularly those requiring traversal of hierarchical, structured knowledge (e.g., medical codes). We hypothesize these gains stem not from newly acquired data, but from improved procedural skills in navigating and searching existing knowledge hierarchies within the model parameters. To support this hypothesis, we show that structured prompting, which explicitly guides SFTed models through hierarchical traversal, recovers most of the performance gap (reducing 24pp to 7pp on MedConceptsQA for DeepSeek-V3/R1). We further find that while prompting improves final-answer accuracy, RL-enhanced models retain superior ability to recall correct procedural paths on deep-retrieval tasks. Finally our layer-wise internal activation analysis reveals that while factual representations (e.g., activations for the statement \"code 57.95 refers to urinary infection\") maintain high cosine similarity between SFT and RL models, query representations (e.g., \"what is code 57.95\") diverge noticeably, indicating that RL primarily transforms how models traverse knowledge rather than the knowledge representation itself.",
            "score": 7,
            "issue_id": 1,
            "pub_date": "2025-11-08",
            "pub_date_card": {
                "ru": "8 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 8",
                "zh": "11æœˆ8æ—¥"
            },
            "hash": "056ea072f9097921",
            "authors": [
                "Renfei Zhang",
                "Manasa Kaniselvan",
                "Niloofar Mireshghallah"
            ],
            "affiliations": [
                "ETH Zurich",
                "FAIR at Meta",
                "Simon Fraser University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.05933.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#interpretability",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ ÑƒÑ‡Ğ¸Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸ÑĞ¼ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ñ‚ÑŒ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ RL-ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¸ supervised fine-tuned Ğ²ĞµÑ€ÑĞ¸Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ‡Ğ¸ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸ÑĞ¼. ĞĞ½Ğ¸ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ½Ğµ Ğ¸Ğ·-Ğ·Ğ° ÑƒÑĞ²Ğ¾ĞµĞ½Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ° Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ RL Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ± Ğ¾Ğ±Ñ…Ğ¾Ğ´Ğ° Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ğ¹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ° Ğ½Ğµ ÑĞ°Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Reinforcement Learning: Enhancing Knowledge Recall in Language Models",
                    "desc": "This paper explores how reinforcement learning (RL) can improve language models' ability to recall structured knowledge without losing their memorized facts. The authors demonstrate that RL-enhanced models outperform traditional supervised fine-tuned models on tasks that require navigating hierarchical information, such as medical coding. They suggest that the improvements come from better procedural skills in searching through existing knowledge rather than acquiring new data. Additionally, their analysis shows that while the factual knowledge remains similar between models, the way queries are processed changes significantly with RL, enhancing the model's retrieval capabilities."
                },
                "zh": {
                    "title": "å¼ºåŒ–å­¦ä¹ æå‡è¯­è¨€æ¨¡å‹çš„çŸ¥è¯†å›å¿†èƒ½åŠ›",
                    "desc": "å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¢å¼ºäº†è¯­è¨€æ¨¡å‹åœ¨å›å¿†å±‚æ¬¡çŸ¥è¯†æ–¹é¢çš„èƒ½åŠ›ï¼Œè€Œä¸ä¼šé™ä½å·²è®°å¿†äº‹å®çš„å‡†ç¡®æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç»è¿‡å¼ºåŒ–å­¦ä¹ çš„æ¨¡å‹åœ¨çŸ¥è¯†å›å¿†ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äºåŸºç¡€æ¨¡å‹å’Œç›‘ç£å¾®è°ƒæ¨¡å‹ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦éå†å±‚æ¬¡ç»“æ„çŸ¥è¯†çš„ä»»åŠ¡ä¸­ã€‚æˆ‘ä»¬è®¤ä¸ºè¿™äº›æå‡å¹¶éæºäºæ–°æ•°æ®çš„è·å–ï¼Œè€Œæ˜¯æ¨¡å‹åœ¨å¯¼èˆªå’Œæœç´¢ç°æœ‰çŸ¥è¯†å±‚æ¬¡æ–¹é¢çš„æŠ€èƒ½æé«˜ã€‚é€šè¿‡ç»“æ„åŒ–æç¤ºï¼Œæˆ‘ä»¬å‘ç°å¯ä»¥æ˜¾è‘—ç¼©å°æ€§èƒ½å·®è·ï¼Œè¿›ä¸€æ­¥åˆ†ææ˜¾ç¤ºï¼Œå¼ºåŒ–å­¦ä¹ ä¸»è¦æ”¹å˜äº†æ¨¡å‹éå†çŸ¥è¯†çš„æ–¹å¼ï¼Œè€Œä¸æ˜¯çŸ¥è¯†æœ¬èº«çš„è¡¨ç¤ºã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.04285",
            "title": "RLoop: An Self-Improving Framework for Reinforcement Learning with\n  Iterative Policy Initialization",
            "url": "https://huggingface.co/papers/2511.04285",
            "abstract": "RLoop, a self-improving framework using iterative policy initialization and Rejection-sampling Fine-Tuning, mitigates overfitting and enhances generalization in Reinforcement Learning for Verifiable Rewards.  \t\t\t\t\tAI-generated summary \t\t\t\t While Reinforcement Learning for Verifiable Rewards (RLVR) is powerful for training large reasoning models, its training dynamics harbor a critical challenge: RL overfitting, where models gain training rewards but lose generalization. Our analysis reveals this is driven by policy over-specialization and catastrophic forgetting of diverse solutions generated during training. Standard optimization discards this valuable inter-step policy diversity. To address this, we introduce RLoop, a self-improving framework built on iterative policy initialization. RLoop transforms the standard training process into a virtuous cycle: it first uses RL to explore the solution space from a given policy, then filters the successful trajectories to create an expert dataset. This dataset is used via Rejection-sampling Fine-Tuning (RFT) to refine the initial policy, creating a superior starting point for the next iteration. This loop of exploration and exploitation via iterative re-initialization effectively converts transient policy variations into robust performance gains. Our experiments show RLoop mitigates forgetting and substantially improves generalization, boosting average accuracy by 9% and pass@32 by over 15% compared to vanilla RL.",
            "score": 7,
            "issue_id": 1,
            "pub_date": "2025-11-06",
            "pub_date_card": {
                "ru": "6 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 6",
                "zh": "11æœˆ6æ—¥"
            },
            "hash": "2933ca6bff910550",
            "authors": [
                "Zeng Zhiyuan",
                "Jiashuo Liu",
                "Zhangyue Yin",
                "Ge Zhang",
                "Wenhao Huang",
                "Xipeng Qiu"
            ],
            "affiliations": [
                "Fudan University",
                "M-A-P",
                "Shanghai Innovation Institute"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.04285.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#rl",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ¦Ğ¸ĞºĞ»Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ: Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ",
                    "desc": "RLoop â€” ÑÑ‚Ğ¾ ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¹ÑÑ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ†Ğ¸ĞºĞ»Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ‡ĞµÑ€ĞµĞ´ÑƒĞµÑ‚ Ñ„Ğ°Ğ·Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· RL Ğ¸ Ñ„Ğ°Ğ·Ñ‹ ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Rejection-sampling Fine-Tuning Ğ½Ğ° Ğ¾Ñ‚Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑÑ…. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ‚ÑĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ ĞºĞ°Ñ‚Ğ°ÑÑ‚Ñ€Ğ¾Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ³Ğ¸Ğ¿ĞµÑ€-ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 9% Ğ¸ pass@32 Ğ½Ğ° 15% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼."
                },
                "en": {
                    "title": "RLoop: Enhancing Reinforcement Learning Through Iterative Policy Improvement",
                    "desc": "RLoop is a novel framework designed to improve Reinforcement Learning (RL) by addressing the issue of overfitting, which occurs when models perform well on training data but poorly on new data. It does this by implementing iterative policy initialization and Rejection-sampling Fine-Tuning (RFT), which helps maintain a diverse set of policies during training. The framework creates a cycle where successful training trajectories are used to refine the policy, leading to better generalization and performance. Experiments demonstrate that RLoop significantly enhances accuracy and reduces the risk of forgetting previously learned solutions."
                },
                "zh": {
                    "title": "RLoopï¼šå¼ºåŒ–å­¦ä¹ ä¸­çš„è‡ªæˆ‘æ”¹è¿›ä¸æ³›åŒ–æå‡",
                    "desc": "RLoopæ˜¯ä¸€ä¸ªè‡ªæˆ‘æ”¹è¿›çš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡è¿­ä»£ç­–ç•¥åˆå§‹åŒ–å’Œæ‹’ç»é‡‡æ ·å¾®è°ƒæ¥å‡è½»å¼ºåŒ–å­¦ä¹ ä¸­çš„è¿‡æ‹Ÿåˆé—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡æ¢ç´¢è§£å†³æ–¹æ¡ˆç©ºé—´å¹¶è¿‡æ»¤æˆåŠŸçš„è½¨è¿¹ï¼Œåˆ›å»ºä¸€ä¸ªä¸“å®¶æ•°æ®é›†ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚RLoopå°†æ ‡å‡†è®­ç»ƒè¿‡ç¨‹è½¬å˜ä¸ºä¸€ä¸ªè‰¯æ€§å¾ªç¯ï¼Œä½¿å¾—æ¯æ¬¡è¿­ä»£éƒ½èƒ½åœ¨åˆå§‹ç­–ç•¥çš„åŸºç¡€ä¸Šè¿›è¡Œæ”¹è¿›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRLoopæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„å‡†ç¡®æ€§å’Œé€šè¿‡ç‡ï¼Œå‡å°‘äº†é—å¿˜ç°è±¡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.05705",
            "title": "Long Grounded Thoughts: Distilling Compositional Visual Reasoning Chains at Scale",
            "url": "https://huggingface.co/papers/2511.05705",
            "abstract": "A new reasoning data generation framework creates a large-scale vision-centric dataset with over 1M synthetic questions, enhancing performance across various benchmarks and improving cross-modality transfer.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent progress in multimodal reasoning has been driven largely by undisclosed datasets and proprietary data synthesis recipes, leaving open questions about how to systematically build large-scale, vision-centric reasoning datasets, particularly for tasks that go beyond visual math. In this work, we introduce a new reasoning data generation framework spanning diverse skills and levels of complexity with over 1M high-quality synthetic vision-centric questions. The dataset also includes preference data and instruction prompts supporting both offline and online RL. Our synthesis framework proceeds in two stages: (1) scale; and (2) complexity. Reasoning traces are then synthesized through a two-stage process that leverages VLMs and reasoning LLMs, producing CoT traces for VLMs that capture the richness and diverse cognitive behaviors found in frontier reasoning models. Remarkably, we show that finetuning Qwen2.5-VL-7B on our data outperforms all open-data baselines across all evaluated vision-centric benchmarks, and even surpasses strong closed-data models such as MiMo-VL-7B-RL on V* Bench, CV-Bench and MMStar-V. Perhaps most surprising, despite being entirely vision-centric, our data transfers positively to text-only reasoning (MMLU-Pro) and audio reasoning (MMAU), demonstrating its effectiveness. Similarly, despite not containing videos or embodied visual data, we observe notable gains when evaluating on a single-evidence embodied QA benchmark (NiEH). Finally, we use our data to analyze the entire VLM post-training pipeline. Our empirical analysis highlights that (i) SFT on high-quality data with non-linear reasoning traces is essential for effective online RL, (ii) staged offline RL matches online RL's performance while reducing compute demands, and (iii) careful SFT on high quality data can substantially improve out-of-domain, cross-modality transfer.",
            "score": 6,
            "issue_id": 1,
            "pub_date": "2025-11-07",
            "pub_date_card": {
                "ru": "7 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 7",
                "zh": "11æœˆ7æ—¥"
            },
            "hash": "afe423cea99de0e2",
            "authors": [
                "David Acuna",
                "Chao-Han Huck Yang",
                "Yuntian Deng",
                "Jaehun Jung",
                "Ximing Lu",
                "Prithviraj Ammanabrolu",
                "Hyunwoo Kim",
                "Yuan-Hong Liao",
                "Yejin Choi"
            ],
            "affiliations": [
                "NVIDIA",
                "UCSD",
                "University of Toronto",
                "University of Waterloo"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.05705.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#transfer_learning",
                    "#cv",
                    "#multimodal",
                    "#training",
                    "#rlhf",
                    "#dataset",
                    "#rl",
                    "#reasoning",
                    "#synthetic"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑÑÑ‚ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 1 Ğ¼Ğ»Ğ½ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ·Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ. Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ² Ğ´Ğ²Ğ° ÑÑ‚Ğ°Ğ¿Ğ°: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¿Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ñƒ, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Vision Language Models Ğ¸ reasoning LLM Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ (Chain-of-Thought). Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ°Ğº Ğ² Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½, Ñ‚Ğ°Ğº Ğ¸ Ğ² Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ°Ñ…, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° ÑÑ‚Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ²ÑĞµÑ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¸ Ğ½ĞµĞ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ½Ğ¾ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°."
                },
                "en": {
                    "title": "Revolutionizing Reasoning with 1M Synthetic Vision Questions",
                    "desc": "This paper presents a new framework for generating reasoning data, creating a large-scale dataset with over 1 million synthetic vision-centric questions. The dataset is designed to enhance performance in multimodal reasoning tasks, particularly those that extend beyond simple visual math. The authors demonstrate that fine-tuning a model called Qwen2.5-VL-7B on this dataset outperforms existing open-data and even some closed-data models across various benchmarks. Additionally, the dataset shows positive transfer effects to text-only and audio reasoning tasks, indicating its broad applicability in machine learning."
                },
                "zh": {
                    "title": "åˆ›æ–°æ¨ç†æ•°æ®ç”Ÿæˆï¼Œæå‡è·¨æ¨¡æ€æ€§èƒ½",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¨ç†æ•°æ®ç”Ÿæˆæ¡†æ¶ï¼Œåˆ›å»ºäº†ä¸€ä¸ªåŒ…å«è¶…è¿‡100ä¸‡åˆæˆè§†è§‰ä¸­å¿ƒé—®é¢˜çš„å¤§è§„æ¨¡æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†ä¸ä»…å¢å¼ºäº†å¤šç§åŸºå‡†æµ‹è¯•çš„æ€§èƒ½ï¼Œè¿˜æ”¹å–„äº†è·¨æ¨¡æ€è¿ç§»èƒ½åŠ›ã€‚é€šè¿‡ä¸¤é˜¶æ®µçš„åˆæˆè¿‡ç¨‹ï¼Œåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å’Œæ¨ç†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œç”Ÿæˆäº†ä¸°å¯Œçš„æ¨ç†è½¨è¿¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºè¯¥æ•°æ®é›†å¾®è°ƒçš„æ¨¡å‹åœ¨è§†è§‰ä¸­å¿ƒåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºæ‰€æœ‰å¼€æ”¾æ•°æ®åŸºçº¿ï¼Œç”šè‡³è¶…è¿‡äº†ä¸€äº›å¼ºå¤§çš„é—­æºæ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.03317",
            "title": "Diffusion-SDPO: Safeguarded Direct Preference Optimization for Diffusion\n  Models",
            "url": "https://huggingface.co/papers/2511.03317",
            "abstract": "Diffusion-SDPO improves text-to-image generation quality by adaptively scaling the loser gradient in preference optimization, ensuring the preferred output's error does not increase.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-image diffusion models deliver high-quality images, yet aligning them with human preferences remains challenging. We revisit diffusion-based Direct Preference Optimization (DPO) for these models and identify a critical pathology: enlarging the preference margin does not necessarily improve generation quality. In particular, the standard Diffusion-DPO objective can increase the reconstruction error of both winner and loser branches. Consequently, degradation of the less-preferred outputs can become sufficiently severe that the preferred branch is also adversely affected even as the margin grows. To address this, we introduce Diffusion-SDPO, a safeguarded update rule that preserves the winner by adaptively scaling the loser gradient according to its alignment with the winner gradient. A first-order analysis yields a closed-form scaling coefficient that guarantees the error of the preferred output is non-increasing at each optimization step. Our method is simple, model-agnostic, broadly compatible with existing DPO-style alignment frameworks and adds only marginal computational overhead. Across standard text-to-image benchmarks, Diffusion-SDPO delivers consistent gains over preference-learning baselines on automated preference, aesthetic, and prompt alignment metrics. Code is publicly available at https://github.com/AIDC-AI/Diffusion-SDPO.",
            "score": 6,
            "issue_id": 1,
            "pub_date": "2025-11-05",
            "pub_date_card": {
                "ru": "5 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 5",
                "zh": "11æœˆ5æ—¥"
            },
            "hash": "37fbb5e8cc8e00b0",
            "authors": [
                "Minghao Fu",
                "Guo-Hua Wang",
                "Tianyu Cui",
                "Qing-Guo Chen",
                "Zhao Xu",
                "Weihua Luo",
                "Kaifu Zhang"
            ],
            "affiliations": [
                "Alibaba International Digital Commerce Group",
                "National Key Laboratory for Novel Software Technology, Nanjing University",
                "School of Artificial Intelligence, Nanjing University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.03317.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#alignment",
                    "#diffusion",
                    "#multimodal",
                    "#training",
                    "#rlhf"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¾Ñ…Ñ€Ğ°Ğ½Ğ¾Ğ¹ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¶ĞµĞ»Ğ°ĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ°",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Diffusion-SDPO Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Direct Preference Optimization (DPO): ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ°Ñ€Ğ¶Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ğ½Ğµ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ²ĞµÑÑ‚Ğ¸ Ğº Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ĞµĞ¸Ñ… Ğ²ĞµÑ‚Ğ²ĞµĞ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ° Ğ½ĞµĞ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ° Ğ² ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ ĞµĞ³Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ Ñ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¾ÑÑ‚, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµĞ½ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ½ĞµÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ° Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Image Quality with Adaptive Preference Scaling",
                    "desc": "The paper presents Diffusion-SDPO, a method that enhances text-to-image generation by improving how models align with human preferences. It identifies a problem in existing Direct Preference Optimization (DPO) methods where increasing the preference margin can worsen the quality of generated images. To solve this, Diffusion-SDPO introduces a new update rule that carefully adjusts the loser gradient based on its relationship to the winner gradient, ensuring that the quality of preferred outputs does not decline. The method is easy to implement, works with various models, and shows significant improvements in image quality across multiple evaluation metrics."
                },
                "zh": {
                    "title": "è‡ªé€‚åº”ä¼˜åŒ–ï¼Œæå‡ç”Ÿæˆè´¨é‡ï¼",
                    "desc": "Diffusion-SDPOæ˜¯ä¸€ç§æ”¹è¿›æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆè´¨é‡çš„æ–¹æ³•ï¼Œé€šè¿‡è‡ªé€‚åº”è°ƒæ•´åŠ£åŠ¿æ¢¯åº¦æ¥ä¼˜åŒ–åå¥½ï¼Œç¡®ä¿ä¼˜é€‰è¾“å‡ºçš„è¯¯å·®ä¸ä¼šå¢åŠ ã€‚ç ”ç©¶å‘ç°ï¼Œç®€å•æ‰©å¤§åå¥½è¾¹é™…å¹¶ä¸ä¸€å®šèƒ½æé«˜ç”Ÿæˆè´¨é‡ï¼Œåè€Œå¯èƒ½å¯¼è‡´ä¼˜é€‰å’ŒåŠ£é€‰è¾“å‡ºçš„é‡å»ºè¯¯å·®å¢åŠ ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒDiffusion-SDPOå¼•å…¥äº†ä¸€ç§å®‰å…¨æ›´æ–°è§„åˆ™ï¼Œæ ¹æ®åŠ£åŠ¿æ¢¯åº¦ä¸ä¼˜åŠ¿æ¢¯åº¦çš„å¯¹é½ç¨‹åº¦è‡ªé€‚åº”åœ°ç¼©æ”¾åŠ£åŠ¿æ¢¯åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDiffusion-SDPOåœ¨æ–‡æœ¬åˆ°å›¾åƒçš„æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­ï¼Œç›¸è¾ƒäºä¼ ç»Ÿçš„åå¥½å­¦ä¹ æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨è‡ªåŠ¨åå¥½ã€ç¾å­¦å’Œæç¤ºå¯¹é½æŒ‡æ ‡ä¸ŠæŒç»­å–å¾—æ›´å¥½çš„æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.07413",
            "title": "DigiData: Training and Evaluating General-Purpose Mobile Control Agents",
            "url": "https://huggingface.co/papers/2511.07413",
            "abstract": "DigiData and DigiData-Bench advance mobile control agents by providing a diverse, high-quality dataset and dynamic evaluation protocols, respectively.  \t\t\t\t\tAI-generated summary \t\t\t\t AI agents capable of controlling user interfaces have the potential to transform human interaction with digital devices. To accelerate this transformation, two fundamental building blocks are essential: high-quality datasets that enable agents to achieve complex and human-relevant goals, and robust evaluation methods that allow researchers and practitioners to rapidly enhance agent performance. In this paper, we introduce DigiData, a large-scale, high-quality, diverse, multi-modal dataset designed for training mobile control agents. Unlike existing datasets, which derive goals from unstructured interactions, DigiData is meticulously constructed through comprehensive exploration of app features, resulting in greater diversity and higher goal complexity. Additionally, we present DigiData-Bench, a benchmark for evaluating mobile control agents on real-world complex tasks. We demonstrate that the commonly used step-accuracy metric falls short in reliably assessing mobile control agents and, to address this, we propose dynamic evaluation protocols and AI-powered evaluations as rigorous alternatives for agent assessment. Our contributions aim to significantly advance the development of mobile control agents, paving the way for more intuitive and effective human-device interactions.",
            "score": 5,
            "issue_id": 1,
            "pub_date": "2025-11-10",
            "pub_date_card": {
                "ru": "10 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 10",
                "zh": "11æœˆ10æ—¥"
            },
            "hash": "cacafb8e80467e4a",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#benchmark",
                    "#open_source",
                    "#agents",
                    "#multimodal",
                    "#dataset",
                    "#synthetic"
                ],
                "emoji": "ğŸ“±",
                "ru": {
                    "title": "ĞšĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° â€” Ğ¾ÑĞ½Ğ¾Ğ²Ğ° Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ DigiData â€” Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹, Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ¼, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ DigiData-Bench â€” Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ‚Ğ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ° Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑˆĞ°Ğ³Ğ°Ğ¼ Ğ½ĞµĞ°Ğ´ĞµĞºĞ²Ğ°Ñ‚Ğ½Ğ° Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². Ğ’ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ½Ğ° ÑÑ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ»Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ AI Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ñ… Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²."
                },
                "en": {
                    "title": "Empowering Mobile Control Agents with DigiData and DigiData-Bench",
                    "desc": "This paper introduces DigiData, a comprehensive dataset designed specifically for training mobile control agents, which are AI systems that interact with user interfaces. DigiData is unique because it is built from a thorough analysis of app features, leading to a more diverse and complex set of goals compared to existing datasets. Additionally, the authors present DigiData-Bench, a new evaluation framework that offers dynamic protocols for assessing the performance of these agents on real-world tasks. The paper critiques traditional evaluation metrics and proposes innovative AI-driven methods to better measure agent effectiveness, ultimately enhancing human-device interactions."
                },
                "zh": {
                    "title": "æ¨åŠ¨ç§»åŠ¨æ§åˆ¶ä»£ç†å‘å±•çš„æ–°å·¥å…·",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†DigiDataå’ŒDigiData-Benchï¼Œè¿™ä¸¤ä¸ªå·¥å…·æ—¨åœ¨æå‡ç§»åŠ¨æ§åˆ¶ä»£ç†çš„èƒ½åŠ›ã€‚DigiDataæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡ã€é«˜è´¨é‡ã€å¤šæ ·åŒ–çš„å¤šæ¨¡æ€æ•°æ®é›†ï¼Œä¸“ä¸ºè®­ç»ƒç§»åŠ¨æ§åˆ¶ä»£ç†è€Œè®¾è®¡ï¼Œå…·æœ‰æ›´é«˜çš„ç›®æ ‡å¤æ‚æ€§å’Œå¤šæ ·æ€§ã€‚DigiData-Benchåˆ™æ˜¯ä¸€ä¸ªè¯„ä¼°ç§»åŠ¨æ§åˆ¶ä»£ç†åœ¨ç°å®å¤æ‚ä»»åŠ¡ä¸­çš„åŸºå‡†ï¼Œæå‡ºäº†åŠ¨æ€è¯„ä¼°åè®®å’ŒAIé©±åŠ¨çš„è¯„ä¼°æ–¹æ³•ï¼Œä»¥æ›´å‡†ç¡®åœ°è¯„ä¼°ä»£ç†çš„è¡¨ç°ã€‚é€šè¿‡è¿™äº›è´¡çŒ®ï¼Œæœ¬æ–‡å¸Œæœ›æ¨åŠ¨ç§»åŠ¨æ§åˆ¶ä»£ç†çš„å‘å±•ï¼Œæ”¹å–„äººæœºäº¤äº’çš„ç›´è§‚æ€§å’Œæœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.07137",
            "title": "MPJudge: Towards Perceptual Assessment of Music-Induced Paintings",
            "url": "https://huggingface.co/papers/2511.07137",
            "abstract": "A novel framework MPJudge assesses music-induced paintings by integrating music features into a visual encoder using a modulation-based fusion mechanism, outperforming existing emotion recognition models.  \t\t\t\t\tAI-generated summary \t\t\t\t Music induced painting is a unique artistic practice, where visual artworks are created under the influence of music. Evaluating whether a painting faithfully reflects the music that inspired it poses a challenging perceptual assessment task. Existing methods primarily rely on emotion recognition models to assess the similarity between music and painting, but such models introduce considerable noise and overlook broader perceptual cues beyond emotion. To address these limitations, we propose a novel framework for music induced painting assessment that directly models perceptual coherence between music and visual art. We introduce MPD, the first large scale dataset of music painting pairs annotated by domain experts based on perceptual coherence. To better handle ambiguous cases, we further collect pairwise preference annotations. Building on this dataset, we present MPJudge, a model that integrates music features into a visual encoder via a modulation based fusion mechanism. To effectively learn from ambiguous cases, we adopt Direct Preference Optimization for training. Extensive experiments demonstrate that our method outperforms existing approaches. Qualitative results further show that our model more accurately identifies music relevant regions in paintings.",
            "score": 5,
            "issue_id": 1,
            "pub_date": "2025-11-10",
            "pub_date_card": {
                "ru": "10 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 10",
                "zh": "11æœˆ10æ—¥"
            },
            "hash": "198031443c510e80",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#multimodal"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ½Ñ‚ĞµĞ· Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸ Ğ¸ Ğ¶Ğ¸Ğ²Ğ¾Ğ¿Ğ¸ÑĞ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° MPJudge Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ€Ñ‚Ğ¸Ğ½, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸ĞµĞ¼ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸, Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ñ‡ĞµÑ€ĞµĞ· Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ MPD Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ², Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿ĞµÑ€Ñ†ĞµĞ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¾Ğ¹ Ğ¸ Ğ¶Ğ¸Ğ²Ğ¾Ğ¿Ğ¸ÑÑŒÑ. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ÑĞ¼Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹ Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ğµ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ñ‹ Ğ² ĞºĞ°Ñ€Ñ‚Ğ¸Ğ½Ğ°Ñ…."
                },
                "en": {
                    "title": "Harmonizing Music and Art: MPJudge Unveils New Insights in Perceptual Assessment",
                    "desc": "The paper introduces MPJudge, a new framework designed to evaluate music-induced paintings by linking music features with visual art through a modulation-based fusion mechanism. This approach improves upon traditional emotion recognition models, which often fail to capture the full range of perceptual cues. The authors also present MPD, a large dataset of music-painting pairs annotated for perceptual coherence, which aids in training the model. By utilizing Direct Preference Optimization, MPJudge effectively learns from ambiguous cases, leading to superior performance in identifying music-related elements in visual artworks."
                },
                "zh": {
                    "title": "éŸ³ä¹ä¸ç»˜ç”»çš„æ„ŸçŸ¥ä¸€è‡´æ€§è¯„ä¼°æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶MPJudgeï¼Œç”¨äºè¯„ä¼°éŸ³ä¹å¼•å‘çš„ç»˜ç”»ä½œå“ã€‚è¯¥æ¡†æ¶é€šè¿‡è°ƒåˆ¶èåˆæœºåˆ¶å°†éŸ³ä¹ç‰¹å¾æ•´åˆåˆ°è§†è§‰ç¼–ç å™¨ä¸­ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰éŸ³ä¹ä¸è§†è§‰è‰ºæœ¯ä¹‹é—´çš„æ„ŸçŸ¥ä¸€è‡´æ€§ã€‚æˆ‘ä»¬è¿˜æ„å»ºäº†ç¬¬ä¸€ä¸ªå¤§è§„æ¨¡çš„éŸ³ä¹ä¸ç»˜ç”»å¯¹æ•°æ®é›†MPDï¼Œå¹¶é€šè¿‡ä¸“å®¶æ³¨é‡Šæ¥æ ‡æ³¨æ„ŸçŸ¥ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMPJudgeåœ¨æƒ…æ„Ÿè¯†åˆ«æ¨¡å‹çš„åŸºç¡€ä¸Šï¼Œæ˜¾è‘—æé«˜äº†è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.06174",
            "title": "LUT-LLM: Efficient Large Language Model Inference with Memory-based\n  Computations on FPGAs",
            "url": "https://huggingface.co/papers/2511.06174",
            "abstract": "LUT-LLM, an FPGA accelerator, improves LLM inference efficiency by shifting computation to memory-based operations, achieving lower latency and higher energy efficiency compared to GPUs.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid progress of large language models (LLMs) has advanced numerous applications, yet efficient single-batch inference remains vital for on-device intelligence. While FPGAs offer fine-grained data control and high energy efficiency, recent GPU optimizations have narrowed their advantage, especially under arithmetic-based computation. To overcome this, we leverage FPGAs' abundant on-chip memory to shift LLM inference from arithmetic- to memory-based computation through table lookups. We present LUT-LLM, the first FPGA accelerator enabling 1B+ LLM inference via vector-quantized memory operations. Our analysis identifies activation-weight co-quantization as the most effective scheme, supported by (1) bandwidth-aware parallel centroid search, (2) efficient 2D table lookups, and (3) a spatial-temporal hybrid design minimizing data caching. Implemented on an AMD V80 FPGA for a customized Qwen 3 1.7B model, LUT-LLM achieves 1.66x lower latency than AMD MI210 and 1.72x higher energy efficiency than NVIDIA A100, scaling to 32B models with 2.16x efficiency gain over A100.",
            "score": 5,
            "issue_id": 1,
            "pub_date": "2025-11-09",
            "pub_date_card": {
                "ru": "9 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 9",
                "zh": "11æœˆ9æ—¥"
            },
            "hash": "a382a6e3f876b2da",
            "authors": [
                "Zifan He",
                "Shengyu Ye",
                "Rui Ma",
                "Yang Wang",
                "Jason Cong"
            ],
            "affiliations": [
                "Microsoft Research Asia",
                "University of California, Los Angeles"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.06174.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#small_models",
                    "#optimization"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "ĞÑ‚ Ğ°Ñ€Ğ¸Ñ„Ğ¼ĞµÑ‚Ğ¸ĞºĞ¸ Ğº Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸: FPGA Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ Ñ‡ĞµÑ€ĞµĞ· Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ¸ÑĞºĞ¸",
                    "desc": "LUT-LLM â€” ÑÑ‚Ğ¾ FPGA ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ñ€Ğ¸Ñ„Ğ¼ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ² Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ°Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½ÑƒÑ Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½ÑƒÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ FPGA. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½ÑƒÑ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ Ğ²ĞµÑĞ¾Ğ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ´Ğ²ÑƒĞ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ² Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ°Ñ… Ñ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹. ĞĞ° FPGA AMD V80 ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 1.66x Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ñ‡ĞµĞ¼ GPU MI210 Ğ¸ 1.72x Ğ»ÑƒÑ‡ÑˆÑƒÑ ÑĞ½ĞµÑ€Ğ³Ğ¾ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ A100. Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ´Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 32B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ° Ñ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾Ğ¼ 2.16x Ğ² ÑĞ½ĞµÑ€Ğ³Ğ¾ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ°Ğ´ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ GPU."
                },
                "en": {
                    "title": "Accelerating LLM Inference with Memory-Based FPGA Operations",
                    "desc": "LUT-LLM is an FPGA accelerator designed to enhance the efficiency of large language model (LLM) inference by utilizing memory-based operations instead of traditional arithmetic computations. This approach allows for lower latency and improved energy efficiency, making it suitable for on-device intelligence applications. The paper introduces a novel method of activation-weight co-quantization and employs techniques like bandwidth-aware parallel centroid search and efficient 2D table lookups. When tested on an AMD V80 FPGA with a customized Qwen 3 model, LUT-LLM demonstrated significant performance improvements over leading GPU alternatives."
                },
                "zh": {
                    "title": "LUT-LLMï¼šå†…å­˜è®¡ç®—æå‡å¤§è¯­è¨€æ¨¡å‹æ¨ç†æ•ˆç‡",
                    "desc": "LUT-LLMæ˜¯ä¸€ç§FPGAåŠ é€Ÿå™¨ï¼Œé€šè¿‡å°†è®¡ç®—è½¬ç§»åˆ°åŸºäºå†…å­˜çš„æ“ä½œï¼Œæ˜¾è‘—æé«˜äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†æ•ˆç‡ã€‚ä¸GPUç›¸æ¯”ï¼Œå®ƒå®ç°äº†æ›´ä½çš„å»¶è¿Ÿå’Œæ›´é«˜çš„èƒ½æ•ˆï¼Œå°¤å…¶åœ¨å•æ‰¹æ¬¡æ¨ç†ä¸­è¡¨ç°çªå‡ºã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨FPGAä¸°å¯Œçš„ç‰‡ä¸Šå†…å­˜ï¼Œé€šè¿‡æŸ¥æ‰¾è¡¨å°†æ¨ç†è¿‡ç¨‹ä»ç®—æœ¯è®¡ç®—è½¬å˜ä¸ºå†…å­˜è®¡ç®—ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œæ¿€æ´»æƒé‡å…±åŒé‡åŒ–æ˜¯æœ€æœ‰æ•ˆçš„æ–¹æ¡ˆï¼Œç»“åˆå¸¦å®½æ„ŸçŸ¥çš„å¹¶è¡Œè´¨å¿ƒæœç´¢å’Œé«˜æ•ˆçš„äºŒç»´æŸ¥æ‰¾è¡¨è®¾è®¡ï¼Œæœ€ç»ˆåœ¨AMD V80 FPGAä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.05936",
            "title": "10 Open Challenges Steering the Future of Vision-Language-Action Models",
            "url": "https://huggingface.co/papers/2511.05936",
            "abstract": "VLA models, combining vision, language, and action, are advancing through milestones like multimodality, reasoning, and safety, with trends focusing on spatial understanding and human coordination.  \t\t\t\t\tAI-generated summary \t\t\t\t Due to their ability of follow natural language instructions, vision-language-action (VLA) models are increasingly prevalent in the embodied AI arena, following the widespread success of their precursors -- LLMs and VLMs. In this paper, we discuss 10 principal milestones in the ongoing development of VLA models -- multimodality, reasoning, data, evaluation, cross-robot action generalization, efficiency, whole-body coordination, safety, agents, and coordination with humans. Furthermore, we discuss the emerging trends of using spatial understanding, modeling world dynamics, post training, and data synthesis -- all aiming to reach these milestones. Through these discussions, we hope to bring attention to the research avenues that may accelerate the development of VLA models into wider acceptability.",
            "score": 5,
            "issue_id": 1,
            "pub_date": "2025-11-08",
            "pub_date_card": {
                "ru": "8 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 8",
                "zh": "11æœˆ8æ—¥"
            },
            "hash": "84ab99f7a376398c",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#benchmark",
                    "#survey",
                    "#agents",
                    "#multimodal",
                    "#training",
                    "#robotics",
                    "#reasoning",
                    "#synthetic"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğº Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ: Ğ²ĞµÑ…Ğ¸ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ ÑÑ‚Ğ°Ğ¿Ñ‹ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ vision-language-action (VLA) Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‚ Ğ·Ñ€ĞµĞ½Ğ¸Ğµ, ÑĞ·Ñ‹Ğº Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ´Ğ»Ñ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹Ğ´ĞµĞ»ÑÑÑ‚ Ğ´ĞµÑÑÑ‚ÑŒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ²ĞµÑ… Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ: Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ, Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ¾Ñ†ĞµĞ½ĞºĞ°, Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ, ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ğ²ÑĞµĞ³Ğ¾ Ñ‚ĞµĞ»Ğ°, Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ, Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ Ğ»ÑĞ´ÑŒĞ¼Ğ¸. Ğ”Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ñ†ĞµĞ»ĞµĞ¹ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ: Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°, Ğ¿Ğ¾ÑÑ‚-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¾ Ğ½Ğ° ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ VLA Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ¸Ñ… Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Advancing VLA Models: Bridging Vision, Language, and Action",
                    "desc": "This paper explores the advancements in vision-language-action (VLA) models, which integrate visual perception, language understanding, and action execution. It identifies ten key milestones in their development, including multimodality, reasoning, and safety, highlighting the importance of these aspects for effective embodied AI. The authors also discuss emerging trends such as spatial understanding and data synthesis, which are crucial for improving the models' performance and generalization capabilities. The goal is to encourage further research that will enhance the acceptance and application of VLA models in real-world scenarios."
                },
                "zh": {
                    "title": "æ¨åŠ¨è§†è§‰-è¯­è¨€-è¡ŒåŠ¨æ¨¡å‹çš„æœªæ¥å‘å±•",
                    "desc": "æœ¬æ–‡è®¨è®ºäº†è§†è§‰-è¯­è¨€-è¡ŒåŠ¨ï¼ˆVLAï¼‰æ¨¡å‹çš„å‘å±•ï¼Œå¼ºè°ƒäº†å…¶åœ¨å¤šæ¨¡æ€ã€æ¨ç†å’Œå®‰å…¨æ€§ç­‰æ–¹é¢çš„é‡è¦é‡Œç¨‹ç¢‘ã€‚è¿™äº›æ¨¡å‹èƒ½å¤Ÿç†è§£è‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œå› è€Œåœ¨å…·èº«äººå·¥æ™ºèƒ½é¢†åŸŸè¶Šæ¥è¶Šå—åˆ°å…³æ³¨ã€‚æ–‡ç« è¿˜æ¢è®¨äº†ç©ºé—´ç†è§£ã€ä¸–ç•ŒåŠ¨æ€å»ºæ¨¡å’Œæ•°æ®åˆæˆç­‰æ–°å…´è¶‹åŠ¿ï¼Œæ—¨åœ¨æ¨åŠ¨VLAæ¨¡å‹çš„è¿›ä¸€æ­¥å‘å±•ã€‚é€šè¿‡è¿™äº›è®¨è®ºï¼Œæˆ‘ä»¬å¸Œæœ›å¼•èµ·å¯¹åŠ é€ŸVLAæ¨¡å‹ç ”ç©¶çš„å…³æ³¨ï¼Œä»¥å®ç°æ›´å¹¿æ³›çš„åº”ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.07409",
            "title": "DIMO: Diverse 3D Motion Generation for Arbitrary Objects",
            "url": "https://huggingface.co/papers/2511.07409",
            "abstract": "A generative approach extracts motion patterns from video models, embeds them into a latent space, and uses neural key point trajectories to generate diverse 3D motions from a single image.  \t\t\t\t\tAI-generated summary \t\t\t\t We present DIMO, a generative approach capable of generating diverse 3D motions for arbitrary objects from a single image. The core idea of our work is to leverage the rich priors in well-trained video models to extract the common motion patterns and then embed them into a shared low-dimensional latent space. Specifically, we first generate multiple videos of the same object with diverse motions. We then embed each motion into a latent vector and train a shared motion decoder to learn the distribution of motions represented by a structured and compact motion representation, i.e., neural key point trajectories. The canonical 3D Gaussians are then driven by these key points and fused to model the geometry and appearance. During inference time with learned latent space, we can instantly sample diverse 3D motions in a single-forward pass and support several interesting applications including 3D motion interpolation and language-guided motion generation. Our project page is available at https://linzhanm.github.io/dimo.",
            "score": 4,
            "issue_id": 1,
            "pub_date": "2025-11-10",
            "pub_date_card": {
                "ru": "10 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 10",
                "zh": "11æœˆ10æ—¥"
            },
            "hash": "4706b9fc94b55c38",
            "authors": [
                "Linzhan Mou",
                "Jiahui Lei",
                "Chen Wang",
                "Lingjie Liu",
                "Kostas Daniilidis"
            ],
            "affiliations": [
                "Archimedes, Athena RC",
                "University of Pennsylvania"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.07409.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#3d",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğº Ğ¶Ğ¸Ğ·Ğ½Ğ¸: Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… 3D-Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ğ´Ñ€Ğ°",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ DIMO â€” Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… 3D-Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¸Ñ… Ğ² Ğ¾Ğ±Ñ‰ĞµĞµ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾. Ğ”Ğ°Ğ»ĞµĞµ Ğ¾Ğ½Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ·ÑƒÑ‡Ğ°Ñ‚ÑŒ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ 3D Ğ“Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¸ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ°. ĞĞ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¼Ğ³Ğ½Ğ¾Ğ²ĞµĞ½Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ 3D-Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€ĞµÑĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "DIMO: Generating Diverse 3D Motions from a Single Image",
                    "desc": "This paper introduces DIMO, a generative model that creates diverse 3D motions for various objects using just a single image. It utilizes well-trained video models to identify common motion patterns, which are then embedded into a low-dimensional latent space. By generating multiple videos of the same object with different motions, the model learns a structured representation of these motions through neural key point trajectories. This allows for efficient sampling of diverse 3D motions during inference, enabling applications like 3D motion interpolation and language-guided motion generation."
                },
                "zh": {
                    "title": "ä»å•å›¾åƒç”Ÿæˆå¤šæ ·3Dè¿åŠ¨çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºDIMOçš„ç”Ÿæˆæ–¹æ³•ï¼Œèƒ½å¤Ÿä»å•å¼ å›¾åƒç”Ÿæˆå¤šæ ·çš„3Dè¿åŠ¨ã€‚æˆ‘ä»¬åˆ©ç”¨ç»è¿‡è‰¯å¥½è®­ç»ƒçš„è§†é¢‘æ¨¡å‹æå–å¸¸è§çš„è¿åŠ¨æ¨¡å¼ï¼Œå¹¶å°†å…¶åµŒå…¥åˆ°ä¸€ä¸ªå…±äº«çš„ä½ç»´æ½œåœ¨ç©ºé—´ä¸­ã€‚é€šè¿‡ç”ŸæˆåŒä¸€ç‰©ä½“çš„å¤šæ®µä¸åŒè¿åŠ¨è§†é¢‘ï¼Œæˆ‘ä»¬å°†æ¯ç§è¿åŠ¨åµŒå…¥ä¸ºæ½œåœ¨å‘é‡ï¼Œå¹¶è®­ç»ƒä¸€ä¸ªå…±äº«çš„è¿åŠ¨è§£ç å™¨æ¥å­¦ä¹ è¿™äº›è¿åŠ¨çš„åˆ†å¸ƒã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬å¯ä»¥åœ¨æ¨ç†æ—¶å¿«é€Ÿé‡‡æ ·å¤šæ ·çš„3Dè¿åŠ¨ï¼Œæ”¯æŒ3Dè¿åŠ¨æ’å€¼å’ŒåŸºäºè¯­è¨€çš„è¿åŠ¨ç”Ÿæˆç­‰åº”ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.07299",
            "title": "VADER: Towards Causal Video Anomaly Understanding with Relation-Aware\n  Large Language Models",
            "url": "https://huggingface.co/papers/2511.07299",
            "abstract": "VADER, an LLM-driven framework, enhances video anomaly understanding by integrating keyframe object relations and visual cues to provide detailed, causally grounded descriptions and robust question answering.  \t\t\t\t\tAI-generated summary \t\t\t\t Video anomaly understanding (VAU) aims to provide detailed interpretation and semantic comprehension of anomalous events within videos, addressing limitations of traditional methods that focus solely on detecting and localizing anomalies. However, existing approaches often neglect the deeper causal relationships and interactions between objects, which are critical for understanding anomalous behaviors. In this paper, we propose VADER, an LLM-driven framework for Video Anomaly unDErstanding, which integrates keyframe object Relation features with visual cues to enhance anomaly comprehension from video. Specifically, VADER first applies an Anomaly Scorer to assign per-frame anomaly scores, followed by a Context-AwarE Sampling (CAES) strategy to capture the causal context of each anomalous event. A Relation Feature Extractor and a COntrastive Relation Encoder (CORE) jointly model dynamic object interactions, producing compact relational representations for downstream reasoning. These visual and relational cues are integrated with LLMs to generate detailed, causally grounded descriptions and support robust anomaly-related question answering. Experiments on multiple real-world VAU benchmarks demonstrate that VADER achieves strong results across anomaly description, explanation, and causal reasoning tasks, advancing the frontier of explainable video anomaly analysis.",
            "score": 4,
            "issue_id": 1,
            "pub_date": "2025-11-10",
            "pub_date_card": {
                "ru": "10 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 10",
                "zh": "11æœˆ10æ—¥"
            },
            "hash": "e301f3b8cfe481b3",
            "authors": [
                "Ying Cheng",
                "Yu-Ho Lin",
                "Min-Hung Chen",
                "Fu-En Yang",
                "Shang-Hong Lai"
            ],
            "affiliations": [
                "NVIDIA",
                "National Tsing Hua University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.07299.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞŸĞ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° VADER, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ½Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ°Ñ… Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹. VADER Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸Ğº Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸ÑĞ²Ğ¾ĞµĞ½Ğ¸Ñ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼Ñƒ ĞºĞ°Ğ´Ñ€Ñƒ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ñ LLM Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "VADER: Enhancing Video Anomaly Understanding with Causal Insights",
                    "desc": "VADER is a framework that improves the understanding of unusual events in videos by using advanced language models. It goes beyond just spotting anomalies by analyzing the relationships between objects and their visual context. The framework uses an Anomaly Scorer to evaluate each frame and a Context-Aware Sampling strategy to understand the causal context of anomalies. By combining relational features with language models, VADER provides detailed descriptions and answers questions about the anomalies, showing significant improvements in explainable video analysis."
                },
                "zh": {
                    "title": "VADERï¼šæå‡è§†é¢‘å¼‚å¸¸ç†è§£çš„æ™ºèƒ½æ¡†æ¶",
                    "desc": "VADERæ˜¯ä¸€ä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºè§†é¢‘å¼‚å¸¸ç†è§£ã€‚å®ƒé€šè¿‡æ•´åˆå…³é”®å¸§å¯¹è±¡å…³ç³»å’Œè§†è§‰çº¿ç´¢ï¼Œæä¾›è¯¦ç»†çš„å› æœæè¿°å’Œå¼ºå¤§çš„é—®ç­”èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é¦–å…ˆä¸ºæ¯å¸§åˆ†é…å¼‚å¸¸åˆ†æ•°ï¼Œç„¶åä½¿ç”¨ä¸Šä¸‹æ–‡æ„ŸçŸ¥é‡‡æ ·ç­–ç•¥æ•æ‰å¼‚å¸¸äº‹ä»¶çš„å› æœèƒŒæ™¯ã€‚VADERåœ¨å¤šä¸ªçœŸå®ä¸–ç•Œçš„è§†é¢‘å¼‚å¸¸ç†è§£åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ¨åŠ¨äº†å¯è§£é‡Šè§†é¢‘å¼‚å¸¸åˆ†æçš„å‰æ²¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.06090",
            "title": "SWE-fficiency: Can Language Models Optimize Real-World Repositories on Real Workloads?",
            "url": "https://huggingface.co/papers/2511.06090",
            "abstract": "SWE-fficiency is a benchmark for evaluating repository-level performance optimization using real workloads, focusing on identifying and implementing performance improvements while maintaining code correctness.  \t\t\t\t\tAI-generated summary \t\t\t\t Optimizing the performance of large-scale software repositories demands expertise in code reasoning and software engineering (SWE) to reduce runtime while preserving program correctness. However, most benchmarks emphasize what to fix rather than how to fix code. We introduce SWE-fficiency, a benchmark for evaluating repository-level performance optimization on real workloads. Our suite contains 498 tasks across nine widely used data-science, machine-learning, and HPC repositories (e.g., numpy, pandas, scipy): given a complete codebase and a slow workload, an agent must investigate code semantics, localize bottlenecks and relevant tests, and produce a patch that matches or exceeds expert speedup while passing the same unit tests. To enable this how-to-fix evaluation, our automated pipeline scrapes GitHub pull requests for performance-improving edits, combining keyword filtering, static analysis, coverage tooling, and execution validation to both confirm expert speedup baselines and identify relevant repository unit tests. Empirical evaluation of state-of-the-art agents reveals significant underperformance. On average, agents achieve less than 0.15x the expert speedup: agents struggle in localizing optimization opportunities, reasoning about execution across functions, and maintaining correctness in proposed edits. We release the benchmark and accompanying data pipeline to facilitate research on automated performance engineering and long-horizon software reasoning.",
            "score": 4,
            "issue_id": 1,
            "pub_date": "2025-11-08",
            "pub_date_card": {
                "ru": "8 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 8",
                "zh": "11æœˆ8æ—¥"
            },
            "hash": "4e80fa9ff3d8a948",
            "authors": [
                "Jeffrey Jian Ma",
                "Milad Hashemi",
                "Amir Yazdanbakhsh",
                "Kevin Swersky",
                "Ofir Press",
                "Enhui Li",
                "Vijay Janapa Reddi",
                "Parthasarathy Ranganathan"
            ],
            "affiliations": [
                "Google",
                "Google DeepMind",
                "Harvard University",
                "Princeton University",
                "Xian Jiaotong University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.06090.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#open_source",
                    "#optimization",
                    "#agents",
                    "#plp",
                    "#dataset",
                    "#science"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "ĞšĞ°Ğº Ğ½Ğ°ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ°",
                    "desc": "SWE-fficiency - ÑÑ‚Ğ¾ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ñ… Ğ½Ğ°Ğ³Ñ€ÑƒĞ·Ğ¾Ğº. Ğ—Ğ°Ğ´Ğ°Ñ‡Ğ° Ğ°Ğ³ĞµĞ½Ñ‚Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ² Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸ ĞºĞ¾Ğ´Ğ°, Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑƒĞ·ĞºĞ¸Ñ… Ğ¼ĞµÑÑ‚ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ğ°Ñ‚Ñ‡ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¾Ğ±Ñ€Ğ°Ğ»Ğ¸ 498 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸Ğ· Ğ´ĞµĞ²ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸ĞµĞ² (numpy, pandas, scipy Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ…), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Pull Request Ñ GitHub Ğ¸ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ÑĞ½Ğ¸Ñ‚-Ñ‚ĞµÑÑ‚Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚ÑÑ‚Ğ°ÑÑ‚ Ğ¾Ñ‚ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ¼ĞµĞ½ĞµĞµ 0.15x ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "SWE-fficiency: Optimizing Performance with Code Correctness",
                    "desc": "The paper introduces SWE-fficiency, a benchmark designed to evaluate the performance optimization of software repositories while ensuring code correctness. It focuses on real workloads and includes 498 tasks from popular data-science and machine-learning libraries. The benchmark challenges agents to identify performance bottlenecks and propose code improvements that match expert-level speedups while passing unit tests. The study reveals that current state-of-the-art agents significantly underperform, achieving only a fraction of the expert speedup due to difficulties in code reasoning and maintaining correctness."
                },
                "zh": {
                    "title": "SWE-fficiencyï¼šä¼˜åŒ–è½¯ä»¶æ€§èƒ½çš„æ–°åŸºå‡†",
                    "desc": "SWE-fficiencyæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°è½¯ä»¶åº“æ€§èƒ½ä¼˜åŒ–çš„åŸºå‡†ï¼Œä¸“æ³¨äºåœ¨ä¿æŒä»£ç æ­£ç¡®æ€§çš„åŒæ—¶è¯†åˆ«å’Œå®æ–½æ€§èƒ½æ”¹è¿›ã€‚è¯¥åŸºå‡†åŒ…å«498ä¸ªä»»åŠ¡ï¼Œæ¶µç›–ä¹ä¸ªå¹¿æ³›ä½¿ç”¨çš„æ•°æ®ç§‘å­¦ã€æœºå™¨å­¦ä¹ å’Œé«˜æ€§èƒ½è®¡ç®—åº“ã€‚é€šè¿‡åˆ†æä»£ç è¯­ä¹‰å’Œå®šä½ç“¶é¢ˆï¼Œä»£ç†éœ€è¦ç”Ÿæˆä¸€ä¸ªè¡¥ä¸ï¼Œä»¥åŒ¹é…æˆ–è¶…è¿‡ä¸“å®¶çš„åŠ é€Ÿæ•ˆæœï¼ŒåŒæ—¶é€šè¿‡ç›¸åŒçš„å•å…ƒæµ‹è¯•ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œç°æœ‰çš„è‡ªåŠ¨åŒ–ä»£ç†åœ¨æ€§èƒ½ä¼˜åŒ–æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œå¹³å‡åªèƒ½è¾¾åˆ°ä¸“å®¶åŠ é€Ÿçš„0.15å€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.00710",
            "title": "Ariadne: A Controllable Framework for Probing and Extending VLM\n  Reasoning Boundaries",
            "url": "https://huggingface.co/papers/2511.00710",
            "abstract": "Ariadne, a framework using synthetic mazes and RLVR, expands VLMs' capability in visual-centric spatial reasoning and improves zero-shot generalization on real-world benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t While Vision-Language Models (VLMs) post-trained with Reinforcement Learning (RL) show impressive general reasoning, their evaluation is often confined to language-dominant tasks (e.g., math). This raises a critical question: can RL post-training truly extend the inherent capability boundary of a base VLM, particularly for visual-centric spatial tasks where it initially fails? To investigate this, we introduce Ariadne, a framework utilizing synthetic mazes for multi-step spatial reasoning where task difficulty (e.g., path length, turns) is precisely controlled. We leverage this controllable environment to train VLMs using Reinforcement Learning with Verified Rewards (RLVR) in a difficulty-aware curriculum. Surprisingly, post-RLVR training, the VLM achieves over 50% accuracy on a problem set where the base model scored 0%, demonstrating that our approach expands the model's initial capability boundary. To assess real-world viability, we evaluate out-of-distribution (OOD) generalization on practical benchmarks. Despite training only on synthetic maze samples, Ariadne achieves significant zero-shot improvements, averaging 16% on MapBench (e.g., museum navigation) and 24% on ReasonMap (subway transfer tasks). These results confirm that our method not only broadens the model's fundamental limits but also enhances its generalization to real-world spatial reasoning. We acknowledge our study is limited to the post-training phase, given the opaqueness of pre-training data, and hope our research motivates further work on specialized, capability-extending alignment.",
            "score": 4,
            "issue_id": 1,
            "pub_date": "2025-11-01",
            "pub_date_card": {
                "ru": "1 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 1",
                "zh": "11æœˆ1æ—¥"
            },
            "hash": "e5ee61ff06448bba",
            "authors": [
                "Minghe Shen",
                "Zhuo Zhi",
                "Chonghan Liu",
                "Shuo Xing",
                "Zhengzhong Tu",
                "Che Liu"
            ],
            "affiliations": [
                "Imperial College London",
                "Texas A&M University",
                "University College London",
                "University of California, Los Angeles"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.00710.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#transfer_learning",
                    "#cv",
                    "#multimodal",
                    "#rl",
                    "#reasoning",
                    "#synthetic"
                ],
                "emoji": "ğŸ§­",
                "ru": {
                    "title": "Ğ Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ VLM Ñ‡ĞµÑ€ĞµĞ· ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ»Ğ°Ğ±Ğ¸Ñ€Ğ¸Ğ½Ñ‚Ñ‹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ariadne, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ»Ğ°Ğ±Ğ¸Ñ€Ğ¸Ğ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ VLM Ğ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²Ğ¾Ğ¼ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ (RLVR). Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€Ğ°ÑÑˆĞ¸Ñ€Ğ¸Ñ‚ÑŒ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ´Ğ¾Ğ²Ğ¾Ğ´Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ 0% Ğ´Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 50% Ğ½Ğ° ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ. Ğ’Ğ°Ğ¶Ğ½Ğ¾ Ğ¾Ñ‚Ğ¼ĞµÑ‚Ğ¸Ñ‚ÑŒ, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…: Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 16% Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚Ğ° Ğ½Ğ° MapBench Ğ¸ 24% Ğ½Ğ° ReasonMap Ğ±ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ¾Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ğ¾Ğ´ ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğµ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Ariadne: Expanding VLMs' Spatial Reasoning with Synthetic Mazes",
                    "desc": "Ariadne is a new framework that enhances Vision-Language Models (VLMs) by using synthetic mazes to improve their ability in visual-centric spatial reasoning. It employs Reinforcement Learning with Verified Rewards (RLVR) to train these models in a controlled environment, allowing for precise adjustments in task difficulty. The results show that after this training, the VLM significantly improves its performance on spatial reasoning tasks, achieving over 50% accuracy where it previously scored 0%. Additionally, Ariadne demonstrates strong zero-shot generalization on real-world benchmarks, indicating that it effectively extends the model's capabilities beyond its initial limitations."
                },
                "zh": {
                    "title": "æ‰©å±•è§†è§‰è¯­è¨€æ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›",
                    "desc": "Ariadneæ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œåˆ©ç”¨åˆæˆè¿·å®«å’Œå¼ºåŒ–å­¦ä¹ éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰ï¼Œæ‰©å±•äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨è§†è§‰ä¸­å¿ƒç©ºé—´æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚é€šè¿‡åœ¨å¯æ§ç¯å¢ƒä¸­è¿›è¡Œå¤šæ­¥ç©ºé—´æ¨ç†è®­ç»ƒï¼ŒAriadneæ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨çœŸå®ä¸–ç•ŒåŸºå‡†ä¸Šçš„é›¶-shotæ³›åŒ–èƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç»è¿‡RLVRè®­ç»ƒåï¼ŒVLMåœ¨åŸæœ¬å¾—åˆ†ä¸º0%çš„ä»»åŠ¡ä¸Šè¾¾åˆ°äº†è¶…è¿‡50%çš„å‡†ç¡®ç‡ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•èƒ½å¤Ÿæ‰©å±•æ¨¡å‹çš„èƒ½åŠ›è¾¹ç•Œã€‚å°½ç®¡è®­ç»ƒä»…åŸºäºåˆæˆè¿·å®«æ ·æœ¬ï¼ŒAriadneåœ¨å®é™…åŸºå‡†æµ‹è¯•ä¸­ä»å®ç°äº†æ˜¾è‘—çš„é›¶-shotæ”¹è¿›ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨çœŸå®ä¸–ç•Œç©ºé—´æ¨ç†ä¸­çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.07061",
            "title": "Do LLMs Feel? Teaching Emotion Recognition with Prompts, Retrieval, and\n  Curriculum Learning",
            "url": "https://huggingface.co/papers/2511.07061",
            "abstract": "A novel ERC training framework, PRC-Emo, integrates prompt engineering, demonstration retrieval, and curriculum learning to enhance LLMs' ability to perceive emotions in conversations, achieving state-of-the-art performance on benchmark datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Emotion Recognition in Conversation (ERC) is a crucial task for understanding human emotions and enabling natural human-computer interaction. Although Large Language Models (LLMs) have recently shown great potential in this field, their ability to capture the intrinsic connections between explicit and implicit emotions remains limited. We propose a novel ERC training framework, PRC-Emo, which integrates Prompt engineering, demonstration Retrieval, and Curriculum learning, with the goal of exploring whether LLMs can effectively perceive emotions in conversational contexts. Specifically, we design emotion-sensitive prompt templates based on both explicit and implicit emotional cues to better guide the model in understanding the speaker's psychological states. We construct the first dedicated demonstration retrieval repository for ERC, which includes training samples from widely used datasets, as well as high-quality dialogue examples generated by LLMs and manually verified. Moreover, we introduce a curriculum learning strategy into the LoRA fine-tuning process, incorporating weighted emotional shifts between same-speaker and different-speaker utterances to assign difficulty levels to dialogue samples, which are then organized in an easy-to-hard training sequence. Experimental results on two benchmark datasets-- IEMOCAP and MELD --show that our method achieves new state-of-the-art (SOTA) performance, demonstrating the effectiveness and generalizability of our approach in improving LLM-based emotional understanding.",
            "score": 3,
            "issue_id": 1,
            "pub_date": "2025-11-10",
            "pub_date_card": {
                "ru": "10 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 10",
                "zh": "11æœˆ10æ—¥"
            },
            "hash": "e675233a34194f87",
            "authors": [
                "Xinran Li",
                "Yu Liu",
                "Jiaqi Qiao",
                "Xiujuan Xu"
            ],
            "affiliations": [
                "School of Software Technology, Dalian University of Technology"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.07061.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#benchmark",
                    "#training"
                ],
                "emoji": "ğŸ˜Š",
                "ru": {
                    "title": "Ğ¢Ñ€Ğ¸ ĞºĞ»ÑÑ‡Ğ° Ğº ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ: Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹, Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºĞ° PRC-Emo Ğ´Ğ»Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹ Ğ² Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ², Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ğ¾ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸ÑĞ¼ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ñ€Ğ°ÑĞ¿Ğ¸ÑĞ°Ğ½Ğ¸ĞµĞ¼ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ñ‹ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ², Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğº ÑĞ²Ğ½Ñ‹Ğ¼ Ğ¸ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°Ğ¼, Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´Ğ¸ĞºĞ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ curriculum learning Ğ¿Ñ€Ğ¸ Ñ„Ğ°Ğ¹Ğ½Ñ‚ÑĞ½Ğ¸Ğ½Ğ³Ğµ Ñ LoRA, ÑƒĞ¿Ğ¾Ñ€ÑĞ´Ğ¾Ñ‡Ğ¸Ğ²Ğ°Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğº ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… IEMOCAP Ğ¸ MELD Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞºĞ¾Ñ€Ğ´Ğ¾Ğ² Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ."
                },
                "en": {
                    "title": "Enhancing Emotion Recognition in Conversations with PRC-Emo",
                    "desc": "The paper presents PRC-Emo, a new training framework designed to improve Large Language Models' (LLMs) ability to recognize emotions in conversations. It combines prompt engineering, demonstration retrieval, and curriculum learning to enhance the model's understanding of both explicit and implicit emotional cues. By creating emotion-sensitive prompts and a dedicated repository of dialogue examples, the framework guides LLMs in interpreting emotional states more accurately. The results show that PRC-Emo achieves state-of-the-art performance on benchmark datasets, indicating its effectiveness in emotion recognition tasks."
                },
                "zh": {
                    "title": "æå‡å¯¹è¯æƒ…æ„Ÿè¯†åˆ«çš„å…¨æ–°æ¡†æ¶PRC-Emo",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æƒ…æ„Ÿè¯†åˆ«è®­ç»ƒæ¡†æ¶PRC-Emoï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¯¹è¯ä¸­æ„ŸçŸ¥æƒ…æ„Ÿçš„èƒ½åŠ›ã€‚è¯¥æ¡†æ¶ç»“åˆäº†æç¤ºå·¥ç¨‹ã€ç¤ºä¾‹æ£€ç´¢å’Œè¯¾ç¨‹å­¦ä¹ ï¼Œè®¾è®¡äº†æƒ…æ„Ÿæ•æ„Ÿçš„æç¤ºæ¨¡æ¿ï¼Œä»¥æ›´å¥½åœ°å¼•å¯¼æ¨¡å‹ç†è§£è¯´è¯è€…çš„å¿ƒç†çŠ¶æ€ã€‚æˆ‘ä»¬è¿˜æ„å»ºäº†ç¬¬ä¸€ä¸ªä¸“é—¨ç”¨äºæƒ…æ„Ÿè¯†åˆ«çš„ç¤ºä¾‹æ£€ç´¢åº“ï¼ŒåŒ…å«æ¥è‡ªå¹¿æ³›ä½¿ç”¨çš„æ•°æ®é›†çš„è®­ç»ƒæ ·æœ¬å’Œé«˜è´¨é‡çš„å¯¹è¯ç¤ºä¾‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPRC-Emoåœ¨IEMOCAPå’ŒMELDä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€æ–°çš„æ€§èƒ½ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨æå‡LLMæƒ…æ„Ÿç†è§£èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§å’Œæ™®é€‚æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.07253",
            "title": "Omni-AVSR: Towards Unified Multimodal Speech Recognition with Large\n  Language Models",
            "url": "https://huggingface.co/papers/2511.07253",
            "abstract": "Omni-AVSR is a unified audio-visual LLM that efficiently supports ASR, VSR, and AVSR through multi-granularity training and parameter-efficient adaptation, achieving high accuracy with reduced resource use.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have recently achieved impressive results in speech recognition across multiple modalities, including Auditory Speech Recognition (ASR), Visual Speech Recognition (VSR), and Audio-Visual Speech Recognition (AVSR). Despite this progress, current LLM-based approaches typically address each task independently, training separate models that raise computational and deployment resource use while missing potential cross-task synergies. They also rely on fixed-rate token compression, which restricts flexibility in balancing accuracy with efficiency. These limitations highlight the need for a unified framework that can support ASR, VSR, and AVSR while enabling elastic inference. To this end, we present Omni-AVSR, a unified audio-visual LLM that combines efficient multi-granularity training with parameter-efficient adaptation. Specifically, we adapt the matryoshka representation learning paradigm to efficiently train across multiple audio and visual granularities, reducing its inherent training resource use. Furthermore, we explore three LoRA-based strategies for adapting the backbone LLM, balancing shared and task-specific specialization. Experiments on LRS2 and LRS3 show that Omni-AVSR achieves comparable or superior accuracy to state-of-the-art baselines while training a single model at substantially lower training and deployment resource use. The model also remains robust under acoustic noise, and we analyze its scaling behavior as LLM size increases, providing insights into the trade-off between performance and efficiency.",
            "score": 2,
            "issue_id": 1,
            "pub_date": "2025-11-10",
            "pub_date_card": {
                "ru": "10 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 10",
                "zh": "11æœˆ10æ—¥"
            },
            "hash": "d843529659b96907",
            "authors": [
                "Umberto Cappellazzo",
                "Xubo Liu",
                "Pingchuan Ma",
                "Stavros Petridis",
                "Maja Pantic"
            ],
            "affiliations": [
                "Imperial College London, UK",
                "University of Surrey, UK"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.07253.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#audio",
                    "#multimodal",
                    "#training"
                ],
                "emoji": "ğŸ‘ï¸â€ğŸ—¨ï¸",
                "ru": {
                    "title": "ĞĞ´Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ‚Ñ€Ñ‘Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµÑ‡Ğ¸",
                    "desc": "Omni-AVSR â€” ÑÑ‚Ğ¾ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½Ñ‘Ğ½Ğ½Ğ°Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµÑ‡Ğ¸ Ğ¸Ğ· Ğ°ÑƒĞ´Ğ¸Ğ¾ (ASR), Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ (VSR) Ğ¸ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² (AVSR) Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ³Ñ€Ğ°Ğ½ÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¼Ğ°Ñ‚Ñ€Ñ‘ÑˆĞµÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ğ½ÑƒĞ»ÑÑ€Ğ½Ğ¾ÑÑ‚ÑÑ… Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ‡Ñ‚Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹. Ğ”Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ñ‚Ñ€Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LoRA, Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±Ñ‰Ğ¸Ğ¼Ğ¸ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾Ğ¹ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰ĞµĞ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ."
                },
                "en": {
                    "title": "Omni-AVSR: Unifying Speech Recognition for Efficiency and Accuracy",
                    "desc": "Omni-AVSR is a unified audio-visual large language model (LLM) designed to enhance speech recognition across three modalities: Auditory Speech Recognition (ASR), Visual Speech Recognition (VSR), and Audio-Visual Speech Recognition (AVSR). It employs multi-granularity training and parameter-efficient adaptation to achieve high accuracy while minimizing resource consumption. By integrating various training strategies, Omni-AVSR allows for flexible inference and reduces the need for separate models for each task, thus leveraging cross-task synergies. Experimental results demonstrate that it performs comparably or better than existing models, even in noisy environments, while being more efficient in terms of training and deployment resources."
                },
                "zh": {
                    "title": "ç»Ÿä¸€éŸ³é¢‘-è§†è§‰æ¨¡å‹ï¼Œæå‡è¯†åˆ«æ•ˆç‡ä¸å‡†ç¡®æ€§",
                    "desc": "Omni-AVSRæ˜¯ä¸€ç§ç»Ÿä¸€çš„éŸ³é¢‘-è§†è§‰å¤§è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿé«˜æ•ˆæ”¯æŒå¬è§‰è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€è§†è§‰è¯­éŸ³è¯†åˆ«ï¼ˆVSRï¼‰å’ŒéŸ³é¢‘-è§†è§‰è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰ã€‚è¯¥æ¨¡å‹é€šè¿‡å¤šç²’åº¦è®­ç»ƒå’Œå‚æ•°é«˜æ•ˆé€‚åº”ï¼Œæ˜¾è‘—é™ä½äº†èµ„æºä½¿ç”¨ï¼ŒåŒæ—¶å®ç°äº†é«˜å‡†ç¡®ç‡ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒOmni-AVSRé‡‡ç”¨äº†ç»Ÿä¸€æ¡†æ¶ï¼Œé¿å…äº†ç‹¬ç«‹è®­ç»ƒå¤šä¸ªæ¨¡å‹å¸¦æ¥çš„è®¡ç®—å’Œéƒ¨ç½²èµ„æºæµªè´¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOmni-AVSRåœ¨LRS2å’ŒLRS3æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿåœ¨å™ªå£°ç¯å¢ƒä¸‹ä¿æŒé²æ£’æ€§ï¼Œå¹¶æä¾›äº†æ€§èƒ½ä¸æ•ˆç‡ä¹‹é—´çš„æƒè¡¡åˆ†æã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-11-10.html",
    "link_next": "2025-11-12.html",
    "link_month": "2025-11.html",
    "short_date_prev": {
        "ru": "10.11",
        "en": "11/10",
        "zh": "11æœˆ10æ—¥"
    },
    "short_date_next": {
        "ru": "12.11",
        "en": "11/12",
        "zh": "11æœˆ12æ—¥"
    },
    "categories": {
        "#dataset": 9,
        "#data": 2,
        "#benchmark": 16,
        "#agents": 6,
        "#cv": 3,
        "#rl": 10,
        "#rlhf": 2,
        "#rag": 1,
        "#plp": 2,
        "#inference": 2,
        "#3d": 3,
        "#audio": 1,
        "#video": 4,
        "#multimodal": 11,
        "#math": 0,
        "#multilingual": 2,
        "#architecture": 4,
        "#healthcare": 0,
        "#training": 17,
        "#robotics": 2,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 2,
        "#reasoning": 12,
        "#transfer_learning": 2,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 10,
        "#survey": 1,
        "#diffusion": 2,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 3,
        "#synthetic": 5,
        "#machine_translation": 0,
        "#leakage": 1,
        "#open_source": 9,
        "#small_models": 4,
        "#science": 3,
        "#low_resource": 1
    }
}