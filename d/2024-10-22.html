
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 23 papers. October 22.</title>
    <link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #0989eacf;
            --secondary-color: #fff;
            --background-color: #f5f5f5;
            --text-color: #333333;
            --header-color: #0989eacf;
            --body-color: #f5f5f5;
            --menu-color: #002370;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 2em;
            padding: 10px 0 20px 0;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.tags {
            color: #555;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);
            transition: background-color 0.2s ease;
            display: flex;
            flex-direction: column;
            position: relative;
        }
        .article-content {
            padding: 1.5em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
        }
        body.dark-theme>div>main>article {
            background-color: #444;
        }
        body.light-theme>div>main>article {
            background-color: #fff;
        }
        body.dark-theme>div>main>article:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
        }
        .pub-date {
            font-size: 0.9em;
            margin-bottom: 0.8em;
            font-weight: 300;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 1em;
            position: absolute;
            bottom: 10px;
            font-weight: 300;
            font-family: 'Roboto Slab';
        }
        .background-digit {
            position: absolute;
            bottom: -20px;
            right: -10px;
            font-size: 12em;
            font-weight: bold;
            color: rgba(0, 0, 0, 0.03);
            z-index: 0;
            line-height: 1;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 80px;
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        a:hover {
            color: #e73838;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            padding: 1em 0;
            margin-top: 2em;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: fixed;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
        }
        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
        }
        .update-info-container {
            flex: 1;
        }
        .sort-container {
            flex: 2;
        }
        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
                display: block;
                margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .category-toggle {
            display: inline-block;
            margin-bottom: 10px;
            margin-top: 15px;
            cursor: pointer;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 0px;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }
        
        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        body.light-theme>div>main>article.xb3e061a400165087 { background: url("https://hfday.ru/img/20241021/b3e061a400165087.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.xb3e061a400165087:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.xb3e061a400165087 { background: url("https://hfday.ru/img/20241021/b3e061a400165087.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.xb3e061a400165087:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x3b5265f629378d5d { background: url("https://hfday.ru/img/20241021/3b5265f629378d5d.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x3b5265f629378d5d:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x3b5265f629378d5d { background: url("https://hfday.ru/img/20241021/3b5265f629378d5d.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x3b5265f629378d5d:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x4daeb1e76417273d { background: url("https://hfday.ru/img/20241021/4daeb1e76417273d.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x4daeb1e76417273d:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x4daeb1e76417273d { background: url("https://hfday.ru/img/20241021/4daeb1e76417273d.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x4daeb1e76417273d:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x31995f0502d70f2f { background: url("https://hfday.ru/img/20241017/31995f0502d70f2f.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x31995f0502d70f2f:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x31995f0502d70f2f { background: url("https://hfday.ru/img/20241017/31995f0502d70f2f.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x31995f0502d70f2f:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x62b6ded4f9dc1d27 { background: url("https://hfday.ru/img/20241021/62b6ded4f9dc1d27.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x62b6ded4f9dc1d27:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x62b6ded4f9dc1d27 { background: url("https://hfday.ru/img/20241021/62b6ded4f9dc1d27.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x62b6ded4f9dc1d27:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x863b95d8e45c3aa2 { background: url("https://hfday.ru/img/20241017/863b95d8e45c3aa2.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x863b95d8e45c3aa2:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x863b95d8e45c3aa2 { background: url("https://hfday.ru/img/20241017/863b95d8e45c3aa2.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x863b95d8e45c3aa2:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.xe1222b08a95a2911 { background: url("https://hfday.ru/img/20241019/e1222b08a95a2911.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.xe1222b08a95a2911:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.xe1222b08a95a2911 { background: url("https://hfday.ru/img/20241019/e1222b08a95a2911.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.xe1222b08a95a2911:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x023f2f5e4c6c7a7c { background: url("https://hfday.ru/img/20241021/023f2f5e4c6c7a7c.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x023f2f5e4c6c7a7c:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x023f2f5e4c6c7a7c { background: url("https://hfday.ru/img/20241021/023f2f5e4c6c7a7c.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x023f2f5e4c6c7a7c:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x6fff842f967d7207 { background: url("https://hfday.ru/img/20241021/6fff842f967d7207.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x6fff842f967d7207:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x6fff842f967d7207 { background: url("https://hfday.ru/img/20241021/6fff842f967d7207.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x6fff842f967d7207:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x0fbcf072cc98c553 { background: url("https://hfday.ru/img/20241016/0fbcf072cc98c553.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x0fbcf072cc98c553:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x0fbcf072cc98c553 { background: url("https://hfday.ru/img/20241016/0fbcf072cc98c553.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x0fbcf072cc98c553:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x0d2e73e11d2b2b3f { background: url("https://hfday.ru/img/20241021/0d2e73e11d2b2b3f.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x0d2e73e11d2b2b3f:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x0d2e73e11d2b2b3f { background: url("https://hfday.ru/img/20241021/0d2e73e11d2b2b3f.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x0d2e73e11d2b2b3f:hover { background-color: rgba(60,60,60,0.92) !important;}

        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .sort-container {
                margin-top: 0px;
                text-align: left;
                width: 100%;
            .sort-dropdown {
                float: right;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">22 октября</span> | <span id="title-articles-count">23 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item" id="nav-prev"><a href="/d/2024-10-21.html">⬅️ <span id="prev-date">21.10</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-10-23.html">➡️ <span id="next-date">23.10</span></a></span>
            <!--<span class="nav-item" id="nav-weekly">Топ за неделю</span>
            <span class="nav-item" id="nav-weekly">Топ за месяц</span>-->
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="category-toggle">
            <div class="svg-container">
                <span id="category-toggle">🏷️ Фильтр</span>
                <svg height="3" width="200">
                    <line x1="0" y1="0" x2="200" y2="0" 
                        stroke="black" 
                        stroke-width="2" 
                        stroke-dasharray="3, 3" />
                </svg>
            </div>
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '22 октября', 'en': 'October 22', 'zh': '10月22日'};
        let feedDateNext = {'ru': '23.10', 'en': '10/23', 'zh': '10月23日'};
        let feedDatePrev = {'ru': '21.10', 'en': '10/21', 'zh': '10月21日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'Статья от ', 'en': 'Published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2410.16271', 'title': 'FrugalNeRF: Fast Convergence for Few-shot Novel View Synthesis without Learned Priors', 'url': 'https://huggingface.co/papers/2410.16271', 'abstract': 'Neural Radiance Fields (NeRF) face significant challenges in few-shot scenarios, primarily due to overfitting and long training times for high-fidelity rendering. Existing methods, such as FreeNeRF and SparseNeRF, use frequency regularization or pre-trained priors but struggle with complex scheduling and bias. We introduce FrugalNeRF, a novel few-shot NeRF framework that leverages weight-sharing voxels across multiple scales to efficiently represent scene details. Our key contribution is a cross-scale geometric adaptation scheme that selects pseudo ground truth depth based on reprojection errors across scales. This guides training without relying on externally learned priors, enabling full utilization of the training data. It can also integrate pre-trained priors, enhancing quality without slowing convergence. Experiments on LLFF, DTU, and RealEstate-10K show that FrugalNeRF outperforms other few-shot NeRF methods while significantly reducing training time, making it a practical solution for efficient and accurate 3D scene reconstruction.', 'score': 53, 'issue_id': 208, 'pub_date': '2024-10-21', 'pub_date_ru': '21 октября', 'hash': 'b3e061a400165087', 'data': {'categories': ['#3d', '#cv'], 'emoji': '🔍', 'ru': {'title': 'Эффективное 3D-моделирование с минимумом данных', 'desc': 'FrugalNeRF - это новый фреймворк для обучения нейронных полей излучения (NeRF) на малом количестве данных. Он использует разделение весов воксельной структуры на разных масштабах для эффективного представления деталей сцены. Ключевой особенностью является схема геометрической адаптации между масштабами, выбирающая псевдо-истинную глубину на основе ошибок репроекции. FrugalNeRF превосходит другие few-shot NeRF методы, значительно сокращая время обучения.'}, 'en': {'title': 'FrugalNeRF: Efficient 3D Scene Reconstruction with Cross-Scale Adaptation', 'desc': 'The paper introduces FrugalNeRF, a new approach to improve Neural Radiance Fields (NeRF) in few-shot scenarios by using weight-sharing voxels across multiple scales. This method addresses the challenges of overfitting and long training times by employing a cross-scale geometric adaptation scheme that selects pseudo ground truth depth based on reprojection errors. Unlike previous methods, FrugalNeRF does not rely on externally learned priors, allowing for full utilization of the training data while still being able to integrate pre-trained priors for enhanced quality. Experiments demonstrate that FrugalNeRF outperforms existing few-shot NeRF methods in terms of efficiency and accuracy, making it a practical solution for 3D scene reconstruction.'}, 'zh': {'title': 'FrugalNeRF：高效少样本3D重建新方案', 'desc': 'NeRF在少样本场景中面临过拟合和训练时间长的问题。FrugalNeRF通过跨尺度几何适应方案，利用重投影误差选择伪真实深度，避免依赖外部学习先验。该方法有效利用训练数据，并可结合预训练先验，提高质量且不影响收敛速度。实验表明，FrugalNeRF在减少训练时间的同时，提升了3D场景重建的效率和准确性。'}}, 'pub_date_card': {'ru': '21 октября', 'en': 'October 21', 'zh': '10月21日'}}, {'id': 'https://huggingface.co/papers/2410.16256', 'title': 'CompassJudger-1: All-in-one Judge Model Helps Model Evaluation and Evolution', 'url': 'https://huggingface.co/papers/2410.16256', 'abstract': 'Efficient and accurate evaluation is crucial for the continuous improvement of large language models (LLMs). Among various assessment methods, subjective evaluation has garnered significant attention due to its superior alignment with real-world usage scenarios and human preferences. However, human-based evaluations are costly and lack reproducibility, making precise automated evaluators (judgers) vital in this process. In this report, we introduce CompassJudger-1, the first open-source all-in-one judge LLM. CompassJudger-1 is a general-purpose LLM that demonstrates remarkable versatility. It is capable of: 1. Performing unitary scoring and two-model comparisons as a reward model; 2. Conducting evaluations according to specified formats; 3. Generating critiques; 4. Executing diverse tasks like a general LLM. To assess the evaluation capabilities of different judge models under a unified setting, we have also established JudgerBench, a new benchmark that encompasses various subjective evaluation tasks and covers a wide range of topics. CompassJudger-1 offers a comprehensive solution for various evaluation tasks while maintaining the flexibility to adapt to diverse requirements. Both CompassJudger and JudgerBench are released and available to the research community athttps://github.com/open-compass/CompassJudger. We believe that by open-sourcing these tools, we can foster collaboration and accelerate progress in LLM evaluation methodologies.', 'score': 51, 'issue_id': 208, 'pub_date': '2024-10-21', 'pub_date_ru': '21 октября', 'hash': '3b5265f629378d5d', 'data': {'categories': ['#alignment', '#benchmark'], 'emoji': '⚖️', 'ru': {'title': 'CompassJudger-1: Универсальный судья для языковых моделей', 'desc': 'В статье представлен CompassJudger-1 - первая открытая универсальная модель-оценщик для больших языковых моделей (LLM). Эта модель способна выполнять разнообразные задачи оценки, включая сравнение моделей, генерацию критики и выполнение общих задач как обычная LLM. Авторы также создали бенчмарк JudgerBench для сравнения различных моделей-оценщиков. CompassJudger-1 и JudgerBench доступны сообществу исследователей для дальнейшего развития методов оценки LLM.'}, 'en': {'title': '"CompassJudger-1: Revolutionizing LLM Evaluation with Open-Source Precision"', 'desc': 'The paper introduces CompassJudger-1, an open-source large language model designed to automate the evaluation of other language models, reducing the need for costly human-based assessments. CompassJudger-1 can perform tasks such as scoring, model comparison, and critique generation, making it a versatile tool for various evaluation scenarios. To test the effectiveness of different judge models, the authors have also developed JudgerBench, a benchmark that includes a wide range of subjective evaluation tasks. By releasing these tools to the public, the authors aim to encourage collaboration and speed up advancements in evaluating language models.'}, 'zh': {'title': 'CompassJudger-1：大语言模型评估的全能工具', 'desc': '本文介绍了一种名为CompassJudger-1的开源评估工具，它是一个通用的大语言模型，能够进行单一评分、双模型比较、生成评论等多种任务。CompassJudger-1的设计旨在解决人工评估成本高且难以重复的问题，通过自动化的方式提高评估的效率和准确性。为了评估不同评估模型的能力，研究者还建立了一个名为JudgerBench的新基准，涵盖了多种主观评估任务。通过开源这些工具，研究者希望促进合作，加速大语言模型评估方法的发展。'}}, 'pub_date_card': {'ru': '21 октября', 'en': 'October 21', 'zh': '10月21日'}}, {'id': 'https://huggingface.co/papers/2410.16268', 'title': 'SAM2Long: Enhancing SAM 2 for Long Video Segmentation with a Training-Free Memory Tree', 'url': 'https://huggingface.co/papers/2410.16268', 'abstract': 'The Segment Anything Model 2 (SAM 2) has emerged as a powerful foundation model for object segmentation in both images and videos, paving the way for various downstream video applications. The crucial design of SAM 2 for video segmentation is its memory module, which prompts object-aware memories from previous frames for current frame prediction. However, its greedy-selection memory design suffers from the "error accumulation" problem, where an errored or missed mask will cascade and influence the segmentation of the subsequent frames, which limits the performance of SAM 2 toward complex long-term videos. To this end, we introduce SAM2Long, an improved training-free video object segmentation strategy, which considers the segmentation uncertainty within each frame and chooses the video-level optimal results from multiple segmentation pathways in a constrained tree search manner. In practice, we maintain a fixed number of segmentation pathways throughout the video. For each frame, multiple masks are proposed based on the existing pathways, creating various candidate branches. We then select the same fixed number of branches with higher cumulative scores as the new pathways for the next frame. After processing the final frame, the pathway with the highest cumulative score is chosen as the final segmentation result. Benefiting from its heuristic search design, SAM2Long is robust toward occlusions and object reappearances, and can effectively segment and track objects for complex long-term videos. Notably, SAM2Long achieves an average improvement of 3.0 points across all 24 head-to-head comparisons, with gains of up to 5.3 points in J&F on long-term video object segmentation benchmarks such as SA-V and LVOS. The code is released at https://github.com/Mark12Ding/SAM2Long.', 'score': 46, 'issue_id': 209, 'pub_date': '2024-10-21', 'pub_date_ru': '21 октября', 'hash': '4daeb1e76417273d', 'data': {'categories': ['#benchmark', '#cv', '#video'], 'emoji': '🎬', 'ru': {'title': 'SAM2Long: Преодоление долгосрочных проблем в сегментации видео', 'desc': 'SAM2Long - это улучшенная стратегия сегментации видеообъектов, основанная на модели SAM 2. Она решает проблему накопления ошибок при долгосрочной сегментации видео путем использования нескольких путей сегментации и выбора оптимального результата. SAM2Long поддерживает фиксированное количество путей сегментации на протяжении всего видео, выбирая ветви с наивысшими накопленными оценками для каждого кадра. Этот подход позволяет эффективно сегментировать и отслеживать объекты в сложных долгосрочных видео, демонстрируя улучшение до 5.3 пунктов по метрике J&F на бенчмарках долгосрочной сегментации видеообъектов.'}, 'en': {'title': '"SAM2Long: Navigating Complexity in Video Segmentation"', 'desc': 'The Segment Anything Model 2 (SAM 2) is a foundation model designed for object segmentation in images and videos, but it struggles with error accumulation in long-term video segmentation. SAM2Long is introduced as an improved strategy that uses a constrained tree search to select optimal segmentation pathways, addressing the error accumulation issue. By maintaining multiple segmentation pathways and selecting the best candidates based on cumulative scores, SAM2Long effectively handles occlusions and object reappearances. This approach results in significant performance improvements, achieving up to 5.3 points gain in long-term video segmentation benchmarks.'}, 'zh': {'title': 'SAM2Long：提升长视频对象分割的新策略', 'desc': 'Segment Anything Model 2（SAM 2）是一个强大的基础模型，用于图像和视频中的对象分割。它的关键设计是记忆模块，可以从前一帧中提取对象相关的记忆来预测当前帧。然而，SAM 2 的贪婪选择记忆设计存在“错误积累”问题，影响复杂长视频的分割性能。为此，我们引入了 SAM2Long，通过多路径搜索策略来提高视频对象分割的准确性。'}}, 'pub_date_card': {'ru': '21 октября', 'en': 'October 21', 'zh': '10月21日'}}, {'id': 'https://huggingface.co/papers/2410.13861', 'title': 'PUMA: Empowering Unified MLLM with Multi-granular Visual Generation', 'url': 'https://huggingface.co/papers/2410.13861', 'abstract': 'Recent advancements in multimodal foundation models have yielded significant progress in vision-language understanding. Initial attempts have also explored the potential of multimodal large language models (MLLMs) for visual content generation. However, existing works have insufficiently addressed the varying granularity demands of different image generation tasks within a unified MLLM paradigm - from the diversity required in text-to-image generation to the precise controllability needed in image manipulation. In this work, we propose PUMA, emPowering Unified MLLM with Multi-grAnular visual generation. PUMA unifies multi-granular visual features as both inputs and outputs of MLLMs, elegantly addressing the different granularity requirements of various image generation tasks within a unified MLLM framework. Following multimodal pretraining and task-specific instruction tuning, PUMA demonstrates proficiency in a wide range of multimodal tasks. This work represents a significant step towards a truly unified MLLM capable of adapting to the granularity demands of various visual tasks. The code and model will be released in https://github.com/rongyaofang/PUMA.', 'score': 41, 'issue_id': 208, 'pub_date': '2024-10-17', 'pub_date_ru': '17 октября', 'hash': '31995f0502d70f2f', 'data': {'categories': ['#cv', '#multimodal'], 'emoji': '🐆', 'ru': {'title': 'PUMA: Универсальная MLLM для мультигранулярной генерации изображений', 'desc': 'Статья представляет PUMA - новую модель многомодального большого языкового моделирования (MLLM) для генерации визуального контента. PUMA объединяет визуальные признаки разной гранулярности как для входных, так и для выходных данных MLLM, что позволяет решать различные задачи генерации изображений в рамках единой парадигмы. Модель демонстрирует высокую эффективность в широком спектре мультимодальных задач после предварительного обучения и тонкой настройки под конкретные инструкции. Это значительный шаг к созданию унифицированной MLLM, способной адаптироваться к требованиям гранулярности различных визуальных задач.'}, 'en': {'title': 'PUMA: Unifying Visual Generation with Multi-Granular Precision', 'desc': 'The paper introduces PUMA, a new approach to improve multimodal large language models (MLLMs) for visual content generation. PUMA addresses the challenge of varying granularity in image generation tasks, such as the need for diversity in text-to-image generation and precision in image manipulation. By integrating multi-granular visual features, PUMA enhances the ability of MLLMs to handle different visual tasks within a single framework. This advancement marks a significant step towards creating a unified model that can adapt to diverse visual generation needs.'}, 'zh': {'title': 'PUMA：统一多模态视觉生成的新突破', 'desc': '这篇论文介绍了一种名为PUMA的新方法，旨在提升多模态大语言模型（MLLMs）的视觉生成能力。PUMA通过统一多层次的视觉特征，解决了不同图像生成任务对细节和多样性的不同需求。通过多模态预训练和任务特定的指令调优，PUMA在多种多模态任务中表现出色。该研究为实现能够适应各种视觉任务细节需求的统一MLLM迈出了重要一步。'}}, 'pub_date_card': {'ru': '17 октября', 'en': 'October 17', 'zh': '10月17日'}}, {'id': 'https://huggingface.co/papers/2410.15735', 'title': 'AutoTrain: No-code training for state-of-the-art models', 'url': 'https://huggingface.co/papers/2410.15735', 'abstract': 'With the advancements in open-source models, training (or finetuning) models on custom datasets has become a crucial part of developing solutions which are tailored to specific industrial or open-source applications. Yet, there is no single tool which simplifies the process of training across different types of modalities or tasks. We introduce AutoTrain (aka AutoTrain Advanced) -- an open-source, no code tool/library which can be used to train (or finetune) models for different kinds of tasks such as: large language model (LLM) finetuning, text classification/regression, token classification, sequence-to-sequence task, finetuning of sentence transformers, visual language model (VLM) finetuning, image classification/regression and even classification and regression tasks on tabular data. AutoTrain Advanced is an open-source library providing best practices for training models on custom datasets. The library is available at https://github.com/huggingface/autotrain-advanced. AutoTrain can be used in fully local mode or on cloud machines and works with tens of thousands of models shared on Hugging Face Hub and their variations.', 'score': 39, 'issue_id': 209, 'pub_date': '2024-10-21', 'pub_date_ru': '21 октября', 'hash': '62b6ded4f9dc1d27', 'data': {'categories': ['#multimodal', '#training'], 'emoji': '🤖', 'ru': {'title': 'AutoTrain: Упрощение обучения ML-моделей для всех', 'desc': 'AutoTrain Advanced - это открытый инструмент для обучения моделей машинного обучения без написания кода. Он поддерживает различные задачи, включая дообучение больших языковых моделей, классификацию текста и изображений, и работу с табличными данными. AutoTrain использует лучшие практики для обучения на пользовательских датасетах и может работать как локально, так и в облаке. Инструмент совместим с тысячами моделей из Hugging Face Hub.'}, 'en': {'title': 'AutoTrain: Simplifying Model Training Across Modalities', 'desc': 'AutoTrain Advanced is a no-code, open-source tool designed to simplify the process of training and finetuning machine learning models across various tasks and modalities. It supports a wide range of applications, including large language models, text and image classification, and even tabular data tasks. The tool integrates with the Hugging Face Hub, allowing users to leverage a vast collection of pre-trained models. AutoTrain can be used both locally and on cloud platforms, making it versatile for different user needs.'}, 'zh': {'title': 'AutoTrain：无代码模型训练的革命', 'desc': 'AutoTrain是一个开源的无代码工具库，旨在简化不同任务和模态下的模型训练过程。它支持多种任务，包括大语言模型微调、文本分类、序列到序列任务、视觉语言模型微调等。用户可以在本地或云端使用AutoTrain，并与Hugging Face Hub上的数万个模型兼容。这个工具为在自定义数据集上训练模型提供了最佳实践。'}}, 'pub_date_card': {'ru': '21 октября', 'en': 'October 21', 'zh': '10月21日'}}, {'id': 'https://huggingface.co/papers/2410.14745', 'title': 'SemiEvol: Semi-supervised Fine-tuning for LLM Adaptation', 'url': 'https://huggingface.co/papers/2410.14745', 'abstract': 'Supervised fine-tuning (SFT) is crucial in adapting large language models (LLMs) to a specific domain or task. However, only a limited amount of labeled data is available in practical applications, which poses a severe challenge for SFT in yielding satisfactory results. Therefore, a data-efficient framework that can fully exploit labeled and unlabeled data for LLM fine-tuning is highly anticipated. Towards this end, we introduce a semi-supervised fine-tuning framework named SemiEvol for LLM adaptation from a propagate-and-select manner. For knowledge propagation, SemiEvol adopts a bi-level approach, propagating knowledge from labeled data to unlabeled data through both in-weight and in-context methods. For knowledge selection, SemiEvol incorporates a collaborative learning mechanism, selecting higher-quality pseudo-response samples. We conducted experiments using GPT-4o-mini and Llama-3.1 on seven general or domain-specific datasets, demonstrating significant improvements in model performance on target data. Furthermore, we compared SemiEvol with SFT and self-evolution methods, highlighting its practicality in hybrid data scenarios.', 'score': 35, 'issue_id': 207, 'pub_date': '2024-10-17', 'pub_date_ru': '17 октября', 'hash': '863b95d8e45c3aa2', 'data': {'categories': ['#dataset'], 'emoji': '🧠', 'ru': {'title': 'SemiEvol: эффективная полу-контролируемая настройка языковых моделей', 'desc': 'Статья представляет новый полу-контролируемый метод тонкой настройки больших языковых моделей под названием SemiEvol. Этот подход эффективно использует как размеченные, так и неразмеченные данные для адаптации модели к конкретной задаче или домену. SemiEvol применяет двухуровневый метод распространения знаний и механизм совместного обучения для отбора высококачественных псевдо-ответов. Эксперименты на семи наборах данных показали значительное улучшение производительности модели по сравнению с традиционными методами тонкой настройки.'}, 'en': {'title': 'SemiEvol: Maximizing Model Potential with Minimal Data', 'desc': "The paper introduces SemiEvol, a semi-supervised fine-tuning framework designed to adapt large language models using both labeled and unlabeled data. SemiEvol uses a bi-level approach to propagate knowledge from labeled to unlabeled data, enhancing the model's learning process. It also employs a collaborative learning mechanism to select high-quality pseudo-responses, improving the model's performance. Experiments show that SemiEvol outperforms traditional supervised fine-tuning and self-evolution methods, making it effective for tasks with limited labeled data."}, 'zh': {'title': 'SemiEvol：高效利用标注与未标注数据的半监督微调框架', 'desc': '这篇论文介绍了一种名为SemiEvol的半监督微调框架，用于大语言模型的适应。SemiEvol通过双层方法将标注数据的知识传播到未标注数据中，并通过协作学习机制选择高质量的伪响应样本。实验表明，SemiEvol在多种数据集上显著提高了模型性能。与传统的监督微调和自我进化方法相比，SemiEvol在混合数据场景中表现出更高的实用性。'}}, 'pub_date_card': {'ru': '17 октября', 'en': 'October 17', 'zh': '10月17日'}}, {'id': 'https://huggingface.co/papers/2410.14940', 'title': 'Baichuan Alignment Technical Report', 'url': 'https://huggingface.co/papers/2410.14940', 'abstract': "We introduce Baichuan Alignment, a detailed analysis of the alignment techniques employed in the Baichuan series of models. This represents the industry's first comprehensive account of alignment methodologies, offering valuable insights for advancing AI research. We investigate the critical components that enhance model performance during the alignment process, including optimization methods, data strategies, capability enhancements, and evaluation processes. The process spans three key stages: Prompt Augmentation System (PAS), Supervised Fine-Tuning (SFT), and Preference Alignment. The problems encountered, the solutions applied, and the improvements made are thoroughly recorded.   Through comparisons across well-established benchmarks, we highlight the technological advancements enabled by Baichuan Alignment. Baichuan-Instruct is an internal model, while Qwen2-Nova-72B and Llama3-PBM-Nova-70B are instruct versions of the Qwen2-72B and Llama-3-70B base models, optimized through Baichuan Alignment. Baichuan-Instruct demonstrates significant improvements in core capabilities, with user experience gains ranging from 17% to 28%, and performs exceptionally well on specialized benchmarks. In open-source benchmark evaluations, both Qwen2-Nova-72B and Llama3-PBM-Nova-70B consistently outperform their respective official instruct versions across nearly all datasets. This report aims to clarify the key technologies behind the alignment process, fostering a deeper understanding within the community. Llama3-PBM-Nova-70B model is available at https://huggingface.co/PKU-Baichuan-MLSystemLab/Llama3-PBM-Nova-70B.", 'score': 32, 'issue_id': 208, 'pub_date': '2024-10-19', 'pub_date_ru': '19 октября', 'hash': 'e1222b08a95a2911', 'data': {'categories': ['#alignment', '#benchmark', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Революция в выравнивании языковых моделей: метод Baichuan Alignment', 'desc': 'В статье представлен детальный анализ методов выравнивания (alignment), применяемых в серии моделей Baichuan. Описываются три ключевых этапа: система расширения промптов (PAS), контролируемая тонкая настройка (SFT) и выравнивание предпочтений. Исследуются критические компоненты, улучшающие производительность модели в процессе выравнивания, включая методы оптимизации, стратегии работы с данными и процессы оценки. Результаты показывают значительные улучшения в основных возможностях моделей и их превосходство над официальными версиями на различных бенчмарках.'}, 'en': {'title': "Aligning for Excellence: Baichuan's Path to Superior AI Models", 'desc': 'Baichuan Alignment is a comprehensive study of alignment techniques used in the Baichuan model series, providing insights to advance AI research. The paper explores key components that improve model performance during alignment, such as optimization methods, data strategies, and evaluation processes. The alignment process is divided into three stages: Prompt Augmentation System, Supervised Fine-Tuning, and Preference Alignment. The study shows that models optimized with Baichuan Alignment, like Baichuan-Instruct, achieve significant performance improvements and outperform other models on various benchmarks.'}, 'zh': {'title': 'Baichuan Alignment：AI 对齐技术的全面突破', 'desc': 'Baichuan Alignment 是对 Baichuan 系列模型中对齐技术的详细分析，首次全面记录了对齐方法。研究中探讨了优化方法、数据策略、能力增强和评估过程等关键组件。对齐过程分为三个阶段：提示增强系统、监督微调和偏好对齐。通过对比基准测试，展示了 Baichuan Alignment 带来的技术进步。'}}, 'pub_date_card': {'ru': '19 октября', 'en': 'October 19', 'zh': '10月19日'}}, {'id': 'https://huggingface.co/papers/2410.16153', 'title': 'Pangea: A Fully Open Multilingual Multimodal LLM for 39 Languages', 'url': 'https://huggingface.co/papers/2410.16153', 'abstract': "Despite recent advances in multimodal large language models (MLLMs), their development has predominantly focused on English- and western-centric datasets and tasks, leaving most of the world's languages and diverse cultural contexts underrepresented. This paper introduces Pangea, a multilingual multimodal LLM trained on PangeaIns, a diverse 6M instruction dataset spanning 39 languages. PangeaIns features: 1) high-quality English instructions, 2) carefully machine-translated instructions, and 3) culturally relevant multimodal tasks to ensure cross-cultural coverage. To rigorously assess models' capabilities, we introduce PangeaBench, a holistic evaluation suite encompassing 14 datasets covering 47 languages. Results show that Pangea significantly outperforms existing open-source models in multilingual settings and diverse cultural contexts. Ablation studies further reveal the importance of English data proportions, language popularity, and the number of multimodal training samples on overall performance. We fully open-source our data, code, and trained checkpoints, to facilitate the development of inclusive and robust multilingual MLLMs, promoting equity and accessibility across a broader linguistic and cultural spectrum.", 'score': 25, 'issue_id': 208, 'pub_date': '2024-10-21', 'pub_date_ru': '21 октября', 'hash': '023f2f5e4c6c7a7c', 'data': {'categories': ['#benchmark', '#data', '#dataset', '#multilingual', '#multimodal'], 'emoji': '🌍', 'ru': {'title': 'Pangea: Прорыв в мультиязычных и мультикультурных ИИ-моделях', 'desc': 'Статья представляет Pangea - мультиязычную мультимодальную языковую модель, обученную на разнообразном наборе данных PangeaIns, охватывающем 39 языков. Авторы также вводят PangeaBench - комплексный набор для оценки возможностей моделей на 47 языках. Результаты показывают, что Pangea значительно превосходит существующие модели с открытым исходным кодом в многоязычных настройках и разнообразных культурных контекстах. Исследование направлено на развитие инклюзивных и надежных мультиязычных мультимодальных языковых моделей, способствуя равенству и доступности в широком лингвистическом и культурном спектре.'}, 'en': {'title': "Breaking Language Barriers: Pangea's Multilingual Revolution", 'desc': "The paper presents Pangea, a multilingual multimodal large language model designed to address the underrepresentation of non-English languages and diverse cultural contexts in current models. Pangea is trained on a dataset called PangeaIns, which includes high-quality English instructions, machine-translated instructions, and culturally relevant tasks across 39 languages. The model's performance is evaluated using PangeaBench, a comprehensive suite of 14 datasets covering 47 languages, showing superior results compared to existing models. The study highlights the importance of data diversity and open-sources all resources to encourage the development of more inclusive language models."}, 'zh': {'title': 'Pangea：跨越语言和文化的多模态大语言模型', 'desc': '这篇论文介绍了一个名为Pangea的多语言多模态大语言模型，它使用了一个包含39种语言的多样化数据集PangeaIns进行训练。PangeaIns的数据集包括高质量的英语指令、精心机器翻译的指令以及与文化相关的多模态任务。为了评估模型的能力，研究者们还引入了PangeaBench，一个涵盖47种语言的评估套件。结果显示，Pangea在多语言和多文化背景下的表现优于现有的开源模型。'}}, 'pub_date_card': {'ru': '21 октября', 'en': 'October 21', 'zh': '10月21日'}}, {'id': 'https://huggingface.co/papers/2410.16184', 'title': 'RM-Bench: Benchmarking Reward Models of Language Models with Subtlety and Style', 'url': 'https://huggingface.co/papers/2410.16184', 'abstract': 'Reward models are critical in techniques like Reinforcement Learning from Human Feedback (RLHF) and Inference Scaling Laws, where they guide language model alignment and select optimal responses. Despite their importance, existing reward model benchmarks often evaluate models by asking them to distinguish between responses generated by models of varying power. However, this approach fails to assess reward models on subtle but critical content changes and variations in style, resulting in a low correlation with policy model performance. To this end, we introduce RM-Bench, a novel benchmark designed to evaluate reward models based on their sensitivity to subtle content differences and resistance to style biases. Extensive experiments demonstrate that RM-Bench strongly correlates with policy model performance, making it a reliable reference for selecting reward models to align language models effectively. We evaluate nearly 40 reward models on RM-Bench. Our results reveal that even state-of-the-art models achieve an average performance of only 46.6%, which falls short of random-level accuracy (50%) when faced with style bias interference. These findings highlight the significant room for improvement in current reward models. Related code and data are available at https://github.com/THU-KEG/RM-Bench.', 'score': 20, 'issue_id': 209, 'pub_date': '2024-10-21', 'pub_date_ru': '21 октября', 'hash': '6fff842f967d7207', 'data': {'categories': ['#alignment', '#benchmark', '#rlhf'], 'emoji': '🎯', 'ru': {'title': 'RM-Bench: точная оценка моделей вознаграждения для эффективной настройки языковых моделей', 'desc': 'Статья представляет новый бенчмарк RM-Bench для оценки моделей вознаграждения в контексте обучения с подкреплением по обратной связи от человека (RLHF) и законов масштабирования вывода. RM-Bench оценивает чувствительность моделей к тонким различиям в содержании и их устойчивость к стилистическим смещениям. Эксперименты показывают, что RM-Bench хорошо коррелирует с производительностью моделей политики, что делает его надежным инструментом для выбора моделей вознаграждения при настройке языковых моделей. Результаты демонстрируют, что даже современные модели вознаграждения достигают средней производительности всего 46.6%, что ниже случайного уровня точности при наличии стилистических смещений.'}, 'en': {'title': 'Enhancing Reward Models: Beyond Style Bias and Subtlety', 'desc': "The paper introduces RM-Bench, a new benchmark for evaluating reward models used in Reinforcement Learning from Human Feedback (RLHF) and Inference Scaling Laws. RM-Bench focuses on assessing models' sensitivity to subtle content changes and their resistance to style biases, which are often overlooked in traditional benchmarks. The study finds that even advanced reward models perform poorly, with an average accuracy of 46.6% when style bias is present, indicating a need for improvement. This benchmark provides a more reliable way to select reward models that align language models effectively."}, 'zh': {'title': '提升奖励模型：超越风格偏见的挑战', 'desc': '这篇论文介绍了一种新的基准测试RM-Bench，用于评估奖励模型在细微内容差异和风格偏见下的表现。现有的奖励模型基准测试往往只关注模型之间的简单区分，而忽略了对内容和风格变化的敏感性。RM-Bench通过实验表明，与策略模型表现有很强的相关性，是选择奖励模型的可靠参考。研究结果显示，即使是最先进的模型在面对风格偏见时，表现也仅为46.6%，低于随机水平，表明当前奖励模型还有很大的改进空间。'}}, 'pub_date_card': {'ru': '21 октября', 'en': 'October 21', 'zh': '10月21日'}}, {'id': 'https://huggingface.co/papers/2410.12788', 'title': 'Meta-Chunking: Learning Efficient Text Segmentation via Logical Perception', 'url': 'https://huggingface.co/papers/2410.12788', 'abstract': 'Retrieval-Augmented Generation (RAG), while serving as a viable complement to large language models (LLMs), often overlooks the crucial aspect of text chunking within its pipeline, which impacts the quality of knowledge-intensive tasks. This paper introduces the concept of Meta-Chunking, which refers to a granularity between sentences and paragraphs, consisting of a collection of sentences within a paragraph that have deep linguistic logical connections. To implement Meta-Chunking, we designed two strategies based on LLMs: Margin Sampling Chunking and Perplexity Chunking. The former employs LLMs to perform binary classification on whether consecutive sentences need to be segmented, making decisions based on the probability difference obtained from margin sampling. The latter precisely identifies text chunk boundaries by analyzing the characteristics of perplexity distribution. Additionally, considering the inherent complexity of different texts, we propose a strategy that combines Meta-Chunking with dynamic merging to achieve a balance between fine-grained and coarse-grained text chunking. Experiments conducted on eleven datasets demonstrate that Meta-Chunking can more efficiently improve the performance of single-hop and multi-hop question answering based on RAG. For instance, on the 2WikiMultihopQA dataset, it outperforms similarity chunking by 1.32 while only consuming 45.8% of the time. Our code is available at https://github.com/IAAR-Shanghai/Meta-Chunking.', 'score': 15, 'issue_id': 208, 'pub_date': '2024-10-16', 'pub_date_ru': '16 октября', 'hash': '0fbcf072cc98c553', 'data': {'categories': ['#data', '#dataset', '#rag'], 'emoji': '🧩', 'ru': {'title': 'Мета-Чанкинг: Новый подход к разбиению текста для повышения эффективности RAG', 'desc': 'Статья представляет концепцию Мета-Чанкинга для улучшения Retrieval-Augmented Generation (RAG). Мета-Чанкинг - это метод разбиения текста на фрагменты, учитывающий лингвистические связи между предложениями. Авторы предлагают две стратегии реализации: Margin Sampling Chunking и Perplexity Chunking, использующие большие языковые модели. Эксперименты на одиннадцати наборах данных показывают, что Мета-Чанкинг повышает эффективность RAG в задачах вопросно-ответных систем.'}, 'en': {'title': '"Meta-Chunking: Enhancing RAG with Smarter Text Segmentation"', 'desc': 'The paper introduces Meta-Chunking, a method to improve text chunking in Retrieval-Augmented Generation (RAG) systems by focusing on the logical connections between sentences within paragraphs. Two strategies, Margin Sampling Chunking and Perplexity Chunking, are developed using large language models to determine optimal chunk boundaries. These methods enhance the efficiency and accuracy of question answering tasks by balancing fine-grained and coarse-grained chunking. Experiments show that Meta-Chunking significantly improves performance while reducing processing time compared to traditional chunking methods.'}, 'zh': {'title': 'Meta-Chunking：提升文本分块的新方法', 'desc': '这篇论文介绍了一种新的文本分块方法，称为Meta-Chunking，它在句子和段落之间找到一种新的粒度。Meta-Chunking通过两种策略实现：边缘采样分块和困惑度分块，分别利用大语言模型进行二元分类和困惑度分布分析。为了适应不同文本的复杂性，论文还提出了一种结合动态合并的策略，以在细粒度和粗粒度分块之间取得平衡。实验结果表明，Meta-Chunking在提高基于RAG的单跳和多跳问答性能方面更有效率。'}}, 'pub_date_card': {'ru': '16 октября', 'en': 'October 16', 'zh': '10月16日'}}, {'id': 'https://huggingface.co/papers/2410.16215', 'title': 'Pre-training Distillation for Large Language Models: A Design Space Exploration', 'url': 'https://huggingface.co/papers/2410.16215', 'abstract': 'Knowledge distillation (KD) aims to transfer knowledge from a large teacher model to a smaller student model. Previous work applying KD in the field of large language models (LLMs) typically focused on the post-training phase, where the student LLM learns directly from instructions and corresponding responses generated by the teacher model. In this paper, we extend KD to the pre-training phase of LLMs, named pre-training distillation (PD). We first conduct a preliminary experiment using GLM-4-9B as the teacher LLM to distill a 1.9B parameter student LLM, validating the effectiveness of PD. Considering the key impact factors of distillation, we systematically explore the design space of pre-training distillation across four aspects: logits processing, loss selection, scaling law, and offline or online logits. We conduct extensive experiments to explore the design space of pre-training distillation and find better configurations and interesting conclusions, such as larger student LLMs generally benefiting more from pre-training distillation, while a larger teacher LLM does not necessarily guarantee better results. We hope our exploration of the design space will inform future practices in pre-training distillation.', 'score': 12, 'issue_id': 209, 'pub_date': '2024-10-21', 'pub_date_ru': '21 октября', 'hash': '0d2e73e11d2b2b3f', 'data': {'categories': ['#architecture'], 'emoji': '🧠', 'ru': {'title': 'Дистилляция знаний на этапе предобучения: новый подход к созданию эффективных языковых моделей', 'desc': 'Эта статья исследует применение дистилляции знаний (KD) на этапе предобучения больших языковых моделей (LLM). Авторы проводят эксперименты с использованием GLM-4-9B в качестве учителя для дистилляции студента с 1,9 миллиардами параметров. Они систематически изучают пространство дизайна предобучающей дистилляции по четырем аспектам: обработка логитов, выбор функции потерь, закон масштабирования и офлайн или онлайн логиты. Результаты показывают, что более крупные студенческие LLM обычно больше выигрывают от предобучающей дистилляции, в то время как более крупная модель-учитель не обязательно гарантирует лучшие результаты.'}, 'en': {'title': '"Pre-Training Distillation: A New Frontier in Model Efficiency"', 'desc': 'This paper explores a new approach to knowledge distillation by applying it during the pre-training phase of large language models, rather than the post-training phase. The authors conduct experiments using a large teacher model to distill knowledge into a smaller student model, demonstrating the effectiveness of this method. They investigate various factors that influence the success of pre-training distillation, such as how logits are processed and the choice of loss functions. The study reveals that larger student models benefit more from pre-training distillation, while the size of the teacher model does not always correlate with better outcomes.'}, 'zh': {'title': '预训练阶段的知识蒸馏：小模型的大智慧', 'desc': '这篇论文研究了如何在大语言模型的预训练阶段进行知识蒸馏。通过使用GLM-4-9B作为教师模型，验证了预训练蒸馏的有效性。研究发现，较大的学生模型通常能从预训练蒸馏中获益更多，而较大的教师模型不一定带来更好的结果。作者希望这些发现能为未来的预训练蒸馏实践提供指导。'}}, 'pub_date_card': {'ru': '21 октября', 'en': 'October 21', 'zh': '10月21日'}}, {'id': 'https://huggingface.co/papers/2410.15748', 'title': 'Alchemy: Amplifying Theorem-Proving Capability through Symbolic Mutation', 'url': 'https://huggingface.co/papers/2410.15748', 'abstract': 'Formal proofs are challenging to write even for experienced experts. Recent progress in Neural Theorem Proving (NTP) shows promise in expediting this process. However, the formal corpora available on the Internet are limited compared to the general text, posing a significant data scarcity challenge for NTP. To address this issue, this work proposes Alchemy, a general framework for data synthesis that constructs formal theorems through symbolic mutation. Specifically, for each candidate theorem in Mathlib, we identify all invocable theorems that can be used to rewrite or apply to it. Subsequently, we mutate the candidate theorem by replacing the corresponding term in the statement with its equivalent form or antecedent. As a result, our method increases the number of theorems in Mathlib by an order of magnitude, from 110k to 6M. Furthermore, we perform continual pretraining and supervised finetuning on this augmented corpus for large language models. Experimental results demonstrate the effectiveness of our approach, achieving a 5% absolute performance improvement on Leandojo benchmark. Additionally, our synthetic data achieve a 2.5% absolute performance gain on the out-of-distribution miniF2F benchmark. To provide further insights, we conduct a comprehensive analysis of synthetic data composition and the training paradigm, offering valuable guidance for developing a strong theorem prover.', 'score': 9, 'issue_id': 212, 'pub_date': '2024-10-21', 'pub_date_ru': '21 октября', 'hash': 'b6b22326cba00346', 'data': {'categories': ['#data', '#dataset', '#math', '#synthetic', '#training'], 'emoji': '🧪', 'ru': {'title': 'Алхимия данных: превращаем сотни тысяч теорем в миллионы', 'desc': 'Статья представляет Alchemy - фреймворк для синтеза данных в области нейронного доказательства теорем (NTP). Авторы предлагают метод символьной мутации для создания новых теорем на основе существующих в библиотеке Mathlib. Это позволяет увеличить объем обучающих данных с 110 тысяч до 6 миллионов теорем. Эксперименты показывают улучшение производительности больших языковых моделей на 5% в тесте Leandojo и на 2.5% на out-of-distribution данных miniF2F после дообучения на синтезированном корпусе.'}, 'en': {'title': 'Alchemy: Transforming Theorem Proving with Synthetic Data', 'desc': 'The paper introduces Alchemy, a framework designed to tackle the data scarcity problem in Neural Theorem Proving by generating synthetic formal theorems. By using symbolic mutation, Alchemy expands the number of theorems in Mathlib significantly, enhancing the training data available for large language models. The approach involves rewriting candidate theorems using invocable theorems, which increases the corpus size and improves model performance on benchmarks like Leandojo and miniF2F. The study also provides an analysis of the synthetic data and training methods, offering insights for building more effective theorem provers.'}, 'zh': {'title': 'Alchemy：通过数据合成增强神经定理证明', 'desc': '这篇论文介绍了一种名为Alchemy的数据合成框架，用于通过符号变异构建形式定理。通过识别和应用可调用的定理，Alchemy将Mathlib中的定理数量从11万增加到600万。然后，研究人员在扩展后的语料库上对大型语言模型进行持续预训练和监督微调。实验结果表明，这种方法在Leandojo基准上提高了5%的性能，并在miniF2F基准上提高了2.5%的性能。'}}, 'pub_date_card': {'ru': '21 октября', 'en': 'October 21', 'zh': '10月21日'}}, {'id': 'https://huggingface.co/papers/2410.15316', 'title': 'Ichigo: Mixed-Modal Early-Fusion Realtime Voice Assistant', 'url': 'https://huggingface.co/papers/2410.15316', 'abstract': 'Large Language Models (LLMs) have revolutionized natural language processing, but their application to speech-based tasks remains challenging due to the complexities of integrating audio and text modalities. This paper introduces Ichigo, a mixed-modal model that seamlessly processes interleaved sequences of speech and text. Utilizing a tokenized early-fusion approach, Ichigo quantizes speech into discrete tokens and employs a uniform transformer-based architecture for both speech and text modalities. This method enables joint reasoning and generation across modalities without the need for separate adapters. We present a comprehensive training methodology, including pre-training on multilingual speech recognition datasets and fine-tuning on a curated instruction dataset. Ichigo demonstrates state-of-the-art performance on speech question-answering benchmarks, outperforming existing open-source speech language models and achieving comparable results to cascaded systems. Notably, Ichigo exhibits a latency of just 111 ms to first token generation, significantly lower than current models. Our approach not only advances the field of multimodal AI but also provides a framework for smaller research teams to contribute effectively to open-source speech-language models.', 'score': 6, 'issue_id': 214, 'pub_date': '2024-10-20', 'pub_date_ru': '20 октября', 'hash': '0fbe64f20b885346', 'data': {'categories': ['#audio', '#multilingual', '#multimodal', '#training'], 'emoji': '🍓', 'ru': {'title': 'Ичиго: объединяя речь и текст в единой языковой модели', 'desc': 'Ичиго - это мультимодальная языковая модель, способная обрабатывать как речь, так и текст. Модель использует ранний подход к слиянию модальностей, квантуя речь в дискретные токены и применяя единую архитектуру трансформера для обеих модальностей. Ичиго демонстрирует передовые результаты в задачах ответов на вопросы по речи, превосходя существующие модели с открытым исходным кодом. Важным преимуществом модели является низкая задержка генерации первого токена - всего 111 мс.'}, 'en': {'title': 'Ichigo: Bridging Speech and Text with Unified AI', 'desc': 'The paper introduces Ichigo, a model that processes both speech and text together using a unified transformer architecture. By converting speech into tokens, Ichigo can handle both modalities without needing separate systems, allowing for efficient joint reasoning. The model is trained on diverse datasets, achieving top performance in speech question-answering tasks with low latency. This approach not only enhances multimodal AI but also empowers smaller teams to develop competitive open-source models.'}, 'zh': {'title': 'Ichigo：跨越语音与文本的无缝融合', 'desc': '这篇论文介绍了Ichigo，一种能够同时处理语音和文本的混合模态模型。Ichigo通过将语音量化为离散的token，并使用统一的transformer架构来处理语音和文本，从而实现了跨模态的联合推理和生成。该模型在多语言语音识别数据集上进行预训练，并在精心挑选的指令数据集上进行微调，表现出色。Ichigo在语音问答基准测试中表现优异，且首次生成token的延迟仅为111毫秒，显著低于现有模型。'}}, 'pub_date_card': {'ru': '20 октября', 'en': 'October 20', 'zh': '10月20日'}}, {'id': 'https://huggingface.co/papers/2410.11711', 'title': 'Zero-shot Model-based Reinforcement Learning using Large Language Models', 'url': 'https://huggingface.co/papers/2410.11711', 'abstract': "The emerging zero-shot capabilities of Large Language Models (LLMs) have led to their applications in areas extending well beyond natural language processing tasks. In reinforcement learning, while LLMs have been extensively used in text-based environments, their integration with continuous state spaces remains understudied. In this paper, we investigate how pre-trained LLMs can be leveraged to predict in context the dynamics of continuous Markov decision processes. We identify handling multivariate data and incorporating the control signal as key challenges that limit the potential of LLMs' deployment in this setup and propose Disentangled In-Context Learning (DICL) to address them. We present proof-of-concept applications in two reinforcement learning settings: model-based policy evaluation and data-augmented off-policy reinforcement learning, supported by theoretical analysis of the proposed methods. Our experiments further demonstrate that our approach produces well-calibrated uncertainty estimates. We release the code at https://github.com/abenechehab/dicl.", 'score': 6, 'issue_id': 212, 'pub_date': '2024-10-15', 'pub_date_ru': '15 октября', 'hash': '5868a5a1652a8da3', 'data': {'categories': ['#rl', '#rlhf'], 'emoji': '🤖', 'ru': {'title': 'LLM покоряют непрерывное пространство состояний в обучении с подкреплением', 'desc': 'В этой статье исследуется применение больших языковых моделей (LLM) для предсказания динамики непрерывных марковских процессов принятия решений. Авторы предлагают метод разделенного обучения в контексте (DICL) для решения проблем обработки многомерных данных и учета управляющего сигнала. Метод применяется в двух сценариях обучения с подкреплением: оценке политик на основе модели и обучении с дополнительными данными вне политики. Эксперименты показывают, что подход позволяет получить хорошо откалиброванные оценки неопределенности.'}, 'en': {'title': 'Unlocking LLMs for Continuous Reinforcement Learning', 'desc': 'This paper explores how Large Language Models (LLMs) can be used in reinforcement learning with continuous state spaces, a less-studied area. The authors propose a method called Disentangled In-Context Learning (DICL) to address challenges like handling multivariate data and incorporating control signals. They demonstrate the effectiveness of DICL in model-based policy evaluation and data-augmented off-policy reinforcement learning. The approach also provides well-calibrated uncertainty estimates, enhancing the reliability of predictions.'}, 'zh': {'title': '解锁大型语言模型在强化学习中的新潜力', 'desc': '这篇论文探讨了如何利用预训练的大型语言模型（LLMs）来预测连续马尔可夫决策过程中的动态。研究发现，处理多变量数据和整合控制信号是限制LLMs在这种环境中应用的关键挑战。为了解决这些问题，作者提出了一种名为解耦上下文学习（DICL）的新方法。实验结果表明，该方法能够提供良好的不确定性估计。'}}, 'pub_date_card': {'ru': '15 октября', 'en': 'October 15', 'zh': '10月15日'}}, {'id': 'https://huggingface.co/papers/2410.15633', 'title': "Selecting Influential Samples for Long Context Alignment via Homologous Models' Guidance and Contextual Awareness Measurement", 'url': 'https://huggingface.co/papers/2410.15633', 'abstract': "The expansion of large language models to effectively handle instructions with extremely long contexts has yet to be fully investigated. The primary obstacle lies in constructing a high-quality long instruction-following dataset devised for long context alignment. Existing studies have attempted to scale up the available data volume by synthesizing long instruction-following samples. However, indiscriminately increasing the quantity of data without a well-defined strategy for ensuring data quality may introduce low-quality samples and restrict the final performance. To bridge this gap, we aim to address the unique challenge of long-context alignment, i.e., modeling the long-range dependencies for handling instructions and lengthy input contexts. We propose GATEAU, a novel framework designed to identify the influential and high-quality samples enriched with long-range dependency relations by utilizing crafted Homologous Models' Guidance (HMG) and Contextual Awareness Measurement (CAM). Specifically, HMG attempts to measure the difficulty of generating corresponding responses due to the long-range dependencies, using the perplexity scores of the response from two homologous models with different context windows. Also, the role of CAM is to measure the difficulty of understanding the long input contexts due to long-range dependencies by evaluating whether the model's attention is focused on important segments. Built upon both proposed methods, we select the most challenging samples as the influential data to effectively frame the long-range dependencies, thereby achieving better performance of LLMs. Comprehensive experiments indicate that GATEAU effectively identifies samples enriched with long-range dependency relations and the model trained on these selected samples exhibits better instruction-following and long-context understanding capabilities.", 'score': 6, 'issue_id': 209, 'pub_date': '2024-10-21', 'pub_date_ru': '21 октября', 'hash': '0b470bc767fc516b', 'data': {'categories': ['#data', '#dataset', '#long_context', '#synthetic'], 'emoji': '🧠', 'ru': {'title': 'GATEAU: Умное обучение LLM на длинных контекстах', 'desc': "Этот научный труд представляет GATEAU - новую систему для улучшения способности больших языковых моделей (LLM) работать с длинными контекстами. GATEAU использует два метода: Homologous Models' Guidance (HMG) для измерения сложности генерации ответов, и Contextual Awareness Measurement (CAM) для оценки понимания длинного контекста. Система выбирает наиболее сложные и информативные образцы данных для обучения LLM. Эксперименты показывают, что модели, обученные на отобранных GATEAU данных, демонстрируют улучшенные способности в понимании длинных инструкций и контекстов."}, 'en': {'title': 'Mastering Long Contexts with GATEAU: Quality Over Quantity', 'desc': "The paper introduces GATEAU, a framework designed to improve large language models' ability to handle long instructions by focusing on high-quality data samples. It uses Homologous Models' Guidance (HMG) to assess the difficulty of generating responses with long-range dependencies and Contextual Awareness Measurement (CAM) to ensure the model's attention is on important segments. By selecting challenging samples, GATEAU enhances the model's performance in understanding and following long-context instructions. Experiments show that models trained with GATEAU-selected data perform better in long-context tasks."}, 'zh': {'title': 'GATEAU：提升长上下文指令处理的新框架', 'desc': '这篇论文探讨了如何让大型语言模型更好地处理长上下文的指令。主要挑战在于构建一个高质量的长指令数据集，以便模型能更好地对齐长上下文。为了解决这个问题，作者提出了一个名为GATEAU的新框架，通过同源模型指导和上下文意识测量来识别高质量样本。实验结果表明，使用这些样本训练的模型在指令跟随和长上下文理解方面表现更好。'}}, 'pub_date_card': {'ru': '21 октября', 'en': 'October 21', 'zh': '10月21日'}}, {'id': 'https://huggingface.co/papers/2410.13218', 'title': 'CBT-Bench: Evaluating Large Language Models on Assisting Cognitive Behavior Therapy', 'url': 'https://huggingface.co/papers/2410.13218', 'abstract': "There is a significant gap between patient needs and available mental health support today. In this paper, we aim to thoroughly examine the potential of using Large Language Models (LLMs) to assist professional psychotherapy. To this end, we propose a new benchmark, CBT-BENCH, for the systematic evaluation of cognitive behavioral therapy (CBT) assistance. We include three levels of tasks in CBT-BENCH: I: Basic CBT knowledge acquisition, with the task of multiple-choice questions; II: Cognitive model understanding, with the tasks of cognitive distortion classification, primary core belief classification, and fine-grained core belief classification; III: Therapeutic response generation, with the task of generating responses to patient speech in CBT therapy sessions. These tasks encompass key aspects of CBT that could potentially be enhanced through AI assistance, while also outlining a hierarchy of capability requirements, ranging from basic knowledge recitation to engaging in real therapeutic conversations. We evaluated representative LLMs on our benchmark. Experimental results indicate that while LLMs perform well in reciting CBT knowledge, they fall short in complex real-world scenarios requiring deep analysis of patients' cognitive structures and generating effective responses, suggesting potential future work.", 'score': 3, 'issue_id': 207, 'pub_date': '2024-10-17', 'pub_date_ru': '17 октября', 'hash': 'f4f24ef7771a745c', 'data': {'categories': ['#benchmark', '#medicine'], 'emoji': '🧠', 'ru': {'title': 'Оценка потенциала ИИ в когнитивно-поведенческой терапии', 'desc': 'Исследователи представили новый бенчмарк CBT-BENCH для оценки потенциала использования больших языковых моделей в когнитивно-поведенческой терапии (КПТ). Бенчмарк включает три уровня задач: базовые знания КПТ, понимание когнитивной модели и генерацию терапевтических ответов. Эксперименты показали, что языковые модели хорошо справляются с воспроизведением знаний КПТ, но испытывают трудности в сложных сценариях, требующих глубокого анализа когнитивных структур пациентов. Результаты указывают на потенциальные направления для будущих исследований в области применения ИИ в психотерапии.'}, 'en': {'title': 'Bridging the Gap: AI Meets Mental Health Support', 'desc': "This paper explores the use of Large Language Models (LLMs) to support psychotherapy, specifically cognitive behavioral therapy (CBT). It introduces CBT-BENCH, a benchmark designed to evaluate LLMs' ability to assist in CBT through tasks like knowledge acquisition, cognitive model understanding, and therapeutic response generation. The study finds that while LLMs can effectively recall CBT knowledge, they struggle with more complex tasks that require deep understanding and interaction. This highlights the need for further development to enhance LLMs' capabilities in real-world therapeutic settings."}, 'zh': {'title': '大型语言模型：心理治疗的未来助手？', 'desc': '这篇论文探讨了使用大型语言模型（LLMs）来辅助专业心理治疗的潜力。为此，作者提出了一个新的基准测试，名为CBT-BENCH，用于系统评估认知行为疗法（CBT）的辅助能力。CBT-BENCH包括三个任务层次：基础知识获取、认知模型理解和治疗性回应生成。实验结果表明，虽然LLMs在知识背诵方面表现良好，但在需要深入分析患者认知结构和生成有效回应的复杂场景中表现不足。'}}, 'pub_date_card': {'ru': '17 октября', 'en': 'October 17', 'zh': '10月17日'}}, {'id': 'https://huggingface.co/papers/2410.16259', 'title': 'Agent-to-Sim: Learning Interactive Behavior Models from Casual Longitudinal Videos', 'url': 'https://huggingface.co/papers/2410.16259', 'abstract': 'We present Agent-to-Sim (ATS), a framework for learning interactive behavior models of 3D agents from casual longitudinal video collections. Different from prior works that rely on marker-based tracking and multiview cameras, ATS learns natural behaviors of animal and human agents non-invasively through video observations recorded over a long time-span (e.g., a month) in a single environment. Modeling 3D behavior of an agent requires persistent 3D tracking (e.g., knowing which point corresponds to which) over a long time period. To obtain such data, we develop a coarse-to-fine registration method that tracks the agent and the camera over time through a canonical 3D space, resulting in a complete and persistent spacetime 4D representation. We then train a generative model of agent behaviors using paired data of perception and motion of an agent queried from the 4D reconstruction. ATS enables real-to-sim transfer from video recordings of an agent to an interactive behavior simulator. We demonstrate results on pets (e.g., cat, dog, bunny) and human given monocular RGBD videos captured by a smartphone.', 'score': 2, 'issue_id': 216, 'pub_date': '2024-10-21', 'pub_date_ru': '21 октября', 'hash': '2af6ae782ab41cc9', 'data': {'categories': ['#3d', '#agents', '#cv'], 'emoji': '🎥', 'ru': {'title': 'От видео к виртуальному поведению: обучение 3D-агентов на основе естественных наблюдений', 'desc': 'Представлена система Agent-to-Sim (ATS) для обучения интерактивных моделей поведения 3D-агентов на основе обычных видеоколлекций. ATS использует неинвазивный подход, анализируя естественное поведение животных и людей по видеозаписям, сделанным в течение длительного периода в одной среде. Разработан метод регистрации от грубой к точной для отслеживания агента и камеры во времени через каноническое 3D-пространство. На основе полученных данных обучается генеративная модель поведения агента, позволяющая создавать интерактивный симулятор поведения.'}, 'en': {'title': 'From Video to Virtual: Simulating Real-World Behaviors', 'desc': "The paper introduces Agent-to-Sim (ATS), a framework for learning 3D behavior models of agents from long-term video recordings without using invasive tracking methods. ATS uses a novel coarse-to-fine registration technique to maintain persistent 3D tracking of agents over time, creating a comprehensive 4D representation. This data is then used to train a generative model that simulates the agent's behavior based on its perception and motion. The framework successfully transfers real-world video observations into interactive behavior simulations, demonstrated with pets and humans using simple smartphone recordings."}, 'zh': {'title': '从视频到模拟：非侵入性3D行为建模', 'desc': '这篇论文介绍了一种名为Agent-to-Sim (ATS)的框架，用于从长期视频中学习3D代理的交互行为模型。与依赖标记跟踪和多视角相机的传统方法不同，ATS通过单一环境中长时间的视频观察，非侵入性地学习动物和人类代理的自然行为。为了实现3D行为建模，研究人员开发了一种粗到细的注册方法，通过一个标准的3D空间跟踪代理和相机，形成完整的时空4D表示。最终，ATS可以将视频记录中的代理行为转移到交互行为模拟器中。'}}, 'pub_date_card': {'ru': '21 октября', 'en': 'October 21', 'zh': '10月21日'}}, {'id': 'https://huggingface.co/papers/2410.15002', 'title': 'How Many Van Goghs Does It Take to Van Gogh? Finding the Imitation Threshold', 'url': 'https://huggingface.co/papers/2410.15002', 'abstract': "Text-to-image models are trained using large datasets collected by scraping image-text pairs from the internet. These datasets often include private, copyrighted, and licensed material. Training models on such datasets enables them to generate images with such content, which might violate copyright laws and individual privacy. This phenomenon is termed imitation -- generation of images with content that has recognizable similarity to its training images. In this work we study the relationship between a concept's frequency in the training dataset and the ability of a model to imitate it. We seek to determine the point at which a model was trained on enough instances to imitate a concept -- the imitation threshold. We posit this question as a new problem: Finding the Imitation Threshold (FIT) and propose an efficient approach that estimates the imitation threshold without incurring the colossal cost of training multiple models from scratch. We experiment with two domains -- human faces and art styles -- for which we create four datasets, and evaluate three text-to-image models which were trained on two pretraining datasets. Our results reveal that the imitation threshold of these models is in the range of 200-600 images, depending on the domain and the model. The imitation threshold can provide an empirical basis for copyright violation claims and acts as a guiding principle for text-to-image model developers that aim to comply with copyright and privacy laws. We release the code and data at https://github.com/vsahil/MIMETIC-2.git and the project's website is hosted at https://how-many-van-goghs-does-it-take.github.io.", 'score': 1, 'issue_id': 216, 'pub_date': '2024-10-19', 'pub_date_ru': '19 октября', 'hash': 'f538a28b342207ac', 'data': {'categories': ['#cv', '#data', '#dataset', '#ethics'], 'emoji': '🖼️', 'ru': {'title': 'Сколько Ван Гогов нужно, чтобы обучить ИИ?', 'desc': "Статья исследует проблему имитации в моделях генерации изображений по тексту. Авторы вводят понятие 'порога имитации' - минимального количества примеров, необходимого модели для воспроизведения определенного концепта. Исследование показало, что для современных моделей этот порог составляет 200-600 изображений в зависимости от домена и архитектуры. Результаты имеют важное значение для соблюдения авторских прав и конфиденциальности при разработке генеративных моделей."}, 'en': {'title': 'Finding the Imitation Threshold: Balancing Creativity and Copyright in AI', 'desc': 'This paper explores how text-to-image models can generate images that closely resemble copyrighted or private content from their training datasets, a phenomenon known as imitation. The authors introduce the concept of the imitation threshold, which is the minimum number of instances a model needs to see in its training data to replicate a concept. They propose a method to estimate this threshold efficiently, without the need to train multiple models from scratch. Their experiments show that the imitation threshold varies between 200-600 images, providing a basis for understanding potential copyright violations and guiding model developers in legal compliance.'}, 'zh': {'title': '揭示文本到图像模型的模仿阈值', 'desc': '这篇论文研究了文本到图像模型在训练数据集中模仿概念的能力。研究发现，当一个概念在训练数据集中出现200到600次时，模型就能有效模仿该概念，这被称为模仿阈值。通过找到模仿阈值，可以帮助模型开发者遵循版权和隐私法律。研究结果为版权侵权的判断提供了实证依据。'}}, 'pub_date_card': {'ru': '19 октября', 'en': 'October 19', 'zh': '10月19日'}}, {'id': 'https://huggingface.co/papers/2410.14086', 'title': "In-context learning and Occam's razor", 'url': 'https://huggingface.co/papers/2410.14086', 'abstract': "The goal of machine learning is generalization. While the No Free Lunch Theorem states that we cannot obtain theoretical guarantees for generalization without further assumptions, in practice we observe that simple models which explain the training data generalize best: a principle called Occam's razor. Despite the need for simple models, most current approaches in machine learning only minimize the training error, and at best indirectly promote simplicity through regularization or architecture design. Here, we draw a connection between Occam's razor and in-context learning: an emergent ability of certain sequence models like Transformers to learn at inference time from past observations in a sequence. In particular, we show that the next-token prediction loss used to train in-context learners is directly equivalent to a data compression technique called prequential coding, and that minimizing this loss amounts to jointly minimizing both the training error and the complexity of the model that was implicitly learned from context. Our theory and the empirical experiments we use to support it not only provide a normative account of in-context learning, but also elucidate the shortcomings of current in-context learning methods, suggesting ways in which they can be improved. We make our code available at https://github.com/3rdCore/PrequentialCode.", 'score': 1, 'issue_id': 214, 'pub_date': '2024-10-17', 'pub_date_ru': '17 октября', 'hash': '6f9c84cdfe1502b2', 'data': {'categories': ['#interpretability', '#math'], 'emoji': '🪒', 'ru': {'title': 'Бритва Оккама в действии: как обучение в контексте воплощает принцип простоты', 'desc': "Статья исследует связь между бритвой Оккама и обучением в контексте в машинном обучении. Авторы показывают, что функция потерь для предсказания следующего токена эквивалентна методу сжатия данных под названием 'прекваентиальное кодирование'. Минимизация этой функции потерь одновременно минимизирует ошибку обучения и сложность модели, неявно изученной из контекста. Исследование предлагает нормативное объяснение обучения в контексте и указывает на недостатки текущих методов, предлагая пути их улучшения."}, 'en': {'title': "Simplifying Complexity: Bridging Occam's Razor and In-Context Learning", 'desc': "The paper explores the connection between Occam's razor, which favors simpler models for better generalization, and in-context learning, a feature of sequence models like Transformers. It demonstrates that the next-token prediction loss used in training these models is akin to prequential coding, a data compression method. This approach effectively minimizes both training error and model complexity, aligning with the principle of Occam's razor. The authors provide theoretical insights and empirical evidence, highlighting the limitations of current in-context learning methods and suggesting improvements."}, 'zh': {'title': '奥卡姆剃刀与上下文学习的巧妙结合', 'desc': '机器学习的目标是实现泛化，即模型不仅能在训练数据上表现良好，还能在新数据上表现出色。奥卡姆剃刀原则指出，简单的模型往往能更好地泛化，但目前大多数方法只关注最小化训练误差。本文将奥卡姆剃刀与上下文学习联系起来，展示了如何通过最小化下一个词预测损失来同时减少训练误差和模型复杂性。我们的理论和实验揭示了当前上下文学习方法的不足，并提出了改进建议。'}}, 'pub_date_card': {'ru': '17 октября', 'en': 'October 17', 'zh': '10月17日'}}, {'id': 'https://huggingface.co/papers/2410.13184', 'title': 'Router-Tuning: A Simple and Effective Approach for Enabling Dynamic-Depth in Transformers', 'url': 'https://huggingface.co/papers/2410.13184', 'abstract': "Traditional transformer models often allocate a fixed amount of computational resources to every input token, leading to inefficient and unnecessary computation. To address this, the Mixture of Depths (MoD) was introduced to dynamically adjust the computational depth by skipping less important layers. Despite its promise, current MoD approaches remain under-explored and face two main challenges: (1) high training costs due to the need to train the entire model along with the routers that determine which layers to skip, and (2) the risk of performance degradation when important layers are bypassed. In response to the first issue, we propose Router-Tuning, a method that fine-tunes only the router on a small dataset, drastically reducing the computational overhead associated with full model training. For the second challenge, we propose MindSkip, which deploys Attention with Dynamic Depths. This method preserves the model's performance while significantly enhancing computational and memory efficiency. Extensive experiments demonstrate that our approach delivers competitive results while dramatically improving the computation efficiency, e.g., 21\\% speedup and only a 0.2\\% performance drop. The code is released at https://github.com/CASE-Lab-UMD/Router-Tuning.", 'score': 1, 'issue_id': 214, 'pub_date': '2024-10-17', 'pub_date_ru': '17 октября', 'hash': '0e28d555da53ebff', 'data': {'categories': ['#architecture', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'Эффективные трансформеры: умное пропускание слоев без потери качества', 'desc': 'В статье предлагается метод Router-Tuning для оптимизации вычислительных ресурсов в трансформерных моделях. Авторы разработали подход MindSkip, использующий внимание с динамической глубиной, что позволяет сохранить производительность модели при повышении эффективности. Эксперименты показали 21% ускорение работы при минимальном снижении качества на 0.2%. Предложенный метод решает проблемы высоких затрат на обучение и риска пропуска важных слоев в существующих подходах Mixture of Depths.'}, 'en': {'title': 'Efficient Transformers: Skipping the Unnecessary', 'desc': 'The paper introduces a novel approach to improve the efficiency of transformer models by using a Mixture of Depths (MoD) strategy, which dynamically adjusts computational depth by skipping less important layers. To tackle the high training costs associated with MoD, the authors propose Router-Tuning, a method that fine-tunes only the router on a small dataset, reducing computational overhead. To prevent performance degradation, they introduce MindSkip, which uses Attention with Dynamic Depths to maintain model performance while enhancing efficiency. Experiments show that their approach achieves a 21% speedup with only a 0.2% performance drop, demonstrating its effectiveness in improving computation efficiency.'}, 'zh': {'title': '动态深度：提升Transformer计算效率的新方法', 'desc': '传统的Transformer模型对每个输入标记分配固定的计算资源，导致计算效率低下。为了解决这个问题，引入了深度混合（MoD）方法，通过跳过不重要的层来动态调整计算深度。然而，现有的MoD方法面临高训练成本和性能下降的风险。我们提出了Router-Tuning和MindSkip方法，分别通过微调路由器和使用动态深度注意力来解决这些问题，显著提高了计算效率。'}}, 'pub_date_card': {'ru': '17 октября', 'en': 'October 17', 'zh': '10月17日'}}, {'id': 'https://huggingface.co/papers/2410.15460', 'title': 'Hallucination Detox: Sensitive Neuron Dropout (SeND) for Large Language Model Training', 'url': 'https://huggingface.co/papers/2410.15460', 'abstract': 'As large language models (LLMs) become increasingly deployed across various industries, concerns regarding their reliability, particularly due to hallucinations-outputs that are factually inaccurate or irrelevant to user input-have grown. Our research investigates the relationship between the training process and the emergence of hallucinations to address a key gap in existing research that focuses primarily on post hoc detection and mitigation strategies. Using models from the Pythia suite (70M-12B parameters) and several hallucination detection metrics, we analyze hallucination trends throughout training and explore LLM internal dynamics. We introduce SEnsitive Neuron Dropout (SeND), a novel training protocol designed to mitigate hallucinations by reducing variance during training. SeND achieves this by deterministically dropping neurons with significant variability on a dataset, referred to as Sensitive Neurons. In addition, we develop an unsupervised hallucination detection metric, Efficient EigenScore (EES), which approximates the traditional EigenScore in 2x speed. This efficient metric is integrated into our protocol, allowing SeND to be both computationally scalable and effective at reducing hallucinations. Our empirical evaluation demonstrates that our approach improves LLM reliability at test time by up to 40% compared to normal training while also providing an efficient method to improve factual accuracy when adapting LLMs to domains such as Wikipedia and Medical datasets.', 'score': 1, 'issue_id': 214, 'pub_date': '2024-10-20', 'pub_date_ru': '20 октября', 'hash': '5c4104bbacb9ac91', 'data': {'categories': ['#hallucinations', '#interpretability', '#medicine', '#training'], 'emoji': '🧠', 'ru': {'title': 'Борьба с галлюцинациями в LLM через управление чувствительными нейронами', 'desc': 'Исследование изучает связь между процессом обучения и возникновением галлюцинаций в больших языковых моделях (LLM). Авторы анализируют тренды галлюцинаций на протяжении обучения, используя модели из набора Pythia и несколько метрик обнаружения галлюцинаций. Они представляют новый протокол обучения SeND, который снижает вариативность во время обучения путем детерминированного исключения чувствительных нейронов. Также разработана несупервизорная метрика обнаружения галлюцинаций EES, которая интегрирована в протокол для повышения его эффективности и масштабируемости.'}, 'en': {'title': '"Training Smarter: Reducing Hallucinations in Language Models"', 'desc': 'The paper explores how large language models (LLMs) sometimes produce incorrect or irrelevant outputs, known as hallucinations, and how these can be reduced during the training process. The researchers introduce a new training method called Sensitive Neuron Dropout (SeND), which helps decrease hallucinations by selectively dropping neurons that show high variability. They also develop a fast, unsupervised metric called Efficient EigenScore (EES) to detect hallucinations more quickly. Their approach shows a significant improvement in the reliability and factual accuracy of LLMs, especially when applied to specific domains like Wikipedia and medical datasets.'}, 'zh': {'title': '减少幻觉，提升语言模型可靠性', 'desc': '随着大型语言模型在各行业的广泛应用，人们对其可靠性，尤其是幻觉现象的担忧日益增加。我们的研究探讨了训练过程与幻觉现象之间的关系，填补了现有研究主要集中于事后检测和缓解策略的空白。我们引入了一种新的训练协议，称为敏感神经元丢弃（SeND），通过在训练中有选择地丢弃变异性大的神经元来减少幻觉。我们还开发了一种无监督的幻觉检测指标，称为高效特征值分数（EES），使得SeND在减少幻觉的同时具有计算可扩展性。'}}, 'pub_date_card': {'ru': '20 октября', 'en': 'October 20', 'zh': '10月20日'}}, {'id': 'https://huggingface.co/papers/2410.13394', 'title': 'Cross-Lingual Auto Evaluation for Assessing Multilingual LLMs', 'url': 'https://huggingface.co/papers/2410.13394', 'abstract': 'Evaluating machine-generated text remains a significant challenge in NLP, especially for non-English languages. Current methodologies, including automated metrics, human assessments, and LLM-based evaluations, predominantly focus on English, revealing a significant gap in multilingual evaluation frameworks. We introduce the Cross Lingual Auto Evaluation (CIA) Suite, an extensible framework that includes evaluator LLMs (Hercule) and a novel test set (Recon) specifically designed for multilingual evaluation. Our test set features 500 human-annotated instructions spanning various task capabilities along with human judgment scores across six languages. This would enable benchmarking of general-purpose multilingual LLMs and facilitate meta-evaluation of Evaluator LLMs. The proposed model, Hercule, is a cross-lingual evaluation model that addresses the scarcity of reference answers in the target language by learning to assign scores to responses based on easily available reference answers in English. Our experiments demonstrate that Hercule aligns more closely with human judgments compared to proprietary models, demonstrating the effectiveness of such cross-lingual evaluation in low resource scenarios. Further, it is also effective in zero-shot evaluation on unseen languages. This study is the first comprehensive examination of cross-lingual evaluation using LLMs, presenting a scalable and effective approach for multilingual assessment. All code, datasets, and models will be publicly available to enable further research in this important area.', 'score': 1, 'issue_id': 208, 'pub_date': '2024-10-17', 'pub_date_ru': '17 октября', 'hash': 'd456f53989a80b51', 'data': {'categories': ['#benchmark', '#multilingual', '#translation'], 'emoji': '🌐', 'ru': {'title': 'Преодоление языковых барьеров в оценке ИИ', 'desc': 'Статья представляет новый фреймворк CIA Suite для многоязычной оценки генеративных языковых моделей. Он включает в себя модель-оценщик Hercule и тестовый набор Recon с 500 аннотированными инструкциями на шести языках. Hercule способна оценивать ответы на целевом языке, используя эталонные ответы на английском, что решает проблему нехватки ресурсов. Эксперименты показывают, что Hercule лучше соответствует оценкам людей, чем проприетарные модели, и эффективна даже для ранее не виденных языков.'}, 'en': {'title': 'Breaking Language Barriers in NLP Evaluation', 'desc': "The paper addresses the challenge of evaluating machine-generated text in multiple languages, which is often overlooked in favor of English. It introduces the Cross Lingual Auto Evaluation (CIA) Suite, featuring the Hercule model and a novel test set called Recon, designed for multilingual evaluation. Hercule is a cross-lingual evaluation model that uses English reference answers to score responses in other languages, aligning closely with human judgments. The study demonstrates Hercule's effectiveness in low-resource and zero-shot scenarios, marking a significant step forward in multilingual NLP evaluation."}, 'zh': {'title': '跨语言评估的新突破：Hercule模型的多语言评估', 'desc': '这篇论文探讨了在自然语言处理领域中，评估机器生成文本的挑战，尤其是在非英语语言中。作者提出了一个名为跨语言自动评估套件（CIA）的框架，其中包括一个新的评估模型Hercule和一个多语言测试集Recon。Hercule模型通过学习从英语参考答案中评分，解决了目标语言中缺乏参考答案的问题。实验表明，Hercule在低资源场景中与人类判断更为一致，并且在未见过的语言上也能进行零样本评估。'}}, 'pub_date_card': {'ru': '17 октября', 'en': 'October 17', 'zh': '10月17日'}}, {'id': 'https://huggingface.co/papers/2410.15017', 'title': 'DM-Codec: Distilling Multimodal Representations for Speech Tokenization', 'url': 'https://huggingface.co/papers/2410.15017', 'abstract': 'Recent advancements in speech-language models have yielded significant improvements in speech tokenization and synthesis. However, effectively mapping the complex, multidimensional attributes of speech into discrete tokens remains challenging. This process demands acoustic, semantic, and contextual information for precise speech representations. Existing speech representations generally fall into two categories: acoustic tokens from audio codecs and semantic tokens from speech self-supervised learning models. Although recent efforts have unified acoustic and semantic tokens for improved performance, they overlook the crucial role of contextual representation in comprehensive speech modeling. Our empirical investigations reveal that the absence of contextual representations results in elevated Word Error Rate (WER) and Word Information Lost (WIL) scores in speech transcriptions. To address these limitations, we propose two novel distillation approaches: (1) a language model (LM)-guided distillation method that incorporates contextual information, and (2) a combined LM and self-supervised speech model (SM)-guided distillation technique that effectively distills multimodal representations (acoustic, semantic, and contextual) into a comprehensive speech tokenizer, termed DM-Codec. The DM-Codec architecture adopts a streamlined encoder-decoder framework with a Residual Vector Quantizer (RVQ) and incorporates the LM and SM during the training process. Experiments show DM-Codec significantly outperforms state-of-the-art speech tokenization models, reducing WER by up to 13.46%, WIL by 9.82%, and improving speech quality by 5.84% and intelligibility by 1.85% on the LibriSpeech benchmark dataset. The code, samples, and model checkpoints are available at https://github.com/mubtasimahasan/DM-Codec.', 'score': 1, 'issue_id': 208, 'pub_date': '2024-10-19', 'pub_date_ru': '19 октября', 'hash': 'd2c6c349adfb4796', 'data': {'categories': ['#audio', '#benchmark', '#multimodal'], 'emoji': '🗣️', 'ru': {'title': 'DM-Codec: Революция в токенизации речи с использованием мультимодальных представлений', 'desc': 'В статье представлен новый подход к токенизации речи, названный DM-Codec. Он объединяет акустическую, семантическую и контекстную информацию для создания более точных речевых представлений. Авторы предлагают два метода дистилляции: с использованием языковой модели и комбинированный метод с языковой моделью и самообучающейся речевой моделью. Эксперименты показывают, что DM-Codec значительно превосходит современные модели токенизации речи, снижая ошибки распознавания и улучшая качество и разборчивость речи.'}, 'en': {'title': '"DM-Codec: Revolutionizing Speech Tokenization with Contextual Intelligence"', 'desc': 'This paper addresses the challenge of effectively mapping speech into discrete tokens by incorporating acoustic, semantic, and contextual information. The authors propose two novel distillation methods that integrate language models and self-supervised speech models to create a comprehensive speech tokenizer called DM-Codec. The DM-Codec uses a streamlined encoder-decoder framework with a Residual Vector Quantizer to improve speech tokenization. Experiments demonstrate that DM-Codec significantly reduces Word Error Rate and Word Information Lost, enhancing speech quality and intelligibility.'}, 'zh': {'title': 'DM-Codec：融合多模态信息的语音标记化新突破', 'desc': '这篇论文探讨了语音模型中语音标记化和合成的最新进展，强调了将复杂的语音属性映射为离散标记的挑战。研究发现，缺乏上下文表示会导致语音转录中的词错误率和词信息丢失率升高。为解决这些问题，作者提出了两种新的蒸馏方法，结合语言模型和自监督语音模型来改进语音标记器。实验结果表明，所提出的DM-Codec模型在语音标记化性能上显著优于现有模型。'}}, 'pub_date_card': {'ru': '19 октября', 'en': 'October 19', 'zh': '10月19日'}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            const themeToggle = document.getElementById('theme-toggle');
            let settingSortBy = localStorage.getItem('sort_by');
            const sortDropdown = document.getElementById('sort-dropdown');
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }
            
            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (1)', '#agi', '#alignment (3)', '#architecture (2)', '#audio (2)', '#benchmark (8)', '#cv (5)', '#data (5)', '#dataset (6)', '#diffusion', '#edge_computing', '#ethics (1)', '#games', '#graphs', '#hallucinations (1)', '#inference', '#interpretability (2)', '#long_context (1)', '#math (2)', '#medicine (2)', '#multilingual (3)', '#multimodal (5)', '#optimization (2)', '#plp', '#rag (1)', '#reasoning', '#rl (1)', '#rlhf (2)', '#robotics', '#security', '#story_generation', '#survey', '#synthetic (2)', '#training (4)', '#transfer_learning', '#translation (1)', '#video (1)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles = selectedCategories.length === 0
                ? articlesData
                : articlesData.filter(article => 
                    article.data && article.data.categories && 
                    article.data.categories.some(cat => selectedCategories.includes(cat))
                );

            console.log('filteredArticles', filteredArticles)

            //if (filteredArticles.length === 0) {
            //    selectedArticles = articlesData;
            //    selectedCategories = [];
            //    cleanCategorySelection();
            //} else {
            //    selectedArticles = filteredArticles;
            //}

            selectedArticles = filteredArticles;

            console.log('selectedArticles', selectedArticles)

            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].join(" ");
                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="background-digit">${index + 1}</div>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <h2>${item['data']['emoji']} ${item['title']}</h2>
                            <p class="meta"><svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path transform="translate(0, 2)" fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> ${item['score']}. ${title}</p>
                            <p class="pub-date">📝 ${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>
                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>
                            <p class="tags">${cats}</p>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            }
            if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
        });

        clearCategoriesButton.addEventListener('click', clearAllCategories);
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2024-10-22 20:13',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];       
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink() {
            if (isToday('2024-10-22 20:13')) {
                const element = document.getElementById('nav-next');
                if (element) {    
                    element.style.display = 'none';
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink(); 
        initializeLanguageFlags();
        updateLocalization();
    </script>
</body>
</html>
    