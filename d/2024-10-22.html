
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 23 papers. October 22.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #0989eacf;
            --secondary-color: #fff;
            --background-color: #f5f5f5;
            --text-color: #333333;
            --header-color: #0989eacf;
            --body-color: #f5f5f5;
            --menu-color: #002370;
        }        
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 0;
            line-height: 1;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 2em;
            padding: 10px 0 20px 0;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.tags {
            color: #555;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);
            transition: background-color 0.2s ease;
            display: flex;
            flex-direction: column;
            position: relative;
        }
        .article-content {
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
        }
        body.dark-theme>div>main>article {
            background-color: #444;
        }
        body.light-theme>div>main>article {
            background-color: #fff;
        }
        body.dark-theme>div>main>article:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
        }
        .pub-date {
            font-size: 0.9em;
            margin-bottom: 0.8em;
            font-weight: 300;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 1em;
            position: absolute;
            bottom: 10px;
            font-weight: 300;
            font-family: 'Roboto Slab';
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 80px;
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        a:hover {
            color: #e73838;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            padding: 1em 0;
            margin-top: 2em;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 0px;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }
        
        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        body.light-theme>div>main>article.xb3e061a400165087 { background: url("https://hfday.ru/img/20241021/b3e061a400165087.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.xb3e061a400165087:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.xb3e061a400165087 { background: url("https://hfday.ru/img/20241021/b3e061a400165087.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.xb3e061a400165087:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x4daeb1e76417273d { background: url("https://hfday.ru/img/20241021/4daeb1e76417273d.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x4daeb1e76417273d:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x4daeb1e76417273d { background: url("https://hfday.ru/img/20241021/4daeb1e76417273d.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x4daeb1e76417273d:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x3b5265f629378d5d { background: url("https://hfday.ru/img/20241021/3b5265f629378d5d.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x3b5265f629378d5d:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x3b5265f629378d5d { background: url("https://hfday.ru/img/20241021/3b5265f629378d5d.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x3b5265f629378d5d:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x31995f0502d70f2f { background: url("https://hfday.ru/img/20241017/31995f0502d70f2f.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x31995f0502d70f2f:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x31995f0502d70f2f { background: url("https://hfday.ru/img/20241017/31995f0502d70f2f.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x31995f0502d70f2f:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x62b6ded4f9dc1d27 { background: url("https://hfday.ru/img/20241021/62b6ded4f9dc1d27.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x62b6ded4f9dc1d27:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x62b6ded4f9dc1d27 { background: url("https://hfday.ru/img/20241021/62b6ded4f9dc1d27.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x62b6ded4f9dc1d27:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.xe1222b08a95a2911 { background: url("https://hfday.ru/img/20241019/e1222b08a95a2911.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.xe1222b08a95a2911:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.xe1222b08a95a2911 { background: url("https://hfday.ru/img/20241019/e1222b08a95a2911.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.xe1222b08a95a2911:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x863b95d8e45c3aa2 { background: url("https://hfday.ru/img/20241017/863b95d8e45c3aa2.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x863b95d8e45c3aa2:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x863b95d8e45c3aa2 { background: url("https://hfday.ru/img/20241017/863b95d8e45c3aa2.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x863b95d8e45c3aa2:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x023f2f5e4c6c7a7c { background: url("https://hfday.ru/img/20241021/023f2f5e4c6c7a7c.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x023f2f5e4c6c7a7c:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x023f2f5e4c6c7a7c { background: url("https://hfday.ru/img/20241021/023f2f5e4c6c7a7c.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x023f2f5e4c6c7a7c:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x6fff842f967d7207 { background: url("https://hfday.ru/img/20241021/6fff842f967d7207.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x6fff842f967d7207:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x6fff842f967d7207 { background: url("https://hfday.ru/img/20241021/6fff842f967d7207.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x6fff842f967d7207:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x0fbcf072cc98c553 { background: url("https://hfday.ru/img/20241016/0fbcf072cc98c553.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x0fbcf072cc98c553:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x0fbcf072cc98c553 { background: url("https://hfday.ru/img/20241016/0fbcf072cc98c553.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x0fbcf072cc98c553:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x0d2e73e11d2b2b3f { background: url("https://hfday.ru/img/20241021/0d2e73e11d2b2b3f.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x0d2e73e11d2b2b3f:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x0d2e73e11d2b2b3f { background: url("https://hfday.ru/img/20241021/0d2e73e11d2b2b3f.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x0d2e73e11d2b2b3f:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.xb6b22326cba00346 { background: url("https://hfday.ru/img/20241021/b6b22326cba00346.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.xb6b22326cba00346:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.xb6b22326cba00346 { background: url("https://hfday.ru/img/20241021/b6b22326cba00346.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.xb6b22326cba00346:hover { background-color: rgba(60,60,60,0.92) !important;}

        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["–º–∏–Ω—É—Ç—É", "–º–∏–Ω—É—Ç—ã", "–º–∏–Ω—É—Ç"],
                hour: ["—á–∞—Å", "—á–∞—Å–∞", "—á–∞—Å–æ–≤"],
                day: ["–¥–µ–Ω—å", "–¥–Ω—è", "–¥–Ω–µ–π"],
                justNow: "—Ç–æ–ª—å–∫–æ —á—Ç–æ",
                ago: "–Ω–∞–∑–∞–¥"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["ÂàÜÈíü", "ÂàÜÈíü", "ÂàÜÈíü"],
                hour: ["Â∞èÊó∂", "Â∞èÊó∂", "Â∞èÊó∂"],
                day: ["Â§©", "Â§©", "Â§©"],
                justNow: "ÂàöÂàö",
                ago: "Ââç"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "—Å—Ç–∞—Ç–µ–π";
            } else if (lastDigit === 1) {
                word = "—Å—Ç–∞—Ç—å—è";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "—Å—Ç–∞—Ç—å–∏";
            } else {
                word = "—Å—Ç–∞—Ç–µ–π";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ÁØáËÆ∫Êñá"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">üî∫</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">22 –æ–∫—Ç—è–±—Ä—è</span> | <span id="title-articles-count">23 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item" id="nav-prev"><a href="/d/2024-10-21.html">‚¨ÖÔ∏è <span id="prev-date">21.10</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-10-23.html">‚û°Ô∏è <span id="next-date">23.10</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2024-10.html">üìà <span id='top-month-label'>–¢–æ–ø –∑–∞ –º–µ—Å—è—Ü</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">üîÄ <span id="sort-label-text">–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">—Ä–µ–π—Ç–∏–Ω–≥—É</option>
                    <option value="pub_date">–¥–∞—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏</option>
                    <option value="issue_id">–¥–æ–±–∞–≤–ª–µ–Ω–∏—é –Ω–∞ HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">üè∑Ô∏è –§–∏–ª—å—Ç—Ä</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A‚à™B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A‚à©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">üßπ</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ‚úñÔ∏è <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '22 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 22', 'zh': '10Êúà22Êó•'};
        let feedDateNext = {'ru': '23.10', 'en': '10/23', 'zh': '10Êúà23Êó•'};
        let feedDatePrev = {'ru': '21.10', 'en': '10/21', 'zh': '10Êúà21Êó•'};
        let filterLabel = {'ru': '–§–∏–ª—å—Ç—Ä', 'en': 'Topics', 'zh': '‰∏ªÈ¢òÁ≠õÈÄâ'}
        let publishedLabel = {'ru': '–°—Ç–∞—Ç—å—è –æ—Ç ', 'en': 'Published on ', 'zh': 'ÂèëË°®‰∫é'}
        let sortLabel = {'ru': '–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ', 'en': 'Sort by', 'zh': 'ÊéíÂ∫èÊñπÂºè'}
        let paperLabel = {'ru': '–°—Ç–∞—Ç—å—è', 'en': 'Paper', 'zh': 'ËÆ∫Êñá'}
        let topMonthLabel = {'ru': '–¢–æ–ø –∑–∞ –º–µ—Å—è—Ü', 'en': 'Top by Month', 'zh': 'ÊúàÂ∫¶ÁÉ≠Èó®ËÆ∫Êñá'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2410.16271', 'title': 'FrugalNeRF: Fast Convergence for Few-shot Novel View Synthesis without Learned Priors', 'url': 'https://huggingface.co/papers/2410.16271', 'abstract': 'Neural Radiance Fields (NeRF) face significant challenges in few-shot scenarios, primarily due to overfitting and long training times for high-fidelity rendering. Existing methods, such as FreeNeRF and SparseNeRF, use frequency regularization or pre-trained priors but struggle with complex scheduling and bias. We introduce FrugalNeRF, a novel few-shot NeRF framework that leverages weight-sharing voxels across multiple scales to efficiently represent scene details. Our key contribution is a cross-scale geometric adaptation scheme that selects pseudo ground truth depth based on reprojection errors across scales. This guides training without relying on externally learned priors, enabling full utilization of the training data. It can also integrate pre-trained priors, enhancing quality without slowing convergence. Experiments on LLFF, DTU, and RealEstate-10K show that FrugalNeRF outperforms other few-shot NeRF methods while significantly reducing training time, making it a practical solution for efficient and accurate 3D scene reconstruction.', 'score': 80, 'issue_id': 208, 'pub_date': '2024-10-21', 'pub_date_ru': '21 –æ–∫—Ç—è–±—Ä—è', 'hash': 'b3e061a400165087', 'data': {'categories': ['#3d', '#cv'], 'emoji': 'üîç', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ 3D-–º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –º–∏–Ω–∏–º—É–º–æ–º –¥–∞–Ω–Ω—ã—Ö', 'desc': 'FrugalNeRF - —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö –ø–æ–ª–µ–π –∏–∑–ª—É—á–µ–Ω–∏—è (NeRF) –Ω–∞ –º–∞–ª–æ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –¥–∞–Ω–Ω—ã—Ö. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –≤–æ–∫—Å–µ–ª—å–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –º–∞—Å—à—Ç–∞–±–∞—Ö –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –¥–µ—Ç–∞–ª–µ–π —Å—Ü–µ–Ω—ã. –ö–ª—é—á–µ–≤–æ–π –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—å—é —è–≤–ª—è–µ—Ç—Å—è —Å—Ö–µ–º–∞ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –º–µ–∂–¥—É –º–∞—Å—à—Ç–∞–±–∞–º–∏, –≤—ã–±–∏—Ä–∞—é—â–∞—è –ø—Å–µ–≤–¥–æ-–∏—Å—Ç–∏–Ω–Ω—É—é –≥–ª—É–±–∏–Ω—É –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ—à–∏–±–æ–∫ —Ä–µ–ø—Ä–æ–µ–∫—Ü–∏–∏. FrugalNeRF –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –¥—Ä—É–≥–∏–µ few-shot NeRF –º–µ—Ç–æ–¥—ã, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∫—Ä–∞—â–∞—è –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è.'}, 'en': {'title': 'FrugalNeRF: Efficient 3D Scene Reconstruction with Cross-Scale Adaptation', 'desc': 'The paper introduces FrugalNeRF, a new approach to improve Neural Radiance Fields (NeRF) in few-shot scenarios by using weight-sharing voxels across multiple scales. This method addresses the challenges of overfitting and long training times by employing a cross-scale geometric adaptation scheme that selects pseudo ground truth depth based on reprojection errors. Unlike previous methods, FrugalNeRF does not rely on externally learned priors, allowing for full utilization of the training data while still being able to integrate pre-trained priors for enhanced quality. Experiments demonstrate that FrugalNeRF outperforms existing few-shot NeRF methods in terms of efficiency and accuracy, making it a practical solution for 3D scene reconstruction.'}, 'zh': {'title': 'FrugalNeRFÔºöÈ´òÊïàÂ∞ëÊ†∑Êú¨3DÈáçÂª∫Êñ∞ÊñπÊ°à', 'desc': 'NeRFÂú®Â∞ëÊ†∑Êú¨Âú∫ÊôØ‰∏≠Èù¢‰∏¥ËøáÊãüÂêàÂíåËÆ≠ÁªÉÊó∂Èó¥ÈïøÁöÑÈóÆÈ¢ò„ÄÇFrugalNeRFÈÄöËøáË∑®Â∞∫Â∫¶Âá†‰ΩïÈÄÇÂ∫îÊñπÊ°àÔºåÂà©Áî®ÈáçÊäïÂΩ±ËØØÂ∑ÆÈÄâÊã©‰º™ÁúüÂÆûÊ∑±Â∫¶ÔºåÈÅøÂÖç‰æùËµñÂ§ñÈÉ®Â≠¶‰π†ÂÖàÈ™å„ÄÇËØ•ÊñπÊ≥ïÊúâÊïàÂà©Áî®ËÆ≠ÁªÉÊï∞ÊçÆÔºåÂπ∂ÂèØÁªìÂêàÈ¢ÑËÆ≠ÁªÉÂÖàÈ™åÔºåÊèêÈ´òË¥®Èáè‰∏î‰∏çÂΩ±ÂìçÊî∂ÊïõÈÄüÂ∫¶„ÄÇÂÆûÈ™åË°®ÊòéÔºåFrugalNeRFÂú®ÂáèÂ∞ëËÆ≠ÁªÉÊó∂Èó¥ÁöÑÂêåÊó∂ÔºåÊèêÂçá‰∫Ü3DÂú∫ÊôØÈáçÂª∫ÁöÑÊïàÁéáÂíåÂáÜÁ°ÆÊÄß„ÄÇ'}}, 'pub_date_card': {'ru': '21 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 21', 'zh': '10Êúà21Êó•'}}, {'id': 'https://huggingface.co/papers/2410.16268', 'title': 'SAM2Long: Enhancing SAM 2 for Long Video Segmentation with a Training-Free Memory Tree', 'url': 'https://huggingface.co/papers/2410.16268', 'abstract': 'The Segment Anything Model 2 (SAM 2) has emerged as a powerful foundation model for object segmentation in both images and videos, paving the way for various downstream video applications. The crucial design of SAM 2 for video segmentation is its memory module, which prompts object-aware memories from previous frames for current frame prediction. However, its greedy-selection memory design suffers from the "error accumulation" problem, where an errored or missed mask will cascade and influence the segmentation of the subsequent frames, which limits the performance of SAM 2 toward complex long-term videos. To this end, we introduce SAM2Long, an improved training-free video object segmentation strategy, which considers the segmentation uncertainty within each frame and chooses the video-level optimal results from multiple segmentation pathways in a constrained tree search manner. In practice, we maintain a fixed number of segmentation pathways throughout the video. For each frame, multiple masks are proposed based on the existing pathways, creating various candidate branches. We then select the same fixed number of branches with higher cumulative scores as the new pathways for the next frame. After processing the final frame, the pathway with the highest cumulative score is chosen as the final segmentation result. Benefiting from its heuristic search design, SAM2Long is robust toward occlusions and object reappearances, and can effectively segment and track objects for complex long-term videos. Notably, SAM2Long achieves an average improvement of 3.0 points across all 24 head-to-head comparisons, with gains of up to 5.3 points in J&F on long-term video object segmentation benchmarks such as SA-V and LVOS. The code is released at https://github.com/Mark12Ding/SAM2Long.', 'score': 65, 'issue_id': 209, 'pub_date': '2024-10-21', 'pub_date_ru': '21 –æ–∫—Ç—è–±—Ä—è', 'hash': '4daeb1e76417273d', 'data': {'categories': ['#benchmark', '#cv', '#video'], 'emoji': 'üé¨', 'ru': {'title': 'SAM2Long: –ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º –≤ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –≤–∏–¥–µ–æ', 'desc': 'SAM2Long - —ç—Ç–æ —É–ª—É—á—à–µ–Ω–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –≤–∏–¥–µ–æ–æ–±—ä–µ–∫—Ç–æ–≤, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ –º–æ–¥–µ–ª–∏ SAM 2. –û–Ω–∞ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –æ—à–∏–±–æ–∫ –ø—Ä–∏ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –ø—É—Ç–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø—É—Ç–µ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏ –≤—ã–±–æ—Ä–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞. SAM2Long –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—É—Ç–µ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –Ω–∞ –ø—Ä–æ—Ç—è–∂–µ–Ω–∏–∏ –≤—Å–µ–≥–æ –≤–∏–¥–µ–æ, –≤—ã–±–∏—Ä–∞—è –≤–µ—Ç–≤–∏ —Å –Ω–∞–∏–≤—ã—Å—à–∏–º–∏ –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–º–∏ –æ—Ü–µ–Ω–∫–∞–º–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–∞–¥—Ä–∞. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Å–µ–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –∏ –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å –æ–±—ä–µ–∫—Ç—ã –≤ —Å–ª–æ–∂–Ω—ã—Ö –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã—Ö –≤–∏–¥–µ–æ, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è —É–ª—É—á—à–µ–Ω–∏–µ –¥–æ 5.3 –ø—É–Ω–∫—Ç–æ–≤ –ø–æ –º–µ—Ç—Ä–∏–∫–µ J&F –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –≤–∏–¥–µ–æ–æ–±—ä–µ–∫—Ç–æ–≤.'}, 'en': {'title': '"SAM2Long: Navigating Complexity in Video Segmentation"', 'desc': 'The Segment Anything Model 2 (SAM 2) is a foundation model designed for object segmentation in images and videos, but it struggles with error accumulation in long-term video segmentation. SAM2Long is introduced as an improved strategy that uses a constrained tree search to select optimal segmentation pathways, addressing the error accumulation issue. By maintaining multiple segmentation pathways and selecting the best candidates based on cumulative scores, SAM2Long effectively handles occlusions and object reappearances. This approach results in significant performance improvements, achieving up to 5.3 points gain in long-term video segmentation benchmarks.'}, 'zh': {'title': 'SAM2LongÔºöÊèêÂçáÈïøËßÜÈ¢ëÂØπË±°ÂàÜÂâ≤ÁöÑÊñ∞Á≠ñÁï•', 'desc': 'Segment Anything Model 2ÔºàSAM 2ÔºâÊòØ‰∏Ä‰∏™Âº∫Â§ßÁöÑÂü∫Á°ÄÊ®°ÂûãÔºåÁî®‰∫éÂõæÂÉèÂíåËßÜÈ¢ë‰∏≠ÁöÑÂØπË±°ÂàÜÂâ≤„ÄÇÂÆÉÁöÑÂÖ≥ÈîÆËÆæËÆ°ÊòØËÆ∞ÂøÜÊ®°ÂùóÔºåÂèØ‰ª•‰ªéÂâç‰∏ÄÂ∏ß‰∏≠ÊèêÂèñÂØπË±°Áõ∏ÂÖ≥ÁöÑËÆ∞ÂøÜÊù•È¢ÑÊµãÂΩìÂâçÂ∏ß„ÄÇÁÑ∂ËÄåÔºåSAM 2 ÁöÑË¥™Â©™ÈÄâÊã©ËÆ∞ÂøÜËÆæËÆ°Â≠òÂú®‚ÄúÈîôËØØÁßØÁ¥Ø‚ÄùÈóÆÈ¢òÔºåÂΩ±ÂìçÂ§çÊùÇÈïøËßÜÈ¢ëÁöÑÂàÜÂâ≤ÊÄßËÉΩ„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü SAM2LongÔºåÈÄöËøáÂ§öË∑ØÂæÑÊêúÁ¥¢Á≠ñÁï•Êù•ÊèêÈ´òËßÜÈ¢ëÂØπË±°ÂàÜÂâ≤ÁöÑÂáÜÁ°ÆÊÄß„ÄÇ'}}, 'pub_date_card': {'ru': '21 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 21', 'zh': '10Êúà21Êó•'}}, {'id': 'https://huggingface.co/papers/2410.16256', 'title': 'CompassJudger-1: All-in-one Judge Model Helps Model Evaluation and Evolution', 'url': 'https://huggingface.co/papers/2410.16256', 'abstract': 'Efficient and accurate evaluation is crucial for the continuous improvement of large language models (LLMs). Among various assessment methods, subjective evaluation has garnered significant attention due to its superior alignment with real-world usage scenarios and human preferences. However, human-based evaluations are costly and lack reproducibility, making precise automated evaluators (judgers) vital in this process. In this report, we introduce CompassJudger-1, the first open-source all-in-one judge LLM. CompassJudger-1 is a general-purpose LLM that demonstrates remarkable versatility. It is capable of: 1. Performing unitary scoring and two-model comparisons as a reward model; 2. Conducting evaluations according to specified formats; 3. Generating critiques; 4. Executing diverse tasks like a general LLM. To assess the evaluation capabilities of different judge models under a unified setting, we have also established JudgerBench, a new benchmark that encompasses various subjective evaluation tasks and covers a wide range of topics. CompassJudger-1 offers a comprehensive solution for various evaluation tasks while maintaining the flexibility to adapt to diverse requirements. Both CompassJudger and JudgerBench are released and available to the research community athttps://github.com/open-compass/CompassJudger. We believe that by open-sourcing these tools, we can foster collaboration and accelerate progress in LLM evaluation methodologies.', 'score': 58, 'issue_id': 208, 'pub_date': '2024-10-21', 'pub_date_ru': '21 –æ–∫—Ç—è–±—Ä—è', 'hash': '3b5265f629378d5d', 'data': {'categories': ['#alignment', '#benchmark'], 'emoji': '‚öñÔ∏è', 'ru': {'title': 'CompassJudger-1: –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Å—É–¥—å—è –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω CompassJudger-1 - –ø–µ—Ä–≤–∞—è –æ—Ç–∫—Ä—ã—Ç–∞—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å-–æ—Ü–µ–Ω—â–∏–∫ –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –≠—Ç–∞ –º–æ–¥–µ–ª—å —Å–ø–æ—Å–æ–±–Ω–∞ –≤—ã–ø–æ–ª–Ω—è—Ç—å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∑–∞–¥–∞—á–∏ –æ—Ü–µ–Ω–∫–∏, –≤–∫–ª—é—á–∞—è —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π, –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∫—Ä–∏—Ç–∏–∫–∏ –∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –æ–±—â–∏—Ö –∑–∞–¥–∞—á –∫–∞–∫ –æ–±—ã—á–Ω–∞—è LLM. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ —Å–æ–∑–¥–∞–ª–∏ –±–µ–Ω—á–º–∞—Ä–∫ JudgerBench –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π-–æ—Ü–µ–Ω—â–∏–∫–æ–≤. CompassJudger-1 –∏ JudgerBench –¥–æ—Å—Ç—É–ø–Ω—ã —Å–æ–æ–±—â–µ—Å—Ç–≤—É –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–µ–π –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è –º–µ—Ç–æ–¥–æ–≤ –æ—Ü–µ–Ω–∫–∏ LLM.'}, 'en': {'title': '"CompassJudger-1: Revolutionizing LLM Evaluation with Open-Source Precision"', 'desc': 'The paper introduces CompassJudger-1, an open-source large language model designed to automate the evaluation of other language models, reducing the need for costly human-based assessments. CompassJudger-1 can perform tasks such as scoring, model comparison, and critique generation, making it a versatile tool for various evaluation scenarios. To test the effectiveness of different judge models, the authors have also developed JudgerBench, a benchmark that includes a wide range of subjective evaluation tasks. By releasing these tools to the public, the authors aim to encourage collaboration and speed up advancements in evaluating language models.'}, 'zh': {'title': 'CompassJudger-1ÔºöÂ§ßËØ≠Ë®ÄÊ®°ÂûãËØÑ‰º∞ÁöÑÂÖ®ËÉΩÂ∑•ÂÖ∑', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫CompassJudger-1ÁöÑÂºÄÊ∫êËØÑ‰º∞Â∑•ÂÖ∑ÔºåÂÆÉÊòØ‰∏Ä‰∏™ÈÄöÁî®ÁöÑÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºåËÉΩÂ§üËøõË°åÂçï‰∏ÄËØÑÂàÜ„ÄÅÂèåÊ®°ÂûãÊØîËæÉ„ÄÅÁîüÊàêËØÑËÆ∫Á≠âÂ§öÁßç‰ªªÂä°„ÄÇCompassJudger-1ÁöÑËÆæËÆ°Êó®Âú®Ëß£ÂÜ≥‰∫∫Â∑•ËØÑ‰º∞ÊàêÊú¨È´ò‰∏îÈöæ‰ª•ÈáçÂ§çÁöÑÈóÆÈ¢òÔºåÈÄöËøáËá™Âä®ÂåñÁöÑÊñπÂºèÊèêÈ´òËØÑ‰º∞ÁöÑÊïàÁéáÂíåÂáÜÁ°ÆÊÄß„ÄÇ‰∏∫‰∫ÜËØÑ‰º∞‰∏çÂêåËØÑ‰º∞Ê®°ÂûãÁöÑËÉΩÂäõÔºåÁ†îÁ©∂ËÄÖËøòÂª∫Á´ã‰∫Ü‰∏Ä‰∏™Âêç‰∏∫JudgerBenchÁöÑÊñ∞Âü∫ÂáÜÔºåÊ∂µÁõñ‰∫ÜÂ§öÁßç‰∏ªËßÇËØÑ‰º∞‰ªªÂä°„ÄÇÈÄöËøáÂºÄÊ∫êËøô‰∫õÂ∑•ÂÖ∑ÔºåÁ†îÁ©∂ËÄÖÂ∏åÊúõ‰øÉËøõÂêà‰ΩúÔºåÂä†ÈÄüÂ§ßËØ≠Ë®ÄÊ®°ÂûãËØÑ‰º∞ÊñπÊ≥ïÁöÑÂèëÂ±ï„ÄÇ'}}, 'pub_date_card': {'ru': '21 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 21', 'zh': '10Êúà21Êó•'}}, {'id': 'https://huggingface.co/papers/2410.13861', 'title': 'PUMA: Empowering Unified MLLM with Multi-granular Visual Generation', 'url': 'https://huggingface.co/papers/2410.13861', 'abstract': 'Recent advancements in multimodal foundation models have yielded significant progress in vision-language understanding. Initial attempts have also explored the potential of multimodal large language models (MLLMs) for visual content generation. However, existing works have insufficiently addressed the varying granularity demands of different image generation tasks within a unified MLLM paradigm - from the diversity required in text-to-image generation to the precise controllability needed in image manipulation. In this work, we propose PUMA, emPowering Unified MLLM with Multi-grAnular visual generation. PUMA unifies multi-granular visual features as both inputs and outputs of MLLMs, elegantly addressing the different granularity requirements of various image generation tasks within a unified MLLM framework. Following multimodal pretraining and task-specific instruction tuning, PUMA demonstrates proficiency in a wide range of multimodal tasks. This work represents a significant step towards a truly unified MLLM capable of adapting to the granularity demands of various visual tasks. The code and model will be released in https://github.com/rongyaofang/PUMA.', 'score': 53, 'issue_id': 208, 'pub_date': '2024-10-17', 'pub_date_ru': '17 –æ–∫—Ç—è–±—Ä—è', 'hash': '31995f0502d70f2f', 'data': {'categories': ['#cv', '#multimodal'], 'emoji': 'üêÜ', 'ru': {'title': 'PUMA: –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è MLLM –¥–ª—è –º—É–ª—å—Ç–∏–≥—Ä–∞–Ω—É–ª—è—Ä–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç PUMA - –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –±–æ–ª—å—à–æ–≥–æ —è–∑—ã–∫–æ–≤–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è (MLLM) –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞. PUMA –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Ä–∞–∑–Ω–æ–π –≥—Ä–∞–Ω—É–ª—è—Ä–Ω–æ—Å—Ç–∏ –∫–∞–∫ –¥–ª—è –≤—Ö–æ–¥–Ω—ã—Ö, —Ç–∞–∫ –∏ –¥–ª—è –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö MLLM, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Ä–µ—à–∞—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∑–∞–¥–∞—á–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ —Ä–∞–º–∫–∞—Ö –µ–¥–∏–Ω–æ–π –ø–∞—Ä–∞–¥–∏–≥–º—ã. –ú–æ–¥–µ–ª—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤ —à–∏—Ä–æ–∫–æ–º —Å–ø–µ–∫—Ç—Ä–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á –ø–æ—Å–ª–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏. –≠—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π —à–∞–≥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–π MLLM, —Å–ø–æ—Å–æ–±–Ω–æ–π –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –∫ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º –≥—Ä–∞–Ω—É–ª—è—Ä–Ω–æ—Å—Ç–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á.'}, 'en': {'title': 'PUMA: Unifying Visual Generation with Multi-Granular Precision', 'desc': 'The paper introduces PUMA, a new approach to improve multimodal large language models (MLLMs) for visual content generation. PUMA addresses the challenge of varying granularity in image generation tasks, such as the need for diversity in text-to-image generation and precision in image manipulation. By integrating multi-granular visual features, PUMA enhances the ability of MLLMs to handle different visual tasks within a single framework. This advancement marks a significant step towards creating a unified model that can adapt to diverse visual generation needs.'}, 'zh': {'title': 'PUMAÔºöÁªü‰∏ÄÂ§öÊ®°ÊÄÅËßÜËßâÁîüÊàêÁöÑÊñ∞Á™ÅÁ†¥', 'desc': 'ËøôÁØáËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫PUMAÁöÑÊñ∞ÊñπÊ≥ïÔºåÊó®Âú®ÊèêÂçáÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÁöÑËßÜËßâÁîüÊàêËÉΩÂäõ„ÄÇPUMAÈÄöËøáÁªü‰∏ÄÂ§öÂ±ÇÊ¨°ÁöÑËßÜËßâÁâπÂæÅÔºåËß£ÂÜ≥‰∫Ü‰∏çÂêåÂõæÂÉèÁîüÊàê‰ªªÂä°ÂØπÁªÜËäÇÂíåÂ§öÊ†∑ÊÄßÁöÑ‰∏çÂêåÈúÄÊ±Ç„ÄÇÈÄöËøáÂ§öÊ®°ÊÄÅÈ¢ÑËÆ≠ÁªÉÂíå‰ªªÂä°ÁâπÂÆöÁöÑÊåá‰ª§Ë∞É‰ºòÔºåPUMAÂú®Â§öÁßçÂ§öÊ®°ÊÄÅ‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇËØ•Á†îÁ©∂‰∏∫ÂÆûÁé∞ËÉΩÂ§üÈÄÇÂ∫îÂêÑÁßçËßÜËßâ‰ªªÂä°ÁªÜËäÇÈúÄÊ±ÇÁöÑÁªü‰∏ÄMLLMËøàÂá∫‰∫ÜÈáçË¶Å‰∏ÄÊ≠•„ÄÇ'}}, 'pub_date_card': {'ru': '17 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 17', 'zh': '10Êúà17Êó•'}}, {'id': 'https://huggingface.co/papers/2410.15735', 'title': 'AutoTrain: No-code training for state-of-the-art models', 'url': 'https://huggingface.co/papers/2410.15735', 'abstract': 'With the advancements in open-source models, training (or finetuning) models on custom datasets has become a crucial part of developing solutions which are tailored to specific industrial or open-source applications. Yet, there is no single tool which simplifies the process of training across different types of modalities or tasks. We introduce AutoTrain (aka AutoTrain Advanced) -- an open-source, no code tool/library which can be used to train (or finetune) models for different kinds of tasks such as: large language model (LLM) finetuning, text classification/regression, token classification, sequence-to-sequence task, finetuning of sentence transformers, visual language model (VLM) finetuning, image classification/regression and even classification and regression tasks on tabular data. AutoTrain Advanced is an open-source library providing best practices for training models on custom datasets. The library is available at https://github.com/huggingface/autotrain-advanced. AutoTrain can be used in fully local mode or on cloud machines and works with tens of thousands of models shared on Hugging Face Hub and their variations.', 'score': 52, 'issue_id': 209, 'pub_date': '2024-10-21', 'pub_date_ru': '21 –æ–∫—Ç—è–±—Ä—è', 'hash': '62b6ded4f9dc1d27', 'data': {'categories': ['#multimodal', '#training'], 'emoji': 'ü§ñ', 'ru': {'title': 'AutoTrain: –£–ø—Ä–æ—â–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è ML-–º–æ–¥–µ–ª–µ–π –¥–ª—è –≤—Å–µ—Ö', 'desc': 'AutoTrain Advanced - —ç—Ç–æ –æ—Ç–∫—Ä—ã—Ç—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –±–µ–∑ –Ω–∞–ø–∏—Å–∞–Ω–∏—è –∫–æ–¥–∞. –û–Ω –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∑–∞–¥–∞—á–∏, –≤–∫–ª—é—á–∞—è –¥–æ–æ–±—É—á–µ–Ω–∏–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é —Ç–µ–∫—Å—Ç–∞ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∏ —Ä–∞–±–æ—Ç—É —Å —Ç–∞–±–ª–∏—á–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏. AutoTrain –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ª—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö –∏ –º–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –∫–∞–∫ –ª–æ–∫–∞–ª—å–Ω–æ, —Ç–∞–∫ –∏ –≤ –æ–±–ª–∞–∫–µ. –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç —Å–æ–≤–º–µ—Å—Ç–∏–º —Å —Ç—ã—Å—è—á–∞–º–∏ –º–æ–¥–µ–ª–µ–π –∏–∑ Hugging Face Hub.'}, 'en': {'title': 'AutoTrain: Simplifying Model Training Across Modalities', 'desc': 'AutoTrain Advanced is a no-code, open-source tool designed to simplify the process of training and finetuning machine learning models across various tasks and modalities. It supports a wide range of applications, including large language models, text and image classification, and even tabular data tasks. The tool integrates with the Hugging Face Hub, allowing users to leverage a vast collection of pre-trained models. AutoTrain can be used both locally and on cloud platforms, making it versatile for different user needs.'}, 'zh': {'title': 'AutoTrainÔºöÊó†‰ª£Á†ÅÊ®°ÂûãËÆ≠ÁªÉÁöÑÈù©ÂëΩ', 'desc': 'AutoTrainÊòØ‰∏Ä‰∏™ÂºÄÊ∫êÁöÑÊó†‰ª£Á†ÅÂ∑•ÂÖ∑Â∫ìÔºåÊó®Âú®ÁÆÄÂåñ‰∏çÂêå‰ªªÂä°ÂíåÊ®°ÊÄÅ‰∏ãÁöÑÊ®°ÂûãËÆ≠ÁªÉËøáÁ®ã„ÄÇÂÆÉÊîØÊåÅÂ§öÁßç‰ªªÂä°ÔºåÂåÖÊã¨Â§ßËØ≠Ë®ÄÊ®°ÂûãÂæÆË∞É„ÄÅÊñáÊú¨ÂàÜÁ±ª„ÄÅÂ∫èÂàóÂà∞Â∫èÂàó‰ªªÂä°„ÄÅËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÂæÆË∞ÉÁ≠â„ÄÇÁî®Êà∑ÂèØ‰ª•Âú®Êú¨Âú∞Êàñ‰∫ëÁ´Ø‰ΩøÁî®AutoTrainÔºåÂπ∂‰∏éHugging Face Hub‰∏äÁöÑÊï∞‰∏á‰∏™Ê®°ÂûãÂÖºÂÆπ„ÄÇËøô‰∏™Â∑•ÂÖ∑‰∏∫Âú®Ëá™ÂÆö‰πâÊï∞ÊçÆÈõÜ‰∏äËÆ≠ÁªÉÊ®°ÂûãÊèê‰æõ‰∫ÜÊúÄ‰Ω≥ÂÆûË∑µ„ÄÇ'}}, 'pub_date_card': {'ru': '21 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 21', 'zh': '10Êúà21Êó•'}}, {'id': 'https://huggingface.co/papers/2410.14940', 'title': 'Baichuan Alignment Technical Report', 'url': 'https://huggingface.co/papers/2410.14940', 'abstract': "We introduce Baichuan Alignment, a detailed analysis of the alignment techniques employed in the Baichuan series of models. This represents the industry's first comprehensive account of alignment methodologies, offering valuable insights for advancing AI research. We investigate the critical components that enhance model performance during the alignment process, including optimization methods, data strategies, capability enhancements, and evaluation processes. The process spans three key stages: Prompt Augmentation System (PAS), Supervised Fine-Tuning (SFT), and Preference Alignment. The problems encountered, the solutions applied, and the improvements made are thoroughly recorded.   Through comparisons across well-established benchmarks, we highlight the technological advancements enabled by Baichuan Alignment. Baichuan-Instruct is an internal model, while Qwen2-Nova-72B and Llama3-PBM-Nova-70B are instruct versions of the Qwen2-72B and Llama-3-70B base models, optimized through Baichuan Alignment. Baichuan-Instruct demonstrates significant improvements in core capabilities, with user experience gains ranging from 17% to 28%, and performs exceptionally well on specialized benchmarks. In open-source benchmark evaluations, both Qwen2-Nova-72B and Llama3-PBM-Nova-70B consistently outperform their respective official instruct versions across nearly all datasets. This report aims to clarify the key technologies behind the alignment process, fostering a deeper understanding within the community. Llama3-PBM-Nova-70B model is available at https://huggingface.co/PKU-Baichuan-MLSystemLab/Llama3-PBM-Nova-70B.", 'score': 46, 'issue_id': 208, 'pub_date': '2024-10-19', 'pub_date_ru': '19 –æ–∫—Ç—è–±—Ä—è', 'hash': 'e1222b08a95a2911', 'data': {'categories': ['#alignment', '#benchmark', '#optimization'], 'emoji': 'üß†', 'ru': {'title': '–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π: –º–µ—Ç–æ–¥ Baichuan Alignment', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –º–µ—Ç–æ–¥–æ–≤ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è (alignment), –ø—Ä–∏–º–µ–Ω—è–µ–º—ã—Ö –≤ —Å–µ—Ä–∏–∏ –º–æ–¥–µ–ª–µ–π Baichuan. –û–ø–∏—Å—ã–≤–∞—é—Ç—Å—è —Ç—Ä–∏ –∫–ª—é—á–µ–≤—ã—Ö —ç—Ç–∞–ø–∞: —Å–∏—Å—Ç–µ–º–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤ (PAS), –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–∞—è —Ç–æ–Ω–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ (SFT) –∏ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π. –ò—Å—Å–ª–µ–¥—É—é—Ç—Å—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã, —É–ª—É—á—à–∞—é—â–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è, –≤–∫–ª—é—á–∞—è –º–µ—Ç–æ–¥—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏, —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Ä–∞–±–æ—Ç—ã —Å –¥–∞–Ω–Ω—ã–º–∏ –∏ –ø—Ä–æ—Ü–µ—Å—Å—ã –æ—Ü–µ–Ω–∫–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –≤ –æ—Å–Ω–æ–≤–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è—Ö –º–æ–¥–µ–ª–µ–π –∏ –∏—Ö –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ –Ω–∞–¥ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–º–∏ –≤–µ—Ä—Å–∏—è–º–∏ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö.'}, 'en': {'title': "Aligning for Excellence: Baichuan's Path to Superior AI Models", 'desc': 'Baichuan Alignment is a comprehensive study of alignment techniques used in the Baichuan model series, providing insights to advance AI research. The paper explores key components that improve model performance during alignment, such as optimization methods, data strategies, and evaluation processes. The alignment process is divided into three stages: Prompt Augmentation System, Supervised Fine-Tuning, and Preference Alignment. The study shows that models optimized with Baichuan Alignment, like Baichuan-Instruct, achieve significant performance improvements and outperform other models on various benchmarks.'}, 'zh': {'title': 'Baichuan AlignmentÔºöAI ÂØπÈΩêÊäÄÊúØÁöÑÂÖ®Èù¢Á™ÅÁ†¥', 'desc': 'Baichuan Alignment ÊòØÂØπ Baichuan Á≥ªÂàóÊ®°Âûã‰∏≠ÂØπÈΩêÊäÄÊúØÁöÑËØ¶ÁªÜÂàÜÊûêÔºåÈ¶ñÊ¨°ÂÖ®Èù¢ËÆ∞ÂΩï‰∫ÜÂØπÈΩêÊñπÊ≥ï„ÄÇÁ†îÁ©∂‰∏≠Êé¢ËÆ®‰∫Ü‰ºòÂåñÊñπÊ≥ï„ÄÅÊï∞ÊçÆÁ≠ñÁï•„ÄÅËÉΩÂäõÂ¢ûÂº∫ÂíåËØÑ‰º∞ËøáÁ®ãÁ≠âÂÖ≥ÈîÆÁªÑ‰ª∂„ÄÇÂØπÈΩêËøáÁ®ãÂàÜ‰∏∫‰∏â‰∏™Èò∂ÊÆµÔºöÊèêÁ§∫Â¢ûÂº∫Á≥ªÁªü„ÄÅÁõëÁù£ÂæÆË∞ÉÂíåÂÅèÂ•ΩÂØπÈΩê„ÄÇÈÄöËøáÂØπÊØîÂü∫ÂáÜÊµãËØïÔºåÂ±ïÁ§∫‰∫Ü Baichuan Alignment Â∏¶Êù•ÁöÑÊäÄÊúØËøõÊ≠•„ÄÇ'}}, 'pub_date_card': {'ru': '19 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 19', 'zh': '10Êúà19Êó•'}}, {'id': 'https://huggingface.co/papers/2410.14745', 'title': 'SemiEvol: Semi-supervised Fine-tuning for LLM Adaptation', 'url': 'https://huggingface.co/papers/2410.14745', 'abstract': 'Supervised fine-tuning (SFT) is crucial in adapting large language models (LLMs) to a specific domain or task. However, only a limited amount of labeled data is available in practical applications, which poses a severe challenge for SFT in yielding satisfactory results. Therefore, a data-efficient framework that can fully exploit labeled and unlabeled data for LLM fine-tuning is highly anticipated. Towards this end, we introduce a semi-supervised fine-tuning framework named SemiEvol for LLM adaptation from a propagate-and-select manner. For knowledge propagation, SemiEvol adopts a bi-level approach, propagating knowledge from labeled data to unlabeled data through both in-weight and in-context methods. For knowledge selection, SemiEvol incorporates a collaborative learning mechanism, selecting higher-quality pseudo-response samples. We conducted experiments using GPT-4o-mini and Llama-3.1 on seven general or domain-specific datasets, demonstrating significant improvements in model performance on target data. Furthermore, we compared SemiEvol with SFT and self-evolution methods, highlighting its practicality in hybrid data scenarios.', 'score': 45, 'issue_id': 207, 'pub_date': '2024-10-17', 'pub_date_ru': '17 –æ–∫—Ç—è–±—Ä—è', 'hash': '863b95d8e45c3aa2', 'data': {'categories': ['#dataset'], 'emoji': 'üß†', 'ru': {'title': 'SemiEvol: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –ø–æ–ª—É-–∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–ª—É-–∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—ã–π –º–µ—Ç–æ–¥ —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º SemiEvol. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–∞–∫ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–µ, —Ç–∞–∫ –∏ –Ω–µ—Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏ –∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∑–∞–¥–∞—á–µ –∏–ª–∏ –¥–æ–º–µ–Ω—É. SemiEvol –ø—Ä–∏–º–µ–Ω—è–µ—Ç –¥–≤—É—Ö—É—Ä–æ–≤–Ω–µ–≤—ã–π –º–µ—Ç–æ–¥ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –∑–Ω–∞–Ω–∏–π –∏ –º–µ—Ö–∞–Ω–∏–∑–º —Å–æ–≤–º–µ—Å—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –æ—Ç–±–æ—Ä–∞ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø—Å–µ–≤–¥–æ-–æ—Ç–≤–µ—Ç–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ —Å–µ–º–∏ –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –ø–æ–∫–∞–∑–∞–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏.'}, 'en': {'title': 'SemiEvol: Maximizing Model Potential with Minimal Data', 'desc': "The paper introduces SemiEvol, a semi-supervised fine-tuning framework designed to adapt large language models using both labeled and unlabeled data. SemiEvol uses a bi-level approach to propagate knowledge from labeled to unlabeled data, enhancing the model's learning process. It also employs a collaborative learning mechanism to select high-quality pseudo-responses, improving the model's performance. Experiments show that SemiEvol outperforms traditional supervised fine-tuning and self-evolution methods, making it effective for tasks with limited labeled data."}, 'zh': {'title': 'SemiEvolÔºöÈ´òÊïàÂà©Áî®Ê†áÊ≥®‰∏éÊú™Ê†áÊ≥®Êï∞ÊçÆÁöÑÂçäÁõëÁù£ÂæÆË∞ÉÊ°ÜÊû∂', 'desc': 'ËøôÁØáËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫SemiEvolÁöÑÂçäÁõëÁù£ÂæÆË∞ÉÊ°ÜÊû∂ÔºåÁî®‰∫éÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÈÄÇÂ∫î„ÄÇSemiEvolÈÄöËøáÂèåÂ±ÇÊñπÊ≥ïÂ∞ÜÊ†áÊ≥®Êï∞ÊçÆÁöÑÁü•ËØÜ‰º†Êí≠Âà∞Êú™Ê†áÊ≥®Êï∞ÊçÆ‰∏≠ÔºåÂπ∂ÈÄöËøáÂçè‰ΩúÂ≠¶‰π†Êú∫Âà∂ÈÄâÊã©È´òË¥®ÈáèÁöÑ‰º™ÂìçÂ∫îÊ†∑Êú¨„ÄÇÂÆûÈ™åË°®ÊòéÔºåSemiEvolÂú®Â§öÁßçÊï∞ÊçÆÈõÜ‰∏äÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÊÄßËÉΩ„ÄÇ‰∏é‰º†ÁªüÁöÑÁõëÁù£ÂæÆË∞ÉÂíåËá™ÊàëËøõÂåñÊñπÊ≥ïÁõ∏ÊØîÔºåSemiEvolÂú®Ê∑∑ÂêàÊï∞ÊçÆÂú∫ÊôØ‰∏≠Ë°®Áé∞Âá∫Êõ¥È´òÁöÑÂÆûÁî®ÊÄß„ÄÇ'}}, 'pub_date_card': {'ru': '17 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 17', 'zh': '10Êúà17Êó•'}}, {'id': 'https://huggingface.co/papers/2410.16153', 'title': 'Pangea: A Fully Open Multilingual Multimodal LLM for 39 Languages', 'url': 'https://huggingface.co/papers/2410.16153', 'abstract': "Despite recent advances in multimodal large language models (MLLMs), their development has predominantly focused on English- and western-centric datasets and tasks, leaving most of the world's languages and diverse cultural contexts underrepresented. This paper introduces Pangea, a multilingual multimodal LLM trained on PangeaIns, a diverse 6M instruction dataset spanning 39 languages. PangeaIns features: 1) high-quality English instructions, 2) carefully machine-translated instructions, and 3) culturally relevant multimodal tasks to ensure cross-cultural coverage. To rigorously assess models' capabilities, we introduce PangeaBench, a holistic evaluation suite encompassing 14 datasets covering 47 languages. Results show that Pangea significantly outperforms existing open-source models in multilingual settings and diverse cultural contexts. Ablation studies further reveal the importance of English data proportions, language popularity, and the number of multimodal training samples on overall performance. We fully open-source our data, code, and trained checkpoints, to facilitate the development of inclusive and robust multilingual MLLMs, promoting equity and accessibility across a broader linguistic and cultural spectrum.", 'score': 42, 'issue_id': 208, 'pub_date': '2024-10-21', 'pub_date_ru': '21 –æ–∫—Ç—è–±—Ä—è', 'hash': '023f2f5e4c6c7a7c', 'data': {'categories': ['#benchmark', '#data', '#dataset', '#multilingual', '#multimodal'], 'emoji': 'üåç', 'ru': {'title': 'Pangea: –ü—Ä–æ—Ä—ã–≤ –≤ –º—É–ª—å—Ç–∏—è–∑—ã—á–Ω—ã—Ö –∏ –º—É–ª—å—Ç–∏–∫—É–ª—å—Ç—É—Ä–Ω—ã—Ö –ò–ò-–º–æ–¥–µ–ª—è—Ö', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Pangea - –º—É–ª—å—Ç–∏—è–∑—ã—á–Ω—É—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å, –æ–±—É—á–µ–Ω–Ω—É—é –Ω–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö PangeaIns, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–µ–º 39 —è–∑—ã–∫–æ–≤. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –≤–≤–æ–¥—è—Ç PangeaBench - –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –Ω–∞–±–æ—Ä –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–µ–π –Ω–∞ 47 —è–∑—ã–∫–∞—Ö. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ Pangea –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–æ–¥–µ–ª–∏ —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–¥–æ–º –≤ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –∫—É–ª—å—Ç—É—Ä–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–æ –Ω–∞ —Ä–∞–∑–≤–∏—Ç–∏–µ –∏–Ω–∫–ª—é–∑–∏–≤–Ω—ã—Ö –∏ –Ω–∞–¥–µ–∂–Ω—ã—Ö –º—É–ª—å—Ç–∏—è–∑—ã—á–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, —Å–ø–æ—Å–æ–±—Å—Ç–≤—É—è —Ä–∞–≤–µ–Ω—Å—Ç–≤—É –∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –≤ —à–∏—Ä–æ–∫–æ–º –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–æ–º –∏ –∫—É–ª—å—Ç—É—Ä–Ω–æ–º —Å–ø–µ–∫—Ç—Ä–µ.'}, 'en': {'title': "Breaking Language Barriers: Pangea's Multilingual Revolution", 'desc': "The paper presents Pangea, a multilingual multimodal large language model designed to address the underrepresentation of non-English languages and diverse cultural contexts in current models. Pangea is trained on a dataset called PangeaIns, which includes high-quality English instructions, machine-translated instructions, and culturally relevant tasks across 39 languages. The model's performance is evaluated using PangeaBench, a comprehensive suite of 14 datasets covering 47 languages, showing superior results compared to existing models. The study highlights the importance of data diversity and open-sources all resources to encourage the development of more inclusive language models."}, 'zh': {'title': 'PangeaÔºöË∑®Ë∂äËØ≠Ë®ÄÂíåÊñáÂåñÁöÑÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã', 'desc': 'ËøôÁØáËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏Ä‰∏™Âêç‰∏∫PangeaÁöÑÂ§öËØ≠Ë®ÄÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºåÂÆÉ‰ΩøÁî®‰∫Ü‰∏Ä‰∏™ÂåÖÂê´39ÁßçËØ≠Ë®ÄÁöÑÂ§öÊ†∑ÂåñÊï∞ÊçÆÈõÜPangeaInsËøõË°åËÆ≠ÁªÉ„ÄÇPangeaInsÁöÑÊï∞ÊçÆÈõÜÂåÖÊã¨È´òË¥®ÈáèÁöÑËã±ËØ≠Êåá‰ª§„ÄÅÁ≤æÂøÉÊú∫Âô®ÁøªËØëÁöÑÊåá‰ª§‰ª•Âèä‰∏éÊñáÂåñÁõ∏ÂÖ≥ÁöÑÂ§öÊ®°ÊÄÅ‰ªªÂä°„ÄÇ‰∏∫‰∫ÜËØÑ‰º∞Ê®°ÂûãÁöÑËÉΩÂäõÔºåÁ†îÁ©∂ËÄÖ‰ª¨ËøòÂºïÂÖ•‰∫ÜPangeaBenchÔºå‰∏Ä‰∏™Ê∂µÁõñ47ÁßçËØ≠Ë®ÄÁöÑËØÑ‰º∞Â•ó‰ª∂„ÄÇÁªìÊûúÊòæÁ§∫ÔºåPangeaÂú®Â§öËØ≠Ë®ÄÂíåÂ§öÊñáÂåñËÉåÊôØ‰∏ãÁöÑË°®Áé∞‰ºò‰∫éÁé∞ÊúâÁöÑÂºÄÊ∫êÊ®°Âûã„ÄÇ'}}, 'pub_date_card': {'ru': '21 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 21', 'zh': '10Êúà21Êó•'}}, {'id': 'https://huggingface.co/papers/2410.16184', 'title': 'RM-Bench: Benchmarking Reward Models of Language Models with Subtlety and Style', 'url': 'https://huggingface.co/papers/2410.16184', 'abstract': 'Reward models are critical in techniques like Reinforcement Learning from Human Feedback (RLHF) and Inference Scaling Laws, where they guide language model alignment and select optimal responses. Despite their importance, existing reward model benchmarks often evaluate models by asking them to distinguish between responses generated by models of varying power. However, this approach fails to assess reward models on subtle but critical content changes and variations in style, resulting in a low correlation with policy model performance. To this end, we introduce RM-Bench, a novel benchmark designed to evaluate reward models based on their sensitivity to subtle content differences and resistance to style biases. Extensive experiments demonstrate that RM-Bench strongly correlates with policy model performance, making it a reliable reference for selecting reward models to align language models effectively. We evaluate nearly 40 reward models on RM-Bench. Our results reveal that even state-of-the-art models achieve an average performance of only 46.6%, which falls short of random-level accuracy (50%) when faced with style bias interference. These findings highlight the significant room for improvement in current reward models. Related code and data are available at https://github.com/THU-KEG/RM-Bench.', 'score': 23, 'issue_id': 209, 'pub_date': '2024-10-21', 'pub_date_ru': '21 –æ–∫—Ç—è–±—Ä—è', 'hash': '6fff842f967d7207', 'data': {'categories': ['#alignment', '#benchmark', '#rlhf'], 'emoji': 'üéØ', 'ru': {'title': 'RM-Bench: —Ç–æ—á–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ RM-Bench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –ø–æ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –æ—Ç —á–µ–ª–æ–≤–µ–∫–∞ (RLHF) –∏ –∑–∞–∫–æ–Ω–æ–≤ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤—ã–≤–æ–¥–∞. RM-Bench –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –∫ —Ç–æ–Ω–∫–∏–º —Ä–∞–∑–ª–∏—á–∏—è–º –≤ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–∏ –∏ –∏—Ö —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ —Å—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–∏–º —Å–º–µ—â–µ–Ω–∏—è–º. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ RM-Bench —Ö–æ—Ä–æ—à–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É–µ—Ç —Å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –º–æ–¥–µ–ª–µ–π –ø–æ–ª–∏—Ç–∏–∫–∏, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –µ–≥–æ –Ω–∞–¥–µ–∂–Ω—ã–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–º –¥–ª—è –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –ø—Ä–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ –¥–∞–∂–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –¥–æ—Å—Ç–∏–≥–∞—é—Ç —Å—Ä–µ–¥–Ω–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤—Å–µ–≥–æ 46.6%, —á—Ç–æ –Ω–∏–∂–µ —Å–ª—É—á–∞–π–Ω–æ–≥–æ —É—Ä–æ–≤–Ω—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ —Å—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö —Å–º–µ—â–µ–Ω–∏–π.'}, 'en': {'title': 'Enhancing Reward Models: Beyond Style Bias and Subtlety', 'desc': "The paper introduces RM-Bench, a new benchmark for evaluating reward models used in Reinforcement Learning from Human Feedback (RLHF) and Inference Scaling Laws. RM-Bench focuses on assessing models' sensitivity to subtle content changes and their resistance to style biases, which are often overlooked in traditional benchmarks. The study finds that even advanced reward models perform poorly, with an average accuracy of 46.6% when style bias is present, indicating a need for improvement. This benchmark provides a more reliable way to select reward models that align language models effectively."}, 'zh': {'title': 'ÊèêÂçáÂ•ñÂä±Ê®°ÂûãÔºöË∂ÖË∂äÈ£éÊ†ºÂÅèËßÅÁöÑÊåëÊàò', 'desc': 'ËøôÁØáËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂü∫ÂáÜÊµãËØïRM-BenchÔºåÁî®‰∫éËØÑ‰º∞Â•ñÂä±Ê®°ÂûãÂú®ÁªÜÂæÆÂÜÖÂÆπÂ∑ÆÂºÇÂíåÈ£éÊ†ºÂÅèËßÅ‰∏ãÁöÑË°®Áé∞„ÄÇÁé∞ÊúâÁöÑÂ•ñÂä±Ê®°ÂûãÂü∫ÂáÜÊµãËØïÂæÄÂæÄÂè™ÂÖ≥Ê≥®Ê®°Âûã‰πãÈó¥ÁöÑÁÆÄÂçïÂå∫ÂàÜÔºåËÄåÂøΩÁï•‰∫ÜÂØπÂÜÖÂÆπÂíåÈ£éÊ†ºÂèòÂåñÁöÑÊïèÊÑüÊÄß„ÄÇRM-BenchÈÄöËøáÂÆûÈ™åË°®ÊòéÔºå‰∏éÁ≠ñÁï•Ê®°ÂûãË°®Áé∞ÊúâÂæàÂº∫ÁöÑÁõ∏ÂÖ≥ÊÄßÔºåÊòØÈÄâÊã©Â•ñÂä±Ê®°ÂûãÁöÑÂèØÈù†ÂèÇËÄÉ„ÄÇÁ†îÁ©∂ÁªìÊûúÊòæÁ§∫ÔºåÂç≥‰ΩøÊòØÊúÄÂÖàËøõÁöÑÊ®°ÂûãÂú®Èù¢ÂØπÈ£éÊ†ºÂÅèËßÅÊó∂ÔºåË°®Áé∞‰πü‰ªÖ‰∏∫46.6%Ôºå‰Ωé‰∫éÈöèÊú∫Ê∞¥Âπ≥ÔºåË°®ÊòéÂΩìÂâçÂ•ñÂä±Ê®°ÂûãËøòÊúâÂæàÂ§ßÁöÑÊîπËøõÁ©∫Èó¥„ÄÇ'}}, 'pub_date_card': {'ru': '21 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 21', 'zh': '10Êúà21Êó•'}}, {'id': 'https://huggingface.co/papers/2410.12788', 'title': 'Meta-Chunking: Learning Efficient Text Segmentation via Logical Perception', 'url': 'https://huggingface.co/papers/2410.12788', 'abstract': 'Retrieval-Augmented Generation (RAG), while serving as a viable complement to large language models (LLMs), often overlooks the crucial aspect of text chunking within its pipeline, which impacts the quality of knowledge-intensive tasks. This paper introduces the concept of Meta-Chunking, which refers to a granularity between sentences and paragraphs, consisting of a collection of sentences within a paragraph that have deep linguistic logical connections. To implement Meta-Chunking, we designed two strategies based on LLMs: Margin Sampling Chunking and Perplexity Chunking. The former employs LLMs to perform binary classification on whether consecutive sentences need to be segmented, making decisions based on the probability difference obtained from margin sampling. The latter precisely identifies text chunk boundaries by analyzing the characteristics of perplexity distribution. Additionally, considering the inherent complexity of different texts, we propose a strategy that combines Meta-Chunking with dynamic merging to achieve a balance between fine-grained and coarse-grained text chunking. Experiments conducted on eleven datasets demonstrate that Meta-Chunking can more efficiently improve the performance of single-hop and multi-hop question answering based on RAG. For instance, on the 2WikiMultihopQA dataset, it outperforms similarity chunking by 1.32 while only consuming 45.8% of the time. Our code is available at https://github.com/IAAR-Shanghai/Meta-Chunking.', 'score': 20, 'issue_id': 208, 'pub_date': '2024-10-16', 'pub_date_ru': '16 –æ–∫—Ç—è–±—Ä—è', 'hash': '0fbcf072cc98c553', 'data': {'categories': ['#data', '#dataset', '#rag'], 'emoji': 'üß©', 'ru': {'title': '–ú–µ—Ç–∞-–ß–∞–Ω–∫–∏–Ω–≥: –ù–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–∞–∑–±–∏–µ–Ω–∏—é —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ RAG', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∫–æ–Ω—Ü–µ–ø—Ü–∏—é –ú–µ—Ç–∞-–ß–∞–Ω–∫–∏–Ω–≥–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è Retrieval-Augmented Generation (RAG). –ú–µ—Ç–∞-–ß–∞–Ω–∫–∏–Ω–≥ - —ç—Ç–æ –º–µ—Ç–æ–¥ —Ä–∞–∑–±–∏–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã, —É—á–∏—Ç—ã–≤–∞—é—â–∏–π –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∏ –º–µ–∂–¥—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –¥–≤–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏: Margin Sampling Chunking –∏ Perplexity Chunking, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–µ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ –æ–¥–∏–Ω–Ω–∞–¥—Ü–∞—Ç–∏ –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ú–µ—Ç–∞-–ß–∞–Ω–∫–∏–Ω–≥ –ø–æ–≤—ã—à–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å RAG –≤ –∑–∞–¥–∞—á–∞—Ö –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º.'}, 'en': {'title': '"Meta-Chunking: Enhancing RAG with Smarter Text Segmentation"', 'desc': 'The paper introduces Meta-Chunking, a method to improve text chunking in Retrieval-Augmented Generation (RAG) systems by focusing on the logical connections between sentences within paragraphs. Two strategies, Margin Sampling Chunking and Perplexity Chunking, are developed using large language models to determine optimal chunk boundaries. These methods enhance the efficiency and accuracy of question answering tasks by balancing fine-grained and coarse-grained chunking. Experiments show that Meta-Chunking significantly improves performance while reducing processing time compared to traditional chunking methods.'}, 'zh': {'title': 'Meta-ChunkingÔºöÊèêÂçáÊñáÊú¨ÂàÜÂùóÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'ËøôÁØáËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊñáÊú¨ÂàÜÂùóÊñπÊ≥ïÔºåÁß∞‰∏∫Meta-ChunkingÔºåÂÆÉÂú®Âè•Â≠êÂíåÊÆµËêΩ‰πãÈó¥ÊâæÂà∞‰∏ÄÁßçÊñ∞ÁöÑÁ≤íÂ∫¶„ÄÇMeta-ChunkingÈÄöËøá‰∏§ÁßçÁ≠ñÁï•ÂÆûÁé∞ÔºöËæπÁºòÈááÊ†∑ÂàÜÂùóÂíåÂõ∞ÊÉëÂ∫¶ÂàÜÂùóÔºåÂàÜÂà´Âà©Áî®Â§ßËØ≠Ë®ÄÊ®°ÂûãËøõË°å‰∫åÂÖÉÂàÜÁ±ªÂíåÂõ∞ÊÉëÂ∫¶ÂàÜÂ∏ÉÂàÜÊûê„ÄÇ‰∏∫‰∫ÜÈÄÇÂ∫î‰∏çÂêåÊñáÊú¨ÁöÑÂ§çÊùÇÊÄßÔºåËÆ∫ÊñáËøòÊèêÂá∫‰∫Ü‰∏ÄÁßçÁªìÂêàÂä®ÊÄÅÂêàÂπ∂ÁöÑÁ≠ñÁï•Ôºå‰ª•Âú®ÁªÜÁ≤íÂ∫¶ÂíåÁ≤óÁ≤íÂ∫¶ÂàÜÂùó‰πãÈó¥ÂèñÂæóÂπ≥Ë°°„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMeta-ChunkingÂú®ÊèêÈ´òÂü∫‰∫éRAGÁöÑÂçïË∑≥ÂíåÂ§öË∑≥ÈóÆÁ≠îÊÄßËÉΩÊñπÈù¢Êõ¥ÊúâÊïàÁéá„ÄÇ'}}, 'pub_date_card': {'ru': '16 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 16', 'zh': '10Êúà16Êó•'}}, {'id': 'https://huggingface.co/papers/2410.16215', 'title': 'Pre-training Distillation for Large Language Models: A Design Space Exploration', 'url': 'https://huggingface.co/papers/2410.16215', 'abstract': 'Knowledge distillation (KD) aims to transfer knowledge from a large teacher model to a smaller student model. Previous work applying KD in the field of large language models (LLMs) typically focused on the post-training phase, where the student LLM learns directly from instructions and corresponding responses generated by the teacher model. In this paper, we extend KD to the pre-training phase of LLMs, named pre-training distillation (PD). We first conduct a preliminary experiment using GLM-4-9B as the teacher LLM to distill a 1.9B parameter student LLM, validating the effectiveness of PD. Considering the key impact factors of distillation, we systematically explore the design space of pre-training distillation across four aspects: logits processing, loss selection, scaling law, and offline or online logits. We conduct extensive experiments to explore the design space of pre-training distillation and find better configurations and interesting conclusions, such as larger student LLMs generally benefiting more from pre-training distillation, while a larger teacher LLM does not necessarily guarantee better results. We hope our exploration of the design space will inform future practices in pre-training distillation.', 'score': 15, 'issue_id': 209, 'pub_date': '2024-10-21', 'pub_date_ru': '21 –æ–∫—Ç—è–±—Ä—è', 'hash': '0d2e73e11d2b2b3f', 'data': {'categories': ['#architecture'], 'emoji': 'üß†', 'ru': {'title': '–î–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è –∑–Ω–∞–Ω–∏–π –Ω–∞ —ç—Ç–∞–ø–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –∑–Ω–∞–Ω–∏–π (KD) –Ω–∞ —ç—Ç–∞–ø–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–æ–¥—è—Ç —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º GLM-4-9B –≤ –∫–∞—á–µ—Å—Ç–≤–µ —É—á–∏—Ç–µ–ª—è –¥–ª—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ —Å—Ç—É–¥–µ–Ω—Ç–∞ —Å 1,9 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –û–Ω–∏ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏–∑—É—á–∞—é—Ç –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –¥–∏–∑–∞–π–Ω–∞ –ø—Ä–µ–¥–æ–±—É—á–∞—é—â–µ–π –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –ø–æ —á–µ—Ç—ã—Ä–µ–º –∞—Å–ø–µ–∫—Ç–∞–º: –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ª–æ–≥–∏—Ç–æ–≤, –≤—ã–±–æ—Ä —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å, –∑–∞–∫–æ–Ω –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –æ—Ñ–ª–∞–π–Ω –∏–ª–∏ –æ–Ω–ª–∞–π–Ω –ª–æ–≥–∏—Ç—ã. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–µ —Å—Ç—É–¥–µ–Ω—á–µ—Å–∫–∏–µ LLM –æ–±—ã—á–Ω–æ –±–æ–ª—å—à–µ –≤—ã–∏–≥—Ä—ã–≤–∞—é—Ç –æ—Ç –ø—Ä–µ–¥–æ–±—É—á–∞—é—â–µ–π –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω–∞—è –º–æ–¥–µ–ª—å-—É—á–∏—Ç–µ–ª—å –Ω–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.'}, 'en': {'title': '"Pre-Training Distillation: A New Frontier in Model Efficiency"', 'desc': 'This paper explores a new approach to knowledge distillation by applying it during the pre-training phase of large language models, rather than the post-training phase. The authors conduct experiments using a large teacher model to distill knowledge into a smaller student model, demonstrating the effectiveness of this method. They investigate various factors that influence the success of pre-training distillation, such as how logits are processed and the choice of loss functions. The study reveals that larger student models benefit more from pre-training distillation, while the size of the teacher model does not always correlate with better outcomes.'}, 'zh': {'title': 'È¢ÑËÆ≠ÁªÉÈò∂ÊÆµÁöÑÁü•ËØÜËí∏È¶èÔºöÂ∞èÊ®°ÂûãÁöÑÂ§ßÊô∫ÊÖß', 'desc': 'ËøôÁØáËÆ∫ÊñáÁ†îÁ©∂‰∫ÜÂ¶Ç‰ΩïÂú®Â§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÈ¢ÑËÆ≠ÁªÉÈò∂ÊÆµËøõË°åÁü•ËØÜËí∏È¶è„ÄÇÈÄöËøá‰ΩøÁî®GLM-4-9B‰Ωú‰∏∫ÊïôÂ∏àÊ®°ÂûãÔºåÈ™åËØÅ‰∫ÜÈ¢ÑËÆ≠ÁªÉËí∏È¶èÁöÑÊúâÊïàÊÄß„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåËæÉÂ§ßÁöÑÂ≠¶ÁîüÊ®°ÂûãÈÄöÂ∏∏ËÉΩ‰ªéÈ¢ÑËÆ≠ÁªÉËí∏È¶è‰∏≠Ëé∑ÁõäÊõ¥Â§öÔºåËÄåËæÉÂ§ßÁöÑÊïôÂ∏àÊ®°Âûã‰∏ç‰∏ÄÂÆöÂ∏¶Êù•Êõ¥Â•ΩÁöÑÁªìÊûú„ÄÇ‰ΩúËÄÖÂ∏åÊúõËøô‰∫õÂèëÁé∞ËÉΩ‰∏∫Êú™Êù•ÁöÑÈ¢ÑËÆ≠ÁªÉËí∏È¶èÂÆûË∑µÊèê‰æõÊåáÂØº„ÄÇ'}}, 'pub_date_card': {'ru': '21 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 21', 'zh': '10Êúà21Êó•'}}, {'id': 'https://huggingface.co/papers/2410.15748', 'title': 'Alchemy: Amplifying Theorem-Proving Capability through Symbolic Mutation', 'url': 'https://huggingface.co/papers/2410.15748', 'abstract': 'Formal proofs are challenging to write even for experienced experts. Recent progress in Neural Theorem Proving (NTP) shows promise in expediting this process. However, the formal corpora available on the Internet are limited compared to the general text, posing a significant data scarcity challenge for NTP. To address this issue, this work proposes Alchemy, a general framework for data synthesis that constructs formal theorems through symbolic mutation. Specifically, for each candidate theorem in Mathlib, we identify all invocable theorems that can be used to rewrite or apply to it. Subsequently, we mutate the candidate theorem by replacing the corresponding term in the statement with its equivalent form or antecedent. As a result, our method increases the number of theorems in Mathlib by an order of magnitude, from 110k to 6M. Furthermore, we perform continual pretraining and supervised finetuning on this augmented corpus for large language models. Experimental results demonstrate the effectiveness of our approach, achieving a 5% absolute performance improvement on Leandojo benchmark. Additionally, our synthetic data achieve a 2.5% absolute performance gain on the out-of-distribution miniF2F benchmark. To provide further insights, we conduct a comprehensive analysis of synthetic data composition and the training paradigm, offering valuable guidance for developing a strong theorem prover.', 'score': 12, 'issue_id': 212, 'pub_date': '2024-10-21', 'pub_date_ru': '21 –æ–∫—Ç—è–±—Ä—è', 'hash': 'b6b22326cba00346', 'data': {'categories': ['#data', '#dataset', '#math', '#synthetic', '#training'], 'emoji': 'üß™', 'ru': {'title': '–ê–ª—Ö–∏–º–∏—è –¥–∞–Ω–Ω—ã—Ö: –ø—Ä–µ–≤—Ä–∞—â–∞–µ–º —Å–æ—Ç–Ω–∏ —Ç—ã—Å—è—á —Ç–µ–æ—Ä–µ–º –≤ –º–∏–ª–ª–∏–æ–Ω—ã', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Alchemy - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –¥–∞–Ω–Ω—ã—Ö –≤ –æ–±–ª–∞—Å—Ç–∏ –Ω–µ–π—Ä–æ–Ω–Ω–æ–≥–æ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ —Ç–µ–æ—Ä–µ–º (NTP). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ —Å–∏–º–≤–æ–ª—å–Ω–æ–π –º—É—Ç–∞—Ü–∏–∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –Ω–æ–≤—ã—Ö —Ç–µ–æ—Ä–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –≤ –±–∏–±–ª–∏–æ—Ç–µ–∫–µ Mathlib. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —É–≤–µ–ª–∏—á–∏—Ç—å –æ–±—ä–µ–º –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö —Å 110 —Ç—ã—Å—è—á –¥–æ 6 –º–∏–ª–ª–∏–æ–Ω–æ–≤ —Ç–µ–æ—Ä–µ–º. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ 5% –≤ —Ç–µ—Å—Ç–µ Leandojo –∏ –Ω–∞ 2.5% –Ω–∞ out-of-distribution –¥–∞–Ω–Ω—ã—Ö miniF2F –ø–æ—Å–ª–µ –¥–æ–æ–±—É—á–µ–Ω–∏—è –Ω–∞ —Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –∫–æ—Ä–ø—É—Å–µ.'}, 'en': {'title': 'Alchemy: Transforming Theorem Proving with Synthetic Data', 'desc': 'The paper introduces Alchemy, a framework designed to tackle the data scarcity problem in Neural Theorem Proving by generating synthetic formal theorems. By using symbolic mutation, Alchemy expands the number of theorems in Mathlib significantly, enhancing the training data available for large language models. The approach involves rewriting candidate theorems using invocable theorems, which increases the corpus size and improves model performance on benchmarks like Leandojo and miniF2F. The study also provides an analysis of the synthetic data and training methods, offering insights for building more effective theorem provers.'}, 'zh': {'title': 'AlchemyÔºöÈÄöËøáÊï∞ÊçÆÂêàÊàêÂ¢ûÂº∫Á•ûÁªèÂÆöÁêÜËØÅÊòé', 'desc': 'ËøôÁØáËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫AlchemyÁöÑÊï∞ÊçÆÂêàÊàêÊ°ÜÊû∂ÔºåÁî®‰∫éÈÄöËøáÁ¨¶Âè∑ÂèòÂºÇÊûÑÂª∫ÂΩ¢ÂºèÂÆöÁêÜ„ÄÇÈÄöËøáËØÜÂà´ÂíåÂ∫îÁî®ÂèØË∞ÉÁî®ÁöÑÂÆöÁêÜÔºåAlchemyÂ∞ÜMathlib‰∏≠ÁöÑÂÆöÁêÜÊï∞Èáè‰ªé11‰∏áÂ¢ûÂä†Âà∞600‰∏á„ÄÇÁÑ∂ÂêéÔºåÁ†îÁ©∂‰∫∫ÂëòÂú®Êâ©Â±ïÂêéÁöÑËØ≠ÊñôÂ∫ì‰∏äÂØπÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãËøõË°åÊåÅÁª≠È¢ÑËÆ≠ÁªÉÂíåÁõëÁù£ÂæÆË∞É„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËøôÁßçÊñπÊ≥ïÂú®LeandojoÂü∫ÂáÜ‰∏äÊèêÈ´ò‰∫Ü5%ÁöÑÊÄßËÉΩÔºåÂπ∂Âú®miniF2FÂü∫ÂáÜ‰∏äÊèêÈ´ò‰∫Ü2.5%ÁöÑÊÄßËÉΩ„ÄÇ'}}, 'pub_date_card': {'ru': '21 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 21', 'zh': '10Êúà21Êó•'}}, {'id': 'https://huggingface.co/papers/2410.15316', 'title': 'Ichigo: Mixed-Modal Early-Fusion Realtime Voice Assistant', 'url': 'https://huggingface.co/papers/2410.15316', 'abstract': 'Large Language Models (LLMs) have revolutionized natural language processing, but their application to speech-based tasks remains challenging due to the complexities of integrating audio and text modalities. This paper introduces Ichigo, a mixed-modal model that seamlessly processes interleaved sequences of speech and text. Utilizing a tokenized early-fusion approach, Ichigo quantizes speech into discrete tokens and employs a uniform transformer-based architecture for both speech and text modalities. This method enables joint reasoning and generation across modalities without the need for separate adapters. We present a comprehensive training methodology, including pre-training on multilingual speech recognition datasets and fine-tuning on a curated instruction dataset. Ichigo demonstrates state-of-the-art performance on speech question-answering benchmarks, outperforming existing open-source speech language models and achieving comparable results to cascaded systems. Notably, Ichigo exhibits a latency of just 111 ms to first token generation, significantly lower than current models. Our approach not only advances the field of multimodal AI but also provides a framework for smaller research teams to contribute effectively to open-source speech-language models.', 'score': 8, 'issue_id': 214, 'pub_date': '2024-10-20', 'pub_date_ru': '20 –æ–∫—Ç—è–±—Ä—è', 'hash': '0fbe64f20b885346', 'data': {'categories': ['#audio', '#multilingual', '#multimodal', '#training'], 'emoji': 'üçì', 'ru': {'title': '–ò—á–∏–≥–æ: –æ–±—ä–µ–¥–∏–Ω—è—è —Ä–µ—á—å –∏ —Ç–µ–∫—Å—Ç –≤ –µ–¥–∏–Ω–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏', 'desc': '–ò—á–∏–≥–æ - —ç—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, —Å–ø–æ—Å–æ–±–Ω–∞—è –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∫–∞–∫ —Ä–µ—á—å, —Ç–∞–∫ –∏ —Ç–µ–∫—Å—Ç. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ä–∞–Ω–Ω–∏–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–ª–∏—è–Ω–∏—é –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π, –∫–≤–∞–Ω—Ç—É—è —Ä–µ—á—å –≤ –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –∏ –ø—Ä–∏–º–µ–Ω—è—è –µ–¥–∏–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –¥–ª—è –æ–±–µ–∏—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π. –ò—á–∏–≥–æ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø–µ—Ä–µ–¥–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –∑–∞–¥–∞—á–∞—Ö –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ —Ä–µ—á–∏, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–æ–¥–µ–ª–∏ —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–¥–æ–º. –í–∞–∂–Ω—ã–º –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ–º –º–æ–¥–µ–ª–∏ —è–≤–ª—è–µ—Ç—Å—è –Ω–∏–∑–∫–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–µ—Ä–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ - –≤—Å–µ–≥–æ 111 –º—Å.'}, 'en': {'title': 'Ichigo: Bridging Speech and Text with Unified AI', 'desc': 'The paper introduces Ichigo, a model that processes both speech and text together using a unified transformer architecture. By converting speech into tokens, Ichigo can handle both modalities without needing separate systems, allowing for efficient joint reasoning. The model is trained on diverse datasets, achieving top performance in speech question-answering tasks with low latency. This approach not only enhances multimodal AI but also empowers smaller teams to develop competitive open-source models.'}, 'zh': {'title': 'IchigoÔºöË∑®Ë∂äËØ≠Èü≥‰∏éÊñáÊú¨ÁöÑÊó†ÁºùËûçÂêà', 'desc': 'ËøôÁØáËÆ∫Êñá‰ªãÁªç‰∫ÜIchigoÔºå‰∏ÄÁßçËÉΩÂ§üÂêåÊó∂Â§ÑÁêÜËØ≠Èü≥ÂíåÊñáÊú¨ÁöÑÊ∑∑ÂêàÊ®°ÊÄÅÊ®°Âûã„ÄÇIchigoÈÄöËøáÂ∞ÜËØ≠Èü≥ÈáèÂåñ‰∏∫Á¶ªÊï£ÁöÑtokenÔºåÂπ∂‰ΩøÁî®Áªü‰∏ÄÁöÑtransformerÊû∂ÊûÑÊù•Â§ÑÁêÜËØ≠Èü≥ÂíåÊñáÊú¨Ôºå‰ªéËÄåÂÆûÁé∞‰∫ÜË∑®Ê®°ÊÄÅÁöÑËÅîÂêàÊé®ÁêÜÂíåÁîüÊàê„ÄÇËØ•Ê®°ÂûãÂú®Â§öËØ≠Ë®ÄËØ≠Èü≥ËØÜÂà´Êï∞ÊçÆÈõÜ‰∏äËøõË°åÈ¢ÑËÆ≠ÁªÉÔºåÂπ∂Âú®Á≤æÂøÉÊåëÈÄâÁöÑÊåá‰ª§Êï∞ÊçÆÈõÜ‰∏äËøõË°åÂæÆË∞ÉÔºåË°®Áé∞Âá∫Ëâ≤„ÄÇIchigoÂú®ËØ≠Èü≥ÈóÆÁ≠îÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇÔºå‰∏îÈ¶ñÊ¨°ÁîüÊàêtokenÁöÑÂª∂Ëøü‰ªÖ‰∏∫111ÊØ´ÁßíÔºåÊòæËëó‰Ωé‰∫éÁé∞ÊúâÊ®°Âûã„ÄÇ'}}, 'pub_date_card': {'ru': '20 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 20', 'zh': '10Êúà20Êó•'}}, {'id': 'https://huggingface.co/papers/2410.11711', 'title': 'Zero-shot Model-based Reinforcement Learning using Large Language Models', 'url': 'https://huggingface.co/papers/2410.11711', 'abstract': "The emerging zero-shot capabilities of Large Language Models (LLMs) have led to their applications in areas extending well beyond natural language processing tasks. In reinforcement learning, while LLMs have been extensively used in text-based environments, their integration with continuous state spaces remains understudied. In this paper, we investigate how pre-trained LLMs can be leveraged to predict in context the dynamics of continuous Markov decision processes. We identify handling multivariate data and incorporating the control signal as key challenges that limit the potential of LLMs' deployment in this setup and propose Disentangled In-Context Learning (DICL) to address them. We present proof-of-concept applications in two reinforcement learning settings: model-based policy evaluation and data-augmented off-policy reinforcement learning, supported by theoretical analysis of the proposed methods. Our experiments further demonstrate that our approach produces well-calibrated uncertainty estimates. We release the code at https://github.com/abenechehab/dicl.", 'score': 8, 'issue_id': 212, 'pub_date': '2024-10-15', 'pub_date_ru': '15 –æ–∫—Ç—è–±—Ä—è', 'hash': '5868a5a1652a8da3', 'data': {'categories': ['#rl', '#rlhf'], 'emoji': 'ü§ñ', 'ru': {'title': 'LLM –ø–æ–∫–æ—Ä—è—é—Ç –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ —Å–æ—Å—Ç–æ—è–Ω–∏–π –≤ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º', 'desc': '–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–∏–Ω–∞–º–∏–∫–∏ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã—Ö –º–∞—Ä–∫–æ–≤—Å–∫–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ —Ä–∞–∑–¥–µ–ª–µ–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ (DICL) –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º –æ–±—Ä–∞–±–æ—Ç–∫–∏ –º–Ω–æ–≥–æ–º–µ—Ä–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ —É—á–µ—Ç–∞ —É–ø—Ä–∞–≤–ª—è—é—â–µ–≥–æ —Å–∏–≥–Ω–∞–ª–∞. –ú–µ—Ç–æ–¥ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –≤ –¥–≤—É—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º: –æ—Ü–µ–Ω–∫–µ –ø–æ–ª–∏—Ç–∏–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–æ–¥–µ–ª–∏ –∏ –æ–±—É—á–µ–Ω–∏–∏ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –≤–Ω–µ –ø–æ–ª–∏—Ç–∏–∫–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–ª—É—á–∏—Ç—å —Ö–æ—Ä–æ—à–æ –æ—Ç–∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏.'}, 'en': {'title': 'Unlocking LLMs for Continuous Reinforcement Learning', 'desc': 'This paper explores how Large Language Models (LLMs) can be used in reinforcement learning with continuous state spaces, a less-studied area. The authors propose a method called Disentangled In-Context Learning (DICL) to address challenges like handling multivariate data and incorporating control signals. They demonstrate the effectiveness of DICL in model-based policy evaluation and data-augmented off-policy reinforcement learning. The approach also provides well-calibrated uncertainty estimates, enhancing the reliability of predictions.'}, 'zh': {'title': 'Ëß£ÈîÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®Âº∫ÂåñÂ≠¶‰π†‰∏≠ÁöÑÊñ∞ÊΩúÂäõ', 'desc': 'ËøôÁØáËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂ¶Ç‰ΩïÂà©Áî®È¢ÑËÆ≠ÁªÉÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÊù•È¢ÑÊµãËøûÁª≠È©¨Â∞îÂèØÂ§´ÂÜ≥Á≠ñËøáÁ®ã‰∏≠ÁöÑÂä®ÊÄÅ„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÂ§ÑÁêÜÂ§öÂèòÈáèÊï∞ÊçÆÂíåÊï¥ÂêàÊéßÂà∂‰ø°Âè∑ÊòØÈôêÂà∂LLMsÂú®ËøôÁßçÁéØÂ¢É‰∏≠Â∫îÁî®ÁöÑÂÖ≥ÈîÆÊåëÊàò„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºå‰ΩúËÄÖÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫Ëß£ËÄ¶‰∏ä‰∏ãÊñáÂ≠¶‰π†ÔºàDICLÔºâÁöÑÊñ∞ÊñπÊ≥ï„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üÊèê‰æõËâØÂ•ΩÁöÑ‰∏çÁ°ÆÂÆöÊÄß‰º∞ËÆ°„ÄÇ'}}, 'pub_date_card': {'ru': '15 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 15', 'zh': '10Êúà15Êó•'}}, {'id': 'https://huggingface.co/papers/2410.15633', 'title': "Selecting Influential Samples for Long Context Alignment via Homologous Models' Guidance and Contextual Awareness Measurement", 'url': 'https://huggingface.co/papers/2410.15633', 'abstract': "The expansion of large language models to effectively handle instructions with extremely long contexts has yet to be fully investigated. The primary obstacle lies in constructing a high-quality long instruction-following dataset devised for long context alignment. Existing studies have attempted to scale up the available data volume by synthesizing long instruction-following samples. However, indiscriminately increasing the quantity of data without a well-defined strategy for ensuring data quality may introduce low-quality samples and restrict the final performance. To bridge this gap, we aim to address the unique challenge of long-context alignment, i.e., modeling the long-range dependencies for handling instructions and lengthy input contexts. We propose GATEAU, a novel framework designed to identify the influential and high-quality samples enriched with long-range dependency relations by utilizing crafted Homologous Models' Guidance (HMG) and Contextual Awareness Measurement (CAM). Specifically, HMG attempts to measure the difficulty of generating corresponding responses due to the long-range dependencies, using the perplexity scores of the response from two homologous models with different context windows. Also, the role of CAM is to measure the difficulty of understanding the long input contexts due to long-range dependencies by evaluating whether the model's attention is focused on important segments. Built upon both proposed methods, we select the most challenging samples as the influential data to effectively frame the long-range dependencies, thereby achieving better performance of LLMs. Comprehensive experiments indicate that GATEAU effectively identifies samples enriched with long-range dependency relations and the model trained on these selected samples exhibits better instruction-following and long-context understanding capabilities.", 'score': 7, 'issue_id': 209, 'pub_date': '2024-10-21', 'pub_date_ru': '21 –æ–∫—Ç—è–±—Ä—è', 'hash': '0b470bc767fc516b', 'data': {'categories': ['#data', '#dataset', '#long_context', '#synthetic'], 'emoji': 'üß†', 'ru': {'title': 'GATEAU: –£–º–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ LLM –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö', 'desc': "–≠—Ç–æ—Ç –Ω–∞—É—á–Ω—ã–π —Ç—Ä—É–¥ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç GATEAU - –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Ä–∞–±–æ—Ç–∞—Ç—å —Å –¥–ª–∏–Ω–Ω—ã–º–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞–º–∏. GATEAU –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤–∞ –º–µ—Ç–æ–¥–∞: Homologous Models' Guidance (HMG) –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤, –∏ Contextual Awareness Measurement (CAM) –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –°–∏—Å—Ç–µ–º–∞ –≤—ã–±–∏—Ä–∞–µ—Ç –Ω–∞–∏–±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–µ –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –æ–±—Ä–∞–∑—Ü—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è LLM. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã—Ö GATEAU –¥–∞–Ω–Ω—ã—Ö, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤."}, 'en': {'title': 'Mastering Long Contexts with GATEAU: Quality Over Quantity', 'desc': "The paper introduces GATEAU, a framework designed to improve large language models' ability to handle long instructions by focusing on high-quality data samples. It uses Homologous Models' Guidance (HMG) to assess the difficulty of generating responses with long-range dependencies and Contextual Awareness Measurement (CAM) to ensure the model's attention is on important segments. By selecting challenging samples, GATEAU enhances the model's performance in understanding and following long-context instructions. Experiments show that models trained with GATEAU-selected data perform better in long-context tasks."}, 'zh': {'title': 'GATEAUÔºöÊèêÂçáÈïø‰∏ä‰∏ãÊñáÊåá‰ª§Â§ÑÁêÜÁöÑÊñ∞Ê°ÜÊû∂', 'desc': 'ËøôÁØáËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂ¶Ç‰ΩïËÆ©Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊõ¥Â•ΩÂú∞Â§ÑÁêÜÈïø‰∏ä‰∏ãÊñáÁöÑÊåá‰ª§„ÄÇ‰∏ªË¶ÅÊåëÊàòÂú®‰∫éÊûÑÂª∫‰∏Ä‰∏™È´òË¥®ÈáèÁöÑÈïøÊåá‰ª§Êï∞ÊçÆÈõÜÔºå‰ª•‰æøÊ®°ÂûãËÉΩÊõ¥Â•ΩÂú∞ÂØπÈΩêÈïø‰∏ä‰∏ãÊñá„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºå‰ΩúËÄÖÊèêÂá∫‰∫Ü‰∏Ä‰∏™Âêç‰∏∫GATEAUÁöÑÊñ∞Ê°ÜÊû∂ÔºåÈÄöËøáÂêåÊ∫êÊ®°ÂûãÊåáÂØºÂíå‰∏ä‰∏ãÊñáÊÑèËØÜÊµãÈáèÊù•ËØÜÂà´È´òË¥®ÈáèÊ†∑Êú¨„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºå‰ΩøÁî®Ëøô‰∫õÊ†∑Êú¨ËÆ≠ÁªÉÁöÑÊ®°ÂûãÂú®Êåá‰ª§Ë∑üÈöèÂíåÈïø‰∏ä‰∏ãÊñáÁêÜËß£ÊñπÈù¢Ë°®Áé∞Êõ¥Â•Ω„ÄÇ'}}, 'pub_date_card': {'ru': '21 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 21', 'zh': '10Êúà21Êó•'}}, {'id': 'https://huggingface.co/papers/2410.15002', 'title': 'How Many Van Goghs Does It Take to Van Gogh? Finding the Imitation Threshold', 'url': 'https://huggingface.co/papers/2410.15002', 'abstract': "Text-to-image models are trained using large datasets collected by scraping image-text pairs from the internet. These datasets often include private, copyrighted, and licensed material. Training models on such datasets enables them to generate images with such content, which might violate copyright laws and individual privacy. This phenomenon is termed imitation -- generation of images with content that has recognizable similarity to its training images. In this work we study the relationship between a concept's frequency in the training dataset and the ability of a model to imitate it. We seek to determine the point at which a model was trained on enough instances to imitate a concept -- the imitation threshold. We posit this question as a new problem: Finding the Imitation Threshold (FIT) and propose an efficient approach that estimates the imitation threshold without incurring the colossal cost of training multiple models from scratch. We experiment with two domains -- human faces and art styles -- for which we create four datasets, and evaluate three text-to-image models which were trained on two pretraining datasets. Our results reveal that the imitation threshold of these models is in the range of 200-600 images, depending on the domain and the model. The imitation threshold can provide an empirical basis for copyright violation claims and acts as a guiding principle for text-to-image model developers that aim to comply with copyright and privacy laws. We release the code and data at https://github.com/vsahil/MIMETIC-2.git and the project's website is hosted at https://how-many-van-goghs-does-it-take.github.io.", 'score': 6, 'issue_id': 216, 'pub_date': '2024-10-19', 'pub_date_ru': '19 –æ–∫—Ç—è–±—Ä—è', 'hash': 'f538a28b342207ac', 'data': {'categories': ['#cv', '#data', '#dataset', '#ethics'], 'emoji': 'üñºÔ∏è', 'ru': {'title': '–°–∫–æ–ª—å–∫–æ –í–∞–Ω –ì–æ–≥–æ–≤ –Ω—É–∂–Ω–æ, —á—Ç–æ–±—ã –æ–±—É—á–∏—Ç—å –ò–ò?', 'desc': "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –∏–º–∏—Ç–∞—Ü–∏–∏ –≤ –º–æ–¥–µ–ª—è—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç—É. –ê–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç –ø–æ–Ω—è—Ç–∏–µ '–ø–æ—Ä–æ–≥–∞ –∏–º–∏—Ç–∞—Ü–∏–∏' - –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø—Ä–∏–º–µ—Ä–æ–≤, –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–≥–æ –º–æ–¥–µ–ª–∏ –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ü–µ–ø—Ç–∞. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑–∞–ª–æ, —á—Ç–æ –¥–ª—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —ç—Ç–æ—Ç –ø–æ—Ä–æ–≥ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç 200-600 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –¥–æ–º–µ–Ω–∞ –∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–º–µ—é—Ç –≤–∞–∂–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è —Å–æ–±–ª—é–¥–µ–Ω–∏—è –∞–≤—Ç–æ—Ä—Å–∫–∏—Ö –ø—Ä–∞–≤ –∏ –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π."}, 'en': {'title': 'Finding the Imitation Threshold: Balancing Creativity and Copyright in AI', 'desc': 'This paper explores how text-to-image models can generate images that closely resemble copyrighted or private content from their training datasets, a phenomenon known as imitation. The authors introduce the concept of the imitation threshold, which is the minimum number of instances a model needs to see in its training data to replicate a concept. They propose a method to estimate this threshold efficiently, without the need to train multiple models from scratch. Their experiments show that the imitation threshold varies between 200-600 images, providing a basis for understanding potential copyright violations and guiding model developers in legal compliance.'}, 'zh': {'title': 'Êè≠Á§∫ÊñáÊú¨Âà∞ÂõæÂÉèÊ®°ÂûãÁöÑÊ®°‰ªøÈòàÂÄº', 'desc': 'ËøôÁØáËÆ∫ÊñáÁ†îÁ©∂‰∫ÜÊñáÊú¨Âà∞ÂõæÂÉèÊ®°ÂûãÂú®ËÆ≠ÁªÉÊï∞ÊçÆÈõÜ‰∏≠Ê®°‰ªøÊ¶ÇÂøµÁöÑËÉΩÂäõ„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÂΩì‰∏Ä‰∏™Ê¶ÇÂøµÂú®ËÆ≠ÁªÉÊï∞ÊçÆÈõÜ‰∏≠Âá∫Áé∞200Âà∞600Ê¨°Êó∂ÔºåÊ®°ÂûãÂ∞±ËÉΩÊúâÊïàÊ®°‰ªøËØ•Ê¶ÇÂøµÔºåËøôË¢´Áß∞‰∏∫Ê®°‰ªøÈòàÂÄº„ÄÇÈÄöËøáÊâæÂà∞Ê®°‰ªøÈòàÂÄºÔºåÂèØ‰ª•Â∏ÆÂä©Ê®°ÂûãÂºÄÂèëËÄÖÈÅµÂæ™ÁâàÊùÉÂíåÈöêÁßÅÊ≥ïÂæã„ÄÇÁ†îÁ©∂ÁªìÊûú‰∏∫ÁâàÊùÉ‰æµÊùÉÁöÑÂà§Êñ≠Êèê‰æõ‰∫ÜÂÆûËØÅ‰æùÊçÆ„ÄÇ'}}, 'pub_date_card': {'ru': '19 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 19', 'zh': '10Êúà19Êó•'}}, {'id': 'https://huggingface.co/papers/2410.16259', 'title': 'Agent-to-Sim: Learning Interactive Behavior Models from Casual Longitudinal Videos', 'url': 'https://huggingface.co/papers/2410.16259', 'abstract': 'We present Agent-to-Sim (ATS), a framework for learning interactive behavior models of 3D agents from casual longitudinal video collections. Different from prior works that rely on marker-based tracking and multiview cameras, ATS learns natural behaviors of animal and human agents non-invasively through video observations recorded over a long time-span (e.g., a month) in a single environment. Modeling 3D behavior of an agent requires persistent 3D tracking (e.g., knowing which point corresponds to which) over a long time period. To obtain such data, we develop a coarse-to-fine registration method that tracks the agent and the camera over time through a canonical 3D space, resulting in a complete and persistent spacetime 4D representation. We then train a generative model of agent behaviors using paired data of perception and motion of an agent queried from the 4D reconstruction. ATS enables real-to-sim transfer from video recordings of an agent to an interactive behavior simulator. We demonstrate results on pets (e.g., cat, dog, bunny) and human given monocular RGBD videos captured by a smartphone.', 'score': 4, 'issue_id': 216, 'pub_date': '2024-10-21', 'pub_date_ru': '21 –æ–∫—Ç—è–±—Ä—è', 'hash': '2af6ae782ab41cc9', 'data': {'categories': ['#3d', '#agents', '#cv'], 'emoji': 'üé•', 'ru': {'title': '–û—Ç –≤–∏–¥–µ–æ –∫ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–º—É –ø–æ–≤–µ–¥–µ–Ω–∏—é: –æ–±—É—á–µ–Ω–∏–µ 3D-–∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –Ω–∞–±–ª—é–¥–µ–Ω–∏–π', 'desc': '–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ Agent-to-Sim (ATS) –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–≤–µ–¥–µ–Ω–∏—è 3D-–∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—ã—á–Ω—ã—Ö –≤–∏–¥–µ–æ–∫–æ–ª–ª–µ–∫—Ü–∏–π. ATS –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ–∏–Ω–≤–∞–∑–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥, –∞–Ω–∞–ª–∏–∑–∏—Ä—É—è –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ –∂–∏–≤–æ—Ç–Ω—ã—Ö –∏ –ª—é–¥–µ–π –ø–æ –≤–∏–¥–µ–æ–∑–∞–ø–∏—Å—è–º, —Å–¥–µ–ª–∞–Ω–Ω—ã–º –≤ —Ç–µ—á–µ–Ω–∏–µ –¥–ª–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø–µ—Ä–∏–æ–¥–∞ –≤ –æ–¥–Ω–æ–π —Å—Ä–µ–¥–µ. –†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω –º–µ—Ç–æ–¥ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –æ—Ç –≥—Ä—É–±–æ–π –∫ —Ç–æ—á–Ω–æ–π –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∞–≥–µ–Ω—Ç–∞ –∏ –∫–∞–º–µ—Ä—ã –≤–æ –≤—Ä–µ–º–µ–Ω–∏ —á–µ—Ä–µ–∑ –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–æ–µ 3D-–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ. –ù–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –æ–±—É—á–∞–µ—Ç—Å—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å –ø–æ–≤–µ–¥–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–∞, –ø–æ–∑–≤–æ–ª—è—é—â–∞—è —Å–æ–∑–¥–∞–≤–∞—Ç—å –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Å–∏–º—É–ª—è—Ç–æ—Ä –ø–æ–≤–µ–¥–µ–Ω–∏—è.'}, 'en': {'title': 'From Video to Virtual: Simulating Real-World Behaviors', 'desc': "The paper introduces Agent-to-Sim (ATS), a framework for learning 3D behavior models of agents from long-term video recordings without using invasive tracking methods. ATS uses a novel coarse-to-fine registration technique to maintain persistent 3D tracking of agents over time, creating a comprehensive 4D representation. This data is then used to train a generative model that simulates the agent's behavior based on its perception and motion. The framework successfully transfers real-world video observations into interactive behavior simulations, demonstrated with pets and humans using simple smartphone recordings."}, 'zh': {'title': '‰ªéËßÜÈ¢ëÂà∞Ê®°ÊãüÔºöÈùû‰æµÂÖ•ÊÄß3DË°å‰∏∫Âª∫Ê®°', 'desc': 'ËøôÁØáËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫Agent-to-Sim (ATS)ÁöÑÊ°ÜÊû∂ÔºåÁî®‰∫é‰ªéÈïøÊúüËßÜÈ¢ë‰∏≠Â≠¶‰π†3D‰ª£ÁêÜÁöÑ‰∫§‰∫íË°å‰∏∫Ê®°Âûã„ÄÇ‰∏é‰æùËµñÊ†áËÆ∞Ë∑üË∏™ÂíåÂ§öËßÜËßíÁõ∏Êú∫ÁöÑ‰º†ÁªüÊñπÊ≥ï‰∏çÂêåÔºåATSÈÄöËøáÂçï‰∏ÄÁéØÂ¢É‰∏≠ÈïøÊó∂Èó¥ÁöÑËßÜÈ¢ëËßÇÂØüÔºåÈùû‰æµÂÖ•ÊÄßÂú∞Â≠¶‰π†Âä®Áâ©Âíå‰∫∫Á±ª‰ª£ÁêÜÁöÑËá™ÁÑ∂Ë°å‰∏∫„ÄÇ‰∏∫‰∫ÜÂÆûÁé∞3DË°å‰∏∫Âª∫Ê®°ÔºåÁ†îÁ©∂‰∫∫ÂëòÂºÄÂèë‰∫Ü‰∏ÄÁßçÁ≤óÂà∞ÁªÜÁöÑÊ≥®ÂÜåÊñπÊ≥ïÔºåÈÄöËøá‰∏Ä‰∏™Ê†áÂáÜÁöÑ3DÁ©∫Èó¥Ë∑üË∏™‰ª£ÁêÜÂíåÁõ∏Êú∫ÔºåÂΩ¢ÊàêÂÆåÊï¥ÁöÑÊó∂Á©∫4DË°®Á§∫„ÄÇÊúÄÁªàÔºåATSÂèØ‰ª•Â∞ÜËßÜÈ¢ëËÆ∞ÂΩï‰∏≠ÁöÑ‰ª£ÁêÜË°å‰∏∫ËΩ¨ÁßªÂà∞‰∫§‰∫íË°å‰∏∫Ê®°ÊãüÂô®‰∏≠„ÄÇ'}}, 'pub_date_card': {'ru': '21 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 21', 'zh': '10Êúà21Êó•'}}, {'id': 'https://huggingface.co/papers/2410.13218', 'title': 'CBT-Bench: Evaluating Large Language Models on Assisting Cognitive Behavior Therapy', 'url': 'https://huggingface.co/papers/2410.13218', 'abstract': "There is a significant gap between patient needs and available mental health support today. In this paper, we aim to thoroughly examine the potential of using Large Language Models (LLMs) to assist professional psychotherapy. To this end, we propose a new benchmark, CBT-BENCH, for the systematic evaluation of cognitive behavioral therapy (CBT) assistance. We include three levels of tasks in CBT-BENCH: I: Basic CBT knowledge acquisition, with the task of multiple-choice questions; II: Cognitive model understanding, with the tasks of cognitive distortion classification, primary core belief classification, and fine-grained core belief classification; III: Therapeutic response generation, with the task of generating responses to patient speech in CBT therapy sessions. These tasks encompass key aspects of CBT that could potentially be enhanced through AI assistance, while also outlining a hierarchy of capability requirements, ranging from basic knowledge recitation to engaging in real therapeutic conversations. We evaluated representative LLMs on our benchmark. Experimental results indicate that while LLMs perform well in reciting CBT knowledge, they fall short in complex real-world scenarios requiring deep analysis of patients' cognitive structures and generating effective responses, suggesting potential future work.", 'score': 4, 'issue_id': 207, 'pub_date': '2024-10-17', 'pub_date_ru': '17 –æ–∫—Ç—è–±—Ä—è', 'hash': 'f4f24ef7771a745c', 'data': {'categories': ['#benchmark', '#medicine'], 'emoji': 'üß†', 'ru': {'title': '–û—Ü–µ–Ω–∫–∞ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞ –ò–ò –≤ –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–æ-–ø–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–æ–π —Ç–µ—Ä–∞–ø–∏–∏', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ CBT-BENCH –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–æ-–ø–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–æ–π —Ç–µ—Ä–∞–ø–∏–∏ (–ö–ü–¢). –ë–µ–Ω—á–º–∞—Ä–∫ –≤–∫–ª—é—á–∞–µ—Ç —Ç—Ä–∏ —É—Ä–æ–≤–Ω—è –∑–∞–¥–∞—á: –±–∞–∑–æ–≤—ã–µ –∑–Ω–∞–Ω–∏—è –ö–ü–¢, –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–æ–π –º–æ–¥–µ–ª–∏ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Ç–µ—Ä–∞–ø–µ–≤—Ç–∏—á–µ—Å–∫–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —Ö–æ—Ä–æ—à–æ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ–º –∑–Ω–∞–Ω–∏–π –ö–ü–¢, –Ω–æ –∏—Å–ø—ã—Ç—ã–≤–∞—é—Ç —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ –≤ —Å–ª–æ–∂–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö, —Ç—Ä–µ–±—É—é—â–∏—Ö –≥–ª—É–±–æ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä –ø–∞—Ü–∏–µ–Ω—Ç–æ–≤. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–ª—è –±—É–¥—É—â–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ –æ–±–ª–∞—Å—Ç–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –ò–ò –≤ –ø—Å–∏—Ö–æ—Ç–µ—Ä–∞–ø–∏–∏.'}, 'en': {'title': 'Bridging the Gap: AI Meets Mental Health Support', 'desc': "This paper explores the use of Large Language Models (LLMs) to support psychotherapy, specifically cognitive behavioral therapy (CBT). It introduces CBT-BENCH, a benchmark designed to evaluate LLMs' ability to assist in CBT through tasks like knowledge acquisition, cognitive model understanding, and therapeutic response generation. The study finds that while LLMs can effectively recall CBT knowledge, they struggle with more complex tasks that require deep understanding and interaction. This highlights the need for further development to enhance LLMs' capabilities in real-world therapeutic settings."}, 'zh': {'title': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºöÂøÉÁêÜÊ≤ªÁñóÁöÑÊú™Êù•Âä©ÊâãÔºü', 'desc': 'ËøôÁØáËÆ∫ÊñáÊé¢ËÆ®‰∫Ü‰ΩøÁî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÊù•ËæÖÂä©‰∏ì‰∏öÂøÉÁêÜÊ≤ªÁñóÁöÑÊΩúÂäõ„ÄÇ‰∏∫Ê≠§Ôºå‰ΩúËÄÖÊèêÂá∫‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑÂü∫ÂáÜÊµãËØïÔºåÂêç‰∏∫CBT-BENCHÔºåÁî®‰∫éÁ≥ªÁªüËØÑ‰º∞ËÆ§Áü•Ë°å‰∏∫ÁñóÊ≥ïÔºàCBTÔºâÁöÑËæÖÂä©ËÉΩÂäõ„ÄÇCBT-BENCHÂåÖÊã¨‰∏â‰∏™‰ªªÂä°Â±ÇÊ¨°ÔºöÂü∫Á°ÄÁü•ËØÜËé∑Âèñ„ÄÅËÆ§Áü•Ê®°ÂûãÁêÜËß£ÂíåÊ≤ªÁñóÊÄßÂõûÂ∫îÁîüÊàê„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËôΩÁÑ∂LLMsÂú®Áü•ËØÜËÉåËØµÊñπÈù¢Ë°®Áé∞ËâØÂ•ΩÔºå‰ΩÜÂú®ÈúÄË¶ÅÊ∑±ÂÖ•ÂàÜÊûêÊÇ£ËÄÖËÆ§Áü•ÁªìÊûÑÂíåÁîüÊàêÊúâÊïàÂõûÂ∫îÁöÑÂ§çÊùÇÂú∫ÊôØ‰∏≠Ë°®Áé∞‰∏çË∂≥„ÄÇ'}}, 'pub_date_card': {'ru': '17 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 17', 'zh': '10Êúà17Êó•'}}, {'id': 'https://huggingface.co/papers/2410.14086', 'title': "In-context learning and Occam's razor", 'url': 'https://huggingface.co/papers/2410.14086', 'abstract': "The goal of machine learning is generalization. While the No Free Lunch Theorem states that we cannot obtain theoretical guarantees for generalization without further assumptions, in practice we observe that simple models which explain the training data generalize best: a principle called Occam's razor. Despite the need for simple models, most current approaches in machine learning only minimize the training error, and at best indirectly promote simplicity through regularization or architecture design. Here, we draw a connection between Occam's razor and in-context learning: an emergent ability of certain sequence models like Transformers to learn at inference time from past observations in a sequence. In particular, we show that the next-token prediction loss used to train in-context learners is directly equivalent to a data compression technique called prequential coding, and that minimizing this loss amounts to jointly minimizing both the training error and the complexity of the model that was implicitly learned from context. Our theory and the empirical experiments we use to support it not only provide a normative account of in-context learning, but also elucidate the shortcomings of current in-context learning methods, suggesting ways in which they can be improved. We make our code available at https://github.com/3rdCore/PrequentialCode.", 'score': 2, 'issue_id': 214, 'pub_date': '2024-10-17', 'pub_date_ru': '17 –æ–∫—Ç—è–±—Ä—è', 'hash': '6f9c84cdfe1502b2', 'data': {'categories': ['#interpretability', '#math'], 'emoji': 'ü™í', 'ru': {'title': '–ë—Ä–∏—Ç–≤–∞ –û–∫–∫–∞–º–∞ –≤ –¥–µ–π—Å—Ç–≤–∏–∏: –∫–∞–∫ –æ–±—É—á–µ–Ω–∏–µ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –≤–æ–ø–ª–æ—â–∞–µ—Ç –ø—Ä–∏–Ω—Ü–∏–ø –ø—Ä–æ—Å—Ç–æ—Ç—ã', 'desc': "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —Å–≤—è–∑—å –º–µ–∂–¥—É –±—Ä–∏—Ç–≤–æ–π –û–∫–∫–∞–º–∞ –∏ –æ–±—É—á–µ–Ω–∏–µ–º –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –≤ –º–∞—à–∏–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞ —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω–∞ –º–µ—Ç–æ–¥—É —Å–∂–∞—Ç–∏—è –¥–∞–Ω–Ω—ã—Ö –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º '–ø—Ä–µ–∫–≤–∞–µ–Ω—Ç–∏–∞–ª—å–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ'. –ú–∏–Ω–∏–º–∏–∑–∞—Ü–∏—è —ç—Ç–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –º–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ—Ç –æ—à–∏–±–∫—É –æ–±—É—á–µ–Ω–∏—è –∏ —Å–ª–æ–∂–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏, –Ω–µ—è–≤–Ω–æ –∏–∑—É—á–µ–Ω–Ω–æ–π –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –∏ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏ —Ç–µ–∫—É—â–∏—Ö –º–µ—Ç–æ–¥–æ–≤, –ø—Ä–µ–¥–ª–∞–≥–∞—è –ø—É—Ç–∏ –∏—Ö —É–ª—É—á—à–µ–Ω–∏—è."}, 'en': {'title': "Simplifying Complexity: Bridging Occam's Razor and In-Context Learning", 'desc': "The paper explores the connection between Occam's razor, which favors simpler models for better generalization, and in-context learning, a feature of sequence models like Transformers. It demonstrates that the next-token prediction loss used in training these models is akin to prequential coding, a data compression method. This approach effectively minimizes both training error and model complexity, aligning with the principle of Occam's razor. The authors provide theoretical insights and empirical evidence, highlighting the limitations of current in-context learning methods and suggesting improvements."}, 'zh': {'title': 'Â••Âç°ÂßÜÂâÉÂàÄ‰∏é‰∏ä‰∏ãÊñáÂ≠¶‰π†ÁöÑÂ∑ßÂ¶ôÁªìÂêà', 'desc': 'Êú∫Âô®Â≠¶‰π†ÁöÑÁõÆÊ†áÊòØÂÆûÁé∞Ê≥õÂåñÔºåÂç≥Ê®°Âûã‰∏ç‰ªÖËÉΩÂú®ËÆ≠ÁªÉÊï∞ÊçÆ‰∏äË°®Áé∞ËâØÂ•ΩÔºåËøòËÉΩÂú®Êñ∞Êï∞ÊçÆ‰∏äË°®Áé∞Âá∫Ëâ≤„ÄÇÂ••Âç°ÂßÜÂâÉÂàÄÂéüÂàôÊåáÂá∫ÔºåÁÆÄÂçïÁöÑÊ®°ÂûãÂæÄÂæÄËÉΩÊõ¥Â•ΩÂú∞Ê≥õÂåñÔºå‰ΩÜÁõÆÂâçÂ§ßÂ§öÊï∞ÊñπÊ≥ïÂè™ÂÖ≥Ê≥®ÊúÄÂ∞èÂåñËÆ≠ÁªÉËØØÂ∑Æ„ÄÇÊú¨ÊñáÂ∞ÜÂ••Âç°ÂßÜÂâÉÂàÄ‰∏é‰∏ä‰∏ãÊñáÂ≠¶‰π†ËÅîÁ≥ªËµ∑Êù•ÔºåÂ±ïÁ§∫‰∫ÜÂ¶Ç‰ΩïÈÄöËøáÊúÄÂ∞èÂåñ‰∏ã‰∏Ä‰∏™ËØçÈ¢ÑÊµãÊçüÂ§±Êù•ÂêåÊó∂ÂáèÂ∞ëËÆ≠ÁªÉËØØÂ∑ÆÂíåÊ®°ÂûãÂ§çÊùÇÊÄß„ÄÇÊàë‰ª¨ÁöÑÁêÜËÆ∫ÂíåÂÆûÈ™åÊè≠Á§∫‰∫ÜÂΩìÂâç‰∏ä‰∏ãÊñáÂ≠¶‰π†ÊñπÊ≥ïÁöÑ‰∏çË∂≥ÔºåÂπ∂ÊèêÂá∫‰∫ÜÊîπËøõÂª∫ËÆÆ„ÄÇ'}}, 'pub_date_card': {'ru': '17 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 17', 'zh': '10Êúà17Êó•'}}, {'id': 'https://huggingface.co/papers/2410.13184', 'title': 'Router-Tuning: A Simple and Effective Approach for Enabling Dynamic-Depth in Transformers', 'url': 'https://huggingface.co/papers/2410.13184', 'abstract': "Traditional transformer models often allocate a fixed amount of computational resources to every input token, leading to inefficient and unnecessary computation. To address this, the Mixture of Depths (MoD) was introduced to dynamically adjust the computational depth by skipping less important layers. Despite its promise, current MoD approaches remain under-explored and face two main challenges: (1) high training costs due to the need to train the entire model along with the routers that determine which layers to skip, and (2) the risk of performance degradation when important layers are bypassed. In response to the first issue, we propose Router-Tuning, a method that fine-tunes only the router on a small dataset, drastically reducing the computational overhead associated with full model training. For the second challenge, we propose MindSkip, which deploys Attention with Dynamic Depths. This method preserves the model's performance while significantly enhancing computational and memory efficiency. Extensive experiments demonstrate that our approach delivers competitive results while dramatically improving the computation efficiency, e.g., 21\\% speedup and only a 0.2\\% performance drop. The code is released at https://github.com/CASE-Lab-UMD/Router-Tuning.", 'score': 2, 'issue_id': 214, 'pub_date': '2024-10-17', 'pub_date_ru': '17 –æ–∫—Ç—è–±—Ä—è', 'hash': '0e28d555da53ebff', 'data': {'categories': ['#architecture', '#optimization'], 'emoji': 'üöÄ', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã: —É–º–Ω–æ–µ –ø—Ä–æ–ø—É—Å–∫–∞–Ω–∏–µ —Å–ª–æ–µ–≤ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ Router-Tuning –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –ø–æ–¥—Ö–æ–¥ MindSkip, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –≤–Ω–∏–º–∞–Ω–∏–µ —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –≥–ª—É–±–∏–Ω–æ–π, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –ø—Ä–∏ –ø–æ–≤—ã—à–µ–Ω–∏–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏ 21% —É—Å–∫–æ—Ä–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã –ø—Ä–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–º —Å–Ω–∏–∂–µ–Ω–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞ 0.2%. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –≤—ã—Å–æ–∫–∏—Ö –∑–∞—Ç—Ä–∞—Ç –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ –∏ —Ä–∏—Å–∫–∞ –ø—Ä–æ–ø—É—Å–∫–∞ –≤–∞–∂–Ω—ã—Ö —Å–ª–æ–µ–≤ –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø–æ–¥—Ö–æ–¥–∞—Ö Mixture of Depths.'}, 'en': {'title': 'Efficient Transformers: Skipping the Unnecessary', 'desc': 'The paper introduces a novel approach to improve the efficiency of transformer models by using a Mixture of Depths (MoD) strategy, which dynamically adjusts computational depth by skipping less important layers. To tackle the high training costs associated with MoD, the authors propose Router-Tuning, a method that fine-tunes only the router on a small dataset, reducing computational overhead. To prevent performance degradation, they introduce MindSkip, which uses Attention with Dynamic Depths to maintain model performance while enhancing efficiency. Experiments show that their approach achieves a 21% speedup with only a 0.2% performance drop, demonstrating its effectiveness in improving computation efficiency.'}, 'zh': {'title': 'Âä®ÊÄÅÊ∑±Â∫¶ÔºöÊèêÂçáTransformerËÆ°ÁÆóÊïàÁéáÁöÑÊñ∞ÊñπÊ≥ï', 'desc': '‰º†ÁªüÁöÑTransformerÊ®°ÂûãÂØπÊØè‰∏™ËæìÂÖ•Ê†áËÆ∞ÂàÜÈÖçÂõ∫ÂÆöÁöÑËÆ°ÁÆóËµÑÊ∫êÔºåÂØºËá¥ËÆ°ÁÆóÊïàÁéá‰Ωé‰∏ã„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÂºïÂÖ•‰∫ÜÊ∑±Â∫¶Ê∑∑ÂêàÔºàMoDÔºâÊñπÊ≥ïÔºåÈÄöËøáË∑≥Ëøá‰∏çÈáçË¶ÅÁöÑÂ±ÇÊù•Âä®ÊÄÅË∞ÉÊï¥ËÆ°ÁÆóÊ∑±Â∫¶„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÁöÑMoDÊñπÊ≥ïÈù¢‰∏¥È´òËÆ≠ÁªÉÊàêÊú¨ÂíåÊÄßËÉΩ‰∏ãÈôçÁöÑÈ£éÈô©„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜRouter-TuningÂíåMindSkipÊñπÊ≥ïÔºåÂàÜÂà´ÈÄöËøáÂæÆË∞ÉË∑ØÁî±Âô®Âíå‰ΩøÁî®Âä®ÊÄÅÊ∑±Â∫¶Ê≥®ÊÑèÂäõÊù•Ëß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊòæËëóÊèêÈ´ò‰∫ÜËÆ°ÁÆóÊïàÁéá„ÄÇ'}}, 'pub_date_card': {'ru': '17 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 17', 'zh': '10Êúà17Êó•'}}, {'id': 'https://huggingface.co/papers/2410.15460', 'title': 'Hallucination Detox: Sensitive Neuron Dropout (SeND) for Large Language Model Training', 'url': 'https://huggingface.co/papers/2410.15460', 'abstract': 'As large language models (LLMs) become increasingly deployed across various industries, concerns regarding their reliability, particularly due to hallucinations-outputs that are factually inaccurate or irrelevant to user input-have grown. Our research investigates the relationship between the training process and the emergence of hallucinations to address a key gap in existing research that focuses primarily on post hoc detection and mitigation strategies. Using models from the Pythia suite (70M-12B parameters) and several hallucination detection metrics, we analyze hallucination trends throughout training and explore LLM internal dynamics. We introduce SEnsitive Neuron Dropout (SeND), a novel training protocol designed to mitigate hallucinations by reducing variance during training. SeND achieves this by deterministically dropping neurons with significant variability on a dataset, referred to as Sensitive Neurons. In addition, we develop an unsupervised hallucination detection metric, Efficient EigenScore (EES), which approximates the traditional EigenScore in 2x speed. This efficient metric is integrated into our protocol, allowing SeND to be both computationally scalable and effective at reducing hallucinations. Our empirical evaluation demonstrates that our approach improves LLM reliability at test time by up to 40% compared to normal training while also providing an efficient method to improve factual accuracy when adapting LLMs to domains such as Wikipedia and Medical datasets.', 'score': 1, 'issue_id': 214, 'pub_date': '2024-10-20', 'pub_date_ru': '20 –æ–∫—Ç—è–±—Ä—è', 'hash': '5c4104bbacb9ac91', 'data': {'categories': ['#hallucinations', '#interpretability', '#medicine', '#training'], 'emoji': 'üß†', 'ru': {'title': '–ë–æ—Ä—å–±–∞ —Å –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏—è–º–∏ –≤ LLM —á–µ—Ä–µ–∑ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–º–∏ –Ω–µ–π—Ä–æ–Ω–∞–º–∏', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–∑—É—á–∞–µ—Ç —Å–≤—è–∑—å –º–µ–∂–¥—É –ø—Ä–æ—Ü–µ—Å—Å–æ–º –æ–±—É—á–µ–Ω–∏—è –∏ –≤–æ–∑–Ω–∏–∫–Ω–æ–≤–µ–Ω–∏–µ–º –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM). –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç —Ç—Ä–µ–Ω–¥—ã –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –Ω–∞ –ø—Ä–æ—Ç—è–∂–µ–Ω–∏–∏ –æ–±—É—á–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É—è –º–æ–¥–µ–ª–∏ –∏–∑ –Ω–∞–±–æ—Ä–∞ Pythia –∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–µ—Ç—Ä–∏–∫ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π. –û–Ω–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –Ω–æ–≤—ã–π –ø—Ä–æ—Ç–æ–∫–æ–ª –æ–±—É—á–µ–Ω–∏—è SeND, –∫–æ—Ç–æ—Ä—ã–π —Å–Ω–∏–∂–∞–µ—Ç –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –ø—É—Ç–µ–º –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∏—Å–∫–ª—é—á–µ–Ω–∏—è —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö –Ω–µ–π—Ä–æ–Ω–æ–≤. –¢–∞–∫–∂–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–∞ –Ω–µ—Å—É–ø–µ—Ä–≤–∏–∑–æ—Ä–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π EES, –∫–æ—Ç–æ—Ä–∞—è –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–∞ –≤ –ø—Ä–æ—Ç–æ–∫–æ–ª –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –µ–≥–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç–∏.'}, 'en': {'title': '"Training Smarter: Reducing Hallucinations in Language Models"', 'desc': 'The paper explores how large language models (LLMs) sometimes produce incorrect or irrelevant outputs, known as hallucinations, and how these can be reduced during the training process. The researchers introduce a new training method called Sensitive Neuron Dropout (SeND), which helps decrease hallucinations by selectively dropping neurons that show high variability. They also develop a fast, unsupervised metric called Efficient EigenScore (EES) to detect hallucinations more quickly. Their approach shows a significant improvement in the reliability and factual accuracy of LLMs, especially when applied to specific domains like Wikipedia and medical datasets.'}, 'zh': {'title': 'ÂáèÂ∞ëÂπªËßâÔºåÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÂèØÈù†ÊÄß', 'desc': 'ÈöèÁùÄÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®ÂêÑË°å‰∏öÁöÑÂπøÊ≥õÂ∫îÁî®Ôºå‰∫∫‰ª¨ÂØπÂÖ∂ÂèØÈù†ÊÄßÔºåÂ∞§ÂÖ∂ÊòØÂπªËßâÁé∞Ë±°ÁöÑÊãÖÂøßÊó•ÁõäÂ¢ûÂä†„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Êé¢ËÆ®‰∫ÜËÆ≠ÁªÉËøáÁ®ã‰∏éÂπªËßâÁé∞Ë±°‰πãÈó¥ÁöÑÂÖ≥Á≥ªÔºåÂ°´Ë°•‰∫ÜÁé∞ÊúâÁ†îÁ©∂‰∏ªË¶ÅÈõÜ‰∏≠‰∫é‰∫ãÂêéÊ£ÄÊµãÂíåÁºìËß£Á≠ñÁï•ÁöÑÁ©∫ÁôΩ„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËÆ≠ÁªÉÂçèËÆÆÔºåÁß∞‰∏∫ÊïèÊÑüÁ•ûÁªèÂÖÉ‰∏¢ÂºÉÔºàSeNDÔºâÔºåÈÄöËøáÂú®ËÆ≠ÁªÉ‰∏≠ÊúâÈÄâÊã©Âú∞‰∏¢ÂºÉÂèòÂºÇÊÄßÂ§ßÁöÑÁ•ûÁªèÂÖÉÊù•ÂáèÂ∞ëÂπªËßâ„ÄÇÊàë‰ª¨ËøòÂºÄÂèë‰∫Ü‰∏ÄÁßçÊó†ÁõëÁù£ÁöÑÂπªËßâÊ£ÄÊµãÊåáÊ†áÔºåÁß∞‰∏∫È´òÊïàÁâπÂæÅÂÄºÂàÜÊï∞ÔºàEESÔºâÔºå‰ΩøÂæóSeNDÂú®ÂáèÂ∞ëÂπªËßâÁöÑÂêåÊó∂ÂÖ∑ÊúâËÆ°ÁÆóÂèØÊâ©Â±ïÊÄß„ÄÇ'}}, 'pub_date_card': {'ru': '20 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 20', 'zh': '10Êúà20Êó•'}}, {'id': 'https://huggingface.co/papers/2410.13394', 'title': 'Cross-Lingual Auto Evaluation for Assessing Multilingual LLMs', 'url': 'https://huggingface.co/papers/2410.13394', 'abstract': 'Evaluating machine-generated text remains a significant challenge in NLP, especially for non-English languages. Current methodologies, including automated metrics, human assessments, and LLM-based evaluations, predominantly focus on English, revealing a significant gap in multilingual evaluation frameworks. We introduce the Cross Lingual Auto Evaluation (CIA) Suite, an extensible framework that includes evaluator LLMs (Hercule) and a novel test set (Recon) specifically designed for multilingual evaluation. Our test set features 500 human-annotated instructions spanning various task capabilities along with human judgment scores across six languages. This would enable benchmarking of general-purpose multilingual LLMs and facilitate meta-evaluation of Evaluator LLMs. The proposed model, Hercule, is a cross-lingual evaluation model that addresses the scarcity of reference answers in the target language by learning to assign scores to responses based on easily available reference answers in English. Our experiments demonstrate that Hercule aligns more closely with human judgments compared to proprietary models, demonstrating the effectiveness of such cross-lingual evaluation in low resource scenarios. Further, it is also effective in zero-shot evaluation on unseen languages. This study is the first comprehensive examination of cross-lingual evaluation using LLMs, presenting a scalable and effective approach for multilingual assessment. All code, datasets, and models will be publicly available to enable further research in this important area.', 'score': 1, 'issue_id': 208, 'pub_date': '2024-10-17', 'pub_date_ru': '17 –æ–∫—Ç—è–±—Ä—è', 'hash': 'd456f53989a80b51', 'data': {'categories': ['#benchmark', '#multilingual', '#translation'], 'emoji': 'üåê', 'ru': {'title': '–ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –±–∞—Ä—å–µ—Ä–æ–≤ –≤ –æ—Ü–µ–Ω–∫–µ –ò–ò', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ CIA Suite –¥–ª—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è –º–æ–¥–µ–ª—å-–æ—Ü–µ–Ω—â–∏–∫ Hercule –∏ —Ç–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä Recon —Å 500 –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏ –Ω–∞ —à–µ—Å—Ç–∏ —è–∑—ã–∫–∞—Ö. Hercule —Å–ø–æ—Å–æ–±–Ω–∞ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å –æ—Ç–≤–µ—Ç—ã –Ω–∞ —Ü–µ–ª–µ–≤–æ–º —è–∑—ã–∫–µ, –∏—Å–ø–æ–ª—å–∑—É—è —ç—Ç–∞–ª–æ–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º, —á—Ç–æ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –Ω–µ—Ö–≤–∞—Ç–∫–∏ —Ä–µ—Å—É—Ä—Å–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ Hercule –ª—É—á—à–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –æ—Ü–µ–Ω–∫–∞–º –ª—é–¥–µ–π, —á–µ–º –ø—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω—ã–µ –º–æ–¥–µ–ª–∏, –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞ –¥–∞–∂–µ –¥–ª—è —Ä–∞–Ω–µ–µ –Ω–µ –≤–∏–¥–µ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤.'}, 'en': {'title': 'Breaking Language Barriers in NLP Evaluation', 'desc': "The paper addresses the challenge of evaluating machine-generated text in multiple languages, which is often overlooked in favor of English. It introduces the Cross Lingual Auto Evaluation (CIA) Suite, featuring the Hercule model and a novel test set called Recon, designed for multilingual evaluation. Hercule is a cross-lingual evaluation model that uses English reference answers to score responses in other languages, aligning closely with human judgments. The study demonstrates Hercule's effectiveness in low-resource and zero-shot scenarios, marking a significant step forward in multilingual NLP evaluation."}, 'zh': {'title': 'Ë∑®ËØ≠Ë®ÄËØÑ‰º∞ÁöÑÊñ∞Á™ÅÁ†¥ÔºöHerculeÊ®°ÂûãÁöÑÂ§öËØ≠Ë®ÄËØÑ‰º∞', 'desc': 'ËøôÁØáËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂú®Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÈ¢ÜÂüü‰∏≠ÔºåËØÑ‰º∞Êú∫Âô®ÁîüÊàêÊñáÊú¨ÁöÑÊåëÊàòÔºåÂ∞§ÂÖ∂ÊòØÂú®ÈùûËã±ËØ≠ËØ≠Ë®Ä‰∏≠„ÄÇ‰ΩúËÄÖÊèêÂá∫‰∫Ü‰∏Ä‰∏™Âêç‰∏∫Ë∑®ËØ≠Ë®ÄËá™Âä®ËØÑ‰º∞Â•ó‰ª∂ÔºàCIAÔºâÁöÑÊ°ÜÊû∂ÔºåÂÖ∂‰∏≠ÂåÖÊã¨‰∏Ä‰∏™Êñ∞ÁöÑËØÑ‰º∞Ê®°ÂûãHerculeÂíå‰∏Ä‰∏™Â§öËØ≠Ë®ÄÊµãËØïÈõÜRecon„ÄÇHerculeÊ®°ÂûãÈÄöËøáÂ≠¶‰π†‰ªéËã±ËØ≠ÂèÇËÄÉÁ≠îÊ°à‰∏≠ËØÑÂàÜÔºåËß£ÂÜ≥‰∫ÜÁõÆÊ†áËØ≠Ë®Ä‰∏≠Áº∫‰πèÂèÇËÄÉÁ≠îÊ°àÁöÑÈóÆÈ¢ò„ÄÇÂÆûÈ™åË°®ÊòéÔºåHerculeÂú®‰ΩéËµÑÊ∫êÂú∫ÊôØ‰∏≠‰∏é‰∫∫Á±ªÂà§Êñ≠Êõ¥‰∏∫‰∏ÄËá¥ÔºåÂπ∂‰∏îÂú®Êú™ËßÅËøáÁöÑËØ≠Ë®Ä‰∏ä‰πüËÉΩËøõË°åÈõ∂Ê†∑Êú¨ËØÑ‰º∞„ÄÇ'}}, 'pub_date_card': {'ru': '17 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 17', 'zh': '10Êúà17Êó•'}}, {'id': 'https://huggingface.co/papers/2410.15017', 'title': 'DM-Codec: Distilling Multimodal Representations for Speech Tokenization', 'url': 'https://huggingface.co/papers/2410.15017', 'abstract': 'Recent advancements in speech-language models have yielded significant improvements in speech tokenization and synthesis. However, effectively mapping the complex, multidimensional attributes of speech into discrete tokens remains challenging. This process demands acoustic, semantic, and contextual information for precise speech representations. Existing speech representations generally fall into two categories: acoustic tokens from audio codecs and semantic tokens from speech self-supervised learning models. Although recent efforts have unified acoustic and semantic tokens for improved performance, they overlook the crucial role of contextual representation in comprehensive speech modeling. Our empirical investigations reveal that the absence of contextual representations results in elevated Word Error Rate (WER) and Word Information Lost (WIL) scores in speech transcriptions. To address these limitations, we propose two novel distillation approaches: (1) a language model (LM)-guided distillation method that incorporates contextual information, and (2) a combined LM and self-supervised speech model (SM)-guided distillation technique that effectively distills multimodal representations (acoustic, semantic, and contextual) into a comprehensive speech tokenizer, termed DM-Codec. The DM-Codec architecture adopts a streamlined encoder-decoder framework with a Residual Vector Quantizer (RVQ) and incorporates the LM and SM during the training process. Experiments show DM-Codec significantly outperforms state-of-the-art speech tokenization models, reducing WER by up to 13.46%, WIL by 9.82%, and improving speech quality by 5.84% and intelligibility by 1.85% on the LibriSpeech benchmark dataset. The code, samples, and model checkpoints are available at https://github.com/mubtasimahasan/DM-Codec.', 'score': 1, 'issue_id': 208, 'pub_date': '2024-10-19', 'pub_date_ru': '19 –æ–∫—Ç—è–±—Ä—è', 'hash': 'd2c6c349adfb4796', 'data': {'categories': ['#audio', '#benchmark', '#multimodal'], 'emoji': 'üó£Ô∏è', 'ru': {'title': 'DM-Codec: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ —Ä–µ—á–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ —Ä–µ—á–∏, –Ω–∞–∑–≤–∞–Ω–Ω—ã–π DM-Codec. –û–Ω –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –∞–∫—É—Å—Ç–∏—á–µ—Å–∫—É—é, —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã—Ö —Ä–µ—á–µ–≤—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –¥–≤–∞ –º–µ—Ç–æ–¥–∞ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏: —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –∏ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–µ—Ç–æ–¥ —Å —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª—å—é –∏ —Å–∞–º–æ–æ–±—É—á–∞—é—â–µ–π—Å—è —Ä–µ—á–µ–≤–æ–π –º–æ–¥–µ–ª—å—é. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ DM-Codec –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ —Ä–µ—á–∏, —Å–Ω–∏–∂–∞—è –æ—à–∏–±–∫–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –∏ —É–ª—É—á—à–∞—è –∫–∞—á–µ—Å—Ç–≤–æ –∏ —Ä–∞–∑–±–æ—Ä—á–∏–≤–æ—Å—Ç—å —Ä–µ—á–∏.'}, 'en': {'title': '"DM-Codec: Revolutionizing Speech Tokenization with Contextual Intelligence"', 'desc': 'This paper addresses the challenge of effectively mapping speech into discrete tokens by incorporating acoustic, semantic, and contextual information. The authors propose two novel distillation methods that integrate language models and self-supervised speech models to create a comprehensive speech tokenizer called DM-Codec. The DM-Codec uses a streamlined encoder-decoder framework with a Residual Vector Quantizer to improve speech tokenization. Experiments demonstrate that DM-Codec significantly reduces Word Error Rate and Word Information Lost, enhancing speech quality and intelligibility.'}, 'zh': {'title': 'DM-CodecÔºöËûçÂêàÂ§öÊ®°ÊÄÅ‰ø°ÊÅØÁöÑËØ≠Èü≥Ê†áËÆ∞ÂåñÊñ∞Á™ÅÁ†¥', 'desc': 'ËøôÁØáËÆ∫ÊñáÊé¢ËÆ®‰∫ÜËØ≠Èü≥Ê®°Âûã‰∏≠ËØ≠Èü≥Ê†áËÆ∞ÂåñÂíåÂêàÊàêÁöÑÊúÄÊñ∞ËøõÂ±ïÔºåÂº∫Ë∞É‰∫ÜÂ∞ÜÂ§çÊùÇÁöÑËØ≠Èü≥Â±ûÊÄßÊò†Â∞Ñ‰∏∫Á¶ªÊï£Ê†áËÆ∞ÁöÑÊåëÊàò„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÁº∫‰πè‰∏ä‰∏ãÊñáË°®Á§∫‰ºöÂØºËá¥ËØ≠Èü≥ËΩ¨ÂΩï‰∏≠ÁöÑËØçÈîôËØØÁéáÂíåËØç‰ø°ÊÅØ‰∏¢Â§±ÁéáÂçáÈ´ò„ÄÇ‰∏∫Ëß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºå‰ΩúËÄÖÊèêÂá∫‰∫Ü‰∏§ÁßçÊñ∞ÁöÑËí∏È¶èÊñπÊ≥ïÔºåÁªìÂêàËØ≠Ë®ÄÊ®°ÂûãÂíåËá™ÁõëÁù£ËØ≠Èü≥Ê®°ÂûãÊù•ÊîπËøõËØ≠Èü≥Ê†áËÆ∞Âô®„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊâÄÊèêÂá∫ÁöÑDM-CodecÊ®°ÂûãÂú®ËØ≠Èü≥Ê†áËÆ∞ÂåñÊÄßËÉΩ‰∏äÊòæËëó‰ºò‰∫éÁé∞ÊúâÊ®°Âûã„ÄÇ'}}, 'pub_date_card': {'ru': '19 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 19', 'zh': '10Êúà19Êó•'}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (1)', '#agi', '#alignment (3)', '#architecture (2)', '#audio (2)', '#benchmark (8)', '#cv (5)', '#data (5)', '#dataset (6)', '#diffusion', '#edge_computing', '#ethics (1)', '#games', '#graphs', '#hallucinations (1)', '#inference', '#interpretability (2)', '#long_context (1)', '#math (2)', '#medicine (2)', '#multilingual (3)', '#multimodal (5)', '#optimization (2)', '#plp', '#rag (1)', '#reasoning', '#rl (1)', '#rlhf (2)', '#robotics', '#security', '#story_generation', '#survey', '#synthetic (2)', '#training (4)', '#transfer_learning', '#translation (1)', '#video (1)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `üè∑Ô∏è ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `üè∑Ô∏è ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].join(" ");
                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="background-digit">${index + 1}</div>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <h2>${item['data']['emoji']} ${item['title']}</h2>
                            <p class="meta"><svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path transform="translate(0, 2)" fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> ${item['score']}. ${title}</p>
                            <p class="pub-date">üìù ${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>
                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>
                            <p class="tags">${cats}</p>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'üîÑ ' + getTimeDiff('2024-10-22 20:13',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "—Ä–µ–π—Ç–∏–Ω–≥—É",
                    pub_date: "–¥–∞—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏",
                    issue_id: "–¥–æ–±–∞–≤–ª–µ–Ω–∏—é –Ω–∞ HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "ËØÑÂàÜ",
                    pub_date: "ÂèëÂ∏ÉÊó•Êúü",
                    issue_id: "HF‰∏ä‰º†Êó•Êúü"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];   
            topMonth.innerHTML = topMonthLabel[currentLang];
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-10-22 20:13')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-10-22 20:13')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    