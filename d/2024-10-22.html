
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 23 papers. October 22.</title>
    <link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #0989eacf;
            --secondary-color: #fff;
            --background-color: #f5f5f5;
            --text-color: #333333;
            --header-color: #0989eacf;
            --body-color: #f5f5f5;
            --menu-color: #002370;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 2em;
            padding: 10px 0 20px 0;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.tags {
            color: #555;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);
            transition: background-color 0.2s ease;
            display: flex;
            flex-direction: column;
            position: relative;
        }
        .article-content {
            padding: 1.5em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
        }
        body.dark-theme>div>main>article {
            background-color: #444;
        }
        body.light-theme>div>main>article {
            background-color: #fff;
        }
        body.dark-theme>div>main>article:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
        }
        .pub-date {
            font-size: 0.9em;
            margin-bottom: 0.8em;
            font-weight: 300;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 1em;
            position: absolute;
            bottom: 10px;
            font-weight: 300;
            font-family: 'Roboto Slab';
        }
        .background-digit {
            position: absolute;
            bottom: -20px;
            right: -10px;
            font-size: 12em;
            font-weight: bold;
            color: rgba(0, 0, 0, 0.03);
            z-index: 0;
            line-height: 1;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 80px;
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        a:hover {
            color: #e73838;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            padding: 1em 0;
            margin-top: 2em;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: fixed;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
        }
        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
        }
        .update-info-container {
            flex: 1;
        }
        .sort-container {
            flex: 2;
        }
        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
                display: block;
                margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .category-toggle {
            display: inline-block;
            margin-bottom: 10px;
            margin-top: 15px;
            cursor: pointer;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 0px;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }
        
        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        body.light-theme>div>main>article.xb3e061a400165087 { background: url("https://hfday.ru/img/20241021/b3e061a400165087.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.xb3e061a400165087:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.xb3e061a400165087 { background: url("https://hfday.ru/img/20241021/b3e061a400165087.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.xb3e061a400165087:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x3b5265f629378d5d { background: url("https://hfday.ru/img/20241021/3b5265f629378d5d.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x3b5265f629378d5d:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x3b5265f629378d5d { background: url("https://hfday.ru/img/20241021/3b5265f629378d5d.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x3b5265f629378d5d:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x4daeb1e76417273d { background: url("https://hfday.ru/img/20241021/4daeb1e76417273d.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x4daeb1e76417273d:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x4daeb1e76417273d { background: url("https://hfday.ru/img/20241021/4daeb1e76417273d.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x4daeb1e76417273d:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x31995f0502d70f2f { background: url("https://hfday.ru/img/20241017/31995f0502d70f2f.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x31995f0502d70f2f:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x31995f0502d70f2f { background: url("https://hfday.ru/img/20241017/31995f0502d70f2f.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x31995f0502d70f2f:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x62b6ded4f9dc1d27 { background: url("https://hfday.ru/img/20241021/62b6ded4f9dc1d27.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x62b6ded4f9dc1d27:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x62b6ded4f9dc1d27 { background: url("https://hfday.ru/img/20241021/62b6ded4f9dc1d27.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x62b6ded4f9dc1d27:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x863b95d8e45c3aa2 { background: url("https://hfday.ru/img/20241017/863b95d8e45c3aa2.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x863b95d8e45c3aa2:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x863b95d8e45c3aa2 { background: url("https://hfday.ru/img/20241017/863b95d8e45c3aa2.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x863b95d8e45c3aa2:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.xe1222b08a95a2911 { background: url("https://hfday.ru/img/20241019/e1222b08a95a2911.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.xe1222b08a95a2911:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.xe1222b08a95a2911 { background: url("https://hfday.ru/img/20241019/e1222b08a95a2911.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.xe1222b08a95a2911:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x023f2f5e4c6c7a7c { background: url("https://hfday.ru/img/20241021/023f2f5e4c6c7a7c.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x023f2f5e4c6c7a7c:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x023f2f5e4c6c7a7c { background: url("https://hfday.ru/img/20241021/023f2f5e4c6c7a7c.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x023f2f5e4c6c7a7c:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x6fff842f967d7207 { background: url("https://hfday.ru/img/20241021/6fff842f967d7207.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x6fff842f967d7207:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x6fff842f967d7207 { background: url("https://hfday.ru/img/20241021/6fff842f967d7207.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x6fff842f967d7207:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x0fbcf072cc98c553 { background: url("https://hfday.ru/img/20241016/0fbcf072cc98c553.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x0fbcf072cc98c553:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x0fbcf072cc98c553 { background: url("https://hfday.ru/img/20241016/0fbcf072cc98c553.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x0fbcf072cc98c553:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x0d2e73e11d2b2b3f { background: url("https://hfday.ru/img/20241021/0d2e73e11d2b2b3f.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x0d2e73e11d2b2b3f:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x0d2e73e11d2b2b3f { background: url("https://hfday.ru/img/20241021/0d2e73e11d2b2b3f.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x0d2e73e11d2b2b3f:hover { background-color: rgba(60,60,60,0.92) !important;}

        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .sort-container {
                margin-top: 0px;
                text-align: left;
                width: 100%;
            .sort-dropdown {
                float: right;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">22 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ</span> | <span id="title-articles-count">23 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item" id="nav-prev"><a href="/d/2024-10-21.html">â¬…ï¸ <span id="prev-date">21.10</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-10-23.html">â¡ï¸ <span id="next-date">23.10</span></a></span>
            <!--<span class="nav-item" id="nav-weekly">Ğ¢Ğ¾Ğ¿ Ğ·Ğ° Ğ½ĞµĞ´ĞµĞ»Ñ</span>
            <span class="nav-item" id="nav-weekly">Ğ¢Ğ¾Ğ¿ Ğ·Ğ° Ğ¼ĞµÑÑÑ†</span>-->
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="category-toggle">
            <div class="svg-container">
                <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                <svg height="3" width="200">
                    <line x1="0" y1="0" x2="200" y2="0" 
                        stroke="black" 
                        stroke-width="2" 
                        stroke-dasharray="3, 3" />
                </svg>
            </div>
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '22 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 22', 'zh': '10æœˆ22æ—¥'};
        let feedDateNext = {'ru': '23.10', 'en': '10/23', 'zh': '10æœˆ23æ—¥'};
        let feedDatePrev = {'ru': '21.10', 'en': '10/21', 'zh': '10æœˆ21æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'Published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2410.16271', 'title': 'FrugalNeRF: Fast Convergence for Few-shot Novel View Synthesis without Learned Priors', 'url': 'https://huggingface.co/papers/2410.16271', 'abstract': 'Neural Radiance Fields (NeRF) face significant challenges in few-shot scenarios, primarily due to overfitting and long training times for high-fidelity rendering. Existing methods, such as FreeNeRF and SparseNeRF, use frequency regularization or pre-trained priors but struggle with complex scheduling and bias. We introduce FrugalNeRF, a novel few-shot NeRF framework that leverages weight-sharing voxels across multiple scales to efficiently represent scene details. Our key contribution is a cross-scale geometric adaptation scheme that selects pseudo ground truth depth based on reprojection errors across scales. This guides training without relying on externally learned priors, enabling full utilization of the training data. It can also integrate pre-trained priors, enhancing quality without slowing convergence. Experiments on LLFF, DTU, and RealEstate-10K show that FrugalNeRF outperforms other few-shot NeRF methods while significantly reducing training time, making it a practical solution for efficient and accurate 3D scene reconstruction.', 'score': 53, 'issue_id': 208, 'pub_date': '2024-10-21', 'pub_date_ru': '21 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'hash': 'b3e061a400165087', 'data': {'categories': ['#3d', '#cv'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼ÑƒĞ¼Ğ¾Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'FrugalNeRF - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ĞµĞ¹ Ğ¸Ğ·Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ (NeRF) Ğ½Ğ° Ğ¼Ğ°Ğ»Ğ¾Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²ĞµÑĞ¾Ğ² Ğ²Ğ¾ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°Ñ… Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ ÑÑ†ĞµĞ½Ñ‹. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒÑ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ÑÑ…ĞµĞ¼Ğ° Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°Ğ¼Ğ¸, Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ÑÑ‰Ğ°Ñ Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½ÑƒÑ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ñ€ĞµĞ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ğ¸. FrugalNeRF Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ few-shot NeRF Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°Ñ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'FrugalNeRF: Efficient 3D Scene Reconstruction with Cross-Scale Adaptation', 'desc': 'The paper introduces FrugalNeRF, a new approach to improve Neural Radiance Fields (NeRF) in few-shot scenarios by using weight-sharing voxels across multiple scales. This method addresses the challenges of overfitting and long training times by employing a cross-scale geometric adaptation scheme that selects pseudo ground truth depth based on reprojection errors. Unlike previous methods, FrugalNeRF does not rely on externally learned priors, allowing for full utilization of the training data while still being able to integrate pre-trained priors for enhanced quality. Experiments demonstrate that FrugalNeRF outperforms existing few-shot NeRF methods in terms of efficiency and accuracy, making it a practical solution for 3D scene reconstruction.'}, 'zh': {'title': 'FrugalNeRFï¼šé«˜æ•ˆå°‘æ ·æœ¬3Dé‡å»ºæ–°æ–¹æ¡ˆ', 'desc': 'NeRFåœ¨å°‘æ ·æœ¬åœºæ™¯ä¸­é¢ä¸´è¿‡æ‹Ÿåˆå’Œè®­ç»ƒæ—¶é—´é•¿çš„é—®é¢˜ã€‚FrugalNeRFé€šè¿‡è·¨å°ºåº¦å‡ ä½•é€‚åº”æ–¹æ¡ˆï¼Œåˆ©ç”¨é‡æŠ•å½±è¯¯å·®é€‰æ‹©ä¼ªçœŸå®æ·±åº¦ï¼Œé¿å…ä¾èµ–å¤–éƒ¨å­¦ä¹ å…ˆéªŒã€‚è¯¥æ–¹æ³•æœ‰æ•ˆåˆ©ç”¨è®­ç»ƒæ•°æ®ï¼Œå¹¶å¯ç»“åˆé¢„è®­ç»ƒå…ˆéªŒï¼Œæé«˜è´¨é‡ä¸”ä¸å½±å“æ”¶æ•›é€Ÿåº¦ã€‚å®éªŒè¡¨æ˜ï¼ŒFrugalNeRFåœ¨å‡å°‘è®­ç»ƒæ—¶é—´çš„åŒæ—¶ï¼Œæå‡äº†3Dåœºæ™¯é‡å»ºçš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚'}}, 'pub_date_card': {'ru': '21 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 21', 'zh': '10æœˆ21æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.16256', 'title': 'CompassJudger-1: All-in-one Judge Model Helps Model Evaluation and Evolution', 'url': 'https://huggingface.co/papers/2410.16256', 'abstract': 'Efficient and accurate evaluation is crucial for the continuous improvement of large language models (LLMs). Among various assessment methods, subjective evaluation has garnered significant attention due to its superior alignment with real-world usage scenarios and human preferences. However, human-based evaluations are costly and lack reproducibility, making precise automated evaluators (judgers) vital in this process. In this report, we introduce CompassJudger-1, the first open-source all-in-one judge LLM. CompassJudger-1 is a general-purpose LLM that demonstrates remarkable versatility. It is capable of: 1. Performing unitary scoring and two-model comparisons as a reward model; 2. Conducting evaluations according to specified formats; 3. Generating critiques; 4. Executing diverse tasks like a general LLM. To assess the evaluation capabilities of different judge models under a unified setting, we have also established JudgerBench, a new benchmark that encompasses various subjective evaluation tasks and covers a wide range of topics. CompassJudger-1 offers a comprehensive solution for various evaluation tasks while maintaining the flexibility to adapt to diverse requirements. Both CompassJudger and JudgerBench are released and available to the research community athttps://github.com/open-compass/CompassJudger. We believe that by open-sourcing these tools, we can foster collaboration and accelerate progress in LLM evaluation methodologies.', 'score': 51, 'issue_id': 208, 'pub_date': '2024-10-21', 'pub_date_ru': '21 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'hash': '3b5265f629378d5d', 'data': {'categories': ['#alignment', '#benchmark'], 'emoji': 'âš–ï¸', 'ru': {'title': 'CompassJudger-1: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑÑƒĞ´ÑŒÑ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ CompassJudger-1 - Ğ¿ĞµÑ€Ğ²Ğ°Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸Ğº Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). Ğ­Ñ‚Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ ĞºĞ°Ğº Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ°Ñ LLM. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº JudgerBench Ğ´Ğ»Ñ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹-Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸ĞºĞ¾Ğ². CompassJudger-1 Ğ¸ JudgerBench Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ñƒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞ³Ğ¾ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞ¸ LLM.'}, 'en': {'title': '"CompassJudger-1: Revolutionizing LLM Evaluation with Open-Source Precision"', 'desc': 'The paper introduces CompassJudger-1, an open-source large language model designed to automate the evaluation of other language models, reducing the need for costly human-based assessments. CompassJudger-1 can perform tasks such as scoring, model comparison, and critique generation, making it a versatile tool for various evaluation scenarios. To test the effectiveness of different judge models, the authors have also developed JudgerBench, a benchmark that includes a wide range of subjective evaluation tasks. By releasing these tools to the public, the authors aim to encourage collaboration and speed up advancements in evaluating language models.'}, 'zh': {'title': 'CompassJudger-1ï¼šå¤§è¯­è¨€æ¨¡å‹è¯„ä¼°çš„å…¨èƒ½å·¥å…·', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºCompassJudger-1çš„å¼€æºè¯„ä¼°å·¥å…·ï¼Œå®ƒæ˜¯ä¸€ä¸ªé€šç”¨çš„å¤§è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿè¿›è¡Œå•ä¸€è¯„åˆ†ã€åŒæ¨¡å‹æ¯”è¾ƒã€ç”Ÿæˆè¯„è®ºç­‰å¤šç§ä»»åŠ¡ã€‚CompassJudger-1çš„è®¾è®¡æ—¨åœ¨è§£å†³äººå·¥è¯„ä¼°æˆæœ¬é«˜ä¸”éš¾ä»¥é‡å¤çš„é—®é¢˜ï¼Œé€šè¿‡è‡ªåŠ¨åŒ–çš„æ–¹å¼æé«˜è¯„ä¼°çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚ä¸ºäº†è¯„ä¼°ä¸åŒè¯„ä¼°æ¨¡å‹çš„èƒ½åŠ›ï¼Œç ”ç©¶è€…è¿˜å»ºç«‹äº†ä¸€ä¸ªåä¸ºJudgerBenchçš„æ–°åŸºå‡†ï¼Œæ¶µç›–äº†å¤šç§ä¸»è§‚è¯„ä¼°ä»»åŠ¡ã€‚é€šè¿‡å¼€æºè¿™äº›å·¥å…·ï¼Œç ”ç©¶è€…å¸Œæœ›ä¿ƒè¿›åˆä½œï¼ŒåŠ é€Ÿå¤§è¯­è¨€æ¨¡å‹è¯„ä¼°æ–¹æ³•çš„å‘å±•ã€‚'}}, 'pub_date_card': {'ru': '21 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 21', 'zh': '10æœˆ21æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.16268', 'title': 'SAM2Long: Enhancing SAM 2 for Long Video Segmentation with a Training-Free Memory Tree', 'url': 'https://huggingface.co/papers/2410.16268', 'abstract': 'The Segment Anything Model 2 (SAM 2) has emerged as a powerful foundation model for object segmentation in both images and videos, paving the way for various downstream video applications. The crucial design of SAM 2 for video segmentation is its memory module, which prompts object-aware memories from previous frames for current frame prediction. However, its greedy-selection memory design suffers from the "error accumulation" problem, where an errored or missed mask will cascade and influence the segmentation of the subsequent frames, which limits the performance of SAM 2 toward complex long-term videos. To this end, we introduce SAM2Long, an improved training-free video object segmentation strategy, which considers the segmentation uncertainty within each frame and chooses the video-level optimal results from multiple segmentation pathways in a constrained tree search manner. In practice, we maintain a fixed number of segmentation pathways throughout the video. For each frame, multiple masks are proposed based on the existing pathways, creating various candidate branches. We then select the same fixed number of branches with higher cumulative scores as the new pathways for the next frame. After processing the final frame, the pathway with the highest cumulative score is chosen as the final segmentation result. Benefiting from its heuristic search design, SAM2Long is robust toward occlusions and object reappearances, and can effectively segment and track objects for complex long-term videos. Notably, SAM2Long achieves an average improvement of 3.0 points across all 24 head-to-head comparisons, with gains of up to 5.3 points in J&F on long-term video object segmentation benchmarks such as SA-V and LVOS. The code is released at https://github.com/Mark12Ding/SAM2Long.', 'score': 46, 'issue_id': 209, 'pub_date': '2024-10-21', 'pub_date_ru': '21 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'hash': '4daeb1e76417273d', 'data': {'categories': ['#benchmark', '#cv', '#video'], 'emoji': 'ğŸ¬', 'ru': {'title': 'SAM2Long: ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ² ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'SAM2Long - ÑÑ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ SAM 2. ĞĞ½Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¿Ñ€Ğ¸ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¿ÑƒÑ‚ĞµĞ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°. SAM2Long Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿ÑƒÑ‚ĞµĞ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ‚ÑĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ²ÑĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ Ğ²ĞµÑ‚Ğ²Ğ¸ Ñ Ğ½Ğ°Ğ¸Ğ²Ñ‹ÑÑˆĞ¸Ğ¼Ğ¸ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ĞºĞ°Ğ´Ñ€Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ 5.3 Ğ¿ÑƒĞ½ĞºÑ‚Ğ¾Ğ² Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ J&F Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ².'}, 'en': {'title': '"SAM2Long: Navigating Complexity in Video Segmentation"', 'desc': 'The Segment Anything Model 2 (SAM 2) is a foundation model designed for object segmentation in images and videos, but it struggles with error accumulation in long-term video segmentation. SAM2Long is introduced as an improved strategy that uses a constrained tree search to select optimal segmentation pathways, addressing the error accumulation issue. By maintaining multiple segmentation pathways and selecting the best candidates based on cumulative scores, SAM2Long effectively handles occlusions and object reappearances. This approach results in significant performance improvements, achieving up to 5.3 points gain in long-term video segmentation benchmarks.'}, 'zh': {'title': 'SAM2Longï¼šæå‡é•¿è§†é¢‘å¯¹è±¡åˆ†å‰²çš„æ–°ç­–ç•¥', 'desc': 'Segment Anything Model 2ï¼ˆSAM 2ï¼‰æ˜¯ä¸€ä¸ªå¼ºå¤§çš„åŸºç¡€æ¨¡å‹ï¼Œç”¨äºå›¾åƒå’Œè§†é¢‘ä¸­çš„å¯¹è±¡åˆ†å‰²ã€‚å®ƒçš„å…³é”®è®¾è®¡æ˜¯è®°å¿†æ¨¡å—ï¼Œå¯ä»¥ä»å‰ä¸€å¸§ä¸­æå–å¯¹è±¡ç›¸å…³çš„è®°å¿†æ¥é¢„æµ‹å½“å‰å¸§ã€‚ç„¶è€Œï¼ŒSAM 2 çš„è´ªå©ªé€‰æ‹©è®°å¿†è®¾è®¡å­˜åœ¨â€œé”™è¯¯ç§¯ç´¯â€é—®é¢˜ï¼Œå½±å“å¤æ‚é•¿è§†é¢‘çš„åˆ†å‰²æ€§èƒ½ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº† SAM2Longï¼Œé€šè¿‡å¤šè·¯å¾„æœç´¢ç­–ç•¥æ¥æé«˜è§†é¢‘å¯¹è±¡åˆ†å‰²çš„å‡†ç¡®æ€§ã€‚'}}, 'pub_date_card': {'ru': '21 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 21', 'zh': '10æœˆ21æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.13861', 'title': 'PUMA: Empowering Unified MLLM with Multi-granular Visual Generation', 'url': 'https://huggingface.co/papers/2410.13861', 'abstract': 'Recent advancements in multimodal foundation models have yielded significant progress in vision-language understanding. Initial attempts have also explored the potential of multimodal large language models (MLLMs) for visual content generation. However, existing works have insufficiently addressed the varying granularity demands of different image generation tasks within a unified MLLM paradigm - from the diversity required in text-to-image generation to the precise controllability needed in image manipulation. In this work, we propose PUMA, emPowering Unified MLLM with Multi-grAnular visual generation. PUMA unifies multi-granular visual features as both inputs and outputs of MLLMs, elegantly addressing the different granularity requirements of various image generation tasks within a unified MLLM framework. Following multimodal pretraining and task-specific instruction tuning, PUMA demonstrates proficiency in a wide range of multimodal tasks. This work represents a significant step towards a truly unified MLLM capable of adapting to the granularity demands of various visual tasks. The code and model will be released in https://github.com/rongyaofang/PUMA.', 'score': 41, 'issue_id': 208, 'pub_date': '2024-10-17', 'pub_date_ru': '17 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'hash': '31995f0502d70f2f', 'data': {'categories': ['#cv', '#multimodal'], 'emoji': 'ğŸ†', 'ru': {'title': 'PUMA: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ MLLM Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ³Ñ€Ğ°Ğ½ÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ PUMA - Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (MLLM) Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. PUMA Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ Ğ³Ñ€Ğ°Ğ½ÑƒĞ»ÑÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğº Ğ´Ğ»Ñ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ»Ñ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… MLLM, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ¼ ÑĞ¿ĞµĞºÑ‚Ñ€Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¿Ğ¾Ğ´ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸. Ğ­Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ ÑˆĞ°Ğ³ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ MLLM, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğº Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼ Ğ³Ñ€Ğ°Ğ½ÑƒĞ»ÑÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡.'}, 'en': {'title': 'PUMA: Unifying Visual Generation with Multi-Granular Precision', 'desc': 'The paper introduces PUMA, a new approach to improve multimodal large language models (MLLMs) for visual content generation. PUMA addresses the challenge of varying granularity in image generation tasks, such as the need for diversity in text-to-image generation and precision in image manipulation. By integrating multi-granular visual features, PUMA enhances the ability of MLLMs to handle different visual tasks within a single framework. This advancement marks a significant step towards creating a unified model that can adapt to diverse visual generation needs.'}, 'zh': {'title': 'PUMAï¼šç»Ÿä¸€å¤šæ¨¡æ€è§†è§‰ç”Ÿæˆçš„æ–°çªç ´', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºPUMAçš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è§†è§‰ç”Ÿæˆèƒ½åŠ›ã€‚PUMAé€šè¿‡ç»Ÿä¸€å¤šå±‚æ¬¡çš„è§†è§‰ç‰¹å¾ï¼Œè§£å†³äº†ä¸åŒå›¾åƒç”Ÿæˆä»»åŠ¡å¯¹ç»†èŠ‚å’Œå¤šæ ·æ€§çš„ä¸åŒéœ€æ±‚ã€‚é€šè¿‡å¤šæ¨¡æ€é¢„è®­ç»ƒå’Œä»»åŠ¡ç‰¹å®šçš„æŒ‡ä»¤è°ƒä¼˜ï¼ŒPUMAåœ¨å¤šç§å¤šæ¨¡æ€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚è¯¥ç ”ç©¶ä¸ºå®ç°èƒ½å¤Ÿé€‚åº”å„ç§è§†è§‰ä»»åŠ¡ç»†èŠ‚éœ€æ±‚çš„ç»Ÿä¸€MLLMè¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚'}}, 'pub_date_card': {'ru': '17 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 17', 'zh': '10æœˆ17æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.15735', 'title': 'AutoTrain: No-code training for state-of-the-art models', 'url': 'https://huggingface.co/papers/2410.15735', 'abstract': 'With the advancements in open-source models, training (or finetuning) models on custom datasets has become a crucial part of developing solutions which are tailored to specific industrial or open-source applications. Yet, there is no single tool which simplifies the process of training across different types of modalities or tasks. We introduce AutoTrain (aka AutoTrain Advanced) -- an open-source, no code tool/library which can be used to train (or finetune) models for different kinds of tasks such as: large language model (LLM) finetuning, text classification/regression, token classification, sequence-to-sequence task, finetuning of sentence transformers, visual language model (VLM) finetuning, image classification/regression and even classification and regression tasks on tabular data. AutoTrain Advanced is an open-source library providing best practices for training models on custom datasets. The library is available at https://github.com/huggingface/autotrain-advanced. AutoTrain can be used in fully local mode or on cloud machines and works with tens of thousands of models shared on Hugging Face Hub and their variations.', 'score': 39, 'issue_id': 209, 'pub_date': '2024-10-21', 'pub_date_ru': '21 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'hash': '62b6ded4f9dc1d27', 'data': {'categories': ['#multimodal', '#training'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'AutoTrain: Ğ£Ğ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ML-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ²ÑĞµÑ…', 'desc': 'AutoTrain Advanced - ÑÑ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ°. ĞĞ½ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸. AutoTrain Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ ĞºĞ°Ğº Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾, Ñ‚Ğ°Ğº Ğ¸ Ğ² Ğ¾Ğ±Ğ»Ğ°ĞºĞµ. Ğ˜Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼ Ñ Ñ‚Ñ‹ÑÑÑ‡Ğ°Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ· Hugging Face Hub.'}, 'en': {'title': 'AutoTrain: Simplifying Model Training Across Modalities', 'desc': 'AutoTrain Advanced is a no-code, open-source tool designed to simplify the process of training and finetuning machine learning models across various tasks and modalities. It supports a wide range of applications, including large language models, text and image classification, and even tabular data tasks. The tool integrates with the Hugging Face Hub, allowing users to leverage a vast collection of pre-trained models. AutoTrain can be used both locally and on cloud platforms, making it versatile for different user needs.'}, 'zh': {'title': 'AutoTrainï¼šæ— ä»£ç æ¨¡å‹è®­ç»ƒçš„é©å‘½', 'desc': 'AutoTrainæ˜¯ä¸€ä¸ªå¼€æºçš„æ— ä»£ç å·¥å…·åº“ï¼Œæ—¨åœ¨ç®€åŒ–ä¸åŒä»»åŠ¡å’Œæ¨¡æ€ä¸‹çš„æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ã€‚å®ƒæ”¯æŒå¤šç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒã€æ–‡æœ¬åˆ†ç±»ã€åºåˆ—åˆ°åºåˆ—ä»»åŠ¡ã€è§†è§‰è¯­è¨€æ¨¡å‹å¾®è°ƒç­‰ã€‚ç”¨æˆ·å¯ä»¥åœ¨æœ¬åœ°æˆ–äº‘ç«¯ä½¿ç”¨AutoTrainï¼Œå¹¶ä¸Hugging Face Hubä¸Šçš„æ•°ä¸‡ä¸ªæ¨¡å‹å…¼å®¹ã€‚è¿™ä¸ªå·¥å…·ä¸ºåœ¨è‡ªå®šä¹‰æ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹æä¾›äº†æœ€ä½³å®è·µã€‚'}}, 'pub_date_card': {'ru': '21 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 21', 'zh': '10æœˆ21æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.14745', 'title': 'SemiEvol: Semi-supervised Fine-tuning for LLM Adaptation', 'url': 'https://huggingface.co/papers/2410.14745', 'abstract': 'Supervised fine-tuning (SFT) is crucial in adapting large language models (LLMs) to a specific domain or task. However, only a limited amount of labeled data is available in practical applications, which poses a severe challenge for SFT in yielding satisfactory results. Therefore, a data-efficient framework that can fully exploit labeled and unlabeled data for LLM fine-tuning is highly anticipated. Towards this end, we introduce a semi-supervised fine-tuning framework named SemiEvol for LLM adaptation from a propagate-and-select manner. For knowledge propagation, SemiEvol adopts a bi-level approach, propagating knowledge from labeled data to unlabeled data through both in-weight and in-context methods. For knowledge selection, SemiEvol incorporates a collaborative learning mechanism, selecting higher-quality pseudo-response samples. We conducted experiments using GPT-4o-mini and Llama-3.1 on seven general or domain-specific datasets, demonstrating significant improvements in model performance on target data. Furthermore, we compared SemiEvol with SFT and self-evolution methods, highlighting its practicality in hybrid data scenarios.', 'score': 35, 'issue_id': 207, 'pub_date': '2024-10-17', 'pub_date_ru': '17 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'hash': '863b95d8e45c3aa2', 'data': {'categories': ['#dataset'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'SemiEvol: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¿Ğ¾Ğ»Ñƒ-ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ»Ñƒ-ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ SemiEvol. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ°Ğº Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ½ĞµÑ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¸Ğ»Ğ¸ Ğ´Ğ¾Ğ¼ĞµĞ½Ñƒ. SemiEvol Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ÑĞµĞ¼Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸.'}, 'en': {'title': 'SemiEvol: Maximizing Model Potential with Minimal Data', 'desc': "The paper introduces SemiEvol, a semi-supervised fine-tuning framework designed to adapt large language models using both labeled and unlabeled data. SemiEvol uses a bi-level approach to propagate knowledge from labeled to unlabeled data, enhancing the model's learning process. It also employs a collaborative learning mechanism to select high-quality pseudo-responses, improving the model's performance. Experiments show that SemiEvol outperforms traditional supervised fine-tuning and self-evolution methods, making it effective for tasks with limited labeled data."}, 'zh': {'title': 'SemiEvolï¼šé«˜æ•ˆåˆ©ç”¨æ ‡æ³¨ä¸æœªæ ‡æ³¨æ•°æ®çš„åŠç›‘ç£å¾®è°ƒæ¡†æ¶', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSemiEvolçš„åŠç›‘ç£å¾®è°ƒæ¡†æ¶ï¼Œç”¨äºå¤§è¯­è¨€æ¨¡å‹çš„é€‚åº”ã€‚SemiEvolé€šè¿‡åŒå±‚æ–¹æ³•å°†æ ‡æ³¨æ•°æ®çš„çŸ¥è¯†ä¼ æ’­åˆ°æœªæ ‡æ³¨æ•°æ®ä¸­ï¼Œå¹¶é€šè¿‡åä½œå­¦ä¹ æœºåˆ¶é€‰æ‹©é«˜è´¨é‡çš„ä¼ªå“åº”æ ·æœ¬ã€‚å®éªŒè¡¨æ˜ï¼ŒSemiEvolåœ¨å¤šç§æ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜äº†æ¨¡å‹æ€§èƒ½ã€‚ä¸ä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒå’Œè‡ªæˆ‘è¿›åŒ–æ–¹æ³•ç›¸æ¯”ï¼ŒSemiEvolåœ¨æ··åˆæ•°æ®åœºæ™¯ä¸­è¡¨ç°å‡ºæ›´é«˜çš„å®ç”¨æ€§ã€‚'}}, 'pub_date_card': {'ru': '17 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 17', 'zh': '10æœˆ17æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.14940', 'title': 'Baichuan Alignment Technical Report', 'url': 'https://huggingface.co/papers/2410.14940', 'abstract': "We introduce Baichuan Alignment, a detailed analysis of the alignment techniques employed in the Baichuan series of models. This represents the industry's first comprehensive account of alignment methodologies, offering valuable insights for advancing AI research. We investigate the critical components that enhance model performance during the alignment process, including optimization methods, data strategies, capability enhancements, and evaluation processes. The process spans three key stages: Prompt Augmentation System (PAS), Supervised Fine-Tuning (SFT), and Preference Alignment. The problems encountered, the solutions applied, and the improvements made are thoroughly recorded.   Through comparisons across well-established benchmarks, we highlight the technological advancements enabled by Baichuan Alignment. Baichuan-Instruct is an internal model, while Qwen2-Nova-72B and Llama3-PBM-Nova-70B are instruct versions of the Qwen2-72B and Llama-3-70B base models, optimized through Baichuan Alignment. Baichuan-Instruct demonstrates significant improvements in core capabilities, with user experience gains ranging from 17% to 28%, and performs exceptionally well on specialized benchmarks. In open-source benchmark evaluations, both Qwen2-Nova-72B and Llama3-PBM-Nova-70B consistently outperform their respective official instruct versions across nearly all datasets. This report aims to clarify the key technologies behind the alignment process, fostering a deeper understanding within the community. Llama3-PBM-Nova-70B model is available at https://huggingface.co/PKU-Baichuan-MLSystemLab/Llama3-PBM-Nova-70B.", 'score': 32, 'issue_id': 208, 'pub_date': '2024-10-19', 'pub_date_ru': '19 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'hash': 'e1222b08a95a2911', 'data': {'categories': ['#alignment', '#benchmark', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ¼ĞµÑ‚Ğ¾Ğ´ Baichuan Alignment', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ (alignment), Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµĞ¼Ñ‹Ñ… Ğ² ÑĞµÑ€Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Baichuan. ĞĞ¿Ğ¸ÑÑ‹Ğ²Ğ°ÑÑ‚ÑÑ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑÑ‚Ğ°Ğ¿Ğ°: ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² (PAS), ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ Ñ‚Ğ¾Ğ½ĞºĞ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° (SFT) Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ÑÑ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑÑ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¸Ñ… Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ Ğ¾Ñ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ²ĞµÑ€ÑĞ¸ÑĞ¼Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ….'}, 'en': {'title': "Aligning for Excellence: Baichuan's Path to Superior AI Models", 'desc': 'Baichuan Alignment is a comprehensive study of alignment techniques used in the Baichuan model series, providing insights to advance AI research. The paper explores key components that improve model performance during alignment, such as optimization methods, data strategies, and evaluation processes. The alignment process is divided into three stages: Prompt Augmentation System, Supervised Fine-Tuning, and Preference Alignment. The study shows that models optimized with Baichuan Alignment, like Baichuan-Instruct, achieve significant performance improvements and outperform other models on various benchmarks.'}, 'zh': {'title': 'Baichuan Alignmentï¼šAI å¯¹é½æŠ€æœ¯çš„å…¨é¢çªç ´', 'desc': 'Baichuan Alignment æ˜¯å¯¹ Baichuan ç³»åˆ—æ¨¡å‹ä¸­å¯¹é½æŠ€æœ¯çš„è¯¦ç»†åˆ†æï¼Œé¦–æ¬¡å…¨é¢è®°å½•äº†å¯¹é½æ–¹æ³•ã€‚ç ”ç©¶ä¸­æ¢è®¨äº†ä¼˜åŒ–æ–¹æ³•ã€æ•°æ®ç­–ç•¥ã€èƒ½åŠ›å¢å¼ºå’Œè¯„ä¼°è¿‡ç¨‹ç­‰å…³é”®ç»„ä»¶ã€‚å¯¹é½è¿‡ç¨‹åˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼šæç¤ºå¢å¼ºç³»ç»Ÿã€ç›‘ç£å¾®è°ƒå’Œåå¥½å¯¹é½ã€‚é€šè¿‡å¯¹æ¯”åŸºå‡†æµ‹è¯•ï¼Œå±•ç¤ºäº† Baichuan Alignment å¸¦æ¥çš„æŠ€æœ¯è¿›æ­¥ã€‚'}}, 'pub_date_card': {'ru': '19 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 19', 'zh': '10æœˆ19æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.16153', 'title': 'Pangea: A Fully Open Multilingual Multimodal LLM for 39 Languages', 'url': 'https://huggingface.co/papers/2410.16153', 'abstract': "Despite recent advances in multimodal large language models (MLLMs), their development has predominantly focused on English- and western-centric datasets and tasks, leaving most of the world's languages and diverse cultural contexts underrepresented. This paper introduces Pangea, a multilingual multimodal LLM trained on PangeaIns, a diverse 6M instruction dataset spanning 39 languages. PangeaIns features: 1) high-quality English instructions, 2) carefully machine-translated instructions, and 3) culturally relevant multimodal tasks to ensure cross-cultural coverage. To rigorously assess models' capabilities, we introduce PangeaBench, a holistic evaluation suite encompassing 14 datasets covering 47 languages. Results show that Pangea significantly outperforms existing open-source models in multilingual settings and diverse cultural contexts. Ablation studies further reveal the importance of English data proportions, language popularity, and the number of multimodal training samples on overall performance. We fully open-source our data, code, and trained checkpoints, to facilitate the development of inclusive and robust multilingual MLLMs, promoting equity and accessibility across a broader linguistic and cultural spectrum.", 'score': 25, 'issue_id': 208, 'pub_date': '2024-10-21', 'pub_date_ru': '21 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'hash': '023f2f5e4c6c7a7c', 'data': {'categories': ['#benchmark', '#data', '#dataset', '#multilingual', '#multimodal'], 'emoji': 'ğŸŒ', 'ru': {'title': 'Pangea: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Pangea - Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ÑĞ·Ñ‹Ñ‡Ğ½ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… PangeaIns, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰ĞµĞ¼ 39 ÑĞ·Ñ‹ĞºĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ PangeaBench - ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° 47 ÑĞ·Ñ‹ĞºĞ°Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Pangea Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ°Ñ… Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¾ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğµ Ğ¸Ğ½ĞºĞ»ÑĞ·Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒÑ Ñ€Ğ°Ğ²ĞµĞ½ÑÑ‚Ğ²Ñƒ Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ¼ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¸ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¼ ÑĞ¿ĞµĞºÑ‚Ñ€Ğµ.'}, 'en': {'title': "Breaking Language Barriers: Pangea's Multilingual Revolution", 'desc': "The paper presents Pangea, a multilingual multimodal large language model designed to address the underrepresentation of non-English languages and diverse cultural contexts in current models. Pangea is trained on a dataset called PangeaIns, which includes high-quality English instructions, machine-translated instructions, and culturally relevant tasks across 39 languages. The model's performance is evaluated using PangeaBench, a comprehensive suite of 14 datasets covering 47 languages, showing superior results compared to existing models. The study highlights the importance of data diversity and open-sources all resources to encourage the development of more inclusive language models."}, 'zh': {'title': 'Pangeaï¼šè·¨è¶Šè¯­è¨€å’Œæ–‡åŒ–çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºPangeaçš„å¤šè¯­è¨€å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œå®ƒä½¿ç”¨äº†ä¸€ä¸ªåŒ…å«39ç§è¯­è¨€çš„å¤šæ ·åŒ–æ•°æ®é›†PangeaInsè¿›è¡Œè®­ç»ƒã€‚PangeaInsçš„æ•°æ®é›†åŒ…æ‹¬é«˜è´¨é‡çš„è‹±è¯­æŒ‡ä»¤ã€ç²¾å¿ƒæœºå™¨ç¿»è¯‘çš„æŒ‡ä»¤ä»¥åŠä¸æ–‡åŒ–ç›¸å…³çš„å¤šæ¨¡æ€ä»»åŠ¡ã€‚ä¸ºäº†è¯„ä¼°æ¨¡å‹çš„èƒ½åŠ›ï¼Œç ”ç©¶è€…ä»¬è¿˜å¼•å…¥äº†PangeaBenchï¼Œä¸€ä¸ªæ¶µç›–47ç§è¯­è¨€çš„è¯„ä¼°å¥—ä»¶ã€‚ç»“æœæ˜¾ç¤ºï¼ŒPangeaåœ¨å¤šè¯­è¨€å’Œå¤šæ–‡åŒ–èƒŒæ™¯ä¸‹çš„è¡¨ç°ä¼˜äºç°æœ‰çš„å¼€æºæ¨¡å‹ã€‚'}}, 'pub_date_card': {'ru': '21 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 21', 'zh': '10æœˆ21æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.16184', 'title': 'RM-Bench: Benchmarking Reward Models of Language Models with Subtlety and Style', 'url': 'https://huggingface.co/papers/2410.16184', 'abstract': 'Reward models are critical in techniques like Reinforcement Learning from Human Feedback (RLHF) and Inference Scaling Laws, where they guide language model alignment and select optimal responses. Despite their importance, existing reward model benchmarks often evaluate models by asking them to distinguish between responses generated by models of varying power. However, this approach fails to assess reward models on subtle but critical content changes and variations in style, resulting in a low correlation with policy model performance. To this end, we introduce RM-Bench, a novel benchmark designed to evaluate reward models based on their sensitivity to subtle content differences and resistance to style biases. Extensive experiments demonstrate that RM-Bench strongly correlates with policy model performance, making it a reliable reference for selecting reward models to align language models effectively. We evaluate nearly 40 reward models on RM-Bench. Our results reveal that even state-of-the-art models achieve an average performance of only 46.6%, which falls short of random-level accuracy (50%) when faced with style bias interference. These findings highlight the significant room for improvement in current reward models. Related code and data are available at https://github.com/THU-KEG/RM-Bench.', 'score': 20, 'issue_id': 209, 'pub_date': '2024-10-21', 'pub_date_ru': '21 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'hash': '6fff842f967d7207', 'data': {'categories': ['#alignment', '#benchmark', '#rlhf'], 'emoji': 'ğŸ¯', 'ru': {'title': 'RM-Bench: Ñ‚Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº RM-Bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¾Ñ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° (RLHF) Ğ¸ Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ² Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. RM-Bench Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ‚Ğ¾Ğ½ĞºĞ¸Ğ¼ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸ÑĞ¼ Ğ² ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ Ğ¸Ñ… ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº ÑÑ‚Ğ¸Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸ÑĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ RM-Bench Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ĞºĞ¾Ñ€Ñ€ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞµĞ³Ğ¾ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ ÑÑ€ĞµĞ´Ğ½ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²ÑĞµĞ³Ğ¾ 46.6%, Ñ‡Ñ‚Ğ¾ Ğ½Ğ¸Ğ¶Ğµ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğ¸ ÑÑ‚Ğ¸Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing Reward Models: Beyond Style Bias and Subtlety', 'desc': "The paper introduces RM-Bench, a new benchmark for evaluating reward models used in Reinforcement Learning from Human Feedback (RLHF) and Inference Scaling Laws. RM-Bench focuses on assessing models' sensitivity to subtle content changes and their resistance to style biases, which are often overlooked in traditional benchmarks. The study finds that even advanced reward models perform poorly, with an average accuracy of 46.6% when style bias is present, indicating a need for improvement. This benchmark provides a more reliable way to select reward models that align language models effectively."}, 'zh': {'title': 'æå‡å¥–åŠ±æ¨¡å‹ï¼šè¶…è¶Šé£æ ¼åè§çš„æŒ‘æˆ˜', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„åŸºå‡†æµ‹è¯•RM-Benchï¼Œç”¨äºè¯„ä¼°å¥–åŠ±æ¨¡å‹åœ¨ç»†å¾®å†…å®¹å·®å¼‚å’Œé£æ ¼åè§ä¸‹çš„è¡¨ç°ã€‚ç°æœ‰çš„å¥–åŠ±æ¨¡å‹åŸºå‡†æµ‹è¯•å¾€å¾€åªå…³æ³¨æ¨¡å‹ä¹‹é—´çš„ç®€å•åŒºåˆ†ï¼Œè€Œå¿½ç•¥äº†å¯¹å†…å®¹å’Œé£æ ¼å˜åŒ–çš„æ•æ„Ÿæ€§ã€‚RM-Benché€šè¿‡å®éªŒè¡¨æ˜ï¼Œä¸ç­–ç•¥æ¨¡å‹è¡¨ç°æœ‰å¾ˆå¼ºçš„ç›¸å…³æ€§ï¼Œæ˜¯é€‰æ‹©å¥–åŠ±æ¨¡å‹çš„å¯é å‚è€ƒã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨é¢å¯¹é£æ ¼åè§æ—¶ï¼Œè¡¨ç°ä¹Ÿä»…ä¸º46.6%ï¼Œä½äºéšæœºæ°´å¹³ï¼Œè¡¨æ˜å½“å‰å¥–åŠ±æ¨¡å‹è¿˜æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚'}}, 'pub_date_card': {'ru': '21 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 21', 'zh': '10æœˆ21æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.12788', 'title': 'Meta-Chunking: Learning Efficient Text Segmentation via Logical Perception', 'url': 'https://huggingface.co/papers/2410.12788', 'abstract': 'Retrieval-Augmented Generation (RAG), while serving as a viable complement to large language models (LLMs), often overlooks the crucial aspect of text chunking within its pipeline, which impacts the quality of knowledge-intensive tasks. This paper introduces the concept of Meta-Chunking, which refers to a granularity between sentences and paragraphs, consisting of a collection of sentences within a paragraph that have deep linguistic logical connections. To implement Meta-Chunking, we designed two strategies based on LLMs: Margin Sampling Chunking and Perplexity Chunking. The former employs LLMs to perform binary classification on whether consecutive sentences need to be segmented, making decisions based on the probability difference obtained from margin sampling. The latter precisely identifies text chunk boundaries by analyzing the characteristics of perplexity distribution. Additionally, considering the inherent complexity of different texts, we propose a strategy that combines Meta-Chunking with dynamic merging to achieve a balance between fine-grained and coarse-grained text chunking. Experiments conducted on eleven datasets demonstrate that Meta-Chunking can more efficiently improve the performance of single-hop and multi-hop question answering based on RAG. For instance, on the 2WikiMultihopQA dataset, it outperforms similarity chunking by 1.32 while only consuming 45.8% of the time. Our code is available at https://github.com/IAAR-Shanghai/Meta-Chunking.', 'score': 15, 'issue_id': 208, 'pub_date': '2024-10-16', 'pub_date_ru': '16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'hash': '0fbcf072cc98c553', 'data': {'categories': ['#data', '#dataset', '#rag'], 'emoji': 'ğŸ§©', 'ru': {'title': 'ĞœĞµÑ‚Ğ°-Ğ§Ğ°Ğ½ĞºĞ¸Ğ½Ğ³: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€Ğ°Ğ·Ğ±Ğ¸ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ RAG', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ ĞœĞµÑ‚Ğ°-Ğ§Ğ°Ğ½ĞºĞ¸Ğ½Ğ³Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Retrieval-Augmented Generation (RAG). ĞœĞµÑ‚Ğ°-Ğ§Ğ°Ğ½ĞºĞ¸Ğ½Ğ³ - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ·Ğ±Ğ¸ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ° Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ñ‹, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸: Margin Sampling Chunking Ğ¸ Perplexity Chunking, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¾Ğ´Ğ¸Ğ½Ğ½Ğ°Ğ´Ñ†Ğ°Ñ‚Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ĞœĞµÑ‚Ğ°-Ğ§Ğ°Ğ½ĞºĞ¸Ğ½Ğ³ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ RAG Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼.'}, 'en': {'title': '"Meta-Chunking: Enhancing RAG with Smarter Text Segmentation"', 'desc': 'The paper introduces Meta-Chunking, a method to improve text chunking in Retrieval-Augmented Generation (RAG) systems by focusing on the logical connections between sentences within paragraphs. Two strategies, Margin Sampling Chunking and Perplexity Chunking, are developed using large language models to determine optimal chunk boundaries. These methods enhance the efficiency and accuracy of question answering tasks by balancing fine-grained and coarse-grained chunking. Experiments show that Meta-Chunking significantly improves performance while reducing processing time compared to traditional chunking methods.'}, 'zh': {'title': 'Meta-Chunkingï¼šæå‡æ–‡æœ¬åˆ†å—çš„æ–°æ–¹æ³•', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ–‡æœ¬åˆ†å—æ–¹æ³•ï¼Œç§°ä¸ºMeta-Chunkingï¼Œå®ƒåœ¨å¥å­å’Œæ®µè½ä¹‹é—´æ‰¾åˆ°ä¸€ç§æ–°çš„ç²’åº¦ã€‚Meta-Chunkingé€šè¿‡ä¸¤ç§ç­–ç•¥å®ç°ï¼šè¾¹ç¼˜é‡‡æ ·åˆ†å—å’Œå›°æƒ‘åº¦åˆ†å—ï¼Œåˆ†åˆ«åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡ŒäºŒå…ƒåˆ†ç±»å’Œå›°æƒ‘åº¦åˆ†å¸ƒåˆ†æã€‚ä¸ºäº†é€‚åº”ä¸åŒæ–‡æœ¬çš„å¤æ‚æ€§ï¼Œè®ºæ–‡è¿˜æå‡ºäº†ä¸€ç§ç»“åˆåŠ¨æ€åˆå¹¶çš„ç­–ç•¥ï¼Œä»¥åœ¨ç»†ç²’åº¦å’Œç²—ç²’åº¦åˆ†å—ä¹‹é—´å–å¾—å¹³è¡¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMeta-Chunkingåœ¨æé«˜åŸºäºRAGçš„å•è·³å’Œå¤šè·³é—®ç­”æ€§èƒ½æ–¹é¢æ›´æœ‰æ•ˆç‡ã€‚'}}, 'pub_date_card': {'ru': '16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 16', 'zh': '10æœˆ16æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.16215', 'title': 'Pre-training Distillation for Large Language Models: A Design Space Exploration', 'url': 'https://huggingface.co/papers/2410.16215', 'abstract': 'Knowledge distillation (KD) aims to transfer knowledge from a large teacher model to a smaller student model. Previous work applying KD in the field of large language models (LLMs) typically focused on the post-training phase, where the student LLM learns directly from instructions and corresponding responses generated by the teacher model. In this paper, we extend KD to the pre-training phase of LLMs, named pre-training distillation (PD). We first conduct a preliminary experiment using GLM-4-9B as the teacher LLM to distill a 1.9B parameter student LLM, validating the effectiveness of PD. Considering the key impact factors of distillation, we systematically explore the design space of pre-training distillation across four aspects: logits processing, loss selection, scaling law, and offline or online logits. We conduct extensive experiments to explore the design space of pre-training distillation and find better configurations and interesting conclusions, such as larger student LLMs generally benefiting more from pre-training distillation, while a larger teacher LLM does not necessarily guarantee better results. We hope our exploration of the design space will inform future practices in pre-training distillation.', 'score': 12, 'issue_id': 209, 'pub_date': '2024-10-21', 'pub_date_ru': '21 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'hash': '0d2e73e11d2b2b3f', 'data': {'categories': ['#architecture'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ”Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ (KD) Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ GLM-4-9B Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ Ğ´Ğ»Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚Ğ° Ñ 1,9 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞĞ½Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ¹ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµĞ¼ Ğ°ÑĞ¿ĞµĞºÑ‚Ğ°Ğ¼: Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ»Ğ¾Ğ³Ğ¸Ñ‚Ğ¾Ğ², Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ, Ğ·Ğ°ĞºĞ¾Ğ½ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½ Ğ¸Ğ»Ğ¸ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½ Ğ»Ğ¾Ğ³Ğ¸Ñ‚Ñ‹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‡ĞµÑĞºĞ¸Ğµ LLM Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ²Ñ‹Ğ¸Ğ³Ñ€Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ¹ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒ Ğ½Ğµ Ğ¾Ğ±ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹.'}, 'en': {'title': '"Pre-Training Distillation: A New Frontier in Model Efficiency"', 'desc': 'This paper explores a new approach to knowledge distillation by applying it during the pre-training phase of large language models, rather than the post-training phase. The authors conduct experiments using a large teacher model to distill knowledge into a smaller student model, demonstrating the effectiveness of this method. They investigate various factors that influence the success of pre-training distillation, such as how logits are processed and the choice of loss functions. The study reveals that larger student models benefit more from pre-training distillation, while the size of the teacher model does not always correlate with better outcomes.'}, 'zh': {'title': 'é¢„è®­ç»ƒé˜¶æ®µçš„çŸ¥è¯†è’¸é¦ï¼šå°æ¨¡å‹çš„å¤§æ™ºæ…§', 'desc': 'è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†å¦‚ä½•åœ¨å¤§è¯­è¨€æ¨¡å‹çš„é¢„è®­ç»ƒé˜¶æ®µè¿›è¡ŒçŸ¥è¯†è’¸é¦ã€‚é€šè¿‡ä½¿ç”¨GLM-4-9Bä½œä¸ºæ•™å¸ˆæ¨¡å‹ï¼ŒéªŒè¯äº†é¢„è®­ç»ƒè’¸é¦çš„æœ‰æ•ˆæ€§ã€‚ç ”ç©¶å‘ç°ï¼Œè¾ƒå¤§çš„å­¦ç”Ÿæ¨¡å‹é€šå¸¸èƒ½ä»é¢„è®­ç»ƒè’¸é¦ä¸­è·ç›Šæ›´å¤šï¼Œè€Œè¾ƒå¤§çš„æ•™å¸ˆæ¨¡å‹ä¸ä¸€å®šå¸¦æ¥æ›´å¥½çš„ç»“æœã€‚ä½œè€…å¸Œæœ›è¿™äº›å‘ç°èƒ½ä¸ºæœªæ¥çš„é¢„è®­ç»ƒè’¸é¦å®è·µæä¾›æŒ‡å¯¼ã€‚'}}, 'pub_date_card': {'ru': '21 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 21', 'zh': '10æœˆ21æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.15748', 'title': 'Alchemy: Amplifying Theorem-Proving Capability through Symbolic Mutation', 'url': 'https://huggingface.co/papers/2410.15748', 'abstract': 'Formal proofs are challenging to write even for experienced experts. Recent progress in Neural Theorem Proving (NTP) shows promise in expediting this process. However, the formal corpora available on the Internet are limited compared to the general text, posing a significant data scarcity challenge for NTP. To address this issue, this work proposes Alchemy, a general framework for data synthesis that constructs formal theorems through symbolic mutation. Specifically, for each candidate theorem in Mathlib, we identify all invocable theorems that can be used to rewrite or apply to it. Subsequently, we mutate the candidate theorem by replacing the corresponding term in the statement with its equivalent form or antecedent. As a result, our method increases the number of theorems in Mathlib by an order of magnitude, from 110k to 6M. Furthermore, we perform continual pretraining and supervised finetuning on this augmented corpus for large language models. Experimental results demonstrate the effectiveness of our approach, achieving a 5% absolute performance improvement on Leandojo benchmark. Additionally, our synthetic data achieve a 2.5% absolute performance gain on the out-of-distribution miniF2F benchmark. To provide further insights, we conduct a comprehensive analysis of synthetic data composition and the training paradigm, offering valuable guidance for developing a strong theorem prover.', 'score': 9, 'issue_id': 212, 'pub_date': '2024-10-21', 'pub_date_ru': '21 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'hash': 'b6b22326cba00346', 'data': {'categories': ['#data', '#dataset', '#math', '#synthetic', '#training'], 'emoji': 'ğŸ§ª', 'ru': {'title': 'ĞĞ»Ñ…Ğ¸Ğ¼Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…: Ğ¿Ñ€ĞµĞ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ ÑĞ¾Ñ‚Ğ½Ğ¸ Ñ‚Ñ‹ÑÑÑ‡ Ñ‚ĞµĞ¾Ñ€ĞµĞ¼ Ğ² Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ñ‹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Alchemy - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ñ‚ĞµĞ¾Ñ€ĞµĞ¼ (NTP). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼ÑƒÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ‚ĞµĞ¾Ñ€ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ² Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞµ Mathlib. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ñ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ 110 Ñ‚Ñ‹ÑÑÑ‡ Ğ´Ğ¾ 6 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ñ‚ĞµĞ¾Ñ€ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° 5% Ğ² Ñ‚ĞµÑÑ‚Ğµ Leandojo Ğ¸ Ğ½Ğ° 2.5% Ğ½Ğ° out-of-distribution Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… miniF2F Ğ¿Ğ¾ÑĞ»Ğµ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞµ.'}, 'en': {'title': 'Alchemy: Transforming Theorem Proving with Synthetic Data', 'desc': 'The paper introduces Alchemy, a framework designed to tackle the data scarcity problem in Neural Theorem Proving by generating synthetic formal theorems. By using symbolic mutation, Alchemy expands the number of theorems in Mathlib significantly, enhancing the training data available for large language models. The approach involves rewriting candidate theorems using invocable theorems, which increases the corpus size and improves model performance on benchmarks like Leandojo and miniF2F. The study also provides an analysis of the synthetic data and training methods, offering insights for building more effective theorem provers.'}, 'zh': {'title': 'Alchemyï¼šé€šè¿‡æ•°æ®åˆæˆå¢å¼ºç¥ç»å®šç†è¯æ˜', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºAlchemyçš„æ•°æ®åˆæˆæ¡†æ¶ï¼Œç”¨äºé€šè¿‡ç¬¦å·å˜å¼‚æ„å»ºå½¢å¼å®šç†ã€‚é€šè¿‡è¯†åˆ«å’Œåº”ç”¨å¯è°ƒç”¨çš„å®šç†ï¼ŒAlchemyå°†Mathlibä¸­çš„å®šç†æ•°é‡ä»11ä¸‡å¢åŠ åˆ°600ä¸‡ã€‚ç„¶åï¼Œç ”ç©¶äººå‘˜åœ¨æ‰©å±•åçš„è¯­æ–™åº“ä¸Šå¯¹å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡ŒæŒç»­é¢„è®­ç»ƒå’Œç›‘ç£å¾®è°ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•åœ¨LeandojoåŸºå‡†ä¸Šæé«˜äº†5%çš„æ€§èƒ½ï¼Œå¹¶åœ¨miniF2FåŸºå‡†ä¸Šæé«˜äº†2.5%çš„æ€§èƒ½ã€‚'}}, 'pub_date_card': {'ru': '21 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 21', 'zh': '10æœˆ21æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.15316', 'title': 'Ichigo: Mixed-Modal Early-Fusion Realtime Voice Assistant', 'url': 'https://huggingface.co/papers/2410.15316', 'abstract': 'Large Language Models (LLMs) have revolutionized natural language processing, but their application to speech-based tasks remains challenging due to the complexities of integrating audio and text modalities. This paper introduces Ichigo, a mixed-modal model that seamlessly processes interleaved sequences of speech and text. Utilizing a tokenized early-fusion approach, Ichigo quantizes speech into discrete tokens and employs a uniform transformer-based architecture for both speech and text modalities. This method enables joint reasoning and generation across modalities without the need for separate adapters. We present a comprehensive training methodology, including pre-training on multilingual speech recognition datasets and fine-tuning on a curated instruction dataset. Ichigo demonstrates state-of-the-art performance on speech question-answering benchmarks, outperforming existing open-source speech language models and achieving comparable results to cascaded systems. Notably, Ichigo exhibits a latency of just 111 ms to first token generation, significantly lower than current models. Our approach not only advances the field of multimodal AI but also provides a framework for smaller research teams to contribute effectively to open-source speech-language models.', 'score': 6, 'issue_id': 214, 'pub_date': '2024-10-20', 'pub_date_ru': '20 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'hash': '0fbe64f20b885346', 'data': {'categories': ['#audio', '#multilingual', '#multimodal', '#training'], 'emoji': 'ğŸ“', 'ru': {'title': 'Ğ˜Ñ‡Ğ¸Ğ³Ğ¾: Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑ Ñ€ĞµÑ‡ÑŒ Ğ¸ Ñ‚ĞµĞºÑÑ‚ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸', 'desc': 'Ğ˜Ñ‡Ğ¸Ğ³Ğ¾ - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ğº Ñ€ĞµÑ‡ÑŒ, Ñ‚Ğ°Ğº Ğ¸ Ñ‚ĞµĞºÑÑ‚. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ½Ğ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹, ĞºĞ²Ğ°Ğ½Ñ‚ÑƒÑ Ñ€ĞµÑ‡ÑŒ Ğ² Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ĞµĞ¸Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ˜Ñ‡Ğ¸Ğ³Ğ¾ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾ Ñ€ĞµÑ‡Ğ¸, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼. Ğ’Ğ°Ğ¶Ğ½Ñ‹Ğ¼ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ½Ğ¸Ğ·ĞºĞ°Ñ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° - Ğ²ÑĞµĞ³Ğ¾ 111 Ğ¼Ñ.'}, 'en': {'title': 'Ichigo: Bridging Speech and Text with Unified AI', 'desc': 'The paper introduces Ichigo, a model that processes both speech and text together using a unified transformer architecture. By converting speech into tokens, Ichigo can handle both modalities without needing separate systems, allowing for efficient joint reasoning. The model is trained on diverse datasets, achieving top performance in speech question-answering tasks with low latency. This approach not only enhances multimodal AI but also empowers smaller teams to develop competitive open-source models.'}, 'zh': {'title': 'Ichigoï¼šè·¨è¶Šè¯­éŸ³ä¸æ–‡æœ¬çš„æ— ç¼èåˆ', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†Ichigoï¼Œä¸€ç§èƒ½å¤ŸåŒæ—¶å¤„ç†è¯­éŸ³å’Œæ–‡æœ¬çš„æ··åˆæ¨¡æ€æ¨¡å‹ã€‚Ichigoé€šè¿‡å°†è¯­éŸ³é‡åŒ–ä¸ºç¦»æ•£çš„tokenï¼Œå¹¶ä½¿ç”¨ç»Ÿä¸€çš„transformeræ¶æ„æ¥å¤„ç†è¯­éŸ³å’Œæ–‡æœ¬ï¼Œä»è€Œå®ç°äº†è·¨æ¨¡æ€çš„è”åˆæ¨ç†å’Œç”Ÿæˆã€‚è¯¥æ¨¡å‹åœ¨å¤šè¯­è¨€è¯­éŸ³è¯†åˆ«æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶åœ¨ç²¾å¿ƒæŒ‘é€‰çš„æŒ‡ä»¤æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œè¡¨ç°å‡ºè‰²ã€‚Ichigoåœ¨è¯­éŸ³é—®ç­”åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¸”é¦–æ¬¡ç”Ÿæˆtokençš„å»¶è¿Ÿä»…ä¸º111æ¯«ç§’ï¼Œæ˜¾è‘—ä½äºç°æœ‰æ¨¡å‹ã€‚'}}, 'pub_date_card': {'ru': '20 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 20', 'zh': '10æœˆ20æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.11711', 'title': 'Zero-shot Model-based Reinforcement Learning using Large Language Models', 'url': 'https://huggingface.co/papers/2410.11711', 'abstract': "The emerging zero-shot capabilities of Large Language Models (LLMs) have led to their applications in areas extending well beyond natural language processing tasks. In reinforcement learning, while LLMs have been extensively used in text-based environments, their integration with continuous state spaces remains understudied. In this paper, we investigate how pre-trained LLMs can be leveraged to predict in context the dynamics of continuous Markov decision processes. We identify handling multivariate data and incorporating the control signal as key challenges that limit the potential of LLMs' deployment in this setup and propose Disentangled In-Context Learning (DICL) to address them. We present proof-of-concept applications in two reinforcement learning settings: model-based policy evaluation and data-augmented off-policy reinforcement learning, supported by theoretical analysis of the proposed methods. Our experiments further demonstrate that our approach produces well-calibrated uncertainty estimates. We release the code at https://github.com/abenechehab/dicl.", 'score': 6, 'issue_id': 212, 'pub_date': '2024-10-15', 'pub_date_ru': '15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'hash': '5868a5a1652a8da3', 'data': {'categories': ['#rl', '#rlhf'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'LLM Ğ¿Ğ¾ĞºĞ¾Ñ€ÑÑÑ‚ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ€ĞºĞ¾Ğ²ÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ (DICL) Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑƒÑ‡ĞµÑ‚Ğ° ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰ĞµĞ³Ğ¾ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ² Ğ´Ğ²ÑƒÑ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼: Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ½Ğµ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¾Ñ‚ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Unlocking LLMs for Continuous Reinforcement Learning', 'desc': 'This paper explores how Large Language Models (LLMs) can be used in reinforcement learning with continuous state spaces, a less-studied area. The authors propose a method called Disentangled In-Context Learning (DICL) to address challenges like handling multivariate data and incorporating control signals. They demonstrate the effectiveness of DICL in model-based policy evaluation and data-augmented off-policy reinforcement learning. The approach also provides well-calibrated uncertainty estimates, enhancing the reliability of predictions.'}, 'zh': {'title': 'è§£é”å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„æ–°æ½œåŠ›', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¥é¢„æµ‹è¿ç»­é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ä¸­çš„åŠ¨æ€ã€‚ç ”ç©¶å‘ç°ï¼Œå¤„ç†å¤šå˜é‡æ•°æ®å’Œæ•´åˆæ§åˆ¶ä¿¡å·æ˜¯é™åˆ¶LLMsåœ¨è¿™ç§ç¯å¢ƒä¸­åº”ç”¨çš„å…³é”®æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åä¸ºè§£è€¦ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆDICLï¼‰çš„æ–°æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæä¾›è‰¯å¥½çš„ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚'}}, 'pub_date_card': {'ru': '15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 15', 'zh': '10æœˆ15æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.15633', 'title': "Selecting Influential Samples for Long Context Alignment via Homologous Models' Guidance and Contextual Awareness Measurement", 'url': 'https://huggingface.co/papers/2410.15633', 'abstract': "The expansion of large language models to effectively handle instructions with extremely long contexts has yet to be fully investigated. The primary obstacle lies in constructing a high-quality long instruction-following dataset devised for long context alignment. Existing studies have attempted to scale up the available data volume by synthesizing long instruction-following samples. However, indiscriminately increasing the quantity of data without a well-defined strategy for ensuring data quality may introduce low-quality samples and restrict the final performance. To bridge this gap, we aim to address the unique challenge of long-context alignment, i.e., modeling the long-range dependencies for handling instructions and lengthy input contexts. We propose GATEAU, a novel framework designed to identify the influential and high-quality samples enriched with long-range dependency relations by utilizing crafted Homologous Models' Guidance (HMG) and Contextual Awareness Measurement (CAM). Specifically, HMG attempts to measure the difficulty of generating corresponding responses due to the long-range dependencies, using the perplexity scores of the response from two homologous models with different context windows. Also, the role of CAM is to measure the difficulty of understanding the long input contexts due to long-range dependencies by evaluating whether the model's attention is focused on important segments. Built upon both proposed methods, we select the most challenging samples as the influential data to effectively frame the long-range dependencies, thereby achieving better performance of LLMs. Comprehensive experiments indicate that GATEAU effectively identifies samples enriched with long-range dependency relations and the model trained on these selected samples exhibits better instruction-following and long-context understanding capabilities.", 'score': 6, 'issue_id': 209, 'pub_date': '2024-10-21', 'pub_date_ru': '21 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'hash': '0b470bc767fc516b', 'data': {'categories': ['#data', '#dataset', '#long_context', '#synthetic'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'GATEAU: Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LLM Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…', 'desc': "Ğ­Ñ‚Ğ¾Ñ‚ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğ¹ Ñ‚Ñ€ÑƒĞ´ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ GATEAU - Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸. GATEAU Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°: Homologous Models' Guidance (HMG) Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ğ¸ Contextual Awareness Measurement (CAM) Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… GATEAU Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²."}, 'en': {'title': 'Mastering Long Contexts with GATEAU: Quality Over Quantity', 'desc': "The paper introduces GATEAU, a framework designed to improve large language models' ability to handle long instructions by focusing on high-quality data samples. It uses Homologous Models' Guidance (HMG) to assess the difficulty of generating responses with long-range dependencies and Contextual Awareness Measurement (CAM) to ensure the model's attention is on important segments. By selecting challenging samples, GATEAU enhances the model's performance in understanding and following long-context instructions. Experiments show that models trained with GATEAU-selected data perform better in long-context tasks."}, 'zh': {'title': 'GATEAUï¼šæå‡é•¿ä¸Šä¸‹æ–‡æŒ‡ä»¤å¤„ç†çš„æ–°æ¡†æ¶', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¦‚ä½•è®©å¤§å‹è¯­è¨€æ¨¡å‹æ›´å¥½åœ°å¤„ç†é•¿ä¸Šä¸‹æ–‡çš„æŒ‡ä»¤ã€‚ä¸»è¦æŒ‘æˆ˜åœ¨äºæ„å»ºä¸€ä¸ªé«˜è´¨é‡çš„é•¿æŒ‡ä»¤æ•°æ®é›†ï¼Œä»¥ä¾¿æ¨¡å‹èƒ½æ›´å¥½åœ°å¯¹é½é•¿ä¸Šä¸‹æ–‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªåä¸ºGATEAUçš„æ–°æ¡†æ¶ï¼Œé€šè¿‡åŒæºæ¨¡å‹æŒ‡å¯¼å’Œä¸Šä¸‹æ–‡æ„è¯†æµ‹é‡æ¥è¯†åˆ«é«˜è´¨é‡æ ·æœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨è¿™äº›æ ·æœ¬è®­ç»ƒçš„æ¨¡å‹åœ¨æŒ‡ä»¤è·Ÿéšå’Œé•¿ä¸Šä¸‹æ–‡ç†è§£æ–¹é¢è¡¨ç°æ›´å¥½ã€‚'}}, 'pub_date_card': {'ru': '21 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 21', 'zh': '10æœˆ21æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.13218', 'title': 'CBT-Bench: Evaluating Large Language Models on Assisting Cognitive Behavior Therapy', 'url': 'https://huggingface.co/papers/2410.13218', 'abstract': "There is a significant gap between patient needs and available mental health support today. In this paper, we aim to thoroughly examine the potential of using Large Language Models (LLMs) to assist professional psychotherapy. To this end, we propose a new benchmark, CBT-BENCH, for the systematic evaluation of cognitive behavioral therapy (CBT) assistance. We include three levels of tasks in CBT-BENCH: I: Basic CBT knowledge acquisition, with the task of multiple-choice questions; II: Cognitive model understanding, with the tasks of cognitive distortion classification, primary core belief classification, and fine-grained core belief classification; III: Therapeutic response generation, with the task of generating responses to patient speech in CBT therapy sessions. These tasks encompass key aspects of CBT that could potentially be enhanced through AI assistance, while also outlining a hierarchy of capability requirements, ranging from basic knowledge recitation to engaging in real therapeutic conversations. We evaluated representative LLMs on our benchmark. Experimental results indicate that while LLMs perform well in reciting CBT knowledge, they fall short in complex real-world scenarios requiring deep analysis of patients' cognitive structures and generating effective responses, suggesting potential future work.", 'score': 3, 'issue_id': 207, 'pub_date': '2024-10-17', 'pub_date_ru': '17 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'hash': 'f4f24ef7771a745c', 'data': {'categories': ['#benchmark', '#medicine'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞÑ†ĞµĞ½ĞºĞ° Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»Ğ° Ğ˜Ğ˜ Ğ² ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾-Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚ĞµÑ€Ğ°Ğ¿Ğ¸Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº CBT-BENCH Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾-Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚ĞµÑ€Ğ°Ğ¿Ğ¸Ğ¸ (ĞšĞŸĞ¢). Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡: Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ ĞšĞŸĞ¢, Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµÑ€Ğ°Ğ¿ĞµĞ²Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ ĞšĞŸĞ¢, Ğ½Ğ¾ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€ Ğ¿Ğ°Ñ†Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜ Ğ² Ğ¿ÑĞ¸Ñ…Ğ¾Ñ‚ĞµÑ€Ğ°Ğ¿Ğ¸Ğ¸.'}, 'en': {'title': 'Bridging the Gap: AI Meets Mental Health Support', 'desc': "This paper explores the use of Large Language Models (LLMs) to support psychotherapy, specifically cognitive behavioral therapy (CBT). It introduces CBT-BENCH, a benchmark designed to evaluate LLMs' ability to assist in CBT through tasks like knowledge acquisition, cognitive model understanding, and therapeutic response generation. The study finds that while LLMs can effectively recall CBT knowledge, they struggle with more complex tasks that require deep understanding and interaction. This highlights the need for further development to enhance LLMs' capabilities in real-world therapeutic settings."}, 'zh': {'title': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼šå¿ƒç†æ²»ç–—çš„æœªæ¥åŠ©æ‰‹ï¼Ÿ', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¥è¾…åŠ©ä¸“ä¸šå¿ƒç†æ²»ç–—çš„æ½œåŠ›ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œåä¸ºCBT-BENCHï¼Œç”¨äºç³»ç»Ÿè¯„ä¼°è®¤çŸ¥è¡Œä¸ºç–—æ³•ï¼ˆCBTï¼‰çš„è¾…åŠ©èƒ½åŠ›ã€‚CBT-BENCHåŒ…æ‹¬ä¸‰ä¸ªä»»åŠ¡å±‚æ¬¡ï¼šåŸºç¡€çŸ¥è¯†è·å–ã€è®¤çŸ¥æ¨¡å‹ç†è§£å’Œæ²»ç–—æ€§å›åº”ç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè™½ç„¶LLMsåœ¨çŸ¥è¯†èƒŒè¯µæ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨éœ€è¦æ·±å…¥åˆ†ææ‚£è€…è®¤çŸ¥ç»“æ„å’Œç”Ÿæˆæœ‰æ•ˆå›åº”çš„å¤æ‚åœºæ™¯ä¸­è¡¨ç°ä¸è¶³ã€‚'}}, 'pub_date_card': {'ru': '17 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 17', 'zh': '10æœˆ17æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.16259', 'title': 'Agent-to-Sim: Learning Interactive Behavior Models from Casual Longitudinal Videos', 'url': 'https://huggingface.co/papers/2410.16259', 'abstract': 'We present Agent-to-Sim (ATS), a framework for learning interactive behavior models of 3D agents from casual longitudinal video collections. Different from prior works that rely on marker-based tracking and multiview cameras, ATS learns natural behaviors of animal and human agents non-invasively through video observations recorded over a long time-span (e.g., a month) in a single environment. Modeling 3D behavior of an agent requires persistent 3D tracking (e.g., knowing which point corresponds to which) over a long time period. To obtain such data, we develop a coarse-to-fine registration method that tracks the agent and the camera over time through a canonical 3D space, resulting in a complete and persistent spacetime 4D representation. We then train a generative model of agent behaviors using paired data of perception and motion of an agent queried from the 4D reconstruction. ATS enables real-to-sim transfer from video recordings of an agent to an interactive behavior simulator. We demonstrate results on pets (e.g., cat, dog, bunny) and human given monocular RGBD videos captured by a smartphone.', 'score': 2, 'issue_id': 216, 'pub_date': '2024-10-21', 'pub_date_ru': '21 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'hash': '2af6ae782ab41cc9', 'data': {'categories': ['#3d', '#agents', '#cv'], 'emoji': 'ğŸ¥', 'ru': {'title': 'ĞÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğº Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ 3D-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Agent-to-Sim (ATS) Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ 3D-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ¾Ğ»Ğ»ĞµĞºÑ†Ğ¸Ğ¹. ATS Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½ĞµĞ¸Ğ½Ğ²Ğ°Ğ·Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¶Ğ¸Ğ²Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ¸ Ğ»ÑĞ´ĞµĞ¹ Ğ¿Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ·Ğ°Ğ¿Ğ¸ÑÑĞ¼, ÑĞ´ĞµĞ»Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ² Ñ‚ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´Ğ° Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚ Ğ³Ñ€ÑƒĞ±Ğ¾Ğ¹ Ğº Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ñ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ°Ğ½Ğ¾Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ 3D-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ°Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'From Video to Virtual: Simulating Real-World Behaviors', 'desc': "The paper introduces Agent-to-Sim (ATS), a framework for learning 3D behavior models of agents from long-term video recordings without using invasive tracking methods. ATS uses a novel coarse-to-fine registration technique to maintain persistent 3D tracking of agents over time, creating a comprehensive 4D representation. This data is then used to train a generative model that simulates the agent's behavior based on its perception and motion. The framework successfully transfers real-world video observations into interactive behavior simulations, demonstrated with pets and humans using simple smartphone recordings."}, 'zh': {'title': 'ä»è§†é¢‘åˆ°æ¨¡æ‹Ÿï¼šéä¾µå…¥æ€§3Dè¡Œä¸ºå»ºæ¨¡', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºAgent-to-Sim (ATS)çš„æ¡†æ¶ï¼Œç”¨äºä»é•¿æœŸè§†é¢‘ä¸­å­¦ä¹ 3Dä»£ç†çš„äº¤äº’è¡Œä¸ºæ¨¡å‹ã€‚ä¸ä¾èµ–æ ‡è®°è·Ÿè¸ªå’Œå¤šè§†è§’ç›¸æœºçš„ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒATSé€šè¿‡å•ä¸€ç¯å¢ƒä¸­é•¿æ—¶é—´çš„è§†é¢‘è§‚å¯Ÿï¼Œéä¾µå…¥æ€§åœ°å­¦ä¹ åŠ¨ç‰©å’Œäººç±»ä»£ç†çš„è‡ªç„¶è¡Œä¸ºã€‚ä¸ºäº†å®ç°3Dè¡Œä¸ºå»ºæ¨¡ï¼Œç ”ç©¶äººå‘˜å¼€å‘äº†ä¸€ç§ç²—åˆ°ç»†çš„æ³¨å†Œæ–¹æ³•ï¼Œé€šè¿‡ä¸€ä¸ªæ ‡å‡†çš„3Dç©ºé—´è·Ÿè¸ªä»£ç†å’Œç›¸æœºï¼Œå½¢æˆå®Œæ•´çš„æ—¶ç©º4Dè¡¨ç¤ºã€‚æœ€ç»ˆï¼ŒATSå¯ä»¥å°†è§†é¢‘è®°å½•ä¸­çš„ä»£ç†è¡Œä¸ºè½¬ç§»åˆ°äº¤äº’è¡Œä¸ºæ¨¡æ‹Ÿå™¨ä¸­ã€‚'}}, 'pub_date_card': {'ru': '21 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 21', 'zh': '10æœˆ21æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.15002', 'title': 'How Many Van Goghs Does It Take to Van Gogh? Finding the Imitation Threshold', 'url': 'https://huggingface.co/papers/2410.15002', 'abstract': "Text-to-image models are trained using large datasets collected by scraping image-text pairs from the internet. These datasets often include private, copyrighted, and licensed material. Training models on such datasets enables them to generate images with such content, which might violate copyright laws and individual privacy. This phenomenon is termed imitation -- generation of images with content that has recognizable similarity to its training images. In this work we study the relationship between a concept's frequency in the training dataset and the ability of a model to imitate it. We seek to determine the point at which a model was trained on enough instances to imitate a concept -- the imitation threshold. We posit this question as a new problem: Finding the Imitation Threshold (FIT) and propose an efficient approach that estimates the imitation threshold without incurring the colossal cost of training multiple models from scratch. We experiment with two domains -- human faces and art styles -- for which we create four datasets, and evaluate three text-to-image models which were trained on two pretraining datasets. Our results reveal that the imitation threshold of these models is in the range of 200-600 images, depending on the domain and the model. The imitation threshold can provide an empirical basis for copyright violation claims and acts as a guiding principle for text-to-image model developers that aim to comply with copyright and privacy laws. We release the code and data at https://github.com/vsahil/MIMETIC-2.git and the project's website is hosted at https://how-many-van-goghs-does-it-take.github.io.", 'score': 1, 'issue_id': 216, 'pub_date': '2024-10-19', 'pub_date_ru': '19 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'hash': 'f538a28b342207ac', 'data': {'categories': ['#cv', '#data', '#dataset', '#ethics'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ¡ĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ’Ğ°Ğ½ Ğ“Ğ¾Ğ³Ğ¾Ğ² Ğ½ÑƒĞ¶Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ˜Ğ˜?', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ¸Ğµ 'Ğ¿Ğ¾Ñ€Ğ¾Ğ³Ğ° Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¸' - Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ´Ğ»Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑÑ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ñ€Ğ¾Ğ³ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ 200-600 Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ° Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸Ğ¼ĞµÑÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ¾Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ°Ğ² Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."}, 'en': {'title': 'Finding the Imitation Threshold: Balancing Creativity and Copyright in AI', 'desc': 'This paper explores how text-to-image models can generate images that closely resemble copyrighted or private content from their training datasets, a phenomenon known as imitation. The authors introduce the concept of the imitation threshold, which is the minimum number of instances a model needs to see in its training data to replicate a concept. They propose a method to estimate this threshold efficiently, without the need to train multiple models from scratch. Their experiments show that the imitation threshold varies between 200-600 images, providing a basis for understanding potential copyright violations and guiding model developers in legal compliance.'}, 'zh': {'title': 'æ­ç¤ºæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„æ¨¡ä»¿é˜ˆå€¼', 'desc': 'è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹åœ¨è®­ç»ƒæ•°æ®é›†ä¸­æ¨¡ä»¿æ¦‚å¿µçš„èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œå½“ä¸€ä¸ªæ¦‚å¿µåœ¨è®­ç»ƒæ•°æ®é›†ä¸­å‡ºç°200åˆ°600æ¬¡æ—¶ï¼Œæ¨¡å‹å°±èƒ½æœ‰æ•ˆæ¨¡ä»¿è¯¥æ¦‚å¿µï¼Œè¿™è¢«ç§°ä¸ºæ¨¡ä»¿é˜ˆå€¼ã€‚é€šè¿‡æ‰¾åˆ°æ¨¡ä»¿é˜ˆå€¼ï¼Œå¯ä»¥å¸®åŠ©æ¨¡å‹å¼€å‘è€…éµå¾ªç‰ˆæƒå’Œéšç§æ³•å¾‹ã€‚ç ”ç©¶ç»“æœä¸ºç‰ˆæƒä¾µæƒçš„åˆ¤æ–­æä¾›äº†å®è¯ä¾æ®ã€‚'}}, 'pub_date_card': {'ru': '19 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 19', 'zh': '10æœˆ19æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.14086', 'title': "In-context learning and Occam's razor", 'url': 'https://huggingface.co/papers/2410.14086', 'abstract': "The goal of machine learning is generalization. While the No Free Lunch Theorem states that we cannot obtain theoretical guarantees for generalization without further assumptions, in practice we observe that simple models which explain the training data generalize best: a principle called Occam's razor. Despite the need for simple models, most current approaches in machine learning only minimize the training error, and at best indirectly promote simplicity through regularization or architecture design. Here, we draw a connection between Occam's razor and in-context learning: an emergent ability of certain sequence models like Transformers to learn at inference time from past observations in a sequence. In particular, we show that the next-token prediction loss used to train in-context learners is directly equivalent to a data compression technique called prequential coding, and that minimizing this loss amounts to jointly minimizing both the training error and the complexity of the model that was implicitly learned from context. Our theory and the empirical experiments we use to support it not only provide a normative account of in-context learning, but also elucidate the shortcomings of current in-context learning methods, suggesting ways in which they can be improved. We make our code available at https://github.com/3rdCore/PrequentialCode.", 'score': 1, 'issue_id': 214, 'pub_date': '2024-10-17', 'pub_date_ru': '17 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'hash': '6f9c84cdfe1502b2', 'data': {'categories': ['#interpretability', '#math'], 'emoji': 'ğŸª’', 'ru': {'title': 'Ğ‘Ñ€Ğ¸Ñ‚Ğ²Ğ° ĞĞºĞºĞ°Ğ¼Ğ° Ğ² Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸: ĞºĞ°Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ñ‚Ñ‹', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ ÑĞ²ÑĞ·ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ±Ñ€Ğ¸Ñ‚Ğ²Ğ¾Ğ¹ ĞĞºĞºĞ°Ğ¼Ğ° Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° ÑĞºĞ²Ğ¸Ğ²Ğ°Ğ»ĞµĞ½Ñ‚Ğ½Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ñƒ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 'Ğ¿Ñ€ĞµĞºĞ²Ğ°ĞµĞ½Ñ‚Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ'. ĞœĞ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾ÑˆĞ¸Ğ±ĞºÑƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ½ĞµÑĞ²Ğ½Ğ¾ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ· ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¸ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¸ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°Ñ Ğ¿ÑƒÑ‚Ğ¸ Ğ¸Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ."}, 'en': {'title': "Simplifying Complexity: Bridging Occam's Razor and In-Context Learning", 'desc': "The paper explores the connection between Occam's razor, which favors simpler models for better generalization, and in-context learning, a feature of sequence models like Transformers. It demonstrates that the next-token prediction loss used in training these models is akin to prequential coding, a data compression method. This approach effectively minimizes both training error and model complexity, aligning with the principle of Occam's razor. The authors provide theoretical insights and empirical evidence, highlighting the limitations of current in-context learning methods and suggesting improvements."}, 'zh': {'title': 'å¥¥å¡å§†å‰ƒåˆ€ä¸ä¸Šä¸‹æ–‡å­¦ä¹ çš„å·§å¦™ç»“åˆ', 'desc': 'æœºå™¨å­¦ä¹ çš„ç›®æ ‡æ˜¯å®ç°æ³›åŒ–ï¼Œå³æ¨¡å‹ä¸ä»…èƒ½åœ¨è®­ç»ƒæ•°æ®ä¸Šè¡¨ç°è‰¯å¥½ï¼Œè¿˜èƒ½åœ¨æ–°æ•°æ®ä¸Šè¡¨ç°å‡ºè‰²ã€‚å¥¥å¡å§†å‰ƒåˆ€åŸåˆ™æŒ‡å‡ºï¼Œç®€å•çš„æ¨¡å‹å¾€å¾€èƒ½æ›´å¥½åœ°æ³›åŒ–ï¼Œä½†ç›®å‰å¤§å¤šæ•°æ–¹æ³•åªå…³æ³¨æœ€å°åŒ–è®­ç»ƒè¯¯å·®ã€‚æœ¬æ–‡å°†å¥¥å¡å§†å‰ƒåˆ€ä¸ä¸Šä¸‹æ–‡å­¦ä¹ è”ç³»èµ·æ¥ï¼Œå±•ç¤ºäº†å¦‚ä½•é€šè¿‡æœ€å°åŒ–ä¸‹ä¸€ä¸ªè¯é¢„æµ‹æŸå¤±æ¥åŒæ—¶å‡å°‘è®­ç»ƒè¯¯å·®å’Œæ¨¡å‹å¤æ‚æ€§ã€‚æˆ‘ä»¬çš„ç†è®ºå’Œå®éªŒæ­ç¤ºäº†å½“å‰ä¸Šä¸‹æ–‡å­¦ä¹ æ–¹æ³•çš„ä¸è¶³ï¼Œå¹¶æå‡ºäº†æ”¹è¿›å»ºè®®ã€‚'}}, 'pub_date_card': {'ru': '17 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 17', 'zh': '10æœˆ17æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.13184', 'title': 'Router-Tuning: A Simple and Effective Approach for Enabling Dynamic-Depth in Transformers', 'url': 'https://huggingface.co/papers/2410.13184', 'abstract': "Traditional transformer models often allocate a fixed amount of computational resources to every input token, leading to inefficient and unnecessary computation. To address this, the Mixture of Depths (MoD) was introduced to dynamically adjust the computational depth by skipping less important layers. Despite its promise, current MoD approaches remain under-explored and face two main challenges: (1) high training costs due to the need to train the entire model along with the routers that determine which layers to skip, and (2) the risk of performance degradation when important layers are bypassed. In response to the first issue, we propose Router-Tuning, a method that fine-tunes only the router on a small dataset, drastically reducing the computational overhead associated with full model training. For the second challenge, we propose MindSkip, which deploys Attention with Dynamic Depths. This method preserves the model's performance while significantly enhancing computational and memory efficiency. Extensive experiments demonstrate that our approach delivers competitive results while dramatically improving the computation efficiency, e.g., 21\\% speedup and only a 0.2\\% performance drop. The code is released at https://github.com/CASE-Lab-UMD/Router-Tuning.", 'score': 1, 'issue_id': 214, 'pub_date': '2024-10-17', 'pub_date_ru': '17 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'hash': '0e28d555da53ebff', 'data': {'categories': ['#architecture', '#optimization'], 'emoji': 'ğŸš€', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹: ÑƒĞ¼Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°Ğ½Ğ¸Ğµ ÑĞ»Ğ¾ĞµĞ² Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Router-Tuning Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ² Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ MindSkip, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ¾Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ 21% ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ° 0.2%. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ñ€Ğ¸ÑĞºĞ° Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ° Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… ÑĞ»Ğ¾ĞµĞ² Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ñ… Mixture of Depths.'}, 'en': {'title': 'Efficient Transformers: Skipping the Unnecessary', 'desc': 'The paper introduces a novel approach to improve the efficiency of transformer models by using a Mixture of Depths (MoD) strategy, which dynamically adjusts computational depth by skipping less important layers. To tackle the high training costs associated with MoD, the authors propose Router-Tuning, a method that fine-tunes only the router on a small dataset, reducing computational overhead. To prevent performance degradation, they introduce MindSkip, which uses Attention with Dynamic Depths to maintain model performance while enhancing efficiency. Experiments show that their approach achieves a 21% speedup with only a 0.2% performance drop, demonstrating its effectiveness in improving computation efficiency.'}, 'zh': {'title': 'åŠ¨æ€æ·±åº¦ï¼šæå‡Transformerè®¡ç®—æ•ˆç‡çš„æ–°æ–¹æ³•', 'desc': 'ä¼ ç»Ÿçš„Transformeræ¨¡å‹å¯¹æ¯ä¸ªè¾“å…¥æ ‡è®°åˆ†é…å›ºå®šçš„è®¡ç®—èµ„æºï¼Œå¯¼è‡´è®¡ç®—æ•ˆç‡ä½ä¸‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå¼•å…¥äº†æ·±åº¦æ··åˆï¼ˆMoDï¼‰æ–¹æ³•ï¼Œé€šè¿‡è·³è¿‡ä¸é‡è¦çš„å±‚æ¥åŠ¨æ€è°ƒæ•´è®¡ç®—æ·±åº¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„MoDæ–¹æ³•é¢ä¸´é«˜è®­ç»ƒæˆæœ¬å’Œæ€§èƒ½ä¸‹é™çš„é£é™©ã€‚æˆ‘ä»¬æå‡ºäº†Router-Tuningå’ŒMindSkipæ–¹æ³•ï¼Œåˆ†åˆ«é€šè¿‡å¾®è°ƒè·¯ç”±å™¨å’Œä½¿ç”¨åŠ¨æ€æ·±åº¦æ³¨æ„åŠ›æ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œæ˜¾è‘—æé«˜äº†è®¡ç®—æ•ˆç‡ã€‚'}}, 'pub_date_card': {'ru': '17 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 17', 'zh': '10æœˆ17æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.15460', 'title': 'Hallucination Detox: Sensitive Neuron Dropout (SeND) for Large Language Model Training', 'url': 'https://huggingface.co/papers/2410.15460', 'abstract': 'As large language models (LLMs) become increasingly deployed across various industries, concerns regarding their reliability, particularly due to hallucinations-outputs that are factually inaccurate or irrelevant to user input-have grown. Our research investigates the relationship between the training process and the emergence of hallucinations to address a key gap in existing research that focuses primarily on post hoc detection and mitigation strategies. Using models from the Pythia suite (70M-12B parameters) and several hallucination detection metrics, we analyze hallucination trends throughout training and explore LLM internal dynamics. We introduce SEnsitive Neuron Dropout (SeND), a novel training protocol designed to mitigate hallucinations by reducing variance during training. SeND achieves this by deterministically dropping neurons with significant variability on a dataset, referred to as Sensitive Neurons. In addition, we develop an unsupervised hallucination detection metric, Efficient EigenScore (EES), which approximates the traditional EigenScore in 2x speed. This efficient metric is integrated into our protocol, allowing SeND to be both computationally scalable and effective at reducing hallucinations. Our empirical evaluation demonstrates that our approach improves LLM reliability at test time by up to 40% compared to normal training while also providing an efficient method to improve factual accuracy when adapting LLMs to domains such as Wikipedia and Medical datasets.', 'score': 1, 'issue_id': 214, 'pub_date': '2024-10-20', 'pub_date_ru': '20 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'hash': '5c4104bbacb9ac91', 'data': {'categories': ['#hallucinations', '#interpretability', '#medicine', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ‘Ğ¾Ñ€ÑŒĞ±Ğ° Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ² LLM Ñ‡ĞµÑ€ĞµĞ· ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ°Ğ¼Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·ÑƒÑ‡Ğ°ĞµÑ‚ ÑĞ²ÑĞ·ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ½Ğ¾Ğ²ĞµĞ½Ğ¸ĞµĞ¼ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ñ‚Ñ€ĞµĞ½Ğ´Ñ‹ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ‚ÑĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ· Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Pythia Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ SeND, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ´ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ². Ğ¢Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ½ĞµÑÑƒĞ¿ĞµÑ€Ğ²Ğ¸Ğ·Ğ¾Ñ€Ğ½Ğ°Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ° Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ EES, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ² Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ĞµĞ³Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': '"Training Smarter: Reducing Hallucinations in Language Models"', 'desc': 'The paper explores how large language models (LLMs) sometimes produce incorrect or irrelevant outputs, known as hallucinations, and how these can be reduced during the training process. The researchers introduce a new training method called Sensitive Neuron Dropout (SeND), which helps decrease hallucinations by selectively dropping neurons that show high variability. They also develop a fast, unsupervised metric called Efficient EigenScore (EES) to detect hallucinations more quickly. Their approach shows a significant improvement in the reliability and factual accuracy of LLMs, especially when applied to specific domains like Wikipedia and medical datasets.'}, 'zh': {'title': 'å‡å°‘å¹»è§‰ï¼Œæå‡è¯­è¨€æ¨¡å‹å¯é æ€§', 'desc': 'éšç€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å„è¡Œä¸šçš„å¹¿æ³›åº”ç”¨ï¼Œäººä»¬å¯¹å…¶å¯é æ€§ï¼Œå°¤å…¶æ˜¯å¹»è§‰ç°è±¡çš„æ‹…å¿§æ—¥ç›Šå¢åŠ ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ¢è®¨äº†è®­ç»ƒè¿‡ç¨‹ä¸å¹»è§‰ç°è±¡ä¹‹é—´çš„å…³ç³»ï¼Œå¡«è¡¥äº†ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­äºäº‹åæ£€æµ‹å’Œç¼“è§£ç­–ç•¥çš„ç©ºç™½ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„è®­ç»ƒåè®®ï¼Œç§°ä¸ºæ•æ„Ÿç¥ç»å…ƒä¸¢å¼ƒï¼ˆSeNDï¼‰ï¼Œé€šè¿‡åœ¨è®­ç»ƒä¸­æœ‰é€‰æ‹©åœ°ä¸¢å¼ƒå˜å¼‚æ€§å¤§çš„ç¥ç»å…ƒæ¥å‡å°‘å¹»è§‰ã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€ç§æ— ç›‘ç£çš„å¹»è§‰æ£€æµ‹æŒ‡æ ‡ï¼Œç§°ä¸ºé«˜æ•ˆç‰¹å¾å€¼åˆ†æ•°ï¼ˆEESï¼‰ï¼Œä½¿å¾—SeNDåœ¨å‡å°‘å¹»è§‰çš„åŒæ—¶å…·æœ‰è®¡ç®—å¯æ‰©å±•æ€§ã€‚'}}, 'pub_date_card': {'ru': '20 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 20', 'zh': '10æœˆ20æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.13394', 'title': 'Cross-Lingual Auto Evaluation for Assessing Multilingual LLMs', 'url': 'https://huggingface.co/papers/2410.13394', 'abstract': 'Evaluating machine-generated text remains a significant challenge in NLP, especially for non-English languages. Current methodologies, including automated metrics, human assessments, and LLM-based evaluations, predominantly focus on English, revealing a significant gap in multilingual evaluation frameworks. We introduce the Cross Lingual Auto Evaluation (CIA) Suite, an extensible framework that includes evaluator LLMs (Hercule) and a novel test set (Recon) specifically designed for multilingual evaluation. Our test set features 500 human-annotated instructions spanning various task capabilities along with human judgment scores across six languages. This would enable benchmarking of general-purpose multilingual LLMs and facilitate meta-evaluation of Evaluator LLMs. The proposed model, Hercule, is a cross-lingual evaluation model that addresses the scarcity of reference answers in the target language by learning to assign scores to responses based on easily available reference answers in English. Our experiments demonstrate that Hercule aligns more closely with human judgments compared to proprietary models, demonstrating the effectiveness of such cross-lingual evaluation in low resource scenarios. Further, it is also effective in zero-shot evaluation on unseen languages. This study is the first comprehensive examination of cross-lingual evaluation using LLMs, presenting a scalable and effective approach for multilingual assessment. All code, datasets, and models will be publicly available to enable further research in this important area.', 'score': 1, 'issue_id': 208, 'pub_date': '2024-10-17', 'pub_date_ru': '17 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'hash': 'd456f53989a80b51', 'data': {'categories': ['#benchmark', '#multilingual', '#translation'], 'emoji': 'ğŸŒ', 'ru': {'title': 'ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ±Ğ°Ñ€ÑŒĞµÑ€Ğ¾Ğ² Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº CIA Suite Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸Ğº Hercule Ğ¸ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Recon Ñ 500 Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸ Ğ½Ğ° ÑˆĞµÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ°Ñ…. Hercule ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ½Ğ° Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ½Ğ° Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑ…Ğ²Ğ°Ñ‚ĞºĞ¸ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Hercule Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼ Ğ»ÑĞ´ĞµĞ¹, Ñ‡ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ° Ğ´Ğ°Ğ¶Ğµ Ğ´Ğ»Ñ Ñ€Ğ°Ğ½ĞµĞµ Ğ½Ğµ Ğ²Ğ¸Ğ´ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ².'}, 'en': {'title': 'Breaking Language Barriers in NLP Evaluation', 'desc': "The paper addresses the challenge of evaluating machine-generated text in multiple languages, which is often overlooked in favor of English. It introduces the Cross Lingual Auto Evaluation (CIA) Suite, featuring the Hercule model and a novel test set called Recon, designed for multilingual evaluation. Hercule is a cross-lingual evaluation model that uses English reference answers to score responses in other languages, aligning closely with human judgments. The study demonstrates Hercule's effectiveness in low-resource and zero-shot scenarios, marking a significant step forward in multilingual NLP evaluation."}, 'zh': {'title': 'è·¨è¯­è¨€è¯„ä¼°çš„æ–°çªç ´ï¼šHerculeæ¨¡å‹çš„å¤šè¯­è¨€è¯„ä¼°', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸä¸­ï¼Œè¯„ä¼°æœºå™¨ç”Ÿæˆæ–‡æœ¬çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨éè‹±è¯­è¯­è¨€ä¸­ã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªåä¸ºè·¨è¯­è¨€è‡ªåŠ¨è¯„ä¼°å¥—ä»¶ï¼ˆCIAï¼‰çš„æ¡†æ¶ï¼Œå…¶ä¸­åŒ…æ‹¬ä¸€ä¸ªæ–°çš„è¯„ä¼°æ¨¡å‹Herculeå’Œä¸€ä¸ªå¤šè¯­è¨€æµ‹è¯•é›†Reconã€‚Herculeæ¨¡å‹é€šè¿‡å­¦ä¹ ä»è‹±è¯­å‚è€ƒç­”æ¡ˆä¸­è¯„åˆ†ï¼Œè§£å†³äº†ç›®æ ‡è¯­è¨€ä¸­ç¼ºä¹å‚è€ƒç­”æ¡ˆçš„é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼ŒHerculeåœ¨ä½èµ„æºåœºæ™¯ä¸­ä¸äººç±»åˆ¤æ–­æ›´ä¸ºä¸€è‡´ï¼Œå¹¶ä¸”åœ¨æœªè§è¿‡çš„è¯­è¨€ä¸Šä¹Ÿèƒ½è¿›è¡Œé›¶æ ·æœ¬è¯„ä¼°ã€‚'}}, 'pub_date_card': {'ru': '17 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 17', 'zh': '10æœˆ17æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.15017', 'title': 'DM-Codec: Distilling Multimodal Representations for Speech Tokenization', 'url': 'https://huggingface.co/papers/2410.15017', 'abstract': 'Recent advancements in speech-language models have yielded significant improvements in speech tokenization and synthesis. However, effectively mapping the complex, multidimensional attributes of speech into discrete tokens remains challenging. This process demands acoustic, semantic, and contextual information for precise speech representations. Existing speech representations generally fall into two categories: acoustic tokens from audio codecs and semantic tokens from speech self-supervised learning models. Although recent efforts have unified acoustic and semantic tokens for improved performance, they overlook the crucial role of contextual representation in comprehensive speech modeling. Our empirical investigations reveal that the absence of contextual representations results in elevated Word Error Rate (WER) and Word Information Lost (WIL) scores in speech transcriptions. To address these limitations, we propose two novel distillation approaches: (1) a language model (LM)-guided distillation method that incorporates contextual information, and (2) a combined LM and self-supervised speech model (SM)-guided distillation technique that effectively distills multimodal representations (acoustic, semantic, and contextual) into a comprehensive speech tokenizer, termed DM-Codec. The DM-Codec architecture adopts a streamlined encoder-decoder framework with a Residual Vector Quantizer (RVQ) and incorporates the LM and SM during the training process. Experiments show DM-Codec significantly outperforms state-of-the-art speech tokenization models, reducing WER by up to 13.46%, WIL by 9.82%, and improving speech quality by 5.84% and intelligibility by 1.85% on the LibriSpeech benchmark dataset. The code, samples, and model checkpoints are available at https://github.com/mubtasimahasan/DM-Codec.', 'score': 1, 'issue_id': 208, 'pub_date': '2024-10-19', 'pub_date_ru': '19 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'hash': 'd2c6c349adfb4796', 'data': {'categories': ['#audio', '#benchmark', '#multimodal'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'DM-Codec: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµÑ‡Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµÑ‡Ğ¸, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ DM-Codec. ĞĞ½ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ°ĞºÑƒÑÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ, ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸: Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ¸ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ¹ÑÑ Ñ€ĞµÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ DM-Codec Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµÑ‡Ğ¸, ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ Ñ€Ğ°Ğ·Ğ±Ğ¾Ñ€Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ñ€ĞµÑ‡Ğ¸.'}, 'en': {'title': '"DM-Codec: Revolutionizing Speech Tokenization with Contextual Intelligence"', 'desc': 'This paper addresses the challenge of effectively mapping speech into discrete tokens by incorporating acoustic, semantic, and contextual information. The authors propose two novel distillation methods that integrate language models and self-supervised speech models to create a comprehensive speech tokenizer called DM-Codec. The DM-Codec uses a streamlined encoder-decoder framework with a Residual Vector Quantizer to improve speech tokenization. Experiments demonstrate that DM-Codec significantly reduces Word Error Rate and Word Information Lost, enhancing speech quality and intelligibility.'}, 'zh': {'title': 'DM-Codecï¼šèåˆå¤šæ¨¡æ€ä¿¡æ¯çš„è¯­éŸ³æ ‡è®°åŒ–æ–°çªç ´', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†è¯­éŸ³æ¨¡å‹ä¸­è¯­éŸ³æ ‡è®°åŒ–å’Œåˆæˆçš„æœ€æ–°è¿›å±•ï¼Œå¼ºè°ƒäº†å°†å¤æ‚çš„è¯­éŸ³å±æ€§æ˜ å°„ä¸ºç¦»æ•£æ ‡è®°çš„æŒ‘æˆ˜ã€‚ç ”ç©¶å‘ç°ï¼Œç¼ºä¹ä¸Šä¸‹æ–‡è¡¨ç¤ºä¼šå¯¼è‡´è¯­éŸ³è½¬å½•ä¸­çš„è¯é”™è¯¯ç‡å’Œè¯ä¿¡æ¯ä¸¢å¤±ç‡å‡é«˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸¤ç§æ–°çš„è’¸é¦æ–¹æ³•ï¼Œç»“åˆè¯­è¨€æ¨¡å‹å’Œè‡ªç›‘ç£è¯­éŸ³æ¨¡å‹æ¥æ”¹è¿›è¯­éŸ³æ ‡è®°å™¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„DM-Codecæ¨¡å‹åœ¨è¯­éŸ³æ ‡è®°åŒ–æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ã€‚'}}, 'pub_date_card': {'ru': '19 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 19', 'zh': '10æœˆ19æ—¥'}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            const themeToggle = document.getElementById('theme-toggle');
            let settingSortBy = localStorage.getItem('sort_by');
            const sortDropdown = document.getElementById('sort-dropdown');
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }
            
            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (1)', '#agi', '#alignment (3)', '#architecture (2)', '#audio (2)', '#benchmark (8)', '#cv (5)', '#data (5)', '#dataset (6)', '#diffusion', '#edge_computing', '#ethics (1)', '#games', '#graphs', '#hallucinations (1)', '#inference', '#interpretability (2)', '#long_context (1)', '#math (2)', '#medicine (2)', '#multilingual (3)', '#multimodal (5)', '#optimization (2)', '#plp', '#rag (1)', '#reasoning', '#rl (1)', '#rlhf (2)', '#robotics', '#security', '#story_generation', '#survey', '#synthetic (2)', '#training (4)', '#transfer_learning', '#translation (1)', '#video (1)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles = selectedCategories.length === 0
                ? articlesData
                : articlesData.filter(article => 
                    article.data && article.data.categories && 
                    article.data.categories.some(cat => selectedCategories.includes(cat))
                );

            console.log('filteredArticles', filteredArticles)

            //if (filteredArticles.length === 0) {
            //    selectedArticles = articlesData;
            //    selectedCategories = [];
            //    cleanCategorySelection();
            //} else {
            //    selectedArticles = filteredArticles;
            //}

            selectedArticles = filteredArticles;

            console.log('selectedArticles', selectedArticles)

            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].join(" ");
                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="background-digit">${index + 1}</div>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <h2>${item['data']['emoji']} ${item['title']}</h2>
                            <p class="meta"><svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path transform="translate(0, 2)" fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> ${item['score']}. ${title}</p>
                            <p class="pub-date">ğŸ“ ${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>
                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>
                            <p class="tags">${cats}</p>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            }
            if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
        });

        clearCategoriesButton.addEventListener('click', clearAllCategories);
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2024-10-22 20:13',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];       
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink() {
            if (isToday('2024-10-22 20:13')) {
                const element = document.getElementById('nav-next');
                if (element) {    
                    element.style.display = 'none';
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink(); 
        initializeLanguageFlags();
        updateLocalization();
    </script>
</body>
</html>
    