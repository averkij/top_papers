{
    "date": {
        "ru": "18 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
        "en": "April 18",
        "zh": "4æœˆ18æ—¥"
    },
    "time_utc": "2025-04-18 02:20",
    "weekday": 4,
    "issue_id": 3303,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.12626",
            "title": "Packing Input Frame Context in Next-Frame Prediction Models for Video\n  Generation",
            "url": "https://huggingface.co/papers/2504.12626",
            "abstract": "We present a neural network structure, FramePack, to train next-frame (or next-frame-section) prediction models for video generation. The FramePack compresses input frames to make the transformer context length a fixed number regardless of the video length. As a result, we are able to process a large number of frames using video diffusion with computation bottleneck similar to image diffusion. This also makes the training video batch sizes significantly higher (batch sizes become comparable to image diffusion training). We also propose an anti-drifting sampling method that generates frames in inverted temporal order with early-established endpoints to avoid exposure bias (error accumulation over iterations). Finally, we show that existing video diffusion models can be finetuned with FramePack, and their visual quality may be improved because the next-frame prediction supports more balanced diffusion schedulers with less extreme flow shift timesteps.",
            "score": 2,
            "issue_id": 3303,
            "pub_date": "2025-04-17",
            "pub_date_card": {
                "ru": "17 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 17",
                "zh": "4æœˆ17æ—¥"
            },
            "hash": "fd1688a4e26dbb32",
            "authors": [
                "Lvmin Zhang",
                "Maneesh Agrawala"
            ],
            "affiliations": [
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.12626.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#diffusion",
                    "#video",
                    "#training"
                ],
                "emoji": "ğŸï¸",
                "ru": {
                    "title": "FramePack: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "FramePack - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ ĞºĞ°Ğ´Ñ€Ğ° Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ½Ğ° ÑĞ¶Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ ĞºĞ°Ğ´Ñ€Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ¹ Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. FramePack Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½ Ğ´Ğ»Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾."
                },
                "en": {
                    "title": "FramePack: Efficient Video Generation with Next-Frame Prediction",
                    "desc": "The paper introduces FramePack, a novel neural network architecture designed for predicting the next frame in video generation. By compressing input frames, FramePack ensures that the transformer can handle a fixed context length, making it efficient for processing long videos. This approach allows for larger training batch sizes, similar to those used in image diffusion, while maintaining computational efficiency. Additionally, the authors present an anti-drifting sampling method to mitigate exposure bias, enhancing the quality of generated frames and improving existing video diffusion models through fine-tuning."
                },
                "zh": {
                    "title": "FramePackï¼šæå‡è§†é¢‘ç”Ÿæˆçš„ä¸‹ä¸€å¸§é¢„æµ‹èƒ½åŠ›",
                    "desc": "æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç¥ç»ç½‘ç»œç»“æ„ï¼ŒFramePackï¼Œç”¨äºè®­ç»ƒè§†é¢‘ç”Ÿæˆçš„ä¸‹ä¸€å¸§é¢„æµ‹æ¨¡å‹ã€‚FramePacké€šè¿‡å‹ç¼©è¾“å…¥å¸§ï¼Œä½¿å¾—å˜æ¢å™¨çš„ä¸Šä¸‹æ–‡é•¿åº¦å›ºå®šï¼Œæ— è®ºè§†é¢‘é•¿åº¦å¦‚ä½•ã€‚è¿™æ ·ï¼Œæˆ‘ä»¬èƒ½å¤Ÿä½¿ç”¨ä¸å›¾åƒæ‰©æ•£ç›¸ä¼¼çš„è®¡ç®—ç“¶é¢ˆå¤„ç†å¤§é‡å¸§ï¼Œä»è€Œæ˜¾è‘—æé«˜è§†é¢‘è®­ç»ƒçš„æ‰¹é‡å¤§å°ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§åæ¼‚ç§»é‡‡æ ·æ–¹æ³•ï¼Œä»¥é¿å…è¿­ä»£è¿‡ç¨‹ä¸­çš„æ›å…‰åå·®ï¼Œä»è€Œæé«˜ç”Ÿæˆå¸§çš„è´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.05506",
            "title": "ChartQAPro: A More Diverse and Challenging Benchmark for Chart Question\n  Answering",
            "url": "https://huggingface.co/papers/2504.05506",
            "abstract": "Charts are ubiquitous, as people often use them to analyze data, answer questions, and discover critical insights. However, performing complex analytical tasks with charts requires significant perceptual and cognitive effort. Chart Question Answering (CQA) systems automate this process by enabling models to interpret and reason with visual representations of data. However, existing benchmarks like ChartQA lack real-world diversity and have recently shown performance saturation with modern large vision-language models (LVLMs). To address these limitations, we introduce ChartQAPro, a new benchmark that includes 1,341 charts from 157 diverse sources, spanning various chart types, including infographics and dashboards, and featuring 1,948 questions in various types, such as multiple-choice, conversational, hypothetical, and unanswerable questions, to better reflect real-world challenges. Our evaluations with 21 models show a substantial performance drop for LVLMs on ChartQAPro; e.g., Claude Sonnet 3.5 scores 90.5% on ChartQA but only 55.81% on ChartQAPro, underscoring the complexity of chart reasoning. We complement our findings with detailed error analyses and ablation studies, identifying key challenges and opportunities for advancing LVLMs in chart understanding and reasoning. We release ChartQAPro at https://github.com/vis-nlp/ChartQAPro.",
            "score": 2,
            "issue_id": 3303,
            "pub_date": "2025-04-07",
            "pub_date_card": {
                "ru": "7 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 7",
                "zh": "4æœˆ7æ—¥"
            },
            "hash": "a727d08eac22e920",
            "authors": [
                "Ahmed Masry",
                "Mohammed Saidul Islam",
                "Mahir Ahmed",
                "Aayush Bajaj",
                "Firoz Kabir",
                "Aaryaman Kartha",
                "Md Tahmid Rahman Laskar",
                "Mizanur Rahman",
                "Shadikur Rahman",
                "Mehrad Shahmohammadi",
                "Megh Thakkar",
                "Md Rizwan Parvez",
                "Enamul Hoque",
                "Shafiq Joty"
            ],
            "affiliations": [
                "Dialpad Inc., Canada",
                "MILA - Quebec AI Institute, Canada",
                "Nanyang Technological University, Singapore",
                "Qatar Computing Research Institute (QCRI)",
                "RBC, Canada",
                "Salesforce Research, USA",
                "York University, Canada"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.05506.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#interpretability",
                    "#cv",
                    "#dataset",
                    "#reasoning"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "ChartQAPro: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ´Ğ»Ñ Ğ˜Ğ˜ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ²",
                    "desc": "ChartQAPro - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ°Ğ¼. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 1300 Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ² Ğ¸Ğ· Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² Ğ¸ Ğ¾ĞºĞ¾Ğ»Ğ¾ 2000 Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ². Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ…ÑƒĞ¶Ğµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ ChartQAPro Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¸ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸ Ğ¾ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ°Ñ… Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "ChartQAPro: Elevating Chart Understanding for AI",
                    "desc": "This paper introduces ChartQAPro, a new benchmark designed to improve Chart Question Answering (CQA) systems by providing a more diverse and realistic set of charts and questions. Unlike previous benchmarks, ChartQAPro includes 1,341 charts from 157 sources and features various question types, which better reflect the complexities of real-world data analysis. The study reveals that modern large vision-language models (LVLMs) struggle significantly with this new benchmark, demonstrating a performance drop from 90.5% on the previous ChartQA to only 55.81% on ChartQAPro. Through error analyses and ablation studies, the authors identify challenges in chart reasoning that can guide future improvements in LVLMs."
                },
                "zh": {
                    "title": "æå‡å›¾è¡¨é—®ç­”ç³»ç»Ÿçš„æŒ‘æˆ˜ä¸æœºé‡",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ChartQAProï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨æé«˜å›¾è¡¨é—®ç­”ç³»ç»Ÿçš„æ€§èƒ½ã€‚å®ƒåŒ…å«æ¥è‡ª157ä¸ªä¸åŒæ¥æºçš„1,341ä¸ªå›¾è¡¨ï¼Œæ¶µç›–å¤šç§å›¾è¡¨ç±»å‹ï¼Œå¹¶æä¾›1,948ä¸ªå¤šæ ·åŒ–çš„é—®é¢˜ã€‚é€šè¿‡å¯¹21ä¸ªæ¨¡å‹çš„è¯„ä¼°ï¼Œæˆ‘ä»¬å‘ç°ç°ä»£å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ChartQAProä¸Šçš„è¡¨ç°æ˜¾è‘—ä¸‹é™ï¼Œæ˜¾ç¤ºå‡ºå›¾è¡¨æ¨ç†çš„å¤æ‚æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¿˜åŒ…æ‹¬è¯¦ç»†çš„é”™è¯¯åˆ†æå’Œæ¶ˆèç ”ç©¶ï¼Œä»¥è¯†åˆ«å›¾è¡¨ç†è§£å’Œæ¨ç†ä¸­çš„å…³é”®æŒ‘æˆ˜å’Œæœºé‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.13122",
            "title": "VistaDPO: Video Hierarchical Spatial-Temporal Direct Preference\n  Optimization for Large Video Models",
            "url": "https://huggingface.co/papers/2504.13122",
            "abstract": "Large Video Models (LVMs) built upon Large Language Models (LLMs) have shown promise in video understanding but often suffer from misalignment with human intuition and video hallucination issues. To address these challenges, we introduce VistaDPO, a novel framework for Video Hierarchical Spatial-Temporal Direct Preference Optimization. VistaDPO enhances text-video preference alignment across three hierarchical levels: i) Instance Level, aligning overall video content with responses; ii) Temporal Level, aligning video temporal semantics with event descriptions; and iii) Perceptive Level, aligning spatial objects with language tokens. Given the lack of datasets for fine-grained video-language preference alignment, we construct VistaDPO-7k, a dataset of 7.2K QA pairs annotated with chosen and rejected responses, along with spatial-temporal grounding information such as timestamps, keyframes, and bounding boxes. Extensive experiments on benchmarks such as Video Hallucination, Video QA, and Captioning performance tasks demonstrate that VistaDPO significantly improves the performance of existing LVMs, effectively mitigating video-language misalignment and hallucination. The code and data are available at https://github.com/HaroldChen19/VistaDPO.",
            "score": 1,
            "issue_id": 3303,
            "pub_date": "2025-04-17",
            "pub_date_card": {
                "ru": "17 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 17",
                "zh": "4æœˆ17æ—¥"
            },
            "hash": "efbc3b240498ce70",
            "authors": [
                "Haojian Huang",
                "Haodong Chen",
                "Shengqiong Wu",
                "Meng Luo",
                "Jinlan Fu",
                "Xinya Du",
                "Hanwang Zhang",
                "Hao Fei"
            ],
            "affiliations": [
                "Nanyang Technological University",
                "National University of Singapore",
                "The Hong Kong University of Science and Technology",
                "The University of Hong Kong",
                "University of Texas at Dallas"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.13122.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#alignment",
                    "#hallucinations",
                    "#dataset"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "VistaDPO: Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ ÑĞ·Ñ‹ĞºĞ° Ğ½Ğ° Ğ²ÑĞµÑ… ÑƒÑ€Ğ¾Ğ²Ğ½ÑÑ…",
                    "desc": "VistaDPO - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°. ĞĞ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ñ‚Ñ€ĞµÑ… ÑƒÑ€Ğ¾Ğ²Ğ½ÑÑ…: Ğ¾Ğ±Ñ‰ĞµĞ¼ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğ¸, Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞµ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ VistaDPO-7k Ñ 7200 Ğ¿Ğ°Ñ€Ğ°Ğ¼Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ€Ğ³Ğ½ÑƒÑ‚Ñ‹Ğ¼Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚ĞºĞ°Ğ¼Ğ¸ Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ€Ğ°Ğ¼ĞºĞ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ VistaDPO Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ñ€Ğ°ÑÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ ÑĞ·Ñ‹ĞºĞ° Ğ¸ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Aligning Video and Language: Introducing VistaDPO",
                    "desc": "This paper presents VistaDPO, a new framework designed to improve the alignment between video content and human language understanding in Large Video Models (LVMs). It operates on three hierarchical levels: aligning overall video content with user responses, matching temporal semantics of videos with event descriptions, and correlating spatial objects with language tokens. To support this framework, the authors created a dataset called VistaDPO-7k, which includes 7.2K question-answer pairs with detailed annotations for better preference alignment. The results show that VistaDPO enhances the performance of LVMs by reducing issues related to video hallucination and misalignment with human intuition."
                },
                "zh": {
                    "title": "VistaDPOï¼šæå‡è§†é¢‘ç†è§£çš„åå¥½å¯¹é½",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°æ¡†æ¶VistaDPOï¼Œç”¨äºè§†é¢‘å±‚æ¬¡ç©ºé—´-æ—¶é—´ç›´æ¥åå¥½ä¼˜åŒ–ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è§†é¢‘æ¨¡å‹ï¼ˆLVMsï¼‰åœ¨è§†é¢‘ç†è§£ä¸­çš„äººç±»ç›´è§‰ä¸ä¸€è‡´å’Œè§†é¢‘å¹»è§‰é—®é¢˜ã€‚VistaDPOé€šè¿‡ä¸‰ä¸ªå±‚æ¬¡å¢å¼ºæ–‡æœ¬-è§†é¢‘åå¥½å¯¹é½ï¼šå®ä¾‹å±‚ã€æ—¶é—´å±‚å’Œæ„ŸçŸ¥å±‚ï¼Œåˆ†åˆ«å¯¹é½è§†é¢‘å†…å®¹ã€æ—¶é—´è¯­ä¹‰å’Œç©ºé—´å¯¹è±¡ã€‚ä¸ºäº†æ”¯æŒç»†ç²’åº¦è§†é¢‘-è¯­è¨€åå¥½å¯¹é½ï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†VistaDPO-7kæ•°æ®é›†ï¼ŒåŒ…å«7200ä¸ªé—®ç­”å¯¹åŠå…¶ç©ºé—´-æ—¶é—´ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVistaDPOæ˜¾è‘—æå‡äº†ç°æœ‰LVMsçš„æ€§èƒ½ï¼Œæœ‰æ•ˆå‡è½»äº†è§†é¢‘-è¯­è¨€çš„ä¸ä¸€è‡´æ€§å’Œå¹»è§‰ç°è±¡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.12369",
            "title": "WORLDMEM: Long-term Consistent World Simulation with Memory",
            "url": "https://huggingface.co/papers/2504.12369",
            "abstract": "World simulation has gained increasing popularity due to its ability to model virtual environments and predict the consequences of actions. However, the limited temporal context window often leads to failures in maintaining long-term consistency, particularly in preserving 3D spatial consistency. In this work, we present WorldMem, a framework that enhances scene generation with a memory bank consisting of memory units that store memory frames and states (e.g., poses and timestamps). By employing a memory attention mechanism that effectively extracts relevant information from these memory frames based on their states, our method is capable of accurately reconstructing previously observed scenes, even under significant viewpoint or temporal gaps. Furthermore, by incorporating timestamps into the states, our framework not only models a static world but also captures its dynamic evolution over time, enabling both perception and interaction within the simulated world. Extensive experiments in both virtual and real scenarios validate the effectiveness of our approach.",
            "score": 1,
            "issue_id": 3303,
            "pub_date": "2025-04-16",
            "pub_date_card": {
                "ru": "16 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 16",
                "zh": "4æœˆ16æ—¥"
            },
            "hash": "79cf162a1b60f887",
            "authors": [
                "Zeqi Xiao",
                "Yushi Lan",
                "Yifan Zhou",
                "Wenqi Ouyang",
                "Shuai Yang",
                "Yanhong Zeng",
                "Xingang Pan"
            ],
            "affiliations": [
                "S-Lab, Nanyang Technological University",
                "Shanghai AI Laboratory",
                "Wangxuan Institute of Computer Technology, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.12369.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#3d",
                    "#long_context"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "WorldMem: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¼Ğ¸Ñ€Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸",
                    "desc": "WorldMem - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¸Ñ€Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ°Ğ½ĞºĞ° Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ½ĞµĞµ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ñ‹Ğµ ÑÑ†ĞµĞ½Ñ‹ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑÑ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ° Ğ¸Ğ»Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚ĞºĞ°Ñ…. Ğ’ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğº Ğ² ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ¼Ğ¸Ñ€, Ğ½Ğ¾ Ğ¸ ĞµĞ³Ğ¾ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğµ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸."
                },
                "en": {
                    "title": "Enhancing World Simulation with Memory-Driven Consistency",
                    "desc": "This paper introduces WorldMem, a novel framework designed to improve world simulation by utilizing a memory bank that stores various memory frames and states. The framework addresses the challenge of maintaining long-term consistency in 3D spatial representations, which is often hindered by limited temporal context. By implementing a memory attention mechanism, WorldMem can retrieve relevant information from stored memory frames, allowing for accurate scene reconstruction despite changes in viewpoint or time. Additionally, the integration of timestamps enables the framework to model both static and dynamic aspects of the environment, enhancing interaction and perception in simulated worlds."
                },
                "zh": {
                    "title": "å¢å¼ºä¸–ç•Œæ¨¡æ‹Ÿçš„ä¸€è‡´æ€§ä¸åŠ¨æ€æ€§",
                    "desc": "ä¸–ç•Œæ¨¡æ‹Ÿå› å…¶å»ºæ¨¡è™šæ‹Ÿç¯å¢ƒå’Œé¢„æµ‹è¡Œä¸ºåæœçš„èƒ½åŠ›è€Œè¶Šæ¥è¶Šå—æ¬¢è¿ã€‚ç„¶è€Œï¼Œæœ‰é™çš„æ—¶é—´ä¸Šä¸‹æ–‡çª—å£å¸¸å¸¸å¯¼è‡´é•¿æœŸä¸€è‡´æ€§ç»´æŠ¤çš„å¤±è´¥ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¿æŒä¸‰ç»´ç©ºé—´ä¸€è‡´æ€§æ–¹é¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†WorldMemæ¡†æ¶ï¼Œé€šè¿‡ä¸€ä¸ªåŒ…å«è®°å¿†å•å…ƒçš„è®°å¿†åº“æ¥å¢å¼ºåœºæ™¯ç”Ÿæˆï¼Œè¿™äº›è®°å¿†å•å…ƒå­˜å‚¨è®°å¿†å¸§å’ŒçŠ¶æ€ï¼ˆä¾‹å¦‚ï¼Œå§¿åŠ¿å’Œæ—¶é—´æˆ³ï¼‰ã€‚é€šè¿‡é‡‡ç”¨è®°å¿†æ³¨æ„æœºåˆ¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿå‡†ç¡®é‡å»ºå…ˆå‰è§‚å¯Ÿåˆ°çš„åœºæ™¯ï¼Œå³ä½¿åœ¨æ˜¾è‘—çš„è§†è§’æˆ–æ—¶é—´é—´éš”ä¸‹ä¹Ÿèƒ½æœ‰æ•ˆæå–ç›¸å…³ä¿¡æ¯ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-04-17.html",
    "link_next": "2025-04-21.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "17.04",
        "en": "04/17",
        "zh": "4æœˆ17æ—¥"
    },
    "short_date_next": {
        "ru": "21.04",
        "en": "04/21",
        "zh": "4æœˆ21æ—¥"
    },
    "categories": {
        "#dataset": 2,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "This article discusses the role of color in how humans and vision-language models (VLMs) perceive and understand the world. It introduces ColorBench, a new benchmark for testing VLMs' color understanding skills. The study finds that while larger models perform better, current VLMs generally neglect color understanding. It also shows that using Chain of Thought (CoT) reasoning can improve color task accuracy. The authors hope ColorBench will help advance research in this area.",
        "title": "ColorBench: Can VLMs See and Understand the Colorful World? A\n  Comprehensive Benchmark for Color Perception, Reasoning, and Robustness",
        "pinyin": "Sure, here is the pinyin transcription for the text:\n\nZhÃ¨ wÃ©nzhÄng tÇolÃ¹n zhÃ¨ zhÇ’ng yÇnsÃ¨ zÃ i rÃ©nlÃ¨i hÃ© shÃ¬yÇn yÇ”yÃ¡n mÃ³xÃ­ng (VLMs) zhÄ«jiÃ n hÃ© lÇjiÄ› shÃ¬jiÃ¨ zhÅng de zuÃ²yÃ²ng. TÄ jiÃ¨shÃ o YÇnsÃ¨ BÄ›nch, yÄ«gÃ¨ xÄ«n de bÄ›nchmark yÇ cÃ¨shÃ¬ VLMs de yÇnsÃ¨ lÇjiÄ› jÃ¬nÃ©ng. YÃ¡njiÅ« fÄxiÃ n zhÇyÇ’u dÃ xÃ­ng mÃ³xÃ­ng biÇoxiÃ n gÃ¨ng hÇo, dÄngqiÃ¡n VLMs yÄ«bÄn shÅ« huÇng yÇnsÃ¨ lÇjiÄ›. TÄ yÄ› shuÅmÃ­ng yÃ²ng LiÃ¡n de SÄ«xiÇng (CoT) tuÇlÇ nÃ©ng gÇishÃ n yÇnsÃ¨ rÃ¨nwÃ¹ de zhÇ”nquÃ¨du. ZhÃ¨xiÄ“ zuÃ²zhÄ› xÄ«wÃ ng YÇnsÃ¨ BÄ›nch huÃ¬ bÄngzhÃ¹ tuÄ«jÃ¬n zhÃ¨ ge lÇngyÃ¹ de yÃ¡njiÅ«.\n\nPlease note that the pinyin transcription is based on the pronunciation of the Chinese characters that would be used to translate the English text. The actual translation might vary slightly depending on the context and specific word choices.",
        "vocab": "[\n    {\"word\": \"perceive\", \"pinyin\": \"pÉ™rËˆsiËv\", \"trans\": \"æ„ŸçŸ¥\"},\n    {\"word\": \"vision-language models\", \"pinyin\": \"ËˆvÉªÊ’É™n ËˆlÃ¦Å‹É¡wÉªdÊ’ mÉ’dÉ™lz\", \"trans\": \"è§†è§‰-è¯­è¨€æ¨¡å‹\"},\n    {\"word\": \"benchmark\", \"pinyin\": \"ËˆbÉ›nÊ§mÉ‘Ëk\", \"trans\": \"åŸºå‡†\"},\n    {\"word\": \"neglect\", \"pinyin\": \"nÉªËˆÉ¡lÉ›kt\", \"trans\": \"å¿½è§†\"},\n    {\"word\": \"Chain of Thought\", \"pinyin\": \"Ê§eÉªn É’v Î¸É”Ët\", \"trans\": \"æ€ç»´é“¾\"},\n    {\"word\": \"reasoning\", \"pinyin\": \"ËˆriËz(É™)nÉªÅ‹\", \"trans\": \"æ¨ç†\"},\n    {\"word\": \"accuracy\", \"pinyin\": \"ËˆÃ¦kjÉ™rÉ™si\", \"trans\": \"å‡†ç¡®æ€§\"},\n    {\"word\": \"advance\", \"pinyin\": \"Ã¦dËˆvÉ‘Ëns\", \"trans\": \"æ¨è¿›\"}\n]",
        "trans": "This article explores the role of color in how both humans and vision-language models (VLMs) perceive and understand the world. It introduces ColorBench, a new benchmark designed to evaluate VLMs' ability to understand color. The study reveals that while larger models tend to perform better, current VLMs generally overlook color understanding. Additionally, the research demonstrates that employing Chain of Thought (CoT) reasoning can enhance the accuracy of color-related tasks. The authors express hope that ColorBench will contribute to further advancements in this field of research.",
        "update_ts": "2025-04-17 09:12"
    }
}