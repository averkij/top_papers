{
    "date": {
        "ru": "18 апреля",
        "en": "April 18",
        "zh": "4月18日"
    },
    "time_utc": "2025-04-18 04:13",
    "weekday": 4,
    "issue_id": 3305,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.13146",
            "title": "Antidistillation Sampling",
            "url": "https://huggingface.co/papers/2504.13146",
            "abstract": "Frontier models that generate extended reasoning traces inadvertently produce rich token sequences that can facilitate model distillation. Recognizing this vulnerability, model owners may seek sampling strategies that limit the effectiveness of distillation without compromising model performance. Antidistillation sampling provides exactly this capability. By strategically modifying a model's next-token probability distribution, antidistillation sampling poisons reasoning traces, rendering them significantly less effective for distillation while preserving the model's practical utility. For further details, see https://antidistillation.com.",
            "score": 24,
            "issue_id": 3304,
            "pub_date": "2025-04-17",
            "pub_date_card": {
                "ru": "17 апреля",
                "en": "April 17",
                "zh": "4月17日"
            },
            "hash": "6aa117b5c441eb3d",
            "authors": [
                "Yash Savani",
                "Asher Trockman",
                "Zhili Feng",
                "Avi Schwarzschild",
                "Alexander Robey",
                "Marc Finzi",
                "J. Zico Kolter"
            ],
            "affiliations": [
                "Carnegie Mellon University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.13146.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "Защита моделей от дистилляции с помощью антидистилляции",
                    "desc": "В статье рассматривается проблема защиты моделей от дистилляции, когда модели генерируют сложные последовательности токенов. Авторы предлагают метод антидистилляционной выборки, который изменяет распределение вероятностей следующего токена. Это делает следы рассуждений менее полезными для дистилляции, сохраняя при этом производительность модели. Таким образом, можно защитить интеллектуальную собственность моделей, не ухудшая их работу."
                },
                "en": {
                    "title": "Protecting Model Knowledge with Antidistillation Sampling",
                    "desc": "This paper discusses a method called antidistillation sampling, which aims to protect advanced models from being easily distilled into simpler versions. Distillation is a process where a complex model's knowledge is transferred to a smaller model, but this can be exploited if the complex model generates detailed reasoning traces. Antidistillation sampling works by altering the probability distribution of the next token generated by the model, making the reasoning less useful for distillation. This approach allows the model to maintain its performance while reducing the risk of its knowledge being easily extracted."
                },
                "zh": {
                    "title": "抗蒸馏采样：保护模型性能的创新策略",
                    "desc": "前沿模型生成的推理轨迹会产生丰富的标记序列，这些序列可以帮助模型蒸馏。为了应对这一漏洞，模型拥有者可能会寻找采样策略，以限制蒸馏的有效性，同时不影响模型性能。抗蒸馏采样正是提供这种能力的策略。通过战略性地修改模型的下一个标记概率分布，抗蒸馏采样可以破坏推理轨迹，使其在蒸馏中变得不那么有效，同时保持模型的实际效用。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.13122",
            "title": "VistaDPO: Video Hierarchical Spatial-Temporal Direct Preference\n  Optimization for Large Video Models",
            "url": "https://huggingface.co/papers/2504.13122",
            "abstract": "Large Video Models (LVMs) built upon Large Language Models (LLMs) have shown promise in video understanding but often suffer from misalignment with human intuition and video hallucination issues. To address these challenges, we introduce VistaDPO, a novel framework for Video Hierarchical Spatial-Temporal Direct Preference Optimization. VistaDPO enhances text-video preference alignment across three hierarchical levels: i) Instance Level, aligning overall video content with responses; ii) Temporal Level, aligning video temporal semantics with event descriptions; and iii) Perceptive Level, aligning spatial objects with language tokens. Given the lack of datasets for fine-grained video-language preference alignment, we construct VistaDPO-7k, a dataset of 7.2K QA pairs annotated with chosen and rejected responses, along with spatial-temporal grounding information such as timestamps, keyframes, and bounding boxes. Extensive experiments on benchmarks such as Video Hallucination, Video QA, and Captioning performance tasks demonstrate that VistaDPO significantly improves the performance of existing LVMs, effectively mitigating video-language misalignment and hallucination. The code and data are available at https://github.com/HaroldChen19/VistaDPO.",
            "score": 9,
            "issue_id": 3303,
            "pub_date": "2025-04-17",
            "pub_date_card": {
                "ru": "17 апреля",
                "en": "April 17",
                "zh": "4月17日"
            },
            "hash": "efbc3b240498ce70",
            "authors": [
                "Haojian Huang",
                "Haodong Chen",
                "Shengqiong Wu",
                "Meng Luo",
                "Jinlan Fu",
                "Xinya Du",
                "Hanwang Zhang",
                "Hao Fei"
            ],
            "affiliations": [
                "Nanyang Technological University",
                "National University of Singapore",
                "The Hong Kong University of Science and Technology",
                "The University of Hong Kong",
                "University of Texas at Dallas"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.13122.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#alignment",
                    "#hallucinations",
                    "#dataset"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "VistaDPO: Точное согласование видео и языка на всех уровнях",
                    "desc": "VistaDPO - это новая система для оптимизации предпочтений в видео с использованием иерархического пространственно-временного подхода. Она улучшает согласование текста и видео на трех уровнях: общем содержании, временной семантике и пространственных объектах. Авторы создали датасет VistaDPO-7k с 7200 парами вопросов-ответов, аннотированными предпочтительными и отвергнутыми ответами, а также временными метками и ограничивающими рамками. Эксперименты показали, что VistaDPO значительно улучшает работу существующих больших видеомоделей, эффективно снижая рассогласование видео и языка и галлюцинации."
                },
                "en": {
                    "title": "Aligning Video and Language: Introducing VistaDPO",
                    "desc": "This paper presents VistaDPO, a new framework designed to improve the alignment between video content and human language understanding in Large Video Models (LVMs). It operates on three hierarchical levels: aligning overall video content with user responses, matching temporal semantics of videos with event descriptions, and correlating spatial objects with language tokens. To support this framework, the authors created a dataset called VistaDPO-7k, which includes 7.2K question-answer pairs with detailed annotations for better preference alignment. The results show that VistaDPO enhances the performance of LVMs by reducing issues related to video hallucination and misalignment with human intuition."
                },
                "zh": {
                    "title": "VistaDPO：提升视频理解的偏好对齐",
                    "desc": "本文介绍了一种新框架VistaDPO，用于视频层次空间-时间直接偏好优化，旨在解决大型视频模型（LVMs）在视频理解中的人类直觉不一致和视频幻觉问题。VistaDPO通过三个层次增强文本-视频偏好对齐：实例层、时间层和感知层，分别对齐视频内容、时间语义和空间对象。为了支持细粒度视频-语言偏好对齐，研究团队构建了VistaDPO-7k数据集，包含7200个问答对及其空间-时间信息。实验结果表明，VistaDPO显著提升了现有LVMs的性能，有效减轻了视频-语言的不一致性和幻觉现象。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.13169",
            "title": "Generate, but Verify: Reducing Hallucination in Vision-Language Models\n  with Retrospective Resampling",
            "url": "https://huggingface.co/papers/2504.13169",
            "abstract": "Vision-Language Models (VLMs) excel at visual understanding but often suffer from visual hallucinations, where they generate descriptions of nonexistent objects, actions, or concepts, posing significant risks in safety-critical applications. Existing hallucination mitigation methods typically follow one of two paradigms: generation adjustment, which modifies decoding behavior to align text with visual inputs, and post-hoc verification, where external models assess and correct outputs. While effective, generation adjustment methods often rely on heuristics and lack correction mechanisms, while post-hoc verification is complicated, typically requiring multiple models and tending to reject outputs rather than refine them. In this work, we introduce REVERSE, a unified framework that integrates hallucination-aware training with on-the-fly self-verification. By leveraging a new hallucination-verification dataset containing over 1.3M semi-synthetic samples, along with a novel inference-time retrospective resampling technique, our approach enables VLMs to both detect hallucinations during generation and dynamically revise those hallucinations. Our evaluations show that REVERSE achieves state-of-the-art hallucination reduction, outperforming the best existing methods by up to 12% on CHAIR-MSCOCO and 28% on HaloQuest. Our dataset, model, and code are available at: https://reverse-vlm.github.io.",
            "score": 8,
            "issue_id": 3304,
            "pub_date": "2025-04-17",
            "pub_date_card": {
                "ru": "17 апреля",
                "en": "April 17",
                "zh": "4月17日"
            },
            "hash": "f1ebb64cfce24e47",
            "authors": [
                "Tsung-Han Wu",
                "Heekyung Lee",
                "Jiaxin Ge",
                "Joseph E. Gonzalez",
                "Trevor Darrell",
                "David M. Chan"
            ],
            "affiliations": [
                "POSTECH",
                "University of California, Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.13169.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#inference",
                    "#hallucinations",
                    "#data",
                    "#cv"
                ],
                "emoji": "👁️",
                "ru": {
                    "title": "REVERSE: самокорректирующиеся VLM без галлюцинаций",
                    "desc": "Исследователи представили REVERSE - новый подход к снижению визуальных галлюцинаций в моделях компьютерного зрения и обработки естественного языка (VLM). Метод объединяет обучение с учетом галлюцинаций и самопроверку в режиме реального времени. REVERSE использует новый датасет из 1,3 млн полусинтетических образцов и технику ретроспективной выборки во время вывода. Эксперименты показали, что REVERSE превосходит существующие методы на 12-28% по снижению галлюцинаций на стандартных бенчмарках."
                },
                "en": {
                    "title": "REVERSE: Correcting Visual Hallucinations in VLMs Dynamically",
                    "desc": "This paper addresses the issue of visual hallucinations in Vision-Language Models (VLMs), where models incorrectly describe non-existent elements. The authors propose a new framework called REVERSE, which combines hallucination-aware training with real-time self-verification to improve the accuracy of VLM outputs. By utilizing a large dataset of semi-synthetic samples, REVERSE can identify and correct hallucinations during the generation process. The results demonstrate that this approach significantly reduces hallucinations, outperforming existing methods in various benchmarks."
                },
                "zh": {
                    "title": "REVERSE：动态修正视觉幻觉的统一框架",
                    "desc": "视觉语言模型（VLMs）在视觉理解方面表现出色，但常常出现视觉幻觉，即生成不存在的物体、动作或概念的描述，这在安全关键应用中带来了重大风险。现有的幻觉缓解方法通常分为两类：生成调整和后期验证。生成调整方法依赖启发式规则，缺乏有效的修正机制，而后期验证则复杂，通常需要多个模型，并倾向于拒绝输出而不是进行修正。我们提出的REVERSE框架结合了幻觉感知训练和实时自我验证，能够在生成过程中检测幻觉并动态修正，从而显著降低幻觉的发生。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.12626",
            "title": "Packing Input Frame Context in Next-Frame Prediction Models for Video\n  Generation",
            "url": "https://huggingface.co/papers/2504.12626",
            "abstract": "We present a neural network structure, FramePack, to train next-frame (or next-frame-section) prediction models for video generation. The FramePack compresses input frames to make the transformer context length a fixed number regardless of the video length. As a result, we are able to process a large number of frames using video diffusion with computation bottleneck similar to image diffusion. This also makes the training video batch sizes significantly higher (batch sizes become comparable to image diffusion training). We also propose an anti-drifting sampling method that generates frames in inverted temporal order with early-established endpoints to avoid exposure bias (error accumulation over iterations). Finally, we show that existing video diffusion models can be finetuned with FramePack, and their visual quality may be improved because the next-frame prediction supports more balanced diffusion schedulers with less extreme flow shift timesteps.",
            "score": 8,
            "issue_id": 3303,
            "pub_date": "2025-04-17",
            "pub_date_card": {
                "ru": "17 апреля",
                "en": "April 17",
                "zh": "4月17日"
            },
            "hash": "fd1688a4e26dbb32",
            "authors": [
                "Lvmin Zhang",
                "Maneesh Agrawala"
            ],
            "affiliations": [
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.12626.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#diffusion",
                    "#video",
                    "#training"
                ],
                "emoji": "🎞️",
                "ru": {
                    "title": "FramePack: эффективное предсказание кадров для генерации видео",
                    "desc": "FramePack - это новая структура нейронной сети для предсказания следующего кадра в видео. Она сжимает входные кадры, что позволяет обрабатывать большое количество кадров с вычислительной сложностью, сравнимой с диффузией изображений. Авторы также предлагают метод сэмплирования, предотвращающий накопление ошибок. FramePack может быть использован для дообучения существующих моделей видеодиффузии, потенциально улучшая их визуальное качество."
                },
                "en": {
                    "title": "FramePack: Efficient Video Generation with Next-Frame Prediction",
                    "desc": "The paper introduces FramePack, a novel neural network architecture designed for predicting the next frame in video generation. By compressing input frames, FramePack ensures that the transformer can handle a fixed context length, making it efficient for processing long videos. This approach allows for larger training batch sizes, similar to those used in image diffusion, while maintaining computational efficiency. Additionally, the authors present an anti-drifting sampling method to mitigate exposure bias, enhancing the quality of generated frames and improving existing video diffusion models through fine-tuning."
                },
                "zh": {
                    "title": "FramePack：提升视频生成的下一帧预测能力",
                    "desc": "我们提出了一种神经网络结构，FramePack，用于训练视频生成的下一帧预测模型。FramePack通过压缩输入帧，使得变换器的上下文长度固定，无论视频长度如何。这样，我们能够使用与图像扩散相似的计算瓶颈处理大量帧，从而显著提高视频训练的批量大小。我们还提出了一种反漂移采样方法，以避免迭代过程中的曝光偏差，从而提高生成帧的质量。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.12369",
            "title": "WORLDMEM: Long-term Consistent World Simulation with Memory",
            "url": "https://huggingface.co/papers/2504.12369",
            "abstract": "World simulation has gained increasing popularity due to its ability to model virtual environments and predict the consequences of actions. However, the limited temporal context window often leads to failures in maintaining long-term consistency, particularly in preserving 3D spatial consistency. In this work, we present WorldMem, a framework that enhances scene generation with a memory bank consisting of memory units that store memory frames and states (e.g., poses and timestamps). By employing a memory attention mechanism that effectively extracts relevant information from these memory frames based on their states, our method is capable of accurately reconstructing previously observed scenes, even under significant viewpoint or temporal gaps. Furthermore, by incorporating timestamps into the states, our framework not only models a static world but also captures its dynamic evolution over time, enabling both perception and interaction within the simulated world. Extensive experiments in both virtual and real scenarios validate the effectiveness of our approach.",
            "score": 6,
            "issue_id": 3303,
            "pub_date": "2025-04-16",
            "pub_date_card": {
                "ru": "16 апреля",
                "en": "April 16",
                "zh": "4月16日"
            },
            "hash": "79cf162a1b60f887",
            "authors": [
                "Zeqi Xiao",
                "Yushi Lan",
                "Yifan Zhou",
                "Wenqi Ouyang",
                "Shuai Yang",
                "Yanhong Zeng",
                "Xingang Pan"
            ],
            "affiliations": [
                "S-Lab, Nanyang Technological University",
                "Shanghai AI Laboratory",
                "Wangxuan Institute of Computer Technology, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.12369.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#3d",
                    "#long_context"
                ],
                "emoji": "🌐",
                "ru": {
                    "title": "WorldMem: Улучшение долгосрочной согласованности в симуляции миров с помощью памяти",
                    "desc": "WorldMem - это фреймворк для улучшения симуляции виртуальных миров с помощью банка памяти. Он использует механизм внимания для извлечения релевантной информации из кадров памяти на основе их состояний. Это позволяет точно реконструировать ранее наблюдаемые сцены даже при значительных изменениях ракурса или временных промежутках. Включение временных меток в состояния позволяет моделировать не только статичный мир, но и его динамическое развитие во времени."
                },
                "en": {
                    "title": "Enhancing World Simulation with Memory-Driven Consistency",
                    "desc": "This paper introduces WorldMem, a novel framework designed to improve world simulation by utilizing a memory bank that stores various memory frames and states. The framework addresses the challenge of maintaining long-term consistency in 3D spatial representations, which is often hindered by limited temporal context. By implementing a memory attention mechanism, WorldMem can retrieve relevant information from stored memory frames, allowing for accurate scene reconstruction despite changes in viewpoint or time. Additionally, the integration of timestamps enables the framework to model both static and dynamic aspects of the environment, enhancing interaction and perception in simulated worlds."
                },
                "zh": {
                    "title": "增强世界模拟的一致性与动态性",
                    "desc": "世界模拟因其建模虚拟环境和预测行为后果的能力而越来越受欢迎。然而，有限的时间上下文窗口常常导致长期一致性维护的失败，特别是在保持三维空间一致性方面。在这项工作中，我们提出了WorldMem框架，通过一个包含记忆单元的记忆库来增强场景生成，这些记忆单元存储记忆帧和状态（例如，姿势和时间戳）。通过采用记忆注意机制，我们的方法能够准确重建先前观察到的场景，即使在显著的视角或时间间隔下也能有效提取相关信息。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.05506",
            "title": "ChartQAPro: A More Diverse and Challenging Benchmark for Chart Question\n  Answering",
            "url": "https://huggingface.co/papers/2504.05506",
            "abstract": "Charts are ubiquitous, as people often use them to analyze data, answer questions, and discover critical insights. However, performing complex analytical tasks with charts requires significant perceptual and cognitive effort. Chart Question Answering (CQA) systems automate this process by enabling models to interpret and reason with visual representations of data. However, existing benchmarks like ChartQA lack real-world diversity and have recently shown performance saturation with modern large vision-language models (LVLMs). To address these limitations, we introduce ChartQAPro, a new benchmark that includes 1,341 charts from 157 diverse sources, spanning various chart types, including infographics and dashboards, and featuring 1,948 questions in various types, such as multiple-choice, conversational, hypothetical, and unanswerable questions, to better reflect real-world challenges. Our evaluations with 21 models show a substantial performance drop for LVLMs on ChartQAPro; e.g., Claude Sonnet 3.5 scores 90.5% on ChartQA but only 55.81% on ChartQAPro, underscoring the complexity of chart reasoning. We complement our findings with detailed error analyses and ablation studies, identifying key challenges and opportunities for advancing LVLMs in chart understanding and reasoning. We release ChartQAPro at https://github.com/vis-nlp/ChartQAPro.",
            "score": 4,
            "issue_id": 3303,
            "pub_date": "2025-04-07",
            "pub_date_card": {
                "ru": "7 апреля",
                "en": "April 7",
                "zh": "4月7日"
            },
            "hash": "a727d08eac22e920",
            "authors": [
                "Ahmed Masry",
                "Mohammed Saidul Islam",
                "Mahir Ahmed",
                "Aayush Bajaj",
                "Firoz Kabir",
                "Aaryaman Kartha",
                "Md Tahmid Rahman Laskar",
                "Mizanur Rahman",
                "Shadikur Rahman",
                "Mehrad Shahmohammadi",
                "Megh Thakkar",
                "Md Rizwan Parvez",
                "Enamul Hoque",
                "Shafiq Joty"
            ],
            "affiliations": [
                "Dialpad Inc., Canada",
                "MILA - Quebec AI Institute, Canada",
                "Nanyang Technological University, Singapore",
                "Qatar Computing Research Institute (QCRI)",
                "RBC, Canada",
                "Salesforce Research, USA",
                "York University, Canada"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.05506.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#interpretability",
                    "#cv",
                    "#dataset",
                    "#reasoning"
                ],
                "emoji": "📊",
                "ru": {
                    "title": "ChartQAPro: новый вызов для ИИ в понимании графиков",
                    "desc": "ChartQAPro - новый набор данных для задачи ответов на вопросы по графикам. Он включает более 1300 разнообразных графиков из реальных источников и около 2000 вопросов различных типов. Тестирование показало, что современные мультимодальные языковые модели значительно хуже справляются с ChartQAPro по сравнению с предыдущими наборами данных. Авторы провели детальный анализ ошибок и выявили ключевые проблемы в понимании и рассуждении о графиках для языковых моделей."
                },
                "en": {
                    "title": "ChartQAPro: Elevating Chart Understanding for AI",
                    "desc": "This paper introduces ChartQAPro, a new benchmark designed to improve Chart Question Answering (CQA) systems by providing a more diverse and realistic set of charts and questions. Unlike previous benchmarks, ChartQAPro includes 1,341 charts from 157 sources and features various question types, which better reflect the complexities of real-world data analysis. The study reveals that modern large vision-language models (LVLMs) struggle significantly with this new benchmark, demonstrating a performance drop from 90.5% on the previous ChartQA to only 55.81% on ChartQAPro. Through error analyses and ablation studies, the authors identify challenges in chart reasoning that can guide future improvements in LVLMs."
                },
                "zh": {
                    "title": "提升图表问答系统的挑战与机遇",
                    "desc": "本论文介绍了ChartQAPro，这是一个新的基准测试，旨在提高图表问答系统的性能。它包含来自157个不同来源的1,341个图表，涵盖多种图表类型，并提供1,948个多样化的问题。通过对21个模型的评估，我们发现现代大型视觉语言模型在ChartQAPro上的表现显著下降，显示出图表推理的复杂性。我们的研究还包括详细的错误分析和消融研究，以识别图表理解和推理中的关键挑战和机遇。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.13079",
            "title": "Retrieval-Augmented Generation with Conflicting Evidence",
            "url": "https://huggingface.co/papers/2504.13079",
            "abstract": "Large language model (LLM) agents are increasingly employing retrieval-augmented generation (RAG) to improve the factuality of their responses. However, in practice, these systems often need to handle ambiguous user queries and potentially conflicting information from multiple sources while also suppressing inaccurate information from noisy or irrelevant documents. Prior work has generally studied and addressed these challenges in isolation, considering only one aspect at a time, such as handling ambiguity or robustness to noise and misinformation. We instead consider multiple factors simultaneously, proposing (i) RAMDocs (Retrieval with Ambiguity and Misinformation in Documents), a new dataset that simulates complex and realistic scenarios for conflicting evidence for a user query, including ambiguity, misinformation, and noise; and (ii) MADAM-RAG, a multi-agent approach in which LLM agents debate over the merits of an answer over multiple rounds, allowing an aggregator to collate responses corresponding to disambiguated entities while discarding misinformation and noise, thereby handling diverse sources of conflict jointly. We demonstrate the effectiveness of MADAM-RAG using both closed and open-source models on AmbigDocs -- which requires presenting all valid answers for ambiguous queries -- improving over strong RAG baselines by up to 11.40% and on FaithEval -- which requires suppressing misinformation -- where we improve by up to 15.80% (absolute) with Llama3.3-70B-Instruct. Furthermore, we find that RAMDocs poses a challenge for existing RAG baselines (Llama3.3-70B-Instruct only obtains 32.60 exact match score). While MADAM-RAG begins to address these conflicting factors, our analysis indicates that a substantial gap remains especially when increasing the level of imbalance in supporting evidence and misinformation.",
            "score": 3,
            "issue_id": 3304,
            "pub_date": "2025-04-17",
            "pub_date_card": {
                "ru": "17 апреля",
                "en": "April 17",
                "zh": "4月17日"
            },
            "hash": "13305db862567e7f",
            "authors": [
                "Han Wang",
                "Archiki Prasad",
                "Elias Stengel-Eskin",
                "Mohit Bansal"
            ],
            "affiliations": [
                "University of North Carolina at Chapel Hill"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.13079.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#dataset",
                    "#rag",
                    "#optimization",
                    "#agents",
                    "#hallucinations"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Многоагентный подход для борьбы с неоднозначностью и дезинформацией в RAG-системах",
                    "desc": "Статья представляет новый подход к решению проблем неоднозначности и дезинформации в системах генерации текста с использованием извлечения информации (RAG). Авторы предлагают датасет RAMDocs, моделирующий сложные сценарии с противоречивыми данными, и метод MADAM-RAG, использующий несколько агентов на основе больших языковых моделей для обсуждения ответов. MADAM-RAG показывает улучшение результатов на 11.40% на датасете AmbigDocs и на 15.80% на FaithEval по сравнению с базовыми методами RAG. Однако, несмотря на прогресс, остаются значительные проблемы, особенно при увеличении дисбаланса между поддерживающими и дезинформирующими данными."
                },
                "en": {
                    "title": "Enhancing LLM Accuracy through Debate and Retrieval",
                    "desc": "This paper discusses the challenges faced by large language model (LLM) agents when using retrieval-augmented generation (RAG) to provide accurate responses to user queries. It introduces RAMDocs, a new dataset designed to simulate complex scenarios involving ambiguity, misinformation, and noise in documents. The authors propose MADAM-RAG, a multi-agent system where LLMs debate answers over multiple rounds, allowing for better aggregation of responses while filtering out inaccuracies. The results show that MADAM-RAG significantly improves performance on tasks requiring disambiguation and misinformation suppression compared to traditional RAG methods, although challenges remain in handling conflicting evidence."
                },
                "zh": {
                    "title": "多代理辩论：提升语言模型的准确性与鲁棒性",
                    "desc": "大型语言模型（LLM）代理越来越多地使用检索增强生成（RAG）来提高回答的准确性。然而，这些系统在处理模糊用户查询和来自多个来源的潜在冲突信息时，常常需要抑制来自嘈杂或无关文档的不准确信息。本文提出了RAMDocs数据集，模拟了复杂的用户查询场景，并提出了MADAM-RAG多代理方法，通过多轮辩论来处理答案的优劣，从而有效整合不同来源的响应。实验结果表明，MADAM-RAG在处理模糊查询和抑制错误信息方面显著优于现有的RAG基线。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.13145",
            "title": "Exploring Expert Failures Improves LLM Agent Tuning",
            "url": "https://huggingface.co/papers/2504.13145",
            "abstract": "Large Language Models (LLMs) have shown tremendous potential as agents, excelling at tasks that require multiple rounds of reasoning and interactions. Rejection Sampling Fine-Tuning (RFT) has emerged as an effective method for finetuning LLMs as agents: it first imitates expert-generated successful trajectories and further improves agentic skills through iterative fine-tuning on successful, self-generated trajectories. However, since the expert (e.g., GPT-4) succeeds primarily on simpler subtasks and RFT inherently favors simpler scenarios, many complex subtasks remain unsolved and persistently out-of-distribution (OOD). Upon investigating these challenging subtasks, we discovered that previously failed expert trajectories can often provide valuable guidance, e.g., plans and key actions, that can significantly improve agent exploration efficiency and acquisition of critical skills. Motivated by these observations, we propose Exploring Expert Failures (EEF), which identifies beneficial actions from failed expert trajectories and integrates them into the training dataset. Potentially harmful actions are meticulously excluded to prevent contamination of the model learning process. By leveraging the beneficial actions in expert failures, EEF successfully solves some previously unsolvable subtasks and improves agent tuning performance. Remarkably, our approach achieved a 62\\% win rate in WebShop, outperforming RFT (53. 6\\%) and GPT-4 (35. 6\\%), and to the best of our knowledge, setting a new state-of-the-art as the first method to surpass a score of 0.81 in WebShop and exceed 81 in SciWorld.",
            "score": 2,
            "issue_id": 3305,
            "pub_date": "2025-04-17",
            "pub_date_card": {
                "ru": "17 апреля",
                "en": "April 17",
                "zh": "4月17日"
            },
            "hash": "597cd9806c8a07ff",
            "authors": [
                "Li-Cheng Lan",
                "Andrew Bai",
                "Minhao Cheng",
                "Ruochen Wang",
                "Cho-Jui Hsieh",
                "Tianyi Zhou"
            ],
            "affiliations": [
                "OpenAI",
                "Pennsylvania State University",
                "UCLA",
                "University of Maryland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.13145.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#agents",
                    "#training",
                    "#optimization"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Учимся на ошибках: новый метод обучения ИИ-агентов",
                    "desc": "Статья представляет новый метод обучения больших языковых моделей (LLM) для выполнения сложных задач. Метод называется Exploring Expert Failures (EEF) и использует информацию из неудачных попыток экспертной модели для улучшения процесса обучения. EEF превзошел предыдущие методы, такие как Rejection Sampling Fine-Tuning (RFT), в задачах WebShop и SciWorld. Авторы показывают, что использование полезных действий из неудачных экспертных траекторий может значительно улучшить эффективность исследования и приобретение критических навыков агентом."
                },
                "en": {
                    "title": "Learning from Mistakes: Enhancing LLMs with Expert Failures",
                    "desc": "This paper introduces Exploring Expert Failures (EEF), a novel approach to enhance the performance of Large Language Models (LLMs) in complex tasks. EEF builds on Rejection Sampling Fine-Tuning (RFT) by utilizing insights from previously failed expert trajectories to identify beneficial actions that can improve agent exploration and skill acquisition. By integrating these valuable actions into the training dataset while excluding harmful ones, EEF addresses previously unsolvable subtasks and boosts overall agent performance. The results demonstrate that EEF outperforms existing methods, achieving a 62% win rate in WebShop and setting new benchmarks in task performance."
                },
                "zh": {
                    "title": "从失败中学习，提升智能体能力",
                    "desc": "大型语言模型（LLMs）在多轮推理和交互任务中表现出色。拒绝采样微调（RFT）是一种有效的微调方法，通过模仿专家生成的成功轨迹并在自生成的成功轨迹上进行迭代微调来提升模型的能力。然而，由于RFT偏向于简单场景，许多复杂子任务仍然未能解决。我们提出的探索专家失败（EEF）方法，通过从失败的专家轨迹中提取有益的行动，显著提高了模型的探索效率和关键技能的获取。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.12782",
            "title": "Set You Straight: Auto-Steering Denoising Trajectories to Sidestep\n  Unwanted Concepts",
            "url": "https://huggingface.co/papers/2504.12782",
            "abstract": "Ensuring the ethical deployment of text-to-image models requires effective techniques to prevent the generation of harmful or inappropriate content. While concept erasure methods offer a promising solution, existing finetuning-based approaches suffer from notable limitations. Anchor-free methods risk disrupting sampling trajectories, leading to visual artifacts, while anchor-based methods rely on the heuristic selection of anchor concepts. To overcome these shortcomings, we introduce a finetuning framework, dubbed ANT, which Automatically guides deNoising Trajectories to avoid unwanted concepts. ANT is built on a key insight: reversing the condition direction of classifier-free guidance during mid-to-late denoising stages enables precise content modification without sacrificing early-stage structural integrity. This inspires a trajectory-aware objective that preserves the integrity of the early-stage score function field, which steers samples toward the natural image manifold, without relying on heuristic anchor concept selection. For single-concept erasure, we propose an augmentation-enhanced weight saliency map to precisely identify the critical parameters that most significantly contribute to the unwanted concept, enabling more thorough and efficient erasure. For multi-concept erasure, our objective function offers a versatile plug-and-play solution that significantly boosts performance. Extensive experiments demonstrate that ANT achieves state-of-the-art results in both single and multi-concept erasure, delivering high-quality, safe outputs without compromising the generative fidelity. Code is available at https://github.com/lileyang1210/ANT",
            "score": 1,
            "issue_id": 3305,
            "pub_date": "2025-04-17",
            "pub_date_card": {
                "ru": "17 апреля",
                "en": "April 17",
                "zh": "4月17日"
            },
            "hash": "6c74497f6fd8b96b",
            "authors": [
                "Leyang Li",
                "Shilin Lu",
                "Yan Ren",
                "Adams Wai-Kin Kong"
            ],
            "affiliations": [
                "Nanyang Technological University, Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.12782.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#data",
                    "#training",
                    "#optimization",
                    "#ethics"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "ANT: Точное удаление нежелательных концепций из генеративных моделей изображений",
                    "desc": "Статья представляет новый метод ANT для удаления нежелательных концепций из генеративных моделей изображений. ANT использует обращение направления условной генерации на средних и поздних этапах шумоподавления для точной модификации контента. Метод сохраняет целостность ранних этапов генерации, не полагаясь на эвристический выбор якорных концепций. ANT показывает отличные результаты как для удаления одиночных, так и множественных концепций, обеспечивая высокое качество и безопасность сгенерированных изображений."
                },
                "en": {
                    "title": "ANT: Ethical Image Generation Through Smart Concept Erasure",
                    "desc": "This paper presents ANT, a new finetuning framework designed to improve the ethical deployment of text-to-image models by effectively erasing harmful or inappropriate content. ANT addresses the limitations of existing methods by guiding denoising trajectories to avoid unwanted concepts without disrupting the image quality. It introduces a trajectory-aware objective that maintains the integrity of early-stage image features while allowing for precise content modification. The framework also includes an innovative weight saliency map for identifying critical parameters in single-concept erasure and offers a versatile solution for multi-concept erasure, achieving state-of-the-art results in generating safe and high-quality images."
                },
                "zh": {
                    "title": "ANT：高效去除不当内容的文本到图像模型框架",
                    "desc": "本文提出了一种名为ANT的微调框架，用于确保文本到图像模型的伦理部署，特别是防止生成有害或不当内容。ANT通过自动引导去噪轨迹，避免了现有方法中的一些局限性，如锚点方法的启发式选择和无锚点方法导致的视觉伪影。该框架利用了在去噪中后期反转分类器无指导的条件方向的关键见解，从而实现了精确的内容修改，同时保持了早期阶段的结构完整性。通过增强的权重显著性图，ANT能够有效识别并去除单一或多个不当概念，实验结果表明其在去除效果和生成质量上均达到了最先进的水平。"
                }
            }
        }
    ],
    "link_prev": "2025-04-17.html",
    "link_next": "2025-04-21.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "17.04",
        "en": "04/17",
        "zh": "4月17日"
    },
    "short_date_next": {
        "ru": "21.04",
        "en": "04/21",
        "zh": "4月21日"
    },
    "categories": {
        "#dataset": 4,
        "#data": 2,
        "#benchmark": 2,
        "#agents": 2,
        "#cv": 3,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 1,
        "#3d": 1,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 2,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 5,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 3,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "This article discusses the role of color in how humans and vision-language models (VLMs) perceive and understand the world. It introduces ColorBench, a new benchmark for testing VLMs' color understanding skills. The study finds that while larger models perform better, current VLMs generally neglect color understanding. It also shows that using Chain of Thought (CoT) reasoning can improve color task accuracy. The authors hope ColorBench will help advance research in this area.",
        "title": "ColorBench: Can VLMs See and Understand the Colorful World? A\n  Comprehensive Benchmark for Color Perception, Reasoning, and Robustness",
        "pinyin": "Sure, here is the pinyin transcription for the text:\n\nZhè wénzhāng tǎolùn zhè zhǒng yǎnsè zài rénlèi hé shìyǎn yǔyán móxíng (VLMs) zhījiàn hé lǐjiě shìjiè zhōng de zuòyòng. Tā jièshào Yǎnsè Běnch, yīgè xīn de běnchmark yǐ cèshì VLMs de yǎnsè lǐjiě jìnéng. Yánjiū fāxiàn zhǐyǒu dàxíng móxíng biǎoxiàn gèng hǎo, dāngqián VLMs yībān shū huǎng yǎnsè lǐjiě. Tā yě shuōmíng yòng Lián de Sīxiǎng (CoT) tuǐlǐ néng gǎishàn yǎnsè rènwù de zhǔnquèdu. Zhèxiē zuòzhě xīwàng Yǎnsè Běnch huì bāngzhù tuījìn zhè ge lǐngyù de yánjiū.\n\nPlease note that the pinyin transcription is based on the pronunciation of the Chinese characters that would be used to translate the English text. The actual translation might vary slightly depending on the context and specific word choices.",
        "vocab": "[\n    {\"word\": \"perceive\", \"pinyin\": \"pərˈsiːv\", \"trans\": \"感知\"},\n    {\"word\": \"vision-language models\", \"pinyin\": \"ˈvɪʒən ˈlæŋɡwɪdʒ mɒdəlz\", \"trans\": \"视觉-语言模型\"},\n    {\"word\": \"benchmark\", \"pinyin\": \"ˈbɛnʧmɑːk\", \"trans\": \"基准\"},\n    {\"word\": \"neglect\", \"pinyin\": \"nɪˈɡlɛkt\", \"trans\": \"忽视\"},\n    {\"word\": \"Chain of Thought\", \"pinyin\": \"ʧeɪn ɒv θɔːt\", \"trans\": \"思维链\"},\n    {\"word\": \"reasoning\", \"pinyin\": \"ˈriːz(ə)nɪŋ\", \"trans\": \"推理\"},\n    {\"word\": \"accuracy\", \"pinyin\": \"ˈækjərəsi\", \"trans\": \"准确性\"},\n    {\"word\": \"advance\", \"pinyin\": \"ædˈvɑːns\", \"trans\": \"推进\"}\n]",
        "trans": "This article explores the role of color in how both humans and vision-language models (VLMs) perceive and understand the world. It introduces ColorBench, a new benchmark designed to evaluate VLMs' ability to understand color. The study reveals that while larger models tend to perform better, current VLMs generally overlook color understanding. Additionally, the research demonstrates that employing Chain of Thought (CoT) reasoning can enhance the accuracy of color-related tasks. The authors express hope that ColorBench will contribute to further advancements in this field of research.",
        "update_ts": "2025-04-17 09:12"
    }
}