
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 12 papers. January 28.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["–º–∏–Ω—É—Ç—É", "–º–∏–Ω—É—Ç—ã", "–º–∏–Ω—É—Ç"],
                hour: ["—á–∞—Å", "—á–∞—Å–∞", "—á–∞—Å–æ–≤"],
                day: ["–¥–µ–Ω—å", "–¥–Ω—è", "–¥–Ω–µ–π"],
                justNow: "—Ç–æ–ª—å–∫–æ —á—Ç–æ",
                ago: "–Ω–∞–∑–∞–¥"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["ÂàÜÈíü", "ÂàÜÈíü", "ÂàÜÈíü"],
                hour: ["Â∞èÊó∂", "Â∞èÊó∂", "Â∞èÊó∂"],
                day: ["Â§©", "Â§©", "Â§©"],
                justNow: "ÂàöÂàö",
                ago: "Ââç"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "—Å—Ç–∞—Ç–µ–π";
            } else if (lastDigit === 1) {
                word = "—Å—Ç–∞—Ç—å—è";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "—Å—Ç–∞—Ç—å–∏";
            } else {
                word = "—Å—Ç–∞—Ç–µ–π";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ÁØáËÆ∫Êñá"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">üî∫</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">28 —è–Ω–≤–∞—Ä—è</span> | <span id="title-articles-count">12 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-01-27.html">‚¨ÖÔ∏è <span id="prev-date">27.01</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-01-29.html">‚û°Ô∏è <span id="next-date">29.01</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-01.html">üìà <span id='top-month-label'>–ú–µ—Å—è—Ü</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">üîÄ <span id="sort-label-text">–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">—Ä–µ–π—Ç–∏–Ω–≥—É</option>
                    <option value="pub_date">–¥–∞—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏</option>
                    <option value="issue_id">–¥–æ–±–∞–≤–ª–µ–Ω–∏—é –Ω–∞ HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">üè∑Ô∏è –§–∏–ª—å—Ç—Ä</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A‚à™B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A‚à©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">üßπ</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ‚úñÔ∏è <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '28 —è–Ω–≤–∞—Ä—è', 'en': 'January 28', 'zh': '1Êúà28Êó•'};
        let feedDateNext = {'ru': '29.01', 'en': '01/29', 'zh': '1Êúà29Êó•'};
        let feedDatePrev = {'ru': '27.01', 'en': '01/27', 'zh': '1Êúà27Êó•'};
        let filterLabel = {'ru': '–§–∏–ª—å—Ç—Ä', 'en': 'Topics', 'zh': '‰∏ªÈ¢òÁ≠õÈÄâ'}
        let publishedLabel = {'ru': '—Å—Ç–∞—Ç—å—è –æ—Ç ', 'en': 'published on ', 'zh': 'ÂèëË°®‰∫é'}
        let sortLabel = {'ru': '–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ', 'en': 'Sort by', 'zh': 'ÊéíÂ∫èÊñπÂºè'}
        let paperLabel = {'ru': '–°—Ç–∞—Ç—å—è', 'en': 'Paper', 'zh': 'ËÆ∫Êñá'}
        let topMonthLabel = {'ru': '–ú–µ—Å—è—Ü', 'en': 'Month', 'zh': 'ÊúàÂ∫¶ËÆ∫Êñá'}
        let topDayLabel = {'ru': '–î–µ–Ω—å', 'en': 'Day', 'zh': 'Êó•Â∫¶ËÆ∫Êñá'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2501.14249', 'title': "Humanity's Last Exam", 'url': 'https://huggingface.co/papers/2501.14249', 'abstract': "Benchmarks are important tools for tracking the rapid advancements in large language model (LLM) capabilities. However, benchmarks are not keeping pace in difficulty: LLMs now achieve over 90\\% accuracy on popular benchmarks like MMLU, limiting informed measurement of state-of-the-art LLM capabilities. In response, we introduce Humanity's Last Exam (HLE), a multi-modal benchmark at the frontier of human knowledge, designed to be the final closed-ended academic benchmark of its kind with broad subject coverage. HLE consists of 3,000 questions across dozens of subjects, including mathematics, humanities, and the natural sciences. HLE is developed globally by subject-matter experts and consists of multiple-choice and short-answer questions suitable for automated grading. Each question has a known solution that is unambiguous and easily verifiable, but cannot be quickly answered via internet retrieval. State-of-the-art LLMs demonstrate low accuracy and calibration on HLE, highlighting a significant gap between current LLM capabilities and the expert human frontier on closed-ended academic questions. To inform research and policymaking upon a clear understanding of model capabilities, we publicly release HLE at https://lastexam.ai.", 'score': 35, 'issue_id': 1873, 'pub_date': '2025-01-24', 'pub_date_card': {'ru': '24 —è–Ω–≤–∞—Ä—è', 'en': 'January 24', 'zh': '1Êúà24Êó•'}, 'hash': '4d614974221756d3', 'authors': ['Long Phan', 'Alice Gatti', 'Ziwen Han', 'Nathaniel Li', 'Josephina Hu', 'Hugh Zhang', 'Sean Shi', 'Michael Choi', 'Anish Agrawal', 'Arnav Chopra', 'Adam Khoja', 'Ryan Kim', 'Jason Hausenloy', 'Oliver Zhang', 'Mantas Mazeika', 'Daron Anderson', 'Tung Nguyen', 'Mobeen Mahmood', 'Fiona Feng', 'Steven Y. Feng', 'Haoran Zhao', 'Michael Yu', 'Varun Gangal', 'Chelsea Zou', 'Zihan Wang', 'Jessica P. Wang', 'Pawan Kumar', 'Oleksandr Pokutnyi', 'Robert Gerbicz', 'Serguei Popov', 'John-Clark Levin', 'Mstyslav Kazakov', 'Johannes Schmitt', 'Geoff Galgon', 'Alvaro Sanchez', 'Yongki Lee', 'Will Yeadon', 'Scott Sauers', 'Marc Roth', 'Chidozie Agu', 'S√∏ren Riis', 'Fabian Giska', 'Saiteja Utpala', 'Zachary Giboney', 'Gashaw M. Goshu', 'Joan of Arc Xavier', 'Sarah-Jane Crowson', 'Mohinder Maheshbhai Naiya', 'Noah Burns', 'Lennart Finke', 'Zerui Cheng', 'Hyunwoo Park', 'Francesco Fournier-Facio', 'John Wydallis', 'Mark Nandor', 'Ankit Singh', 'Tim Gehrunger', 'Jiaqi Cai', 'Ben McCarty', 'Darling Duclosel', 'Jungbae Nam', 'Jennifer Zampese', 'Ryan G. Hoerr', 'Aras Bacho', 'Gautier Abou Loume', 'Abdallah Galal', 'Hangrui Cao', 'Alexis C Garretson', 'Damien Sileo', 'Qiuyu Ren', 'Doru Cojoc', 'Pavel Arkhipov', 'Usman Qazi', 'Lianghui Li', 'Sumeet Motwani', 'Christian Schroeder de Witt', 'Edwin Taylor', 'Johannes Veith', 'Eric Singer', 'Taylor D. Hartman', 'Paolo Rissone', 'Jaehyeok Jin', 'Jack Wei Lun Shi', 'Chris G. Willcocks', 'Joshua Robinson', 'Aleksandar Mikov', 'Ameya Prabhu', 'Longke Tang', 'Xavier Alapont', 'Justine Leon Uro', 'Kevin Zhou', 'Emily de Oliveira Santos', 'Andrey Pupasov Maksimov', 'Edward Vendrow', 'Kengo Zenitani', 'Julien Guillod', 'Yuqi Li', 'Joshua Vendrow', 'Vladyslav Kuchkin', 'Ng Ze-An', 'Pierre Marion', 'Denis Efremov', 'Jayson Lynch', 'Kaiqu Liang', 'Andrew Gritsevskiy', 'Dakotah Martinez', 'Ben Pageler', 'Nick Crispino', 'Dimitri Zvonkine', 'Natanael Wildner Fraga', 'Saeed Soori', 'Ori Press', 'Henry Tang', 'Julian Salazar', 'Sean R. Green', 'Lina Br√ºssel', 'Moon Twayana', 'Aymeric Dieuleveut', 'T. Ryan Rogers', 'Wenjin Zhang', 'Bikun Li', 'Jinzhou Yang', 'Arun Rao', 'Gabriel Loiseau', 'Mikhail Kalinin', 'Marco Lukas', 'Ciprian Manolescu', 'Subrata Mishra', 'Ariel Ghislain Kemogne Kamdoum', 'Tobias Kreiman', 'Tad Hogg', 'Alvin Jin', 'Carlo Bosio', 'Gongbo Sun', 'Brian P Coppola', 'Tim Tarver', 'Haline Heidinger', 'Rafael Sayous', 'Stefan Ivanov', 'Joseph M Cavanagh', 'Jiawei Shen', 'Joseph Marvin Imperial', 'Philippe Schwaller', 'Shaipranesh Senthilkuma', 'Andres M Bran', 'Ali Dehghan', 'Andres Algaba', 'Brecht Verbeken', 'David Noever', 'Ragavendran P V', 'Lisa Schut', 'Ilia Sucholutsky', 'Evgenii Zheltonozhskii', 'Derek Lim', 'Richard Stanley', 'Shankar Sivarajan', 'Tong Yang', 'John Maar', 'Julian Wykowski', 'Mart√≠ Oller', 'Jennifer Sandlin', 'Anmol Sahu', 'Yuzheng Hu', 'Sara Fish', 'Nasser Heydari', 'Archimedes Apronti', 'Kaivalya Rawal', 'Tobias Garcia Vilchis', 'Yuexuan Zu', 'Martin Lackner', 'James Koppel', 'Jeremy Nguyen', 'Daniil S. Antonenko', 'Steffi Chern', 'Bingchen Zhao', 'Pierrot Arsene', 'Alan Goldfarb', 'Sergey Ivanov', 'Rafa≈Ç Po≈õwiata', 'Chenguang Wang', 'Daofeng Li', 'Donato Crisostomi', 'Andrea Achilleos', 'Benjamin Myklebust', 'Archan Sen', 'David Perrella', 'Nurdin Kaparov', 'Mark H Inlow', 'Allen Zang', 'Elliott Thornley', 'Daniil Orel', 'Vladislav Poritski', 'Shalev Ben-David', 'Zachary Berger', 'Parker Whitfill', 'Michael Foster', 'Daniel Munro', 'Linh Ho', 'Dan Bar Hava', 'Aleksey Kuchkin', 'Robert Lauff', 'David Holmes', 'Frank Sommerhage', 'Keith Schneider', 'Zakayo Kazibwe', 'Nate Stambaugh', 'Mukhwinder Singh', 'Ilias Magoulas', 'Don Clarke', 'Dae Hyun Kim', 'Felipe Meneguitti Dias', 'Veit Elser', 'Kanu Priya Agarwal', 'Victor Efren Guadarrama Vilchis', 'Immo Klose', 'Christoph Demian', 'Ujjwala Anantheswaran', 'Adam Zweiger', 'Guglielmo Albani', 'Jeffery Li', 'Nicolas Daans', 'Maksim Radionov', 'V√°clav Rozho≈à', 'Ziqiao Ma', 'Christian Stump', 'Mohammed Berkani', 'Jacob Platnick', 'Volodymyr Nevirkovets', 'Luke Basler', 'Marco Piccardo', 'Ferenc Jeanplong', 'Niv Cohen', 'Josef Tkadlec', 'Paul Rosu', 'Piotr Padlewski', 'Stanislaw Barzowski', 'Kyle Montgomery', 'Aline Menezes', 'Arkil Patel', 'Zixuan Wang', 'Jamie Tucker-Foltz', 'Jack Stade', 'Tom Goertzen', 'Fereshteh Kazemi', 'Jeremiah Milbauer', 'John Arnold Ambay', 'Abhishek Shukla', 'Yan Carlos Leyva Labrador', 'Alan Givr√©', 'Hew Wolff', 'Vivien Rossbach', 'Muhammad Fayez Aziz', 'Younesse Kaddar', 'Yanxu Chen', 'Robin Zhang', 'Jiayi Pan', 'Antonio Terpin', 'Niklas Muennighoff', 'Hailey Schoelkopf', 'Eric Zheng', 'Avishy Carmi', 'Adam Jones', 'Jainam Shah', 'Ethan D. L. Brown', 'Kelin Zhu', 'Max Bartolo', 'Richard Wheeler', 'Andrew Ho', 'Shaul Barkan', 'Jiaqi Wang', 'Martin Stehberger', 'Egor Kretov', 'Kaustubh Sridhar', 'Zienab EL-Wasif', 'Anji Zhang', 'Daniel Pyda', 'Joanna Tam', 'David M. Cunningham', 'Vladimir Goryachev', 'Demosthenes Patramanis', 'Michael Krause', 'Andrew Redenti', 'Daniel Bugas', 'David Aldous', 'Jesyin Lai', 'Shannon Coleman', 'Mohsen Bahaloo', 'Jiangnan Xu', 'Sangwon Lee', 'Sandy Zhao', 'Ning Tang', 'Michael K. Cohen', 'Micah Carroll', 'Orr Paradise', 'Jan Hendrik Kirchner', 'Stefan Steinerberger', 'Maksym Ovchynnikov', 'Jason O. Matos', 'Adithya Shenoy', 'Benedito Alves de Oliveira Junior', 'Michael Wang', 'Yuzhou Nie', 'Paolo Giordano', 'Philipp Petersen', 'Anna Sztyber-Betley', 'Priti Shukla', 'Jonathan Crozier', 'Antonella Pinto', 'Shreyas Verma', 'Prashant Joshi', 'Zheng-Xin Yong', 'Allison Tee', 'J√©r√©my Andr√©oletti', 'Orion Weller', 'Raghav Singhal', 'Gang Zhang', 'Alexander Ivanov', 'Seri Khoury', 'Hamid Mostaghimi', 'Kunvar Thaman', 'Qijia Chen', 'Tran Quoc Kh√°nh', 'Jacob Loader', 'Stefano Cavalleri', 'Hannah Szlyk', 'Zachary Brown', 'Jonathan Roberts', 'William Alley', 'Kunyang Sun', 'Ryan Stendall', 'Max Lamparth', 'Anka Reuel', 'Ting Wang', 'Hanmeng Xu', 'Sreenivas Goud Raparthi', 'Pablo Hern√°ndez-C√°mara', 'Freddie Martin', 'Dmitry Malishev', 'Thomas Preu', 'Tomek Korbak', 'Marcus Abramovitch', 'Dominic Williamson', 'Ziye Chen', 'Bir√≥ B√°lint', 'M Saiful Bari', 'Peyman Kassani', 'Zihao Wang', 'Behzad Ansarinejad', 'Laxman Prasad Goswami', 'Yewen Sun', 'Hossam Elgnainy', 'Daniel Tordera', 'George Balabanian', 'Earth Anderson', 'Lynna Kvistad', 'Alejandro Jos√© Moyano', 'Rajat Maheshwari', 'Ahmad Sakor', 'Murat Eron', 'Isaac C. McAlister', 'Javier Gimenez', 'Innocent Enyekwe', 'Andrew Favre D. O.', 'Shailesh Shah', 'Xiaoxiang Zhou', 'Firuz Kamalov', 'Ronald Clark', 'Sherwin Abdoli', 'Tim Santens', 'Khalida Meer', 'Harrison K Wang', 'Kalyan Ramakrishnan', 'Evan Chen', 'Alessandro Tomasiello', 'G. Bruno De Luca', 'Shi-Zhuo Looi', 'Vinh-Kha Le', 'Noam Kolt', 'Niels M√ºndler', 'Avi Semler', 'Emma Rodman', 'Jacob Drori', 'Carl J Fossum', 'Milind Jagota', 'Ronak Pradeep', 'Honglu Fan', 'Tej Shah', 'Jonathan Eicher', 'Michael Chen', 'Kushal Thaman', 'William Merrill', 'Carter Harris', 'Jason Gross', 'Ilya Gusev', 'Asankhaya Sharma', 'Shashank Agnihotri', 'Pavel Zhelnov', 'Siranut Usawasutsakorn', 'Mohammadreza Mofayezi', 'Sergei Bogdanov', 'Alexander Piperski', 'Marc Carauleanu', 'David K. Zhang', 'Dylan Ler', 'Roman Leventov', 'Ignat Soroko', 'Thorben Jansen', 'Pascal Lauer', 'Joshua Duersch', 'Vage Taamazyan', 'Wiktor Morak', 'Wenjie Ma', 'William Held', 'Tran ƒêuc Huy', 'Ruicheng Xian', 'Armel Randy Zebaze', 'Mohanad Mohamed', 'Julian Noah Leser', 'Michelle X Yuan', 'Laila Yacar', 'Johannes Lengler', 'Hossein Shahrtash', 'Edson Oliveira', 'Joseph W. Jackson', 'Daniel Espinosa Gonzalez', 'Andy Zou', 'Muthu Chidambaram', 'Timothy Manik', 'Hector Haffenden', 'Dashiell Stander', 'Ali Dasouqi', 'Alexander Shen', 'Emilien Duc', 'Bita Golshani', 'David Stap', 'Mikalai Uzhou', 'Alina Borisovna Zhidkovskaya', 'Lukas Lewark', 'M√°ty√°s Vincze', 'Dustin Wehr', 'Colin Tang', 'Zaki Hossain', 'Shaun Phillips', 'Jiang Muzhen', 'Fredrik Ekstr√∂m', 'Angela Hammon', 'Oam Patel', 'Nicolas Remy', 'Faraz Farhidi', 'George Medley', 'Forough Mohammadzadeh', 'Madellene Pe√±aflor', 'Haile Kassahun', 'Alena Friedrich', 'Claire Sparrow', 'Taom Sakal', 'Omkar Dhamane', 'Ali Khajegili Mirabadi', 'Eric Hallman', 'Mike Battaglia', 'Mohammad Maghsoudimehrabani', 'Hieu Hoang', 'Alon Amit', 'Dave Hulbert', 'Roberto Pereira', 'Simon Weber', 'Stephen Mensah', 'Nathan Andre', 'Anton Peristyy', 'Chris Harjadi', 'Himanshu Gupta', 'Stephen Malina', 'Samuel Albanie', 'Will Cai', 'Mustafa Mehkary', 'Frank Reidegeld', 'Anna-Katharina Dick', 'Cary Friday', 'Jasdeep Sidhu', 'Wanyoung Kim', 'Mariana Costa', 'Hubeyb Gurdogan', 'Brian Weber', 'Harsh Kumar', 'Tong Jiang', 'Arunim Agarwal', 'Chiara Ceconello', 'Warren S. Vaz', 'Chao Zhuang', 'Haon Park', 'Andrew R. Tawfeek', 'Daattavya Aggarwal', 'Michael Kirchhof', 'Linjie Dai', 'Evan Kim', 'Johan Ferret', 'Yuzhou Wang', 'Minghao Yan', 'Krzysztof Burdzy', 'Lixin Zhang', 'Antonio Franca', 'Diana T. Pham', 'Kang Yong Loh', 'Joshua Robinson', 'Shreen Gul', 'Gunjan Chhablani', 'Zhehang Du', 'Adrian Cosma', 'Colin White', 'Robin Riblet', 'Prajvi Saxena', 'Jacob Votava', 'Vladimir Vinnikov', 'Ethan Delaney', 'Shiv Halasyamani', 'Syed M. Shahid', 'Jean-Christophe Mourrat', 'Lavr Vetoshkin', 'Renas Bacho', 'Vincent Ginis', 'Aleksandr Maksapetyan', 'Florencia de la Rosa', 'Xiuyu Li', 'Guillaume Malod', 'Leon Lang', 'Julien Laurendeau', 'Fatimah Adesanya', 'Julien Portier', 'Lawrence Hollom', 'Victor Souza', 'Yuchen Anna Zhou', 'Yiƒüit Yalƒ±n', 'Gbenga Daniel Obikoya', 'Luca Arnaboldi', 'Rai', 'Filippo Bigi', 'Kaniuar Bacho', 'Pierre Clavier', 'Gabriel Recchia', 'Mara Popescu', 'Nikita Shulga', 'Ngefor Mildred Tanwie', 'Thomas C. H. Lux', 'Ben Rank', 'Colin Ni', 'Alesia Yakimchyk', 'Huanxu', 'Liu', 'Olle H√§ggstr√∂m', 'Emil Verkama', 'Himanshu Narayan', 'Hans Gundlach', 'Leonor Brito-Santana', 'Brian Amaro', 'Vivek Vajipey', 'Rynaa Grover', 'Yiyang Fan', 'Gabriel Poesia Reis e Silva', 'Linwei Xin', 'Yosi Kratish', 'Jakub ≈Åucki', 'Wen-Ding Li', 'Justin Xu', 'Kevin Joseph Scaria', 'Freddie Vargus', 'Farzad Habibi', 'Long', 'Lian', 'Emanuele Rodol√†', 'Jules Robins', 'Vincent Cheng', 'Declan Grabb', 'Ida Bosio', 'Tony Fruhauff', 'Ido Akov', 'Eve J. Y. Lo', 'Hao Qi', 'Xi Jiang', 'Ben Segev', 'Jingxuan Fan', 'Sarah Martinson', 'Erik Y. Wang', 'Kaylie Hausknecht', 'Michael P. Brenner', 'Mao Mao', 'Yibo Jiang', 'Xinyu Zhang', 'David Avagian', 'Eshawn Jessica Scipio', 'Muhammad Rehan Siddiqi', 'Alon Ragoler', 'Justin Tan', 'Deepakkumar Patil', 'Rebeka Plecnik', 'Aaron Kirtland', 'Roselynn Grace Montecillo', 'Stephane Durand', 'Omer Faruk Bodur', 'Zahra Adoul', 'Mohamed Zekry', 'Guillaume Douville', 'Ali Karakoc', 'Tania C. B. Santos', 'Samir Shamseldeen', 'Loukmane Karim', 'Anna Liakhovitskaia', 'Nate Resman', 'Nicholas Farina', 'Juan Carlos Gonzalez', 'Gabe Maayan', 'Sarah Hoback', 'Rodrigo De Oliveira Pena', 'Glen Sherman', 'Hodjat Mariji', 'Rasoul Pouriamanesh', 'Wentao Wu', 'G√∂zdenur Demir', 'Sandra Mendoza', 'Ismail Alarab', 'Joshua Cole', 'Danyelle Ferreira', 'Bryan Johnson', 'Hsiaoyun Milliron', 'Mohammad Safdari', 'Liangti Dai', 'Siriphan Arthornthurasuk', 'Alexey Pronin', 'Jing Fan', 'Angel Ramirez-Trinidad', 'Ashley Cartwright', 'Daphiny Pottmaier', 'Omid Taheri', 'David Outevsky', 'Stanley Stepanic', 'Samuel Perry', 'Luke Askew', 'Ra√∫l Adri√°n Huerta Rodr√≠guez', 'Abdelkader Dendane', 'Sam Ali', 'Ricardo Lorena', 'Krishnamurthy Iyer', 'Sk Md Salauddin', 'Murat Islam', 'Juan Gonzalez', 'Josh Ducey', 'Russell Campbell', 'Maja Somrak', 'Vasilios Mavroudis', 'Eric Vergo', 'Juehang Qin', 'Benj√°min Borb√°s', 'Eric Chu', 'Jack Lindsey', 'Anil Radhakrishnan', 'Antoine Jallon', 'I. M. J. McInnis', 'Alex Hoover', 'S√∂ren M√∂ller', 'Song Bian', 'John Lai', 'Tejal Patwardhan', 'Summer Yue', 'Alexandr Wang', 'Dan Hendrycks'], 'affiliations': ['Center for AI Safety', 'Scale AI'], 'pdf_title_img': 'assets/pdf/title_img/2501.14249.jpg', 'data': {'categories': ['#benchmark', '#science', '#multimodal'], 'emoji': 'üß†', 'ru': {'title': '–ù–æ–≤—ã–π —Ä—É–±–µ–∂ –¥–ª—è –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞: —Ç–µ—Å—Ç –Ω–∞ –ø—Ä–µ–¥–µ–ª–µ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –∑–Ω–∞–Ω–∏–π', 'desc': "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º '–ü–æ—Å–ª–µ–¥–Ω–∏–π —ç–∫–∑–∞–º–µ–Ω —á–µ–ª–æ–≤–µ—á–µ—Å—Ç–≤–∞' (HLE). HLE —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ 3000 –≤–æ–ø—Ä–æ—Å–æ–≤ –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º –ø—Ä–µ–¥–º–µ—Ç–∞–º, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–∞–º–∏ —Å–æ –≤—Å–µ–≥–æ –º–∏—Ä–∞. –ë–µ–Ω—á–º–∞—Ä–∫ —Å–æ–∑–¥–∞–Ω –¥–ª—è –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ç–µ—Å—Ç–æ–≤, –Ω–∞ –∫–æ—Ç–æ—Ä—ã—Ö —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ LLM –¥–æ—Å—Ç–∏–≥–∞—é—Ç —Ç–æ—á–Ω–æ—Å—Ç–∏ –±–æ–ª–µ–µ 90%. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ LLM –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –Ω–∏–∑–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ HLE, —á—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∞–∑—Ä—ã–≤ –º–µ–∂–¥—É –∏—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏ –∏ —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–º–∏ –∑–Ω–∞–Ω–∏—è–º–∏ —á–µ–ª–æ–≤–µ–∫–∞."}, 'en': {'title': "Raising the Bar: Humanity's Last Exam for LLMs", 'desc': "This paper introduces a new benchmark called Humanity's Last Exam (HLE) to evaluate the capabilities of large language models (LLMs). HLE consists of 3,000 questions across various subjects, including mathematics and humanities, designed to be challenging for LLMs. Unlike existing benchmarks, HLE questions cannot be easily answered through internet searches, making them a better measure of true understanding. The results show that current state-of-the-art LLMs struggle with HLE, indicating a significant gap between their performance and that of expert humans."}, 'zh': {'title': '‰∫∫Á±ªÁöÑÊúÄÂêéËÄÉËØïÔºöÊåëÊàòLLMÁöÑÊûÅÈôê', 'desc': 'Âü∫ÂáÜÊµãËØïÊòØË∑üË∏™Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâËÉΩÂäõÂø´ÈÄüÂèëÂ±ïÁöÑÈáçË¶ÅÂ∑•ÂÖ∑„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÁöÑÂü∫ÂáÜÊµãËØïÈöæÂ∫¶Êú™ËÉΩ‰∏éLLMÁöÑËøõÊ≠•Áõ∏ÂåπÈÖçÔºåÂØºËá¥LLMÂú®ÊµÅË°åÂü∫ÂáÜÊµãËØïÔºàÂ¶ÇMMLUÔºâ‰∏äËææÂà∞90%‰ª•‰∏äÁöÑÂáÜÁ°ÆÁéá„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨Êé®Âá∫‰∫Ü‰∫∫Á±ªÁöÑÊúÄÂêéËÄÉËØïÔºàHLEÔºâÔºåËøôÊòØ‰∏Ä‰∏™Ê∂µÁõñÂπøÊ≥õÂ≠¶ÁßëÁöÑÂ§öÊ®°ÊÄÅÂü∫ÂáÜÔºåÊó®Âú®Êàê‰∏∫Ê≠§Á±ªÂ≠¶ÊúØÂü∫ÂáÜÁöÑÊúÄÁªàÁâàÊú¨„ÄÇHLEÂåÖÂê´3000‰∏™ÈóÆÈ¢òÔºåÊ∂âÂèäÊï∞Â≠¶„ÄÅ‰∫∫ÊñáÂ≠¶ÁßëÂíåËá™ÁÑ∂ÁßëÂ≠¶ÔºåÊó®Âú®Êè≠Á§∫ÂΩìÂâçLLMËÉΩÂäõ‰∏é‰∏ìÂÆ∂‰∫∫Á±ªÊ∞¥Âπ≥‰πãÈó¥ÁöÑÊòæËëóÂ∑ÆË∑ù„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2501.14342', 'title': 'Chain-of-Retrieval Augmented Generation', 'url': 'https://huggingface.co/papers/2501.14342', 'abstract': "This paper introduces an approach for training o1-like RAG models that retrieve and reason over relevant information step by step before generating the final answer. Conventional RAG methods usually perform a single retrieval step before the generation process, which limits their effectiveness in addressing complex queries due to imperfect retrieval results. In contrast, our proposed method, CoRAG (Chain-of-Retrieval Augmented Generation), allows the model to dynamically reformulate the query based on the evolving state. To train CoRAG effectively, we utilize rejection sampling to automatically generate intermediate retrieval chains, thereby augmenting existing RAG datasets that only provide the correct final answer. At test time, we propose various decoding strategies to scale the model's test-time compute by controlling the length and number of sampled retrieval chains. Experimental results across multiple benchmarks validate the efficacy of CoRAG, particularly in multi-hop question answering tasks, where we observe more than 10 points improvement in EM score compared to strong baselines. On the KILT benchmark, CoRAG establishes a new state-of-the-art performance across a diverse range of knowledge-intensive tasks. Furthermore, we offer comprehensive analyses to understand the scaling behavior of CoRAG, laying the groundwork for future research aimed at developing factual and grounded foundation models.", 'score': 27, 'issue_id': 1873, 'pub_date': '2025-01-24', 'pub_date_card': {'ru': '24 —è–Ω–≤–∞—Ä—è', 'en': 'January 24', 'zh': '1Êúà24Êó•'}, 'hash': 'cd489ba1638c5496', 'authors': ['Liang Wang', 'Haonan Chen', 'Nan Yang', 'Xiaolong Huang', 'Zhicheng Dou', 'Furu Wei'], 'affiliations': ['Microsoft Corporation', 'Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2501.14342.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#rag', '#reasoning'], 'emoji': 'üîó', 'ru': {'title': 'CoRAG: –ü–æ—à–∞–≥–æ–≤—ã–π –ø–æ–∏—Å–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –º–æ–¥–µ–ª–µ–π –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (RAG), –ø–æ–∑–≤–æ–ª—è—é—â–∏–π –≤—ã–ø–æ–ª–Ω—è—Ç—å –ø–æ—à–∞–≥–æ–≤—ã–π –ø–æ–∏—Å–∫ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –ø–µ—Ä–µ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞. –ú–µ—Ç–æ–¥ CoRAG (Chain-of-Retrieval Augmented Generation) –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –∑–∞–ø—Ä–æ—Å –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑–≤–∏–≤–∞—é—â–µ–≥–æ—Å—è —Å–æ—Å—Ç–æ—è–Ω–∏—è. –î–ª—è –æ–±—É—á–µ–Ω–∏—è CoRAG –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –æ—Ç–±–æ—Ä —Å –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ–º –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Ü–µ–ø–æ—á–µ–∫ –ø–æ–∏—Å–∫–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –∑–∞–¥–∞—á–∞—Ö –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã.'}, 'en': {'title': 'CoRAG: Enhancing RAG with Dynamic Retrieval for Complex Queries', 'desc': 'This paper presents CoRAG, a novel approach for training retrieval-augmented generation (RAG) models that enhances their ability to handle complex queries. Unlike traditional RAG methods that rely on a single retrieval step, CoRAG employs a dynamic query reformulation process, allowing the model to retrieve information iteratively. The training process utilizes rejection sampling to create intermediate retrieval chains, enriching the dataset beyond just the final answers. Experimental results demonstrate that CoRAG significantly improves performance in multi-hop question answering tasks, achieving state-of-the-art results on the KILT benchmark.'}, 'zh': {'title': 'Âä®ÊÄÅÊ£ÄÁ¥¢ÔºåÊèêÂçáÈóÆÁ≠îËÉΩÂäõÔºÅ', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçËÆ≠ÁªÉÁ±ª‰ººo1ÁöÑRAGÊ®°ÂûãÁöÑÊñ∞ÊñπÊ≥ïÔºåËØ•ÊñπÊ≥ïÂú®ÁîüÊàêÊúÄÁªàÁ≠îÊ°à‰πãÂâçÈÄêÊ≠•Ê£ÄÁ¥¢ÂíåÊé®ÁêÜÁõ∏ÂÖ≥‰ø°ÊÅØ„ÄÇ‰º†ÁªüÁöÑRAGÊñπÊ≥ïÈÄöÂ∏∏Âú®ÁîüÊàêËøáÁ®ã‰πãÂâçÂè™ËøõË°å‰∏ÄÊ¨°Ê£ÄÁ¥¢ÔºåËøôÈôêÂà∂‰∫ÜÂÆÉ‰ª¨Âú®Â§ÑÁêÜÂ§çÊùÇÊü•ËØ¢Êó∂ÁöÑÊúâÊïàÊÄß„ÄÇÊàë‰ª¨ÊèêÂá∫ÁöÑÊñπÊ≥ïCoRAGÔºàÈìæÂºèÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÔºâÂÖÅËÆ∏Ê®°ÂûãÊ†πÊçÆ‰∏çÊñ≠ÂèòÂåñÁöÑÁä∂ÊÄÅÂä®ÊÄÅÈáçÊûÑÊü•ËØ¢„ÄÇÈÄöËøá‰ΩøÁî®ÊãíÁªùÈááÊ†∑Ëá™Âä®ÁîüÊàê‰∏≠Èó¥Ê£ÄÁ¥¢ÈìæÔºåÊàë‰ª¨ÊúâÊïàÂú∞Â¢ûÂº∫‰∫ÜÁé∞ÊúâÁöÑRAGÊï∞ÊçÆÈõÜÔºå‰ªéËÄåÂú®Â§öË∑≥ÈóÆÁ≠î‰ªªÂä°‰∏≠ÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑË°®Áé∞„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2501.13953', 'title': 'Redundancy Principles for MLLMs Benchmarks', 'url': 'https://huggingface.co/papers/2501.13953', 'abstract': "With the rapid iteration of Multi-modality Large Language Models (MLLMs) and the evolving demands of the field, the number of benchmarks produced annually has surged into the hundreds. The rapid growth has inevitably led to significant redundancy among benchmarks. Therefore, it is crucial to take a step back and critically assess the current state of redundancy and propose targeted principles for constructing effective MLLM benchmarks. In this paper, we focus on redundancy from three key perspectives: 1) Redundancy of benchmark capability dimensions, 2) Redundancy in the number of test questions, and 3) Cross-benchmark redundancy within specific domains. Through the comprehensive analysis over hundreds of MLLMs' performance across more than 20 benchmarks, we aim to quantitatively measure the level of redundancy lies in existing MLLM evaluations, provide valuable insights to guide the future development of MLLM benchmarks, and offer strategies to refine and address redundancy issues effectively.", 'score': 23, 'issue_id': 1877, 'pub_date': '2025-01-20', 'pub_date_card': {'ru': '20 —è–Ω–≤–∞—Ä—è', 'en': 'January 20', 'zh': '1Êúà20Êó•'}, 'hash': 'f504e124f29e4140', 'authors': ['Zicheng Zhang', 'Xiangyu Zhao', 'Xinyu Fang', 'Chunyi Li', 'Xiaohong Liu', 'Xiongkuo Min', 'Haodong Duan', 'Kai Chen', 'Guangtao Zhai'], 'affiliations': ['Shanghai AI Lab', 'Shanghai Jiao Tong University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2501.13953.jpg', 'data': {'categories': ['#benchmark', '#survey'], 'emoji': 'üîç', 'ru': {'title': '–ë–æ—Ä—å–±–∞ —Å –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç—å—é: –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–µ –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç–∏ –≤ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM). –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç—å —Å —Ç—Ä–µ—Ö –∫–ª—é—á–µ–≤—ã—Ö –∞—Å–ø–µ–∫—Ç–æ–≤: –∏–∑–º–µ—Ä–µ–Ω–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ—Å—Ç–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –º–µ–∂–¥—É –±–µ–Ω—á–º–∞—Ä–∫–∞–º–∏ –≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö. –ù–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ MLLM –Ω–∞ –±–æ–ª–µ–µ —á–µ–º 20 –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω–æ –æ—Ü–µ–Ω–∏—Ç—å —É—Ä–æ–≤–µ–Ω—å –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ –¥–∞—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é –±–µ–Ω—á–º–∞—Ä–∫–æ–≤. –¶–µ–ª—å —Ä–∞–±–æ—Ç—ã - –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å —Ü–µ–Ω–Ω—ã–µ –∏–¥–µ–∏ –¥–ª—è –±—É–¥—É—â–µ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è –æ—Ü–µ–Ω–∫–∏ MLLM –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø–æ —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—é –ø—Ä–æ–±–ª–µ–º –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç–∏.'}, 'en': {'title': 'Streamlining MLLM Benchmarks: Tackling Redundancy for Better Evaluation', 'desc': 'This paper examines the growing issue of redundancy in benchmarks for Multi-modality Large Language Models (MLLMs). It identifies three main types of redundancy: in the capabilities being tested, the number of test questions, and across different benchmarks within the same domain. By analyzing the performance of numerous MLLMs across over 20 benchmarks, the authors quantitatively measure the extent of this redundancy. The findings aim to inform the development of more effective benchmarks and provide strategies to reduce redundancy in future evaluations.'}, 'zh': {'title': '‰ºòÂåñÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂü∫ÂáÜÊµãËØïÔºåÂáèÂ∞ëÂÜó‰Ωô', 'desc': 'ÈöèÁùÄÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÁöÑÂø´ÈÄüÂèëÂ±ïÔºåÂπ¥Â∫¶Âü∫ÂáÜÊµãËØïÁöÑÊï∞ÈáèÊøÄÂ¢ûÔºåÂØºËá¥Âü∫ÂáÜÊµãËØï‰πãÈó¥ÁöÑÂÜó‰ΩôÁé∞Ë±°ÊòæËëóÂ¢ûÂä†„ÄÇÊú¨Êñá‰ªé‰∏â‰∏™ÂÖ≥ÈîÆËßíÂ∫¶ÂàÜÊûêÂÜó‰ΩôÈóÆÈ¢òÔºöÂü∫ÂáÜËÉΩÂäõÁª¥Â∫¶ÁöÑÂÜó‰Ωô„ÄÅÊµãËØïÈóÆÈ¢òÊï∞ÈáèÁöÑÂÜó‰Ωô‰ª•ÂèäÁâπÂÆöÈ¢ÜÂüüÂÜÖÁöÑË∑®Âü∫ÂáÜÂÜó‰Ωô„ÄÇÈÄöËøáÂØπÊï∞Áôæ‰∏™MLLMÂú®20Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÁöÑË°®Áé∞ËøõË°åÁªºÂêàÂàÜÊûêÔºåÊàë‰ª¨ÂÆöÈáèÊµãÈáèÁé∞ÊúâMLLMËØÑ‰º∞‰∏≠ÁöÑÂÜó‰ΩôÊ∞¥Âπ≥„ÄÇÊàë‰ª¨ÁöÑÁõÆÊ†áÊòØ‰∏∫Êú™Êù•MLLMÂü∫ÂáÜÁöÑÂºÄÂèëÊèê‰æõÊúâ‰ª∑ÂÄºÁöÑËßÅËß£ÔºåÂπ∂ÊèêÂá∫ÊúâÊïàËß£ÂÜ≥ÂÜó‰ΩôÈóÆÈ¢òÁöÑÁ≠ñÁï•„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2501.14492', 'title': 'RealCritic: Towards Effectiveness-Driven Evaluation of Language Model Critiques', 'url': 'https://huggingface.co/papers/2501.14492', 'abstract': 'Critiques are important for enhancing the performance of Large Language Models (LLMs), enabling both self-improvement and constructive feedback for others by identifying flaws and suggesting improvements. However, evaluating the critique capabilities of LLMs presents a significant challenge due to the open-ended nature of the task. In this work, we introduce a new benchmark designed to assess the critique capabilities of LLMs. Unlike existing benchmarks, which typically function in an open-loop fashion, our approach employs a closed-loop methodology that evaluates the quality of corrections generated from critiques. Moreover, the benchmark incorporates features such as self-critique, cross-critique, and iterative critique, which are crucial for distinguishing the abilities of advanced reasoning models from more classical ones. We implement this benchmark using eight challenging reasoning tasks. We have several interesting findings. First, despite demonstrating comparable performance in direct chain-of-thought generation, classical LLMs significantly lag behind the advanced reasoning-based model o1-mini across all critique scenarios. Second, in self-critique and iterative critique settings, classical LLMs may even underperform relative to their baseline capabilities. We hope that this benchmark will serve as a valuable resource to guide future advancements. The code and data are available at https://github.com/tangzhy/RealCritic.', 'score': 13, 'issue_id': 1873, 'pub_date': '2025-01-24', 'pub_date_card': {'ru': '24 —è–Ω–≤–∞—Ä—è', 'en': 'January 24', 'zh': '1Êúà24Êó•'}, 'hash': '683923c8fb1958c2', 'authors': ['Zhengyang Tang', 'Ziniu Li', 'Zhenyang Xiao', 'Tian Ding', 'Ruoyu Sun', 'Benyou Wang', 'Dayiheng Liu', 'Fei Huang', 'Tianyu Liu', 'Bowen Yu', 'Junyang Lin'], 'affiliations': ['Qwen Team, Alibaba Inc.', 'Shenzhen Research Institute of Big Data', 'The Chinese University of Hong Kong, Shenzhen'], 'pdf_title_img': 'assets/pdf/title_img/2501.14492.jpg', 'data': {'categories': ['#benchmark', '#interpretability', '#reasoning'], 'emoji': 'üß†', 'ru': {'title': '–ù–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ —Ä–∞—Å–∫—Ä—ã–≤–∞–µ—Ç –∏—Å—Ç–∏–Ω–Ω—ã–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª LLM –≤ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–º –º—ã—à–ª–µ–Ω–∏–∏', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∫ –∫—Ä–∏—Ç–∏–∫–µ. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤, —ç—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∑–∞–º–∫–Ω—É—Ç—É—é –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—é, –æ—Ü–µ–Ω–∏–≤–∞—é—â—É—é –∫–∞—á–µ—Å—Ç–≤–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫—Ä–∏—Ç–∏–∫–∏. –ë–µ–Ω—á–º–∞—Ä–∫ –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è —Å–∞–º–æ–∫—Ä–∏—Ç–∏–∫—É, –ø–µ—Ä–µ–∫—Ä–µ—Å—Ç–Ω—É—é –∫—Ä–∏—Ç–∏–∫—É –∏ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω—É—é –∫—Ä–∏—Ç–∏–∫—É, —á—Ç–æ –≤–∞–∂–Ω–æ –¥–ª—è —Ä–∞–∑–ª–∏—á–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –æ—Ç –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏—Ö. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑–∞–ª–æ, —á—Ç–æ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–µ LLM –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –æ—Ç—Å—Ç–∞—é—Ç –æ—Ç –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤–æ –≤—Å–µ—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö –∫—Ä–∏—Ç–∏–∫–∏, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ –ø—Ä—è–º–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π.'}, 'en': {'title': 'Enhancing LLMs Through Effective Critique Evaluation', 'desc': 'This paper focuses on improving Large Language Models (LLMs) by evaluating their critique capabilities, which are essential for self-improvement and providing feedback. The authors introduce a new benchmark that uses a closed-loop methodology to assess how well LLMs can generate corrections based on critiques. This benchmark includes features like self-critique, cross-critique, and iterative critique, allowing for a more nuanced evaluation of reasoning abilities. The findings reveal that advanced reasoning models outperform classical LLMs in critique scenarios, highlighting the need for better evaluation methods in machine learning.'}, 'zh': {'title': 'ÊèêÂçáLLMsÊÄßËÉΩÁöÑÊñ∞Âü∫ÂáÜËØÑ‰º∞ÊâπËØÑËÉΩÂäõ', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÊâπËØÑËÉΩÂäõÊñπÈù¢ÁöÑËØÑ‰º∞„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂü∫ÂáÜÔºåÈááÁî®Èó≠ÁéØÊñπÊ≥ïÊù•ËØÑ‰º∞ÊâπËØÑÁîüÊàêÁöÑ‰øÆÊ≠£Ë¥®Èáè„ÄÇËØ•Âü∫ÂáÜÂåÖÊã¨Ëá™ÊàëÊâπËØÑ„ÄÅ‰∫§ÂèâÊâπËØÑÂíåËø≠‰ª£ÊâπËØÑÁ≠âÁâπÊÄßÔºå‰ª•Âå∫ÂàÜÈ´òÁ∫ßÊé®ÁêÜÊ®°Âûã‰∏é‰º†ÁªüÊ®°ÂûãÁöÑËÉΩÂäõ„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÂ∞ΩÁÆ°‰º†ÁªüLLMsÂú®Áõ¥Êé•ÊÄùÁª¥ÁîüÊàêÊñπÈù¢Ë°®Áé∞Áõ∏‰ººÔºå‰ΩÜÂú®ÊâÄÊúâÊâπËØÑÂú∫ÊôØ‰∏≠ÔºåÂÆÉ‰ª¨ÁöÑË°®Áé∞ÊòéÊòæËêΩÂêé‰∫éÂü∫‰∫éÈ´òÁ∫ßÊé®ÁêÜÁöÑÊ®°Âûão1-mini„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2501.14176', 'title': 'RL + Transformer = A General-Purpose Problem Solver', 'url': 'https://huggingface.co/papers/2501.14176', 'abstract': 'What if artificial intelligence could not only solve problems for which it was trained but also learn to teach itself to solve new problems (i.e., meta-learn)? In this study, we demonstrate that a pre-trained transformer fine-tuned with reinforcement learning over multiple episodes develops the ability to solve problems that it has never encountered before - an emergent ability called In-Context Reinforcement Learning (ICRL). This powerful meta-learner not only excels in solving unseen in-distribution environments with remarkable sample efficiency, but also shows strong performance in out-of-distribution environments. In addition, we show that it exhibits robustness to the quality of its training data, seamlessly stitches together behaviors from its context, and adapts to non-stationary environments. These behaviors demonstrate that an RL-trained transformer can iteratively improve upon its own solutions, making it an excellent general-purpose problem solver.', 'score': 7, 'issue_id': 1884, 'pub_date': '2025-01-24', 'pub_date_card': {'ru': '24 —è–Ω–≤–∞—Ä—è', 'en': 'January 24', 'zh': '1Êúà24Êó•'}, 'hash': '708deafdf9ddb570', 'authors': ['Micah Rentschler', 'Jesse Roberts'], 'affiliations': ['Tennessee Technological University'], 'pdf_title_img': 'assets/pdf/title_img/2501.14176.jpg', 'data': {'categories': ['#training', '#transfer_learning', '#optimization', '#agi', '#rl'], 'emoji': 'üß†', 'ru': {'title': '–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä —É—á–∏—Ç—Å—è —É—á–∏—Ç—å—Å—è: –Ω–æ–≤—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç, —á—Ç–æ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä, –¥–æ–æ–±—É—á–µ–Ω–Ω—ã–π —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, —Ä–∞–∑–≤–∏–≤–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —Ä–µ—à–∞—Ç—å –Ω–æ–≤—ã–µ –∑–∞–¥–∞—á–∏ - —Ç–∞–∫ –Ω–∞–∑—ã–≤–∞–µ–º–æ–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (ICRL). –≠—Ç–∞ –º–µ—Ç–∞-–æ–±—É—á–∞—é—â–∞—è—Å—è –º–æ–¥–µ–ª—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Ä–µ—à–∞–µ—Ç –Ω–µ —Ç–æ–ª—å–∫–æ –∑–∞–¥–∞—á–∏ –∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –Ω–æ –∏ –∑–∞–¥–∞—á–∏ –≤–Ω–µ —ç—Ç–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è. –ú–æ–¥–µ–ª—å –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ –∫–∞—á–µ—Å—Ç–≤—É –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –∫ –Ω–µ—Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω—ã–º —Å—Ä–µ–¥–∞–º. –≠—Ç–æ —Å–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤—É–µ—Ç –æ —Ç–æ–º, —á—Ç–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä, –æ–±—É—á–µ–Ω–Ω—ã–π —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –º–æ–∂–µ—Ç –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ —É–ª—É—á—à–∞—Ç—å —Å–≤–æ–∏ —Ä–µ—à–µ–Ω–∏—è.'}, 'en': {'title': 'Empowering AI: Learning to Solve New Problems with In-Context Reinforcement Learning', 'desc': 'This paper explores the concept of In-Context Reinforcement Learning (ICRL), where a pre-trained transformer model learns to solve new problems through reinforcement learning. The model shows remarkable sample efficiency, allowing it to tackle unseen problems effectively, both in familiar and unfamiliar environments. It also demonstrates robustness to varying training data quality and adapts well to changing conditions. Overall, the study highlights the potential of RL-trained transformers as versatile problem solvers capable of self-improvement.'}, 'zh': {'title': 'ÂÖÉÂ≠¶‰π†ÔºöËÆ©AIËá™ÊàëËß£ÂÜ≥Êñ∞ÈóÆÈ¢òÁöÑËÉΩÂäõ', 'desc': 'Êú¨Á†îÁ©∂Â±ïÁ§∫‰∫Ü‰∏ÄÁßçÈ¢ÑËÆ≠ÁªÉÁöÑÂèòÊç¢Âô®Ê®°ÂûãÔºåÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ËøõË°åÂæÆË∞ÉÔºåËÉΩÂ§üËß£ÂÜ≥‰πãÂâçÊú™ÈÅáÂà∞ËøáÁöÑÈóÆÈ¢òÔºåËøôÁßçËÉΩÂäõË¢´Áß∞‰∏∫‰∏ä‰∏ãÊñáÂº∫ÂåñÂ≠¶‰π†ÔºàICRLÔºâ„ÄÇËøôÁßçÂº∫Â§ßÁöÑÂÖÉÂ≠¶‰π†ËÄÖÂú®Â§ÑÁêÜÊú™ËßÅËøáÁöÑÁéØÂ¢ÉÊó∂Ë°®Áé∞Âá∫Ëâ≤ÔºåÂÖ∑ÊúâÊòæËëóÁöÑÊ†∑Êú¨ÊïàÁéáÔºåÂπ∂‰∏îÂú®ÂàÜÂ∏ÉÂ§ñÁéØÂ¢É‰∏≠‰πüË°®Áé∞ËâØÂ•Ω„ÄÇÊ≠§Â§ñÔºåÂÆÉÂØπËÆ≠ÁªÉÊï∞ÊçÆÁöÑË¥®ÈáèÂÖ∑ÊúâÈ≤ÅÊ£íÊÄßÔºåËÉΩÂ§üÊó†ÁºùÂú∞ÁªìÂêà‰∏ä‰∏ãÊñá‰∏≠ÁöÑË°å‰∏∫ÔºåÂπ∂ÈÄÇÂ∫îÈùûÂπ≥Á®≥ÁéØÂ¢É„ÄÇËøô‰∫õÁâπÊÄßË°®ÊòéÔºåÁªèËøáÂº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉÁöÑÂèòÊç¢Âô®ËÉΩÂ§ü‰∏çÊñ≠ÊîπËøõËá™Â∑±ÁöÑËß£ÂÜ≥ÊñπÊ°àÔºåÊàê‰∏∫‰∏ÄÁßç‰ºòÁßÄÁöÑÈÄöÁî®ÈóÆÈ¢òËß£ÂÜ≥ËÄÖ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2501.13687', 'title': 'Question Answering on Patient Medical Records with Private Fine-Tuned LLMs', 'url': 'https://huggingface.co/papers/2501.13687', 'abstract': 'Healthcare systems continuously generate vast amounts of electronic health records (EHRs), commonly stored in the Fast Healthcare Interoperability Resources (FHIR) standard. Despite the wealth of information in these records, their complexity and volume make it difficult for users to retrieve and interpret crucial health insights. Recent advances in Large Language Models (LLMs) offer a solution, enabling semantic question answering (QA) over medical data, allowing users to interact with their health records more effectively. However, ensuring privacy and compliance requires edge and private deployments of LLMs.   This paper proposes a novel approach to semantic QA over EHRs by first identifying the most relevant FHIR resources for a user query (Task1) and subsequently answering the query based on these resources (Task2). We explore the performance of privately hosted, fine-tuned LLMs, evaluating them against benchmark models such as GPT-4 and GPT-4o. Our results demonstrate that fine-tuned LLMs, while 250x smaller in size, outperform GPT-4 family models by 0.55% in F1 score on Task1 and 42% on Meteor Task in Task2. Additionally, we examine advanced aspects of LLM usage, including sequential fine-tuning, model self-evaluation (narcissistic evaluation), and the impact of training data size on performance. The models and datasets are available here: https://huggingface.co/genloop', 'score': 6, 'issue_id': 1885, 'pub_date': '2025-01-23', 'pub_date_card': {'ru': '23 —è–Ω–≤–∞—Ä—è', 'en': 'January 23', 'zh': '1Êúà23Êó•'}, 'hash': '710359a2b4f5f274', 'authors': ['Sara Kothari', 'Ayush Gupta'], 'affiliations': ['Department of Computer Science Stanford University', 'Genloop Labs, Inc. Delaware, USA'], 'pdf_title_img': 'assets/pdf/title_img/2501.13687.jpg', 'data': {'categories': ['#open_source', '#multimodal', '#dataset', '#training', '#science', '#benchmark', '#healthcare'], 'emoji': 'üè•', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö —Å –ø–æ–º–æ—â—å—é LLM', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–º—É –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω–æ–º—É –∞–Ω–∞–ª–∏–∑—É —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω—ã—Ö –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∫–∞—Ä—Ç (–≠–ú–ö) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –º–µ—Ç–æ–¥: —Å–Ω–∞—á–∞–ª–∞ –∏–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä—É—é—Ç—Å—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã FHIR, –∑–∞—Ç–µ–º –Ω–∞ –∏—Ö –æ—Å–Ω–æ–≤–µ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è –æ—Ç–≤–µ—Ç –Ω–∞ –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –¥–æ–æ–±—É—á–µ–Ω–Ω—ã–µ LLM –º–µ–Ω—å—à–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –º–æ–¥–µ–ª–∏ —Å–µ–º–µ–π—Å—Ç–≤–∞ GPT-4 –ø–æ —Ä—è–¥—É –º–µ—Ç—Ä–∏–∫. –¢–∞–∫–∂–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –∞—Å–ø–µ–∫—Ç—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è LLM, –≤–∫–ª—é—á–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—É—é —Ç–æ–Ω–∫—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É –∏ —Å–∞–º–æ–æ—Ü–µ–Ω–∫—É –º–æ–¥–µ–ª–µ–π.'}, 'en': {'title': 'Unlocking Health Insights with Fine-Tuned Language Models', 'desc': 'This paper addresses the challenge of extracting meaningful insights from electronic health records (EHRs) using Large Language Models (LLMs). It introduces a two-step approach for semantic question answering (QA) that first identifies relevant FHIR resources and then answers user queries based on those resources. The study evaluates privately hosted, fine-tuned LLMs against benchmark models like GPT-4, showing that these smaller models can outperform larger ones in specific tasks. Additionally, it explores advanced techniques such as sequential fine-tuning and the effects of training data size on model performance.'}, 'zh': {'title': 'ÊèêÂçáÂåªÁñóÊï∞ÊçÆÈóÆÁ≠îÁöÑÊô∫ËÉΩÂåñ‰∏éÈöêÁßÅ‰øùÊä§', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËØ≠‰πâÈóÆÁ≠îÊñπÊ≥ïÔºåÊó®Âú®ÊèêÈ´òÁî®Êà∑ÂØπÁîµÂ≠êÂÅ•Â∫∑ËÆ∞ÂΩïÔºàEHRsÔºâÁöÑËÆøÈóÆÂíåÁêÜËß£„ÄÇÈ¶ñÂÖàÔºåÈÄöËøáËØÜÂà´‰∏éÁî®Êà∑Êü•ËØ¢ÊúÄÁõ∏ÂÖ≥ÁöÑFHIRËµÑÊ∫êÔºà‰ªªÂä°1ÔºâÔºåÁÑ∂ÂêéÂü∫‰∫éËøô‰∫õËµÑÊ∫êÂõûÁ≠îÊü•ËØ¢Ôºà‰ªªÂä°2Ôºâ„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÁªèËøáÂæÆË∞ÉÁöÑÁßÅÊúâÊâòÁÆ°Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®‰ªªÂä°1ÁöÑF1ÂàÜÊï∞‰∏äÊØîGPT-4Ê®°ÂûãÈ´òÂá∫0.55%ÔºåÂú®‰ªªÂä°2ÁöÑMeteor‰ªªÂä°‰∏äÈ´òÂá∫42%„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáËøòÊé¢ËÆ®‰∫ÜÊ®°ÂûãÁöÑËá™ÊàëËØÑ‰º∞ÂíåËÆ≠ÁªÉÊï∞ÊçÆËßÑÊ®°ÂØπÊÄßËÉΩÁöÑÂΩ±Âìç„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2501.14726', 'title': 'Relightable Full-Body Gaussian Codec Avatars', 'url': 'https://huggingface.co/papers/2501.14726', 'abstract': 'We propose Relightable Full-Body Gaussian Codec Avatars, a new approach for modeling relightable full-body avatars with fine-grained details including face and hands. The unique challenge for relighting full-body avatars lies in the large deformations caused by body articulation and the resulting impact on appearance caused by light transport. Changes in body pose can dramatically change the orientation of body surfaces with respect to lights, resulting in both local appearance changes due to changes in local light transport functions, as well as non-local changes due to occlusion between body parts. To address this, we decompose the light transport into local and non-local effects. Local appearance changes are modeled using learnable zonal harmonics for diffuse radiance transfer. Unlike spherical harmonics, zonal harmonics are highly efficient to rotate under articulation. This allows us to learn diffuse radiance transfer in a local coordinate frame, which disentangles the local radiance transfer from the articulation of the body. To account for non-local appearance changes, we introduce a shadow network that predicts shadows given precomputed incoming irradiance on a base mesh. This facilitates the learning of non-local shadowing between the body parts. Finally, we use a deferred shading approach to model specular radiance transfer and better capture reflections and highlights such as eye glints. We demonstrate that our approach successfully models both the local and non-local light transport required for relightable full-body avatars, with a superior generalization ability under novel illumination conditions and unseen poses.', 'score': 5, 'issue_id': 1873, 'pub_date': '2025-01-24', 'pub_date_card': {'ru': '24 —è–Ω–≤–∞—Ä—è', 'en': 'January 24', 'zh': '1Êúà24Êó•'}, 'hash': '0072ce1869c715b7', 'authors': ['Shaofei Wang', 'Tomas Simon', 'Igor Santesteban', 'Timur Bagautdinov', 'Junxuan Li', 'Vasu Agrawal', 'Fabian Prada', 'Shoou-I Yu', 'Pace Nalbone', 'Matt Gramlich', 'Roman Lubachersky', 'Chenglei Wu', 'Javier Romero', 'Jason Saragih', 'Michael Zollhoefer', 'Andreas Geiger', 'Siyu Tang', 'Shunsuke Saito'], 'affiliations': ['Codec Avatars Lab, Meta, USA', 'ETH Z√ºrich, Switzerland', 'University of T√ºbingen, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2501.14726.jpg', 'data': {'categories': ['#cv', '#3d'], 'emoji': 'üï¥Ô∏è', 'ru': {'title': '–†–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–µ –æ—Å–≤–µ—â–µ–Ω–∏–µ –¥–ª—è –ø–æ–ª–Ω–æ—Ä–∞–∑–º–µ—Ä–Ω—ã—Ö —Ü–∏—Ñ—Ä–æ–≤—ã—Ö –∞–≤–∞—Ç–∞—Ä–æ–≤', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—é –ø–æ–ª–Ω–æ—Ä–∞–∑–º–µ—Ä–Ω—ã—Ö –∞–≤–∞—Ç–∞—Ä–æ–≤ —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –∏–∑–º–µ–Ω–µ–Ω–∏—è –æ—Å–≤–µ—â–µ–Ω–∏—è, –≤–∫–ª—é—á–∞—è –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—é –ª–∏—Ü–∞ –∏ —Ä—É–∫. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—é —Å–≤–µ—Ç–æ–≤—ã—Ö —ç—Ñ—Ñ–µ–∫—Ç–æ–≤ –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—ã–µ –∏ –Ω–µ–ª–æ–∫–∞–ª—å–Ω—ã–µ, –∏—Å–ø–æ–ª—å–∑—É—è –æ–±—É—á–∞–µ–º—ã–µ –∑–æ–Ω–∞–ª—å–Ω—ã–µ –≥–∞—Ä–º–æ–Ω–∏–∫–∏ –¥–ª—è –¥–∏—Ñ—Ñ—É–∑–Ω–æ–≥–æ –ø–µ—Ä–µ–Ω–æ—Å–∞ –æ—Å–≤–µ—â–µ–Ω–∏—è –∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—É—é –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ç–µ–Ω–µ–π. –ú–µ—Ç–æ–¥ —Ç–∞–∫–∂–µ –≤–∫–ª—é—á–∞–µ—Ç –æ—Ç–ª–æ–∂–µ–Ω–Ω—ã–π —à–µ–π–¥–∏–Ω–≥ –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –∑–µ—Ä–∫–∞–ª—å–Ω–æ–≥–æ –ø–µ—Ä–µ–Ω–æ—Å–∞ –æ—Å–≤–µ—â–µ–Ω–∏—è. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —É—Å–ø–µ—à–Ω–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∫ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ, —Ç–∞–∫ –∏ –Ω–µ–ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –ø–µ—Ä–µ–Ω–æ—Å–∞ —Å–≤–µ—Ç–∞ –¥–ª—è –ø–æ–ª–Ω–æ—Ä–∞–∑–º–µ—Ä–Ω—ã—Ö –∞–≤–∞—Ç–∞—Ä–æ–≤ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å—é –∫ –æ–±–æ–±—â–µ–Ω–∏—é –≤ –Ω–æ–≤—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö –æ—Å–≤–µ—â–µ–Ω–∏—è –∏ –ø–æ–∑–∞—Ö.'}, 'en': {'title': 'Realistic Relightable Avatars Through Advanced Light Transport Modeling', 'desc': 'This paper presents a novel method for creating relightable full-body avatars that capture intricate details like facial features and hands. The authors tackle the challenge of how body movements affect lighting and appearance by separating light transport into local and non-local effects. They utilize learnable zonal harmonics to efficiently model local changes in appearance due to body articulation, while a shadow network predicts non-local shadowing effects between body parts. The proposed approach enhances the realism of avatars under varying lighting conditions and poses, demonstrating improved generalization capabilities.'}, 'zh': {'title': 'ÂèØÈáçÂÖâÁÖßÁöÑÂÖ®Ë∫´Â§¥ÂÉèÂª∫Ê®°Êñ∞ÊñπÊ≥ï', 'desc': 'Êàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊñπÊ≥ïÔºåÁß∞‰∏∫ÂèØÈáçÂÖâÁÖßÁöÑÂÖ®Ë∫´È´òÊñØÁºñÁ†ÅÂ§¥ÂÉèÔºåÊó®Âú®Âª∫Ê®°ÂÖ∑ÊúâÁªÜËá¥Èù¢ÈÉ®ÂíåÊâãÈÉ®ÁâπÂæÅÁöÑÂÖ®Ë∫´Â§¥ÂÉè„ÄÇËØ•ÊñπÊ≥ïËß£ÂÜ≥‰∫ÜÁî±‰∫éË∫´‰ΩìÂÖ≥ËäÇËøêÂä®ÂºïËµ∑ÁöÑÂ§ßÂèòÂΩ¢ÂØπÂ§ñËßÇÁöÑÂΩ±ÂìçÔºåÁâπÂà´ÊòØÂÖâ‰º†ËæìÁöÑÂèòÂåñ„ÄÇÊàë‰ª¨Â∞ÜÂÖâ‰º†ËæìÂàÜËß£‰∏∫Â±ÄÈÉ®ÂíåÈùûÂ±ÄÈÉ®ÊïàÂ∫îÔºå‰ΩøÁî®ÂèØÂ≠¶‰π†ÁöÑÂå∫ÂüüË∞êÊ≥¢Êù•Âª∫Ê®°Â±ÄÈÉ®Â§ñËßÇÂèòÂåñÔºåÂπ∂ÂºïÂÖ•Èò¥ÂΩ±ÁΩëÁªúÊù•È¢ÑÊµãË∫´‰ΩìÈÉ®‰Ωç‰πãÈó¥ÁöÑÈò¥ÂΩ±„ÄÇÊúÄÁªàÔºåÊàë‰ª¨ÈááÁî®Âª∂ËøüÁùÄËâ≤ÊñπÊ≥ïÊù•Âª∫Ê®°ÈïúÈù¢ÂèçÂ∞ÑÔºå‰ª•Êõ¥Â•ΩÂú∞ÊçïÊçâÂèçÂ∞ÑÂíåÈ´òÂÖâÊïàÊûú„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2501.13925', 'title': 'GeoPixel: Pixel Grounding Large Multimodal Model in Remote Sensing', 'url': 'https://huggingface.co/papers/2501.13925', 'abstract': 'Recent advances in large multimodal models (LMMs) have recognized fine-grained grounding as an imperative factor of visual understanding and dialogue. However, the benefits of such representation in LMMs are limited to the natural image domain, and these models perform poorly for remote sensing (RS). The distinct overhead viewpoint, scale variation, and presence of small objects in high-resolution RS imagery present a unique challenge in region-level comprehension. Moreover, the development of the grounding conversation capability of LMMs within RS is hindered by the lack of granular, RS domain-specific grounded data. Addressing these limitations, we propose GeoPixel - the first end-to-end high resolution RS-LMM that supports pixel-level grounding. This capability allows fine-grained visual perception by generating interleaved masks in conversation. GeoPixel supports up to 4K HD resolution in any aspect ratio, ideal for high-precision RS image analysis. To support the grounded conversation generation (GCG) in RS imagery, we curate a visually grounded dataset GeoPixelD through a semi-automated pipeline that utilizes set-of-marks prompting and spatial priors tailored for RS data to methodically control the data generation process. GeoPixel demonstrates superior performance in pixel-level comprehension, surpassing existing LMMs in both single-target and multi-target segmentation tasks. Our methodological ablation studies validate the effectiveness of each component in the overall architecture. Our code and data will be publicly released.', 'score': 3, 'issue_id': 1883, 'pub_date': '2025-01-23', 'pub_date_card': {'ru': '23 —è–Ω–≤–∞—Ä—è', 'en': 'January 23', 'zh': '1Êúà23Êó•'}, 'hash': '0c6257aa10e28148', 'authors': ['Akashah Shabbir', 'Mohammed Zumri', 'Mohammed Bennamoun', 'Fahad S. Khan', 'Salman Khan'], 'affiliations': ['Australian National University', 'Linkoping University', 'Mohamed bin Zayed University of AI', 'The University of Western Australia'], 'pdf_title_img': 'assets/pdf/title_img/2501.13925.jpg', 'data': {'categories': ['#open_source', '#architecture', '#dataset', '#multimodal', '#data', '#games', '#optimization'], 'emoji': 'üõ∞Ô∏è', 'ru': {'title': 'GeoPixel: –ù–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏ –≤ –∞–Ω–∞–ª–∏–∑–µ —Å–ø—É—Ç–Ω–∏–∫–æ–≤—ã—Ö —Å–Ω–∏–º–∫–æ–≤', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç GeoPixel - –ø–µ—Ä–≤—É—é –º–æ–¥–µ–ª—å –¥–ª—è –¥–∏—Å—Ç–∞–Ω—Ü–∏–æ–Ω–Ω–æ–≥–æ –∑–æ–Ω–¥–∏—Ä–æ–≤–∞–Ω–∏—è —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø–∏–∫—Å–µ–ª—å–Ω–æ–≥–æ —É—Ä–æ–≤–Ω—è –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏. –ú–æ–¥–µ–ª—å —Å–ø–æ—Å–æ–±–Ω–∞ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –¥–æ 4K, —á—Ç–æ –∏–¥–µ–∞–ª—å–Ω–æ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Å–ø—É—Ç–Ω–∏–∫–æ–≤—ã—Ö —Å–Ω–∏–º–∫–æ–≤. –î–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –±—ã–ª —Å–æ–∑–¥–∞–Ω —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç GeoPixelD —Å –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø–∏–∫—Å–µ–ª–µ–π. GeoPixel –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤ –∑–∞–¥–∞—á–∞—Ö —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∫–∞–∫ –æ–¥–∏–Ω–æ—á–Ω—ã—Ö, —Ç–∞–∫ –∏ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ —Å–ø—É—Ç–Ω–∏–∫–æ–≤—ã—Ö —Å–Ω–∏–º–∫–∞—Ö.'}, 'en': {'title': 'GeoPixel: Revolutionizing Remote Sensing with Pixel-Level Grounding', 'desc': "This paper introduces GeoPixel, a novel large multimodal model designed specifically for remote sensing imagery. It addresses the challenges of fine-grained grounding in high-resolution images, which are often complicated by factors like scale variation and small object presence. GeoPixel enhances visual understanding by enabling pixel-level grounding and generating interleaved masks during conversations. The authors also present a new dataset, GeoPixelD, which is tailored for remote sensing tasks and supports the model's grounded conversation capabilities."}, 'zh': {'title': 'GeoPixelÔºöÈ´òÂàÜËæ®ÁéáÈÅ•ÊÑüÂõæÂÉèÁöÑÂÉèÁ¥†Á∫ßÁêÜËß£', 'desc': 'ÊúÄËøëÔºåÂ§ßÂûãÂ§öÊ®°ÊÄÅÊ®°ÂûãÔºàLMMsÔºâÁöÑËøõÂ±ïË°®ÊòéÔºåÁªÜÁ≤íÂ∫¶ÁöÑÂü∫Á°ÄÊòØËßÜËßâÁêÜËß£ÂíåÂØπËØùÁöÑÈáçË¶ÅÂõ†Á¥†„ÄÇÁÑ∂ËÄåÔºåËøô‰∫õÊ®°ÂûãÂú®ÈÅ•ÊÑüÔºàRSÔºâÈ¢ÜÂüüÁöÑË°®Áé∞ËæÉÂ∑ÆÔºå‰∏ªË¶ÅÊòØÁî±‰∫éÈÅ•ÊÑüÂõæÂÉèÁöÑÁã¨ÁâπÊåëÊàòÔºåÂ¶ÇËßÜËßí„ÄÅÂ∞∫Â∫¶ÂèòÂåñÂíåÂ∞èÁâ©‰ΩìÁöÑÂ≠òÂú®„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜGeoPixelÔºåËøôÊòØÁ¨¨‰∏Ä‰∏™ÊîØÊåÅÂÉèÁ¥†Á∫ßÂü∫Á°ÄÁöÑÈ´òÂàÜËæ®ÁéáRS-LMMÔºåËÉΩÂ§üÁîüÊàê‰∫§ÈîôÁöÑÊé©Á†Å‰ª•ÂÆûÁé∞ÁªÜÁ≤íÂ∫¶ÁöÑËßÜËßâÊÑüÁü•„ÄÇGeoPixelÂú®ÂçïÁõÆÊ†áÂíåÂ§öÁõÆÊ†áÂàÜÂâ≤‰ªªÂä°‰∏≠Ë°®Áé∞‰ºò‰∫éÁé∞ÊúâÁöÑLMMsÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®È´òÁ≤æÂ∫¶ÈÅ•ÊÑüÂõæÂÉèÂàÜÊûê‰∏≠ÁöÑÊΩúÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2501.11325', 'title': 'CatV2TON: Taming Diffusion Transformers for Vision-Based Virtual Try-On with Temporal Concatenation', 'url': 'https://huggingface.co/papers/2501.11325', 'abstract': 'Virtual try-on (VTON) technology has gained attention due to its potential to transform online retail by enabling realistic clothing visualization of images and videos. However, most existing methods struggle to achieve high-quality results across image and video try-on tasks, especially in long video scenarios. In this work, we introduce CatV2TON, a simple and effective vision-based virtual try-on (V2TON) method that supports both image and video try-on tasks with a single diffusion transformer model. By temporally concatenating garment and person inputs and training on a mix of image and video datasets, CatV2TON achieves robust try-on performance across static and dynamic settings. For efficient long-video generation, we propose an overlapping clip-based inference strategy that uses sequential frame guidance and Adaptive Clip Normalization (AdaCN) to maintain temporal consistency with reduced resource demands. We also present ViViD-S, a refined video try-on dataset, achieved by filtering back-facing frames and applying 3D mask smoothing for enhanced temporal consistency. Comprehensive experiments demonstrate that CatV2TON outperforms existing methods in both image and video try-on tasks, offering a versatile and reliable solution for realistic virtual try-ons across diverse scenarios.', 'score': 2, 'issue_id': 1887, 'pub_date': '2025-01-20', 'pub_date_card': {'ru': '20 —è–Ω–≤–∞—Ä—è', 'en': 'January 20', 'zh': '1Êúà20Êó•'}, 'hash': '3b21eab627e1a9f7', 'authors': ['Zheng Chong', 'Wenqing Zhang', 'Shiyue Zhang', 'Jun Zheng', 'Xiao Dong', 'Haoxiang Li', 'Yiling Wu', 'Dongmei Jiang', 'Xiaodan Liang'], 'affiliations': ['National University of Singapore', 'Pengcheng Laboratory', 'Pixocial Technology', 'Sun Yat-Sen University'], 'pdf_title_img': 'assets/pdf/title_img/2501.11325.jpg', 'data': {'categories': ['#cv', '#multimodal', '#dataset', '#video'], 'emoji': 'üëö', 'ru': {'title': '–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –≤–∏—Ä—Ç—É–∞–ª—å–Ω–∞—è –ø—Ä–∏–º–µ—Ä–∫–∞ –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≤–∏–¥–µ–æ', 'desc': 'CatV2TON - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–π –ø—Ä–∏–º–µ—Ä–∫–∏ –æ–¥–µ–∂–¥—ã, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–∞–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —Ç–∞–∫ –∏ –≤–∏–¥–µ–æ. –ú–µ—Ç–æ–¥ –ø—Ä–∏–º–µ–Ω—è–µ—Ç —Ç–µ—Ö–Ω–∏–∫—É –∫–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏–∏ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ —Å–º–µ—à–∞–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≤–∏–¥–µ–æ –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –î–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ —Å –ø–µ—Ä–µ–∫—Ä—ã–≤–∞—é—â–∏–º–∏—Å—è –∫–ª–∏–ø–∞–º–∏ –∏ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ —É–ª—É—á—à–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç ViViD-S –¥–ª—è –∑–∞–¥–∞—á–∏ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–π –ø—Ä–∏–º–µ—Ä–∫–∏ –Ω–∞ –≤–∏–¥–µ–æ.'}, 'en': {'title': 'Transforming Virtual Try-Ons with CatV2TON: One Model, Many Scenarios!', 'desc': 'This paper presents CatV2TON, a novel virtual try-on method that utilizes a single diffusion transformer model for both image and video applications. It addresses the challenges of achieving high-quality results in long video scenarios by employing an overlapping clip-based inference strategy, which enhances temporal consistency. The method is trained on a diverse dataset that includes both images and videos, allowing it to perform effectively in various settings. Experimental results show that CatV2TON outperforms existing techniques, making it a promising solution for realistic virtual clothing visualization.'}, 'zh': {'title': 'CatV2TONÔºöÈ´òÊïàÁöÑËôöÊãüËØïÁ©øËß£ÂÜ≥ÊñπÊ°à', 'desc': 'ËôöÊãüËØïÁ©øÔºàVTONÔºâÊäÄÊúØÂú®Âú®Á∫øÈõ∂ÂîÆ‰∏≠ÂºïËµ∑‰∫ÜÂπøÊ≥õÂÖ≥Ê≥®ÔºåÂõ†‰∏∫ÂÆÉËÉΩÂ§üÂÆûÁé∞ÁúüÂÆûÁöÑÊúçË£ÖÂèØËßÜÂåñ„ÄÇÁé∞ÊúâÁöÑÊñπÊ≥ïÂú®ÂõæÂÉèÂíåËßÜÈ¢ëËØïÁ©ø‰ªªÂä°‰∏≠ÔºåÂ∞§ÂÖ∂ÊòØÈïøËßÜÈ¢ëÂú∫ÊôØ‰∏≠ÔºåÂæÄÂæÄÈöæ‰ª•ËææÂà∞È´òË¥®ÈáèÁöÑÊïàÊûú„ÄÇÊàë‰ª¨ÊèêÂá∫ÁöÑCatV2TONÊòØ‰∏ÄÁßçÁÆÄÂçïÊúâÊïàÁöÑÂü∫‰∫éËßÜËßâÁöÑËôöÊãüËØïÁ©øÊñπÊ≥ïÔºåËÉΩÂ§üÊîØÊåÅÂõæÂÉèÂíåËßÜÈ¢ëËØïÁ©ø‰ªªÂä°ÔºåÂπ∂‰ΩøÁî®Âçï‰∏ÄÁöÑÊâ©Êï£ÂèòÊç¢Âô®Ê®°Âûã„ÄÇÈÄöËøáÊó∂Èó¥‰∏äËøûÊé•ÊúçË£ÖÂíå‰∫∫Áâ©ËæìÂÖ•ÔºåÂπ∂Âú®Ê∑∑ÂêàÁöÑÂõæÂÉèÂíåËßÜÈ¢ëÊï∞ÊçÆÈõÜ‰∏äËøõË°åËÆ≠ÁªÉÔºåCatV2TONÂú®ÈùôÊÄÅÂíåÂä®ÊÄÅÂú∫ÊôØ‰∏≠ÈÉΩË°®Áé∞Âá∫Âº∫Â§ßÁöÑËØïÁ©øÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2403.14614', 'title': 'AdaIR: Adaptive All-in-One Image Restoration via Frequency Mining and Modulation', 'url': 'https://huggingface.co/papers/2403.14614', 'abstract': 'In the image acquisition process, various forms of degradation, including noise, haze, and rain, are frequently introduced. These degradations typically arise from the inherent limitations of cameras or unfavorable ambient conditions. To recover clean images from degraded versions, numerous specialized restoration methods have been developed, each targeting a specific type of degradation. Recently, all-in-one algorithms have garnered significant attention by addressing different types of degradations within a single model without requiring prior information of the input degradation type. However, these methods purely operate in the spatial domain and do not delve into the distinct frequency variations inherent to different degradation types. To address this gap, we propose an adaptive all-in-one image restoration network based on frequency mining and modulation. Our approach is motivated by the observation that different degradation types impact the image content on different frequency subbands, thereby requiring different treatments for each restoration task. Specifically, we first mine low- and high-frequency information from the input features, guided by the adaptively decoupled spectra of the degraded image. The extracted features are then modulated by a bidirectional operator to facilitate interactions between different frequency components. Finally, the modulated features are merged into the original input for a progressively guided restoration. With this approach, the model achieves adaptive reconstruction by accentuating the informative frequency subbands according to different input degradations. Extensive experiments demonstrate that the proposed method achieves state-of-the-art performance on different image restoration tasks, including denoising, dehazing, deraining, motion deblurring, and low-light image enhancement. Our code is available at https://github.com/c-yn/AdaIR.', 'score': 2, 'issue_id': 1883, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 –º–∞—Ä—Ç–∞', 'en': 'March 21', 'zh': '3Êúà21Êó•'}, 'hash': '54f7acd2a97e8313', 'authors': ['Yuning Cui', 'Syed Waqas Zamir', 'Salman Khan', 'Alois Knoll', 'Mubarak Shah', 'Fahad Shahbaz Khan'], 'affiliations': ['Australian National University', 'Inception Institute of Artificial Intelligence', 'Link√∂ping University', 'Mohammed Bin Zayed University of AI', 'Technical University of Munich', 'University of Central Florida'], 'pdf_title_img': 'assets/pdf/title_img/2403.14614.jpg', 'data': {'categories': ['#cv'], 'emoji': 'üñºÔ∏è', 'ru': {'title': '–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —á–∞—Å—Ç–æ—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞', 'desc': '–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –ø–æ—Å—Ç—Ä–∞–¥–∞–≤—à–∏—Ö –æ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –≤–∏–¥–æ–≤ –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–∏ (—à—É–º, —Ç—É–º–∞–Ω, –¥–æ–∂–¥—å –∏ —Ç.–¥.). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å, –∫–æ—Ç–æ—Ä–∞—è –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —á–∞—Å—Ç–æ—Ç–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –∏—Å–∫–∞–∂–µ–Ω–∏–π –∏ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ –ø—Ä–∏–º–µ–Ω—è–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏. –ü–æ–¥—Ö–æ–¥ –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –Ω–∏–∑–∫–æ- –∏ –≤—ã—Å–æ–∫–æ—á–∞—Å—Ç–æ—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –∏—Ö –º–æ–¥—É–ª—è—Ü–∏–∏ —Å –ø–æ–º–æ—â—å—é –¥–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –Ω–∞–∏–ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.'}, 'en': {'title': 'Adaptive Image Restoration through Frequency Modulation', 'desc': 'This paper presents an innovative image restoration network that adapts to various types of image degradation, such as noise, haze, and rain. Unlike traditional methods that focus solely on spatial domain processing, this approach utilizes frequency mining to identify and modulate low- and high-frequency information specific to each degradation type. By employing a bidirectional operator, the model enhances interactions between different frequency components, allowing for more effective restoration. The results show that this adaptive method outperforms existing techniques across multiple restoration tasks, demonstrating its versatility and effectiveness.'}, 'zh': {'title': 'Ëá™ÈÄÇÂ∫î‰∏Ä‰ΩìÂåñÂõæÂÉè‰øÆÂ§çÔºöÈ¢ëÁéáÈ©±Âä®ÁöÑÂàõÊñ∞', 'desc': 'Âú®ÂõæÂÉèËé∑ÂèñËøáÁ®ã‰∏≠ÔºåÂ∏∏Â∏∏‰ºöÂá∫Áé∞Âô™Â£∞„ÄÅÈõæÈúæÂíåÈõ®Ê∞¥Á≠âÂêÑÁßçÈÄÄÂåñÂΩ¢Âºè„ÄÇËøô‰∫õÈÄÄÂåñÈÄöÂ∏∏Ê∫ê‰∫éÁõ∏Êú∫ÁöÑÂõ∫ÊúâÈôêÂà∂Êàñ‰∏çÂà©ÁöÑÁéØÂ¢ÉÊù°‰ª∂„ÄÇ‰∏∫‰∫Ü‰ªéÈÄÄÂåñÂõæÂÉè‰∏≠ÊÅ¢Â§çÊ∏ÖÊô∞ÂõæÂÉèÔºåÂ∑≤ÁªèÂºÄÂèë‰∫ÜËÆ∏Â§ö‰∏ìÈó®ÁöÑ‰øÆÂ§çÊñπÊ≥ï„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÈ¢ëÁéáÊåñÊéòÂíåË∞ÉÂà∂ÁöÑËá™ÈÄÇÂ∫î‰∏Ä‰ΩìÂåñÂõæÂÉè‰øÆÂ§çÁΩëÁªúÔºåËÉΩÂ§üÂú®Âçï‰∏ÄÊ®°Âûã‰∏≠Â§ÑÁêÜ‰∏çÂêåÁ±ªÂûãÁöÑÈÄÄÂåñÔºå‰∏îÊó†ÈúÄËæìÂÖ•ÈÄÄÂåñÁ±ªÂûãÁöÑÂÖàÈ™å‰ø°ÊÅØ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2411.19458', 'title': 'Multiview Equivariance Improves 3D Correspondence Understanding with Minimal Feature Finetuning', 'url': 'https://huggingface.co/papers/2411.19458', 'abstract': 'Vision foundation models, particularly the ViT family, have revolutionized image understanding by providing rich semantic features. However, despite their success in 2D comprehension, their abilities on grasping 3D spatial relationships are still unclear. In this work, we evaluate and enhance the 3D awareness of ViT-based models. We begin by systematically assessing their ability to learn 3D equivariant features, specifically examining the consistency of semantic embeddings across different viewpoints. Our findings indicate that improved 3D equivariance leads to better performance on various downstream tasks, including pose estimation, tracking, and semantic transfer. Building on this insight, we propose a simple yet effective finetuning strategy based on 3D correspondences, which significantly enhances the 3D correspondence understanding of existing vision models. Remarkably, even finetuning on a single object for just one iteration results in substantial performance gains. All code and resources will be made publicly available to support further advancements in 3D-aware vision models. Our code is available at https://github.com/qq456cvb/3DCorrEnhance.', 'score': 2, 'issue_id': 1883, 'pub_date': '2025-11-29', 'pub_date_card': {'ru': '29 –Ω–æ—è–±—Ä—è', 'en': 'November 29', 'zh': '11Êúà29Êó•'}, 'hash': 'df24163a81379619', 'authors': ['Yang You', 'Yixin Li', 'Congyue Deng', 'Yue Wang', 'Leonidas Guibas'], 'affiliations': ['Department of Computer Science, Stanford University, U.S.A.', 'Department of Computer Science, University of Southern California, U.S.A.'], 'pdf_title_img': 'assets/pdf/title_img/2411.19458.jpg', 'data': {'categories': ['#cv', '#open_source', '#3d', '#training'], 'emoji': 'üßä', 'ru': {'title': '–ü–æ–≤—ã—à–µ–Ω–∏–µ 3D-–æ—Å–≤–µ–¥–æ–º–ª–µ–Ω–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è', 'desc': '–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—é –∏ —É–ª—É—á—à–µ–Ω–∏—é –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ç—Ä–µ—Ö–º–µ—Ä–Ω—ã—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—Ç–Ω–æ—à–µ–Ω–∏–π –º–æ–¥–µ–ª—è–º–∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–º–∏ –Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ ViT. –ê–≤—Ç–æ—Ä—ã –æ—Ü–µ–Ω–∏–≤–∞—é—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —ç—Ç–∏—Ö –º–æ–¥–µ–ª–µ–π –∏–∑—É—á–∞—Ç—å 3D-—ç–∫–≤–∏–≤–∞—Ä–∏–∞–Ω—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞—é—Ç, —á—Ç–æ —É–ª—É—á—à–µ–Ω–∏–µ 3D-—ç–∫–≤–∏–≤–∞—Ä–∏–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –ø–æ–≤—ã—à–µ–Ω–∏—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö. –û–Ω–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –¥–æ–æ–±—É—á–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ 3D-—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π, –∫–æ—Ç–æ—Ä–∞—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ç—Ä–µ—Ö–º–µ—Ä–Ω—ã—Ö —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏. –î–∞–∂–µ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –¥–æ–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ–¥–Ω–æ–º –æ–±—ä–µ–∫—Ç–µ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–º—É –ø–æ–≤—ã—à–µ–Ω–∏—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.'}, 'en': {'title': 'Enhancing 3D Awareness in Vision Transformers', 'desc': "This paper focuses on improving the 3D understanding capabilities of Vision Transformer (ViT) models, which are known for their strong performance in 2D image analysis. The authors evaluate how well these models can learn 3D equivariant features, which are essential for maintaining consistent semantic meanings across different viewpoints. They discover that enhancing 3D equivariance significantly boosts the models' performance on tasks like pose estimation and tracking. To achieve this, they introduce a straightforward finetuning method that leverages 3D correspondences, showing that even minimal finetuning can lead to notable improvements in 3D comprehension."}, 'zh': {'title': 'ÊèêÂçáËßÜËßâÊ®°ÂûãÁöÑ3DÁêÜËß£ËÉΩÂäõ', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫ÜËßÜËßâÂü∫Á°ÄÊ®°ÂûãÔºåÁâπÂà´ÊòØViTÁ≥ªÂàóÂú®ÂõæÂÉèÁêÜËß£‰∏≠ÁöÑÂ∫îÁî®ÔºåÂ∞§ÂÖ∂ÊòØÂÖ∂Âú®3DÁ©∫Èó¥ÂÖ≥Á≥ªÁêÜËß£ÊñπÈù¢ÁöÑËÉΩÂäõ„ÄÇÊàë‰ª¨Á≥ªÁªüËØÑ‰º∞‰∫ÜËøô‰∫õÊ®°ÂûãÂ≠¶‰π†3DÁ≠âÂèòÁâπÂæÅÁöÑËÉΩÂäõÔºåÈáçÁÇπÂàÜÊûê‰∫Ü‰∏çÂêåËßÜËßí‰∏ãËØ≠‰πâÂµåÂÖ•ÁöÑ‰∏ÄËá¥ÊÄß„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÊèêÂçá3DÁ≠âÂèòÊÄßÂèØ‰ª•ÊòæËëóÊîπÂñÑÂú®ÂßøÊÄÅ‰º∞ËÆ°„ÄÅË∑üË∏™ÂíåËØ≠‰πâËΩ¨ÁßªÁ≠â‰∏ãÊ∏∏‰ªªÂä°‰∏≠ÁöÑË°®Áé∞„ÄÇÂü∫‰∫éËøô‰∏ÄÂèëÁé∞ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÁÆÄÂçïÊúâÊïàÁöÑÂæÆË∞ÉÁ≠ñÁï•ÔºåÈÄöËøá3DÂØπÂ∫îÂÖ≥Á≥ªÊòæËëóÂ¢ûÂº∫Áé∞ÊúâËßÜËßâÊ®°ÂûãÁöÑ3DÁêÜËß£ËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2406.18516', 'title': 'Denoising as Adaptation: Noise-Space Domain Adaptation for Image Restoration', 'url': 'https://huggingface.co/papers/2406.18516', 'abstract': 'Although learning-based image restoration methods have made significant progress, they still struggle with limited generalization to real-world scenarios due to the substantial domain gap caused by training on synthetic data. Existing methods address this issue by improving data synthesis pipelines, estimating degradation kernels, employing deep internal learning, and performing domain adaptation and regularization. Previous domain adaptation methods have sought to bridge the domain gap by learning domain-invariant knowledge in either feature or pixel space. However, these techniques often struggle to extend to low-level vision tasks within a stable and compact framework. In this paper, we show that it is possible to perform domain adaptation via the noise space using diffusion models. In particular, by leveraging the unique property of how auxiliary conditional inputs influence the multi-step denoising process, we derive a meaningful diffusion loss that guides the restoration model in progressively aligning both restored synthetic and real-world outputs with a target clean distribution. We refer to this method as denoising as adaptation. To prevent shortcuts during joint training, we present crucial strategies such as channel-shuffling layer and residual-swapping contrastive learning in the diffusion model. They implicitly blur the boundaries between conditioned synthetic and real data and prevent the reliance of the model on easily distinguishable features. Experimental results on three classical image restoration tasks, namely denoising, deblurring, and deraining, demonstrate the effectiveness of the proposed method.', 'score': 0, 'issue_id': 1883, 'pub_date': '2025-06-26', 'pub_date_card': {'ru': '26 –∏—é–Ω—è', 'en': 'June 26', 'zh': '6Êúà26Êó•'}, 'hash': 'ef06fd4cf15b3995', 'authors': ['Kang Liao', 'Zongsheng Yue', 'Zhouxia Wang', 'Chen Change Loy'], 'affiliations': ['S-Lab, Nanyang Technological University'], 'pdf_title_img': 'assets/pdf/title_img/2406.18516.jpg', 'data': {'categories': ['#training', '#diffusion', '#data', '#optimization', '#cv', '#transfer_learning'], 'emoji': 'üñºÔ∏è', 'ru': {'title': '–ê–¥–∞–ø—Ç–∞—Ü–∏—è –¥–æ–º–µ–Ω–∞ —á–µ—Ä–µ–∑ —à—É–º: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π', 'desc': "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –¥–æ–º–µ–Ω–∞ –¥–ª—è –∑–∞–¥–∞—á –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –≤—ã–ø–æ–ª–Ω—è—Ç—å –∞–¥–∞–ø—Ç–∞—Ü–∏—é —á–µ—Ä–µ–∑ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ —à—É–º–∞, –∏—Å–ø–æ–ª—å–∑—É—è —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å–≤–æ–π—Å—Ç–≤–∞ –º–Ω–æ–≥–æ—Å—Ç—É–ø–µ–Ω—á–∞—Ç–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞ —É–¥–∞–ª–µ–Ω–∏—è —à—É–º–∞. –ú–µ—Ç–æ–¥, –Ω–∞–∑–≤–∞–Ω–Ω—ã–π 'denoising as adaptation', –Ω–∞–ø—Ä–∞–≤–ª—è–µ—Ç –º–æ–¥–µ–ª—å –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –Ω–∞ –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –∫–∞–∫ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö, —Ç–∞–∫ –∏ —Ä–µ–∞–ª—å–Ω—ã—Ö –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å —Ü–µ–ª–µ–≤—ã–º —á–∏—Å—Ç—ã–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –∑–∞–¥–∞—á–∞—Ö —à—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏—è, —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è —Ä–∞–∑–º—ã—Ç–∏—è –∏ —É–¥–∞–ª–µ–Ω–∏—è –¥–æ–∂–¥—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞."}, 'en': {'title': 'Bridging the Gap: Denoising as Adaptation for Image Restoration', 'desc': "This paper addresses the challenge of image restoration methods that struggle to generalize to real-world scenarios due to the gap between synthetic training data and real data. The authors propose a novel approach called 'denoising as adaptation' that utilizes diffusion models to perform domain adaptation in the noise space. By introducing a diffusion loss that aligns synthetic and real-world outputs, the method effectively guides the restoration process. Additionally, strategies like channel-shuffling and residual-swapping contrastive learning are implemented to enhance the model's robustness against overfitting to distinguishable features."}, 'zh': {'title': 'ÂéªÂô™‰Ωú‰∏∫ÈÄÇÂ∫îÔºöÊèêÂçáÂõæÂÉèÊÅ¢Â§çÁöÑÈ¢ÜÂüüÈÄÇÂ∫îËÉΩÂäõ', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂü∫‰∫éÂ≠¶‰π†ÁöÑÂõæÂÉèÊÅ¢Â§çÊñπÊ≥ïÂú®ÁúüÂÆûÂú∫ÊôØ‰∏≠ÁöÑÊ≥õÂåñËÉΩÂäõ‰∏çË∂≥ÁöÑÈóÆÈ¢òÔºå‰∏ªË¶ÅÊòØÁî±‰∫éËÆ≠ÁªÉÊï∞ÊçÆ‰∏éÁúüÂÆûÊï∞ÊçÆ‰πãÈó¥Â≠òÂú®ÊòæËëóÁöÑÈ¢ÜÂüüÂ∑ÆË∑ù„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÈ¢ÜÂüüÈÄÇÂ∫îÊñπÊ≥ïÔºåÈÄöËøáÂô™Â£∞Á©∫Èó¥Âà©Áî®Êâ©Êï£Ê®°ÂûãÊù•ÂÆûÁé∞ÔºåÁâπÂà´ÊòØÂà©Áî®ËæÖÂä©Êù°‰ª∂ËæìÂÖ•ÂØπÂ§öÊ≠•ÂéªÂô™ËøáÁ®ãÁöÑÂΩ±ÂìçÔºåÂØºÂá∫‰∫Ü‰∏ÄÁßçÊúâÊÑè‰πâÁöÑÊâ©Êï£ÊçüÂ§±„ÄÇËØ•ÊñπÊ≥ïÁß∞‰∏∫ÂéªÂô™‰Ωú‰∏∫ÈÄÇÂ∫îÔºåËÉΩÂ§üÈÄêÊ≠•ÂØπÈΩêÊÅ¢Â§çÁöÑÂêàÊàêÂõæÂÉèÂíåÁúüÂÆûÂõæÂÉè„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®ÂéªÂô™„ÄÅÂéªÊ®°Á≥äÂíåÂéªÈõ®Á≠âÁªèÂÖ∏ÂõæÂÉèÊÅ¢Â§ç‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇ'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents', '#agi (1)', '#alignment', '#architecture (1)', '#audio', '#benchmark (5)', '#cv (5)', '#data (2)', '#dataset (3)', '#diffusion (1)', '#ethics', '#games (1)', '#graphs', '#hallucinations', '#healthcare (1)', '#inference', '#interpretability (1)', '#leakage', '#long_context', '#low_resource', '#machine_translation', '#math', '#multilingual', '#multimodal (4)', '#open_source (3)', '#optimization (4)', '#plp', '#rag (1)', '#reasoning (2)', '#rl (1)', '#rlhf', '#robotics', '#science (2)', '#security', '#small_models', '#story_generation', '#survey (1)', '#synthetic', '#training (4)', '#transfer_learning (2)', '#video (1)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `üè∑Ô∏è ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `üè∑Ô∏è ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            üî∫ ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'üîÑ ' + getTimeDiff('2025-01-28 05:10',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "—Ä–µ–π—Ç–∏–Ω–≥—É",
                    pub_date: "–¥–∞—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏",
                    issue_id: "–¥–æ–±–∞–≤–ª–µ–Ω–∏—é –Ω–∞ HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "ËØÑÂàÜ",
                    pub_date: "ÂèëÂ∏ÉÊó•Êúü",
                    issue_id: "HF‰∏ä‰º†Êó•Êúü"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-01-28 05:10')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-01-28 05:10')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    