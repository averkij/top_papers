
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 30 papers. December 17.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">17 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ</span> | <span id="title-articles-count">30 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-12-16.html">â¬…ï¸ <span id="prev-date">16.12</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-12-18.html">â¡ï¸ <span id="next-date">18.12</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-12.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '17 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 17', 'zh': '12æœˆ17æ—¥'};
        let feedDateNext = {'ru': '18.12', 'en': '12/18', 'zh': '12æœˆ18æ—¥'};
        let feedDatePrev = {'ru': '16.12', 'en': '12/16', 'zh': '12æœˆ16æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2512.14691', 'title': 'MMGR: Multi-Modal Generative Reasoning', 'url': 'https://huggingface.co/papers/2512.14691', 'abstract': 'MMGR evaluates video and image models across reasoning abilities in multiple domains, revealing performance gaps and highlighting limitations in perceptual data and causal correctness.  \t\t\t\t\tAI-generated summary \t\t\t\t Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.', 'score': 52, 'issue_id': 95, 'pub_date': '2025-12-16', 'pub_date_card': {'ru': '16 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 16', 'zh': '12æœˆ16æ—¥'}, 'hash': '8daed328c3a9296c', 'authors': ['Zefan Cai', 'Haoyi Qiu', 'Tianyi Ma', 'Haozhe Zhao', 'Gengze Zhou', 'Kung-Hsiang Huang', 'Parisa Kordjamshidi', 'Minjia Zhang', 'Xiao Wen', 'Jiuxiang Gu', 'Nanyun Peng', 'Junjie Hu'], 'affiliations': ['Adobe Research', 'Michigan State University', 'Microsoft', 'Salesforce AI Research', 'University of Adelaide', 'University of California, Los Angeles', 'University of Illinois Urbana-Champaign', 'University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2512.14691.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#survey', '#multimodal', '#video', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ›Ğ¾Ğ³Ğ¸ĞºĞ° Ğ²Ğ°Ğ¶Ğ½ĞµĞµ ĞºÑ€Ğ°ÑĞ¾Ñ‚Ñ‹: Ğ¾Ñ†ĞµĞ½ĞºĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ MMGR â€” ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹æ¡†æ¶Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ¿ÑÑ‚Ğ¸ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑÑ…: Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ, Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ, Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ (3D Ğ¸ 2D) Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸: Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ, Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰Ñ‘Ğ½Ğ½ÑƒÑ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ¼Ñ‹ÑĞ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ² ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… (Ğ¼ĞµĞ½ĞµĞµ 10% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ARC-AGI). ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ğ°Ñ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚ Ğ¿ĞµÑ€Ñ†ĞµĞ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ¸Ñ Ğ½Ğ°Ğ´ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ.'}, 'en': {'title': 'Bridging the Gap: Evaluating Reasoning in Video and Image Models with MMGR', 'desc': 'The paper introduces MMGR, a new evaluation framework designed to assess the reasoning abilities of video and image models across various domains. It identifies significant performance gaps in current models, particularly in tasks requiring abstract reasoning and long-term spatial planning. The framework emphasizes the importance of causal correctness and global consistency, rather than just perceptual quality. By benchmarking leading models, the study reveals their limitations and suggests a need for more reasoning-aware generative models.'}, 'zh': {'title': 'æ¨ç†èƒ½åŠ›è¯„ä¼°ï¼šæ­ç¤ºç”Ÿæˆæ¨¡å‹çš„å±€é™æ€§', 'desc': 'MMGRï¼ˆå¤šæ¨¡æ€ç”Ÿæˆæ¨ç†è¯„ä¼°ä¸åŸºå‡†ï¼‰æ˜¯ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨æµ‹è¯•è§†é¢‘å’Œå›¾åƒæ¨¡å‹åœ¨å¤šä¸ªé¢†åŸŸçš„æ¨ç†èƒ½åŠ›ã€‚å®ƒåŸºäºäº”ç§æ¨ç†èƒ½åŠ›ï¼šç‰©ç†ã€é€»è¾‘ã€ä¸‰ç»´ç©ºé—´ã€äºŒç»´ç©ºé—´å’Œæ—¶é—´ã€‚é€šè¿‡å¯¹ç°æœ‰è§†é¢‘å’Œå›¾åƒç”Ÿæˆæ¨¡å‹çš„è¯„ä¼°ï¼ŒMMGRæ­ç¤ºäº†å®ƒä»¬åœ¨æŠ½è±¡æ¨ç†å’Œé•¿æ—¶é—´ç©ºé—´è§„åˆ’æ–¹é¢çš„æ€§èƒ½å·®è·ã€‚è¯¥æ¡†æ¶å¼ºè°ƒäº†å½“å‰æ¨¡å‹çš„å±€é™æ€§ï¼ŒåŒ…æ‹¬å¯¹æ„ŸçŸ¥æ•°æ®çš„è¿‡åº¦ä¾èµ–å’Œå› æœæ­£ç¡®æ€§çš„ä¸è¶³ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.14614', 'title': 'WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling', 'url': 'https://huggingface.co/papers/2512.14614', 'abstract': "WorldPlay is a streaming video diffusion model that achieves real-time, interactive world modeling with long-term geometric consistency by using a Dual Action Representation, Reconstituted Context Memory, and Context Forcing.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper presents WorldPlay, a streaming video diffusion model that enables real-time, interactive world modeling with long-term geometric consistency, resolving the trade-off between speed and memory that limits current methods. WorldPlay draws power from three key innovations. 1) We use a Dual Action Representation to enable robust action control in response to the user's keyboard and mouse inputs. 2) To enforce long-term consistency, our Reconstituted Context Memory dynamically rebuilds context from past frames and uses temporal reframing to keep geometrically important but long-past frames accessible, effectively alleviating memory attenuation. 3) We also propose Context Forcing, a novel distillation method designed for memory-aware model. Aligning memory context between the teacher and student preserves the student's capacity to use long-range information, enabling real-time speeds while preventing error drift. Taken together, WorldPlay generates long-horizon streaming 720p video at 24 FPS with superior consistency, comparing favorably with existing techniques and showing strong generalization across diverse scenes. Project page and online demo can be found: https://3d-models.hunyuan.tencent.com/world/ and https://3d.hunyuan.tencent.com/sceneTo3D.", 'score': 42, 'issue_id': 95, 'pub_date': '2025-12-16', 'pub_date_card': {'ru': '16 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 16', 'zh': '12æœˆ16æ—¥'}, 'hash': '1f6f5ff8f0e4af75', 'authors': ['Wenqiang Sun', 'Haiyu Zhang', 'Haoyuan Wang', 'Junta Wu', 'Zehan Wang', 'Zhenwei Wang', 'Yunhong Wang', 'Jun Zhang', 'Tengfei Wang', 'Chunchao Guo'], 'affiliations': ['Beihang University', 'Hong Kong University of Science and Technology', 'Tencent Hunyuan'], 'pdf_title_img': 'assets/pdf/title_img/2512.14614.jpg', 'data': {'categories': ['#training', '#3d', '#diffusion', '#architecture', '#video', '#long_context'], 'emoji': 'ğŸ®', 'ru': {'title': 'Ğ˜Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¼Ğ¸Ñ€Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ', 'desc': 'WorldPlay â€” ÑÑ‚Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€Ñ‘Ñ…ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´: Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¹ Ğ²Ğ²Ğ¾Ğ´, Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½ÑƒÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ° Ğº Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğ¼ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼ Ğ¸Ğ· Ğ¿Ñ€Ğ¾ÑˆĞ»Ğ¾Ğ³Ğ¾, Ğ¸ Context Forcing â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ¾Ğ»Ğ³Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒÑ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¸ Ğ¾Ğ±ÑŠÑ‘Ğ¼Ğ¾Ğ¼ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼ 720p Ğ¿Ñ€Ğ¸ 24 FPS Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ… Ğ¸ ÑÑ†ĞµĞ½ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ñ… ÑÑ€ĞµĞ´.'}, 'en': {'title': 'Real-Time Interactive World Modeling with Long-Term Consistency', 'desc': 'WorldPlay is a novel streaming video diffusion model that allows for real-time interactive world modeling while maintaining long-term geometric consistency. It introduces a Dual Action Representation to effectively manage user inputs, ensuring responsive action control. The model employs a Reconstituted Context Memory that dynamically reconstructs context from previous frames, which helps retain important geometric information over time. Additionally, Context Forcing is utilized to align memory context between teacher and student models, enabling efficient use of long-range information and achieving high-quality video output at 24 frames per second.'}, 'zh': {'title': 'å®æ—¶äº’åŠ¨ä¸–ç•Œå»ºæ¨¡çš„åˆ›æ–°è§£å†³æ–¹æ¡ˆ', 'desc': 'WorldPlayæ˜¯ä¸€ç§æµåª’ä½“è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿå®ç°å®æ—¶ã€äº’åŠ¨çš„ä¸–ç•Œå»ºæ¨¡ï¼Œå¹¶ä¿æŒé•¿æœŸçš„å‡ ä½•ä¸€è‡´æ€§ã€‚è¯¥æ¨¡å‹é€šè¿‡ä¸‰é¡¹å…³é”®åˆ›æ–°æ¥è§£å†³é€Ÿåº¦å’Œå†…å­˜ä¹‹é—´çš„æƒè¡¡ã€‚é¦–å…ˆï¼ŒåŒé‡åŠ¨ä½œè¡¨ç¤ºä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ ¹æ®ç”¨æˆ·çš„é”®ç›˜å’Œé¼ æ ‡è¾“å…¥è¿›è¡Œç¨³å¥çš„åŠ¨ä½œæ§åˆ¶ã€‚å…¶æ¬¡ï¼Œé‡æ„ä¸Šä¸‹æ–‡è®°å¿†åŠ¨æ€é‡å»ºè¿‡å»å¸§çš„ä¸Šä¸‹æ–‡ï¼Œå¹¶é€šè¿‡æ—¶é—´é‡æ„ä¿æŒå‡ ä½•é‡è¦çš„æ—§å¸§å¯è®¿é—®ï¼Œä»è€Œå‡è½»å†…å­˜è¡°å‡çš„é—®é¢˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.12675', 'title': 'Scone: Bridging Composition and Distinction in Subject-Driven Image Generation via Unified Understanding-Generation Modeling', 'url': 'https://huggingface.co/papers/2512.12675', 'abstract': 'Scone integrates composition and distinction in image generation by using a two-stage training scheme with semantic alignment and attention-based masking, outperforming existing models on benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Subject-driven image generation has advanced from single- to multi-subject composition, while neglecting distinction, the ability to identify and generate the correct subject when inputs contain multiple candidates. This limitation restricts effectiveness in complex, realistic visual settings. We propose Scone, a unified understanding-generation method that integrates composition and distinction. Scone enables the understanding expert to act as a semantic bridge, conveying semantic information and guiding the generation expert to preserve subject identity while minimizing interference. A two-stage training scheme first learns composition, then enhances distinction through semantic alignment and attention-based masking. We also introduce SconeEval, a benchmark for evaluating both composition and distinction across diverse scenarios. Experiments demonstrate that Scone outperforms existing open-source models in composition and distinction tasks on two benchmarks. Our model, benchmark, and training data are available at: https://github.com/Ryann-Ran/Scone.', 'score': 35, 'issue_id': 95, 'pub_date': '2025-12-14', 'pub_date_card': {'ru': '14 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 14', 'zh': '12æœˆ14æ—¥'}, 'hash': '2e708e488331ba96', 'authors': ['Yuran Wang', 'Bohan Zeng', 'Chengzhuo Tong', 'Wenxuan Liu', 'Yang Shi', 'Xiaochen Ma', 'Hao Liang', 'Yuanxing Zhang', 'Wentao Zhang'], 'affiliations': ['Kling Team, Kuaishou Technology', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2512.12675.jpg', 'data': {'categories': ['#training', '#dataset', '#cv', '#benchmark', '#multimodal', '#open_source'], 'emoji': 'ğŸ¨', 'ru': {'title': 'ĞšĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼: Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Scone â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¹ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¸Ğ·ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ, Ğ·Ğ°Ñ‚ĞµĞ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ÑÑ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ñ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒĞµÑ‚ ĞºĞ°Ğº ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¼Ğ¾ÑÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ½Ğ¾Ğµ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ SconeEval â€” Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ….'}, 'en': {'title': 'Scone: Bridging Composition and Distinction in Image Generation', 'desc': "The paper introduces Scone, a novel approach to image generation that effectively combines composition and distinction. It addresses the challenge of generating images with multiple subjects by ensuring that each subject is accurately represented and distinguished. Scone employs a two-stage training process that first focuses on learning how to compose images and then enhances the model's ability to distinguish between subjects using semantic alignment and attention-based masking techniques. The results show that Scone outperforms existing models in both composition and distinction tasks, and a new benchmark, SconeEval, is introduced for evaluating these capabilities."}, 'zh': {'title': 'Sconeï¼šå›¾åƒç”Ÿæˆä¸­çš„ç»„åˆä¸åŒºåˆ†çš„å®Œç¾ç»“åˆ', 'desc': 'Sconeæ˜¯ä¸€ç§å›¾åƒç”Ÿæˆæ–¹æ³•ï¼Œå®ƒç»“åˆäº†ç»„åˆå’ŒåŒºåˆ†çš„èƒ½åŠ›ã€‚é€šè¿‡ä¸¤é˜¶æ®µçš„è®­ç»ƒæ–¹æ¡ˆï¼ŒSconeé¦–å…ˆå­¦ä¹ ç»„åˆï¼Œç„¶åé€šè¿‡è¯­ä¹‰å¯¹é½å’ŒåŸºäºæ³¨æ„åŠ›çš„æ©è”½æ¥å¢å¼ºåŒºåˆ†èƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•ä½¿å¾—ç”Ÿæˆæ¨¡å‹èƒ½å¤Ÿåœ¨å¤æ‚çš„è§†è§‰åœºæ™¯ä¸­æ›´å¥½åœ°è¯†åˆ«å’Œç”Ÿæˆæ­£ç¡®çš„ä¸»é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSconeåœ¨ç»„åˆå’ŒåŒºåˆ†ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰çš„å¼€æºæ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.13660', 'title': 'RoboTracer: Mastering Spatial Trace with Reasoning in Vision-Language Models for Robotics', 'url': 'https://huggingface.co/papers/2512.13660', 'abstract': 'RoboTracer, a 3D-aware visual language model, enhances spatial tracing by combining supervised and reinforcement fine-tuning with a universal spatial encoder and regression-supervised decoder, achieving state-of-the-art performance on TraceSpatial-Bench.  \t\t\t\t\tAI-generated summary \t\t\t\t Spatial tracing, as a fundamental embodied interaction ability for robots, is inherently challenging as it requires multi-step metric-grounded reasoning compounded with complex spatial referring and real-world metric measurement. However, existing methods struggle with this compositional task. To this end, we propose RoboTracer, a 3D-aware VLM that first achieves both 3D spatial referring and measuring via a universal spatial encoder and a regression-supervised decoder to enhance scale awareness during supervised fine-tuning (SFT). Moreover, RoboTracer advances multi-step metric-grounded reasoning via reinforcement fine-tuning (RFT) with metric-sensitive process rewards, supervising key intermediate perceptual cues to accurately generate spatial traces. To support SFT and RFT training, we introduce TraceSpatial, a large-scale dataset of 30M QA pairs, spanning outdoor/indoor/tabletop scenes and supporting complex reasoning processes (up to 9 steps). We further present TraceSpatial-Bench, a challenging benchmark filling the gap to evaluate spatial tracing. Experimental results show that RoboTracer surpasses baselines in spatial understanding, measuring, and referring, with an average success rate of 79.1%, and also achieves SOTA performance on TraceSpatial-Bench by a large margin, exceeding Gemini-2.5-Pro by 36% accuracy. Notably, RoboTracer can be integrated with various control policies to execute long-horizon, dynamic tasks across diverse robots (UR5, G1 humanoid) in cluttered real-world scenes.', 'score': 29, 'issue_id': 96, 'pub_date': '2025-12-15', 'pub_date_card': {'ru': '15 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 15', 'zh': '12æœˆ15æ—¥'}, 'hash': '5ffd03cb4f6287d8', 'authors': ['Enshen Zhou', 'Cheng Chi', 'Yibo Li', 'Jingkun An', 'Jiayuan Zhang', 'Shanyu Rong', 'Yi Han', 'Yuheng Ji', 'Mengzhen Liu', 'Pengwei Wang', 'Zhongyuan Wang', 'Lu Sheng', 'Shanghang Zhang'], 'affiliations': ['Beijing Academy of Artificial Intelligence', 'CASIA', 'School of Software, Beihang University', 'State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2512.13660.jpg', 'data': {'categories': ['#dataset', '#rl', '#3d', '#robotics', '#multimodal', '#benchmark', '#training'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ¢Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²', 'desc': 'RoboTracer â€” ÑÑ‚Ğ¾ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ°Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°ÑÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¸ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ğ¸ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑÑÑ‹Ğ»Ğ¾Ğº Ğ² Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ TraceSpatial Ñ 30 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ğ¼Ğ¸ QA-Ğ¿Ğ°Ñ€, Ğ¿Ğ¾ĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ñ‹ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. RoboTracer Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ TraceSpatial-Bench Ğ¸ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ.'}, 'en': {'title': 'RoboTracer: Mastering Spatial Tracing for Robots', 'desc': 'RoboTracer is a 3D-aware visual language model designed to improve spatial tracing, which is crucial for robots to interact with their environment. It combines supervised fine-tuning and reinforcement fine-tuning to enhance its ability to understand and measure spatial relationships. The model uses a universal spatial encoder and a regression-supervised decoder to achieve accurate 3D spatial referring and measuring. With the introduction of the TraceSpatial dataset and the TraceSpatial-Bench benchmark, RoboTracer demonstrates superior performance in spatial reasoning tasks, achieving a success rate of 79.1% and significantly outperforming previous models.'}, 'zh': {'title': 'RoboTracerï¼šæå‡æœºå™¨äººç©ºé—´è¿½è¸ªèƒ½åŠ›çš„åˆ›æ–°æ¨¡å‹', 'desc': 'RoboTraceræ˜¯ä¸€ç§3Dæ„ŸçŸ¥çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨æå‡ç©ºé—´è¿½è¸ªèƒ½åŠ›ã€‚å®ƒç»“åˆäº†ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å¾®è°ƒï¼Œä½¿ç”¨é€šç”¨ç©ºé—´ç¼–ç å™¨å’Œå›å½’ç›‘ç£è§£ç å™¨ï¼Œèƒ½å¤Ÿåœ¨TraceSpatial-Benchä¸Šå®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¯¥æ¨¡å‹é€šè¿‡å¼•å…¥TraceSpatialæ•°æ®é›†ï¼Œæ”¯æŒå¤æ‚çš„æ¨ç†è¿‡ç¨‹ï¼Œå¹¶åœ¨ç©ºé—´ç†è§£ã€æµ‹é‡å’Œå¼•ç”¨æ–¹é¢è¶…è¶Šäº†ç°æœ‰åŸºçº¿ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRoboTraceråœ¨å¤šæ­¥åº¦é‡æ¨ç†ä¸­è¡¨ç°å‡ºè‰²ï¼ŒæˆåŠŸç‡è¾¾åˆ°79.1%ï¼Œå¹¶åœ¨å®é™…åº”ç”¨ä¸­èƒ½å¤Ÿä¸å¤šç§æ§åˆ¶ç­–ç•¥é›†æˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.14051', 'title': 'OpenDataArena: A Fair and Open Arena for Benchmarking Post-Training Dataset Value', 'url': 'https://huggingface.co/papers/2512.14051', 'abstract': 'OpenDataArena (ODA) is an open platform that benchmarks post-training datasets for Large Language Models (LLMs) using a unified pipeline, multi-dimensional scoring, and data lineage exploration to enhance reproducibility and understanding of data impacts on model behavior.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid evolution of Large Language Models (LLMs) is predicated on the quality and diversity of post-training datasets. However, a critical dichotomy persists: while models are rigorously benchmarked, the data fueling them remains a black box--characterized by opaque composition, uncertain provenance, and a lack of systematic evaluation. This opacity hinders reproducibility and obscures the causal link between data characteristics and model behaviors. To bridge this gap, we introduce OpenDataArena (ODA), a holistic and open platform designed to benchmark the intrinsic value of post-training data. ODA establishes a comprehensive ecosystem comprising four key pillars: (i) a unified training-evaluation pipeline that ensures fair, open comparisons across diverse models (e.g., Llama, Qwen) and domains; (ii) a multi-dimensional scoring framework that profiles data quality along tens of distinct axes; (iii) an interactive data lineage explorer to visualize dataset genealogy and dissect component sources; and (iv) a fully open-source toolkit for training, evaluation, and scoring to foster data research. Extensive experiments on ODA--covering over 120 training datasets across multiple domains on 22 benchmarks, validated by more than 600 training runs and 40 million processed data points--reveal non-trivial insights. Our analysis uncovers the inherent trade-offs between data complexity and task performance, identifies redundancy in popular benchmarks through lineage tracing, and maps the genealogical relationships across datasets. We release all results, tools, and configurations to democratize access to high-quality data evaluation. Rather than merely expanding a leaderboard, ODA envisions a shift from trial-and-error data curation to a principled science of Data-Centric AI, paving the way for rigorous studies on data mixing laws and the strategic composition of foundation models.', 'score': 23, 'issue_id': 98, 'pub_date': '2025-12-16', 'pub_date_card': {'ru': '16 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 16', 'zh': '12æœˆ16æ—¥'}, 'hash': '122a06ee4d53b1f0', 'authors': ['Mengzhang Cai', 'Xin Gao', 'Yu Li', 'Honglin Lin', 'Zheng Liu', 'Zhuoshi Pan', 'Qizhi Pei', 'Xiaoran Shang', 'Mengyuan Sun', 'Zinan Tang', 'Xiaoyang Wang', 'Zhanping Zhong', 'Yun Zhu', 'Dahua Lin', 'Conghui He', 'Lijun Wu'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory, OpenDataLab'], 'pdf_title_img': 'assets/pdf/title_img/2512.14051.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#training', '#data'], 'emoji': 'ğŸ”', 'ru': {'title': 'é€è§†Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ: Ğ¾Ñ‚ Ñ‡Ñ‘Ñ€Ğ½Ğ¾Ğ³Ğ¾ ÑÑ‰Ğ¸ĞºĞ° Ğº Ğ½Ğ°ÑƒĞºĞµ Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ²', 'desc': 'OpenDataArena Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ÑƒÑ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñƒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ñ… Ğ¿Ñ€Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞŸĞ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ pipeline Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ². ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 120 Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ 40 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ğ¼Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ½ĞµÑ‚Ñ€Ğ¸Ğ²Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğ¸ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸Ğº Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞŸÑ€Ğ¾ĞµĞºÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ° Ğ¾Ñ‚ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğº Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñƒ Ğ² Data-Centric AI.'}, 'en': {'title': 'Unlocking Data Insights for Better AI Models', 'desc': 'OpenDataArena (ODA) is a platform that benchmarks post-training datasets for Large Language Models (LLMs) to improve understanding of how data affects model performance. It addresses the issue of data opacity by providing a unified pipeline for training and evaluation, allowing for fair comparisons across different models. ODA features a multi-dimensional scoring system to assess data quality and an interactive tool to explore the lineage of datasets, revealing their sources and relationships. By making its tools and findings publicly available, ODA aims to promote a more scientific approach to data-centric AI, moving beyond simple data collection to informed data curation.'}, 'zh': {'title': 'å¼€æ”¾æ•°æ®å¹³å°ï¼Œæå‡æ¨¡å‹æ•°æ®é€æ˜åº¦', 'desc': 'OpenDataArenaï¼ˆODAï¼‰æ˜¯ä¸€ä¸ªå¼€æ”¾å¹³å°ï¼Œæ—¨åœ¨é€šè¿‡ç»Ÿä¸€çš„æµç¨‹ã€å¤šç»´è¯„åˆ†å’Œæ•°æ®æ¥æºæ¢ç´¢æ¥è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åè®­ç»ƒæ•°æ®é›†çš„ä»·å€¼ã€‚è¯¥å¹³å°è§£å†³äº†æ•°æ®é€æ˜åº¦ä¸è¶³çš„é—®é¢˜ï¼Œå¸®åŠ©ç ”ç©¶äººå‘˜ç†è§£æ•°æ®ç‰¹æ€§ä¸æ¨¡å‹è¡Œä¸ºä¹‹é—´çš„å› æœå…³ç³»ã€‚ODAæä¾›äº†ä¸€ä¸ªå…¨é¢çš„ç”Ÿæ€ç³»ç»Ÿï¼ŒåŒ…æ‹¬ç»Ÿä¸€çš„è®­ç»ƒè¯„ä¼°æµç¨‹ã€å¤šç»´è¯„åˆ†æ¡†æ¶ã€äº¤äº’å¼æ•°æ®æ¥æºæ¢ç´¢å·¥å…·å’Œå¼€æºå·¥å…·åŒ…ï¼Œä»¥ä¿ƒè¿›æ•°æ®ç ”ç©¶ã€‚é€šè¿‡å¯¹120å¤šä¸ªè®­ç»ƒæ•°æ®é›†çš„å¹¿æ³›å®éªŒï¼ŒODAæ­ç¤ºäº†æ•°æ®å¤æ‚æ€§ä¸ä»»åŠ¡æ€§èƒ½ä¹‹é—´çš„æƒè¡¡ï¼Œæ¨åŠ¨äº†æ•°æ®ä¸­å¿ƒäººå·¥æ™ºèƒ½çš„ç§‘å­¦ç ”ç©¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.14336', 'title': 'Vector Prism: Animating Vector Graphics by Stratifying Semantic Structure', 'url': 'https://huggingface.co/papers/2512.14336', 'abstract': 'A framework aggregates weak predictions to recover semantic structure, enabling coherent SVG animations and improving VLM interactions with vector graphics.  \t\t\t\t\tAI-generated summary \t\t\t\t Scalable Vector Graphics (SVG) are central to modern web design, and the demand to animate them continues to grow as web environments become increasingly dynamic. Yet automating the animation of vector graphics remains challenging for vision-language models (VLMs) despite recent progress in code generation and motion planning. VLMs routinely mis-handle SVGs, since visually coherent parts are often fragmented into low-level shapes that offer little guidance of which elements should move together. In this paper, we introduce a framework that recovers the semantic structure required for reliable SVG animation and reveals the missing layer that current VLM systems overlook. This is achieved through a statistical aggregation of multiple weak part predictions, allowing the system to stably infer semantics from noisy predictions. By reorganizing SVGs into semantic groups, our approach enables VLMs to produce animations with far greater coherence. Our experiments demonstrate substantial gains over existing approaches, suggesting that semantic recovery is the key step that unlocks robust SVG animation and supports more interpretable interactions between VLMs and vector graphics.', 'score': 21, 'issue_id': 98, 'pub_date': '2025-12-16', 'pub_date_card': {'ru': '16 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 16', 'zh': '12æœˆ16æ—¥'}, 'hash': '0534175ef9f69f39', 'authors': ['Jooyeol Yun', 'Jaegul Choo'], 'affiliations': ['KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2512.14336.jpg', 'data': {'categories': ['#multimodal', '#cv'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ’Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸: ĞºĞ»ÑÑ‡ Ğº ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ SVG-Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ‡Ğ°ÑÑ‚Ğ¾ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ½Ğ° Ğ½Ğ¸Ğ·ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑĞ»Ğ°Ğ±Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸ Ğ¸Ğ· Ğ·Ğ°ÑˆÑƒĞ¼Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞŸÑƒÑ‚ĞµĞ¼ Ñ€ĞµĞ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ SVG Ğ² ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ³Ñ€ÑƒĞ¿Ğ¿Ñ‹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ - ÑÑ‚Ğ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ÑÑ‚Ğ°Ğ¿ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸.'}, 'en': {'title': 'Unlocking Coherent SVG Animation through Semantic Recovery', 'desc': 'This paper presents a new framework that enhances the animation of Scalable Vector Graphics (SVG) by recovering their semantic structure. Current vision-language models (VLMs) struggle with SVGs because they often break down coherent visual elements into smaller, less meaningful parts. The proposed method aggregates weak predictions to identify and group these parts semantically, allowing for more coherent animations. Experimental results show that this semantic recovery significantly improves the performance of VLMs in generating SVG animations and facilitates better interactions with vector graphics.'}, 'zh': {'title': 'æ¢å¤è¯­ä¹‰ç»“æ„ï¼Œæå‡SVGåŠ¨ç”»è¿è´¯æ€§', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ¡†æ¶ï¼Œé€šè¿‡èšåˆå¼±é¢„æµ‹æ¥æ¢å¤è¯­ä¹‰ç»“æ„ï¼Œä»è€Œå®ç°è¿è´¯çš„SVGåŠ¨ç”»ï¼Œå¹¶æ”¹å–„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä¸çŸ¢é‡å›¾å½¢çš„äº¤äº’ã€‚å°½ç®¡åœ¨ä»£ç ç”Ÿæˆå’Œè¿åŠ¨è§„åˆ’æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†è‡ªåŠ¨åŒ–çŸ¢é‡å›¾å½¢çš„åŠ¨ç”»ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç°æœ‰çš„VLMç³»ç»Ÿå¸¸å¸¸å°†è§†è§‰ä¸Šè¿è´¯çš„éƒ¨åˆ†åˆ†å‰²æˆä½çº§å½¢çŠ¶ï¼Œå¯¼è‡´æ— æ³•æœ‰æ•ˆæŒ‡å¯¼å…ƒç´ çš„ç§»åŠ¨ã€‚æˆ‘ä»¬çš„æ¡†æ¶é€šè¿‡ç»Ÿè®¡èšåˆå¤šä¸ªå¼±é¢„æµ‹ï¼Œç¨³å®šåœ°æ¨æ–­å‡ºè¯­ä¹‰ï¼Œä»è€Œä½¿VLMèƒ½å¤Ÿç”Ÿæˆæ›´å…·è¿è´¯æ€§çš„åŠ¨ç”»ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.12980', 'title': 'Reveal Hidden Pitfalls and Navigate Next Generation of Vector Similarity Search from Task-Centric Views', 'url': 'https://huggingface.co/papers/2512.12980', 'abstract': 'Iceberg is a benchmark suite that evaluates vector similarity search methods in real-world contexts, identifying key sources of performance degradation and providing guidance for selecting and tuning these methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Vector Similarity Search (VSS) in high-dimensional spaces is rapidly emerging as core functionality in next-generation database systems for numerous data-intensive services -- from embedding lookups in large language models (LLMs), to semantic information retrieval and recommendation engines. Current benchmarks, however, evaluate VSS primarily on the recall-latency trade-off against a ground truth defined solely by distance metrics, neglecting how retrieval quality ultimately impacts downstream tasks. This disconnect can mislead both academic research and industrial practice.   We present Iceberg, a holistic benchmark suite for end-to-end evaluation of VSS methods in realistic application contexts. From a task-centric view, Iceberg uncovers the Information Loss Funnel, which identifies three principal sources of end-to-end performance degradation: (1) Embedding Loss during feature extraction; (2) Metric Misuse, where distances poorly reflect task relevance; (3) Data Distribution Sensitivity, highlighting index robustness across skews and modalities. For a more comprehensive assessment, Iceberg spans eight diverse datasets across key domains such as image classification, face recognition, text retrieval, and recommendation systems. Each dataset, ranging from 1M to 100M vectors, includes rich, task-specific labels and evaluation metrics, enabling assessment of retrieval algorithms within the full application pipeline rather than in isolation. Iceberg benchmarks 13 state-of-the-art VSS methods and re-ranks them based on application-level metrics, revealing substantial deviations from traditional rankings derived purely from recall-latency evaluations. Building on these insights, we define a set of task-centric meta-features and derive an interpretable decision tree to guide practitioners in selecting and tuning VSS methods for their specific workloads.', 'score': 20, 'issue_id': 101, 'pub_date': '2025-12-15', 'pub_date_card': {'ru': '15 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 15', 'zh': '12æœˆ15æ—¥'}, 'hash': '4fe24544cc4a6a7e', 'authors': ['Tingyang Chen', 'Cong Fu', 'Jiahua Wu', 'Haotian Wu', 'Hua Fan', 'Xiangyu Ke', 'Yunjun Gao', 'Yabo Ni', 'Anxiang Zeng'], 'affiliations': ['Alibaba Cloud Computing', 'Nanyang Technological University', 'Shopee Pte. Ltd.', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2512.12980.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#dataset'], 'emoji': 'ğŸ§Š', 'ru': {'title': 'ĞÑ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğº Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼: Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ…', 'desc': 'Iceberg â€” ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Â«Ğ’Ğ¾Ñ€Ğ¾Ğ½ĞºÑƒ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸Â», Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑÑ‰ÑƒÑ Ñ‚Ñ€Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ° Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸: Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¸ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğº Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ²Ğ¾ÑĞµĞ¼ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ñ 1-100 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ğ¼Ğ¸ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ 13 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ² Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ, Ğ° Ğ½Ğµ Ğ¸Ğ·Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… insights Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ´ĞµÑ€ĞµĞ²Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞ°Ğ¼ Ğ² Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğµ Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ğ¾Ğ´ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸.'}, 'en': {'title': 'Iceberg: Elevating Vector Similarity Search Evaluation for Real-World Applications', 'desc': 'Iceberg is a benchmark suite designed to evaluate vector similarity search (VSS) methods in real-world applications, focusing on how retrieval quality affects downstream tasks. It identifies three main sources of performance degradation: embedding loss, metric misuse, and data distribution sensitivity. By using diverse datasets and task-specific metrics, Iceberg provides a more comprehensive assessment of VSS methods compared to traditional recall-latency benchmarks. The suite also offers guidance for practitioners through a decision tree that helps in selecting and tuning VSS methods based on specific workloads.'}, 'zh': {'title': 'Icebergï¼šçœŸå®åœºæ™¯ä¸‹çš„å‘é‡ç›¸ä¼¼æ€§æœç´¢è¯„ä¼°', 'desc': 'Icebergæ˜¯ä¸€ä¸ªåŸºå‡†æµ‹è¯•å¥—ä»¶ï¼Œç”¨äºè¯„ä¼°åœ¨çœŸå®åº”ç”¨åœºæ™¯ä¸­å‘é‡ç›¸ä¼¼æ€§æœç´¢ï¼ˆVSSï¼‰æ–¹æ³•çš„æ€§èƒ½ã€‚å®ƒæ­ç¤ºäº†ä¿¡æ¯æŸå¤±æ¼æ–—ï¼Œè¯†åˆ«å‡ºå¯¼è‡´æ€§èƒ½ä¸‹é™çš„ä¸‰ä¸ªä¸»è¦æ¥æºï¼šç‰¹å¾æå–ä¸­çš„åµŒå…¥æŸå¤±ã€åº¦é‡è¯¯ç”¨ä»¥åŠæ•°æ®åˆ†å¸ƒæ•æ„Ÿæ€§ã€‚Icebergæ¶µç›–äº†å…«ä¸ªä¸åŒçš„æ•°æ®é›†ï¼Œæ”¯æŒå›¾åƒåˆ†ç±»ã€äººè„¸è¯†åˆ«ã€æ–‡æœ¬æ£€ç´¢å’Œæ¨èç³»ç»Ÿç­‰å…³é”®é¢†åŸŸçš„è¯„ä¼°ã€‚é€šè¿‡å¯¹13ç§æœ€å…ˆè¿›çš„VSSæ–¹æ³•è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼ŒIcebergæä¾›äº†åŸºäºåº”ç”¨çº§æŒ‡æ ‡çš„é‡æ–°æ’åï¼Œå¸®åŠ©å®è·µè€…é€‰æ‹©å’Œè°ƒæ•´é€‚åˆå…¶ç‰¹å®šå·¥ä½œè´Ÿè½½çš„VSSæ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.14503', 'title': 'RecGPT-V2 Technical Report', 'url': 'https://huggingface.co/papers/2512.14503', 'abstract': 'RecGPT-V2 enhances recommender systems by integrating a Hierarchical Multi-Agent System, Hybrid Representation Inference, Meta-Prompting, constrained reinforcement learning, and an Agent-as-a-Judge framework to improve efficiency, explanation diversity, generalization, and human preference alignment.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have demonstrated remarkable potential in transforming recommender systems from implicit behavioral pattern matching to explicit intent reasoning. While RecGPT-V1 successfully pioneered this paradigm by integrating LLM-based reasoning into user interest mining and item tag prediction, it suffers from four fundamental limitations: (1) computational inefficiency and cognitive redundancy across multiple reasoning routes; (2) insufficient explanation diversity in fixed-template generation; (3) limited generalization under supervised learning paradigms; and (4) simplistic outcome-focused evaluation that fails to match human standards.   To address these challenges, we present RecGPT-V2 with four key innovations. First, a Hierarchical Multi-Agent System restructures intent reasoning through coordinated collaboration, eliminating cognitive duplication while enabling diverse intent coverage. Combined with Hybrid Representation Inference that compresses user-behavior contexts, our framework reduces GPU consumption by 60% and improves exclusive recall from 9.39% to 10.99%. Second, a Meta-Prompting framework dynamically generates contextually adaptive prompts, improving explanation diversity by +7.3%. Third, constrained reinforcement learning mitigates multi-reward conflicts, achieving +24.1% improvement in tag prediction and +13.0% in explanation acceptance. Fourth, an Agent-as-a-Judge framework decomposes assessment into multi-step reasoning, improving human preference alignment. Online A/B tests on Taobao demonstrate significant improvements: +2.98% CTR, +3.71% IPV, +2.19% TV, and +11.46% NER. RecGPT-V2 establishes both the technical feasibility and commercial viability of deploying LLM-powered intent reasoning at scale, bridging the gap between cognitive exploration and industrial utility.', 'score': 14, 'issue_id': 95, 'pub_date': '2025-12-16', 'pub_date_card': {'ru': '16 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 16', 'zh': '12æœˆ16æ—¥'}, 'hash': '6c1b9760c14723cf', 'authors': ['Chao Yi', 'Dian Chen', 'Gaoyang Guo', 'Jiakai Tang', 'Jian Wu', 'Jing Yu', 'Mao Zhang', 'Wen Chen', 'Wenjun Yang', 'Yujie Luo', 'Yuning Jiang', 'Zhujin Gao', 'Bo Zheng', 'Binbin Cao', 'Changfa Wu', 'Dixuan Wang', 'Han Wu', 'Haoyi Hu', 'Kewei Zhu', 'Lang Tian', 'Lin Yang', 'Qiqi Huang', 'Siqi Yang', 'Wenbo Su', 'Xiaoxiao He', 'Xin Tong', 'Xu Chen', 'Xunke Xi', 'Xiaowei Huang', 'Yaxuan Wu', 'Yeqiu Yang', 'Yi Hu', 'Yujin Yuan', 'Yuliang Yan', 'Zile Zhou'], 'affiliations': ['Taobao'], 'pdf_title_img': 'assets/pdf/title_img/2512.14503.jpg', 'data': {'categories': ['#training', '#rl', '#optimization', '#alignment', '#agents', '#rlhf', '#reasoning'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞœĞ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ LLM Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹ Ñ ÑĞ²Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ğ± Ğ¸Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°Ñ…', 'desc': 'RecGPT-V2 ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¾Ğ± Ğ¸Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ½Ğ° 60%. Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ°Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¼ĞµÑ‚Ğ°-Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğº ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ€ĞµÑˆĞ°ĞµÑ‚ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ñ‹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğ¼Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ÑĞ¼Ğ¸, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞ³Ğ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ğµ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğ¹. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Agent-as-a-Judge Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ»ÑƒÑ‡ÑˆĞµĞµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ² Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Revolutionizing Recommendations with Intelligent Intent Reasoning', 'desc': 'RecGPT-V2 improves recommender systems by using a Hierarchical Multi-Agent System that enhances intent reasoning while reducing computational redundancy. It introduces Hybrid Representation Inference to optimize user behavior context, leading to significant reductions in GPU usage and better recall rates. The Meta-Prompting framework increases the diversity of explanations provided to users, while constrained reinforcement learning resolves conflicts in reward systems, enhancing tag prediction and user acceptance of explanations. Finally, the Agent-as-a-Judge framework refines evaluation processes to align more closely with human preferences, demonstrating substantial performance gains in real-world applications.'}, 'zh': {'title': 'RecGPT-V2ï¼šæ™ºèƒ½æ¨èç³»ç»Ÿçš„æ–°çªç ´', 'desc': 'RecGPT-V2é€šè¿‡æ•´åˆå±‚æ¬¡å¤šæ™ºèƒ½ä½“ç³»ç»Ÿã€æ··åˆè¡¨ç¤ºæ¨ç†ã€å…ƒæç¤ºã€å—é™å¼ºåŒ–å­¦ä¹ å’Œä»£ç†è¯„åˆ¤æ¡†æ¶ï¼Œæå‡äº†æ¨èç³»ç»Ÿçš„æ•ˆç‡å’Œè§£é‡Šå¤šæ ·æ€§ã€‚è¯¥ç³»ç»Ÿè§£å†³äº†RecGPT-V1çš„è®¡ç®—æ•ˆç‡ä½ã€è§£é‡Šå¤šæ ·æ€§ä¸è¶³ã€æ³›åŒ–èƒ½åŠ›æœ‰é™å’Œè¯„ä¼°æ ‡å‡†ç®€å•ç­‰é—®é¢˜ã€‚é€šè¿‡åŠ¨æ€ç”Ÿæˆé€‚åº”ä¸Šä¸‹æ–‡çš„æç¤ºï¼ŒRecGPT-V2æé«˜äº†ç”¨æˆ·å…´è¶£æŒ–æ˜å’Œæ ‡ç­¾é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚åœ¨çº¿æµ‹è¯•ç»“æœæ˜¾ç¤ºï¼ŒRecGPT-V2åœ¨å¤šä¸ªå…³é”®æŒ‡æ ‡ä¸Šå‡æœ‰æ˜¾è‘—æå‡ï¼Œè¯æ˜äº†å…¶åœ¨å¤§è§„æ¨¡åº”ç”¨ä¸­çš„æŠ€æœ¯å¯è¡Œæ€§å’Œå•†ä¸šä»·å€¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.13303', 'title': 'ShowTable: Unlocking Creative Table Visualization with Collaborative Reflection and Refinement', 'url': 'https://huggingface.co/papers/2512.13303', 'abstract': 'ShowTable, a pipeline combining MLLMs and diffusion models, excels in creative table visualization by generating high-fidelity infographics from data tables, outperforming existing methods in multi-modal reasoning and error correction.  \t\t\t\t\tAI-generated summary \t\t\t\t While existing generation and unified models excel at general image generation, they struggle with tasks requiring deep reasoning, planning, and precise data-to-visual mapping abilities beyond general scenarios. To push beyond the existing limitations, we introduce a new and challenging task: creative table visualization, requiring the model to generate an infographic that faithfully and aesthetically visualizes the data from a given table. To address this challenge, we propose ShowTable, a pipeline that synergizes MLLMs with diffusion models via a progressive self-correcting process. The MLLM acts as the central orchestrator for reasoning the visual plan and judging visual errors to provide refined instructions, the diffusion execute the commands from MLLM, achieving high-fidelity results. To support this task and our pipeline, we introduce three automated data construction pipelines for training different modules. Furthermore, we introduce TableVisBench, a new benchmark with 800 challenging instances across 5 evaluation dimensions, to assess performance on this task. Experiments demonstrate that our pipeline, instantiated with different models, significantly outperforms baselines, highlighting its effective multi-modal reasoning, generation, and error correction capabilities.', 'score': 14, 'issue_id': 96, 'pub_date': '2025-12-15', 'pub_date_card': {'ru': '15 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 15', 'zh': '12æœˆ15æ—¥'}, 'hash': '613d8f8e6a0347ec', 'authors': ['Zhihang Liu', 'Xiaoyi Bao', 'Pandeng Li', 'Junjie Zhou', 'Zhaohe Liao', 'Yefei He', 'Kaixun Jiang', 'Chen-Wei Xie', 'Yun Zheng', 'Hongtao Xie'], 'affiliations': ['CASIA', 'FDU', 'NJU', 'SJTU', 'Tongyi Lab', 'USTC', 'ZJU'], 'pdf_title_img': 'assets/pdf/title_img/2512.13303.jpg', 'data': {'categories': ['#synthetic', '#cv', '#dataset', '#reasoning', '#benchmark', '#multimodal', '#diffusion'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'ĞŸÑ€ĞµĞ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğ¾: ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ÑÑ pipeline Ğ´Ğ»Ñ Ñ‚Ğ²Ğ¾Ñ€Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†', 'desc': 'ShowTable â€” ÑÑ‚Ğ¾ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ pipeline, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (MLLM) Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ½Ñ„Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ğº Ğ¸Ğ· Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ† Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. MLLM Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ñ€Ğ¾Ğ»ÑŒ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ğ¾Ñ€Ğ°, Ğ¾ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ»ÑÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ğ»Ğ°Ğ½Ğµ Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº, Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ‚Ñ€Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº TableVisBench Ñ 800 ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº.'}, 'en': {'title': 'Revolutionizing Table Visualization with ShowTable!', 'desc': 'The paper presents ShowTable, a novel pipeline that combines Multi-Modal Language Models (MLLMs) and diffusion models to enhance creative table visualization. This approach generates high-quality infographics from data tables, addressing the limitations of existing models in multi-modal reasoning and error correction. ShowTable utilizes MLLMs to orchestrate visual planning and error judgment, while diffusion models execute the visual commands, resulting in superior fidelity. Additionally, the authors introduce TableVisBench, a benchmark for evaluating the performance of their method across various dimensions, demonstrating significant improvements over traditional methods.'}, 'zh': {'title': 'ShowTableï¼šåˆ›æ–°è¡¨æ ¼å¯è§†åŒ–çš„è§£å†³æ–¹æ¡ˆ', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºShowTableçš„ç®¡é“ï¼Œå®ƒç»“åˆäº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å’Œæ‰©æ•£æ¨¡å‹ï¼Œä¸“æ³¨äºåˆ›é€ æ€§è¡¨æ ¼å¯è§†åŒ–ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿä»æ•°æ®è¡¨ä¸­ç”Ÿæˆé«˜ä¿çœŸä¿¡æ¯å›¾ï¼Œè¶…è¶Šäº†ç°æœ‰æ–¹æ³•åœ¨å¤šæ¨¡æ€æ¨ç†å’Œé”™è¯¯ä¿®æ­£æ–¹é¢çš„è¡¨ç°ã€‚ShowTableé€šè¿‡æ¸è¿›å¼è‡ªæˆ‘ä¿®æ­£è¿‡ç¨‹ï¼Œåˆ©ç”¨MLLMè¿›è¡Œè§†è§‰è§„åˆ’å’Œé”™è¯¯åˆ¤æ–­ï¼Œå¹¶ç”±æ‰©æ•£æ¨¡å‹æ‰§è¡ŒæŒ‡ä»¤ï¼Œä»è€Œå®ç°é«˜è´¨é‡çš„å¯è§†åŒ–ç»“æœã€‚æˆ‘ä»¬è¿˜æå‡ºäº†TableVisBenchï¼Œä¸€ä¸ªåŒ…å«800ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„å®ä¾‹çš„æ–°åŸºå‡†ï¼Œç”¨äºè¯„ä¼°è¯¥ä»»åŠ¡çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.14699', 'title': 'MemFlow: Flowing Adaptive Memory for Consistent and Efficient Long Video Narratives', 'url': 'https://huggingface.co/papers/2512.14699', 'abstract': 'MemFlow dynamically updates a memory bank by retrieving relevant historical frames for each video chunk, ensuring narrative coherence and generation efficiency with minimal computational overhead.  \t\t\t\t\tAI-generated summary \t\t\t\t The core challenge for streaming video generation is maintaining the content consistency in long context, which poses high requirement for the memory design. Most existing solutions maintain the memory by compressing historical frames with predefined strategies. However, different to-generate video chunks should refer to different historical cues, which is hard to satisfy with fixed strategies. In this work, we propose MemFlow to address this problem. Specifically, before generating the coming chunk, we dynamically update the memory bank by retrieving the most relevant historical frames with the text prompt of this chunk. This design enables narrative coherence even if new event happens or scenario switches in future frames. In addition, during generation, we only activate the most relevant tokens in the memory bank for each query in the attention layers, which effectively guarantees the generation efficiency. In this way, MemFlow achieves outstanding long-context consistency with negligible computation burden (7.9% speed reduction compared with the memory-free baseline) and keeps the compatibility with any streaming video generation model with KV cache.', 'score': 12, 'issue_id': 95, 'pub_date': '2025-12-16', 'pub_date_card': {'ru': '16 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 16', 'zh': '12æœˆ16æ—¥'}, 'hash': '540fd982b661f559', 'authors': ['Sihui Ji', 'Xi Chen', 'Shuai Yang', 'Xin Tao', 'Pengfei Wan', 'Hengshuang Zhao'], 'affiliations': ['HKU', 'HKUST(GZ)', 'Kling Team, Kuaishou Technology'], 'pdf_title_img': 'assets/pdf/title_img/2512.14699.jpg', 'data': {'categories': ['#long_context'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ ÑĞ²ÑĞ·Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'MemFlow â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ, Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ĞºĞ°Ğ´Ñ€Ñ‹ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°. Ğ’Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑĞ°Ğ¼Ñ‹Ğµ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¸Ğ· Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼ Ñ Ğ»ÑĞ±Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ KV-ĞºÑÑˆĞµĞ¼ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ½Ğ°Ñ€Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑĞ²ÑĞ·Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ ÑĞ¼ĞµĞ½Ğµ ÑÑ†ĞµĞ½.'}, 'en': {'title': 'Dynamic Memory for Coherent Video Generation', 'desc': 'MemFlow is a novel approach for streaming video generation that enhances narrative coherence by dynamically updating a memory bank. Instead of using fixed strategies to compress historical frames, MemFlow retrieves relevant frames based on the specific text prompt of the upcoming video chunk. This allows the model to maintain content consistency even when new events or scenarios arise. Additionally, by activating only the most pertinent tokens during the generation process, MemFlow achieves efficient computation with minimal speed reduction compared to traditional methods.'}, 'zh': {'title': 'åŠ¨æ€è®°å¿†æ›´æ–°ï¼Œç¡®ä¿è§†é¢‘ç”Ÿæˆä¸€è‡´æ€§', 'desc': 'MemFlowæ˜¯ä¸€ç§åŠ¨æ€æ›´æ–°è®°å¿†åº“çš„æ–¹æ³•ï¼Œé€šè¿‡æ£€ç´¢ä¸æ¯ä¸ªè§†é¢‘ç‰‡æ®µç›¸å…³çš„å†å²å¸§ï¼Œç¡®ä¿å™äº‹çš„ä¸€è‡´æ€§å’Œç”Ÿæˆçš„é«˜æ•ˆæ€§ã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸ä½¿ç”¨é¢„å®šä¹‰ç­–ç•¥å‹ç¼©å†å²å¸§ï¼Œéš¾ä»¥æ»¡è¶³ä¸åŒè§†é¢‘ç‰‡æ®µå¯¹å†å²çº¿ç´¢çš„ä¸åŒéœ€æ±‚ã€‚MemFlowåœ¨ç”Ÿæˆæ–°ç‰‡æ®µä¹‹å‰ï¼ŒåŠ¨æ€æ›´æ–°è®°å¿†åº“ï¼Œæå–ä¸å½“å‰æ–‡æœ¬æç¤ºæœ€ç›¸å…³çš„å†å²å¸§ã€‚è¿™æ ·ï¼Œå³ä½¿åœ¨æœªæ¥å¸§ä¸­å‘ç”Ÿæ–°äº‹ä»¶æˆ–åœºæ™¯åˆ‡æ¢ï¼ŒMemFlowä¹Ÿèƒ½ä¿æŒå™äº‹çš„è¿è´¯æ€§ï¼ŒåŒæ—¶åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ä»…æ¿€æ´»æœ€ç›¸å…³çš„è®°å¿†ä»¤ç‰Œï¼Œä»è€Œæœ‰æ•ˆæé«˜ç”Ÿæˆæ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.13281', 'title': 'Video Reality Test: Can AI-Generated ASMR Videos fool VLMs and Humans?', 'url': 'https://huggingface.co/papers/2512.13281', 'abstract': 'The Video Reality Test benchmark evaluates the realism and detection of AI-generated ASMR videos with audio, revealing that even the best models can deceive VLMs and humans, highlighting limitations in perceptual fidelity and audio-visual consistency.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in video generation have produced vivid content that are often indistinguishable from real videos, making AI-generated video detection an emerging societal challenge. Prior AIGC detection benchmarks mostly evaluate video without audio, target broad narrative domains, and focus on classification solely. Yet it remains unclear whether state-of-the-art video generation models can produce immersive, audio-paired videos that reliably deceive humans and VLMs. To this end, we introduce Video Reality Test, an ASMR-sourced video benchmark suite for testing perceptual realism under tight audio-visual coupling, featuring the following dimensions: (i) Immersive ASMR video-audio sources. Built on carefully curated real ASMR videos, the benchmark targets fine-grained action-object interactions with diversity across objects, actions, and backgrounds. (ii) Peer-Review evaluation. An adversarial creator-reviewer protocol where video generation models act as creators aiming to fool reviewers, while VLMs serve as reviewers seeking to identify fakeness. Our experimental findings show: The best creator Veo3.1-Fast even fools most VLMs: the strongest reviewer (Gemini 2.5-Pro) achieves only 56\\% accuracy (random 50\\%), far below that of human experts (81.25\\%). Adding audio improves real-fake discrimination, yet superficial cues such as watermarks can still significantly mislead models. These findings delineate the current boundary of video generation realism and expose limitations of VLMs in perceptual fidelity and audio-visual consistency. Our code is available at https://github.com/video-reality-test/video-reality-test.', 'score': 11, 'issue_id': 103, 'pub_date': '2025-12-15', 'pub_date_card': {'ru': '15 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 15', 'zh': '12æœˆ15æ—¥'}, 'hash': '2d4888772001806a', 'authors': ['Jiaqi Wang', 'Weijia Wu', 'Yi Zhan', 'Rui Zhao', 'Ming Hu', 'James Cheng', 'Wei Liu', 'Philip Torr', 'Kevin Qinghong Lin'], 'affiliations': ['CUHK', 'NUS', 'University of Oxford', 'Video Rebirth'], 'pdf_title_img': 'assets/pdf/title_img/2512.13281.jpg', 'data': {'categories': ['#multimodal', '#audio', '#video', '#benchmark'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ“Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ° Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¼Ğ°: ĞºĞ¾Ğ³Ğ´Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¾Ğ±Ğ¼Ğ°Ğ½Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ˜Ğ˜', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Video Reality Test Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ ASMR Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ´Ğ¾Ñ€Ğ¾Ğ¶ĞºĞ¾Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ğ¼Ğ°Ğ½Ñ‹Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ğº Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (VLMs), Ñ‚Ğ°Ğº Ğ¸ Ğ»ÑĞ´ĞµĞ¹ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ñ‚ĞµÑĞ½Ğ¾Ğ¹ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ’ Ñ€Ğ°Ğ¼ĞºĞ°Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ³Ğ´Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒÑÑ‚ ĞºĞ°Ğº Â«ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ĞµĞ»Ğ¸Â», ÑÑ‚Ñ€ĞµĞ¼ÑÑÑŒ Ğ¾Ğ±Ğ¼Ğ°Ğ½ÑƒÑ‚ÑŒ Â«Ñ€ĞµÑ†ĞµĞ½Ğ·ĞµĞ½Ñ‚Ğ¾Ğ²Â» â€” VLMs, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ‹Ñ‚Ğ°ÑÑ‚ÑÑ Ğ²Ñ‹ÑĞ²Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾Ğ´Ğ´ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… VLMs Ğ² Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğ¸ Ñ‚Ğ¾Ğ½ĞºĞ¸Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°.'}, 'en': {'title': 'Testing the Limits of AI-Generated Video Realism', 'desc': 'The Video Reality Test benchmark assesses how realistic AI-generated ASMR videos are and how well they can be detected by both humans and Vision-Language Models (VLMs). It highlights that even advanced video generation models can trick these detection systems, revealing weaknesses in their ability to judge perceptual fidelity and audio-visual coherence. The benchmark focuses on immersive ASMR videos, emphasizing the importance of audio paired with video for accurate detection. Findings show that while audio can enhance discrimination between real and fake videos, certain misleading cues still pose challenges for detection models.'}, 'zh': {'title': 'è§†é¢‘ç”Ÿæˆçš„çœŸå®æ„ŸæŒ‘æˆ˜', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†è§†é¢‘ç°å®æµ‹è¯•åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°AIç”Ÿæˆçš„ASMRè§†é¢‘çš„çœŸå®æ„Ÿå’Œæ£€æµ‹èƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹ä¹Ÿèƒ½æ¬ºéª—è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å’Œäººç±»ï¼Œæ˜¾ç¤ºå‡ºåœ¨æ„ŸçŸ¥çœŸå®æ„Ÿå’ŒéŸ³è§†é¢‘ä¸€è‡´æ€§æ–¹é¢çš„å±€é™æ€§ã€‚è¯¥åŸºå‡†é€šè¿‡ç²¾å¿ƒæŒ‘é€‰çš„çœŸå®ASMRè§†é¢‘ï¼Œæµ‹è¯•éŸ³è§†é¢‘ç´§å¯†ç»“åˆä¸‹çš„æ„ŸçŸ¥çœŸå®æ„Ÿã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæœ€ä½³ç”Ÿæˆæ¨¡å‹Veo3.1-Fastèƒ½å¤Ÿæ¬ºéª—å¤§å¤šæ•°VLMsï¼Œä¸”å…¶æ£€æµ‹å‡†ç¡®ç‡è¿œä½äºäººç±»ä¸“å®¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.14531', 'title': 'VersatileFFN: Achieving Parameter Efficiency in LLMs via Adaptive Wide-and-Deep Reuse', 'url': 'https://huggingface.co/papers/2512.14531', 'abstract': 'The rapid scaling of Large Language Models (LLMs) has achieved remarkable performance, but it also leads to prohibitive memory costs. Existing parameter-efficient approaches such as pruning and quantization mainly compress pretrained models without enhancing architectural capacity, thereby hitting the representational ceiling of the base model. In this work, we propose VersatileFFN, a novel feed-forward network (FFN) that enables flexible reuse of parameters in both width and depth dimensions within a fixed parameter budget. Inspired by the dual-process theory of cognition, VersatileFFN comprises two adaptive pathways: a width-versatile path that generates a mixture of sub-experts from a single shared FFN, mimicking sparse expert routing without increasing parameters, and a depth-versatile path that recursively applies the same FFN to emulate deeper processing for complex tokens. A difficulty-aware gating dynamically balances the two pathways, steering "easy" tokens through the efficient width-wise route and allocating deeper iterative refinement to "hard" tokens. Crucially, both pathways reuse the same parameters, so all additional capacity comes from computation rather than memory. Experiments across diverse benchmarks and model scales demonstrate the effectiveness of the method. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/VersatileFFN.', 'score': 9, 'issue_id': 95, 'pub_date': '2025-12-16', 'pub_date_card': {'ru': '16 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 16', 'zh': '12æœˆ16æ—¥'}, 'hash': 'fe4e0736a1391e23', 'authors': ['Ying Nie', 'Kai Han', 'Hongguang Li', 'Hang Zhou', 'Tianyu Guo', 'Enhua Wu', 'Xinghao Chen', 'Yunhe Wang'], 'affiliations': ["Huawei Noah's Ark Lab", 'ISCAS', 'University of Macau'], 'pdf_title_img': 'assets/pdf/title_img/2512.14531.jpg', 'data': {'categories': ['#inference', '#architecture', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ“Ğ¸Ğ±ĞºĞ¾Ğµ Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ LLM', 'desc': 'Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ VersatileFFN â€” Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑĞ²ÑĞ·Ğ½Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ñ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±ĞµĞ· ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ: Ğ¿ÑƒÑ‚ÑŒ Ğ¿Ğ¾ ÑˆĞ¸Ñ€Ğ¸Ğ½Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¼ĞµÑÑŒ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸, Ğ° Ğ¿ÑƒÑ‚ÑŒ Ğ¿Ğ¾ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğµ Ñ€ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ‚Ñƒ Ğ¶Ğµ ÑĞµÑ‚ÑŒ Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ³ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ñ‡ĞµÑ€ĞµĞ· ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚, Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ. Ğ’ÑĞµ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ÑÑ‚ÑÑ Ğ·Ğ° ÑÑ‡Ñ‘Ñ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹, Ğ° Ğ½Ğµ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Enhancing LLM Efficiency with VersatileFFN', 'desc': 'This paper introduces VersatileFFN, a new type of feed-forward network designed to improve the efficiency of Large Language Models (LLMs) without increasing memory costs. It features two adaptive pathways: one that allows for flexible width adjustments to create a mixture of sub-experts, and another that enables deeper processing for complex inputs. By using a difficulty-aware gating mechanism, the model can efficiently route simpler tokens through a less complex path while applying deeper processing to more challenging tokens. This innovative approach allows for enhanced model capacity through computation rather than additional parameters, making it a significant advancement in parameter-efficient model design.'}, 'zh': {'title': 'çµæ´»é‡ç”¨å‚æ•°ï¼Œæå‡æ¨¡å‹èƒ½åŠ›', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„å‰é¦ˆç½‘ç»œVersatileFFNï¼Œæ—¨åœ¨æé«˜å¤§è¯­è¨€æ¨¡å‹çš„å‚æ•°åˆ©ç”¨æ•ˆç‡ã€‚è¯¥ç½‘ç»œé€šè¿‡åœ¨å›ºå®šçš„å‚æ•°é¢„ç®—å†…çµæ´»åœ°é‡ç”¨å‚æ•°ï¼Œå¢å¼ºäº†æ¨¡å‹çš„è¡¨ç¤ºèƒ½åŠ›ã€‚VersatileFFNåŒ…å«ä¸¤ä¸ªè‡ªé€‚åº”è·¯å¾„ï¼Œåˆ†åˆ«å¤„ç†ç®€å•å’Œå¤æ‚çš„è¾“å…¥ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„è®¡ç®—è€Œä¸å¢åŠ å†…å­˜æ¶ˆè€—ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.13678', 'title': 'Feedforward 3D Editing via Text-Steerable Image-to-3D', 'url': 'https://huggingface.co/papers/2512.13678', 'abstract': 'Steer3D enables text-based editing of AI-generated 3D assets by adapting ControlNet for image-to-3D generation with flow-matching training and Direct Preference Optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent progress in image-to-3D has opened up immense possibilities for design, AR/VR, and robotics. However, to use AI-generated 3D assets in real applications, a critical requirement is the capability to edit them easily. We present a feedforward method, Steer3D, to add text steerability to image-to-3D models, which enables editing of generated 3D assets with language. Our approach is inspired by ControlNet, which we adapt to image-to-3D generation to enable text steering directly in a forward pass. We build a scalable data engine for automatic data generation, and develop a two-stage training recipe based on flow-matching training and Direct Preference Optimization (DPO). Compared to competing methods, Steer3D more faithfully follows the language instruction and maintains better consistency with the original 3D asset, while being 2.4x to 28.5x faster. Steer3D demonstrates that it is possible to add a new modality (text) to steer the generation of pretrained image-to-3D generative models with 100k data. Project website: https://glab-caltech.github.io/steer3d/', 'score': 8, 'issue_id': 95, 'pub_date': '2025-12-15', 'pub_date_card': {'ru': '15 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 15', 'zh': '12æœˆ15æ—¥'}, 'hash': '91a4ee8c2c1e471e', 'authors': ['Ziqi Ma', 'Hongqiao Chen', 'Yisong Yue', 'Georgia Gkioxari'], 'affiliations': ['Caltech'], 'pdf_title_img': 'assets/pdf/title_img/2512.13678.jpg', 'data': {'categories': ['#rlhf', '#3d', '#training', '#multimodal'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ 3D Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ°Ğ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ControlNet', 'desc': 'Steer3D â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ÑĞ¼Ğ¸, Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ ControlNet Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ 3D Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ¸Ğ² ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ² Ğ´Ğ²Ğ° ÑÑ‚Ğ°Ğ¿Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ flow-matching Ğ¸ Direct Preference Optimization, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² 2.4-28.5 Ñ€Ğ°Ğ· Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ğ¸ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ²ÑĞµĞ³Ğ¾ 100 Ñ‚Ñ‹ÑÑÑ‡ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Text-Driven Editing for 3D Assets with Steer3D', 'desc': 'Steer3D is a novel method that enhances the editing of AI-generated 3D assets by integrating text-based commands into the image-to-3D generation process. It builds on the ControlNet framework, allowing users to modify 3D models directly through language inputs during a single forward pass. The method employs flow-matching training and Direct Preference Optimization to ensure that the generated assets align closely with user instructions while maintaining high fidelity to the original designs. Steer3D significantly improves the speed and accuracy of 3D asset editing, making it a powerful tool for applications in design, AR/VR, and robotics.'}, 'zh': {'title': 'æ–‡æœ¬å¼•å¯¼ï¼Œè½»æ¾ç¼–è¾‘3Dèµ„äº§', 'desc': 'Steer3D æ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œå…è®¸ç”¨æˆ·é€šè¿‡æ–‡æœ¬ç¼–è¾‘ AI ç”Ÿæˆçš„ 3D èµ„äº§ã€‚å®ƒåŸºäº ControlNet çš„ç†å¿µï¼Œé€‚ç”¨äºå›¾åƒåˆ° 3D çš„ç”Ÿæˆï¼Œèƒ½å¤Ÿåœ¨å‰å‘ä¼ é€’ä¸­ç›´æ¥è¿›è¡Œæ–‡æœ¬å¼•å¯¼ã€‚è¯¥æ–¹æ³•ä½¿ç”¨æµåŒ¹é…è®­ç»ƒå’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰è¿›è¡Œä¸¤é˜¶æ®µè®­ç»ƒï¼Œæ˜¾è‘—æé«˜äº†ç”Ÿæˆé€Ÿåº¦å’Œä¸åŸå§‹ 3D èµ„äº§çš„ä¸€è‡´æ€§ã€‚Steer3D å±•ç¤ºäº†å¦‚ä½•å°†æ–‡æœ¬è¿™ä¸€æ–°æ¨¡æ€æ·»åŠ åˆ°é¢„è®­ç»ƒçš„å›¾åƒåˆ° 3D ç”Ÿæˆæ¨¡å‹ä¸­ï¼Œæå‡äº†ç”¨æˆ·çš„ç¼–è¾‘ä½“éªŒã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.14442', 'title': 'A4-Agent: An Agentic Framework for Zero-Shot Affordance Reasoning', 'url': 'https://huggingface.co/papers/2512.14442', 'abstract': 'A4-Agent, a training-free framework, decouples affordance prediction into three stages using specialized pre-trained models to enhance generalization and performance in real-world settings.  \t\t\t\t\tAI-generated summary \t\t\t\t Affordance prediction, which identifies interaction regions on objects based on language instructions, is critical for embodied AI. Prevailing end-to-end models couple high-level reasoning and low-level grounding into a single monolithic pipeline and rely on training over annotated datasets, which leads to poor generalization on novel objects and unseen environments. In this paper, we move beyond this paradigm by proposing A4-Agent, a training-free agentic framework that decouples affordance prediction into a three-stage pipeline. Our framework coordinates specialized foundation models at test time: (1) a Dreamer that employs generative models to visualize how an interaction would look; (2) a Thinker that utilizes large vision-language models to decide what object part to interact with; and (3) a Spotter that orchestrates vision foundation models to precisely locate where the interaction area is. By leveraging the complementary strengths of pre-trained models without any task-specific fine-tuning, our zero-shot framework significantly outperforms state-of-the-art supervised methods across multiple benchmarks and demonstrates robust generalization to real-world settings.', 'score': 7, 'issue_id': 96, 'pub_date': '2025-12-16', 'pub_date_card': {'ru': '16 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 16', 'zh': '12æœˆ16æ—¥'}, 'hash': '2a25782f9d642d9c', 'authors': ['Zixin Zhang', 'Kanghao Chen', 'Hanqing Wang', 'Hongfei Zhang', 'Harold Haodong Chen', 'Chenfei Liao', 'Litao Guo', 'Ying-Cong Chen'], 'affiliations': ['HKUST', 'HKUST(GZ)', 'Knowin', 'SJTU'], 'pdf_title_img': 'assets/pdf/title_img/2512.14442.jpg', 'data': {'categories': [], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ”ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'A4-Agent â€” ÑÑ‚Ğ¾ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ğ¾-ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ affordance (Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹) Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ½Ğ° Ñ‚Ñ€Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑÑ‚Ğ°Ğ¿Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ, Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ñ‡Ğ°ÑÑ‚Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°, Ğ¸ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. ĞŸĞ¾ÑĞºĞ¾Ğ»ÑŒĞºÑƒ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ğ½Ğ° Ğ»ÑƒÑ‡ÑˆĞµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ¸ Ğ½ĞµĞ·Ğ½Ğ°ĞºĞ¾Ğ¼Ñ‹Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ² ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½ÑƒÑ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ….'}, 'en': {'title': 'Decoupling Affordance Prediction for Better Generalization', 'desc': 'The paper introduces A4-Agent, a novel framework for affordance prediction that operates without the need for training on specific datasets. It breaks down the prediction process into three distinct stages: visualization of interactions, decision-making on object parts, and precise location identification. By using specialized pre-trained models at test time, A4-Agent enhances generalization and performance in real-world scenarios. This approach allows the framework to outperform traditional supervised methods while maintaining robustness across various benchmarks.'}, 'zh': {'title': 'A4-Agentï¼šæ— è®­ç»ƒçš„å¯ç”¨æ€§é¢„æµ‹æ–°æ¡†æ¶', 'desc': 'A4-Agentæ˜¯ä¸€ä¸ªæ— è®­ç»ƒçš„æ¡†æ¶ï¼Œå°†å¯ç”¨æ€§é¢„æµ‹åˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼Œä½¿ç”¨ä¸“é—¨çš„é¢„è®­ç»ƒæ¨¡å‹æ¥å¢å¼ºåœ¨ç°å®ä¸–ç•Œä¸­çš„æ³›åŒ–èƒ½åŠ›å’Œæ€§èƒ½ã€‚å¯ç”¨æ€§é¢„æµ‹æ˜¯åŸºäºè¯­è¨€æŒ‡ä»¤è¯†åˆ«ç‰©ä½“äº¤äº’åŒºåŸŸçš„å…³é”®æŠ€æœ¯ï¼Œä¼ ç»Ÿçš„ç«¯åˆ°ç«¯æ¨¡å‹å°†é«˜å±‚æ¨ç†å’Œä½å±‚åŸºç¡€ç»“åˆåœ¨ä¸€èµ·ï¼Œå¯¼è‡´åœ¨æ–°ç‰©ä½“å’Œæœªè§ç¯å¢ƒä¸­çš„æ³›åŒ–èƒ½åŠ›å·®ã€‚A4-Agenté€šè¿‡åè°ƒä¸‰ä¸ªä¸“é—¨çš„åŸºç¡€æ¨¡å‹æ¥å®ç°ï¼šDreamerç”¨äºç”Ÿæˆäº¤äº’çš„å¯è§†åŒ–ï¼ŒThinkerå†³å®šä¸å“ªä¸ªç‰©ä½“éƒ¨åˆ†äº¤äº’ï¼ŒSpotterç²¾ç¡®å®šä½äº¤äº’åŒºåŸŸã€‚æˆ‘ä»¬çš„é›¶-shotæ¡†æ¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—è¶…è¶Šäº†æœ€å…ˆè¿›çš„ç›‘ç£æ–¹æ³•ï¼Œå¹¶åœ¨ç°å®ä¸–ç•Œä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.14284', 'title': 'SS4D: Native 4D Generative Model via Structured Spacetime Latents', 'url': 'https://huggingface.co/papers/2512.14284', 'abstract': 'SS4D synthesizes dynamic 3D objects from monocular video using a native 4D generative model with structured spacetime latents, ensuring high fidelity, temporal coherence, and structural consistency.  \t\t\t\t\tAI-generated summary \t\t\t\t We present SS4D, a native 4D generative model that synthesizes dynamic 3D objects directly from monocular video. Unlike prior approaches that construct 4D representations by optimizing over 3D or video generative models, we train a generator directly on 4D data, achieving high fidelity, temporal coherence, and structural consistency. At the core of our method is a compressed set of structured spacetime latents. Specifically, (1) To address the scarcity of 4D training data, we build on a pre-trained single-image-to-3D model, preserving strong spatial consistency. (2) Temporal consistency is enforced by introducing dedicated temporal layers that reason across frames. (3) To support efficient training and inference over long video sequences, we compress the latent sequence along the temporal axis using factorized 4D convolutions and temporal downsampling blocks. In addition, we employ a carefully designed training strategy to enhance robustness against occlusion', 'score': 7, 'issue_id': 97, 'pub_date': '2025-12-16', 'pub_date_card': {'ru': '16 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 16', 'zh': '12æœˆ16æ—¥'}, 'hash': '4068c45a3a3e8d0a', 'authors': ['Zhibing Li', 'Mengchen Zhang', 'Tong Wu', 'Jing Tan', 'Jiaqi Wang', 'Dahua Lin'], 'affiliations': ['Shanghai AI Laboratory, China', 'Stanford University, USA', 'The Chinese University of Hong Kong, China', 'Zhejiang University, China'], 'pdf_title_img': 'assets/pdf/title_img/2512.14284.jpg', 'data': {'categories': ['#training', '#video', '#3d', '#architecture'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ¡Ğ¸Ğ½Ñ‚ĞµĞ· Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… 3D Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ 4D Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸', 'desc': 'SS4D â€” ÑÑ‚Ğ¾ Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ 3D Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¸Ğ· Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑÑ‚Ñ€Ğ¾Ğ¸Ğ»Ğ¸ 4D Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ°Ğ´ 3D Ğ¸Ğ»Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ½Ğ° 4D Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° â€” ÑĞ¶Ğ°Ñ‚Ñ‹Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğµ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ»Ğ¾Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸ Ğ¸ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ 4D ÑĞ²Ñ‘Ñ€Ñ‚ĞºĞ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞµ.'}, 'en': {'title': 'Revolutionizing 3D Object Synthesis from Video with SS4D', 'desc': 'The paper introduces SS4D, a novel 4D generative model that creates dynamic 3D objects from single-camera video input. It improves upon previous methods by directly training on 4D data, which enhances the quality and consistency of the generated objects. The model utilizes structured spacetime latents to maintain spatial and temporal coherence, addressing challenges like data scarcity and occlusion. Key innovations include the use of pre-trained models for spatial consistency and specialized temporal layers for frame reasoning, along with efficient training techniques for long video sequences.'}, 'zh': {'title': 'ä»å•ç›®è§†é¢‘åˆæˆåŠ¨æ€3Dç‰©ä½“çš„åˆ›æ–°æ–¹æ³•', 'desc': 'SS4Dæ˜¯ä¸€ç§åŸç”Ÿçš„4Dç”Ÿæˆæ¨¡å‹ï¼Œå¯ä»¥ä»å•ç›®è§†é¢‘ä¸­åˆæˆåŠ¨æ€3Dç‰©ä½“ã€‚ä¸ä¹‹å‰çš„æ–¹æ³•ä¸åŒï¼ŒSS4Dç›´æ¥åœ¨4Dæ•°æ®ä¸Šè®­ç»ƒç”Ÿæˆå™¨ï¼Œç¡®ä¿äº†é«˜ä¿çœŸåº¦ã€æ—¶é—´ä¸€è‡´æ€§å’Œç»“æ„ä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªå‹ç¼©çš„ç»“æ„åŒ–æ—¶ç©ºæ½œå˜é‡é›†ï¼Œé€šè¿‡é¢„è®­ç»ƒçš„å•å›¾åƒåˆ°3Dæ¨¡å‹æ¥è§£å†³4Dè®­ç»ƒæ•°æ®ç¨€ç¼ºçš„é—®é¢˜ã€‚ä¸ºäº†æ”¯æŒé•¿è§†é¢‘åºåˆ—çš„é«˜æ•ˆè®­ç»ƒå’Œæ¨ç†ï¼ŒSS4Dä½¿ç”¨äº†åˆ†è§£çš„4Då·ç§¯å’Œæ—¶é—´ä¸‹é‡‡æ ·å—æ¥å‹ç¼©æ½œå˜é‡åºåˆ—ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.13961', 'title': 'Olmo 3', 'url': 'https://huggingface.co/papers/2512.13961', 'abstract': 'Olmo 3, a family of state-of-the-art fully-open language models at 7B and 32B parameter scales, excels in long-context reasoning, function calling, coding, instruction following, general chat, and knowledge recall.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Olmo 3, a family of state-of-the-art, fully-open language models at the 7B and 32B parameter scales. Olmo 3 model construction targets long-context reasoning, function calling, coding, instruction following, general chat, and knowledge recall. This release includes the entire model flow, i.e., the full lifecycle of the family of models, including every stage, checkpoint, data point, and dependency used to build it. Our flagship model, Olmo 3 Think 32B, is the strongest fully-open thinking model released to-date.', 'score': 5, 'issue_id': 95, 'pub_date': '2025-12-15', 'pub_date_card': {'ru': '15 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 15', 'zh': '12æœˆ15æ—¥'}, 'hash': 'd6b303f339039660', 'authors': ['Team Olmo', ':', 'Allyson Ettinger', 'Amanda Bertsch', 'Bailey Kuehl', 'David Graham', 'David Heineman', 'Dirk Groeneveld', 'Faeze Brahman', 'Finbarr Timbers', 'Hamish Ivison', 'Jacob Morrison', 'Jake Poznanski', 'Kyle Lo', 'Luca Soldaini', 'Matt Jordan', 'Mayee Chen', 'Michael Noukhovitch', 'Nathan Lambert', 'Pete Walsh', 'Pradeep Dasigi', 'Robert Berry', 'Saumya Malik', 'Saurabh Shah', 'Scott Geng', 'Shane Arora', 'Shashank Gupta', 'Taira Anderson', 'Teng Xiao', 'Tyler Murray', 'Tyler Romero', 'Victoria Graf', 'Akari Asai', 'Akshita Bhagia', 'Alexander Wettig', 'Alisa Liu', 'Aman Rangapur', 'Chloe Anastasiades', 'Costa Huang', 'Dustin Schwenk', 'Harsh Trivedi', 'Ian Magnusson', 'Jaron Lochner', 'Jiacheng Liu', 'Lester James V. Miranda', 'Maarten Sap', 'Malia Morgan', 'Michael Schmitz', 'Michal Guerquin', 'Michael Wilson', 'Regan Huff', 'Ronan Le Bras', 'Rui Xin', 'Rulin Shao', 'Sam Skjonsberg', 'Shannon Zejiang Shen', 'Shuyue Stella Li', 'Tucker Wilde', 'Valentina Pyatkin', 'Will Merrill', 'Yapei Chang', 'Yuling Gu', 'Zhiyuan Zeng', 'Ashish Sabharwal', 'Luke Zettlemoyer', 'Pang Wei Koh', 'Ali Farhadi', 'Noah A. Smith', 'Hannaneh Hajishirzi'], 'affiliations': ['Allen Institute for AI', 'Carnegie Mellon University', 'Massachusetts Institute of Technology', 'Mila', 'Princeton University', 'Stanford University', 'University of Maryland', 'University of Washington', 'UniversitÃ© de MontrÃ©al'], 'pdf_title_img': 'assets/pdf/title_img/2512.13961.jpg', 'data': {'categories': ['#open_source', '#reasoning', '#long_context'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ»Ğ¼Ğ¾ 3: Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞµĞ¼ÑŒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Olmo 3 â€” Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 7 Ğ¼Ğ»Ñ€Ğ´ Ğ¸ 32 Ğ¼Ğ»Ñ€Ğ´ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…, Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ¸ Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ñ‹Ğ»Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ´Ğ»Ñ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼, Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ° Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ¶Ğ¸Ğ·Ğ½ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ†Ğ¸ĞºĞ» Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ²ÑĞµ ÑÑ‚Ğ°Ğ¿Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸, Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾Ñ‡ĞºĞ¸, Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸. Ğ¤Ğ»Ğ°Ğ³Ğ¼Ğ°Ğ½ÑĞºĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Olmo 3 Think 32B Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°Ğ½Ğ° ÑĞ°Ğ¼Ğ¾Ğ¹ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ñ Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚.'}, 'en': {'title': 'Unlocking Advanced Language Understanding with Olmo 3', 'desc': 'Olmo 3 is a new family of advanced language models with 7 billion and 32 billion parameters, designed to perform well in various tasks such as long-context reasoning and coding. These models are fully open, meaning that researchers and developers can access the entire model lifecycle, including all stages and data used in their creation. The models excel in instruction following, general chat, and knowledge recall, making them versatile tools for different applications. The standout model, Olmo 3 Think 32B, is highlighted as the most powerful open thinking model available so far.'}, 'zh': {'title': 'Olmo 3ï¼šæœ€å¼ºå¤§çš„å¼€æ”¾è¯­è¨€æ¨¡å‹', 'desc': 'Olmo 3æ˜¯ä¸€ç³»åˆ—æœ€å…ˆè¿›çš„å®Œå…¨å¼€æ”¾çš„è¯­è¨€æ¨¡å‹ï¼Œå…·æœ‰7Bå’Œ32Bå‚æ•°è§„æ¨¡ã€‚è¯¥æ¨¡å‹ä¸“æ³¨äºé•¿ä¸Šä¸‹æ–‡æ¨ç†ã€å‡½æ•°è°ƒç”¨ã€ç¼–ç ã€æŒ‡ä»¤è·Ÿéšã€ä¸€èˆ¬èŠå¤©å’ŒçŸ¥è¯†å›å¿†ç­‰ä»»åŠ¡ã€‚æ­¤æ¬¡å‘å¸ƒåŒ…æ‹¬æ¨¡å‹çš„å®Œæ•´ç”Ÿå‘½å‘¨æœŸï¼Œæ¶µç›–äº†æ„å»ºæ¨¡å‹æ‰€éœ€çš„æ¯ä¸ªé˜¶æ®µã€æ£€æŸ¥ç‚¹ã€æ•°æ®ç‚¹å’Œä¾èµ–å…³ç³»ã€‚æˆ‘ä»¬çš„æ——èˆ°æ¨¡å‹Olmo 3 Think 32Bæ˜¯è¿„ä»Šä¸ºæ­¢å‘å¸ƒçš„æœ€å¼ºå¤§çš„å®Œå…¨å¼€æ”¾æ€ç»´æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.13607', 'title': 'Nemotron-Cascade: Scaling Cascaded Reinforcement Learning for General-Purpose Reasoning Models', 'url': 'https://huggingface.co/papers/2512.13607', 'abstract': "Cascaded domain-wise reinforcement learning (Cascade RL) is proposed to enhance general-purpose reasoning models, achieving state-of-the-art performance across benchmarks and outperforming the teacher model in coding competitions.  \t\t\t\t\tAI-generated summary \t\t\t\t Building general-purpose reasoning models with reinforcement learning (RL) entails substantial cross-domain heterogeneity, including large variation in inference-time response lengths and verification latency. Such variability complicates the RL infrastructure, slows training, and makes training curriculum (e.g., response length extension) and hyperparameter selection challenging. In this work, we propose cascaded domain-wise reinforcement learning (Cascade RL) to develop general-purpose reasoning models, Nemotron-Cascade, capable of operating in both instruct and deep thinking modes. Departing from conventional approaches that blend heterogeneous prompts from different domains, Cascade RL orchestrates sequential, domain-wise RL, reducing engineering complexity and delivering state-of-the-art performance across a wide range of benchmarks. Notably, RLHF for alignment, when used as a pre-step, boosts the model's reasoning ability far beyond mere preference optimization, and subsequent domain-wise RLVR stages rarely degrade the benchmark performance attained in earlier domains and may even improve it (see an illustration in Figure 1). Our 14B model, after RL, outperforms its SFT teacher, DeepSeek-R1-0528, on LiveCodeBench v5/v6/Pro and achieves silver-medal performance in the 2025 International Olympiad in Informatics (IOI). We transparently share our training and data recipes.", 'score': 5, 'issue_id': 98, 'pub_date': '2025-12-15', 'pub_date_card': {'ru': '15 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 15', 'zh': '12æœˆ15æ—¥'}, 'hash': '8e229655d2e19f44', 'authors': ['Boxin Wang', 'Chankyu Lee', 'Nayeon Lee', 'Sheng-Chieh Lin', 'Wenliang Dai', 'Yang Chen', 'Yangyi Chen', 'Zhuolin Yang', 'Zihan Liu', 'Mohammad Shoeybi', 'Bryan Catanzaro', 'Wei Ping'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2512.13607.jpg', 'data': {'categories': ['#reasoning', '#rl', '#optimization', '#alignment', '#rlhf', '#benchmark', '#training'], 'emoji': 'ğŸ†', 'ru': {'title': 'ĞšĞ°ÑĞºĞ°Ğ´Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ ĞºĞ°ÑĞºĞ°Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (Cascade RL) Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ñ…, Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ ÑƒĞ¿Ñ€Ğ¾Ñ‰Ğ°ĞµÑ‚ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ñ Ğ¸ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Nemotron-Cascade Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğ° Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ DeepSeek-R1 Ğ½Ğ° ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑÑ… Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ RLHF Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑÑ‚Ğ°Ğ´Ğ¸Ğ¸ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğµ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ½Ğ½Ğ¸Ñ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ….'}, 'en': {'title': 'Cascade RL: Simplifying Training for Superior Reasoning Models', 'desc': 'This paper introduces Cascaded Domain-Wise Reinforcement Learning (Cascade RL) to improve general-purpose reasoning models. It addresses challenges in reinforcement learning caused by variability in response lengths and verification times across different domains. By using a sequential approach to domain-specific reinforcement learning, Cascade RL simplifies the training process and enhances model performance. The proposed model, Nemotron-Cascade, demonstrates superior reasoning capabilities and outperforms its teacher model in competitive coding benchmarks.'}, 'zh': {'title': 'çº§è”é¢†åŸŸå¼ºåŒ–å­¦ä¹ ï¼šæå‡é€šç”¨æ¨ç†æ¨¡å‹çš„åˆ©å™¨', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§çº§è”é¢†åŸŸå¼ºåŒ–å­¦ä¹ ï¼ˆCascade RLï¼‰çš„æ–¹æ³•ï¼Œä»¥å¢å¼ºé€šç”¨æ¨ç†æ¨¡å‹çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„è¡¨ç°ï¼Œå¹¶åœ¨ç¼–ç ç«èµ›ä¸­è¶…è¶Šäº†æ•™å¸ˆæ¨¡å‹ã€‚Cascade RLé€šè¿‡é¡ºåºçš„é¢†åŸŸå¼ºåŒ–å­¦ä¹ ï¼Œç®€åŒ–äº†å·¥ç¨‹å¤æ‚æ€§ï¼Œå¹¶åœ¨ä¸åŒé¢†åŸŸä¸­ä¿æŒäº†é«˜æ€§èƒ½ã€‚æˆ‘ä»¬çš„14Bæ¨¡å‹åœ¨ç»è¿‡å¼ºåŒ–å­¦ä¹ åï¼Œåœ¨LiveCodeBench v5/v6/Proä¸Šè¶…è¶Šäº†å…¶SFTæ•™å¸ˆæ¨¡å‹ï¼Œå¹¶åœ¨2025å¹´å›½é™…ä¿¡æ¯å­¦å¥¥æ—åŒ¹å…‹ç«èµ›ä¸­è·å¾—é“¶ç‰Œã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.14666', 'title': 'EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models', 'url': 'https://huggingface.co/papers/2512.14666', 'abstract': "EVOLVE-VLA, a test-time training framework for Vision-Language-Action models, enables continuous adaptation through environmental interaction with minimal task-specific demonstrations, achieving significant improvements in performance and generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Achieving truly adaptive embodied intelligence requires agents that learn not just by imitating static demonstrations, but by continuously improving through environmental interaction, which is akin to how humans master skills through practice. Vision-Language-Action (VLA) models have advanced robotic manipulation by leveraging large language models, yet remain fundamentally limited by Supervised Finetuning (SFT): requiring hundreds of demonstrations per task, rigidly memorizing trajectories, and failing to adapt when deployment conditions deviate from training. We introduce EVOLVE-VLA, a test-time training framework enabling VLAs to continuously adapt through environment interaction with minimal or zero task-specific demonstrations. The key technical challenge is replacing oracle reward signals (unavailable at test time) with autonomous feedback. We address this through a learned progress estimator providing dense feedback, and critically, we design our framework to ``tame'' this inherently noisy signal via two mechanisms: (1) an accumulative progress estimation mechanism smoothing noisy point-wise estimates, and (2) a progressive horizon extension strategy enabling gradual policy evolution. EVOLVE-VLA achieves substantial gains: +8.6\\% on long-horizon tasks, +22.0\\% in 1-shot learning, and enables cross-task generalization -- achieving 20.8\\% success on unseen tasks without task-specific demonstrations training (vs. 0\\% for pure SFT). Qualitative analysis reveals emergent capabilities absent in demonstrations, including error recovery and novel strategies. This work represents a critical step toward VLAs that truly learn and adapt, moving beyond static imitation toward continuous self-improvements.", 'score': 4, 'issue_id': 97, 'pub_date': '2025-12-16', 'pub_date_card': {'ru': '16 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 16', 'zh': '12æœˆ16æ—¥'}, 'hash': '97dec52536c641e8', 'authors': ['Zechen Bai', 'Chen Gao', 'Mike Zheng Shou'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2512.14666.jpg', 'data': {'categories': ['#multimodal', '#robotics', '#rl', '#training'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞÑ‚ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ€Ğ°Ğ¶Ğ°Ğ½Ğ¸Ñ Ğº Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ', 'desc': 'EVOLVE-VLA â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Vision-Language-Action Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¹. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸Ğº Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ° Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğ° Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑˆÑƒĞ¼Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°Ğ¼Ğ¸: ĞºÑƒĞ¼ÑƒĞ»ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ° Ğ¸ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ: +8.6% Ğ½Ğ° Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, +22.0% Ğ¿Ñ€Ğ¸ Ğ¾Ğ´Ğ½Ğ¾ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¸ 20.8% ÑƒÑĞ¿ĞµÑ…Ğ° Ğ½Ğ° Ğ½ĞµĞ²Ğ¸Ğ´Ğ¸Ğ¼Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ±ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¹.'}, 'en': {'title': 'Continuous Learning for Adaptive Intelligence in VLA Models', 'desc': 'EVOLVE-VLA is a framework designed for Vision-Language-Action (VLA) models that allows them to adapt continuously during test time by interacting with their environment. Unlike traditional methods that rely heavily on numerous task-specific demonstrations, EVOLVE-VLA requires minimal or no such demonstrations, enabling more flexible learning. The framework addresses the challenge of lacking reward signals during testing by using a learned progress estimator for feedback, which is refined through two innovative strategies. This approach leads to significant performance improvements, including better long-horizon task execution and the ability to generalize across different tasks without prior specific training.'}, 'zh': {'title': 'æŒç»­é€‚åº”çš„æ™ºèƒ½ä½“ï¼šEVOLVE-VLAæ¡†æ¶', 'desc': 'EVOLVE-VLAæ˜¯ä¸€ç§ç”¨äºè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„æµ‹è¯•æ—¶è®­ç»ƒæ¡†æ¶ï¼Œèƒ½å¤Ÿé€šè¿‡ä¸ç¯å¢ƒçš„äº’åŠ¨å®ç°æŒç»­é€‚åº”ï¼Œä¸”åªéœ€æå°‘çš„ç‰¹å®šä»»åŠ¡ç¤ºèŒƒã€‚è¯¥æ¡†æ¶è§£å†³äº†ä¼ ç»Ÿç›‘ç£å¾®è°ƒæ–¹æ³•çš„å±€é™æ€§ï¼Œå…è®¸æ¨¡å‹åœ¨æ²¡æœ‰å¤§é‡ç¤ºèŒƒçš„æƒ…å†µä¸‹è¿›è¡Œè‡ªæˆ‘æ”¹è¿›ã€‚é€šè¿‡å¼•å…¥å­¦ä¹ çš„è¿›åº¦ä¼°è®¡å™¨å’Œä¸¤ç§æœºåˆ¶æ¥å¹³æ»‘å™ªå£°ä¿¡å·ï¼ŒEVOLVE-VLAæ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨é•¿æ—¶é—´ä»»åŠ¡å’Œå•æ¬¡å­¦ä¹ ä¸­çš„è¡¨ç°ã€‚æœ€ç»ˆï¼Œè¯¥æ–¹æ³•å±•ç¤ºäº†æ¨¡å‹åœ¨æœªè§ä»»åŠ¡ä¸Šçš„è·¨ä»»åŠ¡æ³›åŒ–èƒ½åŠ›ï¼Œæ ‡å¿—ç€å‘çœŸæ­£è‡ªæˆ‘å­¦ä¹ å’Œé€‚åº”çš„æ™ºèƒ½ä½“è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.14273', 'title': 'Zoom-Zero: Reinforced Coarse-to-Fine Video Understanding via Temporal Zoom-in', 'url': 'https://huggingface.co/papers/2512.14273', 'abstract': "Zoom-Zero, a coarse-to-fine framework, enhances grounded video question answering by improving temporal grounding and answer accuracy through a zoom-in accuracy reward and token-selective credit assignment.  \t\t\t\t\tAI-generated summary \t\t\t\t Grounded video question answering (GVQA) aims to localize relevant temporal segments in videos and generate accurate answers to a given question; however, large video-language models (LVLMs) exhibit limited temporal awareness. Although existing approaches based on Group Relative Policy Optimization (GRPO) attempt to improve temporal grounding, they still struggle to faithfully ground their answers in the relevant video evidence, leading to temporal mislocalization and hallucinations. In this work, we present Zoom-Zero, a coarse-to-fine framework that first localizes query-relevant segments and then temporally zooms into the most salient frames for finer-grained visual verification. Our method addresses the limits of GRPO for the GVQA task with two key innovations: (i) a zoom-in accuracy reward that validates the fidelity of temporal grounding prediction and facilitates fine-grained visual verification on grounded frames; (ii) token-selective credit assignment, which attributes rewards to the tokens responsible for temporal localization or answer generation, mitigating GRPO's issue in handling multi-faceted reward signals. Our proposed method advances grounded video question answering, improving temporal grounding by 5.2\\% on NExT-GQA and 4.6\\% on ReXTime, while also enhancing average answer accuracy by 2.4\\%. Additionally, the coarse-to-fine zoom-in during inference further benefits long-form video understanding by preserving critical visual details without compromising global context, yielding an average improvement of 6.4\\% on long-video benchmarks.", 'score': 4, 'issue_id': 98, 'pub_date': '2025-12-16', 'pub_date_card': {'ru': '16 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 16', 'zh': '12æœˆ16æ—¥'}, 'hash': '031ced5c63d5290f', 'authors': ['Xiaoqian Shen', 'Min-Hung Chen', 'Yu-Chiang Frank Wang', 'Mohamed Elhoseiny', 'Ryo Hachiuma'], 'affiliations': ['KAUST', 'NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2512.14273.jpg', 'data': {'categories': ['#video', '#multimodal', '#optimization', '#rlhf', '#long_context', '#hallucinations', '#benchmark'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞÑ‚ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğº Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ: Ñ‚Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ñ€Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğµ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹', 'desc': 'Zoom-Zero â€” ÑÑ‚Ğ¾ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‚Ğ¸Ğ¿Ğ° "Ğ³Ñ€ÑƒĞ±Ñ‹Ğ¹-Ğº-Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¼Ñƒ", ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¾Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ³Ñ€ÑƒĞ¿Ğ¿, Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑÑ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñƒ Ğ·Ğ° Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ° Ğ¸ ÑĞµĞ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ¿Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼. Ğ­Ñ‚Ğ° Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ° Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¾Ğ² Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºÑƒ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸ Ğ½Ğ° Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¸ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Zoom-Zero: Enhancing Video Question Answering with Precision and Context', 'desc': 'The paper introduces Zoom-Zero, a new framework designed to improve grounded video question answering (GVQA) by enhancing both temporal grounding and answer accuracy. It addresses the limitations of existing large video-language models (LVLMs) that struggle with temporal awareness and often mislocalize answers. Zoom-Zero employs a coarse-to-fine approach, first identifying relevant video segments and then zooming in on key frames for detailed visual verification. Key innovations include a zoom-in accuracy reward for validating temporal predictions and token-selective credit assignment to better manage reward signals, resulting in significant improvements in both grounding and answer accuracy across various benchmarks.'}, 'zh': {'title': 'Zoom-Zeroï¼šæå‡è§†é¢‘é—®ç­”çš„æ—¶é—´å®šä½ä¸å‡†ç¡®æ€§', 'desc': 'Zoom-Zero æ˜¯ä¸€ä¸ªç²—åˆ°ç»†çš„æ¡†æ¶ï¼Œæ—¨åœ¨æå‡è§†é¢‘é—®ç­”çš„å‡†ç¡®æ€§å’Œæ—¶é—´å®šä½èƒ½åŠ›ã€‚å®ƒé€šè¿‡å¼•å…¥æ”¾å¤§å‡†ç¡®æ€§å¥–åŠ±å’Œé€‰æ‹©æ€§ä¿¡ç”¨åˆ†é…æ¥è§£å†³ç°æœ‰æ–¹æ³•åœ¨æ—¶é—´å®šä½ä¸Šçš„ä¸è¶³ã€‚è¯¥æ–¹æ³•é¦–å…ˆå®šä½ä¸æŸ¥è¯¢ç›¸å…³çš„æ—¶é—´æ®µï¼Œç„¶åèšç„¦äºæœ€é‡è¦çš„å¸§è¿›è¡Œæ›´ç²¾ç»†çš„è§†è§‰éªŒè¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒZoom-Zero åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†æ—¶é—´å®šä½å’Œç­”æ¡ˆå‡†ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.14008', 'title': 'Sparse-LaViDa: Sparse Multimodal Discrete Diffusion Language Models', 'url': 'https://huggingface.co/papers/2512.14008', 'abstract': 'Sparse-LaViDa accelerates Masked Discrete Diffusion Models by dynamically truncating masked tokens during inference, maintaining quality and achieving up to a 2x speedup across various tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Masked Discrete Diffusion Models (MDMs) have achieved strong performance across a wide range of multimodal tasks, including image understanding, generation, and editing. However, their inference speed remains suboptimal due to the need to repeatedly process redundant masked tokens at every sampling step. In this work, we propose Sparse-LaViDa, a novel modeling framework that dynamically truncates unnecessary masked tokens at each inference step to accelerate MDM sampling. To preserve generation quality, we introduce specialized register tokens that serve as compact representations for the truncated tokens. Furthermore, to ensure consistency between training and inference, we design a specialized attention mask that faithfully matches the truncated sampling procedure during training. Built upon the state-of-the-art unified MDM LaViDa-O, Sparse-LaViDa achieves up to a 2x speedup across diverse tasks including text-to-image generation, image editing, and mathematical reasoning, while maintaining generation quality.', 'score': 4, 'issue_id': 97, 'pub_date': '2025-12-16', 'pub_date_card': {'ru': '16 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 16', 'zh': '12æœˆ16æ—¥'}, 'hash': 'e3e3c8fae87d0b10', 'authors': ['Shufan Li', 'Jiuxiang Gu', 'Kangning Liu', 'Zhe Lin', 'Zijun Wei', 'Aditya Grover', 'Jason Kuen'], 'affiliations': ['Adobe', 'UCLA'], 'pdf_title_img': 'assets/pdf/title_img/2512.14008.jpg', 'data': {'categories': ['#inference', '#multimodal', '#architecture'], 'emoji': 'âš¡', 'ru': {'title': 'Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑƒÑĞµÑ‡ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Sparse-LaViDa Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑƒÑĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ¾Ğ²Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ»ÑƒĞ¶Ğ°Ñ‚ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¼ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ”Ğ»Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ¾Ğ¼ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ°ÑĞºĞ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ°Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğµ ÑƒÑĞµÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ´Ğ²ÑƒĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸Ñ… Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ, Ğ±ĞµĞ· ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Accelerating MDMs with Sparse Token Truncation', 'desc': 'Sparse-LaViDa is a new framework that speeds up Masked Discrete Diffusion Models (MDMs) by removing unnecessary masked tokens during the inference process. This approach helps to reduce the time taken for sampling without compromising the quality of the generated outputs. To achieve this, the framework uses specialized register tokens that represent the truncated tokens and a tailored attention mask to ensure consistency between training and inference. As a result, Sparse-LaViDa can deliver up to a 2x increase in speed for various tasks like text-to-image generation and image editing while maintaining high-quality results.'}, 'zh': {'title': 'åŠ¨æ€æˆªæ–­ï¼Œæå‡æ¨ç†é€Ÿåº¦ï¼', 'desc': 'Sparse-LaViDaæ˜¯ä¸€ç§æ–°é¢–çš„å»ºæ¨¡æ¡†æ¶ï¼Œæ—¨åœ¨åŠ é€Ÿæ©ç ç¦»æ•£æ‰©æ•£æ¨¡å‹ï¼ˆMDMsï¼‰çš„æ¨ç†è¿‡ç¨‹ã€‚é€šè¿‡åœ¨æ¯ä¸ªæ¨ç†æ­¥éª¤åŠ¨æ€æˆªæ–­ä¸å¿…è¦çš„æ©ç ä»¤ç‰Œï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæé«˜é‡‡æ ·é€Ÿåº¦ï¼Œæœ€é«˜å¯å®ç°2å€çš„åŠ é€Ÿã€‚ä¸ºäº†ä¿æŒç”Ÿæˆè´¨é‡ï¼ŒSparse-LaViDaå¼•å…¥äº†ä¸“é—¨çš„å¯„å­˜å™¨ä»¤ç‰Œï¼Œä½œä¸ºè¢«æˆªæ–­ä»¤ç‰Œçš„ç´§å‡‘è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œè®¾è®¡äº†ä¸“é—¨çš„æ³¨æ„åŠ›æ©ç ï¼Œä»¥ç¡®ä¿è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.13525', 'title': 'Janus: Disaggregating Attention and Experts for Scalable MoE Inference', 'url': 'https://huggingface.co/papers/2512.13525', 'abstract': 'Janus is a scalable Mixture-of-Experts (MoE) inference system that disaggregates attention and expert modules for independent scaling, improving throughput and latency.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Mixture-of-Experts (MoE) model inference is challenging due to high resource demands and dynamic workloads. Existing solutions often deploy the entire model as a single monolithic unit, which applies a unified resource configuration to both attention and expert modules despite their different requirements, leading to limited scalability and resource inefficiency. In this paper, we propose Janus, a scalable MoE inference system that disaggregates attention and experts on separate GPU sub-clusters, enabling each module to be managed and scaled independently. Janus incorporates three key designs for efficient, disaggregated MoE inference. First, it proposes an adaptive two-phase communication scheme that exploits intra- and inter-node bandwidth hierarchies for low-latency data exchange. Second, motivated by the memory-bound nature of MoE modules, Janus introduces a lightweight scheduler and implements it as a GPU kernel to balance the number of activated experts across GPUs at minimal overhead, thereby reducing inference latency. Third, Janus performs fine-grained resource management to dynamically adjust expert placement and independently scale attention and MoE resources to improve overall efficiency. Evaluation shows Janus achieves up to 3.9 higher perGPU throughput than state-of-the-art systems while meeting per-token latency requirements.', 'score': 4, 'issue_id': 103, 'pub_date': '2025-12-15', 'pub_date_card': {'ru': '15 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 15', 'zh': '12æœˆ15æ—¥'}, 'hash': 'd34d20055112d961', 'authors': ['Zhexiang Zhang', 'Ye Wang', 'Xiangyu Wang', 'Yumiao Zhao', 'Jingzhe Jiang', 'Qizhen Weng', 'Shaohuai Shi', 'Yin Chen', 'Minchen Yu'], 'affiliations': ['Harbin Institute of Technology, Shenzhen', 'Institute of Artificial Intelligence (TeleAI), China Telecom', 'Shenzhen Loop Area Institute', 'The Chinese University of Hong Kong, Shenzhen'], 'pdf_title_img': 'assets/pdf/title_img/2512.13525.jpg', 'data': {'categories': ['#architecture', '#inference'], 'emoji': 'âš¡', 'ru': {'title': 'ĞĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ² MoE-Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Janus â€” ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Mixture-of-Experts, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ±Ğ»Ğ¾ĞºĞ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸ Ğ½Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ GPU-ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ñ‹ Ğ´Ğ»Ñ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ´Ğ²ÑƒÑ…Ñ„Ğ°Ğ·Ğ½ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑƒĞ·Ğ»Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞµĞº Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ğ¼ĞµĞ½Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¾Ğ±Ğ»ĞµĞ³Ñ‡Ñ‘Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºÑƒ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ GPU-ÑĞ´Ñ€Ğ°, Janus Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒĞµÑ‚ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ñ‹ Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ latency Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½Ğ¾Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° GPU Ğ² 3,9 Ñ€Ğ°Ğ·Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¸ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğº Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞµ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ².'}, 'en': {'title': 'Janus: Scalable Inference for Efficient Mixture-of-Experts Models', 'desc': 'Janus is a new system designed to improve the efficiency of Mixture-of-Experts (MoE) model inference by separating the attention and expert components. This separation allows each part to be scaled independently, which enhances throughput and reduces latency. The system uses an adaptive communication method to optimize data transfer and a lightweight scheduler to manage expert activation across GPUs, minimizing delays. Overall, Janus demonstrates significant performance improvements, achieving higher throughput while maintaining low latency for processing tasks.'}, 'zh': {'title': 'Janusï¼šé«˜æ•ˆçš„ä¸“å®¶æ··åˆæ¨¡å‹æ¨ç†ç³»ç»Ÿ', 'desc': 'Janus æ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„ä¸“å®¶æ··åˆæ¨¡å‹ï¼ˆMoEï¼‰æ¨ç†ç³»ç»Ÿï¼Œå®ƒå°†æ³¨æ„åŠ›æ¨¡å—å’Œä¸“å®¶æ¨¡å—åˆ†å¼€ï¼Œä»¥ä¾¿ç‹¬ç«‹æ‰©å±•ï¼Œä»è€Œæé«˜ååé‡å’Œé™ä½å»¶è¿Ÿã€‚ç°æœ‰çš„è§£å†³æ–¹æ¡ˆé€šå¸¸å°†æ•´ä¸ªæ¨¡å‹ä½œä¸ºä¸€ä¸ªå•ä¸€çš„æ•´ä½“æ¥éƒ¨ç½²ï¼Œå¯¼è‡´èµ„æºé…ç½®ä¸çµæ´»ï¼Œé™åˆ¶äº†å¯æ‰©å±•æ€§å’Œèµ„æºæ•ˆç‡ã€‚Janus é€šè¿‡åœ¨ä¸åŒçš„ GPU å­é›†ç¾¤ä¸Šåˆ†ç¦»æ³¨æ„åŠ›å’Œä¸“å®¶æ¨¡å—ï¼Œå…è®¸å¯¹æ¯ä¸ªæ¨¡å—è¿›è¡Œç‹¬ç«‹ç®¡ç†å’Œæ‰©å±•ã€‚é€šè¿‡è‡ªé€‚åº”çš„ä¸¤é˜¶æ®µé€šä¿¡æ–¹æ¡ˆã€è½»é‡çº§è°ƒåº¦å™¨å’Œç»†ç²’åº¦èµ„æºç®¡ç†ï¼ŒJanus æ˜¾è‘—æé«˜äº†æ¨ç†æ•ˆç‡ï¼Œè¯„ä¼°ç»“æœæ˜¾ç¤ºå…¶æ¯ä¸ª GPU çš„ååé‡æ¯”ç°æœ‰ç³»ç»Ÿé«˜å‡º 3.9 å€ï¼ŒåŒæ—¶æ»¡è¶³æ¯ä¸ª token çš„å»¶è¿Ÿè¦æ±‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.14550', 'title': 'TAT: Task-Adaptive Transformer for All-in-One Medical Image Restoration', 'url': 'https://huggingface.co/papers/2512.14550', 'abstract': 'A task-adaptive Transformer (TAT) framework addresses challenges in medical image restoration by dynamically adjusting task-specific weights and loss balances, achieving state-of-the-art performance across various tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Medical image restoration (MedIR) aims to recover high-quality medical images from their low-quality counterparts. Recent advancements in MedIR have focused on All-in-One models capable of simultaneously addressing multiple different MedIR tasks. However, due to significant differences in both modality and degradation types, using a shared model for these diverse tasks requires careful consideration of two critical inter-task relationships: task interference, which occurs when conflicting gradient update directions arise across tasks on the same parameter, and task imbalance, which refers to uneven optimization caused by varying learning difficulties inherent to each task. To address these challenges, we propose a task-adaptive Transformer (TAT), a novel framework that dynamically adapts to different tasks through two key innovations. First, a task-adaptive weight generation strategy is introduced to mitigate task interference by generating task-specific weight parameters for each task, thereby eliminating potential gradient conflicts on shared weight parameters. Second, a task-adaptive loss balancing strategy is introduced to dynamically adjust loss weights based on task-specific learning difficulties, preventing task domination or undertraining. Extensive experiments demonstrate that our proposed TAT achieves state-of-the-art performance in three MedIR tasks--PET synthesis, CT denoising, and MRI super-resolution--both in task-specific and All-in-One settings. Code is available at https://github.com/Yaziwel/TAT.', 'score': 3, 'issue_id': 98, 'pub_date': '2025-12-16', 'pub_date_card': {'ru': '16 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 16', 'zh': '12æœˆ16æ—¥'}, 'hash': 'a2030d1d193c99ab', 'authors': ['Zhiwen Yang', 'Jiaju Zhang', 'Yang Yi', 'Jian Liang', 'Bingzheng Wei', 'Yan Xu'], 'affiliations': ['ByteDance Inc., Beijing 100098, China', 'School of Biological Science and Medical Engineering, State Key Laboratory of Software Development Environment, Key Laboratory of Biomechanics and Mechanobiology of Ministry of Education, Beijing Advanced Innovation Center for Biomedical Engineering, Beihang University, Beijing 100191, China', 'Sinovision Technologies (Beijing) Co., Ltd., Beijing 101102, China'], 'pdf_title_img': 'assets/pdf/title_img/2512.14550.jpg', 'data': {'categories': [], 'emoji': 'ğŸ¥', 'ru': {'title': 'ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° TAT (Task-Adaptive Transformer) Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ²ĞºĞ»Ğ°Ğ´ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ´Ğ²ÑƒÑ… ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ÑÑ…: Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ¾-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ²ĞµÑĞ¾Ğ² Ğ´Ğ»Ñ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ¾Ğ² Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ° Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ¸Ñ… Ğ´Ğ¸ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğ° Ğ² Ñ‚Ñ€Ñ‘Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…: ÑĞ¸Ğ½Ñ‚ĞµĞ· PET, ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ CT Ğ¸ ÑÑƒĞ¿ĞµÑ€Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ MRI.'}, 'en': {'title': 'Dynamic Adaptation for Superior Medical Image Restoration', 'desc': 'The paper presents a Task-Adaptive Transformer (TAT) framework designed to improve medical image restoration (MedIR) by addressing task interference and task imbalance. It introduces a task-adaptive weight generation strategy that creates specific weights for each task, reducing conflicts during gradient updates. Additionally, a task-adaptive loss balancing strategy is implemented to adjust loss weights according to the learning difficulties of each task, ensuring balanced optimization. The TAT framework demonstrates superior performance in multiple MedIR tasks, including PET synthesis, CT denoising, and MRI super-resolution, outperforming existing models.'}, 'zh': {'title': 'åŠ¨æ€é€‚åº”ï¼Œæå‡åŒ»å­¦å›¾åƒæ¢å¤æ•ˆæœ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ä»»åŠ¡è‡ªé€‚åº”å˜æ¢å™¨ï¼ˆTATï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³åŒ»å­¦å›¾åƒæ¢å¤ä¸­çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é€šè¿‡åŠ¨æ€è°ƒæ•´ä»»åŠ¡ç‰¹å®šçš„æƒé‡å’ŒæŸå¤±å¹³è¡¡ï¼Œæ˜¾è‘—æé«˜äº†åœ¨ä¸åŒåŒ»å­¦å›¾åƒæ¢å¤ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚TATå¼•å…¥äº†ä»»åŠ¡è‡ªé€‚åº”æƒé‡ç”Ÿæˆç­–ç•¥ï¼Œä»¥å‡å°‘ä»»åŠ¡å¹²æ‰°ï¼Œå¹¶é€šè¿‡ä»»åŠ¡è‡ªé€‚åº”æŸå¤±å¹³è¡¡ç­–ç•¥æ¥åº”å¯¹ä»»åŠ¡ä¸å¹³è¡¡é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTATåœ¨PETåˆæˆã€CTå»å™ªå’ŒMRIè¶…åˆ†è¾¨ç‡ç­‰ä»»åŠ¡ä¸­å‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.14391', 'title': 'RePo: Language Models with Context Re-Positioning', 'url': 'https://huggingface.co/papers/2512.14391', 'abstract': 'RePo, a novel context re-positioning mechanism in LLMs, reduces extraneous cognitive load by differentiably assigning token positions, enhancing performance on noisy and long contexts without compromising short-context tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t In-context learning is fundamental to modern Large Language Models (LLMs); however, prevailing architectures impose a rigid and fixed contextual structure by assigning linear or constant positional indices. Drawing on Cognitive Load Theory (CLT), we argue that this uninformative structure increases extraneous cognitive load, consuming finite working memory capacity that should be allocated to deep reasoning and attention allocation. To address this, we propose RePo, a novel mechanism that reduces extraneous load via context re-positioning. Unlike standard approaches, RePo utilizes a differentiable module, f_Ï†, to assign token positions that capture contextual dependencies, rather than replying on pre-defined integer range. By continually pre-training on the OLMo-2 1B backbone, we demonstrate that RePo significantly enhances performance on tasks involving noisy contexts, structured data, and longer context length, while maintaining competitive performance on general short-context tasks. Detailed analysis reveals that RePo successfully allocate higher attention to distant but relevant information, assign positions in dense and non-linear space, and capture the intrinsic structure of the input context. Our code is available at https://github.com/SakanaAI/repo.', 'score': 2, 'issue_id': 97, 'pub_date': '2025-12-16', 'pub_date_card': {'ru': '16 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 16', 'zh': '12æœˆ16æ—¥'}, 'hash': '04e999cf001c6c9a', 'authors': ['Huayang Li', 'Tianyu Zhao', 'Richard Sproat'], 'affiliations': ['Nara Institute of Science and Technology', 'Sakana AI, Japan'], 'pdf_title_img': 'assets/pdf/title_img/2512.14391.jpg', 'data': {'categories': ['#long_context', '#open_source'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ¿ĞµÑ€ĞµÑƒĞ¿Ğ¾Ñ€ÑĞ´Ğ¾Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ LLM', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ RePo â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¿ĞµÑ€ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ñ… Ğ¸Ğ½Ğ´ĞµĞºÑĞ¾Ğ². ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ¸ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ° ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½ĞµĞ¹ Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ½Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‡ÑƒÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ ĞµĞ¹ ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ¾Ñ‡Ğ¸Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ¼ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ RePo ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ ÑˆÑƒĞ¼Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸, ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…. ĞœĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ²Ñ‹ÑƒÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¾Ñ‚ ĞµÑ‘ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°.'}, 'en': {'title': 'RePo: Redefining Context for Enhanced LLM Performance', 'desc': 'This paper introduces RePo, a new mechanism for Large Language Models (LLMs) that improves how token positions are assigned in context. By using a differentiable approach, RePo reduces unnecessary cognitive load, allowing the model to focus more on important information rather than being constrained by fixed positional indices. The authors show that RePo enhances performance on tasks with noisy or lengthy contexts while still performing well on shorter contexts. This method allows the model to better capture contextual dependencies and allocate attention effectively, leading to improved reasoning capabilities.'}, 'zh': {'title': 'RePoï¼šä¼˜åŒ–ä¸Šä¸‹æ–‡å¤„ç†çš„åˆ›æ–°æœºåˆ¶', 'desc': 'RePoæ˜¯ä¸€ç§æ–°é¢–çš„ä¸Šä¸‹æ–‡é‡æ–°å®šä½æœºåˆ¶ï¼Œæ—¨åœ¨å‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„é¢å¤–è®¤çŸ¥è´Ÿæ‹…ã€‚é€šè¿‡å¯å¾®åˆ†åœ°åˆ†é…ä»¤ç‰Œä½ç½®ï¼ŒRePoèƒ½å¤Ÿåœ¨å¤„ç†å˜ˆæ‚å’Œé•¿ä¸Šä¸‹æ–‡æ—¶æé«˜æ€§èƒ½ï¼ŒåŒæ—¶ä¸å½±å“çŸ­ä¸Šä¸‹æ–‡ä»»åŠ¡çš„è¡¨ç°ã€‚è¯¥æœºåˆ¶åŸºäºè®¤çŸ¥è´Ÿè·ç†è®ºï¼Œä¼˜åŒ–äº†ä¸Šä¸‹æ–‡ç»“æ„ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨å·¥ä½œè®°å¿†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRePoåœ¨å¤æ‚ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°å…³æ³¨ç›¸å…³ä¿¡æ¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.14067', 'title': 'Efficient-DLM: From Autoregressive to Diffusion Language Models, and Beyond in Speed', 'url': 'https://huggingface.co/papers/2512.14067', 'abstract': "AR-to-dLM conversion enhances diffusion language models' efficiency and speed while maintaining task accuracy through refined attention patterns and token masking strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion language models (dLMs) have emerged as a promising paradigm that enables parallel, non-autoregressive generation, but their learning efficiency lags behind that of autoregressive (AR) language models when trained from scratch. To this end, we study AR-to-dLM conversion to transform pretrained AR models into efficient dLMs that excel in speed while preserving AR models' task accuracy. We achieve this by identifying limitations in the attention patterns and objectives of existing AR-to-dLM methods and then proposing principles and methodologies for more effective AR-to-dLM conversion. Specifically, we first systematically compare different attention patterns and find that maintaining pretrained AR weight distributions is critical for effective AR-to-dLM conversion. As such, we introduce a continuous pretraining scheme with a block-wise attention pattern, which remains causal across blocks while enabling bidirectional modeling within each block. We find that this approach can better preserve pretrained AR models' weight distributions than fully bidirectional modeling, in addition to its known benefit of enabling KV caching, and leads to a win-win in accuracy and efficiency. Second, to mitigate the training-test gap in mask token distributions (uniform vs. highly left-to-right), we propose a position-dependent token masking strategy that assigns higher masking probabilities to later tokens during training to better mimic test-time behavior. Leveraging this framework, we conduct extensive studies of dLMs' attention patterns, training dynamics, and other design choices, providing actionable insights into scalable AR-to-dLM conversion. These studies lead to the Efficient-DLM family, which outperforms state-of-the-art AR models and dLMs, e.g., our Efficient-DLM 8B achieves +5.4%/+2.7% higher accuracy with 4.5x/2.7x higher throughput compared to Dream 7B and Qwen3 4B, respectively.", 'score': 2, 'issue_id': 95, 'pub_date': '2025-12-16', 'pub_date_card': {'ru': '16 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 16', 'zh': '12æœˆ16æ—¥'}, 'hash': '6705a84657cf61dd', 'authors': ['Yonggan Fu', 'Lexington Whalen', 'Zhifan Ye', 'Xin Dong', 'Shizhe Diao', 'Jingyu Liu', 'Chengyue Wu', 'Hao Zhang', 'Enze Xie', 'Song Han', 'Maksim Khadkevich', 'Jan Kautz', 'Yingyan Celine Lin', 'Pavlo Molchanov'], 'affiliations': ['Georgia Tech', 'MIT', 'NVIDIA', 'University of Chicago', 'University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2512.14067.jpg', 'data': {'categories': ['#optimization', '#transfer_learning', '#diffusion'], 'emoji': 'âš¡', 'ru': {'title': 'ĞŸÑ€ĞµĞ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ñ‚ĞµĞºÑÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ĞºĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ±Ğ»Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²ĞµÑĞ¾Ğ² Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ¾Ğ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼ÑƒÑ Ğ¾Ñ‚ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ¸Ñ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼Ğ°ÑĞ¾Ğº Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ¸Ñ€ÑƒÑÑ‰ĞµĞµ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Efficient-DLM Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½Ğ¾Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ….'}, 'en': {'title': 'Transforming AR Models for Faster, Accurate dLMs', 'desc': 'This paper discusses a method to improve diffusion language models (dLMs) by converting pretrained autoregressive (AR) models into more efficient dLMs. The authors identify issues with existing AR-to-dLM conversion techniques, particularly in attention patterns and token masking strategies. They propose a continuous pretraining scheme that maintains the weight distributions of AR models while allowing for efficient bidirectional modeling. Their approach results in significant improvements in both speed and accuracy, leading to the development of the Efficient-DLM family, which surpasses current state-of-the-art models.'}, 'zh': {'title': 'é«˜æ•ˆè½¬æ¢ï¼šæå‡æ‰©æ•£è¯­è¨€æ¨¡å‹çš„é€Ÿåº¦ä¸å‡†ç¡®æ€§', 'desc': 'æœ¬æ–‡ç ”ç©¶äº†å°†è‡ªå›å½’ï¼ˆARï¼‰è¯­è¨€æ¨¡å‹è½¬æ¢ä¸ºæ‰©æ•£è¯­è¨€æ¨¡å‹ï¼ˆdLMï¼‰çš„è¿‡ç¨‹ï¼Œä»¥æé«˜æ¨¡å‹çš„æ•ˆç‡å’Œé€Ÿåº¦ï¼ŒåŒæ—¶ä¿æŒä»»åŠ¡çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬å‘ç°ï¼Œä¿æŒé¢„è®­ç»ƒARæ¨¡å‹çš„æƒé‡åˆ†å¸ƒå¯¹äºæœ‰æ•ˆçš„ARåˆ°dLMè½¬æ¢è‡³å…³é‡è¦ï¼Œå› æ­¤æå‡ºäº†ä¸€ç§è¿ç»­é¢„è®­ç»ƒæ–¹æ¡ˆï¼Œé‡‡ç”¨å—çº§æ³¨æ„åŠ›æ¨¡å¼ã€‚ä¸ºäº†ç¼©å°è®­ç»ƒå’Œæµ‹è¯•ä¹‹é—´çš„å·®è·ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§ä½ç½®ä¾èµ–çš„æ ‡è®°æ©è”½ç­–ç•¥ï¼Œä»¥æ›´å¥½åœ°æ¨¡æ‹Ÿæµ‹è¯•æ—¶çš„è¡Œä¸ºã€‚é€šè¿‡è¿™äº›æ–¹æ³•ï¼Œæˆ‘ä»¬çš„Efficient-DLMç³»åˆ—æ¨¡å‹åœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡ä¸Šå‡ä¼˜äºç°æœ‰çš„ARå’ŒdLMæ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.14697', 'title': 'Spherical Leech Quantization for Visual Tokenization and Generation', 'url': 'https://huggingface.co/papers/2512.14697', 'abstract': 'Lattice coding provides a unified framework for non-parametric quantization, with Leech lattice-based quantization achieving superior performance in image tokenization, compression, and generation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Non-parametric quantization has received much attention due to its efficiency on parameters and scalability to a large codebook. In this paper, we present a unified formulation of different non-parametric quantization methods through the lens of lattice coding. The geometry of lattice codes explains the necessity of auxiliary loss terms when training auto-encoders with certain existing lookup-free quantization variants such as BSQ. As a step forward, we explore a few possible candidates, including random lattices, generalized Fibonacci lattices, and densest sphere packing lattices. Among all, we find the Leech lattice-based quantization method, which is dubbed as Spherical Leech Quantization (Î›_{24}-SQ), leads to both a simplified training recipe and an improved reconstruction-compression tradeoff thanks to its high symmetry and even distribution on the hypersphere. In image tokenization and compression tasks, this quantization approach achieves better reconstruction quality across all metrics than BSQ, the best prior art, while consuming slightly fewer bits. The improvement also extends to state-of-the-art auto-regressive image generation frameworks.', 'score': 1, 'issue_id': 100, 'pub_date': '2025-12-16', 'pub_date_card': {'ru': '16 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 16', 'zh': '12æœˆ16æ—¥'}, 'hash': '31463b824e3c7a76', 'authors': ['Yue Zhao', 'Hanwen Jiang', 'Zhenlin Xu', 'Chutong Yang', 'Ehsan Adeli', 'Philipp KrÃ¤henbÃ¼hl'], 'affiliations': ['Adobe Research', 'Mistral AI', 'Stanford University', 'UT Austin'], 'pdf_title_img': 'assets/pdf/title_img/2512.14697.jpg', 'data': {'categories': ['#optimization'], 'emoji': 'ğŸ”·', 'ru': {'title': 'Ğ ĞµÑˆÑ‘Ñ‚ĞºĞ¸ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ½ĞµĞ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ½ĞµĞ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ñ‚ĞµĞ¾Ñ€Ğ¸Ñ Ñ€ĞµÑˆÑ‘Ñ‚Ğ¾Ğº ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±ÑŠÑÑĞ½ÑÑÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ñ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ñ€ĞµÑˆÑ‘Ñ‚Ğ¾Ğº. Ğ˜ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ÑÑ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ñ‹ Ñ€ĞµÑˆÑ‘Ñ‚Ğ¾Ğº, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ğµ, Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ñ‘Ğ½Ğ½Ñ‹Ğµ Ñ€ĞµÑˆÑ‘Ñ‚ĞºĞ¸ Ğ¤Ğ¸Ğ±Ğ¾Ğ½Ğ°Ñ‡Ñ‡Ğ¸ Ğ¸ Ñ€ĞµÑˆÑ‘Ñ‚ĞºĞ¸ Ğ¿Ğ»Ğ¾Ñ‚Ğ½ĞµĞ¹ÑˆĞµĞ¹ ÑƒĞ¿Ğ°ĞºĞ¾Ğ²ĞºĞ¸ ÑÑ„ĞµÑ€. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Spherical Leech Quantization Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ ÑĞ¶Ğ°Ñ‚Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¼ Ñ€Ğ°ÑÑ…Ğ¾Ğ´Ğµ Ğ±Ğ¸Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Unlocking Image Compression with Lattice Coding', 'desc': 'This paper introduces lattice coding as a comprehensive approach to non-parametric quantization, which is important for efficiently managing large codebooks. It highlights the advantages of using the Leech lattice for quantization, specifically in tasks like image tokenization and compression. The authors demonstrate that their method, Spherical Leech Quantization (Î›_{24}-SQ), not only simplifies the training process but also enhances the balance between reconstruction quality and compression efficiency. Overall, this approach outperforms existing methods, such as BSQ, in various metrics while requiring fewer bits for representation.'}, 'zh': {'title': 'åŸºäºLeechæ ¼çš„é‡åŒ–ï¼šå›¾åƒå¤„ç†çš„æ–°çªç ´', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ ¼ç¼–ç çš„ç»Ÿä¸€æ¡†æ¶ï¼Œç”¨äºéå‚æ•°é‡åŒ–ï¼Œç‰¹åˆ«æ˜¯åœ¨å›¾åƒæ ‡è®°ã€å‹ç¼©å’Œç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚é€šè¿‡æ ¼ç¼–ç çš„å‡ ä½•ç‰¹æ€§ï¼Œè§£é‡Šäº†åœ¨è®­ç»ƒæŸäº›æ— æŸ¥æ‰¾é‡åŒ–å˜ä½“ï¼ˆå¦‚BSQï¼‰æ—¶ï¼Œè¾…åŠ©æŸå¤±é¡¹çš„å¿…è¦æ€§ã€‚æˆ‘ä»¬æ¢ç´¢äº†å‡ ç§å¯èƒ½çš„å€™é€‰æ ¼ï¼ŒåŒ…æ‹¬éšæœºæ ¼ã€å¹¿ä¹‰æ–æ³¢é‚£å¥‘æ ¼å’Œæœ€å¯†çƒå¡«å……æ ¼ã€‚æœ€ç»ˆå‘ç°ï¼ŒåŸºäºLeechæ ¼çš„é‡åŒ–æ–¹æ³•ï¼ˆç§°ä¸ºSpherical Leech Quantizationï¼‰åœ¨è®­ç»ƒç®€åŒ–å’Œé‡å»º-å‹ç¼©æƒè¡¡æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œä¸”åœ¨å›¾åƒä»»åŠ¡ä¸­è¶…è¶Šäº†ç°æœ‰æœ€ä½³æ–¹æ³•BSQã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.14620', 'title': 'JMMMU-Pro: Image-based Japanese Multi-discipline Multimodal Understanding Benchmark via Vibe Benchmark Construction', 'url': 'https://huggingface.co/papers/2512.14620', 'abstract': "JMMMU-Pro, an image-based Japanese multi-discipline multimodal understanding benchmark, challenges open-source large multimodal models through integrated visual-textual understanding and is constructed using Vibe Benchmark Construction, a cost-effective method leveraging realistic image generation.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper introduces JMMMU-Pro, an image-based Japanese Multi-discipline Multimodal Understanding Benchmark, and Vibe Benchmark Construction, a scalable construction method. Following the evolution from MMMU to MMMU-Pro, JMMMU-Pro extends JMMMU by composing the question image and question text into a single image, thereby creating a benchmark that requires integrated visual-textual understanding through visual perception. To build JMMMU-Pro, we propose Vibe Benchmark Construction, a methodology in which an image generative model (e.g., Nano Banana Pro) produces candidate visual questions, and humans verify the outputs and, when necessary, regenerate with adjusted prompts to ensure quality. By leveraging Nano Banana Pro's highly realistic image generation capabilities and its ability to embed clean Japanese text, we construct a high-quality benchmark at low cost, covering a wide range of background and layout designs. Experimental results show that all open-source LMMs struggle substantially with JMMMU-Pro, underscoring JMMMU-Pro as an important benchmark for guiding future efforts in the open-source community. We believe that JMMMU-Pro provides a more rigorous evaluation tool for assessing the Japanese capabilities of LMMs and that our Vibe Benchmark Construction also offers an efficient guideline for future development of image-based VQA benchmarks.", 'score': 1, 'issue_id': 101, 'pub_date': '2025-12-16', 'pub_date_card': {'ru': '16 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 16', 'zh': '12æœˆ16æ—¥'}, 'hash': '00543e33dc1154bd', 'authors': ['Atsuyuki Miyai', 'Shota Onohara', 'Jeonghun Baek', 'Kiyoharu Aizawa'], 'affiliations': ['The University of Tokyo'], 'pdf_title_img': 'assets/pdf/title_img/2512.14620.jpg', 'data': {'categories': ['#open_source', '#synthetic', '#low_resource', '#dataset', '#benchmark', '#multilingual', '#multimodal'], 'emoji': 'ğŸ‡¯ğŸ‡µ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ JMMMU-Pro â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑĞ¿Ğ¾Ğ½ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Vibe Benchmark Construction, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ñ Ğ½Ğ¸Ğ·ĞºĞ¸Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ Ğ¸Ñ… Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚ÑÑ‚Ğ°ÑÑ‚ Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ ĞµĞ³Ğ¾ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ ÑĞ¿Ğ¾Ğ½ÑĞºĞ¸Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ LMM. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡.'}, 'en': {'title': 'JMMMU-Pro: A New Benchmark for Multimodal Understanding in Japanese', 'desc': 'This paper presents JMMMU-Pro, a benchmark designed to evaluate large multimodal models on their ability to understand both images and text in Japanese. It introduces a novel construction method called Vibe Benchmark Construction, which uses an image generative model to create realistic visual questions that are then verified by humans. By combining visual and textual elements into a single image, JMMMU-Pro challenges models to demonstrate integrated visual-textual understanding. The results indicate that current open-source large multimodal models face significant difficulties with this benchmark, highlighting its importance for future research in the field.'}, 'zh': {'title': 'JMMMU-Proï¼šæŒ‘æˆ˜å¤šæ¨¡æ€ç†è§£çš„æ–°åŸºå‡†', 'desc': 'æœ¬æ–‡ä»‹ç»äº†JMMMU-Proï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå›¾åƒçš„æ—¥æœ¬å¤šå­¦ç§‘å¤šæ¨¡æ€ç†è§£åŸºå‡†ï¼Œæ—¨åœ¨é€šè¿‡ç»¼åˆè§†è§‰-æ–‡æœ¬ç†è§£æ¥æŒ‘æˆ˜å¼€æºçš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ã€‚JMMMU-Proé€šè¿‡å°†é—®é¢˜å›¾åƒå’Œé—®é¢˜æ–‡æœ¬åˆæˆåˆ°å•ä¸€å›¾åƒä¸­ï¼Œæ‰©å±•äº†ä¹‹å‰çš„JMMUåŸºå‡†ï¼Œè¦æ±‚æ¨¡å‹å…·å¤‡æ›´å¼ºçš„è§†è§‰æ„ŸçŸ¥èƒ½åŠ›ã€‚ä¸ºäº†æ„å»ºJMMMU-Proï¼Œæå‡ºäº†VibeåŸºå‡†æ„å»ºæ–¹æ³•ï¼Œåˆ©ç”¨å›¾åƒç”Ÿæˆæ¨¡å‹ç”Ÿæˆå€™é€‰è§†è§‰é—®é¢˜ï¼Œå¹¶é€šè¿‡äººå·¥éªŒè¯ç¡®ä¿è¾“å‡ºè´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æœ‰å¼€æºçš„å¤šæ¨¡æ€æ¨¡å‹åœ¨JMMMU-Proä¸Šè¡¨ç°ä¸ä½³ï¼Œå¼ºè°ƒäº†è¯¥åŸºå‡†åœ¨æœªæ¥å¼€æºç¤¾åŒºä¸­çš„é‡è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.14014', 'title': 'MobileWorldBench: Towards Semantic World Modeling For Mobile Agents', 'url': 'https://huggingface.co/papers/2512.14014', 'abstract': 'A novel vision-language model framework improves task success rates for mobile GUI agents by using semantic world models instead of pixel-based predictions.  \t\t\t\t\tAI-generated summary \t\t\t\t World models have shown great utility in improving the task performance of embodied agents. While prior work largely focuses on pixel-space world models, these approaches face practical limitations in GUI settings, where predicting complex visual elements in future states is often difficult. In this work, we explore an alternative formulation of world modeling for GUI agents, where state transitions are described in natural language rather than predicting raw pixels. First, we introduce MobileWorldBench, a benchmark that evaluates the ability of vision-language models (VLMs) to function as world models for mobile GUI agents. Second, we release MobileWorld, a large-scale dataset consisting of 1.4M samples, that significantly improves the world modeling capabilities of VLMs. Finally, we propose a novel framework that integrates VLM world models into the planning framework of mobile agents, demonstrating that semantic world models can directly benefit mobile agents by improving task success rates. The code and dataset is available at https://github.com/jacklishufan/MobileWorld', 'score': 1, 'issue_id': 97, 'pub_date': '2025-12-16', 'pub_date_card': {'ru': '16 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 16', 'zh': '12æœˆ16æ—¥'}, 'hash': '76fb2cefb37b558d', 'authors': ['Shufan Li', 'Konstantinos Kallidromitis', 'Akash Gokul', 'Yusuke Kato', 'Kazuki Kozuka', 'Aditya Grover'], 'affiliations': ['Panasonic AI Research', 'Salesforce AI Research', 'UCLA'], 'pdf_title_img': 'assets/pdf/title_img/2512.14014.jpg', 'data': {'categories': ['#cv', '#benchmark', '#open_source', '#multimodal', '#agents', '#dataset'], 'emoji': 'ğŸŒ', 'ru': {'title': 'Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¸Ñ€Ğ° Ğ´Ğ»Ñ ÑƒĞ¼Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¸Ñ€Ğ° Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ·Ğ°Ğ¼ĞµĞ½Ğ¸Ğ² Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. ĞĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ MobileWorldBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM) Ğ²Ñ‹ÑÑ‚ÑƒĞ¿Ğ°Ñ‚ÑŒ Ğ² Ñ€Ğ¾Ğ»Ğ¸ Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹Ğ¿ÑƒÑÑ‚Ğ¸Ğ»Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ MobileWorld Ñ 1.4 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ VLM Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¸Ğ¼ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¸Ñ€Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ² ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Empowering Mobile Agents with Semantic World Models', 'desc': 'This paper presents a new framework for vision-language models (VLMs) that enhances the performance of mobile GUI agents by utilizing semantic world models instead of traditional pixel-based predictions. The authors introduce MobileWorldBench, a benchmark designed to assess how well VLMs can serve as world models in mobile GUI contexts. They also provide MobileWorld, a comprehensive dataset with 1.4 million samples that boosts the world modeling capabilities of VLMs. The proposed framework shows that using semantic descriptions for state transitions can significantly improve the task success rates of mobile agents.'}, 'zh': {'title': 'è¯­ä¹‰ä¸–ç•Œæ¨¡å‹æå‡ç§»åŠ¨ä»£ç†ä»»åŠ¡æˆåŠŸç‡', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è§†è§‰-è¯­è¨€æ¨¡å‹æ¡†æ¶ï¼Œé€šè¿‡ä½¿ç”¨è¯­ä¹‰ä¸–ç•Œæ¨¡å‹æ¥æé«˜ç§»åŠ¨GUIä»£ç†çš„ä»»åŠ¡æˆåŠŸç‡ï¼Œè€Œä¸æ˜¯ä¾èµ–äºåŸºäºåƒç´ çš„é¢„æµ‹ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä¼ ç»Ÿçš„åƒç´ ç©ºé—´ä¸–ç•Œæ¨¡å‹åœ¨GUIç¯å¢ƒä¸­å­˜åœ¨å±€é™æ€§ï¼Œå› æ­¤æˆ‘ä»¬æ¢ç´¢äº†ä¸€ç§æ–°çš„ä¸–ç•Œå»ºæ¨¡æ–¹æ³•ï¼Œä½¿ç”¨è‡ªç„¶è¯­è¨€æè¿°çŠ¶æ€è½¬ç§»ã€‚æˆ‘ä»¬å¼•å…¥äº†MobileWorldBenchåŸºå‡†ï¼Œè¯„ä¼°è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨ç§»åŠ¨GUIä»£ç†ä¸­çš„è¡¨ç°ï¼Œå¹¶å‘å¸ƒäº†åŒ…å«140ä¸‡æ ·æœ¬çš„å¤§è§„æ¨¡æ•°æ®é›†MobileWorldï¼Œä»¥å¢å¼ºæ¨¡å‹çš„ä¸–ç•Œå»ºæ¨¡èƒ½åŠ›ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å°†è§†è§‰-è¯­è¨€æ¨¡å‹ä¸–ç•Œæ¨¡å‹æ•´åˆåˆ°ç§»åŠ¨ä»£ç†è§„åˆ’æ¡†æ¶ä¸­çš„æ–°æ–¹æ³•ï¼Œè¯æ˜äº†è¯­ä¹‰ä¸–ç•Œæ¨¡å‹å¯ä»¥ç›´æ¥æé«˜ç§»åŠ¨ä»£ç†çš„ä»»åŠ¡æˆåŠŸç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.13655', 'title': 'Comparative Analysis of LLM Abliteration Methods: A Cross-Architecture Evaluation', 'url': 'https://huggingface.co/papers/2512.13655', 'abstract': 'Four abliteration tools are evaluated for their effectiveness in removing refusal representations from large language models, with findings showing variability in capability preservation and distribution shift across different models and tools.  \t\t\t\t\tAI-generated summary \t\t\t\t Safety alignment mechanisms in large language models prevent responses to harmful queries through learned refusal behavior, yet these same mechanisms impede legitimate research applications including cognitive modeling, adversarial testing, and security analysis. While abliteration techniques enable surgical removal of refusal representations through directional orthogonalization, the relative effectiveness of available implementations remains uncharacterized. This study evaluates four abliteration tools (Heretic, DECCP, ErisForge, FailSpy) across sixteen instruction-tuned models (7B-14B parameters), reporting tool compatibility on all 16 models and quantitative metrics on subsets dictated by tool support. Single-pass methods demonstrated superior capability preservation on the benchmarked subset (avg GSM8K change across three models: ErisForge -0.28 pp; DECCP -0.13 pp), while Bayesian-optimized abliteration produced variable distribution shift (KL divergence: 0.043-1.646) with model-dependent capability impact. These findings provide researchers with evidence-based selection criteria for abliteration tool deployment across diverse model architectures. The principal finding indicates that mathematical reasoning capabilities exhibit the highest sensitivity to abliteration interventions, with GSM8K change ranging from +1.51 pp to -18.81 pp (-26.5% relative) depending on tool selection and model architecture.', 'score': 1, 'issue_id': 95, 'pub_date': '2025-12-15', 'pub_date_card': {'ru': '15 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 15', 'zh': '12æœˆ15æ—¥'}, 'hash': '7535b4116fe7160c', 'authors': ['Richard J. Young'], 'affiliations': ['University of Nevada Las Vegas, Department of Neuroscience'], 'pdf_title_img': 'assets/pdf/title_img/2512.13655.jpg', 'data': {'categories': ['#training', '#security', '#benchmark', '#alignment', '#interpretability'], 'emoji': 'ğŸ”ª', 'ru': {'title': 'Ğ¥Ğ¸Ñ€ÑƒÑ€Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚ĞºĞ°Ğ·Ğ¾Ğ²: ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ°Ğ±Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ LLM', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‡ĞµÑ‚Ñ‹Ñ€Ñ‘Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ°Ğ±Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ¾Ñ‚ĞºĞ°Ğ·Ğ° Ğ¸Ğ· Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ ÑÑ‚Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° 16 Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº Ğ±Ğ°Ğ¹ĞµÑĞ¾Ğ²ÑĞºĞ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ²Ñ‹Ğ²Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğº Ğ°Ğ±Ğ»ÑÑ†Ğ¸Ğ¸, Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ +1.51% Ğ´Ğ¾ -26.5% Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ° Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Choosing the Right Tool for Effective Abliteration in Language Models', 'desc': 'This paper evaluates four different abliteration tools designed to remove refusal representations from large language models. The study finds that these tools vary in their ability to preserve model capabilities and manage distribution shifts across different model architectures. It highlights that single-pass methods tend to maintain better performance compared to Bayesian-optimized approaches, which can lead to significant variability in model behavior. The results provide valuable insights for researchers on how to choose the right abliteration tool based on the specific model and application needs.'}, 'zh': {'title': 'æ¶ˆé™¤å·¥å…·çš„æœ‰æ•ˆæ€§è¯„ä¼°ä¸é€‰æ‹©æ ‡å‡†', 'desc': 'æœ¬ç ”ç©¶è¯„ä¼°äº†å››ç§æ¶ˆé™¤å·¥å…·åœ¨å»é™¤å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æ‹’ç»è¡¨ç¤ºçš„æœ‰æ•ˆæ€§ã€‚è¿™äº›å·¥å…·åœ¨ä¸åŒæ¨¡å‹å’Œå·¥å…·ä¹‹é—´çš„èƒ½åŠ›ä¿ç•™å’Œåˆ†å¸ƒå˜åŒ–ä¸Šè¡¨ç°å‡ºå·®å¼‚ã€‚ç ”ç©¶å‘ç°ï¼Œæ•°å­¦æ¨ç†èƒ½åŠ›å¯¹æ¶ˆé™¤å¹²é¢„çš„æ•æ„Ÿæ€§æœ€é«˜ï¼Œå˜åŒ–èŒƒå›´ä»+1.51 ppåˆ°-18.81 ppã€‚ç ”ç©¶ç»“æœä¸ºç ”ç©¶äººå‘˜åœ¨ä¸åŒæ¨¡å‹æ¶æ„ä¸­é€‰æ‹©æ¶ˆé™¤å·¥å…·æä¾›äº†åŸºäºè¯æ®çš„æ ‡å‡†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.14440', 'title': 'S2D: Sparse-To-Dense Keymask Distillation for Unsupervised Video Instance Segmentation', 'url': 'https://huggingface.co/papers/2512.14440', 'abstract': 'An unsupervised video instance segmentation model using real video data and deep motion priors outperforms existing methods by establishing temporal coherence and using sparse-to-dense distillation.  \t\t\t\t\tAI-generated summary \t\t\t\t In recent years, the state-of-the-art in unsupervised video instance segmentation has heavily relied on synthetic video data, generated from object-centric image datasets such as ImageNet. However, video synthesis by artificially shifting and scaling image instance masks fails to accurately model realistic motion in videos, such as perspective changes, movement by parts of one or multiple instances, or camera motion. To tackle this issue, we propose an unsupervised video instance segmentation model trained exclusively on real video data. We start from unsupervised instance segmentation masks on individual video frames. However, these single-frame segmentations exhibit temporal noise and their quality varies through the video. Therefore, we establish temporal coherence by identifying high-quality keymasks in the video by leveraging deep motion priors. The sparse keymask pseudo-annotations are then used to train a segmentation model for implicit mask propagation, for which we propose a Sparse-To-Dense Distillation approach aided by a Temporal DropLoss. After training the final model on the resulting dense labelset, our approach outperforms the current state-of-the-art across various benchmarks.', 'score': 0, 'issue_id': 98, 'pub_date': '2025-12-16', 'pub_date_card': {'ru': '16 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 16', 'zh': '12æœˆ16æ—¥'}, 'hash': '9588223dcec26f85', 'authors': ['Leon Sick', 'Lukas Hoyer', 'Dominik Engel', 'Pedro Hermosilla', 'Timo Ropinski'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2512.14440.jpg', 'data': {'categories': ['#video', '#dataset', '#cv'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ’Ğ¸Ğ´ĞµĞ¾ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‡ĞµÑ€ĞµĞ· Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ĞºĞ¾Ğ³ĞµÑ€ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ', 'desc': 'Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½ĞµĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ¾Ğ² Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑˆÑƒĞ¼Ğ° Ğ² Ğ¼Ğ°ÑĞºĞ°Ñ… ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸, ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ¿Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ¼Ğ°ÑĞ¾Ğº Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚Ğ¾Ğ² Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ° Sparse-To-Dense Distillation Ñ Temporal DropLoss Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¼Ğ°ÑĞ¾Ğº Ğ½Ğ° Ğ²ĞµÑÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ….'}, 'en': {'title': 'Real Data, Real Motion: Advancing Video Instance Segmentation', 'desc': 'This paper presents a novel unsupervised video instance segmentation model that utilizes real video data instead of synthetic data, which often fails to capture realistic motion dynamics. The model improves temporal coherence by identifying high-quality keymasks using deep motion priors, addressing the noise and variability in single-frame segmentations. It employs a Sparse-To-Dense Distillation technique to enhance the training process, allowing for better mask propagation across frames. As a result, the proposed method achieves superior performance compared to existing techniques on multiple benchmarks.'}, 'zh': {'title': 'çœŸå®è§†é¢‘æ•°æ®é©±åŠ¨çš„æ— ç›‘ç£å®ä¾‹åˆ†å‰²æ–°çªç ´', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— ç›‘ç£è§†é¢‘å®ä¾‹åˆ†å‰²æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä»…ä½¿ç”¨çœŸå®è§†é¢‘æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå…‹æœäº†ä»¥å¾€ä¾èµ–åˆæˆè§†é¢‘æ•°æ®çš„å±€é™ã€‚é€šè¿‡åˆ©ç”¨æ·±åº¦è¿åŠ¨å…ˆéªŒï¼Œæˆ‘ä»¬åœ¨è§†é¢‘ä¸­è¯†åˆ«é«˜è´¨é‡çš„å…³é”®æ©ç ï¼Œä»è€Œå»ºç«‹æ—¶é—´ä¸€è‡´æ€§ã€‚æ¥ç€ï¼Œé‡‡ç”¨ç¨€ç–åˆ°å¯†é›†çš„è’¸é¦æ–¹æ³•ï¼Œåˆ©ç”¨ç¨€ç–çš„å…³é”®æ©ç ä¼ªæ ‡æ³¨æ¥è®­ç»ƒåˆ†å‰²æ¨¡å‹ï¼Œå®ç°éšå¼æ©ç ä¼ æ’­ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æŠ€æœ¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.10945', 'title': 'MeViS: A Multi-Modal Dataset for Referring Motion Expression Video Segmentation', 'url': 'https://huggingface.co/papers/2512.10945', 'abstract': "A dataset for referring motion expression video segmentation, MeViS, is introduced to explore motion expression-guided video understanding and benchmarking.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper proposes a large-scale multi-modal dataset for referring motion expression video segmentation, focusing on segmenting and tracking target objects in videos based on language description of objects' motions. Existing referring video segmentation datasets often focus on salient objects and use language expressions rich in static attributes, potentially allowing the target object to be identified in a single frame. Such datasets underemphasize the role of motion in both videos and languages. To explore the feasibility of using motion expressions and motion reasoning clues for pixel-level video understanding, we introduce MeViS, a dataset containing 33,072 human-annotated motion expressions in both text and audio, covering 8,171 objects in 2,006 videos of complex scenarios. We benchmark 15 existing methods across 4 tasks supported by MeViS, including 6 referring video object segmentation (RVOS) methods, 3 audio-guided video object segmentation (AVOS) methods, 2 referring multi-object tracking (RMOT) methods, and 4 video captioning methods for the newly introduced referring motion expression generation (RMEG) task. The results demonstrate weaknesses and limitations of existing methods in addressing motion expression-guided video understanding. We further analyze the challenges and propose an approach LMPM++ for RVOS/AVOS/RMOT that achieves new state-of-the-art results. Our dataset provides a platform that facilitates the development of motion expression-guided video understanding algorithms in complex video scenes. The proposed MeViS dataset and the method's source code are publicly available at https://henghuiding.com/MeViS/", 'score': 0, 'issue_id': 99, 'pub_date': '2025-12-11', 'pub_date_card': {'ru': '11 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 11', 'zh': '12æœˆ11æ—¥'}, 'hash': '512d0a300c0d6a23', 'authors': ['Henghui Ding', 'Chang Liu', 'Shuting He', 'Kaining Ying', 'Xudong Jiang', 'Chen Change Loy', 'Yu-Gang Jiang'], 'affiliations': ['Fudan University', 'Nanyang Technological University', 'Shanghai University of Finance and Economics'], 'pdf_title_img': 'assets/pdf/title_img/2512.10945.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#dataset', '#video'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ”Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğº ĞºĞ»ÑÑ‡ Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾: Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ MeViS Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 33,072 Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… 8,171 Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² 2,006 Ğ²Ğ¸Ğ´ĞµĞ¾Ñ€Ğ¾Ğ»Ğ¸ĞºĞ°Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸Ğ½Ğ³ 15 ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ğ¾ 4 Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼: ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ, ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ Ğ°ÑƒĞ´Ğ¸Ğ¾, Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ LMPM++, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³ÑˆĞ¸Ğ¹ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Unlocking Video Understanding with Motion Expressions', 'desc': 'This paper introduces the MeViS dataset, which focuses on video segmentation guided by motion expressions. Unlike existing datasets that emphasize static attributes, MeViS highlights the importance of motion in understanding and tracking objects in videos. It includes over 33,000 annotated motion expressions across various scenarios, enabling researchers to benchmark multiple video segmentation methods. The study reveals limitations in current techniques and proposes a new approach, LMPM++, which sets new performance standards in motion expression-guided video understanding.'}, 'zh': {'title': 'è¿åŠ¨è¡¨è¾¾å¼•å¯¼çš„è§†é¢‘ç†è§£æ–°å¹³å°', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºMeViSçš„å¤§è§„æ¨¡å¤šæ¨¡æ€æ•°æ®é›†ï¼Œæ—¨åœ¨æ¢ç´¢åŸºäºè¿åŠ¨è¡¨è¾¾çš„å‚è€ƒè§†é¢‘åˆ†å‰²ã€‚è¯¥æ•°æ®é›†åŒ…å«33,072ä¸ªç»è¿‡äººå·¥æ ‡æ³¨çš„è¿åŠ¨è¡¨è¾¾ï¼Œæ¶µç›–8,171ä¸ªå¯¹è±¡å’Œ2,006ä¸ªå¤æ‚åœºæ™¯çš„è§†é¢‘ã€‚ä¸ç°æœ‰çš„æ•°æ®é›†ä¸åŒï¼ŒMeViSå¼ºè°ƒäº†è¿åŠ¨åœ¨è§†é¢‘å’Œè¯­è¨€ä¸­çš„é‡è¦æ€§ï¼Œæ”¯æŒå¤šç§è§†é¢‘ç†è§£ä»»åŠ¡ã€‚æˆ‘ä»¬æå‡ºçš„LMPM++æ–¹æ³•åœ¨å¤šä¸ªä»»åŠ¡ä¸Šå–å¾—äº†æ–°çš„æœ€å…ˆè¿›ç»“æœï¼Œæ¨åŠ¨äº†è¿åŠ¨è¡¨è¾¾å¼•å¯¼çš„è§†é¢‘ç†è§£ç®—æ³•çš„å‘å±•ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (4)', '#agents (2)', '#agi', '#alignment (3)', '#architecture (5)', '#audio (1)', '#benchmark (13)', '#cv (5)', '#data (1)', '#dataset (10)', '#diffusion (3)', '#ethics', '#games', '#graphs', '#hallucinations (1)', '#healthcare', '#inference (3)', '#interpretability (1)', '#leakage', '#long_context (5)', '#low_resource (1)', '#machine_translation', '#math', '#multilingual (1)', '#multimodal (14)', '#open_source (5)', '#optimization (5)', '#plp', '#rag', '#reasoning (5)', '#rl (4)', '#rlhf (4)', '#robotics (2)', '#science', '#security (1)', '#small_models', '#story_generation', '#survey (1)', '#synthetic (2)', '#training (11)', '#transfer_learning (1)', '#video (7)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-12-17 11:22',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-12-17 11:22')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-12-17 11:22')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    