{
    "date": {
        "ru": "25 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
        "en": "February 25",
        "zh": "2æœˆ25æ—¥"
    },
    "time_utc": "2026-02-25 05:57",
    "weekday": 2,
    "issue_id": 1213,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2602.21193",
            "title": "On Data Engineering for Scaling LLM Terminal Capabilities",
            "url": "https://huggingface.co/papers/2602.21193",
            "abstract": "Abstract Researchers developed a synthetic task generation pipeline and analyzed data strategies to improve terminal agent performance, creating a large-scale dataset and models that outperform larger counterparts on benchmark tests.  \t\t\t\t\tAI-generated summary Despite rapid recent progress in the terminal capabilities of large language models, the training data strategies behind state-of-the-art terminal agents remain largely undisclosed. We address this gap through a systematic study of data engineering practices for terminal agents, making two key contributions: (1) Terminal-Task-Gen, a lightweight synthetic task generation pipeline that supports seed-based and skill-based task construction, and (2) a comprehensive analysis of data and training strategies, including filtering, curriculum learning, long context training, and scaling behavior. Our pipeline yields Terminal-Corpus, a large-scale open-source dataset for terminal tasks. Using this dataset, we train Nemotron-Terminal, a family of models initialized from Qwen3(8B, 14B, 32B) that achieve substantial gains on Terminal-Bench 2.0: Nemotron-Terminal-8B improves from 2.5% to 13.0% Nemotron-Terminal-14B improves from 4.0% to 20.2%, and Nemotron-Terminal-32B improves from 3.4% to 27.4%, matching the performance of significantly larger models. To accelerate research in this domain, we open-source our model checkpoints and most of our synthetic datasets at https://huggingface.co/collections/nvidia/nemotron-terminal.",
            "score": 15,
            "issue_id": 1212,
            "pub_date": "2026-02-24",
            "pub_date_card": {
                "ru": "24 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 24",
                "zh": "2æœˆ24æ—¥"
            },
            "hash": "efca03aa1b046219",
            "authors": [
                "Renjie Pi",
                "Grace Lam",
                "Mohammad Shoeybi",
                "Pooya Jannaty",
                "Bryan Catanzaro",
                "Wei Ping"
            ],
            "affiliations": [
                "NVIDIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.21193.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#agents",
                    "#benchmark",
                    "#long_context",
                    "#training",
                    "#synthetic",
                    "#dataset",
                    "#data"
                ],
                "emoji": "âš™ï¸",
                "ru": {
                    "title": "Ğ˜ÑĞºÑƒÑÑÑ‚Ğ²Ğ¾ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…: ĞºĞ°Ğº Ğ¼Ğ°Ğ»Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒÑ‡Ğ°Ñ‚ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ°Ğ»Ğ¾Ğ¼",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Terminal-Task-Gen â€” Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ğ²Ğ¾Ğº Ğ¸ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ, curriculum learning Ğ¸ Ğ´Ğ¾Ğ»Ğ³Ğ¾ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ. Ğ¡Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Terminal-Corpus Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ» Ğ¾Ğ±ÑƒÑ‡Ğ¸Ñ‚ÑŒ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Nemotron-Terminal, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ Terminal-Bench 2.0, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ³Ğ¾Ñ€Ğ°Ğ·Ğ´Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ’ÑĞµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ Ñ‡Ğ°ÑÑ‚ÑŒ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ±Ñ‹Ğ»Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ°."
                },
                "en": {
                    "title": "Boosting Terminal Agents with Synthetic Task Generation",
                    "desc": "This paper presents a new method for generating synthetic tasks to enhance the performance of terminal agents in machine learning. The authors introduce Terminal-Task-Gen, a pipeline that allows for the creation of tasks based on specific skills and seeds. They also analyze various data strategies, such as filtering and curriculum learning, to optimize training processes. The resulting models, Nemotron-Terminal, demonstrate significant performance improvements on benchmark tests, even outperforming larger models, and the authors provide open access to their datasets and model checkpoints for further research."
                },
                "zh": {
                    "title": "æå‡ç»ˆç«¯ä»£ç†æ€§èƒ½çš„åˆæˆä»»åŠ¡ç”Ÿæˆä¸æ•°æ®ç­–ç•¥",
                    "desc": "ç ”ç©¶äººå‘˜å¼€å‘äº†ä¸€ç§åˆæˆä»»åŠ¡ç”Ÿæˆç®¡é“ï¼Œå¹¶åˆ†æäº†æ•°æ®ç­–ç•¥ä»¥æé«˜ç»ˆç«¯ä»£ç†çš„æ€§èƒ½ï¼Œåˆ›å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†å’Œæ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹åœ¨åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†æ›´å¤§çš„å¯¹æ‰‹ã€‚æˆ‘ä»¬æå‡ºäº†Terminal-Task-Genï¼Œè¿™æ˜¯ä¸€ç§è½»é‡çº§çš„åˆæˆä»»åŠ¡ç”Ÿæˆç®¡é“ï¼Œæ”¯æŒåŸºäºç§å­å’ŒæŠ€èƒ½çš„ä»»åŠ¡æ„å»ºã€‚é€šè¿‡æˆ‘ä»¬çš„ç®¡é“ç”Ÿæˆäº†Terminal-Corpusï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºç»ˆç«¯ä»»åŠ¡çš„å¤§è§„æ¨¡å¼€æºæ•°æ®é›†ã€‚ä½¿ç”¨è¯¥æ•°æ®é›†ï¼Œæˆ‘ä»¬è®­ç»ƒäº†Nemotron-Terminalç³»åˆ—æ¨¡å‹ï¼Œæ˜¾è‘—æé«˜äº†åœ¨Terminal-Bench 2.0ä¸Šçš„è¡¨ç°ï¼Œå±•ç¤ºäº†æ•°æ®å·¥ç¨‹å®è·µçš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.21015",
            "title": "From Perception to Action: An Interactive Benchmark for Vision Reasoning",
            "url": "https://huggingface.co/papers/2602.21015",
            "abstract": "Abstract Current vision-language models lack capability to understand physical structures and causal constraints needed for complex, interactive 3D tasks, as demonstrated by the CHAIN benchmark evaluating structured action planning under physical constraints.  \t\t\t\t\tAI-generated summary Understanding the physical structure is essential for real-world applications such as embodied agents, interactive design, and long-horizon manipulation. Yet, prevailing Vision-Language Model (VLM) evaluations still center on structure-agnostic, single-turn setups (e.g., VQA), which fail to assess agents' ability to reason about how geometry, contact, and support relations jointly constrain what actions are possible in a dynamic environment. To address this gap, we introduce the Causal Hierarchy of Actions and Interactions (CHAIN) benchmark, an interactive 3D, physics-driven testbed designed to evaluate whether models can understand, plan, and execute structured action sequences grounded in physical constraints. CHAIN shifts evaluation from passive perception to active problem solving, spanning tasks such as interlocking mechanical puzzles and 3D stacking and packing. We conduct a comprehensive study of state-of-the-art VLMs and diffusion-based models under unified interactive settings. Our results show that top-performing models still struggle to internalize physical structure and causal constraints, often failing to produce reliable long-horizon plans and cannot robustly translate perceived structure into effective actions. The project is available at https://social-ai-studio.github.io/CHAIN/.",
            "score": 12,
            "issue_id": 1212,
            "pub_date": "2026-02-24",
            "pub_date_card": {
                "ru": "24 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 24",
                "zh": "2æœˆ24æ—¥"
            },
            "hash": "28d09e54597b0fc5",
            "authors": [
                "Yuhao Wu",
                "Maojia Song",
                "Yihuai Lan",
                "Lei Wang",
                "Zhiqiang Hu",
                "Yao Xiao",
                "Heng Zhou",
                "Weihua Zheng",
                "Dylan Raharja",
                "Soujanya Poria",
                "Roy Ka-Wei Lee"
            ],
            "affiliations": [
                "Nanyang Technological University (NTU), Singapore",
                "Singapore Management University (SMU), Singapore",
                "Singapore University of Technology and Design (SUTD), Singapore",
                "University of Science and Technology of China (USTC), China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.21015.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#multimodal",
                    "#robotics",
                    "#benchmark",
                    "#3d",
                    "#cv"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "Ğ¤Ğ¸Ğ·Ğ¸ĞºĞ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹: Ğ¾Ñ‚ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğº Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº CHAIN Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ° Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… ÑƒÑ‡Ñ‘Ñ‚Ğ° Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ĞºĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¾Ñ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğ»Ğ¾Ğ¼Ğ¾Ğº Ğ´Ğ¾ ÑƒĞºĞ»Ğ°Ğ´ĞºĞ¸ Ğ¸ ÑƒĞ¿Ğ°ĞºĞ¾Ğ²ĞºĞ¸ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼, Ğ° Ğ½Ğµ Ğ¿Ğ°ÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ VLM-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ Ğ½Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸Ğ½ÑÑ‚Ñ‹Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ."
                },
                "en": {
                    "title": "Empowering AI with Physical Understanding for 3D Interaction",
                    "desc": "This paper addresses the limitations of current vision-language models (VLMs) in understanding physical structures and causal relationships necessary for complex 3D tasks. It introduces the Causal Hierarchy of Actions and Interactions (CHAIN) benchmark, which evaluates models on their ability to plan and execute actions based on physical constraints in an interactive 3D environment. The study reveals that even state-of-the-art models struggle with long-horizon planning and translating perceived structures into effective actions. By shifting the focus from passive perception to active problem-solving, this benchmark aims to enhance the evaluation of AI in real-world applications."
                },
                "zh": {
                    "title": "ç†è§£ç‰©ç†ç»“æ„ï¼Œæå‡äº¤äº’èƒ½åŠ›",
                    "desc": "å½“å‰çš„è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨ç†è§£ç‰©ç†ç»“æ„å’Œå› æœçº¦æŸæ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œè¿™å¯¹äºå¤æ‚çš„äº¤äº’å¼3Dä»»åŠ¡è‡³å…³é‡è¦ã€‚æˆ‘ä»¬æå‡ºäº†å› æœè¡ŒåŠ¨ä¸äº¤äº’å±‚æ¬¡ï¼ˆCHAINï¼‰åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹åœ¨ç‰©ç†çº¦æŸä¸‹ç†è§£ã€è§„åˆ’å’Œæ‰§è¡Œç»“æ„åŒ–è¡ŒåŠ¨åºåˆ—çš„èƒ½åŠ›ã€‚CHAINåŸºå‡†å°†è¯„ä¼°ä»è¢«åŠ¨æ„ŸçŸ¥è½¬å‘ä¸»åŠ¨é—®é¢˜è§£å†³ï¼Œæ¶µç›–æœºæ¢°æ‹¼å›¾å’Œ3Då †å ç­‰ä»»åŠ¡ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡ä¸€äº›é¡¶å°–æ¨¡å‹è¡¨ç°è‰¯å¥½ï¼Œä½†ä»ç„¶éš¾ä»¥å†…åŒ–ç‰©ç†ç»“æ„å’Œå› æœçº¦æŸï¼Œæ— æ³•å¯é åœ°ç”Ÿæˆé•¿æ—¶é—´çš„è®¡åˆ’ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.20739",
            "title": "PyVision-RL: Forging Open Agentic Vision Models via RL",
            "url": "https://huggingface.co/papers/2602.20739",
            "abstract": "Abstract PyVision-RL framework addresses interaction collapse in multimodal models through enhanced reinforcement learning techniques and efficient video processing strategies.  \t\t\t\t\tAI-generated summary Reinforcement learning for agentic multimodal models often suffers from interaction collapse, where models learn to reduce tool usage and multi-turn reasoning, limiting the benefits of agentic behavior. We introduce PyVision-RL, a reinforcement learning framework for open-weight multimodal models that stabilizes training and sustains interaction. Our approach combines an oversampling-filtering-ranking rollout strategy with an accumulative tool reward to prevent collapse and encourage multi-turn tool use. Using a unified training pipeline, we develop PyVision-Image and PyVision-Video for image and video understanding. For video reasoning, PyVision-Video employs on-demand context construction, selectively sampling task-relevant frames during reasoning to significantly reduce visual token usage. Experiments show strong performance and improved efficiency, demonstrating that sustained interaction and on-demand visual processing are critical for scalable multimodal agents.",
            "score": 11,
            "issue_id": 1212,
            "pub_date": "2026-02-24",
            "pub_date_card": {
                "ru": "24 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 24",
                "zh": "2æœˆ24æ—¥"
            },
            "hash": "4f9be2e1a2a14149",
            "authors": [
                "Shitian Zhao",
                "Shaoheng Lin",
                "Ming Li",
                "Haoquan Zhang",
                "Wenshuo Peng",
                "Kaipeng Zhang",
                "Chen Wei"
            ],
            "affiliations": [
                "CUHK",
                "Rice University",
                "Shanda AI Research, Tokyo",
                "Shanghai AI Lab",
                "THU",
                "UMD"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.20739.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#agents",
                    "#multimodal",
                    "#rl",
                    "#reasoning",
                    "#video",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ PyVision-RL â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºĞ¾Ğ»Ğ»Ğ°Ğ¿ÑĞ° Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ, ĞºĞ¾Ğ³Ğ´Ğ° Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ¿ĞµÑ€ĞµÑÑ‚Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ-Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸-Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ·Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ”Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ·Ğ° ÑÑ‡Ñ‘Ñ‚ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğµ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing Multimodal Interaction with PyVision-RL",
                    "desc": "The PyVision-RL framework tackles the problem of interaction collapse in multimodal models by utilizing advanced reinforcement learning techniques. This issue arises when models minimize tool usage and multi-turn reasoning, which hinders their ability to act effectively. PyVision-RL introduces a novel strategy that combines oversampling, filtering, and ranking to stabilize training and promote sustained interaction. Additionally, it features a unique approach for video processing that selectively samples relevant frames, enhancing efficiency and performance in multimodal tasks."
                },
                "zh": {
                    "title": "æŒç»­äº¤äº’ï¼Œæå‡å¤šæ¨¡æ€æ™ºèƒ½ä½“çš„èƒ½åŠ›",
                    "desc": "PyVision-RLæ¡†æ¶é€šè¿‡å¢å¼ºçš„å¼ºåŒ–å­¦ä¹ æŠ€æœ¯å’Œé«˜æ•ˆçš„è§†é¢‘å¤„ç†ç­–ç•¥ï¼Œè§£å†³äº†å¤šæ¨¡æ€æ¨¡å‹ä¸­çš„äº¤äº’å´©æºƒé—®é¢˜ã€‚è¯¥æ¡†æ¶æ—¨åœ¨ç¨³å®šè®­ç»ƒè¿‡ç¨‹ï¼Œå¹¶ä¿æŒæ¨¡å‹çš„äº¤äº’èƒ½åŠ›ï¼Œé¿å…å·¥å…·ä½¿ç”¨å‡å°‘å’Œå¤šè½®æ¨ç†çš„é™åˆ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†è¿‡é‡‡æ ·-è¿‡æ»¤-æ’åºçš„å›æ”¾ç­–ç•¥å’Œç´¯ç§¯å·¥å…·å¥–åŠ±ï¼Œä»¥é¼“åŠ±å¤šè½®å·¥å…·ä½¿ç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒæŒç»­çš„äº¤äº’å’ŒæŒ‰éœ€è§†è§‰å¤„ç†å¯¹äºå¯æ‰©å±•çš„å¤šæ¨¡æ€æ™ºèƒ½ä½“è‡³å…³é‡è¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.14337",
            "title": "LongCLI-Bench: A Preliminary Benchmark and Study for Long-horizon Agentic Programming in Command-Line Interfaces",
            "url": "https://huggingface.co/papers/2602.14337",
            "abstract": "Abstract LongCLI-Bench evaluates AI agents' ability to complete complex, multi-step programming tasks through command-line interfaces with detailed failure analysis and human-agent collaboration insights.  \t\t\t\t\tAI-generated summary Recent advances in AI-assisted programming have empowered agents to execute complex workflows via command-line interfaces, however, existing benchmarks are limited by short task horizons, data contamination from GitHub scraping, and a lack of fine-grained evaluation metrics, fail to rigorously evaluate the long-horizon planning and execution capabilities essential for realistic software engineering. To address these gaps, we introduce LongCLI-Bench, a comprehensive benchmark designed to evaluate agentic capabilities across long-horizon, realistic tasks. We curated 20 high-quality, long-horizon tasks from over 1,000 computer science assignments and real-world workflows, covering four engineering categories: from scratch, feature addition, bug fixing, and refactoring. We propose a dual-set testing protocol for LongCLI-Bench, which measures requirement fulfillment (fail-to-pass) and regression avoidance (pass-to-pass), and incorporates step-level scoring to pinpoint execution failures. Extensive experiments reveal that even state-of-the-art agents achieve pass rates below 20% in LongCLI-Bench. Step-level analysis further indicates that the majority of tasks stall at less than 30% completion, highlighting that critical failures often occur in the early stages. Although self-correction offers marginal gains, human-agent collaboration through plan injection and interactive guidance yields significantly higher improvements. These results highlight that future research must emphasize the development of synergistic human-agent workflows alongside advances in agents' planning and execution capabilities to overcome key challenges in long-horizon task performance.",
            "score": 7,
            "issue_id": 1212,
            "pub_date": "2026-02-15",
            "pub_date_card": {
                "ru": "15 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 15",
                "zh": "2æœˆ15æ—¥"
            },
            "hash": "62e389ea8e6d8823",
            "authors": [
                "Yukang Feng",
                "Jianwen Sun",
                "Zelai Yang",
                "Jiaxin Ai",
                "Chuanhao Li",
                "Zizhen Li",
                "Fanrui Zhang",
                "Kang He",
                "Rui Ma",
                "Jifan Lin",
                "Jie Sun",
                "Yang Xiao",
                "Sizhuo Zhou",
                "Wenxiao Wu",
                "Yiming Liu",
                "Pengfei Liu",
                "Yu Qiao",
                "Shenglin Zhang",
                "Kaipeng Zhang"
            ],
            "affiliations": [
                "NKU",
                "SII",
                "SJTU",
                "Shanda AI Research Tokyo",
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.14337.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#plp",
                    "#long_context",
                    "#benchmark"
                ],
                "emoji": "ğŸ› ï¸",
                "ru": {
                    "title": "Ğ­Ñ‚Ğ°Ğ»Ğ¾Ğ½ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğº Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ LongCLI-Bench â€” ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ¾ĞºÑƒ. ĞĞ°Ğ±Ğ¾Ñ€ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 20 Ğ²Ñ‹ÑĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ñ Ğ½ÑƒĞ»Ñ, Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹, Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¸ Ñ€ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ ĞºĞ¾Ğ´Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²ÑƒÑ…ÑĞµÑ‚Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¾Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµÑ‚ ĞºĞ°Ğº Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ¼ĞµĞ½ĞµĞµ 20% ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸, Ğ¾Ğ´Ğ½Ğ°ĞºĞ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹."
                },
                "en": {
                    "title": "Enhancing AI Agents with LongCLI-Bench: A New Era in Programming Tasks",
                    "desc": "The paper introduces LongCLI-Bench, a new benchmark for evaluating AI agents on complex programming tasks using command-line interfaces. It addresses limitations of existing benchmarks by focusing on long-horizon tasks and providing detailed metrics for evaluating agent performance. The study reveals that even advanced AI agents struggle with these tasks, achieving less than 20% success rates, particularly stalling in early execution stages. The findings suggest that enhancing human-agent collaboration can significantly improve task completion rates, indicating a need for future research to focus on synergistic workflows between humans and AI."
                },
                "zh": {
                    "title": "é•¿æ—¶é—´ä»»åŠ¡æ€§èƒ½çš„è¯„ä¼°ä¸äººæœºåä½œçš„é‡è¦æ€§",
                    "desc": "LongCLI-Bench æ˜¯ä¸€ä¸ªè¯„ä¼° AI ä»£ç†åœ¨å‘½ä»¤è¡Œç•Œé¢ä¸­å®Œæˆå¤æ‚å¤šæ­¥éª¤ç¼–ç¨‹ä»»åŠ¡èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚è¯¥ç ”ç©¶æŒ‡å‡ºï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•åœ¨é•¿æ—¶é—´ä»»åŠ¡è§„åˆ’å’Œæ‰§è¡Œèƒ½åŠ›çš„è¯„ä¼°ä¸Šå­˜åœ¨ä¸è¶³ã€‚é€šè¿‡ä» 1000 å¤šä¸ªè®¡ç®—æœºç§‘å­¦ä½œä¸šä¸­æŒ‘é€‰å‡º 20 ä¸ªé«˜è´¨é‡çš„é•¿æ—¶é—´ä»»åŠ¡ï¼ŒLongCLI-Bench æä¾›äº†æ›´ç»†è‡´çš„è¯„ä¼°æŒ‡æ ‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„ä»£ç†åœ¨ LongCLI-Bench ä¸­çš„é€šè¿‡ç‡ä¹Ÿä½äº 20%ï¼Œå¼ºè°ƒäº†äººæœºåä½œåœ¨æå‡ä»»åŠ¡å®Œæˆç‡æ–¹é¢çš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.16990",
            "title": "Conv-FinRe: A Conversational and Longitudinal Benchmark for Utility-Grounded Financial Recommendation",
            "url": "https://huggingface.co/papers/2602.16990",
            "abstract": "Abstract A new conversational financial recommendation benchmark evaluates large language models' ability to balance rational decision-making with user behavior alignment using multi-view references derived from real market data and human decision trajectories.  \t\t\t\t\tAI-generated summary Most recommendation benchmarks evaluate how well a model imitates user behavior. In financial advisory, however, observed actions can be noisy or short-sighted under market volatility and may conflict with a user's long-term goals. Treating what users chose as the sole ground truth, therefore, conflates behavioral imitation with decision quality. We introduce Conv-FinRe, a conversational and longitudinal benchmark for stock recommendation that evaluates LLMs beyond behavior matching. Given an onboarding interview, step-wise market context, and advisory dialogues, models must generate rankings over a fixed investment horizon. Crucially, Conv-FinRe provides multi-view references that distinguish descriptive behavior from normative utility grounded in investor-specific risk preferences, enabling diagnosis of whether an LLM follows rational analysis, mimics user noise, or is driven by market momentum. We build the benchmark from real market data and human decision trajectories, instantiate controlled advisory conversations, and evaluate a suite of state-of-the-art LLMs. Results reveal a persistent tension between rational decision quality and behavioral alignment: models that perform well on utility-based ranking often fail to match user choices, whereas behaviorally aligned models can overfit short-term noise. The dataset is publicly released on Hugging Face, and the codebase is available on GitHub.",
            "score": 3,
            "issue_id": 1212,
            "pub_date": "2026-02-19",
            "pub_date_card": {
                "ru": "19 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 19",
                "zh": "2æœˆ19æ—¥"
            },
            "hash": "f9665a7ad14a7cb3",
            "authors": [
                "Yan Wang",
                "Yi Han",
                "Lingfei Qian",
                "Yueru He",
                "Xueqing Peng",
                "Dongji Feng",
                "Zhuohan Xie",
                "Vincent Jim Zhang",
                "Rosie Guo",
                "Fengran Mo",
                "Jimin Huang",
                "Yankai Chen",
                "Xue Liu",
                "Jian-Yun Nie"
            ],
            "affiliations": [
                "MBZUAI",
                "McGill University",
                "The Fin AI USA",
                "University of Montreal"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.16990.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ“ˆ",
                "ru": {
                    "title": "Ğ‘Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸ÑÑ… LLM",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Conv-FinRe â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ°ĞºÑ†Ğ¸ÑĞ¼ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½ÑÑƒĞ»ÑŒÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ñ‹ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸ĞµĞ¼ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ½Ğ²ĞµÑÑ‚Ğ¾Ñ€Ğ° Ğ¸ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ ÑˆÑƒĞ¼Ğ½Ñ‹Ğ¼ Ğ¸Ğ»Ğ¸ Ğ±Ğ»Ğ¸Ğ·Ğ¾Ñ€ÑƒĞºĞ¸Ğ¼. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸-Ñ€ĞµÑ„ĞµÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ñ‹Ğ½ĞºĞ° Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°Ñ‚ÑŒ Ğ½Ğ°ÑÑ‚Ğ¾ÑÑ‰ĞµĞµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚Ğ¸. ĞÑ†ĞµĞ½ĞºĞ° ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ğµ: Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¸Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ñƒ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹, Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ, Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ Ğ½Ğ° ĞºÑ€Ğ°Ñ‚ĞºĞ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ñ€Ñ‹Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ğµ ĞºĞ¾Ğ»ĞµĞ±Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Balancing Rationality and User Behavior in Financial Recommendations",
                    "desc": "This paper introduces Conv-FinRe, a new benchmark for evaluating large language models (LLMs) in the context of financial recommendations. Unlike traditional benchmarks that focus solely on how well models mimic user behavior, Conv-FinRe assesses the models' ability to balance rational decision-making with user preferences over time. It incorporates multi-view references derived from real market data and human decision-making patterns, allowing for a nuanced evaluation of model performance. The findings highlight a trade-off between making rational investment recommendations and aligning with user behavior, revealing that models excelling in one area often struggle in the other."
                },
                "zh": {
                    "title": "ç†æ€§å†³ç­–ä¸ç”¨æˆ·è¡Œä¸ºçš„å¹³è¡¡æŒ‘æˆ˜",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å¯¹è¯å¼é‡‘èæ¨èåŸºå‡†Conv-FinReï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç†æ€§å†³ç­–ä¸ç”¨æˆ·è¡Œä¸ºä¸€è‡´æ€§ä¹‹é—´çš„å¹³è¡¡èƒ½åŠ›ã€‚è¯¥åŸºå‡†ä½¿ç”¨çœŸå®å¸‚åœºæ•°æ®å’Œäººç±»å†³ç­–è½¨è¿¹ç”Ÿæˆå¤šè§†è§’å‚è€ƒï¼Œå¸®åŠ©åŒºåˆ†æè¿°æ€§è¡Œä¸ºä¸åŸºäºæŠ•èµ„è€…ç‰¹å®šé£é™©åå¥½çš„è§„èŒƒæ•ˆç”¨ã€‚é€šè¿‡å¯¹æ¨¡å‹åœ¨å›ºå®šæŠ•èµ„æœŸé™å†…ç”Ÿæˆæ¨èæ’åçš„è¯„ä¼°ï¼Œç ”ç©¶å‘ç°ç†æ€§å†³ç­–è´¨é‡ä¸è¡Œä¸ºä¸€è‡´æ€§ä¹‹é—´å­˜åœ¨æŒç»­çš„ç´§å¼ å…³ç³»ã€‚æœ€ç»ˆï¼Œç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè™½ç„¶ä¸€äº›æ¨¡å‹åœ¨æ•ˆç”¨æ’åä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†å¾€å¾€æ— æ³•åŒ¹é…ç”¨æˆ·é€‰æ‹©ï¼Œè€Œè¡Œä¸ºä¸€è‡´æ€§å¼ºçš„æ¨¡å‹åˆ™å¯èƒ½è¿‡åº¦æ‹ŸåˆçŸ­æœŸå™ªå£°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.21198",
            "title": "Learning from Trials and Errors: Reflective Test-Time Planning for Embodied LLMs",
            "url": "https://huggingface.co/papers/2602.21198",
            "abstract": "Abstract Reflective Test-Time Planning enhances robot decision-making by integrating multiple reflection mechanisms that enable learning from experience and improving long-horizon task performance.  \t\t\t\t\tAI-generated summary Embodied LLMs endow robots with high-level task reasoning, but they cannot reflect on what went wrong or why, turning deployment into a sequence of independent trials where mistakes repeat rather than accumulate into experience. Drawing upon human reflective practitioners, we introduce Reflective Test-Time Planning, which integrates two modes of reflection: reflection-in-action, where the agent uses test-time scaling to generate and score multiple candidate actions using internal reflections before execution; and reflection-on-action, which uses test-time training to update both its internal reflection model and its action policy based on external reflections after execution. We also include retrospective reflection, allowing the agent to re-evaluate earlier decisions and perform model updates with hindsight for proper long-horizon credit assignment. Experiments on our newly-designed Long-Horizon Household benchmark and MuJoCo Cupboard Fitting benchmark show significant gains over baseline models, with ablative studies validating the complementary roles of reflection-in-action and reflection-on-action. Qualitative analyses, including real-robot trials, highlight behavioral correction through reflection.",
            "score": 2,
            "issue_id": 1212,
            "pub_date": "2026-02-24",
            "pub_date_card": {
                "ru": "24 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 24",
                "zh": "2æœˆ24æ—¥"
            },
            "hash": "6a95fb8f0218e443",
            "authors": [
                "Yining Hong",
                "Huang Huang",
                "Manling Li",
                "Li Fei-Fei",
                "Jiajun Wu",
                "Yejin Choi"
            ],
            "affiliations": [
                "Northwestern University",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.21198.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#robotics",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ° Ñ‡ĞµÑ€ĞµĞ· Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ñ: ĞºĞ°Ğº Ğ½Ğ°ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ¼Ğ°ÑˆĞ¸Ğ½Ñƒ ÑƒÑ‡Ğ¸Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ñ…",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Reflective Test-Time Planning, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ° Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ Ñ‚Ğ¸Ğ¿Ğ° Ñ€Ğ°Ğ·Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ: Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ, ĞºĞ¾Ğ³Ğ´Ğ° Ğ°Ğ³ĞµĞ½Ñ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ² Ğ¿ĞµÑ€ĞµĞ´ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼; Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ñ Ğ¿Ğ¾ÑĞ»Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºÑƒ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ğ¾Ğ²; Ğ¸ Ñ€ĞµÑ‚Ñ€Ğ¾ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ñ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾ÑˆĞ»Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ ÑƒÑ‡Ğ¸Ñ‚ÑŒÑÑ Ğ½Ğ° ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ñ… Ğ¸ Ğ½Ğ°ĞºĞ°Ğ¿Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ¿Ñ‹Ñ‚ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¾Ğ´Ğ½Ğ¸Ñ… Ğ¸ Ñ‚ĞµÑ… Ğ¶Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Long-Horizon Household Ğ¸ MuJoCo Cupboard Fitting Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²ĞºÑƒ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Robot Decision-Making Through Reflective Learning",
                    "desc": "This paper presents Reflective Test-Time Planning, a method that improves robot decision-making by incorporating reflection mechanisms. It combines two types of reflection: reflection-in-action, which allows robots to evaluate multiple actions in real-time, and reflection-on-action, which updates their strategies based on past experiences. Additionally, it introduces retrospective reflection for better long-term decision-making by reassessing earlier choices. Experiments demonstrate that this approach significantly enhances performance in complex tasks compared to traditional methods."
                },
                "zh": {
                    "title": "åæ€æå‡æœºå™¨äººå†³ç­–èƒ½åŠ›",
                    "desc": "åæ€æµ‹è¯•æ—¶è§„åˆ’ï¼ˆReflective Test-Time Planningï¼‰é€šè¿‡æ•´åˆå¤šç§åæ€æœºåˆ¶ï¼Œå¢å¼ºäº†æœºå™¨äººå†³ç­–èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿä»ç»éªŒä¸­å­¦ä¹ å¹¶æé«˜é•¿æ—¶é—´ä»»åŠ¡çš„è¡¨ç°ã€‚è¯¥æ–¹æ³•åŒ…æ‹¬ä¸¤ç§åæ€æ¨¡å¼ï¼šè¡ŒåŠ¨ä¸­çš„åæ€ï¼ˆreflection-in-actionï¼‰ï¼Œåœ¨æ‰§è¡Œå‰ç”Ÿæˆå’Œè¯„åˆ†å¤šä¸ªå€™é€‰åŠ¨ä½œï¼›ä»¥åŠè¡ŒåŠ¨åçš„åæ€ï¼ˆreflection-on-actionï¼‰ï¼Œåœ¨æ‰§è¡Œåæ›´æ–°å†…éƒ¨åæ€æ¨¡å‹å’Œè¡ŒåŠ¨ç­–ç•¥ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†å›é¡¾æ€§åæ€ï¼Œå…è®¸ä»£ç†é‡æ–°è¯„ä¼°æ—©æœŸå†³ç­–å¹¶è¿›è¡Œæ¨¡å‹æ›´æ–°ï¼Œä»¥ä¾¿æ›´å¥½åœ°åˆ†é…é•¿æœŸä¿¡ç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ–°è®¾è®¡çš„é•¿æ—¶é—´å®¶åº­åŸºå‡†å’ŒMuJoCoæ©±æŸœæ‹ŸåˆåŸºå‡†ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.20945",
            "title": "The Art of Efficient Reasoning: Data, Reward, and Optimization",
            "url": "https://huggingface.co/papers/2602.20945",
            "abstract": "Abstract Large language models benefit from scaled chain-of-thought reasoning through efficient training methods that balance trajectory length and accuracy using reinforcement learning with reward shaping.  \t\t\t\t\tAI-generated summary Large Language Models (LLMs) consistently benefit from scaled Chain-of-Thought (CoT) reasoning, but also suffer from heavy computational overhead. To address this issue, efficient reasoning aims to incentivize short yet accurate thinking trajectories, typically through reward shaping with Reinforcement Learning (RL). In this paper, we systematically investigate the mechanics of efficient reasoning for LLMs. For comprehensive evaluation, we advocate for more fine-grained metrics, including length distribution conditioned on correctness and performance across a wide spectrum of token budgets ranging from 2k to 32k. First, we reveal that the training process follows a two-stage paradigm: length adaptation and reasoning refinement. After that, we conduct extensive experiments (about 0.2 million GPU hours) in a unified protocol, deconstructing training prompts and rollouts, reward shaping, and optimization strategies. In particular, a key finding is to train on relatively easier prompts, ensuring the density of positive reward signals and thus avoiding the length collapse. Meanwhile, the learned length bias can be generalized across domains. We distill all findings into valuable insights and practical guidelines, and further validate them across the Qwen3 series, ranging from 0.6B to 30B, demonstrating the robustness and generalization.",
            "score": 1,
            "issue_id": 1212,
            "pub_date": "2026-02-24",
            "pub_date_card": {
                "ru": "24 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 24",
                "zh": "2æœˆ24æ—¥"
            },
            "hash": "677987c12bb77e28",
            "authors": [
                "Taiqiang Wu",
                "Zenan Zu",
                "Bo Zhou",
                "Ngai Wong"
            ],
            "affiliations": [
                "Tencent",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.20945.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM Ñ‡ĞµÑ€ĞµĞ· RL",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ÑÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (Chain-of-Thought) Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑĞ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ğ¹ Ğ¸Ğ· Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ… ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ»Ğ°Ğ¿Ñ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‚ÑÑ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ² (Ğ¾Ñ‚ 0.6B Ğ´Ğ¾ 30B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²), Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ€Ğ¾Ğ±ustĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Efficient Reasoning for Smarter Language Models",
                    "desc": "This paper explores how large language models (LLMs) can improve their reasoning abilities while reducing computational costs. It introduces efficient reasoning techniques that use reinforcement learning with reward shaping to encourage shorter, more accurate thought processes. The authors conduct extensive experiments to analyze the training dynamics, revealing a two-stage process of length adaptation followed by reasoning refinement. Their findings provide practical guidelines for training LLMs effectively, ensuring that they maintain a balance between prompt difficulty and reward density to enhance performance across various tasks."
                },
                "zh": {
                    "title": "é«˜æ•ˆæ¨ç†ï¼šä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹çš„æ€ç»´è½¨è¿¹",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é«˜æ•ˆæ¨ç†ä¸­çš„æœºåˆ¶ï¼Œç‰¹åˆ«æ˜¯å¦‚ä½•é€šè¿‡å¼ºåŒ–å­¦ä¹ å’Œå¥–åŠ±å¡‘é€ æ¥å¹³è¡¡æ¨ç†é•¿åº¦å’Œå‡†ç¡®æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè®­ç»ƒè¿‡ç¨‹åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé•¿åº¦é€‚åº”å’Œæ¨ç†ç²¾ç‚¼ã€‚ä½œè€…è¿›è¡Œäº†å¤§é‡å®éªŒï¼Œå‘ç°è®­ç»ƒç›¸å¯¹ç®€å•çš„æç¤ºå¯ä»¥æé«˜æ­£å¥–åŠ±ä¿¡å·çš„å¯†åº¦ï¼Œä»è€Œé¿å…æ¨ç†é•¿åº¦çš„å´©æºƒã€‚æœ€ç»ˆï¼Œç ”ç©¶ç»“æœä¸ºå¤§è¯­è¨€æ¨¡å‹çš„é«˜æ•ˆæ¨ç†æä¾›äº†å®ç”¨çš„æŒ‡å¯¼å’Œè§è§£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.16603",
            "title": "FlowPrefill: Decoupling Preemption from Prefill Scheduling Granularity to Mitigate Head-of-Line Blocking in LLM Serving",
            "url": "https://huggingface.co/papers/2602.16603",
            "abstract": "Abstract FlowPrefill addresses head-of-line blocking in large language model serving by decoupling preemption granularity from scheduling frequency through operator-level preemption and event-driven scheduling, achieving up to 5.6x better goodput than existing systems.  \t\t\t\t\tAI-generated summary The growing demand for large language models (LLMs) requires serving systems to handle many concurrent requests with diverse service level objectives (SLOs). This exacerbates head-of-line (HoL) blocking during the compute-intensive prefill phase, where long-running requests monopolize resources and delay higher-priority ones, leading to widespread time-to-first-token (TTFT) SLO violations. While chunked prefill enables interruptibility, it introduces an inherent trade-off between responsiveness and throughput: reducing chunk size improves response latency but degrades computational efficiency, whereas increasing chunk size maximizes throughput but exacerbates blocking. This necessitates an adaptive preemption mechanism. However, dynamically balancing execution granularity against scheduling overheads remains a key challenge.   In this paper, we propose FlowPrefill, a TTFT-goodput-optimized serving system that resolves this conflict by decoupling preemption granularity from scheduling frequency. To achieve adaptive prefill scheduling, FlowPrefill introduces two key innovations: 1) Operator-Level Preemption, which leverages operator boundaries to enable fine-grained execution interruption without the efficiency loss associated with fixed small chunking; and 2) Event-Driven Scheduling, which triggers scheduling decisions only upon request arrival or completion events, thereby supporting efficient preemption responsiveness while minimizing control-plane overhead. Evaluation on real-world production traces shows that FlowPrefill improves maximum goodput by up to 5.6times compared to state-of-the-art systems while satisfying heterogeneous SLOs.",
            "score": 1,
            "issue_id": 1212,
            "pub_date": "2026-02-18",
            "pub_date_card": {
                "ru": "18 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 18",
                "zh": "2æœˆ18æ—¥"
            },
            "hash": "0813b8a077b15c47",
            "authors": [
                "Chia-chi Hsieh",
                "Zan Zong",
                "Xinyang Chen",
                "Jianjiang Li",
                "Jidong Zhai",
                "Lijie Wen"
            ],
            "affiliations": [
                "Tsinghua University",
                "University of Science and Technology Beijing"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.16603.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#inference"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ Ğ°ÑÑ†ĞµĞ¿Ğ»ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ğ½ÑƒĞ»ÑÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ñ‹ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ â€” Ğ¿Ğ¾Ğ±ĞµĞ´Ğ° Ğ½Ğ°Ğ´ Ğ¾Ñ‡ĞµÑ€ĞµĞ´Ğ½Ğ¾Ğ¹ Ğ±Ğ»Ğ¾ĞºĞ¸Ñ€Ğ¾Ğ²ĞºĞ¾Ğ¹",
                    "desc": "FlowPrefill â€” ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ÑĞ»ÑƒĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ±Ğ»Ğ¾ĞºĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¾Ñ‡ĞµÑ€ĞµĞ´Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¿Ñ€ĞµÑ„Ğ¸Ğ»Ğ»Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸ Ğ¸ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑĞ¿Ğ¸ÑĞ°Ğ½Ğ¸ĞµĞ¼, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ³Ğ¸Ğ±ĞºĞ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ñ€ĞµĞ¼ĞµĞ½ĞµĞ¼ Ğ¾Ñ‚ĞºĞ»Ğ¸ĞºĞ° Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½Ğ¾Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒÑ. Ğ”ĞµĞºĞ¾ÑƒĞ¿Ğ»Ğ¸Ğ½Ğ³ Ğ³Ñ€Ğ°Ğ½ÑƒĞ»ÑÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ñ‚ĞµÑĞ½ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ñ‹ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ Ğ±Ğ»Ğ¾ĞºĞ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½Ğ¾Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² 5.6 Ñ€Ğ°Ğ·Ğ° Ğ²Ñ‹ÑˆĞµ, Ñ‡ĞµĞ¼ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑƒĞ´Ğ¾Ğ²Ğ»ĞµÑ‚Ğ²Ğ¾Ñ€ÑÑ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğº ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¾Ğ±ÑĞ»ÑƒĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Decoupling Preemption for Enhanced Language Model Serving",
                    "desc": "FlowPrefill is a novel serving system designed to improve the efficiency of large language models by addressing head-of-line blocking during the prefill phase. It introduces operator-level preemption, allowing for fine-grained interruption of long-running requests without sacrificing computational efficiency. Additionally, it employs event-driven scheduling to make scheduling decisions based on request events, which enhances responsiveness while reducing overhead. The system demonstrates a significant increase in goodput, achieving up to 5.6 times better performance than existing solutions while meeting diverse service level objectives."
                },
                "zh": {
                    "title": "FlowPrefillï¼šä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹æœåŠ¡çš„åˆ›æ–°æ–¹æ¡ˆ",
                    "desc": "FlowPrefill æ˜¯ä¸€ç§ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹æœåŠ¡çš„ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³å¤´éƒ¨é˜»å¡é—®é¢˜ã€‚å®ƒé€šè¿‡æ“ä½œçº§åˆ«çš„æŠ¢å å’Œäº‹ä»¶é©±åŠ¨è°ƒåº¦ï¼Œå°†æŠ¢å ç²’åº¦ä¸è°ƒåº¦é¢‘ç‡è§£è€¦ï¼Œä»è€Œæé«˜äº†ç³»ç»Ÿçš„ååé‡ã€‚è¯¥ç³»ç»Ÿåœ¨å¤„ç†å¹¶å‘è¯·æ±‚æ—¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¹³è¡¡å“åº”æ—¶é—´å’Œè®¡ç®—æ•ˆç‡ï¼Œé¿å…äº†é•¿æ—¶é—´è¯·æ±‚å¯¹é«˜ä¼˜å…ˆçº§è¯·æ±‚çš„é˜»å¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFlowPrefill çš„æœ€å¤§ååé‡æ¯”ç°æœ‰ç³»ç»Ÿæé«˜äº† 5.6 å€ï¼ŒåŒæ—¶æ»¡è¶³äº†ä¸åŒçš„æœåŠ¡æ°´å¹³ç›®æ ‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.21204",
            "title": "Test-Time Training with KV Binding Is Secretly Linear Attention",
            "url": "https://huggingface.co/papers/2602.21204",
            "abstract": "Abstract Test-time training is reinterpreted as learned linear attention rather than memorization, offering architectural simplifications and improved efficiency.  \t\t\t\t\tAI-generated summary Test-time training (TTT) with KV binding as sequence modeling layer is commonly interpreted as a form of online meta-learning that memorizes a key-value mapping at test time. However, our analysis reveals multiple phenomena that contradict this memorization-based interpretation. Motivated by these findings, we revisit the formulation of TTT and show that a broad class of TTT architectures can be expressed as a form of learned linear attention operator. Beyond explaining previously puzzling model behaviors, this perspective yields multiple practical benefits: it enables principled architectural simplifications, admits fully parallel formulations that preserve performance while improving efficiency, and provides a systematic reduction of diverse TTT variants to a standard linear attention form. Overall, our results reframe TTT not as test-time memorization, but as learned linear attention with enhanced representational capacity.",
            "score": 0,
            "issue_id": 1213,
            "pub_date": "2026-02-24",
            "pub_date_card": {
                "ru": "24 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 24",
                "zh": "2æœˆ24æ—¥"
            },
            "hash": "2e3d59e73d47a15a",
            "authors": [
                "Junchen Liu",
                "Sven Elflein",
                "Or Litany",
                "Zan Gojcic",
                "Ruilong Li"
            ],
            "affiliations": [
                "NVIDIA",
                "Technion",
                "University of Toronto",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.21204.jpg",
            "data": {
                "categories": [],
                "emoji": "âš¡",
                "ru": {
                    "title": "Test-time Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğº Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿ĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»ÑĞµÑ‚ÑÑ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ test-time training (Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ) ĞºĞ°Ğº Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğµ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ, Ğ° Ğ½Ğµ ĞºĞ°Ğº Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¹ ĞºĞ»Ğ°ÑÑ TTT Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ learned linear attention, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑŠÑÑĞ½ÑĞµÑ‚ Ñ€Ğ°Ğ½ĞµĞµ Ğ½ĞµĞ¿Ğ¾Ğ½ÑÑ‚Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­Ñ‚Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ¿Ñ€Ğ¾ÑÑ‚Ğ¸Ñ‚ÑŒ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ, ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ TTT Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ñ‹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‚ test-time training ĞºĞ°Ğº Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ñ€ĞµĞ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Reframing Test-Time Training: From Memorization to Learned Linear Attention",
                    "desc": "This paper reinterprets test-time training (TTT) as a learned linear attention mechanism instead of a memorization process. The authors argue that TTT can be better understood through the lens of sequence modeling, which reveals inconsistencies in the traditional view of TTT as merely memorizing key-value pairs. By framing TTT in this way, the paper highlights architectural simplifications and efficiency improvements that can be achieved. Ultimately, this new perspective enhances the representational capacity of TTT architectures, making them more effective and easier to implement."
                },
                "zh": {
                    "title": "æµ‹è¯•æ—¶è®­ç»ƒï¼šå­¦ä¹ çš„çº¿æ€§æ³¨æ„åŠ›",
                    "desc": "æœ¬æ–‡é‡æ–°è§£é‡Šäº†æµ‹è¯•æ—¶è®­ç»ƒï¼ˆTTTï¼‰ï¼Œå°†å…¶è§†ä¸ºä¸€ç§å­¦ä¹ çš„çº¿æ€§æ³¨æ„åŠ›ï¼Œè€Œéç®€å•çš„è®°å¿†ã€‚æˆ‘ä»¬å‘ç°TTTçš„å¤šç§ç°è±¡ä¸è®°å¿†çš„è§£é‡Šç›¸çŸ›ç›¾ï¼Œå› æ­¤é‡æ–°å®¡è§†äº†TTTçš„å…¬å¼ã€‚é€šè¿‡è¿™ç§æ–°çš„è§†è§’ï¼Œæˆ‘ä»¬èƒ½å¤Ÿç®€åŒ–æ¶æ„ï¼Œæé«˜æ•ˆç‡ï¼Œå¹¶å°†å¤šç§TTTå˜ä½“å½’çº³ä¸ºæ ‡å‡†çš„çº¿æ€§æ³¨æ„åŠ›å½¢å¼ã€‚æ€»çš„æ¥è¯´ï¼Œç ”ç©¶ç»“æœè¡¨æ˜TTTæ˜¯ä¸€ç§å…·æœ‰å¢å¼ºè¡¨ç¤ºèƒ½åŠ›çš„å­¦ä¹ çº¿æ€§æ³¨æ„åŠ›ï¼Œè€Œéä»…ä»…æ˜¯æµ‹è¯•æ—¶çš„è®°å¿†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.21201",
            "title": "Aletheia tackles FirstProof autonomously",
            "url": "https://huggingface.co/papers/2602.21201",
            "abstract": "Abstract  We report the performance of Aletheia (Feng et al., 2026b), a mathematics research agent powered by Gemini 3 Deep Think, on the inaugural FirstProof challenge. Within the allowed timeframe of the challenge, Aletheia autonomously solved 6 problems (2, 5, 7, 8, 9, 10) out of 10 according to majority expert assessments; we note that experts were not unanimous on Problem 8 (only). For full transparency, we explain our interpretation of FirstProof and disclose details about our experiments as well as our evaluation. Raw prompts and outputs are available at https://github.com/google-deepmind/superhuman/tree/main/aletheia.",
            "score": 0,
            "issue_id": 1212,
            "pub_date": "2026-02-24",
            "pub_date_card": {
                "ru": "24 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 24",
                "zh": "2æœˆ24æ—¥"
            },
            "hash": "04a1706ab5b55298",
            "authors": [
                "Tony Feng",
                "Junehyuk Jung",
                "Sang-hyun Kim",
                "Carlo Pagano",
                "Sergei Gukov",
                "Chiang-Chiang Tsai",
                "David Woodruff",
                "Adel Javanmard",
                "Aryan Mokhtari",
                "Dawsen Hwang",
                "Yuri Chervonyi",
                "Jonathan N. Lee",
                "Garrett Bingham",
                "Trieu H. Trinh",
                "Vahab Mirrokni",
                "Quoc V. Le",
                "Thang Luong"
            ],
            "affiliations": [
                "Google DeepMind"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.21201.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ§®",
                "ru": {
                    "title": "ĞœĞ°ÑˆĞ¸Ğ½Ñ‹ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ñ‚ĞµĞ¾Ñ€ĞµĞ¼Ñ‹: Ğ°Ğ³ĞµĞ½Ñ‚ Aletheia Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Aletheia, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Gemini 3 Deep Think, Ğ½Ğ° ĞºĞ¾Ğ½ĞºÑƒÑ€ÑĞµ FirstProof. ĞĞ³ĞµĞ½Ñ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ Ñ€ĞµÑˆĞ¸Ğ» 6 Ğ¸Ğ· 10 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞµ Ğ² ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ ÑÑ€Ğ¾ĞºĞ¸, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ñƒ Ñ‚ĞµĞ¾Ñ€ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ»Ğ¸ Ğ¿Ğ¾Ğ»Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ² Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡."
                },
                "en": {
                    "title": "Aletheia: Autonomous Math Problem Solver Shines in FirstProof Challenge",
                    "desc": "The paper discusses Aletheia, a mathematics research agent that utilizes the Gemini 3 Deep Think model. During the FirstProof challenge, Aletheia successfully solved 6 out of 10 mathematical problems, demonstrating its capability in autonomous problem-solving. The authors provide a detailed explanation of their methodology and evaluation process, ensuring transparency in their findings. Additionally, they share raw prompts and outputs to facilitate further research and understanding of Aletheia's performance."
                },
                "zh": {
                    "title": "Aletheiaï¼šæ•°å­¦ç ”ç©¶çš„æ™ºèƒ½ä»£ç†",
                    "desc": "æœ¬æ–‡æŠ¥å‘Šäº†Aletheiaï¼ˆFengç­‰ï¼Œ2026bï¼‰åœ¨é¦–å±ŠFirstProofæŒ‘æˆ˜èµ›ä¸­çš„è¡¨ç°ã€‚Aletheiaæ˜¯ä¸€ä¸ªç”±Gemini 3 Deep Thinké©±åŠ¨çš„æ•°å­¦ç ”ç©¶ä»£ç†ï¼Œåœ¨è§„å®šçš„æ—¶é—´å†…è‡ªä¸»è§£å†³äº†10ä¸ªé—®é¢˜ä¸­çš„6ä¸ªã€‚æ ¹æ®å¤šæ•°ä¸“å®¶çš„è¯„ä¼°ï¼Œåªæœ‰ç¬¬8ä¸ªé—®é¢˜çš„ä¸“å®¶æ„è§ä¸ä¸€è‡´ã€‚æˆ‘ä»¬æä¾›äº†å¯¹FirstProofçš„è§£é‡Šï¼Œå¹¶æŠ«éœ²äº†å®éªŒå’Œè¯„ä¼°çš„è¯¦ç»†ä¿¡æ¯ï¼ŒåŸå§‹æç¤ºå’Œè¾“å‡ºå¯åœ¨æŒ‡å®šé“¾æ¥ä¸­æ‰¾åˆ°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.21185",
            "title": "The Diffusion Duality, Chapter II: Î¨-Samplers and Efficient Curriculum",
            "url": "https://huggingface.co/papers/2602.21185",
            "abstract": "Abstract Discrete diffusion models with predictor-corrector samplers surpass traditional methods in generation quality and efficiency, challenging assumptions about masked diffusion's necessity in language modeling.  \t\t\t\t\tAI-generated summary Uniform-state discrete diffusion models excel at few-step generation and guidance due to their ability to self-correct, making them preferred over autoregressive or Masked diffusion models in these settings. However, their sampling quality plateaus with ancestral samplers as the number of steps increases. We introduce a family of Predictor-Corrector (PC) samplers for discrete diffusion that generalize prior methods and apply to arbitrary noise processes. When paired with uniform-state diffusion, our samplers outperform ancestral sampling on both language and image modeling, achieving lower generative perplexity at matched unigram entropy on OpenWebText and better FID/IS scores on CIFAR10. Crucially, unlike conventional samplers, our PC methods continue to improve with more sampling steps. Taken together, these findings call into question the assumption that Masked diffusion is the inevitable future of diffusion-based language modeling. Beyond sampling, we develop a memory-efficient curriculum for the Gaussian relaxation training phase, reducing training time by 25% and memory by 33% compared to Duo while maintaining comparable perplexity on OpenWebText and LM1B and strong downstream performance. We release code, checkpoints, and a video-tutorial on: https://s-sahoo.com/duo-ch2",
            "score": 0,
            "issue_id": 1213,
            "pub_date": "2026-02-24",
            "pub_date_card": {
                "ru": "24 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 24",
                "zh": "2æœˆ24æ—¥"
            },
            "hash": "4b44e83c2b923e24",
            "authors": [
                "Justin Deschenaux",
                "Caglar Gulcehre",
                "Subham Sekhar Sahoo"
            ],
            "affiliations": [
                "Cornell Tech, NY",
                "EPFL, Lausanne, Switzerland",
                "Microsoft AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.21185.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#optimization",
                    "#open_source"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Predictor-Corrector ÑÑĞ¼Ğ¿Ğ»ĞµÑ€Ñ‹ Ğ´Ğ»Ñ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Predictor-Corrector Ğ´Ğ»Ñ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ÑÑ Ğº Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ğ¼ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑˆÑƒĞ¼Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ PC-ÑÑĞ¼Ğ¿Ğ»ĞµÑ€Ğ°Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğµ Ğ¸ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ°Ğº Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¿ĞµÑ€Ğ¿Ğ»ĞµĞºÑĞ¸Ğ¸ Ğ¸ FID-Ğ¾Ñ†ĞµĞ½Ğ¾Ğº. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒÑÑ Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ‡Ğ¸ÑĞ»Ğ° ÑˆĞ°Ğ³Ğ¾Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ² Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ². Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ ĞºÑƒÑ€Ğ¸ĞºÑƒĞ»ÑƒĞ¼ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ğ¾Ğ¹ Ñ€ĞµĞ»Ğ°ĞºÑĞ°Ñ†Ğ¸ĞµĞ¹, ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ğ² Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° 25% Ğ¸ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ½Ğ° 33%."
                },
                "en": {
                    "title": "Revolutionizing Diffusion Models with Predictor-Corrector Samplers",
                    "desc": "This paper presents a new approach to discrete diffusion models using Predictor-Corrector (PC) samplers, which enhance the quality and efficiency of generative tasks in language and image modeling. Unlike traditional methods, these PC samplers improve performance with more sampling steps, challenging the reliance on masked diffusion techniques. The authors demonstrate that their models achieve lower generative perplexity and better FID/IS scores, indicating superior output quality. Additionally, they introduce a memory-efficient training curriculum that significantly reduces training time and memory usage while maintaining performance."
                },
                "zh": {
                    "title": "é¢„æµ‹-æ ¡æ­£é‡‡æ ·å™¨ï¼šæ‰©æ•£æ¨¡å‹çš„æ–°æœªæ¥",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„ç¦»æ•£æ‰©æ•£æ¨¡å‹ï¼Œä½¿ç”¨é¢„æµ‹-æ ¡æ­£é‡‡æ ·å™¨æ¥æé«˜ç”Ÿæˆè´¨é‡å’Œæ•ˆç‡ã€‚è¿™ç§æ–¹æ³•æŒ‘æˆ˜äº†ä¼ ç»Ÿçš„æ©è”½æ‰©æ•£åœ¨è¯­è¨€å»ºæ¨¡ä¸­çš„å¿…è¦æ€§ï¼Œå°¤å…¶åœ¨å°‘æ­¥ç”Ÿæˆå’Œå¼•å¯¼æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚ä¸ä¼ ç»Ÿçš„ç¥–å…ˆé‡‡æ ·å™¨ç›¸æ¯”ï¼Œé¢„æµ‹-æ ¡æ­£é‡‡æ ·å™¨åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­èƒ½å¤ŸæŒç»­æ”¹è¿›ï¼Œå°¤å…¶åœ¨å¤„ç†è¯­è¨€å’Œå›¾åƒå»ºæ¨¡æ—¶æ•ˆæœæ˜¾è‘—ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§å†…å­˜é«˜æ•ˆçš„è®­ç»ƒè¯¾ç¨‹ï¼Œæ˜¾è‘—å‡å°‘äº†è®­ç»ƒæ—¶é—´å’Œå†…å­˜æ¶ˆè€—ï¼ŒåŒæ—¶ä¿æŒäº†è‰¯å¥½çš„ç”Ÿæˆæ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.20424",
            "title": "Implicit Intelligence -- Evaluating Agents on What Users Don't Say",
            "url": "https://huggingface.co/papers/2602.20424",
            "abstract": "Abstract AI agents struggle to interpret implicitly specified real-world requests that require contextual reasoning beyond explicit instructions, as demonstrated by an evaluation framework using interactive YAML-defined worlds.  \t\t\t\t\tAI-generated summary Real-world requests to AI agents are fundamentally underspecified. Natural human communication relies on shared context and unstated constraints that speakers expect listeners to infer. Current agentic benchmarks test explicit instruction-following but fail to evaluate whether agents can reason about implicit requirements spanning accessibility needs, privacy boundaries, catastrophic risks, and contextual constraints. We present Implicit Intelligence, an evaluation framework testing whether AI agents can move beyond prompt-following to become genuine goal-fulfillers, paired with Agent-as-a-World (AaW), a harness where interactive worlds are defined in human-readable YAML files and simulated by language models. Our scenarios feature apparent simplicity in user requests, hidden complexity in correct solutions, and discoverability of constraints through environmental exploration. Evaluating 16 frontier and open-weight models across 205 scenarios, we find that even the best-performing model achieves only 48.3% scenario pass rate, revealing substantial room for improvement in bridging the gap between literal instruction-following and human-like contextual reasoning.",
            "score": 0,
            "issue_id": 1212,
            "pub_date": "2026-02-23",
            "pub_date_card": {
                "ru": "23 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 23",
                "zh": "2æœˆ23æ—¥"
            },
            "hash": "ae9b77b695d3f18a",
            "authors": [
                "Ved Sirdeshmukh",
                "Marc Wetter"
            ],
            "affiliations": [
                "Labelbox"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.20424.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#reasoning",
                    "#alignment",
                    "#benchmark"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "ĞÑ‚ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğº ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ: Ğ¾Ñ†ĞµĞ½ĞºĞ° ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° AI Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ AI Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµÑĞ²Ğ½Ğ¾ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ·Ğ° Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ°Ğ¼Ğ¸ ÑĞ²Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Implicit Intelligence Ğ¸ ÑÑ€ĞµĞ´Ñƒ Agent-as-a-World, Ğ³Ğ´Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¸Ñ€Ñ‹ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑÑ‚ÑÑ Ğ² YAML-Ñ„Ğ°Ğ¹Ğ»Ğ°Ñ… Ğ¸ ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‚ÑÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. Ğ—Ğ°Ğ´Ğ°Ñ‡Ğ¸ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚ÑŒÑ, Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ñ€Ğ¸ÑĞºĞ°Ğ¼Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ñ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ĞµĞ¼. ĞÑ†ĞµĞ½ĞºĞ° 16 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° 205 ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°, Ñ‡Ñ‚Ğ¾ Ğ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 48.3% ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸, Ğ²Ñ‹ÑĞ²Ğ»ÑÑ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ±ÑƒĞºĞ²Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼, Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ğ¼ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¼Ñƒ."
                },
                "en": {
                    "title": "Bridging the Gap: From Instruction-Following to Contextual Reasoning in AI",
                    "desc": "This paper introduces a new evaluation framework called Implicit Intelligence, which assesses AI agents' ability to understand and fulfill real-world requests that are not explicitly stated. It highlights the limitations of current benchmarks that focus solely on following direct instructions, neglecting the need for contextual reasoning. The framework uses interactive YAML-defined worlds to create scenarios where user requests appear simple but contain hidden complexities and constraints. The study reveals that even the best AI models struggle with these tasks, achieving only a 48.3% success rate, indicating a significant need for improvement in AI's contextual understanding capabilities."
                },
                "zh": {
                    "title": "è¶…è¶ŠæŒ‡ä»¤ï¼šå®ç°ç±»äººä¸Šä¸‹æ–‡æ¨ç†çš„AIä»£ç†",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†äººå·¥æ™ºèƒ½ä»£ç†åœ¨å¤„ç†éšå«è¯·æ±‚æ—¶çš„æŒ‘æˆ˜ï¼Œè¿™äº›è¯·æ±‚éœ€è¦è¶…è¶Šæ˜ç¡®æŒ‡ä»¤çš„ä¸Šä¸‹æ–‡æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†éšå«æ™ºèƒ½ï¼ˆImplicit Intelligenceï¼‰è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨æµ‹è¯•AIä»£ç†æ˜¯å¦èƒ½å¤Ÿè¶…è¶Šç®€å•çš„æŒ‡ä»¤éµå¾ªï¼Œæˆä¸ºçœŸæ­£çš„ç›®æ ‡å®ç°è€…ã€‚é€šè¿‡ä½¿ç”¨äººç±»å¯è¯»çš„YAMLæ–‡ä»¶å®šä¹‰çš„äº¤äº’å¼ä¸–ç•Œï¼Œæˆ‘ä»¬æ¨¡æ‹Ÿäº†ç”¨æˆ·è¯·æ±‚çš„è¡¨é¢ç®€å•æ€§å’Œè§£å†³æ–¹æ¡ˆçš„éšè—å¤æ‚æ€§ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯è¡¨ç°æœ€å¥½çš„æ¨¡å‹ï¼Œå…¶åœºæ™¯é€šè¿‡ç‡ä¹Ÿä»…ä¸º48.3%ï¼Œè¡¨æ˜åœ¨å®ç°ç±»äººä¸Šä¸‹æ–‡æ¨ç†æ–¹é¢ä»æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚"
                }
            }
        }
    ],
    "link_prev": "2026-02-24.html",
    "link_next": "2026-02-26.html",
    "link_month": "2026-02.html",
    "short_date_prev": {
        "ru": "24.02",
        "en": "02/24",
        "zh": "2æœˆ24æ—¥"
    },
    "short_date_next": {
        "ru": "26.02",
        "en": "02/26",
        "zh": "2æœˆ26æ—¥"
    },
    "categories": {
        "#dataset": 2,
        "#data": 1,
        "#benchmark": 6,
        "#agents": 6,
        "#cv": 1,
        "#rl": 2,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 1,
        "#inference": 1,
        "#3d": 1,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 3,
        "#robotics": 2,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 2,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}