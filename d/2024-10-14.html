
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF (19 ÑÑ‚Ğ°Ñ‚ĞµĞ¹)</title>
    <link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #0989eacf;
            --secondary-color: #fff;
            --background-color: #f5f5f5;
            --text-color: #333333;
            --header-color: #0989eacf;
            --body-color: #f5f5f5;
            --menu-color: #002370;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 2em;
            padding: 10px 0 20px 0;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.tags {
            color: #555;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);
            transition: background-color 0.2s ease;
            display: flex;
            flex-direction: column;
            position: relative;
        }
        .article-content {
            padding: 1.5em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
        }
        body.dark-theme>div>main>article {
            background-color: #444;
        }
        body.light-theme>div>main>article {
            background-color: #fff;
        }
        body.dark-theme>div>main>article:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
        }
        .pub-date {
            font-size: 0.9em;
            margin-bottom: 0.8em;
            font-weight: 300;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 1em;
            position: absolute;
            bottom: 10px;
            font-weight: 300;
            font-family: 'Roboto Slab';
        }
        .background-digit {
            position: absolute;
            bottom: -20px;
            right: -10px;
            font-size: 12em;
            font-weight: bold;
            color: rgba(0, 0, 0, 0.03);
            z-index: 0;
            line-height: 1;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 80px;
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        a:hover {
            color: #e73838;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            padding: 1em 0;
            margin-top: 2em;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: fixed;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
        }
        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
        }
        .update-info-container {
            flex: 1;
        }
        .sort-container {
            flex: 2;
        }
        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
                display: block;
                margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .category-toggle {
            display: inline-block;
            margin-bottom: 10px;
            margin-top: 15px;
            cursor: pointer;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 0px;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }
        
        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        body.light-theme>div>main>article.xec9d4127c5348f2d { background: url("https://hfday.ru/img/20241013/ec9d4127c5348f2d.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.xec9d4127c5348f2d:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.xec9d4127c5348f2d { background: url("https://hfday.ru/img/20241013/ec9d4127c5348f2d.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.xec9d4127c5348f2d:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x6dc4d9e284dd99ca { background: url("https://hfday.ru/img/20241014/6dc4d9e284dd99ca.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x6dc4d9e284dd99ca:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x6dc4d9e284dd99ca { background: url("https://hfday.ru/img/20241014/6dc4d9e284dd99ca.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x6dc4d9e284dd99ca:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.xd7ccc55af0bc4ea5 { background: url("https://hfday.ru/img/20241012/d7ccc55af0bc4ea5.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.xd7ccc55af0bc4ea5:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.xd7ccc55af0bc4ea5 { background: url("https://hfday.ru/img/20241012/d7ccc55af0bc4ea5.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.xd7ccc55af0bc4ea5:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.xda7093c8b8de76a8 { background: url("https://hfday.ru/img/20241014/da7093c8b8de76a8.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.xda7093c8b8de76a8:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.xda7093c8b8de76a8 { background: url("https://hfday.ru/img/20241014/da7093c8b8de76a8.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.xda7093c8b8de76a8:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x11a6963fbbeabd49 { background: url("https://hfday.ru/img/20241014/11a6963fbbeabd49.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x11a6963fbbeabd49:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x11a6963fbbeabd49 { background: url("https://hfday.ru/img/20241014/11a6963fbbeabd49.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x11a6963fbbeabd49:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x14085b9b484efc2d { background: url("https://hfday.ru/img/20241014/14085b9b484efc2d.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x14085b9b484efc2d:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x14085b9b484efc2d { background: url("https://hfday.ru/img/20241014/14085b9b484efc2d.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x14085b9b484efc2d:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x3d68a72c482a94bf { background: url("https://hfday.ru/img/20241010/3d68a72c482a94bf.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x3d68a72c482a94bf:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x3d68a72c482a94bf { background: url("https://hfday.ru/img/20241010/3d68a72c482a94bf.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x3d68a72c482a94bf:hover { background-color: rgba(60,60,60,0.92) !important;}

        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .sort-container {
                margin-top: 0px;
                text-align: left;
                width: 100%;
            .sort-dropdown {
                float: right;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiffRu(dateString) {
        const timeUnits = {
            minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
            hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
            day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"]
        };

        function getRussianPlural(number, words) {
            if (number % 10 === 1 && number % 100 !== 11) {
                return words[0];
            } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                return words[1];
            } else {
                return words[2];
            }
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);

        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes == 0) {
            return 'Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾';
        }
        else if (minutes < 60) {
            return `${minutes} ${getRussianPlural(minutes, timeUnits.minute)} Ğ½Ğ°Ğ·Ğ°Ğ´`;
        } else if (hours < 24) {
            return `${hours} ${getRussianPlural(hours, timeUnits.hour)} Ğ½Ğ°Ğ·Ğ°Ğ´`;
        } else {
            return `${days} ${getRussianPlural(days, timeUnits.day)} Ğ½Ğ°Ğ·Ğ°Ğ´`;
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function formatArticlesTitle(number) {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;

        let word;

        if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
            word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
        } else if (lastDigit === 1) {
            word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
        } else if (lastDigit >= 2 && lastDigit <= 4) {
            word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
        } else {
            word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
        }

        return `${number} ${word}`;
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p>14 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ | 19 ÑÑ‚Ğ°Ñ‚ĞµĞ¹</p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item" id="nav-prev"><a href="/d/2024-10-11.html">â¬…ï¸ 11.10</a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-10-15.html">â¡ï¸ 15.10</a></span>
            <!--<span class="nav-item" id="nav-weekly">Ğ¢Ğ¾Ğ¿ Ğ·Ğ° Ğ½ĞµĞ´ĞµĞ»Ñ</span>
            <span class="nav-item" id="nav-weekly">Ğ¢Ğ¾Ğ¿ Ğ·Ğ° Ğ¼ĞµÑÑÑ†</span>-->
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="category-toggle">
            <div class="svg-container">
                <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                <svg height="3" width="200">
                    <line x1="0" y1="0" x2="200" y2="0" 
                        stroke="black" 
                        stroke-width="2" 
                        stroke-dasharray="3, 3" />
                </svg>
            </div>
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚ Ğ¾Ğ±Ñ€ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'ru';
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2410.09732', 'title': 'LOKI: A Comprehensive Synthetic Data Detection Benchmark using Large Multimodal Models', 'url': 'https://huggingface.co/papers/2410.09732', 'abstract': 'With the rapid development of AI-generated content, the future internet may be inundated with synthetic data, making the discrimination of authentic and credible multimodal data increasingly challenging. Synthetic data detection has thus garnered widespread attention, and the performance of large multimodal models (LMMs) in this task has attracted significant interest. LMMs can provide natural language explanations for their authenticity judgments, enhancing the explainability of synthetic content detection. Simultaneously, the task of distinguishing between real and synthetic data effectively tests the perception, knowledge, and reasoning capabilities of LMMs. In response, we introduce LOKI, a novel benchmark designed to evaluate the ability of LMMs to detect synthetic data across multiple modalities. LOKI encompasses video, image, 3D, text, and audio modalities, comprising 18K carefully curated questions across 26 subcategories with clear difficulty levels. The benchmark includes coarse-grained judgment and multiple-choice questions, as well as fine-grained anomaly selection and explanation tasks, allowing for a comprehensive analysis of LMMs. We evaluated 22 open-source LMMs and 6 closed-source models on LOKI, highlighting their potential as synthetic data detectors and also revealing some limitations in the development of LMM capabilities. More information about LOKI can be found at https://opendatalab.github.io/LOKI/', 'score': 44, 'issue_id': 107, 'pub_date': '2024-10-13', 'pub_date_ru': '13 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#benchmark', '#multimodal', '#dataset'], 'emoji': 'ğŸ•µï¸', 'ru': {'title': 'LOKI: ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¸Ğ² Ğ´Ğ»Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°', 'desc': 'LOKI - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LMM) Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ…. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ 18 Ñ‚Ñ‹ÑÑÑ‡ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¿Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼, 3D, Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾, Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ° 26 Ğ¿Ğ¾Ğ´ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹ Ñ Ñ‡ĞµÑ‚ĞºĞ¸Ğ¼Ğ¸ ÑƒÑ€Ğ¾Ğ²Ğ½ÑĞ¼Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. LOKI Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· LMM Ñ‡ĞµÑ€ĞµĞ· Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ³Ñ€ÑƒĞ±Ğ¾Ğ¹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸, Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¸Ğ· Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ¾Ğ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ»Ğ¸ 28 Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ LMM Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ, Ğ²Ñ‹ÑĞ²Ğ¸Ğ² Ğ¸Ñ… Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'LOKI: Unmasking Synthetic Data with Multimodal Models', 'desc': "The paper introduces LOKI, a benchmark designed to evaluate large multimodal models (LMMs) in detecting synthetic data across various modalities like video, image, and text. LOKI includes 18,000 questions that test the models' ability to distinguish real from synthetic data, providing insights into their perception and reasoning skills. The benchmark features different types of questions, such as multiple-choice and anomaly detection, to thoroughly assess the models' capabilities. The study evaluates 28 models, revealing both their potential and limitations in synthetic data detection."}, 'zh': {'title': 'LOKIï¼šå¤šæ¨¡æ€åˆæˆæ•°æ®æ£€æµ‹çš„æ–°åŸºå‡†', 'desc': 'éšç€AIç”Ÿæˆå†…å®¹çš„å¿«é€Ÿå‘å±•ï¼Œæœªæ¥çš„äº’è”ç½‘å¯èƒ½ä¼šå……æ–¥ç€åˆæˆæ•°æ®ï¼Œä½¿å¾—è¾¨åˆ«çœŸå®å’Œå¯ä¿¡çš„å¤šæ¨¡æ€æ•°æ®å˜å¾—è¶Šæ¥è¶Šå›°éš¾ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†LOKIï¼Œä¸€ä¸ªç”¨äºè¯„ä¼°å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰æ£€æµ‹åˆæˆæ•°æ®èƒ½åŠ›çš„æ–°åŸºå‡†ã€‚LOKIæ¶µç›–è§†é¢‘ã€å›¾åƒã€3Dã€æ–‡æœ¬å’ŒéŸ³é¢‘ç­‰å¤šç§æ¨¡æ€ï¼Œæä¾›äº†18Kä¸ªç²¾å¿ƒè®¾è®¡çš„é—®é¢˜ï¼Œå¸®åŠ©å…¨é¢åˆ†æLMMsçš„èƒ½åŠ›ã€‚æˆ‘ä»¬å¯¹22ä¸ªå¼€æºLMMså’Œ6ä¸ªé—­æºæ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œæ­ç¤ºäº†å®ƒä»¬ä½œä¸ºåˆæˆæ•°æ®æ£€æµ‹å™¨çš„æ½œåŠ›å’Œä¸€äº›èƒ½åŠ›å‘å±•çš„å±€é™æ€§ã€‚'}}, 'hash': 'ec9d4127c5348f2d'}, {'id': 'https://huggingface.co/papers/2410.10139', 'title': 'MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large Vision-Language Models', 'url': 'https://huggingface.co/papers/2410.10139', 'abstract': 'Interleaved multimodal comprehension and generation, enabling models to produce and interpret both images and text in arbitrary sequences, have become a pivotal area in multimodal learning. Despite significant advancements, the evaluation of this capability remains insufficient. Existing benchmarks suffer from limitations in data scale, scope, and evaluation depth, while current evaluation metrics are often costly or biased, lacking in reliability for practical applications. To address these challenges, we introduce MMIE, a large-scale knowledge-intensive benchmark for evaluating interleaved multimodal comprehension and generation in Large Vision-Language Models (LVLMs). MMIE comprises 20K meticulously curated multimodal queries, spanning 3 categories, 12 fields, and 102 subfields, including mathematics, coding, physics, literature, health, and arts. It supports both interleaved inputs and outputs, offering a mix of multiple-choice and open-ended question formats to evaluate diverse competencies. Moreover, we propose a reliable automated evaluation metric, leveraging a scoring model fine-tuned with human-annotated data and systematic evaluation criteria, aimed at reducing bias and improving evaluation accuracy. Extensive experiments demonstrate the effectiveness of our benchmark and metrics in providing a comprehensive evaluation of interleaved LVLMs. Specifically, we evaluate eight LVLMs, revealing that even the best models show significant room for improvement, with most achieving only moderate results. We believe MMIE will drive further advancements in the development of interleaved LVLMs. We publicly release our benchmark and code in https://mmie-bench.github.io/.', 'score': 42, 'issue_id': 107, 'pub_date': '2024-10-14', 'pub_date_ru': '14 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#multimodal', '#benchmark', '#dataset'], 'emoji': 'ğŸ”„', 'ru': {'title': 'MMIE: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‡ĞµÑ€ĞµĞ´ÑƒÑÑ‰Ğ¸Ñ…ÑÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'MMIE - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ñ‡ĞµÑ€ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¼ÑÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ (LVLMs). ĞĞ½ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 20 000 Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸Ğ· 12 Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. MMIE Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ğº Ñ‡ĞµÑ€ĞµĞ´ÑƒÑÑ‰Ğ¸ĞµÑÑ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ, Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ñ‹ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'MMIE: Elevating Multimodal Model Evaluation', 'desc': 'The paper introduces MMIE, a large-scale benchmark designed to evaluate the ability of Large Vision-Language Models (LVLMs) to understand and generate both images and text in mixed sequences. MMIE includes 20,000 multimodal queries across various fields like mathematics and arts, using both multiple-choice and open-ended questions to test different skills. The authors also propose a new automated evaluation metric that uses a scoring model fine-tuned with human data to reduce bias and improve accuracy. Experiments show that while current LVLMs have room for improvement, MMIE provides a comprehensive framework for assessing their capabilities.'}, 'zh': {'title': 'æ¨åŠ¨å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆçš„æ–°æ—¶ä»£', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„è¯„ä¼°åŸºå‡†MMIEï¼Œç”¨äºè¯„ä¼°å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚MMIEåŒ…å«2ä¸‡æ¡ç²¾å¿ƒè®¾è®¡çš„å¤šæ¨¡æ€æŸ¥è¯¢ï¼Œæ¶µç›–æ•°å­¦ã€ç¼–ç¨‹ã€ç‰©ç†ç­‰å¤šä¸ªé¢†åŸŸï¼Œæ”¯æŒäº¤é”™è¾“å…¥å’Œè¾“å‡ºã€‚ç ”ç©¶è€…è¿˜æå‡ºäº†ä¸€ç§å¯é çš„è‡ªåŠ¨åŒ–è¯„ä¼°æŒ‡æ ‡ï¼Œåˆ©ç”¨ç»è¿‡äººç±»æ ‡æ³¨æ•°æ®å¾®è°ƒçš„è¯„åˆ†æ¨¡å‹æ¥å‡å°‘åå·®ï¼Œæé«˜è¯„ä¼°å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯æœ€å¥½çš„æ¨¡å‹ä¹Ÿæœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ï¼ŒMMIEå°†æ¨åŠ¨äº¤é”™LVLMsçš„å‘å±•ã€‚'}}, 'hash': '6dc4d9e284dd99ca'}, {'id': 'https://huggingface.co/papers/2410.09584', 'title': 'Toward General Instruction-Following Alignment for Retrieval-Augmented Generation', 'url': 'https://huggingface.co/papers/2410.09584', 'abstract': 'Following natural instructions is crucial for the effective application of Retrieval-Augmented Generation (RAG) systems. Despite recent advancements in Large Language Models (LLMs), research on assessing and improving instruction-following (IF) alignment within the RAG domain remains limited. To address this issue, we propose VIF-RAG, the first automated, scalable, and verifiable synthetic pipeline for instruction-following alignment in RAG systems. We start by manually crafting a minimal set of atomic instructions (<100) and developing combination rules to synthesize and verify complex instructions for a seed set. We then use supervised models for instruction rewriting while simultaneously generating code to automate the verification of instruction quality via a Python executor. Finally, we integrate these instructions with extensive RAG and general data samples, scaling up to a high-quality VIF-RAG-QA dataset (>100k) through automated processes. To further bridge the gap in instruction-following auto-evaluation for RAG systems, we introduce FollowRAG Benchmark, which includes approximately 3K test samples, covering 22 categories of general instruction constraints and four knowledge-intensive QA datasets. Due to its robust pipeline design, FollowRAG can seamlessly integrate with different RAG benchmarks. Using FollowRAG and eight widely-used IF and foundational abilities benchmarks for LLMs, we demonstrate that VIF-RAG markedly enhances LLM performance across a broad range of general instruction constraints while effectively leveraging its capabilities in RAG scenarios. Further analysis offers practical insights for achieving IF alignment in RAG systems. Our code and datasets are released at https://FollowRAG.github.io.', 'score': 32, 'issue_id': 107, 'pub_date': '2024-10-12', 'pub_date_ru': '12 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#rag', '#benchmark', '#dataset'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'VIF-RAG: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ RAG-ÑĞ¸ÑÑ‚ĞµĞ¼ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼', 'desc': 'VIF-RAG - ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… RAG. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ°Ñ‚Ğ¾Ğ¼Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹, Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ° Ğ¸Ñ… ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµÑ„Ñ€Ğ°Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ±Ñ‹Ğ» ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ VIF-RAG-QA Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº FollowRAG Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ RAG-ÑĞ¸ÑÑ‚ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ VIF-RAG Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ….'}, 'en': {'title': 'Enhancing RAG Systems with VIF-RAG: A New Era of Instruction-Following Alignment', 'desc': 'The paper introduces VIF-RAG, a novel system designed to improve instruction-following alignment in Retrieval-Augmented Generation (RAG) systems. It uses a minimal set of manually crafted instructions and combination rules to create complex instructions, which are then verified using a Python executor. The system generates a high-quality dataset, VIF-RAG-QA, and introduces the FollowRAG Benchmark to evaluate instruction-following capabilities across various categories. The results show that VIF-RAG significantly enhances the performance of Large Language Models in following instructions within RAG contexts.'}, 'zh': {'title': 'æå‡RAGç³»ç»Ÿçš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›', 'desc': 'è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºVIF-RAGçš„æ–°æ–¹æ³•ï¼Œç”¨äºæé«˜æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿä¸­çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚ç ”ç©¶äººå‘˜é€šè¿‡æ‰‹åŠ¨è®¾è®¡å°‘é‡åŸºç¡€æŒ‡ä»¤ï¼Œå¹¶å¼€å‘ç»„åˆè§„åˆ™æ¥ç”Ÿæˆå¤æ‚æŒ‡ä»¤ï¼Œä»è€Œåˆ›å»ºäº†ä¸€ä¸ªé«˜è´¨é‡çš„æ•°æ®é›†ã€‚é€šè¿‡ç›‘ç£æ¨¡å‹å’ŒPythonæ‰§è¡Œå™¨ï¼Œä»–ä»¬å®ç°äº†æŒ‡ä»¤é‡å†™å’Œè´¨é‡éªŒè¯çš„è‡ªåŠ¨åŒ–ã€‚æœ€ç»ˆï¼ŒVIF-RAGæ˜¾è‘—æå‡äº†å¤§è¯­è¨€æ¨¡å‹åœ¨å¤šç§æŒ‡ä»¤çº¦æŸä¸‹çš„è¡¨ç°ã€‚'}}, 'hash': 'd7ccc55af0bc4ea5'}, {'id': 'https://huggingface.co/papers/2410.10306', 'title': 'Animate-X: Universal Character Image Animation with Enhanced Motion Representation', 'url': 'https://huggingface.co/papers/2410.10306', 'abstract': 'Character image animation, which generates high-quality videos from a reference image and target pose sequence, has seen significant progress in recent years. However, most existing methods only apply to human figures, which usually do not generalize well on anthropomorphic characters commonly used in industries like gaming and entertainment. Our in-depth analysis suggests to attribute this limitation to their insufficient modeling of motion, which is unable to comprehend the movement pattern of the driving video, thus imposing a pose sequence rigidly onto the target character. To this end, this paper proposes Animate-X, a universal animation framework based on LDM for various character types (collectively named X), including anthropomorphic characters. To enhance motion representation, we introduce the Pose Indicator, which captures comprehensive motion pattern from the driving video through both implicit and explicit manner. The former leverages CLIP visual features of a driving video to extract its gist of motion, like the overall movement pattern and temporal relations among motions, while the latter strengthens the generalization of LDM by simulating possible inputs in advance that may arise during inference. Moreover, we introduce a new Animated Anthropomorphic Benchmark (A^2Bench) to evaluate the performance of Animate-X on universal and widely applicable animation images. Extensive experiments demonstrate the superiority and effectiveness of Animate-X compared to state-of-the-art methods.', 'score': 23, 'issue_id': 108, 'pub_date': '2024-10-14', 'pub_date_ru': '14 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#cv', '#video', '#benchmark', '#multimodal'], 'emoji': 'ğŸ­', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Animate-X - ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LDM, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ°Ğ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¾Ğ¼Ğ¾Ñ€Ñ„Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Pose Indicator Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ĞºĞ°Ğº Ğ½ĞµÑĞ²Ğ½Ñ‹Ğµ, Ñ‚Ğ°Ğº Ğ¸ ÑĞ²Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ CLIP Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‚Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ· ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº A^2Bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ….'}, 'en': {'title': 'Animate-X: Bringing Characters to Life with Universal Animation', 'desc': "The paper introduces Animate-X, a novel animation framework designed to generate high-quality videos from reference images and pose sequences, applicable to a wide range of character types, including anthropomorphic ones. Unlike existing methods that struggle with non-human figures, Animate-X uses a Pose Indicator to capture motion patterns from driving videos, enhancing motion representation. This is achieved through both implicit methods, using CLIP visual features, and explicit methods, simulating potential inputs to improve generalization. The framework's effectiveness is validated using a new benchmark, A^2Bench, showing superior performance over current state-of-the-art techniques."}, 'zh': {'title': 'Animate-Xï¼šé€‚ç”¨äºå„ç§è§’è‰²çš„é€šç”¨åŠ¨ç”»æ¡†æ¶', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºAnimate-Xçš„åŠ¨ç”»æ¡†æ¶ï¼Œå¯ä»¥ä»å‚è€ƒå›¾åƒå’Œç›®æ ‡å§¿åŠ¿åºåˆ—ç”Ÿæˆé«˜è´¨é‡è§†é¢‘ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒAnimate-Xé€‚ç”¨äºå„ç§è§’è‰²ç±»å‹ï¼ŒåŒ…æ‹¬æ‹ŸäººåŒ–è§’è‰²ã€‚ä¸ºäº†å¢å¼ºåŠ¨ä½œè¡¨ç¤ºï¼Œä½œè€…å¼•å…¥äº†å§¿åŠ¿æŒ‡ç¤ºå™¨ï¼Œé€šè¿‡éšå¼å’Œæ˜¾å¼æ–¹å¼æ•æ‰é©±åŠ¨è§†é¢‘çš„è¿åŠ¨æ¨¡å¼ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†A^2Benchï¼Œç”¨äºè¯„ä¼°Animate-Xåœ¨åŠ¨ç”»å›¾åƒä¸Šçš„è¡¨ç°ã€‚'}}, 'hash': 'da7093c8b8de76a8'}, {'id': 'https://huggingface.co/papers/2410.10563', 'title': 'MEGA-Bench: Scaling Multimodal Evaluation to over 500 Real-World Tasks', 'url': 'https://huggingface.co/papers/2410.10563', 'abstract': 'We present MEGA-Bench, an evaluation suite that scales multimodal evaluation to over 500 real-world tasks, to address the highly heterogeneous daily use cases of end users. Our objective is to optimize for a set of high-quality data samples that cover a highly diverse and rich set of multimodal tasks, while enabling cost-effective and accurate model evaluation. In particular, we collected 505 realistic tasks encompassing over 8,000 samples from 16 expert annotators to extensively cover the multimodal task space. Instead of unifying these problems into standard multi-choice questions (like MMMU, MMBench, and MMT-Bench), we embrace a wide range of output formats like numbers, phrases, code, \\LaTeX, coordinates, JSON, free-form, etc. To accommodate these formats, we developed over 40 metrics to evaluate these tasks. Unlike existing benchmarks, MEGA-Bench offers a fine-grained capability report across multiple dimensions (e.g., application, input type, output format, skill), allowing users to interact with and visualize model capabilities in depth. We evaluate a wide variety of frontier vision-language models on MEGA-Bench to understand their capabilities across these dimensions.', 'score': 23, 'issue_id': 108, 'pub_date': '2024-10-14', 'pub_date_ru': '14 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#multimodal', '#benchmark', '#dataset'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'MEGA-Bench: ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…', 'desc': 'MEGA-Bench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ĞµĞµ 500 Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ½ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¹ ÑĞ¿ĞµĞºÑ‚Ñ€ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¾Ğ² Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 40 Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. MEGA-Bench Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ñ‡ĞµÑ‚ Ğ¾ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑÑ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸ÑĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'MEGA-Bench: Unleashing Multimodal Evaluation Power', 'desc': 'MEGA-Bench is a comprehensive evaluation suite designed to test machine learning models on over 500 real-world multimodal tasks. It focuses on providing high-quality, diverse data samples to ensure accurate and cost-effective model evaluation. Unlike traditional benchmarks, MEGA-Bench supports a variety of output formats, such as numbers, phrases, and JSON, and uses over 40 metrics to assess model performance. This approach allows for a detailed analysis of model capabilities across different dimensions, offering users a deeper understanding of model strengths and weaknesses.'}, 'zh': {'title': 'MEGA-Benchï¼šå¤šæ¨¡æ€ä»»åŠ¡çš„å…¨é¢è¯„ä¼°', 'desc': 'MEGA-Bench æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€è¯„ä¼°å¥—ä»¶ï¼Œæ¶µç›–äº†è¶…è¿‡ 500 ä¸ªçœŸå®ä¸–ç•Œä»»åŠ¡ï¼Œæ—¨åœ¨ä¼˜åŒ–é«˜è´¨é‡æ•°æ®æ ·æœ¬çš„å¤šæ ·æ€§å’Œä¸°å¯Œæ€§ã€‚æˆ‘ä»¬æ”¶é›†äº† 505 ä¸ªä»»åŠ¡å’Œ 8000 å¤šä¸ªæ ·æœ¬ï¼Œæ”¯æŒå¤šç§è¾“å‡ºæ ¼å¼ï¼Œå¦‚æ•°å­—ã€çŸ­è¯­ã€ä»£ç ç­‰ã€‚ä¸ºäº†è¯„ä¼°è¿™äº›ä»»åŠ¡ï¼Œæˆ‘ä»¬å¼€å‘äº† 40 å¤šç§æŒ‡æ ‡ï¼Œæä¾›ç»†è‡´çš„èƒ½åŠ›æŠ¥å‘Šã€‚é€šè¿‡ MEGA-Benchï¼Œæˆ‘ä»¬å¯ä»¥æ·±å…¥äº†è§£è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ä¸åŒç»´åº¦ä¸Šçš„èƒ½åŠ›ã€‚'}}, 'hash': '11a6963fbbeabd49'}, {'id': 'https://huggingface.co/papers/2410.10783', 'title': 'LiveXiv -- A Multi-Modal Live Benchmark Based on Arxiv Papers Content', 'url': 'https://huggingface.co/papers/2410.10783', 'abstract': 'The large-scale training of multi-modal models on data scraped from the web has shown outstanding utility in infusing these models with the required world knowledge to perform effectively on multiple downstream tasks. However, one downside of scraping data from the web can be the potential sacrifice of the benchmarks on which the abilities of these models are often evaluated. To safeguard against test data contamination and to truly test the abilities of these foundation models we propose LiveXiv: A scalable evolving live benchmark based on scientific ArXiv papers. LiveXiv accesses domain-specific manuscripts at any given timestamp and proposes to automatically generate visual question-answer pairs (VQA). This is done without any human-in-the-loop, using the multi-modal content in the manuscripts, like graphs, charts, and tables. Moreover, we introduce an efficient evaluation approach that estimates the performance of all models on the evolving benchmark using evaluations of only a subset of models. This significantly reduces the overall evaluation cost. We benchmark multiple open and proprietary Large Multi-modal Models (LMMs) on the first version of our benchmark, showing its challenging nature and exposing the models true abilities, avoiding contamination. Lastly, in our commitment to high quality, we have collected and evaluated a manually verified subset. By comparing its overall results to our automatic annotations, we have found that the performance variance is indeed minimal (<2.5%). Our dataset is available online on HuggingFace, and our code will be available here.', 'score': 22, 'issue_id': 109, 'pub_date': '2024-10-14', 'pub_date_ru': '14 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#multimodal', '#benchmark', '#dataset'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'LiveXiv: Ğ­Ğ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'LiveXiv - ÑÑ‚Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ÑÑ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚ÑŒÑÑ… ArXiv. ĞĞ½ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ°Ñ€Ñ‹ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¿Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ñƒ (VQA) Ğ¸Ğ· Ñ€ÑƒĞºĞ¾Ğ¿Ğ¸ÑĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸, Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹ Ğ¸ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¾Ñ†ĞµĞ½ĞºÑƒ. LiveXiv Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ñ†ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ Ğ·Ğ°Ğ³Ñ€ÑĞ·Ğ½ĞµĞ½Ğ¸Ñ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'LiveXiv: Unveiling True Model Capabilities with Dynamic Benchmarks', 'desc': 'The paper introduces LiveXiv, a dynamic benchmark designed to evaluate multi-modal models using scientific papers from ArXiv, ensuring no test data contamination. It automatically generates visual question-answer pairs from the content of these papers, like graphs and tables, without human intervention. The authors propose an efficient evaluation method that reduces costs by assessing only a subset of models, yet accurately estimates performance across all models. The benchmark reveals the true capabilities of large multi-modal models, with minimal performance variance between automatic and manually verified results.'}, 'zh': {'title': 'LiveXivï¼šç§‘å­¦è®ºæ–‡é©±åŠ¨çš„åŠ¨æ€åŸºå‡†æµ‹è¯•', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºLiveXivçš„æ–°æ–¹æ³•ï¼Œç”¨äºè¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹çš„èƒ½åŠ›ã€‚é€šè¿‡ä»ç§‘å­¦ArXivè®ºæ–‡ä¸­è‡ªåŠ¨ç”Ÿæˆè§†è§‰é—®ç­”å¯¹ï¼ŒLiveXivé¿å…äº†æµ‹è¯•æ•°æ®æ±¡æŸ“çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•ä¸éœ€è¦äººå·¥å‚ä¸ï¼Œåˆ©ç”¨è®ºæ–‡ä¸­çš„å›¾è¡¨å’Œè¡¨æ ¼ç­‰å¤šæ¨¡æ€å†…å®¹ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•ä¸ä»…é™ä½äº†è¯„ä¼°æˆæœ¬ï¼Œè¿˜èƒ½å‡†ç¡®åæ˜ æ¨¡å‹çš„çœŸå®èƒ½åŠ›ã€‚'}}, 'hash': '14085b9b484efc2d'}, {'id': 'https://huggingface.co/papers/2410.07985', 'title': 'Omni-MATH: A Universal Olympiad Level Mathematic Benchmark For Large Language Models', 'url': 'https://huggingface.co/papers/2410.07985', 'abstract': "Recent advancements in large language models (LLMs) have led to significant breakthroughs in mathematical reasoning capabilities. However, existing benchmarks like GSM8K or MATH are now being solved with high accuracy (e.g., OpenAI o1 achieves 94.8% on MATH dataset), indicating their inadequacy for truly challenging these models. To bridge this gap, we propose a comprehensive and challenging benchmark specifically designed to assess LLMs' mathematical reasoning at the Olympiad level. Unlike existing Olympiad-related benchmarks, our dataset focuses exclusively on mathematics and comprises a vast collection of 4428 competition-level problems with rigorous human annotation. These problems are meticulously categorized into over 33 sub-domains and span more than 10 distinct difficulty levels, enabling a holistic assessment of model performance in Olympiad-mathematical reasoning. Furthermore, we conducted an in-depth analysis based on this benchmark. Our experimental results show that even the most advanced models, OpenAI o1-mini and OpenAI o1-preview, struggle with highly challenging Olympiad-level problems, with 60.54% and 52.55% accuracy, highlighting significant challenges in Olympiad-level mathematical reasoning.", 'score': 20, 'issue_id': 107, 'pub_date': '2024-10-10', 'pub_date_ru': '10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#dataset', '#benchmark', '#math'], 'emoji': 'ğŸ§®', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ´Ğ»Ñ Ğ˜Ğ˜: Ğ¾Ğ»Ğ¸Ğ¼Ğ¿Ğ¸Ğ°Ğ´Ğ½Ğ°Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ° ĞºĞ°Ğº Ñ‚ĞµÑÑ‚ Ğ½Ğ° Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ğ»Ğ¸Ğ¼Ğ¿Ğ¸Ğ°Ğ´. ĞĞ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 4428 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¾Ğ»Ğ¸Ğ¼Ğ¿Ğ¸Ğ°Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ñ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹, Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ° 33 Ğ¿Ğ¾Ğ´Ğ´Ğ¾Ğ¼ĞµĞ½Ğ° Ğ¸ 10 ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ ÑĞ°Ğ¼Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº OpenAI o1-mini Ğ¸ OpenAI o1-preview, Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¾Ğ»Ğ¸Ğ¼Ğ¿Ğ¸Ğ°Ğ´Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ 60.54% Ğ¸ 52.55% ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾. Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾Ğ»Ğ¸Ğ¼Ğ¿Ğ¸Ğ°Ğ´Ğ½Ğ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ´Ğ»Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Pushing the Limits: Challenging LLMs with Olympiad-Level Math', 'desc': "The paper introduces a new benchmark designed to test large language models (LLMs) on Olympiad-level mathematical reasoning, as existing benchmarks are no longer challenging enough. This new dataset includes 4428 competition-level problems, categorized into over 33 sub-domains and 10 difficulty levels, providing a comprehensive assessment of LLMs' capabilities. The study reveals that even advanced models like OpenAI o1-mini and OpenAI o1-preview struggle with these problems, achieving only 60.54% and 52.55% accuracy, respectively. This highlights the need for further advancements in LLMs to tackle complex mathematical reasoning tasks."}, 'zh': {'title': 'æŒ‘æˆ˜å¤§å‹è¯­è¨€æ¨¡å‹çš„å¥¥æ—åŒ¹å…‹æ•°å­¦æ¨ç†èƒ½åŠ›', 'desc': 'è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†èƒ½åŠ›ä¸Šå–å¾—äº†æ˜¾è‘—çªç ´ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•å¦‚GSM8Kæˆ–MATHå·²ç»è¢«é«˜ç²¾åº¦è§£å†³ï¼Œæ˜¾ç¤ºå‡ºå®ƒä»¬å¯¹è¿™äº›æ¨¡å‹çš„æŒ‘æˆ˜æ€§ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä¸“é—¨è®¾è®¡çš„ç»¼åˆæ€§æŒ‘æˆ˜åŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¥¥æ—åŒ¹å…‹æ°´å¹³çš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨é¢å¯¹å¥¥æ—åŒ¹å…‹çº§åˆ«çš„æ•°å­¦é—®é¢˜æ—¶ä»ç„¶é¢ä¸´å·¨å¤§æŒ‘æˆ˜ã€‚'}}, 'hash': '3d68a72c482a94bf'}, {'id': 'https://huggingface.co/papers/2410.10774', 'title': 'Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention', 'url': 'https://huggingface.co/papers/2410.10774', 'abstract': 'In recent years there have been remarkable breakthroughs in image-to-video generation. However, the 3D consistency and camera controllability of generated frames have remained unsolved. Recent studies have attempted to incorporate camera control into the generation process, but their results are often limited to simple trajectories or lack the ability to generate consistent videos from multiple distinct camera paths for the same scene. To address these limitations, we introduce Cavia, a novel framework for camera-controllable, multi-view video generation, capable of converting an input image into multiple spatiotemporally consistent videos. Our framework extends the spatial and temporal attention modules into view-integrated attention modules, improving both viewpoint and temporal consistency. This flexible design allows for joint training with diverse curated data sources, including scene-level static videos, object-level synthetic multi-view dynamic videos, and real-world monocular dynamic videos. To our best knowledge, Cavia is the first of its kind that allows the user to precisely specify camera motion while obtaining object motion. Extensive experiments demonstrate that Cavia surpasses state-of-the-art methods in terms of geometric consistency and perceptual quality. Project Page: https://ir1d.github.io/Cavia/', 'score': 18, 'issue_id': 108, 'pub_date': '2024-10-14', 'pub_date_ru': '14 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#cv', '#video', '#multimodal'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Cavia: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ ĞºĞ°Ğ¼ĞµÑ€Ñ‹', 'desc': 'Cavia - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ ĞºĞ°Ğ¼ĞµÑ€Ğ¾Ğ¹ Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ². ĞĞ½Ğ° Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ. Cavia Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğ¼ĞµÑ€Ñ‹, Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Cavia Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ.'}, 'en': {'title': 'Cavia: Mastering Camera Control in Image-to-Video Generation', 'desc': 'The paper introduces Cavia, a new framework for generating videos from images with improved 3D consistency and camera control. Cavia enhances video generation by using view-integrated attention modules, which ensure both viewpoint and temporal consistency across frames. This approach allows for precise camera motion control and can be trained with a variety of data sources, including static, synthetic, and real-world videos. Experiments show that Cavia outperforms existing methods in maintaining geometric consistency and high perceptual quality in the generated videos.'}, 'zh': {'title': 'Caviaï¼šå®ç°ç›¸æœºå¯æ§çš„å¤šè§†è§’è§†é¢‘ç”Ÿæˆ', 'desc': 'è¿‘å¹´æ¥ï¼Œå›¾åƒåˆ°è§†é¢‘ç”ŸæˆæŠ€æœ¯å–å¾—äº†æ˜¾è‘—çªç ´ï¼Œä½†ç”Ÿæˆå¸§çš„ä¸‰ç»´ä¸€è‡´æ€§å’Œç›¸æœºå¯æ§æ€§ä»æœªè§£å†³ã€‚Caviaæ˜¯ä¸€ä¸ªæ–°æ¡†æ¶ï¼Œèƒ½å¤Ÿå°†è¾“å…¥å›¾åƒè½¬æ¢ä¸ºå¤šä¸ªæ—¶ç©ºä¸€è‡´çš„è§†é¢‘ï¼Œå¹¶å…è®¸ç”¨æˆ·ç²¾ç¡®æ§åˆ¶ç›¸æœºè¿åŠ¨ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†ç©ºé—´å’Œæ—¶é—´æ³¨æ„åŠ›æ¨¡å—æ‰©å±•ä¸ºè§†å›¾é›†æˆæ³¨æ„åŠ›æ¨¡å—ï¼Œæé«˜äº†è§†ç‚¹å’Œæ—¶é—´ä¸€è‡´æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒCaviaåœ¨å‡ ä½•ä¸€è‡´æ€§å’Œæ„ŸçŸ¥è´¨é‡æ–¹é¢è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚'}}, 'hash': '95f9847ff43aeaf2'}, {'id': 'https://huggingface.co/papers/2410.10792', 'title': 'Semantic Image Inversion and Editing using Rectified Stochastic Differential Equations', 'url': 'https://huggingface.co/papers/2410.10792', 'abstract': 'Generative models transform random noise into images; their inversion aims to transform images back to structured noise for recovery and editing. This paper addresses two key tasks: (i) inversion and (ii) editing of a real image using stochastic equivalents of rectified flow models (such as Flux). Although Diffusion Models (DMs) have recently dominated the field of generative modeling for images, their inversion presents faithfulness and editability challenges due to nonlinearities in drift and diffusion. Existing state-of-the-art DM inversion approaches rely on training of additional parameters or test-time optimization of latent variables; both are expensive in practice. Rectified Flows (RFs) offer a promising alternative to diffusion models, yet their inversion has been underexplored. We propose RF inversion using dynamic optimal control derived via a linear quadratic regulator. We prove that the resulting vector field is equivalent to a rectified stochastic differential equation. Additionally, we extend our framework to design a stochastic sampler for Flux. Our inversion method allows for state-of-the-art performance in zero-shot inversion and editing, outperforming prior works in stroke-to-image synthesis and semantic image editing, with large-scale human evaluations confirming user preference.', 'score': 11, 'issue_id': 109, 'pub_date': '2024-10-14', 'pub_date_ru': '14 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#cv', '#generative_models'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ¸Ñ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²Ñ‹Ğ¿Ñ€ÑĞ¼Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞºĞ²Ğ¸Ğ²Ğ°Ğ»ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ñ‹Ğ¿Ñ€ÑĞ¼Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° (Rectified Flow). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ¸Ñ RF Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğ¹ ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ³ÑƒĞ»ÑÑ‚Ğ¾Ñ€. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞŸÑ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ½Ñ‹Ğµ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ¿ĞµÑ€ĞµĞ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Revolutionizing Image Inversion and Editing with Rectified Flows', 'desc': 'This paper explores how to reverse the process of generating images from noise, using a method called Rectified Flows (RFs), which is less explored compared to popular Diffusion Models (DMs). The authors propose a new way to invert images back to noise using a technique called dynamic optimal control, which is more efficient than current methods that require extra training or complex optimization. They also introduce a new tool for editing images, which allows for better performance in tasks like turning sketches into images or changing parts of an image. The results show that their method is preferred by users, offering a promising alternative to existing techniques.'}, 'zh': {'title': 'é€†è½¬ç”Ÿæˆæ¨¡å‹ï¼šä»å›¾åƒåˆ°å™ªå£°çš„åˆ›æ–°ä¹‹è·¯', 'desc': 'è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†å¦‚ä½•å°†ç”Ÿæˆæ¨¡å‹ç”Ÿæˆçš„å›¾åƒé€†è½¬å›ç»“æ„åŒ–çš„å™ªå£°ï¼Œä»¥ä¾¿è¿›è¡Œæ¢å¤å’Œç¼–è¾‘ã€‚ä½œè€…æå‡ºäº†ä¸€ç§åŸºäºåŠ¨æ€æœ€ä¼˜æ§åˆ¶çš„é€†è½¬æ–¹æ³•ï¼Œä½¿ç”¨çº¿æ€§äºŒæ¬¡è°ƒèŠ‚å™¨æ¥å®ç°ã€‚è¯¥æ–¹æ³•åœ¨é›¶æ ·æœ¬é€†è½¬å’Œç¼–è¾‘ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä¼˜äºç°æœ‰çš„æ–¹æ³•ã€‚å¤§è§„æ¨¡çš„äººç±»è¯„ä¼°æ˜¾ç¤ºï¼Œç”¨æˆ·æ›´åå¥½è¿™ç§æ–°æ–¹æ³•ã€‚'}}, 'hash': '064d3a6fd8a2ab06'}, {'id': 'https://huggingface.co/papers/2410.10818', 'title': 'TemporalBench: Benchmarking Fine-grained Temporal Understanding for Multimodal Video Models', 'url': 'https://huggingface.co/papers/2410.10818', 'abstract': "Understanding fine-grained temporal dynamics is crucial for multimodal video comprehension and generation. Due to the lack of fine-grained temporal annotations, existing video benchmarks mostly resemble static image benchmarks and are incompetent at evaluating models for temporal understanding. In this paper, we introduce TemporalBench, a new benchmark dedicated to evaluating fine-grained temporal understanding in videos. TemporalBench consists of ~10K video question-answer pairs, derived from ~2K high-quality human annotations detailing the temporal dynamics in video clips. As a result, our benchmark provides a unique testbed for evaluating various temporal understanding and reasoning abilities such as action frequency, motion magnitude, event order, etc. Moreover, it enables evaluations on various tasks like both video question answering and captioning, both short and long video understanding, as well as different models such as multimodal video embedding models and text generation models. Results show that state-of-the-art models like GPT-4o achieve only 38.5% question answering accuracy on TemporalBench, demonstrating a significant gap (~30%) between humans and AI in temporal understanding. Furthermore, we notice a critical pitfall for multi-choice QA where LLMs can detect the subtle changes in negative captions and find a centralized description as a cue for its prediction, where we propose Multiple Binary Accuracy (MBA) to correct such bias. We hope that TemporalBench can foster research on improving models' temporal reasoning capabilities. Both dataset and evaluation code will be made available.", 'score': 10, 'issue_id': 108, 'pub_date': '2024-10-14', 'pub_date_ru': '14 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#benchmark', '#video', '#multimodal', '#dataset'], 'emoji': 'â³', 'ru': {'title': 'TemporalBench: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'TemporalBench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ½ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ¾ 10 Ñ‚Ñ‹ÑÑÑ‡ Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¿Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑÑ… Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ»Ğ¸Ğ¿Ğ°Ñ…. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ½Ğ° Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ñ€ÑĞ´Ğ¾Ğº ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ»ÑĞ´ÑŒĞ¼Ğ¸ Ğ¸ Ğ˜Ğ˜ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'TemporalBench: Bridging the Gap in Video Temporal Understanding', 'desc': 'The paper introduces TemporalBench, a new benchmark designed to evaluate fine-grained temporal understanding in videos, addressing the limitations of existing video benchmarks that lack detailed temporal annotations. TemporalBench includes around 10,000 video question-answer pairs based on high-quality human annotations, providing a unique platform for assessing temporal reasoning abilities like action frequency and event order. The benchmark reveals a significant gap in temporal understanding between humans and AI, with state-of-the-art models achieving only 38.5% accuracy. The authors propose a new evaluation metric, Multiple Binary Accuracy (MBA), to address biases in multi-choice question answering, aiming to enhance research in temporal reasoning capabilities of AI models.'}, 'zh': {'title': 'æå‡è§†é¢‘æ—¶é—´ç†è§£çš„æ–°åŸºå‡†ï¼šTemporalBench', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºTemporalBenchçš„æ–°åŸºå‡†ï¼Œç”¨äºè¯„ä¼°è§†é¢‘ä¸­ç»†ç²’åº¦çš„æ—¶é—´ç†è§£èƒ½åŠ›ã€‚TemporalBenchåŒ…å«çº¦1ä¸‡ä¸ªè§†é¢‘é—®ç­”å¯¹ï¼ŒåŸºäºçº¦2000ä¸ªé«˜è´¨é‡çš„äººç±»æ³¨é‡Šï¼Œè¯¦ç»†æè¿°äº†è§†é¢‘ç‰‡æ®µä¸­çš„æ—¶é—´åŠ¨æ€ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç°æœ‰çš„é¡¶å°–æ¨¡å‹åœ¨TemporalBenchä¸Šçš„é—®ç­”å‡†ç¡®ç‡ä»…ä¸º38.5%ï¼Œæ˜¾ç¤ºå‡ºäººç±»ä¸AIåœ¨æ—¶é—´ç†è§£ä¸Šçš„æ˜¾è‘—å·®è·ã€‚è®ºæ–‡è¿˜æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æ–¹æ³•ï¼Œç§°ä¸ºå¤šé‡äºŒå…ƒå‡†ç¡®ç‡ï¼ˆMBAï¼‰ï¼Œä»¥çº æ­£å¤šé€‰é¢˜ä¸­çš„åå·®ã€‚'}}, 'hash': '96570f7d74bf91e1'}, {'id': 'https://huggingface.co/papers/2410.10594', 'title': 'VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents', 'url': 'https://huggingface.co/papers/2410.10594', 'abstract': 'Retrieval-augmented generation (RAG) is an effective technique that enables large language models (LLMs) to utilize external knowledge sources for generation. However, current RAG systems are solely based on text, rendering it impossible to utilize vision information like layout and images that play crucial roles in real-world multi-modality documents. In this paper, we introduce VisRAG, which tackles this issue by establishing a vision-language model (VLM)-based RAG pipeline. In this pipeline, instead of first parsing the document to obtain text, the document is directly embedded using a VLM as an image and then retrieved to enhance the generation of a VLM. Compared to traditional text-based RAG, VisRAG maximizes the retention and utilization of the data information in the original documents, eliminating the information loss introduced during the parsing process. We collect both open-source and synthetic data to train the retriever in VisRAG and explore a variety of generation methods. Experiments demonstrate that VisRAG outperforms traditional RAG in both the retrieval and generation stages, achieving a 25--39\\% end-to-end performance gain over traditional text-based RAG pipeline. Further analysis reveals that VisRAG is effective in utilizing training data and demonstrates strong generalization capability, positioning it as a promising solution for RAG on multi-modality documents. Our code and data are available at https://github.com/openbmb/visrag .', 'score': 8, 'issue_id': 118, 'pub_date': '2024-10-14', 'pub_date_ru': '14 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#rag', '#multimodal', '#cv'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'VisRAG: ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ RAG Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ ĞºĞ°Ğº Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸', 'desc': 'VisRAG - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ (RAG), Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM). Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… RAG-ÑĞ¸ÑÑ‚ĞµĞ¼, VisRAG Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ²ÑÑ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ VisRAG Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ RAG-ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ½Ğ° 25-39% Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ ĞºĞ¾Ğ´ Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğµ Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹.'}, 'en': {'title': 'VisRAG: Bridging Text and Vision for Smarter Document Understanding', 'desc': "The paper introduces VisRAG, a novel approach that enhances retrieval-augmented generation by incorporating visual information into the process. Unlike traditional RAG systems that rely solely on text, VisRAG uses a vision-language model to embed documents as images, preserving more information from the original source. This method significantly improves the performance of both retrieval and generation tasks, showing a 25-39% increase over text-based RAG systems. The study highlights VisRAG's strong generalization capabilities and its potential as a robust solution for handling multi-modality documents."}, 'zh': {'title': 'VisRAGï¼šå¤šæ¨¡æ€æ–‡æ¡£å¤„ç†çš„æ–°çªç ´', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºVisRAGçš„æ–°æŠ€æœ¯ï¼Œå®ƒé€šè¿‡è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ¥å¢å¼ºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿçš„èƒ½åŠ›ã€‚VisRAGç›´æ¥å°†æ–‡æ¡£ä½œä¸ºå›¾åƒåµŒå…¥ï¼Œè€Œä¸æ˜¯å…ˆè§£ææˆæ–‡æœ¬ï¼Œä»è€Œæœ€å¤§é™åº¦åœ°ä¿ç•™å’Œåˆ©ç”¨åŸå§‹æ–‡æ¡£ä¸­çš„ä¿¡æ¯ã€‚å®éªŒè¡¨æ˜ï¼ŒVisRAGåœ¨æ£€ç´¢å’Œç”Ÿæˆé˜¶æ®µçš„è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„åŸºäºæ–‡æœ¬çš„RAGç³»ç»Ÿï¼Œæ€§èƒ½æå‡è¾¾25%åˆ°39%ã€‚è¿™ç§æ–¹æ³•åœ¨å¤šæ¨¡æ€æ–‡æ¡£å¤„ç†ä¸­å±•ç¤ºäº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œæ˜¯ä¸€ç§æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚'}}, 'hash': '21d912d0f36db525'}, {'id': 'https://huggingface.co/papers/2410.09335', 'title': 'Rethinking Data Selection at Scale: Random Selection is Almost All You Need', 'url': 'https://huggingface.co/papers/2410.09335', 'abstract': 'Supervised fine-tuning (SFT) is crucial for aligning Large Language Models (LLMs) with human instructions. The primary goal during SFT is to select a small yet representative subset of training data from the larger pool, such that fine-tuning with this subset achieves results comparable to or even exceeding those obtained using the entire dataset. However, most existing data selection techniques are designed for small-scale data pools, which fail to meet the demands of real-world SFT scenarios. In this paper, we replicated several self-scoring methods those that do not rely on external model assistance on two million scale datasets, and found that nearly all methods struggled to significantly outperform random selection when dealing with such large-scale data pools. Moreover, our comparisons suggest that, during SFT, diversity in data selection is more critical than simply focusing on high quality data. We also analyzed the limitations of several current approaches, explaining why they perform poorly on large-scale datasets and why they are unsuitable for such contexts. Finally, we found that filtering data by token length offers a stable and efficient method for improving results. This approach, particularly when training on long text data, proves highly beneficial for relatively weaker base models, such as Llama3.', 'score': 8, 'issue_id': 107, 'pub_date': '2024-10-12', 'pub_date_ru': '12 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#dataset', '#data', '#benchmark'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ´Ğ¾Ğ²Ğ¾Ğ´ĞºĞ¸ (SFT). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ñ€ĞµĞ¿Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ÑĞ°Ğ¼Ğ¾Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ½Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ°Ğ¶Ğ½ĞµĞµ Ğ¸Ñ… ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¸ SFT. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹, Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ Ğ´Ğ»Ğ¸Ğ½Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ğ¾Ğ»ĞµĞ·ĞµĞ½ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ°Ğ±Ñ‹Ñ… Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Diversity Over Quality: Rethinking Data Selection for LLM Fine-Tuning', 'desc': 'The paper explores the challenges of selecting a representative subset of data for supervised fine-tuning of Large Language Models (LLMs). It finds that most existing data selection methods, which work well on small datasets, struggle with large-scale datasets, often performing no better than random selection. The study highlights the importance of diversity in data selection over merely focusing on high-quality data. Additionally, it suggests that filtering data by token length can enhance the performance of weaker models during fine-tuning.'}, 'zh': {'title': 'å¤šæ ·æ€§èƒœäºè´¨é‡ï¼šå¤§è§„æ¨¡æ•°æ®é›†ä¸­çš„ç›‘ç£å¾®è°ƒç­–ç•¥', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„é‡è¦æ€§ï¼Œå°¤å…¶æ˜¯åœ¨é€‰æ‹©è®­ç»ƒæ•°æ®å­é›†æ—¶ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰çš„æ•°æ®é€‰æ‹©æ–¹æ³•åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¡¨ç°ä¸ä½³ï¼Œéšæœºé€‰æ‹©åè€Œæ•ˆæœæ›´å¥½ã€‚è®ºæ–‡æŒ‡å‡ºï¼Œåœ¨SFTä¸­ï¼Œæ•°æ®é€‰æ‹©çš„å¤šæ ·æ€§æ¯”å•çº¯è¿½æ±‚é«˜è´¨é‡æ•°æ®æ›´ä¸ºé‡è¦ã€‚é€šè¿‡åˆ†æï¼Œä½œè€…å‘ç°é€šè¿‡ç­›é€‰æ•°æ®çš„è¯å…ƒé•¿åº¦å¯ä»¥ç¨³å®šæé«˜ç»“æœï¼Œå°¤å…¶å¯¹è¾ƒå¼±çš„åŸºç¡€æ¨¡å‹å¦‚Llama3æ•ˆæœæ˜¾è‘—ã€‚'}}, 'hash': 'b683d10c4851f00f'}, {'id': 'https://huggingface.co/papers/2410.06634', 'title': 'Tree of Problems: Improving structured problem solving with compositionality', 'url': 'https://huggingface.co/papers/2410.06634', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable performance across multiple tasks through in-context learning. For complex reasoning tasks that require step-by-step thinking, Chain-of-Thought (CoT) prompting has given impressive results, especially when combined with self-consistency. Nonetheless, some tasks remain particularly difficult for LLMs to solve. Tree of Thoughts (ToT) and Graph of Thoughts (GoT) emerged as alternatives, dividing the complex problem into paths of subproblems. In this paper, we propose Tree of Problems (ToP), a simpler version of ToT, which we hypothesise can work better for complex tasks that can be divided into identical subtasks. Our empirical results show that our approach outperforms ToT and GoT, and in addition performs better than CoT on complex reasoning tasks. All code for this paper is publicly available here: https://github.com/ArmelRandy/tree-of-problems.', 'score': 5, 'issue_id': 109, 'pub_date': '2024-10-09', 'pub_date_ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#reasoning', '#rlhf'], 'emoji': 'ğŸŒ³', 'ru': {'title': 'Ğ”Ñ€ĞµĞ²Ğ¾ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Tree of Problems (ToP) Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ToP ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ÑƒĞ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ²ĞµÑ€ÑĞ¸ĞµĞ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Tree of Thoughts (ToT) Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ğ½Ğ° Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ²ÑˆĞ¸Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ ToP Ğ½Ğ°Ğ´ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ ToT Ğ¸ Graph of Thoughts (GoT). ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, ToP Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, Ñ‡ĞµĞ¼ Chain-of-Thought (CoT) Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Simplifying Complexity: Tree of Problems Revolutionizes Reasoning', 'desc': "The paper introduces a new method called Tree of Problems (ToP) to improve the performance of Large Language Models (LLMs) on complex reasoning tasks. ToP simplifies the Tree of Thoughts (ToT) approach by breaking down complex problems into identical subtasks, which enhances the model's ability to solve them. The authors demonstrate that ToP outperforms both ToT and Graph of Thoughts (GoT), as well as Chain-of-Thought (CoT) prompting, in handling intricate reasoning challenges. This advancement suggests a promising direction for enhancing LLMs' problem-solving capabilities."}, 'zh': {'title': 'é—®é¢˜æ ‘ï¼šç®€åŒ–å¤æ‚ä»»åŠ¡çš„æœ‰æ•ˆæ–¹æ³•', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­ã€‚å¯¹äºéœ€è¦é€æ­¥æ¨ç†çš„å¤æ‚ä»»åŠ¡ï¼Œé“¾å¼æ€ç»´æç¤ºç»“åˆè‡ªæˆ‘ä¸€è‡´æ€§å–å¾—äº†æ˜¾è‘—æ•ˆæœã€‚ç„¶è€Œï¼Œä¸€äº›ä»»åŠ¡å¯¹å¤§å‹è¯­è¨€æ¨¡å‹æ¥è¯´ä»ç„¶å¾ˆéš¾è§£å†³ã€‚æœ¬æ–‡æå‡ºäº†é—®é¢˜æ ‘ï¼ˆToPï¼‰ï¼Œä¸€ç§æ›´ç®€å•çš„æ€ç»´æ ‘ç‰ˆæœ¬ï¼Œå®éªŒç»“æœè¡¨æ˜å…¶åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­ä¼˜äºæ€ç»´æ ‘å’Œæ€ç»´å›¾ã€‚'}}, 'hash': '0f052b591f948ce8'}, {'id': 'https://huggingface.co/papers/2410.10803', 'title': 'Generalizable Humanoid Manipulation with Improved 3D Diffusion Policies', 'url': 'https://huggingface.co/papers/2410.10803', 'abstract': 'Humanoid robots capable of autonomous operation in diverse environments have long been a goal for roboticists. However, autonomous manipulation by humanoid robots has largely been restricted to one specific scene, primarily due to the difficulty of acquiring generalizable skills. Recent advances in 3D visuomotor policies, such as the 3D Diffusion Policy (DP3), have shown promise in extending these capabilities to wilder environments. However, 3D visuomotor policies often rely on camera calibration and point-cloud segmentation, which present challenges for deployment on mobile robots like humanoids. In this work, we introduce the Improved 3D Diffusion Policy (iDP3), a novel 3D visuomotor policy that eliminates these constraints by leveraging egocentric 3D visual representations. We demonstrate that iDP3 enables a full-sized humanoid robot to autonomously perform skills in diverse real-world scenarios, using only data collected in the lab. Videos are available at: https://humanoid-manipulation.github.io', 'score': 5, 'issue_id': 108, 'pub_date': '2024-10-14', 'pub_date_ru': '14 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#robotics', '#3d', '#rl'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ³ÑƒĞ¼Ğ°Ğ½Ğ¾Ğ¸Ğ´Ğ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ³ÑƒĞ¼Ğ°Ğ½Ğ¾Ğ¸Ğ´Ğ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² - Improved 3D Diffusion Policy (iDP3). Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ 3D Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¾Ğ¹ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾Ğ±Ğ»Ğ°ĞºĞ° Ñ‚Ğ¾Ñ‡ĞµĞº. iDP3 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ³ÑƒĞ¼Ğ°Ğ½Ğ¾Ğ¸Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, ÑĞ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ² Ğ»Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ‚Ğ¾Ñ€Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…. Ğ­Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ ÑˆĞ°Ğ³ Ğ²Ğ¿ĞµÑ€ĞµĞ´ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ³ÑƒĞ¼Ğ°Ğ½Ğ¾Ğ¸Ğ´Ğ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ….'}, 'en': {'title': 'Breaking Barriers: Humanoids in the Real World', 'desc': 'The paper introduces the Improved 3D Diffusion Policy (iDP3), a new approach in 3D visuomotor policies for humanoid robots. iDP3 overcomes the limitations of previous methods by using egocentric 3D visual representations, eliminating the need for camera calibration and point-cloud segmentation. This advancement allows humanoid robots to autonomously perform tasks in various real-world environments using only lab-collected data. The research demonstrates significant progress in enabling humanoid robots to operate independently in diverse settings.'}, 'zh': {'title': 'çªç ´é™åˆ¶ï¼šç±»äººæœºå™¨äººè‡ªä¸»æ“ä½œçš„æ–°ç­–ç•¥', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„3Dè§†è§‰è¿åŠ¨ç­–ç•¥ï¼Œç§°ä¸ºæ”¹è¿›çš„3Dæ‰©æ•£ç­–ç•¥ï¼ˆiDP3ï¼‰ï¼Œå®ƒå¯ä»¥è®©ç±»äººæœºå™¨äººåœ¨å¤šç§çœŸå®åœºæ™¯ä¸­è‡ªä¸»æ“ä½œã€‚ä¼ ç»Ÿçš„3Dè§†è§‰è¿åŠ¨ç­–ç•¥ä¾èµ–äºç›¸æœºæ ¡å‡†å’Œç‚¹äº‘åˆ†å‰²ï¼Œè¿™å¯¹ç§»åŠ¨æœºå™¨äººæ¥è¯´æ˜¯ä¸ªæŒ‘æˆ˜ã€‚iDP3é€šè¿‡åˆ©ç”¨è‡ªæˆ‘ä¸­å¿ƒçš„3Dè§†è§‰è¡¨ç¤ºï¼Œæ¶ˆé™¤äº†è¿™äº›é™åˆ¶ã€‚å®éªŒè¡¨æ˜ï¼ŒiDP3ä½¿å¾—å…¨å°ºå¯¸ç±»äººæœºå™¨äººä»…å‡­å®éªŒå®¤æ”¶é›†çš„æ•°æ®å°±èƒ½åœ¨å¤šæ ·åŒ–çš„ç¯å¢ƒä¸­æ‰§è¡ŒæŠ€èƒ½ã€‚'}}, 'hash': '753b160b8fdc53e0'}, {'id': 'https://huggingface.co/papers/2410.10813', 'title': 'LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory', 'url': 'https://huggingface.co/papers/2410.10813', 'abstract': 'Recent large language model (LLM)-driven chat assistant systems have integrated memory components to track user-assistant chat histories, enabling more accurate and personalized responses. However, their long-term memory capabilities in sustained interactions remain underexplored. This paper introduces LongMemEval, a comprehensive benchmark designed to evaluate five core long-term memory abilities of chat assistants: information extraction, multi-session reasoning, temporal reasoning, knowledge updates, and abstention. With 500 meticulously curated questions embedded within freely scalable user-assistant chat histories, LongMemEval presents a significant challenge to existing long-term memory systems, with commercial chat assistants and long-context LLMs showing 30% accuracy drop on memorizing information across sustained interactions. We then present a unified framework that breaks down the long-term memory design into four design choices across the indexing, retrieval, and reading stages. Built upon key experimental insights, we propose several memory designs including session decomposition for optimizing value granularity, fact-augmented key expansion for enhancing the index structure, and time-aware query expansion for refining the search scope. Experiment results show that these optimizations greatly improve both memory recall and downstream question answering on LongMemEval. Overall, our study provides valuable resources and guidance for advancing the long-term memory capabilities of LLM-based chat assistants, paving the way toward more personalized and reliable conversational AI.', 'score': 4, 'issue_id': 109, 'pub_date': '2024-10-14', 'pub_date_ru': '14 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#benchmark', '#dataset'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'LongMemEval: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ¾Ğ»Ğ³Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ñ‡Ğ°Ñ‚-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ LongMemEval - ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ñ‡Ğ°Ñ‚-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹Ğ´ĞµĞ»ÑÑÑ‚ Ğ¿ÑÑ‚ÑŒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ· 500 Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ´Ğ»Ñ Ğ¸Ñ… Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¿Ñ€Ğ¸ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½Ğ´ĞµĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Enhancing Chat Assistants with Long-Term Memory Mastery', 'desc': 'The paper introduces LongMemEval, a benchmark to test the long-term memory abilities of chat assistants, focusing on five key areas like information extraction and temporal reasoning. It highlights the challenges faced by current systems, which show a significant drop in accuracy when dealing with sustained interactions. The authors propose a framework with innovative memory designs, such as session decomposition and time-aware query expansion, to enhance memory recall and question answering. These improvements aim to advance the personalization and reliability of conversational AI systems.'}, 'zh': {'title': 'æå‡èŠå¤©åŠ©æ‰‹çš„é•¿æœŸè®°å¿†èƒ½åŠ›', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºLongMemEvalçš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°èŠå¤©åŠ©æ‰‹çš„äº”ç§æ ¸å¿ƒé•¿æœŸè®°å¿†èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰çš„å•†ä¸šèŠå¤©åŠ©æ‰‹åœ¨é•¿æ—¶é—´äº¤äº’ä¸­è®°å¿†ä¿¡æ¯çš„å‡†ç¡®ç‡ä¸‹é™äº†30%ã€‚ä½œè€…æå‡ºäº†ä¸€ç§ç»Ÿä¸€æ¡†æ¶ï¼Œé€šè¿‡ä¼˜åŒ–ç´¢å¼•ã€æ£€ç´¢å’Œè¯»å–é˜¶æ®µæ¥æå‡è®°å¿†æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™äº›ä¼˜åŒ–æ˜¾è‘—æé«˜äº†è®°å¿†å¬å›ç‡å’Œé—®é¢˜å›ç­”èƒ½åŠ›ã€‚'}}, 'hash': 'b100ebbfd8b25c2e'}, {'id': 'https://huggingface.co/papers/2410.07752', 'title': 'TVBench: Redesigning Video-Language Evaluation', 'url': 'https://huggingface.co/papers/2410.07752', 'abstract': 'Large language models have demonstrated impressive performance when integrated with vision models even enabling video understanding. However, evaluating these video models presents its own unique challenges, for which several benchmarks have been proposed. In this paper, we show that the currently most used video-language benchmarks can be solved without requiring much temporal reasoning. We identified three main issues in existing datasets: (i) static information from single frames is often sufficient to solve the tasks (ii) the text of the questions and candidate answers is overly informative, allowing models to answer correctly without relying on any visual input (iii) world knowledge alone can answer many of the questions, making the benchmarks a test of knowledge replication rather than visual reasoning. In addition, we found that open-ended question-answering benchmarks for video understanding suffer from similar issues while the automatic evaluation process with LLMs is unreliable, making it an unsuitable alternative. As a solution, we propose TVBench, a novel open-source video multiple-choice question-answering benchmark, and demonstrate through extensive evaluations that it requires a high level of temporal understanding. Surprisingly, we find that most recent state-of-the-art video-language models perform similarly to random performance on TVBench, with only Gemini-Pro and Tarsier clearly surpassing this baseline.', 'score': 4, 'issue_id': 107, 'pub_date': '2024-10-10', 'pub_date_ru': '10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#benchmark', '#video', '#multimodal'], 'emoji': 'ğŸ¥', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ñ€ĞµÑˆĞ¸Ñ‚ÑŒ Ğ±ĞµĞ· Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº TVBench, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ TVBench, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ³Ğ°Ğ´Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Rethinking Video Understanding: Beyond Static Frames', 'desc': "The paper discusses the limitations of current video-language benchmarks, which often don't require true temporal reasoning to solve. It identifies three main issues: tasks can be solved with static frame information, overly informative text, and reliance on world knowledge. The authors propose a new benchmark, TVBench, which demands genuine temporal understanding for video question-answering. They find that most state-of-the-art models perform poorly on TVBench, highlighting the need for improved temporal reasoning in video models."}, 'zh': {'title': 'æå‡è§†é¢‘ç†è§£ï¼šä»é™æ€åˆ°åŠ¨æ€çš„æŒ‘æˆ˜', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†è§†é¢‘è¯­è¨€æ¨¡å‹çš„è¯„ä¼°æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºç°æœ‰çš„åŸºå‡†æµ‹è¯•å­˜åœ¨é—®é¢˜ã€‚ç ”ç©¶å‘ç°ï¼Œè®¸å¤šä»»åŠ¡å¯ä»¥é€šè¿‡å•å¸§é™æ€ä¿¡æ¯æˆ–æ–‡æœ¬ä¿¡æ¯è§£å†³ï¼Œè€Œä¸éœ€è¦çœŸæ­£çš„è§†è§‰æ¨ç†ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•TVBenchï¼Œå¼ºè°ƒæ—¶é—´ç†è§£çš„é‡è¦æ€§ã€‚ç»“æœæ˜¾ç¤ºï¼Œè®¸å¤šå…ˆè¿›çš„æ¨¡å‹åœ¨TVBenchä¸Šçš„è¡¨ç°ä¸éšæœºæ°´å¹³ç›¸å½“ï¼Œåªæœ‰Gemini-Proå’ŒTarsierè¡¨ç°ä¼˜å¼‚ã€‚'}}, 'hash': 'dd1e54c62f09078e'}, {'id': 'https://huggingface.co/papers/2410.09733', 'title': 'MMCOMPOSITION: Revisiting the Compositionality of Pre-trained Vision-Language Models', 'url': 'https://huggingface.co/papers/2410.09733', 'abstract': "The advent of large Vision-Language Models (VLMs) has significantly advanced multimodal understanding, enabling more sophisticated and accurate integration of visual and textual information across various tasks, including image and video captioning, visual question answering, and cross-modal retrieval. Despite VLMs' superior capabilities, researchers lack a comprehensive understanding of their compositionality -- the ability to understand and produce novel combinations of known visual and textual components. Prior benchmarks provide only a relatively rough compositionality evaluation from the perspectives of objects, relations, and attributes while neglecting deeper reasoning about object interactions, counting, and complex compositions. However, compositionality is a critical ability that facilitates coherent reasoning and understanding across modalities for VLMs. To address this limitation, we propose MMCOMPOSITION, a novel human-annotated benchmark for comprehensively and accurately evaluating VLMs' compositionality. Our proposed benchmark serves as a complement to these earlier works. With MMCOMPOSITION, we can quantify and explore the compositionality of the mainstream VLMs. Surprisingly, we find GPT-4o's compositionality inferior to the best open-source model, and we analyze the underlying reasons. Our experimental analysis reveals the limitations of VLMs in fine-grained compositional perception and reasoning, and points to areas for improvement in VLM design and training. Resources available at: https://hanghuacs.github.io/MMComposition/", 'score': 3, 'issue_id': 116, 'pub_date': '2024-10-13', 'pub_date_ru': '13 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#multimodal', '#benchmark', '#cv'], 'emoji': 'ğŸ§©', 'ru': {'title': 'MMCOMPOSITION: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'MMCOMPOSITION - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM). ĞĞ½ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ². Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¿Ğ¾Ğ´ÑÑ‡ĞµÑ‚Ğµ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸ÑÑ…. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ MMCOMPOSITION Ğ²Ñ‹ÑĞ²Ğ¸Ğ» Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… VLM Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğ¸ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸.'}, 'en': {'title': 'Unlocking the Compositional Power of Vision-Language Models', 'desc': 'The paper introduces MMCOMPOSITION, a new benchmark designed to evaluate the compositionality of Vision-Language Models (VLMs), which is their ability to understand and generate new combinations of visual and textual elements. This benchmark addresses the shortcomings of previous evaluations by focusing on deeper reasoning tasks such as object interactions and complex compositions. The study reveals that some open-source models outperform GPT-4o in compositionality, highlighting areas where VLMs need improvement. The findings suggest that enhancing compositional perception and reasoning is crucial for advancing VLM capabilities.'}, 'zh': {'title': 'æ¢ç´¢è§†è§‰è¯­è¨€æ¨¡å‹çš„ç»„åˆèƒ½åŠ›', 'desc': 'å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šæ¨¡æ€ç†è§£ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶ç»„åˆèƒ½åŠ›ä»éœ€æ·±å…¥ç ”ç©¶ã€‚ç»„åˆèƒ½åŠ›æ˜¯æŒ‡ç†è§£å’Œç”Ÿæˆå·²çŸ¥è§†è§‰å’Œæ–‡æœ¬å…ƒç´ çš„æ–°ç»„åˆçš„èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºäº†ä¸€ä¸ªåä¸ºMMCOMPOSITIONçš„æ–°åŸºå‡†ï¼Œç”¨äºå…¨é¢è¯„ä¼°VLMsçš„ç»„åˆèƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGPT-4oçš„ç»„åˆèƒ½åŠ›ä¸å¦‚æŸäº›å¼€æºæ¨¡å‹ï¼Œè¿™æ­ç¤ºäº†VLMsåœ¨ç»†ç²’åº¦ç»„åˆæ„ŸçŸ¥å’Œæ¨ç†ä¸Šçš„å±€é™æ€§ã€‚'}}, 'hash': 'fb02158c865832bc'}, {'id': 'https://huggingface.co/papers/2410.09223', 'title': 'The Same But Different: Structural Similarities and Differences in Multilingual Language Modeling', 'url': 'https://huggingface.co/papers/2410.09223', 'abstract': 'We employ new tools from mechanistic interpretability in order to ask whether the internal structure of large language models (LLMs) shows correspondence to the linguistic structures which underlie the languages on which they are trained. In particular, we ask (1) when two languages employ the same morphosyntactic processes, do LLMs handle them using shared internal circuitry? and (2) when two languages require different morphosyntactic processes, do LLMs handle them using different internal circuitry? Using English and Chinese multilingual and monolingual models, we analyze the internal circuitry involved in two tasks. We find evidence that models employ the same circuit to handle the same syntactic process independently of the language in which it occurs, and that this is the case even for monolingual models trained completely independently. Moreover, we show that multilingual models employ language-specific components (attention heads and feed-forward networks) when needed to handle linguistic processes (e.g., morphological marking) that only exist in some languages. Together, our results provide new insights into how LLMs trade off between exploiting common structures and preserving linguistic differences when tasked with modeling multiple languages simultaneously.', 'score': 1, 'issue_id': 119, 'pub_date': '2024-10-11', 'pub_date_ru': '11 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#multilingual', '#interpretability'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€ Ğ² LLM', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¸ Ğ¸Ñ… ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ°Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ°Ğ»Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ»Ğ¸ LLM Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ ÑÑ…ĞµĞ¼Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ÑÑ…Ğ¾Ğ¶Ğ¸Ñ… Ğ¼Ğ¾Ñ€Ñ„Ğ¾ÑĞ¸Ğ½Ñ‚Ğ°ĞºÑĞ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ…, Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ…ĞµĞ¼Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°ÑÑ‰Ğ¸Ñ…ÑÑ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ². ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ñ… Ğ¸ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ½Ğ¾- Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ», Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ñ‹Ğµ ÑÑ…ĞµĞ¼Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ñ‹Ñ… ÑĞ¸Ğ½Ñ‚Ğ°ĞºÑĞ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¾Ñ‚ ÑĞ·Ñ‹ĞºĞ°, Ğ´Ğ°Ğ¶Ğµ Ğ² ÑĞ»ÑƒÑ‡Ğ°Ğµ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ½Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ° ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ², ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ² Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ….'}, 'en': {'title': 'Unveiling the Linguistic Brain of AI: Shared Circuits and Unique Paths', 'desc': 'This paper explores how large language models (LLMs) internally process linguistic structures from different languages. It investigates whether LLMs use the same internal mechanisms for similar morphosyntactic processes across languages and different mechanisms for distinct processes. The study uses English and Chinese models to show that LLMs often use the same circuitry for similar syntactic tasks, even in monolingual models. Additionally, it finds that multilingual models can adapt by using language-specific components for unique linguistic features.'}, 'zh': {'title': 'æ¢ç´¢LLMå†…éƒ¨ç»“æ„ä¸è¯­è¨€ç»“æ„çš„å¯¹åº”å…³ç³»', 'desc': 'è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å†…éƒ¨ç»“æ„æ˜¯å¦ä¸å…¶è®­ç»ƒè¯­è¨€çš„è¯­è¨€ç»“æ„ç›¸å¯¹åº”ã€‚ç ”ç©¶å‘ç°ï¼Œå½“ä¸¤ç§è¯­è¨€ä½¿ç”¨ç›¸åŒçš„å½¢æ€å¥æ³•è¿‡ç¨‹æ—¶ï¼Œæ¨¡å‹ä¼šä½¿ç”¨ç›¸åŒçš„å†…éƒ¨ç”µè·¯æ¥å¤„ç†ï¼Œå³ä½¿æ˜¯å®Œå…¨ç‹¬ç«‹è®­ç»ƒçš„å•è¯­æ¨¡å‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚æ­¤å¤–ï¼Œå¤šè¯­è¨€æ¨¡å‹åœ¨å¤„ç†æŸäº›è¯­è¨€ç‰¹æœ‰çš„è¯­è¨€è¿‡ç¨‹æ—¶ï¼Œä¼šä½¿ç”¨ç‰¹å®šçš„è¯­è¨€ç»„ä»¶ã€‚ç ”ç©¶ç»“æœæ­ç¤ºäº†LLMåœ¨åŒæ—¶å»ºæ¨¡å¤šç§è¯­è¨€æ—¶ï¼Œå¦‚ä½•åœ¨åˆ©ç”¨å…±åŒç»“æ„å’Œä¿ç•™è¯­è¨€å·®å¼‚ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚'}}, 'hash': '025fd1d188bbd7f3'}, {'id': 'https://huggingface.co/papers/2410.10630', 'title': 'Thinking LLMs: General Instruction Following with Thought Generation', 'url': 'https://huggingface.co/papers/2410.10630', 'abstract': 'LLMs are typically trained to answer user questions or follow instructions similarly to how human experts respond. However, in the standard alignment framework they lack the basic ability of explicit thinking before answering. Thinking is important for complex questions that require reasoning and planning -- but can be applied to any task. We propose a training method for equipping existing LLMs with such thinking abilities for general instruction following without use of additional human data. We achieve this by an iterative search and optimization procedure that explores the space of possible thought generations, allowing the model to learn how to think without direct supervision. For each instruction, the thought candidates are scored using a judge model to evaluate their responses only, and then optimized via preference optimization. We show that this procedure leads to superior performance on AlpacaEval and Arena-Hard, and shows gains from thinking on non-reasoning categories such as marketing, health and general knowledge, in addition to more traditional reasoning & problem-solving tasks.', 'score': 1, 'issue_id': 118, 'pub_date': '2024-10-14', 'pub_date_ru': '14 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#rlhf', '#benchmark', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜ Ğ´ÑƒĞ¼Ğ°Ñ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ´ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº ÑĞ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ´ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ¼ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ñƒ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ¼Ñ‹ÑĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-ÑÑƒĞ´ÑŒÑ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… AlpacaEval Ğ¸ Arena-Hard, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑÑ… Ğ·Ğ°Ğ´Ğ°Ñ‡.'}, 'en': {'title': 'Teaching LLMs to Think Before They Speak', 'desc': "The paper introduces a new training method for large language models (LLMs) to enhance their ability to think explicitly before answering questions. This method involves an iterative search and optimization process that allows the model to explore and learn from different thought processes without needing additional human data. The approach uses a judge model to score thought candidates and optimizes them through preference optimization, leading to improved performance across various tasks. The results show that this thinking ability enhances the model's performance not only in reasoning tasks but also in areas like marketing and health."}, 'zh': {'title': 'è®©AIå­¦ä¼šæ€è€ƒï¼šæ— éœ€äººç±»æ•°æ®çš„æ™ºèƒ½è®­ç»ƒæ–¹æ³•', 'desc': 'è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒæ–¹æ³•ï¼Œä½¿å¾—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å›ç­”é—®é¢˜å‰èƒ½å¤Ÿè¿›è¡Œæ˜ç¡®çš„æ€è€ƒã€‚é€šè¿‡è¿­ä»£æœç´¢å’Œä¼˜åŒ–è¿‡ç¨‹ï¼Œæ¨¡å‹å¯ä»¥åœ¨æ²¡æœ‰é¢å¤–äººç±»æ•°æ®çš„æƒ…å†µä¸‹å­¦ä¹ å¦‚ä½•æ€è€ƒã€‚æ¯ä¸ªæŒ‡ä»¤çš„æ€è€ƒå€™é€‰é¡¹é€šè¿‡ä¸€ä¸ªè¯„åˆ¤æ¨¡å‹è¿›è¡Œè¯„åˆ†ï¼Œå¹¶é€šè¿‡åå¥½ä¼˜åŒ–è¿›è¡Œä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•åœ¨å¤æ‚æ¨ç†ä»»åŠ¡å’Œéæ¨ç†ä»»åŠ¡ï¼ˆå¦‚å¸‚åœºè¥é”€ã€å¥åº·å’Œå¸¸è¯†ï¼‰ä¸Šéƒ½è¡¨ç°å‡ºè‰²ã€‚'}}, 'hash': 'ce8e4d06cae69001'}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            const themeToggle = document.getElementById('theme-toggle');
            let settingSortBy = localStorage.getItem('sort_by');
            const sortDropdown = document.getElementById('sort-dropdown');
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }
            
            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents', '#agi', '#alignment', '#architecture', '#audio', '#benchmark (13)', '#cv (5)', '#data (1)', '#dataset (9)', '#diffusion', '#edge_computing', '#ethics', '#games', '#generative_models', '#graphs', '#hallucinations', '#inference', '#interpretability (1)', '#math (1)', '#medicine', '#multilingual (1)', '#multimodal (10)', '#optimization', '#plp', '#quantum', '#rag (2)', '#reasoning (2)', '#rl (1)', '#rlhf (2)', '#robotics (1)', '#security', '#story_generation', '#survey', '#training', '#transfer_learning', '#video (4)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = 'ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€';
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€ (${formatArticlesTitle(selectedArticles.length)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles = selectedCategories.length === 0
                ? articlesData
                : articlesData.filter(article => 
                    article.data && article.data.categories && 
                    article.data.categories.some(cat => selectedCategories.includes(cat))
                );

            console.log('filteredArticles', filteredArticles)

            //if (filteredArticles.length === 0) {
            //    selectedArticles = articlesData;
            //    selectedCategories = [];
            //    cleanCategorySelection();
            //} else {
            //    selectedArticles = filteredArticles;
            //}

            selectedArticles = filteredArticles;

            console.log('selectedArticles', selectedArticles)

            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            console.log(articles);
            let lang = 'ru'
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].join(" ");
                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="background-digit">${index + 1}</div>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <h2>${item['data']['emoji']} ${item['title']}</h2>
                            <p class="meta"><svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path transform="translate(0, 2)" fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> ${item['score']}. ${title}</p>
                            <p class="pub-date">ğŸ“… Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ${item['pub_date_ru']}</p>
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>
                            <div class="links">
                                <a href="${item['url']}" target="_blank">Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ</a>
                            </div>
                            <p class="tags">${cats}</p>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            }
            if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
        });

        clearCategoriesButton.addEventListener('click', clearAllCategories);
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiffRu('2024-10-15 16:15');
        } 
        function hideNextLink() {
            if (isToday('2024-10-15 16:15')) {
                const element = document.getElementById('nav-next');
                if (element) {    
                    element.style.display = 'none';
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink(); 
        initializeLanguageFlags();
    </script>
</body>
</html>
    