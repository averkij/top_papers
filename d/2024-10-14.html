
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 20 papers. October 14.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            padding: 0 20px;
            flex: 1 0 auto;
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 0 20px 0;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            display: flex;
            flex-direction: column;
            position: relative;
        }
        .article-content {
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
        }
        body.dark-theme>div>main>article {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article {
            background-color: #fff;
        }
        body.dark-theme>div>main>article:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            padding: 0 20px;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 0px;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
                margin: 0 -20px;
            }
            footer {
                margin-top: -20px;
            }
            article {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">14 октября</span> | <span id="title-articles-count">20 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item" id="nav-prev"><a href="/d/2024-10-11.html">⬅️ <span id="prev-date">11.10</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-10-15.html">➡️ <span id="next-date">15.10</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2024-10.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '14 октября', 'en': 'October 14', 'zh': '10月14日'};
        let feedDateNext = {'ru': '15.10', 'en': '10/15', 'zh': '10月15日'};
        let feedDatePrev = {'ru': '11.10', 'en': '10/11', 'zh': '10月11日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2410.08565', 'title': 'Baichuan-Omni Technical Report', 'url': 'https://huggingface.co/papers/2410.08565', 'abstract': 'The salient multimodal capabilities and interactive experience of GPT-4o highlight its critical role in practical applications, yet it lacks a high-performing open-source counterpart. In this paper, we introduce Baichuan-Omni, the first open-source 7B Multimodal Large Language Model (MLLM) adept at concurrently processing and analyzing modalities of image, video, audio, and text, while delivering an advanced multimodal interactive experience and strong performance. We propose an effective multimodal training schema starting with 7B model and proceeding through two stages of multimodal alignment and multitask fine-tuning across audio, image, video, and text modal. This approach equips the language model with the ability to handle visual and audio data effectively. Demonstrating strong performance across various omni-modal and multimodal benchmarks, we aim for this contribution to serve as a competitive baseline for the open-source community in advancing multimodal understanding and real-time interaction.', 'score': 82, 'issue_id': 90, 'pub_date': '2024-10-11', 'pub_date_card': {'ru': '11 октября', 'en': 'October 11', 'zh': '10月11日'}, 'hash': '635ebaf87323c35f', 'data': {'categories': ['#training', '#benchmark', '#alignment', '#open_source', '#small_models', '#architecture', '#multimodal'], 'emoji': '🌐', 'ru': {'title': 'Baichuan-Omni: открытая мультимодальная модель для обработки текста, изображений, видео и аудио', 'desc': 'Статья представляет Baichuan-Omni - первую открытую мультимодальную языковую модель на 7 миллиардов параметров. Модель способна одновременно обрабатывать изображения, видео, аудио и текст, обеспечивая продвинутый интерактивный опыт. Авторы предлагают эффективную схему мультимодального обучения в два этапа: мультимодальное выравнивание и мультизадачная донастройка. Baichuan-Omni демонстрирует высокие результаты на различных мультимодальных бенчмарках.'}, 'en': {'title': 'Baichuan-Omni: Bridging Multimodal Gaps in Open-Source AI', 'desc': 'The paper introduces Baichuan-Omni, an open-source 7 billion parameter Multimodal Large Language Model (MLLM) capable of processing images, videos, audio, and text simultaneously. It employs a two-stage training process involving multimodal alignment and multitask fine-tuning to enhance its ability to handle diverse data types. The model demonstrates strong performance across various benchmarks, showcasing its potential as a competitive tool for multimodal understanding. This work aims to provide a robust baseline for the open-source community to further develop real-time multimodal interaction capabilities.'}, 'zh': {'title': 'Baichuan-Omni：开源多模态大语言模型的先锋', 'desc': '这篇论文介绍了Baichuan-Omni，这是第一个开源的7B多模态大语言模型，能够同时处理图像、视频、音频和文本。通过两阶段的多模态对齐和多任务微调训练方法，该模型在多模态交互体验和性能上表现出色。Baichuan-Omni展示了在多种多模态基准测试中的强大性能。我们希望这一贡献能成为开源社区在多模态理解和实时交互方面的竞争基准。'}}}, {'id': 'https://huggingface.co/papers/2410.08261', 'title': 'Meissonic: Revitalizing Masked Generative Transformers for Efficient High-Resolution Text-to-Image Synthesis', 'url': 'https://huggingface.co/papers/2410.08261', 'abstract': "Diffusion models, such as Stable Diffusion, have made significant strides in visual generation, yet their paradigm remains fundamentally different from autoregressive language models, complicating the development of unified language-vision models. Recent efforts like LlamaGen have attempted autoregressive image generation using discrete VQVAE tokens, but the large number of tokens involved renders this approach inefficient and slow. In this work, we present Meissonic, which elevates non-autoregressive masked image modeling (MIM) text-to-image to a level comparable with state-of-the-art diffusion models like SDXL. By incorporating a comprehensive suite of architectural innovations, advanced positional encoding strategies, and optimized sampling conditions, Meissonic substantially improves MIM's performance and efficiency. Additionally, we leverage high-quality training data, integrate micro-conditions informed by human preference scores, and employ feature compression layers to further enhance image fidelity and resolution. Our model not only matches but often exceeds the performance of existing models like SDXL in generating high-quality, high-resolution images. Extensive experiments validate Meissonic's capabilities, demonstrating its potential as a new standard in text-to-image synthesis. We release a model checkpoint capable of producing 1024 times 1024 resolution images.", 'score': 49, 'issue_id': 91, 'pub_date': '2024-10-10', 'pub_date_card': {'ru': '10 октября', 'en': 'October 10', 'zh': '10月10日'}, 'hash': '3b6012d644e53308', 'data': {'categories': ['#cv', '#training', '#data', '#optimization', '#alignment', '#open_source', '#diffusion', '#architecture'], 'emoji': '🎨', 'ru': {'title': 'Meissonic: Революция в генерации изображений с MIM', 'desc': 'Meissonic - это новая модель для генерации изображений по текстовому описанию, основанная на подходе masked image modeling (MIM). Она сочетает архитектурные инновации, продвинутые стратегии позиционного кодирования и оптимизированные условия сэмплирования для улучшения производительности и эффективности MIM. Meissonic использует качественные обучающие данные, микро-условия на основе оценок предпочтений человека и слои сжатия признаков для повышения качества и разрешения изображений. Модель демонстрирует результаты на уровне или превосходящие современные диффузионные модели вроде SDXL.'}, 'en': {'title': 'Meissonic: Redefining Text-to-Image Synthesis with Efficiency and Quality', 'desc': 'The paper introduces Meissonic, a new approach to text-to-image generation that improves upon existing diffusion models like Stable Diffusion. Unlike autoregressive models, Meissonic uses non-autoregressive masked image modeling, which is more efficient and faster. The model incorporates advanced architectural designs, positional encoding, and optimized sampling to enhance image quality and resolution. Extensive testing shows that Meissonic not only matches but often surpasses current state-of-the-art models in generating high-resolution images.'}, 'zh': {'title': 'Meissonic：引领文本到图像生成的新标准', 'desc': '这篇论文介绍了一种名为Meissonic的新模型，它在非自回归的掩码图像建模中取得了显著进展。通过引入一系列架构创新、先进的位置编码策略和优化的采样条件，Meissonic大大提高了性能和效率。该模型利用高质量的训练数据和人类偏好评分的微条件，结合特征压缩层，提升了图像的清晰度和分辨率。实验结果表明，Meissonic在生成高质量、高分辨率图像方面不仅能与现有模型媲美，甚至在某些方面超越了它们。'}}}, {'id': 'https://huggingface.co/papers/2410.08815', 'title': 'StructRAG: Boosting Knowledge Intensive Reasoning of LLMs via Inference-time Hybrid Information Structurization', 'url': 'https://huggingface.co/papers/2410.08815', 'abstract': 'Retrieval-augmented generation (RAG) is a key means to effectively enhance large language models (LLMs) in many knowledge-based tasks. However, existing RAG methods struggle with knowledge-intensive reasoning tasks, because useful information required to these tasks are badly scattered. This characteristic makes it difficult for existing RAG methods to accurately identify key information and perform global reasoning with such noisy augmentation. In this paper, motivated by the cognitive theories that humans convert raw information into various structured knowledge when tackling knowledge-intensive reasoning, we proposes a new framework, StructRAG, which can identify the optimal structure type for the task at hand, reconstruct original documents into this structured format, and infer answers based on the resulting structure. Extensive experiments across various knowledge-intensive tasks show that StructRAG achieves state-of-the-art performance, particularly excelling in challenging scenarios, demonstrating its potential as an effective solution for enhancing LLMs in complex real-world applications.', 'score': 39, 'issue_id': 92, 'pub_date': '2024-10-11', 'pub_date_card': {'ru': '11 октября', 'en': 'October 11', 'zh': '10月11日'}, 'hash': '0b6586df53e81f2e', 'data': {'categories': ['#reasoning', '#optimization', '#architecture', '#rag'], 'emoji': '🧠', 'ru': {'title': 'StructRAG: структурированное знание для эффективного рассуждения', 'desc': 'StructRAG - новый фреймворк для улучшения работы больших языковых моделей в задачах, требующих интенсивного использования знаний. Он преодолевает ограничения существующих методов RAG, структурируя исходную информацию оптимальным для конкретной задачи образом. StructRAG идентифицирует наиболее подходящий тип структуры, реконструирует исходные документы в этот формат и делает выводы на основе полученной структуры. Эксперименты показывают, что StructRAG достигает передовых результатов, особенно в сложных сценариях.'}, 'en': {'title': 'StructRAG: Structuring Knowledge for Superior Reasoning', 'desc': 'The paper introduces StructRAG, a novel framework designed to improve retrieval-augmented generation (RAG) methods for large language models (LLMs) in knowledge-intensive reasoning tasks. Traditional RAG methods often struggle because the necessary information is scattered and noisy, making it hard to perform accurate reasoning. StructRAG addresses this by converting raw information into structured knowledge, allowing for better identification and reasoning of key information. Experiments show that StructRAG significantly enhances performance in complex tasks, setting a new standard for LLMs in real-world applications.'}, 'zh': {'title': 'StructRAG：结构化知识提升大语言模型', 'desc': '这篇论文介绍了一种名为StructRAG的新框架，用于改进大语言模型在知识密集型任务中的表现。现有的检索增强生成方法在处理这些任务时常常难以准确识别关键信息，因为信息分散且噪声较多。StructRAG通过将原始信息转换为结构化知识，帮助模型更好地进行全局推理。实验表明，StructRAG在多种复杂任务中表现优异，尤其在具有挑战性的场景中展现了其潜力。'}}}, {'id': 'https://huggingface.co/papers/2410.06456', 'title': 'From Generalist to Specialist: Adapting Vision Language Models via Task-Specific Visual Instruction Tuning', 'url': 'https://huggingface.co/papers/2410.06456', 'abstract': 'Large vision language models (VLMs) combine large language models with vision encoders, demonstrating promise across various tasks. However, they often underperform in task-specific applications due to domain gaps between pre-training and fine-tuning. We introduce VITask, a novel framework that enhances task-specific adaptability of VLMs by integrating task-specific models (TSMs). VITask employs three key strategies: exemplar prompting (EP), response distribution alignment (RDA), and contrastive response tuning (CRT) to improve the task-specific performance of VLMs by adjusting their response distributions. EP allows TSM features to guide VLMs, while RDA enables VLMs to adapt without TSMs during inference by learning from exemplar-prompted models. CRT further optimizes the ranking of correct image-response pairs, thereby reducing the risk of generating undesired responses. Experiments on 12 medical diagnosis datasets across 9 imaging modalities show that VITask outperforms both vanilla instruction-tuned VLMs and TSMs, showcasing its ability to integrate complementary features from both models effectively. Additionally, VITask offers practical advantages such as flexible TSM integration and robustness to incomplete instructions, making it a versatile and efficient solution for task-specific VLM tuning. Our code are available at https://github.com/baiyang4/VITask.', 'score': 36, 'issue_id': 91, 'pub_date': '2024-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': 'b9de6611a2cfd8cb', 'data': {'categories': ['#cv', '#training', '#healthcare', '#optimization', '#transfer_learning', '#open_source', '#architecture', '#multimodal'], 'emoji': '🔬', 'ru': {'title': 'VITask: Преодоление разрыва между предобучением и тонкой настройкой в визуально-языковых моделях', 'desc': 'VITask - новый фреймворк для улучшения адаптации крупных визуально-языковых моделей (VLM) к конкретным задачам. Он использует три стратегии: примерное подсказывание, выравнивание распределения ответов и контрастную настройку ответов. VITask интегрирует специализированные модели для конкретных задач, чтобы улучшить производительность VLM. Эксперименты на 12 наборах данных медицинской диагностики показали превосходство VITask над обычными VLM и специализированными моделями.'}, 'en': {'title': '"VITask: Bridging the Gap for Task-Specific Vision Language Models"', 'desc': 'The paper introduces VITask, a framework designed to improve the task-specific performance of large vision language models (VLMs) by integrating task-specific models (TSMs). VITask uses exemplar prompting, response distribution alignment, and contrastive response tuning to enhance adaptability and accuracy in specific tasks. These strategies help VLMs learn from task-specific features and improve their response accuracy without relying on TSMs during inference. Experiments demonstrate that VITask outperforms traditional VLMs and TSMs, especially in medical diagnosis tasks, by effectively combining features from both models.'}, 'zh': {'title': 'VITask：提升视觉语言模型任务适应性的创新框架', 'desc': '这篇论文介绍了一种名为VITask的新框架，旨在提高大型视觉语言模型（VLMs）在特定任务上的适应性。VITask通过整合任务特定模型（TSMs）来实现这一目标，并采用了示例提示、响应分布对齐和对比响应调优三种策略。实验表明，VITask在12个医学诊断数据集上表现优异，能够有效结合VLMs和TSMs的优势。此外，VITask还具有灵活的TSM集成和对不完整指令的鲁棒性，成为任务特定VLM调优的高效解决方案。'}}}, {'id': 'https://huggingface.co/papers/2410.08102', 'title': 'Multi-Agent Collaborative Data Selection for Efficient LLM Pretraining', 'url': 'https://huggingface.co/papers/2410.08102', 'abstract': 'Efficient data selection is crucial to accelerate the pretraining of large language models (LLMs). While various methods have been proposed to enhance data efficiency, limited research has addressed the inherent conflicts between these approaches to achieve optimal data selection for LLM pretraining. To tackle this problem, we propose a novel multi-agent collaborative data selection mechanism. In this framework, each data selection method serves as an independent agent, and an agent console is designed to dynamically integrate the information from all agents throughout the LLM training process. We conduct extensive empirical studies to evaluate our multi-agent framework. The experimental results demonstrate that our approach significantly improves data efficiency, accelerates convergence in LLM training, and achieves an average performance gain of 10.5% across multiple language model benchmarks compared to the state-of-the-art methods.', 'score': 19, 'issue_id': 93, 'pub_date': '2024-10-10', 'pub_date_card': {'ru': '10 октября', 'en': 'October 10', 'zh': '10月10日'}, 'hash': 'e6e97c0fdfa09a15', 'data': {'categories': ['#training', '#optimization', '#data', '#agents', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'Многоагентный подход к оптимизации данных для LLM', 'desc': 'Представлен новый механизм многоагентного совместного отбора данных для предобучения больших языковых моделей (LLM). Каждый метод отбора данных выступает в роли независимого агента, а консоль агентов динамически интегрирует информацию от всех агентов в процессе обучения LLM. Экспериментальные результаты показывают значительное повышение эффективности использования данных и ускорение сходимости при обучении LLM. Предложенный подход достигает среднего прироста производительности в 10.5% по сравнению с современными методами на нескольких эталонных тестах языковых моделей.'}, 'en': {'title': 'Collaborative Agents: Boosting Language Model Training Efficiency', 'desc': 'The paper introduces a new way to choose data for training large language models more efficiently by using a multi-agent system. Each data selection method acts as an independent agent, and a central console combines their insights to optimize the training process. This approach helps the model learn faster and better, showing a 10.5% improvement in performance over existing methods. The study highlights the importance of collaboration between different data selection strategies to enhance the pretraining of language models.'}, 'zh': {'title': '多智能体协作：提升大语言模型预训练效率的新方法', 'desc': '本文提出了一种多智能体协作的数据选择机制，以提高大语言模型的预训练效率。在这个框架中，每种数据选择方法都作为一个独立的智能体，通过一个控制台动态整合所有智能体的信息。实验结果表明，这种方法显著提高了数据效率，加速了大语言模型的训练收敛速度。与现有最先进的方法相比，我们的方法在多个语言模型基准上平均性能提升了10.5%。'}}}, {'id': 'https://huggingface.co/papers/2410.07133', 'title': 'EvolveDirector: Approaching Advanced Text-to-Image Generation with Large Vision-Language Models', 'url': 'https://huggingface.co/papers/2410.07133', 'abstract': 'Recent advancements in generation models have showcased remarkable capabilities in generating fantastic content. However, most of them are trained on proprietary high-quality data, and some models withhold their parameters and only provide accessible application programming interfaces (APIs), limiting their benefits for downstream tasks. To explore the feasibility of training a text-to-image generation model comparable to advanced models using publicly available resources, we introduce EvolveDirector. This framework interacts with advanced models through their public APIs to obtain text-image data pairs to train a base model. Our experiments with extensive data indicate that the model trained on generated data of the advanced model can approximate its generation capability. However, it requires large-scale samples of 10 million or more. This incurs significant expenses in time, computational resources, and especially the costs associated with calling fee-based APIs. To address this problem, we leverage pre-trained large vision-language models (VLMs) to guide the evolution of the base model. VLM continuously evaluates the base model during training and dynamically updates and refines the training dataset by the discrimination, expansion, deletion, and mutation operations. Experimental results show that this paradigm significantly reduces the required data volume. Furthermore, when approaching multiple advanced models, EvolveDirector can select the best samples generated by them to learn powerful and balanced abilities. The final trained model Edgen is demonstrated to outperform these advanced models. The code and model weights are available at https://github.com/showlab/EvolveDirector.', 'score': 18, 'issue_id': 90, 'pub_date': '2024-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': 'c49f4ef8183585ee', 'data': {'categories': ['#cv', '#training', '#data', '#transfer_learning', '#open_source', '#architecture', '#synthetic', '#multimodal'], 'emoji': '🎨', 'ru': {'title': 'Эволюционное обучение генеративных моделей на публичных данных', 'desc': 'Статья представляет EvolveDirector - фреймворк для обучения модели генерации изображений по тексту, сопоставимой с продвинутыми моделями, но используя только общедоступные ресурсы. Авторы используют API существующих моделей для получения пар текст-изображение и обучения базовой модели. Для уменьшения необходимого объема данных применяются предобученные мультимодальные модели, которые оценивают и корректируют датасет в процессе обучения. Результатом стала модель Edgen, превосходящая по возможностям исходные продвинутые модели.'}, 'en': {'title': 'EvolveDirector: Democratizing Advanced Text-to-Image Generation', 'desc': "The paper introduces EvolveDirector, a framework designed to train a text-to-image generation model using publicly available resources by interacting with advanced models through their APIs. It addresses the challenge of high costs and resource demands by employing pre-trained vision-language models to guide the training process, reducing the need for large datasets. The framework dynamically refines the training data through operations like discrimination and mutation, enhancing the model's learning efficiency. The resulting model, Edgen, not only approximates but also surpasses the capabilities of existing advanced models, demonstrating the potential of this innovative approach."}, 'zh': {'title': 'EvolveDirector：用公共资源实现高级生成能力', 'desc': 'EvolveDirector 是一个框架，通过与高级模型的公共 API 交互，获取文本-图像数据对来训练基础模型。实验表明，使用高级模型生成的数据训练的模型可以接近其生成能力，但需要大量样本。为解决这一问题，利用预训练的大型视觉语言模型（VLM）来指导基础模型的演化，显著减少所需数据量。最终训练的模型 Edgen 超越了这些高级模型。'}}}, {'id': 'https://huggingface.co/papers/2410.07656', 'title': 'Mechanistic Permutability: Match Features Across Layers', 'url': 'https://huggingface.co/papers/2410.07656', 'abstract': 'Understanding how features evolve across layers in deep neural networks is a fundamental challenge in mechanistic interpretability, particularly due to polysemanticity and feature superposition. While Sparse Autoencoders (SAEs) have been used to extract interpretable features from individual layers, aligning these features across layers has remained an open problem. In this paper, we introduce SAE Match, a novel, data-free method for aligning SAE features across different layers of a neural network. Our approach involves matching features by minimizing the mean squared error between the folded parameters of SAEs, a technique that incorporates activation thresholds into the encoder and decoder weights to account for differences in feature scales. Through extensive experiments on the Gemma 2 language model, we demonstrate that our method effectively captures feature evolution across layers, improving feature matching quality. We also show that features persist over several layers and that our approach can approximate hidden states across layers. Our work advances the understanding of feature dynamics in neural networks and provides a new tool for mechanistic interpretability studies.', 'score': 16, 'issue_id': 96, 'pub_date': '2024-10-10', 'pub_date_card': {'ru': '10 октября', 'en': 'October 10', 'zh': '10月10日'}, 'hash': '6a4c86357dc6667d', 'data': {'categories': ['#training', '#optimization', '#interpretability', '#architecture'], 'emoji': '🔍', 'ru': {'title': 'SAE Match: Новый взгляд на эволюцию признаков в глубоких нейронных сетях', 'desc': 'Статья представляет новый метод SAE Match для выравнивания признаков, извлеченных разреженными автоэнкодерами (SAE), между слоями нейронной сети. Метод минимизирует среднеквадратичную ошибку между свернутыми параметрами SAE, учитывая пороги активации. Эксперименты на языковой модели Gemma 2 показывают эффективность метода в отслеживании эволюции признаков. Исследование также демонстрирует, что признаки сохраняются на протяжении нескольких слоев.'}, 'en': {'title': 'Aligning Features Across Layers: A New Approach to Neural Network Interpretability', 'desc': 'This paper introduces SAE Match, a new method to align features across layers in deep neural networks using Sparse Autoencoders. The technique minimizes the mean squared error between folded parameters, incorporating activation thresholds to handle different feature scales. Experiments on the Gemma 2 language model show that SAE Match effectively tracks feature evolution and improves feature matching quality. This work enhances our understanding of feature dynamics and offers a tool for mechanistic interpretability in neural networks.'}, 'zh': {'title': '揭示神经网络特征演变的奥秘', 'desc': '这篇论文探讨了深度神经网络中特征在各层之间的演变，特别是多义性和特征叠加的问题。作者提出了一种名为SAE Match的新方法，通过最小化稀疏自编码器参数的均方误差来对齐不同层的特征。该方法在Gemma 2语言模型上的实验表明，它能有效捕捉特征在各层的演变，提高特征匹配质量。研究结果有助于理解神经网络中的特征动态，并为机械解释性研究提供了新工具。'}}}, {'id': 'https://huggingface.co/papers/2410.07035', 'title': 'PositionID: LLMs can Control Lengths, Copy and Paste with Explicit Positional Awareness', 'url': 'https://huggingface.co/papers/2410.07035', 'abstract': "Large Language Models (LLMs) demonstrate impressive capabilities across various domains, including role-playing, creative writing, mathematical reasoning, and coding. Despite these advancements, LLMs still encounter challenges with length control, frequently failing to adhere to specific length constraints due to their token-level operations and insufficient training on data with strict length limitations. We identify this issue as stemming from a lack of positional awareness and propose novel approaches--PositionID Prompting and PositionID Fine-Tuning--to address it. These methods enhance the model's ability to continuously monitor and manage text length during generation. Additionally, we introduce PositionID CP Prompting to enable LLMs to perform copy and paste operations accurately. Furthermore, we develop two benchmarks for evaluating length control and copy-paste abilities. Our experiments demonstrate that our methods significantly improve the model's adherence to length constraints and copy-paste accuracy without compromising response quality.", 'score': 16, 'issue_id': 91, 'pub_date': '2024-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': '20096ea1f7372f0f', 'data': {'categories': ['#reasoning', '#training', '#optimization', '#benchmark', '#architecture'], 'emoji': '📏', 'ru': {'title': 'Точный контроль длины текста в LLM с помощью позиционной осведомленности', 'desc': 'Исследователи предлагают новые методы для улучшения контроля длины текста в больших языковых моделях (LLM). Они вводят PositionID Prompting и PositionID Fine-Tuning для повышения позиционной осведомленности моделей. Также представлен метод PositionID CP Prompting для выполнения операций копирования и вставки. Разработаны два новых бенчмарка для оценки контроля длины и возможностей копирования-вставки.'}, 'en': {'title': 'Mastering Length: Enhancing LLMs with PositionID Techniques', 'desc': 'Large Language Models (LLMs) are powerful but struggle with controlling the length of their outputs due to token-level operations and insufficient training on length-specific data. This paper identifies the problem as a lack of positional awareness and introduces PositionID Prompting and PositionID Fine-Tuning to improve length management. Additionally, PositionID CP Prompting is developed to enhance copy-paste accuracy. The proposed methods are tested with new benchmarks, showing significant improvements in length control and copy-paste tasks without affecting the quality of responses.'}, 'zh': {'title': '提升大型语言模型的长度控制能力', 'desc': '大型语言模型在许多领域表现出色，但在控制文本长度方面仍然存在挑战。我们发现问题的根源在于模型缺乏位置感知能力，因此提出了PositionID提示和微调方法来解决。通过这些方法，模型可以更好地监控和管理生成文本的长度。此外，我们还引入了PositionID CP提示来提高复制粘贴操作的准确性。实验结果表明，这些方法显著提高了模型在长度控制和复制粘贴方面的表现。'}}}, {'id': 'https://huggingface.co/papers/2410.09008', 'title': 'SuperCorrect: Supervising and Correcting Language Models with Error-Driven Insights', 'url': 'https://huggingface.co/papers/2410.09008', 'abstract': "Large language models (LLMs) like GPT-4, PaLM, and LLaMA have shown significant improvements in various reasoning tasks. However, smaller models such as Llama-3-8B and DeepSeekMath-Base still struggle with complex mathematical reasoning because they fail to effectively identify and correct reasoning errors. Recent reflection-based methods aim to address these issues by enabling self-reflection and self-correction, but they still face challenges in independently detecting errors in their reasoning steps. To overcome these limitations, we propose SuperCorrect, a novel two-stage framework that uses a large teacher model to supervise and correct both the reasoning and reflection processes of a smaller student model. In the first stage, we extract hierarchical high-level and detailed thought templates from the teacher model to guide the student model in eliciting more fine-grained reasoning thoughts. In the second stage, we introduce cross-model collaborative direct preference optimization (DPO) to enhance the self-correction abilities of the student model by following the teacher's correction traces during training. This cross-model DPO approach teaches the student model to effectively locate and resolve erroneous thoughts with error-driven insights from the teacher model, breaking the bottleneck of its thoughts and acquiring new skills and knowledge to tackle challenging problems. Extensive experiments consistently demonstrate our superiority over previous methods. Notably, our SuperCorrect-7B model significantly surpasses powerful DeepSeekMath-7B by 7.8%/5.3% and Qwen2.5-Math-7B by 15.1%/6.3% on MATH/GSM8K benchmarks, achieving new SOTA performance among all 7B models. Code: https://github.com/YangLing0818/SuperCorrect-llm", 'score': 16, 'issue_id': 90, 'pub_date': '2024-10-11', 'pub_date_card': {'ru': '11 октября', 'en': 'October 11', 'zh': '10月11日'}, 'hash': '5c5ecb064656bbe6', 'data': {'categories': ['#reasoning', '#training', '#optimization', '#benchmark', '#open_source', '#rlhf', '#small_models', '#architecture'], 'emoji': '🧮', 'ru': {'title': "SuperCorrect: Подход 'учитель-ученик' для улучшения математических рассуждений малых языковых моделей", 'desc': 'SuperCorrect - это новый двухэтапный фреймворк для улучшения математических рассуждений маленьких языковых моделей. Он использует большую модель-учителя для руководства и коррекции процессов рассуждения и рефлексии меньшей модели-ученика. На первом этапе извлекаются иерархические шаблоны мыслей из модели-учителя. На втором этапе применяется кросс-модельная оптимизация прямых предпочтений для улучшения способностей модели-ученика к самокоррекции.'}, 'en': {'title': 'SuperCorrect: Elevating Small Models with Big Guidance', 'desc': "The paper introduces SuperCorrect, a two-stage framework designed to improve the reasoning and self-correction abilities of smaller language models by using a large teacher model for guidance. In the first stage, the teacher model provides hierarchical templates to help the student model develop more detailed reasoning processes. The second stage involves cross-model collaborative direct preference optimization, where the student model learns to identify and correct errors by following the teacher's correction patterns. This approach significantly enhances the performance of smaller models, as demonstrated by the SuperCorrect-7B model's superior results on mathematical reasoning benchmarks compared to other models of similar size."}, 'zh': {'title': 'SuperCorrect：小模型的推理与纠错新突破', 'desc': '大型语言模型在推理任务中表现出色，但小型模型在复杂数学推理上仍有困难。反思方法试图解决这个问题，但在独立检测推理错误上仍有挑战。SuperCorrect框架通过大模型指导小模型，提升其推理和自我纠正能力。实验表明，SuperCorrect在多个基准测试中表现优异，超越了之前的方法。'}}}, {'id': 'https://huggingface.co/papers/2410.09009', 'title': 'Semantic Score Distillation Sampling for Compositional Text-to-3D Generation', 'url': 'https://huggingface.co/papers/2410.09009', 'abstract': 'Generating high-quality 3D assets from textual descriptions remains a pivotal challenge in computer graphics and vision research. Due to the scarcity of 3D data, state-of-the-art approaches utilize pre-trained 2D diffusion priors, optimized through Score Distillation Sampling (SDS). Despite progress, crafting complex 3D scenes featuring multiple objects or intricate interactions is still difficult. To tackle this, recent methods have incorporated box or layout guidance. However, these layout-guided compositional methods often struggle to provide fine-grained control, as they are generally coarse and lack expressiveness. To overcome these challenges, we introduce a novel SDS approach, Semantic Score Distillation Sampling (SemanticSDS), designed to effectively improve the expressiveness and accuracy of compositional text-to-3D generation. Our approach integrates new semantic embeddings that maintain consistency across different rendering views and clearly differentiate between various objects and parts. These embeddings are transformed into a semantic map, which directs a region-specific SDS process, enabling precise optimization and compositional generation. By leveraging explicit semantic guidance, our method unlocks the compositional capabilities of existing pre-trained diffusion models, thereby achieving superior quality in 3D content generation, particularly for complex objects and scenes. Experimental results demonstrate that our SemanticSDS framework is highly effective for generating state-of-the-art complex 3D content. Code: https://github.com/YangLing0818/SemanticSDS-3D', 'score': 13, 'issue_id': 90, 'pub_date': '2024-10-11', 'pub_date_card': {'ru': '11 октября', 'en': 'October 11', 'zh': '10月11日'}, 'hash': '2c0887a19dd7fec9', 'data': {'categories': ['#cv', '#optimization', '#open_source', '#diffusion', '#architecture', '#3d'], 'emoji': '🎨', 'ru': {'title': 'SemanticSDS: Точный контроль над генерацией сложных 3D-сцен', 'desc': 'В статье представлен новый подход к генерации 3D-контента на основе текстовых описаний - Semantic Score Distillation Sampling (SemanticSDS). Метод использует семантические эмбеддинги для улучшения выразительности и точности композиционной генерации 3D-объектов. SemanticSDS трансформирует эмбеддинги в семантическую карту, которая направляет процесс оптимизации для каждого региона изображения. Эксперименты показывают, что предложенный подход позволяет создавать сложные 3D-сцены и объекты высокого качества.'}, 'en': {'title': 'Unlocking 3D Creativity: SemanticSDS for Detailed Text-to-3D Generation', 'desc': 'The paper addresses the challenge of generating high-quality 3D assets from text descriptions by introducing a new method called Semantic Score Distillation Sampling (SemanticSDS). This approach enhances the expressiveness and accuracy of text-to-3D generation by using semantic embeddings that ensure consistency across different views and distinguish between various objects. These embeddings are used to create a semantic map that guides a region-specific optimization process, allowing for precise and detailed 3D scene generation. The method significantly improves the quality of 3D content, especially for complex scenes, by leveraging existing pre-trained diffusion models with explicit semantic guidance.'}, 'zh': {'title': '语义引导，提升3D生成精度', 'desc': '这篇论文探讨了如何从文本描述生成高质量的3D资产，这是计算机图形学和视觉研究中的一个重要挑战。由于3D数据稀缺，现有方法利用预训练的2D扩散先验，通过得分蒸馏采样（SDS）进行优化。为了提高复杂3D场景的生成能力，作者提出了一种新的语义得分蒸馏采样（SemanticSDS）方法。该方法通过引入新的语义嵌入，能够在不同渲染视图中保持一致性，并清晰区分不同对象和部分，从而实现更精确的优化和组合生成。'}}}, {'id': 'https://huggingface.co/papers/2410.08391', 'title': 'KV Prediction for Improved Time to First Token', 'url': 'https://huggingface.co/papers/2410.08391', 'abstract': "Inference with transformer-based language models begins with a prompt processing step. In this step, the model generates the first output token and stores the KV cache needed for future generation steps. This prompt processing step can be computationally expensive, taking 10s of seconds or more for billion-parameter models on edge devices when prompt lengths or batch sizes rise. This degrades user experience by introducing significant latency into the model's outputs. To reduce the time spent producing the first output (known as the ``time to first token'', or TTFT) of a pretrained model, we introduce a novel method called KV Prediction. In our method, a small auxiliary model is used to process the prompt and produce an approximation of the KV cache used by a base model. This approximated KV cache is then used with the base model for autoregressive generation without the need to query the auxiliary model again. We demonstrate that our method produces a pareto-optimal efficiency-accuracy trade-off when compared to baselines. On TriviaQA, we demonstrate relative accuracy improvements in the range of 15%-50% across a range of TTFT FLOPs budgets. We also demonstrate accuracy improvements of up to 30% on HumanEval python code completion at fixed TTFT FLOPs budgets. Additionally, we benchmark models on an Apple M2 Pro CPU and demonstrate that our improvement in FLOPs translates to a TTFT speedup on hardware. We release our code at https://github.com/apple/corenet/tree/main/projects/kv-prediction .", 'score': 11, 'issue_id': 91, 'pub_date': '2024-10-10', 'pub_date_card': {'ru': '10 октября', 'en': 'October 10', 'zh': '10月10日'}, 'hash': '447ad32759dde03e', 'data': {'categories': ['#inference', '#optimization', '#plp', '#benchmark', '#open_source', '#small_models'], 'emoji': '⚡', 'ru': {'title': 'KV Prediction: молниеносный старт для языковых моделей', 'desc': 'Статья представляет новый метод под названием KV Prediction для ускорения вывода трансформерных языковых моделей. Метод использует вспомогательную модель для аппроксимации KV-кэша основной модели, что значительно сокращает время до первого токена (TTFT). Авторы демонстрируют улучшение точности на 15-50% на наборе данных TriviaQA и до 30% на HumanEval при фиксированном бюджете FLOPS для TTFT. Эксперименты на CPU Apple M2 Pro подтверждают ускорение TTFT на реальном оборудовании.'}, 'en': {'title': 'Speed Up Your AI: Faster First Outputs with KV Prediction', 'desc': 'The paper introduces a method called KV Prediction to reduce the time to first token (TTFT) in transformer-based language models. This method uses a small auxiliary model to approximate the key-value (KV) cache, which speeds up the initial output generation without repeatedly querying the auxiliary model. The approach achieves a balance between efficiency and accuracy, showing significant improvements in tasks like TriviaQA and HumanEval. The method also demonstrates hardware speedup on an Apple M2 Pro CPU, enhancing user experience by reducing latency.'}, 'zh': {'title': 'KV预测：加速生成的创新方法', 'desc': '这篇论文介绍了一种名为KV预测的新方法，用于减少预训练模型生成第一个输出的时间。通过使用一个小型辅助模型来近似生成KV缓存，从而加快生成速度。实验表明，该方法在效率和准确性之间达到了帕累托最优的平衡。在TriviaQA和HumanEval等任务中，方法表现出显著的准确性提升。'}}}, {'id': 'https://huggingface.co/papers/2410.06264', 'title': 'Think While You Generate: Discrete Diffusion with Planned Denoising', 'url': 'https://huggingface.co/papers/2410.06264', 'abstract': 'Discrete diffusion has achieved state-of-the-art performance, outperforming or approaching autoregressive models on standard benchmarks. In this work, we introduce Discrete Diffusion with Planned Denoising (DDPD), a novel framework that separates the generation process into two models: a planner and a denoiser. At inference time, the planner selects which positions to denoise next by identifying the most corrupted positions in need of denoising, including both initially corrupted and those requiring additional refinement. This plan-and-denoise approach enables more efficient reconstruction during generation by iteratively identifying and denoising corruptions in the optimal order. DDPD outperforms traditional denoiser-only mask diffusion methods, achieving superior results on language modeling benchmarks such as text8, OpenWebText, and token-based generation on ImageNet 256 times 256. Notably, in language modeling, DDPD significantly reduces the performance gap between diffusion-based and autoregressive methods in terms of generative perplexity. Code is available at https://github.com/liusulin/DDPD.', 'score': 9, 'issue_id': 120, 'pub_date': '2024-10-08', 'pub_date_card': {'ru': '8 октября', 'en': 'October 8', 'zh': '10月8日'}, 'hash': '2e52dacdaf42baa1', 'data': {'categories': ['#inference', '#benchmark', '#open_source', '#diffusion', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Плановое шумоподавление для улучшения дискретной диффузии', 'desc': 'В статье представлен новый подход к дискретной диффузии под названием DDPD. Эта модель разделяет процесс генерации на две части: планировщик и шумоподавитель. Планировщик выбирает, какие позиции нужно обработать в первую очередь, определяя наиболее искаженные участки. Такой подход позволяет более эффективно восстанавливать данные во время генерации и превосходит традиционные методы диффузии с маскированием.'}, 'en': {'title': 'Efficient Data Generation with Planned Denoising', 'desc': 'This paper presents a new method called Discrete Diffusion with Planned Denoising (DDPD) that enhances the process of generating data. DDPD uses two models: a planner that decides which parts of the data need fixing and a denoiser that corrects those parts. By focusing on the most corrupted areas first, DDPD improves the efficiency of data reconstruction. The results show that DDPD performs better than traditional methods on various language modeling tasks, narrowing the gap between diffusion models and autoregressive models.'}, 'zh': {'title': '计划去噪，提升生成效率！', 'desc': '离散扩散模型在标准基准测试中表现出色，超越或接近自回归模型。本文提出了一种新的框架——计划去噪的离散扩散（DDPD），将生成过程分为两个模型：规划者和去噪器。在推理时，规划者通过识别最需要去噪的位置来选择下一个去噪的目标，从而实现更高效的重建。DDPD在语言建模基准测试中表现优异，显著缩小了扩散模型与自回归方法之间的性能差距。'}}}, {'id': 'https://huggingface.co/papers/2410.08168', 'title': 'ZeroComp: Zero-shot Object Compositing from Image Intrinsics via Diffusion', 'url': 'https://huggingface.co/papers/2410.08168', 'abstract': 'We present ZeroComp, an effective zero-shot 3D object compositing approach that does not require paired composite-scene images during training. Our method leverages ControlNet to condition from intrinsic images and combines it with a Stable Diffusion model to utilize its scene priors, together operating as an effective rendering engine. During training, ZeroComp uses intrinsic images based on geometry, albedo, and masked shading, all without the need for paired images of scenes with and without composite objects. Once trained, it seamlessly integrates virtual 3D objects into scenes, adjusting shading to create realistic composites. We developed a high-quality evaluation dataset and demonstrate that ZeroComp outperforms methods using explicit lighting estimations and generative techniques in quantitative and human perception benchmarks. Additionally, ZeroComp extends to real and outdoor image compositing, even when trained solely on synthetic indoor data, showcasing its effectiveness in image compositing.', 'score': 7, 'issue_id': 98, 'pub_date': '2024-10-10', 'pub_date_card': {'ru': '10 октября', 'en': 'October 10', 'zh': '10月10日'}, 'hash': '8857b485f78603ed', 'data': {'categories': ['#dataset', '#cv', '#benchmark', '#diffusion', '#architecture', '#synthetic', '#3d'], 'emoji': '🎭', 'ru': {'title': 'Реалистичная композиция 3D-объектов без парных данных', 'desc': 'ZeroComp — это эффективный подход к композиции 3D-объектов с нулевым обучением, не требующий парных изображений сцен во время тренировки. Метод использует ControlNet для обусловливания по внутренним изображениям и комбинирует его с моделью Stable Diffusion для использования её априорных знаний о сценах. ZeroComp обучается на внутренних изображениях, основанных на геометрии, альбедо и маскированном затенении. После обучения метод может реалистично интегрировать виртуальные 3D-объекты в сцены, корректируя затенение.'}, 'en': {'title': 'ZeroComp: Mastering 3D Object Integration Without Paired Data', 'desc': 'ZeroComp is a novel method for integrating 3D objects into images without needing paired training data. It uses ControlNet to guide the process with intrinsic images and combines it with Stable Diffusion to leverage scene knowledge. The approach focuses on geometry, albedo, and masked shading to create realistic composites without explicit lighting data. ZeroComp excels in both synthetic and real-world scenarios, outperforming traditional methods in quality and perception tests.'}, 'zh': {'title': 'ZeroComp：无需配对图像的3D对象合成新突破', 'desc': 'ZeroComp是一种无需配对图像的零样本3D对象合成方法。它利用ControlNet从内在图像中获取条件，并结合稳定扩散模型来使用场景先验知识。训练时，ZeroComp使用基于几何、反照率和遮罩阴影的内在图像，无需配对的场景图像。经过训练后，它可以无缝地将虚拟3D对象整合到场景中，调整阴影以创建逼真的合成效果。'}}}, {'id': 'https://huggingface.co/papers/2410.07536', 'title': 'I-Max: Maximize the Resolution Potential of Pre-trained Rectified Flow Transformers with Projected Flow', 'url': 'https://huggingface.co/papers/2410.07536', 'abstract': "Rectified Flow Transformers (RFTs) offer superior training and inference efficiency, making them likely the most viable direction for scaling up diffusion models. However, progress in generation resolution has been relatively slow due to data quality and training costs. Tuning-free resolution extrapolation presents an alternative, but current methods often reduce generative stability, limiting practical application. In this paper, we review existing resolution extrapolation methods and introduce the I-Max framework to maximize the resolution potential of Text-to-Image RFTs. I-Max features: (i) a novel Projected Flow strategy for stable extrapolation and (ii) an advanced inference toolkit for generalizing model knowledge to higher resolutions. Experiments with Lumina-Next-2K and Flux.1-dev demonstrate I-Max's ability to enhance stability in resolution extrapolation and show that it can bring image detail emergence and artifact correction, confirming the practical value of tuning-free resolution extrapolation.", 'score': 5, 'issue_id': 99, 'pub_date': '2024-10-10', 'pub_date_card': {'ru': '10 октября', 'en': 'October 10', 'zh': '10月10日'}, 'hash': '2d11f82cc2214114', 'data': {'categories': ['#survey', '#cv', '#training', '#inference', '#optimization', '#diffusion', '#architecture'], 'emoji': '🔍', 'ru': {'title': 'I-Max: Революция в экстраполяции разрешения для генерации изображений', 'desc': 'Статья представляет новый фреймворк I-Max для улучшения экстраполяции разрешения в трансформерах с выпрямленным потоком (RFT) для генерации изображений по тексту. I-Max включает стратегию Projected Flow для стабильной экстраполяции и продвинутый инструментарий для вывода, позволяющий обобщать знания модели на более высокие разрешения. Эксперименты показывают, что I-Max повышает стабильность экстраполяции разрешения и улучшает детализацию изображений. Авторы утверждают, что этот подход демонстрирует практическую ценность экстраполяции разрешения без дополнительного обучения.'}, 'en': {'title': '"Boosting Image Resolution: The I-Max Revolution in RFTs"', 'desc': 'The paper introduces Rectified Flow Transformers (RFTs) as a promising approach for improving the efficiency of training and inference in diffusion models. It highlights the challenges of increasing generation resolution due to data quality and training costs, and critiques current tuning-free resolution extrapolation methods for their instability. The authors propose the I-Max framework, which includes a Projected Flow strategy and an advanced inference toolkit, to enhance the resolution potential of Text-to-Image RFTs. Experiments demonstrate that I-Max improves stability and detail in image generation, making tuning-free resolution extrapolation more practical.'}, 'zh': {'title': '提升分辨率的I-Max框架：稳定且无需调参', 'desc': '这篇论文介绍了一种新的方法来提高文本到图像生成模型的分辨率，称为I-Max框架。I-Max框架通过引入投影流策略，解决了现有方法在分辨率外推时的不稳定性问题。实验表明，I-Max不仅能提高生成图像的细节和修正瑕疵，还能在不需要调参的情况下实现高分辨率生成。该方法在Lumina-Next-2K和Flux.1-dev上的实验验证了其实际应用价值。'}}}, {'id': 'https://huggingface.co/papers/2410.09038', 'title': 'SimpleStrat: Diversifying Language Model Generation with Stratification', 'url': 'https://huggingface.co/papers/2410.09038', 'abstract': "Generating diverse responses from large language models (LLMs) is crucial for applications such as planning/search and synthetic data generation, where diversity provides distinct answers across generations. Prior approaches rely on increasing temperature to increase diversity. However, contrary to popular belief, we show not only does this approach produce lower quality individual generations as temperature increases, but it depends on model's next-token probabilities being similar to the true distribution of answers. We propose , an alternative approach that uses the language model itself to partition the space into strata. At inference, a random stratum is selected and a sample drawn from within the strata. To measure diversity, we introduce CoverageQA, a dataset of underspecified questions with multiple equally plausible answers, and assess diversity by measuring KL Divergence between the output distribution and uniform distribution over valid ground truth answers. As computing probability per response/solution for proprietary models is infeasible, we measure recall on ground truth solutions. Our evaluation show using SimpleStrat achieves higher recall by 0.05 compared to GPT-4o and 0.36 average reduction in KL Divergence compared to Llama 3.", 'score': 4, 'issue_id': 102, 'pub_date': '2024-10-11', 'pub_date_card': {'ru': '11 октября', 'en': 'October 11', 'zh': '10月11日'}, 'hash': '06e284b0650ef57d', 'data': {'categories': ['#dataset', '#inference', '#optimization', '#architecture', '#synthetic'], 'emoji': '🎭', 'ru': {'title': 'SimpleStrat: новый подход к разнообразной генерации текста', 'desc': 'В статье представлен новый метод генерации разнообразных ответов от больших языковых моделей (LLM) под названием SimpleStrat. Авторы показывают, что традиционный подход увеличения температуры для повышения разнообразия имеет недостатки. SimpleStrat использует саму языковую модель для разделения пространства на страты, из которых затем выбирается случайная страта для генерации ответа. Для оценки разнообразия был создан датасет CoverageQA с недоопределенными вопросами, имеющими несколько равновероятных ответов. Эксперименты показали, что SimpleStrat достигает более высокого recall и меньшего KL-расхождения по сравнению с базовыми методами.'}, 'en': {'title': 'SimpleStrat: Boosting Diversity Without Sacrificing Quality', 'desc': 'The paper discusses a new method for generating diverse responses from large language models, which is important for tasks like planning and synthetic data generation. Traditional methods increase the temperature to boost diversity, but this can lower the quality of responses. The authors propose a novel approach called SimpleStrat, which uses the model to divide the response space into different sections and randomly selects from these sections to generate diverse outputs. They introduce a new dataset, CoverageQA, to measure diversity and show that SimpleStrat improves recall and reduces KL Divergence compared to other models.'}, 'zh': {'title': '用语言模型自身提升响应多样性', 'desc': '这篇论文探讨了如何从大型语言模型中生成多样化的响应，这对于规划、搜索和合成数据生成等应用至关重要。传统方法通过增加温度来提高多样性，但这可能导致生成质量下降。作者提出了一种新方法，利用语言模型自身将空间划分为不同的层次，并从中随机选择一个层次进行采样。通过引入CoverageQA数据集，作者证明了这种方法在多样性和召回率上优于现有方法。'}}}, {'id': 'https://huggingface.co/papers/2410.09045', 'title': 'MiRAGeNews: Multimodal Realistic AI-Generated News Detection', 'url': 'https://huggingface.co/papers/2410.09045', 'abstract': 'The proliferation of inflammatory or misleading "fake" news content has become increasingly common in recent years. Simultaneously, it has become easier than ever to use AI tools to generate photorealistic images depicting any scene imaginable. Combining these two -- AI-generated fake news content -- is particularly potent and dangerous. To combat the spread of AI-generated fake news, we propose the MiRAGeNews Dataset, a dataset of 12,500 high-quality real and AI-generated image-caption pairs from state-of-the-art generators. We find that our dataset poses a significant challenge to humans (60% F-1) and state-of-the-art multi-modal LLMs (< 24% F-1). Using our dataset we train a multi-modal detector (MiRAGe) that improves by +5.1% F-1 over state-of-the-art baselines on image-caption pairs from out-of-domain image generators and news publishers. We release our code and data to aid future work on detecting AI-generated content.', 'score': 4, 'issue_id': 101, 'pub_date': '2024-10-11', 'pub_date_card': {'ru': '11 октября', 'en': 'October 11', 'zh': '10月11日'}, 'hash': '0fc6dec55a36d111', 'data': {'categories': ['#dataset', '#cv', '#security', '#ethics', '#open_source', '#multimodal'], 'emoji': '🕵️', 'ru': {'title': 'MiRAGeNews: Борьба с дезинформацией в эпоху ИИ-генерации', 'desc': 'Статья представляет набор данных MiRAGeNews, состоящий из 12 500 пар изображений и подписей, как реальных, так и сгенерированных ИИ. Авторы обнаружили, что этот датасет представляет значительную сложность как для людей (60% F1-меры), так и для современных мультимодальных языковых моделей (<24% F1-меры). На основе этого набора данных был обучен мультимодальный детектор MiRAGe, который превосходит современные базовые модели на 5,1% по F1-мере при работе с парами изображение-подпись из сторонних генераторов изображений и новостных издателей. Код и данные опубликованы для содействия будущим исследованиям по обнаружению контента, сгенерированного ИИ.'}, 'en': {'title': 'Detecting the Mirage: Battling AI-Generated Fake News', 'desc': 'The paper addresses the growing issue of AI-generated fake news by introducing the MiRAGeNews Dataset, which includes 12,500 image-caption pairs from both real and AI-generated sources. This dataset is challenging for both humans and advanced AI models to accurately classify, highlighting the difficulty in detecting AI-generated content. The authors developed a multi-modal detector called MiRAGe, which outperforms existing models by 5.1% in F-1 score on unseen data. By releasing their dataset and code, they aim to support further research in identifying AI-generated news content.'}, 'zh': {'title': 'MiRAGeNews：打击AI生成虚假新闻的利器', 'desc': '近年来，虚假新闻的传播变得越来越普遍，而使用人工智能工具生成逼真图像也变得更加容易。为了应对这种结合了AI生成的虚假新闻的威胁，我们提出了MiRAGeNews数据集，其中包含12,500对高质量的真实和AI生成的图像-标题对。我们的研究发现，这个数据集对人类和最先进的多模态大语言模型都构成了显著挑战。通过使用该数据集，我们训练了一个多模态检测器MiRAGe，在检测跨领域图像生成器和新闻发布者的图像-标题对时，性能提高了5.1%。'}}}, {'id': 'https://huggingface.co/papers/2410.09037', 'title': 'Mentor-KD: Making Small Language Models Better Multi-step Reasoners', 'url': 'https://huggingface.co/papers/2410.09037', 'abstract': "Large Language Models (LLMs) have displayed remarkable performances across various complex tasks by leveraging Chain-of-Thought (CoT) prompting. Recently, studies have proposed a Knowledge Distillation (KD) approach, reasoning distillation, which transfers such reasoning ability of LLMs through fine-tuning language models of multi-step rationales generated by LLM teachers. However, they have inadequately considered two challenges regarding insufficient distillation sets from the LLM teacher model, in terms of 1) data quality and 2) soft label provision. In this paper, we propose Mentor-KD, which effectively distills the multi-step reasoning capability of LLMs to smaller LMs while addressing the aforementioned challenges. Specifically, we exploit a mentor, intermediate-sized task-specific fine-tuned model, to augment additional CoT annotations and provide soft labels for the student model during reasoning distillation. We conduct extensive experiments and confirm Mentor-KD's effectiveness across various models and complex reasoning tasks.", 'score': 4, 'issue_id': 97, 'pub_date': '2024-10-11', 'pub_date_card': {'ru': '11 октября', 'en': 'October 11', 'zh': '10月11日'}, 'hash': '2a269c59131978ed', 'data': {'categories': ['#reasoning', '#training', '#rl', '#optimization', '#transfer_learning', '#small_models'], 'emoji': '🧠', 'ru': {'title': 'Mentor-KD: Эффективная передача навыков рассуждения от больших языковых моделей к меньшим', 'desc': 'Статья представляет новый метод под названием Mentor-KD для эффективной дистилляции способности к многошаговым рассуждениям от больших языковых моделей (LLM) к меньшим моделям. Метод использует промежуточную модель-наставника для расширения набора аннотаций Chain-of-Thought и предоставления мягких меток для модели-ученика. Авторы решают проблемы, связанные с качеством данных и предоставлением мягких меток при дистилляции рассуждений. Эксперименты подтверждают эффективность Mentor-KD для различных моделей и сложных задач рассуждения.'}, 'en': {'title': 'Mentor-KD: Elevating Small Models with Big Reasoning Skills', 'desc': 'The paper introduces Mentor-KD, a method to improve the transfer of reasoning skills from large language models (LLMs) to smaller models. It addresses challenges in knowledge distillation, such as the quality of data and the provision of soft labels, by using an intermediate-sized mentor model. This mentor model generates additional Chain-of-Thought annotations and soft labels to enhance the learning process of the student model. Experiments show that Mentor-KD effectively enhances the reasoning capabilities of smaller models across different tasks.'}, 'zh': {'title': 'Mentor-KD：提升小模型推理能力的新方法', 'desc': '这篇论文探讨了如何通过知识蒸馏技术将大型语言模型的多步推理能力传递给较小的模型。研究中提出了一种名为Mentor-KD的方法，通过使用中等大小的任务特定模型来增强数据质量和提供软标签。Mentor-KD有效地解决了以往方法中数据质量和软标签不足的问题。实验结果表明，Mentor-KD在多种模型和复杂推理任务中表现出色。'}}}, {'id': 'https://huggingface.co/papers/2410.07331', 'title': 'DA-Code: Agent Data Science Code Generation Benchmark for Large Language Models', 'url': 'https://huggingface.co/papers/2410.07331', 'abstract': 'We introduce DA-Code, a code generation benchmark specifically designed to assess LLMs on agent-based data science tasks. This benchmark features three core elements: First, the tasks within DA-Code are inherently challenging, setting them apart from traditional code generation tasks and demanding advanced coding skills in grounding and planning. Second, examples in DA-Code are all based on real and diverse data, covering a wide range of complex data wrangling and analytics tasks. Third, to solve the tasks, the models must utilize complex data science programming languages, to perform intricate data processing and derive the answers. We set up the benchmark in a controllable and executable environment that aligns with real-world data analysis scenarios and is scalable. The annotators meticulously design the evaluation suite to ensure the accuracy and robustness of the evaluation. We develop the DA-Agent baseline. Experiments show that although the baseline performs better than other existing frameworks, using the current best LLMs achieves only 30.5% accuracy, leaving ample room for improvement. We release our benchmark at https://da-code-bench.github.io.', 'score': 4, 'issue_id': 95, 'pub_date': '2024-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': 'c2d3d40ef1bad864', 'data': {'categories': ['#reasoning', '#data', '#plp', '#agents', '#benchmark', '#games', '#open_source'], 'emoji': '📊', 'ru': {'title': 'DA-Code: Новый рубеж в оценке ИИ для анализа данных', 'desc': 'DA-Code - это новый бенчмарк для оценки способностей языковых моделей в задачах генерации кода для анализа данных. Он включает сложные задачи, требующие продвинутых навыков кодирования, основан на реальных разнообразных данных и охватывает широкий спектр задач по обработке и анализу данных. Бенчмарк создан в контролируемой исполняемой среде, имитирующей реальные сценарии анализа данных. Эксперименты показали, что даже лучшие современные языковые модели достигают точности лишь 30.5% на этом бенчмарке.'}, 'en': {'title': 'Pushing LLMs to Master Data Science Challenges', 'desc': 'DA-Code is a new benchmark designed to test large language models (LLMs) on complex data science tasks that require advanced coding skills. It includes challenging tasks based on real-world data, focusing on data wrangling and analytics. The benchmark requires models to use sophisticated programming languages to process data and find solutions. Initial tests show that even the best current models only achieve 30.5% accuracy, indicating significant potential for improvement.'}, 'zh': {'title': 'DA-Code：挑战大型语言模型的数据科学任务', 'desc': 'DA-Code 是一个专门为评估大型语言模型在基于代理的数据科学任务上的代码生成基准。这个基准包含三个核心元素：首先，DA-Code 中的任务具有挑战性，需要高级编码技能。其次，DA-Code 的例子基于真实多样的数据，涵盖复杂的数据处理和分析任务。最后，模型需要使用复杂的数据科学编程语言来解决任务。'}}}, {'id': 'https://huggingface.co/papers/2410.08193', 'title': 'GenARM: Reward Guided Generation with Autoregressive Reward Model for Test-time Alignment', 'url': 'https://huggingface.co/papers/2410.08193', 'abstract': 'Large Language Models (LLMs) exhibit impressive capabilities but require careful alignment with human preferences. Traditional training-time methods finetune LLMs using human preference datasets but incur significant training costs and require repeated training to handle diverse user preferences. Test-time alignment methods address this by using reward models (RMs) to guide frozen LLMs without retraining. However, existing test-time approaches rely on trajectory-level RMs which are designed to evaluate complete responses, making them unsuitable for autoregressive text generation that requires computing next-token rewards from partial responses. To address this, we introduce GenARM, a test-time alignment approach that leverages the Autoregressive Reward Model--a novel reward parametrization designed to predict next-token rewards for efficient and effective autoregressive generation. Theoretically, we demonstrate that this parametrization can provably guide frozen LLMs toward any distribution achievable by traditional RMs within the KL-regularized reinforcement learning framework. Experimental results show that GenARM significantly outperforms prior test-time alignment baselines and matches the performance of training-time methods. Additionally, GenARM enables efficient weak-to-strong guidance, aligning larger LLMs with smaller RMs without the high costs of training larger models. Furthermore, GenARM supports multi-objective alignment, allowing real-time trade-offs between preference dimensions and catering to diverse user preferences without retraining.', 'score': 3, 'issue_id': 102, 'pub_date': '2024-10-10', 'pub_date_card': {'ru': '10 октября', 'en': 'October 10', 'zh': '10月10日'}, 'hash': 'ebce9bee397ae5dd', 'data': {'categories': ['#training', '#inference', '#rl', '#optimization', '#alignment', '#rlhf'], 'emoji': '🎯', 'ru': {'title': 'GenARM: Эффективное тестовое выравнивание LLM без переобучения', 'desc': 'Статья представляет GenARM - новый подход к тестовому выравниванию больших языковых моделей (LLM) с использованием авторегрессивных моделей вознаграждения. Этот метод позволяет эффективно направлять замороженные LLM к желаемому распределению без переобучения. Экспериментальные результаты показывают, что GenARM превосходит существующие методы тестового выравнивания и соответствует производительности методов обучения. Подход также обеспечивает эффективное руководство от слабого к сильному и поддерживает многоцелевое выравнивание для удовлетворения разнообразных предпочтений пользователей.'}, 'en': {'title': 'Efficiently Aligning Language Models with GenARM: No Retraining Needed!', 'desc': 'The paper introduces GenARM, a novel method for aligning large language models (LLMs) with human preferences at test time without retraining. GenARM uses an Autoregressive Reward Model to predict next-token rewards, making it suitable for autoregressive text generation. This approach allows frozen LLMs to be guided efficiently, matching the performance of traditional training-time methods while avoiding high retraining costs. GenARM also supports multi-objective alignment, enabling real-time adjustments to cater to diverse user preferences.'}, 'zh': {'title': 'GenARM：高效对齐大型语言模型的新方法', 'desc': '大型语言模型（LLMs）功能强大，但需要与人类偏好对齐。传统方法在训练时微调模型，但成本高且需多次训练。GenARM是一种测试时对齐方法，使用自回归奖励模型来预测下一个词的奖励。实验表明，GenARM在效率和效果上优于现有方法，并支持多目标对齐。'}}}, {'id': 'https://huggingface.co/papers/2410.08612', 'title': 'Synth-SONAR: Sonar Image Synthesis with Enhanced Diversity and Realism via Dual Diffusion Models and GPT Prompting', 'url': 'https://huggingface.co/papers/2410.08612', 'abstract': 'Sonar image synthesis is crucial for advancing applications in underwater exploration, marine biology, and defence. Traditional methods often rely on extensive and costly data collection using sonar sensors, jeopardizing data quality and diversity. To overcome these limitations, this study proposes a new sonar image synthesis framework, Synth-SONAR leveraging diffusion models and GPT prompting. The key novelties of Synth-SONAR are threefold: First, by integrating Generative AI-based style injection techniques along with publicly available real/simulated data, thereby producing one of the largest sonar data corpus for sonar research. Second, a dual text-conditioning sonar diffusion model hierarchy synthesizes coarse and fine-grained sonar images with enhanced quality and diversity. Third, high-level (coarse) and low-level (detailed) text-based sonar generation methods leverage advanced semantic information available in visual language models (VLMs) and GPT-prompting. During inference, the method generates diverse and realistic sonar images from textual prompts, bridging the gap between textual descriptions and sonar image generation. This marks the application of GPT-prompting in sonar imagery for the first time, to the best of our knowledge. Synth-SONAR achieves state-of-the-art results in producing high-quality synthetic sonar datasets, significantly enhancing their diversity and realism.', 'score': 1, 'issue_id': 104, 'pub_date': '2024-10-11', 'pub_date_card': {'ru': '11 октября', 'en': 'October 11', 'zh': '10月11日'}, 'hash': '8000e8d6f78ec110', 'data': {'categories': ['#science', '#audio', '#dataset', '#cv', '#inference', '#data', '#diffusion', '#architecture', '#synthetic', '#multimodal'], 'emoji': '🌊', 'ru': {'title': 'Революция в синтезе сонарных изображений с помощью ИИ', 'desc': 'Статья представляет новый фреймворк Synth-SONAR для синтеза сонарных изображений, используя диффузионные модели и GPT-промптинг. Ключевые особенности включают интеграцию методов генеративного ИИ для инжекции стиля, двухуровневую иерархию диффузионных моделей для синтеза изображений разной детализации, а также использование визуальных языковых моделей и GPT-промптинга для генерации текстовых описаний. Synth-SONAR позволяет создавать разнообразные и реалистичные сонарные изображения на основе текстовых запросов, что является первым применением GPT-промптинга в области сонарных изображений.'}, 'en': {'title': '"Revolutionizing Sonar Imagery with AI: Synth-SONAR\'s Breakthrough"', 'desc': 'The paper introduces Synth-SONAR, a novel framework for generating synthetic sonar images using diffusion models and GPT prompting. By integrating Generative AI techniques with existing data, it creates a large and diverse sonar data corpus. The framework employs a dual text-conditioning model to produce high-quality, varied sonar images from textual descriptions. This approach marks the first use of GPT-prompting in sonar imagery, achieving state-of-the-art results in dataset quality and diversity.'}, 'zh': {'title': 'Synth-SONAR：用GPT提示革新声呐图像合成', 'desc': '这篇论文介绍了一种新的声呐图像合成框架，名为Synth-SONAR，利用扩散模型和GPT提示技术。该框架通过生成式AI风格注入技术和公开的真实/模拟数据，创建了一个大型声呐数据集。它采用双重文本条件的声呐扩散模型层次结构，生成高质量和多样化的粗粒度和细粒度声呐图像。该方法首次在声呐图像生成中应用GPT提示，显著提高了合成声呐数据集的多样性和真实性。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (2)', '#agi', '#alignment (3)', '#architecture (14)', '#audio (1)', '#benchmark (8)', '#cv (8)', '#data (5)', '#dataset (4)', '#diffusion (6)', '#ethics (1)', '#games (1)', '#graphs', '#hallucinations', '#healthcare (1)', '#inference (6)', '#interpretability (1)', '#leakage', '#long_context', '#low_resource', '#machine_translation', '#math', '#multilingual', '#multimodal (5)', '#open_source (10)', '#optimization (13)', '#plp (2)', '#rag (1)', '#reasoning (5)', '#rl (2)', '#rlhf (2)', '#robotics', '#science (1)', '#security (1)', '#small_models (4)', '#story_generation', '#survey (1)', '#synthetic (4)', '#training (11)', '#transfer_learning (3)', '#video'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="background-digit">${index + 1}</div>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2024-10-14 09:00',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-10-14 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-10-14 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    