{
    "date": {
        "ru": "24 октября",
        "en": "October 24",
        "zh": "10月24日"
    },
    "time_utc": "2024-10-24 09:00",
    "weekday": 3,
    "issue_id": 255,
    "home_page_url": "https://huggingface.co/papers?date=2024-10-24",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2410.17637",
            "title": "MIA-DPO: Multi-Image Augmented Direct Preference Optimization For Large Vision-Language Models",
            "url": "https://huggingface.co/papers/2410.17637",
            "abstract": "Visual preference alignment involves training Large Vision-Language Models (LVLMs) to predict human preferences between visual inputs. This is typically achieved by using labeled datasets of chosen/rejected pairs and employing optimization algorithms like direct preference optimization (DPO). Existing visual alignment methods, primarily designed for single-image scenarios, struggle to effectively handle the complexity of multi-image tasks due to the scarcity of diverse training data and the high cost of annotating chosen/rejected pairs. We present Multi-Image Augmented Direct Preference Optimization (MIA-DPO), a visual preference alignment approach that effectively handles multi-image inputs. MIA-DPO mitigates the scarcity of diverse multi-image training data by extending single-image data with unrelated images arranged in grid collages or pic-in-pic formats, significantly reducing the costs associated with multi-image data annotations. Our observation reveals that attention values of LVLMs vary considerably across different images. We use attention values to identify and filter out rejected responses the model may have mistakenly focused on. Our attention-aware selection for constructing the chosen/rejected pairs without relying on (i) human annotation, (ii) extra data, and (iii) external models or APIs. MIA-DPO is compatible with various architectures and outperforms existing methods on five multi-image benchmarks, achieving an average performance boost of 3.0% on LLaVA-v1.5 and 4.3% on the recent InternLM-XC2.5. Moreover, MIA-DPO has a minimal effect on the model's ability to understand single images.",
            "score": 34,
            "issue_id": 243,
            "pub_date": "2024-10-23",
            "pub_date_card": {
                "ru": "23 октября",
                "en": "October 23",
                "zh": "10月23日"
            },
            "hash": "2dc1395b8aa096fc",
            "data": {
                "categories": [
                    "#architecture",
                    "#benchmark",
                    "#cv",
                    "#optimization",
                    "#rlhf"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "MIA-DPO: эффективное обучение LVLM на многоизображительных данных",
                    "desc": "Статья представляет новый метод обучения больших визуально-языковых моделей (LVLM) под названием MIA-DPO. Этот подход эффективно работает с многоизображительными входными данными, решая проблему нехватки разнообразных обучающих данных. MIA-DPO расширяет однозображительные данные, добавляя несвязанные изображения в виде сеток или картинок-в-картинке. Метод использует значения внимания LVLM для идентификации и фильтрации ошибочно отвергнутых ответов, не требуя дополнительной аннотации или внешних моделей."
                },
                "en": {
                    "title": "Enhancing Visual Preference Prediction with MIA-DPO",
                    "desc": "The paper introduces a new method called Multi-Image Augmented Direct Preference Optimization (MIA-DPO) to improve how Large Vision-Language Models (LVLMs) predict human preferences between multiple images. MIA-DPO cleverly uses existing single-image data by combining them into collages, which helps reduce the need for expensive multi-image annotations. The method also uses attention values from the models to automatically filter out less relevant images, avoiding the need for human input or extra data. This approach is shown to work well with different model architectures, improving performance on several benchmarks without affecting single-image understanding."
                },
                "zh": {
                    "title": "多图像任务的视觉偏好对齐新突破",
                    "desc": "这篇论文介绍了一种新的视觉偏好对齐方法，称为多图像增强直接偏好优化（MIA-DPO）。MIA-DPO通过将单图像数据与不相关的图像组合在一起，解决了多图像任务中训练数据稀缺的问题，从而降低了数据标注的成本。该方法利用大规模视觉语言模型的注意力值来识别和过滤掉错误关注的图像，无需依赖人工标注或额外数据。实验结果表明，MIA-DPO在多个多图像基准测试中表现优异，并且对单图像理解能力影响较小。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.17434",
            "title": "LongVU: Spatiotemporal Adaptive Compression for Long Video-Language Understanding",
            "url": "https://huggingface.co/papers/2410.17434",
            "abstract": "Multimodal Large Language Models (MLLMs) have shown promising progress in understanding and analyzing video content. However, processing long videos remains a significant challenge constrained by LLM's context size. To address this limitation, we propose LongVU, a spatiotemporal adaptive compression mechanism thats reduces the number of video tokens while preserving visual details of long videos. Our idea is based on leveraging cross-modal query and inter-frame dependencies to adaptively reduce temporal and spatial redundancy in videos. Specifically, we leverage DINOv2 features to remove redundant frames that exhibit high similarity. Then we utilize text-guided cross-modal query for selective frame feature reduction. Further, we perform spatial token reduction across frames based on their temporal dependencies. Our adaptive compression strategy effectively processes a large number of frames with little visual information loss within given context length. Our LongVU consistently surpass existing methods across a variety of video understanding benchmarks, especially on hour-long video understanding tasks such as VideoMME and MLVU. Given a light-weight LLM, our LongVU also scales effectively into a smaller size with state-of-the-art video understanding performance.",
            "score": 24,
            "issue_id": 253,
            "pub_date": "2024-10-22",
            "pub_date_card": {
                "ru": "22 октября",
                "en": "October 22",
                "zh": "10月22日"
            },
            "hash": "ab4b27c05d7611e1",
            "data": {
                "categories": [
                    "#architecture",
                    "#benchmark",
                    "#long_context",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "LongVU: Эффективное сжатие для понимания длинных видео",
                    "desc": "LongVU - это новый механизм адаптивного сжатия для анализа длинных видео мультимодальными большими языковыми моделями (MLLM). Он использует кросс-модальные запросы и межкадровые зависимости для уменьшения временной и пространственной избыточности в видео. LongVU применяет признаки DINOv2 для удаления похожих кадров и текстовые запросы для выборочного сокращения признаков кадров. Этот подход позволяет эффективно обрабатывать большое количество кадров с минимальной потерей визуальной информации в рамках заданного контекста."
                },
                "en": {
                    "title": "\"LongVU: Mastering Long Video Understanding with Efficient Compression\"",
                    "desc": "The paper introduces LongVU, a method to efficiently process long videos using Multimodal Large Language Models (MLLMs) by reducing video tokens while maintaining visual details. It uses a spatiotemporal adaptive compression mechanism that leverages cross-modal queries and inter-frame dependencies to minimize redundancy. By employing DINOv2 features, it removes similar frames and uses text-guided queries for selective frame reduction. LongVU outperforms existing methods in video understanding tasks, especially with hour-long videos, and scales well with smaller LLMs."
                },
                "zh": {
                    "title": "LongVU：长视频理解的新突破",
                    "desc": "这篇论文介绍了一种名为LongVU的新方法，用于处理长视频中的信息冗余问题。LongVU通过跨模态查询和帧间依赖性，适应性地减少视频中的时间和空间冗余。具体来说，它利用DINOv2特征去除相似度高的冗余帧，并通过文本引导的跨模态查询选择性地减少帧特征。该方法在处理长视频时，能够在保持视觉细节的同时，显著减少视频令牌数量。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.18072",
            "title": "WorldSimBench: Towards Video Generation Models as World Simulators",
            "url": "https://huggingface.co/papers/2410.18072",
            "abstract": "Recent advancements in predictive models have demonstrated exceptional capabilities in predicting the future state of objects and scenes. However, the lack of categorization based on inherent characteristics continues to hinder the progress of predictive model development. Additionally, existing benchmarks are unable to effectively evaluate higher-capability, highly embodied predictive models from an embodied perspective. In this work, we classify the functionalities of predictive models into a hierarchy and take the first step in evaluating World Simulators by proposing a dual evaluation framework called WorldSimBench. WorldSimBench includes Explicit Perceptual Evaluation and Implicit Manipulative Evaluation, encompassing human preference assessments from the visual perspective and action-level evaluations in embodied tasks, covering three representative embodied scenarios: Open-Ended Embodied Environment, Autonomous, Driving, and Robot Manipulation. In the Explicit Perceptual Evaluation, we introduce the HF-Embodied Dataset, a video assessment dataset based on fine-grained human feedback, which we use to train a Human Preference Evaluator that aligns with human perception and explicitly assesses the visual fidelity of World Simulators. In the Implicit Manipulative Evaluation, we assess the video-action consistency of World Simulators by evaluating whether the generated situation-aware video can be accurately translated into the correct control signals in dynamic environments. Our comprehensive evaluation offers key insights that can drive further innovation in video generation models, positioning World Simulators as a pivotal advancement toward embodied artificial intelligence.",
            "score": 17,
            "issue_id": 243,
            "pub_date": "2024-10-23",
            "pub_date_card": {
                "ru": "23 октября",
                "en": "October 23",
                "zh": "10月23日"
            },
            "hash": "e4c42676df8ded0d",
            "data": {
                "categories": [
                    "#agents",
                    "#benchmark",
                    "#rlhf",
                    "#video"
                ],
                "emoji": "🌐",
                "ru": {
                    "title": "WorldSimBench: Комплексная оценка симуляторов мира для воплощенного ИИ",
                    "desc": "Статья представляет новый подход к оценке предиктивных моделей, называемый WorldSimBench. Эта система включает в себя явную перцептивную оценку и неявную манипулятивную оценку, охватывающие три сценария: открытую среду, автономное вождение и робототехнику. Авторы вводят датасет HF-Embodied для оценки визуальной точности симуляторов мира на основе человеческих предпочтений. Также предлагается оценка соответствия видео и действий, проверяя, могут ли сгенерированные видео точно преобразовываться в сигналы управления в динамических средах."
                },
                "en": {
                    "title": "Advancing Predictive Models with WorldSimBench: A New Era for Embodied AI",
                    "desc": "This paper addresses the challenge of categorizing predictive models based on their inherent characteristics to enhance their development. It introduces WorldSimBench, a dual evaluation framework for assessing World Simulators, which includes Explicit Perceptual Evaluation and Implicit Manipulative Evaluation. The framework uses the HF-Embodied Dataset to train a Human Preference Evaluator for visual fidelity and assesses video-action consistency in dynamic environments. This comprehensive evaluation aims to advance video generation models and promote the development of embodied artificial intelligence."
                },
                "zh": {
                    "title": "推动具身人工智能的世界模拟器评估",
                    "desc": "这篇论文探讨了预测模型在预测物体和场景未来状态方面的能力，但指出缺乏基于内在特征的分类限制了其发展。为了解决这个问题，作者提出了一个名为WorldSimBench的双重评估框架，用于评估世界模拟器。WorldSimBench包括显式感知评估和隐式操作评估，涵盖了开放式环境、自主驾驶和机器人操作等场景。通过这种评估，研究人员希望推动视频生成模型的创新，促进具身人工智能的发展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.17891",
            "title": "Scaling Diffusion Language Models via Adaptation from Autoregressive Models",
            "url": "https://huggingface.co/papers/2410.17891",
            "abstract": "Diffusion Language Models (DLMs) have emerged as a promising new paradigm for text generative modeling, potentially addressing limitations of autoregressive (AR) models. However, current DLMs have been studied at a smaller scale compared to their AR counterparts and lack fair comparison on language modeling benchmarks. Additionally, training diffusion models from scratch at scale remains challenging. Given the prevalence of open-source AR language models, we propose adapting these models to build text diffusion models. We demonstrate connections between AR and diffusion modeling objectives and introduce a simple continual pre-training approach for training diffusion models. Through systematic evaluation on language modeling, reasoning, and commonsense benchmarks, we show that we can convert AR models ranging from 127M to 7B parameters (GPT2 and LLaMA) into diffusion models DiffuGPT and DiffuLLaMA, using less than 200B tokens for training. Our experimental results reveal that these models outperform earlier DLMs and are competitive with their AR counterparts. We release a suite of DLMs (with 127M, 355M, and 7B parameters) capable of generating fluent text, performing in-context learning, filling in the middle without prompt re-ordering, and following instructions https://github.com/HKUNLP/DiffuLLaMA.",
            "score": 15,
            "issue_id": 243,
            "pub_date": "2024-10-23",
            "pub_date_card": {
                "ru": "23 октября",
                "en": "October 23",
                "zh": "10月23日"
            },
            "hash": "a6f6dc7932b17a7f",
            "data": {
                "categories": [
                    "#benchmark",
                    "#diffusion",
                    "#multimodal"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "Преобразование авторегрессионных моделей в диффузионные: новый шаг в языковом моделировании",
                    "desc": "Исследователи предлагают новый подход к созданию диффузионных языковых моделей (DLM), адаптируя существующие авторегрессионные модели. Они демонстрируют связь между целевыми функциями авторегрессионных и диффузионных моделей и вводят метод дообучения для создания DLM. Эксперименты показывают, что полученные модели DiffuGPT и DiffuLLaMA превосходят предыдущие DLM и конкурентоспособны с авторегрессионными аналогами. Авторы представляют набор DLM различных размеров, способных генерировать связный текст и выполнять различные языковые задачи."
                },
                "en": {
                    "title": "Revolutionizing Text Generation: From Autoregressive to Diffusion Models",
                    "desc": "This paper explores the potential of Diffusion Language Models (DLMs) as an alternative to traditional autoregressive (AR) models for text generation. The authors propose a method to adapt existing AR models into DLMs, demonstrating that these adapted models can perform competitively on various language tasks. They introduce a continual pre-training approach to efficiently train these diffusion models using fewer resources. The study shows that the adapted models, DiffuGPT and DiffuLLaMA, outperform previous DLMs and match the performance of AR models on several benchmarks."
                },
                "zh": {
                    "title": "扩散语言模型：突破自回归模型的局限",
                    "desc": "扩散语言模型（DLMs）是一种新兴的文本生成模型，可能解决自回归（AR）模型的局限性。研究表明，通过将开源的AR模型适配为扩散模型，可以在语言建模基准上实现更好的性能。我们提出了一种简单的持续预训练方法，将AR模型转换为扩散模型，并在多个基准上进行了系统评估。实验结果显示，这些转换后的模型在生成流畅文本和上下文学习等任务上表现优异。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.18013",
            "title": "Scalable Ranked Preference Optimization for Text-to-Image Generation",
            "url": "https://huggingface.co/papers/2410.18013",
            "abstract": "Direct Preference Optimization (DPO) has emerged as a powerful approach to align text-to-image (T2I) models with human feedback. Unfortunately, successful application of DPO to T2I models requires a huge amount of resources to collect and label large-scale datasets, e.g., millions of generated paired images annotated with human preferences. In addition, these human preference datasets can get outdated quickly as the rapid improvements of T2I models lead to higher quality images. In this work, we investigate a scalable approach for collecting large-scale and fully synthetic datasets for DPO training. Specifically, the preferences for paired images are generated using a pre-trained reward function, eliminating the need for involving humans in the annotation process, greatly improving the dataset collection efficiency. Moreover, we demonstrate that such datasets allow averaging predictions across multiple models and collecting ranked preferences as opposed to pairwise preferences. Furthermore, we introduce RankDPO to enhance DPO-based methods using the ranking feedback. Applying RankDPO on SDXL and SD3-Medium models with our synthetically generated preference dataset ``Syn-Pic'' improves both prompt-following (on benchmarks like T2I-Compbench, GenEval, and DPG-Bench) and visual quality (through user studies). This pipeline presents a practical and scalable solution to develop better preference datasets to enhance the performance of text-to-image models.",
            "score": 14,
            "issue_id": 246,
            "pub_date": "2024-10-23",
            "pub_date_card": {
                "ru": "23 октября",
                "en": "October 23",
                "zh": "10月23日"
            },
            "hash": "05611e12662f347d",
            "data": {
                "categories": [
                    "#dataset",
                    "#rag",
                    "#rlhf",
                    "#synthetic",
                    "#training"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Синтетические данные для улучшения генерации изображений по тексту",
                    "desc": "Статья представляет новый подход к улучшению моделей генерации изображений по текстовому описанию (text-to-image). Авторы предлагают использовать синтетические наборы данных для обучения методом Direct Preference Optimization (DPO), что позволяет избежать трудоемкого процесса сбора предпочтений от людей. Они вводят метод RankDPO, который использует ранжированные предпочтения вместо попарных. Эксперименты показывают, что предложенный подход улучшает как следование промпту, так и визуальное качество генерируемых изображений."
                },
                "en": {
                    "title": "\"Revolutionizing Text-to-Image Models with Synthetic Preferences\"",
                    "desc": "The paper explores a new method for improving text-to-image models by using Direct Preference Optimization (DPO) without relying on human-annotated datasets. Instead of collecting human feedback, the authors use a pre-trained reward function to generate synthetic datasets, which makes the process more efficient and scalable. They introduce RankDPO, a technique that uses ranking feedback to further enhance model performance. The results show that using these synthetic datasets improves both the accuracy of following prompts and the visual quality of generated images."
                },
                "zh": {
                    "title": "合成数据集：提升文本到图像模型的新途径",
                    "desc": "这篇论文探讨了一种可扩展的方法来收集用于直接偏好优化（DPO）训练的大规模全合成数据集。通过使用预训练的奖励函数生成图像对的偏好，消除了人工标注的需求，大大提高了数据集收集的效率。此外，研究表明，这种数据集允许在多个模型之间进行预测平均，并收集排名偏好而不是成对偏好。引入的RankDPO方法通过排名反馈增强了基于DPO的方法，提升了文本到图像模型的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.18084",
            "title": "DynamicCity: Large-Scale LiDAR Generation from Dynamic Scenes",
            "url": "https://huggingface.co/papers/2410.18084",
            "abstract": "LiDAR scene generation has been developing rapidly recently. However, existing methods primarily focus on generating static and single-frame scenes, overlooking the inherently dynamic nature of real-world driving environments. In this work, we introduce DynamicCity, a novel 4D LiDAR generation framework capable of generating large-scale, high-quality LiDAR scenes that capture the temporal evolution of dynamic environments. DynamicCity mainly consists of two key models. 1) A VAE model for learning HexPlane as the compact 4D representation. Instead of using naive averaging operations, DynamicCity employs a novel Projection Module to effectively compress 4D LiDAR features into six 2D feature maps for HexPlane construction, which significantly enhances HexPlane fitting quality (up to 12.56 mIoU gain). Furthermore, we utilize an Expansion & Squeeze Strategy to reconstruct 3D feature volumes in parallel, which improves both network training efficiency and reconstruction accuracy than naively querying each 3D point (up to 7.05 mIoU gain, 2.06x training speedup, and 70.84% memory reduction). 2) A DiT-based diffusion model for HexPlane generation. To make HexPlane feasible for DiT generation, a Padded Rollout Operation is proposed to reorganize all six feature planes of the HexPlane as a squared 2D feature map. In particular, various conditions could be introduced in the diffusion or sampling process, supporting versatile 4D generation applications, such as trajectory- and command-driven generation, inpainting, and layout-conditioned generation. Extensive experiments on the CarlaSC and Waymo datasets demonstrate that DynamicCity significantly outperforms existing state-of-the-art 4D LiDAR generation methods across multiple metrics. The code will be released to facilitate future research.",
            "score": 12,
            "issue_id": 250,
            "pub_date": "2024-10-23",
            "pub_date_card": {
                "ru": "23 октября",
                "en": "October 23",
                "zh": "10月23日"
            },
            "hash": "c726f29cc044d5f6",
            "data": {
                "categories": [
                    "#3d",
                    "#dataset",
                    "#diffusion"
                ],
                "emoji": "🚗",
                "ru": {
                    "title": "DynamicCity: революция в генерации динамических 4D LiDAR сцен",
                    "desc": "Статья представляет DynamicCity - новую систему для генерации динамических 4D LiDAR сцен. Основу системы составляют VAE модель для создания компактного 4D представления HexPlane и DiT-based диффузионная модель для генерации HexPlane. Авторы предлагают ряд инновационных методов, включая Projection Module и Expansion & Squeeze Strategy, для повышения качества и эффективности генерации. Эксперименты показывают, что DynamicCity значительно превосходит существующие методы генерации 4D LiDAR сцен по нескольким метрикам."
                },
                "en": {
                    "title": "DynamicCity: Bringing 4D LiDAR Scenes to Life",
                    "desc": "DynamicCity is a new framework for generating 4D LiDAR scenes that capture the dynamic nature of real-world environments. It uses a VAE model to create a compact 4D representation called HexPlane, which improves fitting quality by compressing features into 2D maps. The framework also includes an Expansion & Squeeze Strategy to enhance training efficiency and accuracy. Additionally, a DiT-based diffusion model is used for generating HexPlane, allowing for versatile applications like trajectory-driven generation and inpainting."
                },
                "zh": {
                    "title": "DynamicCity：捕捉动态环境的4D LiDAR生成框架",
                    "desc": "这篇论文介绍了一种名为DynamicCity的新型4D LiDAR生成框架，能够生成大规模、高质量的动态环境LiDAR场景。DynamicCity主要由两个关键模型组成：一个VAE模型用于学习HexPlane作为紧凑的4D表示，通过投影模块将4D特征压缩为六个2D特征图，从而提高了HexPlane的拟合质量。另一个是基于DiT的扩散模型，用于生成HexPlane，并通过填充展开操作将六个特征平面重新组织为一个方形2D特征图。实验表明，DynamicCity在多个指标上显著优于现有的4D LiDAR生成方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.15522",
            "title": "M-RewardBench: Evaluating Reward Models in Multilingual Settings",
            "url": "https://huggingface.co/papers/2410.15522",
            "abstract": "Reward models (RMs) have driven the state-of-the-art performance of LLMs today by enabling the integration of human feedback into the language modeling process. However, RMs are primarily trained and evaluated in English, and their capabilities in multilingual settings remain largely understudied. In this work, we conduct a systematic evaluation of several reward models in multilingual settings. We first construct the first-of-its-kind multilingual RM evaluation benchmark, M-RewardBench, consisting of 2.87k preference instances for 23 typologically diverse languages, that tests the chat, safety, reasoning, and translation capabilities of RMs. We then rigorously evaluate a wide range of reward models on M-RewardBench, offering fresh insights into their performance across diverse languages. We identify a significant gap in RMs' performances between English and non-English languages and show that RM preferences can change substantially from one language to another. We also present several findings on how different multilingual aspects impact RM performance. Specifically, we show that the performance of RMs is improved with improved translation quality. Similarly, we demonstrate that the models exhibit better performance for high-resource languages. We release M-RewardBench dataset and the codebase in this study to facilitate a better understanding of RM evaluation in multilingual settings.",
            "score": 10,
            "issue_id": 247,
            "pub_date": "2024-10-20",
            "pub_date_card": {
                "ru": "20 октября",
                "en": "October 20",
                "zh": "10月20日"
            },
            "hash": "5143b0b6f1067fee",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#multilingual",
                    "#rlhf",
                    "#translation"
                ],
                "emoji": "🌍",
                "ru": {
                    "title": "Многоязычная оценка моделей вознаграждения: новые горизонты и вызовы",
                    "desc": "Данная статья посвящена исследованию моделей вознаграждения (reward models) в многоязычном контексте. Авторы создали первый в своем роде многоязычный эталонный набор данных M-RewardBench для оценки таких моделей на 23 типологически разных языках. Проведя тщательную оценку различных моделей вознаграждения, исследователи выявили значительный разрыв в производительности между английским и другими языками. Результаты показывают, что качество перевода и ресурсообеспеченность языка положительно влияют на эффективность моделей вознаграждения."
                },
                "en": {
                    "title": "Bridging the Language Gap: Evaluating Reward Models Multilingually",
                    "desc": "This paper explores how reward models (RMs), which enhance language models by incorporating human feedback, perform in multilingual contexts. The authors introduce M-RewardBench, a new benchmark with 2.87k preference instances across 23 languages, to evaluate RMs' abilities in chat, safety, reasoning, and translation. Their findings reveal a performance gap between English and other languages, with RMs showing varied preferences across languages. The study highlights that better translation quality and high-resource languages improve RM performance, and they provide the M-RewardBench dataset and codebase for further research."
                },
                "zh": {
                    "title": "多语言奖励模型：跨语言表现的新视角",
                    "desc": "这篇论文研究了奖励模型在多语言环境中的表现。研究者们创建了一个名为M-RewardBench的多语言评估基准，包含23种语言的偏好实例。结果显示，奖励模型在英语和非英语语言之间的表现存在显著差距。研究还发现，翻译质量和语言资源丰富度对模型表现有重要影响。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.17883",
            "title": "Lightweight Neural App Control",
            "url": "https://huggingface.co/papers/2410.17883",
            "abstract": "This paper introduces a novel mobile phone control architecture, termed ``app agents\", for efficient interactions and controls across various Android apps. The proposed Lightweight Multi-modal App Control (LiMAC) takes as input a textual goal and a sequence of past mobile observations, such as screenshots and corresponding UI trees, to generate precise actions. To address the computational constraints inherent to smartphones, within LiMAC, we introduce a small Action Transformer (AcT) integrated with a fine-tuned vision-language model (VLM) for real-time decision-making and task execution. We evaluate LiMAC on two open-source mobile control datasets, demonstrating the superior performance of our small-form-factor approach against fine-tuned versions of open-source VLMs, such as Florence2 and Qwen2-VL. It also significantly outperforms prompt engineering baselines utilising closed-source foundation models like GPT-4o. More specifically, LiMAC increases the overall action accuracy by up to 19% compared to fine-tuned VLMs, and up to 42% compared to prompt-engineering baselines.",
            "score": 8,
            "issue_id": 243,
            "pub_date": "2024-10-23",
            "pub_date_card": {
                "ru": "23 октября",
                "en": "October 23",
                "zh": "10月23日"
            },
            "hash": "66e9ccd38bb979e0",
            "data": {
                "categories": [
                    "#agents",
                    "#architecture",
                    "#cv",
                    "#multimodal"
                ],
                "emoji": "📱",
                "ru": {
                    "title": "Эффективное управление Android-приложениями с помощью легковесных агентов",
                    "desc": "Статья представляет новую архитектуру управления мобильными приложениями под названием 'app agents'. Предложенная система LiMAC использует текстовую цель и последовательность предыдущих наблюдений для генерации точных действий. Авторы вводят компактный Action Transformer (AcT) в сочетании с настроенной мультимодальной моделью для принятия решений в реальном времени. Эксперименты показывают, что LiMAC превосходит базовые модели на основе промптов и настроенные мультимодальные модели, повышая точность действий до 42%."
                },
                "en": {
                    "title": "\"LiMAC: Smarter, Faster Mobile App Control\"",
                    "desc": "This paper presents a new system called \"app agents\" for controlling Android apps more efficiently. The system, LiMAC, uses text goals and past mobile data like screenshots to decide what actions to take. It includes a small Action Transformer and a vision-language model to work quickly on smartphones. Tests show LiMAC performs better than other models, improving action accuracy by up to 42%."
                },
                "zh": {
                    "title": "LiMAC：提升手机应用控制效率的新架构",
                    "desc": "这篇论文介绍了一种新的手机控制架构，称为“应用代理”，用于在各种安卓应用中实现高效的交互和控制。提出的轻量级多模态应用控制（LiMAC）可以根据文本目标和过去的手机观察（如截图和UI树）生成精确的操作。为了应对智能手机的计算限制，LiMAC中引入了一个小型动作变换器（AcT），并与微调的视觉语言模型（VLM）集成，实现实时决策和任务执行。实验结果表明，LiMAC在两个开源移动控制数据集上的表现优于微调的开源VLM，并显著超过了使用闭源基础模型的提示工程基线。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.13458",
            "title": "MedINST: Meta Dataset of Biomedical Instructions",
            "url": "https://huggingface.co/papers/2410.13458",
            "abstract": "The integration of large language model (LLM) techniques in the field of medical analysis has brought about significant advancements, yet the scarcity of large, diverse, and well-annotated datasets remains a major challenge. Medical data and tasks, which vary in format, size, and other parameters, require extensive preprocessing and standardization for effective use in training LLMs. To address these challenges, we introduce MedINST, the Meta Dataset of Biomedical Instructions, a novel multi-domain, multi-task instructional meta-dataset. MedINST comprises 133 biomedical NLP tasks and over 7 million training samples, making it the most comprehensive biomedical instruction dataset to date. Using MedINST as the meta dataset, we curate MedINST32, a challenging benchmark with different task difficulties aiming to evaluate LLMs' generalization ability. We fine-tune several LLMs on MedINST and evaluate on MedINST32, showcasing enhanced cross-task generalization.",
            "score": 6,
            "issue_id": 250,
            "pub_date": "2024-10-17",
            "pub_date_card": {
                "ru": "17 октября",
                "en": "October 17",
                "zh": "10月17日"
            },
            "hash": "838e617c67ce19c3",
            "data": {
                "categories": [
                    "#benchmark",
                    "#data",
                    "#dataset",
                    "#medicine"
                ],
                "emoji": "🧬",
                "ru": {
                    "title": "MedINST: революция в обучении LLM для медицинского анализа",
                    "desc": "Статья представляет MedINST - новый мета-датасет медицинских инструкций для обучения больших языковых моделей (LLM) в области биомедицинского анализа. MedINST включает 133 задачи и более 7 миллионов обучающих примеров, что делает его самым обширным биомедицинским инструкционным датасетом на сегодняшний день. Авторы также создали MedINST32 - сложный бенчмарк для оценки способности LLM к обобщению. Эксперименты показали улучшенную кросс-задачную генерализацию моделей, обученных на MedINST."
                },
                "en": {
                    "title": "MedINST: Revolutionizing Medical NLP with Comprehensive Datasets",
                    "desc": "The paper discusses the development of MedINST, a comprehensive meta-dataset designed to improve the training of large language models (LLMs) in the medical field. MedINST includes 133 different biomedical natural language processing tasks and over 7 million training samples, addressing the challenge of limited and diverse medical datasets. The authors also introduce MedINST32, a benchmark to test the generalization abilities of LLMs across various tasks. By fine-tuning LLMs on MedINST, the study demonstrates improved cross-task generalization, highlighting the potential of MedINST in advancing medical data analysis."
                },
                "zh": {
                    "title": "MedINST：突破医学数据集瓶颈的创新解决方案",
                    "desc": "这篇论文介绍了在医学分析领域中整合大型语言模型技术所带来的进步，但也指出了缺乏大型、多样化和良好标注的数据集是一个主要挑战。为了应对这些挑战，作者们引入了MedINST，一个新的多领域、多任务的指令元数据集。MedINST包含133个生物医学自然语言处理任务和超过700万个训练样本，是迄今为止最全面的生物医学指令数据集。通过在MedINST上微调多个大型语言模型，并在MedINST32基准上进行评估，展示了增强的跨任务泛化能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.13924",
            "title": "ARKit LabelMaker: A New Scale for Indoor 3D Scene Understanding",
            "url": "https://huggingface.co/papers/2410.13924",
            "abstract": "The performance of neural networks scales with both their size and the amount of data they have been trained on. This is shown in both language and image generation. However, this requires scaling-friendly network architectures as well as large-scale datasets. Even though scaling-friendly architectures like transformers have emerged for 3D vision tasks, the GPT-moment of 3D vision remains distant due to the lack of training data. In this paper, we introduce ARKit LabelMaker, the first large-scale, real-world 3D dataset with dense semantic annotations. Specifically, we complement ARKitScenes dataset with dense semantic annotations that are automatically generated at scale. To this end, we extend LabelMaker, a recent automatic annotation pipeline, to serve the needs of large-scale pre-training. This involves extending the pipeline with cutting-edge segmentation models as well as making it robust to the challenges of large-scale processing. Further, we push forward the state-of-the-art performance on ScanNet and ScanNet200 dataset with prevalent 3D semantic segmentation models, demonstrating the efficacy of our generated dataset.",
            "score": 6,
            "issue_id": 249,
            "pub_date": "2024-10-17",
            "pub_date_card": {
                "ru": "17 октября",
                "en": "October 17",
                "zh": "10月17日"
            },
            "hash": "37c34e5be2bb734e",
            "data": {
                "categories": [
                    "#3d",
                    "#data",
                    "#dataset"
                ],
                "emoji": "🏷️",
                "ru": {
                    "title": "Большие данные для прорыва в 3D-зрении",
                    "desc": "Эта статья представляет ARKit LabelMaker - первый крупномасштабный набор данных реального мира с плотными семантическими аннотациями для задач 3D-зрения. Авторы расширили существующий инструмент LabelMaker, чтобы автоматически генерировать аннотации в больших масштабах. Они использовали современные модели сегментации и сделали процесс устойчивым к проблемам обработки больших данных. Результаты показывают улучшение производительности моделей семантической сегментации на наборах данных ScanNet и ScanNet200."
                },
                "en": {
                    "title": "Unlocking 3D Vision with ARKit LabelMaker: A New Era of Large-Scale Datasets",
                    "desc": "The paper discusses how the performance of neural networks improves with larger sizes and more data, particularly in language and image generation. It highlights the challenge in 3D vision tasks due to a lack of large-scale datasets, despite having suitable architectures like transformers. The authors introduce ARKit LabelMaker, a new large-scale 3D dataset with dense semantic annotations, enhancing the ARKitScenes dataset. They demonstrate the effectiveness of this dataset by improving state-of-the-art results on existing 3D semantic segmentation benchmarks like ScanNet."
                },
                "zh": {
                    "title": "ARKit LabelMaker：推动3D视觉的未来",
                    "desc": "这篇论文讨论了神经网络的性能如何随着其规模和训练数据量的增加而提高。为了实现这一点，需要适合扩展的网络架构和大规模数据集。作者介绍了ARKit LabelMaker，这是第一个具有密集语义标注的大规模真实3D数据集。通过改进自动标注流程，作者展示了在3D语义分割任务中使用该数据集的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.18071",
            "title": "TP-Eval: Tap Multimodal LLMs' Potential in Evaluation by Customizing Prompts",
            "url": "https://huggingface.co/papers/2410.18071",
            "abstract": "Recently, multimodal large language models (MLLMs) have received much attention for their impressive capabilities. The evaluation of MLLMs is becoming critical to analyzing attributes of MLLMs and providing valuable insights. However, current benchmarks overlook the problem of prompt sensitivity - minor prompt variations may lead to significant performance fluctuations. Thus, inappropriate prompts may obscure the models' capabilities, underestimating the models' performance. Moreover, different models have different preferences for different prompts, and thus, using the same prompt for all models will cause evaluation bias. This paper analyzes this deficiency in existing benchmarks and further introduces a new evaluation framework named TP-Eval, which introduces a prompt customization method to reduce evaluation biases and tap models' potential. TP-Eval will rewrite the original prompts to different customized prompts for different models. In particular, we propose some well-designed modules for prompt customization tailored to the scenario of MLLM evaluation. Extensive experiments demonstrate the effectiveness of our approach to uncovering models' capabilities, and TP-Eval should benefit the community in developing more comprehensive and convincing MLLM evaluation benchmarks.",
            "score": 6,
            "issue_id": 245,
            "pub_date": "2024-10-23",
            "pub_date_card": {
                "ru": "23 октября",
                "en": "October 23",
                "zh": "10月23日"
            },
            "hash": "9fe4ef775d3f190c",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Точная настройка промптов для справедливой оценки мультимодальных языковых моделей",
                    "desc": "Статья посвящена проблеме чувствительности к промптам в мультимодальных больших языковых моделях (MLLM). Авторы предлагают новую систему оценки TP-Eval, которая использует метод настройки промптов для снижения предвзятости оценки и раскрытия потенциала моделей. TP-Eval переписывает исходные промпты в индивидуальные варианты для разных моделей. Эксперименты показывают эффективность этого подхода в выявлении возможностей моделей."
                },
                "en": {
                    "title": "\"Unlocking True Potential: Tailored Prompts for Fair MLLM Evaluation\"",
                    "desc": "The paper discusses the challenges in evaluating multimodal large language models (MLLMs) due to prompt sensitivity, where small changes in prompts can lead to significant performance differences. It highlights that using the same prompt for all models can introduce bias, as different models may respond better to different prompts. To address this, the authors propose a new evaluation framework called TP-Eval, which customizes prompts for each model to reduce bias and better reveal their capabilities. The framework includes specially designed modules for prompt customization, and experiments show that TP-Eval effectively enhances the evaluation of MLLMs."
                },
                "zh": {
                    "title": "TP-Eval：定制化提示词，挖掘模型潜力",
                    "desc": "这篇论文讨论了多模态大语言模型（MLLMs）的评估问题，特别是提示词敏感性的问题。现有的评估标准常常忽视了提示词的微小变化可能导致模型性能的显著波动。为了解决这个问题，作者提出了一种新的评估框架TP-Eval，通过定制化提示词来减少评估偏差。实验结果表明，TP-Eval能够更好地挖掘模型的潜力，为多模态大语言模型的评估提供了更全面和有说服力的基准。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.17242",
            "title": "LVSM: A Large View Synthesis Model with Minimal 3D Inductive Bias",
            "url": "https://huggingface.co/papers/2410.17242",
            "abstract": "We propose the Large View Synthesis Model (LVSM), a novel transformer-based approach for scalable and generalizable novel view synthesis from sparse-view inputs. We introduce two architectures: (1) an encoder-decoder LVSM, which encodes input image tokens into a fixed number of 1D latent tokens, functioning as a fully learned scene representation, and decodes novel-view images from them; and (2) a decoder-only LVSM, which directly maps input images to novel-view outputs, completely eliminating intermediate scene representations. Both models bypass the 3D inductive biases used in previous methods -- from 3D representations (e.g., NeRF, 3DGS) to network designs (e.g., epipolar projections, plane sweeps) -- addressing novel view synthesis with a fully data-driven approach. While the encoder-decoder model offers faster inference due to its independent latent representation, the decoder-only LVSM achieves superior quality, scalability, and zero-shot generalization, outperforming previous state-of-the-art methods by 1.5 to 3.5 dB PSNR. Comprehensive evaluations across multiple datasets demonstrate that both LVSM variants achieve state-of-the-art novel view synthesis quality. Notably, our models surpass all previous methods even with reduced computational resources (1-2 GPUs). Please see our website for more details: https://haian-jin.github.io/projects/LVSM/ .",
            "score": 3,
            "issue_id": 255,
            "pub_date": "2024-10-22",
            "pub_date_card": {
                "ru": "22 октября",
                "en": "October 22",
                "zh": "10月22日"
            },
            "hash": "f91cd290a84f0584",
            "data": {
                "categories": [
                    "#cv",
                    "#3d",
                    "#architecture",
                    "#benchmark"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Трансформеры покоряют синтез новых ракурсов",
                    "desc": "Исследователи представили Large View Synthesis Model (LVSM) - новый подход на основе трансформеров для масштабируемого и обобщаемого синтеза новых ракурсов по небольшому набору входных изображений. Предложены две архитектуры: энкодер-декодер LVSM, кодирующий входные изображения в латентные токены, и декодер-только LVSM, напрямую преобразующий входные изображения в новые ракурсы. Оба варианта LVSM превосходят современные методы по качеству синтеза новых ракурсов, при этом декодер-только LVSM демонстрирует лучшие результаты по качеству, масштабируемости и обобщению на новые данные. Модели достигают state-of-the-art результатов даже при использовании меньших вычислительных ресурсов."
                },
                "en": {
                    "title": "Transforming Sparse Views into Stunning Images with LVSM!",
                    "desc": "The Large View Synthesis Model (LVSM) introduces a new transformer-based method for creating images from limited input views. It features two architectures: an encoder-decoder model that learns a scene representation and a decoder-only model that generates images directly from inputs. Both models avoid traditional 3D biases, relying instead on a data-driven approach for novel view synthesis. The LVSM models demonstrate superior performance in image quality and efficiency, outperforming existing methods while using fewer computational resources."
                },
                "zh": {
                    "title": "大型视图合成模型：数据驱动的新视角",
                    "desc": "我们提出了一种新的大型视图合成模型（LVSM），这是一种基于变换器的可扩展和通用的新视图合成方法。该模型包括两种架构：编码-解码器LVSM和仅解码器LVSM，前者通过将输入图像编码为固定数量的潜在标记来生成新视图图像，后者则直接将输入图像映射到新视图输出。两种模型都避免了以往方法中使用的3D归纳偏见，采用完全数据驱动的方法进行新视图合成。我们的模型在多个数据集上的评估显示，LVSM在新视图合成质量上达到了最先进的水平，且在计算资源减少的情况下仍然表现优异。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.13816",
            "title": "Steering Your Generalists: Improving Robotic Foundation Models via Value Guidance",
            "url": "https://huggingface.co/papers/2410.13816",
            "abstract": "Large, general-purpose robotic policies trained on diverse demonstration datasets have been shown to be remarkably effective both for controlling a variety of robots in a range of different scenes, and for acquiring broad repertoires of manipulation skills. However, the data that such policies are trained on is generally of mixed quality -- not only are human-collected demonstrations unlikely to perform the task perfectly, but the larger the dataset is, the harder it is to curate only the highest quality examples. It also remains unclear how optimal data from one embodiment is for training on another embodiment. In this paper, we present a general and broadly applicable approach that enhances the performance of such generalist robot policies at deployment time by re-ranking their actions according to a value function learned via offline RL. This approach, which we call Value-Guided Policy Steering (V-GPS), is compatible with a wide range of different generalist policies, without needing to fine-tune or even access the weights of the policy. We show that the same value function can improve the performance of five different state-of-the-art policies with different architectures, even though they were trained on distinct datasets, attaining consistent performance improvement on multiple robotic platforms across a total of 12 tasks. Code and videos can be found at: https://nakamotoo.github.io/V-GPS",
            "score": 1,
            "issue_id": 255,
            "pub_date": "2024-10-17",
            "pub_date_card": {
                "ru": "17 октября",
                "en": "October 17",
                "zh": "10月17日"
            },
            "hash": "ee6d24e9422674a5",
            "data": {
                "categories": [
                    "#agents",
                    "#rl",
                    "#training",
                    "#robotics"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Улучшение робототехнических политик без переобучения: V-GPS в действии",
                    "desc": "Статья представляет новый подход под названием Value-Guided Policy Steering (V-GPS) для улучшения производительности обобщенных робототехнических политик. V-GPS использует функцию ценности, обученную с помощью офлайн-обучения с подкреплением, для переранжирования действий политики во время развертывания. Этот метод совместим с различными типами политик и не требует доступа к их весам или дополнительной настройки. Авторы демонстрируют эффективность V-GPS на пяти современных политиках с разными архитектурами, показывая улучшение производительности на нескольких робототехнических платформах в 12 задачах."
                },
                "en": {
                    "title": "Enhancing Robot Performance with Value-Guided Policy Steering",
                    "desc": "This paper introduces a method called Value-Guided Policy Steering (V-GPS) to improve the performance of general-purpose robotic policies during deployment. The authors address the issue of mixed-quality training data by using a value function learned through offline reinforcement learning (RL) to re-rank the actions of the robots. V-GPS is designed to work with various existing policies without requiring any modifications to their underlying models. The results demonstrate that this approach consistently enhances the performance of multiple state-of-the-art robotic policies across different tasks and platforms."
                },
                "zh": {
                    "title": "提升机器人策略性能的新方法",
                    "desc": "本文提出了一种名为价值引导策略引导（V-GPS）的方法，旨在提高通用机器人策略的性能。该方法通过离线强化学习学习的价值函数，对机器人在执行任务时的动作进行重新排序。V-GPS可以与多种不同的通用策略兼容使用，而无需微调或访问策略的权重。实验表明，使用相同的价值函数可以在不同架构的五种最先进策略上实现一致的性能提升，适用于12个不同的机器人任务。"
                }
            }
        }
    ],
    "link_prev": "2024-10-23.html",
    "link_next": "2024-10-25.html",
    "link_month": "2024-10.html",
    "short_date_prev": {
        "ru": "23.10",
        "en": "10/23",
        "zh": "10月23日"
    },
    "short_date_next": {
        "ru": "25.10",
        "en": "10/25",
        "zh": "10月25日"
    },
    "categories": {
        "#dataset": 5,
        "#data": 2,
        "#benchmark": 8,
        "#agents": 3,
        "#cv": 3,
        "#rl": 1,
        "#rlhf": 4,
        "#rag": 1,
        "#plp": 0,
        "#inference": 0,
        "#3d": 3,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 4,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 4,
        "#medicine": 1,
        "#training": 2,
        "#robotics": 1,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#edge_computing": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#translation": 1
    }
}