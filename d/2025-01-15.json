{
    "date": {
        "ru": "15 ÑĞ½Ğ²Ğ°Ñ€Ñ",
        "en": "January 15",
        "zh": "1æœˆ15æ—¥"
    },
    "time_utc": "2025-01-15 03:11",
    "weekday": 2,
    "issue_id": 1672,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.08313",
            "title": "MiniMax-01: Scaling Foundation Models with Lightning Attention",
            "url": "https://huggingface.co/papers/2501.08313",
            "abstract": "We introduce MiniMax-01 series, including MiniMax-Text-01 and MiniMax-VL-01, which are comparable to top-tier models while offering superior capabilities in processing longer contexts. The core lies in lightning attention and its efficient scaling. To maximize computational capacity, we integrate it with Mixture of Experts (MoE), creating a model with 32 experts and 456 billion total parameters, of which 45.9 billion are activated for each token. We develop an optimized parallel strategy and highly efficient computation-communication overlap techniques for MoE and lightning attention. This approach enables us to conduct efficient training and inference on models with hundreds of billions of parameters across contexts spanning millions of tokens. The context window of MiniMax-Text-01 can reach up to 1 million tokens during training and extrapolate to 4 million tokens during inference at an affordable cost. Our vision-language model, MiniMax-VL-01 is built through continued training with 512 billion vision-language tokens. Experiments on both standard and in-house benchmarks show that our models match the performance of state-of-the-art models like GPT-4o and Claude-3.5-Sonnet while offering 20-32 times longer context window. We publicly release MiniMax-01 at https://github.com/MiniMax-AI.",
            "score": 91,
            "issue_id": 1672,
            "pub_date": "2025-01-14",
            "pub_date_card": {
                "ru": "14 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 14",
                "zh": "1æœˆ14æ—¥"
            },
            "hash": "a57d7b1914e7383a",
            "authors": [
                "MiniMax",
                "Aonian Li",
                "Bangwei Gong",
                "Bo Yang",
                "Boji Shan",
                "Chang Liu",
                "Cheng Zhu",
                "Chunhao Zhang",
                "Congchao Guo",
                "Da Chen",
                "Dong Li",
                "Enwei Jiao",
                "Gengxin Li",
                "Guojun Zhang",
                "Haohai Sun",
                "Houze Dong",
                "Jiadai Zhu",
                "Jiaqi Zhuang",
                "Jiayuan Song",
                "Jin Zhu",
                "Jingtao Han",
                "Jingyang Li",
                "Junbin Xie",
                "Junhao Xu",
                "Junjie Yan",
                "Kaishun Zhang",
                "Kecheng Xiao",
                "Kexi Kang",
                "Le Han",
                "Leyang Wang",
                "Lianfei Yu",
                "Liheng Feng",
                "Lin Zheng",
                "Linbo Chai",
                "Long Xing",
                "Meizhi Ju",
                "Mingyuan Chi",
                "Mozhi Zhang",
                "Peikai Huang",
                "Pengcheng Niu",
                "Pengfei Li",
                "Pengyu Zhao",
                "Qi Yang",
                "Qidi Xu",
                "Qiexiang Wang",
                "Qin Wang",
                "Qiuhui Li",
                "Ruitao Leng",
                "Shengmin Shi",
                "Shuqi Yu",
                "Sichen Li",
                "Songquan Zhu",
                "Tao Huang",
                "Tianrun Liang",
                "Weigao Sun",
                "Weixuan Sun",
                "Weiyu Cheng",
                "Wenkai Li",
                "Xiangjun Song",
                "Xiao Su",
                "Xiaodong Han",
                "Xinjie Zhang",
                "Xinzhu Hou",
                "Xu Min",
                "Xun Zou",
                "Xuyang Shen",
                "Yan Gong",
                "Yingjie Zhu",
                "Yipeng Zhou",
                "Yiran Zhong",
                "Yongyi Hu",
                "Yuanxiang Fan",
                "Yue Yu",
                "Yufeng Yang",
                "Yuhao Li",
                "Yunan Huang",
                "Yunji Li",
                "Yunpeng Huang",
                "Yunzhi Xu",
                "Yuxin Mao",
                "Zehan Li",
                "Zekang Li",
                "Zewei Tao",
                "Zewen Ying",
                "Zhaoyang Cong",
                "Zhen Qin",
                "Zhenhua Fan",
                "Zhihang Yu",
                "Zhuo Jiang",
                "Zijia Wu"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2501.08313.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#architecture",
                    "#optimization",
                    "#benchmark",
                    "#long_context",
                    "#training"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "MiniMax-01: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ ÑĞµÑ€Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ MiniMax-01, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ MiniMax-Text-01 Ğ¸ MiniMax-VL-01, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹ Ñ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ½Ğ¾ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ². Ğ’ Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ»ĞµĞ¶Ğ¸Ñ‚ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ lightning attention Ğ¸ ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Mixture of Experts (MoE). ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ¼ĞµĞµÑ‚ 32 ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ° Ğ¸ 456 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¸Ğ· ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… 45,9 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°. ĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğµ Ğ¾ĞºĞ½Ğ¾ MiniMax-Text-01 Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ‚ÑŒ 1 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¸ ÑĞºÑÑ‚Ñ€Ğ°Ğ¿Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ´Ğ¾ 4 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞµ."
                },
                "en": {
                    "title": "Unleashing Long Contexts with MiniMax-01 Models",
                    "desc": "The MiniMax-01 series introduces advanced models, MiniMax-Text-01 and MiniMax-VL-01, designed to handle longer contexts effectively. These models utilize lightning attention and a Mixture of Experts (MoE) architecture, featuring 32 experts and a staggering 456 billion parameters, optimizing the activation of 45.9 billion parameters per token. By implementing efficient parallel strategies and computation-communication overlap techniques, the models can train and infer on extensive datasets, reaching context windows of up to 1 million tokens during training and 4 million during inference. Performance evaluations indicate that MiniMax-01 models rival leading models like GPT-4o and Claude-3.5-Sonnet while significantly extending context capabilities."
                },
                "zh": {
                    "title": "MiniMax-01ï¼šè¶…é•¿ä¸Šä¸‹æ–‡å¤„ç†çš„æ–°çºªå…ƒ",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†MiniMax-01ç³»åˆ—ï¼ŒåŒ…æ‹¬MiniMax-Text-01å’ŒMiniMax-VL-01ï¼Œè¿™äº›æ¨¡å‹åœ¨å¤„ç†æ›´é•¿çš„ä¸Šä¸‹æ–‡æ—¶å…·æœ‰ä¼˜è¶Šçš„èƒ½åŠ›ã€‚æ ¸å¿ƒæŠ€æœ¯æ˜¯é—ªç”µæ³¨æ„åŠ›å’Œé«˜æ•ˆçš„æ‰©å±•èƒ½åŠ›ã€‚ä¸ºäº†æœ€å¤§åŒ–è®¡ç®—èƒ½åŠ›ï¼Œæˆ‘ä»¬å°†å…¶ä¸ä¸“å®¶æ··åˆæ¨¡å‹ï¼ˆMoEï¼‰ç»“åˆï¼Œåˆ›å»ºäº†ä¸€ä¸ªæ‹¥æœ‰32ä¸ªä¸“å®¶å’Œ4560äº¿å‚æ•°çš„æ¨¡å‹ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè¿™äº›æ¨¡å‹åœ¨æ ‡å‡†å’Œå†…éƒ¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿä¸æœ€å…ˆè¿›çš„æ¨¡å‹ç›¸åª²ç¾ï¼ŒåŒæ—¶æä¾›20åˆ°32å€æ›´é•¿çš„ä¸Šä¸‹æ–‡çª—å£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.08187",
            "title": "A Multi-Modal AI Copilot for Single-Cell Analysis with Instruction Following",
            "url": "https://huggingface.co/papers/2501.08187",
            "abstract": "Large language models excel at interpreting complex natural language instructions, enabling them to perform a wide range of tasks. In the life sciences, single-cell RNA sequencing (scRNA-seq) data serves as the \"language of cellular biology\", capturing intricate gene expression patterns at the single-cell level. However, interacting with this \"language\" through conventional tools is often inefficient and unintuitive, posing challenges for researchers. To address these limitations, we present InstructCell, a multi-modal AI copilot that leverages natural language as a medium for more direct and flexible single-cell analysis. We construct a comprehensive multi-modal instruction dataset that pairs text-based instructions with scRNA-seq profiles from diverse tissues and species. Building on this, we develop a multi-modal cell language architecture capable of simultaneously interpreting and processing both modalities. InstructCell empowers researchers to accomplish critical tasks-such as cell type annotation, conditional pseudo-cell generation, and drug sensitivity prediction-using straightforward natural language commands. Extensive evaluations demonstrate that InstructCell consistently meets or exceeds the performance of existing single-cell foundation models, while adapting to diverse experimental conditions. More importantly, InstructCell provides an accessible and intuitive tool for exploring complex single-cell data, lowering technical barriers and enabling deeper biological insights.",
            "score": 9,
            "issue_id": 1672,
            "pub_date": "2025-01-14",
            "pub_date_card": {
                "ru": "14 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 14",
                "zh": "1æœˆ14æ—¥"
            },
            "hash": "de984ce7cc62fa5e",
            "authors": [
                "Yin Fang",
                "Xinle Deng",
                "Kangwei Liu",
                "Ningyu Zhang",
                "Jingyang Qian",
                "Penghui Yang",
                "Xiaohui Fan",
                "Huajun Chen"
            ],
            "affiliations": [
                "College of Computer Science and Technology, Zhejiang University, Hangzhou 310027, China",
                "College of Pharmaceutical Sciences, Zhejiang University, Hangzhou 310058, China",
                "Future Health Laboratory, Innovation Center of Yangtze River Delta, Zhejiang University, Jiaxing 314100, China",
                "Innovation Center in Zhejiang University, State Key Laboratory of Component-Based Chinese Medicine, Hangzhou 310058, China",
                "School of Software Technology, Zhejiang University, Ningbo 315048, China",
                "ZJU-Hangzhou Global Scientific and Technological Innovation Center, Hangzhou 311200, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.08187.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#multimodal",
                    "#dataset",
                    "#science",
                    "#healthcare"
                ],
                "emoji": "ğŸ§¬",
                "ru": {
                    "title": "Ğ•ÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº ĞºĞ°Ğº ĞºĞ»ÑÑ‡ Ğº Ñ€Ğ°ÑÑˆĞ¸Ñ„Ñ€Ğ¾Ğ²ĞºĞµ ĞºĞ»ĞµÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸",
                    "desc": "InstructCell - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ˜Ğ˜-Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸Ğº Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ´Ğ½Ğ¾ĞºĞ»ĞµÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ ĞĞš-ÑĞµĞºĞ²ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (scRNA-seq). ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ğº ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»Ğ¸ ÑĞºÑĞ¿Ñ€ĞµÑÑĞ¸Ğ¸ Ğ³ĞµĞ½Ğ¾Ğ². InstructCell Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ñ‚Ğ°ĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, ĞºĞ°Ğº Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ¸Ğ¿Ğ¾Ğ² ĞºĞ»ĞµÑ‚Ğ¾Ğº Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ»ĞµĞºĞ°Ñ€ÑÑ‚Ğ²Ğ°Ğ¼, Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑĞ¼."
                },
                "en": {
                    "title": "InstructCell: Bridging Language and Biology for Seamless Single-Cell Analysis",
                    "desc": "This paper introduces InstructCell, an AI tool designed to simplify the analysis of single-cell RNA sequencing (scRNA-seq) data using natural language instructions. By creating a dataset that links text commands with scRNA-seq profiles, InstructCell allows researchers to perform complex tasks like cell type annotation and drug sensitivity prediction more intuitively. The model employs a multi-modal architecture that processes both text and biological data simultaneously, enhancing its usability. Evaluations show that InstructCell outperforms existing models, making single-cell analysis more accessible and efficient for researchers in the life sciences."
                },
                "zh": {
                    "title": "ç”¨è‡ªç„¶è¯­è¨€è§£é”å•ç»†èƒæ•°æ®çš„æ½œåŠ›",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†InstructCellï¼Œä¸€ä¸ªå¤šæ¨¡æ€çš„äººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œæ—¨åœ¨é€šè¿‡è‡ªç„¶è¯­è¨€ç®€åŒ–å•ç»†èƒRNAæµ‹åº(scRNA-seq)æ•°æ®çš„åˆ†æã€‚ä¼ ç»Ÿå·¥å…·åœ¨å¤„ç†ç»†èƒç”Ÿç‰©å­¦çš„å¤æ‚æ•°æ®æ—¶æ•ˆç‡ä½ä¸‹ï¼Œè€ŒInstructCellé€šè¿‡å°†æ–‡æœ¬æŒ‡ä»¤ä¸scRNA-seqæ•°æ®ç»“åˆï¼Œæä¾›äº†æ›´ç›´æ¥å’Œçµæ´»çš„åˆ†ææ–¹å¼ã€‚è¯¥ç³»ç»Ÿèƒ½å¤Ÿæ‰§è¡Œç»†èƒç±»å‹æ³¨é‡Šã€æ¡ä»¶ä¼ªç»†èƒç”Ÿæˆå’Œè¯ç‰©æ•æ„Ÿæ€§é¢„æµ‹ç­‰å…³é”®ä»»åŠ¡ï¼Œä¸”ä½¿ç”¨ç®€å•çš„è‡ªç„¶è¯­è¨€å‘½ä»¤å³å¯å®Œæˆã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒInstructCellåœ¨æ€§èƒ½ä¸Šä¼˜äºç°æœ‰çš„å•ç»†èƒåŸºç¡€æ¨¡å‹ï¼ŒåŒæ—¶é€‚åº”å¤šç§å®éªŒæ¡ä»¶ï¼Œé™ä½äº†æŠ€æœ¯é—¨æ§›ï¼Œä¿ƒè¿›äº†ç”Ÿç‰©å­¦çš„æ·±å…¥ç†è§£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.08316",
            "title": "Diffusion Adversarial Post-Training for One-Step Video Generation",
            "url": "https://huggingface.co/papers/2501.08316",
            "abstract": "The diffusion models are widely used for image and video generation, but their iterative generation process is slow and expansive. While existing distillation approaches have demonstrated the potential for one-step generation in the image domain, they still suffer from significant quality degradation. In this work, we propose Adversarial Post-Training (APT) against real data following diffusion pre-training for one-step video generation. To improve the training stability and quality, we introduce several improvements to the model architecture and training procedures, along with an approximated R1 regularization objective. Empirically, our experiments show that our adversarial post-trained model, Seaweed-APT, can generate 2-second, 1280x720, 24fps videos in real time using a single forward evaluation step. Additionally, our model is capable of generating 1024px images in a single step, achieving quality comparable to state-of-the-art methods.",
            "score": 1,
            "issue_id": 1672,
            "pub_date": "2025-01-14",
            "pub_date_card": {
                "ru": "14 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 14",
                "zh": "1æœˆ14æ—¥"
            },
            "hash": "4122a780e8356ce7",
            "authors": [
                "Shanchuan Lin",
                "Xin Xia",
                "Yuxi Ren",
                "Ceyuan Yang",
                "Xuefeng Xiao",
                "Lu Jiang"
            ],
            "affiliations": [
                "ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.08316.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#video",
                    "#diffusion",
                    "#training"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾: Ğ¾Ñ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğº Ğ¼Ğ³Ğ½Ğ¾Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñƒ",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Adversarial Post-Training (APT) Ğ´Ğ»Ñ Ğ¾Ğ´Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ°Ğ¿Ğ¿Ñ€Ğ¾ĞºÑĞ¸Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ R1. Ğ˜Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Seaweed-APT ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ 2-ÑĞµĞºÑƒĞ½Ğ´Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 1024px Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ ÑˆĞ°Ğ³, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Fast and High-Quality Video Generation with Seaweed-APT",
                    "desc": "This paper addresses the slow and costly iterative process of generating images and videos using diffusion models. The authors introduce Adversarial Post-Training (APT) to enhance one-step video generation while maintaining high quality. They implement architectural and procedural improvements, including an approximated R1 regularization, to stabilize training. Their model, Seaweed-APT, successfully generates high-quality 2-second videos and 1024px images in real time with a single forward evaluation step."
                },
                "zh": {
                    "title": "å¯¹æŠ—åè®­ç»ƒï¼šå¿«é€Ÿé«˜è´¨é‡è§†é¢‘ç”Ÿæˆçš„æ–°æ–¹æ³•",
                    "desc": "æ‰©æ•£æ¨¡å‹å¹¿æ³›åº”ç”¨äºå›¾åƒå’Œè§†é¢‘ç”Ÿæˆï¼Œä½†å…¶è¿­ä»£ç”Ÿæˆè¿‡ç¨‹è¾ƒæ…¢ä¸”æˆæœ¬é«˜æ˜‚ã€‚ç°æœ‰çš„è’¸é¦æ–¹æ³•åœ¨å›¾åƒé¢†åŸŸå±•ç¤ºäº†å•æ­¥ç”Ÿæˆçš„æ½œåŠ›ï¼Œä½†ä»å­˜åœ¨æ˜¾è‘—çš„è´¨é‡ä¸‹é™ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹çœŸå®æ•°æ®çš„å¯¹æŠ—åè®­ç»ƒï¼ˆAPTï¼‰æ–¹æ³•ï¼Œä»¥å®ç°å•æ­¥è§†é¢‘ç”Ÿæˆã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œç»è¿‡å¯¹æŠ—åè®­ç»ƒçš„æ¨¡å‹Seaweed-APTèƒ½å¤Ÿå®æ—¶ç”Ÿæˆ1280x720ã€24fpsçš„2ç§’è§†é¢‘ï¼Œå¹¶ä¸”åœ¨å•æ­¥ç”Ÿæˆ1024pxå›¾åƒæ—¶ï¼Œå…¶è´¨é‡å¯ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸åª²ç¾ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-01-14.html",
    "link_next": "2025-01-16.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "14.01",
        "en": "01/14",
        "zh": "1æœˆ14æ—¥"
    },
    "short_date_next": {
        "ru": "16.01",
        "en": "01/16",
        "zh": "1æœˆ16æ—¥"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 1,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå«åšè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰ï¼Œç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•°å­¦æ¨ç†ä¸­çš„è¿‡ç¨‹ç›‘ç£ã€‚ç›®æ ‡æ˜¯è¯†åˆ«å’Œå‡å°‘æ¨ç†è¿‡ç¨‹ä¸­çš„é”™è¯¯ã€‚ç ”ç©¶å‘ç°ï¼Œå¸¸ç”¨çš„è’™ç‰¹å¡ç½—ï¼ˆMCï¼‰ä¼°è®¡æ–¹æ³•æ•ˆæœä¸ä½³ï¼Œå› ä¸ºå®ƒä¾èµ–å®Œæˆæ¨¡å‹è¯„ä¼°å½“å‰æ­¥éª¤çš„æ­£ç¡®æ€§ï¼Œå¯¼è‡´æ­¥éª¤éªŒè¯ä¸å‡†ç¡®ã€‚æ–‡ç« è¿˜æŒ‡å‡ºäº†ä¼ ç»ŸBest-of-Nï¼ˆBoNï¼‰è¯„ä¼°ç­–ç•¥çš„åå·®ï¼Œå¹¶æå‡ºäº†ä¸€ç§å…±è¯†è¿‡æ»¤æœºåˆ¶ï¼Œç»“åˆMCä¼°è®¡å’ŒLLM-as-a-judgeï¼Œæ”¹è¿›äº†æ¨¡å‹æ€§èƒ½å’Œæ•°æ®æ•ˆç‡ã€‚æœ€åï¼Œæ–‡ç« å‘å¸ƒäº†ä¸€ä¸ªæ–°çš„æœ€å…ˆè¿›çš„PRMï¼Œå¹¶æä¾›äº†æœªæ¥ç ”ç©¶çš„å®ç”¨æŒ‡å—ã€‚",
        "title": "The Lessons of Developing Process Reward Models in Mathematical Reasoning",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå«åšè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰ï¼Œç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•°å­¦æ¨ç†ä¸­çš„è¿‡ç¨‹ç›‘ç£ã€‚ç›®æ ‡æ˜¯è¯†åˆ«å’Œå‡å°‘æ¨ç†è¿‡ç¨‹ä¸­çš„é”™è¯¯ã€‚ç ”ç©¶å‘ç°ï¼Œå¸¸ç”¨çš„è’™ç‰¹å¡ç½—ï¼ˆMCï¼‰ä¼°è®¡æ–¹æ³•æ•ˆæœä¸ä½³ï¼Œå› ä¸ºå®ƒä¾èµ–å®Œæˆæ¨¡å‹è¯„ä¼°å½“å‰æ­¥éª¤çš„æ­£ç¡®æ€§ï¼Œå¯¼è‡´æ­¥éª¤éªŒè¯ä¸å‡†ç¡®ã€‚æ–‡ç« è¿˜æŒ‡å‡ºäº†ä¼ ç»ŸBest-of-Nï¼ˆBoNï¼‰è¯„ä¼°ç­–ç•¥çš„åå·®ï¼Œå¹¶æå‡ºäº†ä¸€ç§å…±è¯†è¿‡æ»¤æœºåˆ¶ï¼Œç»“åˆMCä¼°è®¡å’ŒLLM-as-a-judgeï¼Œæ”¹è¿›äº†æ¨¡å‹æ€§èƒ½å’Œæ•°æ®æ•ˆç‡ã€‚æœ€åï¼Œæ–‡ç« å‘å¸ƒäº†ä¸€ä¸ªæ–°çš„æœ€å…ˆè¿›çš„PRMï¼Œå¹¶æä¾›äº†æœªæ¥ç ”ç©¶çš„å®ç”¨æŒ‡å—ã€‚\n\nzhÃ¨ piÄn wÃ©nzhÄng jiÃ¨shÃ o le yÄ« zhÇ’ng xÄ«n de fÄngfÇ, jiÃ ozuÃ² guÃ²chÃ©ng jiÇnglÃ¬ mÃ³xÃ­ng (PRMs), yÃ²ngyÃº dÃ xÃ­ng yÇ”yÃ¡n mÃ³xÃ­ng (LLMs) zÃ i shÃ¹xuÃ© tuÄ«lÇ zhÅng de guÃ²chÃ©ng jiÃ ndÅ«. MÃ¹biÄo shÃ¬ shÃ­biÃ© hÃ© jiÇnshÇo tuÄ«lÇ guÃ²chÃ©ng zhÅng de cuÃ²wÃ¹. YÃ¡njiÅ« fÄxiÃ n, chÃ¡ngyÃ²ng de mÃ©ngtÃ¨kÇluÃ³ (MC) gÅ«jÃ¬ fÄngfÇ xiÃ ojiÃ , yÄ«nwÃ¨i tÄ yÄ«lÃ i wÃ¡nchÃ©ng mÃ³xÃ­ng pÃ­ngjiÃ  dÄngqiÃ¡n bÃ¹zhÃ²u de zhÃ¨ngquÃ¨xÃ¬ng, dÇozhÃ¬ bÃ¹zhÃ²u yÃ nzhÃ¨ng bÃ¹ zhÇ”nquÃ¨. WÃ©nzhÄng hÃ¡i zhÇchÅ« le chuÃ¡ntÇ’ng Best-of-N (BoN) pÃ­ngjiÃ  cÃ¨lÃ¼Ã¨ de piÄnchÄ, bÃ¬ng tÃ­chÅ« le yÄ« zhÇ’ng gÃ²ngshÃ¬ guÃ²lÇœ jÄ«zhÃ¬, jiÃ©hÃ© MC gÅ«jÃ¬ hÃ© LLM-as-a-judge, gÇijÃ¬n le mÃ³xÃ­ng xÃ¬ngnÃ©ng hÃ© shÃ¹jÃ¹ xiÃ oyÃ²ng. ZuÃ¬hÃ²u, wÃ©nzhÄng fÄbÃ¹ le yÄ«gÃ¨ xÄ«n de zuÃ¬ xiÄnjÃ¬n de PRM, bÃ¬ng tÃ­gÅng le wÃ¨ilÃ¡i yÃ¡njiÅ« de shÃ­yÃ²ng zhÇnÃ¡n.",
        "vocab": "[\n    {\"word\": \"è¿‡ç¨‹å¥–åŠ±æ¨¡å‹\", \"pinyin\": \"guÃ²chÃ©ng jiÇnglÃ¬ mÃ³xÃ­ng\", \"trans\": \"Process Reward Model\"},\n    {\"word\": \"å¤§å‹è¯­è¨€æ¨¡å‹\", \"pinyin\": \"dÃ xÃ­ng yÇ”yÃ¡n mÃ³xÃ­ng\", \"trans\": \"Large Language Model\"},\n    {\"word\": \"æ•°å­¦æ¨ç†\", \"pinyin\": \"shÃ¹xuÃ© tuÄ«lÇ\", \"trans\": \"Mathematical Reasoning\"},\n    {\"word\": \"è¿‡ç¨‹ç›‘ç£\", \"pinyin\": \"guÃ²chÃ©ng jiÃ ndÅ«\", \"trans\": \"Process Supervision\"},\n    {\"word\": \"è’™ç‰¹å¡ç½—\", \"pinyin\": \"mÃ©ngtÃ¨kÇluÃ³\", \"trans\": \"Monte Carlo\"},\n    {\"word\": \"ä¼°è®¡\", \"pinyin\": \"gÅ«jÃ¬\", \"trans\": \"Estimation\"},\n    {\"word\": \"ä¾èµ–\", \"pinyin\": \"yÄ«lÃ i\", \"trans\": \"Depend\"},\n    {\"word\": \"è¯„ä¼°\", \"pinyin\": \"pÃ­nggÅ«\", \"trans\": \"Evaluate\"},\n    {\"word\": \"æ­¥éª¤éªŒè¯\", \"pinyin\": \"bÃ¹zhÃ²u yÃ nzhÃ¨ng\", \"trans\": \"Step Verification\"},\n    {\"word\": \"åå·®\", \"pinyin\": \"piÄnchÄ\", \"trans\": \"Bias\"},\n    {\"word\": \"å…±è¯†è¿‡æ»¤æœºåˆ¶\", \"pinyin\": \"gÃ²ngshÃ­ guÃ²lÇœ jÄ«zhÃ¬\", \"trans\": \"Consensus Filtering Mechanism\"},\n    {\"word\": \"LLM-as-a-judge\", \"pinyin\": \"LLM-as-a-judge\", \"trans\": \"LLM-as-a-judge\"},\n    {\"word\": \"æœ€å…ˆè¿›\", \"pinyin\": \"zuÃ¬xiÄnjÃ¬n\", \"trans\": \"State-of-the-art\"},\n    {\"word\": \"å®ç”¨æŒ‡å—\", \"pinyin\": \"shÃ­yÃ²ng zhÇnÃ¡n\", \"trans\": \"Practical Guide\"}\n]",
        "trans": "This article introduces a new method called Process Reward Models (PRMs) for process supervision of large language models (LLMs) in mathematical reasoning. The goal is to identify and reduce errors in the reasoning process. The research found that the commonly used Monte Carlo (MC) estimation method performs poorly because it relies on the completion model to evaluate the correctness of the current step, leading to inaccurate step verification. The article also points out the bias in traditional Best-of-N (BoN) evaluation strategies and proposes a consensus filtering mechanism that combines MC estimation and LLM-as-a-judge to improve model performance and data efficiency. Finally, the article releases a new state-of-the-art PRM and provides practical guidelines for future research.",
        "update_ts": "2025-01-14 09:10"
    }
}