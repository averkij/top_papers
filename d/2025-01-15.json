{
    "date": {
        "ru": "15 января",
        "en": "January 15",
        "zh": "1月15日"
    },
    "time_utc": "2025-01-15 04:12",
    "weekday": 2,
    "issue_id": 1673,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.08313",
            "title": "MiniMax-01: Scaling Foundation Models with Lightning Attention",
            "url": "https://huggingface.co/papers/2501.08313",
            "abstract": "We introduce MiniMax-01 series, including MiniMax-Text-01 and MiniMax-VL-01, which are comparable to top-tier models while offering superior capabilities in processing longer contexts. The core lies in lightning attention and its efficient scaling. To maximize computational capacity, we integrate it with Mixture of Experts (MoE), creating a model with 32 experts and 456 billion total parameters, of which 45.9 billion are activated for each token. We develop an optimized parallel strategy and highly efficient computation-communication overlap techniques for MoE and lightning attention. This approach enables us to conduct efficient training and inference on models with hundreds of billions of parameters across contexts spanning millions of tokens. The context window of MiniMax-Text-01 can reach up to 1 million tokens during training and extrapolate to 4 million tokens during inference at an affordable cost. Our vision-language model, MiniMax-VL-01 is built through continued training with 512 billion vision-language tokens. Experiments on both standard and in-house benchmarks show that our models match the performance of state-of-the-art models like GPT-4o and Claude-3.5-Sonnet while offering 20-32 times longer context window. We publicly release MiniMax-01 at https://github.com/MiniMax-AI.",
            "score": 118,
            "issue_id": 1672,
            "pub_date": "2025-01-14",
            "pub_date_card": {
                "ru": "14 января",
                "en": "January 14",
                "zh": "1月14日"
            },
            "hash": "a57d7b1914e7383a",
            "authors": [
                "MiniMax",
                "Aonian Li",
                "Bangwei Gong",
                "Bo Yang",
                "Boji Shan",
                "Chang Liu",
                "Cheng Zhu",
                "Chunhao Zhang",
                "Congchao Guo",
                "Da Chen",
                "Dong Li",
                "Enwei Jiao",
                "Gengxin Li",
                "Guojun Zhang",
                "Haohai Sun",
                "Houze Dong",
                "Jiadai Zhu",
                "Jiaqi Zhuang",
                "Jiayuan Song",
                "Jin Zhu",
                "Jingtao Han",
                "Jingyang Li",
                "Junbin Xie",
                "Junhao Xu",
                "Junjie Yan",
                "Kaishun Zhang",
                "Kecheng Xiao",
                "Kexi Kang",
                "Le Han",
                "Leyang Wang",
                "Lianfei Yu",
                "Liheng Feng",
                "Lin Zheng",
                "Linbo Chai",
                "Long Xing",
                "Meizhi Ju",
                "Mingyuan Chi",
                "Mozhi Zhang",
                "Peikai Huang",
                "Pengcheng Niu",
                "Pengfei Li",
                "Pengyu Zhao",
                "Qi Yang",
                "Qidi Xu",
                "Qiexiang Wang",
                "Qin Wang",
                "Qiuhui Li",
                "Ruitao Leng",
                "Shengmin Shi",
                "Shuqi Yu",
                "Sichen Li",
                "Songquan Zhu",
                "Tao Huang",
                "Tianrun Liang",
                "Weigao Sun",
                "Weixuan Sun",
                "Weiyu Cheng",
                "Wenkai Li",
                "Xiangjun Song",
                "Xiao Su",
                "Xiaodong Han",
                "Xinjie Zhang",
                "Xinzhu Hou",
                "Xu Min",
                "Xun Zou",
                "Xuyang Shen",
                "Yan Gong",
                "Yingjie Zhu",
                "Yipeng Zhou",
                "Yiran Zhong",
                "Yongyi Hu",
                "Yuanxiang Fan",
                "Yue Yu",
                "Yufeng Yang",
                "Yuhao Li",
                "Yunan Huang",
                "Yunji Li",
                "Yunpeng Huang",
                "Yunzhi Xu",
                "Yuxin Mao",
                "Zehan Li",
                "Zekang Li",
                "Zewei Tao",
                "Zewen Ying",
                "Zhaoyang Cong",
                "Zhen Qin",
                "Zhenhua Fan",
                "Zhihang Yu",
                "Zhuo Jiang",
                "Zijia Wu"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2501.08313.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#architecture",
                    "#optimization",
                    "#benchmark",
                    "#long_context",
                    "#training"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "MiniMax-01: Революция в обработке длинных контекстов",
                    "desc": "Исследователи представили серию моделей MiniMax-01, включая MiniMax-Text-01 и MiniMax-VL-01, которые сравнимы с лучшими моделями, но обладают улучшенными возможностями обработки длинных контекстов. В основе лежит технология lightning attention и ее эффективное масштабирование, интегрированные с Mixture of Experts (MoE). Модель имеет 32 эксперта и 456 миллиардов параметров, из которых 45,9 миллиардов активируются для каждого токена. Контекстное окно MiniMax-Text-01 может достигать 1 миллиона токенов при обучении и экстраполироваться до 4 миллионов токенов при инференсе."
                },
                "en": {
                    "title": "Unleashing Long Contexts with MiniMax-01 Models",
                    "desc": "The MiniMax-01 series introduces advanced models, MiniMax-Text-01 and MiniMax-VL-01, designed to handle longer contexts effectively. These models utilize lightning attention and a Mixture of Experts (MoE) architecture, featuring 32 experts and a staggering 456 billion parameters, optimizing the activation of 45.9 billion parameters per token. By implementing efficient parallel strategies and computation-communication overlap techniques, the models can train and infer on extensive datasets, reaching context windows of up to 1 million tokens during training and 4 million during inference. Performance evaluations indicate that MiniMax-01 models rival leading models like GPT-4o and Claude-3.5-Sonnet while significantly extending context capabilities."
                },
                "zh": {
                    "title": "MiniMax-01：超长上下文处理的新纪元",
                    "desc": "我们介绍了MiniMax-01系列，包括MiniMax-Text-01和MiniMax-VL-01，这些模型在处理更长的上下文时具有优越的能力。核心技术是闪电注意力和高效的扩展能力。为了最大化计算能力，我们将其与专家混合模型（MoE）结合，创建了一个拥有32个专家和4560亿参数的模型。我们的实验表明，这些模型在标准和内部基准测试中表现出色，能够与最先进的模型相媲美，同时提供20到32倍更长的上下文窗口。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.08187",
            "title": "A Multi-Modal AI Copilot for Single-Cell Analysis with Instruction Following",
            "url": "https://huggingface.co/papers/2501.08187",
            "abstract": "Large language models excel at interpreting complex natural language instructions, enabling them to perform a wide range of tasks. In the life sciences, single-cell RNA sequencing (scRNA-seq) data serves as the \"language of cellular biology\", capturing intricate gene expression patterns at the single-cell level. However, interacting with this \"language\" through conventional tools is often inefficient and unintuitive, posing challenges for researchers. To address these limitations, we present InstructCell, a multi-modal AI copilot that leverages natural language as a medium for more direct and flexible single-cell analysis. We construct a comprehensive multi-modal instruction dataset that pairs text-based instructions with scRNA-seq profiles from diverse tissues and species. Building on this, we develop a multi-modal cell language architecture capable of simultaneously interpreting and processing both modalities. InstructCell empowers researchers to accomplish critical tasks-such as cell type annotation, conditional pseudo-cell generation, and drug sensitivity prediction-using straightforward natural language commands. Extensive evaluations demonstrate that InstructCell consistently meets or exceeds the performance of existing single-cell foundation models, while adapting to diverse experimental conditions. More importantly, InstructCell provides an accessible and intuitive tool for exploring complex single-cell data, lowering technical barriers and enabling deeper biological insights.",
            "score": 14,
            "issue_id": 1672,
            "pub_date": "2025-01-14",
            "pub_date_card": {
                "ru": "14 января",
                "en": "January 14",
                "zh": "1月14日"
            },
            "hash": "de984ce7cc62fa5e",
            "authors": [
                "Yin Fang",
                "Xinle Deng",
                "Kangwei Liu",
                "Ningyu Zhang",
                "Jingyang Qian",
                "Penghui Yang",
                "Xiaohui Fan",
                "Huajun Chen"
            ],
            "affiliations": [
                "College of Computer Science and Technology, Zhejiang University, Hangzhou 310027, China",
                "College of Pharmaceutical Sciences, Zhejiang University, Hangzhou 310058, China",
                "Future Health Laboratory, Innovation Center of Yangtze River Delta, Zhejiang University, Jiaxing 314100, China",
                "Innovation Center in Zhejiang University, State Key Laboratory of Component-Based Chinese Medicine, Hangzhou 310058, China",
                "School of Software Technology, Zhejiang University, Ningbo 315048, China",
                "ZJU-Hangzhou Global Scientific and Technological Innovation Center, Hangzhou 311200, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.08187.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#multimodal",
                    "#dataset",
                    "#science",
                    "#healthcare"
                ],
                "emoji": "🧬",
                "ru": {
                    "title": "Естественный язык как ключ к расшифровке клеточной биологии",
                    "desc": "InstructCell - это мультимодальный ИИ-помощник для анализа данных одноклеточного РНК-секвенирования (scRNA-seq). Он использует архитектуру, способную интерпретировать как естественный язык, так и профили экспрессии генов. InstructCell позволяет исследователям выполнять такие задачи, как аннотация типов клеток и предсказание чувствительности к лекарствам, с помощью простых текстовых команд. Модель демонстрирует высокую производительность и адаптивность к различным экспериментальным условиям."
                },
                "en": {
                    "title": "InstructCell: Bridging Language and Biology for Seamless Single-Cell Analysis",
                    "desc": "This paper introduces InstructCell, an AI tool designed to simplify the analysis of single-cell RNA sequencing (scRNA-seq) data using natural language instructions. By creating a dataset that links text commands with scRNA-seq profiles, InstructCell allows researchers to perform complex tasks like cell type annotation and drug sensitivity prediction more intuitively. The model employs a multi-modal architecture that processes both text and biological data simultaneously, enhancing its usability. Evaluations show that InstructCell outperforms existing models, making single-cell analysis more accessible and efficient for researchers in the life sciences."
                },
                "zh": {
                    "title": "用自然语言解锁单细胞数据的潜力",
                    "desc": "这篇论文介绍了InstructCell，一个多模态的人工智能助手，旨在通过自然语言简化单细胞RNA测序(scRNA-seq)数据的分析。传统工具在处理细胞生物学的复杂数据时效率低下，而InstructCell通过将文本指令与scRNA-seq数据结合，提供了更直接和灵活的分析方式。该系统能够执行细胞类型注释、条件伪细胞生成和药物敏感性预测等关键任务，且使用简单的自然语言命令即可完成。评估结果表明，InstructCell在性能上优于现有的单细胞基础模型，同时适应多种实验条件，降低了技术门槛，促进了生物学的深入理解。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.08332",
            "title": "MangaNinja: Line Art Colorization with Precise Reference Following",
            "url": "https://huggingface.co/papers/2501.08332",
            "abstract": "Derived from diffusion models, MangaNinjia specializes in the task of reference-guided line art colorization. We incorporate two thoughtful designs to ensure precise character detail transcription, including a patch shuffling module to facilitate correspondence learning between the reference color image and the target line art, and a point-driven control scheme to enable fine-grained color matching. Experiments on a self-collected benchmark demonstrate the superiority of our model over current solutions in terms of precise colorization. We further showcase the potential of the proposed interactive point control in handling challenging cases, cross-character colorization, multi-reference harmonization, beyond the reach of existing algorithms.",
            "score": 7,
            "issue_id": 1673,
            "pub_date": "2025-01-14",
            "pub_date_card": {
                "ru": "14 января",
                "en": "January 14",
                "zh": "1月14日"
            },
            "hash": "20ea6b75639e2ced",
            "authors": [
                "Zhiheng Liu",
                "Ka Leong Cheng",
                "Xi Chen",
                "Jie Xiao",
                "Hao Ouyang",
                "Kai Zhu",
                "Yu Liu",
                "Yujun Shen",
                "Qifeng Chen",
                "Ping Luo"
            ],
            "affiliations": [
                "Ant Group",
                "HKU",
                "HKUST",
                "Tongyi Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.08332.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#diffusion",
                    "#benchmark"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Прецизионное раскрашивание манги с помощью ИИ",
                    "desc": "MangaNinjia - это модель для раскрашивания линейных рисунков манги, основанная на диффузионных моделях. Она использует модуль перемешивания патчей для обучения соответствиям между цветным изображением-образцом и целевым линейным рисунком. Модель также включает схему точечного контроля для точного подбора цветов. Эксперименты показывают превосходство MangaNinjia над существующими решениями в точности раскрашивания."
                },
                "en": {
                    "title": "MangaNinjia: Mastering Line Art Colorization with Precision",
                    "desc": "MangaNinjia is a model designed for coloring line art by using reference images. It employs a patch shuffling module to help the model learn how to match colors from the reference image to the target line art accurately. Additionally, it features a point-driven control scheme that allows for detailed color adjustments, ensuring that colors are applied precisely. Our experiments show that MangaNinjia outperforms existing methods in colorization tasks, especially in complex scenarios involving multiple references and different characters."
                },
                "zh": {
                    "title": "MangaNinjia：精准上色的新方法",
                    "desc": "MangaNinjia 是一种基于扩散模型的参考引导线条艺术上色技术。我们设计了两个模块来确保角色细节的准确转录，包括补丁洗牌模块和点驱动控制方案，以实现精细的颜色匹配。实验结果表明，我们的模型在精确上色方面优于现有解决方案。我们还展示了所提议的交互式点控制在处理复杂案例和多参考协调方面的潜力，超越了现有算法的能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.08316",
            "title": "Diffusion Adversarial Post-Training for One-Step Video Generation",
            "url": "https://huggingface.co/papers/2501.08316",
            "abstract": "The diffusion models are widely used for image and video generation, but their iterative generation process is slow and expansive. While existing distillation approaches have demonstrated the potential for one-step generation in the image domain, they still suffer from significant quality degradation. In this work, we propose Adversarial Post-Training (APT) against real data following diffusion pre-training for one-step video generation. To improve the training stability and quality, we introduce several improvements to the model architecture and training procedures, along with an approximated R1 regularization objective. Empirically, our experiments show that our adversarial post-trained model, Seaweed-APT, can generate 2-second, 1280x720, 24fps videos in real time using a single forward evaluation step. Additionally, our model is capable of generating 1024px images in a single step, achieving quality comparable to state-of-the-art methods.",
            "score": 7,
            "issue_id": 1672,
            "pub_date": "2025-01-14",
            "pub_date_card": {
                "ru": "14 января",
                "en": "January 14",
                "zh": "1月14日"
            },
            "hash": "4122a780e8356ce7",
            "authors": [
                "Shanchuan Lin",
                "Xin Xia",
                "Yuxi Ren",
                "Ceyuan Yang",
                "Xuefeng Xiao",
                "Lu Jiang"
            ],
            "affiliations": [
                "ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.08316.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#video",
                    "#diffusion",
                    "#training"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Революция в генерации видео: от итераций к мгновенному результату",
                    "desc": "Эта статья представляет новый метод под названием Adversarial Post-Training (APT) для одношаговой генерации видео. Авторы предлагают улучшения архитектуры модели и процедур обучения, включая аппроксимированную регуляризацию R1. Их модель Seaweed-APT способна генерировать 2-секундные видео высокого разрешения в реальном времени за один проход. Кроме того, модель может создавать изображения размером 1024px за один шаг, достигая качества, сравнимого с современными методами."
                },
                "en": {
                    "title": "Fast and High-Quality Video Generation with Seaweed-APT",
                    "desc": "This paper addresses the slow and costly iterative process of generating images and videos using diffusion models. The authors introduce Adversarial Post-Training (APT) to enhance one-step video generation while maintaining high quality. They implement architectural and procedural improvements, including an approximated R1 regularization, to stabilize training. Their model, Seaweed-APT, successfully generates high-quality 2-second videos and 1024px images in real time with a single forward evaluation step."
                },
                "zh": {
                    "title": "对抗后训练：快速高质量视频生成的新方法",
                    "desc": "扩散模型广泛应用于图像和视频生成，但其迭代生成过程较慢且成本高昂。现有的蒸馏方法在图像领域展示了单步生成的潜力，但仍存在显著的质量下降。本文提出了一种针对真实数据的对抗后训练（APT）方法，以实现单步视频生成。我们的实验表明，经过对抗后训练的模型Seaweed-APT能够实时生成1280x720、24fps的2秒视频，并且在单步生成1024px图像时，其质量可与最先进的方法相媲美。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.08225",
            "title": "FramePainter: Endowing Interactive Image Editing with Video Diffusion Priors",
            "url": "https://huggingface.co/papers/2501.08225",
            "abstract": "Interactive image editing allows users to modify images through visual interaction operations such as drawing, clicking, and dragging. Existing methods construct such supervision signals from videos, as they capture how objects change with various physical interactions. However, these models are usually built upon text-to-image diffusion models, so necessitate (i) massive training samples and (ii) an additional reference encoder to learn real-world dynamics and visual consistency. In this paper, we reformulate this task as an image-to-video generation problem, so that inherit powerful video diffusion priors to reduce training costs and ensure temporal consistency. Specifically, we introduce FramePainter as an efficient instantiation of this formulation. Initialized with Stable Video Diffusion, it only uses a lightweight sparse control encoder to inject editing signals. Considering the limitations of temporal attention in handling large motion between two frames, we further propose matching attention to enlarge the receptive field while encouraging dense correspondence between edited and source image tokens. We highlight the effectiveness and efficiency of FramePainter across various of editing signals: it domainantly outperforms previous state-of-the-art methods with far less training data, achieving highly seamless and coherent editing of images, \\eg, automatically adjust the reflection of the cup. Moreover, FramePainter also exhibits exceptional generalization in scenarios not present in real-world videos, \\eg, transform the clownfish into shark-like shape. Our code will be available at https://github.com/YBYBZhang/FramePainter.",
            "score": 3,
            "issue_id": 1673,
            "pub_date": "2025-01-14",
            "pub_date_card": {
                "ru": "14 января",
                "en": "January 14",
                "zh": "1月14日"
            },
            "hash": "811cfd0f18eb1e53",
            "authors": [
                "Yabo Zhang",
                "Xinpeng Zhou",
                "Yihan Zeng",
                "Hang Xu",
                "Hui Li",
                "Wangmeng Zuo"
            ],
            "affiliations": [
                "Harbin Institute of Technology",
                "Huawei Noahs Ark Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.08225.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#cv",
                    "#optimization",
                    "#diffusion"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "FramePainter: эффективное редактирование изображений через генерацию видео",
                    "desc": "Статья представляет FramePainter - новый подход к интерактивному редактированию изображений, основанный на генерации видео. В отличие от существующих методов, использующих модели диффузии текст-изображение, FramePainter опирается на мощные видео-диффузионные модели для обеспечения временной согласованности и снижения затрат на обучение. Метод использует легковесный энкодер для внедрения сигналов редактирования и вводит механизм согласованного внимания для улучшения обработки крупных движений между кадрами. FramePainter превосходит современные методы, требуя значительно меньше обучающих данных и демонстрируя высокую обобщающую способность."
                },
                "en": {
                    "title": "Revolutionizing Image Editing with Efficient Video Diffusion",
                    "desc": "This paper presents FramePainter, a novel approach to interactive image editing that reformulates the task as image-to-video generation. By leveraging video diffusion models, FramePainter reduces the need for extensive training data while ensuring temporal consistency in edited images. It utilizes a lightweight sparse control encoder to effectively incorporate editing signals, and introduces matching attention to improve the handling of large motion between frames. The results demonstrate that FramePainter significantly outperforms existing methods, achieving seamless image edits and showcasing strong generalization capabilities."
                },
                "zh": {
                    "title": "FramePainter：高效的图像编辑新方法",
                    "desc": "本文提出了一种交互式图像编辑的新方法，称为FramePainter。该方法将图像编辑任务重新定义为图像到视频的生成问题，从而利用强大的视频扩散先验，降低训练成本并确保时间一致性。FramePainter使用轻量级的稀疏控制编码器来注入编辑信号，并通过匹配注意力机制增强了对大运动的处理能力。实验结果表明，FramePainter在各种编辑信号下表现优异，能够实现无缝且连贯的图像编辑，且在未见过的场景中也展现出卓越的泛化能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.07730",
            "title": "Democratizing Text-to-Image Masked Generative Models with Compact Text-Aware One-Dimensional Tokens",
            "url": "https://huggingface.co/papers/2501.07730",
            "abstract": "Image tokenizers form the foundation of modern text-to-image generative models but are notoriously difficult to train. Furthermore, most existing text-to-image models rely on large-scale, high-quality private datasets, making them challenging to replicate. In this work, we introduce Text-Aware Transformer-based 1-Dimensional Tokenizer (TA-TiTok), an efficient and powerful image tokenizer that can utilize either discrete or continuous 1-dimensional tokens. TA-TiTok uniquely integrates textual information during the tokenizer decoding stage (i.e., de-tokenization), accelerating convergence and enhancing performance. TA-TiTok also benefits from a simplified, yet effective, one-stage training process, eliminating the need for the complex two-stage distillation used in previous 1-dimensional tokenizers. This design allows for seamless scalability to large datasets. Building on this, we introduce a family of text-to-image Masked Generative Models (MaskGen), trained exclusively on open data while achieving comparable performance to models trained on private data. We aim to release both the efficient, strong TA-TiTok tokenizers and the open-data, open-weight MaskGen models to promote broader access and democratize the field of text-to-image masked generative models.",
            "score": 0,
            "issue_id": 1673,
            "pub_date": "2025-01-13",
            "pub_date_card": {
                "ru": "13 января",
                "en": "January 13",
                "zh": "1月13日"
            },
            "hash": "80f40715084c602b",
            "authors": [
                "Dongwon Kim",
                "Ju He",
                "Qihang Yu",
                "Chenglin Yang",
                "Xiaohui Shen",
                "Suha Kwak",
                "Liang-Chieh Chen"
            ],
            "affiliations": [
                "ByteDance Seed",
                "POSTECH"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.07730.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#data",
                    "#training",
                    "#cv",
                    "#open_source"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Демократизация генерации изображений с помощью эффективной токенизации и открытых данных",
                    "desc": "В этой статье представлен новый подход к токенизации изображений для генеративных моделей текст-в-изображение под названием TA-TiTok. Данный токенизатор использует одномерные токены и интегрирует текстовую информацию на этапе детокенизации, что ускоряет сходимость и улучшает производительность. На основе TA-TiTok авторы разработали семейство моделей MaskGen, обученных исключительно на открытых данных. Целью работы является демократизация области генеративных моделей текст-в-изображение путем публикации эффективных токенизаторов и моделей с открытыми весами."
                },
                "en": {
                    "title": "Democratizing Text-to-Image Generation with TA-TiTok",
                    "desc": "This paper presents TA-TiTok, a novel image tokenizer designed for text-to-image generative models, which simplifies the training process and improves performance. Unlike traditional models that require large private datasets, TA-TiTok can effectively utilize open data, making it more accessible for researchers. The tokenizer incorporates textual information during the decoding stage, which helps it learn faster and perform better. Additionally, the authors introduce MaskGen, a family of generative models that leverage TA-TiTok and are trained on publicly available datasets, aiming to democratize access to advanced text-to-image generation technology."
                },
                "zh": {
                    "title": "高效的文本到图像生成模型，推动开放数据的使用",
                    "desc": "本文介绍了一种新的图像标记器，称为TA-TiTok，它可以有效地处理文本到图像的生成任务。TA-TiTok在解码阶段整合了文本信息，从而加快了模型的收敛速度并提高了性能。与以往的标记器不同，TA-TiTok采用了一种简化的一阶段训练过程，避免了复杂的两阶段蒸馏过程。我们还提出了一系列基于开放数据训练的文本到图像生成模型MaskGen，旨在促进更广泛的访问和民主化。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.08292",
            "title": "HALoGEN: Fantastic LLM Hallucinations and Where to Find Them",
            "url": "https://huggingface.co/papers/2501.08292",
            "abstract": "Despite their impressive ability to generate high-quality and fluent text, generative large language models (LLMs) also produce hallucinations: statements that are misaligned with established world knowledge or provided input context. However, measuring hallucination can be challenging, as having humans verify model generations on-the-fly is both expensive and time-consuming. In this work, we release HALoGEN, a comprehensive hallucination benchmark consisting of: (1) 10,923 prompts for generative models spanning nine domains including programming, scientific attribution, and summarization, and (2) automatic high-precision verifiers for each use case that decompose LLM generations into atomic units, and verify each unit against a high-quality knowledge source. We use this framework to evaluate ~150,000 generations from 14 language models, finding that even the best-performing models are riddled with hallucinations (sometimes up to 86% of generated atomic facts depending on the domain). We further define a novel error classification for LLM hallucinations based on whether they likely stem from incorrect recollection of training data (Type A errors), or incorrect knowledge in training data (Type B errors), or are fabrication (Type C errors). We hope our framework provides a foundation to enable the principled study of why generative models hallucinate, and advances the development of trustworthy large language models.",
            "score": 0,
            "issue_id": 1673,
            "pub_date": "2025-01-14",
            "pub_date_card": {
                "ru": "14 января",
                "en": "January 14",
                "zh": "1月14日"
            },
            "hash": "f6751d682ff824ed",
            "authors": [
                "Abhilasha Ravichander",
                "Shrusti Ghela",
                "David Wadden",
                "Yejin Choi"
            ],
            "affiliations": [
                "Google",
                "NVIDIA",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.08292.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#hallucinations",
                    "#benchmark"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "HALoGEN: Автоматическая проверка галлюцинаций в языковых моделях",
                    "desc": "Эта статья представляет HALoGEN - комплексный инструмент для оценки галлюцинаций в больших языковых моделях (LLM). Авторы создали набор из 10,923 промптов в девяти различных областях и автоматические верификаторы высокой точности для проверки генераций LLM. Исследование выявило, что даже лучшие модели страдают от галлюцинаций, иногда до 86% сгенерированных фактов оказываются неверными. Авторы также предложили новую классификацию ошибок LLM, разделив их на три типа в зависимости от источника галлюцинаций."
                },
                "en": {
                    "title": "HALoGEN: A Benchmark for Measuring Hallucinations in Language Models",
                    "desc": "This paper introduces HALoGEN, a new benchmark designed to measure hallucinations in generative large language models (LLMs). Hallucinations refer to incorrect statements generated by these models that do not align with known facts or the given context. The benchmark includes over 10,000 prompts across various domains and employs automatic verifiers to assess the accuracy of model outputs. The study reveals that even top-performing models exhibit significant hallucinations, prompting a classification system for different types of errors to better understand their origins and improve model reliability."
                },
                "zh": {
                    "title": "揭示生成模型的幻觉问题",
                    "desc": "尽管生成性大型语言模型（LLMs）能够生成高质量和流畅的文本，但它们也会产生幻觉，即与已知世界知识或输入上下文不一致的陈述。测量幻觉的难度在于，实时验证模型生成的内容既昂贵又耗时。为此，我们推出了HALoGEN，这是一个全面的幻觉基准，包含10,923个跨越九个领域的提示和自动高精度验证器。我们的研究发现，即使是表现最好的模型，其生成的原子事实中也有高达86%可能存在幻觉，这为理解生成模型的幻觉提供了基础。"
                }
            }
        }
    ],
    "link_prev": "2025-01-14.html",
    "link_next": "2025-01-16.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "14.01",
        "en": "01/14",
        "zh": "1月14日"
    },
    "short_date_next": {
        "ru": "16.01",
        "en": "01/16",
        "zh": "1月16日"
    },
    "categories": {
        "#dataset": 3,
        "#data": 1,
        "#benchmark": 3,
        "#agents": 0,
        "#cv": 3,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 1,
        "#training": 3,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 3,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章介绍了一种新的方法，叫做过程奖励模型（PRMs），用于大型语言模型（LLMs）在数学推理中的过程监督。目标是识别和减少推理过程中的错误。研究发现，常用的蒙特卡罗（MC）估计方法效果不佳，因为它依赖完成模型评估当前步骤的正确性，导致步骤验证不准确。文章还指出了传统Best-of-N（BoN）评估策略的偏差，并提出了一种共识过滤机制，结合MC估计和LLM-as-a-judge，改进了模型性能和数据效率。最后，文章发布了一个新的最先进的PRM，并提供了未来研究的实用指南。",
        "title": "The Lessons of Developing Process Reward Models in Mathematical Reasoning",
        "pinyin": "这篇文章介绍了一种新的方法，叫做过程奖励模型（PRMs），用于大型语言模型（LLMs）在数学推理中的过程监督。目标是识别和减少推理过程中的错误。研究发现，常用的蒙特卡罗（MC）估计方法效果不佳，因为它依赖完成模型评估当前步骤的正确性，导致步骤验证不准确。文章还指出了传统Best-of-N（BoN）评估策略的偏差，并提出了一种共识过滤机制，结合MC估计和LLM-as-a-judge，改进了模型性能和数据效率。最后，文章发布了一个新的最先进的PRM，并提供了未来研究的实用指南。\n\nzhè piān wénzhāng jièshào le yī zhǒng xīn de fāngfǎ, jiàozuò guòchéng jiǎnglì móxíng (PRMs), yòngyú dàxíng yǔyán móxíng (LLMs) zài shùxué tuīlǐ zhōng de guòchéng jiàndū. Mùbiāo shì shíbié hé jiǎnshǎo tuīlǐ guòchéng zhōng de cuòwù. Yánjiū fāxiàn, chángyòng de méngtèkǎluó (MC) gūjì fāngfǎ xiàojià, yīnwèi tā yīlài wánchéng móxíng píngjià dāngqián bùzhòu de zhèngquèxìng, dǎozhì bùzhòu yànzhèng bù zhǔnquè. Wénzhāng hái zhǐchū le chuántǒng Best-of-N (BoN) píngjià cèlüè de piānchā, bìng tíchū le yī zhǒng gòngshì guòlǜ jīzhì, jiéhé MC gūjì hé LLM-as-a-judge, gǎijìn le móxíng xìngnéng hé shùjù xiàoyòng. Zuìhòu, wénzhāng fābù le yīgè xīn de zuì xiānjìn de PRM, bìng tígōng le wèilái yánjiū de shíyòng zhǐnán.",
        "vocab": "[\n    {\"word\": \"过程奖励模型\", \"pinyin\": \"guòchéng jiǎnglì móxíng\", \"trans\": \"Process Reward Model\"},\n    {\"word\": \"大型语言模型\", \"pinyin\": \"dàxíng yǔyán móxíng\", \"trans\": \"Large Language Model\"},\n    {\"word\": \"数学推理\", \"pinyin\": \"shùxué tuīlǐ\", \"trans\": \"Mathematical Reasoning\"},\n    {\"word\": \"过程监督\", \"pinyin\": \"guòchéng jiàndū\", \"trans\": \"Process Supervision\"},\n    {\"word\": \"蒙特卡罗\", \"pinyin\": \"méngtèkǎluó\", \"trans\": \"Monte Carlo\"},\n    {\"word\": \"估计\", \"pinyin\": \"gūjì\", \"trans\": \"Estimation\"},\n    {\"word\": \"依赖\", \"pinyin\": \"yīlài\", \"trans\": \"Depend\"},\n    {\"word\": \"评估\", \"pinyin\": \"pínggū\", \"trans\": \"Evaluate\"},\n    {\"word\": \"步骤验证\", \"pinyin\": \"bùzhòu yànzhèng\", \"trans\": \"Step Verification\"},\n    {\"word\": \"偏差\", \"pinyin\": \"piānchā\", \"trans\": \"Bias\"},\n    {\"word\": \"共识过滤机制\", \"pinyin\": \"gòngshí guòlǜ jīzhì\", \"trans\": \"Consensus Filtering Mechanism\"},\n    {\"word\": \"LLM-as-a-judge\", \"pinyin\": \"LLM-as-a-judge\", \"trans\": \"LLM-as-a-judge\"},\n    {\"word\": \"最先进\", \"pinyin\": \"zuìxiānjìn\", \"trans\": \"State-of-the-art\"},\n    {\"word\": \"实用指南\", \"pinyin\": \"shíyòng zhǐnán\", \"trans\": \"Practical Guide\"}\n]",
        "trans": "This article introduces a new method called Process Reward Models (PRMs) for process supervision of large language models (LLMs) in mathematical reasoning. The goal is to identify and reduce errors in the reasoning process. The research found that the commonly used Monte Carlo (MC) estimation method performs poorly because it relies on the completion model to evaluate the correctness of the current step, leading to inaccurate step verification. The article also points out the bias in traditional Best-of-N (BoN) evaluation strategies and proposes a consensus filtering mechanism that combines MC estimation and LLM-as-a-judge to improve model performance and data efficiency. Finally, the article releases a new state-of-the-art PRM and provides practical guidelines for future research.",
        "update_ts": "2025-01-14 09:10"
    }
}