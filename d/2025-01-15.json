{
    "date": {
        "ru": "15 ÑĞ½Ğ²Ğ°Ñ€Ñ",
        "en": "January 15",
        "zh": "1æœˆ15æ—¥"
    },
    "time_utc": "2025-01-15 04:12",
    "weekday": 2,
    "issue_id": 1673,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.08313",
            "title": "MiniMax-01: Scaling Foundation Models with Lightning Attention",
            "url": "https://huggingface.co/papers/2501.08313",
            "abstract": "We introduce MiniMax-01 series, including MiniMax-Text-01 and MiniMax-VL-01, which are comparable to top-tier models while offering superior capabilities in processing longer contexts. The core lies in lightning attention and its efficient scaling. To maximize computational capacity, we integrate it with Mixture of Experts (MoE), creating a model with 32 experts and 456 billion total parameters, of which 45.9 billion are activated for each token. We develop an optimized parallel strategy and highly efficient computation-communication overlap techniques for MoE and lightning attention. This approach enables us to conduct efficient training and inference on models with hundreds of billions of parameters across contexts spanning millions of tokens. The context window of MiniMax-Text-01 can reach up to 1 million tokens during training and extrapolate to 4 million tokens during inference at an affordable cost. Our vision-language model, MiniMax-VL-01 is built through continued training with 512 billion vision-language tokens. Experiments on both standard and in-house benchmarks show that our models match the performance of state-of-the-art models like GPT-4o and Claude-3.5-Sonnet while offering 20-32 times longer context window. We publicly release MiniMax-01 at https://github.com/MiniMax-AI.",
            "score": 118,
            "issue_id": 1672,
            "pub_date": "2025-01-14",
            "pub_date_card": {
                "ru": "14 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 14",
                "zh": "1æœˆ14æ—¥"
            },
            "hash": "a57d7b1914e7383a",
            "authors": [
                "MiniMax",
                "Aonian Li",
                "Bangwei Gong",
                "Bo Yang",
                "Boji Shan",
                "Chang Liu",
                "Cheng Zhu",
                "Chunhao Zhang",
                "Congchao Guo",
                "Da Chen",
                "Dong Li",
                "Enwei Jiao",
                "Gengxin Li",
                "Guojun Zhang",
                "Haohai Sun",
                "Houze Dong",
                "Jiadai Zhu",
                "Jiaqi Zhuang",
                "Jiayuan Song",
                "Jin Zhu",
                "Jingtao Han",
                "Jingyang Li",
                "Junbin Xie",
                "Junhao Xu",
                "Junjie Yan",
                "Kaishun Zhang",
                "Kecheng Xiao",
                "Kexi Kang",
                "Le Han",
                "Leyang Wang",
                "Lianfei Yu",
                "Liheng Feng",
                "Lin Zheng",
                "Linbo Chai",
                "Long Xing",
                "Meizhi Ju",
                "Mingyuan Chi",
                "Mozhi Zhang",
                "Peikai Huang",
                "Pengcheng Niu",
                "Pengfei Li",
                "Pengyu Zhao",
                "Qi Yang",
                "Qidi Xu",
                "Qiexiang Wang",
                "Qin Wang",
                "Qiuhui Li",
                "Ruitao Leng",
                "Shengmin Shi",
                "Shuqi Yu",
                "Sichen Li",
                "Songquan Zhu",
                "Tao Huang",
                "Tianrun Liang",
                "Weigao Sun",
                "Weixuan Sun",
                "Weiyu Cheng",
                "Wenkai Li",
                "Xiangjun Song",
                "Xiao Su",
                "Xiaodong Han",
                "Xinjie Zhang",
                "Xinzhu Hou",
                "Xu Min",
                "Xun Zou",
                "Xuyang Shen",
                "Yan Gong",
                "Yingjie Zhu",
                "Yipeng Zhou",
                "Yiran Zhong",
                "Yongyi Hu",
                "Yuanxiang Fan",
                "Yue Yu",
                "Yufeng Yang",
                "Yuhao Li",
                "Yunan Huang",
                "Yunji Li",
                "Yunpeng Huang",
                "Yunzhi Xu",
                "Yuxin Mao",
                "Zehan Li",
                "Zekang Li",
                "Zewei Tao",
                "Zewen Ying",
                "Zhaoyang Cong",
                "Zhen Qin",
                "Zhenhua Fan",
                "Zhihang Yu",
                "Zhuo Jiang",
                "Zijia Wu"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2501.08313.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#architecture",
                    "#optimization",
                    "#benchmark",
                    "#long_context",
                    "#training"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "MiniMax-01: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ ÑĞµÑ€Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ MiniMax-01, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ MiniMax-Text-01 Ğ¸ MiniMax-VL-01, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹ Ñ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ½Ğ¾ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ². Ğ’ Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ»ĞµĞ¶Ğ¸Ñ‚ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ lightning attention Ğ¸ ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Mixture of Experts (MoE). ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ¼ĞµĞµÑ‚ 32 ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ° Ğ¸ 456 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¸Ğ· ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… 45,9 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°. ĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğµ Ğ¾ĞºĞ½Ğ¾ MiniMax-Text-01 Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ‚ÑŒ 1 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¸ ÑĞºÑÑ‚Ñ€Ğ°Ğ¿Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ´Ğ¾ 4 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞµ."
                },
                "en": {
                    "title": "Unleashing Long Contexts with MiniMax-01 Models",
                    "desc": "The MiniMax-01 series introduces advanced models, MiniMax-Text-01 and MiniMax-VL-01, designed to handle longer contexts effectively. These models utilize lightning attention and a Mixture of Experts (MoE) architecture, featuring 32 experts and a staggering 456 billion parameters, optimizing the activation of 45.9 billion parameters per token. By implementing efficient parallel strategies and computation-communication overlap techniques, the models can train and infer on extensive datasets, reaching context windows of up to 1 million tokens during training and 4 million during inference. Performance evaluations indicate that MiniMax-01 models rival leading models like GPT-4o and Claude-3.5-Sonnet while significantly extending context capabilities."
                },
                "zh": {
                    "title": "MiniMax-01ï¼šè¶…é•¿ä¸Šä¸‹æ–‡å¤„ç†çš„æ–°çºªå…ƒ",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†MiniMax-01ç³»åˆ—ï¼ŒåŒ…æ‹¬MiniMax-Text-01å’ŒMiniMax-VL-01ï¼Œè¿™äº›æ¨¡å‹åœ¨å¤„ç†æ›´é•¿çš„ä¸Šä¸‹æ–‡æ—¶å…·æœ‰ä¼˜è¶Šçš„èƒ½åŠ›ã€‚æ ¸å¿ƒæŠ€æœ¯æ˜¯é—ªç”µæ³¨æ„åŠ›å’Œé«˜æ•ˆçš„æ‰©å±•èƒ½åŠ›ã€‚ä¸ºäº†æœ€å¤§åŒ–è®¡ç®—èƒ½åŠ›ï¼Œæˆ‘ä»¬å°†å…¶ä¸ä¸“å®¶æ··åˆæ¨¡å‹ï¼ˆMoEï¼‰ç»“åˆï¼Œåˆ›å»ºäº†ä¸€ä¸ªæ‹¥æœ‰32ä¸ªä¸“å®¶å’Œ4560äº¿å‚æ•°çš„æ¨¡å‹ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè¿™äº›æ¨¡å‹åœ¨æ ‡å‡†å’Œå†…éƒ¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿä¸æœ€å…ˆè¿›çš„æ¨¡å‹ç›¸åª²ç¾ï¼ŒåŒæ—¶æä¾›20åˆ°32å€æ›´é•¿çš„ä¸Šä¸‹æ–‡çª—å£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.08187",
            "title": "A Multi-Modal AI Copilot for Single-Cell Analysis with Instruction Following",
            "url": "https://huggingface.co/papers/2501.08187",
            "abstract": "Large language models excel at interpreting complex natural language instructions, enabling them to perform a wide range of tasks. In the life sciences, single-cell RNA sequencing (scRNA-seq) data serves as the \"language of cellular biology\", capturing intricate gene expression patterns at the single-cell level. However, interacting with this \"language\" through conventional tools is often inefficient and unintuitive, posing challenges for researchers. To address these limitations, we present InstructCell, a multi-modal AI copilot that leverages natural language as a medium for more direct and flexible single-cell analysis. We construct a comprehensive multi-modal instruction dataset that pairs text-based instructions with scRNA-seq profiles from diverse tissues and species. Building on this, we develop a multi-modal cell language architecture capable of simultaneously interpreting and processing both modalities. InstructCell empowers researchers to accomplish critical tasks-such as cell type annotation, conditional pseudo-cell generation, and drug sensitivity prediction-using straightforward natural language commands. Extensive evaluations demonstrate that InstructCell consistently meets or exceeds the performance of existing single-cell foundation models, while adapting to diverse experimental conditions. More importantly, InstructCell provides an accessible and intuitive tool for exploring complex single-cell data, lowering technical barriers and enabling deeper biological insights.",
            "score": 14,
            "issue_id": 1672,
            "pub_date": "2025-01-14",
            "pub_date_card": {
                "ru": "14 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 14",
                "zh": "1æœˆ14æ—¥"
            },
            "hash": "de984ce7cc62fa5e",
            "authors": [
                "Yin Fang",
                "Xinle Deng",
                "Kangwei Liu",
                "Ningyu Zhang",
                "Jingyang Qian",
                "Penghui Yang",
                "Xiaohui Fan",
                "Huajun Chen"
            ],
            "affiliations": [
                "College of Computer Science and Technology, Zhejiang University, Hangzhou 310027, China",
                "College of Pharmaceutical Sciences, Zhejiang University, Hangzhou 310058, China",
                "Future Health Laboratory, Innovation Center of Yangtze River Delta, Zhejiang University, Jiaxing 314100, China",
                "Innovation Center in Zhejiang University, State Key Laboratory of Component-Based Chinese Medicine, Hangzhou 310058, China",
                "School of Software Technology, Zhejiang University, Ningbo 315048, China",
                "ZJU-Hangzhou Global Scientific and Technological Innovation Center, Hangzhou 311200, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.08187.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#multimodal",
                    "#dataset",
                    "#science",
                    "#healthcare"
                ],
                "emoji": "ğŸ§¬",
                "ru": {
                    "title": "Ğ•ÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº ĞºĞ°Ğº ĞºĞ»ÑÑ‡ Ğº Ñ€Ğ°ÑÑˆĞ¸Ñ„Ñ€Ğ¾Ğ²ĞºĞµ ĞºĞ»ĞµÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸",
                    "desc": "InstructCell - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ˜Ğ˜-Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸Ğº Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ´Ğ½Ğ¾ĞºĞ»ĞµÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ ĞĞš-ÑĞµĞºĞ²ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (scRNA-seq). ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ğº ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»Ğ¸ ÑĞºÑĞ¿Ñ€ĞµÑÑĞ¸Ğ¸ Ğ³ĞµĞ½Ğ¾Ğ². InstructCell Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ñ‚Ğ°ĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, ĞºĞ°Ğº Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ¸Ğ¿Ğ¾Ğ² ĞºĞ»ĞµÑ‚Ğ¾Ğº Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ»ĞµĞºĞ°Ñ€ÑÑ‚Ğ²Ğ°Ğ¼, Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑĞ¼."
                },
                "en": {
                    "title": "InstructCell: Bridging Language and Biology for Seamless Single-Cell Analysis",
                    "desc": "This paper introduces InstructCell, an AI tool designed to simplify the analysis of single-cell RNA sequencing (scRNA-seq) data using natural language instructions. By creating a dataset that links text commands with scRNA-seq profiles, InstructCell allows researchers to perform complex tasks like cell type annotation and drug sensitivity prediction more intuitively. The model employs a multi-modal architecture that processes both text and biological data simultaneously, enhancing its usability. Evaluations show that InstructCell outperforms existing models, making single-cell analysis more accessible and efficient for researchers in the life sciences."
                },
                "zh": {
                    "title": "ç”¨è‡ªç„¶è¯­è¨€è§£é”å•ç»†èƒæ•°æ®çš„æ½œåŠ›",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†InstructCellï¼Œä¸€ä¸ªå¤šæ¨¡æ€çš„äººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œæ—¨åœ¨é€šè¿‡è‡ªç„¶è¯­è¨€ç®€åŒ–å•ç»†èƒRNAæµ‹åº(scRNA-seq)æ•°æ®çš„åˆ†æã€‚ä¼ ç»Ÿå·¥å…·åœ¨å¤„ç†ç»†èƒç”Ÿç‰©å­¦çš„å¤æ‚æ•°æ®æ—¶æ•ˆç‡ä½ä¸‹ï¼Œè€ŒInstructCellé€šè¿‡å°†æ–‡æœ¬æŒ‡ä»¤ä¸scRNA-seqæ•°æ®ç»“åˆï¼Œæä¾›äº†æ›´ç›´æ¥å’Œçµæ´»çš„åˆ†ææ–¹å¼ã€‚è¯¥ç³»ç»Ÿèƒ½å¤Ÿæ‰§è¡Œç»†èƒç±»å‹æ³¨é‡Šã€æ¡ä»¶ä¼ªç»†èƒç”Ÿæˆå’Œè¯ç‰©æ•æ„Ÿæ€§é¢„æµ‹ç­‰å…³é”®ä»»åŠ¡ï¼Œä¸”ä½¿ç”¨ç®€å•çš„è‡ªç„¶è¯­è¨€å‘½ä»¤å³å¯å®Œæˆã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒInstructCellåœ¨æ€§èƒ½ä¸Šä¼˜äºç°æœ‰çš„å•ç»†èƒåŸºç¡€æ¨¡å‹ï¼ŒåŒæ—¶é€‚åº”å¤šç§å®éªŒæ¡ä»¶ï¼Œé™ä½äº†æŠ€æœ¯é—¨æ§›ï¼Œä¿ƒè¿›äº†ç”Ÿç‰©å­¦çš„æ·±å…¥ç†è§£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.08332",
            "title": "MangaNinja: Line Art Colorization with Precise Reference Following",
            "url": "https://huggingface.co/papers/2501.08332",
            "abstract": "Derived from diffusion models, MangaNinjia specializes in the task of reference-guided line art colorization. We incorporate two thoughtful designs to ensure precise character detail transcription, including a patch shuffling module to facilitate correspondence learning between the reference color image and the target line art, and a point-driven control scheme to enable fine-grained color matching. Experiments on a self-collected benchmark demonstrate the superiority of our model over current solutions in terms of precise colorization. We further showcase the potential of the proposed interactive point control in handling challenging cases, cross-character colorization, multi-reference harmonization, beyond the reach of existing algorithms.",
            "score": 7,
            "issue_id": 1673,
            "pub_date": "2025-01-14",
            "pub_date_card": {
                "ru": "14 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 14",
                "zh": "1æœˆ14æ—¥"
            },
            "hash": "20ea6b75639e2ced",
            "authors": [
                "Zhiheng Liu",
                "Ka Leong Cheng",
                "Xi Chen",
                "Jie Xiao",
                "Hao Ouyang",
                "Kai Zhu",
                "Yu Liu",
                "Yujun Shen",
                "Qifeng Chen",
                "Ping Luo"
            ],
            "affiliations": [
                "Ant Group",
                "HKU",
                "HKUST",
                "Tongyi Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.08332.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#diffusion",
                    "#benchmark"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "ĞŸÑ€ĞµÑ†Ğ¸Ğ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞºÑ€Ğ°ÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ°Ğ½Ğ³Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "MangaNinjia - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€Ğ°ÑĞºÑ€Ğ°ÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ñ… Ñ€Ğ¸ÑÑƒĞ½ĞºĞ¾Ğ² Ğ¼Ğ°Ğ½Ğ³Ğ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ¿ĞµÑ€ĞµĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ñ‚Ñ‡ĞµĞ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸ÑĞ¼ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ†Ğ²ĞµÑ‚Ğ½Ñ‹Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ĞµĞ¼-Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ¼ Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğ¼ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğ¼ Ñ€Ğ¸ÑÑƒĞ½ĞºĞ¾Ğ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑÑ…ĞµĞ¼Ñƒ Ñ‚Ğ¾Ñ‡ĞµÑ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ğ±Ğ¾Ñ€Ğ° Ñ†Ğ²ĞµÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ MangaNinjia Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑĞºÑ€Ğ°ÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "MangaNinjia: Mastering Line Art Colorization with Precision",
                    "desc": "MangaNinjia is a model designed for coloring line art by using reference images. It employs a patch shuffling module to help the model learn how to match colors from the reference image to the target line art accurately. Additionally, it features a point-driven control scheme that allows for detailed color adjustments, ensuring that colors are applied precisely. Our experiments show that MangaNinjia outperforms existing methods in colorization tasks, especially in complex scenarios involving multiple references and different characters."
                },
                "zh": {
                    "title": "MangaNinjiaï¼šç²¾å‡†ä¸Šè‰²çš„æ–°æ–¹æ³•",
                    "desc": "MangaNinjia æ˜¯ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„å‚è€ƒå¼•å¯¼çº¿æ¡è‰ºæœ¯ä¸Šè‰²æŠ€æœ¯ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸¤ä¸ªæ¨¡å—æ¥ç¡®ä¿è§’è‰²ç»†èŠ‚çš„å‡†ç¡®è½¬å½•ï¼ŒåŒ…æ‹¬è¡¥ä¸æ´—ç‰Œæ¨¡å—å’Œç‚¹é©±åŠ¨æ§åˆ¶æ–¹æ¡ˆï¼Œä»¥å®ç°ç²¾ç»†çš„é¢œè‰²åŒ¹é…ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨ç²¾ç¡®ä¸Šè‰²æ–¹é¢ä¼˜äºç°æœ‰è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†æ‰€æè®®çš„äº¤äº’å¼ç‚¹æ§åˆ¶åœ¨å¤„ç†å¤æ‚æ¡ˆä¾‹å’Œå¤šå‚è€ƒåè°ƒæ–¹é¢çš„æ½œåŠ›ï¼Œè¶…è¶Šäº†ç°æœ‰ç®—æ³•çš„èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.08316",
            "title": "Diffusion Adversarial Post-Training for One-Step Video Generation",
            "url": "https://huggingface.co/papers/2501.08316",
            "abstract": "The diffusion models are widely used for image and video generation, but their iterative generation process is slow and expansive. While existing distillation approaches have demonstrated the potential for one-step generation in the image domain, they still suffer from significant quality degradation. In this work, we propose Adversarial Post-Training (APT) against real data following diffusion pre-training for one-step video generation. To improve the training stability and quality, we introduce several improvements to the model architecture and training procedures, along with an approximated R1 regularization objective. Empirically, our experiments show that our adversarial post-trained model, Seaweed-APT, can generate 2-second, 1280x720, 24fps videos in real time using a single forward evaluation step. Additionally, our model is capable of generating 1024px images in a single step, achieving quality comparable to state-of-the-art methods.",
            "score": 7,
            "issue_id": 1672,
            "pub_date": "2025-01-14",
            "pub_date_card": {
                "ru": "14 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 14",
                "zh": "1æœˆ14æ—¥"
            },
            "hash": "4122a780e8356ce7",
            "authors": [
                "Shanchuan Lin",
                "Xin Xia",
                "Yuxi Ren",
                "Ceyuan Yang",
                "Xuefeng Xiao",
                "Lu Jiang"
            ],
            "affiliations": [
                "ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.08316.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#video",
                    "#diffusion",
                    "#training"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾: Ğ¾Ñ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğº Ğ¼Ğ³Ğ½Ğ¾Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñƒ",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Adversarial Post-Training (APT) Ğ´Ğ»Ñ Ğ¾Ğ´Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ°Ğ¿Ğ¿Ñ€Ğ¾ĞºÑĞ¸Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ R1. Ğ˜Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Seaweed-APT ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ 2-ÑĞµĞºÑƒĞ½Ğ´Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 1024px Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ ÑˆĞ°Ğ³, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Fast and High-Quality Video Generation with Seaweed-APT",
                    "desc": "This paper addresses the slow and costly iterative process of generating images and videos using diffusion models. The authors introduce Adversarial Post-Training (APT) to enhance one-step video generation while maintaining high quality. They implement architectural and procedural improvements, including an approximated R1 regularization, to stabilize training. Their model, Seaweed-APT, successfully generates high-quality 2-second videos and 1024px images in real time with a single forward evaluation step."
                },
                "zh": {
                    "title": "å¯¹æŠ—åè®­ç»ƒï¼šå¿«é€Ÿé«˜è´¨é‡è§†é¢‘ç”Ÿæˆçš„æ–°æ–¹æ³•",
                    "desc": "æ‰©æ•£æ¨¡å‹å¹¿æ³›åº”ç”¨äºå›¾åƒå’Œè§†é¢‘ç”Ÿæˆï¼Œä½†å…¶è¿­ä»£ç”Ÿæˆè¿‡ç¨‹è¾ƒæ…¢ä¸”æˆæœ¬é«˜æ˜‚ã€‚ç°æœ‰çš„è’¸é¦æ–¹æ³•åœ¨å›¾åƒé¢†åŸŸå±•ç¤ºäº†å•æ­¥ç”Ÿæˆçš„æ½œåŠ›ï¼Œä½†ä»å­˜åœ¨æ˜¾è‘—çš„è´¨é‡ä¸‹é™ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹çœŸå®æ•°æ®çš„å¯¹æŠ—åè®­ç»ƒï¼ˆAPTï¼‰æ–¹æ³•ï¼Œä»¥å®ç°å•æ­¥è§†é¢‘ç”Ÿæˆã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œç»è¿‡å¯¹æŠ—åè®­ç»ƒçš„æ¨¡å‹Seaweed-APTèƒ½å¤Ÿå®æ—¶ç”Ÿæˆ1280x720ã€24fpsçš„2ç§’è§†é¢‘ï¼Œå¹¶ä¸”åœ¨å•æ­¥ç”Ÿæˆ1024pxå›¾åƒæ—¶ï¼Œå…¶è´¨é‡å¯ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸åª²ç¾ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.08225",
            "title": "FramePainter: Endowing Interactive Image Editing with Video Diffusion Priors",
            "url": "https://huggingface.co/papers/2501.08225",
            "abstract": "Interactive image editing allows users to modify images through visual interaction operations such as drawing, clicking, and dragging. Existing methods construct such supervision signals from videos, as they capture how objects change with various physical interactions. However, these models are usually built upon text-to-image diffusion models, so necessitate (i) massive training samples and (ii) an additional reference encoder to learn real-world dynamics and visual consistency. In this paper, we reformulate this task as an image-to-video generation problem, so that inherit powerful video diffusion priors to reduce training costs and ensure temporal consistency. Specifically, we introduce FramePainter as an efficient instantiation of this formulation. Initialized with Stable Video Diffusion, it only uses a lightweight sparse control encoder to inject editing signals. Considering the limitations of temporal attention in handling large motion between two frames, we further propose matching attention to enlarge the receptive field while encouraging dense correspondence between edited and source image tokens. We highlight the effectiveness and efficiency of FramePainter across various of editing signals: it domainantly outperforms previous state-of-the-art methods with far less training data, achieving highly seamless and coherent editing of images, \\eg, automatically adjust the reflection of the cup. Moreover, FramePainter also exhibits exceptional generalization in scenarios not present in real-world videos, \\eg, transform the clownfish into shark-like shape. Our code will be available at https://github.com/YBYBZhang/FramePainter.",
            "score": 3,
            "issue_id": 1673,
            "pub_date": "2025-01-14",
            "pub_date_card": {
                "ru": "14 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 14",
                "zh": "1æœˆ14æ—¥"
            },
            "hash": "811cfd0f18eb1e53",
            "authors": [
                "Yabo Zhang",
                "Xinpeng Zhou",
                "Yihan Zeng",
                "Hang Xu",
                "Hui Li",
                "Wangmeng Zuo"
            ],
            "affiliations": [
                "Harbin Institute of Technology",
                "Huawei Noahs Ark Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.08225.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#cv",
                    "#optimization",
                    "#diffusion"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "FramePainter: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ FramePainter - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ, FramePainter Ğ¾Ğ¿Ğ¸Ñ€Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ´Ğ»Ñ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸. FramePainter Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ñ‚Ñ€ĞµĞ±ÑƒÑ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "Revolutionizing Image Editing with Efficient Video Diffusion",
                    "desc": "This paper presents FramePainter, a novel approach to interactive image editing that reformulates the task as image-to-video generation. By leveraging video diffusion models, FramePainter reduces the need for extensive training data while ensuring temporal consistency in edited images. It utilizes a lightweight sparse control encoder to effectively incorporate editing signals, and introduces matching attention to improve the handling of large motion between frames. The results demonstrate that FramePainter significantly outperforms existing methods, achieving seamless image edits and showcasing strong generalization capabilities."
                },
                "zh": {
                    "title": "FramePainterï¼šé«˜æ•ˆçš„å›¾åƒç¼–è¾‘æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§äº¤äº’å¼å›¾åƒç¼–è¾‘çš„æ–°æ–¹æ³•ï¼Œç§°ä¸ºFramePainterã€‚è¯¥æ–¹æ³•å°†å›¾åƒç¼–è¾‘ä»»åŠ¡é‡æ–°å®šä¹‰ä¸ºå›¾åƒåˆ°è§†é¢‘çš„ç”Ÿæˆé—®é¢˜ï¼Œä»è€Œåˆ©ç”¨å¼ºå¤§çš„è§†é¢‘æ‰©æ•£å…ˆéªŒï¼Œé™ä½è®­ç»ƒæˆæœ¬å¹¶ç¡®ä¿æ—¶é—´ä¸€è‡´æ€§ã€‚FramePainterä½¿ç”¨è½»é‡çº§çš„ç¨€ç–æ§åˆ¶ç¼–ç å™¨æ¥æ³¨å…¥ç¼–è¾‘ä¿¡å·ï¼Œå¹¶é€šè¿‡åŒ¹é…æ³¨æ„åŠ›æœºåˆ¶å¢å¼ºäº†å¯¹å¤§è¿åŠ¨çš„å¤„ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFramePainteråœ¨å„ç§ç¼–è¾‘ä¿¡å·ä¸‹è¡¨ç°ä¼˜å¼‚ï¼Œèƒ½å¤Ÿå®ç°æ— ç¼ä¸”è¿è´¯çš„å›¾åƒç¼–è¾‘ï¼Œä¸”åœ¨æœªè§è¿‡çš„åœºæ™¯ä¸­ä¹Ÿå±•ç°å‡ºå“è¶Šçš„æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.07730",
            "title": "Democratizing Text-to-Image Masked Generative Models with Compact Text-Aware One-Dimensional Tokens",
            "url": "https://huggingface.co/papers/2501.07730",
            "abstract": "Image tokenizers form the foundation of modern text-to-image generative models but are notoriously difficult to train. Furthermore, most existing text-to-image models rely on large-scale, high-quality private datasets, making them challenging to replicate. In this work, we introduce Text-Aware Transformer-based 1-Dimensional Tokenizer (TA-TiTok), an efficient and powerful image tokenizer that can utilize either discrete or continuous 1-dimensional tokens. TA-TiTok uniquely integrates textual information during the tokenizer decoding stage (i.e., de-tokenization), accelerating convergence and enhancing performance. TA-TiTok also benefits from a simplified, yet effective, one-stage training process, eliminating the need for the complex two-stage distillation used in previous 1-dimensional tokenizers. This design allows for seamless scalability to large datasets. Building on this, we introduce a family of text-to-image Masked Generative Models (MaskGen), trained exclusively on open data while achieving comparable performance to models trained on private data. We aim to release both the efficient, strong TA-TiTok tokenizers and the open-data, open-weight MaskGen models to promote broader access and democratize the field of text-to-image masked generative models.",
            "score": 0,
            "issue_id": 1673,
            "pub_date": "2025-01-13",
            "pub_date_card": {
                "ru": "13 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 13",
                "zh": "1æœˆ13æ—¥"
            },
            "hash": "80f40715084c602b",
            "authors": [
                "Dongwon Kim",
                "Ju He",
                "Qihang Yu",
                "Chenglin Yang",
                "Xiaohui Shen",
                "Suha Kwak",
                "Liang-Chieh Chen"
            ],
            "affiliations": [
                "ByteDance Seed",
                "POSTECH"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.07730.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#data",
                    "#training",
                    "#cv",
                    "#open_source"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ”ĞµĞ¼Ğ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‚ĞµĞºÑÑ‚-Ğ²-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ TA-TiTok. Ğ”Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ´ĞµÑ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ TA-TiTok Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ MaskGen, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½Ğ° Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¦ĞµĞ»ÑŒÑ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ´ĞµĞ¼Ğ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‚ĞµĞºÑÑ‚-Ğ²-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ²ĞµÑĞ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Democratizing Text-to-Image Generation with TA-TiTok",
                    "desc": "This paper presents TA-TiTok, a novel image tokenizer designed for text-to-image generative models, which simplifies the training process and improves performance. Unlike traditional models that require large private datasets, TA-TiTok can effectively utilize open data, making it more accessible for researchers. The tokenizer incorporates textual information during the decoding stage, which helps it learn faster and perform better. Additionally, the authors introduce MaskGen, a family of generative models that leverage TA-TiTok and are trained on publicly available datasets, aiming to democratize access to advanced text-to-image generation technology."
                },
                "zh": {
                    "title": "é«˜æ•ˆçš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œæ¨åŠ¨å¼€æ”¾æ•°æ®çš„ä½¿ç”¨",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å›¾åƒæ ‡è®°å™¨ï¼Œç§°ä¸ºTA-TiTokï¼Œå®ƒå¯ä»¥æœ‰æ•ˆåœ°å¤„ç†æ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆä»»åŠ¡ã€‚TA-TiTokåœ¨è§£ç é˜¶æ®µæ•´åˆäº†æ–‡æœ¬ä¿¡æ¯ï¼Œä»è€ŒåŠ å¿«äº†æ¨¡å‹çš„æ”¶æ•›é€Ÿåº¦å¹¶æé«˜äº†æ€§èƒ½ã€‚ä¸ä»¥å¾€çš„æ ‡è®°å™¨ä¸åŒï¼ŒTA-TiToké‡‡ç”¨äº†ä¸€ç§ç®€åŒ–çš„ä¸€é˜¶æ®µè®­ç»ƒè¿‡ç¨‹ï¼Œé¿å…äº†å¤æ‚çš„ä¸¤é˜¶æ®µè’¸é¦è¿‡ç¨‹ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç³»åˆ—åŸºäºå¼€æ”¾æ•°æ®è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹MaskGenï¼Œæ—¨åœ¨ä¿ƒè¿›æ›´å¹¿æ³›çš„è®¿é—®å’Œæ°‘ä¸»åŒ–ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.08292",
            "title": "HALoGEN: Fantastic LLM Hallucinations and Where to Find Them",
            "url": "https://huggingface.co/papers/2501.08292",
            "abstract": "Despite their impressive ability to generate high-quality and fluent text, generative large language models (LLMs) also produce hallucinations: statements that are misaligned with established world knowledge or provided input context. However, measuring hallucination can be challenging, as having humans verify model generations on-the-fly is both expensive and time-consuming. In this work, we release HALoGEN, a comprehensive hallucination benchmark consisting of: (1) 10,923 prompts for generative models spanning nine domains including programming, scientific attribution, and summarization, and (2) automatic high-precision verifiers for each use case that decompose LLM generations into atomic units, and verify each unit against a high-quality knowledge source. We use this framework to evaluate ~150,000 generations from 14 language models, finding that even the best-performing models are riddled with hallucinations (sometimes up to 86% of generated atomic facts depending on the domain). We further define a novel error classification for LLM hallucinations based on whether they likely stem from incorrect recollection of training data (Type A errors), or incorrect knowledge in training data (Type B errors), or are fabrication (Type C errors). We hope our framework provides a foundation to enable the principled study of why generative models hallucinate, and advances the development of trustworthy large language models.",
            "score": 0,
            "issue_id": 1673,
            "pub_date": "2025-01-14",
            "pub_date_card": {
                "ru": "14 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 14",
                "zh": "1æœˆ14æ—¥"
            },
            "hash": "f6751d682ff824ed",
            "authors": [
                "Abhilasha Ravichander",
                "Shrusti Ghela",
                "David Wadden",
                "Yejin Choi"
            ],
            "affiliations": [
                "Google",
                "NVIDIA",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.08292.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#hallucinations",
                    "#benchmark"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "HALoGEN: ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ HALoGEN - ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ· 10,923 Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ² Ğ´ĞµĞ²ÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ LLM. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑÑ‚Ñ€Ğ°Ğ´Ğ°ÑÑ‚ Ğ¾Ñ‚ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹, Ğ¸Ğ½Ğ¾Ğ³Ğ´Ğ° Ğ´Ğ¾ 86% ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ÑÑ Ğ½ĞµĞ²ĞµÑ€Ğ½Ñ‹Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²ÑƒÑ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº LLM, Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ğ¸Ğ² Ğ¸Ñ… Ğ½Ğ° Ñ‚Ñ€Ğ¸ Ñ‚Ğ¸Ğ¿Ğ° Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ° Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹."
                },
                "en": {
                    "title": "HALoGEN: A Benchmark for Measuring Hallucinations in Language Models",
                    "desc": "This paper introduces HALoGEN, a new benchmark designed to measure hallucinations in generative large language models (LLMs). Hallucinations refer to incorrect statements generated by these models that do not align with known facts or the given context. The benchmark includes over 10,000 prompts across various domains and employs automatic verifiers to assess the accuracy of model outputs. The study reveals that even top-performing models exhibit significant hallucinations, prompting a classification system for different types of errors to better understand their origins and improve model reliability."
                },
                "zh": {
                    "title": "æ­ç¤ºç”Ÿæˆæ¨¡å‹çš„å¹»è§‰é—®é¢˜",
                    "desc": "å°½ç®¡ç”Ÿæˆæ€§å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡å’Œæµç•…çš„æ–‡æœ¬ï¼Œä½†å®ƒä»¬ä¹Ÿä¼šäº§ç”Ÿå¹»è§‰ï¼Œå³ä¸å·²çŸ¥ä¸–ç•ŒçŸ¥è¯†æˆ–è¾“å…¥ä¸Šä¸‹æ–‡ä¸ä¸€è‡´çš„é™ˆè¿°ã€‚æµ‹é‡å¹»è§‰çš„éš¾åº¦åœ¨äºï¼Œå®æ—¶éªŒè¯æ¨¡å‹ç”Ÿæˆçš„å†…å®¹æ—¢æ˜‚è´µåˆè€—æ—¶ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†HALoGENï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„å¹»è§‰åŸºå‡†ï¼ŒåŒ…å«10,923ä¸ªè·¨è¶Šä¹ä¸ªé¢†åŸŸçš„æç¤ºå’Œè‡ªåŠ¨é«˜ç²¾åº¦éªŒè¯å™¨ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œå³ä½¿æ˜¯è¡¨ç°æœ€å¥½çš„æ¨¡å‹ï¼Œå…¶ç”Ÿæˆçš„åŸå­äº‹å®ä¸­ä¹Ÿæœ‰é«˜è¾¾86%å¯èƒ½å­˜åœ¨å¹»è§‰ï¼Œè¿™ä¸ºç†è§£ç”Ÿæˆæ¨¡å‹çš„å¹»è§‰æä¾›äº†åŸºç¡€ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-01-14.html",
    "link_next": "2025-01-16.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "14.01",
        "en": "01/14",
        "zh": "1æœˆ14æ—¥"
    },
    "short_date_next": {
        "ru": "16.01",
        "en": "01/16",
        "zh": "1æœˆ16æ—¥"
    },
    "categories": {
        "#dataset": 3,
        "#data": 1,
        "#benchmark": 3,
        "#agents": 0,
        "#cv": 3,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 1,
        "#training": 3,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 3,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå«åšè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰ï¼Œç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•°å­¦æ¨ç†ä¸­çš„è¿‡ç¨‹ç›‘ç£ã€‚ç›®æ ‡æ˜¯è¯†åˆ«å’Œå‡å°‘æ¨ç†è¿‡ç¨‹ä¸­çš„é”™è¯¯ã€‚ç ”ç©¶å‘ç°ï¼Œå¸¸ç”¨çš„è’™ç‰¹å¡ç½—ï¼ˆMCï¼‰ä¼°è®¡æ–¹æ³•æ•ˆæœä¸ä½³ï¼Œå› ä¸ºå®ƒä¾èµ–å®Œæˆæ¨¡å‹è¯„ä¼°å½“å‰æ­¥éª¤çš„æ­£ç¡®æ€§ï¼Œå¯¼è‡´æ­¥éª¤éªŒè¯ä¸å‡†ç¡®ã€‚æ–‡ç« è¿˜æŒ‡å‡ºäº†ä¼ ç»ŸBest-of-Nï¼ˆBoNï¼‰è¯„ä¼°ç­–ç•¥çš„åå·®ï¼Œå¹¶æå‡ºäº†ä¸€ç§å…±è¯†è¿‡æ»¤æœºåˆ¶ï¼Œç»“åˆMCä¼°è®¡å’ŒLLM-as-a-judgeï¼Œæ”¹è¿›äº†æ¨¡å‹æ€§èƒ½å’Œæ•°æ®æ•ˆç‡ã€‚æœ€åï¼Œæ–‡ç« å‘å¸ƒäº†ä¸€ä¸ªæ–°çš„æœ€å…ˆè¿›çš„PRMï¼Œå¹¶æä¾›äº†æœªæ¥ç ”ç©¶çš„å®ç”¨æŒ‡å—ã€‚",
        "title": "The Lessons of Developing Process Reward Models in Mathematical Reasoning",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå«åšè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰ï¼Œç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•°å­¦æ¨ç†ä¸­çš„è¿‡ç¨‹ç›‘ç£ã€‚ç›®æ ‡æ˜¯è¯†åˆ«å’Œå‡å°‘æ¨ç†è¿‡ç¨‹ä¸­çš„é”™è¯¯ã€‚ç ”ç©¶å‘ç°ï¼Œå¸¸ç”¨çš„è’™ç‰¹å¡ç½—ï¼ˆMCï¼‰ä¼°è®¡æ–¹æ³•æ•ˆæœä¸ä½³ï¼Œå› ä¸ºå®ƒä¾èµ–å®Œæˆæ¨¡å‹è¯„ä¼°å½“å‰æ­¥éª¤çš„æ­£ç¡®æ€§ï¼Œå¯¼è‡´æ­¥éª¤éªŒè¯ä¸å‡†ç¡®ã€‚æ–‡ç« è¿˜æŒ‡å‡ºäº†ä¼ ç»ŸBest-of-Nï¼ˆBoNï¼‰è¯„ä¼°ç­–ç•¥çš„åå·®ï¼Œå¹¶æå‡ºäº†ä¸€ç§å…±è¯†è¿‡æ»¤æœºåˆ¶ï¼Œç»“åˆMCä¼°è®¡å’ŒLLM-as-a-judgeï¼Œæ”¹è¿›äº†æ¨¡å‹æ€§èƒ½å’Œæ•°æ®æ•ˆç‡ã€‚æœ€åï¼Œæ–‡ç« å‘å¸ƒäº†ä¸€ä¸ªæ–°çš„æœ€å…ˆè¿›çš„PRMï¼Œå¹¶æä¾›äº†æœªæ¥ç ”ç©¶çš„å®ç”¨æŒ‡å—ã€‚\n\nzhÃ¨ piÄn wÃ©nzhÄng jiÃ¨shÃ o le yÄ« zhÇ’ng xÄ«n de fÄngfÇ, jiÃ ozuÃ² guÃ²chÃ©ng jiÇnglÃ¬ mÃ³xÃ­ng (PRMs), yÃ²ngyÃº dÃ xÃ­ng yÇ”yÃ¡n mÃ³xÃ­ng (LLMs) zÃ i shÃ¹xuÃ© tuÄ«lÇ zhÅng de guÃ²chÃ©ng jiÃ ndÅ«. MÃ¹biÄo shÃ¬ shÃ­biÃ© hÃ© jiÇnshÇo tuÄ«lÇ guÃ²chÃ©ng zhÅng de cuÃ²wÃ¹. YÃ¡njiÅ« fÄxiÃ n, chÃ¡ngyÃ²ng de mÃ©ngtÃ¨kÇluÃ³ (MC) gÅ«jÃ¬ fÄngfÇ xiÃ ojiÃ , yÄ«nwÃ¨i tÄ yÄ«lÃ i wÃ¡nchÃ©ng mÃ³xÃ­ng pÃ­ngjiÃ  dÄngqiÃ¡n bÃ¹zhÃ²u de zhÃ¨ngquÃ¨xÃ¬ng, dÇozhÃ¬ bÃ¹zhÃ²u yÃ nzhÃ¨ng bÃ¹ zhÇ”nquÃ¨. WÃ©nzhÄng hÃ¡i zhÇchÅ« le chuÃ¡ntÇ’ng Best-of-N (BoN) pÃ­ngjiÃ  cÃ¨lÃ¼Ã¨ de piÄnchÄ, bÃ¬ng tÃ­chÅ« le yÄ« zhÇ’ng gÃ²ngshÃ¬ guÃ²lÇœ jÄ«zhÃ¬, jiÃ©hÃ© MC gÅ«jÃ¬ hÃ© LLM-as-a-judge, gÇijÃ¬n le mÃ³xÃ­ng xÃ¬ngnÃ©ng hÃ© shÃ¹jÃ¹ xiÃ oyÃ²ng. ZuÃ¬hÃ²u, wÃ©nzhÄng fÄbÃ¹ le yÄ«gÃ¨ xÄ«n de zuÃ¬ xiÄnjÃ¬n de PRM, bÃ¬ng tÃ­gÅng le wÃ¨ilÃ¡i yÃ¡njiÅ« de shÃ­yÃ²ng zhÇnÃ¡n.",
        "vocab": "[\n    {\"word\": \"è¿‡ç¨‹å¥–åŠ±æ¨¡å‹\", \"pinyin\": \"guÃ²chÃ©ng jiÇnglÃ¬ mÃ³xÃ­ng\", \"trans\": \"Process Reward Model\"},\n    {\"word\": \"å¤§å‹è¯­è¨€æ¨¡å‹\", \"pinyin\": \"dÃ xÃ­ng yÇ”yÃ¡n mÃ³xÃ­ng\", \"trans\": \"Large Language Model\"},\n    {\"word\": \"æ•°å­¦æ¨ç†\", \"pinyin\": \"shÃ¹xuÃ© tuÄ«lÇ\", \"trans\": \"Mathematical Reasoning\"},\n    {\"word\": \"è¿‡ç¨‹ç›‘ç£\", \"pinyin\": \"guÃ²chÃ©ng jiÃ ndÅ«\", \"trans\": \"Process Supervision\"},\n    {\"word\": \"è’™ç‰¹å¡ç½—\", \"pinyin\": \"mÃ©ngtÃ¨kÇluÃ³\", \"trans\": \"Monte Carlo\"},\n    {\"word\": \"ä¼°è®¡\", \"pinyin\": \"gÅ«jÃ¬\", \"trans\": \"Estimation\"},\n    {\"word\": \"ä¾èµ–\", \"pinyin\": \"yÄ«lÃ i\", \"trans\": \"Depend\"},\n    {\"word\": \"è¯„ä¼°\", \"pinyin\": \"pÃ­nggÅ«\", \"trans\": \"Evaluate\"},\n    {\"word\": \"æ­¥éª¤éªŒè¯\", \"pinyin\": \"bÃ¹zhÃ²u yÃ nzhÃ¨ng\", \"trans\": \"Step Verification\"},\n    {\"word\": \"åå·®\", \"pinyin\": \"piÄnchÄ\", \"trans\": \"Bias\"},\n    {\"word\": \"å…±è¯†è¿‡æ»¤æœºåˆ¶\", \"pinyin\": \"gÃ²ngshÃ­ guÃ²lÇœ jÄ«zhÃ¬\", \"trans\": \"Consensus Filtering Mechanism\"},\n    {\"word\": \"LLM-as-a-judge\", \"pinyin\": \"LLM-as-a-judge\", \"trans\": \"LLM-as-a-judge\"},\n    {\"word\": \"æœ€å…ˆè¿›\", \"pinyin\": \"zuÃ¬xiÄnjÃ¬n\", \"trans\": \"State-of-the-art\"},\n    {\"word\": \"å®ç”¨æŒ‡å—\", \"pinyin\": \"shÃ­yÃ²ng zhÇnÃ¡n\", \"trans\": \"Practical Guide\"}\n]",
        "trans": "This article introduces a new method called Process Reward Models (PRMs) for process supervision of large language models (LLMs) in mathematical reasoning. The goal is to identify and reduce errors in the reasoning process. The research found that the commonly used Monte Carlo (MC) estimation method performs poorly because it relies on the completion model to evaluate the correctness of the current step, leading to inaccurate step verification. The article also points out the bias in traditional Best-of-N (BoN) evaluation strategies and proposes a consensus filtering mechanism that combines MC estimation and LLM-as-a-judge to improve model performance and data efficiency. Finally, the article releases a new state-of-the-art PRM and provides practical guidelines for future research.",
        "update_ts": "2025-01-14 09:10"
    }
}