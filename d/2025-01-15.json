{
    "date": {
        "ru": "15 января",
        "en": "January 15",
        "zh": "1月15日"
    },
    "time_utc": "2025-01-15 06:13",
    "weekday": 2,
    "issue_id": 1675,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.08313",
            "title": "MiniMax-01: Scaling Foundation Models with Lightning Attention",
            "url": "https://huggingface.co/papers/2501.08313",
            "abstract": "We introduce MiniMax-01 series, including MiniMax-Text-01 and MiniMax-VL-01, which are comparable to top-tier models while offering superior capabilities in processing longer contexts. The core lies in lightning attention and its efficient scaling. To maximize computational capacity, we integrate it with Mixture of Experts (MoE), creating a model with 32 experts and 456 billion total parameters, of which 45.9 billion are activated for each token. We develop an optimized parallel strategy and highly efficient computation-communication overlap techniques for MoE and lightning attention. This approach enables us to conduct efficient training and inference on models with hundreds of billions of parameters across contexts spanning millions of tokens. The context window of MiniMax-Text-01 can reach up to 1 million tokens during training and extrapolate to 4 million tokens during inference at an affordable cost. Our vision-language model, MiniMax-VL-01 is built through continued training with 512 billion vision-language tokens. Experiments on both standard and in-house benchmarks show that our models match the performance of state-of-the-art models like GPT-4o and Claude-3.5-Sonnet while offering 20-32 times longer context window. We publicly release MiniMax-01 at https://github.com/MiniMax-AI.",
            "score": 136,
            "issue_id": 1672,
            "pub_date": "2025-01-14",
            "pub_date_card": {
                "ru": "14 января",
                "en": "January 14",
                "zh": "1月14日"
            },
            "hash": "a57d7b1914e7383a",
            "authors": [
                "MiniMax",
                "Aonian Li",
                "Bangwei Gong",
                "Bo Yang",
                "Boji Shan",
                "Chang Liu",
                "Cheng Zhu",
                "Chunhao Zhang",
                "Congchao Guo",
                "Da Chen",
                "Dong Li",
                "Enwei Jiao",
                "Gengxin Li",
                "Guojun Zhang",
                "Haohai Sun",
                "Houze Dong",
                "Jiadai Zhu",
                "Jiaqi Zhuang",
                "Jiayuan Song",
                "Jin Zhu",
                "Jingtao Han",
                "Jingyang Li",
                "Junbin Xie",
                "Junhao Xu",
                "Junjie Yan",
                "Kaishun Zhang",
                "Kecheng Xiao",
                "Kexi Kang",
                "Le Han",
                "Leyang Wang",
                "Lianfei Yu",
                "Liheng Feng",
                "Lin Zheng",
                "Linbo Chai",
                "Long Xing",
                "Meizhi Ju",
                "Mingyuan Chi",
                "Mozhi Zhang",
                "Peikai Huang",
                "Pengcheng Niu",
                "Pengfei Li",
                "Pengyu Zhao",
                "Qi Yang",
                "Qidi Xu",
                "Qiexiang Wang",
                "Qin Wang",
                "Qiuhui Li",
                "Ruitao Leng",
                "Shengmin Shi",
                "Shuqi Yu",
                "Sichen Li",
                "Songquan Zhu",
                "Tao Huang",
                "Tianrun Liang",
                "Weigao Sun",
                "Weixuan Sun",
                "Weiyu Cheng",
                "Wenkai Li",
                "Xiangjun Song",
                "Xiao Su",
                "Xiaodong Han",
                "Xinjie Zhang",
                "Xinzhu Hou",
                "Xu Min",
                "Xun Zou",
                "Xuyang Shen",
                "Yan Gong",
                "Yingjie Zhu",
                "Yipeng Zhou",
                "Yiran Zhong",
                "Yongyi Hu",
                "Yuanxiang Fan",
                "Yue Yu",
                "Yufeng Yang",
                "Yuhao Li",
                "Yunan Huang",
                "Yunji Li",
                "Yunpeng Huang",
                "Yunzhi Xu",
                "Yuxin Mao",
                "Zehan Li",
                "Zekang Li",
                "Zewei Tao",
                "Zewen Ying",
                "Zhaoyang Cong",
                "Zhen Qin",
                "Zhenhua Fan",
                "Zhihang Yu",
                "Zhuo Jiang",
                "Zijia Wu"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2501.08313.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#architecture",
                    "#optimization",
                    "#benchmark",
                    "#long_context",
                    "#training"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "MiniMax-01: Революция в обработке длинных контекстов",
                    "desc": "Исследователи представили серию моделей MiniMax-01, включая MiniMax-Text-01 и MiniMax-VL-01, которые сравнимы с лучшими моделями, но обладают улучшенными возможностями обработки длинных контекстов. В основе лежит технология lightning attention и ее эффективное масштабирование, интегрированные с Mixture of Experts (MoE). Модель имеет 32 эксперта и 456 миллиардов параметров, из которых 45,9 миллиардов активируются для каждого токена. Контекстное окно MiniMax-Text-01 может достигать 1 миллиона токенов при обучении и экстраполироваться до 4 миллионов токенов при инференсе."
                },
                "en": {
                    "title": "Unleashing Long Contexts with MiniMax-01 Models",
                    "desc": "The MiniMax-01 series introduces advanced models, MiniMax-Text-01 and MiniMax-VL-01, designed to handle longer contexts effectively. These models utilize lightning attention and a Mixture of Experts (MoE) architecture, featuring 32 experts and a staggering 456 billion parameters, optimizing the activation of 45.9 billion parameters per token. By implementing efficient parallel strategies and computation-communication overlap techniques, the models can train and infer on extensive datasets, reaching context windows of up to 1 million tokens during training and 4 million during inference. Performance evaluations indicate that MiniMax-01 models rival leading models like GPT-4o and Claude-3.5-Sonnet while significantly extending context capabilities."
                },
                "zh": {
                    "title": "MiniMax-01：超长上下文处理的新纪元",
                    "desc": "我们介绍了MiniMax-01系列，包括MiniMax-Text-01和MiniMax-VL-01，这些模型在处理更长的上下文时具有优越的能力。核心技术是闪电注意力和高效的扩展能力。为了最大化计算能力，我们将其与专家混合模型（MoE）结合，创建了一个拥有32个专家和4560亿参数的模型。我们的实验表明，这些模型在标准和内部基准测试中表现出色，能够与最先进的模型相媲美，同时提供20到32倍更长的上下文窗口。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.08187",
            "title": "A Multi-Modal AI Copilot for Single-Cell Analysis with Instruction Following",
            "url": "https://huggingface.co/papers/2501.08187",
            "abstract": "Large language models excel at interpreting complex natural language instructions, enabling them to perform a wide range of tasks. In the life sciences, single-cell RNA sequencing (scRNA-seq) data serves as the \"language of cellular biology\", capturing intricate gene expression patterns at the single-cell level. However, interacting with this \"language\" through conventional tools is often inefficient and unintuitive, posing challenges for researchers. To address these limitations, we present InstructCell, a multi-modal AI copilot that leverages natural language as a medium for more direct and flexible single-cell analysis. We construct a comprehensive multi-modal instruction dataset that pairs text-based instructions with scRNA-seq profiles from diverse tissues and species. Building on this, we develop a multi-modal cell language architecture capable of simultaneously interpreting and processing both modalities. InstructCell empowers researchers to accomplish critical tasks-such as cell type annotation, conditional pseudo-cell generation, and drug sensitivity prediction-using straightforward natural language commands. Extensive evaluations demonstrate that InstructCell consistently meets or exceeds the performance of existing single-cell foundation models, while adapting to diverse experimental conditions. More importantly, InstructCell provides an accessible and intuitive tool for exploring complex single-cell data, lowering technical barriers and enabling deeper biological insights.",
            "score": 14,
            "issue_id": 1672,
            "pub_date": "2025-01-14",
            "pub_date_card": {
                "ru": "14 января",
                "en": "January 14",
                "zh": "1月14日"
            },
            "hash": "de984ce7cc62fa5e",
            "authors": [
                "Yin Fang",
                "Xinle Deng",
                "Kangwei Liu",
                "Ningyu Zhang",
                "Jingyang Qian",
                "Penghui Yang",
                "Xiaohui Fan",
                "Huajun Chen"
            ],
            "affiliations": [
                "College of Computer Science and Technology, Zhejiang University, Hangzhou 310027, China",
                "College of Pharmaceutical Sciences, Zhejiang University, Hangzhou 310058, China",
                "Future Health Laboratory, Innovation Center of Yangtze River Delta, Zhejiang University, Jiaxing 314100, China",
                "Innovation Center in Zhejiang University, State Key Laboratory of Component-Based Chinese Medicine, Hangzhou 310058, China",
                "School of Software Technology, Zhejiang University, Ningbo 315048, China",
                "ZJU-Hangzhou Global Scientific and Technological Innovation Center, Hangzhou 311200, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.08187.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#multimodal",
                    "#dataset",
                    "#science",
                    "#healthcare"
                ],
                "emoji": "🧬",
                "ru": {
                    "title": "Естественный язык как ключ к расшифровке клеточной биологии",
                    "desc": "InstructCell - это мультимодальный ИИ-помощник для анализа данных одноклеточного РНК-секвенирования (scRNA-seq). Он использует архитектуру, способную интерпретировать как естественный язык, так и профили экспрессии генов. InstructCell позволяет исследователям выполнять такие задачи, как аннотация типов клеток и предсказание чувствительности к лекарствам, с помощью простых текстовых команд. Модель демонстрирует высокую производительность и адаптивность к различным экспериментальным условиям."
                },
                "en": {
                    "title": "InstructCell: Bridging Language and Biology for Seamless Single-Cell Analysis",
                    "desc": "This paper introduces InstructCell, an AI tool designed to simplify the analysis of single-cell RNA sequencing (scRNA-seq) data using natural language instructions. By creating a dataset that links text commands with scRNA-seq profiles, InstructCell allows researchers to perform complex tasks like cell type annotation and drug sensitivity prediction more intuitively. The model employs a multi-modal architecture that processes both text and biological data simultaneously, enhancing its usability. Evaluations show that InstructCell outperforms existing models, making single-cell analysis more accessible and efficient for researchers in the life sciences."
                },
                "zh": {
                    "title": "用自然语言解锁单细胞数据的潜力",
                    "desc": "这篇论文介绍了InstructCell，一个多模态的人工智能助手，旨在通过自然语言简化单细胞RNA测序(scRNA-seq)数据的分析。传统工具在处理细胞生物学的复杂数据时效率低下，而InstructCell通过将文本指令与scRNA-seq数据结合，提供了更直接和灵活的分析方式。该系统能够执行细胞类型注释、条件伪细胞生成和药物敏感性预测等关键任务，且使用简单的自然语言命令即可完成。评估结果表明，InstructCell在性能上优于现有的单细胞基础模型，同时适应多种实验条件，降低了技术门槛，促进了生物学的深入理解。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.08332",
            "title": "MangaNinja: Line Art Colorization with Precise Reference Following",
            "url": "https://huggingface.co/papers/2501.08332",
            "abstract": "Derived from diffusion models, MangaNinjia specializes in the task of reference-guided line art colorization. We incorporate two thoughtful designs to ensure precise character detail transcription, including a patch shuffling module to facilitate correspondence learning between the reference color image and the target line art, and a point-driven control scheme to enable fine-grained color matching. Experiments on a self-collected benchmark demonstrate the superiority of our model over current solutions in terms of precise colorization. We further showcase the potential of the proposed interactive point control in handling challenging cases, cross-character colorization, multi-reference harmonization, beyond the reach of existing algorithms.",
            "score": 12,
            "issue_id": 1673,
            "pub_date": "2025-01-14",
            "pub_date_card": {
                "ru": "14 января",
                "en": "January 14",
                "zh": "1月14日"
            },
            "hash": "20ea6b75639e2ced",
            "authors": [
                "Zhiheng Liu",
                "Ka Leong Cheng",
                "Xi Chen",
                "Jie Xiao",
                "Hao Ouyang",
                "Kai Zhu",
                "Yu Liu",
                "Yujun Shen",
                "Qifeng Chen",
                "Ping Luo"
            ],
            "affiliations": [
                "Ant Group",
                "HKU",
                "HKUST",
                "Tongyi Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.08332.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#diffusion",
                    "#benchmark"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Прецизионное раскрашивание манги с помощью ИИ",
                    "desc": "MangaNinjia - это модель для раскрашивания линейных рисунков манги, основанная на диффузионных моделях. Она использует модуль перемешивания патчей для обучения соответствиям между цветным изображением-образцом и целевым линейным рисунком. Модель также включает схему точечного контроля для точного подбора цветов. Эксперименты показывают превосходство MangaNinjia над существующими решениями в точности раскрашивания."
                },
                "en": {
                    "title": "MangaNinjia: Mastering Line Art Colorization with Precision",
                    "desc": "MangaNinjia is a model designed for coloring line art by using reference images. It employs a patch shuffling module to help the model learn how to match colors from the reference image to the target line art accurately. Additionally, it features a point-driven control scheme that allows for detailed color adjustments, ensuring that colors are applied precisely. Our experiments show that MangaNinjia outperforms existing methods in colorization tasks, especially in complex scenarios involving multiple references and different characters."
                },
                "zh": {
                    "title": "MangaNinjia：精准上色的新方法",
                    "desc": "MangaNinjia 是一种基于扩散模型的参考引导线条艺术上色技术。我们设计了两个模块来确保角色细节的准确转录，包括补丁洗牌模块和点驱动控制方案，以实现精细的颜色匹配。实验结果表明，我们的模型在精确上色方面优于现有解决方案。我们还展示了所提议的交互式点控制在处理复杂案例和多参考协调方面的潜力，超越了现有算法的能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.08316",
            "title": "Diffusion Adversarial Post-Training for One-Step Video Generation",
            "url": "https://huggingface.co/papers/2501.08316",
            "abstract": "The diffusion models are widely used for image and video generation, but their iterative generation process is slow and expansive. While existing distillation approaches have demonstrated the potential for one-step generation in the image domain, they still suffer from significant quality degradation. In this work, we propose Adversarial Post-Training (APT) against real data following diffusion pre-training for one-step video generation. To improve the training stability and quality, we introduce several improvements to the model architecture and training procedures, along with an approximated R1 regularization objective. Empirically, our experiments show that our adversarial post-trained model, Seaweed-APT, can generate 2-second, 1280x720, 24fps videos in real time using a single forward evaluation step. Additionally, our model is capable of generating 1024px images in a single step, achieving quality comparable to state-of-the-art methods.",
            "score": 8,
            "issue_id": 1672,
            "pub_date": "2025-01-14",
            "pub_date_card": {
                "ru": "14 января",
                "en": "January 14",
                "zh": "1月14日"
            },
            "hash": "4122a780e8356ce7",
            "authors": [
                "Shanchuan Lin",
                "Xin Xia",
                "Yuxi Ren",
                "Ceyuan Yang",
                "Xuefeng Xiao",
                "Lu Jiang"
            ],
            "affiliations": [
                "ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.08316.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#video",
                    "#diffusion",
                    "#training"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Революция в генерации видео: от итераций к мгновенному результату",
                    "desc": "Эта статья представляет новый метод под названием Adversarial Post-Training (APT) для одношаговой генерации видео. Авторы предлагают улучшения архитектуры модели и процедур обучения, включая аппроксимированную регуляризацию R1. Их модель Seaweed-APT способна генерировать 2-секундные видео высокого разрешения в реальном времени за один проход. Кроме того, модель может создавать изображения размером 1024px за один шаг, достигая качества, сравнимого с современными методами."
                },
                "en": {
                    "title": "Fast and High-Quality Video Generation with Seaweed-APT",
                    "desc": "This paper addresses the slow and costly iterative process of generating images and videos using diffusion models. The authors introduce Adversarial Post-Training (APT) to enhance one-step video generation while maintaining high quality. They implement architectural and procedural improvements, including an approximated R1 regularization, to stabilize training. Their model, Seaweed-APT, successfully generates high-quality 2-second videos and 1024px images in real time with a single forward evaluation step."
                },
                "zh": {
                    "title": "对抗后训练：快速高质量视频生成的新方法",
                    "desc": "扩散模型广泛应用于图像和视频生成，但其迭代生成过程较慢且成本高昂。现有的蒸馏方法在图像领域展示了单步生成的潜力，但仍存在显著的质量下降。本文提出了一种针对真实数据的对抗后训练（APT）方法，以实现单步视频生成。我们的实验表明，经过对抗后训练的模型Seaweed-APT能够实时生成1280x720、24fps的2秒视频，并且在单步生成1024px图像时，其质量可与最先进的方法相媲美。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.07730",
            "title": "Democratizing Text-to-Image Masked Generative Models with Compact Text-Aware One-Dimensional Tokens",
            "url": "https://huggingface.co/papers/2501.07730",
            "abstract": "Image tokenizers form the foundation of modern text-to-image generative models but are notoriously difficult to train. Furthermore, most existing text-to-image models rely on large-scale, high-quality private datasets, making them challenging to replicate. In this work, we introduce Text-Aware Transformer-based 1-Dimensional Tokenizer (TA-TiTok), an efficient and powerful image tokenizer that can utilize either discrete or continuous 1-dimensional tokens. TA-TiTok uniquely integrates textual information during the tokenizer decoding stage (i.e., de-tokenization), accelerating convergence and enhancing performance. TA-TiTok also benefits from a simplified, yet effective, one-stage training process, eliminating the need for the complex two-stage distillation used in previous 1-dimensional tokenizers. This design allows for seamless scalability to large datasets. Building on this, we introduce a family of text-to-image Masked Generative Models (MaskGen), trained exclusively on open data while achieving comparable performance to models trained on private data. We aim to release both the efficient, strong TA-TiTok tokenizers and the open-data, open-weight MaskGen models to promote broader access and democratize the field of text-to-image masked generative models.",
            "score": 4,
            "issue_id": 1673,
            "pub_date": "2025-01-13",
            "pub_date_card": {
                "ru": "13 января",
                "en": "January 13",
                "zh": "1月13日"
            },
            "hash": "80f40715084c602b",
            "authors": [
                "Dongwon Kim",
                "Ju He",
                "Qihang Yu",
                "Chenglin Yang",
                "Xiaohui Shen",
                "Suha Kwak",
                "Liang-Chieh Chen"
            ],
            "affiliations": [
                "ByteDance Seed",
                "POSTECH"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.07730.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#data",
                    "#training",
                    "#cv",
                    "#open_source"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Демократизация генерации изображений с помощью эффективной токенизации и открытых данных",
                    "desc": "В этой статье представлен новый подход к токенизации изображений для генеративных моделей текст-в-изображение под названием TA-TiTok. Данный токенизатор использует одномерные токены и интегрирует текстовую информацию на этапе детокенизации, что ускоряет сходимость и улучшает производительность. На основе TA-TiTok авторы разработали семейство моделей MaskGen, обученных исключительно на открытых данных. Целью работы является демократизация области генеративных моделей текст-в-изображение путем публикации эффективных токенизаторов и моделей с открытыми весами."
                },
                "en": {
                    "title": "Democratizing Text-to-Image Generation with TA-TiTok",
                    "desc": "This paper presents TA-TiTok, a novel image tokenizer designed for text-to-image generative models, which simplifies the training process and improves performance. Unlike traditional models that require large private datasets, TA-TiTok can effectively utilize open data, making it more accessible for researchers. The tokenizer incorporates textual information during the decoding stage, which helps it learn faster and perform better. Additionally, the authors introduce MaskGen, a family of generative models that leverage TA-TiTok and are trained on publicly available datasets, aiming to democratize access to advanced text-to-image generation technology."
                },
                "zh": {
                    "title": "高效的文本到图像生成模型，推动开放数据的使用",
                    "desc": "本文介绍了一种新的图像标记器，称为TA-TiTok，它可以有效地处理文本到图像的生成任务。TA-TiTok在解码阶段整合了文本信息，从而加快了模型的收敛速度并提高了性能。与以往的标记器不同，TA-TiTok采用了一种简化的一阶段训练过程，避免了复杂的两阶段蒸馏过程。我们还提出了一系列基于开放数据训练的文本到图像生成模型MaskGen，旨在促进更广泛的访问和民主化。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.08225",
            "title": "FramePainter: Endowing Interactive Image Editing with Video Diffusion Priors",
            "url": "https://huggingface.co/papers/2501.08225",
            "abstract": "Interactive image editing allows users to modify images through visual interaction operations such as drawing, clicking, and dragging. Existing methods construct such supervision signals from videos, as they capture how objects change with various physical interactions. However, these models are usually built upon text-to-image diffusion models, so necessitate (i) massive training samples and (ii) an additional reference encoder to learn real-world dynamics and visual consistency. In this paper, we reformulate this task as an image-to-video generation problem, so that inherit powerful video diffusion priors to reduce training costs and ensure temporal consistency. Specifically, we introduce FramePainter as an efficient instantiation of this formulation. Initialized with Stable Video Diffusion, it only uses a lightweight sparse control encoder to inject editing signals. Considering the limitations of temporal attention in handling large motion between two frames, we further propose matching attention to enlarge the receptive field while encouraging dense correspondence between edited and source image tokens. We highlight the effectiveness and efficiency of FramePainter across various of editing signals: it domainantly outperforms previous state-of-the-art methods with far less training data, achieving highly seamless and coherent editing of images, \\eg, automatically adjust the reflection of the cup. Moreover, FramePainter also exhibits exceptional generalization in scenarios not present in real-world videos, \\eg, transform the clownfish into shark-like shape. Our code will be available at https://github.com/YBYBZhang/FramePainter.",
            "score": 3,
            "issue_id": 1673,
            "pub_date": "2025-01-14",
            "pub_date_card": {
                "ru": "14 января",
                "en": "January 14",
                "zh": "1月14日"
            },
            "hash": "811cfd0f18eb1e53",
            "authors": [
                "Yabo Zhang",
                "Xinpeng Zhou",
                "Yihan Zeng",
                "Hang Xu",
                "Hui Li",
                "Wangmeng Zuo"
            ],
            "affiliations": [
                "Harbin Institute of Technology",
                "Huawei Noahs Ark Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.08225.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#cv",
                    "#optimization",
                    "#diffusion"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "FramePainter: эффективное редактирование изображений через генерацию видео",
                    "desc": "Статья представляет FramePainter - новый подход к интерактивному редактированию изображений, основанный на генерации видео. В отличие от существующих методов, использующих модели диффузии текст-изображение, FramePainter опирается на мощные видео-диффузионные модели для обеспечения временной согласованности и снижения затрат на обучение. Метод использует легковесный энкодер для внедрения сигналов редактирования и вводит механизм согласованного внимания для улучшения обработки крупных движений между кадрами. FramePainter превосходит современные методы, требуя значительно меньше обучающих данных и демонстрируя высокую обобщающую способность."
                },
                "en": {
                    "title": "Revolutionizing Image Editing with Efficient Video Diffusion",
                    "desc": "This paper presents FramePainter, a novel approach to interactive image editing that reformulates the task as image-to-video generation. By leveraging video diffusion models, FramePainter reduces the need for extensive training data while ensuring temporal consistency in edited images. It utilizes a lightweight sparse control encoder to effectively incorporate editing signals, and introduces matching attention to improve the handling of large motion between frames. The results demonstrate that FramePainter significantly outperforms existing methods, achieving seamless image edits and showcasing strong generalization capabilities."
                },
                "zh": {
                    "title": "FramePainter：高效的图像编辑新方法",
                    "desc": "本文提出了一种交互式图像编辑的新方法，称为FramePainter。该方法将图像编辑任务重新定义为图像到视频的生成问题，从而利用强大的视频扩散先验，降低训练成本并确保时间一致性。FramePainter使用轻量级的稀疏控制编码器来注入编辑信号，并通过匹配注意力机制增强了对大运动的处理能力。实验结果表明，FramePainter在各种编辑信号下表现优异，能够实现无缝且连贯的图像编辑，且在未见过的场景中也展现出卓越的泛化能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.08328",
            "title": "PokerBench: Training Large Language Models to become Professional Poker Players",
            "url": "https://huggingface.co/papers/2501.08328",
            "abstract": "We introduce PokerBench - a benchmark for evaluating the poker-playing abilities of large language models (LLMs). As LLMs excel in traditional NLP tasks, their application to complex, strategic games like poker poses a new challenge. Poker, an incomplete information game, demands a multitude of skills such as mathematics, reasoning, planning, strategy, and a deep understanding of game theory and human psychology. This makes Poker the ideal next frontier for large language models. PokerBench consists of a comprehensive compilation of 11,000 most important scenarios, split between pre-flop and post-flop play, developed in collaboration with trained poker players. We evaluate prominent models including GPT-4, ChatGPT 3.5, and various Llama and Gemma series models, finding that all state-of-the-art LLMs underperform in playing optimal poker. However, after fine-tuning, these models show marked improvements. We validate PokerBench by having models with different scores compete with each other, demonstrating that higher scores on PokerBench lead to higher win rates in actual poker games. Through gameplay between our fine-tuned model and GPT-4, we also identify limitations of simple supervised fine-tuning for learning optimal playing strategy, suggesting the need for more advanced methodologies for effectively training language models to excel in games. PokerBench thus presents a unique benchmark for a quick and reliable evaluation of the poker-playing ability of LLMs as well as a comprehensive benchmark to study the progress of LLMs in complex game-playing scenarios. The dataset and code will be made available at: https://github.com/pokerllm/pokerbench.",
            "score": 2,
            "issue_id": 1674,
            "pub_date": "2025-01-14",
            "pub_date_card": {
                "ru": "14 января",
                "en": "January 14",
                "zh": "1月14日"
            },
            "hash": "7b4dacedffdbfa15",
            "authors": [
                "Richard Zhuang",
                "Akshat Gupta",
                "Richard Yang",
                "Aniket Rahane",
                "Zhengyu Li",
                "Gopala Anumanchipalli"
            ],
            "affiliations": [
                "Georgia Institute of Technology",
                "University of California, Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.08328.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#reasoning",
                    "#games",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "🃏",
                "ru": {
                    "title": "PokerBench: новый рубеж для оценки стратегических способностей языковых моделей",
                    "desc": "PokerBench - это новый бенчмарк для оценки способностей больших языковых моделей (LLM) играть в покер. Он включает 11000 важнейших сценариев игры, разработанных совместно с профессиональными игроками. Авторы оценили производительность современных LLM, таких как GPT-4 и ChatGPT 3.5, обнаружив, что все модели показывают результаты ниже оптимальных. После дообучения модели демонстрируют значительное улучшение, но авторы отмечают ограничения простого обучения с учителем для освоения оптимальной стратегии игры."
                },
                "en": {
                    "title": "PokerBench: Elevating LLMs to Master the Game of Poker",
                    "desc": "PokerBench is a new benchmark designed to assess the poker-playing skills of large language models (LLMs). It focuses on the unique challenges of poker, which requires a blend of mathematical skills, strategic reasoning, and an understanding of human psychology. The benchmark includes 11,000 scenarios that cover various aspects of the game, and it has been tested on several leading models, revealing that they initially struggle with optimal poker play. However, after fine-tuning, these models show significant improvement, highlighting the need for advanced training techniques to enhance their performance in complex games."
                },
                "zh": {
                    "title": "PokerBench：评估语言模型扑克能力的新基准",
                    "desc": "我们介绍了PokerBench，这是一个用于评估大型语言模型（LLMs）扑克游戏能力的基准。扑克是一种不完全信息游戏，需要数学、推理、规划、策略以及对博弈论和人类心理的深刻理解。PokerBench包含11,000个重要场景，分为翻牌前和翻牌后游戏，经过训练的扑克玩家共同开发。通过对不同模型的评估，我们发现尽管当前的LLMs在扑克游戏中表现不佳，但经过微调后，它们的表现有显著提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.08197",
            "title": "OpenCSG Chinese Corpus: A Series of High-quality Chinese Datasets for LLM Training",
            "url": "https://huggingface.co/papers/2501.08197",
            "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities, but their success heavily relies on the quality of pretraining corpora. For Chinese LLMs, the scarcity of high-quality Chinese datasets presents a significant challenge, often limiting their performance. To address this issue, we propose the OpenCSG Chinese Corpus, a series of high-quality datasets specifically designed for LLM pretraining, post-training, and fine-tuning. This corpus includes Fineweb-edu-chinese, Fineweb-edu-chinese-v2, Cosmopedia-chinese, and Smoltalk-chinese, each with distinct characteristics: Fineweb-edu datasets focus on filtered, high-quality content derived from diverse Chinese web sources; Cosmopedia-chinese provides synthetic, textbook-style data for knowledge-intensive training; and Smoltalk-chinese emphasizes stylistic and diverse chat-format data. The OpenCSG Chinese Corpus is characterized by its high-quality text, diverse coverage across domains, and scalable, reproducible data curation processes. Additionally, we conducted extensive experimental analyses, including evaluations on smaller parameter models, which demonstrated significant performance improvements in tasks such as C-Eval, showcasing the effectiveness of the corpus for training Chinese LLMs.",
            "score": 1,
            "issue_id": 1675,
            "pub_date": "2025-01-14",
            "pub_date_card": {
                "ru": "14 января",
                "en": "January 14",
                "zh": "1月14日"
            },
            "hash": "27267ae1a569051c",
            "authors": [
                "Yijiong Yu",
                "Ziyun Dai",
                "Zekun Wang",
                "Wei Wang",
                "Ran Chen",
                "Ji Pei"
            ],
            "affiliations": [
                "OpenCSG",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.08197.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#open_source",
                    "#dataset",
                    "#synthetic",
                    "#training",
                    "#low_resource"
                ],
                "emoji": "🐉",
                "ru": {
                    "title": "Прорыв в обучении китайских языковых моделей: OpenCSG Chinese Corpus",
                    "desc": "Эта статья представляет OpenCSG Chinese Corpus - набор высококачественных китайских датасетов для предобучения, пост-обучения и тонкой настройки больших языковых моделей (LLM). Корпус включает в себя несколько датасетов, каждый с уникальными характеристиками: от отфильтрованного веб-контента до синтетических учебных данных и разговорных форматов. Авторы подчеркивают высокое качество текста, разнообразие тематик и масштабируемость процесса сбора данных. Эксперименты показали значительное улучшение производительности моделей на различных задачах, включая C-Eval."
                },
                "en": {
                    "title": "Empowering Chinese LLMs with OpenCSG Corpus",
                    "desc": "This paper introduces the OpenCSG Chinese Corpus, a collection of high-quality datasets aimed at improving the performance of Chinese large language models (LLMs). The corpus includes several datasets, each tailored for different training needs: Fineweb-edu datasets focus on high-quality web content, Cosmopedia-chinese offers synthetic textbook-style data, and Smoltalk-chinese provides diverse chat-format data. The authors highlight the importance of quality pretraining data for LLMs and demonstrate through experiments that using this corpus leads to significant performance gains in various evaluation tasks. Overall, the OpenCSG Chinese Corpus addresses the challenge of limited high-quality datasets for Chinese LLMs, promoting better training outcomes."
                },
                "zh": {
                    "title": "提升中文LLM性能的高质量语料库",
                    "desc": "大型语言模型（LLMs）在处理自然语言方面表现出色，但其成功依赖于高质量的预训练语料库。针对中文LLMs，优质中文数据集的稀缺性成为了一个重大挑战，限制了它们的性能。为了解决这个问题，我们提出了OpenCSG中文语料库，这是一系列专门为LLM预训练、后训练和微调设计的高质量数据集。该语料库包括Fineweb-edu-chinese、Fineweb-edu-chinese-v2、Cosmopedia-chinese和Smoltalk-chinese，涵盖了多样化的内容和风格，显著提升了中文LLMs的训练效果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.08167",
            "title": "Potential and Perils of Large Language Models as Judges of Unstructured Textual Data",
            "url": "https://huggingface.co/papers/2501.08167",
            "abstract": "Rapid advancements in large language models have unlocked remarkable capabilities when it comes to processing and summarizing unstructured text data. This has implications for the analysis of rich, open-ended datasets, such as survey responses, where LLMs hold the promise of efficiently distilling key themes and sentiments. However, as organizations increasingly turn to these powerful AI systems to make sense of textual feedback, a critical question arises, can we trust LLMs to accurately represent the perspectives contained within these text based datasets? While LLMs excel at generating human-like summaries, there is a risk that their outputs may inadvertently diverge from the true substance of the original responses. Discrepancies between the LLM-generated outputs and the actual themes present in the data could lead to flawed decision-making, with far-reaching consequences for organizations. This research investigates the effectiveness of LLMs as judge models to evaluate the thematic alignment of summaries generated by other LLMs. We utilized an Anthropic Claude model to generate thematic summaries from open-ended survey responses, with Amazon's Titan Express, Nova Pro, and Meta's Llama serving as LLM judges. The LLM-as-judge approach was compared to human evaluations using Cohen's kappa, Spearman's rho, and Krippendorff's alpha, validating a scalable alternative to traditional human centric evaluation methods. Our findings reveal that while LLMs as judges offer a scalable solution comparable to human raters, humans may still excel at detecting subtle, context-specific nuances. This research contributes to the growing body of knowledge on AI assisted text analysis. We discuss limitations and provide recommendations for future research, emphasizing the need for careful consideration when generalizing LLM judge models across various contexts and use cases.",
            "score": 1,
            "issue_id": 1675,
            "pub_date": "2025-01-14",
            "pub_date_card": {
                "ru": "14 января",
                "en": "January 14",
                "zh": "1月14日"
            },
            "hash": "866161709624c632",
            "authors": [
                "Rewina Bedemariam",
                "Natalie Perez",
                "Sreyoshi Bhaduri",
                "Satya Kapoor",
                "Alex Gil",
                "Elizabeth Conjar",
                "Ikkei Itoku",
                "David Theil",
                "Aman Chadha",
                "Naumaan Nayyar"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2501.08167.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#dataset",
                    "#science",
                    "#ethics",
                    "#multimodal",
                    "#benchmark",
                    "#interpretability"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "LLM как судьи: масштабируемая альтернатива человеческим оценкам в анализе текста",
                    "desc": "Исследование посвящено использованию больших языковых моделей (LLM) для анализа неструктурированных текстовых данных, таких как ответы на опросы. Авторы изучают эффективность применения LLM в качестве судей для оценки тематического соответствия сгенерированных другими LLM резюме. Результаты показывают, что LLM-судьи предлагают масштабируемое решение, сопоставимое с оценками людей, хотя люди все еще могут превосходить их в обнаружении тонких, контекстно-зависимых нюансов. Исследование вносит вклад в растущий объем знаний об анализе текста с помощью искусственного интеллекта."
                },
                "en": {
                    "title": "Trusting AI: Evaluating LLMs for Accurate Text Analysis",
                    "desc": "This paper explores the use of large language models (LLMs) for summarizing and analyzing unstructured text data, particularly from open-ended survey responses. It raises concerns about the trustworthiness of LLM-generated summaries, as they may not accurately reflect the original sentiments and themes present in the data. The research introduces an LLM-as-judge framework, where one LLM generates summaries while others evaluate their thematic alignment, comparing this method to human evaluations. The findings suggest that while LLMs can provide a scalable alternative to human raters, they may struggle with detecting subtle nuances that humans can identify, highlighting the importance of careful application in different contexts."
                },
                "zh": {
                    "title": "信任大型语言模型的总结能力吗？",
                    "desc": "这篇论文探讨了大型语言模型（LLMs）在处理和总结非结构化文本数据方面的能力，尤其是在分析开放式调查反馈时的应用。研究表明，虽然LLMs能够生成类似人类的总结，但它们的输出可能与原始文本的真实主题存在偏差，这可能导致错误的决策。为了评估LLMs生成的总结与实际主题的一致性，研究使用了LLMs作为评判模型，并与人类评估进行了比较。结果显示，LLMs作为评判者提供了一种可扩展的解决方案，但人类在捕捉细微的上下文特征方面仍然表现更佳。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.08292",
            "title": "HALoGEN: Fantastic LLM Hallucinations and Where to Find Them",
            "url": "https://huggingface.co/papers/2501.08292",
            "abstract": "Despite their impressive ability to generate high-quality and fluent text, generative large language models (LLMs) also produce hallucinations: statements that are misaligned with established world knowledge or provided input context. However, measuring hallucination can be challenging, as having humans verify model generations on-the-fly is both expensive and time-consuming. In this work, we release HALoGEN, a comprehensive hallucination benchmark consisting of: (1) 10,923 prompts for generative models spanning nine domains including programming, scientific attribution, and summarization, and (2) automatic high-precision verifiers for each use case that decompose LLM generations into atomic units, and verify each unit against a high-quality knowledge source. We use this framework to evaluate ~150,000 generations from 14 language models, finding that even the best-performing models are riddled with hallucinations (sometimes up to 86% of generated atomic facts depending on the domain). We further define a novel error classification for LLM hallucinations based on whether they likely stem from incorrect recollection of training data (Type A errors), or incorrect knowledge in training data (Type B errors), or are fabrication (Type C errors). We hope our framework provides a foundation to enable the principled study of why generative models hallucinate, and advances the development of trustworthy large language models.",
            "score": 1,
            "issue_id": 1673,
            "pub_date": "2025-01-14",
            "pub_date_card": {
                "ru": "14 января",
                "en": "January 14",
                "zh": "1月14日"
            },
            "hash": "f6751d682ff824ed",
            "authors": [
                "Abhilasha Ravichander",
                "Shrusti Ghela",
                "David Wadden",
                "Yejin Choi"
            ],
            "affiliations": [
                "Google",
                "NVIDIA",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.08292.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#hallucinations",
                    "#benchmark"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "HALoGEN: Автоматическая проверка галлюцинаций в языковых моделях",
                    "desc": "Эта статья представляет HALoGEN - комплексный инструмент для оценки галлюцинаций в больших языковых моделях (LLM). Авторы создали набор из 10,923 промптов в девяти различных областях и автоматические верификаторы высокой точности для проверки генераций LLM. Исследование выявило, что даже лучшие модели страдают от галлюцинаций, иногда до 86% сгенерированных фактов оказываются неверными. Авторы также предложили новую классификацию ошибок LLM, разделив их на три типа в зависимости от источника галлюцинаций."
                },
                "en": {
                    "title": "HALoGEN: A Benchmark for Measuring Hallucinations in Language Models",
                    "desc": "This paper introduces HALoGEN, a new benchmark designed to measure hallucinations in generative large language models (LLMs). Hallucinations refer to incorrect statements generated by these models that do not align with known facts or the given context. The benchmark includes over 10,000 prompts across various domains and employs automatic verifiers to assess the accuracy of model outputs. The study reveals that even top-performing models exhibit significant hallucinations, prompting a classification system for different types of errors to better understand their origins and improve model reliability."
                },
                "zh": {
                    "title": "揭示生成模型的幻觉问题",
                    "desc": "尽管生成性大型语言模型（LLMs）能够生成高质量和流畅的文本，但它们也会产生幻觉，即与已知世界知识或输入上下文不一致的陈述。测量幻觉的难度在于，实时验证模型生成的内容既昂贵又耗时。为此，我们推出了HALoGEN，这是一个全面的幻觉基准，包含10,923个跨越九个领域的提示和自动高精度验证器。我们的研究发现，即使是表现最好的模型，其生成的原子事实中也有高达86%可能存在幻觉，这为理解生成模型的幻觉提供了基础。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.07888",
            "title": "Tarsier2: Advancing Large Vision-Language Models from Detailed Video Description to Comprehensive Video Understanding",
            "url": "https://huggingface.co/papers/2501.07888",
            "abstract": "We introduce Tarsier2, a state-of-the-art large vision-language model (LVLM) designed for generating detailed and accurate video descriptions, while also exhibiting superior general video understanding capabilities. Tarsier2 achieves significant advancements through three key upgrades: (1) Scaling pre-training data from 11M to 40M video-text pairs, enriching both volume and diversity; (2) Performing fine-grained temporal alignment during supervised fine-tuning; (3) Using model-based sampling to automatically construct preference data and applying DPO training for optimization. Extensive experiments show that Tarsier2-7B consistently outperforms leading proprietary models, including GPT-4o and Gemini 1.5 Pro, in detailed video description tasks. On the DREAM-1K benchmark, Tarsier2-7B improves F1 by 2.8\\% over GPT-4o and 5.8\\% over Gemini-1.5-Pro. In human side-by-side evaluations, Tarsier2-7B shows a +8.6\\% performance advantage over GPT-4o and +24.9\\% over Gemini-1.5-Pro. Tarsier2-7B also sets new state-of-the-art results across 15 public benchmarks, spanning tasks such as video question-answering, video grounding, hallucination test, and embodied question-answering, demonstrating its versatility as a robust generalist vision-language model.",
            "score": 0,
            "issue_id": 1674,
            "pub_date": "2025-01-14",
            "pub_date_card": {
                "ru": "14 января",
                "en": "January 14",
                "zh": "1月14日"
            },
            "hash": "54780a4b6f93fb10",
            "authors": [
                "Liping Yuan",
                "Jiawei Wang",
                "Haomiao Sun",
                "Yuchen Zhang",
                "Yuan Lin"
            ],
            "affiliations": [
                "ByteDance Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.07888.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#cv",
                    "#hallucinations",
                    "#optimization",
                    "#video",
                    "#benchmark"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Tarsier2: Революция в понимании видео искусственным интеллектом",
                    "desc": "Tarsier2 - это современная крупномасштабная модель для понимания видео и языка (LVLM), разработанная для создания детальных и точных описаний видео. Модель достигает значительных улучшений благодаря увеличению объема обучающих данных, точной временной синхронизации при тонкой настройке и применению обучения с предпочтениями (DPO). Tarsier2-7B превосходит ведущие проприетарные модели, такие как GPT-4o и Gemini 1.5 Pro, в задачах детального описания видео. Модель также устанавливает новые рекорды в 15 публичных бенчмарках, демонстрируя свою универсальность как надежная модель общего назначения для понимания видео и языка."
                },
                "en": {
                    "title": "Tarsier2: Redefining Video Understanding with Advanced LVLM Technology",
                    "desc": "Tarsier2 is a cutting-edge large vision-language model (LVLM) that excels in generating precise and detailed descriptions of videos while showcasing advanced video comprehension skills. The model's improvements stem from three main enhancements: increasing the pre-training dataset from 11 million to 40 million video-text pairs, implementing fine-grained temporal alignment during fine-tuning, and utilizing model-based sampling for preference data construction with DPO training for optimization. Extensive testing reveals that Tarsier2-7B surpasses top proprietary models like GPT-4o and Gemini 1.5 Pro in video description tasks, achieving notable F1 score improvements on the DREAM-1K benchmark. Additionally, Tarsier2-7B sets new records across 15 public benchmarks, proving its effectiveness in various tasks such as video question-answering and video grounding."
                },
                "zh": {
                    "title": "Tarsier2：视频描述的新标杆",
                    "desc": "Tarsier2是一种先进的大型视觉语言模型，专门用于生成详细且准确的视频描述，同时具备出色的视频理解能力。该模型通过三个关键升级实现了显著进步：首先，预训练数据从1100万对视频文本扩展到4000万对，增加了数据的数量和多样性；其次，在监督微调过程中进行精细的时间对齐；最后，采用基于模型的采样自动构建偏好数据，并应用DPO训练进行优化。实验结果表明，Tarsier2-7B在视频描述任务中持续超越领先的专有模型，展现出其作为强大通用视觉语言模型的多样性。"
                }
            }
        }
    ],
    "link_prev": "2025-01-14.html",
    "link_next": "2025-01-16.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "14.01",
        "en": "01/14",
        "zh": "1月14日"
    },
    "short_date_next": {
        "ru": "16.01",
        "en": "01/16",
        "zh": "1月16日"
    },
    "categories": {
        "#dataset": 6,
        "#data": 3,
        "#benchmark": 6,
        "#agents": 0,
        "#cv": 4,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 3,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 1,
        "#training": 6,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 5,
        "#survey": 0,
        "#diffusion": 3,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 2,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 2,
        "#low_resource": 1
    },
    "zh": {
        "text": "这篇文章介绍了一种新的方法，叫做过程奖励模型（PRMs），用于大型语言模型（LLMs）在数学推理中的过程监督。目标是识别和减少推理过程中的错误。研究发现，常用的蒙特卡罗（MC）估计方法效果不佳，因为它依赖完成模型评估当前步骤的正确性，导致步骤验证不准确。文章还指出了传统Best-of-N（BoN）评估策略的偏差，并提出了一种共识过滤机制，结合MC估计和LLM-as-a-judge，改进了模型性能和数据效率。最后，文章发布了一个新的最先进的PRM，并提供了未来研究的实用指南。",
        "title": "The Lessons of Developing Process Reward Models in Mathematical Reasoning",
        "pinyin": "这篇文章介绍了一种新的方法，叫做过程奖励模型（PRMs），用于大型语言模型（LLMs）在数学推理中的过程监督。目标是识别和减少推理过程中的错误。研究发现，常用的蒙特卡罗（MC）估计方法效果不佳，因为它依赖完成模型评估当前步骤的正确性，导致步骤验证不准确。文章还指出了传统Best-of-N（BoN）评估策略的偏差，并提出了一种共识过滤机制，结合MC估计和LLM-as-a-judge，改进了模型性能和数据效率。最后，文章发布了一个新的最先进的PRM，并提供了未来研究的实用指南。\n\nzhè piān wénzhāng jièshào le yī zhǒng xīn de fāngfǎ, jiàozuò guòchéng jiǎnglì móxíng (PRMs), yòngyú dàxíng yǔyán móxíng (LLMs) zài shùxué tuīlǐ zhōng de guòchéng jiàndū. Mùbiāo shì shíbié hé jiǎnshǎo tuīlǐ guòchéng zhōng de cuòwù. Yánjiū fāxiàn, chángyòng de méngtèkǎluó (MC) gūjì fāngfǎ xiàojià, yīnwèi tā yīlài wánchéng móxíng píngjià dāngqián bùzhòu de zhèngquèxìng, dǎozhì bùzhòu yànzhèng bù zhǔnquè. Wénzhāng hái zhǐchū le chuántǒng Best-of-N (BoN) píngjià cèlüè de piānchā, bìng tíchū le yī zhǒng gòngshì guòlǜ jīzhì, jiéhé MC gūjì hé LLM-as-a-judge, gǎijìn le móxíng xìngnéng hé shùjù xiàoyòng. Zuìhòu, wénzhāng fābù le yīgè xīn de zuì xiānjìn de PRM, bìng tígōng le wèilái yánjiū de shíyòng zhǐnán.",
        "vocab": "[\n    {\"word\": \"过程奖励模型\", \"pinyin\": \"guòchéng jiǎnglì móxíng\", \"trans\": \"Process Reward Model\"},\n    {\"word\": \"大型语言模型\", \"pinyin\": \"dàxíng yǔyán móxíng\", \"trans\": \"Large Language Model\"},\n    {\"word\": \"数学推理\", \"pinyin\": \"shùxué tuīlǐ\", \"trans\": \"Mathematical Reasoning\"},\n    {\"word\": \"过程监督\", \"pinyin\": \"guòchéng jiàndū\", \"trans\": \"Process Supervision\"},\n    {\"word\": \"蒙特卡罗\", \"pinyin\": \"méngtèkǎluó\", \"trans\": \"Monte Carlo\"},\n    {\"word\": \"估计\", \"pinyin\": \"gūjì\", \"trans\": \"Estimation\"},\n    {\"word\": \"依赖\", \"pinyin\": \"yīlài\", \"trans\": \"Depend\"},\n    {\"word\": \"评估\", \"pinyin\": \"pínggū\", \"trans\": \"Evaluate\"},\n    {\"word\": \"步骤验证\", \"pinyin\": \"bùzhòu yànzhèng\", \"trans\": \"Step Verification\"},\n    {\"word\": \"偏差\", \"pinyin\": \"piānchā\", \"trans\": \"Bias\"},\n    {\"word\": \"共识过滤机制\", \"pinyin\": \"gòngshí guòlǜ jīzhì\", \"trans\": \"Consensus Filtering Mechanism\"},\n    {\"word\": \"LLM-as-a-judge\", \"pinyin\": \"LLM-as-a-judge\", \"trans\": \"LLM-as-a-judge\"},\n    {\"word\": \"最先进\", \"pinyin\": \"zuìxiānjìn\", \"trans\": \"State-of-the-art\"},\n    {\"word\": \"实用指南\", \"pinyin\": \"shíyòng zhǐnán\", \"trans\": \"Practical Guide\"}\n]",
        "trans": "This article introduces a new method called Process Reward Models (PRMs) for process supervision of large language models (LLMs) in mathematical reasoning. The goal is to identify and reduce errors in the reasoning process. The research found that the commonly used Monte Carlo (MC) estimation method performs poorly because it relies on the completion model to evaluate the correctness of the current step, leading to inaccurate step verification. The article also points out the bias in traditional Best-of-N (BoN) evaluation strategies and proposes a consensus filtering mechanism that combines MC estimation and LLM-as-a-judge to improve model performance and data efficiency. Finally, the article releases a new state-of-the-art PRM and provides practical guidelines for future research.",
        "update_ts": "2025-01-14 09:10"
    }
}