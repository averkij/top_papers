{
    "date": {
        "ru": "12 мая",
        "en": "May 12",
        "zh": "5月12日"
    },
    "time_utc": "2025-05-12 06:17",
    "weekday": 0,
    "issue_id": 3704,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2505.06111",
            "title": "UniVLA: Learning to Act Anywhere with Task-centric Latent Actions",
            "url": "https://huggingface.co/papers/2505.06111",
            "abstract": "A generalist robot should perform effectively across various environments. However, most existing approaches heavily rely on scaling action-annotated data to enhance their capabilities. Consequently, they are often limited to single physical specification and struggle to learn transferable knowledge across different embodiments and environments. To confront these limitations, we propose UniVLA, a new framework for learning cross-embodiment vision-language-action (VLA) policies. Our key innovation is to derive task-centric action representations from videos with a latent action model. This enables us to exploit extensive data across a wide spectrum of embodiments and perspectives. To mitigate the effect of task-irrelevant dynamics, we incorporate language instructions and establish a latent action model within the DINO feature space. Learned from internet-scale videos, the generalist policy can be deployed to various robots through efficient latent action decoding. We obtain state-of-the-art results across multiple manipulation and navigation benchmarks, as well as real-robot deployments. UniVLA achieves superior performance over OpenVLA with less than 1/20 of pretraining compute and 1/10 of downstream data. Continuous performance improvements are observed as heterogeneous data, even including human videos, are incorporated into the training pipeline. The results underscore UniVLA's potential to facilitate scalable and efficient robot policy learning.",
            "score": 0,
            "issue_id": 3704,
            "pub_date": "2025-05-09",
            "pub_date_card": {
                "ru": "9 мая",
                "en": "May 9",
                "zh": "5月9日"
            },
            "hash": "bf19981dd100b8fb",
            "authors": [
                "Qingwen Bu",
                "Yanting Yang",
                "Jisong Cai",
                "Shenyuan Gao",
                "Guanghui Ren",
                "Maoqing Yao",
                "Ping Luo",
                "Hongyang Li"
            ],
            "affiliations": [
                "AgiBot",
                "OpenDriveLab",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.06111.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#robotics",
                    "#training",
                    "#benchmark",
                    "#agents",
                    "#transfer_learning"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Универсальное обучение роботов через видео и язык",
                    "desc": "UniVLA - это новый фреймворк для обучения универсальных политик взаимодействия робота с окружающей средой на основе зрения, языка и действий. Ключевая инновация заключается в использовании скрытой модели действий для извлечения представлений из видео, что позволяет использовать разнородные данные из различных воплощений и перспектив. Фреймворк демонстрирует превосходные результаты на нескольких бенчмарках по манипуляции и навигации, а также в реальных роботизированных системах. UniVLA достигает лучшей производительности по сравнению с OpenVLA, используя при этом значительно меньше вычислительных ресурсов и данных."
                },
                "en": {
                    "title": "UniVLA: Empowering Robots with Cross-Embodiment Learning",
                    "desc": "The paper introduces UniVLA, a framework designed to enhance the capabilities of generalist robots by learning cross-embodiment vision-language-action (VLA) policies. It addresses the limitations of existing methods that depend on large amounts of action-annotated data and are restricted to specific physical forms. By utilizing a latent action model derived from videos, UniVLA can leverage diverse data sources and improve knowledge transfer across different robot embodiments and environments. The framework demonstrates state-of-the-art performance in various tasks while requiring significantly less computational resources and data compared to previous approaches."
                },
                "zh": {
                    "title": "UniVLA：提升通用机器人学习效率的新框架",
                    "desc": "本文提出了一种新的框架UniVLA，用于学习跨体现的视觉-语言-动作（VLA）策略，以提高通用机器人在不同环境中的表现。我们通过视频中的潜在动作模型提取以任务为中心的动作表示，从而利用广泛的多样化数据。为了减少与任务无关的动态影响，我们结合了语言指令，并在DINO特征空间中建立了潜在动作模型。实验结果表明，UniVLA在多个操作和导航基准测试中表现优异，且在预训练计算和下游数据方面的需求显著低于现有方法。"
                }
            }
        }
    ],
    "link_prev": "2025-05-09.html",
    "link_next": "2025-05-13.html",
    "link_month": "2025-05.html",
    "short_date_prev": {
        "ru": "09.05",
        "en": "05/09",
        "zh": "5月9日"
    },
    "short_date_next": {
        "ru": "13.05",
        "en": "05/13",
        "zh": "5月13日"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 1,
        "#agents": 1,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 1,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章讨论了推理在人工智能中的重要性，特别是在开放、不确定和多模态环境中。大型多模态推理模型（LMRMs）结合了文本、图像、音频和视频等模态，支持复杂推理能力。研究从模块化、感知驱动的流水线发展到统一的、以语言为中心的框架。文章回顾了早期任务特定模块的努力，并检查了最近将推理统一到多模态LLMs的方法，最后讨论了未来的方向。",
        "title": "Perception, Reason, Think, and Plan: A Survey on Large Multimodal\n  Reasoning Models",
        "pinyin": "这篇文章讨论了推理在人工智能中的重要性，特别是在开放、不确定和多模态环境中。大型多模态推理模型（LMRMs）结合了文本、图像、音频和视频等模态，支持复杂推理能力。研究从模块化、感知驱动的流水线发展到统一的、以语言为中心的框架。文章回顾了早期任务特定模块的努力，并检查了最近将推理统一到多模态LLMs的方法，最后讨论了未来的方向。\n\nZhè piān wénzhāng tǎolùn le tuīlǐ zài réngōng zhìnéng zhōng de zhòngyàoxìng, tèbié shì zài kāifàng, bù quèdìng hé duō móshì huánjìng zhōng. Dàxíng duō móshì tuīlǐ móxíng (LMRMs) jiéhé le wénběn, túxiàng, yīnpiàn hé shìpín děng móshì, zhīchí fùzá tuīlǐ nénglì. Yánjiū cóng mókùhuà, gǎnjué qūdòng de liúshuǐxiàn fāzhǎn dào tǒngyī de, yǐ yǔyán wéi zhōngxīn de kuàngjià. Wénzhāng huígù le zǎoqī rènwù tèdìng mókù de nǔlì, bìng jiǎnchá le zuìjìn jiāng tuīlǐ tǒngyī dào duō móshì LLMs de fāngfǎ, zuìhòu tǎolùn le wèilái de fāngxiàng.",
        "vocab": "[\n    {\"word\": \"推理\", \"pinyin\": \"tuī lǐ\", \"trans\": \"reasoning\"},\n    {\"word\": \"人工智能\", \"pinyin\": \"rén gōng zhì néng\", \"trans\": \"artificial intelligence\"},\n    {\"word\": \"不确定\", \"pinyin\": \"bù què dìng\", \"trans\": \"uncertain\"},\n    {\"word\": \"多模态\", \"pinyin\": \"duō mó tài\", \"trans\": \"multimodal\"},\n    {\"word\": \"模态\", \"pinyin\": \"mó tài\", \"trans\": \"modality\"},\n    {\"word\": \"结合\", \"pinyin\": \"jié hé\", \"trans\": \"combine\"},\n    {\"word\": \"支持\", \"pinyin\": \"zhī chí\", \"trans\": \"support\"},\n    {\"word\": \"复杂\", \"pinyin\": \"fù zá\", \"trans\": \"complex\"},\n    {\"word\": \"模块化\", \"pinyin\": \"mó kuài huà\", \"trans\": \"modularization\"},\n    {\"word\": \"感知\", \"pinyin\": \"gǎn zhī\", \"trans\": \"perception\"},\n    {\"word\": \"驱动\", \"pinyin\": \"qū dòng\", \"trans\": \"drive\"},\n    {\"word\": \"流水线\", \"pinyin\": \"liú shuǐ xiàn\", \"trans\": \"pipeline\"},\n    {\"word\": \"统一\", \"pinyin\": \"tǒng yī\", \"trans\": \"unified\"},\n    {\"word\": \"框架\", \"pinyin\": \"kuàng jià\", \"trans\": \"framework\"},\n    {\"word\": \"回顾\", \"pinyin\": \"huí gù\", \"trans\": \"review\"},\n    {\"word\": \"任务\", \"pinyin\": \"rèn wù\", \"trans\": \"task\"},\n    {\"word\": \"特定\", \"pinyin\": \"tè dìng\", \"trans\": \"specific\"},\n    {\"word\": \"检查\", \"pinyin\": \"jiǎn chá\", \"trans\": \"examine\"},\n    {\"word\": \"方法\", \"pinyin\": \"fāng fǎ\", \"trans\": \"method\"},\n    {\"word\": \"方向\", \"pinyin\": \"fāng xiàng\", \"trans\": \"direction\"}\n]",
        "trans": "This article discusses the importance of reasoning in artificial intelligence, particularly in open, uncertain, and multimodal environments. Large Multimodal Reasoning Models (LMRMs) integrate modalities such as text, images, audio, and video, supporting complex reasoning capabilities. Research has evolved from modular, perception-driven pipelines to unified, language-centric frameworks. The article reviews early efforts with task-specific modules and examines recent methods that unify reasoning into multimodal LLMs, concluding with a discussion on future directions.",
        "update_ts": "2025-05-11 18:30"
    }
}