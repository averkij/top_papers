{
    "date": {
        "ru": "12 Ğ¼Ğ°Ñ",
        "en": "May 12",
        "zh": "5æœˆ12æ—¥"
    },
    "time_utc": "2025-05-12 06:17",
    "weekday": 0,
    "issue_id": 3704,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2505.06111",
            "title": "UniVLA: Learning to Act Anywhere with Task-centric Latent Actions",
            "url": "https://huggingface.co/papers/2505.06111",
            "abstract": "A generalist robot should perform effectively across various environments. However, most existing approaches heavily rely on scaling action-annotated data to enhance their capabilities. Consequently, they are often limited to single physical specification and struggle to learn transferable knowledge across different embodiments and environments. To confront these limitations, we propose UniVLA, a new framework for learning cross-embodiment vision-language-action (VLA) policies. Our key innovation is to derive task-centric action representations from videos with a latent action model. This enables us to exploit extensive data across a wide spectrum of embodiments and perspectives. To mitigate the effect of task-irrelevant dynamics, we incorporate language instructions and establish a latent action model within the DINO feature space. Learned from internet-scale videos, the generalist policy can be deployed to various robots through efficient latent action decoding. We obtain state-of-the-art results across multiple manipulation and navigation benchmarks, as well as real-robot deployments. UniVLA achieves superior performance over OpenVLA with less than 1/20 of pretraining compute and 1/10 of downstream data. Continuous performance improvements are observed as heterogeneous data, even including human videos, are incorporated into the training pipeline. The results underscore UniVLA's potential to facilitate scalable and efficient robot policy learning.",
            "score": 0,
            "issue_id": 3704,
            "pub_date": "2025-05-09",
            "pub_date_card": {
                "ru": "9 Ğ¼Ğ°Ñ",
                "en": "May 9",
                "zh": "5æœˆ9æ—¥"
            },
            "hash": "bf19981dd100b8fb",
            "authors": [
                "Qingwen Bu",
                "Yanting Yang",
                "Jisong Cai",
                "Shenyuan Gao",
                "Guanghui Ren",
                "Maoqing Yao",
                "Ping Luo",
                "Hongyang Li"
            ],
            "affiliations": [
                "AgiBot",
                "OpenDriveLab",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.06111.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#robotics",
                    "#training",
                    "#benchmark",
                    "#agents",
                    "#transfer_learning"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ ÑĞ·Ñ‹Ğº",
                    "desc": "UniVLA - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ° Ñ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ğ¾Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ·Ñ€ĞµĞ½Ğ¸Ñ, ÑĞ·Ñ‹ĞºĞ° Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ². Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¸ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…. UniVLA Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ OpenVLA, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "UniVLA: Empowering Robots with Cross-Embodiment Learning",
                    "desc": "The paper introduces UniVLA, a framework designed to enhance the capabilities of generalist robots by learning cross-embodiment vision-language-action (VLA) policies. It addresses the limitations of existing methods that depend on large amounts of action-annotated data and are restricted to specific physical forms. By utilizing a latent action model derived from videos, UniVLA can leverage diverse data sources and improve knowledge transfer across different robot embodiments and environments. The framework demonstrates state-of-the-art performance in various tasks while requiring significantly less computational resources and data compared to previous approaches."
                },
                "zh": {
                    "title": "UniVLAï¼šæå‡é€šç”¨æœºå™¨äººå­¦ä¹ æ•ˆç‡çš„æ–°æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶UniVLAï¼Œç”¨äºå­¦ä¹ è·¨ä½“ç°çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰ç­–ç•¥ï¼Œä»¥æé«˜é€šç”¨æœºå™¨äººåœ¨ä¸åŒç¯å¢ƒä¸­çš„è¡¨ç°ã€‚æˆ‘ä»¬é€šè¿‡è§†é¢‘ä¸­çš„æ½œåœ¨åŠ¨ä½œæ¨¡å‹æå–ä»¥ä»»åŠ¡ä¸ºä¸­å¿ƒçš„åŠ¨ä½œè¡¨ç¤ºï¼Œä»è€Œåˆ©ç”¨å¹¿æ³›çš„å¤šæ ·åŒ–æ•°æ®ã€‚ä¸ºäº†å‡å°‘ä¸ä»»åŠ¡æ— å…³çš„åŠ¨æ€å½±å“ï¼Œæˆ‘ä»¬ç»“åˆäº†è¯­è¨€æŒ‡ä»¤ï¼Œå¹¶åœ¨DINOç‰¹å¾ç©ºé—´ä¸­å»ºç«‹äº†æ½œåœ¨åŠ¨ä½œæ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUniVLAåœ¨å¤šä¸ªæ“ä½œå’Œå¯¼èˆªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¸”åœ¨é¢„è®­ç»ƒè®¡ç®—å’Œä¸‹æ¸¸æ•°æ®æ–¹é¢çš„éœ€æ±‚æ˜¾è‘—ä½äºç°æœ‰æ–¹æ³•ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-05-09.html",
    "link_next": "2025-05-13.html",
    "link_month": "2025-05.html",
    "short_date_prev": {
        "ru": "09.05",
        "en": "05/09",
        "zh": "5æœˆ9æ—¥"
    },
    "short_date_next": {
        "ru": "13.05",
        "en": "05/13",
        "zh": "5æœˆ13æ—¥"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 1,
        "#agents": 1,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 1,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†æ¨ç†åœ¨äººå·¥æ™ºèƒ½ä¸­çš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¼€æ”¾ã€ä¸ç¡®å®šå’Œå¤šæ¨¡æ€ç¯å¢ƒä¸­ã€‚å¤§å‹å¤šæ¨¡æ€æ¨ç†æ¨¡å‹ï¼ˆLMRMsï¼‰ç»“åˆäº†æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘ç­‰æ¨¡æ€ï¼Œæ”¯æŒå¤æ‚æ¨ç†èƒ½åŠ›ã€‚ç ”ç©¶ä»æ¨¡å—åŒ–ã€æ„ŸçŸ¥é©±åŠ¨çš„æµæ°´çº¿å‘å±•åˆ°ç»Ÿä¸€çš„ã€ä»¥è¯­è¨€ä¸ºä¸­å¿ƒçš„æ¡†æ¶ã€‚æ–‡ç« å›é¡¾äº†æ—©æœŸä»»åŠ¡ç‰¹å®šæ¨¡å—çš„åŠªåŠ›ï¼Œå¹¶æ£€æŸ¥äº†æœ€è¿‘å°†æ¨ç†ç»Ÿä¸€åˆ°å¤šæ¨¡æ€LLMsçš„æ–¹æ³•ï¼Œæœ€åè®¨è®ºäº†æœªæ¥çš„æ–¹å‘ã€‚",
        "title": "Perception, Reason, Think, and Plan: A Survey on Large Multimodal\n  Reasoning Models",
        "pinyin": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†æ¨ç†åœ¨äººå·¥æ™ºèƒ½ä¸­çš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¼€æ”¾ã€ä¸ç¡®å®šå’Œå¤šæ¨¡æ€ç¯å¢ƒä¸­ã€‚å¤§å‹å¤šæ¨¡æ€æ¨ç†æ¨¡å‹ï¼ˆLMRMsï¼‰ç»“åˆäº†æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘ç­‰æ¨¡æ€ï¼Œæ”¯æŒå¤æ‚æ¨ç†èƒ½åŠ›ã€‚ç ”ç©¶ä»æ¨¡å—åŒ–ã€æ„ŸçŸ¥é©±åŠ¨çš„æµæ°´çº¿å‘å±•åˆ°ç»Ÿä¸€çš„ã€ä»¥è¯­è¨€ä¸ºä¸­å¿ƒçš„æ¡†æ¶ã€‚æ–‡ç« å›é¡¾äº†æ—©æœŸä»»åŠ¡ç‰¹å®šæ¨¡å—çš„åŠªåŠ›ï¼Œå¹¶æ£€æŸ¥äº†æœ€è¿‘å°†æ¨ç†ç»Ÿä¸€åˆ°å¤šæ¨¡æ€LLMsçš„æ–¹æ³•ï¼Œæœ€åè®¨è®ºäº†æœªæ¥çš„æ–¹å‘ã€‚\n\nZhÃ¨ piÄn wÃ©nzhÄng tÇolÃ¹n le tuÄ«lÇ zÃ i rÃ©ngÅng zhÃ¬nÃ©ng zhÅng de zhÃ²ngyÃ oxÃ¬ng, tÃ¨biÃ© shÃ¬ zÃ i kÄifÃ ng, bÃ¹ quÃ¨dÃ¬ng hÃ© duÅ mÃ³shÃ¬ huÃ¡njÃ¬ng zhÅng. DÃ xÃ­ng duÅ mÃ³shÃ¬ tuÄ«lÇ mÃ³xÃ­ng (LMRMs) jiÃ©hÃ© le wÃ©nbÄ›n, tÃºxiÃ ng, yÄ«npiÃ n hÃ© shÃ¬pÃ­n dÄ›ng mÃ³shÃ¬, zhÄ«chÃ­ fÃ¹zÃ¡ tuÄ«lÇ nÃ©nglÃ¬. YÃ¡njiÅ« cÃ³ng mÃ³kÃ¹huÃ , gÇnjuÃ© qÅ«dÃ²ng de liÃºshuÇxiÃ n fÄzhÇn dÃ o tÇ’ngyÄ« de, yÇ yÇ”yÃ¡n wÃ©i zhÅngxÄ«n de kuÃ ngjiÃ . WÃ©nzhÄng huÃ­gÃ¹ le zÇoqÄ« rÃ¨nwÃ¹ tÃ¨dÃ¬ng mÃ³kÃ¹ de nÇ”lÃ¬, bÃ¬ng jiÇnchÃ¡ le zuÃ¬jÃ¬n jiÄng tuÄ«lÇ tÇ’ngyÄ« dÃ o duÅ mÃ³shÃ¬ LLMs de fÄngfÇ, zuÃ¬hÃ²u tÇolÃ¹n le wÃ¨ilÃ¡i de fÄngxiÃ ng.",
        "vocab": "[\n    {\"word\": \"æ¨ç†\", \"pinyin\": \"tuÄ« lÇ\", \"trans\": \"reasoning\"},\n    {\"word\": \"äººå·¥æ™ºèƒ½\", \"pinyin\": \"rÃ©n gÅng zhÃ¬ nÃ©ng\", \"trans\": \"artificial intelligence\"},\n    {\"word\": \"ä¸ç¡®å®š\", \"pinyin\": \"bÃ¹ quÃ¨ dÃ¬ng\", \"trans\": \"uncertain\"},\n    {\"word\": \"å¤šæ¨¡æ€\", \"pinyin\": \"duÅ mÃ³ tÃ i\", \"trans\": \"multimodal\"},\n    {\"word\": \"æ¨¡æ€\", \"pinyin\": \"mÃ³ tÃ i\", \"trans\": \"modality\"},\n    {\"word\": \"ç»“åˆ\", \"pinyin\": \"jiÃ© hÃ©\", \"trans\": \"combine\"},\n    {\"word\": \"æ”¯æŒ\", \"pinyin\": \"zhÄ« chÃ­\", \"trans\": \"support\"},\n    {\"word\": \"å¤æ‚\", \"pinyin\": \"fÃ¹ zÃ¡\", \"trans\": \"complex\"},\n    {\"word\": \"æ¨¡å—åŒ–\", \"pinyin\": \"mÃ³ kuÃ i huÃ \", \"trans\": \"modularization\"},\n    {\"word\": \"æ„ŸçŸ¥\", \"pinyin\": \"gÇn zhÄ«\", \"trans\": \"perception\"},\n    {\"word\": \"é©±åŠ¨\", \"pinyin\": \"qÅ« dÃ²ng\", \"trans\": \"drive\"},\n    {\"word\": \"æµæ°´çº¿\", \"pinyin\": \"liÃº shuÇ xiÃ n\", \"trans\": \"pipeline\"},\n    {\"word\": \"ç»Ÿä¸€\", \"pinyin\": \"tÇ’ng yÄ«\", \"trans\": \"unified\"},\n    {\"word\": \"æ¡†æ¶\", \"pinyin\": \"kuÃ ng jiÃ \", \"trans\": \"framework\"},\n    {\"word\": \"å›é¡¾\", \"pinyin\": \"huÃ­ gÃ¹\", \"trans\": \"review\"},\n    {\"word\": \"ä»»åŠ¡\", \"pinyin\": \"rÃ¨n wÃ¹\", \"trans\": \"task\"},\n    {\"word\": \"ç‰¹å®š\", \"pinyin\": \"tÃ¨ dÃ¬ng\", \"trans\": \"specific\"},\n    {\"word\": \"æ£€æŸ¥\", \"pinyin\": \"jiÇn chÃ¡\", \"trans\": \"examine\"},\n    {\"word\": \"æ–¹æ³•\", \"pinyin\": \"fÄng fÇ\", \"trans\": \"method\"},\n    {\"word\": \"æ–¹å‘\", \"pinyin\": \"fÄng xiÃ ng\", \"trans\": \"direction\"}\n]",
        "trans": "This article discusses the importance of reasoning in artificial intelligence, particularly in open, uncertain, and multimodal environments. Large Multimodal Reasoning Models (LMRMs) integrate modalities such as text, images, audio, and video, supporting complex reasoning capabilities. Research has evolved from modular, perception-driven pipelines to unified, language-centric frameworks. The article reviews early efforts with task-specific modules and examines recent methods that unify reasoning into multimodal LLMs, concluding with a discussion on future directions.",
        "update_ts": "2025-05-11 18:30"
    }
}