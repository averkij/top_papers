
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 13 papers. August 22.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">22 августа</span> | <span id="title-articles-count">13 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-08-21.html">⬅️ <span id="prev-date">21.08</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-08-25.html">➡️ <span id="next-date">25.08</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-08.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '22 августа', 'en': 'August 22', 'zh': '8月22日'};
        let feedDateNext = {'ru': '25.08', 'en': '08/25', 'zh': '8月25日'};
        let feedDatePrev = {'ru': '21.08', 'en': '08/21', 'zh': '8月21日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2508.15763', 'title': 'Intern-S1: A Scientific Multimodal Foundation Model', 'url': 'https://huggingface.co/papers/2508.15763', 'abstract': 'Intern-S1, a multimodal Mixture-of-Experts model with extensive pre-training and reinforcement learning, achieves top-tier performance in general reasoning and outperforms closed-source models in scientific tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t In recent years, a plethora of open-source foundation models have emerged, achieving remarkable progress in some widely attended fields, with performance being quite close to that of closed-source models. However, in high-value but more challenging scientific professional fields, either the fields still rely on expert models, or the progress of general foundation models lags significantly compared to those in popular areas, far from sufficient for transforming scientific research and leaving substantial gap between open-source models and closed-source models in these scientific domains. To mitigate this gap and explore a step further toward Artificial General Intelligence (AGI), we introduce Intern-S1, a specialized generalist equipped with general understanding and reasoning capabilities with expertise to analyze multiple science modal data. Intern-S1 is a multimodal Mixture-of-Experts (MoE) model with 28 billion activated parameters and 241 billion total parameters, continually pre-trained on 5T tokens, including over 2.5T tokens from scientific domains. In the post-training stage, Intern-S1 undergoes offline and then online reinforcement learning (RL) in InternBootCamp, where we propose Mixture-of-Rewards (MoR) to synergize the RL training on more than 1000 tasks simultaneously. Through integrated innovations in algorithms, data, and training systems, Intern-S1 achieved top-tier performance in online RL training.On comprehensive evaluation benchmarks, Intern-S1 demonstrates competitive performance on general reasoning tasks among open-source models and significantly outperforms open-source models in scientific domains, surpassing closed-source state-of-the-art models in professional tasks, such as molecular synthesis planning, reaction condition prediction, predicting thermodynamic stabilities for crystals. Our models are available at https://huggingface.co/internlm/Intern-S1.', 'score': 143, 'issue_id': 5485, 'pub_date': '2025-08-21', 'pub_date_card': {'ru': '21 августа', 'en': 'August 21', 'zh': '8月21日'}, 'hash': '55fe9cc77ae55101', 'authors': ['Lei Bai', 'Zhongrui Cai', 'Maosong Cao', 'Weihan Cao', 'Chiyu Chen', 'Haojiong Chen', 'Kai Chen', 'Pengcheng Chen', 'Ying Chen', 'Yongkang Chen', 'Yu Cheng', 'Yu Cheng', 'Pei Chu', 'Tao Chu', 'Erfei Cui', 'Ganqu Cui', 'Long Cui', 'Ziyun Cui', 'Nianchen Deng', 'Ning Ding', 'Nanqin Dong', 'Peijie Dong', 'Shihan Dou', 'Sinan Du', 'Haodong Duan', 'Caihua Fan', 'Ben Gao', 'Changjiang Gao', 'Jianfei Gao', 'Songyang Gao', 'Yang Gao', 'Zhangwei Gao', 'Jiaye Ge', 'Qiming Ge', 'Lixin Gu', 'Yuzhe Gu', 'Aijia Guo', 'Qipeng Guo', 'Xu Guo', 'Conghui He', 'Junjun He', 'Yili Hong', 'Siyuan Hou', 'Caiyu Hu', 'Hanglei Hu', 'Jucheng Hu', 'Ming Hu', 'Zhouqi Hua', 'Haian Huang', 'Junhao Huang', 'Xu Huang', 'Zixian Huang', 'Zhe Jiang', 'Lingkai Kong', 'Linyang Li', 'Peiji Li', 'Pengze Li', 'Shuaibin Li', 'Tianbin Li', 'Wei Li', 'Yuqiang Li', 'Dahua Lin', 'Junyao Lin', 'Tianyi Lin', 'Zhishan Lin', 'Hongwei Liu', 'Jiangning Liu', 'Jiyao Liu', 'Junnan Liu', 'Kai Liu', 'Kaiwen Liu', 'Kuikun Liu', 'Shichun Liu', 'Shudong Liu', 'Wei Liu', 'Xinyao Liu', 'Yuhong Liu', 'Zhan Liu', 'Yinquan Lu', 'Haijun Lv', 'Hongxia Lv', 'Huijie Lv', 'Qidang Lv', 'Ying Lv', 'Chengqi Lyu', 'Chenglong Ma', 'Jianpeng Ma', 'Ren Ma', 'Runmin Ma', 'Runyuan Ma', 'Xinzhu Ma', 'Yichuan Ma', 'Zihan Ma', 'Sixuan Mi', 'Junzhi Ning', 'Wenchang Ning', 'Xinle Pang', 'Jiahui Peng', 'Runyu Peng', 'Yu Qiao', 'Jiantao Qiu', 'Xiaoye Qu', 'Yuan Qu', 'Yuchen Ren', 'Fukai Shang', 'Wenqi Shao', 'Junhao Shen', 'Shuaike Shen', 'Chunfeng Song', 'Demin Song', 'Diping Song', 'Chenlin Su', 'Weijie Su', 'Weigao Sun', 'Yu Sun', 'Qian Tan', 'Cheng Tang', 'Huanze Tang', 'Kexian Tang', 'Shixiang Tang', 'Jian Tong', 'Aoran Wang', 'Bin Wang', 'Dong Wang', 'Lintao Wang', 'Rui Wang', 'Weiyun Wang', 'Wenhai Wang', 'Yi Wang', 'Ziyi Wang', 'Ling-I Wu', 'Wen Wu', 'Yue Wu', 'Zijian Wu', 'Linchen Xiao', 'Shuhao Xing', 'Chao Xu', 'Huihui Xu', 'Jun Xu', 'Ruiliang Xu', 'Wanghan Xu', 'GanLin Yang', 'Yuming Yang', 'Haochen Ye', 'Jin Ye', 'Shenglong Ye', 'Jia Yu', 'Jiashuo Yu', 'Jing Yu', 'Fei Yuan', 'Bo Zhang', 'Chao Zhang', 'Chen Zhang', 'Hongjie Zhang', 'Jin Zhang', 'Qiaosheng Zhang', 'Qiuyinzhe Zhang', 'Songyang Zhang', 'Taolin Zhang', 'Wenlong Zhang', 'Wenwei Zhang', 'Yechen Zhang', 'Ziyang Zhang', 'Haiteng Zhao', 'Qian Zhao', 'Xiangyu Zhao', 'Xiangyu Zhao', 'Bowen Zhou', 'Dongzhan Zhou', 'Peiheng Zhou', 'Yuhao Zhou', 'Yunhua Zhou', 'Dongsheng Zhu', 'Lin Zhu', 'Yicheng Zou'], 'affiliations': ['Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2508.15763.jpg', 'data': {'categories': ['#agi', '#science', '#reasoning', '#dataset', '#open_source', '#rl', '#training', '#multimodal', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Intern-S1: Мультимодальная модель-эксперт для научных задач', 'desc': 'Intern-S1 - это мультимодальная модель типа Mixture-of-Experts с 28 миллиардами активированных параметров. Модель прошла масштабное предобучение на 5 триллионах токенов, включая 2.5 триллиона токенов из научных областей. Intern-S1 использует офлайн и онлайн обучение с подкреплением с применением техники Mixture-of-Rewards для одновременного обучения более чем 1000 задачам. Модель демонстрирует высокую производительность в задачах общего рассуждения и превосходит закрытые модели в научных задачах, таких как планирование молекулярного синтеза и прогнозирование термодинамической стабильности кристаллов.'}, 'en': {'title': 'Bridging the Gap in Scientific AI with Intern-S1', 'desc': 'Intern-S1 is a multimodal Mixture-of-Experts model designed to enhance performance in scientific reasoning tasks. It utilizes extensive pre-training on a vast dataset, including a significant portion from scientific domains, to develop a deep understanding of complex data. The model employs reinforcement learning techniques, specifically Mixture-of-Rewards, to optimize its performance across numerous tasks simultaneously. As a result, Intern-S1 not only excels in general reasoning but also surpasses both open-source and closed-source models in specialized scientific applications.'}, 'zh': {'title': 'Intern-S1：科学领域的智能专家', 'desc': 'Intern-S1是一种多模态专家混合模型，经过广泛的预训练和强化学习，能够在一般推理任务中表现出色，并在科学任务中超越闭源模型。该模型拥有280亿个激活参数和2410亿个总参数，经过5万亿个标记的持续预训练，其中超过2.5万亿个标记来自科学领域。为了缩小开放源模型与闭源模型在科学领域的差距，Intern-S1采用了混合奖励机制，在超过1000个任务上进行强化学习训练。通过算法、数据和训练系统的综合创新，Intern-S1在在线强化学习训练中取得了顶尖的表现。'}}}, {'id': 'https://huggingface.co/papers/2508.15144', 'title': 'Mobile-Agent-v3: Foundamental Agents for GUI Automation', 'url': 'https://huggingface.co/papers/2508.15144', 'abstract': 'GUI-Owl and Mobile-Agent-v3 are open-source GUI agent models and frameworks that achieve state-of-the-art performance across various benchmarks using innovations in environment infrastructure, agent capabilities, and scalable reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper introduces GUI-Owl, a foundational GUI agent model that achieves state-of-the-art performance among open-source end-to-end models on ten GUI benchmarks across desktop and mobile environments, covering grounding, question answering, planning, decision-making, and procedural knowledge. GUI-Owl-7B achieves 66.4 on AndroidWorld and 29.4 on OSWorld. Building on this, we propose Mobile-Agent-v3, a general-purpose GUI agent framework that further improves performance to 73.3 on AndroidWorld and 37.7 on OSWorld, setting a new state-of-the-art for open-source GUI agent frameworks. GUI-Owl incorporates three key innovations: (1) Large-scale Environment Infrastructure: a cloud-based virtual environment spanning Android, Ubuntu, macOS, and Windows, enabling our Self-Evolving GUI Trajectory Production framework. This generates high-quality interaction data via automated query generation and correctness validation, leveraging GUI-Owl to refine trajectories iteratively, forming a self-improving loop. It supports diverse data pipelines and reduces manual annotation. (2) Diverse Foundational Agent Capabilities: by integrating UI grounding, planning, action semantics, and reasoning patterns, GUI-Owl supports end-to-end decision-making and can act as a modular component in multi-agent systems. (3) Scalable Environment RL: we develop a scalable reinforcement learning framework with fully asynchronous training for real-world alignment. We also introduce Trajectory-aware Relative Policy Optimization (TRPO) for online RL, achieving 34.9 on OSWorld. GUI-Owl and Mobile-Agent-v3 are open-sourced at https://github.com/X-PLUG/MobileAgent.', 'score': 31, 'issue_id': 5486, 'pub_date': '2025-08-21', 'pub_date_card': {'ru': '21 августа', 'en': 'August 21', 'zh': '8月21日'}, 'hash': 'f41c368d3d1b16f8', 'authors': ['Jiabo Ye', 'Xi Zhang', 'Haiyang Xu', 'Haowei Liu', 'Junyang Wang', 'Zhaoqing Zhu', 'Ziwei Zheng', 'Feiyu Gao', 'Junjie Cao', 'Zhengxi Lu', 'Jitong Liao', 'Qi Zheng', 'Fei Huang', 'Jingren Zhou', 'Ming Yan'], 'affiliations': ['Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2508.15144.jpg', 'data': {'categories': ['#games', '#open_source', '#agents', '#rl', '#training'], 'emoji': '🤖', 'ru': {'title': 'Открытые модели GUI-агентов достигают прорыва в автоматизации интерфейсов', 'desc': 'Статья представляет GUI-Owl и Mobile-Agent-v3 - открытые модели и фреймворки для взаимодействия с графическим интерфейсом. Эти системы достигают высоких результатов на различных бенчмарках благодаря инновациям в инфраструктуре виртуальной среды, возможностях агентов и масштабируемом обучении с подкреплением. GUI-Owl использует облачную виртуальную среду для генерации высококачественных данных взаимодействия. Mobile-Agent-v3 дополнительно улучшает результаты, устанавливая новый стандарт для открытых фреймворков GUI-агентов.'}, 'en': {'title': 'Revolutionizing GUI Agents with State-of-the-Art Performance', 'desc': "This paper presents GUI-Owl, an advanced GUI agent model that excels in performance on various benchmarks for desktop and mobile environments. It introduces Mobile-Agent-v3, which enhances GUI-Owl's capabilities, achieving new state-of-the-art scores on AndroidWorld and OSWorld. Key innovations include a large-scale cloud-based environment for generating high-quality interaction data, diverse foundational agent capabilities for end-to-end decision-making, and a scalable reinforcement learning framework for real-world applications. Both models are open-source, promoting further research and development in GUI agent technology."}, 'zh': {'title': '开源GUI代理的新时代', 'desc': '本文介绍了GUI-Owl和Mobile-Agent-v3这两个开源GUI代理模型和框架，它们在多个基准测试中表现出色。GUI-Owl是一个基础的GUI代理模型，在桌面和移动环境的十个基准测试中达到了最先进的性能。Mobile-Agent-v3则是一个通用的GUI代理框架，进一步提升了性能，创造了新的开源GUI代理框架的最佳记录。该研究的关键创新包括大规模环境基础设施、多样化的基础代理能力和可扩展的强化学习框架。'}}}, {'id': 'https://huggingface.co/papers/2508.15260', 'title': 'Deep Think with Confidence', 'url': 'https://huggingface.co/papers/2508.15260', 'abstract': 'DeepConf enhances reasoning efficiency and performance by filtering low-quality reasoning traces using model-internal confidence signals, achieving high accuracy and reducing token generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have shown great potential in reasoning tasks through test-time scaling methods like self-consistency with majority voting. However, this approach often leads to diminishing returns in accuracy and high computational overhead. To address these challenges, we introduce Deep Think with Confidence (DeepConf), a simple yet powerful method that enhances both reasoning efficiency and performance at test time. DeepConf leverages model-internal confidence signals to dynamically filter out low-quality reasoning traces during or after generation. It requires no additional model training or hyperparameter tuning and can be seamlessly integrated into existing serving frameworks. We evaluate DeepConf across a variety of reasoning tasks and the latest open-source models, including Qwen 3 and GPT-OSS series. Notably, on challenging benchmarks such as AIME 2025, DeepConf@512 achieves up to 99.9% accuracy and reduces generated tokens by up to 84.7% compared to full parallel thinking.', 'score': 16, 'issue_id': 5485, 'pub_date': '2025-08-21', 'pub_date_card': {'ru': '21 августа', 'en': 'August 21', 'zh': '8月21日'}, 'hash': '2ad4b155cf2ef39e', 'authors': ['Yichao Fu', 'Xuewei Wang', 'Yuandong Tian', 'Jiawei Zhao'], 'affiliations': ['Meta AI', 'UCSD'], 'pdf_title_img': 'assets/pdf/title_img/2508.15260.jpg', 'data': {'categories': ['#reasoning', '#training', '#inference', '#optimization', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Умнее и эффективнее: DeepConf повышает качество рассуждений ИИ', 'desc': 'DeepConf - это метод, улучшающий эффективность и производительность рассуждений в больших языковых моделях (LLM). Он использует внутренние сигналы уверенности модели для фильтрации низкокачественных цепочек рассуждений. DeepConf не требует дополнительного обучения модели и легко интегрируется в существующие фреймворки. В экспериментах на различных задачах рассуждений метод показал высокую точность и значительное сокращение генерируемых токенов.'}, 'en': {'title': 'Boosting Reasoning Efficiency with Confidence Filtering', 'desc': 'DeepConf is a novel method that improves the efficiency and accuracy of reasoning in large language models by using internal confidence signals to filter out low-quality reasoning traces. This approach addresses the limitations of traditional test-time scaling methods, which often result in high computational costs and minimal accuracy gains. By dynamically selecting the most reliable reasoning paths, DeepConf enhances performance without requiring additional training or tuning. Evaluations show that DeepConf significantly boosts accuracy and reduces token generation, making it a valuable tool for various reasoning tasks.'}, 'zh': {'title': '深度置信，提升推理效率与准确性', 'desc': 'DeepConf是一种增强推理效率和性能的方法，通过使用模型内部的置信信号来过滤低质量的推理轨迹。该方法在测试时无需额外的模型训练或超参数调整，可以无缝集成到现有的服务框架中。DeepConf在多种推理任务中表现出色，尤其是在AIME 2025等具有挑战性的基准测试中，达到了99.9%的高准确率，并减少了多达84.7%的生成令牌。通过动态过滤，DeepConf有效提高了推理的准确性和效率。'}}}, {'id': 'https://huggingface.co/papers/2508.15769', 'title': 'SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass', 'url': 'https://huggingface.co/papers/2508.15769', 'abstract': "SceneGen generates multiple 3D assets from a single scene image using a novel framework that integrates local and global scene information, enabling efficient and robust 3D content creation.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D content generation has recently attracted significant research interest due to its applications in VR/AR and embodied AI. In this work, we address the challenging task of synthesizing multiple 3D assets within a single scene image. Concretely, our contributions are fourfold: (i) we present SceneGen, a novel framework that takes a scene image and corresponding object masks as input, simultaneously producing multiple 3D assets with geometry and texture. Notably, SceneGen operates with no need for optimization or asset retrieval; (ii) we introduce a novel feature aggregation module that integrates local and global scene information from visual and geometric encoders within the feature extraction module. Coupled with a position head, this enables the generation of 3D assets and their relative spatial positions in a single feedforward pass; (iii) we demonstrate SceneGen's direct extensibility to multi-image input scenarios. Despite being trained solely on single-image inputs, our architectural design enables improved generation performance with multi-image inputs; and (iv) extensive quantitative and qualitative evaluations confirm the efficiency and robust generation abilities of our approach. We believe this paradigm offers a novel solution for high-quality 3D content generation, potentially advancing its practical applications in downstream tasks. The code and model will be publicly available at: https://mengmouxu.github.io/SceneGen.", 'score': 9, 'issue_id': 5485, 'pub_date': '2025-08-21', 'pub_date_card': {'ru': '21 августа', 'en': 'August 21', 'zh': '8月21日'}, 'hash': 'deff71bdd7ad6d46', 'authors': ['Yanxu Meng', 'Haoning Wu', 'Ya Zhang', 'Weidi Xie'], 'affiliations': ['School of Artificial Intelligence, Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2508.15769.jpg', 'data': {'categories': ['#3d'], 'emoji': '🖼️', 'ru': {'title': 'От 2D к 3D: Революция в генерации трехмерных сцен', 'desc': 'SceneGen - это новая система для генерации множественных 3D-объектов из одного изображения сцены. Она использует инновационный подход, объединяющий локальную и глобальную информацию о сцене для эффективного создания 3D-контента. Система не требует оптимизации или поиска готовых ресурсов и может работать с несколькими входными изображениями. SceneGen демонстрирует высокое качество и надежность в генерации 3D-объектов, что открывает новые возможности для применения в VR/AR и воплощенном ИИ.'}, 'en': {'title': 'Revolutionizing 3D Asset Creation from Single Images', 'desc': 'SceneGen is a new framework designed to create multiple 3D assets from a single scene image by effectively combining local and global scene information. It uses a feature aggregation module that processes visual and geometric data to generate 3D models with their textures and spatial arrangements in one go, without needing optimization or asset retrieval. The framework is also adaptable, showing improved performance when handling multiple images, even though it was initially trained on single images. Overall, SceneGen represents a significant advancement in the field of 3D content generation, with promising applications in virtual and augmented reality.'}, 'zh': {'title': 'SceneGen：高效生成3D资产的新框架', 'desc': 'SceneGen是一个新颖的框架，可以从单一场景图像生成多个3D资产。它通过整合局部和全局场景信息，能够高效且稳健地创建3D内容。该框架无需优化或资产检索，直接生成具有几何形状和纹理的3D资产。我们的评估结果表明，SceneGen在生成性能上表现出色，具有良好的扩展性，适用于多图像输入场景。'}}}, {'id': 'https://huggingface.co/papers/2508.15761', 'title': 'Waver: Wave Your Way to Lifelike Video Generation', 'url': 'https://huggingface.co/papers/2508.15761', 'abstract': 'Waver, a high-performance foundation model, generates high-quality videos and images using a Hybrid Stream DiT architecture, excelling in text-to-video, image-to-video, and text-to-image tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Waver, a high-performance foundation model for unified image and video generation. Waver can directly generate videos with durations ranging from 5 to 10 seconds at a native resolution of 720p, which are subsequently upscaled to 1080p. The model simultaneously supports text-to-video (T2V), image-to-video (I2V), and text-to-image (T2I) generation within a single, integrated framework. We introduce a Hybrid Stream DiT architecture to enhance modality alignment and accelerate training convergence. To ensure training data quality, we establish a comprehensive data curation pipeline and manually annotate and train an MLLM-based video quality model to filter for the highest-quality samples. Furthermore, we provide detailed training and inference recipes to facilitate the generation of high-quality videos. Building on these contributions, Waver excels at capturing complex motion, achieving superior motion amplitude and temporal consistency in video synthesis. Notably, it ranks among the Top 3 on both the T2V and I2V leaderboards at Artificial Analysis (data as of 2025-07-30 10:00 GMT+8), consistently outperforming existing open-source models and matching or surpassing state-of-the-art commercial solutions. We hope this technical report will help the community more efficiently train high-quality video generation models and accelerate progress in video generation technologies. Official page: https://github.com/FoundationVision/Waver.', 'score': 7, 'issue_id': 5488, 'pub_date': '2025-08-21', 'pub_date_card': {'ru': '21 августа', 'en': 'August 21', 'zh': '8月21日'}, 'hash': '9168ada481044deb', 'authors': ['Yifu Zhang', 'Hao Yang', 'Yuqi Zhang', 'Yifei Hu', 'Fengda Zhu', 'Chuang Lin', 'Xiaofeng Mei', 'Yi Jiang', 'Zehuan Yuan', 'Bingyue Peng'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2508.15761.jpg', 'data': {'categories': ['#games', '#training', '#multimodal', '#data', '#open_source', '#video'], 'emoji': '🎬', 'ru': {'title': 'Waver: Единая модель для генерации видео и изображений нового поколения', 'desc': 'Waver - это высокопроизводительная фундаментальная модель для генерации видео и изображений высокого качества. Она использует архитектуру Hybrid Stream DiT для улучшения согласования модальностей и ускорения сходимости при обучении. Модель поддерживает задачи преобразования текста в видео, изображения в видео и текста в изображение в рамках единой интегрированной системы. Waver демонстрирует превосходные результаты в захвате сложных движений, достигая высокой амплитуды движения и временной согласованности при синтезе видео.'}, 'en': {'title': 'Waver: Unifying Image and Video Generation with High Performance', 'desc': 'Waver is a cutting-edge foundation model designed for generating high-quality images and videos using a Hybrid Stream DiT architecture. It can create videos lasting 5 to 10 seconds at a resolution of 720p, which can be upscaled to 1080p. The model integrates text-to-video, image-to-video, and text-to-image generation in one framework, enhancing modality alignment and speeding up training. With a robust data curation pipeline and a specialized video quality model, Waver achieves superior motion capture and consistency, ranking among the top models in its category.'}, 'zh': {'title': 'Waver：高性能的图像与视频生成模型', 'desc': 'Waver是一种高性能的基础模型，专注于统一的图像和视频生成。它能够直接生成时长为5到10秒的720p视频，并可提升至1080p分辨率。该模型支持文本到视频、图像到视频和文本到图像的生成，采用了混合流DiT架构以增强模态对齐和加速训练收敛。Waver在视频合成中表现出色，能够捕捉复杂的运动，具有优越的运动幅度和时间一致性。'}}}, {'id': 'https://huggingface.co/papers/2508.15760', 'title': 'LiveMCP-101: Stress Testing and Diagnosing MCP-enabled Agents on\n  Challenging Queries', 'url': 'https://huggingface.co/papers/2508.15760', 'abstract': "LiveMCP-101 benchmarks AI agents' ability to use multiple tools in real-world scenarios, revealing challenges in tool orchestration and inefficiencies in token usage.  \t\t\t\t\tAI-generated summary \t\t\t\t Tool calling has emerged as a critical capability for AI agents to interact with the real world and solve complex tasks. While the Model Context Protocol (MCP) provides a powerful standardized framework for tool integration, there is a significant gap in benchmarking how well AI agents can effectively solve multi-step tasks using diverse MCP tools in realistic, dynamic scenarios. In this work, we present LiveMCP-101, a benchmark of 101 carefully curated real-world queries, refined through iterative LLM rewriting and manual review, that require coordinated use of multiple MCP tools including web search, file operations, mathematical reasoning, and data analysis. Moreover, we introduce a novel evaluation approach that leverages ground-truth execution plans rather than raw API outputs, better reflecting the evolving nature of real-world environments. Experiments show that even frontier LLMs achieve a success rate below 60\\%, highlighting major challenges in tool orchestration. Detailed ablations and error analysis further reveal distinct failure modes and inefficiencies in token usage, pointing to concrete directions for advancing current models. LiveMCP-101 sets a rigorous standard for evaluating real-world agent capabilities, advancing toward autonomous AI systems that reliably execute complex tasks through tool use.", 'score': 6, 'issue_id': 5490, 'pub_date': '2025-08-21', 'pub_date_card': {'ru': '21 августа', 'en': 'August 21', 'zh': '8月21日'}, 'hash': '04f8a909c0d3944e', 'authors': ['Ming Yin', 'Dinghan Shen', 'Silei Xu', 'Jianbing Han', 'Sixun Dong', 'Mian Zhang', 'Yebowen Hu', 'Shujian Liu', 'Simin Ma', 'Song Wang', 'Sathish Reddy Indurthi', 'Xun Wang', 'Yiran Chen', 'Kaiqiang Song'], 'affiliations': ['Duke University', 'Zoom Video Communications'], 'pdf_title_img': 'assets/pdf/title_img/2508.15760.jpg', 'data': {'categories': ['#agents', '#optimization', '#benchmark', '#reasoning', '#agi'], 'emoji': '🤖', 'ru': {'title': 'LiveMCP-101: Испытание ИИ-агентов в реальных условиях', 'desc': 'LiveMCP-101 - это новый бенчмарк для оценки способности ИИ-агентов использовать несколько инструментов в реалистичных сценариях. Он включает 101 запрос, требующий координированного использования различных MCP-инструментов. Эксперименты показали, что даже передовые языковые модели достигают уровня успешности ниже 60%, выявляя серьезные проблемы в оркестровке инструментов. Бенчмарк устанавливает строгий стандарт для оценки возможностей агентов в реальном мире, продвигаясь к автономным системам ИИ.'}, 'en': {'title': 'Benchmarking AI Agents: Mastering Tool Use in Real-World Tasks', 'desc': 'LiveMCP-101 is a benchmark designed to evaluate how well AI agents can use multiple tools to solve complex, real-world tasks. It highlights the challenges of tool orchestration and inefficiencies in token usage when agents attempt to perform multi-step operations. The benchmark consists of 101 real-world queries that require the coordinated use of various tools, such as web search and data analysis, and introduces a new evaluation method based on ground-truth execution plans. Results show that even advanced language models struggle with these tasks, achieving less than 60% success, indicating significant areas for improvement in AI tool integration.'}, 'zh': {'title': '评估AI代理工具使用能力的新基准', 'desc': 'LiveMCP-101是一个基准测试，旨在评估人工智能代理在现实场景中使用多种工具的能力。该研究揭示了工具协调的挑战以及令牌使用的低效性。通过引入基于真实执行计划的评估方法，研究更准确地反映了现实环境的动态特性。实验结果显示，即使是最先进的语言模型，其成功率也低于60%，这表明在多步骤任务中仍存在显著的困难。'}}}, {'id': 'https://huggingface.co/papers/2508.15767', 'title': 'ATLAS: Decoupling Skeletal and Shape Parameters for Expressive\n  Parametric Human Modeling', 'url': 'https://huggingface.co/papers/2508.15767', 'abstract': 'ATLAS is a high-fidelity body model that decouples shape and skeleton bases, improving pose accuracy and shape expressivity using 600k high-resolution scans.  \t\t\t\t\tAI-generated summary \t\t\t\t Parametric body models offer expressive 3D representation of humans across a wide range of poses, shapes, and facial expressions, typically derived by learning a basis over registered 3D meshes. However, existing human mesh modeling approaches struggle to capture detailed variations across diverse body poses and shapes, largely due to limited training data diversity and restrictive modeling assumptions. Moreover, the common paradigm first optimizes the external body surface using a linear basis, then regresses internal skeletal joints from surface vertices. This approach introduces problematic dependencies between internal skeleton and outer soft tissue, limiting direct control over body height and bone lengths. To address these issues, we present ATLAS, a high-fidelity body model learned from 600k high-resolution scans captured using 240 synchronized cameras. Unlike previous methods, we explicitly decouple the shape and skeleton bases by grounding our mesh representation in the human skeleton. This decoupling enables enhanced shape expressivity, fine-grained customization of body attributes, and keypoint fitting independent of external soft-tissue characteristics. ATLAS outperforms existing methods by fitting unseen subjects in diverse poses more accurately, and quantitative evaluations show that our non-linear pose correctives more effectively capture complex poses compared to linear models.', 'score': 5, 'issue_id': 5488, 'pub_date': '2025-08-21', 'pub_date_card': {'ru': '21 августа', 'en': 'August 21', 'zh': '8月21日'}, 'hash': '0adb5eab0443b587', 'authors': ['Jinhyung Park', 'Javier Romero', 'Shunsuke Saito', 'Fabian Prada', 'Takaaki Shiratori', 'Yichen Xu', 'Federica Bogo', 'Shoou-I Yu', 'Kris Kitani', 'Rawal Khirodkar'], 'affiliations': ['Carnegie Mellon University', 'Meta'], 'pdf_title_img': 'assets/pdf/title_img/2508.15767.jpg', 'data': {'categories': ['#3d'], 'emoji': '🦾', 'ru': {'title': 'Революция в 3D-моделировании тела: разделение формы и скелета для улучшенной точности', 'desc': 'ATLAS - это высокоточная модель тела человека, которая разделяет основы формы и скелета, улучшая точность позы и выразительность формы с использованием 600 тысяч высокоразрешающих сканов. Модель использует нелинейные корректировки позы для более эффективного захвата сложных поз по сравнению с линейными моделями. ATLAS превосходит существующие методы в точности подгонки к невидимым субъектам в различных позах. Модель позволяет улучшить выразительность формы и точную настройку атрибутов тела, а также подгонку ключевых точек независимо от характеристик внешних мягких тканей.'}, 'en': {'title': 'Decoupling Shape and Skeleton for Enhanced 3D Body Modeling', 'desc': 'ATLAS is a new body model that improves how we represent human shapes and poses in 3D. It uses a large dataset of 600,000 high-resolution scans to learn better representations by separating the shape of the body from its skeleton. This separation allows for more detailed and customizable body attributes, making it easier to fit different poses accurately. Overall, ATLAS provides a more flexible and precise way to model human figures compared to previous methods.'}, 'zh': {'title': 'ATLAS：解耦形状与骨架的高保真身体模型', 'desc': 'ATLAS是一种高保真身体模型，通过解耦形状和骨架基础，提高了姿势准确性和形状表现力。该模型基于60万张高分辨率扫描图像，使用240台同步摄像机捕获数据。与传统方法不同，ATLAS明确地将形状和骨架基础分开，使得身体属性的定制更加精细，并且关键点拟合不再依赖于外部软组织特征。实验结果表明，ATLAS在多样化姿势下对未见对象的拟合精度优于现有方法，且其非线性姿势修正能够更有效地捕捉复杂姿势。'}}}, {'id': 'https://huggingface.co/papers/2508.15752', 'title': '"Does the cafe entrance look accessible? Where is the door?" Towards\n  Geospatial AI Agents for Visual Inquiries', 'url': 'https://huggingface.co/papers/2508.15752', 'abstract': 'Geo-Visual Agents integrate geospatial images and GIS data to answer nuanced visual-spatial inquiries about the world.  \t\t\t\t\tAI-generated summary \t\t\t\t Interactive digital maps have revolutionized how people travel and learn about the world; however, they rely on pre-existing structured data in GIS databases (e.g., road networks, POI indices), limiting their ability to address geo-visual questions related to what the world looks like. We introduce our vision for Geo-Visual Agents--multimodal AI agents capable of understanding and responding to nuanced visual-spatial inquiries about the world by analyzing large-scale repositories of geospatial images, including streetscapes (e.g., Google Street View), place-based photos (e.g., TripAdvisor, Yelp), and aerial imagery (e.g., satellite photos) combined with traditional GIS data sources. We define our vision, describe sensing and interaction approaches, provide three exemplars, and enumerate key challenges and opportunities for future work.', 'score': 3, 'issue_id': 5488, 'pub_date': '2025-08-21', 'pub_date_card': {'ru': '21 августа', 'en': 'August 21', 'zh': '8月21日'}, 'hash': '9c9ca9863fa564a1', 'authors': ['Jon E. Froehlich', 'Jared Hwang', 'Zeyu Wang', "John S. O'Meara", 'Xia Su', 'William Huang', 'Yang Zhang', 'Alex Fiannaca', 'Philip Nelson', 'Shaun Kane'], 'affiliations': ['Google DeepMind', 'Google Research', 'UCLA', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2508.15752.jpg', 'data': {'categories': ['#multimodal', '#agents', '#cv'], 'emoji': '🌍', 'ru': {'title': 'Гео-Визуальные Агенты: Новый взгляд на анализ геопространственных данных', 'desc': 'Статья представляет концепцию Гео-Визуальных Агентов - мультимодальных ИИ-агентов, способных анализировать и отвечать на сложные визуально-пространственные запросы о мире. Эти агенты используют масштабные репозитории геопространственных изображений, включая панорамы улиц, фотографии мест и спутниковые снимки, в сочетании с традиционными ГИС-данными. Авторы описывают подходы к сенсорному восприятию и взаимодействию, приводят примеры применения и обсуждают ключевые проблемы и возможности для будущих исследований. Гео-Визуальные Агенты призваны преодолеть ограничения существующих интерактивных карт, которые опираются только на структурированные данные ГИС.'}, 'en': {'title': 'Revolutionizing Geo-Visual Inquiry with AI Agents', 'desc': 'Geo-Visual Agents are advanced AI systems designed to interpret and respond to complex questions about the world using both geospatial images and GIS data. Unlike traditional digital maps that depend solely on structured GIS databases, these agents leverage a variety of visual data sources, such as streetscapes and aerial imagery, to provide richer insights. The paper outlines the methods for how these agents can sense and interact with their environment, showcasing three practical examples of their application. It also discusses the challenges and future opportunities in developing these multimodal AI agents for enhanced geo-visual understanding.'}, 'zh': {'title': 'Geo-Visual Agents：重新定义地理空间理解', 'desc': 'Geo-Visual Agents 是一种结合地理空间图像和地理信息系统（GIS）数据的多模态人工智能代理，旨在回答关于世界的复杂视觉空间问题。这些代理能够分析大规模的地理空间图像库，包括街景、地点照片和航空影像，并与传统的GIS数据源相结合。通过这种方式，Geo-Visual Agents 可以超越传统数字地图的局限，提供更丰富的地理信息和视觉体验。本文还讨论了感知和交互方法，并列举了未来工作的关键挑战和机遇。'}}}, {'id': 'https://huggingface.co/papers/2508.15361', 'title': 'A Survey on Large Language Model Benchmarks', 'url': 'https://huggingface.co/papers/2508.15361', 'abstract': "A systematic review of large language model benchmarks identifies issues such as data contamination, cultural biases, and lack of process credibility, and proposes a design paradigm for future improvements.  \t\t\t\t\tAI-generated summary \t\t\t\t In recent years, with the rapid development of the depth and breadth of large language models' capabilities, various corresponding evaluation benchmarks have been emerging in increasing numbers. As a quantitative assessment tool for model performance, benchmarks are not only a core means to measure model capabilities but also a key element in guiding the direction of model development and promoting technological innovation. We systematically review the current status and development of large language model benchmarks for the first time, categorizing 283 representative benchmarks into three categories: general capabilities, domain-specific, and target-specific. General capability benchmarks cover aspects such as core linguistics, knowledge, and reasoning; domain-specific benchmarks focus on fields like natural sciences, humanities and social sciences, and engineering technology; target-specific benchmarks pay attention to risks, reliability, agents, etc. We point out that current benchmarks have problems such as inflated scores caused by data contamination, unfair evaluation due to cultural and linguistic biases, and lack of evaluation on process credibility and dynamic environments, and provide a referable design paradigm for future benchmark innovation.", 'score': 3, 'issue_id': 5488, 'pub_date': '2025-08-21', 'pub_date_card': {'ru': '21 августа', 'en': 'August 21', 'zh': '8月21日'}, 'hash': 'd2096a14a9fdc644', 'authors': ['Shiwen Ni', 'Guhong Chen', 'Shuaimin Li', 'Xuanang Chen', 'Siyi Li', 'Bingli Wang', 'Qiyao Wang', 'Xingjian Wang', 'Yifan Zhang', 'Liyang Fan', 'Chengming Li', 'Ruifeng Xu', 'Le Sun', 'Min Yang'], 'affiliations': ['Harbin Institute of Technology, Shenzhen', 'Institute of Software, Chinese Academy of Sciences', 'Shanghai AI Lab', 'Shanghai University of Electric Power', 'Shenzhen Key Laboratory for High Performance Data Mining, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences', 'Shenzhen MSU-BIT University', 'Shenzhen University', 'Shenzhen University of Advanced Technology', 'South China University of Technology', 'Southern University of Science and Technology', 'University of Chinese Academy of Sciences', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2508.15361.jpg', 'data': {'categories': ['#ethics', '#benchmark', '#survey'], 'emoji': '🎯', 'ru': {'title': 'Новый взгляд на оценку языковых моделей: от проблем к инновациям', 'desc': 'В статье проводится систематический обзор бенчмарков для оценки больших языковых моделей (LLM). Авторы выявляют проблемы существующих бенчмарков, такие как загрязнение данных, культурные предубеждения и недостаток достоверности процесса оценки. Предлагается новая парадигма дизайна для улучшения бенчмарков в будущем. Исследователи классифицируют 283 бенчмарка по трем категориям: общие способности, специфичные для предметных областей и целевые.'}, 'en': {'title': 'Enhancing Language Model Benchmarks for Fair and Credible Evaluation', 'desc': 'This paper reviews the current benchmarks used to evaluate large language models, highlighting their importance in measuring model performance and guiding development. It categorizes 283 benchmarks into three types: general capabilities, domain-specific, and target-specific, each focusing on different aspects of model evaluation. The authors identify significant issues such as data contamination leading to inflated scores, cultural biases affecting fairness, and a lack of credibility in evaluation processes. To address these challenges, the paper proposes a new design paradigm aimed at improving future benchmarks for more reliable assessments.'}, 'zh': {'title': '提升大型语言模型基准的设计与可信度', 'desc': '这篇论文系统性地回顾了大型语言模型基准的现状，识别出数据污染、文化偏见和过程可信度缺乏等问题。作者将283个代表性基准分为三类：通用能力、领域特定和目标特定。通用能力基准涵盖核心语言学、知识和推理等方面，而领域特定基准则关注自然科学、人文学科和工程技术等领域。论文还提出了未来基准创新的设计范式，以解决当前基准存在的问题。'}}}, {'id': 'https://huggingface.co/papers/2508.15126', 'title': 'aiXiv: A Next-Generation Open Access Ecosystem for Scientific Discovery\n  Generated by AI Scientists', 'url': 'https://huggingface.co/papers/2508.15126', 'abstract': 'aiXiv is an open-access platform that facilitates the submission, review, and refinement of scientific proposals and papers by both human and AI scientists, enhancing the quality and dissemination of AI-generated research.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models (LLMs) have enabled AI agents to autonomously generate scientific proposals, conduct experiments, author papers, and perform peer reviews. Yet this flood of AI-generated research content collides with a fragmented and largely closed publication ecosystem. Traditional journals and conferences rely on human peer review, making them difficult to scale and often reluctant to accept AI-generated research content; existing preprint servers (e.g. arXiv) lack rigorous quality-control mechanisms. Consequently, a significant amount of high-quality AI-generated research lacks appropriate venues for dissemination, hindering its potential to advance scientific progress. To address these challenges, we introduce aiXiv, a next-generation open-access platform for human and AI scientists. Its multi-agent architecture allows research proposals and papers to be submitted, reviewed, and iteratively refined by both human and AI scientists. It also provides API and MCP interfaces that enable seamless integration of heterogeneous human and AI scientists, creating a scalable and extensible ecosystem for autonomous scientific discovery. Through extensive experiments, we demonstrate that aiXiv is a reliable and robust platform that significantly enhances the quality of AI-generated research proposals and papers after iterative revising and reviewing on aiXiv. Our work lays the groundwork for a next-generation open-access ecosystem for AI scientists, accelerating the publication and dissemination of high-quality AI-generated research content. Code is available at https://github.com/aixiv-org. Website is available at https://forms.gle/DxQgCtXFsJ4paMtn8.', 'score': 3, 'issue_id': 5490, 'pub_date': '2025-08-20', 'pub_date_card': {'ru': '20 августа', 'en': 'August 20', 'zh': '8月20日'}, 'hash': 'db938abd67d28b70', 'authors': ['Pengsong Zhang', 'Xiang Hu', 'Guowei Huang', 'Yang Qi', 'Heng Zhang', 'Xiuxu Li', 'Jiaxing Song', 'Jiabin Luo', 'Yijiang Li', 'Shuo Yin', 'Chengxiao Dai', 'Eric Hanchen Jiang', 'Xiaoyan Zhou', 'Zhenfei Yin', 'Boqin Yuan', 'Jing Dong', 'Guinan Su', 'Guanren Qiao', 'Haiming Tang', 'Anghong Du', 'Lili Pan', 'Zhenzhong Lan', 'Xinyu Liu'], 'affiliations': ['Columbia University', 'Istituto Italiano di Tecnologia', 'Max Planck Institute for Intelligent Systems', 'National University of Singapore', 'Peking University', 'The Chinese University of Hong Kong', 'Tsinghua University', 'University of Birmingham', 'University of California, Los Angeles', 'University of California, San Diego', 'University of Electronic Science and Technology of China', 'University of Manchester', 'University of Oxford', 'University of Sydney', 'University of Toronto', 'University of Utah', 'Università degli Studi di Genova', 'Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2508.15126.jpg', 'data': {'categories': ['#multimodal', '#agents', '#science', '#architecture', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'aiXiv: открытая платформа для совместных исследований человека и ИИ', 'desc': 'В статье представлена платформа aiXiv, которая обеспечивает подачу, рецензирование и доработку научных работ как людьми, так и ИИ-учеными. Платформа использует мультиагентную архитектуру для итеративного улучшения качества ИИ-генерируемых исследований. aiXiv предоставляет API и MCP интерфейсы для интеграции разнородных человеческих и ИИ-ученых. Эксперименты показывают, что платформа значительно повышает качество ИИ-генерируемых научных работ после итеративного пересмотра и рецензирования.'}, 'en': {'title': 'aiXiv: Bridging Human and AI Research for Scientific Progress', 'desc': 'The paper introduces aiXiv, an innovative open-access platform designed for both human and AI scientists to collaboratively submit, review, and refine scientific research. It addresses the challenges posed by traditional publication systems that struggle to accommodate the growing volume of AI-generated content. By utilizing a multi-agent architecture, aiXiv facilitates seamless integration and interaction between human and AI researchers, enhancing the quality and dissemination of research proposals and papers. The platform demonstrates significant improvements in the reliability and robustness of AI-generated research through iterative revisions and peer reviews, paving the way for a more inclusive scientific ecosystem.'}, 'zh': {'title': 'aiXiv：推动AI生成研究的开放获取平台', 'desc': 'aiXiv是一个新一代的开放获取平台，旨在促进人类和人工智能科学家的科学提案和论文的提交、审查和改进。该平台采用多代理架构，使得研究提案和论文可以由人类和AI科学家共同提交和迭代审查，从而提高AI生成研究的质量。传统的期刊和会议依赖人类同行评审，难以适应AI生成内容的快速增长，而aiXiv则提供了一个可扩展的生态系统，支持自主科学发现。通过大量实验，我们证明aiXiv是一个可靠的平台，显著提升了AI生成研究提案和论文的质量。'}}}, {'id': 'https://huggingface.co/papers/2508.15202', 'title': 'Fin-PRM: A Domain-Specialized Process Reward Model for Financial\n  Reasoning in Large Language Models', 'url': 'https://huggingface.co/papers/2508.15202', 'abstract': 'Fin-PRM, a domain-specialized reward model for finance, enhances intermediate reasoning in LLMs through step-level and trajectory-level supervision, improving performance in supervised learning, reinforcement learning, and test-time inference.  \t\t\t\t\tAI-generated summary \t\t\t\t Process Reward Models (PRMs) have emerged as a promising framework for supervising intermediate reasoning in large language models (LLMs), yet existing PRMs are primarily trained on general or Science, Technology, Engineering, and Mathematics (STEM) domains and fall short in domain-specific contexts such as finance, where reasoning is more structured, symbolic, and sensitive to factual and regulatory correctness. We introduce Fin-PRM, a domain-specialized, trajectory-aware PRM tailored to evaluate intermediate reasoning steps in financial tasks. Fin-PRM integrates step-level and trajectory-level reward supervision, enabling fine-grained evaluation of reasoning traces aligned with financial logic. We apply Fin-PRM in both offline and online reward learning settings, supporting three key applications: (i) selecting high-quality reasoning trajectories for distillation-based supervised fine-tuning, (ii) providing dense process-level rewards for reinforcement learning, and (iii) guiding reward-informed Best-of-N inference at test time. Experimental results on financial reasoning benchmarks, including CFLUE and FinQA, demonstrate that Fin-PRM consistently outperforms general-purpose PRMs and strong domain baselines in trajectory selection quality. Downstream models trained with Fin-PRM yield substantial improvements with baselines, with gains of 12.9\\% in supervised learning, 5.2\\% in reinforcement learning, and 5.1\\% in test-time performance. These findings highlight the value of domain-specialized reward modeling for aligning LLMs with expert-level financial reasoning. Our project resources will be available at https://github.com/aliyun/qwen-dianjin.', 'score': 2, 'issue_id': 5485, 'pub_date': '2025-08-21', 'pub_date_card': {'ru': '21 августа', 'en': 'August 21', 'zh': '8月21日'}, 'hash': '05b2af7facdfcd18', 'authors': ['Yuanchen Zhou', 'Shuo Jiang', 'Jie Zhu', 'Junhui Li', 'Lifan Guo', 'Feng Chen', 'Chi Zhang'], 'affiliations': ['Osaka University', 'Qwen DianJin Team, Alibaba Cloud Computing', 'Soochow University'], 'pdf_title_img': 'assets/pdf/title_img/2508.15202.jpg', 'data': {'categories': ['#science', '#reasoning', '#rlhf', '#healthcare', '#training', '#optimization'], 'emoji': '💹', 'ru': {'title': 'Fin-PRM: Улучшение финансовых рассуждений в LLM с помощью специализированной модели вознаграждения', 'desc': 'Fin-PRM - это специализированная модель вознаграждения для финансовой сферы, улучшающая промежуточные рассуждения в больших языковых моделях (LLM). Она использует пошаговый и траекторный уровни обучения, что повышает эффективность в задачах обучения с учителем, обучения с подкреплением и вывода во время тестирования. Fin-PRM превосходит общие модели вознаграждения и базовые модели в финансовой области на таких бенчмарках, как CFLUE и FinQA. Применение Fin-PRM приводит к значительному улучшению производительности моделей в различных сценариях обучения и вывода.'}, 'en': {'title': 'Fin-PRM: Elevating Financial Reasoning in LLMs', 'desc': 'Fin-PRM is a specialized reward model designed for the finance domain, enhancing the reasoning capabilities of large language models (LLMs). It employs both step-level and trajectory-level supervision to evaluate and improve the reasoning process in financial tasks, which require precise and structured logic. By integrating this tailored approach, Fin-PRM significantly boosts performance in supervised learning, reinforcement learning, and inference tasks compared to general-purpose models. Experimental results show that models trained with Fin-PRM achieve notable improvements in financial reasoning benchmarks, demonstrating the effectiveness of domain-specific reward modeling.'}, 'zh': {'title': '金融领域的专用奖励模型提升推理能力', 'desc': 'Fin-PRM是一种专门针对金融领域的奖励模型，旨在增强大型语言模型（LLMs）在中间推理过程中的表现。它通过逐步和轨迹级别的监督，提供对金融任务中推理步骤的细致评估。研究表明，Fin-PRM在金融推理基准测试中表现优于通用奖励模型，显著提高了监督学习和强化学习的效果。该模型的应用展示了领域专用奖励建模在提升LLMs与金融专家推理一致性方面的重要性。'}}}, {'id': 'https://huggingface.co/papers/2508.14892', 'title': 'Snap-Snap: Taking Two Images to Reconstruct 3D Human Gaussians in\n  Milliseconds', 'url': 'https://huggingface.co/papers/2508.14892', 'abstract': 'A method reconstructs 3D human bodies from two sparse views using a redesigned geometry model and enhancement algorithm, achieving state-of-the-art performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Reconstructing 3D human bodies from sparse views has been an appealing topic, which is crucial to broader the related applications. In this paper, we propose a quite challenging but valuable task to reconstruct the human body from only two images, i.e., the front and back view, which can largely lower the barrier for users to create their own 3D digital humans. The main challenges lie in the difficulty of building 3D consistency and recovering missing information from the highly sparse input. We redesign a geometry reconstruction model based on foundation reconstruction models to predict consistent point clouds even input images have scarce overlaps with extensive human data training. Furthermore, an enhancement algorithm is applied to supplement the missing color information, and then the complete human point clouds with colors can be obtained, which are directly transformed into 3D Gaussians for better rendering quality. Experiments show that our method can reconstruct the entire human in 190 ms on a single NVIDIA RTX 4090, with two images at a resolution of 1024x1024, demonstrating state-of-the-art performance on the THuman2.0 and cross-domain datasets. Additionally, our method can complete human reconstruction even with images captured by low-cost mobile devices, reducing the requirements for data collection. Demos and code are available at https://hustvl.github.io/Snap-Snap/.', 'score': 2, 'issue_id': 5491, 'pub_date': '2025-08-20', 'pub_date_card': {'ru': '20 августа', 'en': 'August 20', 'zh': '8月20日'}, 'hash': '4eda0215bbe80cbe', 'authors': ['Jia Lu', 'Taoran Yi', 'Jiemin Fang', 'Chen Yang', 'Chuiyun Wu', 'Wei Shen', 'Wenyu Liu', 'Qi Tian', 'Xinggang Wang'], 'affiliations': ['Huawei Inc.', 'Huazhong University of Science and Technology', 'Shanghai Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2508.14892.jpg', 'data': {'categories': ['#3d'], 'emoji': '👤', 'ru': {'title': 'Реконструкция 3D-модели человека по двум фото за доли секунды', 'desc': 'Статья представляет метод реконструкции трехмерных моделей человеческого тела по двум изображениям - фронтальному и заднему виду. Авторы redesigned geometry reconstruction model для предсказания согласованных облаков точек даже при минимальном перекрытии входных изображений. Применяется алгоритм улучшения для восполнения недостающей цветовой информации. Эксперименты показывают state-of-the-art производительность на датасетах THuman2.0 и cross-domain, а также возможность работы с изображениями с мобильных устройств.'}, 'en': {'title': 'Revolutionizing 3D Human Reconstruction from Sparse Views', 'desc': 'This paper presents a novel method for reconstructing 3D human bodies from just two sparse images, specifically the front and back views. The approach addresses the challenges of maintaining 3D consistency and filling in missing data from limited input. By redesigning a geometry reconstruction model and applying an enhancement algorithm, the method generates complete and colored point clouds that can be rendered into high-quality 3D representations. The results demonstrate impressive performance, achieving full human reconstruction in 190 ms on advanced hardware, even with images from low-cost devices.'}, 'zh': {'title': '从稀疏视图重建三维人体的新突破', 'desc': '本论文提出了一种从两幅稀疏视图重建三维人体的新方法，使用了重新设计的几何模型和增强算法，达到了最先进的性能。该方法仅需前后视图的两张图片，降低了用户创建三维数字人类的门槛。主要挑战在于如何建立三维一致性和从稀疏输入中恢复缺失信息。通过基于基础重建模型的几何重建模型和增强算法，我们能够生成完整的带颜色的人体点云，并实现高质量的渲染。'}}}, {'id': 'https://huggingface.co/papers/2508.15641', 'title': 'When and What: Diffusion-Grounded VideoLLM with Entity Aware\n  Segmentation for Long Video Understanding', 'url': 'https://huggingface.co/papers/2508.15641', 'abstract': 'Grounded VideoDiT addresses temporal perception and entity interaction in videos through a Diffusion Temporal Latent encoder, object grounded representations, and mixed token scheme, achieving state-of-the-art results on VideoQA benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Understanding videos requires more than answering open ended questions, it demands the ability to pinpoint when events occur and how entities interact across time. While recent Video LLMs have achieved remarkable progress in holistic reasoning, they remain coarse in temporal perception: timestamps are encoded only implicitly, frame level features are weak in capturing continuity, and language vision alignment often drifts from the entities of interest. In this paper, we present Grounded VideoDiT, a Video LLM designed to overcome these limitations by introducing three key innovations. First, a Diffusion Temporal Latent (DTL) encoder enhances boundary sensitivity and maintains temporal consistency. Second, object grounded representations explicitly bind query entities to localized visual evidence, strengthening alignment. Third, a mixed token scheme with discrete temporal tokens provides explicit timestamp modeling, enabling fine grained temporal reasoning. Together, these designs equip Grounded VideoDiT with robust grounding capabilities, as validated by state of the art results on Charades STA, NExT GQA, and multiple VideoQA benchmarks.', 'score': 1, 'issue_id': 5488, 'pub_date': '2025-08-21', 'pub_date_card': {'ru': '21 августа', 'en': 'August 21', 'zh': '8月21日'}, 'hash': '7ce8d878ebdf333f', 'authors': ['Pengcheng Fang', 'Yuxia Chen', 'Rui Guo'], 'affiliations': ['Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China', 'Tencent AI Lab, Shenzhen, China', 'University of Chinese Academy of Sciences, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2508.15641.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#diffusion', '#benchmark', '#video'], 'emoji': '🎬', 'ru': {'title': 'Точное восприятие времени и взаимодействия объектов в видео с помощью ИИ', 'desc': 'Grounded VideoDiT - это новая модель машинного обучения для анализа видео. Она использует энкодер Diffusion Temporal Latent для улучшения восприятия временных границ и сохранения временной согласованности. Модель применяет объектно-привязанные представления для усиления связи между запросами и визуальными данными. Кроме того, Grounded VideoDiT использует смешанную схему токенов с дискретными временными токенами для точного моделирования временных меток.'}, 'en': {'title': 'Enhancing Video Understanding with Grounded Temporal Reasoning', 'desc': "Grounded VideoDiT is a novel Video Language Model (VLM) that improves how we understand videos by focusing on when events happen and how different entities interact over time. It introduces a Diffusion Temporal Latent (DTL) encoder that enhances the model's ability to recognize event boundaries and maintain consistency in time. Additionally, it uses object grounded representations to connect specific entities to their visual context, improving alignment between language and vision. Finally, a mixed token scheme incorporates discrete temporal tokens for precise timestamp modeling, allowing for detailed temporal reasoning, which has led to state-of-the-art performance on various VideoQA benchmarks."}, 'zh': {'title': '提升视频理解的时间感知与实体交互能力', 'desc': '本论文介绍了Grounded VideoDiT，这是一种针对视频理解的模型，旨在改善时间感知和实体交互的能力。通过引入扩散时间潜在编码器、对象绑定表示和混合标记方案，该模型在视频问答基准测试中取得了最先进的结果。扩散时间潜在编码器增强了边界敏感性并保持时间一致性，而对象绑定表示则明确将查询实体与局部视觉证据相结合。混合标记方案提供了明确的时间戳建模，使得模型能够进行细粒度的时间推理。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (3)', '#agents (4)', '#agi (2)', '#alignment', '#architecture (1)', '#audio', '#benchmark (5)', '#cv (1)', '#data (1)', '#dataset (1)', '#diffusion (1)', '#ethics (1)', '#games (2)', '#graphs', '#hallucinations', '#healthcare (1)', '#inference (1)', '#interpretability', '#leakage', '#long_context', '#low_resource', '#machine_translation', '#math', '#multilingual', '#multimodal (5)', '#open_source (4)', '#optimization (3)', '#plp', '#rag', '#reasoning (5)', '#rl (2)', '#rlhf (1)', '#robotics', '#science (3)', '#security', '#small_models', '#story_generation', '#survey (1)', '#synthetic', '#training (5)', '#transfer_learning', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-08-22 10:12',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-08-22 10:12')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-08-22 10:12')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    