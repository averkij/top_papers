{
    "date": {
        "ru": "6 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
        "en": "March 6",
        "zh": "3æœˆ6æ—¥"
    },
    "time_utc": "2025-03-06 07:10",
    "weekday": 3,
    "issue_id": 2559,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2503.00865",
            "title": "Babel: Open Multilingual Large Language Models Serving Over 90% of Global Speakers",
            "url": "https://huggingface.co/papers/2503.00865",
            "abstract": "Large language models (LLMs) have revolutionized natural language processing (NLP), yet open-source multilingual LLMs remain scarce, with existing models often limited in language coverage. Such models typically prioritize well-resourced languages, while widely spoken but under-resourced languages are often overlooked. To address this disparity, we introduce Babel, an open multilingual LLM that covers the top 25 languages by number of speakers, supports over 90% of the global population, and includes many languages neglected by other open multilingual LLMs. Unlike traditional continue pretraining approaches, Babel expands its parameter count through a layer extension technique that elevates Babel's performance ceiling. We introduce two variants: Babel-9B, designed for efficient inference and fine-tuning, and Babel-83B, which sets a new standard for open multilingual LLMs. Extensive evaluations on multilingual tasks demonstrate its superior performance compared to open LLMs of comparable size. In addition, using open-source supervised fine-tuning datasets, Babel achieves remarkable performance, with Babel-9B-Chat leading among 10B-sized LLMs and Babel-83B-Chat setting a new standard for multilingual tasks, reaching the same level of commercial models.",
            "score": 25,
            "issue_id": 2555,
            "pub_date": "2025-03-02",
            "pub_date_card": {
                "ru": "2 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 2",
                "zh": "3æœˆ2æ—¥"
            },
            "hash": "bc2424e709a2dd78",
            "authors": [
                "Yiran Zhao",
                "Chaoqun Liu",
                "Yue Deng",
                "Jiahao Ying",
                "Mahani Aljunied",
                "Zhaodonghui Li",
                "Lidong Bing",
                "Hou Pong Chan",
                "Yu Rong",
                "Deli Zhao",
                "Wenxuan Zhang"
            ],
            "affiliations": [
                "DAMO Academy, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.00865.jpg",
            "data": {
                "categories": [
                    "#low_resource",
                    "#architecture",
                    "#open_source",
                    "#training",
                    "#multilingual"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Babel: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¼ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Babel, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ°Ñ 25 ÑĞ°Ğ¼Ñ‹Ñ… Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¼Ğ¸Ñ€Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾ĞµĞ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ´Ğ²Ğµ Ğ²ĞµÑ€ÑĞ¸Ğ¸: Babel-9B Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¸ Babel-83B, ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ÑÑ‰Ğ°Ñ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ±Ğµ Ğ²ĞµÑ€ÑĞ¸Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Babel: Bridging the Language Gap with Open Multilingual LLMs",
                    "desc": "This paper presents Babel, an innovative open-source multilingual large language model (LLM) that addresses the lack of coverage for under-resourced languages in existing models. Babel supports the top 25 languages spoken globally, reaching over 90% of the world's population, and includes many languages that are often neglected. The model employs a unique layer extension technique to increase its parameter count, enhancing its performance beyond traditional pretraining methods. With two variants, Babel-9B and Babel-83B, the model demonstrates superior performance on multilingual tasks, outperforming other open LLMs of similar size and achieving results comparable to commercial models."
                },
                "zh": {
                    "title": "Babelï¼šæ‰“ç ´è¯­è¨€å£å’çš„å¤šè¯­è¨€æ¨¡å‹",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸå¸¦æ¥äº†é©å‘½æ€§çš„å˜åŒ–ï¼Œä½†å¼€æºçš„å¤šè¯­è¨€LLMsä»ç„¶ç¨€ç¼ºï¼Œç°æœ‰æ¨¡å‹é€šå¸¸åœ¨è¯­è¨€è¦†ç›–ä¸Šæœ‰é™ã€‚è®¸å¤šæ¨¡å‹ä¼˜å…ˆè€ƒè™‘èµ„æºä¸°å¯Œçš„è¯­è¨€ï¼Œè€Œå¹¿æ³›ä½¿ç”¨ä½†èµ„æºä¸è¶³çš„è¯­è¨€å¸¸å¸¸è¢«å¿½è§†ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Babelï¼Œä¸€ä¸ªå¼€æ”¾çš„å¤šè¯­è¨€LLMï¼Œè¦†ç›–å…¨çƒå‰25ç§è¯­è¨€ï¼Œæ”¯æŒè¶…è¿‡90%çš„äººå£ï¼Œå¹¶åŒ…æ‹¬è®¸å¤šå…¶ä»–å¼€æºå¤šè¯­è¨€LLMså¿½è§†çš„è¯­è¨€ã€‚Babelé€šè¿‡å±‚æ‰©å±•æŠ€æœ¯å¢åŠ å‚æ•°æ•°é‡ï¼Œæå‡äº†æ€§èƒ½ï¼Œå¹¶æ¨å‡ºäº†ä¸¤ä¸ªå˜ä½“ï¼šBabel-9Bå’ŒBabel-83Bï¼Œåè€…åœ¨å¤šè¯­è¨€ä»»åŠ¡ä¸­è®¾å®šäº†æ–°çš„æ ‡å‡†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.00329",
            "title": "ABC: Achieving Better Control of Multimodal Embeddings using VLMs",
            "url": "https://huggingface.co/papers/2503.00329",
            "abstract": "Visual embedding models excel at zero-shot tasks like visual retrieval and classification. However, these models cannot be used for tasks that contain ambiguity or require user instruction. These tasks necessitate a multimodal embedding model, which outputs embeddings that combine visual and natural language input. Existing CLIP-based approaches embed images and text independently, and fuse the result. We find that this results in weak interactions between modalities, and poor user control over the representation. We introduce ABC, an open-source multimodal embedding model that uses a vision-language model backbone to deeply integrate image features with natural language instructions. ABC achieves bestfor-size performance on MSCOCO image-to-text retrieval and is the top performing model on classification and VQA tasks in the Massive Multimodal Embedding Benchmark. With a strongly unified vision-language representation, ABC can use natural language to solve subtle and potentially ambiguous visual retrieval problems. To evaluate this capability, we design CtrlBench, a benchmark that requires interleaving textual instructions with image content for correct retrieval. ABC advances the state of multimodal embeddings by offering high-quality representations and flexible natural language control. Our model and datasets are available at our project page.",
            "score": 8,
            "issue_id": 2555,
            "pub_date": "2025-03-01",
            "pub_date_card": {
                "ru": "1 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 1",
                "zh": "3æœˆ1æ—¥"
            },
            "hash": "0483c542c8885777",
            "authors": [
                "Benjamin Schneider",
                "Florian Kerschbaum",
                "Wenhu Chen"
            ],
            "affiliations": [
                "Cheriton School of Computer Science, University of Waterloo",
                "Vector Institute, Toronto"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.00329.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multimodal",
                    "#open_source",
                    "#dataset"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ABC: ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ³Ğ¸Ğ±ĞºĞ¸Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ABC, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², ABC Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºÑƒÑ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸. ABC Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ñ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "ABC: Unifying Vision and Language for Enhanced Multimodal Understanding",
                    "desc": "This paper presents ABC, a new multimodal embedding model that integrates visual and natural language inputs more effectively than existing CLIP-based methods. Unlike previous models that treat images and text separately, ABC combines these modalities deeply, allowing for better interaction and user control. The model excels in zero-shot tasks, particularly in image-to-text retrieval and classification, outperforming others in the Massive Multimodal Embedding Benchmark. To assess its capabilities, the authors introduce CtrlBench, a benchmark designed to evaluate the model's performance in handling complex visual retrieval tasks with natural language instructions."
                },
                "zh": {
                    "title": "ABCï¼šå¤šæ¨¡æ€åµŒå…¥çš„æ–°çªç ´",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºABCçš„å¤šæ¨¡æ€åµŒå…¥æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³è§†è§‰æ£€ç´¢å’Œåˆ†ç±»ä¸­çš„æ¨¡ç³Šæ€§é—®é¢˜ã€‚ä¸ç°æœ‰çš„CLIPæ–¹æ³•ä¸åŒï¼ŒABCé€šè¿‡æ·±åº¦æ•´åˆå›¾åƒç‰¹å¾å’Œè‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œæä¾›æ›´å¼ºçš„æ¨¡æ€äº¤äº’ã€‚ABCåœ¨MSCOCOå›¾åƒåˆ°æ–‡æœ¬æ£€ç´¢ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶åœ¨åˆ†ç±»å’Œè§†è§‰é—®ç­”ä»»åŠ¡ä¸­å–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚é€šè¿‡è®¾è®¡CtrlBenchåŸºå‡†ï¼Œè¯„ä¼°äº†ABCåœ¨å¤„ç†å¤æ‚è§†è§‰æ£€ç´¢é—®é¢˜æ—¶çš„èƒ½åŠ›ï¼Œå±•ç¤ºäº†å…¶é«˜è´¨é‡çš„è¡¨ç¤ºå’Œçµæ´»çš„è‡ªç„¶è¯­è¨€æ§åˆ¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.02951",
            "title": "KodCode: A Diverse, Challenging, and Verifiable Synthetic Dataset for Coding",
            "url": "https://huggingface.co/papers/2503.02951",
            "abstract": "We introduce KodCode, a synthetic dataset that addresses the persistent challenge of acquiring high-quality, verifiable training data across diverse difficulties and domains for training Large Language Models for coding. Existing code-focused resources typically fail to ensure either the breadth of coverage (e.g., spanning simple coding tasks to advanced algorithmic problems) or verifiable correctness (e.g., unit tests). In contrast, KodCode comprises question-solution-test triplets that are systematically validated via a self-verification procedure. Our pipeline begins by synthesizing a broad range of coding questions, then generates solutions and test cases with additional attempts allocated to challenging problems. Finally, post-training data synthesis is done by rewriting questions into diverse formats and generating responses under a test-based reject sampling procedure from a reasoning model (DeepSeek R1). This pipeline yields a large-scale, robust and diverse coding dataset. KodCode is suitable for supervised fine-tuning and the paired unit tests also provide great potential for RL tuning. Fine-tuning experiments on coding benchmarks (HumanEval(+), MBPP(+), BigCodeBench, and LiveCodeBench) demonstrate that KodCode-tuned models achieve state-of-the-art performance, surpassing models like Qwen2.5-Coder-32B-Instruct and DeepSeek-R1-Distill-Llama-70B.",
            "score": 6,
            "issue_id": 2555,
            "pub_date": "2025-03-04",
            "pub_date_card": {
                "ru": "4 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 4",
                "zh": "3æœˆ4æ—¥"
            },
            "hash": "6c344ba0bf71ac84",
            "authors": [
                "Zhangchen Xu",
                "Yang Liu",
                "Yueqin Yin",
                "Mingyuan Zhou",
                "Radha Poovendran"
            ],
            "affiliations": [
                "Microsoft",
                "The University of Texas at Austin",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.02951.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#rl",
                    "#optimization",
                    "#synthetic",
                    "#training"
                ],
                "emoji": "ğŸ§‘â€ğŸ’»",
                "ru": {
                    "title": "KodCode: Ğ¡Ğ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "KodCode - ÑÑ‚Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ½ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ñ‚Ñ€Ğ¸Ğ¿Ğ»ĞµÑ‚Ğ¾Ğ² Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ-Ñ‚ĞµÑÑ‚, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´ÑÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ñƒ ÑĞ°Ğ¼Ğ¾Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸. ĞŸÑ€Ğ¾Ñ†ĞµÑÑ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ KodCode Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ĞµĞ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° KodCode, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "KodCode: Elevating Coding Models with Verified Data",
                    "desc": "KodCode is a synthetic dataset designed to improve the training of Large Language Models (LLMs) for coding tasks by providing high-quality, verifiable data. It includes question-solution-test triplets that are validated through a self-verification process, ensuring both correctness and a wide range of coding difficulties. The dataset is generated using a systematic pipeline that synthesizes coding questions, creates solutions, and develops test cases, particularly focusing on challenging problems. Fine-tuning experiments show that models trained on KodCode outperform existing models on various coding benchmarks, demonstrating its effectiveness in enhancing LLM performance."
                },
                "zh": {
                    "title": "KodCodeï¼šé«˜è´¨é‡ç¼–ç æ•°æ®é›†çš„è§£å†³æ–¹æ¡ˆ",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†KodCodeï¼Œè¿™æ˜¯ä¸€ä¸ªåˆæˆæ•°æ®é›†ï¼Œæ—¨åœ¨è§£å†³è·å–é«˜è´¨é‡ã€å¯éªŒè¯çš„è®­ç»ƒæ•°æ®çš„æŒ‘æˆ˜ï¼Œä»¥è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œç¼–ç ã€‚ç°æœ‰çš„ä»£ç èµ„æºé€šå¸¸æ— æ³•ç¡®ä¿è¦†ç›–èŒƒå›´å¹¿æ³›æˆ–æ­£ç¡®æ€§å¯éªŒè¯ã€‚KodCodeç”±é—®é¢˜-è§£å†³æ–¹æ¡ˆ-æµ‹è¯•ä¸‰å…ƒç»„ç»„æˆï¼Œé€šè¿‡è‡ªæˆ‘éªŒè¯ç¨‹åºç³»ç»Ÿåœ°éªŒè¯ã€‚æˆ‘ä»¬çš„æµç¨‹åŒ…æ‹¬åˆæˆå„ç§ç¼–ç é—®é¢˜ï¼Œç”Ÿæˆè§£å†³æ–¹æ¡ˆå’Œæµ‹è¯•ç”¨ä¾‹ï¼Œå¹¶åœ¨åæœŸé€šè¿‡é‡å†™é—®é¢˜å’Œç”Ÿæˆå“åº”æ¥è¿›è¡Œæ•°æ®åˆæˆï¼Œæœ€ç»ˆç”Ÿæˆä¸€ä¸ªå¤§è§„æ¨¡ã€å¼ºå¤§ä¸”å¤šæ ·åŒ–çš„ç¼–ç æ•°æ®é›†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.03751",
            "title": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control",
            "url": "https://huggingface.co/papers/2503.03751",
            "abstract": "We present GEN3C, a generative video model with precise Camera Control and temporal 3D Consistency. Prior video models already generate realistic videos, but they tend to leverage little 3D information, leading to inconsistencies, such as objects popping in and out of existence. Camera control, if implemented at all, is imprecise, because camera parameters are mere inputs to the neural network which must then infer how the video depends on the camera. In contrast, GEN3C is guided by a 3D cache: point clouds obtained by predicting the pixel-wise depth of seed images or previously generated frames. When generating the next frames, GEN3C is conditioned on the 2D renderings of the 3D cache with the new camera trajectory provided by the user. Crucially, this means that GEN3C neither has to remember what it previously generated nor does it have to infer the image structure from the camera pose. The model, instead, can focus all its generative power on previously unobserved regions, as well as advancing the scene state to the next frame. Our results demonstrate more precise camera control than prior work, as well as state-of-the-art results in sparse-view novel view synthesis, even in challenging settings such as driving scenes and monocular dynamic video. Results are best viewed in videos. Check out our webpage! https://research.nvidia.com/labs/toronto-ai/GEN3C/",
            "score": 3,
            "issue_id": 2555,
            "pub_date": "2025-03-05",
            "pub_date_card": {
                "ru": "5 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 5",
                "zh": "3æœˆ5æ—¥"
            },
            "hash": "8f5f2ad910a260c0",
            "authors": [
                "Xuanchi Ren",
                "Tianchang Shen",
                "Jiahui Huang",
                "Huan Ling",
                "Yifan Lu",
                "Merlin Nimier-David",
                "Thomas MÃ¼ller",
                "Alexander Keller",
                "Sanja Fidler",
                "Jun Gao"
            ],
            "affiliations": [
                "NVIDIA",
                "University of Toronto",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.03751.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#video"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ 3D-ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "GEN3C - ÑÑ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ 3D-ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ 3D-ĞºÑÑˆ Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ¾Ğ±Ğ»Ğ°ĞºĞ¾Ğ² Ñ‚Ğ¾Ñ‡ĞµĞº, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ°Ñ€Ñ‚ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ»Ğ¸ Ñ€Ğ°Ğ½ĞµĞµ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ². ĞŸÑ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² GEN3C Ğ¾Ğ¿Ğ¸Ñ€Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° 2D-Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³ 3D-ĞºÑÑˆĞ° Ñ Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ĞµĞ¹ ĞºĞ°Ğ¼ĞµÑ€Ñ‹, Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑÑ„Ğ¾ĞºÑƒÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ñ€Ğ°Ğ½ĞµĞµ Ğ½ĞµĞ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ğ¸ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ ÑÑ†ĞµĞ½Ñ‹, Ğ½Ğµ Ñ‚Ñ€Ğ°Ñ‚Ñ Ñ€ĞµÑÑƒÑ€ÑÑ‹ Ğ½Ğ° Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¸Ğ»Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ· Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹."
                },
                "en": {
                    "title": "GEN3C: Mastering Video Generation with 3D Precision and Camera Control",
                    "desc": "GEN3C is a generative video model that enhances video generation by incorporating precise camera control and maintaining temporal 3D consistency. Unlike previous models that often lack 3D information, GEN3C utilizes a 3D cache of point clouds derived from depth predictions, allowing for more coherent object presence in videos. The model is conditioned on 2D renderings from this cache, enabling it to generate new frames without needing to remember past outputs or infer scene structure from camera angles. This approach results in superior camera control and state-of-the-art performance in generating novel views, particularly in complex scenarios like driving scenes."
                },
                "zh": {
                    "title": "GEN3Cï¼šç²¾ç¡®ç›¸æœºæ§åˆ¶ä¸æ—¶é—´ä¸€è‡´æ€§çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹",
                    "desc": "æˆ‘ä»¬æå‡ºäº†GEN3Cï¼Œè¿™æ˜¯ä¸€ç§å…·æœ‰ç²¾ç¡®ç›¸æœºæ§åˆ¶å’Œæ—¶é—´ä¸€è‡´æ€§çš„ç”Ÿæˆè§†é¢‘æ¨¡å‹ã€‚ä»¥å¾€çš„è§†é¢‘æ¨¡å‹è™½ç„¶èƒ½å¤Ÿç”Ÿæˆé€¼çœŸçš„è§†é¢‘ï¼Œä½†å¾€å¾€ç¼ºä¹3Dä¿¡æ¯ï¼Œå¯¼è‡´ç‰©ä½“å‡ºç°å’Œæ¶ˆå¤±çš„ä¸ä¸€è‡´æ€§ã€‚GEN3Cé€šè¿‡3Dç¼“å­˜æ¥æŒ‡å¯¼ç”Ÿæˆè¿‡ç¨‹ï¼Œåˆ©ç”¨ä»ç§å­å›¾åƒæˆ–å…ˆå‰ç”Ÿæˆå¸§ä¸­é¢„æµ‹çš„åƒç´ æ·±åº¦è·å¾—çš„ç‚¹äº‘ã€‚è¿™æ ·ï¼ŒGEN3Cèƒ½å¤Ÿåœ¨ç”¨æˆ·æä¾›çš„æ–°ç›¸æœºè½¨è¿¹ä¸‹ï¼Œä¸“æ³¨äºç”Ÿæˆæœªè§‚å¯Ÿåˆ°çš„åŒºåŸŸï¼Œå¹¶æœ‰æ•ˆæ¨è¿›åœºæ™¯çŠ¶æ€åˆ°ä¸‹ä¸€ä¸ªå¸§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.01729",
            "title": "FLAME: A Federated Learning Benchmark for Robotic Manipulation",
            "url": "https://huggingface.co/papers/2503.01729",
            "abstract": "Recent progress in robotic manipulation has been fueled by large-scale datasets collected across diverse environments. Training robotic manipulation policies on these datasets is traditionally performed in a centralized manner, raising concerns regarding scalability, adaptability, and data privacy. While federated learning enables decentralized, privacy-preserving training, its application to robotic manipulation remains largely unexplored. We introduce FLAME (Federated Learning Across Manipulation Environments), the first benchmark designed for federated learning in robotic manipulation. FLAME consists of: (i) a set of large-scale datasets of over 160,000 expert demonstrations of multiple manipulation tasks, collected across a wide range of simulated environments; (ii) a training and evaluation framework for robotic policy learning in a federated setting. We evaluate standard federated learning algorithms in FLAME, showing their potential for distributed policy learning and highlighting key challenges. Our benchmark establishes a foundation for scalable, adaptive, and privacy-aware robotic learning.",
            "score": 1,
            "issue_id": 2558,
            "pub_date": "2025-03-03",
            "pub_date_card": {
                "ru": "3 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 3",
                "zh": "3æœˆ3æ—¥"
            },
            "hash": "893358a382c79250",
            "authors": [
                "Santiago Bou Betran",
                "Alberta Longhini",
                "Miguel Vasco",
                "Yuchong Zhang",
                "Danica Kragic"
            ],
            "affiliations": [
                "KTH Royal Institute of Technology, Stockholm, Sweden"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.01729.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ¤ĞµĞ´ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ FLAME - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ñ„ĞµĞ´ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸. FLAME Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 160 000 ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸, ÑĞ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ² Ñ„ĞµĞ´ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ Ñ„ĞµĞ´ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° FLAME, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¸Ñ… Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ¸ Ğ²Ñ‹ÑĞ²Ğ»ÑÑ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹."
                },
                "en": {
                    "title": "Empowering Robots with Federated Learning for Privacy and Scalability",
                    "desc": "This paper presents FLAME, a benchmark for applying federated learning to robotic manipulation tasks. It addresses the limitations of centralized training by allowing robots to learn from diverse datasets while preserving data privacy. FLAME includes over 160,000 expert demonstrations from various simulated environments, facilitating decentralized training. The study evaluates existing federated learning algorithms, demonstrating their effectiveness and identifying challenges in distributed policy learning for robotics."
                },
                "zh": {
                    "title": "è”é‚¦å­¦ä¹ åŠ©åŠ›æœºå™¨äººæ“æ§çš„æœªæ¥",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†FLAMEï¼ˆè·¨æ“æ§ç¯å¢ƒçš„è”é‚¦å­¦ä¹ ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºæœºå™¨äººæ“æ§è®¾è®¡çš„åŸºå‡†æµ‹è¯•ã€‚FLAMEåŒ…å«è¶…è¿‡160,000ä¸ªä¸“å®¶æ¼”ç¤ºçš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œæ¶µç›–å¤šç§æ“æ§ä»»åŠ¡ï¼Œæ”¶é›†è‡ªå¤šç§æ¨¡æ‹Ÿç¯å¢ƒã€‚é€šè¿‡åœ¨FLAMEä¸­è¯„ä¼°æ ‡å‡†çš„è”é‚¦å­¦ä¹ ç®—æ³•ï¼Œè®ºæ–‡å±•ç¤ºäº†åˆ†å¸ƒå¼ç­–ç•¥å­¦ä¹ çš„æ½œåŠ›ï¼Œå¹¶æŒ‡å‡ºäº†å…³é”®æŒ‘æˆ˜ã€‚è¯¥åŸºå‡†ä¸ºå¯æ‰©å±•ã€é€‚åº”æ€§å¼ºä¸”æ³¨é‡éšç§çš„æœºå™¨äººå­¦ä¹ å¥ å®šäº†åŸºç¡€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.01378",
            "title": "CognitiveDrone: A VLA Model and Evaluation Benchmark for Real-Time Cognitive Task Solving and Reasoning in UAVs",
            "url": "https://huggingface.co/papers/2503.01378",
            "abstract": "This paper introduces CognitiveDrone, a novel Vision-Language-Action (VLA) model tailored for complex Unmanned Aerial Vehicles (UAVs) tasks that demand advanced cognitive abilities. Trained on a dataset comprising over 8,000 simulated flight trajectories across three key categories-Human Recognition, Symbol Understanding, and Reasoning-the model generates real-time 4D action commands based on first-person visual inputs and textual instructions. To further enhance performance in intricate scenarios, we propose CognitiveDrone-R1, which integrates an additional Vision-Language Model (VLM) reasoning module to simplify task directives prior to high-frequency control. Experimental evaluations using our open-source benchmark, CognitiveDroneBench, reveal that while a racing-oriented model (RaceVLA) achieves an overall success rate of 31.3%, the base CognitiveDrone model reaches 59.6%, and CognitiveDrone-R1 attains a success rate of 77.2%. These results demonstrate improvements of up to 30% in critical cognitive tasks, underscoring the effectiveness of incorporating advanced reasoning capabilities into UAV control systems. Our contributions include the development of a state-of-the-art VLA model for UAV control and the introduction of the first dedicated benchmark for assessing cognitive tasks in drone operations. The complete repository is available at cognitivedrone.github.io",
            "score": 1,
            "issue_id": 2558,
            "pub_date": "2025-03-03",
            "pub_date_card": {
                "ru": "3 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 3",
                "zh": "3æœˆ3æ—¥"
            },
            "hash": "8a4aab69ce92453d",
            "authors": [
                "Artem Lykov",
                "Valerii Serpiva",
                "Muhammad Haris Khan",
                "Oleg Sautenkov",
                "Artyom Myshlyaev",
                "Grik Tadevosyan",
                "Yasheerah Yaqoot",
                "Dzmitry Tsetserukou"
            ],
            "affiliations": [
                "Intelligent Space Robotics Laboratory, Science Center for Digital Engineering, Technology. Skolkovo Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.01378.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#reasoning",
                    "#benchmark",
                    "#dataset",
                    "#multimodal",
                    "#cv"
                ],
                "emoji": "ğŸš",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ñ‹Ğµ Ğ´Ñ€Ğ¾Ğ½Ñ‹: ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ‘ĞŸĞ›Ğ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ CognitiveDrone - Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ—Ñ€ĞµĞ½Ğ¸Ğµ-Ğ¯Ğ·Ñ‹Ğº-Ğ”ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ (VLA) Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ±ĞµÑĞ¿Ğ¸Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ»ĞµÑ‚Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ¿Ğ¿Ğ°Ñ€Ğ°Ñ‚Ğ¾Ğ² (Ğ‘ĞŸĞ›Ğ). ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 8000 ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ĞµÑ‚Ğ¾Ğ² Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. Ğ£ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ CognitiveDrone-R1 Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ—Ñ€ĞµĞ½Ğ¸Ñ-Ğ¯Ğ·Ñ‹ĞºĞ° (VLM) Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ CognitiveDrone-R1 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² 77.2%, Ñ‡Ñ‚Ğ¾ Ğ½Ğ° 30% Ğ»ÑƒÑ‡ÑˆĞµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "CognitiveDrone: Elevating UAV Intelligence with Vision-Language-Action!",
                    "desc": "This paper presents CognitiveDrone, a new Vision-Language-Action (VLA) model designed for complex tasks performed by Unmanned Aerial Vehicles (UAVs). It is trained on a dataset of over 8,000 simulated flight paths focusing on Human Recognition, Symbol Understanding, and Reasoning. The model can generate real-time 4D action commands from visual inputs and text instructions, with an enhanced version, CognitiveDrone-R1, that includes a Vision-Language Model (VLM) reasoning module for better task management. Experimental results show significant performance improvements, with CognitiveDrone-R1 achieving a 77.2% success rate, highlighting the importance of advanced reasoning in UAV operations."
                },
                "zh": {
                    "title": "æ™ºèƒ½æ— äººæœºçš„è®¤çŸ¥é£è¡Œæ–°çºªå…ƒ",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºCognitiveDroneçš„æ–°å‹è§†è§‰-è¯­è¨€-è¡ŒåŠ¨ï¼ˆVLAï¼‰æ¨¡å‹ï¼Œä¸“ä¸ºå¤æ‚çš„æ— äººæœºä»»åŠ¡è®¾è®¡ï¼Œå…·å¤‡é«˜çº§è®¤çŸ¥èƒ½åŠ›ã€‚è¯¥æ¨¡å‹åœ¨è¶…è¿‡8000æ¡æ¨¡æ‹Ÿé£è¡Œè½¨è¿¹çš„æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæ¶µç›–äººç±»è¯†åˆ«ã€ç¬¦å·ç†è§£å’Œæ¨ç†ä¸‰ä¸ªå…³é”®ç±»åˆ«ã€‚CognitiveDrone-R1é€šè¿‡é›†æˆé¢å¤–çš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ¨ç†æ¨¡å—ï¼Œè¿›ä¸€æ­¥æå‡åœ¨å¤æ‚åœºæ™¯ä¸­çš„è¡¨ç°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCognitiveDroneæ¨¡å‹çš„æˆåŠŸç‡è¾¾åˆ°59.6%ï¼Œè€ŒCognitiveDrone-R1çš„æˆåŠŸç‡æ›´æ˜¯é«˜è¾¾77.2%ï¼Œè¯æ˜äº†å°†é«˜çº§æ¨ç†èƒ½åŠ›èå…¥æ— äººæœºæ§åˆ¶ç³»ç»Ÿçš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.00502",
            "title": "Interact, Instruct to Improve: A LLM-Driven Parallel Actor-Reasoner Framework for Enhancing Autonomous Vehicle Interactions",
            "url": "https://huggingface.co/papers/2503.00502",
            "abstract": "Autonomous Vehicles (AVs) have entered the commercialization stage, but their limited ability to interact and express intentions still poses challenges in interactions with Human-driven Vehicles (HVs). Recent advances in large language models (LLMs) enable bidirectional human-machine communication, but the conflict between slow inference speed and the need for real-time decision-making challenges practical deployment. To address these issues, this paper introduces a parallel Actor-Reasoner framework designed to enable explicit bidirectional AV-HV interactions across multiple scenarios. First, by facilitating interactions between the LLM-driven Reasoner and heterogeneous simulated HVs during training, an interaction memory database, referred to as the Actor, is established. Then, by introducing the memory partition module and the two-layer memory retrieval module, the Actor's ability to handle heterogeneous HVs is significantly enhanced. Ablation studies and comparisons with other decision-making methods demonstrate that the proposed Actor-Reasoner framework significantly improves safety and efficiency. Finally, with the combination of the external Human-Machine Interface (eHMI) information derived from Reasoner's reasoning and the feasible action solutions retrieved from the Actor, the effectiveness of the proposed Actor-Reasoner is confirmed in multi-scenario field interactions. Our code is available at https://github.com/FanGShiYuu/Actor-Reasoner.",
            "score": 1,
            "issue_id": 2555,
            "pub_date": "2025-03-01",
            "pub_date_card": {
                "ru": "1 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 1",
                "zh": "3æœˆ1æ—¥"
            },
            "hash": "d184a5cae68093d5",
            "authors": [
                "Shiyu Fang",
                "Jiaqi Liu",
                "Chengkai Xu",
                "Chen Lv",
                "Peng Hang",
                "Jian Sun"
            ],
            "affiliations": [
                "College of Transportation, Tongji University, Shanghai 201804, China",
                "Nanyang Technological University, 639798, Singapore",
                "State Key Lab of Intelligent Transportation System, Beijing 100088, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.00502.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#robotics",
                    "#inference",
                    "#optimization",
                    "#agents",
                    "#reasoning"
                ],
                "emoji": "ğŸš—",
                "ru": {
                    "title": "Ğ˜Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ¾Ğ±Ğ¸Ğ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Actor-Reasoner Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ€ĞµĞ´ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ±Ğ°Ğ·Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ´Ğ²ÑƒÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ€ĞµĞ´ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Actor-Reasoner Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Enhancing AV-HV Interactions with the Actor-Reasoner Framework",
                    "desc": "This paper presents a new framework called the Actor-Reasoner to improve interactions between Autonomous Vehicles (AVs) and Human-driven Vehicles (HVs). It leverages large language models (LLMs) to facilitate real-time communication and decision-making, addressing the challenge of slow inference speeds. The framework includes an interaction memory database, which enhances the AV's ability to understand and respond to various HV behaviors. Experimental results show that this approach significantly boosts both safety and efficiency in multi-scenario driving situations."
                },
                "zh": {
                    "title": "æå‡è‡ªåŠ¨é©¾é©¶ä¸äººç±»é©¾é©¶äº’åŠ¨çš„æ™ºèƒ½æ¡†æ¶",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å¹¶è¡Œæ¼”å‘˜-æ¨ç†å™¨æ¡†æ¶ï¼Œæ—¨åœ¨æ”¹å–„è‡ªåŠ¨é©¾é©¶æ±½è½¦ï¼ˆAVï¼‰ä¸äººç±»é©¾é©¶æ±½è½¦ï¼ˆHVï¼‰ä¹‹é—´çš„äº’åŠ¨ã€‚é€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¿ƒè¿›å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„æ¨ç†å™¨ä¸ä¸åŒç±»å‹çš„æ¨¡æ‹ŸHVä¹‹é—´çš„äº’åŠ¨ï¼Œå»ºç«‹äº†ä¸€ä¸ªäº’åŠ¨è®°å¿†æ•°æ®åº“ã€‚å¼•å…¥è®°å¿†åˆ†åŒºæ¨¡å—å’ŒåŒå±‚è®°å¿†æ£€ç´¢æ¨¡å—åï¼Œæ¼”å‘˜çš„å¤„ç†èƒ½åŠ›å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šåœºæ™¯äº¤äº’ä¸­æ˜¾è‘—æé«˜äº†å®‰å…¨æ€§å’Œæ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.01763",
            "title": "Retrieval Models Aren't Tool-Savvy: Benchmarking Tool Retrieval for Large Language Models",
            "url": "https://huggingface.co/papers/2503.01763",
            "abstract": "Tool learning aims to augment large language models (LLMs) with diverse tools, enabling them to act as agents for solving practical tasks. Due to the limited context length of tool-using LLMs, adopting information retrieval (IR) models to select useful tools from large toolsets is a critical initial step. However, the performance of IR models in tool retrieval tasks remains underexplored and unclear. Most tool-use benchmarks simplify this step by manually pre-annotating a small set of relevant tools for each task, which is far from the real-world scenarios. In this paper, we propose ToolRet, a heterogeneous tool retrieval benchmark comprising 7.6k diverse retrieval tasks, and a corpus of 43k tools, collected from existing datasets. We benchmark six types of models on ToolRet. Surprisingly, even the models with strong performance in conventional IR benchmarks, exhibit poor performance on ToolRet. This low retrieval quality degrades the task pass rate of tool-use LLMs. As a further step, we contribute a large-scale training dataset with over 200k instances, which substantially optimizes the tool retrieval ability of IR models.",
            "score": 0,
            "issue_id": 2558,
            "pub_date": "2025-03-03",
            "pub_date_card": {
                "ru": "3 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 3",
                "zh": "3æœˆ3æ—¥"
            },
            "hash": "e6a23582f741dc5b",
            "authors": [
                "Zhengliang Shi",
                "Yuhan Wang",
                "Lingyong Yan",
                "Pengjie Ren",
                "Shuaiqiang Wang",
                "Dawei Yin",
                "Zhaochun Ren"
            ],
            "affiliations": [
                "Baidu Inc., Beijing, China",
                "Leiden University, Leiden, The Netherlands",
                "Shandong University, Qingdao, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.01763.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#benchmark",
                    "#dataset",
                    "#data"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ToolRet: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ˜Ğ˜",
                    "desc": "ToolRet - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 7,6 Ñ‚Ñ‹ÑÑÑ‡ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ ĞºĞ¾Ñ€Ğ¿ÑƒÑ Ğ¸Ğ· 43 Ñ‚Ñ‹ÑÑÑ‡ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ² Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ ToolRet. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 200 Ñ‚Ñ‹ÑÑÑ‡ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¿Ğ¾Ğ¸ÑĞºÑƒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing Tool Retrieval for Language Models with ToolRet",
                    "desc": "This paper introduces ToolRet, a benchmark designed to evaluate the effectiveness of information retrieval (IR) models in selecting tools for large language models (LLMs) in practical tasks. The authors highlight that existing benchmarks often rely on a limited set of pre-annotated tools, which does not reflect real-world complexities. Their findings reveal that even high-performing IR models struggle with tool retrieval in this new context, leading to lower task success rates for LLMs. To address this issue, they provide a large-scale training dataset that significantly enhances the tool retrieval capabilities of IR models."
                },
                "zh": {
                    "title": "å·¥å…·æ£€ç´¢ï¼šæå‡LLMsçš„å®ç”¨èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å·¥å…·å­¦ä¹ å¦‚ä½•å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿä½œä¸ºä»£ç†è§£å†³å®é™…ä»»åŠ¡ã€‚ç”±äºå·¥å…·ä½¿ç”¨çš„LLMså…·æœ‰æœ‰é™çš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œå› æ­¤é‡‡ç”¨ä¿¡æ¯æ£€ç´¢ï¼ˆIRï¼‰æ¨¡å‹ä»å¤§é‡å·¥å…·é›†ä¸­é€‰æ‹©æœ‰ç”¨å·¥å…·æ˜¯å…³é”®çš„åˆæ­¥æ­¥éª¤ã€‚æˆ‘ä»¬æå‡ºäº†ToolRetï¼Œä¸€ä¸ªåŒ…å«7.6kå¤šæ ·åŒ–æ£€ç´¢ä»»åŠ¡å’Œ43kå·¥å…·çš„å¼‚æ„å·¥å…·æ£€ç´¢åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°IRæ¨¡å‹åœ¨å·¥å…·æ£€ç´¢ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼Œå³ä½¿åœ¨ä¼ ç»ŸIRåŸºå‡†ä¸Šè¡¨ç°è‰¯å¥½çš„æ¨¡å‹ï¼Œåœ¨ToolRetä¸Šçš„è¡¨ç°å´å¾ˆå·®ï¼Œè¿™é™ä½äº†å·¥å…·ä½¿ç”¨LLMsçš„ä»»åŠ¡é€šè¿‡ç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.01449",
            "title": "Benchmarking Large Language Models for Multi-Language Software Vulnerability Detection",
            "url": "https://huggingface.co/papers/2503.01449",
            "abstract": "Recent advancements in generative AI have led to the widespread adoption of large language models (LLMs) in software engineering, addressing numerous long-standing challenges. However, a comprehensive study examining the capabilities of LLMs in software vulnerability detection (SVD), a crucial aspect of software security, is currently lacking. Existing research primarily focuses on evaluating LLMs using C/C++ datasets. It typically explores only one or two strategies among prompt engineering, instruction tuning, and sequence classification fine-tuning for open-source LLMs. Consequently, there is a significant knowledge gap regarding the effectiveness of diverse LLMs in detecting vulnerabilities across various programming languages. To address this knowledge gap, we present a comprehensive empirical study evaluating the performance of LLMs on the SVD task. We have compiled a comprehensive dataset comprising 8,260 vulnerable functions in Python, 7,505 in Java, and 28,983 in JavaScript. We assess five open-source LLMs using multiple approaches, including prompt engineering, instruction tuning, and sequence classification fine-tuning. These LLMs are benchmarked against five fine-tuned small language models and two open-source static application security testing tools. Furthermore, we explore two avenues to improve LLM performance on SVD: a) Data perspective: Retraining models using downsampled balanced datasets. b) Model perspective: Investigating ensemble learning methods that combine predictions from multiple LLMs. Our comprehensive experiments demonstrate that SVD remains a challenging task for LLMs. This study provides a thorough understanding of the role of LLMs in SVD and offers practical insights for future advancements in leveraging generative AI to enhance software security practices.",
            "score": 0,
            "issue_id": 2558,
            "pub_date": "2025-03-03",
            "pub_date_card": {
                "ru": "3 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 3",
                "zh": "3æœˆ3æ—¥"
            },
            "hash": "1b4593bb9d78ec53",
            "authors": [
                "Ting Zhang",
                "Chengran Yang",
                "Yindu Su",
                "Martin Weyssow",
                "Hung Nguyen",
                "Tan Bui",
                "Hong Jin Kang",
                "Yikun Li",
                "Eng Lieh Ouh",
                "Lwin Khin Shar",
                "David Lo"
            ],
            "affiliations": [
                "School of Computer Science, University of Sydney, Australia",
                "School of Computing and Information Systems, Singapore Management University, Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.01449.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#plp",
                    "#training",
                    "#security",
                    "#benchmark",
                    "#dataset",
                    "#data"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "LLM Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ğ¶Ğµ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ´Ğ°: Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ñ‹ Ğ² Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ (SVD). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿ÑÑ‚Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… LLM Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ñ… ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ½Ğ° Python, Java Ğ¸ JavaScript, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ², Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¸ Ñ‚Ğ¾Ğ½ĞºĞ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸Ğ·ÑƒÑ‡Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ñ‹ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ LLM Ğ² SVD, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ½ÑĞ°Ğ¼Ğ±Ğ»ĞµĞ²Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SVD Ğ¾ÑÑ‚Ğ°ĞµÑ‚ÑÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡ĞµĞ¹ Ğ´Ğ»Ñ LLM, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ñ†ĞµĞ½Ğ½Ñ‹Ğµ insights Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ¾Ğº Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Unlocking LLMs for Software Vulnerability Detection",
                    "desc": "This paper investigates the effectiveness of large language models (LLMs) in detecting software vulnerabilities, an important area for software security. It highlights the lack of comprehensive studies on LLMs' capabilities across various programming languages, as most existing research focuses on C/C++ datasets. The authors present an empirical study using a dataset of over 44,000 vulnerable functions from Python, Java, and JavaScript, evaluating five open-source LLMs with different strategies like prompt engineering and instruction tuning. The findings reveal that while LLMs show promise, software vulnerability detection remains a challenging task, providing valuable insights for future improvements in this field."
                },
                "zh": {
                    "title": "æå‡è½¯ä»¶å®‰å…¨ï¼šå¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¼æ´æ£€æµ‹ä¸­çš„åº”ç”¨",
                    "desc": "æœ€è¿‘ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½çš„è¿›å±•ä½¿å¾—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è½¯ä»¶å·¥ç¨‹ä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ï¼Œè§£å†³äº†è®¸å¤šé•¿æœŸå­˜åœ¨çš„æŒ‘æˆ˜ã€‚ç„¶è€Œï¼Œç›®å‰ç¼ºä¹å¯¹LLMsåœ¨è½¯ä»¶æ¼æ´æ£€æµ‹ï¼ˆSVDï¼‰èƒ½åŠ›çš„å…¨é¢ç ”ç©¶ï¼Œè¿™å¯¹è½¯ä»¶å®‰å…¨è‡³å…³é‡è¦ã€‚ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ä½¿ç”¨C/C++æ•°æ®é›†è¯„ä¼°LLMsï¼Œé€šå¸¸åªæ¢è®¨äº†æç¤ºå·¥ç¨‹ã€æŒ‡ä»¤è°ƒä¼˜å’Œåºåˆ—åˆ†ç±»å¾®è°ƒä¸­çš„ä¸€ä¸¤ç§ç­–ç•¥ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸€é¡¹å…¨é¢çš„å®è¯ç ”ç©¶ï¼Œè¯„ä¼°LLMsåœ¨ä¸åŒç¼–ç¨‹è¯­è¨€ä¸­æ£€æµ‹æ¼æ´çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.01372",
            "title": "SwiLTra-Bench: The Swiss Legal Translation Benchmark",
            "url": "https://huggingface.co/papers/2503.01372",
            "abstract": "In Switzerland legal translation is uniquely important due to the country's four official languages and requirements for multilingual legal documentation. However, this process traditionally relies on professionals who must be both legal experts and skilled translators -- creating bottlenecks and impacting effective access to justice. To address this challenge, we introduce SwiLTra-Bench, a comprehensive multilingual benchmark of over 180K aligned Swiss legal translation pairs comprising laws, headnotes, and press releases across all Swiss languages along with English, designed to evaluate LLM-based translation systems. Our systematic evaluation reveals that frontier models achieve superior translation performance across all document types, while specialized translation systems excel specifically in laws but under-perform in headnotes. Through rigorous testing and human expert validation, we demonstrate that while fine-tuning open SLMs significantly improves their translation quality, they still lag behind the best zero-shot prompted frontier models such as Claude-3.5-Sonnet. Additionally, we present SwiLTra-Judge, a specialized LLM evaluation system that aligns best with human expert assessments.",
            "score": 0,
            "issue_id": 2558,
            "pub_date": "2025-03-03",
            "pub_date_card": {
                "ru": "3 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 3",
                "zh": "3æœˆ3æ—¥"
            },
            "hash": "3de5be81537fa0fd",
            "authors": [
                "Joel Niklaus",
                "Jakob Merane",
                "Luka Nenadic",
                "Sina Ahmadi",
                "Yingqiang Gao",
                "Cyrill A. H. Chevalley",
                "Claude Humbel",
                "Christophe GÃ¶sken",
                "Lorenzo Tanzi",
                "Thomas LÃ¼thi",
                "Stefan Palombo",
                "Spencer Poff",
                "Boling Yang",
                "Nan Wu",
                "Matthew Guillod",
                "Robin MamiÃ©",
                "Daniel Brunner",
                "Julio Pereyra",
                "Niko Grupen"
            ],
            "affiliations": [
                "Canton of Solothurn",
                "ETH Zurich",
                "Max Planck Institute for Research on Collective Goods",
                "Swiss Federal Supreme Court",
                "University of Basel",
                "University of Geneva",
                "University of Lausanne",
                "University of Zurich"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.01372.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#open_source",
                    "#benchmark",
                    "#dataset",
                    "#machine_translation"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğµ: Ğ˜Ğ˜ Ğ¿Ğ¾ĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½ÑƒÑ Ğ¨Ğ²ĞµĞ¹Ñ†Ğ°Ñ€Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SwiLTra-Bench - Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ² Ğ¨Ğ²ĞµĞ¹Ñ†Ğ°Ñ€Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ²Ğ¾ Ğ²ÑĞµÑ… Ñ‚Ğ¸Ğ¿Ğ°Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ° Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° SwiLTra-Judge Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ĞºĞ¾Ñ€Ñ€ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing Legal Translation with SwiLTra-Bench and LLMs",
                    "desc": "This paper addresses the challenges of legal translation in Switzerland, where multiple languages complicate the process. It introduces SwiLTra-Bench, a benchmark dataset with over 180,000 aligned legal translation pairs to evaluate large language model (LLM) translation systems. The findings show that while advanced models perform well across various document types, specialized systems are better for translating laws but struggle with headnotes. The study also highlights the effectiveness of fine-tuning open-source language models, although they still do not match the performance of top zero-shot models like Claude-3.5-Sonnet."
                },
                "zh": {
                    "title": "ç‘å£«æ³•å¾‹ç¿»è¯‘çš„æ™ºèƒ½è§£å†³æ–¹æ¡ˆ",
                    "desc": "åœ¨ç‘å£«ï¼Œç”±äºæœ‰å››ç§å®˜æ–¹è¯­è¨€ï¼Œæ³•å¾‹ç¿»è¯‘æ˜¾å¾—å°¤ä¸ºé‡è¦ã€‚ä¼ ç»Ÿä¸Šï¼Œè¿™ä¸€è¿‡ç¨‹ä¾èµ–äºæ—¢æ˜¯æ³•å¾‹ä¸“å®¶åˆæ˜¯ç¿»è¯‘é«˜æ‰‹çš„ä¸“ä¸šäººå£«ï¼Œå¯¼è‡´äº†ç“¶é¢ˆï¼Œå½±å“äº†å…¬æ­£çš„æœ‰æ•ˆè·å–ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†SwiLTra-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«è¶…è¿‡18ä¸‡å¯¹ç‘å£«æ³•å¾‹ç¿»è¯‘çš„å¤šè¯­è¨€åŸºå‡†æ•°æ®é›†ï¼Œæ—¨åœ¨è¯„ä¼°åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ç¿»è¯‘ç³»ç»Ÿã€‚æˆ‘ä»¬çš„è¯„ä¼°æ˜¾ç¤ºï¼Œå‰æ²¿æ¨¡å‹åœ¨æ‰€æœ‰æ–‡æ¡£ç±»å‹çš„ç¿»è¯‘è¡¨ç°ä¸Šä¼˜äºå…¶ä»–ç³»ç»Ÿï¼Œè€Œä¸“é—¨çš„ç¿»è¯‘ç³»ç»Ÿåœ¨æ³•å¾‹æ–‡æœ¬ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤´æ³¨æ–¹é¢è¡¨ç°ä¸ä½³ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-03-05.html",
    "link_next": "2025-03-07.html",
    "link_month": "2025-03.html",
    "short_date_prev": {
        "ru": "05.03",
        "en": "03/05",
        "zh": "3æœˆ5æ—¥"
    },
    "short_date_next": {
        "ru": "07.03",
        "en": "03/07",
        "zh": "3æœˆ7æ—¥"
    },
    "categories": {
        "#dataset": 7,
        "#data": 2,
        "#benchmark": 6,
        "#agents": 1,
        "#cv": 1,
        "#rl": 2,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 1,
        "#inference": 1,
        "#3d": 1,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 2,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 2,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 1,
        "#leakage": 0,
        "#open_source": 5,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 1
    },
    "zh": {
        "text": "æœ€è¿‘çš„å¤§è¯­è¨€æ¨¡å‹è¿›å±•ä½¿å¾—åŸºäºLLMçš„ä»£ç†èƒ½å¤ŸæˆåŠŸå¤„ç†äº’åŠ¨å¼è§„åˆ’ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¸¸å¸¸å—åˆ°è§„åˆ’å¹»è§‰çš„å›°æ‰°ï¼Œå¹¶ä¸”æ¯ä¸ªæ–°ä»£ç†éƒ½éœ€è¦é‡æ–°è®­ç»ƒã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†å…ƒè§„åˆ’ä¼˜åŒ–ï¼ˆMPOï¼‰æ¡†æ¶ï¼Œé€šè¿‡ç›´æ¥å¼•å…¥æ˜¾å¼æŒ‡å¯¼æ¥å¢å¼ºä»£ç†çš„è§„åˆ’èƒ½åŠ›ã€‚MPOåˆ©ç”¨å…ƒè§„åˆ’æä¾›é«˜å±‚æ¬¡çš„é€šç”¨æŒ‡å¯¼ï¼Œå¸®åŠ©ä»£ç†è§„åˆ’ï¼Œå¹¶æ ¹æ®ä»»åŠ¡æ‰§è¡Œåé¦ˆæŒç»­ä¼˜åŒ–å…ƒè§„åˆ’ã€‚å®éªŒè¡¨æ˜ï¼ŒMPOåœ¨ä¸¤é¡¹ä»£è¡¨æ€§ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰åŸºå‡†ï¼Œå¹¶æé«˜äº†ä»»åŠ¡å®Œæˆæ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚",
        "title": "MPO: Boosting LLM Agents with Meta Plan Optimization",
        "pinyin": "æœ€è¿‘çš„å¤§è¯­è¨€æ¨¡å‹è¿›å±•ä½¿å¾—åŸºäºLLMçš„ä»£ç†èƒ½å¤ŸæˆåŠŸå¤„ç†äº’åŠ¨å¼è§„åˆ’ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¸¸å¸¸å—åˆ°è§„åˆ’å¹»è§‰çš„å›°æ‰°ï¼Œå¹¶ä¸”æ¯ä¸ªæ–°ä»£ç†éƒ½éœ€è¦é‡æ–°è®­ç»ƒã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†å…ƒè§„åˆ’ä¼˜åŒ–ï¼ˆMPOï¼‰æ¡†æ¶ï¼Œé€šè¿‡ç›´æ¥å¼•å…¥æ˜¾å¼æŒ‡å¯¼æ¥å¢å¼ºä»£ç†çš„è§„åˆ’èƒ½åŠ›ã€‚MPOåˆ©ç”¨å…ƒè§„åˆ’æä¾›é«˜å±‚æ¬¡çš„é€šç”¨æŒ‡å¯¼ï¼Œå¸®åŠ©ä»£ç†è§„åˆ’ï¼Œå¹¶æ ¹æ®ä»»åŠ¡æ‰§è¡Œåé¦ˆæŒç»­ä¼˜åŒ–å…ƒè§„åˆ’ã€‚å®éªŒè¡¨æ˜ï¼ŒMPOåœ¨ä¸¤é¡¹ä»£è¡¨æ€§ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰åŸºå‡†ï¼Œå¹¶æé«˜äº†ä»»åŠ¡å®Œæˆæ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚\n\nZuÃ¬jÃ¬n de dÃ  yÇ”yÃ¡n mÃ³xÃ­ng jÃ¬nzhÇn shÇdÃ© jÄ«yÃº LLM de dÃ ilÇ nÃ©nggÃ²u chÃ©nggÅng chÇ”lÇ hÃ¹dÃ²ngshÃ¬ guÄ«huÃ  rÃ¨nwÃ¹. RÃ¡n'Ã©r, xiÃ nyÇ’u fÄngfÇ chÃ¡ngchÃ¡ng shÃ²udÃ o guÄ«huÃ  huÃ njuÃ© de kÃ¹nhuÃ², bÃ¬ngqiÄ› mÄ›i gÃ¨ xÄ«n dÃ ilÇ dÅu xÅ«yÃ o chÃ³ngxÄ«n xÃ¹nliÃ n. WÃ¨i jiÄ›juÃ© zhÃ¨xiÄ“ tiÇozhÃ n, wÇ’men tÃ­chÅ«le yuÃ¡n guÄ«huÃ  yÅuhuÃ  (MPO) kuÃ ngjiÃ , tÅngguÃ² zhÃ­jiÄ“ yÇnrÃ¹ xiÇnshÃ¬ zhÇdÇo lÃ¡i zÄ“ngqiÃ¡ng dÃ ilÇ de guÄ«huÃ  nÃ©nglÃ¬. MPO lÃ¬yÃ²ng yuÃ¡n guÄ«huÃ  tÃ­gÅng gÄo cÃ©ngcÃ¬ de tÅngyÃ²ng zhÇdÇo, bÄngzhÃ¹ dÃ ilÇ guÄ«huÃ , bÃ¬nggÄ“njÃ¹ rÃ¨nwÃ¹ zhÃ­xÃ­ng fÇnkuÃ¬ chÃ­xÃ¹ yÅuhuÃ  yuÃ¡n guÄ«huÃ . ShÃ­yÃ n biÇomÃ­ng, MPO zÃ i liÇng xiÃ ng dÃ ibiÇoxÃ¬ng rÃ¨nwÃ¹ zhÅng xiÇnzhÃ¹ yÅuhuÃ n xiÃ nzhÃ¹n bÇzhÇ”n, bÃ¬ng tÃ­gÄole rÃ¨nwÃ¹ wÃ¡nchÃ©ng xiÃ olÇœ hÃ© fÃ nhuÃ  nÃ©nglÃ¬.",
        "vocab": "[{'word': 'å¤§è¯­è¨€æ¨¡å‹', 'pinyin': 'dÃ  yÇ”yÃ¡n mÃ³xÃ­ng', 'trans': 'large language model'},\n{'word': 'åŸºäº', 'pinyin': 'jÄ«yÃº', 'trans': 'based on'},\n{'word': 'ä»£ç†', 'pinyin': 'dÃ ilÇ', 'trans': 'agent'},\n{'word': 'äº’åŠ¨å¼', 'pinyin': 'hÃ¹dÃ²ngshÃ¬', 'trans': 'interactive'},\n{'word': 'è§„åˆ’', 'pinyin': 'guÄ«huÃ ', 'trans': 'planning'},\n{'word': 'ä»»åŠ¡', 'pinyin': 'rÃ¨nwÃ¹', 'trans': 'task'},\n{'word': 'å¹»è§‰', 'pinyin': 'huÃ njuÃ©', 'trans': 'hallucination'},\n{'word': 'å›°æ‰°', 'pinyin': 'kÃ¹nrÇo', 'trans': 'trouble'},\n{'word': 'é‡æ–°', 'pinyin': 'chÃ³ngxÄ«n', 'trans': 'renew'},\n{'word': 'è®­ç»ƒ', 'pinyin': 'xÃ¹nliÃ n', 'trans': 'training'},\n{'word': 'æŒ‘æˆ˜', 'pinyin': 'tiÇozhÃ n', 'trans': 'challenge'},\n{'word': 'æå‡º', 'pinyin': 'tÃ­chÅ«', 'trans': 'propose'},\n{'word': 'æ¡†æ¶', 'pinyin': 'kuÃ ngjiÃ ', 'trans': 'framework'},\n{'word': 'æ˜¾å¼', 'pinyin': 'xiÇnshÃ¬', 'trans': 'explicit'},\n{'word': 'æŒ‡å¯¼', 'pinyin': 'zhÇdÇo', 'trans': 'guidance'},\n{'word': 'å¢å¼º', 'pinyin': 'zÄ“ngqiÃ¡ng', 'trans': 'enhance'},\n{'word': 'èƒ½åŠ›', 'pinyin': 'nÃ©nglÃ¬', 'trans': 'ability'},\n{'word': 'åˆ©ç”¨', 'pinyin': 'lÃ¬yÃ²ng', 'trans': 'utilize'},\n{'word': 'æä¾›', 'pinyin': 'tÃ­gÅng', 'trans': 'provide'},\n{'word': 'é«˜å±‚æ¬¡', 'pinyin': 'gÄo cÃ©ngcÃ¬', 'trans': 'high-level'},\n{'word': 'é€šç”¨', 'pinyin': 'tÅngyÃ²ng', 'trans': 'general'},\n{'word': 'å¸®åŠ©', 'pinyin': 'bÄngzhÃ¹', 'trans': 'help'},\n{'word': 'æ‰§è¡Œ', 'pinyin': 'zhÃ­xÃ­ng', 'trans': 'execute'},\n{'word': 'åé¦ˆ', 'pinyin': 'fÇnkuÃ¬', 'trans': 'feedback'},\n{'word': 'æŒç»­', 'pinyin': 'chÃ­xÃ¹', 'trans': 'continuous'},\n{'word': 'ä¼˜åŒ–', 'pinyin': 'yÅuhuÃ ', 'trans': 'optimize'},\n{'word': 'å®éªŒ', 'pinyin': 'shÃ­yÃ n', 'trans': 'experiment'},\n{'word': 'è¡¨æ˜', 'pinyin': 'biÇomÃ­ng', 'trans': 'indicate'},\n{'word': 'ä»£è¡¨æ€§', 'pinyin': 'dÃ ibiÇoxÃ¬ng', 'trans': 'representative'},\n{'word': 'æ˜¾è‘—', 'pinyin': 'xiÇnzhÃ¹', 'trans': 'significant'},\n{'word': 'ä¼˜äº', 'pinyin': 'yÅuyÃº', 'trans': 'superior to'},\n{'word': 'ç°æœ‰', 'pinyin': 'xiÃ nyÇ’u', 'trans': 'existing'},\n{'word': 'åŸºå‡†', 'pinyin': 'jÄ«zhÇ”n', 'trans': 'benchmark'},\n{'word': 'æé«˜', 'pinyin': 'tÃ­gÄo', 'trans': 'improve'},\n{'word': 'å®Œæˆ', 'pinyin': 'wÃ¡nchÃ©ng', 'trans': 'complete'},\n{'word': 'æ•ˆç‡', 'pinyin': 'xiÃ olÇœ', 'trans': 'efficiency'},\n{'word': 'æ³›åŒ–', 'pinyin': 'fÃ nhuÃ ', 'trans': 'generalize'}]",
        "trans": "Recent advancements in large language models have enabled LLM-based agents to successfully handle interactive planning tasks. However, existing methods often suffer from planning hallucinations, and each new agent requires retraining. To address these challenges, we propose the Meta-Planning Optimization (MPO) framework, which enhances the agent's planning capability by directly introducing explicit guidance. MPO leverages meta-planning to provide high-level, general guidance to assist the agent in planning and continuously optimizes the meta-planning based on task execution feedback. Experiments demonstrate that MPO significantly outperforms existing benchmarks in two representative tasks and improves task completion efficiency and generalization ability.",
        "update_ts": "2025-03-05 09:11"
    }
}