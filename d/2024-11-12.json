{
    "date": {
        "ru": "12 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
        "en": "November 12",
        "zh": "11æœˆ12æ—¥"
    },
    "time_utc": "2024-11-12 04:14",
    "weekday": 1,
    "issue_id": 521,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.07126",
            "title": "Edify Image: High-Quality Image Generation with Pixel Space Laplacian Diffusion Models",
            "url": "https://huggingface.co/papers/2411.07126",
            "abstract": "We introduce Edify Image, a family of diffusion models capable of generating photorealistic image content with pixel-perfect accuracy. Edify Image utilizes cascaded pixel-space diffusion models trained using a novel Laplacian diffusion process, in which image signals at different frequency bands are attenuated at varying rates. Edify Image supports a wide range of applications, including text-to-image synthesis, 4K upsampling, ControlNets, 360 HDR panorama generation, and finetuning for image customization.",
            "score": 2,
            "issue_id": 521,
            "pub_date": "2024-11-11",
            "pub_date_card": {
                "ru": "11 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 11",
                "zh": "11æœˆ11æ—¥"
            },
            "hash": "a7486a925b416669",
            "data": {
                "categories": [
                    "#3d",
                    "#diffusion",
                    "#cv"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ¤Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ",
                    "desc": "Edify Image - ÑÑ‚Ğ¾ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚ Ñ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ°ÑĞºĞ°Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ»Ğ°Ğ¿Ğ»Ğ°ÑĞ¾Ğ²ÑĞºĞ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. Ğ’ ÑÑ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½Ğ°Ñ… Ğ·Ğ°Ñ‚ÑƒÑ…Ğ°ÑÑ‚ Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒÑ. Edify Image Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¹ ÑĞ¿ĞµĞºÑ‚Ñ€ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ, Ğ°Ğ¿ÑĞºĞµĞ¹Ğ»Ğ¸Ğ½Ğ³ Ğ´Ğ¾ 4K, ControlNets Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼ HDR 360Â°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ»ÑÑ‚ÑŒ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ´Ğ»Ñ ĞºĞ°ÑÑ‚Ğ¾Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Edify Image: Revolutionizing Photorealistic Image Generation with Precision",
                    "desc": "Edify Image is a new set of diffusion models designed to create highly realistic images with precise detail. It employs a unique Laplacian diffusion process that adjusts the diffusion rates for different frequency bands of image signals. This allows for versatile applications such as generating images from text, enhancing image resolution to 4K, and creating panoramic images. Additionally, it offers customization options through finetuning, making it adaptable for various image generation tasks."
                },
                "zh": {
                    "title": "Edify Imageï¼šç”ŸæˆçœŸå®æ„Ÿå›¾åƒçš„æ–°çªç ´",
                    "desc": "Edify Imageæ˜¯ä¸€ç§æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿç”Ÿæˆåƒç´ çº§ç²¾ç¡®çš„çœŸå®æ„Ÿå›¾åƒå†…å®¹ã€‚å®ƒé‡‡ç”¨çº§è”åƒç´ ç©ºé—´æ‰©æ•£æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨æ–°é¢–çš„æ‹‰æ™®æ‹‰æ–¯æ‰©æ•£è¿‡ç¨‹è¿›è¡Œè®­ç»ƒï¼Œèƒ½å¤Ÿä»¥ä¸åŒçš„é€Ÿç‡è¡°å‡ä¸åŒé¢‘ç‡å¸¦çš„å›¾åƒä¿¡å·ã€‚è¯¥æ¨¡å‹æ”¯æŒå¤šç§åº”ç”¨ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ°å›¾åƒåˆæˆã€4Kè¶…åˆ†è¾¨ç‡ã€ControlNetsã€360 HDRå…¨æ™¯ç”Ÿæˆä»¥åŠå›¾åƒå®šåˆ¶çš„å¾®è°ƒã€‚Edify Imageåœ¨å›¾åƒç”Ÿæˆé¢†åŸŸå±•ç°äº†å¼ºå¤§çš„çµæ´»æ€§å’Œé«˜è´¨é‡çš„è¾“å‡ºã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.06176",
            "title": "M-Longdoc: A Benchmark For Multimodal Super-Long Document Understanding And A Retrieval-Aware Tuning Framework",
            "url": "https://huggingface.co/papers/2411.06176",
            "abstract": "The ability to understand and answer questions over documents can be useful in many business and practical applications. However, documents often contain lengthy and diverse multimodal contents such as texts, figures, and tables, which are very time-consuming for humans to read thoroughly. Hence, there is an urgent need to develop effective and automated methods to aid humans in this task. In this work, we introduce M-LongDoc, a benchmark of 851 samples, and an automated framework to evaluate the performance of large multimodal models. We further propose a retrieval-aware tuning approach for efficient and effective multimodal document reading. Compared to existing works, our benchmark consists of more recent and lengthy documents with hundreds of pages, while also requiring open-ended solutions and not just extractive answers. To our knowledge, our training framework is the first to directly address the retrieval setting for multimodal long documents. To enable tuning open-source models, we construct a training corpus in a fully automatic manner for the question-answering task over such documents. Experiments show that our tuning approach achieves a relative improvement of 4.6% for the correctness of model responses, compared to the baseline open-source models. Our data, code, and models are available at https://multimodal-documents.github.io.",
            "score": 2,
            "issue_id": 521,
            "pub_date": "2024-11-09",
            "pub_date_card": {
                "ru": "9 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 9",
                "zh": "11æœˆ9æ—¥"
            },
            "hash": "950719af940fd8d0",
            "data": {
                "categories": [
                    "#benchmark",
                    "#long_context",
                    "#dataset",
                    "#multimodal",
                    "#open_source",
                    "#training"
                ],
                "emoji": "ğŸ“„",
                "ru": {
                    "title": "M-LongDoc: Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ M-LongDoc - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ…. ĞĞ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 851 Ğ¾Ğ±Ñ€Ğ°Ğ·ĞµÑ† ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ…, Ğ° Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑĞºÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° 4.6% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼."
                },
                "en": {
                    "title": "Enhancing Multimodal Document Understanding with M-LongDoc",
                    "desc": "This paper presents M-LongDoc, a benchmark designed to evaluate large multimodal models on lengthy documents that include text, figures, and tables. The authors introduce a retrieval-aware tuning method that enhances the efficiency and effectiveness of multimodal document reading, particularly for open-ended question-answering tasks. Unlike previous benchmarks, M-LongDoc features more recent and extensive documents, requiring models to generate comprehensive answers rather than just extractive responses. Experimental results indicate that the proposed tuning approach improves the accuracy of model responses by 4.6% compared to existing baseline models."
                },
                "zh": {
                    "title": "æå‡å¤šæ¨¡æ€æ–‡æ¡£ç†è§£çš„æ•ˆç‡ä¸æ•ˆæœ",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºM-LongDocçš„åŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«851ä¸ªæ ·æœ¬ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨æ–‡æ¡£ç†è§£å’Œé—®ç­”ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ç”±äºæ–‡æ¡£é€šå¸¸åŒ…å«æ–‡æœ¬ã€å›¾å½¢å’Œè¡¨æ ¼ç­‰å¤šç§å†…å®¹ï¼Œäººå·¥é˜…è¯»è€—æ—¶è¾ƒé•¿ï¼Œå› æ­¤éœ€è¦å¼€å‘æœ‰æ•ˆçš„è‡ªåŠ¨åŒ–æ–¹æ³•æ¥è¾…åŠ©äººç±»ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ£€ç´¢æ„ŸçŸ¥çš„è°ƒä¼˜æ–¹æ³•ï¼Œä»¥æé«˜å¤šæ¨¡æ€æ–‡æ¡£é˜…è¯»çš„æ•ˆç‡å’Œæ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ¨¡å‹å“åº”çš„æ­£ç¡®æ€§ä¸Šç›¸è¾ƒäºåŸºçº¿å¼€æºæ¨¡å‹æœ‰4.6%çš„ç›¸å¯¹æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.06208",
            "title": "IOPO: Empowering LLMs with Complex Instruction Following via Input-Output Preference Optimization",
            "url": "https://huggingface.co/papers/2411.06208",
            "abstract": "In the realm of large language models (LLMs), the ability of models to accurately follow instructions is paramount as more agents and applications leverage LLMs for construction, where the complexity of instructions are rapidly increasing. However, on the one hand, there is only a certain amount of complex instruction evaluation data; on the other hand, there are no dedicated algorithms to improve the ability to follow complex instructions. To this end, this paper introduces TRACE, a benchmark for improving and evaluating the complex instructionfollowing ability, which consists of 120K training data and 1K evaluation data. Furthermore, we propose IOPO (Input-Output Preference Optimization) alignment method which takes both input and output preference pairs into consideration, where LLMs not only rapidly align with response preferences but also meticulously explore the instruction preferences. Extensive experiments on both in-domain and outof-domain datasets confirm the effectiveness of IOPO, showing 8.15%, 2.18% improvements on in-domain data and 6.29%, 3.13% on outof-domain data compared to SFT and DPO respectively.",
            "score": 1,
            "issue_id": 521,
            "pub_date": "2024-11-09",
            "pub_date_card": {
                "ru": "9 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 9",
                "zh": "11æœˆ9æ—¥"
            },
            "hash": "de83b5a8e14da36e",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#alignment",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "TRACE Ğ¸ IOPO: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ TRACE - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ IOPO (Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ²Ğ¾Ğ´Ğ°-Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°) Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. TRACE Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 120 Ñ‚Ñ‹ÑÑÑ‡ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¸ 1000 Ğ¾Ñ†ĞµĞ½Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ ĞºĞ°Ğº Ğ½Ğ° Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ° ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Enhancing LLMs with TRACE and IOPO for Complex Instructions",
                    "desc": "This paper addresses the challenge of large language models (LLMs) in following complex instructions, which is becoming increasingly important as their applications grow. It introduces TRACE, a benchmark designed to enhance and evaluate the ability of LLMs to handle complex instructions, featuring a substantial dataset of 120K training examples and 1K evaluation cases. The authors propose a novel alignment method called IOPO (Input-Output Preference Optimization), which focuses on both input and output preferences to improve LLM responses. Experimental results demonstrate that IOPO significantly enhances performance on both in-domain and out-of-domain datasets, outperforming existing methods like SFT and DPO."
                },
                "zh": {
                    "title": "æå‡å¤æ‚æŒ‡ä»¤è·Ÿéšèƒ½åŠ›çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é¢†åŸŸï¼Œæ¨¡å‹å‡†ç¡®éµå¾ªæŒ‡ä»¤çš„èƒ½åŠ›è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨æŒ‡ä»¤å¤æ‚æ€§è¿…é€Ÿå¢åŠ çš„æƒ…å†µä¸‹ã€‚æœ¬æ–‡æå‡ºäº†TRACEï¼Œä¸€ä¸ªç”¨äºæé«˜å’Œè¯„ä¼°å¤æ‚æŒ‡ä»¤è·Ÿéšèƒ½åŠ›çš„åŸºå‡†ï¼ŒåŒ…å«12ä¸‡æ¡è®­ç»ƒæ•°æ®å’Œ1000æ¡è¯„ä¼°æ•°æ®ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†IOPOï¼ˆè¾“å…¥-è¾“å‡ºåå¥½ä¼˜åŒ–ï¼‰å¯¹é½æ–¹æ³•ï¼Œè€ƒè™‘äº†è¾“å…¥å’Œè¾“å‡ºåå¥½å¯¹ï¼Œå¸®åŠ©LLMså¿«é€Ÿå¯¹é½å“åº”åå¥½å¹¶æ·±å…¥æ¢ç´¢æŒ‡ä»¤åå¥½ã€‚é€šè¿‡åœ¨é¢†åŸŸå†…å’Œé¢†åŸŸå¤–æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒï¼ŒéªŒè¯äº†IOPOçš„æœ‰æ•ˆæ€§ï¼Œæ˜¾ç¤ºå‡ºåœ¨é¢†åŸŸå†…æ•°æ®ä¸Šåˆ†åˆ«æé«˜äº†8.15%å’Œ2.18%ï¼Œåœ¨é¢†åŸŸå¤–æ•°æ®ä¸Šæé«˜äº†6.29%å’Œ3.13%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.06272",
            "title": "Golden Touchstone: A Comprehensive Bilingual Benchmark for Evaluating Financial Large Language Models",
            "url": "https://huggingface.co/papers/2411.06272",
            "abstract": "As large language models become increasingly prevalent in the financial sector, there is a pressing need for a standardized method to comprehensively assess their performance. However, existing finance benchmarks often suffer from limited language and task coverage, as well as challenges such as low-quality datasets and inadequate adaptability for LLM evaluation. To address these limitations, we propose \"Golden Touchstone\", the first comprehensive bilingual benchmark for financial LLMs, which incorporates representative datasets from both Chinese and English across eight core financial NLP tasks. Developed from extensive open source data collection and industry-specific demands, this benchmark includes a variety of financial tasks aimed at thoroughly assessing models' language understanding and generation capabilities. Through comparative analysis of major models on the benchmark, such as GPT-4o Llama3, FinGPT and FinMA, we reveal their strengths and limitations in processing complex financial information. Additionally, we open-sourced Touchstone-GPT, a financial LLM trained through continual pre-training and financial instruction tuning, which demonstrates strong performance on the bilingual benchmark but still has limitations in specific tasks.This research not only provides the financial large language models with a practical evaluation tool but also guides the development and optimization of future research. The source code for Golden Touchstone and model weight of Touchstone-GPT have been made publicly available at https://github.com/IDEA-FinAI/Golden-Touchstone, contributing to the ongoing evolution of FinLLMs and fostering further research in this critical area.",
            "score": 1,
            "issue_id": 520,
            "pub_date": "2024-11-09",
            "pub_date_card": {
                "ru": "9 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 9",
                "zh": "11æœˆ9æ—¥"
            },
            "hash": "2559c023f673c9b4",
            "data": {
                "categories": [
                    "#low_resource",
                    "#optimization",
                    "#open_source",
                    "#multilingual",
                    "#benchmark"
                ],
                "emoji": "ğŸ’¹",
                "ru": {
                    "title": "Ğ­Ñ‚Ğ°Ğ»Ğ¾Ğ½ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ĞµÑ‚ÑÑ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¼ ÑĞµĞºÑ‚Ğ¾Ñ€Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ \"Golden Touchstone\", Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½ Ğ´Ğ»Ñ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ¼ Ğ¸ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑŒĞ¼Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ NLP. Ğ­Ñ‚Ğ¾Ñ‚ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ Ğº ĞºĞ¾Ğ´Ñƒ Ğ¸ Ğ²ĞµÑĞ°Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Touchstone-GPT, Ñ‡Ñ‚Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞ¼Ñƒ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… LLM."
                },
                "en": {
                    "title": "Golden Touchstone: Elevating Financial LLM Evaluation",
                    "desc": "This paper introduces 'Golden Touchstone', a new bilingual benchmark designed to evaluate the performance of large language models (LLMs) in the financial sector. It addresses the shortcomings of existing benchmarks by providing a comprehensive assessment across eight key financial NLP tasks in both Chinese and English. The benchmark is built from high-quality datasets and reflects industry needs, allowing for a thorough evaluation of models like GPT-4o, Llama3, FinGPT, and FinMA. Additionally, the paper presents Touchstone-GPT, a financial LLM that has been fine-tuned for better performance on this benchmark, while also making the resources publicly available to support further research in financial LLMs."
                },
                "zh": {
                    "title": "é‡‘èé¢†åŸŸçš„æ ‡å‡†åŒ–è¯„ä¼°å·¥å…·â€”â€”é‡‘è‰²åŸºå‡†",
                    "desc": "éšç€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é‡‘èé¢†åŸŸçš„å¹¿æ³›åº”ç”¨ï¼Œè¯„ä¼°å…¶æ€§èƒ½çš„æ ‡å‡†åŒ–æ–¹æ³•å˜å¾—å°¤ä¸ºé‡è¦ã€‚ç°æœ‰çš„é‡‘èåŸºå‡†æµ‹è¯•å­˜åœ¨è¯­è¨€å’Œä»»åŠ¡è¦†ç›–é¢æœ‰é™ã€æ•°æ®é›†è´¨é‡ä½ä»¥åŠé€‚åº”æ€§ä¸è¶³ç­‰é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†â€œé‡‘è‰²åŸºå‡†â€ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå…¨é¢çš„åŒè¯­é‡‘èåŸºå‡†ï¼Œæ¶µç›–äº†ä¸­è‹±æ–‡çš„å…«ä¸ªæ ¸å¿ƒé‡‘èè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ã€‚é€šè¿‡å¯¹ä¸»è¦æ¨¡å‹çš„æ¯”è¾ƒåˆ†æï¼Œæˆ‘ä»¬æ­ç¤ºäº†å®ƒä»¬åœ¨å¤„ç†å¤æ‚é‡‘èä¿¡æ¯æ—¶çš„ä¼˜ç¼ºç‚¹ï¼Œå¹¶å¼€æºäº†Touchstone-GPTæ¨¡å‹ï¼Œä»¥ä¿ƒè¿›æœªæ¥çš„ç ”ç©¶å’Œä¼˜åŒ–ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.07180",
            "title": "Counterfactual Generation from Language Models",
            "url": "https://huggingface.co/papers/2411.07180",
            "abstract": "Understanding and manipulating the causal generation mechanisms in language models is essential for controlling their behavior. Previous work has primarily relied on techniques such as representation surgery -- e.g., model ablations or manipulation of linear subspaces tied to specific concepts -- to intervene on these models. To understand the impact of interventions precisely, it is useful to examine counterfactuals -- e.g., how a given sentence would have appeared had it been generated by the model following a specific intervention. We highlight that counterfactual reasoning is conceptually distinct from interventions, as articulated in Pearl's causal hierarchy. Based on this observation, we propose a framework for generating true string counterfactuals by reformulating language models as Generalized Structural-equation. Models using the Gumbel-max trick. This allows us to model the joint distribution over original strings and their counterfactuals resulting from the same instantiation of the sampling noise. We develop an algorithm based on hindsight Gumbel sampling that allows us to infer the latent noise variables and generate counterfactuals of observed strings. Our experiments demonstrate that the approach produces meaningful counterfactuals while at the same time showing that commonly used intervention techniques have considerable undesired side effects.",
            "score": 0,
            "issue_id": 521,
            "pub_date": "2024-11-11",
            "pub_date_card": {
                "ru": "11 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 11",
                "zh": "11æœˆ11æ—¥"
            },
            "hash": "6b57fa07bdf242ce",
            "data": {
                "categories": [
                    "#math",
                    "#interpretability",
                    "#reasoning",
                    "#training",
                    "#data"
                ],
                "emoji": "ğŸ”€",
                "ru": {
                    "title": "ĞšĞ¾Ğ½Ñ‚Ñ€Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ²ÑĞ·Ğ¸",
                    "desc": "Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ°Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ½Ñ‹Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ñ€ÑĞº Ğ“ÑƒĞ¼Ğ±ĞµĞ»Ñ-Ğ¼Ğ°ĞºÑĞ°. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ‚Ñ€Ğ¾Ğº Ğ¸ Ğ¸Ñ… ĞºĞ¾Ğ½Ñ‚Ñ€Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ¾Ğ². Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞµ Ğ“ÑƒĞ¼Ğ±ĞµĞ»Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ÑˆÑƒĞ¼Ğ° Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ½Ñ‚Ñ€Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ´Ğ»Ñ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ñ‹Ñ… ÑÑ‚Ñ€Ğ¾Ğº."
                },
                "en": {
                    "title": "Harnessing Counterfactuals for Better Control of Language Models",
                    "desc": "This paper focuses on understanding how to control language models by manipulating their causal generation mechanisms. It critiques existing methods like representation surgery, which alter model behavior but may not provide precise insights into their effects. The authors introduce a new framework that uses counterfactual reasoning to generate true string counterfactuals, distinguishing it from traditional interventions. Their approach employs Generalized Structural-equation Models and Gumbel-max sampling to effectively model the relationship between original strings and their counterfactuals, revealing the limitations of current intervention techniques."
                },
                "zh": {
                    "title": "æŒæ¡è¯­è¨€æ¨¡å‹çš„å› æœç”Ÿæˆæœºåˆ¶",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†åœ¨è¯­è¨€æ¨¡å‹ä¸­ç†è§£å’Œæ“æ§å› æœç”Ÿæˆæœºåˆ¶çš„é‡è¦æ€§ã€‚ä»¥å¾€çš„ç ”ç©¶ä¸»è¦ä¾èµ–äºè¡¨ç¤ºæ‰‹æœ¯ç­‰æŠ€æœ¯æ¥å¹²é¢„æ¨¡å‹ï¼Œä½†æˆ‘ä»¬å¼ºè°ƒåäº‹å®æ¨ç†ä¸å¹²é¢„æ˜¯ä¸åŒçš„æ¦‚å¿µã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¡†æ¶ï¼Œé€šè¿‡å°†è¯­è¨€æ¨¡å‹é‡æ„ä¸ºå¹¿ä¹‰ç»“æ„æ–¹ç¨‹æ¨¡å‹ï¼Œç”ŸæˆçœŸå®çš„å­—ç¬¦ä¸²åäº‹å®ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆæœ‰æ„ä¹‰çš„åäº‹å®ï¼ŒåŒæ—¶æ­ç¤ºäº†å¸¸ç”¨å¹²é¢„æŠ€æœ¯çš„æ˜¾è‘—å‰¯ä½œç”¨ã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-11-11.html",
    "link_next": "2024-11-13.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "11.11",
        "en": "11/11",
        "zh": "11æœˆ11æ—¥"
    },
    "short_date_next": {
        "ru": "13.11",
        "en": "11/13",
        "zh": "11æœˆ13æ—¥"
    },
    "categories": {
        "#dataset": 2,
        "#data": 1,
        "#benchmark": 3,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 3,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 1
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†StdGENï¼Œä¸€ç§åˆ›æ–°çš„ä»å•å¼ å›¾åƒç”Ÿæˆé«˜è´¨é‡3Dè§’è‰²çš„æ–¹æ³•ã€‚å®ƒèƒ½åœ¨ä¸‰åˆ†é’Ÿå†…ç”Ÿæˆå…·æœ‰åˆ†ç¦»çš„è¯­ä¹‰ç»„ä»¶ï¼ˆå¦‚èº«ä½“ã€è¡£æœå’Œå¤´å‘ï¼‰çš„è¯¦ç»†3Dè§’è‰²ã€‚StdGENçš„æ ¸å¿ƒæ˜¯æå‡ºçš„è¯­ä¹‰æ„ŸçŸ¥å¤§å‹é‡å»ºæ¨¡å‹ï¼ˆS-LRMï¼‰ï¼Œèƒ½å¤Ÿä»å¤šè§†å›¾å›¾åƒä¸­é‡å»ºå‡ ä½•ã€é¢œè‰²å’Œè¯­ä¹‰ã€‚å®éªŒè¯æ˜ï¼ŒStdGENåœ¨3DåŠ¨æ¼«è§’è‰²ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚å®ƒä¸ºè™šæ‹Ÿç°å®ã€æ¸¸æˆå’Œç”µå½±åˆ¶ä½œç­‰æä¾›äº†çµæ´»çš„å®šåˆ¶åŒ–3Dè§’è‰²ã€‚",
        "title": "StdGEN: Semantic-Decomposed 3D Character Generation from Single Images",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†StdGENï¼Œä¸€ç§åˆ›æ–°çš„ä»å•å¼ å›¾åƒç”Ÿæˆé«˜è´¨é‡3Dè§’è‰²çš„æ–¹æ³•ã€‚å®ƒèƒ½åœ¨ä¸‰åˆ†é’Ÿå†…ç”Ÿæˆå…·æœ‰åˆ†ç¦»çš„è¯­ä¹‰ç»„ä»¶ï¼ˆå¦‚èº«ä½“ã€è¡£æœå’Œå¤´å‘ï¼‰çš„è¯¦ç»†3Dè§’è‰²ã€‚StdGENçš„æ ¸å¿ƒæ˜¯æå‡ºçš„è¯­ä¹‰æ„ŸçŸ¥å¤§å‹é‡å»ºæ¨¡å‹ï¼ˆS-LRMï¼‰ï¼Œèƒ½å¤Ÿä»å¤šè§†å›¾å›¾åƒä¸­é‡å»ºå‡ ä½•ã€é¢œè‰²å’Œè¯­ä¹‰ã€‚å®éªŒè¯æ˜ï¼ŒStdGENåœ¨3DåŠ¨æ¼«è§’è‰²ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚å®ƒä¸ºè™šæ‹Ÿç°å®ã€æ¸¸æˆå’Œç”µå½±åˆ¶ä½œç­‰æä¾›äº†çµæ´»çš„å®šåˆ¶åŒ–3Dè§’è‰²ã€‚\n\nzhÃ¨ piÄn wÃ©n zhÄng jiÃ¨ shÃ o le StdGEN, yÄ« zhÇ’ng chuÃ ng xÄ«n de cÃ³ng dÄn zhÄng tÃº xiÃ ng shÄ“ng chÃ©ng gÄo zhÃ¬ liÃ ng 3D juÃ© sÃ¨ de fÄng fÇ. tÄ nÃ©ng zÃ i sÄn fÄ“n zhÅng nÃ¨i shÄ“ng chÃ©ng jÃ¹ yÇ’u fÄ“n lÃ¬ de yÇ” yÃ¬ zÇ” jÃ¬n (rÃº shÄ“n tÇ, yÄ« fÃº hÃ© tÃ³u fÃ ) de xiÃ¡ng xÃ¬ 3D juÃ© sÃ¨. StdGEN de hÃ© xÄ«n shÃ¬ tÃ­ chÅ« de yÇ” yÃ¬ gÇn juÃ© dÃ  xÃ­ng chÃ³ng jiÃ n mÃ³ xÃ­ng (S-LRM), nÃ©ng gÃ²u cÃ³ng duÅ shÃ¬ jÃ¬ tÃº xiÃ ng zhÅng chÃ³ng jiÃ n jÇ hÃ©, yÃ¡n sÃ¨ hÃ© yÇ” yÃ¬. shÃ­ yÃ n zhÃ¨ng mÃ­ng, StdGEN zÃ i 3D dÃ²ng mÃ n juÃ© sÃ¨ shÄ“ng chÃ©ng fÄng miÃ n biÇo xiÇn chÅ« sÃ¨, yÅu yÃº xiÃ n yÇ’u fÄng fÇ. tÄ wÃ¨i xÅ« nÇ xiÃ n shÃ­, yÃ³u xÃ¬ hÃ© diÃ n yÇng zhÃ¬ zuÃ² dÄ›ng ti gÅng gÄ›i le lÃ­nghuÃ³ de dÃ¬ng zhÃ¬ huÃ  3D juÃ© sÃ¨.",
        "vocab": "[\n    {\"word\": \"StdGEN\", \"pinyin\": \"sÄ«tÄ«dÄ« jÄ«n\", \"trans\": \"a method for generating high-quality 3D characters from a single image\"},\n    {\"word\": \"åˆ›æ–°\", \"pinyin\": \"chuÃ ngxÄ«n\", \"trans\": \"innovative\"},\n    {\"word\": \"è§’è‰²\", \"pinyin\": \"juÃ©sÃ¨\", \"trans\": \"character\"},\n    {\"word\": \"è¯­ä¹‰\", \"pinyin\": \"yÇ”yÃ¬\", \"trans\": \"semantic\"},\n    {\"word\": \"ç»„ä»¶\", \"pinyin\": \"zÇ”jiÃ n\", \"trans\": \"component\"},\n    {\"word\": \"å‡ ä½•\", \"pinyin\": \"jÇhÃ©\", \"trans\": \"geometry\"},\n    {\"word\": \"é‡å»º\", \"pinyin\": \"chÃ³ngjiÃ n\", \"trans\": \"reconstruct\"},\n    {\"word\": \"å¤šè§†å›¾\", \"pinyin\": \"duÅshÃ¬tÃº\", \"trans\": \"multi-view\"},\n    {\"word\": \"è¡¨ç°\", \"pinyin\": \"biÇoxiÃ n\", \"trans\": \"performance\"},\n    {\"word\": \"å‡ºè‰²\", \"pinyin\": \"chÅ«sÃ¨\", \"trans\": \"outstanding\"},\n    {\"word\": \"ç°æœ‰\", \"pinyin\": \"xiÃ nyÇ’u\", \"trans\": \"existing\"},\n    {\"word\": \"çµæ´»\", \"pinyin\": \"lÃ­nghuÃ³\", \"trans\": \"flexible\"},\n    {\"word\": \"å®šåˆ¶åŒ–\", \"pinyin\": \"dÃ¬ngzhÃ¬huÃ \", \"trans\": \"customized\"},\n    {\"word\": \"è™šæ‹Ÿç°å®\", \"pinyin\": \"xÅ«nÇ xiÃ nshÃ­\", \"trans\": \"virtual reality\"},\n    {\"word\": \"æ¸¸æˆ\", \"pinyin\": \"yÃ³uxÃ¬\", \"trans\": \"game\"},\n    {\"word\": \"ç”µå½±åˆ¶ä½œ\", \"pinyin\": \"diÃ nyÇng zhÃ¬zuÃ²\", \"trans\": \"film production\"}\n]",
        "trans": "This article introduces StdGEN, an innovative method for generating high-quality 3D characters from a single image. It can produce detailed 3D characters with separate semantic components (such as body, clothing, and hair) in just three minutes. The core of StdGEN is the proposed semantic-aware large reconstruction model (S-LRM), which can reconstruct geometry, color, and semantics from multi-view images. Experiments have shown that StdGEN performs excellently in generating 3D animated characters, outperforming existing methods. It provides flexible customization of 3D characters for virtual reality, gaming, and film production.",
        "update_ts": "2024-11-11 10:13"
    }
}