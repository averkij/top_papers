{
    "date": {
        "ru": "19 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
        "en": "November 19",
        "zh": "11æœˆ19æ—¥"
    },
    "time_utc": "2024-11-19 03:23",
    "weekday": 1,
    "issue_id": 651,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.07641",
            "title": "Top-$nÏƒ$: Not All Logits Are You Need",
            "url": "https://huggingface.co/papers/2411.07641",
            "abstract": "Large language models (LLMs) typically employ greedy decoding or low-temperature sampling for reasoning tasks, reflecting a perceived trade-off between diversity and accuracy. We challenge this convention by introducing top-nsigma, a novel sampling method that operates directly on pre-softmax logits by leveraging a statistical threshold. Our key insight is that logits naturally separate into a Gaussian-distributed noisy region and a distinct informative region, enabling efficient token filtering without complex probability manipulations. Unlike existing methods (e.g., top-p, min-p) that inadvertently include more noise tokens at higher temperatures, top-nsigma maintains a stable sampling space regardless of temperature scaling. We also provide a theoretical analysis of top-nsigma to better understand its behavior. The extensive experimental results across four reasoning-focused datasets demonstrate that our method not only outperforms existing sampling approaches but also surpasses greedy decoding, while maintaining consistent performance even at high temperatures.",
            "score": 0,
            "issue_id": 651,
            "pub_date": "2024-11-12",
            "pub_date_card": {
                "ru": "12 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 12",
                "zh": "11æœˆ12æ—¥"
            },
            "hash": "d3439bf0c336ac57",
            "authors": [
                "Chenxia Tang",
                "Jianchun Liu",
                "Hongli Xu",
                "Liusheng Huang"
            ],
            "affiliations": [
                "School of Computer Science and Technology, University of Science and Technology of China",
                "Suzhou Institute for Advanced Research, University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.07641.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Top-nsigma: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ top-nsigma. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ñ Ğ»Ğ¾Ğ³Ğ¸Ñ‚Ğ°Ğ¼Ğ¸ Ğ¿ĞµÑ€ĞµĞ´ ÑĞ¾Ñ„Ñ‚Ğ¼Ğ°ĞºÑĞ¾Ğ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ñ€Ğ¾Ğ³ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ ÑˆÑƒĞ¼Ğ¾Ğ²Ñ‹Ñ… Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Top-nsigma Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ±ĞµĞ· ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹ Ñ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¾Ñ‚ Ñ‚ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğº ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¶Ğ°Ğ´Ğ½Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Top-NSigma: Enhancing Sampling for Better Reasoning in LLMs",
                    "desc": "This paper introduces a new sampling method called top-nsigma for large language models (LLMs) that improves reasoning tasks. Unlike traditional methods that use greedy decoding or low-temperature sampling, top-nsigma filters tokens based on their pre-softmax logits, which are divided into informative and noisy regions. This approach allows for better token selection without the complications of probability adjustments, ensuring a stable sampling process across different temperature settings. Experimental results show that top-nsigma outperforms existing methods and maintains high performance even when using higher temperatures."
                },
                "zh": {
                    "title": "çªç ´ä¼ ç»Ÿï¼Œæå‡æ¨ç†æ€§èƒ½çš„top-nsigmaæ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„é‡‡æ ·æ–¹æ³•top-nsigmaï¼Œæ—¨åœ¨æ”¹å–„å¤§è¯­è¨€æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚è¯¥æ–¹æ³•ç›´æ¥åœ¨é¢„è½¯æœ€å¤§å€¼çš„logitsä¸Šæ“ä½œï¼Œé€šè¿‡ç»Ÿè®¡é˜ˆå€¼æ¥è¿›è¡Œæœ‰æ•ˆçš„ä»¤ç‰Œè¿‡æ»¤ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œlogitså¯ä»¥è‡ªç„¶åœ°åˆ†ä¸ºé«˜æ–¯åˆ†å¸ƒçš„å™ªå£°åŒºåŸŸå’Œä¿¡æ¯ä¸°å¯Œçš„åŒºåŸŸï¼Œä»è€Œé¿å…äº†å¤æ‚çš„æ¦‚ç‡æ“ä½œã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œtop-nsigmaåœ¨å¤šä¸ªæ¨ç†æ•°æ®é›†ä¸Šè¶…è¶Šäº†ç°æœ‰çš„é‡‡æ ·æ–¹æ³•å’Œè´ªå©ªè§£ç ï¼Œä¸”åœ¨é«˜æ¸©åº¦ä¸‹ä»èƒ½ä¿æŒç¨³å®šçš„æ€§èƒ½ã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-11-18.html",
    "link_next": "2024-11-20.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "18.11",
        "en": "11/18",
        "zh": "11æœˆ18æ—¥"
    },
    "short_date_next": {
        "ru": "20.11",
        "en": "11/20",
        "zh": "11æœˆ20æ—¥"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ä¸ªæ–°çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œç§°ä¸ºLLaVA-o1ï¼Œæ—¨åœ¨è¿›è¡Œè‡ªä¸»å¤šé˜¶æ®µæ¨ç†ã€‚ä¸é“¾å¼æ€ç»´æç¤ºä¸åŒï¼ŒLLaVA-o1ç‹¬ç«‹è¿›è¡Œæ€»ç»“ã€è§†è§‰è§£é‡Šã€é€»è¾‘æ¨ç†å’Œç»“è®ºç”Ÿæˆã€‚è¿™ç§ç»“æ„åŒ–æ–¹æ³•ä½¿å…¶åœ¨æ¨ç†å¯†é›†å‹ä»»åŠ¡ä¸Šå–å¾—æ˜¾è‘—ç²¾åº¦æå‡ã€‚ç ”ç©¶å›¢é˜Ÿç¼–åˆ¶äº†LLaVA-o1-100kæ•°æ®é›†ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ¨ç†æ—¶é˜¶æ®µçº§æŸæœç´¢æ–¹æ³•ï¼Œä»¥å®ç°æœ‰æ•ˆçš„æ¨ç†æ—¶æ‰©å±•ã€‚ç»“æœæ˜¾ç¤ºï¼ŒLLaVA-o1åœ¨å¤šæ¨¡æ€æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†å¤šä¸ªå¤§å‹å’Œå°é—­æºæ¨¡å‹ã€‚",
        "title": "LLaVA-o1: Let Vision Language Models Reason Step-by-Step",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ä¸ªæ–°çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œç§°ä¸ºLLaVA-o1ï¼Œæ—¨åœ¨è¿›è¡Œè‡ªä¸»å¤šé˜¶æ®µæ¨ç†ã€‚ä¸é“¾å¼æ€ç»´æç¤ºä¸åŒï¼ŒLLaVA-o1ç‹¬ç«‹è¿›è¡Œæ€»ç»“ã€è§†è§‰è§£é‡Šã€é€»è¾‘æ¨ç†å’Œç»“è®ºç”Ÿæˆã€‚è¿™ç§ç»“æ„åŒ–æ–¹æ³•ä½¿å…¶åœ¨æ¨ç†å¯†é›†å‹ä»»åŠ¡ä¸Šå–å¾—æ˜¾è‘—ç²¾åº¦æå‡ã€‚ç ”ç©¶å›¢é˜Ÿç¼–åˆ¶äº†LLaVA-o1-100kæ•°æ®é›†ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ¨ç†æ—¶é˜¶æ®µçº§æŸæœç´¢æ–¹æ³•ï¼Œä»¥å®ç°æœ‰æ•ˆçš„æ¨ç†æ—¶æ‰©å±•ã€‚ç»“æœæ˜¾ç¤ºï¼ŒLLaVA-o1åœ¨å¤šæ¨¡æ€æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†å¤šä¸ªå¤§å‹å’Œå°é—­æºæ¨¡å‹ã€‚\n\nzhÃ¨ piÄn wÃ©n zhÄng jiÃ¨ shÃ o le yÄ« gÃ¨ xÄ«n de shÃ¬ juÃ© yÇ” yÃ¡n mÃ³ xÃ¬ng (VLM)ï¼ŒchÄ“ng wÃ©i LLaVA-o1ï¼ŒzhÇ yÇn jÃ¬n xÃ­ng zÃ¬ zhÇ” duÅ jiÄ“ duÃ n tuÃ­ lÇ. yÇ” liÃ n shÃ¬ sÄ« wÃ©i tÃ­ shÃ¬ bÃ¹ tÃ³ngï¼ŒLLaVA-o1 dÃº lÃ¬ jÃ¬n xÃ­ng zÇ’ng jiÄ›ï¼ŒshÃ¬ juÃ© jiÄ› shÃ¬ï¼ŒluÃ³ ji tuÃ­ lÇ hÃ© jiÃ© lÃ¹n shÄ“ng chÃ©ng. zhÃ¨ zhÇ’ng jiÄ“ gÃ²u huÃ  fÇ shÇ qÃ­ zÃ i tuÃ­ lÇ mÃ¬ jÄ« xÃ­ng rÃ¨n wÃ¹ shÃ ng qÇ” dÃ© xiÇn zhÃ¹ jÄ«ng dÃ¹ tÃ­ shÄ“ng. yÃ¡n jiÅ« tuÃ¡n duÃ¬ biÄn zhÃ¬ le LLaVA-o1-100k shÃ¹ jÃ¹ jÃ­ï¼ŒbÃ¬ng tÃ­ chÅ« le yÄ« zhÇ’ng tuÃ­ lÇ shÃ­ jiÄ“ duÃ n jÃ­ shÃ¹ sÅu suÇ’ fÇï¼ŒyÇ shÃ­ xiÃ n yÃ¡n jiÅ« shÃ­ kuÃ² zhÇn. jiÃ© guÇ’ xiÇn shÃ¬ï¼ŒLLaVA-o1 zÃ i duÅ mÃ³ shuÃ i tuÃ­ lÇ jÄ« zhÇ”n cÃ¨ shÃ¬ zhÅng biÇo xiÃ n chÅ« sÃ¨ï¼ŒchÄo yuÃ¨ le duÅ gÃ¨ dÃ  xÃ­ng hÃ© fÄ“ng bÃ¬ yuÃ¡n mÃ³ xÃ¬ng.",
        "vocab": "[\n    {\"word\": \"è§†è§‰è¯­è¨€æ¨¡å‹\", \"pinyin\": \"shÃ¬juÃ© yÇ”yÃ¡n mÃ³xÃ­ng\", \"trans\": \"visual language model\"},\n    {\"word\": \"è‡ªä¸»\", \"pinyin\": \"zÃ¬zhÇ”\", \"trans\": \"autonomous\"},\n    {\"word\": \"å¤šé˜¶æ®µ\", \"pinyin\": \"duÅ jiÄ“duÃ n\", \"trans\": \"multi-stage\"},\n    {\"word\": \"æ¨ç†\", \"pinyin\": \"tuÄ«lÇ\", \"trans\": \"reasoning\"},\n    {\"word\": \"é“¾å¼\", \"pinyin\": \"liÃ nshÃ¬\", \"trans\": \"chain-like\"},\n    {\"word\": \"æç¤º\", \"pinyin\": \"tÃ­shÃ¬\", \"trans\": \"prompt\"},\n    {\"word\": \"ç‹¬ç«‹\", \"pinyin\": \"dÃºlÃ¬\", \"trans\": \"independent\"},\n    {\"word\": \"æ€»ç»“\", \"pinyin\": \"zÇ’ngjiÃ©\", \"trans\": \"summary\"},\n    {\"word\": \"è§†è§‰è§£é‡Š\", \"pinyin\": \"shÃ¬juÃ© jiÄ›shÃ¬\", \"trans\": \"visual explanation\"},\n    {\"word\": \"é€»è¾‘æ¨ç†\", \"pinyin\": \"luÃ³ji tuÄ«lÇ\", \"trans\": \"logical reasoning\"},\n    {\"word\": \"ç»“è®ºç”Ÿæˆ\", \"pinyin\": \"jiÃ©lÃ¹n shÄ“ngchÃ©ng\", \"trans\": \"conclusion generation\"},\n    {\"word\": \"ç»“æ„åŒ–\", \"pinyin\": \"jiÃ©gÃ²uhuÃ \", \"trans\": \"structured\"},\n    {\"word\": \"æ–¹æ³•\", \"pinyin\": \"fÄngfÇ\", \"trans\": \"method\"},\n    {\"word\": \"ç²¾åº¦\", \"pinyin\": \"jÄ«ngdÃ¹\", \"trans\": \"accuracy\"},\n    {\"word\": \"æå‡\", \"pinyin\": \"tÃ­shÄ“ng\", \"trans\": \"improvement\"},\n    {\"word\": \"ç ”ç©¶å›¢é˜Ÿ\", \"pinyin\": \"yÃ¡njiÅ« tuÃ¡nduÃ¬\", \"trans\": \"research team\"},\n    {\"word\": \"ç¼–åˆ¶\", \"pinyin\": \"biÄnzhÃ¬\", \"trans\": \"compile\"},\n    {\"word\": \"æ•°æ®é›†\", \"pinyin\": \"shÃ¹jÃ¹jÃ­\", \"trans\": \"dataset\"},\n    {\"word\": \"æ¨ç†æ—¶\", \"pinyin\": \"tuÄ«lÇ shÃ­\", \"trans\": \"during reasoning\"},\n    {\"word\": \"é˜¶æ®µçº§\", \"pinyin\": \"jiÄ“duÃ n jÃ­\", \"trans\": \"stage-level\"},\n    {\"word\": \"æŸæœç´¢\", \"pinyin\": \"shÃ¹ sÅusuÇ’\", \"trans\": \"beam search\"},\n    {\"word\": \"æ‰©å±•\", \"pinyin\": \"kuÃ²zhÇn\", \"trans\": \"expansion\"},\n    {\"word\": \"å¤šæ¨¡æ€\", \"pinyin\": \"duÅ mÃ³shÃ¬\", \"trans\": \"multimodal\"},\n    {\"word\": \"åŸºå‡†æµ‹è¯•\", \"pinyin\": \"jÄ«zhÇ”n cÃ¨shÃ¬\", \"trans\": \"benchmark test\"},\n    {\"word\": \"è¡¨ç°\", \"pinyin\": \"biÇoxiÃ n\", \"trans\": \"performance\"},\n    {\"word\": \"å‡ºè‰²\", \"pinyin\": \"chÅ«sÃ¨\", \"trans\": \"outstanding\"},\n    {\"word\": \"è¶…è¶Š\", \"pinyin\": \"chÄoyuÃ¨\", \"trans\": \"surpass\"},\n    {\"word\": \"å¤§å‹\", \"pinyin\": \"dÃ xÃ­ng\", \"trans\": \"large-scale\"},\n    {\"word\": \"å°é—­æº\", \"pinyin\": \"fÄ“ngbÃ¬ yuÃ¡n\", \"trans\": \"closed-source\"}\n]",
        "trans": "This article introduces a new visual language model (VLM) called LLaVA-o1, designed for autonomous multi-stage reasoning. Unlike chain-of-thought prompting, LLaVA-o1 independently performs summarization, visual explanation, logical reasoning, and conclusion generation. This structured approach achieves significant accuracy improvements in reasoning-intensive tasks. The research team compiled the LLaVA-o1-100k dataset and proposed a stage-level beam search method during inference to achieve effective reasoning-time expansion. The results show that LLaVA-o1 performs exceptionally well in multimodal reasoning benchmarks, outperforming several large and closed-source models.",
        "update_ts": "2024-11-18 09:12"
    }
}